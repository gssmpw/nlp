\section{Method}
\label{sec:method}

Our FDS regulates the optimization 
of Gaussian radiance field by incorporating 
matching priors from the pretrained deep model.
%
The generation of Radiance Flow and
our proposed FDS loss, along with the equipped camera sampling scheme,  
are detailed in 
\secref{sec:method:subsec:gs} and \secref{sec:method:fds}, 
respectively.
%

\subsection{3D Gaussian Splatting and Radiance Flow}
\label{sec:method:subsec:gs}

We utilize 3DGS as an example to demonstrate
how Radiance Flow is generated.
%
3DGS~\citep{kerbl20233d} employs a 
collection of 3D Gaussians to represent 
the 3D scene. 
%
The expression of $i$-th 3D Gaussian distribution shows below:

\begin{equation}
    \mathcal{G}_i(\bm{X}) 
    = e^{-\frac{1}{2} (\bm{X} - \bm{\mu_i})^T \bm{\Sigma_i} (\bm{X} - \bm{\mu_i})},
\end{equation}

where $\bm{\Sigma_i} = \bm{R_iS_i}\bm{S_i}^T\bm{R_i}^T$, 
and $\bm{\mu_i}$ represents the position of the Gaussian, which is optimized during training.
Both $\bm{S_i}$ and $\bm{R_i}$ are represented by 
a 3D scaling vector $\bm{s_i}$ and a quaternion $\bm{q_i}$, respectively.
%
In addition to the above parameters, each Gaussian $\bm{P_i}$ has extra learnable attributes, 
including opacity $\bm{\alpha_i}$ and color
feature $\bm{f_i}$.
%
To summarize, $\bm{P_i} = \{\bm{\mu_i}. \bm{s_i}, \bm{q_i}, \bm{\alpha_i}, \bm{f_i}\}$.
%

To render a color for pixel $x$, 
a volume rendering based method similar to NeRF~\citep{mildenhall2021nerf}
is adapted:
%

\begin{equation}
    \bm{C}(\bm{x}) = \sum_{i \in N} \bm{c_i} \bm{\hat{\alpha_i}} \prod_{j=1}^{i-1} (1-\bm{\hat{\alpha_j}}),
\end{equation}

where $\bm{c_i}$ is decoded by color feature $\bm{f_i}$ for each Gaussian points, $\hat{\alpha}$ is the blending weight for each 2D Gaussian point which is the projection of 3D Gaussian points on the image plane.
$N$ is the number of Gaussian points.
%
Similarly, the depth of pixel $x$ is rendered using alpha blending.

\begin{equation}
    \bm{D}(\bm{x}) = \frac{\sum_{i \in N} d_i 
    \bm{\hat{\alpha_i}} \prod_{j=1}^{i-1} (1-\bm{\hat{\alpha_j}})}{\sum_{i \in N}\bm{\hat{\alpha_i}} \prod_{j=1}^{i-1} (1-\bm{\hat{\alpha_j}})} ,
\end{equation}
where $d_i$ is the distance from the target camera to the Gaussian Point $\bm{P_i}$.
%
We can also render Radiance Flow between two views of camera $m, n$ using
their camera poses and position of Gaussian points $\bm{\mu_i}$.
%
% Initially, we project the center of each Gaussian point onto 
% the image plane for views $m$ and $n$, 
% utilizing the projection matrix $T_i$ from the $i$-th view. 
As mentioned above, we can project pixel $\bm{x} = (u_1, v_1)$ in $m$-th view image
to the $n$-th view by its corresponding depth and their pose transformation:
\begin{equation}
    \bm{D}^n(u_2 , v_2)
    \begin{bmatrix}
     u_2\\
     v_2\\
     1
    \end{bmatrix}
    = 
    \bm{K}\bm{T_{m}^{n}}\bm{K^{-1}}\bm{D}^m(u_1 , v_1)
    \begin{bmatrix}
     u_1\\
     v_1\\
     1
    \end{bmatrix},
    \label{equ:flow_pose}
\end{equation}

where $\bm{T_{m}^{n}}$ is the relative transformation from $m$-th view to $n$-th view, 
$\bm{K}$ is the intrinsic matrix, and $\bm{D^m}(u_1, v_1)$ is the rendered depth of $(u_1, v_1)$ in $m$-th view.

%
Next, we calculate $\bm{F^{m\rightarrow n}}(\bm{x})$ for pixel $\bm{x} = (u_1, v_1)$ from view $m$ to view $n$:

\begin{equation}
    \bm{F^{m\rightarrow n}}(\bm{x}) = 
    \begin{bmatrix}
     u_2 - u_1\\
     v_2 - v_1
    \end{bmatrix}. 
    \label{equ:flow}
\end{equation}

%
For other type of Gaussian radiance field such as 2DGS~\citep{huang20242d}, 
we only need to replace the alpha blending based depth $\bm{D}(\bm{x})$ with
corresponding formulation.

\subsection{Flow Distillation Sampling}
\label{sec:method:fds}
Given a collection of images $\{\bm{I^i}\}_{i=1,2,\dots N}$, 
Gaussian Radiance Field typically employs the following loss function for rendering optimization:

\begin{equation}
L = \frac{1}{B}\sum_{i=1}^B(1 - \lambda )L_1 + \lambda L_{D-SSIM} +
\lambda_{normal} L_{n} ,
\end{equation}

where $B$ denotes batch size, 
and $L_n$ represents the normal consistency loss from ~\citep{huang20242d}.
However, when $N$ is small, 
this representation suffers from
overfitting~\citep{li2024dngaussian, 
paliwal2024coherentgs}. 
%

\begin{wrapfigure}{htbp}{0.5\textwidth} \centering
% \begin{wrapfigure}{r}{0.38\textwidth} \centering
    \vspace{-2.0em}
    \hspace{2.0em}
    \includegraphics[trim={30pt 0pt 30pt 0pt}, width=0.5\textwidth,height=0.3\textwidth]{figure/camera_radius.pdf}
    % \caption{\textbf{Explanation of depth-adaptive translation radius.} A fixed-radius camera sampling scheme may result in significantly different flows (Flow 1 and Flow 2) in areas with varying depth (Area 1 and Area 2).
    % The magnitude of the flow is inversely proportional to the depth.
    % }
    \caption{\textbf{Explanation of depth-adaptive translation radius.} A fixed-radius camera sampling scheme may result in significantly different flow values (Flow 1 and Flow 2) in areas with varying depth ($d_1$ and $d_2$).} \label{fig:camera}
    \vspace{-4.0em}
\end{wrapfigure}

We propose the Flow Distillation Sampling (FDS) method to 
incorporate pretrained matching priors into the rendering 
optimization process, thereby mitigating overfitting and 
improving rendering performance.



%
The camera sampling scheme and loss function design in FDS
are introduced in ~\secref{sec:method:subsubsec:cs} 
and ~\secref{sec:method:subsubsec:fds}.


\subsubsection{Camera Sampling Scheme}
\label{sec:method:subsubsec:cs}
% The goal of camera sampling is to revise the Radiance Flow 
% with the supervision of Prior flow, since Prior Flow is more 
% robust and accurate than Radiance Flow according to our 
% interpretive experiments. 


During the training process, FDS randomly
samples unobserved camera views nearby the input view and then incorporates 
the matching priors into these views. 
To utilize the matching prior information between
input view and sampled view more profoundly, 
we propose a camera sampling scheme that ensures sufficient movement to perceive the geometry of the scene while avoiding excessively abrupt motions that could make it difficult for the prior model to match.



Given the world to camera transformation matrix $\bm{T^{i}}$ for the $i$-th input view , 
a small translation disturbance $\bm{t}$ and rotation 
disturbance $\bm{R}$ are applied to get the 
sampled transformation matrix $T^s$, expressed as:
\begin{equation}
\bm{T^s} = ((\bm{T^{i}})^{-1} \bm{E})^{-1}
\label{equ:camera_sampling}
\end{equation}
where $\bm{E}=[\bm{R}, \bm{t}]$ denotes the translation and rotation matrix
in camera coordinates, 
and $t$ is a translation vector satisfying 
$|\bm{t}| = \epsilon_t, \bm{t} = (t_1, t_2, t_3)$,
indicating that unobserved view is sampled along a small
spiral circle with a radius of $\epsilon_t$. 
%
% To alleviate the noise of Prior Flow without decreasing 
% errors that can be exposed by Radiance Flow, 
%
As noted in ~\citep{bian2021auto}, the rotation flow in 
image warping is independent of depth. Therefore, we set the 
rotation part to identity matrix. 
So that $E$ is a pure-translation transformation.
%
For the translation radius $\epsilon_t$, as illustrated in~\figref{fig:camera}, using
a uniform radius for all views leads to varying flows due to
the differences in depths; the closer the depth, the larger the flow. 
To ensure flow consistency across all views 
while maintaining controllable overlap between the input views and sampled views,
we hope to implement a depth-adaptive radius, and
its hyperparameter can be shared across different types of datasets.
% which is 
% inversely proportional to the mean depth of the view.
which can help to preserve the same flow 
between all input views and their sampled views.

According to ~\eqref{equ:flow_pose}, 
the pure translation transformation ~\citep{bian2021auto} between the input view $i$ and its sampled view $s$ is shown below:

\begin{equation}
    \bm{D^s}(u_2 , v_2)
    \begin{bmatrix}
     u_2\\
     v_2\\
     1
    \end{bmatrix}
    = 
    \bm{D^i}(u_1 , v_1)
    \begin{bmatrix}
     u_1\\
     v_1\\
     1
    \end{bmatrix} 
         +\bm{K}
    \begin{bmatrix}
     t_1\\
     t_2\\
     t_3
    \end{bmatrix} 
    \label{equ:flow_sampling}
\end{equation}

where $K$ is the intrinsic matrix of the camera, after solving the above equation~\citep{bian2021auto}, we get:


\begin{equation}
\begin{bmatrix}
     u_2\\
     v_2\\
    \end{bmatrix} 
= \begin{bmatrix}
\frac{\bm{D_i}(u_1, v_1)u_1+f_xt_1+c_xt_3}{\bm{D_i}(u_1, v_1) + t_3} \\
\frac{\bm{D_i}(u_1, v_1)u_1+f_yt_2+c_xt_3}{\bm{D_i}(u_1, v_1) + t_3} 
\end{bmatrix} 
\end{equation}


We set $ t_3 = 0 $ in our camera sampling scheme and assume camera intrinsic parameters: $f_x\approx f_y=f$. 
The radiance flow $\bm{F^{i\rightarrow s}}(u_1, v_1) = \begin{bmatrix}
     u_2 - u_1\\
     v_2 - v_1\\
    \end{bmatrix} $from the input view $i$ to its sampled view $s$ is shown below:

\begin{equation}
\bm{F^{i\rightarrow s}}(u_1, v_1) = 
    \begin{bmatrix}
    \frac{f}{\bm{D_i}(u_1, v_1)}t_1 \\
    \frac{f}{\bm{D_i}(u_1, v_1)}t_2
    \end{bmatrix} 
\label{equ:flow_sampling_2}
\end{equation}

We aim to keep the value of
$||\bm{F^{i\rightarrow s}}(u_1, v_1)||_2$  constant for the pixel $x = (u_1, v_1)$
during each camera sampling. By setting $||\bm{F^{i\rightarrow s}}(u_1, v_1)||_2 = \sigma $ and
incorporating this with ~\eqref{equ:flow_sampling_2}, 
we get:

\begin{equation}
    \epsilon_t = \sqrt{t_1^2 + t_2^2}  =  \sigma  \frac{\bm{D_i}(u_1, v_1)}{f}
\end{equation}

Thus, the radius of translation in our camera sampling 
is defined as 
$\epsilon_t = \sigma \frac{\bm{D_i}(u_1, v_1)}{f} $ 
which helps maintain stable flow.
%
The parameter $\sigma$ can be tuned as a hyperparameter,
which represents the mean radiance flow between the input view 
and its sampled view.
%
Given that pixel depths vary within an image, 
we use the mean depth $ \bar{\bm{D_i}} $ of the image 
and set the radius of our translation
$\epsilon_t = \sigma  \frac{\bar{\bm{D_i}}}{f} $.
%

\begin{equation}
    \bm{t} = \begin{bmatrix}
        \sigma  \frac{\bar{\bm{D_i}}}{f} sin(2 \pi \xi ) \\
        \sigma \frac{\bar{\bm{D_i}}}{f} cos(2 \pi \xi ) \\
        0
    \end{bmatrix}
\end{equation}

Where $\xi ~\sim  U(0, 1) $ is a uniform 
random value between $[0, 1]$ during training.



% \begin{algorithm}[t]
%     \caption{Flow Distillation Sampling.}
%     \label{alg:algorithm1}
%     \KwIn{A batch of input training image: $\{I_i\}_{i=1}^{B}$,  
%     Transformation Matrix: $\{T_i\}_{i=1}^{B}$, 
%     Prior Matching Network $M_{\theta}$, 
%     Gaussian Points $\{P_{i}\}_{i=1}^N$ with $\{r_i, t_i, f_i, X_i, \alpha_i\}$.}
%     \KwOut{$L_{fds}$}  
%     \BlankLine
%     \ForEach{i in $\{1, 2, \dots, B\}$}{
%         $T^s \leftarrow ((T^{i})^{-1} E)^{-1}$; \\
%         Render image $C_s$ from sampled view $T_s$;\\
%         $x^s \leftarrow 
%     KT_{i}^{s}K^{-1}D(x^i)x^i / D(x^s)$; \\
%         $F^{i\rightarrow s}(x^i) \leftarrow  x^i - x^s $; \\
%         $\overline{F}^{i\rightarrow s} \leftarrow  M_{\theta}(I_i, C_s)$; \\
%         $L_{fds} \leftarrow L_{fds} + 1/B \left | \overline{F}^{i\rightarrow s} 
%         - F^{i\rightarrow s} \right |$
%     }
%     \textbf{Return}  $L_{fds}$
%     \label{algo:fds}
% \end{algorithm}


\subsubsection{Flow Distillation Sampling Loss}
\label{sec:method:subsubsec:fds}
%
Specifically, to optimize the rendered results of the $i$-th view, 
every iteration, we sample an unobserved view $s$ near the $i$-th input training camera 
to render the color image $\bm{C^s}$. The Radiance Flow $\bm{F^{i\rightarrow s}(p)}$,
guided by \equref{equ:flow}, 
benefits from the introduced matching priors. 
%
We define the Prior Flow $\bm{\overline{F}^{i\rightarrow s}}$ as:

\begin{equation}
\bm{\overline{F}^{i\rightarrow s}} = \mathcal{M}_{\theta}(\bm{I^i}, \bm{C^s}),  
\label{equ:mflow}
\end{equation}

where 
$\theta$ represents the parameters of pretrained network $\mathcal{M}$,
$\bm{I^i}$ is the rgb ground truth of view $i$, and $\bm{C^s}$ is the 
rendered image of sampled view $s$. 
%
Instead of using $\bm{C^i}$ to generate Prior Flow, we find that using 
$\bm{I^i}$ can 
help us to remove the floaters in $\bm{C^s}$ as shown in ~\tabref{tab:ablation_fds}.
%
However, both $\bm{F^{i\rightarrow s}}$ and 
$\bm{\overline{F}^{i\rightarrow s}}$ face challenges:
\begin{itemize}
    \item $\bm{F^{i\rightarrow s}}$, 
    derived from \equref{equ:flow}, 
    is inaccurate due to the incorrect positioning 
    of Gaussian points.
    \item $\bm{\overline{F}^{i\rightarrow s}}$, derived from \equref{equ:mflow}, 
    suffers from the blurred rendering quality of $\bm{C^s}$.
\end{itemize}



Despite these inaccuracies, we observe 
that $\bm{\overline{F}^{i\rightarrow s}}$ is
more robust and precise during training compared with $\bm{F^{i\rightarrow s}}$.
Motivated by this observation, we aim to utilize $\bm{\overline{F}^{i\rightarrow s}}$
to refine $\bm{F^{i\rightarrow s}}$, which can make $\bm{F^{i\rightarrow s}}$ more accurate, 
enhancing the positioning of Gaussian points and subsequently 
improving the rendering quality of $\bm{C^s}$. 
%
This process  also leads to a more accurate $\bm{\overline{F}^{i\rightarrow s}}$. 
Based on these observations, we propose the FDS loss, which distills
matching priors from a pretrained deep model through mutual 
refinement of the two flows:

\begin{equation}
L_{fds} =  || \bm{\overline{F}^{i\rightarrow s}} - \bm{F^{i\rightarrow s}} ||_2
\label{equ:fds}
\end{equation}



\begin{algorithm}[h]
  \caption{Flow Distillation Sampling
   \label{algo:fds}} %

  \begin{algorithmic}[1]
\Require
      A batch of input training image: $\{\bm{I_i}\}_{i=1}^{N}$,  
     Transformation Matrix: $\{\bm{T_i}\}_{i=1}^{N}$, 
     Prior Matching Network $\mathcal{M}_{\theta}$, 
     Gaussian Points $\{\bm{P}_{i}\}_{i=1}^M$ with $\{\bm{r_i}, \bm{t_i}, \bm{f_i}, \bm{\mu_i}, \bm{\alpha_i}\}$.
\Ensure
      $L_{fds}$

 \For {$i$ in $\{1, 2, \dots, B\}$}
 \State \textcolor{blue}{$\xi ~\sim  U(0, 1), \bm{R}\leftarrow \bm{I}$}
 \State  \textcolor{blue}{$t_1 \leftarrow  \sigma \frac{\bar{\bm{D_i}}}{f} sin(2 \pi \xi ), 
 t_2 \leftarrow  \sigma  \frac{\bar{\bm{D_i}}}{f} cos(2 \pi \xi ), t_3 \leftarrow 0$}
 \State  \textcolor{blue}{$\bm{E}  \leftarrow [\bm{R}, \bm{t}], \bm{T^s} \leftarrow ((\bm{T^{i})}^{-1} \bm{E})^{-1}$}
 \State $\bm{C^s}, \bm{D^s} \leftarrow Render(\bm{T^s}, \bm{P})$
 \State $\bm{C^i}, \bm{D^i} \leftarrow Render(\bm{T^i}, \bm{P})$
 \State \textcolor{olive}{$\bm{X^s} \leftarrow \bm{KT_{i}^{s}K}^{-1}\bm{D^i}(X^i)X^i/ \bm{D}(\bm{X^s})$}
 \State \textcolor{olive}{$\bm{F^{i\rightarrow s}} \leftarrow  \bm{X^s} - \bm{X^i} $}
 \State  \textcolor{olive}{$\bm{\overline{F}^{i\rightarrow s}} \leftarrow  \mathcal{M}_{\theta}(\bm{I^i}, \bm{C^s})$}
 \State  \textcolor{olive}{$L_{fds} \leftarrow L_{fds} + 1/B || \bm{\overline{F}^{i\rightarrow s}} 
    - \bm{F^{i\rightarrow s}}  ||_2$}
 
 \EndFor
 \State \Return $L_{fds}$
  \end{algorithmic}

\end{algorithm}

To maintain training stability and reduce computational complexity, 
we detach $\bm{\overline{F}^{i\rightarrow s}}$ from $\bm{P_i}$ 
when calculating loss. 
This prevents the propagation of gradients, as computing them is 
resource-intensive and time-consuming, as noted in \citep{poole2022dreamfusion}.
Additionally, this helps to ensure that $\bm{\overline{F}^{i\rightarrow s}}$ is not directly influenced by the less reliable $\bm{F^{i\rightarrow s}}$.

In summary, the procedures of our proposed 
Flow Distillation Sampling are presented 
in Algorithm~\ref{algo:fds}. The overall training loss is:

\begin{equation}
    L = \frac{1}{B}\sum_{i=1}^B(1 - \lambda )L_1 + \lambda L_{D-SSIM} + \lambda_{normal} L_{n} + \lambda_{fds} L_{fds}.
\end{equation}


