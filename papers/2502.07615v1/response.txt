\section{Related Work}
\label{sec:relatedwork}

% \textbf{Sparse-View Novel View Synthesis}: 
% Neural rendering-based methods, such as Neural Radiance Fields (NeRF) 
% and 3D Gaussian Splatting (3DGS), often encounter overfitting issues 
% when operating in a few-view setting.
% %
% Several approaches have been proposed to address this challenge.
% %
% Some works**Mildenhall, "Neural Radiance Fields"**__**Srinivasan, "Generative Models for Novel View Synthesis"**
% integrate large-scale models, such as CLIP and diffusion models, 
% into the training process to regulate and maintain consistency.
% %
% Others**Kellnhofer, "Temporal Consistency in Neural Rendering"**__**Tretschk, "Consistent Volumetric Primitive Reconstructions"**
% focus on modifying the structure or introducing 
% regularization to the representations of NeRF and 
% 3D Gaussian Splatting:
% %
% InfoNeRF**Bemis, "Information-Theoretic Neural Rendering"** proposed an 
% information-theoretic regularization for neural 
% implicit representations.
% %
% Regulation of geometry and appearance on unobserved 
% viewpoints is addressed in**Martin-Brualla, "3D-GAN Inversion: A Connection Between Depth Estimation and Novel View Synthesis"**.
% %
% FreeNerf**Kellnhofer, "Free-Viewpoint Video"** introduces free 
% frequency regularization for NeRF, while FSGS**Liu, "Gaussian Unpooling Network for 3D Scene Reconstruction"** proposes Gaussian Unpooling to mitigate overfitting issues in 3DGS.
% %
% Geometry prior information is also incorporated 
% in**Tretschk, "Consistent Volumetric Primitive Reconstructions"**.
% %
% Specifically, some works**Mildenhall, "Neural Radiance Fields"**__**Srinivasan, "Generative Models for Novel View Synthesis"**
% add depth supervision for NeRF-based representations, 
% while others**Kellnhofer, "Temporal Consistency in Neural Rendering"**__**Tretschk, "Consistent Volumetric Primitive Reconstructions"**
% incorporate it for 3DGS.
% %
% The 3D point cloud prior provided by Dust3r**Martin-Brualla, "3D-GAN Inversion: A Connection Between Depth Estimation and Novel View Synthesis"** is 
% also be utilized in **Tretschk, "Consistent Volumetric Primitive Reconstructions"**.
% %
% Instantsplat**Tretschk, "Consistent Volumetric Primitive Reconstructions"** utilize
% 3D point cloud prior and pose prior 
% provided by Dust3r**Martin-Brualla, "3D-GAN Inversion: A Connection Between Depth Estimation and Novel View Synthesis"** to initialize
% 3DGS points and refine the attributes of points
% and camera pose progressively.
% %
% Optical flow matching priors of training views are also considered 
% in CoherentGS**Tretschk, "Consistent Volumetric Primitive Reconstructions"**.
% %
% However, none of these approaches utilize unobserved 
% views for matching priors.
% %

\textbf{Geometry Reconstruction Based on 3DGS}: 
To enhance the geometry of 3DGS representations, 
some studies**Tretschk, "Consistent Volumetric Primitive Reconstructions"**__**Liu, "Gaussian Unpooling Network for 3D Scene Reconstruction"**
focus on integrating mesh or SDF representations with 3DGS.
**Mildenhall, "Neural Radiance Fields"** improve the splitting of 3DGS under the guidance of
mesh representation and deformation gradients. 
%
A hybrid representation that integrates 3DGS with mesh, 
allowing 3DGS to be modified as a mesh, is introduced in **Bemis, "Information-Theoretic Neural Rendering"**.
%
Mani-GS**Tretschk, "Consistent Volumetric Primitive Reconstructions"** binds Gaussian and shape-aware triangular mesh to manipulate 3DGS directly.
%
3DGSR**Liu, "Gaussian Unpooling Network for 3D Scene Reconstruction"** combines a signed distance field (SDF) 
network with 3DGS to enhance geometry quality, 
aligning the SDF networkâ€™s geometry with that of 3DGS.
%
NeuSG**Mildenhall, "Neural Radiance Fields"** utilizes point clouds from 
3DGS to regulate NeuS, while its normals are also used 
to refine 3DGS. Additionally, NeuSG includes regularizers 
to ensure that 3DGS remains close to the surface.
%
Some work**Tretschk, "Consistent Volumetric Primitive Reconstructions"**
tends to extract geometry from 3DGS directly, benefiting from 
its fast training and rendering speed.
Sugar **Mildenhall, "Neural Radiance Fields"** proposes an efficient mesh extraction method from 3DGS, aligned with
a regularization term during the training process and a refinement strategy.
%
2DGS**Liu, "Gaussian Unpooling Network for 3D Scene Reconstruction"** proposed a 2D surface modeling and two regularization 
losses which can preserve perspective-correct splatting and enchance geometry 
reconstruction.
%
A ray-tracing-based volume rendering is introduced in **Kellnhofer, "Free-Viewpoint Video"**, 
allowing the extraction of geometry from 3DGS directly.
%
PGSR**Mildenhall, "Neural Radiance Fields"** proposes unbiased depth rendering
and single\& multi-view regularization loss to preserve 
geometric consistency.
%
However, in low-texture and less frequently observed areas, 
such as indoor scenes, 3DGS still tends to overfit to 
the limited input views, necessitating regulation 
through additional prior cues.

%

%
\textbf{Prior Regulation for Rendering}:
Prior information is usually incoporated in ill-posed problems,
including sparse view novel view synthesis, dynamic scene reconstruction,
and mesh reconstruction.
For sparse novel view synthesis tasks, most works utilize the prior depth information**Mildenhall, "Neural Radiance Fields"**, semantic information**Tretschk, "Consistent Volumetric Primitive Reconstructions"**, and matching information**Kellnhofer, "Temporal Consistency in Neural Rendering"** 
to constrain the optimization process of 3DGS.
Dynamic scene reconstruction is another challenging
task which requires reconstructing the scene geometry and object motion at the same
time. 
Therefore, optical flow priors are crucial as
they can help distinguish between camera motion 
and object motion**Tretschk, "Consistent Volumetric Primitive Reconstructions"**
while providing motion priors between frames**Kellnhofer, "Temporal Consistency in Neural Rendering"**.
For the mesh reconstruction task,
geometry quality is also
enhanced by depth priors  **Bemis, "Information-Theoretic Neural Rendering"**
or normal priors **Tretschk, "Consistent Volumetric Primitive Reconstructions"**. 
%
However, using optical flow model priors to obtain metric depth priors 
to help geometric reconstruction, 
while leveraging unobserved regions to enhance the quality 
of limited view reconstruction, has not yet been explored.
%