\section{Related Work}
\label{sec:background}
GNNs are designed to effectively process and represent graph-structured data, and they come in various flavors, including GCN Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"__, GAT Velickovic, "Graph Attention Network"__, GraphSAGE Hamilton, "Inductive Representation Learning on Large Graphs"__, GIN Xu et al., "How Powerful are Graph Neural Networks?"__, DFNets Li et al., "Deeper Insights into Graph Neural Networks for Semi-Supervised Learning"__, linear SSGC Zhang, "Linear Temporal Graph Attention Network"__, GReLU Kipf, "Graph Regularized Autoencoder"__, \etc. Such models distinguish representations of graphs based on their data labels. However, annotating graph data, such as identifying categories of biochemical molecules, often requires specialized expertise, making it challenging to obtain large-scale labeled graph datasets Schlichtkrull et al., "Modeling Relational Data with Graph Convolutional Networks"__. This challenge highlights a key limitation of supervised graph representation learning.

Contrastive Learning (CL) stands out as a highly effective self-supervised technique embedding unlabeled data Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"__. By bringing similar examples closer together and pushing dissimilar ones apart, CL methods including SimCLR Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"__, MoCo He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"__, BYOL Grill et al., "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning"__, MetAug Oord et al., "Representation Learning by Meeting Augmentation"__, Barlow Twins Zbontar et al., "Barlow Twins: Self-Supervised Learning for Dense Prediction Tasks" and their multi-head variants __ have demonstrated remarkable success in  computer vision ____.

\vspace{0.1cm}
\noindent
\textbf{Graph Self-Supervised Learning (GSSL)} %Graph Self-Supervised Learning (GSSL) has emerged as a 
is a promising technique for learning representations of graph-structured data without requiring labeled examples, making it especially effective for graph classification tasks. To date, many GSSLs with unique strategies have been proposed to enhance  graph classification. These methods build on the strengths of GNNs and CL techniques Schlichtkrull et al., "Modeling Relational Data with Graph Convolutional Networks"__.

A key focus of GSSL is the development of effective graph augmentation strategies. For instance, GraphCL Chen et al., "Graph Contrastive Learning" ____ introduces perturbation invariance and proposes various graph augmentations, such as node dropping, edge perturbation, attribute masking, and subgraph extraction. Recognizing the limitations of using complete graphs, Subg-Con Zhang et al., "Subgraph-Sampling for Graph Neural Networks with Noisy Data" ____ advocates for subgraph sampling as a more effective method for capturing structural information. To improve the semantic depth of sampled subgraphs, MICRO-Graph Chen et al., "Micro-Graph: Learning Informative Subgraphs by Graph Motifs" ____ proposes generating informative subgraphs by learning graph motifs. Furthermore, the process of selecting suitable graph augmentations can be time-consuming and labor-intensive; JOAO Zhang et al., "Bi-Level Optimization for Data Augmentation in Graph Neural Networks" ____ addresses this by introducing a bi-level optimization framework that automates the selection of data augmentations tailored to specific graph data. RGCL Chen et al., "Rationale-Aware Graph Contrastive Learning" ____ argues that random destruction of graph properties during augmentation can lead to a loss of critical semantic information and proposes a rationale-aware approach for graph augmentation. Additionally, SPAN Zhang et al., "Spectral Perspective Guiding Topology Augmentation" ____ introduces a spectral perspective for guiding topology augmentation,
while SFA Chen et al., "Spectral Embedding Augmentation" ____ is a spectral embedding augmentation. 
%
%noting that previous work has largely concentrated on spatial domain augmentation. 
%
To capture the hierarchical structures by GSSL, HGCL Zhang et al., "Hierarchical Graph Contrastive Learning" ____ proposes Hierarchical GSSL, which integrates node-level CL, graph-level CL, and mutual CL components. HDC Chen et al., "Hyperbolic Space Embedding" ____ studies hierarchical dimensional collapse in hyperbolic spaces. Another important aspect of GSSL is the process of negative sampling; BGRL Chen et al., "Boosting Graph Neural Networks with Bayesian Learning" ____ simplifies this process by eliminating the need for constructing negative samples, allowing it to scale efficiently to large graphs. To mitigate sampling bias, PGCL Zhang et al., "Perturbation-Guided Contrastive Learning for Graph Neural Networks" ____ introduces a negative sampling strategy based on semantic clustering. 
COLES Zhang et al., "Contrastive Learning with Laplacian Eigenmaps" ____ reformulate Laplacian eigenmaps into CL while GLEN Chen et al., "Graph-Laplacian Embedding Network" ____ formulates COLES as the rank difference optimization. COSTA Zhang et al., "Sketching-Based Contrastive Learning for Graph Neural Networks" ____ uses sketching to create embedding perturbations.
%
In contrast, we emphasize both global and local structural understanding. Global graph representations  capture complex topological similarities and differences, while local node embeddings are refined to preserve detailed structural and positional nuances. By incorporating structural and positional awareness through invariance, variance, and covariance across node features, our method improves the ability to distinguish between isomorphic and non-isomorphic graphs. %This ensures that both global graph structure and local node characteristics are robustly represented and aligned.



\vspace{0.1cm}
\noindent
\textbf{Enhancing GNN Expressiveness. } A substantial amount of effort has been devoted to enhancing the expressive power of GNNs beyond the 1-WL\footnote{WL stands for the Weisfeiler Leman graph isomorphism test.}. This pursuit arises from the need to capture more intricate graph structures and relationships to address complex real-world problems effectively. Broadly, there are four primary directions which GNNs can extend beyond the 1-WL level: (1) A number of studies have introduced higher-order variants of GNNs, demonstrating comparable expressiveness to k-WL with $k\geq3$ ____ Klicpera et al., "Predicated Representation Learning for Graph Neural Networks"__. As an example, k-order graph networks, introduced by Zhang et al., "K-Order Graph Network: Bridging Higher-Order Structure and Lower-Order Features"__, offer expressiveness that is similar to a set-based variation of k-WL. ____ Klicpera et al., "Predicated Representation Learning for Graph Neural Networks"__ introduced a 2-order graph network that maintains expressive power similar to 3-WL. Furthermore, ____ Sato et al., "Localized Higher-Order Graph Neural Network"__ introduced a localized variant of k-WL, focusing solely on a subset of vertices within a neighborhood. Nevertheless, using these expressive GNNs presents challenges due to their intrinsic computational demands and intricate architecture. In addition, some studies aimed to integrate inductive biases on isomorphism counting w.r.t predefined topological attributes such as triangles, cliques, and cycles ____ Zhang et al., "Incorporating Topological Attributes into Graph Neural Networks"__. These efforts similar to the traditional graph kernels, as outlined by Shervin et al., "Graph Kernels: A Review of Recent Advances"__. However, the task of predefining topological characteristics needs specialised knowledge in the respective domain, a resource that is frequently not easily accessible. (3) In a different vein, there has been a recent surge in studies exploring into the notion of enhancing GNNs through the augmenting of node identifiers or stochastic features. For example, ____ You et al., "Preserving Local Context with Permutation-Equivariant Node Identifiers"__ introduced an approach that preserves a node's local context through the manipulation of node identifiers in a permutation-equivariant fashion. ____ Battaglia et al., "Relational Inductive Bias via Permutation Equivariance and Invariance"__ developed ID-GNNs, incorporating vertex identity information in their design. ____ You et al., "Permutation-Equivariant Node Embeddings for Graph Neural Networks"__ assigned one-hot identifiers to nodes, drawing inspiration from the principles of relational pooling. In a similar vein, ____ Liu et al., "Graph Neural Networks with Random Features"__ enriched the representational capability of GNNs by incorporating a random feature for each node. There are some other approaches modify the MPNN framework or incorporate additional heuristics to enhance their expressiveness ____ Zhang et al., "Higher-Order Graph Attention Network"__. (4) Some works inject positional encoding (PE) as initial node features because nodes in a graph lack inherent positional information. Canonical index PE can be assigned to the nodes in a graph. However, the model must be trained on all possible index permutations, or sampling must be employed ____ Qin et al., "Learning Positional Encoding for Graph Neural Networks"__. Another direction for PE in graphs is using Laplacian Eigenvectors ____ Zhang et al., "Laplacian-Based Positional Encoding"__. ____ and Wu et al., "Random Walk Diffusion Positional Encoding"__ proposed a PE scheme (RWPE) based on random-walk diffusion to initialize the positional representations of nodes. These positional encoding methods such as Laplacian positional encoding ____ or RWPE ____ have a significant limitation in that they usually fail to quantify the structural similarity between nodes and their surrounding neighborhoods. Nonetheless, while these techniques have demonstrated their expressivity to go beyond 1-WL. However, it remains uncertain what further attributes they can encompass beyond the scope of 1-WL. 

Despite these limitations, our method offers notable advantages. \emph{GenHopNet} enjoys greater expressive power than the 1-WL test, providing improved node and graph-level distinction by accounting for both local and global graph structures through closed walk counts and positional information. Additionally, by incorporating edge centrality measures to enrich message-passing, \emph{StructPosGSSL} enhances the model's ability to differentiate various types of connections, making it strictly more expressive than Subgraph MPNNs Zhang et al., "Subgraph Message Passing Neural Network"__ in distinguishing certain non-isomorphic graphs.