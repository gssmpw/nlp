\section{Related Work}
\label{sec:background}
GNNs are designed to effectively process and represent graph-structured data, and they come in various flavors, including GCN \cite{kipf2016semi}, GAT \cite{velivckovic2018graph}, GraphSAGE \cite{hamilton2017inductive}, GIN \cite{xu2018powerful}, DFNets \cite{wijesinghe2019dfnets}, linear SSGC \cite{zhu2021simple}, GReLU \cite{zhang2022graph}, \etc. Such models distinguish representations of graphs based on their data labels. However, annotating graph data, such as identifying categories of biochemical molecules, often requires specialized expertise, making it challenging to obtain large-scale labeled graph datasets \cite{you2020graph}. This challenge highlights a key limitation of supervised graph representation learning.

Contrastive Learning (CL) stands out as a highly effective self-supervised technique embedding unlabeled data \cite{li2022metamask}. By bringing similar examples closer together and pushing dissimilar ones apart, CL methods including SimCLR \cite{chen2020simple}, MoCo \cite{he2020momentum}, BYOL \cite{grill2020bootstrap}, MetAug \cite{li2022metaug}, Barlow Twins \cite{zbontar2021barlow} and their multi-head variants \cite{multihead_lei} have demonstrated remarkable success in  computer vision \cite{wu2018unsupervised, qiang2022interventional}.

\vspace{0.1cm}
\noindent
\textbf{Graph Self-Supervised Learning (GSSL)} %Graph Self-Supervised Learning (GSSL) has emerged as a 
is a promising technique for learning representations of graph-structured data without requiring labeled examples, making it especially effective for graph classification tasks. To date, many GSSLs with unique strategies have been proposed to enhance  graph classification. These methods build on the strengths of GNNs and CL techniques \cite{velickovic2019deep, suninfograph, hassani2020contrastive}.

A key focus of GSSL is the development of effective graph augmentation strategies. For instance, GraphCL \cite{you2020graph} introduces perturbation invariance and proposes various graph augmentations, such as node dropping, edge perturbation, attribute masking, and subgraph extraction. Recognizing the limitations of using complete graphs, Subg-Con \cite{jiao2020sub} advocates for subgraph sampling as a more effective method for capturing structural information. To improve the semantic depth of sampled subgraphs, MICRO-Graph \cite{zhang2020motif} proposes generating informative subgraphs by learning graph motifs. Furthermore, the process of selecting suitable graph augmentations can be time-consuming and labor-intensive; JOAO \cite{you2021graph} addresses this by introducing a bi-level optimization framework that automates the selection of data augmentations tailored to specific graph data. RGCL \cite{li2022let} argues that random destruction of graph properties during augmentation can lead to a loss of critical semantic information and proposes a rationale-aware approach for graph augmentation. Additionally, SPAN \cite{linspectral} introduces a spectral perspective for guiding topology augmentation,
while SFA \cite{zhang2023spectral} is a spectral embedding augmentation. 
%
%noting that previous work has largely concentrated on spatial domain augmentation. 
%
To capture the hierarchical structures by GSSL, HGCL \cite{ju2023unsupervised} proposes Hierarchical GSSL, which integrates node-level CL, graph-level CL, and mutual CL components. HDC \cite{hyper_collapse} studies hierarchical dimensional collapse in hyperbolic spaces. Another important aspect of GSSL is the process of negative sampling; BGRL \cite{thakoorlarge} simplifies this process by eliminating the need for constructing negative samples, allowing it to scale efficiently to large graphs. To mitigate sampling bias, PGCL \cite{lin2022prototypical} introduces a negative sampling strategy based on semantic clustering. 
COLES \cite{zhu2021contrastive} reformulate Laplacian eigenmaps into CL while GLEN \cite{glen} formulates COLES as the rank difference optimization. COSTA \cite{zhang2022costa} uses sketching to create embedding perturbations.
%
In contrast, we emphasize both global and local structural understanding. Global graph representations  capture complex topological similarities and differences, while local node embeddings are refined to preserve detailed structural and positional nuances. By incorporating structural and positional awareness through invariance, variance, and covariance across node features, our method improves the ability to distinguish between isomorphic and non-isomorphic graphs. %This ensures that both global graph structure and local node characteristics are robustly represented and aligned.



\vspace{0.1cm}
\noindent
\textbf{Enhancing GNN Expressiveness. } A substantial amount of effort has been devoted to enhancing the expressive power of GNNs beyond the 1-WL\footnote{WL stands for the Weisfeiler Leman graph isomorphism test.}. This pursuit arises from the need to capture more intricate graph structures and relationships to address complex real-world problems effectively. Broadly, there are four primary directions which GNNs can extend beyond the 1-WL level: (1) A number of studies have introduced higher-order variants of GNNs, demonstrating comparable expressiveness to k-WL with $k\geq3$ \cite{azizian2020expressive}. As an example, k-order graph networks, introduced by \cite{morris2019weisfeiler}, offer expressiveness that is similar to a set-based variation of k-WL. \cite{maron2019provably} introduced a 2-order graph network that maintains expressive power similar to 3-WL. Furthermore, \cite{morris2020weisfeiler} introduced a localized variant of k-WL, focusing solely on a subset of vertices within a neighborhood. Nevertheless, using these expressive GNNs presents challenges due to their intrinsic computational demands and intricate architecture. In addition, some studies aimed to integrate inductive biases on isomorphism counting w.r.t predefined topological attributes such as triangles, cliques, and cycles \cite{bouritsas2020improving, liu2020neural, monti2018motifnet}. These efforts similar to the traditional graph kernels, as outlined by \cite{yanardag2015deep}. However, the task of predefining topological characteristics needs specialised knowledge in the respective domain, a resource that is frequently not easily accessible. (3) In a different vein, there has been a recent surge in studies exploring into the notion of enhancing GNNs through the augmenting of node identifiers or stochastic features. For example, \cite{vignac2020building} introduced an approach that preserves a node's local context through the manipulation of node identifiers in a permutation-equivariant fashion. \cite{you2021identity} developed ID-GNNs, incorporating vertex identity information in their design. \cite{chen2020can} and \cite{murphy2019relational} assigned one-hot identifiers to nodes, drawing inspiration from the principles of relational pooling. In a similar vein, \cite{sato2021random} enriched the representational capability of GNNs by incorporating a random feature for each node. There are some other approaches modify the MPNN framework or incorporate additional heuristics to enhance their expressiveness \cite{bouritsas2006improving, bodnar2021weisfeiler, wijesinghe2021new}. (4) Some works inject positional encoding (PE) as initial node features because nodes in a graph lack inherent positional information. Canonical index PE can be assigned to the nodes in a graph. However, the model must be trained on all possible index permutations, or sampling must be employed \cite{murphy2019relational}. Another direction for PE in graphs is using Laplacian Eigenvectors \cite{dwivedi2023benchmarking, dwivedi2020generalization}, as they establish a meaningful local coordinate system while maintaining the global structure of the graph. \cite{dwivedi2021graph} proposed a PE scheme (RWPE) based on random-walk diffusion to initialize the positional representations of nodes. These positional encoding methods such as Laplacian positional encoding \cite{dwivedi2020generalization} or RWPE \cite{dwivedi2021graph} have a significant limitation in that they usually fail to quantify the structural similarity between nodes and their surrounding neighborhoods. Nonetheless, while these techniques have demonstrated their expressivity to go beyond 1-WL. However, it remains uncertain what further attributes they can encompass beyond the scope of 1-WL. 

Despite these limitations, our method offers notable advantages. \emph{GenHopNet} enjoys greater expressive power than the 1-WL test, providing improved node and graph-level distinction by accounting for both local and global graph structures through closed walk counts and positional information. Additionally, by incorporating edge centrality measures to enrich message-passing, \emph{StructPosGSSL} enhances the model's ability to differentiate various types of connections, making it strictly more expressive than Subgraph MPNNs \cite{you2021identity, cotta2021reconstruction, zhang2021nested} in distinguishing certain non-isomorphic graphs.

% \vspace{-0.3cm}