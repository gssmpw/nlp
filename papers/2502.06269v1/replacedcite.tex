\section{RELATED WORK}
% \subsection*{Sequential Recommendation}
 Sequential recommendation models focus on modeling user behavior as a chronologically ordered sequence of interactions, aiming to predict the next item a user will engage with.\\
{\bfseries Traditional Approaches.} Early approaches primarily utilized Markov Chains (MCs)  ____ to capture the transition probabilities between items. With the rise of deep learning, an ID-based recommendation paradigm emerged. Various deep neural network architectures have been developed under this paradigm. For instance, GRU4Rec ____ was the first to employ GRU-based RNNs for sequential recommendations, while SASRec ____ introduced a self-attention mechanism similar to decoder-only transformer models, to capture long-range dependencies. Inspired by the success of masked language modeling, BERT4Rec ____ utilized transformers with masking strategies to enhance sequential recommendation tasks. Building on the masking technique, S\textsuperscript{3}-Rec ____ learns the correlations among attributes, items, subsequences, and sequences through the principle of mutual information maximization (MIM), thereby enhancing data representation.

\noindent{\bfseries Generative Approaches.} Unlike traditional embedding-based methods, which rely on dot-product (cosine) similarity and external ANN search systems for top-k retrieval, generative approaches predict item identifiers directly. 
%
Generative methods can be broadly categorized into two types: prompt fine-tuning strategies based on \textit{off-the-shelf} large language models (LLMs) and training from scratch for custom-designed models.

For LLMs based methods ____, the focus is on designing refined prompts and fine-tuning tasks that help language models better understand recommendation tasks. LC-Rec ____ introduces a learning-based vector quantization approach for semantically meaningful item indexing and fine-tuning tasks that align collaborative signals with LLM representations, achieving superior performance in diverse scenarios. CCF-LLM ____ transforms user-item interactions into hybrid prompts encoding both semantic knowledge and collaborative signals, utilizing an attentive cross-modal fusion strategy to integrate embeddings from different modalities. SC-Rec ____ utilizes multiple item indices and prompt templates, alongside a self-consistent re-ranking mechanism, to more effectively merge collaborative and semantic knowledge.

For methods that train from scratch ____, the primary focus is on converting the raw sequence recommendation task into an autoregressive generation task. Tree-based methods ____, such as RecForest  ____, have shown promising performance by constructing multiple trees and integrating transformer-based structures for routing. Additionally, TIGER ____ introduced the concept of semantic IDs, representing each item as a set of tokens derived from its side information, and predicting next item tokens in a sequence-to-sequence manner. EAGER ____ employs a dual-stream generative framework to parallely utilize semantic and behavioral information with two separate codes, generating recommended items from each respective pipeline, and ultimately selecting the top-k items based on confidence scores. 

In this paper, we aim to seamlessly integrate collaborative and semantic modality knowledge into a unified code for generative recommendation, while ensuring no additional computational and storage overhead compared with solely utilizing one modality.