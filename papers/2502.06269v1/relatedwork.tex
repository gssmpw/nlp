\section{RELATED WORK}
% \subsection*{Sequential Recommendation}
 Sequential recommendation models focus on modeling user behavior as a chronologically ordered sequence of interactions, aiming to predict the next item a user will engage with.\\
{\bfseries Traditional Approaches.} Early approaches primarily utilized Markov Chains (MCs)  \cite{rendle2010factorizing, he2016fusing} to capture the transition probabilities between items. With the rise of deep learning, an ID-based recommendation paradigm emerged. Various deep neural network architectures have been developed under this paradigm. For instance, GRU4Rec \cite{jannach2017recurrent} was the first to employ GRU-based RNNs for sequential recommendations, while SASRec \cite{kang2018self} introduced a self-attention mechanism similar to decoder-only transformer models, to capture long-range dependencies. Inspired by the success of masked language modeling, BERT4Rec \cite{sun2019bert4rec} utilized transformers with masking strategies to enhance sequential recommendation tasks. Building on the masking technique, S\textsuperscript{3}-Rec \cite{zhou2020s3} learns the correlations among attributes, items, subsequences, and sequences through the principle of mutual information maximization (MIM), thereby enhancing data representation.

\noindent{\bfseries Generative Approaches.} Unlike traditional embedding-based methods, which rely on dot-product (cosine) similarity and external ANN search systems for top-k retrieval, generative approaches predict item identifiers directly. 
%
Generative methods can be broadly categorized into two types: prompt fine-tuning strategies based on \textit{off-the-shelf} large language models (LLMs) and training from scratch for custom-designed models.

For LLMs based methods \cite{zheng2024adapting, lin2024bridging, liu2024collaborative, kim2024sc, ji2024genrec}, the focus is on designing refined prompts and fine-tuning tasks that help language models better understand recommendation tasks. LC-Rec \cite{zheng2024adapting} introduces a learning-based vector quantization approach for semantically meaningful item indexing and fine-tuning tasks that align collaborative signals with LLM representations, achieving superior performance in diverse scenarios. CCF-LLM \cite{liu2024collaborative} transforms user-item interactions into hybrid prompts encoding both semantic knowledge and collaborative signals, utilizing an attentive cross-modal fusion strategy to integrate embeddings from different modalities. SC-Rec \cite{kim2024sc} utilizes multiple item indices and prompt templates, alongside a self-consistent re-ranking mechanism, to more effectively merge collaborative and semantic knowledge.

For methods that train from scratch \cite{zeng2024scalable, wang2024enhanced, liu2024mmgrec, feng2022recommender, rajput2023recommender, wang2024eager, zhu2018learning, zhu2019joint}, the primary focus is on converting the raw sequence recommendation task into an autoregressive generation task. Tree-based methods \cite{zhu2018learning, feng2022recommender, zhu2019joint}, such as RecForest  \cite{feng2022recommender}, have shown promising performance by constructing multiple trees and integrating transformer-based structures for routing. Additionally, TIGER \cite{rajput2023recommender} introduced the concept of semantic IDs, representing each item as a set of tokens derived from its side information, and predicting next item tokens in a sequence-to-sequence manner. EAGER \cite{wang2024eager} employs a dual-stream generative framework to parallely utilize semantic and behavioral information with two separate codes, generating recommended items from each respective pipeline, and ultimately selecting the top-k items based on confidence scores. 

In this paper, we aim to seamlessly integrate collaborative and semantic modality knowledge into a unified code for generative recommendation, while ensuring no additional computational and storage overhead compared with solely utilizing one modality.