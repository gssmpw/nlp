%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
% \documentclass[sigconf,natbib=true,anonymous=true]{acmart}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subcaption} % support subfigure env

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Progressive Collaborative and Semantic Knowledge Fusion for Generative Recommendation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Longtao Xiao}
% \authornote{Both authors contributed equally to this research.}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Huazhong University of Science and Technology}
  \city{Wuhan}
  \country{China}
}
\email{xiaolongtao@hust.edu.cn}

\author{Haozhao	Wang}
% \authornote{Both authors contributed equally to this research.}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Huazhong University of Science and Technology}
  \city{Wuhan}
  \country{China}
}
\email{hz_wang@hust.edu.cn}

\author{Cheng Wang}
% \authornote{Both authors contributed equally to this research.}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Huawei Technologies Ltd}
  \city{Shenzhen}
  \country{China}
}
\email{wangcheng250@huawei.com}

\author{Linfei Ji}
% \authornote{Both authors contributed equally to this research.}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Huazhong University of Science and Technology}
  \city{Wuhan}
  \country{China}
}
\email{jilinfei@hust.edu.cn}

\author{Yifan Wang}
\affiliation{%
  \institution{Huazhong University of Science and Technology}
  \city{Wuhan}
  \country{China}
}
\email{d202381481@hust.edu.cn}

\author{Jieming	Zhu}
% \authornote{Both authors contributed equally to this research.}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Huawei Noah’s Ark Lab}
  \city{Shenzhen}
  \country{China}
}
\email{jiemingzhu@ieee.org}

\author{Zhenhua	Dong}
% \authornote{Both authors contributed equally to this research.}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Huawei Noah’s Ark Lab}
  \city{Shenzhen}
  \country{China}
}
\email{dongzhenhua@huawei.com}

\author{Rui	Zhang}
\affiliation{%
  \institution{Huazhong University of Science and Technology}
  \city{Wuhan}
  \country{China}
}
\email{rayteam@yeah.net}

\author{Ruixuan	Li}
\affiliation{%
  \institution{Huazhong University of Science and Technology}
  \city{Wuhan}
  \country{China}
}
\email{rxli@hust.edu.cn}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Xiao et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
With the recent surge in interest surrounding generative paradigms, generative recommendation has increasingly attracted the attention of researchers in the recommendation community. This paradigm generally consists of two stages. In the first stage, pretrained semantic embeddings or collaborative ID embeddings are quantized to create item codes, aiming to capture and preserve rich semantic or collaborative knowledge within these codes. The second stage involves utilizing these discrete codes to perform an autoregressive sequence generation task. Existing generative recommendation methods often either overlook collaborative knowledge or semantic knowledge, or combine the two roughly. In this paper, we observe that naively concatenating representations from semantic modality and collaborative modality leads to a semantic domination issue, where the resulting representation is overly influenced by semantic information, effectively overshadowing the collaborative representation. Consequently, downstream recommendation tasks fail to fully exploit the knowledge from both modalities, resulting in suboptimal performance. 
% With the recent surge in the popularity of generative paradigms, generative recommendation has gradually drawn attention from researchers in the recommendation community. Typically, generative recommendation involves two stages: discrete code generation for items and autoregressive sequence modeling. Nevertheless, existing methods generally employ a set of codes that can only encode knowledge from a single modality, usually collaborative or semantic information. When both modalities are required, two sets of isolated codes must be generated and stored, leading to doubled computational and storage costs. Moreover, this approach hinders the effective utilization of the complementary strengths of collaborative and semantic knowledge. Considering this limitation, \textit{this paper seeks to integrate knowledge from two different modalities into a unified code}, enabling a single code to incorporate complementary modalities information. However, we identify a significant issue: simply concatenating representations from two modalities will result in the semantic domination problem, where the resulting representation aligns predominantly with semantic features, overshadowing collaborative information. This imbalance hinders the full utilization of knowledge from both modalities, greatly degrading the performance in downstream recommendation tasks.


To address this, we propose a \textbf{\underline{pro}}gressive collaborative and semantic knowledge fusion model for generative \textbf{\underline{rec}}ommendation, named PRORec, which integrates semantic and collaborative knowledge with a unified code through a two-stage framework. Specifically, in the first stage, we propose a cross-modality knowledge alignment task during the pre-training of the non-generative recommendation model, which integrates semantic knowledge into collaborative embeddings, enhancing their representational capability. In the second stage, we propose an in-modality knowledge distillation task, designed to encourage the generative recommendation model to effectively capture and integrate knowledge from both semantic and collaborative modalities. Extensive experiments on three widely used benchmarks validate the effectiveness of our approach, demonstrating its superiority compared to existing methods.
  
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Generative Recommendation, Autoregressive Generation, Semantic Tokenization, Collaborative-Semantic Fusion}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{INTRODUCTION}
% To mitigate the issue of information overload, recommendation systems have become widely adopted in modern society, such as in video \cite{zhao2019recommending}, movie \cite{diao2014jointly}, and music \cite{kim2007music} scenarios. Recommendation systems mine users' interests and proactively recommend items that they may be interested in. Modern recommendation systems typically consist of two phases: representation learning and retrieval-ranking. During the recommendation process, the model needs to retrieve the most relevant items from a large candidate pool based on the item embeddings learned through representation learning. However, existing approaches, whether based on dual-tower models or graph models, generally rely on dot-product (cosine) similarity with approximate nearest neighbor (ANN) search. Although ANN accelerates top-k recommendation retrieval, its search mechanism misaligns with the optimization objectives of the recommendation model, thus limiting the model’s performance.
%
Recommendation systems have become widely adopted in modern society, such as in video \cite{zhao2019recommending}, movie \cite{diao2014jointly}, and music \cite{kim2007music} scenarios. Modern recommendation systems typically adopt a retrieve-and-rank strategy. During the retrieval process, the model needs to retrieve the most relevant items from a large candidate pool based on the item embeddings learned through representation learning. However, existing approaches, whether based on dual-tower models \cite{covington2016deep, wang2021cross} or graph models \cite{wang2018billion, ying2018graph}, generally rely on dot-product (cosine) similarity with approximate nearest neighbor (ANN) search \cite{andoni2018approximate, arya1998optimal}. Although ANN accelerates top-k recommendation retrieval, its search indices are constructed using tools such as Faiss \cite{johnson2019billion} and SCANN \cite{guo2020accelerating}, which are independent of the optimization process of the recommendation model, thereby limiting the overall effectiveness of recommendation systems \cite{zhu2024cost}.
%


% To address this limitation, generative recommendation, which frames the recommendation task as autoregressive sequence generation, has emerged as a promising research direction \cite{liu2024multimodal, wang2023generative, wang2023diffusion, rajput2023recommender}. 
%
To address this limitation, generative recommendation \cite{liu2024multimodal, wang2023generative, wang2023diffusion, rajput2023recommender}, which adopts code as the core component and frames the recommendation task as autoregressive code sequence generation, has emerged as a promising research direction owing to its potential to enable more efficient decoding without the need for ANN indexes. Unlike traditional methods that match users and items based on embeddings, generative recommendation predicts candidate item code directly, as depicted in Figure \ref{fig:generative rec}. Specifically, pre-trained semantic encoders \cite{ni2021sentence, lee2018pre} or collaborative encoders \cite{zhou2018deep, kang2018self} are first utilized to encode item semantic or ID information into embeddings. Then, embedding quantization \cite{lee2022autoregressive, qi2017effective} is applied to map these embeddings into discrete codes forming a lookup table of item codes. The input of transformer will be converted into a sequence of codes, enabling the model to autoregressively predict the next item's code step by step. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figure/traditional_vs_generative.pdf}
  \caption{Traditional vs. Generative Recommendation.}
  % \Description{}
  \label{fig:generative rec}
\end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figure/generative_rec_fill.pdf}
%   \caption{A framework of generative recommendation.}
%   \Description{}
%   \label{fig:generative rec}
% \end{figure}

% However, existing methods are limited by their inability to encode information from collaborative and semantic modalities within a unified code. To simultaneously leverage semantic and collaborative knowledge, they require two independent codes for parallel generation. For instance, Recforest \cite{feng2022recommender} encodes the collaborative modality knowledge of items into a set of codes, while TIGER \cite{rajput2023recommender} encodes the semantic modality knowledge of items into a set of codes. EAGER \cite{wang2024eager}, on the other hand, generates two independent sets of codes to utilize both the collaborative and semantic modality knowledge of items. Nevertheless, These approaches are impractical for future generative recommendation system, which aims to integrate multi-modal content seamlessly for more comprehensive recommendations. There is a pressing need for a unified code capable of simultaneously encoding knowledge from both semantic and collaborative modalities. However, the inherent misalignment between semantic and collaborative knowledge \cite{wang2024enhanced} poses significant challenges for effective integration. As demonstrated in Table \ref{tab:relative influence}, adopting the widely used concat approach to fuse both modalities knowledge results in a semantic domination issue, where the fused representation closely aligns with the semantic representation. This phenomenon indicates that collaborative knowledge is underutilized, while semantic knowledge exerts excessive influence. Such a limitation is detrimental to the recommendation model, as it restricts the ability to fully leverage both modalities information and hinders performance improvements.

%
% However, existing methods generally assign separate codes for collaborative knowledge and semantic knowledge. For instance, Recforest \cite{feng2022recommender} encodes item collaborative knowledge into a single set of codes, whereas TIGER \cite{rajput2023recommender} focuses on encoding item semantic knowledge into a separate code set. Building on these, EAGER \cite{wang2024eager} encodes both collaborative and semantic knowledge into two independent sets of codes. For recommendation systems aiming to utilize both semantic and collaborative knowledge, this is unacceptable, as the dual-code framework significantly increases storage and inference costs, rendering it impractical for large-scale deployments. Furthermore, the intrinsic misalignment between semantic and collaborative knowledge limits the potential to fully exploit their complementary strengths \cite{kim2024sc}. 

%尽管code被作为模态知识载体和自回归预测的枢纽，但是现有方法通常为不同模态分配各自独立的code，
Existing methods typically construct independent codes for different modalities knowledge, either collaborative or semantic modality. For instance, Recforest \cite{feng2022recommender} encodes item collaborative knowledge into a single set of codes, whereas TIGER \cite{rajput2023recommender} focuses on encoding item semantic knowledge into a separate code set. Furthermore, EAGER \cite{wang2024eager} encodes both collaborative and semantic knowledge into two independent sets of codes. These approaches have demonstrated that leveraging both collaborative and semantic information is necessary to enhance the effectiveness and robustness of recommendation models. However, for generative recommenders aiming to integrate both semantic and collaborative knowledge, two separate codes are unacceptable, as the dual-code framework significantly increases storage and inference costs, rendering it impractical for large-scale deployments. Besides, the intrinsic misalignment between semantic and collaborative knowledge limits the potential to fully exploit their complementary strengths with two separate codes \cite{kim2024sc}.

To overcome the aforementioned challenges, we propose a novel method, PRORec, which integrates collaborative and semantic knowledge into a unified code for generative recommendation. Unified codes not only reduce storage and computation demands but also accelerate inference and lower latency, which are critical for real-time responses. As illustrated in Figure \ref{fig:speed}, a unified code demonstrates significantly 2.8x faster inference compared to two separate codes. Moreover, dual-code methods inherently treat semantic and collaborative knowledge as independent entities, thereby missing opportunities to harness their synergistic potential. In contrast, a unified code, by integrating both knowledge at an early stage, can more effectively exploit their complementary strengths.

However, it is non-trivial to adopt a unified code. \textit{The core challenge in adopting a unified code lies in how to get a unified representation which integrates knowledge from both collaborative and semantic modalities.} A common practice is to concatenate semantic and collaborative features to create a unified representation. However, this approach suffers from a phenomenon termed \textit{the semantic dominance issue}. As depicted in Figure \ref{fig:concat}, the final representation aligns more closely with the semantic side, leading to underutilization of the collaborative side. This imbalance not only limits the potential of integrating collaborative knowledge but, in extreme cases, can even degrade performance to levels below those achieved with semantic knowledge alone. The root cause of this issue lies in the inability of concatenation to account for the inherent differences between the two different modalities, such as variations in signal strength and representational misalignment \cite{wang2024enhanced}. Consequently, semantic representations dominate the final output, undermining the expected advantages of unified representations.



We analyze the challenges of integrating both collaborative and semantic knowledge into a unified code for generative recommendation and address them from the following three aspects:

% In this paper, we propose a novel method, PRORec, which integrates collaborative and semantic knowledge into a unified code for generative recommendation. We analyze the challenges of modeling both collaborative and semantic knowledge with a unified code and address them from the following three aspects:

\begin{figure}[t]
  \centering
  % first fig
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/speed.pdf}
    \caption{Comparison of inference speed (second per sample, topk=5, beam size=100) between a unified code and two separate codes setups on the Beauty dataset.}
    \label{fig:speed}
  \end{minipage}%
  \hfill
  % second fig
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/concat.pdf}
    \caption{Proportional similarity of semantic modality and collaborative modality to the final representation with concatenation method on the Beauty dataset.}
    \label{fig:concat}
  \end{minipage}
\end{figure}

% \begin{figure}[t]
%   \centering
%   % first fig
%   \begin{minipage}[t]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/concat.pdf}
%     \caption{Proportional similarity of semantic modality and collaborative modality to the final representation with concatenation method on the Beauty dataset.}
%     \label{fig:concat}
%   \end{minipage}%
%   \hfill
%   % second fig
%   \begin{minipage}[t]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/speed.pdf}
%     \caption{Comparison of inference speed (second per sample, topk=5, beam size=100) between a unified code and two separate codes setups on the Beauty dataset.}
%     \label{fig:speed}
%   \end{minipage}
% \end{figure}

Firstly, \textbf{a unified code} that encodes both collaborative and semantic knowledge is crucial for practical deployment. Due to the inherent modality differences \cite{kim2024sc, wang2024enhanced}, adopting a unified code poses challenges. Therefore, we propose a two-stage knowledge integration framework. Specifically, in the first stage, it uses Llama2-7b \cite{touvron2023llama} to extract semantic knowledge and DIN \cite{zhou2018deep} to extract collaborative knowledge, then adaptively learn a unified representation which integrates both knowledge to construct discrete item codes, referred to as Unicodes, which serve as item identifiers for the second stage. In the second stage, our architecture consists of an encoder that models the user's interaction history and a decoder that predicts the unicode of the target item.

Secondly, to \textbf{resolve the semantic domination issue}, we propose a learnable modality adaption layer and a cross-modality knowledge alignment task, which help to adaptively learn an integrated embedding through the joint optimization with next item prediction task. By providing fine-grained contrastive supervision signals, it encourages the representation to leverage both collaborative and semantic knowledge more balanced, avoiding the overemphasis on semantic modality. 


% Thirdly, ensuring \textbf{comprehensive and sufficient learning} is essential for fully exploiting both knowledge. Previous works have primarily used auto-regressive methods to learn each token sequentially, focusing on local patterns without grasping the global context. We introduce an intra-modality knowledge distillation task with a special token in the second stage. Inspired by contrastive learning paradigms and the success of the [CLS] token in BERT \cite{devlin2018bert} for classification tasks, we design a special token to summarize the sequence's knowledge. The embedding of this token is then used in a global contrastive task against the target item embedding, enabling the model to learn more discriminative representations, thereby improving overall recommendation performance.

Thirdly, ensuring \textbf{comprehensive and sufficient learning} is essential for fully exploiting both knowledge. To compensate for the information loss caused by the quantization process, we introduce an intra-modality knowledge distillation task with a special token in the second stage. Inspired by contrastive learning paradigms and the success of the [CLS] token in BERT \cite{devlin2018bert} for classification tasks, we design a special token to summarize the sequence's knowledge. The embedding of this token is then used in a global contrastive task against the target item integrated embedding, enabling the model to distill the integrated knowledge.

In summary, our contributions can be concluded as follows:
% \begin{itemize}
% \item We introduce PRORec, with a unified code, Unicodes, specifically designed for generative recommendation. By utilizing a single encoder and decoder, it effectively integrates knowledge from both collaborative and semantic modalities, significantly mitigating the semantic domination issue.
% \item We propose a cross-modality knowledge alignment task, an intra-modality knowledge distillation task, and a learnable adaption layer to more effectively fuse knowledge from both semantic and collaborative modalities.
% \item Extensive experiments on three public recommendation benchmarks demonstrate PRORec’s superiority over existing methods, including both generative and traditional paradigms. Additionally, we observe that PRORec demonstrates scaling law. By effectively capturing the complementary nature of collaborative and semantic knowledge, deeper models are able to better utilize the fused knowledge, resulting in sustained performance improvements.
% \end{itemize}

\begin{itemize}
\item We introduce PRORec, with a unified code, Unicodes, which integrates both collaborative and semantic knowledge for generative recommendation. By utilizing the unified code, we achieve faster inference while requiring only half the storage space compared to two separate codes setup.
\item We propose a learnable adaption layer and a joint optimization of cross-modality knowledge alignment and next item prediction tasks to adaptively learn an integrated embedding for resolving the semantic domination issue, and introduce an intra-modality knowledge distillation task to ensure comprehensive and sufficient learning, compensating for the information loss.
\item Extensive experiments on three public recommendation benchmarks demonstrate PRORec’s superiority over existing methods, encompassing both generative and traditional paradigms. Additionally, we observe that PRORec demonstrates the scaling law characteristic.
% \item Extensive experiments on three public recommendation benchmarks demonstrate PRORec’s superiority over existing methods, encompassing both generative and traditional paradigms. Additionally, we observe that PRORec demonstrates the scaling law, which integrates both collaborative and semantic knowledge.
\end{itemize}

% Additionally, we observe that our method demonstrates scaling law. By effectively capturing the complementary nature of collaborative and semantic knowledge, deeper models are better able to model the fused knowledge from both modalities, leading to enhanced performance.

\section{RELATED WORK}
% \subsection*{Sequential Recommendation}
 Sequential recommendation models focus on modeling user behavior as a chronologically ordered sequence of interactions, aiming to predict the next item a user will engage with.\\
{\bfseries Traditional Approaches.} Early approaches primarily utilized Markov Chains (MCs)  \cite{rendle2010factorizing, he2016fusing} to capture the transition probabilities between items. With the rise of deep learning, an ID-based recommendation paradigm emerged. Various deep neural network architectures have been developed under this paradigm. For instance, GRU4Rec \cite{jannach2017recurrent} was the first to employ GRU-based RNNs for sequential recommendations, while SASRec \cite{kang2018self} introduced a self-attention mechanism similar to decoder-only transformer models, to capture long-range dependencies. Inspired by the success of masked language modeling, BERT4Rec \cite{sun2019bert4rec} utilized transformers with masking strategies to enhance sequential recommendation tasks. Building on the masking technique, S\textsuperscript{3}-Rec \cite{zhou2020s3} learns the correlations among attributes, items, subsequences, and sequences through the principle of mutual information maximization (MIM), thereby enhancing data representation.

\noindent{\bfseries Generative Approaches.} Unlike traditional embedding-based methods, which rely on dot-product (cosine) similarity and external ANN search systems for top-k retrieval, generative approaches predict item identifiers directly. 
%
Generative methods can be broadly categorized into two types: prompt fine-tuning strategies based on \textit{off-the-shelf} large language models (LLMs) and training from scratch for custom-designed models.

For LLMs based methods \cite{zheng2024adapting, lin2024bridging, liu2024collaborative, kim2024sc, ji2024genrec}, the focus is on designing refined prompts and fine-tuning tasks that help language models better understand recommendation tasks. LC-Rec \cite{zheng2024adapting} introduces a learning-based vector quantization approach for semantically meaningful item indexing and fine-tuning tasks that align collaborative signals with LLM representations, achieving superior performance in diverse scenarios. CCF-LLM \cite{liu2024collaborative} transforms user-item interactions into hybrid prompts encoding both semantic knowledge and collaborative signals, utilizing an attentive cross-modal fusion strategy to integrate embeddings from different modalities. SC-Rec \cite{kim2024sc} utilizes multiple item indices and prompt templates, alongside a self-consistent re-ranking mechanism, to more effectively merge collaborative and semantic knowledge.

For methods that train from scratch \cite{zeng2024scalable, wang2024enhanced, liu2024mmgrec, feng2022recommender, rajput2023recommender, wang2024eager, zhu2018learning, zhu2019joint}, the primary focus is on converting the raw sequence recommendation task into an autoregressive generation task. Tree-based methods \cite{zhu2018learning, feng2022recommender, zhu2019joint}, such as RecForest  \cite{feng2022recommender}, have shown promising performance by constructing multiple trees and integrating transformer-based structures for routing. Additionally, TIGER \cite{rajput2023recommender} introduced the concept of semantic IDs, representing each item as a set of tokens derived from its side information, and predicting next item tokens in a sequence-to-sequence manner. EAGER \cite{wang2024eager} employs a dual-stream generative framework to parallely utilize semantic and behavioral information with two separate codes, generating recommended items from each respective pipeline, and ultimately selecting the top-k items based on confidence scores. 

In this paper, we aim to seamlessly integrate collaborative and semantic modality knowledge into a unified code for generative recommendation, while ensuring no additional computational and storage overhead compared with solely utilizing one modality.

\section{METHOD}

\subsection{Problem Formulation}
Given an item corpus \( I \) and a user’s historical interaction sequence \( U = [u_1, u_2, \dots, u_{t-1}] \) where \( u \in I \), the target of a sequential recommendation system is to predict  the next most likely item \( u_t \in I \) that the user may interact with.

In the generative framework, each item \( u \) is represented by a sequence of codes \( C = [c_1, c_2, \dots, c_L] \), where \( L \) denotes the length of the code sequence. Here, the sequential recommendation task shifts to predicting the codes \( C_t \) of the next item \( u_t \) based on the user's historical interaction sequence \( U \). During training, the model first encodes the user’s historical interaction sequence \( U \) and then generates the codes \( C_t \) of the target item \( u_t \) step by step at the decoder. The decoding process is defined by the following formula:
\begin{equation}
p(C_t|U) = \prod_{i=1}^{L} p(c_i|U, c_1, c_2, \dots, c_{i-1})
\end{equation}
During the inference phase, the decoder performs beam search to autoregressively generate the codes of the top-k items.

\subsection{Overall Pipeline}
% We present our overall PRORec framework in Figure \ref{fig:PRORec}, which consists of two stages. The first stage fuses knowledge from collaborative and semantic modalities to generate more comprehensive, integrated discrete unicodes. To effectively integrate collaborative and semantic knowledge, we introduce an auxiliary cross-modality knowledge alignment task and a learnable modality adaptation layer with AdaLN, which jointly guide the learning of a general sequential recommendation model along with the next item prediction task. The second stage is the generative recommendation phase, comprising an encoder, a decoder and an intra-modality knowledge distillation task to fully exploit fused knowledge. Here, the user’s historical interaction sequence \([ \text{item}_1, \dots, \text{item}_n ]\) is transformed into unicodes sequence \([ \text{code}_{11}, \text{code}_{12}, \dots, \text{code}_{1L}, \dots, \\ \text{code}_{n1}, \text{code}_{n2}, \dots, \text{code}_{nL} ]\) 
% using the item-unicode mapping table obtained in the first stage. The encoder models the user's historical interactions to capture their interests, which are then passed into the decoder to guide the generation of unicodes for candidate items. To enhance the model’s ability to learn discriminative features, we propose an intra-modality knowledge distillation task. Once training is complete, we select the top-\(k\) recommendations based on confidence scores.

% We present our PRORec overall framework in Figure \ref{fig:PRORec}, which consists of two stages. The first stage fuses knowledge from collaborative and semantic modalities to generate more comprehensive, integrated discrete unicodes by introducing an auxiliary cross-modality knowledge alignment task through joint optimization with next item prediction task and a learnable modality adaptation layer. The second stage is the generative recommendation phase, comprising an encoder, a decoder and an intra-modality knowledge distillation task to fully exploit fused knowledge. Here, the user’s historical interaction sequence is transformed into unicodes sequence using the item-unicode mapping table obtained in the first stage. The encoder models the user's historical interactions to capture their interests, which are then passed into the decoder to guide the generation of unicodes for candidate items. Once training is complete, we select the top-\(k\) recommendations based on confidence scores.
We present PRORec overall framework in Figure \ref{fig:PRORec}, which consists of two stages. In the first stage, we first adaptively learn integrated embeddings through the joint optimization of proposed cross-modality knowledge alignment task and next item prediction task, which fuse knowledge from both collaborative and semantic modalities. Subsequently, the integrated embeddings are quantized into discrete unicodes that act as item identifiers in the second stage. The second stage is the generative recommendation phase, comprising an encoder, a decoder and an intra-modality knowledge distillation task to compensate for the information loss caused by the quantization process. Here, the user’s historical interaction sequence is transformed into unicodes sequence using the item-unicode mapping table obtained in the first stage. The encoder models the user's historical interactions to capture their interests, which are then passed into the decoder to guide the generation of unicodes for candidate items. Once training is complete, we select the top-\(k\) recommendations based on confidence scores.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figure/PRORec.pdf}
  \caption{An overview of PRORec. PRORec consists of two stages. The first stage integrates semantic and collaborative knowledge to construct the unified code, Unicode for each item. The second stage utilizes the obtained unicodes to perform generative recommendation. To achieve the goal of utilizing a unified code to encode the two different knowledge for generative recommendation, we introduce two auxiliary tasks at each stage: a cross-modality knowledge alignment task (CKA) in the first stage and an intra-modality knowledge distillation task (IKD) in the second stage. Besides, in the first stage, a modality adaption layer with AdaLN is also introduced to bridge the modality gap between semantic and collaborative space.}
  \label{fig:PRORec}
\end{figure*}

% \subsection{Stage \uppercase\expandafter{\romannumeral 1}: Item Unicodes Generation}
% In this stage, our objective is to generate item unicodes which are capable of encoding both collaborative and semantic knowledge while utilizing a single unified codebook. Given the ID sequence \( X = [x_1, x_2, \dots, x_{t-1}]\) and the item semantic side information (e.g. such as titles) \( S = [s_1, s_2, \dots, s_{t-1}] \) corresponding to the user’s historical interaction sequence $U$, we employ a general pretrained semantic encoder (e.g. Llama2-7b \cite{touvron2023llama}) to encode items’ semantic knowledge into embeddings \( E_S \). Simultaneously, a randomly initialized general sequential recommendation model (e.g. DIN \cite{zhou2018deep}) encodes the user’s historical sequence \( X \) into collaborative embeddings \( E_C \). To bridge the modality gap between two modalities, we introduce a learnable modality adaptation layer with AdaLN that maps the semantic embeddings \( E_S \) into the same embedding space as the collaborative embeddings. AdaLN introduces learnable parameters to adaptively adjust the normalization process according to the input, effectively capturing the distinctive characteristics of the two modalities. This process is represented by the following equation:
%whz
\subsection{Stage \uppercase\expandafter{\romannumeral 1}: Item Unicodes Generation}
In this stage, our objective is to construct item unicodes which are capable of encoding both collaborative and semantic knowledge while utilizing a single unified codebook. Let \( X \) and \( S \) denote the ID sequence and corresponding item semantic side information (e.g. such as titles) of the user’s historical interaction sequence $U$, respectively. Then, we first use a randomly initialized sequential recommendation model (e.g. DIN \cite{zhou2018deep}) to encode the user’s historical sequence \( X \) into collaborative embeddings \( E_C \), and employ a pretrained semantic encoder (e.g. Llama2-7b \cite{touvron2023llama}) to encode items’ semantic knowledge \( S \) into embeddings \( E_S \).  

% To bridge the modality gap between two modalities, we introduce a learnable modality adaptation layer with AdaLN that maps the semantic embeddings \( E_S \) into the same embedding space as the collaborative embeddings. AdaLN introduces learnable parameters to adaptively adjust the normalization process according to the input, effectively capturing the distinctive characteristics of the two modalities. This process is represented by the following equation:
%
\noindent\textbf{Modality adaption layer}. To bridge the modality gap between two modalities, we propose using a learnable modality adaptation layer with AdaLN that maps the semantic embeddings \( E_S \) into the same embedding space as collaborative embeddings. AdaLN \cite{peebles2023scalable} introduces learnable parameters to adaptively adjust the normalization process according to the input, effectively capturing the distinctive characteristics of the two different modalities. The mapping process is defined as follows:
%
% \begin{equation}
% \begin{split}
% E_T &= \text{MAL}(E_S) \\
% \text{MAL}(V) &= \text{AdaLN}(W V + b)
% \end{split}
% \end{equation}
\begin{equation}
E_T = \text{MAL}(E_S) = \text{AdaLN}(W E_S + b) 
% \text{MAL}(V) &= \text{AdaLN}(W V + b)
\end{equation}
%
where MAL refers to the modality adaptation layer, with \( W \) and \( b \) as learnable parameters, and \text{AdaLN} indicating the adaptive normalization layer.

% To more effectively integrate semantic and collaborative knowledge, we introduce a cross-modality knowledge alignment task designed to align embeddings from the collaborative modality \( E_C \) and semantic modality \( E_T \). For a given item \( i \in I\), we obtain its collaborative embedding \( E_{C_i} \) and semantic embedding \( E_{T_i} \). The objective is to bring the collaborative embedding \( E_{C_i} \) closer to the semantic embedding \( E_{T_i} \) for item \( i \) while distancing \( E_{C_i} \) from the semantic embedding \( E_{T_j} \) of a different item \( j \), where \( j \) is an in-batch negative sample selected from the same batch as item \( i \). By structuring the alignment in this way, the learned embedding for item \( i \) is able to encapsulate knowledge from both modalities. To achieve this goal, we adopt the Info-NCE loss \cite{chen2020simple}. The loss function for our cross-modality knowledge alignment task is defined as follows:
%whz
\noindent\textbf{Cross-modality knowledge alignment task}. To effectively integrate semantic and collaborative knowledge, we introduce a cross-modality knowledge alignment task designed to align embeddings from the collaborative modality \( E_C \) and semantic modality \( E_T \). For a given item \( i \in I\), our objective is to bring its collaborative embedding \( E_{C_i} \) closer to its semantic embedding \( E_{T_i} \) for item \( i \) while distancing \( E_{C_i} \) from the semantic embedding \( E_{T_j} \) of a different item \( j \in I\), where \( j \) is an in-batch negative sample selected from the same batch as item \( i \). By structuring the alignment in this way, the learned embedding for item \( i \) is able to encapsulate knowledge from both modalities. To achieve this goal, we adopt the Info-NCE loss \cite{chen2020simple}. The loss function for our cross-modality knowledge alignment task is defined as follows:

\begin{equation}\label{eq:alignLoss}
\mathcal{L}_{\text{align}} = -\log {\sum_{i \in I}} \frac{\exp(\text{sim}(E_{C_i}, E_{T_i}) / \tau)}{\sum\limits_{{\substack{j \in \text{in-batch neg-samples} \\ j \neq i}}} \exp(\text{sim}(E_{C_i}, E_{T_j}) / \tau)}
\end{equation}

\noindent where \(\text{sim}(\cdot, \cdot)\) represents a similarity function between embeddings (e.g. dot product or cosine similarity), and \( \tau \) is a temperature parameter used to control the smoothness of the similarity.

\noindent\textbf{Next item prediction task}. In addition to the alignment loss described above, we adopt next item prediction task to enhance the model’s ability to  capture the sequence dependency, thereby supporting the learning of robust collaborative representations. This task takes the user’s historical interaction sequence \([ x_1, x_2, \dots, x_{t-1} ]\) as input, learns a representation of the user's preferences, and computes matching scores by measuring the similarity between the learned preferences and candidate items. These scores are subsequently normalized to derive the probability distribution over the target item. The goal is to maximize the probability assigned to the actual target item \( x_t \), which can be formally represented as follows:
\begin{equation}\label{eq:seqLoss}
\mathcal{L}_{\text{seq}} = - \sum_{t=2}^{L} \log p(x_t \mid x_1, x_2, \dots, x_{t-1})
\end{equation}

\noindent\textbf{Joint optimization}. In the first stage, we jointly optimize the two losses to train the modality adaption layer and sequential recommendation model. The total loss function for the first stage is defined as follows:
\begin{equation}
\mathcal{L}_{\text{Stage \uppercase\expandafter{\romannumeral 1}}} = \mathcal{L}_{\text{seq}} + \alpha \mathcal{L}_{\text{align}}
\end{equation}

\noindent where $\mathcal{L}_{\text{seq}}$ is the next item prediction loss defined in equation (\ref{eq:alignLoss}), $\mathcal{L}_{\text{align}}$ is the cross-modality knowledge alignment loss defined in equation (\ref{eq:seqLoss}), and \( \alpha \) is a tunable hyperparameter used to adjust the relative importance of the alignment loss.

% As shown in Figure \ref{fig:PRORec}, after training, following previous work \cite{wang2024eager}, we apply the widely-used hierarchical k-means algorithm to the learned item embeddings for quantization, where each cluster is recursively divided into \( K \) child clusters until each child cluster contains only a single item.

\noindent\textbf{Unified Codes}. After the training is finished, following previous work \cite{wang2024eager}, we apply the widely-used hierarchical k-means algorithm to the integrated embeddings for quantization. In this process, the embeddings are initially partitioned into \( K \) clusters. Each cluster is then recursively subdivided into K child clusters until each child cluster contains a single item. Finally, for each item, the cluster indices at each hierarchical level are recorded sequentially, forming a list that serves as the item's unicode.

% After quantization, we derive an item-unicode lookup table. These unicodes will serve as the identifier for items, facilitating generative recommendation in Stage \uppercase\expandafter{\romannumeral 2}.

\subsection{Stage \uppercase\expandafter{\romannumeral 2}: Generative Recommendation}

\noindent\textbf{Encoding process}. On the encoder side, given a user’s interaction history \( X = [x_1, x_2, \dots, x_{t-1}] \), we input it into an encoder consisting of stacked multi-head self-attention layers and feed-forward layers, following the Transformer architecture. The encoder processes \( X \) to produce a feature representation \( H \), which captures the user's interests and will be passed to the decoder.

\noindent\textbf{Decoding process}. On the decoder side, items are mapped to their unicodes derived in Stage  \uppercase\expandafter{\romannumeral 1}. Our objective changes from predicting the next item \( x_t \) to predicting its item unicode \( [c_1, c_2, \dots, c_L] \). For training, a special token \texttt{<BOS>} is prepended to the item unicode to construct the decoder input. The generative recommendation loss is computed using a cross-entropy loss function, defined as follows:
%
\begin{equation}
\mathcal{L}_{\text{gen}} = \sum_{i=1}^L \log p(c_i \mid x, \texttt{<BOS>}, c_1, \dots, c_{i-1}),
\end{equation}

\noindent\textbf{Intra-modality knowledge distillation task}. To compensate for the information loss caused by the quantization process in the first stage, we introduce an intra-modality knowledge distillation task to distill the global knowledge. Inspired by the use of the [CLS] token in BERT for capturing global context in classification tasks, we append a learnable token $[c_{\text{dis}}]$ to the end of the decoder input, allowing gradients to propagate through each preceding unicode. This design encourages $[c_{\text{dis}}]$ to capture global information.

Specifically, the final layer output corresponding to $[c_{\text{dis}}]$ is used in a global contrastive learning objective over the item corpus: the positive sample is the integrated embedding $E_t$ of item $x_t$ learned from the first stage, and the negative sample is a randomly selected integrated embedding $E_{\text{neg}}$ from other items in the corpus, excluding $x_t$.
 This objective pulls the $[c_{\text{dis}}]$ final layer output closer to the positive sample $E_t$ and pushes it away from the negative sample $E_{\text{neg}}$. The loss function for the distillation task is defined as follows:
\begin{equation}
\mathcal{L}_{\text{distillation}} = -\log \frac{\exp(c_{\text{dis}} \cdot E_t )}{\exp(c_{\text{dis}} \cdot E_t ) + \sum \exp(c_{\text{dis}} \cdot E_{\text{neg}} )}
\end{equation}
\noindent where \( c_{\text{dis}} \) denotes the final layer output of the decoder for the special token \([c_{\text{dis}}]\); \( E_{\text{neg}} \) is the integrated embedding for a negative sample as learned in Stage \uppercase\expandafter{\romannumeral 1}, while \( E_{t} \) represents the integrated embedding of the target item, also learned in Stage \uppercase\expandafter{\romannumeral 1}.

\noindent\textbf{Joint optimization}. Finally, in Stage \uppercase\expandafter{\romannumeral 2}, we jointly optimize these two loss functions. The overall loss function for Stage \uppercase\expandafter{\romannumeral 2} is defined as follows:

\begin{equation}
\mathcal{L}_{\text{Stage \uppercase\expandafter{\romannumeral 2}}} = \mathcal{L}_{\text{gen}} + \beta \mathcal{L}_{\text{distillation}}
\end{equation}

\noindent where $\mathcal{L}_{\text{gen}}$ is the generative recommendation loss, $\mathcal{L}_{\text{distillation}}$ is the intra-modality knowledge distillation loss, and $\beta$ is a hyperparameter that balances the two objectives.

\subsection{Training and Inference}
\textbf{Training.} we employ a two-stage training process. In the first stage, we use DIN as the backbone and optimize the model with loss \( \mathcal{L}_{\text{stage \uppercase\expandafter{\romannumeral 1}}} \). After the first stage training is complete, we extract the item integrated embeddings and derive item unicodes. In the second stage, we use a Transformer for generative recommendation, optimizing it with loss \( \mathcal{L}_{\text{stage \uppercase\expandafter{\romannumeral 2}}} \).

\noindent \textbf{Inference.} During inference phase, we rely exclusively on the Transformer model. In the decoding process, beam search is applied iteratively to generate each token within the item unicode sequence. Once finished, the item unicodes are mapped to their corresponding items through the item-unicode lookup table, creating a ranked recommendation list based on confidence scores to produce the final top-$k$ results.





\subsection{Computational and Storage Costs Analysis}
In our approach, we employ a unified code to encode both semantic and collaborative information. Regarding computational costs, the encoder processes the user interaction sequence only once, leaving the main computational burden to the decoder, which iteratively generates each token of the item code. Assuming the code length is fixed and decoding an item code requires $\mathcal{O}(n)$ time, existing methods \cite{wang2024eager, kim2024sc} that use separate codes for each modality require $K$ parallel decoders to predict codes for all modalities. Furthermore, these methods involve a confidence-based ranking step to derive the final recommendation. Excluding the computational cost of this ranking, their decoding cost is $\mathcal{O}(Kn)$, where $K$ is the number of modalities. In contrast, our method, with its single unified code, achieves a decoding cost of only $\mathcal{O}(n)$.

For storage costs, assuming the storage requirement for one code is $\mathcal{O}(m)$, existing methods require $\mathcal{O}(Km)$ space to store codes for all modalities. By contrast, our method requires only $\mathcal{O}(m)$ space to store multi-modal information within a single unified code.

\begin{table}[htbp]
    \centering
    \caption{Comparison of our method with typical generative recommendation methods TIGER and EAGER on computational and storage costs.}
    \label{tab:costs analysis}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc}
        \toprule
         &  TIGER & EAGER & PRORec (Ours)\\
        \midrule
        Computation Cost & $\mathcal{O}(n)$ & $\mathcal{O}(2n)$ & $\mathcal{O}(n)$ \\
        Storage Cost     & $\mathcal{O}(m)$ & $\mathcal{O}(2m)$ & $\mathcal{O}(m)$ \\
        Used Modality   & Semantic Only & Semantic + Collaborative &  Semantic + Collaborative \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Statistics of the Datasets.}
    \label{tab:Statistics of the Datasets}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        Dataset & \#Users & \#Items & \#Interactions & \#Density \\
        \midrule
        Beauty & 22,363 & 12,101 & 198,360 & 0.00073 \\
        Sports and Outdoors & 35,598 & 18,357 & 296,175 & 0.00045 \\
        Toys and Games & 19,412 & 11,924 & 167,526 & 0.00073 \\
        \bottomrule
    \end{tabular}
    }
\end{table}




\begin{table*}[htbp]
    \renewcommand{\arraystretch}{1.5}
    \centering
    \caption{Performance comparison of different methods. The best performance is highlighted in bold while the second best performance is underlined. All the results of PRORec are statistically significant with $p < 0.05$ compared to the best baseline models.}
    \label{tab:overall performance}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llccccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{7}{c}{\textbf{Classical}} & \multicolumn{3}{c}{\textbf{Generative}} & \multirow{2}{*}{\textbf{PRORec (Ours)}} \\
        \cmidrule(lr){3-9} \cmidrule{10-12}
        & & \textbf{GRU4REC} & \textbf{Caser} & \textbf{SASRec} & \textbf{BERT4Rec} & \textbf{HGN} & \textbf{FDSA} & \textbf{S$^3$-Rec} & \textbf{Recorest} & \textbf{TIGER} & \textbf{EAGER} \\
        \midrule
        \multirow{4}{*}{\textbf{Beauty}} 
        & Recall@10 & 0.0283 & 0.0347 & 0.0605 & 0.0347 & 0.0512 & 0.0407 & 0.0647 & 0.0664 & $0.0617^*$ & \underline{0.0836} & \textbf{0.0939} \\
        & Recall@20 & 0.0479 & 0.0556 & 0.0902 & 0.0599 & 0.0773 & 0.0656 & 0.0994 & 0.0915 & $0.0924^*$ & \underline{0.1124} & \textbf{0.1289}  \\
        & NDCG@10 & 0.0137 & 0.0176 & 0.0318 & 0.0170 & 0.0266 & 0.0208 & 0.0327 & 0.0400 & $0.0339^*$ & \underline{0.0525} & \textbf{0.0559} \\
        & NDCG@20 & 0.0187 & 0.0229 & 0.0394 & 0.0233 & 0.0332 & 0.0270 & 0.0414 & 0.0464 & $0.0417^*$ & \underline{0.0599} & \textbf{0.0646}  \\
        \midrule
        \multirow{4}{*}{\textbf{Sports}} 
        & Recall@10 & 0.0204 & 0.0194 & 0.0350 & 0.0191 & 0.0313 & 0.0288 & 0.0385 & 0.0247 & $0.0376^*$ & \underline{0.0441} & \textbf{0.0471}  \\
        & Recall@20 & 0.0333 & 0.0314 & 0.0507 & 0.0315 & 0.0477 & 0.0463 & 0.0607 & 0.0375 & $0.0577^*$ & \underline{0.0659} & \textbf{0.0710}   \\
        & NDCG@10 & 0.0110 & 0.0097 & 0.0192 & 0.0099 & 0.0159 & 0.0156 & 0.0204 & 0.0133 & $0.0196^*$ & \underline{0.0236} & \textbf{0.0259} \\
        & NDCG@20 & 0.0142 & 0.0126 & 0.0231 & 0.0130 & 0.0201 & 0.0200 & 0.0260 & 0.0164 & $0.0246^*$ & \underline{0.0291} & \textbf{0.0319} \\
        \midrule
        \multirow{4}{*}{\textbf{Toys}} 
        & Recall@10 & 0.0176 & 0.0270 & 0.0675 & 0.0203 & 0.0497 & 0.0381 & 0.0700 & 0.0383 & $0.0578^*$ & \underline{0.0714} & \textbf{0.0822}  \\
        & Recall@20 & 0.0301 & 0.0420 & 0.0941 & 0.0358 & 0.0716 & 0.0632 & 0.1065 & 0.0483 & $0.0838^*$ & \underline{0.1024} & \textbf{0.1154}  \\
        & NDCG@10 & 0.0084 & 0.0141 & 0.0374 & 0.0099 & 0.0277 & 0.0189 & 0.0376 & 0.0285 & $0.0321^*$ & \textbf{0.0505} & \underline{0.0489} \\
        & NDCG@20 & 0.0116 & 0.0179 & 0.0441 & 0.0138 & 0.0332 & 0.0252 & 0.0468 & 0.0310 & $0.0386^*$ & \underline{0.0538} & \textbf{0.0573} \\
        \bottomrule
    \end{tabular}}
\end{table*}

\section{EXPERIMENTS}
We analyze the proposed PRORec method and demonstrate its effectiveness by answering the following research questions:


\begin{itemize}[left=0pt]
\item \textbf{RQ1}: How does PRORec perform compared with existing best-performing sequential recommendation methods among different datasets?
\item \textbf{RQ2}: How effective is PRORec in mitigating the semantic domination issue?
\item \textbf{RQ3}: Do cross-modality knowledge alignment and intra-modality knowledge distillation tasks each contribute positively to PRORec’s performance?
\item \textbf{RQ4}: How do different ablation variants and hyper-parameter settings affect the performance of PRORec?
\end{itemize}

\subsection{Experimental Setting}
\textbf{Dataset.} We conduct experiments on three public benchmarks commonly used in the sequential recommendation task. For all datasets, we group the interaction records by users and sort them by the interaction timestamps ascendingly. Following \cite{rendle2010factorizing,zhang2019feature}, we only keep the 5-core dataset, which filters unpopular items and inactive users with fewer than five interaction records. Statistics of these datasets are shown in Table \ref{tab:Statistics of the Datasets}.
\begin{itemize}[left=0pt]
\item \textbf{Amazon}: Amazon Product Reviews dataset \cite{mcauley2015image}, containing user reviews and item metadata from May 1996 to July 2014. In particular, we use three categories of the Amazon Product Reviews dataset for the sequential recommendation task: "Beauty", "Sports and Outdoors", and "Toys and Games".
\end{itemize}




\noindent \textbf{Evaluation Metrics.} We employ two broadly used criteria for the matching phase, \textit{i.e.}, Recall and Normalized Discounted Cumulative Gain (NDCG). We report metrics computed on the top 10/20 recommended candidates. Following the standard evaluation protocol \cite{kang2018self}, we use the leave-one-out strategy for evaluation. For each item sequence, the last item is used for testing, the item before the last is used for validation, and the rest is used for training. During training, we limit the number of items in a user’s history to 20.


\noindent \textbf{Implementation Details.} 
In our experiments, we set the number of encoder layers to 1 and the number of decoder layers to 4. Following previous work \cite{feng2022recommender, wang2024eager}, we adopt DIN as our sequential recommendation model and use pretrained Llama2-7b as our semantic encoder, with the hidden size set to 128 as reported in \cite{rajput2023recommender, wang2024eager}. The number of clusters in the hierarchical k-means is set to 256. To train our model, we use the Adam optimizer with a learning rate of 0.001 and apply a warmup strategy for stable training. The batch size is set to 256. Notably, PRORec demonstrates robustness to the hyperparameters of the CKA and IKD tasks, attributed to their fast convergence. Consequently, the loss coefficients $\alpha$ and $\beta$ are both set to 1.





\subsection{Performance Comparison (RQ1)}
\textbf{Baselines.}We compare PRORec with several representative related baselines, encompassing classical sequential modeling approaches as well as the latest emergent generative techniques.

\noindent (1) For \textit{classical sequential} methods, we have:
\begin{itemize}[left=0pt]
    \item \textbf{GRU4REC} \cite{hidasi2015session} is an RNN-based model that utilizes Gated Recurrent Units (GRUs) to model user click sequences.
    \item \textbf{Caser} \cite{tang2018personalized} is a CNN-based method capturing high-order Markov Chains by modeling user behaviors through both horizontal and vertical convolutional operations.
    \item \textbf{SASRec} \cite{kang2018self} is a self-attention-based sequential recommendation model that utilizes a unidirectional Transformer encoder with a multi-head attention mechanism to effectively model user behavior and predict the next item in a sequence.
    \item \textbf{BERT4Rec} \cite{sun2019bert4rec} adopts a Transformer model with the bidirectional self-attention mechanism and uses a Cloze objective loss for the modeling of item sequences.
    \item \textbf{HGN} \cite{ma2019hierarchical} adopts hierarchical gating networks to capture long-term and short-term user interests.
    \item \textbf{FDSA} \cite{zhang2019feature} leverages self-attention networks to separately model item-level and feature-level sequences, emphasizing the transformation patterns between item features and utilizing a feature-level self-attention block to capture feature transition dynamics.
    \item \textbf{S$^3$-Rec} \cite{zhou2020s3} employs mutual information maximization to pre-train a bi-directional Transformer for sequential recommendation, enhancing the model's ability to learn correlations between items and their attributes through self-supervised tasks.
\end{itemize}

\noindent (2) For \textit{generative} methods, we have:
\begin{itemize}[left=0pt]
    \item \textbf{RecForest} \cite{feng2022recommender} jointly learns latent embeddings and indices through multiple \(K\)-ary trees, utilizing hierarchical balanced clustering and a transformer-based encoder-decoder routing network to enhance accuracy and memory efficiency in identifying top-n items.
    \item \textbf{TIGER} \cite{rajput2023recommender} utilizes a pre-trained T5 encoder to learn semantic identifiers for items, autoregressively decodes target candidates using these identifiers, and incorporates RQ-VAE quantization to build generalized item identifiers without relying on sequential order.
    \item \textbf{EAGER} \cite{wang2024eager} integrates behavioral and semantic information through a two-stream architecture with shared encoding, separate decoding, and strategies for contrastive and semantic-guided learning to enhance collaborative information utilization.
\end{itemize}

\noindent \textbf{Results.} Table \ref{tab:overall performance} reports the overall performance of three datasets. The results for all baselines without the superscript $^*$ are taken from the publicly accessible results \cite{zhou2020s3, wang2024eager}. Due to missing values for TIGER, we reimplement it and report our experimental results. From the results, we have the following observations:
\begin{itemize}[left=0pt]
    \item \textbf{PRORec almost achieves better results than base models among different datasets.} Specifically, on the Beauty dataset, compared to the second best baseline EAGER, we achieve improvements of 14.68\% and 7.85\% in \textit{Recall@20} and \textit{NDCG@20}, respectively, which represents a significant improvement in the recommendation field. We attribute the improvements to PRORec's effective exploitation of the complementary strengths of collaborative and semantic knowledge with a unified code, which allows it to achieve superior performance relative to EAGER, despite utilizing approximately half the number of total parameters.
    \item \textbf{PRORec beats the previous generative models on most datasets.}
    Compared to previous generative models, PRORec employs a two-stage multi-task joint optimization framework to integrate collaborative and semantic knowledge with a unified code, which successfully captures the interplay between these two types of knowledge, allowing for a more thorough and efficient exploitation of their complementary strengths. The improvements validate the effectiveness of our method and demonstrate the necessity of a unified code.
    \item \textbf{In most cases, generative models outperform classical models.}
    Unlike traditional sequence modeling approaches, which use non-informative item IDs as identifiers, generative models assign hierarchical codes to items as identifiers. This hierarchical structure inherently encodes prior knowledge, facilitating a more effective alignment with the token-by-token generation framework of the Transformer decoder.
\end{itemize}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/distribution.png}
  \caption{T-SNE visualization of different embeddings distribution after applying K-means clustering (clusters=10).}
  \Description{}
  \label{fig:distribution}
\end{figure}


\begin{table}[ht]
  \caption{Proportional similarity of the semantic modality and collaborative modality to the final representation across different approaches.}
  \label{tab:relative influence}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{ccc}
    \toprule
    Approach & Semantic modality & Collaborative modality\\
    \midrule
    Concat & 97.33\% & 2.67\% \\
    Ours & 59.89\% & 40.11\% \\
    \bottomrule
\end{tabular}
}
\end{table}


\begin{table}[htbp]
    \centering
    \caption{Performance of different methods.}
    \label{tab:fusion methods}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    Dataset & \multicolumn{4}{c}{Beauty} \\
    \cmidrule(lr){2-5}
    Metric & Recall@10 & Recall@20 & NDCG@10 & NDCG@20 \\
    \midrule
    Semantic Only     & 0.0810 & 0.1102 & 0.0498 & 0.0571 \\
    Collaborative Only & 0.0803 & 0.1095 & 0.0495 & 0.0569 \\
    Concat            & 0.0807 & 0.1085 & 0.0504 & 0.0574 \\
    Ours     & \textbf{0.0939} & \textbf{0.1289} & \textbf{0.0559} & \textbf{0.0646} \\
    \bottomrule
    \end{tabular}
    }
\end{table}




\subsection{Semantic Domination Issue (RQ2)}
To investigate whether our method addresses the semantic dominance issue, we conduct experiments on the Amazon Beauty dataset. We first extract the semantic embeddings of the items, then use only the item IDs from historical interaction data and apply the classical DIN model to obtain the collaborative embeddings. Following the common approach of fusing knowledge from different modalities, we concatenate the semantic embeddings with the collaborative embeddings. Notably, to mitigate the issue caused by the large dimensional differences between embeddings from the two modalities, we first apply PCA to reduce the dimension of the semantic embeddings to match that of the collaborative embeddings, and then concatenate them to obtain the concatenation embeddings. In addition, we obtain the integrated embeddings of items using our proposed method. We use T-SNE to visualize the distribution of these different embeddings. To more clearly illustrate the differences in their distributions, we apply K-means clustering algorithm to these embeddings, using different colors to represent data points from each cluster, as shown in Figure \ref{fig:distribution}. 



Furthermore, to quantitatively measure the relative similarity of semantic embeddings and collaborative embeddings on the final embeddings using different methods, we use KL divergence to assess the similarity of their distribution. The relative similarity of each modality is then calculated based on the KL divergence values. The formula for above process is as follows:
\begin{equation}
\text{Similarity}_{\text{semantic}} = 1-\frac{{\text{KL}}(S \parallel E)}{{\text{KL}}(S \parallel E) + {\text{KL}}(C \parallel E)}
\end{equation}

\begin{equation}
\text{Similarity}_{\text{collaborative}} = 1-\frac{{\text{KL}}(C \parallel E)}{{\text{KL}}(S \parallel E) + {\text{KL}}(C \parallel E)}
\end{equation}
\noindent where \( E \) represents the final embeddings using different methods, \( S \) denotes the semantic embeddings, and \( C \) refers to the collaborative embeddings.

% From the substantial differences observed in Table \ref{tab:relative influence}, where the concat embeddings method was influenced by two distinct modalities, and the strong resemblance between the distributions of concat embeddings and semantic embeddings in Figure \ref{fig:distribution}, we confirm the presence of semantic dominance issue. The traditional concat method excessively favors embeddings from the semantic modality, with minimal integration of embeddings from the collaborative modality, which can significantly impair recommendation performance. Our proposed two-stage progressive fusion approach successfully mitigates this issue. It not only learns embeddings from both collaborative and semantic modalities in a balanced manner within the distribution space but also achieves a more equitable influence between them in the quantitative representation, avoiding semantic modality dominance in the final embeddings. As shown in Table \ref{tab:fusion methods}, our method achieves the best performance compared to single-modality approaches and the concat-based fusion method, which combines semantic and collaborative knowledge through direct concatenation. Notably, the concat method underperforms even the single-modality semantic approach on both Recall@10 and Recall@20 metrics, reflecting a "1+1<1" effect. This observation underscores the critical issue of semantic dominance, where one modality suppresses the contribution of the other, and highlights the effectiveness of our method in mitigating this limitation. We attribute this substantial improvement to the progressive nature of the two-stage process, which facilitates more effective knowledge integration across different modalities compared to the direct concatenation of embeddings. Our method promotes a balanced exchange of information, allowing knowledge from each modality to be shared and integrated more uniformly.



As shown in Table \ref{tab:relative influence}, the concatenation method exhibits a significant disparity in similarity proportions between semantic and collaborative modalities, and Figure \ref{fig:distribution} further demonstrates that the embeddings obtained by the concatenation method are highly similar to the distribution of semantic embeddings. These observations confirm the presence of the semantic dominance problem. The traditional concatenation method tends to overly emphasize semantic knowledge while greatly ignoring collaborative knowledge, which poses a substantial obstacle to downstream recommendation effectiveness. In contrast, our proposed method effectively addresses this issue. It not only learns embeddings from both collaborative and semantic modalities in a more balanced manner at the distributional level but also achieves a more proportional similarity from each modality in quantitative evaluations. This prevents the semantic embeddings from dominating the final representation. As illustrated in Table \ref{tab:fusion methods}, our method significantly outperforms both single-modality approaches and the concatenation-based fusion method, achieving the best results across all evaluation metrics. Interestingly, the concatenation method performs worse than the semantic-only approach in Recall@10 and Recall@20, indicating a negative impact on performance. This further highlights the severity of the semantic dominance problem and underscores the advantages of our proposed approach. We attribute these improvements to the successful integration of collaborative and semantic knowledge, which, unlike the concatenation method, better captures the complementary nature of the two different knowledge, enabling a more balanced and effective fusion of information from both modalities.


\begin{table}[t]
    \centering
    \caption{Performance comparison of different variants.}
    \label{tab:ablation study}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Dataset} & \textbf{Metric} & \textbf{\textbackslash} & \textbf{CKA} & \textbf{CKA + IKD (Ours)} \\
        \midrule
        \multirow{4}{*}{Beauty}
        & Recall@10   & 0.0807 & 0.0827 & 0.0939 \\
        & Recall@20   & 0.1085 & 0.1122 & 0.1289 \\
        & NDCG@10     & 0.0504 & 0.0509 & 0.0559 \\
        & NDCG@20     & 0.0574 & 0.0583 & 0.0646 \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/model_layer.pdf}
  \caption{Impact of the number of model layers on the Beauty dataset. The number of layers varies from 1 to 32.}
  \Description{}
  \label{fig:model layer}
\end{figure}


\subsection{Ablation Study (RQ3)}
We conducted a thorough evaluation of the contribution of each component in PRORec. Specifically, we incrementally removed the intra-modality knowledge distillation (IKD) task and the cross-modality knowledge alignment (CKA) task to create the ablation architectures. The results in Table \ref{tab:ablation study} reveal the following insights:

\begin{itemize}[left=0pt]
\item Removing either the CKA or IKD task degrades the model's performance, while omitting both results in the worst outcome (i.e., the baseline model obtained by simply concatenating semantic and collaborative embeddings). These findings confirm the effectiveness of the two proposed tasks.
\item The introduction of the CKA task enhances recommendation performance, as it effectively integrates knowledge from both collaborative and semantic modalities. By leveraging their complementary characteristics, the CKA task enables the model to learn more expressive and informative item representations.
\item Incorporating both the CKA and IKD tasks allows the model to achieve state-of-the-art performance. Building on the foundation established by the CKA task, the IKD task further promotes the learning of integrated knowledge by encouraging the model to better exploit the complementary strengths of collaborative and semantic knowledge. The observation also highlights the crucial role of both tasks in enabling the model to acquire more powerful unified codes of items.
\end{itemize}

In conclusion, each component of PRORec is essential to enhancing the effectiveness of generative recommendation.


\begin{figure*}[t]
  \centering
  % first fig
  \begin{minipage}[t]{0.325\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/semantic_encoder.pdf}
    \caption{Analysis of different pretrained semantic encoders.}
    \label{fig:semantic_encoder}
  \end{minipage}%
  \hfill
  % second fig
  \begin{minipage}[t]{0.325\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/init_way.pdf}
    \caption{Analysis of different quantization methods.}
    \label{fig:quantization_methods}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.33\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figure/branch_size.pdf}
  \caption{Impact of the number of clusters with $K$ varying from 128 to 512.}
  \label{fig:number of cluster}
  \end{minipage}
\end{figure*}

\subsection{Scaling Law Study (RQ4)}
Generative models have gained considerable attention due to their potential to exhibit scaling law properties. However, research exploring this phenomenon in the context of generative recommendation remains limited. Although recent study like HSTU \cite{zhai2024actions} has examined its scaling law characteristics, they are based solely on collaborative information derived from pure item IDs. To the best of our knowledge, no studies have investigated the scaling law of models that integrate both semantic and collaborative information. In this work, we are the first to explore this and compare our method with HSTU to analyze the scaling law differences between the two approaches.

As shown in Figure \ref{fig:model layer}, while HSTU demonstrates scaling law on the MovieLens-20M dataset in \cite{zhai2024actions}, it exhibits performance stagnation on the Amazon Beauty dataset once the model reaches eight layers. Beyond this point, the model's performance begins to decline as the number of layers increases, a trend consistent with the observations in Table 1 of \cite{guo2024scaling} on the Amazon Book dataset. In contrast, our method continues to show performance improvements on the Beauty dataset as the number of layers increases. Furthermore, our method consistently outperforms HSTU across all configurations, achieving a significant improvement of up to 21.42\% in NDCG@20, demonstrating its superiority and effectiveness.

We attribute the superior performance of our method to its ability to simultaneously incorporate both collaborative and semantic knowledge with a unified code, effectively capturing their complementary nature. As a result, deeper models demonstrate enhanced capability in integrating these two types of knowledge, thus leading to consistent performance improvements.

% \begin{table}[ht]
%     \centering
%     \caption{Analysis of different pretrained semantic encoders on the Beauty dataset.}
%     \label{tab:semantic encoders}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ccccc}
%         \toprule
%         {\textbf{Dataset}} & {\textbf{Metric}} & {\textbf{BERT}} &{\textbf{Sentence-T5}} & {\textbf{Llama2-7b}} \\
%         \midrule
%         \multirow{4}{*}{Beauty} & Recall@10 & 0.0886 & 0.0909 & \textbf{0.0939} \\
%          & Recall@20 & 0.1191 & 0.1230 & \textbf{0.1289} \\
%          & NDCG@10 & 0.0535 & 0.0544 & \textbf{0.0559} \\
%          & NDCG@20 & 0.0611 & 0.0624 & \textbf{0.0646} \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}

% \begin{table}[ht]
%     \centering
%     \caption{Analysis of different quantization methods on the Beauty dataset.}
%     \label{tab:quantization methods}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ccccc}
%         \toprule
%         {\textbf{Dataset}} & {\textbf{Metric}} & {\textbf{Random}} &{\textbf{RQVAE}} & {\textbf{K-means}} \\
%         \midrule
%         \multirow{4}{*}{Beauty} & Recall@10 & 0.0849 & 0.0864 & 0.0939 \\
%          & Recall@20 & 0.1176 & 0.1200 & 0.1289 \\
%          & NDCG@10 & 0.0521 & 0.0526 & 0.0559 \\
%          & NDCG@20 & 0.0604 & 0.0610 & 0.0646 \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}


% \begin{figure}[htbp]
%   \centering
%   % first fig
%   \begin{minipage}[t]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/semantic_encoder.pdf}
%     \caption{Analysis of different pretrained semantic encoders on the Beauty dataset.}
%     \label{fig:semantic_encoder}
%   \end{minipage}%
%   \hfill
%   % second fig
%   \begin{minipage}[t]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/init_way.pdf}
%     \caption{Analysis of different quantization methods on the Beauty dataset.}
%     \label{fig:quantization_methods}
%   \end{minipage}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=\linewidth]{figure/branch_size.pdf}
%   \caption{Impact of the number of clusters. The number of clusters K varies from 128 to 512.}
%   \Description{}
%   \label{fig:number of cluster}
% \end{figure}






\subsection{Hyper-Parameter Analysis (RQ4)}
\noindent \textbf{Choice of different semantic encoders.} To further investigate the influence of different pre-trained semantic encoders on recommendation performance, we conducted additional experiments on the Beauty dataset. Specifically, we employed various pre-trained semantic encoders to extract item semantic knowledge and evaluated their effectiveness using our proposed method. As presented in Figure \ref{fig:semantic_encoder}, the results reveal the following insights: (1) Llama2-7b achieved the best performance, likely due to its substantially larger training corpus compared to Sentence-T5 and BERT. This extensive corpus enables Llama2-7b to produce richer semantic embeddings, which allow the unicodes to capture more comprehensive semantic knowledge, thereby enhancing the model’s ability to perform generative recommendation. (2) Our method is inherently plug-and-play, meaning it can directly incorporate the latest advancements from large language models without requiring any modifications to the underlying model architecture. This flexibility allows us to fully leverage the capability of state-of-the-art large language models, leading to improved recommendation performance.\\

\noindent \textbf{Choice of different quantization methods.} To evaluate the impact of different quantization methods on model performance, we conducted a comparative analysis on the Beauty dataset. We apply three different quantization methods: random assignment, RQVAE, and hierarchical K-means to discretize item representations. As illustrated in Figure \ref{fig:quantization_methods}, we observe that hierarchical K-means outperformed the other two methods. Compared to meaningless ID information, hierarchical K-means effectively captures the hierarchical relationship among items, where items sharing the same prefix codes are more similar than different prefix codes and thus more likely to be recommended to users. Additionally, in practice, we find that the RQVAE method suffers from persistent codebook conflicts during quantization, where multiple items are frequently assigned to a same code. To mitigate this issue, following a common practice \cite{rajput2023recommender}, we append a unique identifier at the end to ensure uniqueness. However, this approach partially disrupts the inherent hierarchical structure of the codebook, which could result in suboptimal performance in downstream recommendation tasks.\\

\noindent \textbf{Number of Clusters.} We analyzed the effect of the number of clusters \(K\) in the hierarchical K-means algorithm on model performance on the Beauty dataset. Specifically, we vary \(K\) across 128, 256, and 512. As shown in Figure \ref{fig:number of cluster}, increasing \(K\) from 128 to 256 enhances model performance, likely because a smaller \(K\) limits the representational capacity of the codebook, making it hard to assign a distinct unicode to each item. Consequently, many dissimilar items are grouped under similar codes, reducing representational accuracy. When \(K\) increases further from 256 to 512, the NDCG@20 metric improves, but the Recall@20 metric declines. This discrepancy may arise due to the larger search space, which increases the complexity of the decoding process. These findings highlight that for generative recommendation systems, selecting an appropriate number of clusters proportional to the total number of items is essential to balancing representation capacity and computational complexity. 

\section{CONCLUSION AND FUTURE WORK} In this paper, we introduce a novel framework, PRORec, aimed at leveraging a unified code to encode knowledge from both semantic and collaborative modalities in generative recommendation systems. PRORec is composed of three key components: (1) A unified code generation framework that integrates semantic and collaborative knowledge, addressing the semantic domination issue by effectively capturing their complementary characteristics. (2) A joint optimization of cross-modality knowledge alignment and next item prediction tasks and a learnable modality adaption layer with AdaLN, which integrate collaborative and semantic knowledge into integrated embeddings, enabling the unified code to represent both modalities information cohesively. (3) An intra-modality knowledge distillation task with a specially designed token, which compensates for the information loss caused by the quantization process and further improves auto-regressive generation quality. Extensive experiments compared with state-of-the-art methods, combined with detailed analyses, validate the effectiveness and robustness of PRORec. In future work, we plan to leverage more available features from the semantic modality, such as item visual features, to further enhance the model's generative recommendation performance.




%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.