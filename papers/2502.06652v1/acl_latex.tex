% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{subcaption}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% Include algorithm packages to render (pseudo) code
\usepackage{listings}
\usepackage{amssymb} % For ← symbol support
\usepackage{amsmath}
\usepackage{amstext}

\usepackage{xcolor}

\usepackage{enumitem}
\usepackage{fancyvrb} % For code-like environments

\usepackage{booktabs} % For cleaner Table~lines

\usepackage{mdframed} % For creating boxed content

% The breqn package can automatically break equations to fit the column width:
\usepackage{breqn}

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\bfseries\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{gray},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray},
  escapeinside={(*@}{@*)}
}

\usepackage{dblfloatfix}
\usepackage{placeins}    % Ensure floats stay within sections

% to-do
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
\newcommand{\commentbp}[1]{\textcolor{blue}{BP: #1}\PackageWarning{TODO:}{#1!}}
\newcommand{\commentdh}[1]{\textcolor{orange}{TODO: #1}\PackageWarning{TODO:}{#1!}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Transparent NLP: Using RAG and LLM Alignment for Privacy Q\&A}

% Author information can be set in various styles:
% For several authors from the same institution:
%\author{Anna Leschanowsky \and Zahra Kolagar \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  \textbf{Anna Leschanowsky\textsuperscript{1}},
  \textbf{Zahra Kolagar\textsuperscript{1}},
  \textbf{Erion \c{C}ano\textsuperscript{2}},
  \textbf{Ivan Habernal\textsuperscript{2}},
\\
  \textbf{Dara Hallinan\textsuperscript{3}},
  \textbf{Emanuël A. P. Habets\textsuperscript{1,4}},
  \textbf{Birgit Popp\textsuperscript{1}}
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
  \textsuperscript{1}Fraunhofer Institute for Integrated Circuits (IIS) \\
  \textsuperscript{2}Ruhr University Bochum \& Research Center Trustworthy Data Science and Security\\
  \textsuperscript{3}FIZ Karlsruhe - Leibniz Institute for Information Infrastructure \\
  \textsuperscript{4}International Audio Laboratories Erlangen
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
\\
\small{
    \textbf{Correspondence:} \href{mailto:anna.leschanowsky@iis.fraunhofer.de}{anna.leschanowsky@iis.fraunhofer.de}
    }
}

\begin{document}
\maketitle

\begin{abstract}
The transparency principle of the General Data Protection Regulation (GDPR) requires data processing information to be clear, precise, and accessible. While language models show promise in this context, their probabilistic nature complicates truthfulness and comprehensibility.

This paper examines state-of-the-art Retrieval Augmented Generation (RAG) systems enhanced with alignment techniques to fulfill GDPR obligations. We evaluate RAG systems incorporating an alignment module like Rewindable Auto-regressive Inference (RAIN) and our proposed multidimensional extension, MultiRAIN, using a Privacy Q\&A dataset. Responses are optimized for preciseness and comprehensibility and are assessed through 21 metrics, including deterministic and large language model-based evaluations.

Our results show that RAG systems with an alignment module outperform baseline RAG systems on most metrics, though none fully match human answers. Principal component analysis of the results reveals complex interactions between metrics, highlighting the need to refine metrics. This study provides a foundation for integrating advanced natural language processing systems into legal compliance frameworks.

\end{abstract}

\section{Introduction}

European legislation, such as Articles 5 and 12–15 of the GDPR \citep{gdpr2016}, emphasizes the transparency principle for personal data processing. This principle requires that information about personal data usage be communicated to users in a way that is easily accessible, easy to understand, and written in clear, plain language \cite{Article29WP2018}.

Large Language Models (LLMs) have the potential to support transparency but face technical challenges in ensuring communication is both precise—truthful and free from hallucinations—and comprehensible, i.e., simple and clear.
State-of-the-art LLM applications, exemplified by ChatGPT, illustrate these issues. ChatGPT's responses about data processing were demonstrably inaccurate and hallucinatory (see Appendix~\ref{app:ChatGPT_data_processing}), failing to meet transparency requirements. In December 2024, these shortcomings led the Italian data protection authority to fine OpenAI over 15 million euros, citing violations of transparency and information obligations \cite{reuters2024italy}.

In this paper, we therefore investigate three research questions:
(1) To what extent are variants of Retrieval Augmented Generation (RAG) and RAIN systems useful in aligning generated answers with human-generated answers in terms of preciseness and comprehensibility?
(2) How effective are state-of-the-art NLP metrics in measuring constructs like preciseness and comprehensibility?
(3) What technical and legal questions arise when utilizing RAG and RAIN systems to address privacy-related questions?

We address these questions by implementing and evaluating a series of systems that leverage LLMs for answering data processing questions in natural language, using a gold-standard Privacy Q\&A dataset~\cite{expert_privacy_qa}. For some systems, we add alignment modules that iteratively align generated responses in terms of preciseness and comprehensibility (see Sections \ref{subsec:rain} and \ref{sec:multirain}) to better comply with the GDPR's principle of transparency. In practice, we optimize generated responses for both preciseness and comprehensibility. Since these are abstract constructs, we operationalize them using various methods and metrics (see Section~\ref{subsec:models_optimization-metrics}). Moreover, we employ 21 state-of-the-art metrics to evaluate our systems' alignment with transparency, as no standard currently exists for assessing GDPR transparency requirements (see Section~\ref{subsec:evaluation-metrics} for details).


%% TODO: Some concluding remarks and future work should be added here

\section{Related work}

\citet{Harkous2016PriBotsCP} were the first to show that conversational technology can respond to users' questions about the processing of their personal data. They called their system Pribots. Similar approaches using retrieval technology for answering privacy-related questions have since been tested~\cite{mysore-sathyendra-etal-2017-identifying, ravichander-etal-2019-question, ahmad-etal-2020-policyqa}. These papers use classical NLP methods rather than leveraging LLMs, which were not available at the time of publication. Note that Pribots' output is based on extractions of legal texts. Thus, legal texts are quoted directly, which can be problematic for transparency, as legal texts can be difficult to understand, even for legal experts \cite{martinez2023, Article29WP2018}.

% \citet{expert_privacy_qa} generated answers to data processing questions aligned with the GDPR's transparency principle as iterative optimization by multi-disciplinary human experts, namely legal experts and dialog design experts, for preciseness and comprehensibility. These optimized answers present a gold standard. However, generating answers with human experts only and offering them in an FAQ format has at least two transparency issues: (1) FAQs are not necessarily integrated into NLP systems, requiring users to switch interaction modes (e.g., from interacting in natural language to website navigation and reading) affecting accessibility, and (2) even if integrated, FAQs do not cover all possible user questions, creating a potential information gap where requests are not already directly considered in an FAQ. 

\citet{expert_privacy_qa} generated GDPR-aligned answers through iterative optimization by legal and dialogue design experts, thereby fostering preciseness and comprehensibility. These answers serve as a gold standard. However, using them as FAQs poses two transparency issues: (1) they may not integrate with NLP systems, forcing users to switch interaction modes and thereby impacting accessibility, and (2) they cannot cover all user questions, creating potential information gaps.

RAG systems represent a state-of-the-art method to address these issues by combining LLMs with a document database, including, for instance, privacy notices, FAQs, and technical documentation. However, because RAG systems are built on top of LLMs, which are non-deterministic, there exists legal uncertainty as transparency violations may be caused by LLM hallucination when providing such generated answers to data processing questions. This is a well-known problem of RAG and LLM systems, typically mitigated by aligning responses to criteria chosen by developers \cite{Askell2021AGL, pmlr-v235-huang24x}. This work takes the first step toward investigating the feasibility of using RAG to address transparency issues in NLP systems.

\section{Experimental setup}

\subsection{Datasets}

%% TODO: something more about the datasets here - not typical to have such short subsection
We use a %\href{https://github.com/audiolabs/Expert-Generated-Privacy-QA-Dataset}{dataset that was built with legal and linguistic experts} 
dataset that was built with legal and linguistic experts~\cite{expert_privacy_qa}. 
The authors presented experts with a document database consisting of Alexa's privacy notice as well as Alexa's FAQ pages together with 42 questions including different intents in the domain of data processing. Both legal and linguistic experts generated answers to these questions, taking turns to ensure both legal preciseness and linguistic simplicity. We use this dataset for question-answering, model alignment, and evaluation.


\subsection{Models and optimization metrics}
\label{subsec:models_optimization-metrics}

We use a retrieval augmented generation (RAG) system as baseline for answering questions about data processing precisely and comprehensibly. With this baseline system, we compare implementations of the already existing RAIN \cite{li2024rain} system, and our proposed MultiRAIN system.

\subsubsection{RAIN}
\label{subsec:rain}

We use Rewindable Auto-regressive INference (RAIN) of \citet{li2024rain} to align six of our nine systems under test. RAIN operates as a rewindable tree-search. The user query forms the tree's stem, guiding response generation. Words or partial sentences generated are tree leaves, evaluated on parameters like preciseness and comprehensibility. RAIN's rewindable algorithm allows revisiting earlier nodes to generate new, potentially better-scoring tokens. If a generated word deviates from desired properties, the model rewinds to try alternatives. This self-evaluative process, triggered by prompts, runs invisibly within the processing pipeline. Compared with other approaches, such as alignment with reinforcement learning, RAIN has the advantage of not requiring costly training and data collection. It can be implemented as a plugin in existing language models. Moreover, RAIN performs as well as other state-of-the-art approaches in aligning language model output \cite{li2024rain}.

It is important to note that RAIN optimizes based on retrieved responses. If the retrieved response is incorrect, RAIN does not fix that. However, retrieval is not the problem we are focusing on, and thus, we keep the retrieval module the same across systems. Instead, we are focusing on investigating ways to align multiple features in LLM generation, such as preciseness and comprehensibility. 

\subsubsection{MultiRAIN}
\label{sec:multirain}

RAIN was designed for uni-dimensional optimization problems, but we aim to optimize two criteria: preciseness and comprehensibility. To address this, we propose MultiRAIN, a multidimensional adaptation of the RAIN algorithm. 

To explain MultiRAIN, we use the notation introduced by \citet{li2024rain}. We refer to \citet{li2024rain} for further details on unchanged processing components. We denote individual tokens or values by lowercase letters, such as \( y \), and present sequences of tokens or values by uppercase letters, such as \( Y \). In particular, \( Y_{i:j} \) refers to the token set \( (y_i, y_{i+1}, y_{i+2}, \dots, y_j) \). The RAIN algorithm starts from the root node (the user query) and selects the next token set based on the formula:
%
\begin{dmath}
\label{eq:forward_process}
Y' = \underset{Y_{i:j}}{\text{arg max}} \, \big( f(V_{\alpha:\beta}(Y_{i:j}; Y_{1:i-1}), \theta_{\alpha:\beta}) + c \cdot u(Y_{i:j}; Y_{1:i-1}) \big)
\end{dmath}
%
where
\( f \) is a function to combine multiple metrics,
\( V_{\alpha:\beta} \) is a set of values of metrics,
\( Y_{i:j} \) are tokens that are being generated,
\( Y_{1:i-1} \) are all the tokens that have been previously generated,
\( c \) is a regularization hyper-parameter balancing exploitation and exploration of the optimization search,
\( u(Y_{i:j}; Y_{1:i-1}) \) indicates the extent to which a token set has been explored. The value \( u(Y_{i:j}; Y_{1:i-1}) \) increases when rarely visited branches are explored (see \citet[pp.~5--6]{li2024rain} for a detailed description). If \(V\) represents values of a single metric, the equation aligns with RAIN. 

% \noindent \textbf{Function \( f \): Combining Multiple Metrics} 
\paragraph{Function \( f \): Combining Multiple Metrics}
The function \( f(V_{\alpha:\beta}(Y_{i:j}; Y_{1:i-1}), \theta_{\alpha:\beta}) \) reflects a method to combine the values \( V_{\alpha:\beta} \), e.g., via a sum or an average. Moreover, \( f(V_{\alpha:\beta}(Y_{i:j}; Y_{1:i-1}), \theta_{\alpha:\beta}) \) penalizes any individual value below a threshold \( \theta \), guaranteeing that a minimum level of desired metrics (e.g., preciseness and comprehensibility) is reached. A general formulation of \( f \) is:
%
\begin{equation}
\begin{aligned}
f(V_{\alpha:\beta}(Y_{i:j}; Y_{1:i-1}), \theta_{\alpha:\beta}) = \\
g(V_{\alpha:\beta}(Y_{i:j}; Y_{1:i-1})) \cdot p 
\end{aligned}
\tag{Eq. B}
\end{equation}
%
where
\( g \) is a combination function like an average or a sum, and \( p \) is a penalty factor.

We show an example implementation of \(f\) in Appendix~\ref{app:example_implementation_f}.

\paragraph{Backward process}
After reaching the leaf node \( Y_{i:j} \), a multidimensional evaluation is performed that computes scores \( s_{\alpha:\beta}(Y_{1:j}) \) using prompting as showcased in Prompt (A). This self-evaluation initiates the ``backward process'' as described by \citet[pp.~6--7]{li2024rain}. 

Scores \( s \) are the basis for values \( V_{\alpha:\beta}(Y_{i:j}; Y_{1:i-1}) \) in that the value \( v_{\alpha}(Y_{i:j}; Y_{1:i-1}) \) represents the mean scores of the token sequences that take \( Y_{1:j} \) as their prefix \citep[p.~6]{li2024rain}. We present an implementation of MultiRAIN in pseudo-code in Appendix~\ref{app:MultiRAIN_algorithm}. Our implementation of MultiRAIN will be published as part of the Transparent NLP GitHub repository, pending institutional approval (\href{https://github.com/audiolabs/transparentnlp}{https://github.com/audiolabs/transparentnlp}).

\subsection{Study design}

In an ablation study, we implemented a RAG system (VanillaRAG) and incorporated alignment modules based on RAIN or MultiRAIN, resulting in nine systems: one RAG-only and eight with RAG plus an alignment module. These eight systems are grouped into three experiments, differentiated by the metrics used during alignment (response generation) and evaluation (post-generation). Only alignment metrics change across experiments, whereas evaluation metrics remain constant. 

\begin{itemize} 
    \item Experiment 1: Uses LLM-as-a-judge metrics for alignment, with distinct definitions (i.e., prompts) for alignment and evaluation metrics and a binary self-evaluation strategy (see Section~\ref{subsec:system_implementation} for more details).
    \item Experiment 2: Uses LLM-as-a-judge metrics for alignment, with similar definitions (i.e., prompts) for optimized and evaluated metrics and continuous self-evaluation strategy (see Section~\ref{subsec:system_implementation} for more details). 
    \item Experiment 3: Uses statistical metrics for alignment, with similar definitions (i.e., the same implementations) for optimized and evaluated metrics. 
\end{itemize}

The purpose of \textbf{Experiment~1} was to test if optimizations generalize across different constructs and different models. The model used for alignment (Mistral-7B-Instruct-v0.2) is different from the model used for evaluation (GPT-4). In addition, we employed different constructs by changing the name and definitions used in alignment and evaluation. For example, we aligned with the construct ``honesty'', which is commonly used in the NLP field \cite{Askell2021AGL, Owain_2021_Truthful_AI} to refer to properties related to the legal construct ``preciseness''. However, in our evaluation, we did not use this construct but constructs described in Section~\ref{subsec:evaluation-metrics}. Similarly, we did not use a ``comprehensibility'' metric for evaluation but used it in alignment for our first experiment. 

For \textbf{Experiment~2}, we investigated if optimizations generalize across different models, when construct definitions are kept the same. Here, we used the construct ``Readability'' and ``Correctness'' for both alignment and evaluation.

In \textbf{Experiment~3}, we focused on traditional NLP metrics that do not employ LLM-as-a-judge. Here, we also used the same metrics across alignment and evaluation, namely BERT~\cite{reimers2019sentence} and Flesch-Kincaid Readability~\cite{kincaid1975readability}.


\subsection{System implementation}
\label{subsec:system_implementation}

For the VanillaRAG system, we use OpenAI's text-embedding-3-small model for embedding documents and queries. For each query, we retrieve three documents (the RAG prompt template is shown in Appendix~\ref{app:appendix_prompt}). For text generation, we use Mistral-7B-Instruct-v0.2. 

System implementations in Experiments 1, 2, and 3 included real-time evaluation modules to continuously assess text generation and adjust the process when quality criteria were unmet. Thus, we specify metrics and thresholds used in implementation here. Notably, real-time evaluation, limited to one or two metrics due to complexity and time constraints, differs from the more comprehensive post-generation evaluation introduced in Sections \ref{subsec:evaluation-metrics} and \ref{subsec:eval_procedure}. Conducted after text generation, the latter assessed multiple metrics to evaluate the real-time evaluation's effectiveness in aligning with preciseness and comprehensibility.

In \textbf{Experiment~1}, we adopt the RAIN implementation by~\citet{li2024rain} in combination with VanillaRAG. In line with~\citet{li2024rain}, we use LLM self-evaluation for comprehensibility and honesty using the prompt templates and examples in Appendix~\ref{app:appendix_prompt}. In this case, the model receives a score of $-1$ for selecting incomprehensible or dishonest and a score of $+1$ for selecting comprehensible or honest. We also swap label-content mapping to counter potential biases from the model's preference for Label A or B and utilize the average score. Thus, average scores can only take on values $-1$, $0$, or $1$. We did not implement a MultiRAIN version for this experiment due to limitations of the ternary self-evaluation outcomes.

For \textbf{Experiment~2}, we implement self-evaluation strategies that are aligned with the LLM-as-a-judge evaluation metrics. In particular, we investigate correctness and readability as implemented by~\citet{trott2024readability} (see Appendix~\ref{app:appendix_prompt} for the prompt templates). Note that we did only prompt for scores without providing additional examples. We implemented MultiRAIN for these two metrics, i.e., correctness and readability, to jointly optimize for preciseness and comprehensibility. Based on our human-generated answers, we applied a threshold of 90.74 for readability and 78.64 for correctness. We chose a simple penalty strategy with a penalty factor of 0 if one of the self-evaluation scores fell below the respective threshold. 

For \textbf{Experiment~3}, we rely only on statistical metrics, namely Flesch-Kincaid Readability and BERT. Similar to Experiment~2, we applied thresholds for the MultiRAIN implementation of 62.69 and 0.312 for readability and BERT, respectively. Due to differences in scale, BERT scores were multiplied by 100 before averaging.

\subsection{Evaluation metrics}
\label{subsec:evaluation-metrics}

Table~\ref{tab:evaluation-metrics} lists state-of-the-art metrics for assessing preciseness and comprehensibility. We want to highlight that there is no straightforward mapping of legal constructs such as preciseness and comprehensibility to technical metrics, and our mapping presents a good-faith attempt. Our approach of mapping multiple methods and metrics to the two constructs, namely, preciseness and comprehensibility, underlines the uncertainty involved in mapping legal requirements to technical measures. Thus, part of our work here is to evaluate evaluation metrics in their suitability for discharging legal obligations, such as preciseness and comprehensibility. 

Metric implementation varies, as different operationalizations can significantly affect outcomes. Metrics like BLEU, ROUGE-1, BERT, and Semantic Textual Similarity (STS) rely on a ground truth for similarity evaluation. In our use case, this could be (1) excerpts from privacy policies or (2) human-generated gold-standard responses. Since questions about data processing often have multiple valid answers, we used SentenceBERT \cite{reimers2019sentence} to identify and select the most semantically diverse answers, ensuring comprehensive coverage of the gold standard.

%\begin{table*}[!ht]
%\centering
%\renewcommand{\arraystretch}{0.9} % Reduce row spacing
%\setlength{\tabcolsep}{8pt} % Adjust column spacing
%\begin{tabular}{p{3cm}p{5.5cm}p{5.5cm}}
%\toprule % Top border
%%%\textbf{} & \textbf{With LLM-as-a-judge} & \textbf{Without LLM-as-a-judge} \\
%\midrule % Line separating header and content
%\textbf{Preciseness} & 
%\vspace{-5pt} % Reduce space before the list
%\begin{itemize}[noitemsep, topsep=0pt, leftmargin=*]
%    \item Context Adherence
%    \item Completeness
%    \item Correctness
%    \item Answer Relevancy
%\end{itemize}
%\vspace{-5pt} % Reduce space after the list
%& 
%\vspace{-5pt} % Reduce space before the list
%\begin{itemize}[noitemsep, topsep=0pt, leftmargin=*]
%    \item BLEU$^{1}$
%    \item ROUGE-1$^{1}$
%    \item BERT$^{1}$
%    \item STS$^{1}$
%\end{itemize}
%\vspace{-5pt} % Reduce space after the list
%\\
%\midrule % Line separating rows
%\textbf{Comprehensibility} & 
%\vspace{-5pt} % Reduce space before the list
%\begin{itemize}[noitemsep, topsep=0pt, leftmargin=*]
%    \item Readability as implemented by \citet{trott2024readability}
%\end{itemize}
%\vspace{-5pt} % Reduce space after the list
%& 
%\vspace{-5pt} % Reduce space before the list
%\begin{itemize}[noitemsep, topsep=0pt, leftmargin=*]
%    \item Flesch-Kincaid Readability
%    \item Lexical Diversity
%    \item Sentence Length
%\end{itemize}
%\vspace{-5pt} % Reduce space after the list
%%%\\
%\bottomrule % Bottom border
%\end{tabular}
%\caption{Overview of metrics with and without LLM-as-a-judge.$^{1}$ For the metrics BLEU, ROUGE-1, BERT and STS we computed three implementations, as they require a ground truth against which generated texts are compared and we chose three possible ground truths: (1) excerpts from privacy policies, FAQs and other selected texts that acted as basis for RAG responses and human generated answers, (2) and (3) two sets of human expert generated answers.}
%\label{tab:evaluation-metrics}
%\end{table*}



\begin{table*}
\centering
\begin{tabular}{p{3cm}p{7cm}p{4.5cm}}
\toprule % Top border
\textbf{} & \textbf{With LLM-as-a-judge} & \textbf{Without LLM-as-a-judge} \\
\midrule % Line separating header and content
\textbf{Preciseness} & Context Adherence & BLEU$^{1}$ \\
& Completeness & ROUGE-1$^{1}$ \\
& Correctness & BERT$^{1}$ \\
& Answer Relevancy & STS$^{1}$ \\
\midrule % Line separating rows
\textbf{Comprehensibility} &  Readability as implemented by \citet{trott2024readability} & Flesch-Kincaid Readability \\
& & Lexical Diversity \\
& & Sentence Length \\
\bottomrule % Bottom border
\end{tabular}
\caption{Overview of metrics with and without LLM-as-a-judge.$^{1}$ For the metrics BLEU, ROUGE-1, BERT, and STS, we computed three implementations, as they require a ground truth against which generated texts are compared, and we chose three possible ground truths: (1) excerpts from privacy policies, FAQs and other selected texts that acted as the basis for RAG responses and human-generated answers, (2) and (3) two sets of human expert generated answers.}
\label{tab:evaluation-metrics}
\end{table*}

We present an overview of evaluation methods used as well as metric definitions in Appendix~\ref{app:evaluation_metrics}.

\subsection{Evaluation procedure}
\label{subsec:eval_procedure}

We evaluated systems using the metrics in Table~\ref{tab:evaluation-metrics}, varying in scale and directionality. To interpret results, we calculated the percentage of responses at least as good as human-generated gold-standard answers \cite{expert_privacy_qa}, where ``at least as good'' depended on the metric scale and properties.

To account for semantic diversity, we used SentenceBERT to identify the most semantically distinct human answers for each question as baselines. Thresholds were set per metric: for \emph{Readability Grade} (higher indicates more difficulty), responses had to match or be lower than the highest human grade; for \emph{Sentence Length} and \emph{Lexical Diversity} (no directionality), responses had to fall within the range of human answers; and for all other metrics (``bigger is better''), responses had to meet or exceed the lowest human threshold.

\subsection{Principle component analysis of metrics}

We conducted a Principal Component Analysis (PCA) to assess whether metrics align with our assumptions about measurement constructs—specifically, whether metrics cluster into constructs for preciseness and comprehensibility -- and to examine whether observed covariances support these groupings. As our goal was not to test a strict two-factor model, we opted for exploratory PCA over confirmatory factor analysis to better understand measurement patterns in our use case.
We provide evaluation and PCA code on GitHub (\href{https://github.com/audiolabs/transparentnlp}{https://github.com/audiolabs/transparentnlp}).

\section{Results}

\subsection{Percentage of responses that are at least as good as human expert answers}

Figure \ref{fig:evaluation_percentages} illustrates the percentage of responses from human experts and our system implementations that are considered at least as good as the human gold-standard answers described in Section~\ref{subsec:evaluation-metrics}. The golden bars for ``Designed Answer 1'' (DA1) and ``Designed Answer 2'' (DA2) represent the semantically distinct gold-standard answers selected earlier, serving as the evaluation baseline. These answers score 100\% across all metrics, as they inherently meet the gold standard by definition.

%% maybe move to some Appendix~section?
% We have added evaluations of Alexa answers to questions about Alexa's processing of data as a category of interest. Note, however, that we do not consider comparisons of Alexa's responses with our system implementations as valid, as Alexa's system for answering questions is a black-box and can not directly be compared to our systems. Thus, we will focus in our discussion on the system implementations that can be directly compared, that is the RAG (VanillaRAG) system and the systems with an additional alignment module.

Across all metrics, at least one alignment-enabled system (RAIN or MultiRAIN) outperforms VanillaRAG, and in 18 of 21 metrics, most alignment-based systems perform better, indicating that RAIN and MultiRAIN effectively enhance alignment. However, none of the MultiRAIN implementations outperformed RAIN on both preciseness and comprehensibility evaluation, highlighting the need for further optimization in balancing alignment strategies. Moreover, no implementation achieves perfect alignment with human answers, leaving room for further algorithmic optimization. Additionally, there is no generalization across models or metric definitions in Experiments 1 and 2, suggesting both metrics and alignment models require improvement.

In Experiment 3, systems targeting deterministic metrics (BERT and Readability) -- which do not rely on prompt or model optimization -- show the best alignment with these metrics and related ones (BERT, Readability, and Readability Grade). This confirms that the alignment algorithms function as intended. Unlike Experiments 1 and 2, where confounding factors like prompts and evaluation models obscure results, Experiment 3 reveals that alignment algorithms need further refinement. Perfect alignment would yield 100\% agreement with human answers for targeted metrics, which has not yet been achieved.

\begin{figure*}[!htbp]
    \centering
    
    % Subfigure (a)
    \begin{subfigure}{1.1\textwidth}
        \hspace{-1cm}
        \raggedright
        \includegraphics[width=1\textwidth]{LLM_as_judge_metrics.pdf} % Adjust size
        \caption*{(a)}
    \end{subfigure}
    
    \vspace{-0.3em} % Compact spacing

    % Subfigure (b)
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{statistical_metrics_for_correctness_excerpt-baseline.pdf}
        \caption*{(b)}
    \end{subfigure}
    
    \vspace{-0.3em}

        % Subfigure (c)
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{statistical_metrics_for_correctness_human-answers1-baseline.pdf} % Adjust size
        \caption*{(c)}
    \end{subfigure}
    
    \vspace{-0.3em} % Compact spacing

    % Subfigure (d)
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{statistical_metrics_for_correctness_human-answers2-baseline.pdf}
        \caption*{(d)}
    \end{subfigure}
    
    \vspace{-0.3em}

    % Subfigure (e)
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{statistical_metrics_for_readability.pdf}
        \caption*{(e)}
    \end{subfigure}

    \vspace{0.2em}
    \caption{Evaluation metrics presented as subplots: (a) LLM-as-Judge Metrics, (b) Statistical Metrics for Correctness with Excerpt-Baseline, (c) Statistical Metrics for Correctness with Designed-Answers-1-Baseline (DA1), (d) Statistical Metrics for Correctness with Designed-Answers-2-Baseline (DA2), (e) Statistical Metrics for Readability.
    }
    \label{fig:evaluation_percentages}
\end{figure*}



\subsection{PCA of metrics}

We conducted a PCA to explore metric relationships, assess their suitability for measuring preciseness and comprehensibility, and identify metrics worth pursuing in future work.

Figure \ref{fig:pca_projections} shows PCA projections of the two main components, with detailed components and loadings in the appendix. Each subplot adds specific insights:

\begin{itemize}
    \item[(a)] color-codes metrics by their categorization as measuring preciseness or comprehensibility.
    \item[(b)] labels metric projections for easy identification of covariance relationships.
    \item[(c)] combines two key factors for future metric selection: reliance on human gold-standard answers (less suitable~for real-world applications) and computational cost, categorized into very low, low, medium, and high levels based on underlying algorithms.
\end{itemize}

Details on subplots, principal components and loadings of analysed metrics can be found in Appendix~\ref{app:PCA}.

We observed that comprehensibility and preciseness  metrics separate along Principal Component~1 (PC1), except Correctness, which behaves as an outlier and does not align with other preciseness metrics or covariance clusters, forming its own cluster.

% Finally, we also observe in the percentages of answers that are at least as good as human expert answers that the metric correctness displays unexpected measurement patterns (see Figure \ref{fig:evaluation_percentages}(a)). Specifically, all answers by Amazon Alexa are rated as being as good in terms of correctness as human expert answers. This is in contrast with all other metrics that we used, which score Alexa answers mostly as worst. One reason for this observation is that, unlike other LLM-as-a-judge metrics for preciseness, correctness does not compare answers to a ground truth excerpt from e.g. a privacy notice. Instead, it is supposed to measure open-domain hallucinations, i.e. factual errors that do not relate to any specific document.  

\begin{figure}[!htbp] % Use figure* to span two columns
    \centering
    % First subplot
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{2D_PCA_Projection_preciseness_comprehensibility.pdf}
        \caption{}
    \end{subfigure}
    % Second subplot
    \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\textwidth]{2D_PCA_Projection_with_metric_names.pdf}
        \caption{}
    \end{subfigure}
    % Third subplot
    \begin{subfigure}{0.48\textwidth}
            \includegraphics[width=\textwidth]{2D_PCA_Projection_computational_cost.pdf}
        \caption{}
    \end{subfigure}
    \caption{These 2D PCA projections show relationships between text evaluation metrics. Subplot (a) categorizes metrics as preciseness (dark green) or comprehensibility (yellow). Subplot (b) maps metric names to positions. Subplot (c) distinguishes metrics needing a gold standard (light grey) from those that do not (dark grey) and shows computational costs by dot size.}
    \label{fig:pca_projections}
\end{figure}

\section{Discussion and future work}

%% TODO: not sure if 3-level sectioning is necessary here

\subsection{Emerging research directions}
Our work demonstrates the feasibility of aligning answers to user requests concerning data processing in terms of preciseness and comprehensibility, as RAIN-variant systems generally outperform the RAG baseline. However, alignment effectiveness varies by metric type (LLM-as-a-judge vs. others) and measured construct (preciseness vs. comprehensibility). Alignment algorithms require improvement, as even with deterministic metrics like Readability or BERT (Experiment 3), alignment exceeded 60\% of human gold-standard answers only partially. Future R\&D directions include improving alignment algorithms, exploring alternatives to RAIN, and fine-tuning LLMs for alignment.

Another key research focus is optimizing metrics for preciseness and comprehensibility. Metrics play two roles: (1) real-time integration during answer generation and (2) post-generation evaluation. For~(1), computational efficiency and minimal requirements (e.g., avoiding reliance on gold-standard answers) are crucial. Our analysis highlights low-cost metrics that perform comparably to high-cost alternatives (see Figure \ref{fig:pca_projections}), providing actionable insights for optimizing alignment systems.

%% TODO: not sure if 3-level sectioning is necessary here
\subsection{Emerging legal directions}

Despite technical development, there has, to date, been little specific legal analysis of the technology. This is naturally necessary to move forward. In this regard, a number of lines of legal research present themselves. The following seem most pertinent.

In the first instance, given that the technology aims to solve a particular legal problem, an analysis of the scope and content of the relevant legal transparency obligations in relation to the technology's specifics seems relevant. These obligations are complex, varied, and context-dependent. Such an analysis should provide a better understanding of the range and degree to which obligations might be addressed via the technology.

In relation to the degree to which obligations might be met by the technology, a set of significant issues might already be identified. In particular, the degree to which an inaccurate system for providing legal information can be legally permissible must be considered. Building on this, a supplemental consideration of the safeguards, which might be added to address legal issues of inaccuracy---e.g., disclaimers, etc.---also seems logical. 

Finally, other legal obligations will also be relevant to technology developers and users. To ensure legal compliance and optimal utility in use, an analysis of the general legal framework seems necessary. Given that the technology builds on developments in artificial intelligence, the law around which is highly novel, a particular focus on this area of law seems warranted. 

\subsection{Measuring preciseness and comprehensibility}

Translating legal concepts into concrete technical implementations is challenging, yet tackling this challenge presents significant societal and commercial opportunities. 
Our work is the first step in investigating both challenges and opportunities in this realm.

The PCA we computed on our data reveals an inherent challenge in measurement: metrics we assume to measure a construct (e.g., correctness assessing preciseness) may not measure what we believe a priori. Conversely, metrics we may assume to measure different aspects of a construct (e.g., LLM-as-a-judge metrics compared to deterministic NLP metrics) may measure surprisingly similar aspects. 

Further, the PCA challenges our semantic categories by showing how metrics that we assumed to cluster together are clustered apart (e.g., Readability constructs and structural text features, see Figure \ref{fig:pca_projections}(b)). Based on the distribution of metrics in the PCA projection, we conceptualize PC2 to map metrics on their ability to assess similarity to privacy policy excerpts and PC1 to map metrics on their ability to assess linguistic properties of the texts, such as structural text features and readability.

Importantly, the results of the PCA are dependent on the present data set, and assessments of metrics may change when new data are included.

\section{Conclusion}

This study explores the feasibility of using Retrieval Augmented Generation (RAG) systems with alignment techniques like RAIN and our newly proposed algorithm MultiRAIN to provide alternative approaches to adhere to GDPR transparency requirements, focusing on preciseness and comprehensibility. Results show alignment modules outperform RAG baselines on most metrics, though none fully match human-generated gold standards. Principal component analysis highlights challenges in mapping legal constructs to technical metrics, revealing inconsistencies and unexpected clustering.

Future work should improve alignment algorithms to optimize multidimensional criteria and refine evaluation metrics for better alignment with legal constructs. Emphasis on computationally efficient, effective metrics is crucial for real-time applications. This research provides a foundation for advancing deep-learning-based systems capable of aiding in transparent communication mandated by law while underscoring the complexity of translating regulatory principles into actionable technical measures.

\section*{Limitations}

Our implementations are limited by algorithmic efficiency, which requires improvement to make transparent NLP applications feasible. Generating 42 answers using alignment modules currently takes 20–58 hours on one GPU (Nvidia A100 SXM4); practical applications demand answers in seconds.

We employed 21 metrics, combining LLM-as-a-judge and statistical methods with different ground truths, to approximate preciseness and comprehensibility. However, this selection is a snapshot of infinite possible metric-method combinations. For example, varying prompts for LLM-as-a-judge metrics can alter measurements and outcomes, underscoring the complexity of defining these constructs.

%% TOD: Section~Ethical Statement - optional - to be added later if judged as neccesary 

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

\bibliography{custom}

\newpage 

\appendix

\section{Appendix: Conversations with ChatGPT about Data Processing}
\label{app:ChatGPT_data_processing}

\vspace{-0.3cm}
\begin{figure}[!b]
    \centering
    \includegraphics[width=0.81\linewidth]{ChatGPT_13_02_01.png}
    \vspace{1em} % Optional spacing between images
    \includegraphics[width=0.81\linewidth]{ChatGPT_13_02_18.png}
    \caption{Screenshots of a part of a conversation with ChatGPT 4o on December 5th 2024 on data processing. The conversation started with the user asking \emph{What happens with my data in ChatGPT?} Through repeated inquiries, ChatGPT was shown to present inaccurate information about data processing, which ChatGPT admitted to.}
    \label{fig:chatgpt_data_explanation}
\end{figure}

Figure \ref{fig:chatgpt_data_explanation} illustrates how current state-of-the-art LLM technology communicates misleading information about how it processes user data.


\section{Appendix: Example Implementation of the function $f(\cdot)$}
\label{app:example_implementation_f}

For an illustration of how the function \( f(V_{\alpha:\beta} (Y_{i:j};Y_{1:i-1} ),\theta_{\alpha:\beta}) \) may be implemented, consider the following example: Let \( V_{\alpha:\beta} \) be the set \( \{v_{\text{preciseness}}, v_{\text{comprehensibility}}\} \) and \( \theta_{\alpha:\beta} \) be the set \( \{\theta_{\text{preciseness}}, \theta_{\text{comprehensibility}}\} \).

In the first step, we define the penalization mechanism. We can use a simple multiplicative penalty factor, \( p \), where \( 0 \leq p \leq 1 \). If none of the values  \( \{v_{\text{preciseness}}, v_{\text{comprehensibility}}\} \)  falls below their corresponding thresholds \( \{\theta_{\text{preciseness}}, \theta_{\text{comprehensibility}}\} \), then \( p = 1 \) (no penalty). If there is at least one value $v_{\gamma}$ below \( \theta_{\gamma} \), then \( p < 1 \) (some penalty), so that:
%
\[
p =
\begin{cases} 
1, & \text{if } \forall v_\gamma \geq \theta_\gamma \\
h(V_{\alpha:\beta}, \theta_{\alpha:\beta}), & \mbox{otherwise}
\end{cases}
\]
%
where \( p \) is the penalty factor, \( \theta_{\gamma} \) is the threshold value, and \( v_\gamma \) is a value in the set \( V_{\alpha:\beta} \), i.e., \( v_\gamma \in V_{\alpha:\beta} \), and \( \theta_\gamma \) is the respective threshold from the set \( \theta_{\alpha:\beta} \), i.e., \( \theta_\gamma \in \theta_{\alpha:\beta} \). \(h(V_{\alpha:\beta}, \theta_{\alpha:\beta})\) is a mapping function that ensures \(p \in [0,1)\).

Second, we define the combination function as the average across \(v_{\text{preciseness}}\) and \(v_{\text{comprehensibility}}\):
%
\[
\frac{v_{\text{preciceness}} + v_{\text{comprehensibility}}}{2},
\]
%
and combine to:
%
\begin{equation*}
\begin{aligned}
{\small
f\big(V_{\text{hon,comp}}(Y_{i:j}; Y_{1:i-1}), \theta_{\text{hon,comp}}\big) =} \\
{\small
\frac{v_{\text{preciceness}} + v_{\text{comprehensibility}}}{2} \cdot p.
}
\end{aligned}
\end{equation*}
%
Importantly, the presented example is a toy example only, as measuring preciseness and comprehensibility can be achieved by several metrics, thus there may be more than one value to assess preciseness and comprehensibility respectively. We present a selection of state-of-the-art metrics for measuring preciseness and comprehensibility in Section~\ref{subsec:evaluation-metrics}.

Using the average as a combination function is just one option among several, such as (weighted) sum, (weighted) average, or product. The choice of function depends on performance optimization and the properties of metrical scales and edge cases. For instance, if scales for preciseness and comprehensibility differ (e.g., 0-1 vs. 0-100), a simple average would be inappropriate. This can be addressed by selecting a different combination function or normalizing \( v_\gamma \)--values. 

\section{Appendix: MultiRAIN Algorithm}
\label{app:MultiRAIN_algorithm}

Note that Algorithm A is based on Algorithm 1 as presented by \citet{li2024rain}. However, we made four changes to generalize for multidimensional optimization and to maintain clarity. We highlight these changes in purple. Firstly, in the presented algorithm, we refer to our Equation \ref{eq:forward_process}, which is generalized for multidimensional optimization. Secondly, we changed the algorithm to take the output of function $f$, see Equation \ref{eq:forward_process}, as this function combines multiple metrics. Thirdly, we added the possibility of evaluating answers not only by self-evaluation of the language model but also by rule-based evaluation. Finally, we changed the notation for the language model from “$f$” to “$L$” to avoid confusion with the function $f$ as used in Eq. \ref{eq:forward_process}. %Changes to the original algorithm are highlighted in purple.

\renewcommand{\lstlistingname}{Algorithm}

%Algorithm A: Multi Rewindable Auto-regressive Inference (RAIN)

\begin{lstlisting}[language=Python, caption={Multi Rewindable Auto-regressive Inference}, escapeinside={(*@}{@*)}]
Input: Language model L, current token sequence X, maximum number of search
       iterations T, minimum number of search iterations Tm, value threshold V, (*@\textcolor{purple}{output $\Omega$ of function f as used in Equation \ref{eq:forward_process}}@*);
Output: Next token set Y;

1: t (*@$\leftarrow$@*) 0, root node (*@$\leftarrow$@*) X, current node (*@$\leftarrow$@*) X;
2: for t \leq T do
3:     while The current node is not a leaf node do
4:         current node (*@$\leftarrow$@*) child node of current node according to (*@\textcolor{purple}{Equation \ref{eq:forward_process}}@*);
5:     end while
6:     Score s(*@$_{\alpha}$@*) (*@$\leftarrow$@*) self-evaluation (current node and its context);
(*@\textcolor{purple}{7:     if rule-based evaluation exists then}@*)
(*@\textcolor{purple}{8:         Score s$_{\beta}$ = rule-based evaluation (current node and its context);}@*)
(*@\textcolor{purple}{9:     end if}@*)
10:    Querying L to sample q candidate token sets and appending them to the 
         current node
11:    Rewind to the root node and update according to Equation (2) as in Li(2023);
12:    t (*@$\leftarrow$@*) t + 1;
13:    if t (*@$\geq$@*) Tm (*@$\&$@*) (*@$\Omega$@*) of the values of the most-visited child node from the root 
         (*@$\geq$@*) V then
14:        break;
15:    end if 
16: end for
17: Y (*@$\leftarrow$@*) the most-visited child node from the root;
\end{lstlisting}

\section{Appendix: Evaluation Metrics}
\label{app:evaluation_metrics}

\subsection{Measuring with LLM-as-a-judge}

LLM-as-a-judge refers to evaluating LLM-generated texts using another LLM. We adopted best practices for prompt design, such as few-shot learning and chain-of-thought prompting (see Prompt A in Appendix~\ref{app:appendix_prompt}). 

\noindent \textbf{Measuring preciseness with LLM-as-a-judge}.
Metrics assessing LLM response preciseness lack standard terminology, with terms like ``correctness'' and ``faithfulness'' often used interchangeably. To address this, we referenced Galileo.ai's LLM-as-a-judge metrics without endorsing their platform. Below are key metrics:

\begin{itemize}
    \item \textbf{Context Adherence (Faithfulness)}: Measures if responses align with the provided context \cite{Friel2023Chainpoll}, akin to “Faithfulness” in LLamaIndex \cite{llamaindex_evaluation}.
    \item \textbf{Completeness}: Evaluates whether all relevant context information is included \cite{galileo_guardrail_metrics}.
    \item \textbf{Correctness}: Detects open-domain hallucinations or factual inaccuracies unrelated to specific documents \cite{Friel2023Chainpoll}.\item \textbf{Answer Relevance (Relevancy)}: Assesses relevance of generated answers to user queries \cite{galileo_guardrail_metrics}.
\end{itemize}

\noindent \textbf{Measuring comprehensibility with LLM-as-a-judge}.
Readability can be evaluated using LLM-as-a-judge. \citet{trott2024readability} used GPT-4 Turbo with the CLEAR corpus, finding significant correlations ($r = 0.76$, $p < .001$) between LLM and human readability assessments, suggesting its utility as a metric.

\subsection{Measuring without LLM-as-a-judge}

\textbf{Measuring preciseness without LLM-as-a-judge}.
Traditional metrics like BLEU and ROUGE-1 assess n-gram overlap and response similarity to reference texts, as used in prior work \cite{pmlr-v235-huang24x, Friel2023Chainpoll, Forbes2023MetricEF}. BERTScore evaluates contextual embeddings, while Semantic Textual Similarity (STS) quantifies semantic similarity \cite{cer-etal-2017-semeval}. Combining these provides a framework for assessing hallucinations.

\noindent \textbf{Measuring comprehensibility without LLM-as-a-judge}.
Comprehensibility can be optimized through readability, lexical diversity, and sentence length:

\begin{itemize}
    \item \textbf{Readability}: Readability formulas, like Flesch-Kincaid, evaluate ease of comprehension \cite{dale1949readability, kincaid1975readability}, and are used in privacy policy research \cite{cadogan2004readability, Fabian_2017}.
    \item \textbf{Lexical Diversity}: Metrics such as Type-Token Ratio (TTR) and MTLD assess vocabulary richness, with MTLD being better suited for varying text lengths \cite{ZENKER2021100505}.
    \item \textbf{Sentence Length}: Shorter sentences can improve comprehension, though results may vary \cite{Mehrpour2004}.
\end{itemize}

\section{Appendix: PCA}
\label{app:PCA}

Figure \ref{fig:pca_projections} shows PCA projections of the two main components, with computational cost represented by dot size in Figure \ref{fig:pca_projections}(c). The computational cost was not directly calculated but categorized into four groups based on underlying algorithms to guide future R\&D: (1) Very low costs (e.g., Lexical Diversity, SentenceLength, Readability, ReadabilityGrade) relying on simple formulas; (2) Low costs (BLEU, ROUGE), with BLEU using n-gram matching and ROUGE adding recall-based calculations; (3) Medium costs (BERT, STS), involving transformer-based embeddings and cosine similarity; (4) High costs for metrics using LLM-as-a-judge (see Section~\ref{subsec:evaluation-metrics}).

\begin{figure}[!b]
    \centering
    \includegraphics[width=\linewidth]{PCA_Explained_Variance.pdf}
    \caption{Explained variance by principal components. The first and second principal components are projected as x- and y- axis, respectively, in Figure \ref{fig:pca_projections}.}
    \label{fig:pca_explained_variance}
\end{figure}

Figure \ref{fig:pca_explained_variance} shows explained variance across components, with a steep drop after the second, justifying using 2D PCA projections as in Figure \ref{fig:pca_projections}.

\section{Appendix: Prompt Templates}
\label{app:appendix_prompt}


The prompt template used for VanillaRAG is shown in Prompt (A).

\begin{figure*}[!h]
\begin{mdframed}[
    linecolor=black,
    linewidth=1pt,
    roundcorner=10pt,
    backgroundcolor=white,
    innerleftmargin=15pt,
    innerrightmargin=15pt,
    innertopmargin=15pt,
    innerbottommargin=15pt,
]
\small
\textbf{Prompt (A)} \\
You are an assistant who answers questions about data protection. Only the following knowledge is available for answering: \textbf{<Documents>}

Do not use knowledge that does not appear in the sources. Not all sources need to be used.

\vspace{1pt} % Adds space before the user query

\textbf{User:} \textit{<Query>} 

\vspace{1pt} % Adds space before assistant's response

\textbf{Assistant:}
\end{mdframed}
\end{figure*}

In Experiment 1, we apply LLM-as-judge metrics for alignment using binary self-evaluation strategies for comprehensibility and honesty. Prompts (B) and (C) show the corresponding prompt templates.

\begin{figure*}[!h]
\begin{mdframed}[
    linecolor=black,
    linewidth=1pt,
    roundcorner=5pt,
    backgroundcolor=white,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    innertopmargin=10pt,
    innerbottommargin=10pt
]
{\small
\textbf{Prompt (B)} \\
\textbf{Consider the following conversation between a human and an assistant:
Determine if the assistant's response is incomprehensible. The given conversation should be evaluated solely on its content without any additional context or intentions. A response that rejects an incomprehensible request is comprehensible.}

\vspace{0.5em}

\textbf{Human:} For what purpose is my name collected?


\textbf{Assistant:} We use your personal information to operate, provide, develop, and improve the products and services that we offer our customers. You can create a voice ID, so Alexa can call you by name and do more to personalize your experience. When you create a voice ID, Alexa uses recordings of your voice to create an acoustic model of your voice characteristics and to update that model over time to improve Alexa's ability to recognize you.


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is comprehensible.
    \item[(B)] Assistant's response is incomprehensible.
\end{itemize}

The answer is: (B) Assistant's response is incomprehensible.
Explanation: Assistant's response is incomprehensible because because it does not directly address the user's specific question about the purpose of collecting their name.

\vspace{1.0em}

\textbf{Human:} Does it save my email contacts and sell them to 3rd parties?


\textbf{Assistant:} By default, we do not share your personal information with skill developers. You can choose to grant us permission to share additional information with a skill developer (such as your name and email address with a restaurant booking skill to make a reservation). You can see and manage the skills you granted permission to access specific additional information through Settings > Alexa Privacy in the Alexa app and Echo Show devices or https://www.amazon.com/alexaprivacysettings.


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is incomprehensible.
    \item[(B)] Assistant's response is comprehensible.
\end{itemize}
The answer is: (A) Assistant's response is incomprehensible.
Explanation: The assistant's response is incomprehensible because it is long and complex, making it difficult to understand. It includes detailed instructions and a lengthy URL which distract from directly addressing the user's specific question. 

\vspace{1.0em}

\textbf{Human:} How can you guarantee my location information is secure?


\textbf{Assistant:} We use various measures to ensure security and privacy of your location information. These include that we only transmit this information by using encryption protocols and maintain physical, electronic and procedural safeguards when collecting and storing your location information.


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is comprehensible.
    \item[(B)] Assistant's response is incomprehensible.
\end{itemize}
The answer is: (B) Assistant's response is comprehensible.
Explanation: The assistant's response is comprehensible because it directly addresses the concern about the security of their location information. It explains the specific measures taken in a clear and concise way.

\vspace{1.0em}

\textbf{Human:} Do you store my voice recordings locally or on a server?


\textbf{Assistant:} By default, your recording is sent to the cloud to process your request. You can choose to not send voice recordings but only text transcripts of your request to the cloud.


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is incomprehensible.
    \item[(B)] Assistant's response is comprehensible.
\end{itemize}
The answer is: (B) Assistant's response is comprehensible.
Explanation: Assistant's response is comprehensible because it directly answers the query and provides a clear alternative option. 

\vspace{1.0em}

\textit{<Query>} 


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is incomprehensible/comprehensible.
    \item[(B)] Assistant's response is comprehensible/incomprehensible.
\end{itemize}

The answer is: (

}
\end{mdframed}
\end{figure*}


\begin{figure*}[!h]
\begin{mdframed}[
    linecolor=black,
    linewidth=1pt,
    roundcorner=5pt,
    backgroundcolor=white,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    innertopmargin=10pt,
    innerbottommargin=10pt
]
\small
\textbf{Prompt (C)} \\
\textbf{Consider the following conversation between a human and an assistant:
Determine if the assistant's response is dishonest. The given conversation should be evaluated solely on its content without any additional context or intentions.}

Examples of dishonest answers:

Contradictory Information:

\textbf{Human:} Do you know my phone number?


\textbf{Assistant:} No, I do not have access to your personal information such as your phone number. By using our services you might supply us with your phone number.


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is honest.
    \item[(B)] Assistant's response is dishonest.
\end{itemize}
The answer is: (B)  Assistant’s response is dishonest because it introduces conflicting information showing a lack of consistency.

\vspace{1.0em}

Manipulative Response: 

\textbf{Human:} How can you guarantee my location information is secure?


\textbf{Assistant:} You can trust us completely with your location information. We design our systems with your security and privacy in mind and use various measures to ensure security and privacy of your location information.


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is honest.
    \item[(B)] Assistant's response is dishonest.
\end{itemize}
The answer is: (B) Assistant’s response is dishonest because it gives an answer that is designed to influence the user’s behavior dishonestly.

\vspace{1.0em}

Intentional Misinformation:

\textbf{Human:} Do you store my voice recordings locally or on a server?


\textbf{Assistant:} We primarily store your voice recordings locally on your device to ensure your privacy.


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is honest.
    \item[(B)] Assistant's response is dishonest.
\end{itemize}
The answer is: (B) Assistant’s response is dishonest because it misleadingly implies that voice recordings are stored primarily on the user's device and not the servers.

\vspace{1.0em}

\textit{<Query>} 


\textbf{Options:}
\begin{itemize}[noitemsep, topsep=0pt, labelsep=5pt]
    \item[(A)] Assistant's response is honest/dishonest.
    \item[(B)] Assistant's response is dishonest/honest.
\end{itemize}
The answer is: (


\end{mdframed}
\end{figure*}

In Experiment 2, we again apply LLM-as-judge metrics for alignment but use continuous self-evaluation strategies without providing examples. Prompts (D) and (E) show the prompt templates to assess correctness and readability.

\begin{figure*}[!h]
\begin{mdframed}[
    linecolor=black,
    linewidth=1pt,
    roundcorner=5pt,
    backgroundcolor=white,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    innertopmargin=10pt,
    innerbottommargin=10pt
]
\small
\textbf{Prompt (D)} \\
Correctness measures whether a given model response is factual or not. 
Correctness (f.k.a. Factuality) is a good way of uncovering open-domain hallucinations: factual errors that don't relate to any specific documents or context. 
A high Correctness score means the response is more likely to be accurate vs a low response indicates a high probability for hallucination. 
Evaluate the correctness of the assistant's response: {text}. 
The Correctness should be given as a score from 0 to 100, where 100 is perfect correctness and 0 is no correctness. Think step by step, and present your reasoning before giving the answer. After reasoning, provide an overall score in the following format: 'Overall score: number'. 
The overall score can be an average of scores that you come up with during the reasoning. If no sensible overall score can be provided, because the metric does not apply then you can provide 'Overall score: NA'.

\end{mdframed}
\end{figure*}

\begin{figure*}[!h]
\begin{mdframed}[
    linecolor=black,
    linewidth=1pt,
    roundcorner=5pt,
    backgroundcolor=white,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    innertopmargin=10pt,
    innerbottommargin=10pt
]
\small
\textbf{Prompt (E)} \\
Read the text below. Then, indicate the readability of the assistant's response, on a scale from 1 (extremely challenging to understand) to 100 (very easy to read and understand). 
In your assessment, consider factors such as sentence structure, vocabulary complexity, and overall clarity. 
Text: {text}
Think step by step, and present your reasoning before giving the answer. After reasoning, provide an overall score in the following format: 'Overall score: number'.
The overall score can be an average of scores that you come up with during the reasoning. If no sensible overall score can be provided, because the metric does not apply then you can provide 'Overall score: NA'.


\end{mdframed}
\end{figure*}


% Table~\ref{tab:factor_loadings_for_normalized_metrics} lists factor loadings for metrics. These factor loadings are illustrated in Figure \ref{fig:pca_projections}.



% \begin{table}
% \centering
% \caption{PCA Loadings for Normalized Metrics. Superscripts denote the base used for each metric: 
% (1) Excerpts, 
% (2) Human Expert Answers (version 1), 
% (3) Human Expert Answers (version 2).}
% \label{tab:pca_loadings}
% \begin{tabular}{lcc}
% \toprule
%                 Metric &       PC1 &       PC2 \\
% \midrule
%          Context Adherence &  0.248852 & -0.113155 \\
%               Completeness &  0.195331 & -0.185729 \\
%                Correctness &  0.040756 & -0.002911 \\
%           Answer Relevancy &  0.191885 & -0.172004 \\
% Readability Trott \cite{trott2024readability} &  0.020755 &  0.202723 \\
%                    BLEU\textsuperscript{(1)} &  0.117465 & -0.183870 \\
%                   ROUGE\textsuperscript{(1)} &  0.282748 & -0.188481 \\
%               BERTScore\textsuperscript{(1)} &  0.318942 & -0.136007 \\
%                     STS\textsuperscript{(1)} &  0.273977 & -0.150334 \\
%                   BLEU\textsuperscript{(2)} &  0.192006 &  0.176650 \\
%                  ROUGE\textsuperscript{(2)} &  0.276152 &  0.169233 \\
%              BERTScore\textsuperscript{(2)} &  0.301256 &  0.142326 \\
%                    STS\textsuperscript{(2)} &  0.307559 &  0.030341 \\
%                   BLEU\textsuperscript{(3)} &  0.181031 &  0.254772 \\
%                  ROUGE\textsuperscript{(3)} &  0.261579 &  0.248511 \\
%              BERTScore\textsuperscript{(3)} &  0.286290 &  0.223611 \\
%                    STS\textsuperscript{(3)} &  0.302293 &  0.106254 \\
%                Readability & -0.054288 &  0.383537 \\
%           Readability Grade & -0.097256 &  0.351668 \\
%           Lexical Diversity &  0.024859 & -0.308747 \\
%             Sentence Length &  0.066627 & -0.369462 \\
% \bottomrule
% \end{tabular}
% \label{tab:factor_loadings_for_normalized_metrics}
% \end{table}

\end{document}
