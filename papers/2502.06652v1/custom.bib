% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@misc{popp2023patent,
  author       = {Popp, Birgit and Leschanowsky, Anna and Kolagar, Zahra and Habets, Emanuel},
  title        = {EU Patent No. EP23218825},
  year         = {2023}
}

@inproceedings{kolagar2023experts,
  author       = {Kolagar, Zahra and Leschanowsky, Anna and Popp, Birgit},
  title        = {Experts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q\&A},
  booktitle    = {CONVERSATIONS 2023 - The 7th International Workshop on Chatbot Research and Design},
  year         = {2023},
  address      = {Oslo}
}

@inproceedings{li2024rain,
  author       = {Li, Y. and FangyunWei and Zhao, J. and Zhang, C. and Zhang, H.},
  title        = {{RAIN}: Your Language Models Can Align Themselves without Finetuning},
  booktitle    = {International Conference on Learning Representations},
  year         = {2024}
}

@online{trott2024readability,
  author       = {Trott, Sean},
  title        = {Measuring the "readability" of texts with Large Language Models},
  year         = {2024},
  month        = {February 7},
  url          = {https://seantrott.substack.com/p/measuring-the-readability-of-texts}
}

@misc{sun2024arxiv,
  author       = {Lichao Sun and Y. H.},
  title        = {From arxiv.org},
  year         = {2024},
  url          = {https://arxiv.org/pdf/2401.05561.pdf}
}



@misc{llamaindex_evaluation,
  author       = {{LlamaIndex}},
  title        = {Evaluating with {LlamaIndex}},
  year         = {2024},
  url          = {https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html},
  note         = {Accessed: 2024-11-25}
}

@article{Gat2023FaithfulEO,
  title={Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals},
  author={Yair Ori Gat and Nitay Calderon and Amir Feder and Alexander Chapanin and Amit Sharma and Roi Reichart},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.00603},
  url={https://api.semanticscholar.org/CorpusID:263334113}
}


@misc{galileo_guardrail_metrics,
  author       = {{Galileo AI}},
  title        = {Guardrail Metrics},
  year         = {2024},
  url          = {https://docs.rungalileo.io/galileo/galileo-gen-ai-studio/guardrail-metrics/},
  note         = {Accessed: 2024-11-25}
}


@misc{forbes2023metric,
  author       = {Forbes, G. C. and Katlana, P. and Ortiz, Z.},
  title        = {Metric Ensembles For Hallucination Detection},
  year         = {2023},
  month        = {October 16},
  url          = {https://arxiv.org/pdf/2310.10495.pdf}
}

@misc{aynetdinov2024semScore,
  author       = {Aynetdinov, A. and Akbik, A.},
  title        = {SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity},
  year         = {2024},
  month        = {February 5},
  url          = {https://arxiv.org/pdf/2401.17072.pdf}
}

@misc{galileo2024context,
  author       = {Galileo AI},
  title        = {Guardrail Metrics: Context Adherence},
  year         = {2024},
  url          = {https://docs.rungalileo.io/galileo/galileo-gen-ai-studio/guardrail-metrics/groundedness},
  note         = {Accessed: 2024-11-25}
}

@misc{galileo2024completeness,
  author       = {Galileo AI},
  title        = {Guardrail Metrics: Completeness},
  year         = {2024},
  url          = {https://docs.rungalileo.io/galileo/galileo-gen-ai-studio/guardrail-metrics/completeness},
  note         = {Accessed: 2024-11-25}
}

@misc{galileo2024correctness,
  author       = {Galileo AI},
  title        = {Guardrail Metrics: Correctness},
  year         = {2024},
  url          = {https://docs.rungalileo.io/galileo/galileo-gen-ai-studio/guardrail-metrics/factuality},
  note         = {Accessed: 2024-11-25}
}

@inproceedings{Manakul2023_selfcheckgpt,
author = {Manakul, Potsawee and Liusie, Adian and Gales, M.J.F.},
year = {2023},
month = {01},
pages = {9004-9017},
title = {SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
doi = {10.18653/v1/2023.emnlp-main.557}
}

@article{dale1949readability,
  author    = {Dale, Edgar and Chall, Jeanne S.},
  title     = {The Concept of Readability},
  journal   = {Elementary English},
  volume    = {26},
  number    = {1},
  pages     = {19--26},
  year      = {1949},
  url       = {http://www.jstor.org/stable/41383594}
}

@article{cadogan2004readability,
  author       = {Cadogan, Rochelle A.},
  title        = {An imbalance of power: the readability of internet privacy policies},
  journal      = {Journal of Business \& Economics Research (JBER)},
  volume       = {2},
  year         = {2004}
}


@article{Mehrpour2004,
title = "The impact of text length on reading comprehension in {English} as a second language",
author = "Saeed Mehrpour and Abdolmehdi Riazi",
year = "2004",
language = "English",
volume = "3",
pages = "1--14",
journal = "Asian EFL Journal",
issn = "1738-1460",
publisher = "Asian EFL Journal Press",
number = "6",

}

@unknown{Owain_2021_Truthful_AI,
author = {Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
year = {2021},
month = {10},
pages = {},
title = {{Truthful AI}: Developing and governing {AI} that does not lie},
doi = {10.48550/arXiv.2110.06674}
}

@article{Askell2021AGL,
  title={A General Language Assistant as a Laboratory for Alignment},
  author={Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and Tom Henighan and Andy Jones and Nicholas Joseph and Benjamin Mann and Nova Dassarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and John Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and Christopher Olah and Jared Kaplan},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.00861},
  url={https://api.semanticscholar.org/CorpusID:244799619}
}


@article{ZENKER2021100505,
title = {Investigating minimum text lengths for lexical diversity indices},
journal = {Assessing Writing},
volume = {47},
pages = {100505},
year = {2021},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2020.100505},
url = {https://www.sciencedirect.com/science/article/pii/S1075293520300660},
author = {Fred Zenker and Kristopher Kyle},
keywords = {Lexical diversity, Text length, SLA, Learner corpus research},
abstract = {Lexical diversity (LD) is an important feature of a second language (L2) writer’s lexical knowledge, and indices of LD have been widely used in the field of writing assessment (e.g., Cumming et al., 2006; Engber, 1995). Research with longer native speaker (L1) texts has indicated, however, that many commonly used LD indices are sensitive to text length and may conflate lexical breadth and fluency. Because of the importance of measuring LD in L2 writing assessment research, it is essential to know the degree to which particular LD indices are resistant to text length effects and the minimum text lengths at which these indices produce stable values. In this study, we investigate text length effects for nine indices of LD in a corpus of 4542 L2 argumentative essays. The results indicate that MATTR (Covington & McFall, 2010) and two versions of MTLD (McCarthy, 2005; McCarthy & Jarvis, 2010) are the most stable of the indices included in the study. MATTR performs particularly well, maintaining a high degree of stability across all text lengths. Comparisons based on essay prompt and proficiency level are also discussed.}
}

@article{johansson2008lexical,
  author       = {Johansson, Victoria},
  title        = {Lexical diversity and lexical density in speech and writing: A developmental perspective},
  journal      = {Working Papers},
  volume       = {53},
  pages        = {61--79},
  year         = {2008}
}


@inproceedings{Fabian_2017,
author = {Fabian, Benjamin and Ermakova, Tatiana and Lentz, Tino},
year = {2017},
month = {08},
pages = {},
title = {Large-Scale Readability Analysis of Privacy Policies},
doi = {10.1145/3106426.3106427}
}

@article{Friel2023Chainpoll,
  title={Chainpoll: A high efficacy method for LLM hallucination detection},
  author={Robert Friel and Atindriyo Sanyal},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.18344},
  url={https://api.semanticscholar.org/CorpusID:264590664}
}

@inbook{Krumay_2020,
author = {Krumay, Barbara and Klar, Jennifer},
year = {2020},
month = {06},
pages = {388-399},
title = {Readability of Privacy Policies},
isbn = {978-3-030-49668-5},
doi = {10.1007/978-3-030-49669-2_22}
}

@techreport{kincaid1975readability,
  author       = {Kincaid, J. Peter and Fishburne, Robert P. Fishburne Jr. and Rogers, Richard L. Rogers and Chissom, Brad S.},
  title        = {Derivation of New Readability Formulas: (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel},
  institution  = {Chief of Naval Technical Training, Naval Air Station Memphis},
  address      = {Millington, Springfield},
  year         = {1975},
  note         = {Distributed by NTIS}
}

@inproceedings{reimers2019sentence,
  title={{Sentence-BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3982--3992},
  year={2019},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/D19-1410.pdf},
  doi={10.18653/v1/D19-1410}
}

@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2001",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
    abstract = "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",
}


@misc{trulens2024rag,
  author       = {TruLens},
  title        = {The RAG Triad},
  year         = {2024},
  url          = {https://trulens.org}
}

@misc{bai2022constitutional,
  author       = {Bai, Y. and K.-J.},
  title        = {Constitutional {AI}: Harmlessness from {AI} Feedback},
  year         = {2022}
}

@article{wang2022selfconsistency,
  author       = {Wang, X. and Y.},
  title        = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  year         = {2022}
}

@misc{gdpr2016,
  title        = {Regulation {(EU)} 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data (General Data Protection Regulation)},
  author={{European Union}},
  howpublished = {Official Journal of the European Union, L 119/1},
  year         = {2016},
  month        = {May 4},
  url          = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679},
  note         = {Accessed: 2024-11-21}
}

@misc{euai2021,
  title        = {Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts},
  howpublished = {European Commission COM/2021/206 final},
  year         = {2021},
  month        = {April 21},
  url          = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206},
  note         = {Accessed: 2024-11-21}
}

@inproceedings{Harkous2016PriBotsCP,
  title={PriBots: Conversational Privacy with Chatbots},
  author={Hamza Harkous and Kassem Fawaz and Kang G. Shin and Karl Aberer},
  booktitle={WSF@SOUPS},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:12173813}
}

@misc{grasselt2022patent,
  author       = {Grasselt, M.W. and Saillet, Y. and Schaefer, M.},
  title        = {US Patent No. 11,537,552 B2: Rule generation in a data governance framework},
  year         = {2022},
  month        = {December 27},
  howpublished = {United States Patent and Trademark Office},
  note         = {Extended several times},
  url          = {https://patents.google.com/patent/US11537552B2/en},
}

@article{reuters2024italy,
  title        = {Italy fines OpenAI over ChatGPT privacy rules breach},
  author       = {Reuters},
  journal      = {Reuters},
  year         = {2024},
  month        = {December},
  day          = {20},
  url          = {https://www.reuters.com/technology/italy-fines-openai-15-million-euros-over-privacy-rules-breach-2024-12-20/},
  note         = {Accessed: 2025-01-22}
}

@unpublished{expert_privacy_qa,
  title        = {Expert-Generated Privacy Q\&A Dataset for Conversational {AI} and User Study Insights},
  author       = {Anna Leschanowsky and Farnaz Salamatjoo and Zahra Kolagar and Birgit Popp},
  note         = {Unpublished manuscript, submitted for publication at CHI Late Breaking Papers 2025, Privacy Track},
  year         = {2025}
}

@misc{transparentnlp_github,
  author       = {Birgit Popp, Anna Leschanowsky, Zahra Kolagar},
  title        = {TransparentNLP},
  year         = {2025},
  url          = {https://github.com/audiolabs/transparentnlp},
  note         = {Accessed: 2025-01-22}
}

@article{
martinez2023,
author = {Eric Martínez  and Francis Mollica  and Edward Gibson },
title = {Even lawyers do not like legalese},
journal = {Proceedings of the National Academy of Sciences},
volume = {120},
number = {23},
pages = {e2302672120},
year = {2023},
doi = {10.1073/pnas.2302672120},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2302672120},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2302672120},
abstract = {Why do lawyers write in such a convoluted manner? Across two preregistered experiments, we find that lawyers a) like laypeople, were less able to understand and recall “legalese” contracts than content of equivalent meaning drafted in a simplified register; and b) rated simplified contracts as equally enforceable as legalese contracts, and rated simplified contracts as preferable to legalese contracts on several important dimensions. Contrary to previous speculation, these results suggest that lawyers who write in a convoluted manner do so as a matter of convenience and tradition as opposed to an outright preference and that simplifying legal documents would be beneficial for lawyers and nonlawyers alike. Across modern civilization, societal norms and rules are established and communicated largely in the form of written laws. Despite their prevalence and importance, legal documents have long been widely acknowledged to be difficult to understand for those who are required to comply with them (i.e., everyone). Why? Across two preregistered experiments, we evaluated five hypotheses for why lawyers write in a complex manner. Experiment 1 revealed that lawyers, like laypeople, were less able to recall and comprehend legal content drafted in a complex “legalese” register than content of equivalent meaning drafted in a simplified register. Experiment 2 revealed that lawyers rated simplified contracts as equally enforceable as legalese contracts, and rated simplified contracts as preferable to legalese contracts on several dimensions–including overall quality, appropriateness of style, and likelihood of being signed by a client. These results suggest that lawyers who write in a convoluted manner do so as a matter of convenience and tradition as opposed to an outright preference and that simplifying legal documents would be both tractable and beneficial for lawyers and nonlawyers alike.}}

@misc{Article29WP2018,
  author       = "{Article 29 Data Protection Working Party}",
  title        = "{Guidelines on Transparency under Regulation 2016/679}",
  year         = 2018,
  url          = "https://ec.europa.eu/newsroom/article29/redirection/document/51025",
  note         = "Accessed: 2024-11-22",
  institution  = "European Commission",
  keywords     = "GDPR, transparency, data protection"
}

@inproceedings{mysore-sathyendra-etal-2017-identifying,
    title = "Identifying the Provision of Choices in Privacy Policy Text",
    author = "Mysore Sathyendra, Kanthashree  and
      Wilson, Shomir  and
      Schaub, Florian  and
      Zimmeck, Sebastian  and
      Sadeh, Norman",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1294",
    doi = "10.18653/v1/D17-1294",
    pages = "2774--2779",
    abstract = "Websites{'} and mobile apps{'} privacy policies, written in natural language, tend to be long and difficult to understand. Information privacy revolves around the fundamental principle of Notice and choice, namely the idea that users should be able to make informed decisions about what information about them can be collected and how it can be used. Internet users want control over their privacy, but their choices are often hidden in long and convoluted privacy policy texts. Moreover, little (if any) prior work has been done to detect the provision of choices in text. We address this challenge of enabling user choice by automatically identifying and extracting pertinent choice language in privacy policies. In particular, we present a two-stage architecture of classification models to identify opt-out choices in privacy policy text, labelling common varieties of choices with a mean F1 score of 0.735. Our techniques enable the creation of systems to help Internet users to learn about their choices, thereby effectuating notice and choice and improving Internet privacy.",
}


@inproceedings{ravichander-etal-2019-question,
    title = "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
    author = "Ravichander, Abhilasha  and
      Black, Alan W  and
      Wilson, Shomir  and
      Norton, Thomas  and
      Sadeh, Norman",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1500",
    doi = "10.18653/v1/D19-1500",
    pages = "4947--4958",
    abstract = "Privacy policies are long and complex documents that are difficult for users to read and understand. Yet, they have legal effects on how user data can be collected, managed and used. Ideally, we would like to empower users to inform themselves about the issues that matter to them, and enable them to selectively explore these issues. We present PrivacyQA, a corpus consisting of 1750 questions about the privacy policies of mobile applications, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms human performance by almost 0.3 F1 on PrivacyQA, suggesting considerable room for improvement for future systems. Further, we use this dataset to categorically identify challenges to question answerability, with domain-general implications for any question answering system. The PrivacyQA corpus offers a challenging corpus for question answering, with genuine real world utility.",
}


@inproceedings{ahmad-etal-2020-policyqa,
    title = "{P}olicy{QA}: A Reading Comprehension Dataset for Privacy Policies",
    author = "Ahmad, Wasi  and
      Chi, Jianfeng  and
      Tian, Yuan  and
      Chang, Kai-Wei",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.66",
    doi = "10.18653/v1/2020.findings-emnlp.66",
    pages = "743--749",
    abstract = "Privacy policy documents are long and verbose. A question answering (QA) system can assist users in finding the information that is relevant and important to them. Prior studies in this domain frame the QA task as retrieving the most relevant text segment or a list of sentences from the policy document given a question. On the contrary, we argue that providing users with a short text span from policy documents reduces the burden of searching the target information from a lengthy text segment. In this paper, we present PolicyQA, a dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices. We evaluate two existing neural QA models and perform rigorous analysis to reveal the advantages and challenges offered by PolicyQA.",
}

@article{Forbes2023MetricEF,
  title={Metric Ensembles For Hallucination Detection},
  author={Grant C. Forbes and Parth Katlana and Zeydy Ortiz},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.10495},
  url={https://api.semanticscholar.org/CorpusID:264147021}
}

@InProceedings{pmlr-v235-huang24x,
  title = 	 {Position: {T}rust{LLM}: Trustworthiness in Large Language Models},
  author =       {Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and Sun, Hanchi and Liu, Zhengliang and Liu, Yixin and Wang, Yijue and Zhang, Zhikun and Vidgen, Bertie and Kailkhura, Bhavya and Xiong, Caiming and Xiao, Chaowei and Li, Chunyuan and Xing, Eric P. and Huang, Furong and Liu, Hao and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang and Wang, Jindong and Vanschoren, Joaquin and Mitchell, John and Shu, Kai and Xu, Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu, Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman and Chen, Tianlong and Liu, Tianming and Zhou, Tianyi and Wang, William Yang and Li, Xiang and Zhang, Xiangliang and Wang, Xiao and Xie, Xing and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang and Cao, Yinzhi and Chen, Yong and Zhao, Yue},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {20166--20270},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24x/huang24x.pdf},
  url = 	 {https://proceedings.mlr.press/v235/huang24x.html},
  abstract = 	 {Large language models (LLMs) have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and capability (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like <em>moderator</em>, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we’ve uncovered key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community to foster collaboration is imperative to advance the trustworthiness of LLMs.}
}

@inproceedings {198044,
author = {Hamza Harkous and Kassem Fawaz and Kang G. Shin and Karl Aberer},
title = {{PriBots}: Conversational Privacy with Chatbots},
booktitle = {Twelfth Symposium on Usable Privacy and Security (SOUPS 2016)},
year = {2016},
address = {Denver, CO},
url = {https://www.usenix.org/conference/soups2016/workshop-program/wfpn/presentation/harkous},
publisher = {USENIX Association},
month = jun
}


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}
