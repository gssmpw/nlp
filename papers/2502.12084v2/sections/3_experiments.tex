

\section{Evaluation }
% \yi{Don't think we can expect readers to remember what the dataset subtask abbreviation stands for; in each new section we should spell out the full name out once}
\subsection{Metric Design}
\label{metrics}

\paragraph{T/F}  \textit{(Matching, Tracking, Comparison)}: Accuracy is computed based on paired evaluation, where a response is correct only if it answers \(T\) (ground-truth True) and \(F\) (ground-truth False) correctly. The overall accuracy across \(N\) test pairs is:

\begin{equation}
    Acc_{pair} = \frac{\sum_{i=1}^{N} \left( T_i^+ \cap F_i^- \right)}{N},
\end{equation}

\noindent where \( T^+ \) and \( F^- \) denote correct predictions for \(T\) and \(F\), respectively.
% The accuracy is computed based on paired evaluation, where a response is considered correct only if it correctly answers both \(T\) when the ground-truth answer is True and \(F\) when the ground-truth answer is False. The overall accuracy across \(N\) test pairs is formulated as:


% \begin{equation}
%     Acc_{pair} = \frac{\sum_{i=1}^{N} \left( T_i^+ \cap F_i^- \right)}{N},
% \end{equation}

% where \( T^+ \) represents correct predictions for \(T\), and \( F^- \) represents correct predictions for \(F\). 




% \paragraph{Numerical} \textit{(Counting)}:  For numerical responses, directly evaluating based on absolute matching fails to capture the degree of error effectively. To better quantify the error between model’s predicted number \( \hat{N}_i \) and the ground truth number \( N_i \), we  calculate the normalized error as follows:
% \begin{equation}
% \epsilon_i = \frac{\left| \hat{N_i} - N_{i} \right|}{\max\left(N_{i} - 1, N^{img}_i - N_i\right)},
% \end{equation}

% where \(N^{img}_i\) denotes the number of images input in this case. We then introduce \( w_i = \max (\{N^{\text{img}}_i\}_{i=1}^n )/{N^{\text{img}}_i} \) to penalize cases where \(N^{img}_i\) is small but an error still occurs.
% Additionally, we introduce \( \alpha \) as the penalty factor that amplifies the impact of larger errors. The final accuracy \( Acc_{num} \) on \( n \) test cases is calculated as:

% \begin{equation}
% Acc_{num} = 1 - \frac{1}{n} \sum_{i=1}^{n} w_i \cdot \epsilon_i^{-\alpha},
% \end{equation}

% here, \( Acc_{num} \) assigns higher accuracy to more difficult cases with smaller errors.
\paragraph{Numerical} \textit{(Counting)}: Absolute matching alone does not effectively reflect the severity of errors in numerical responses. To measure the extent of the error between the predicted count \( \hat{N}_i \) and ground truth \( N_i \), we introduce \(Acc_{num}\). The first step is to calculate the normalized error:

\begin{equation}
\epsilon_i = \frac{\left| \hat{N}_i - N_{i} \right|}{\max\left(N_{i} - 1, N^{img}_i - N_i\right)},
\end{equation}

\noindent where \( N^{img}_i \) is the number of input images. We define \( w_i = \max (\{N^{\text{img}}_i\}_{i=1}^n )/{N^{\text{img}}_i} \) to penalize errors in cases with fewer images and introduce \( \alpha \) as an error amplification factor. The final accuracy over \( n \) cases is:

\begin{equation}
Acc_{num} = 1 - \frac{1}{n} \sum_{i=1}^{n} w_i \cdot \epsilon_i^{-\alpha}.
\end{equation}

% \( Acc_{num} \) assigns higher scores to cases with smaller errors.



\paragraph{Multi-choice} \textit{(Grouping)}: Accuracy is the proportion of correctly predicted choices.    


\paragraph{Open-ended} \textit{(Video Identity Describing)}: We use GPT-4o to score model's descriptions, in combination with rule-based scoring prompts. The final accuracy \(Acc_{oe}\) is obtained by averaging the scores of all open-ended responses and rescaling them to the range of [0,1]. Additionally, we perform manual verification of GPT-4o’s scoring. For each model, we randomly sample 20 scored responses for review, and find only 2 instances with discrepancies, resulting in an accuracy rate of 98.89\% (178/180). Refer to Appendix~\ref{appendix: prompting approaches} for more details.


\subsection{Evaluation Setup}
\paragraph{Evaluated Models.} We evaluate eight open-source VLMs that support multiple-image or video input: LLaVA-OneVision~\citep{llava-onevision}, LLaVA-Video~\citep{llava-video}, LongVA~\citep{longva}, mPLUG-Owl3~\citep{mplug-owl3}, Qwen2-VL~\citep{Qwen2-VL}, Qwen2.5-VL~\citep{Qwen2.5-VL}, and InternVL2.5~\citep{internvl}. Additionally, we include the commercial model GPT-4o~\citep{gpt4o} for comparison.

\paragraph{Baselines.} We introduce chance-level and human-level baselines (details are in Appendix~\ref{appendix: baselines}).





\subsection{Results and Findings}
\paragraph{Results.} Table~\ref{exp:main_exp} presents the comprehensive performance of various models across the  three categories -- General Cue (GC), Object-centric Cue (OC), and Person-centric Cue (PC) -- of our VLM$^2$-Bench, covering a total of nine subtasks.

\paragraph{Finding I: Simple tasks for humans pose significant challenges for VLMs.} We observe that humans achieve near-perfect accuracy across most tasks in our VLM$^2$-Bench. In contrast, even GPT-4o, a state-of-the-art model, performs significantly lower than humans, with an overall performance gap of 34.80\%. For open-source models, many show performance comparable to the chance-level baseline or only slightly outperform it. Specifically, for the \textit{VID}, humans can easily achieve 100\% accuracy in distinguishing and linking individuals in a video. However, even the best-performing model, GPT-4o, reaches only 66.75\%. Errors mainly arise from failing to recognize individuals after changes or misidentifying reappearing persons as new.


\begin{table}[t]
\centering
\resizebox{0.49\textwidth}{!}{%
\begin{tabular}{l||cccc||cccc}
    % \hline
    Model&\multicolumn{4}{c}{Matching (\textit{Mat})}&\multicolumn{4}{c}{Tracking (\textit{Trk})}\\ \hline
     & \textbf{A/R} & \textbf{Swp} & \textbf{Attr} & \textbf{Env} & \textbf{A/R} & \textbf{Swp} & \textbf{Attr} & \textbf{Env} \\ \hline
    LV-OV & \cellcolor[HTML]{d9eaf4}50.68 & \cellcolor[HTML]{fbecd9}49.15 & 53.45 & 52.50  & \cellcolor[HTML]{fbecd9}27.27 & 	\cellcolor[HTML]{d9eaf4}45.51	 & 57.50	 & 70.59\\ \hline
    LV-Vid & 56.08 & \cellcolor[HTML]{fbecd9}49.15 & 53.45 & \cellcolor[HTML]{d9eaf4}51.25  & \cellcolor[HTML]{fbecd9}46.75	 & \cellcolor[HTML]{d9eaf4}48.88 & 	52.50 & 	67.65\\ \hline
    LongVA & \cellcolor[HTML]{fbecd9}37.84 & 46.58 & 53.45 & \cellcolor[HTML]{d9eaf4}46.25  & \cellcolor[HTML]{d9eaf4}46.10	 & 49.44	 & \cellcolor[HTML]{fbecd9}42.50 & 	60.29\\ \hline
    Owl3 & 54.73 & \cellcolor[HTML]{d9eaf4}52.56 & 55.17 & \cellcolor[HTML]{fbecd9}50.00  & \cellcolor[HTML]{fbecd9}41.56	 & \cellcolor[HTML]{d9eaf4}48.88	 & 55.00	 & 73.53\\ \hline
    Qw2-VL & \cellcolor[HTML]{d9eaf4}53.68 & \cellcolor[HTML]{fbecd9}52.56 & 55.17 & 68.75  & 65.58 & 	\cellcolor[HTML]{fbecd9}62.90 & 	77.50 & 	\cellcolor[HTML]{d9eaf4}63.93\\ \hline
    Qw2.5-VL & \cellcolor[HTML]{d9eaf4}64.19 & \cellcolor[HTML]{fbecd9}55.62 & 74.14 & 67.50  & \cellcolor[HTML]{d9eaf4}61.69 & 	69.10 & 	\cellcolor[HTML]{fbecd9}55.00 & 	64.71\\ \hline
    In2.5-8B & 64.86 & \cellcolor[HTML]{fbecd9}51.28 & \cellcolor[HTML]{d9eaf4}52.07 & 66.25 &  \cellcolor[HTML]{fbecd9}54.55 & 	67.42 & 	62.50 & 	\cellcolor[HTML]{d9eaf4}60.65\\ \hline
    In2.5-26B & 60.81 & \cellcolor[HTML]{fbecd9}51.71 & \cellcolor[HTML]{d9eaf4}58.62 & 61.25 &  \cellcolor[HTML]{d9eaf4}56.49 & 	62.92	 & \cellcolor[HTML]{fbecd9}47.50 & 	66.18\\ \hline
    GPT-4o & 75.00 & \cellcolor[HTML]{d9eaf4}61.97 & \cellcolor[HTML]{fbecd9}56.90 & 70.00 &  68.83 & 	\cellcolor[HTML]{d9eaf4}67.98	 & 67.50 & 	\cellcolor[HTML]{fbecd9}64.71\\ 
    % \hline
    \end{tabular}
}   
    \caption{Breakdown of four mis-matched cue types in two subtasks of GC. For each model, the \colorbox[HTML]{fbecd9}{highest} and \colorbox[HTML]{d9eaf4}{second highest}  error (\%) per subtask are highlighted.}
    \label{exp: breakdown gc}
\end{table}


\paragraph{Finding II: Relatively consistent error patterns in \textit{Mat} and \textit{Trk} of GC.} Table~\ref{exp: breakdown gc} shows that models struggle with mismatched cues due to swap in \textit{Mat}, which requires linking two completely different cues. To identify what has changed, models must first link and match all the other cues in the context before they can determine that the swapped cue has been transformed. 
This task requires a deeper understanding of how cues relate to each other across different instances. In contrast, \textit{Trk} challenges models with mismatched cues due to add/remove, which focuses on tracking how a specific cue changes. This suggests that when there is a cue that appears only once, the model struggles to link the non-appearing cue with the appearing cue to track the transformation process effectively. This limitation reveals models' difficulty in handling cases where certain cues are missing but still need to be linked to understand the dynamic changes. 
% \yi{In general, for the entire result analysis session, I suggest to ground your discussion with some concrete error examples to make it clearly understandable for the naive reader.} 

\paragraph{Finding III: Models perform better in linking person-centric cues than object-centric cues.} We selected the top three open-source models (Qwen2.5-VL-8B, InternVL2.5-8B, InternVL2.5-26B) and compared their performance on the three shared tasks (\textit{Cpr, Cnt, Grp}) in both OC and PC. Results show that, on average, the performance on PC is higher than on OC by 7.65\%, 9.75\%, and 11.83\% for the tasks of \textit{Cpr, Cnt, Grp}, respectively. This could be due to the fact that, during training on person-related data, models are likely provided with explicit person names as anchors to person-centric cues, which helps the models better distinguish different individuals. In contrast, objects are typically trained using general category names, which may not provide such clear distinctions. Additionally, these models might have been specifically trained on large datasets that emphasize differentiating and linking individuals~\citep{pi2024personalized,human-data}, thereby enhancing their ability to link person-centric cues.
