\section{Related Work}

%\subsection{Advancements in Vision-language Models}
\paragraph{Advancements in vision-language models}~\citep{gpt4o,Qwen2.5-VL,longva,llava-onevision,mplug-owl3,internvl,liang2024foundations} have significantly broadened their capabilities. Previously restricted to processing single-image inputs, many VLMs can now handle multi-image and even video inputs, allowing them to capture richer and more dynamic visual contexts. Additionally, with access to a growing volume of high-quality visual-textual paired training data~\citep{image-hq-1,image-hq2,image-hq-3,video-hq1,video-hq2}, these models have shown substantial improvements in perceiving subtle visual cues and their relationships, enabling them to engage in more nuanced reasoning about visual content. Furthermore, VLMs are increasingly applied in real-world scenarios, including navigation~\citep{navigation}, planning~\citep{motion}, and autonomous driving~\citep{driving}, solidifying their role in bridging vision and language for practical applications. However, to truly integrate into everyday life, VLMs still have significant room for improvement when it comes to more fundamental but common visual tasks, such as those assessed in our benchmark.


%\subsection{Benchmarking Vision-language Models}
\paragraph{Benchmarking vision-language models} plays a critical role in guiding their future development~\citep{liang2024hemm,mme,mmstar}. These benchmarks typically focus on assessing the models' fine-grained perception~\citep{li2024naturalbench,mmvp}, reasoning abilities~\citep{scienceqa,mmvet,wu-etal-2024-macaroon}, commonsense knowledge~\citep{yue2024mmmu}. In addition, evaluations targeting multi-image and video inputs are designed to measure the new competencies that VLMs require as their visual context extends. These tasks include captioning~\citep{yue2024mmmu,yu2019activitynet}, retrieval~\citep{wang2024muirbench,li2025migician}, comparison~\citep{MICBench,jiao2024img}, and temporal reasoning~\citep{liu2024tempcompass}.
However, existing benchmarks focus on evaluating VLMs' ability to interpret visual cues based on their knowledge. In contrast, humans typically solve such tasks by explicitly matching visual cues without relying on extensive background knowledge. To better assess whether they can replicate this human-like ability, we propose VLM$^2$-Bench, which focuses on linking and matching explicit visual cues.
% single image input: image captioning, image understanding,
% multiple images input: MMDU \cite{liu2024mmdu}
% compare~\citep{kil2024compbench}
% grounding~\citep{li2025migician}
% spot-diff~\citep{jiao2024img} (only contain replace, removal; a data synthesis method, low-quality)
% perceptual error~\citep{PieAPP}
% Vision-linked Textual Knowledgetra~\citep{liu2024mibench}
% video~\citep{yu2019activitynet, liu2024tempcompass}