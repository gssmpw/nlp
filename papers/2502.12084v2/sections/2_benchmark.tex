\section{VLM$^2$-Bench}
VLM$^2$-Bench is a benchmark designed to assess models' ability to visually link matching cues when processing multiple images or videos. This section introduces the three main categories of VLM$^2$-Bench—\textit{general cue} (\S\ref{gc}), \textit{object-centric cue} (\S\ref{oc}), and \textit{person-centric cue} (\S\ref{pc})—detailing their associated subtasks, data collection process, and QA pair construction.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\textwidth]{img/general-pipe.png} 
  % \vspace{-0.8cm}
  \caption{Construction of \textbf{GC}: (i) We start by manually verifying the edited image data based on three key criteria. (ii) A VLM is then prompted to generate captions for each image, followed by salient score-based filtering to retain the challenging cases. (iii) Finally, visual cues are extracted from two sources and incorporated into a QA prompt, guiding an LLM to generate both positive and negative answer pairs. 
  % \yi{"a event hall" is hard to read in the top right -- rule of thumb is need to make paper read-friendly for color blind ppl and when ppl print in black and white (otherwise reviewer might crib); turn that shade into darker shade of color please}
  }
  \label{fig:gc construction}
\end{figure*}

\subsection{General Cue (GC)}
\label{gc}
GC is designed to assess a model's ability to link matching cues across diverse contexts, encompassing a broad range of \textit{general cues}. Given two images containing both matched and mismatched cues, an ideal model should accurately identify mismatched ones and associate matched ones.

\paragraph{Subtasks.}
Here we introduce two subtasks: (i) \textbf{\textit{Matching (Mat)}} evaluates a model’s ability to link corresponding visual cues across two images to determine whether they match. Instead of merely identifying differences, the model must associate identical visual elements in both images to recognize what has remained the same and what has changed. 
(ii) \textbf{\textit{Tracking (Trk)}} focuses on a model’s ability to track a specific visual cue that appears in only one of the two images and determine how it has changed. Rather than simply detecting a difference, the model must link the cue across contexts to understand the transformation process. 



\paragraph{Data Collection.} 
We repurpose data from two image editing datasets~\citep{wei2024omniedit,ku2023imagenhub}, where each data sample includes an original image $I_{ori}$, an edited image with subtle modifications $I_{edit}$, and a corresponding edit instruction $\mathcal{P}$ describing the changes. Our data collection is carried out across two dimensions. First, to ensure diversity in the mismatched cues, GC encompasses various types of changes, such as instance-level modifications (e.g., add/remove, swap, attribute change), which focus on specific items, as well as environment-level changes.

\paragraph{QA Construction.}
We predefine a T/F question template for \textit{Mat} and \textit{Trk} with a placeholder for the candidate answer (refer to Appendix~\ref{appendix: more details on benchmark construction}). Figure~\ref{fig:gc construction} illustrates the construction process, which follows a three-stage approach. 

\textit{Manual Screening \& Refinement:} We ensure that $\mathcal{P}$ accurately reflects the changes (correctness), corresponds uniquely to the modified cues (uniqueness), and is unambiguous (clarity).


\textit{Salient Sampling:} Here, we automate the removal of overly simple cases (e.g., mismatched cues are too salient). To achieve this, a VLM first generates separate descriptions for \( I_{ori} \) and \( I_{edit} \), denoted as \( Cap_{ori} \) and \( Cap_{edit} \). These descriptions are then combined with \( \mathcal{P} \) into a single passage using a predefined template \(\mathcal{T}\) (see Table \ref{template salient score} for details). The probability assigned by a language model (e.g., Llama3-8B~\citep{dubey2024llama}) to \( \mathcal{P} \) given this text-based information is used to compute the salient score, formulated as:

\vspace{-13pt}
\begin{equation}
S_{\text{salient}} = \frac{1}{|\mathcal{P}|} \sum_{i=1}^{|\mathcal{P}|} \log P_{\theta}(p_i \mid C \cup p_{<i}),
\end{equation}

\noindent where \( \mathcal{P} = \{p_1, p_2, ..., p_{|\mathcal{P}|}\} \) represents the tokenized \(\mathcal{P}\), and \( C = \mathcal{T} (Cap_{ori}, Cap_{edit}) \) denotes the context filled with template \(\mathcal{T}\). Samples with scores below \( \theta \) (-2.0 here) are retained, ensuring that the benchmark includes more challenging examples requiring nuanced visual cue association. 
% \yi{what's your actual threshold used? which LLM exactly did you use to compute salient score and construct your dataset here? I know it's tempting to leave many details into appendix but if you don't provide sufficient information for readers to follow and reproduce your methods at bare minimum from the 8 main pgs then you may put the paper at greater risk for reviewer attack (things to consider) -- good to cross reference other benchmark construction papers and see how they write these details}


\textit{Pair-wise Answer Generation:} Finally, we extract visual cues using a dual-level approach. First, cues parsed from VLM-generated descriptions compensate for the limitations of open-set detectors when handling out-of-distribution scenes. Meanwhile, the open-set detector~\citep{wu2022grit} extracts fine-grained cues that VLMs might overlook. With these extracted cues, we prompt an LLM to generate a pair of answers for \textit{Mat} and \textit{Trk}, each consisting of one positive and one negative answer.




\subsection{Object-centric Cue (OC)}
\label{oc}
OC aims to assess a model's ability to link matching cues associated with everyday objects using \textit{object-centric cues}. Even when encountering an object for the first time, a well-aligned model should be able to leverage its unique visual cues to establish associations, enabling it to recognize and track the object across different scenes. This capability is essential for coherent perception and interaction in real-world deployments.

\paragraph{Subtasks.} 
\label{subtasks}
Based on the complexity of linking cues to solve the problem, we define three subtasks in OC. (i) \textbf{\textit{Comparison (Cpr)}} requires the model to determine whether the objects appearing in different images are the same. This task primarily assesses the model’s ability to perceive visual consistency or change. Notably, we observe that models exhibit significant model-specific bias when making a binary decision~\citep{pair, ye2024justice, song2024large,li2024naturalbench}, leading to discrepancies between results and their actual capabilities. To mitigate this, we introduce consistency-pair validation, where for each statement (e.g., ``X is Y”, with the answer being T), we generate a corresponding negation (e.g., ``X is not Y”, with the answer being F). The model is only considered correct if it correctly answers both statements, ensuring consistency in its decision-making.
(ii) \textbf{\textit{Counting (Cnt)}} involves identifying the number of unique objects, requiring the model not only to recognize variations or consistencies but also to track distinct cues to avoid double-counting the same object. (iii) \textbf{\textit{Grouping (Grp)}}, the most challenging one, requires the model to identify all instances of the same object, building on precise cue matching across multiple images.

\paragraph{Data Collection.}  
We manually collect various categories of everyday objects (e.g., pets, cups). For each category, we define multiple subcategories and collect a set of images \( \mathcal{I}_{O_i}\)—four images that depict the same object in different scenarios. Additionally, we also collect a set \( \mathcal{I}_{\neg O_i} \), consisting of four images of different objects, each containing some matching visual cues with \( \mathcal{I}_{O_i} \), which are used as distractors.
% We manually collect 8 major categories of everyday objects (e.g., pets, cups) from various online sources. Within each category, we defined multiple subcategories \( O_i \) and collected \( \mathcal{I}_{O_i} = \{ I_1, I_2, I_3, I_4 \} \)—4 images that depict the same object in different scenarios. Additionally, we also collect \( \mathcal{I}_{\neg O_i} \), a set of 4 images featuring different objects, each containing some matching visual cues with \( \mathcal{I}_{O_i} \). \( \mathcal{I}_{\neg O_i} \) are used as distractors to test the model's ability to differentiate between objects that share many cues but are distinct.


\paragraph{QA Construction.}
For each subtask, we define a question template that includes a placeholder for \( \mathcal{I}_{O_i} \), which allows us to tailor the question based on different objects (see Appendix~\ref{appendix: more details on benchmark construction}). For answer generation, we first curate the multi-image sequences according to predefined rules. For each specific sequence, we generate the ground truth answers for the questions related to \textit{Cpr}, \textit{Cnt}, and \textit{Grp}.




\subsection{Person-centric Cue (PC)}
\label{pc}
PC aims to evaluate a model's ability to link \textit{person-centric cues}. While a model cannot memorize every individual, it should possess the capability to associate the same person across different images or frames by leveraging distinctive visual cues such as facial features, clothing, or body posture. This ability is essential for ensuring coherent perception of human actions and is a fundamental requirement for real-world VLM applications.

\paragraph{Subtasks.} 
Similar to OC's subtasks (refer to \S\ref{subtasks}), PC includes (i) \textbf{\textit{Comparison (Cpr)}}, (ii) \textbf{\textit{Counting (Cnt)}}, and (iii) \textbf{\textit{Grouping (Grp)}}. However, unlike objects, individuals can be observed through their actions in videos. Therefore, we introduce (iv) \textbf{\textit{Video Identity Describing (VID)}}. This subtask assesses whether a model can correctly link the same person by analyzing its description of a video containing that person.


\paragraph{Data Collection.}  
We manually select several individuals, each denoted as \( \mathcal{P}_i \). For each individual, we collect \( \mathcal{I}_{\mathcal{P}_i}\)—4 images depicting the same individual. For each image \( I_i  \in  \mathcal{I}_{\mathcal{P}_i} \), we select the distractor images \( I_{\neg i} \notin \mathcal{I}_{\mathcal{P}_i} \) that has the highest CLIP similarity~\citep{hessel2021clipscore}. This allows us to obtain images of different individuals where most cues are matched.
% We manually select 30 different individuals, each denoted as \( \mathcal{P}_i \). For each individual, we collect \( \mathcal{I}_{\mathcal{P}_i} = \{ I_1, I_2, I_3, I_4 \} \)—4 images depicting the same individual in different scenarios. For each image \( I_i  \in  \mathcal{I}_{\mathcal{P}_i} \), we select the distractor images \( I_{\neg i} \notin \mathcal{I}_{\mathcal{P}_i} \) that has the highest CLIP similarity. This allows us to obtain images of different individuals where most cues are matched.
For the subtask of \textit{VID}, we collect videos of different individuals, denoted as \( V_{\mathcal{P}_i} \), and pair each with another video \( V_{\neg \mathcal{P}_i} \) featuring a different individual with highly similar cues (e.g., actions, scene, clothing). We then construct two video sequences: 
(i) \( \mathcal{P}_i \xrightarrow{} \neg \mathcal{P}_i \), assessing the model's ability to distinguish individuals. 
(ii) \( \mathcal{P}_i \xrightarrow{} \neg \mathcal{P}_i \xrightarrow{} \mathcal{P}_i \), evaluating whether the model detects changes and links the final occurrence of \( \mathcal{P}_i \) to its first appearance.

% Additionally, for the subtask of \textit{VID}, we collect videos of different individuals, denoted as \( V_{\mathcal{P}_i} \). For each \( V_{\mathcal{P}_i} \), we find another video of a different individual, denoted as \( V_{\neg \mathcal{P}_i} \), with highly similar cues (such as actions, scene, clothing, etc.). We then construct two types of videos based on different person sequences. (i) \( \mathcal{P}_i \xrightarrow{} \neg \mathcal{P}_i \), which tests the model's ability to distinguish between the two individuals. (ii) \( \mathcal{P}_i \xrightarrow{} \neg \mathcal{P}_i \xrightarrow{} \mathcal{P}_i \), which examines whether the model can detect changes between \( \mathcal{P}_i \) and \( \neg \mathcal{P}_i \), and link the final occurrence of \( \mathcal{P}_i \) to its first appearance.


\paragraph{QA Construction.} The construction for the overall QA in PC’s \textit{Cpr}, \textit{Cnt}, and \textit{Grp} subtasks follows a similar approach to OC. For the \textit{VID} task, we emphasize the model's ability to describe individuals when designing open-ended questions, aiming to better test the model's capacity to link individuals appearing in different scenes.

\begin{figure}[hb]
  \centering
  \includegraphics[width=0.49\textwidth]{img/bench-stastics.png} 
  % \vspace{-0.8cm}
  \caption{Statistical overview of \textbf{VLM$^2$-Bench}. The pie chart shows the distribution of 9 subtasks across the 3 main categories of visual cues. The bar plot illustrates the percentage breakdown by question format. 
  }
  \label{fig:bench statistics main}
\end{figure}

\begin{figure*}[t]
    \captionsetup{type=table}
    % \vspace{-0.4cm}
    \centering
    \begin{minipage}{0.99\textwidth}
        \centering
        \resizebox{1\textwidth}{!}{%
        \begin{tabular}{l||cc|ccc|cccc|cc}
            \toprule
            \textbf{Baselines or Models} & \multicolumn{2}{c|}{\textbf{GC}} & \multicolumn{3}{c|}{\textbf{OC}} & \multicolumn{4}{c|}{\textbf{PC}} & \multicolumn{2}{c}{\textbf{Overall*}}\\
            % \midrule
            & \textit{Mat} & \textit{Trk} & \textit{Cpr} & \textit{Cnt} & \textit{Grp} & \textit{Cpr} & \textit{Cnt} & \textit{Grp} & \textit{VID} & Avg & \textbf{$\Delta_{human}$} \\
            \midrule
            \textcolor{black!50}{Chance-Level} & 
            \textcolor{black!50}{25.00} & 
            \textcolor{black!50}{25.00} & 
            \textcolor{black!50}{50.00} & 
            \textcolor{black!50}{34.88} & 
            \textcolor{black!50}{25.00} & 
            \textcolor{black!50}{50.00} & 
            \textcolor{black!50}{34.87} & 
            \textcolor{black!50}{25.00} & 
            \textcolor{black!50}{-} & 
            \textcolor{black!50}{33.72} & 
            \textcolor{black!50}{-61.44} \\
            Human-Level  & 95.06 & 98.11 & 96.02 & 94.23 & 91.92 & 97.08 & 92.87 & 91.17 & 100.00 & 95.16 & 0.00 \\
            \midrule
            LLaVA-OneVision-7B    & 16.60 & 13.70 & 47.22 & 56.17 & 27.50 & \cellcolor{yellow!45}62.00 & 46.67 & 37.00 & \cellcolor{yellow!15}47.25 & 39.35 & -55.81 \\
            LLaVA-Video-7B        & 18.53 & 12.79 & 54.72 & \cellcolor{yellow!15}62.47 & 28.50 & \cellcolor{yellow!45}62.00 & \cellcolor{yellow!45}66.91 & 25.00 & \cellcolor{yellow!45}59.00 & 43.32 &  -51.84 \\
            LongVA-7B             & 14.29 & 19.18 & 26.67 & 42.53 & 18.50 & 21.50 & 38.90 & 18.00 & 3.75  & 22.59 & -72.57 \\
            mPLUG-Owl3-7B         & 17.37 & 18.26 & 49.17 & \cellcolor{yellow!45}62.97 & 31.00 & \cellcolor{yellow!15}63.50 & 58.86 & 26.00 & 13.50 & 37.85 & -57.31 \\
            Qwen2-VL-7B  & 27.80 & 19.18 & \cellcolor{yellow!15}68.06 & 45.99 & 35.00 & 61.50 & 58.59 & 49.00 & 16.25 & 42.37 & -52.79 \\
            Qwen2.5-VL-7B & \cellcolor{yellow!45}35.91 & \cellcolor{yellow!75}43.38 & \cellcolor{yellow!45}71.39 & 41.72 & \cellcolor{yellow!15}47.50 & \cellcolor{yellow!75}80.00 & 57.98 & \cellcolor{yellow!75}69.00 & 46.50 & \cellcolor{yellow!45}54.82 & -40.34 \\
            InternVL2.5-8B        & 21.24 & 26.03 & 53.33 & 55.23 & 46.50 & 51.50 & \cellcolor{yellow!15}60.00 & \cellcolor{yellow!15}52.00 & 5.25  & 41.23 & -53.93 \\
            InternVL2.5-26B        & \cellcolor{yellow!15}30.50 & \cellcolor{yellow!15}30.59 & 43.33 & 51.48 & \cellcolor{yellow!45}52.50 & 59.50 & 59.70 & \cellcolor{yellow!45}61.00 & 21.75 & \cellcolor{yellow!15}45.59 & -49.57 \\
            \midrule
            GPT-4o                & \cellcolor{yellow!75}37.45 & \cellcolor{yellow!45}39.27 & \cellcolor{yellow!75}74.17 & \cellcolor{yellow!75}80.62 & \cellcolor{yellow!75}57.50 & 50.00 & \cellcolor{yellow!75}90.50 & 47.00 & \cellcolor{yellow!75}66.75 & \cellcolor{yellow!75}60.36 & -34.80 \\
            \bottomrule
        \end{tabular}
        }
    \end{minipage}
    % \hfill
    % \begin{minipage}[c]{0.22\textwidth}
    %     \centering
    %     \vspace{-0.5em}
    %     \includegraphics[width=\textwidth]{img/radar_chart.png}
    % \end{minipage}
    % \vspace{-0.2cm}
    \caption{Evaluation results on \textbf{VLM$^2$-Bench}, covering \textit{Mat} (Matching), \textit{Trk} (Tracking), \textit{Cpr} (Comparison), \textit{Cnt} (Counting), \textit{Grp} (Grouping), and \textit{VID} (Video Identity Describing). The \colorbox{yellow!75}{highest}, \colorbox{yellow!45}{second}, and \colorbox{yellow!15}{third} highest scores are highlighted. *: Overall excludes the \textit{VID} due to the lack of a chance-level baseline for open-ended tasks.}
    \label{exp:main_exp}
    % \vspace{-0.4cm}
\end{figure*}

\subsection{Benchmark Statistics}
\label{bench_statistics_main}
Our benchmark is organized into three main categories, comprising a total of 9 subtasks. After careful verification, it contains 3,060 question-answer pairs, with varying formats including T/F, multi-choice (MC), numerical (Nu), and open-ended (Oe). To ensure the quality of the annotations, we perform an inter-annotator agreement (IAA) evaluation~\citep{thorne2018fever} involving three annotators, resulting in a high Fleiss' Kappa score~\citep{fleiss1971measuring} of 0.983. Figure~\ref{fig:bench statistics main} presents the distribution of these subtasks across the three categories, along with the breakdown of different question formats. For additional details, refer to Appendix~\ref{appendix: statistics}.

% \yi{TODO: add overall descriptive stats for dataset, and interannotator agreement}

