\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{img/teaser.png} 
  % \vspace{-0.8cm}
  \caption{\textcolor[HTML]{3d5d92}{\textbf{Previous benchmarks}} fail to assess the ability to link matching visual cues, whereas our \textcolor[HTML]{cd7d4e}{\textbf{VLM$^2$-Bench}} explicitly tests this ability, as shown in the example where the model need to identify the reappearance of the same person by linking visual cues, like facial features or clothing, across non-adjacent frames. 
  }
  \label{fig:teaser}
  \vspace{-0.5em}
\end{figure}
\section{Introduction}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\textwidth]{img/bench_overview.pdf} 
  % \vspace{-0.8cm}
  \caption{Overview of \textbf{VLM$^2$-Bench}. The benchmark is categorized into three subsets based on visual cues: GC (General Cue), OC (Object-centric Cue), and PC (Person-centric Cue), each comprising multiple subtasks. To comprehensively evaluate VLMs' ability to visually link matching cues, the benchmark includes diverse question formats—T/F \raisebox{-0cm}{\includegraphics[width=0.5cm]{img/tf-icon.png}}, multiple-choice \raisebox{-0.1cm}{\includegraphics[width=0.4cm]{img/mcq-icon.png}}, numerical \raisebox{-0.05cm}{\includegraphics[width=0.3cm]{img/nq-icon.png}}, and open-ended \raisebox{0.0cm}{\includegraphics[width=0.3cm]{img/oeq.png}}—ensuring a comprehensive evaluation.}
  \label{fig:overview}
\end{figure*}
Humans constantly link matching visual cues to navigate and understand their environment. For instance, we can determine whether objects, and individuals are the same simply by comparing their distinguishing visual features~\citep{face1,face2,object1}. This ability, often without needing additional background knowledge, is fundamental in our daily interactions with the world around us. However, while current vision-language models (VLMs)~\citep{internvl,llava-onevision,llava-video,Qwen2.5-VL} have demonstrated extensive knowledge and expanded their capabilities from single-image understanding to handling multiple images and videos, \textit{whether thay can effectively link matching visual cues across images or frames—an essential skill for coherent multimodal reasoning—remains an open question.} 
% \yi{Note: don't need to cite that many VLMs in intro first paragraph}

As shown in Figure~\ref{fig:teaser}, existing benchmarks on multiple images and videos
% \yi{here it'd be better to add a short descriptive phrase on what category of benchmarks is under consideration for discussion here} 
fall short in exploring this fundamental ability as they: (a) do not require explicitly linking visual cues across images or frames~\citep{liu2024mmdu, yu2019activitynet}; (b) rely on external knowledge rather than assessing models' ability to link explicitly visual cues~\citep{MIRB, liu2024mibench}; (c) emphasize broad and abstract visual comparisons rather than specific cue matching~\citep{MICBench, liu2024tempcompass}; and (d) focus on retrieval-based tasks rather than evaluating the direct association of visual cues across different visual contexts~\citep{wang2024muirbench}.



To bridge this gap, we introduce \textbf{VLM$^2$-Bench}, a benchmark specifically designed to evaluate how well VLMs visually link matching cues. VLM$^2$-Bench is structured around three types of visual cue connection: \textit{general cue}, \textit{person-centric cue}, and \textit{object-centric cue}, encompassing a total of eight subtasks. To balance scalability and quality, we design a semi-automated pipeline with human verification for further refinement. Additionally, our subtasks cover a variety of QA formats—including T/F, multi-choice, numerical, and open-ended questions—totaling over 3,000 question-answer pairs. To better evaluate model performance, we also design specific metrics tailored to various task.

We conduct a comprehensive evaluation of 8 open-source models and GPT-4o on our VLM$^2$-Bench. Despite VLMs generally possessing extensive knowledge, some models perform on par with, or even worse than, the chance-level baseline on our vision-centric tasks. Notably, GPT-4o also underperforms, lagging behind human-level accuracy by 34.80\%. This highlights the significant room for improvement in VLMs' ability to link visual cues. Furthermore, we introduce various language-side and vision-side prompting techniques to explore whether they can enhance the models' performance on the benchmark. Through experimental results and case studies, we present \textit{eight key observations}, hoping that these insights will guide future improvements in VLMs for vision-centric tasks.

