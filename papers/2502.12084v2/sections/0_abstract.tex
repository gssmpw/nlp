\begin{abstract}
    Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce \textbf{VLM$^2$-Bench}, a benchmark designed to assess whether \textbf{VLM}s can \textbf{V}isually \textbf{L}ink \textbf{M}atching cues, with 9 subtasks and over 3,000 test cases. 
    Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80\% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.\footnote{Project page: \url{https://vlm2-bench.github.io/}. \newline $^\spadesuit$Work was done while student was an intern at HKUST.}
    % Comprehensive evaluation across 8 open-sourced VLMs and GPT-4o reveals a significant performance gap between VLMs and humans, where even GPT-4o lags 34.80\% behind human performance. Additionally, we explore the impact of various language-side and vision-side prompting methods. We find that (i) language-based prompting helps with cues that are easy to describe, but may hinder performance with abstract cues, and (ii) vision-based prompting's effectiveness depends on the model's ability to interpret the visual signals and its inherent understanding of cues. 
    % We hope VLM$^2$-Bench and our insights will inspire broader advancements in VLM research, guiding future developments in vision-centric reasoning and multimodal intelligence.

\end{abstract}