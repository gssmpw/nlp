\section{Conclusion}
% Note: don't use "pioneering" -- sounds ChatGPT
In summary, we introduce VLM$^2$-Bench, a novel benchmark designed to probe the capability of vision-language models (VLMs) in visually linking matching cues, an essential yet underexplored skill for models in everyday visual reasoning. 
Through extensive evaluations and further analysis of prompting techniques applied on our benchmark, we identify 8 key findings. Notably, even GPT-4o falls 34.80\% behind human performance. Based on these insights, we advocate for advancements in fundamental visual capabilities, better integration of language-based reasoning, and the evolution of vision-text training paradigms to improve VLMs' performance in vision-centric tasks.
% Our findings urge the AI community to prioritize improvements in how models integrate and interpret cross-context visual information, which is crucial for robust multimodal reasoning in real-world applications. 
% \yi{TODO: recap a bit on 1-2 most exciting insights why problem setting is challenging}