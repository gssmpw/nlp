\section*{Limitations}
VLM$^2$-Bench focuses on evaluating visual cue linking but does not cover all possible scenarios. Additionally, while it provides valuable insights, its scale is limited, and model performance may not fully generalize to all real-world settings. Automated evaluation constraints limit the inclusion of open-ended questions in our benchmark, impacting the assessment of models' vision-centric reasoning abilities. Expanding task diversity and refining evaluation methods remain important directions for future work.
% %\paragraph{Scope and Scalability of Tasks.} 
% Although VLM$^2$-Bench covers several key subtasks, it does not encompass all possible visual cue linking scenarios. The current benchmark focuses on a select range of visual cues, but more complex or nuanced visual tasks—such as those involving motion, occlusion, or multi-frame temporal reasoning—are not well-represented. Consequently, the benchmark may not fully capture the diverse challenges that VLMs might face in real-world visual tasks. Additionally, the number of subtasks and questions in VLM$^2$-Bench is limited. While the benchmark offers valuable insights into model performance, expanding the subtasks and question set could provide a better assessment of model generalization across more complex scenarios.

% %\paragraph{Generalization of Performance.} 
% Therefore, the performance of models evaluated in VLM$^2$-Bench may not generalize well to all types of visual cues or tasks. Although our benchmark includes a range of visual cue types (e.g., general cues, object-centric, person-centric), the performance observed in these tasks might not fully reflect how these models would perform in real-world scenarios with unseen cues or more varied contexts. The models' ability to generalize to new tasks is still a subject for further investigation.

% %\paragraph{Evaluation Limitations.} 
% Moreover, while VLM$^2$-Bench evaluates models based on their ability to link visual cues and solve tasks, there are limitations in how these tasks can be automatically assessed. For instance, in open-ended tasks, the quality of the answers provided by the models can vary significantly depending on the underlying language model’s capabilities, and this may not always reflect their true ability to handle vision-centric tasks. Additionally, the performance on certain types of tasks, such as those requiring more nuanced reasoning, may not always be fully captured by automated evaluation.