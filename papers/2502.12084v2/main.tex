% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage[preprint]{acl}
% \usepackage[final]{acl}

\usepackage[skins,most]{tcolorbox} 
\tcbuselibrary{breakable} 
\newtcolorbox[auto counter, number within=section, list type=subsubsection, list inside=toc]{sectionbox}[2][]{
colback=white!98!gray, colframe=black, 
colbacktitle=white!90!gray, coltitle=black, 
fonttitle=\bfseries,
title={#2}, 
list entry={Comment \thetcbcounter\quad}
}

% Standard package includes
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{color}
\usepackage{listings}
\definecolor{customTeal}{RGB}{0, 128, 128}
\definecolor{emphasisColor}{RGB}{255, 0, 0} % Red color for emphasis
\usepackage{tabularx}
\usepackage{ragged2e}
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X} % 自适应的左对齐列
% \usepackage[most]{tcolorbox}
\usepackage[skins,most]{tcolorbox}
\usepackage{colortbl}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage{wrapfig}



\usepackage{pifont}  % 引入 pifont 以支持 \ding 命令
% 自定义颜色
\definecolor{customgreen}{HTML}{16C47F}  % 使用指定的 16C47F 绿色
\definecolor{customred}{HTML}{C62300}   % 酒红色（不变）

% 定义带颜色的✔和✘
\newcommand{\cmark}{\textcolor{customgreen}{\ding{51}}}  % ✔
\newcommand{\xmark}{\textcolor{customred}{\ding{55}}}   % ✘

\usepackage{float}


\usepackage{caption}

\lstset{
    language=Python,         
    basicstyle=\fontsize{7.0pt}{7.5pt}\ttfamily\selectfont,
    keywordstyle=\color{customTeal},    
    stringstyle=\color{customTeal},    
    commentstyle=\color{customTeal},     
    morecomment=[l][\color{green}]{\#},
    breaklines=true,                
    showstringspaces=false,
    escapeinside={(*@}{@*)}, % 
    numbers=left,          
    stepnumber=1,           
    numberstyle=\tiny\color{gray}, 
    numbersep=5pt,         
    xleftmargin=1.5em,      
    frame=none,              
}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{enumitem} 
\usepackage{lipsum}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\NewDocumentCommand{\yi}
{ mO{} }{\textcolor{blue}{\textsuperscript{\textit{May}}\textsf{\textbf{\small[#1]}}}}

\newcommand{\name}{}
\newcommand{\briefname}{\texttt{PC-Bench} }

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


% \title{No Need to Know Where Cars Were Made to Tell Them Apart: \\How Well Do VLMs Visually Link Matching Cues? \yi{Alternative Candidates: teaser phrase - ("Caught the Connection?" or "Is that the Same Car or a Different One?"); main part - ("A Closer Look at How Well VLMs Implicitly Link Explicitly Matching Visual Cues")} \yi{Current teaser is sort of too low and not on-point/comprehensive/precise enough}} 


% \title{Caught the Connection? A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues}
\title{\raisebox{-0.15cm}{\includegraphics[width=0.8cm]{img/vlm2-bench-icon_final.png}}VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link \\ Explicit Matching Visual Cues}


% Author information can be set in various styles:
% For several authors from the same institution:

\DeclareSymbolFont{extraup}{U}{zavm}{m}{n}
\DeclareMathSymbol{\vardiamond}{\mathalpha}{extraup}{87}
\author{\bf Jianshu Zhang\textsuperscript{$^{\heartsuit}$\thanks{These authors contribute to this work equally.}}, ~ Dongyu Yao$^{\spadesuit*}$,  ~ Renjie Pi$^{\heartsuit}$, ~ Paul Pu Liang$^{\vardiamond}$, ~ Yi R. (May) Fung$^{\heartsuit}$\\
$^{\heartsuit}$HKUST ~~~~~~$^{\spadesuit}$CMU
~~~~~~$^{\vardiamond}$MIT\\
\texttt{jianshu.zhang777@gmail.com}
~~~\texttt{raindy@cmu.edu} ~~~\texttt{rpi@ust.hk} \\
\texttt{ppliang@mit.edu} ~~~\texttt{yrfung@ust.hk}
}


\begin{document}
\maketitle



\input{sections/0_abstract}
\input{sections/1_intro}
\input{sections/2_benchmark}
\input{sections/3_experiments}

\section{How Prompting Methods affect VLMs}
% \yi{TODO: Name this section better}

In this section\footnote{Due to space limits, we reference most case studies, figures, and details in the Appendix within this section.}, we investigate various prompting methods (language-side and vision-side) to evaluate their impact on performance in VLM$^2$-Bench. We select the top 3 performing open-source models (Qwen2.5-VL-8B, InternVL2.5-8B, InternVL2.5-26B), along with GPT-4o, and explore different approaches of CoT~\citep{cot1,cot2} and visual prompting (VP)~\citep{lei2024scaffoldingcoordinatespromotevisionlanguage, vp-zoom-in} (refer to Appendix~\ref{appendix: prompting approaches} for details). The goal is to investigate whether these techniques can improve performance across the benchmark and to identify the underlying factors that contribute to their success or failure. 
% \yi{Some NLP readers may not immediately understand what visual prompting looks like -- some figure in main body of paper could help (you can pick or trim down for the most importantly findings to allot space)}



\subsection{Probing for General Cue (GC)}
\paragraph{Methods.} (i) \textbf{CoT-normal} (Table~\ref{cot prompt}) encourages the model to solve the task step by step, allowing it to reason through the problem. (ii) \textbf{CoT-special} (Table~\ref{cot-special}) guides the model to solve the task using a thought process closer to how humans typically approach it. (iii) \textbf{VP-grid} (Figure~\ref{fig:vp-grid}) is adapted from previous work ~\citep{lei2024scaffoldingcoordinatespromotevisionlanguage} for our tasks, overlaying a dot matrix on the image as visual anchors to provide positional references and enhance the model's performance.

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/gc-analysis.png}
    \caption{Results of CoT-normal, CoT-special, and VP-grid on GC.}
    \label{fig:gc-analysis}
  \end{subfigure}
  \\ % This ensures vertical spacing
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/oc-analysis.png}
    \caption{Results of CoT and VP-zoom-o on OC.}
    \label{fig:oc-analysis}
  \end{subfigure}
  \\ 
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/pc-analysis.png}
    \caption{Results of CoT and VP-zoom-p on PC.}
    \label{fig:pc-analysis}
  \end{subfigure}
  \caption{Performance \textcolor{teal}{gains} or \textcolor{red}{losses} (\%) when applying different prompting methods on VLM$^2$-Bench.}
  \label{fig:analysis}
\end{figure}
\paragraph{Finding IV: Reasoning in language aids models in logically linking visual cues.} From Figure~\ref{fig:gc-analysis}, it is evident that both CoT-normal and CoT-special, which reasoning in language, positively impact model performance in most cases. As demonstrated in Figure~\ref{fig:cot-special increase}, CoT-special improves performance by first having the model explicitly write out the cues present in each image, followed by using language to make inferences. This process helps reduce the model's error rate by structuring the task and providing clearer logical guidance. This suggests that when models are linking general visual cues, using language to help structure the logical flow of the process can be beneficial.

\paragraph{Finding V: Effectiveness of visual prompting depends on models' ability to interpret both prompting cues and the visual content.} 
As shown in Figure~\ref{fig:gc-analysis}, VP-grid negatively impacts GC performance for QwenVL2.5, causing a significant drop compared to the vanilla approach. Figure~\ref{fig:vp-grid decrease} reveals that this decline stems from the model's difficulty in interpreting the visual coordinates within the prompt, leading to misinterpretation of the cues and causing it to fail cases it originally answered correctly under the vanilla setting. However, as shown in Figure~\ref{fig:vp-grid increase}, GPT-4o successfully resolves a previously incorrect case by effectively leveraging the cues introduced through visual prompting while utilizing its strong visual perception abilities.
% As seen for QwenVL2.5-7B in Figure~\ref{fig:gc-analysis}, regardless of the subtask in GC, the performance drops significantly after using VP-grid compared to the vanilla approach. From Figure~\ref{fig:vp-grid decrease} we can observe that this happens because the model struggles to comprehend the visual coordinates we provided. In such cases, asking the model to infer the meaning of these coordinates leads to confusion and misguides the model. However, as shown in Figure~\ref{fig:vp-grid increase}, GPT-4o successfully answer a previously incorrect case by properly understanding the VP-grid.


\subsection{Probing for Object-centric Cue (OC)}
\paragraph{Methods.} (i) \textbf{CoT} (Table~\ref{cot prompt}). (ii) \textbf{VP-zoom-o} (Figure~\ref{fig:VP-zoom-o}) uses an open-set detector~\citep{grounded-sam} to obtain bounding boxes, which are then cropped to focus the model’s attention on object-centric cues. By eliminating irrelevant non-object cues and emphasizing the object-centric cues, it enhances the model's ability to better focus on the most relevant visual information.



\paragraph{Finding VI: The open-ended nature of language may hinder object grouping.}
Unlike GC that link instance-level cues, OC requires grouping similar objects based on fine-grained visual details. As shown in Figure~\ref{fig:oc-analysis}, InternVL2.5 using CoT struggles with this task because the open-ended nature of language leads to both limited coverage of subtle visual cues (see Figure~\ref{fig:oc_cot_normal_decrease}) and inconsistent representations of the same cues, introducing ambiguity, making it harder for models to reliably align and group matching objects.

\paragraph{Finding VII: Amplifying object cues benefits stronger models while having minimal impact on others.} From Figure~\ref{fig:oc-analysis}, we observe that for models with strong vision capabilities like GPT-4o, our VP-zoom-o method further enhances performance. For other models, this method at least ensures that the performance remains on par with the vanilla approach, without causing any degradation.


\subsection{Probing for Person-centric Cue (PC)} 
\paragraph{Methods.} (i) \textbf{CoT} (Table~\ref{cot prompt}). (ii) \textbf{VP-zoom-p} (Figure~\ref{fig:vp-zoom-p}) utilizes a face detector~\citep{facedetector} to obtain bounding boxes of faces-the most distinguishing feature of different individuals. It then crops the image to focus only on the face, thereby minimizing the interference from distractor cues such as clothing and other background elements.

\paragraph{Finding VIII: CoT and visual prompting fail to improve linking on highly abstract person-centric cues, leading to a performance drop.} From Figure~\ref{fig:pc-analysis}, we observe that for almost all models, neither CoT (language-based) nor VP-zoom-p (vision-based) lead to improved performance. This is because facial features are highly abstract, and CoT methods struggle to effectively describe them in words. Additionally, VP-zoom-p fails because current models' visual capabilities are insufficient to accurately perceive facial features.

% \paragraph{Finding IX: Models in comparison tasks prioritize their knowledge over direct matching.} In Figure~\ref{fig:pc-analysis}, we observe that CoT leads to a significant drop in GPT-4o's performance in \textit{Cpr}. We find that when the model compares whether two images depict the same person, its reasoning chain tends to focus on identifying who each person is first, leading to poor performance for individuals it has not encountered before. However, for \textit{Grp}, the model tends to describe the appearance features of each person. For models with strong visual capabilities like GPT-4o, this approach naturally leads to relatively higher performance improvements.

\input{sections/5_related_work}
\input{sections/take_away}
\input{sections/6_conclusion}
\input{sections/7_limitations}


%\section*{Acknowledgments}



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix
% \section{Possible Data Curation Design - Demo and scripts}
% Question format: Currently, all questions are about "counting problems" (direct) and will ask a model to "describe" the scene as well as hints on object number (indirect).


% Counting object categories:
% \begin{itemize}
%     \item Human action: The question will only focus on human attributes
%     \item Object Movement: Questions on objects
%     % \item Human-object Interaction: Questions regarding object and human
% \end{itemize}





% \subsection{Setting 1: Human Action}
% \textbf{Scripts}: 

% \begin{enumerate}[itemsep=0.1em]
%     \item (Fixed camera) A human face appears in the camera, disappears for a few seconds, comes back in \textbf{different clothing}, and \textbf{still faces the camera}.
%     \item (Fixed camera) A human face appears in the camera, disappears for a few seconds, and comes back in different clothing, but \textbf{back towards} the camera.
%     \item (Fixed camera) A human face appears in the camera, disappears for a few seconds and comes back in different clothing with \textbf{extra facial decoration} (probably a half mask), still facing the camera.
% \end{enumerate}

% \textbf{Visual Cues}: human facial features; hairstyle; makeup; clothing; decorations



% \subsection{Setting 2: Object Movement}

% \textbf{Scripts}: 
% \begin{enumerate}[itemsep=0.1em]
%     \item Several cups are placed on the table. The camera moves away and then returns; the number of cups remains \textbf{unchanged}.
%     \item Several cups are placed on the table. \textbf{A new cup is added off-camera}, and when the camera pans back, the number of cups in view changes.
%     \item Several cups are placed on the table. Off-camera, one cup is \textbf{removed and relocated} to the bottom of the previous frame. When the camera returns, the number of cups in view remains \textbf{unchanged}.
%     \item Several cups are placed on the table. Off-camera, one cup is \textbf{removed and replaced with a different one}. When the camera pans back, the number of cups in view remains \textbf{unchanged}.
%     \item Several cups are placed on the table. While off-camera, the \textbf{liquid content or color} inside the cups changes. When the camera returns, the number of cups in view remains \textbf{unchanged}.
% \end{enumerate}

% \textbf{Visual Cues for cup objects}: shape; color; material texture; liquid color and amount;




% \subsection{Setting 3: Human-Object Interaction}

%\clearpage
%\newpage
\onecolumn
\section{Appendix Outline}
In the appendix, we provide:
\begin{itemize} \item \textbf{Appendix \ref{appendix: licence}} provides details on the licensing terms and usage rights for our benchmark.

\item \textbf{Appendix \ref{appendix: statistics}} presents the statistical analysis of the VLM$^2$-Bench.

\item \textbf{Appendix \ref{appendix: baselines}} details on how we obtain the chance-level and human-level baselines.

\item \textbf{Appendix \ref{appendix: more details on benchmark construction}}  elaborates more details on the construction of the VLM$^2$-Bench.

\item \textbf{Appendix \ref{appendix: prompting approaches}} provides a deeper dive into the various prompting techniques we use.

\item \textbf{Appendix \ref{Appendix: case study}} a detailed breakdown and analysis of failure and success examples regarding different prompting methods.
\end{itemize}



\section{Licencing and Intended Use}
\label{appendix: licence}
Our VLM$^2$-Bench is available under the CC-BY 4.0 license for academic use with proper attribution. The images, videos, and annotations in this benchmark are intended solely for research purposes. These data were sourced from publicly available online platforms, and while efforts were made to use them responsibly, explicit permissions may not have been obtained for all content. Users are responsible for ensuring that their use of the data complies with applicable intellectual property laws and ethical guidelines. We encourage users to verify the sources and ensure compliance with any terms of service or licensing agreements.


\section{VLM$^2$-Bench Statistics}
\label{appendix: statistics}
% 1 paragraph (maybe some figures)
% overall statistics for all tasks.

% OC: how many images in total; sequences, questions...

% PC: how many images in total; sequence, questions...

Here we provide additional details regarding the construction and statistics of our \textbf{VLM$^2$-Bench} benchmark. As described in the main paper (\S~\ref{bench_statistics_main}), our benchmark comprises three main categories---\textit{General Cue (GC)}, \textit{Object-centric Cue (OC)}, and \textit{Person-centric Cue (PC)}---with a total of 3,060 visual-text query pairs. Below, we elaborate on the specific data composition, including the distribution of question types (T/F, multiple-choice (MC), numerical (Nu), and open-ended (Oe)) and the rationale behind each subtask.

\subsection{Overall Composition}
\begin{wrapfigure}{r}{0.5\textwidth} % 表格靠右，宽度为文本宽度的50%
\centering
\footnotesize % 缩小表格字体
\setlength{\tabcolsep}{4pt} % 调整列间距
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Category} & \textbf{T/F} & \textbf{MC} & \textbf{Nu} & \textbf{Oe} & \textbf{Total} \\
\midrule
GC & 960 & -- & -- & -- & 960 \\
OC & 720 & 200 & 360 & -- & 1,280 \\
PC & 400 & 100 & 120 & 200 & 820 \\
\midrule
Total & 2,080 & 300 & 480 & 200 & 3,060 \\
\bottomrule
\end{tabular}
\caption{Overview of query distribution across the three categories of VLM$^2$-Bench. T/F = True/False, MC = multiple-choice, Nu = numerical, Oe = open-ended.}
\label{tab:overall_stats}
\end{wrapfigure}
Table~\ref{tab:overall_stats} provides a detailed summary of the total query counts across different categories and subtasks in our benchmark. The dataset is structured into three primary categories: General Cue (GC), Object-centric Cue (OC), and Person-centric Cue (PC), comprising a total of 3,060 visual-text query pairs.

The General Cue (GC) category consists of 960 queries, which include 260 Matching (Mat) true/false pairs, resulting in 520 queries, and 220 Tracking (Trk) true/false pairs, leading to 440 queries. 

The Object-centric Cue (OC) category contains 1,280 queries, covering three subtasks: Comparison (Cpr) with 360 true/false pairs (720 queries), Counting (Cnt) with 360 numerical queries, and Grouping (Grp) with 200 multiple-choice questions. 

Lastly, the Person-centric Cue (PC) category includes 820 queries, comprising 200 Comparison (Cpr) true/false pairs (400 queries), 120 Counting (Cnt) numerical queries, 100 Grouping (Grp) multiple-choice questions, and 200 Free-form (VID) open-ended queries.

Overall, these components collectively sum up to 3,060 visual-text query pairs, offering a comprehensive benchmark for evaluating vision-language models across various types of contextual cues.
% Table~\ref{tab:overall_stats} summarizes the total query counts within each category and subtask. The benchmark is divided as follows:
% \begin{itemize}
%     \item \textbf{General Cue (GC):} 960 queries  
    
%           - \emph{Matching (Mat)}: 260 T/F pairs $\rightarrow$ 520 queries  
          
%           - \emph{Tracking (Trk)}: 220 T/F pairs $\rightarrow$ 440 queries  
%     \item \textbf{Object-centric Cue (OC):} 1,280 queries  
    
%           - \emph{Comparison (Cpr)}: 360 T/F pairs $\rightarrow$ 720 queries  
          
%           - \emph{Counting (Cnt)}: 360 numerical (counting) queries 
          
%           - \emph{Grouping (Grp)}: 200 multiple-choice questions  
%     \item \textbf{Person-centric Cue (PC):} 820 queries  
    
%           - \emph{Comparison (Cpr)}: 200 T/F pairs $\rightarrow$ 400 queries  
          
%           - \emph{Counting (Cnt)}: 120 numerical (counting) queries 
          
%           - \emph{Grouping (Grp)}: 100 multiple-choice questions  
          
%           - \emph{Free-form (VID)}: 200 open-ended queries  
% \end{itemize}

% Summing these yields a total of 3,060 visual-text query pairs.


% \begin{table}[t]
%     \centering
%     \small
%     \setlength{\tabcolsep}{4pt} % Reduce column spacing slightly
%     \renewcommand{\arraystretch}{1.1} % Adjust row spacing slightly
%     \resizebox{\columnwidth}{!}{ % Scale the table to fit within text width
%     \begin{tabular}{lcccccc}
%     \toprule
%     \textbf{Category} & \textbf{Subtask} & \textbf{T/F} & \textbf{MC} & \textbf{Nu} & \textbf{Oe} & \textbf{Total} \\
%     \midrule
%     \multirow{2}{*}{GC} & Mat & 520 & -- & -- & -- & 520 \\
%                         & Trk & 440 & -- & -- & -- & 440 \\
%     \cmidrule(lr){2-7}
%     & \textbf{Subtotal} & 960 & -- & -- & -- & 960 \\
%     \midrule
%     \multirow{3}{*}{OC} & Cpr & 720 & -- & -- & -- & 720 \\
%                         & Cnt & -- & -- & 360 & -- & 360 \\
%                         & Grp & -- & 200 & -- & -- & 200 \\
%     \cmidrule(lr){2-7}
%     & \textbf{Subtotal} & 720 & 200 & 360 & -- & 1{,}280 \\
%     \midrule
%     \multirow{4}{*}{PC} & Cpr & 400 & -- & -- & -- & 400 \\
%                         & Cnt & -- & -- & 120 & -- & 120 \\
%                         & Grp & -- & 100 & -- & -- & 100 \\
%                         & VID & -- & -- & -- & 200 & 200 \\
%     \cmidrule(lr){2-7}
%     & \textbf{Subtotal} & 400 & 100 & 120 & 200 & 820 \\
%     \midrule
%     \textbf{Total} & & 2{,}480 & 300 & 480 & 200 & 3{,}060 \\
%     \bottomrule
%     \end{tabular}
%     } % End of resizebox
%     \caption{Overview of query distribution across the three categories of VLM$^2$-Bench. T/F = True/False, MC = multiple-choice, Nu = numerical, Oe = open-ended.}
%     \label{tab:overall_stats}
% \end{table}


\subsection{Details per Subtask and Question Type}

\paragraph{General Cue (GC).}
\mbox{}\par
\noindent
\texttt{Matching (Mat).} We collect 260 True/False (T/F) pairs focused on verifying the alignment between a visual instance and a textual description (e.g., object presence, basic attributes). Each T/F pair forms two distinct queries (one True, one False), yielding 520 queries in total. 

\noindent
\texttt{Tracking (Trk).} We design 220 T/F pairs that test an understanding of object or entity continuity across frames. For example, a question might ask whether the same object reappears in subsequent frames. Each T/F pair similarly results in two queries, totaling 440.

\paragraph{Object-centric Cue (OC).} All the visual query cases are built upon the 360 image sequences we construct. Details about image sequences can be found in Section~\ref{appendix_oc_construct}.

\noindent
\texttt{Comparison (Cpr).} This subtask examines the model's ability to compare object properties (e.g., size, color, quantity) across different frames. We produce 360 T/F pairs, each yielding two queries (720 total). Among these 360 pairs, we maintain a 1:2 ratio of True to False for ground-truth answers (i.e., 120 True vs.\ 240 False).

\noindent
\texttt{Counting (Cnt).} We provide 360 numerical questions, each asking for a count of objects in a given scene or sequence. Possible numeric answers are typically small integers (e.g., 1, 2, 3), reflecting the number of relevant objects.

\noindent
\texttt{Grouping (Grp).} We generate 200 multiple-choice (MC) questions that ask about grouping objects according to certain criteria (e.g., AAB, ABC, AAAB, AABC, ABCD). Each question presents multiple group-configuration options plus a \textit{``None''} option, which can serve as either a correct or distractor choice. For image sequences of length 4, the options include various plausible groupings (two-of-a-kind, three-of-a-kind, etc.) along with at least one additional distractor grouping that also involves three-of-a-kind to ensure sufficient challenge.

\paragraph{Person-centric Cue (PC).} Similar to OC, the construction of 260 image sequences as well as 200 video clips for PC is detailed in Section~\ref{appendix_pc_construct}.

\par

\noindent
\texttt{Comparison (Cpr).} We create 200 T/F pairs (400 queries total) focusing on comparing attributes or actions related to one or more human individuals across multiple images in a sequence. The ground truth is balanced at 100 True vs.\ 100 False.

\noindent
\texttt{Counting (Cnt).} This subtask involves 120 numerical questions asking for the number of people present or the frequency of certain actions in a sequence. Typical numeric answers range from 1 to 4, given the scope of each visual sequence.

\noindent
\texttt{Grouping (Grp).} We provide 100 MC questions based on sequences containing at least three images, with at least two images featuring the same main ``meta-human.'' The goal is to identify correct groupings of persons based on appearance, role, or action. As with \textit{OC-Grp}, each question includes a ``None'' option as either the correct or a distractor choice.

\noindent
\texttt{Open-ended (VID).} We introduce 200 open-ended queries that focus on various person-centric aspects, such as identifying roles or describing activities. These questions allow more flexibility in model responses and assess the ability to generate context-relevant answers.

\subsection{Annotation Quality and Agreement}
As noted in the main text, three annotators reviewed all 3,060 question-answer pairs. An inter-annotator agreement study showed a high consensus rate of 98.74\%, ensuring that the data is both accurate and consistent.

\subsection{Summary}
Our construction methodology ensures a balanced coverage of both object-centric and person-centric reasoning, as well as basic general cues such as element matching and tracking. The inclusion of multiple question types (T/F, MC, numerical, and open-ended) further promotes comprehensive evaluation of vision-language models. Figure~\ref{fig:bench statistics main} in the main paper illustrates the distribution of these subtasks and their question-format breakdown. We believe that the richness and diversity of VLM$^2$-Bench make it a robust platform for advancing multimodal research.



%\newpage
\section{Baselines}
\label{appendix: baselines}
\subsection{Chance-level}
In this part, we explain the calculation of chance-level accuracy for all subtasks in Table~\ref{exp:main_exp}.

\paragraph{GC-Mat, GC-Trk.} % 1/2*1/2
The Matching (Mat) and Tracking (Trk) tasks in General Cue (GC) follow a \textbf{True-False (TF) paired-question format}, where each pair consists of a \textbf{positive question} and a \textbf{negative question}:

\begin{itemize}
    \item \textbf{Positive Question}: Derive from the correct \textit{element} or \textit{change}.
    The ground truth (GT) answer is True (T).
    \item \textbf{Negative Question}: Derive from the distractor \textit{element} or \textit{change}.
    The ground truth (GT) answer is False (F).
\end{itemize}

A question pair example is shown in Table~\ref{tf_pair_example}.

\begin{table}[h]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{Positive Question:}  
    
    \textit{"Is the answer \textcolor{customgreen}{`the salad'} correct for the given question: 'What object that was present in the first image is no longer visible in the second?'"}  
    \\ GT Answer: \textbf{\textcolor{customgreen}{T}}

    \vspace{3mm}
    
    \textbf{Negative Question:}  
    
    \textit{"Is the answer \textcolor{customred}{`the ciabatta roll'} correct for the given question: 'What object that was present in the first image is no longer visible in the second?'"}  
    \\ GT Answer: \textbf{\textcolor{customred}{F}}
    \end{tcolorbox}
    \caption{Example of True-False paired questions in GC-Mat, with a positive and negative question.}
    \label{tf_pair_example}
\end{table}


During the construction of these questions, we ensure that the queried content originates from either the correct answer or a distractor answer. These elements are designed to be \textbf{independent and identically distributed}. Since each question in the pair has an independent 50\% chance of being answered correctly, the expected accuracy under random guessing would be $P(\text{correct answer}) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4} = 25\%$.





\paragraph{OC-Cpr, PC-Cpr.} % 1/2

The OC-Cpr and PC-Cpr tasks utilize a \textbf{True-False (TF) paired-question format} where both questions in a pair originate from the same correct answer but are framed in two different ways:

\begin{itemize}
    \item \textbf{Positive Question}: A direct affirmative statement that correctly represents the ground truth.
    \item \textbf{Negative Question}: A negated version of the positive question, often by inserting "not" after the verb.
\end{itemize}

An example is shown in Table~\ref{tf_cpr_example}.

\begin{table}[h]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{Positive Question:}  
    
    \textit{"Given the images, the claim `The pets in these images \textcolor{customgreen}{are} the same pet.' is right."}  
    \\ GT Answer: \textbf{\textcolor{customgreen}{T}}

    \vspace{3mm}

    \textbf{Negative Question:}  
    
    \textit{"Given the images, the claim `The pets in these images \textcolor{customred}{are not} the same pet.' is right."}  
    \\ GT Answer: \textbf{\textcolor{customred}{F}}
    \end{tcolorbox}
    \caption{Example of True-False paired questions in OC-Cpr, with a positive and negative question.}
    \label{tf_cpr_example}
\end{table}



This construction aims to eliminate \textbf{language bias} by ensuring that the model does not favor one phrasing over another. For a language model that is free from bias, these two questions are \textbf{logically equivalent}—answering one correctly implies answering the other correctly as well. Consequently, under random guessing, the expectation is $P(\text{correct answer}) = \frac{1}{2} = 50\%$.


\paragraph{OC-Cnt, PC-Cnt.}
The calculation formulas for the accuracy of the chance-level accuracy are the same as in Section \ref{metrics}.





Under a pure random guessing strategy, the predicted answer \( \hat{N}_i \) is uniformly sampled from the set \(\{1,2,\ldots,L\}\), where \(L\) is the number of images (i.e., the sequence length for that instance). For a fixed sequence length \(L\), we can compute the expected normalized accuracy \(E(L)\) by averaging over all possible ground-truth and guess pairs:
\[
E(L) = 1 - \frac{1}{L^2} \sum_{N=1}^{L} \sum_{\hat{N}=1}^{L} w(L) \cdot \epsilon(N,\hat{N})^{-\alpha},
\]
where
\[
\epsilon(N,\hat{N}) = \frac{|\hat{N} - N|}{\max(N-1,\, L-N)}
\]
and the weight is defined as
\[
w(L) = \frac{L_{\max}}{L},
\]
with \( L_{\max} = 4 \) being the maximum sequence length in our dataset.

\textbf{OC-Cnt Task:} The OC-Cnt task exhibits the following distribution:
\begin{itemize}
    \item Length 2: 80 sequences (22.2\%)
    \item Length 3: 120 sequences (33.3\%)
    \item Length 4: 160 sequences (44.4\%)
\end{itemize}
Thus, the overall chance level accuracy is obtained as the weighted average:
$
Acc_{\text{OC-Cnt}} = \frac{80\,E(2) + 120\,E(3) + 160\,E(4)}{360} \approx 34.88\%.
$

\textbf{PC-Cnt Task:} For the PC-Cnt task, the sequence distribution is:
\begin{itemize}
    \item Length 2: 30 sequences (25.0\%)
    \item Length 3: 25 sequences (20.8\%)
    \item Length 4: 65 sequences (54.2\%)
\end{itemize}
Accordingly, the overall chance level accuracy is given by:
$
Acc_{\text{PC-Cnt}} = \frac{30\,E(2) + 25\,E(3) + 65\,E(4)}{120} \approx 34.87\%.
$


% In summary, under a random guessing strategy and using the above normalized error formulation, the expected (chance-level) accuracies are approximately 34.88\% for the OC-Cnt task and 34.87\% for the PC-Cnt task.





\subsection{Human-level}
To facilitate human participants in providing responses to our questions, we integrated all model-prompted questions and answer choices into a graphical user interface (GUI), as illustrated in Figure~\ref{fig: gui_human}. This interface enabled participants to select their answers conveniently, ensuring consistency in data collection. We then gathered all responses and conducted statistical analysis on the collected human evaluations.



\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{img/GUI_human.png} 
  % \vspace{-0.8cm}
  \caption{The GUI used for human-level testing.}
  \label{fig: gui_human}
\end{figure*}





\clearpage
\newpage
\section{More details on Benchmark Construction}
\label{appendix: more details on benchmark construction}

\subsection{GC (General Cue)}
\paragraph{Manual Screening and Refine.}  

Figure~\ref{fig: gui} demonstrates the Graphic User Interface (GUI) we build for manually screening image editing data.






\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{img/gui.jpg} 
  % \vspace{-0.8cm}
  \caption{The GUI used for manually screening image editing data and refining edited prompts in General Cue (GC).}
  \label{fig: gui}
\end{figure*}

\paragraph{Salient Sampling.}

The pseudocode in Figure~\ref{fig:salient score algorithm} and Table~\ref{template salient score} displays the calculation process for the salient sampling score mentioned in Section~\ref{gc}.

\begin{table*}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    Supposed you are looking at two images:

    Image 1: \textcolor{teal!70}{\textbf{<Cap\_{src}>}}

    Image 2: \textcolor{orange!70}{\textbf{<Cap\_{edit}>}}

    From Image 1 to Image 2, the change can be summarized as: \textcolor{blue!70}{\textbf{<P>}}
    \end{tcolorbox}
    \caption{Template for salient-score calculation, which contain three placeholders for each sample.}
    \label{template salient score}
\end{table*}

\begin{figure}[htbp]
\centering
\begin{minipage}{0.90\textwidth} 
    \begin{algorithm}[H]
    \caption{\footnotesize Salient Score Computation}
    \begin{lstlisting}[language=Python]
# cap_src: caption for the source image
# cap_edit: caption for the edited image
# T: template for constructing a paragraph
# P: editing prompt
input_text = concat(cap_src, cap_edit, T)
in_tokens = tokenizer.encode(input_text)
out_tokens = tokenizer.encode(P)
log_sum = 0
tokens = in_tokens

# Model Forward Pass
for i in range(1, len(out_tokens)):
    outputs = model(tokens)
    logits = outputs.logits

    # Extract log probability of next token
    probs = log_softmax(logits[0, -1, :])
    prob = probs[out_tokens[i]]
    log_sum += prob
    
    # Update Input Sequence
    tokens = concat(tokens, out_tokens[i])

# Normalize the total log probability as the salient_score
salient_score = log_sum / len(out_tokens)

# Return: salient_score
    \end{lstlisting}
\end{algorithm}
\end{minipage}
\caption{Pseudocode for salient score computation in the phrase of Salient Sampling in the construction of GC.}
\label{fig:salient score algorithm}
\end{figure}


\paragraph{Prompts for Pair-wise Answer Generation.}

Table~\ref{tab:mat_pair_generation_prompt} and \ref{tab:trk_pair_generation_prompt}
provides the complete prompts used to generate pair-wise answers for our evaluation tasks. The prompts were designed to instruct the language model to produce two distinct answers—a positive (T) answer and a negative (F) answer—for each task. The dual-answer format is intended to capture both the expected response and its direct opposite, thereby offering a more balanced insight into the model's understanding.




\begin{table*}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
\textbf{\#Task Description}

Given the change between the first image and the second image, you need to generate four choices to the question ``What new element can be observed in the second image that was not present in the first?" \textcolor{blue}{(this question varies based on the mis-matched cue types, here shows the question for `add" from the ``Add/Remove" category)}. Remember, the choices' lengths should be similar. Additionally, your response should start with "\textit{Choices:}".

\textbf{\#Pair Design}

In these two choices, you need to contain *only* the names of objects, but be specific:

1. Correct Answer (You need to infer the *only* from the \textit{Editing Information})

2. Distractor (You need to pick a random object *only* in the \textit{Description}, but differ from the correct answer object)

\textbf{\#In-context example}

\textit{Editing Information}: 

Add a katana held in the figure's left hand, angled downwards.

\textit{Description}: 

The image depicts a person dressed in traditional Japanese armor, standing in a misty, snowy landscape. The armor is detailed and appears to be made of metal, with various straps and buckles. The person is wearing a black mask that covers their entire face, adding to the mysterious and stealthy appearance. The background features stone lanterns and other traditional Japanese structures, which are partially obscured by the mist. The overall atmosphere is serene yet somewhat eerie, with the mist adding a sense of mystery and isolation. The scene suggests a historical or fantasy setting, possibly a samurai or ninja in a snowy, misty environment.

\textit{Choices}:

Correct Answer: katana held

Distractor: black mask

\textbf{\#Task}

\textit{Editing Information}: 

\textcolor{teal}{\textbf{<Edit Prompt>}}

\textit{Description}: 

\textcolor{orange}{\textbf{<Description>}}

    \end{tcolorbox}
    \caption{Prompt for generating paired answers in the Matching (Mat) subtask of General Cue (GC).}
    \label{tab:mat_pair_generation_prompt}
\end{table*}



\begin{table*}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
\textbf{Task Description}

Given the change between the first image and the second image, you need to generate four choices to the question "What key visual difference can be observed from the first image to the second image?". Remember, the choices' lengths should be similar. Additionally, your response should start with ``\textit{Choices}:" and must contain Correct Answer and Direct Reverse Answer.

\textbf{Pair Design}

In the two choices, you need to contain:

1. Correct Answer (You need to infer from the \textit{Editing Information})

2. Direct Reverse Answer (You need to infer from the \textit{Editing Information} and change it to the opposite)

\textbf{In-context example} 

\textit{Editing Information}: 

Swap the black ninja gloves with clean white gloves appropriate for serving.

\textit{Description}: 

The image depicts a person dressed in formal attire, standing in a doorway. The individual is wearing a black tuxedo with a white dress shirt and a black bow tie. They are holding a tray with several items on it. The tray contains a small glass container, a bottle, and a small white object, possibly a salt shaker or a similar item. The person is also wearing black gloves, which are typical for serving or formal dining scenarios. The background shows a wooden door with a brass hinge and a light-colored wall. The setting appears to be indoors, possibly in a house or a formal establishment.

\textit{Choices}:

Correct Answer: The black ninja gloves were replaced with clean white gloves.

Direct Reverse Answer: The clean white gloves were replaced with black ninja gloves.

\textbf{\#Task}

\textit{Editing Information}: 

\textcolor{teal}{\textbf{<Edit Prompt>}}

\textit{Description}: 

\textcolor{orange}{\textbf{<Description>}}

    \end{tcolorbox}
    \caption{Prompt for generating paired answers in the Tracking (Trk) subtask of General Cue (GC).}
    \label{tab:trk_pair_generation_prompt}
\end{table*}





% \paragraph{Question Templates.}

% Table \ref{gc_mat_tf_example} and \ref{gc_trk_tf_example} list detailed standard question templates for General Cue - Matching and Tracking tasks, including the format instruction prompt.

% \begin{table}[h]
%     \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
%     \textbf{GC-Mat Positive Question:}  
    
%     \textit{"Is the answer \textcolor{customgreen}{'correct element'} correct for the given question: 'What new element can be observed in the second image that was not present in the first?'"}  
%     \\ GT Answer: \textbf{True (T)}

%     \vspace{3mm}
    
%     \textbf{GC-Mat Negative Question:}  
    
%     \textit{"Is the answer \textcolor{customred}{`distractor element'} correct for the given question: 'What new element can be observed in the second image that was not present in the first?'"}  
%     \\ GT Answer: \textbf{False (F)}
%     \end{tcolorbox}
%     \caption{GC-Mat True-False paired-question}
%     \label{gc_mat_tf_example}
% \end{table}

% \begin{table}[h]
%     \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
%     \textbf{GC-Trk Positive Question:}  

%     \textit{"Is the answer \textcolor{customgreen}{'correct change'} correct for the given question: 'What key visual change can be observed from the first image to the second image?'"}  
%     \\ GT Answer: \textbf{True (T)}

%     \vspace{3mm}
    
%     \textbf{GC-Trk Negative Question:}  
    
%     \textit{"Is the answer \textcolor{customred}{'distractor change (reversed process)'} correct for the given question: 'What key visual change can be observed from the first image to the second image?'"}  
%     \\ GT Answer: \textbf{False (F)}
%     \end{tcolorbox}
%     \caption{GC-Trk True-False paired-question }
%     \label{gc_trk_tf_example}
% \end{table}






\subsection{OC (Object-centric Cue)}
\label{appendix_oc_construct}
% \paragraph{Data Collection.} how to collect: xxx Figure~\ref{fig:oc overview}

\paragraph{Data Collection.} 
To construct the dataset, we follow a structured approach to collect object-centric images, as illustrated in Figure~\ref{fig:oc overview}. In total, we manually collected 320 images for objects.



\paragraph{Main Meta-Object Selection.} 
We predefine 8 types of common objects, with each type containing 5 meta-objects. For each meta-object, we collect four images that represent the same object from different angles and scene conditions.

\paragraph{Distractor Meta-Object Selection.} 
To build meaningful object image sequences, we introduce visually distractive elements for each main meta-object, referred to as ``distractor meta-objects''. Specifically, for each main meta-object, we collect four additional images that belong to different but visually similar meta-objects within the same object category. These images are selected following predefined visual cue confusion principles, ensuring that they provide meaningful challenges for vision language models. We ensure that each distractor image belongs to a different distractor meta-object, fundamentally guaranteeing that the count of different meta-objects in the final constructed sequence strictly follows our design. The principle of selecting distractor meta-objects is illustrated in the outer ring of Figure~\ref{fig:oc overview}. 





\paragraph{Image Sources.} 
The images are gathered from various sources based on the nature of the objects:
\begin{itemize}
    \item \textbf{Plush Objects:} Images of plush toys are entirely sourced from the \href{https://us.jellycat.com/}{Jellycat website} and its review sections, where diverse user-uploaded images provide a wide variety of object angles and scenes.
    \item \textbf{Pet Objects:} For the pet category of meta-objects, we source images from a combination of social media accounts of popular pet influencers' pet photography. We also include images of a ragdoll cat owned by one of the authors. As a result, this approach guarantees that each pet meta-object within the dataset belongs to the same individual cat or dog, minimizing variability unrelated to visual cue confusion.
    \item \textbf{Other Objects:} Most images are collected from \href{https://www.amazon.com/}{Amazon} product listings and review sections containing user-uploaded photos. A smaller portion of the dataset is curated using Google Lens image search, where specific visual distractive cues are used to retrieve and manually select images. The detailed visual cue principles guiding this selection process can be found in Figure~\ref{fig:oc overview}.
\end{itemize}


\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{img/pc-o-all-cases.pdf} 
  % \vspace{-0.8cm}
  \caption{The overview of the structured design of the Object-centric Cue (OC) images.
 \textbf{Central Layer (Main Meta-Objects)}: The innermost circle represents the predefined \textbf{8 object categories}, which serve as the foundation for our dataset. These categories include \textit{Pet, Plush, Bag, Book, Cup, Shirt, Shoes, and Toy}. Each category consists of 4 main meta-objects.
    \textbf{Middle Layer (Example Meta-Objects within Each Category)}: Each segment surrounding the center showcases a representative \textbf{main meta-object} within its category. These meta-objects serve as core instances for data collection. For example, the \textit{Pet} category includes \textit{Cat} and \textit{Dog}, while the \textit{Bag} category includes \textit{Backpack}, \textit{Schoolbag} and \textit{Fashion Bag}.
    \textbf{Outer Layer (Distractor Meta-Objects \& Visual Cue Distraction Principles)}: The outermost ring presents 1 out of 4 \textbf{distractor meta-objects} specifically selected to create challenging image sequences. Each distractor meta-object shares one or more \textbf{distractive visual cues} with its corresponding main meta-object.
  }
  \label{fig:oc overview}
  % \vspace{-4em}
\end{figure*}







\paragraph{Images Sequence Construction.} 
The construction of image sequences in OC (a total of 360 sequences) follows the structure in Table~\ref{tab:sequence construction oc}. More specific details are listed below:

\textbf{Two-Image Sequences (\texttt{image\_seq\_len} = 2)}
\begin{enumerate}
    \item \textbf{Main Meta-Object Only (AA)}:
        Two images are randomly sampled from the same main meta-object.  
        40 sequences are constructed (one for each main meta-object).
    \item \textbf{Main Meta-Object + Distractor Meta-Object (AB)}:
        One image is randomly selected from the main meta-object, and one from the corresponding distractor meta-object.  
        40 sequences are constructed.
\end{enumerate}

\textbf{Three-Image Sequences (\texttt{image\_seq\_len} = 3)}
\begin{enumerate}
    \item \textbf{Main Meta-Object Only (AAA)}:
        Three images are randomly sampled from the same main meta-object.  
        40 sequences are constructed.
    \item \textbf{Main Meta-Object + Distractor Meta-Object (AAB)}:
        Two images are selected from the main meta-object, and one from the distractor meta-object.  
        The order of images is shuffled.  
        40 sequences are constructed.
    \item \textbf{Main Meta-Object + Distractor Meta-Objects (ABC)}:
        One image is selected from the main meta-object, while two are selected from different distractor meta-objects.  
        40 sequences are constructed.
\end{enumerate}

\textbf{Four-Image Sequences (\texttt{image\_seq\_len} = 4)}
\begin{enumerate}
    \item \textbf{Main Meta-Object Only (AAAA)}:
        All four images are sampled from the same main meta-object and shuffled.  
        40 sequences are constructed.
    \item \textbf{Main Meta-Object + Distractor Meta-Object (AAAB)}:
        Three images are sampled from the same main meta-object, while one is selected from a distractor meta-object.  
        40 sequences are constructed.
    \item \textbf{Main Meta-Object + Distractor Meta-Objects (AABC)}:
        Two images are selected from the main meta-object, while two are selected from different distractor meta-objects.  
        40 sequences are constructed.
    \item \textbf{Main Meta-Object + Distractor Meta-Objects (ABCD)}:
        One image is selected from the main meta-object, while three are selected from different distractor meta-objects.  
        40 sequences are constructed.
\end{enumerate}















\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{ccYccc}
\toprule
\textbf{Num} & \textbf{Src} & \textbf{Process of Image Sequences Construction} &  \textbf{\textit{Cpr}} & \textbf{\textit{cnt}} & \textbf{\textit{Grp}} \\ \hline
\multirow{2}{*}{2} & \multirow{2}{*}{\textbf{AA}} & 
2 images from the same object \( O_i \), randomly sampled as \( \mathcal{I}_{O_i} = \{ I_i, I_j \} \), and shuffled. 
& \multirow{2}{*}{T} & \multirow{2}{*}{2} & \multirow{2}{*}{-}  \\ \hline
\multirow{2}{*}{2} & \multirow{2}{*}{\textbf{AB}} & 
1 image \( I_{i} \) from \( \mathcal{I}_{O_i} \) and 1 image \( I_{\neg i} \) from distractor set \( \mathcal{I}_{\neg O_i}\), randomly shuffled. 
& \multirow{2}{*}{F} & \multirow{2}{*}{1} & \multirow{2}{*}{-}  \\ \hline
\multirow{2}{*}{3} & \multirow{2}{*}{\textbf{AAA}} & 
3 images from the same object \( O_i \), randomly sampled as \( \mathcal{I}_{O_i} = \{ I_i, I_j, I_k \} \), and shuffled. 
& \multirow{2}{*}{T} & \multirow{2}{*}{3} & \multirow{2}{*}{-}  \\ \hline
\multirow{3}{*}{3} & \multirow{3}{*}{\textbf{AAB}} & 
2 images from the same object \( O_i \), randomly sampled as \( \mathcal{I}_{O_i} = \{ I_i, I_j\} \) and 1 \( I_{\neg i} \) from distractor set \( \mathcal{I}_{\neg O_i}\), randomly shuffled. 
& \multirow{3}{*}{F} & \multirow{3}{*}{2} & \multirow{3}{*}{[$I_i$, $I_j$]}  \\ \hline
\multirow{3}{*}{3} & \multirow{3}{*}{\textbf{ABC}} & 
1 images from the same object \( O_i \), randomly sampled as \( \mathcal{I}_{O_i} = \{ I_i\} \) and 2 images \( \{I_{\neg i}, I_{\neg j}\} \) from distractor set \( \mathcal{I}_{\neg O_i}\), randomly shuffled. 
& \multirow{3}{*}{F} & \multirow{3}{*}{3} & \multirow{3}{*}{[]}  \\ \hline
\multirow{2}{*}{4} & \multirow{2}{*}{\textbf{AAAA}} & 
4 images from the same object \( O_i \), randomly sampled as \( \mathcal{I}_{O_i} = \{ I_i, I_j, I_k, I_p  \} \), and shuffled. 
& \multirow{2}{*}{T} & \multirow{2}{*}{4} & \multirow{2}{*}{-}  \\ \hline
\multirow{3}{*}{4} & \multirow{3}{*}{\textbf{AAAB}} & 
3 images from the same object \( O_i \), randomly sampled as \( \mathcal{I}_{O_i} = \{ I_i, I_j, I_k \} \) and 1 image \( I_{\neg i} \) from distractor set \( \mathcal{I}_{\neg O_i}\), randomly shuffled. 
& \multirow{3}{*}{F} & \multirow{3}{*}{2} & \multirow{3}{*}{[\(I_i, I_j, I_k\)]}  \\ \hline
\multirow{3}{*}{4} & \multirow{3}{*}{\textbf{AABC}} & 
2 images from the same object \( O_i \), randomly sampled as \( \mathcal{I}_{O_i} = \{ I_i, I_j\} \) and 2 images \( \{I_{\neg i}, I_{\neg j}\} \) from distractor set \( \mathcal{I}_{\neg O_i}\), randomly shuffled. 
& \multirow{3}{*}{F} & \multirow{3}{*}{3} & \multirow{3}{*}{[\(I_i, I_j\)]}  \\ \hline
\multirow{3}{*}{4} & \multirow{3}{*}{\textbf{ABCD}} & 
1 images from the same object \( O_i \), randomly sampled as \( I_i\) and 3 images \( \{I_{\neg i}, I_{\neg j}, I_{\neg k} \} \) from distractor set \( \mathcal{I}_{\neg O_i}\), randomly shuffled. 
& \multirow{3}{*}{F} & \multirow{3}{*}{3} & \multirow{3}{*}{[]}  \\ \bottomrule
\end{tabularx}
\caption{Summary of multi-images sequence construction for Object-centric Cue (OC) tasks.}
\label{tab:sequence construction oc}
\end{table*}



\paragraph{Question Templates.}

Table \ref{oc_cpr_tf_example}, \ref{oc_cnt_example} and \ref{oc_grp_example} list detailed standard question templates (with format instructions) for the Object-centric Cue task, including 3 subtasks: Comparison (cpr), Counting (Cnt), and Grouping (Grp). 

\begin{table}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{OC-Cpr Positive Question:}  
    
    \textit{Judge the following statement based on the images: `The \{obj\}s in these images are the same \{obj\}.' Provide only one correct answer: `T' (True) or `F' (False). Respond with either `T' or `F'.}  
    \\ GT Answer: \textbf{\textcolor{customgreen}{T}}

    \vspace{3mm}
    
    \textbf{OC-Cpr Negative Question:}  
    
    \textit{Judge the following statement based on the images: `The \{obj\}s in these images are \textcolor{customred}{not} the same \{obj\}.' Provide only one correct answer: `T' (True) or `F' (False). Respond with either `T' or `F'.}   
    \\ GT Answer: \textbf{\textcolor{customred}{F}}
    \end{tcolorbox}
    \caption{Question templates used for consistency-pair evaluation in the Comparison (Cpr) subtask of Object-centric Cue (OC).}
    \label{oc_cpr_tf_example}
\end{table}

\begin{table}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{OC-Cnt Question:}  

    \textit{Answer the following question according to this rule: You only need to provide *ONE* correct numerical answer. For example, if you think the answer is `1', your response should only be `1'. The Question is: How many different \{obj\}s are there in the input images?}  
    \\ GT Answer: \textbf{3} \textcolor{blue}{(Example Answer)}
    \end{tcolorbox}
    \caption{The question template used for the counting (Cnt) subtask of Object-centric Cue (OC).}
    \label{oc_cnt_example}
\end{table}

\begin{table}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{OC-Grp Question:}  

    \textit{Answer the following question based on this rule: You only need to provide *ONE* correct answer, selecting from the options listed below. For example, if you think the correct answer is `B) 1 and 2', your response should be `B) 1 and 2'. \\
    The Question is: Which images show the same \{obj\} in the input images? Choices: A) 1 and 3; B) None; C) 2 and 3; D) 1 and 2.}  
    \\ GT Answer: \textbf{A) 1 and 3} \textcolor{blue}{(Example Answer)}
    \end{tcolorbox}
    \caption{The question template used for the grouping (Grp) subtask of Object-centric Cue (OC).}
    \label{oc_grp_example}
\end{table}






\subsection{PC (Person-centric Cue)}

\label{appendix_pc_construct}

\paragraph{Data Collection.}
We collect images of \emph{meta-humans} mainly from \url{https://www.imdb.com/} and some are from the actor or actress's social media.

\paragraph{Main Meta-human Selection.} 

Our dataset is evenly distributed across different racial groups (Asian, Black, and White) and genders (Male and Female). 
For every race-gender combination, we select five main meta-humans, each contributing four images, yielding a total of 120 images. 

To ensure consistency, all selected individuals are within a similar age range, preventing significant age-related facial changes that could interfere with identity recognition. Additionally, each actor's appearance remains relatively consistent in terms of makeup and overall styling, ensuring that different images of the same meta-human retain distinct yet comparable visual cues (e.g. face shape, eye spacing, nose structure, and lip contours). By preserving these features, we avoid manipulating a single individual's visual cues that could potentially mislead VLMs. Rather, we ensure that the evaluation genuinely tests whether the model can visually link matching cues to recognize the same or different individuals without prior identity knowledge.


\paragraph{Distractor Meta-human Selection.}
To introduce challenging distractors in our sequences, we compute the CLIP embedding for every image and store these embeddings in a reference base. 
When a distractor image is needed, we perform an image-to-image similarity search within this base to identify the most visually similar image that originates from a different meta-human. 
This fine-grained matching ensures that the distractor image closely resembles the main meta-human’s image, leading to more challenging image sequences.


\paragraph{Discussion on Why Objects Require Dedicated Distractors, While Humans Do Not.}  
In object-centric tasks, objects are categorized into eight distinct types, with substantial differences among different types (e.g. pets and bags). Therefore, each main meta-object requires dedicated distractors from the same object type to ensure meaningful comparisons.  
In contrast, humans belong to a single category, meaning that any meta-human can serve as a distractor for another. Given that we compute CLIP embeddings to select visually similar distractors, the constructed image sequences already present a significant challenge without the need for type-specific distractors.  
We also ensure diversity by selecting five main meta-humans for each race-gender pair, providing a sufficiently large pool from which to choose suitable distractors. Corresponding to our hypothesis, in the final curated sequences, most distractor meta-humans chosen were of the same race or gender as the main meta-human. Additionally, as shown in Table \ref{exp:main_exp}, these curated image sequences along with our designed questions effectively challenge tested models, revealing their limited performances in visually linking matching cues on person-centric data.




\paragraph{Images Sequence Construction.}
The construction of image sequences in PC (a total of 260 sequences) follows the structure in Table~\ref{tab:sequence construction pc}. 
More specific details are listed below:

\textbf{Two-Image Sequences (\texttt{image\_seq\_len} = 2)}
\begin{enumerate} \item \textbf{Main Meta-Human Only (PP):}
Two images are randomly selected from the same main meta-human, resulting in 50 sequences.
\item \textbf{Main Meta-Human + Distractor Meta-Human (PQ):}  
One image is randomly selected from the main meta-human, and the other from a distractor meta-human. The order of the images is shuffled. This results in 50 sequences.

\end{enumerate}

\textbf{Three-Image Sequences (\texttt{image\_seq\_len} = 3)}
\begin{enumerate}
    \item \textbf{Main Meta-Human Only (PPP)}:  
    Three images are randomly sampled from the same main meta-human.  
    20 sequences are constructed.
    \item \textbf{Main Meta-Human + Distractor Meta-Human (PPQ)}:  
    Two images are selected from the main meta-human, and one from a single distractor meta-human.  
    The order of images is shuffled.  
    30 sequences are constructed.
    \item \textbf{Main Meta-Human + Distractor Meta-Humans (PQR)}:  
    One image is selected from the main meta-human, while the other two come from distinct distractor meta-humans.  
    The order is shuffled.  
    10 sequences are constructed.
\end{enumerate}

\textbf{Four-Image Sequences (\texttt{image\_seq\_len} = 4)}
\begin{enumerate}
    \item \textbf{Main Meta-Human Only (PPPP)}:  
    All four images are sampled from the same main meta-human.  
    The order is shuffled.  
    30 sequences are constructed.
    \item \textbf{Main Meta-Human + Distractor Meta-Human (PPPQ)}:  
    Three images are sampled from the main meta-human, while one is selected from a single distractor meta-human.  
    The order is shuffled.  
    20 sequences are constructed.
    \item \textbf{Main Meta-Human + Distractor Meta-Humans (PPQR)}:  
    Two images are selected from the main meta-human, while two are selected from distinct distractor meta-humans.  
    The order is shuffled.  
    20 sequences are constructed.
    \item \textbf{Main Meta-Human + Distractor Meta-Humans (PQRS)}:  
    One image is selected from the main meta-human, while three are selected from distinct distractor meta-humans.  
    The order is shuffled.  
    30 sequences are constructed.
\end{enumerate}










% \paragraph{Images Sequence Construction.} how to construct images sequence: Table~\ref{tab:sequence construction pc}



\begin{table*}[htbp]
\centering
\begin{tabularx}{\textwidth}{ccYccc}
\toprule
\textbf{Num} & \textbf{Src} & \textbf{Process of Image Sequences Construction} & \textbf{\textit{Cpr}} & \textbf{\textit{cnt}} & \textbf{\textit{Grp}} \\ \hline
\multirow{2}{*}{2} & \multirow{2}{*}{\textbf{PP}} & 2 images from the same person \( P_i \), randomly sampled as \( \mathcal{I}_{P_i} = \{ I_i, I_j \} \), and shuffled. & \multirow{2}{*}{T} & \multirow{2}{*}{2} & \multirow{2}{*}{-} \\ \hline
\multirow{2}{*}{2} & \multirow{2}{*}{\textbf{PQ}} & 1 image \( I_{i} \) from \( \mathcal{I}_{P_i} \) and 1 image \( I_{\neg i} \) from distractor set \( \mathcal{I}_{\neg P_i} \), randomly shuffled. & \multirow{2}{*}{F} & \multirow{2}{*}{1} & \multirow{2}{*}{-} \\ \hline
\multirow{2}{*}{3} & \multirow{2}{*}{\textbf{PPP}} & 3 images from the same person \( P_i \), randomly sampled as \( \mathcal{I}_{P_i} = \{ I_i, I_j, I_k \} \), and shuffled. & \multirow{2}{*}{T} & \multirow{2}{*}{3} & \multirow{2}{*}{-} \\ \hline
\multirow{3}{*}{3} & \multirow{3}{*}{\textbf{PPQ}} & 2 images from the same person \( P_i \), randomly sampled as \( \mathcal{I}_{P_i} = \{ I_i, I_j \} \) and 1 \( I_{\neg i} \) from distractor set \( \mathcal{I}_{\neg P_i} \), randomly shuffled. & \multirow{3}{*}{F} & \multirow{3}{*}{2} & \multirow{3}{*}{[$I_i$, $I_j$]} \\ \hline
\multirow{3}{*}{3} & \multirow{3}{*}{\textbf{PQR}} & 1 image from the same person \( P_i \), randomly sampled as \( \mathcal{I}_{P_i} = \{ I_i \} \) and 2 images \( \{I_{\neg i}, I_{\neg j} \} \) from distractor set \( \mathcal{I}_{\neg P_i} \), randomly shuffled. & \multirow{3}{*}{F} & \multirow{3}{*}{3} & \multirow{3}{*}{[]} \\ \hline
\multirow{2}{*}{4} & \multirow{2}{*}{\textbf{PPPP}} & 4 images from the same person \( P_i \), randomly sampled as \( \mathcal{I}_{P_i} = \{ I_i, I_j, I_k, I_p \} \), and shuffled. & \multirow{2}{*}{T} & \multirow{2}{*}{4} & \multirow{2}{*}{-} \\ \hline
\multirow{3}{*}{4} & \multirow{3}{*}{\textbf{PPPQ}} & 3 images from the same person \( P_i \), randomly sampled as \( \mathcal{I}_{P_i} = \{ I_i, I_j, I_k \} \) and 1 image \( I_{\neg i} \) from distractor set \( \mathcal{I}_{\neg P_i} \), randomly shuffled. & \multirow{3}{*}{F} & \multirow{3}{*}{2} & \multirow{3}{*}{[$I_i$, $I_j$, $I_k$]} \\ \hline
\multirow{3}{*}{4} & \multirow{3}{*}{\textbf{PQQR}} & 2 images from the same person \( P_i \), randomly sampled as \( \mathcal{I}_{P_i} = \{ I_i, I_j \} \) and 2 images \( \{I_{\neg i}, I_{\neg j} \} \) from distractor set \( \mathcal{I}_{\neg P_i} \), randomly shuffled. & \multirow{3}{*}{F} & \multirow{3}{*}{3} & \multirow{3}{*}{[$I_i$, $I_j$]} \\ \hline
\multirow{3}{*}{4} & \multirow{3}{*}{\textbf{PQRV}} & 1 image from the same person \( P_i \), randomly sampled as \( I_i \) and 3 images \( \{I_{\neg i}, I_{\neg j}, I_{\neg k} \} \) from distractor set \( \mathcal{I}_{\neg P_i} \), randomly shuffled. & \multirow{3}{*}{F} & \multirow{3}{*}{3} & \multirow{3}{*}{[]} \\ \bottomrule
\end{tabularx}
\caption{Summary of multi-images sequence construction for Person-centric Cue (PC) tasks.}
\label{tab:sequence construction pc}
\end{table*}



\paragraph{Video Construction.}
\begin{wrapfigure}{r}{0.35\textwidth} % 图片靠右，宽度为文本宽度的50%
  \centering
  \vspace{-10pt} % 调整图片与上方文本的间距
  \includegraphics[width=0.99\linewidth]{img/duration_distribution.png} % 图片宽度为 wrapfigure 宽度的95%
  \caption{Distribution of video duration in the subtask of Video Identity Description (VID) in Person-centric Cue (PC).}
  \label{fig:video distribution}
  \vspace{-15pt} % 调整图片与下方文本的间距
\end{wrapfigure}
% how to let different sampling methods  
% 1, table (summary of different sampling methods)
% 2, ways to concat video to ensure sampling

The video data for this benchmark is manually collected from Shutterstock\footnote{\url{https://www.shutterstock.com}}. We selected ten common activity categories that an individual can perform: \textbf{clean, cook, drink, exercise, listen, play, read, ride, walk, and work}. For each category, we curated \textbf{10 sets of candidate video pairs}, and each set consists of two videos.

To ensure motion consistency and length diversity, we carefully structured the final videos by concatenating clips while keeping the total duration within the \textbf{0-100}s time range. Figure~\ref{fig:video distribution} displays the sketch of concatenated video length distribution. The final compositions followed two formats:
\paragraph{$P$->$\neg P$ format}: A direct concatenation of two distinct clips (same length for each clip).


\paragraph{$P$->$\neg P$->$P$ format}: A sequence where the first clip and the third clip are sampled from the same candidate video, while the second clip is sampled from the second candidate video (same length for the three clips).




% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.70\textwidth]{img/duration_distribution.png} 
%   % \vspace{-0.8cm}
%   \caption{Distribution of video duration in the subtask of Video Identity Description (VID) in Person-centric Cue (PC).}
%   \label{fig:video distribution}
%   % \vspace{-4em}
% \end{figure}





Regardless of the different default sampling methods for our baseline models in Table \ref{different_sampling}, both $P$->$\neg P$ and $P$->$\neg P$->$P$ formats ensure that every video clip has frames included in the sampling process:

\begin{itemize}
    \item \textbf{Uniform Sampling (8/16 frame)}: Each clip contributes a proportionate number of frames based on the total video length. Since in one concatenated video, all the sampled clips are the same length, this method guarantees at least 2 frames for each clip can be sampled as model input frames.
    \item \textbf{FPS Sampling (1fps)}: Since frames are sampled at a fixed rate, the structure of $P$->$\neg P$ and $P$->$\neg P$->$P$ ensures that each clip is present long enough for multiple frames to be captured, regardless of its placement in the sequence.
\end{itemize}



\begin{table}[H]
\centering
{
\begin{tabular}{lcc}
    \toprule
    \textbf{Model Name} & \textbf{Uniform (8/16)} & \textbf{FPS (1fps)} \\
    \midrule
    LLaVA-OneVision-7B  & \cmark & \xmark \\
    LLaVA-Video-7B      & \cmark & \xmark \\
    LongVA-7B           & \cmark & \xmark \\
    mPLUG-Owl3-7B       & \cmark & \xmark \\
    Qwen2-VL-7B         & \xmark & \cmark \\
    Qwen2.5-VL-7B       & \xmark & \cmark \\
    InternVL2.5-8B      & \cmark & \xmark \\
    InternVL2.5-26B     & \cmark & \xmark \\
    \midrule
    GPT-4o              & \cmark & \xmark \\
    \bottomrule
\end{tabular}
}
    \caption{Comparison of different video sampling methods of VLMs.}
    \label{different_sampling}
\end{table}



Thus, by maintaining the integrity of each clip's temporal structure, both $P$->$\neg P$ and $P$->$\neg P$->$P$ formats effectively ensure that every clip contributes frames to the final sampled frame input for all models.



\paragraph{Question Templates.}

Table~\ref{pc_cpr_tf_example}, Table~\ref{pc_cnt_example}, Table~\ref{pc_grp_example}, and Table~\ref{pc_vid_example} present the detailed standard question templates for the Person-centric Cue task, covering the four subtasks: Comparison (PC-Cpr), Counting (PC-Cnt), Grouping (PC-Grp), and Video Identity Description (PC-VID).



\begin{table}[H]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{PC-Cpr Positive Question:}  
    
    \textit{
    Judge the following statement based on the images: `The individuals in these images are the same person.' Provide only one correct answer: `T' (True) or `F' (False). Respond with either `T' or `F'.}  
    \\ GT Answer: \textbf{\textcolor{customgreen}{T}}

    \vspace{3mm}
    
    \textbf{PC-Cpr Negative Question:}  
    
    \textit{Judge the following statement based on the images: `The individuals in these images are \textcolor{customred}{not} the same person.' Provide only one correct answer: `T' (True) or `F' (False). Respond with either `T' or `F'.}  
    \\ GT Answer: \textbf{\textcolor{customred}{F}}
    \end{tcolorbox}
    \caption{Question templates used for consistency-pair evaluation in the Comparison (Cpr) subtask of Person-centric Cue (PC).}
    \label{pc_cpr_tf_example}
\end{table}

\begin{table}[H]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{PC-Cnt Question:}  
    
    \textit{"Answer the following question according to this rule: You only need to provide *ONE* correct numerical answer. For example, if you think the answer is '1', your response should only be '1'. The Question is: How many distinct individuals are in the input images?"}  
    \\ GT Answer: \textbf{2} \textcolor{blue}{(Example Answer)}
    \end{tcolorbox}
    \caption{The question template used for the counting (Cnt) subtask of Person-centric Cue (PC).}
    \label{pc_cnt_example}
\end{table}

\begin{table}[H]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{PC-Grp Question:}  
    
    \textit{Answer the following question according to this rule: You only need to provide *ONE* correct answer, selecting from the options listed below. For example, if you think the correct answer is `B) 2 and 3', your response should only be `B) 2 and 3'. The Question is: Which images correspond to the same person in the input images? Choices: A) None; B) 2 and 3; C) 1 and 3; D) 1 and 2."}  
    \\ GT Answer: \textbf{D) 1 and 2} \textcolor{blue}{(Example Answer)}
    \end{tcolorbox}
    \caption{The question template used for the grouping (Grp) subtask of Person-centric Cue (PC).}
    \label{pc_grp_example}
\end{table}

\begin{table}[H]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{PC-VID Question:}  
    
    \textit{"Give a comprehensive description of the whole video, prioritizing details about the individuals in the video."}  
    \end{tcolorbox}
    \caption{The question template used for the Video Identity Description (VID) subtask of Person-centric Cue (PC).}
    \label{pc_vid_example}
\end{table}





\clearpage
\newpage
\section{More details on Prompting Approaches}
\label{appendix: prompting approaches}
\subsection{Prompts for LLM-as-Evaluator}

When models answer our free-form PC-VID questions, their responses are evaluated by GPT-4o using the scoring prompts detailed in Tables~\ref{score prompt ab} and \ref{score prompt aba}. Specifically, for videos following a \( \mathcal{P} \rightarrow \neg \mathcal{P} \) sequence, GPT-4o assesses whether the model explicitly distinguishes that the first individual (\( \mathcal{P} \)) and the second individual (\( \neg \mathcal{P} \)) are different. In this case, if the model successfully makes this distinction, it receives a score of 1; otherwise, it is given a score of 0.

For videos that exhibit a \( \mathcal{P} \rightarrow \neg \mathcal{P} \rightarrow \mathcal{P} \) (PQP) pattern, the evaluation is more nuanced. The evaluator model (GPT-4o) checks two aspects: (1) whether the model correctly identifies that there are two distinct individuals (i.e., \( \mathcal{P} \) and \( \neg \mathcal{P} \)), and (2) whether the model explicitly recognizes that the final appearance belongs to the same individual as the first (\( \mathcal{P} \)). A perfect identification of both aspects yields a score of 2, while correctly distinguishing the individuals without explicitly linking the final appearance to the first results in a score of 1. If the model fails to distinguish between the individuals, a score of 0 is assigned.

% \paragraph{Task Context and Accuracy Calculation}
% In the subtask of \textit{VID}, we collect videos featuring different individuals. For each video \( V_{\mathcal{P}_i} \) depicting a person \( \mathcal{P}_i \), we identify another video \( V_{\neg \mathcal{P}_i} \) showing a different individual who shares similar visual cues (e.g., actions, scene, clothing). Two types of video sequences are constructed: (i) \( \mathcal{P}_i \xrightarrow{} \neg \mathcal{P}_i \), which tests the model's ability to discern distinct individuals, and (ii) \( \mathcal{P}_i \xrightarrow{} \neg \mathcal{P}_i \xrightarrow{} \mathcal{P}_i \), which examines whether the model can both detect the switch between individuals and correctly link the final occurrence of \( \mathcal{P}_i \) back to its first appearance.

% We prompt the model with the following question: ``Give a comprehensive description of the whole video, prioritizing details about the individuals in the video.'' This free-form question is designed to probe the model’s capacity to observe scene transitions and changes in individual identity solely through visual link matching cues, without any prior knowledge of the individuals' identities.

% The final accuracy \(Acc_{oe}\) is computed by averaging the scores across all open-ended responses and rescaling them to the range \([0,1]\). In addition, we perform manual verification of GPT-4o’s scoring by randomly sampling 20 scored responses per model. This review revealed only 2 discrepancies out of 180 responses, resulting in an accuracy rate of 98.89\%.






\begin{table*}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{\#Task}
    
    You are evaluating a model's ability to accurately distinguish between two different individuals, P and Q, who appear sequentially in a video (first P, then Q). Given a description, your task is to determine if the model explicitly identifies that the first person (P) and the second person (Q) are different individuals.
    
    \textbf{\#Return Format}
    
    You only need return a number after "Score:". If you think the model correctly identifies that the two appearances belong to different individuals, return "Score: 1". If you think the model fails to explicitly state that there are two different individuals, return "Score: 0".
    
    \textbf{\#Description}
    
    \textcolor{teal}{\textbf{<Model's Description>}}
    \end{tcolorbox}
    \caption{Scoring prompt for \textit{VID} (when video belongs to category of $P$->$\neg P$).}
    \label{score prompt ab}
\end{table*}

\begin{table*}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textbf{\#Task}
    
    You are evaluating a model's ability to accurately distinguish between two different individuals, P and Q, who appear sequentially in a video following an PQP pattern (first P, then Q, then P again). Given a description, your task is to determine whether the model explicitly identifies that: (1) P and Q are different individuals, and (2) The person in the final scene is the same as the first (P).
    
    \textbf{\#Return Format}
    
    You only need return a number after "Score:". 
    
    (1) If the model correctly describes that the video follows an PQP sequence, explicitly recognizing that the first and last appearances belong to the same person (P), while the middle appearance is a different person (Q), return "Score: 2".
    
    (2) If the model correctly identifies that there are two different people in the video (P and Q) but does not explicitly mention that the last scene returns to P, return "Score: 1".

    (3) If the model fails to recognize that two different individuals appear (e.g., treats all appearances as the same person or does not distinguish between P and Q), return "Score: 0".
    
    \textbf{\#Description}
    
    \textcolor{teal}{\textbf{<Model's Description>}}
    \end{tcolorbox}
    \caption{Scoring prompt for \textit{VID} (when video belongs to category of $P$->$\neg P$->$P$).}
    \label{score prompt aba}
\end{table*}

\subsection{Prompting Approaches for Probing on VLM$^2$-Bench}
\paragraph{CoT (CoT-normal).}
The normal version of the Chain-of-Thought prompt is shown in Table~\ref{cot prompt}. We simply require the model to think 'step-by-step' to ensure self-reflection and self-correction, as well as the transparent thinking process.





\paragraph{CoT-special for GC.}
Table~\ref{cot-special} shows a special version of the Chain-of-Thought prompt. According to the task features, we carefully analyze how a human being approaches and visually links matching cues for questions in GC, then curate this prompt as an imitation of the human visual linking process.

\paragraph{VP-grid for GC.}
Figure~\ref{fig:vp-grid} displays a complete version of Visual Prompting with Grid assistance (VP-grid). Here we follow ~\citep{lei2024scaffoldingcoordinatespromotevisionlanguage} to print a set of dot matrix onto the input image, accompanied by the image order dimension concatenated with Cartesian coordinates as (\textit{image order index}, \textit{colum index)}, \textit{row index}). In the detailed textual prompt design, we also integrated references and explanations for the grids, allowing VLMs to leverage this visual assistance as spatial and visual matching references.


\paragraph{VP-zoom-o for OC.}
In Figure~\ref{fig:VP-zoom-o}, we demonstrate the visual prompting process for OC. We leverage the Grounded-SAM \citep{grounded-sam} model to detect bounding boxes for objects based on their types then crop the ``zoomed-in'' objects as the image input for further VQA pairs.

\paragraph{VP-zoom-p for PC.}
The visual prompting process in similar to that of OC (Figure~\ref{fig:vp-zoom-p}). We use a face detection model~\citep{facedetector} to ``zoom in'' on the individual's face and occlude other irrelevant information.

\begin{table*}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textcolor{blue!70}{\textbf{<Question>}}
    % \vspace{1em}
    
    Let's think `step by step' to answer this question, you need to output the thinking process of how you get the answer.
    \end{tcolorbox}
    \caption{CoT prompt for GC (here we denote as CoT-normal to distinguish it from the CoT-special in Table~\ref{cot-special} that specifically designed for GC), OC, and PC.}
    \label{cot prompt}
\end{table*}

\begin{table*}[htbp]
    \begin{tcolorbox}[colframe=black, colback=gray!10!white, coltitle=black, boxrule=0.5mm]
    \textcolor{blue!70}{\textbf{<Question>}}
    % \vspace{1em}
    
    Use the following 4 steps to answer the question:
    \vspace{0.5em}
    
    \textbf{Step 1. Understand the Question} \\
    - Identify the question's purpose.\\
    - Check for any format requirements.\\
    % \vspace{0.5em}
    
    \textbf{Step 2. Perceive (List Elements)} \\
    - List every details in each image respectively. \\
    - Note positions and attributes of elements.\\
    % \vspace{0.5em}
    
    \textbf{Step 3. Connect (Compare \& Reason)} \\
    - Compare corresponding elements in each image.\\
    - List all the unchanged elements and the changed element.\\
    % \vspace{0.5em}
    
    \textbf{Step 4. Conclude (Answer the Question)} \\
    \end{tcolorbox}
    \caption{CoT-special specifically designed for GC.}
    \label{cot-special}
\end{table*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{img/grid_prompt.png} 
% \vspace{-0.8cm}
\caption{An illustration of how VP-grid works for GC.}
\label{fig:vp-grid}
% \vspace{-4em}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{img/pc-o_prompt.png} 
% \vspace{-0.8cm}
\caption{An illustration of how VP-zoom-o works for OC.}
\label{fig:VP-zoom-o}
% \vspace{-4em}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{img/pc-p_prompt.png} 
% \vspace{-0.8cm}
\caption{An illustration of how VP-zoom-p works for PC.}
\label{fig:vp-zoom-p}
% \vspace{-4em}
\end{figure*}















\newpage
\section{Case Study}
\label{Appendix: case study}

This section focuses on how various prompting techniques influence model performance, highlighting their successes and limitations across different models.



\subsection{Case for CoT-special prompting in General Cue (GC) Task}

We observe that the CoT-special prompt boosts InternVL2.5-8B's performance by over 25\% than the standard query in both Matching and Tracking tasks for General Cue. While for the traditional CoT-normal prompting technique, this boost is only 13\%. The CoT-special prompt (Table~\ref{cot-special}) directs the model through four explicit steps: understanding the question, perceiving (listing elements), connecting (comparing and reasoning), and concluding. This structured approach mirrors the human process of visual matching and is effective even for a rather smaller model like InternVL2.5-8B, which might otherwise struggle with the ambiguity of a complex generic step-by-step instruction (which we will discuss later in the next Subsection~\ref{vp-decrease case study}).

For example, in the provided InternVL2.5-8B response Figure~\ref{fig:cot-special increase}, the model correctly executes the following: In Step 2, it identifies critical details such as "Vase with flowers on the table" and "Chandelier above" in Image 1, while noting the absence of the vase in Image 2. In Step 3, it systematically compares the two images, highlighting that while many elements remain unchanged (e.g., the chandelier, kitchen area, bowl of fruit, window), the removal of the vase is the key difference. Finally, in Step 4, the model concludes that the statement "The vase on top of the table was removed" accurately describes the visual change, thereby arriving at the correct answer.

This detailed, multi-step breakdown not only ensures that all pertinent visual cues are captured and processed but also reduces errors by structuring the logical flow of reasoning. The CoT-special prompt's explicit instructions help InternVL2.5-8B align visual information with textual descriptions more effectively, thus enhancing overall performance. Compared to the less specific CoT-normal prompt—which may leave the model with gaps in reasoning—the CoT-special prompt provides clear, task-specific guidance that is essential for complex visual reasoning tasks, as evidenced by the substantial performance improvement.




\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{img/gc_intern_pcp_boost.pdf} 
  % \vspace{-0.8cm}
  \caption{Case study illustrating how CoT-special improves performance of the subtask of Tracking (Trk) in General Cue (GC). The model, InternVL2.5-8B, demonstrates a step-by-step reasoning process: In Step 2, it identifies key details such as ``Vase with flowers on the table" and "Chandelier above" in Image 1, while noting the absence of the vase in Image 2. In Step 3, it compares the images, recognizing that while many elements remain unchanged (e.g., chandelier, kitchen area, fruit bowl, window), the vase's removal is the primary difference. In Step 4, the model concludes that the statement "The vase on top of the table was removed" accurately reflects the visual change, leading to the correct answer.}

  \label{fig:cot-special increase}
\end{figure*}





\subsection{Case for VP-grid in General Cue Task}
\label{vp-decrease case study}
The VP-grid (Visual Prompting with Grid assistance) method enhances visual matching in General Cue tasks by overlaying a dot matrix grid onto the input image. Each dot is annotated with a three-dimensional coordinate tuple, \((\textit{image order index}, \textit{column index}, \textit{row index})\), where the first dimension distinguishes the sequence of images (e.g., the first image is indexed as 1 and the second as 2). This grid is further supported by detailed textual descriptions that clarify the coordinate system, enabling Vision-Language Models (VLMs) to use these cues for spatial and visual matching.

\paragraph{A example failure case in VP-grid.}
However, this approach does not yield consistent improvements across all models. For instance, the Qwen2.5-VL-7B model demonstrates a significant performance drop—nearly 20\%—when using VP-grid. An example failure case is in Figure ~\ref{fig:vp-grid decrease}. Our analysis reveals that although the model correctly identifies visual elements (e.g., a pedestrian with a high-visibility vest at coordinates \((2,5,3)\)), it fails to properly interpret the image sequence. Specifically, the model incorrectly associates the coordinates \((2,5,3)\) with the first image, rather than the second, despite the explicit definition provided in the textual prompt. This misinterpretation leads to erroneous linking of visual matching cues and subsequent faulty reasoning. We suspect that the underlying issue is the limited semantic comprehension capability of the relatively smaller 7B model, which struggles with complex, predefined spatial instructions and visual assistance.


\paragraph{A example of success case in VP-grid.} In contrast to models that often misinterpret or neglect spatial cues provided by VP-grid—leading to errors such as mismatching image indices—GPT-4o successfully leverages these visual prompts to achieve correct visual-textual alignment. In the example at Figure~\ref{fig:vp-grid increase}, the model identifies the cat's nose at coordinates \((1,2,4)\) in the first image and at \((2,2,4)\) in the second image, enabling it to accurately capture the change in the visual attribute (from a lighter pink to a darker black).

This success stems from several key aspects of GPT-4o's processing capabilities:
\begin{enumerate}
    \item \textbf{Precise Disambiguation of Image Order:} The VP-grid explicitly encodes image order, which GPT-4o uses to differentiate between multiple images. This prevents the common error of conflating spatial information from distinct images—a problem seen in smaller models.
    \item \textbf{Robust Visual Matching in space:} With clear coordinate annotations, the model effectively locates and compares the same physical regions across images. In this case, the exact correspondence between the cat's nose in different images is recognized, which is crucial for detecting subtle visual changes.
    \item \textbf{Structured Reasoning Process:} GPT-4o adheres to a well-defined reasoning sequence in our textual guidance(perception, connection, and conclusion). By systematically linking the provided grid coordinates with the textual descriptions, it is able to deduce the key visual change accurately.
\end{enumerate}

\paragraph{Implications on Model Scale.} Our analysis suggests that the enhanced performance of GPT-4o with VP-grid can be attributed to its larger model capacity. Although the detailed architecture of GPT-4o is proprietary, its ability to process complex multi-modal prompts implies that:
\begin{itemize}
    \item \textbf{Enhanced Semantic Understanding:} Larger models are inherently better at comprehending intricate, structured prompts that combine visual and textual information. This results in a more nuanced interpretation of spatial cues.
    \item \textbf{Superior Visual-Textual Alignment:} With greater capacity, GPT-4o can integrate and correlate the detailed spatial data (visual assistance) from the VP-grid with the corresponding textual descriptions, minimizing the risk of mis-association or errors.
    \item \textbf{Effective Handling of Complexity:} The advanced reasoning capabilities of larger models enable them to navigate the additional complexity introduced by VP-grid without suffering from the side effects seen in smaller models. This ensures that the additional spatial guidance improves performance rather than causing confusion.
\end{itemize}

The success of GPT-4o in utilizing the VP-grid approach demonstrates that model scale plays a critical role in effectively integrating complex visual and textual cues. By accurately disambiguating image order and performing precise spatial matching, GPT-4o not only avoids the pitfalls encountered by smaller models but also benefits significantly from the additional visual assistance, leading to an overall performance improvement of approximately 10\%.





\begin{figure*}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{img/gc_qwen_vlp_drop.pdf} 
  % \vspace{-0.8cm}
  \caption{Case study illustrating why VP-grid leads to performance degradation in Qwen2.5-VL-7B. The model correctly identifies visual elements (e.g., a pedestrian with a high-visibility vest at coordinates \((2,5,3)\)) but fails to interpret the image sequence correctly. It mistakenly associates the coordinates with the first image instead of the second, despite the explicit definition in the textual prompt. This misinterpretation results in incorrect visual cue linking and faulty reasoning, highlighting the model's difficulty in handling structured spatial instructions and visual prompts.}
  \label{fig:vp-grid decrease}
\end{figure*}






\begin{figure*}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{img/gc_gpt4o_vlp_boost.pdf} 
  % \vspace{-0.8cm}
  \caption{Case study demonstrating why VP-grid leads to performance improvement for GPT-4o. Unlike models that often misinterpret or overlook spatial cues, GPT-4o effectively uses VP-grid to align visual and textual information. In the example shown in Figure~\ref{fig:vp-grid increase}, the model correctly identifies the cat's nose at coordinates \((1,2,4)\) in the first image and \((2,2,4)\) in the second, accurately capturing the visual change in the attribute (from a lighter pink to a darker black). This success highlights GPT-4o’s ability to handle structured spatial prompts and improve performance through visual prompting.}
  \label{fig:vp-grid increase}
\end{figure*}



\subsection{Case for CoT prompting in Object-centric Cue Task}

The task design for Object-centric cue (OC) and person-centric cue (PC) requires multiple images (more than 2) as sequence input. We observe that, unlike General Cue (GC) tasks where models are required to link instance-level cues, OC tasks demand that models group similar objects based on fine-grained visual features. As illustrated in Figure~\ref{fig:oc-analysis}, models using the CoT approach sometimes struggle to provide a comprehensive overview of vision-based cues across a sequence of images. 

A detailed case in Figure~\ref{fig:oc_cot_normal_decrease} is provided by InternVL2.5-26B's response. The ground truth and Vanilla responses correctly identify that there is no grouping for the same meta-object in the sequence, with the answer `D) None'. In the CoT response, the model states: "The second
and third images \textcolor{customgreen}{both have dinosaurs wearing sunglasses}". Although the description here is true, its ambiguity and lack of detailed coverage lead the model to incorrectly select option C) 2 and 3, rather than the correct option D) None. Because if we take a closer look at the design on the backpack in image 3, the dinosaur with sunglasses is actually holding a keyboard instead of a skateboard in image 2. This is a distractive visual matching cue we intend to capture during the distractor meta-object selection. This major difference should have prevented models from grouping image 2 and image 3 together.


According to our findings, this misgrouping occurs for two main reasons:
\begin{enumerate}
    \item \textbf{Insufficient Overview of Visual Cues:} The CoT prompt does not force the model to systematically verify all critical details across multiple images. As a result, the model overlooks nuanced differences, such as the design discrepancy on the backpack in image 3, where the dinosaur holds a keyboard rather than a skateboard.
    \item \textbf{Variability in Descriptive Language:} The open-ended language generated by the CoT approach can lead to inconsistent descriptions. In this case, the model generalized the visual cue of a "dinosaur design" without capturing the specific attribute (i.e., the object the dinosaur is holding), which is crucial for correct grouping.
\end{enumerate}

Thus, the lack of structured guidance in the CoT prompt leads to the dropping or misinterpretation of critical cues, resulting in incorrect grouping decisions for multi-image sequences in OC tasks. This analysis underscores the importance of more detailed structured intermediate reasoning strategies, such as those provided by a tailored CoT-special prompt, to ensure that all relevant visual details are captured and compared accurately.










\begin{figure*}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{img/oc_intern26_cot_drop2.pdf} 
  % \vspace{-0.8cm}
  \caption{Case study illustrating why CoT leads to performance degradation. In the example shown in Figure~\ref{fig:oc_cot_normal_decrease}, InternVL2.5-26B's response correctly identifies that no grouping occurs for the same meta-object in the sequence, with the correct answer being `D) None'. However, in the CoT response, the model incorrectly selects option C) 2 and 3. While it correctly states that ``the second and third images both have dinosaurs wearing sunglasses," the lack of detailed analysis leads to an inaccurate conclusion. A closer examination reveals a key difference between the images—the dinosaur in image 3 is holding a keyboard instead of a skateboard, which should have prevented the grouping of the two images. This highlights the importance of providing more detailed and unambiguous cues in CoT reasoning.}

  \label{fig:oc_cot_normal_decrease}
\end{figure*}




\end{document}
