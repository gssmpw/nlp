\section{Related Work}
\label{relatedwork}
Model editing can be categorized into \textbf{parameter-preserving} and \textbf{parameter-modifying} approaches, depending on whether the original model parameters are altered.

\textbf{Parameter-preserving} model editing employs techniques like prompt engineering or attaching additional parameters \cite{zhong2023mquake,li2024sweaupdatingfactualknowledge,huang2023transformer,wang2024wiserethinkingknowledgememory}. A representative method for prompt engineering is IKE \cite{zheng-etal-2023-edit}, which retrieves $n$ contexts of the edited knowledge for a query to guide the model's response without altering its internal parameters. Methods that attach additional parameters include SERAC \cite{mitchell2022memory} and GRACE \cite{hartvigsen2024aging}, which store new memories externally and internally, respectively, by introducing new parameter modules.

\textbf{Parameter-modifying} approaches achieve model editing by directly or indirectly adjusting model parameters \cite{tan2024massiveeditinglargelanguage,deng2024unke}. Direct methods, such as FT-L \cite{zhu2020modifying}, perform constrained fine-tuning on a small number of layers to integrate new knowledge. Indirect methods can be divided into \textbf{meta-learning} and \textbf{locate-then-edit} methods. \textbf{Meta-learning} methods, like MEND \cite{mitchell2022fast}, leverage a hypernetwork to transform edit-related representations and gradients into parameter updates. In contrast, \textbf{locate-then-edit} methods, such as ROME \cite{Meng2022Locating} and MEMIT \cite{meng2022massediting}, adopt a key-value memory perspective to identify and update single or multiple critical layers using least-squares optimization. Among these, locate-then-edit methods have gained popularity and inspired several variants, such as PMET \cite{li2024pmet}, which focuses on precise editing, and AlphaEdit \cite{fang2024alphaedit}, which enhances the retention of original knowledge and strengthens sequential editing capability.