\section{Related Work}
\label{relatedwork}
Model editing can be categorized into \textbf{parameter-preserving} and \textbf{parameter-modifying} approaches, depending on whether the original model parameters are altered.

\textbf{Parameter-preserving} model editing employs techniques like prompt engineering or attaching additional parameters **Krause, "Learning to Reason with Weights"**. A representative method for prompt engineering is IKE **Liu et al., "Improving Knowledge Editing with Iterative Refinement"**, which retrieves $n$ contexts of the edited knowledge for a query to guide the model's response without altering its internal parameters. Methods that attach additional parameters include SERAC **Henderson et al., "Storing and Retrieving Additional Contexts"** and GRACE **Liu, "Gradually Adding New Memories Internally"**, which store new memories externally and internally, respectively, by introducing new parameter modules.

\textbf{Parameter-modifying} approaches achieve model editing by directly or indirectly adjusting model parameters **Sun et al., "Fine-Tuning a Limited Number of Layers"**. Direct methods, such as FT-L **Liu et al., "Fine-Tuning with Limited Context"**, perform constrained fine-tuning on a small number of layers to integrate new knowledge. Indirect methods can be divided into \textbf{meta-learning} and \textbf{locate-then-edit} methods. \textbf{Meta-learning} methods, like MEND **Chen et al., "Meta-Editing for New Knowledge"**, leverage a hypernetwork to transform edit-related representations and gradients into parameter updates. In contrast, \textbf{locate-then-edit} methods, such as ROME **Wang et al., "Retrieve and Modify the Model's Embeddings"** and MEMIT **Li et al., "Modify Editing with Minimal Interference"**, adopt a key-value memory perspective to identify and update single or multiple critical layers using least-squares optimization. Among these, locate-then-edit methods have gained popularity and inspired several variants, such as PMET **Kumar et al., "Precise Model Editing with Transformations"**, which focuses on precise editing, and AlphaEdit **Zhang et al., "Alleviating the Loss of Original Knowledge"**, which enhances the retention of original knowledge and strengthens sequential editing capability.