\section{Related Work}
\label{relatedwork}
Model editing can be categorized into \textbf{parameter-preserving} and \textbf{parameter-modifying} approaches, depending on whether the original model parameters are altered.

\textbf{Parameter-preserving} model editing employs techniques like prompt engineering or attaching additional parameters ____. A representative method for prompt engineering is IKE ____, which retrieves $n$ contexts of the edited knowledge for a query to guide the model's response without altering its internal parameters. Methods that attach additional parameters include SERAC ____ and GRACE ____, which store new memories externally and internally, respectively, by introducing new parameter modules.

\textbf{Parameter-modifying} approaches achieve model editing by directly or indirectly adjusting model parameters ____. Direct methods, such as FT-L ____, perform constrained fine-tuning on a small number of layers to integrate new knowledge. Indirect methods can be divided into \textbf{meta-learning} and \textbf{locate-then-edit} methods. \textbf{Meta-learning} methods, like MEND ____, leverage a hypernetwork to transform edit-related representations and gradients into parameter updates. In contrast, \textbf{locate-then-edit} methods, such as ROME ____ and MEMIT ____, adopt a key-value memory perspective to identify and update single or multiple critical layers using least-squares optimization. Among these, locate-then-edit methods have gained popularity and inspired several variants, such as PMET ____, which focuses on precise editing, and AlphaEdit ____, which enhances the retention of original knowledge and strengthens sequential editing capability.