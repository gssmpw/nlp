@inproceedings{Ali20242,
author = {Al-Kaswan, Ali and Izadi, Maliheh and van Deursen, Arie},
title = {Traces of Memorisation in Large Language Models for Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639133},
doi = {10.1145/3597503.3639133},
abstract = {Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47\% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {78},
numpages = {12},
keywords = {large language models, privacy, memorisation, data leakage},
location = {<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>},
series = {ICSE '24}
}
@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}
@article{warstadt2019neural,
  title={Neural Network Acceptability Judgments},
  author={Warstadt, A},
  journal={arXiv preprint arXiv:1805.12471},
  year={2019}
}
@article{bentivogli2009fifth,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  journal={TAC},
  volume={7},
  number={8},
  pages={1},
  year={2009},
  publisher={Citeseer}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third international workshop on paraphrasing (IWP2005)},
  year={2005}
}
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@article{huang2023transformer,
  title={Transformer-patcher: One mistake worth one neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2301.09785},
  year={2023}
}
@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}
@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
@misc{hu2024toxicitydetectionfree,
      title={Toxicity Detection for Free}, 
      author={Zhanhao Hu and Julien Piet and Geng Zhao and Jiantao Jiao and David Wagner},
      year={2024},
      eprint={2405.18822},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.18822}, 
}
@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}
@article{10.1145/3529755,
author = {Zini, Julia El and Awad, Mariette},
title = {On the Explainability of Natural Language Processing Deep Models},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3529755},
doi = {10.1145/3529755},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {103},
numpages = {31},
keywords = {explaining decisions, transparent embedding models, neural machine translation, transformers, language models, NLP, ExAI}
}
@inproceedings{peters-etal-2018-dissecting,
    title = "Dissecting Contextual Word Embeddings: Architecture and Representation",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1179",
    doi = "10.18653/v1/D18-1179",
    pages = "1499--1509",
}
@inproceedings{jawahar:hal-02131630,
  TITLE = {{What does BERT learn about the structure of language?}},
  AUTHOR = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  URL = {https://inria.hal.science/hal-02131630},
  BOOKTITLE = {{ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics}},
  ADDRESS = {Florence, Italy},
  YEAR = {2019},
  MONTH = Jul,
  PDF = {https://inria.hal.science/hal-02131630v1/file/intbert_acl19paper-3.pdf},
  HAL_ID = {hal-02131630},
  HAL_VERSION = {v1},
}
@inproceedings{Ali2024,
author = {Al-Kaswan, Ali},
title = {Towards Safe, Secure, and Usable LLMs4Code},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639803},
doi = {10.1145/3639478.3639803},
abstract = {Large Language Models (LLMs) are gaining popularity in the field of Natural Language Processing (NLP) due to their remarkable accuracy in various NLP tasks. LLMs designed for coding are trained on massive datasets, which enables them to learn the structure and syntax of programming languages. These datasets are scraped from the web and LLMs memorise information in these datasets. LLMs for code are also growing, making them more challenging to execute and making users increasingly reliant on external infrastructure. We aim to explore the challenges faced by LLMs for code and propose techniques to measure and prevent memorisation. Additionally, we suggest methods to compress models and run them locally on consumer hardware.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {258–260},
numpages = {3},
keywords = {large language models, privacy, memorisation, data leakage, compression},
location = {<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>},
series = {ICSE-Companion '24}
}
@misc{deepseekv2,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
      author={DeepSeek-AI},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{merrick2024arcticembedscalableefficientaccurate,
      title={Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models}, 
      author={Luke Merrick and Danmei Xu and Gaurav Nuti and Daniel Campos},
      year={2024},
      eprint={2405.05374},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.05374}, 
}
@inproceedings{feng-etal-2020-codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
}
@article{Ji2023,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
keywords = {consistency in NLG, factuality in NLG, faithfulness in NLG, extrinsic hallucination, intrinsic hallucination, Hallucination}
}
@misc{wang2021gpt,
  title={GPT-J-6B: A 6 billion parameter autoregressive language model},
  author={Wang, Ben and Komatsuzaki, Aran},
  year={2021}
}
@inproceedings{akyurek-etal-2023-dune,
    title = "{DU}n{E}: Dataset for Unified Editing",
    author = {Aky{\"u}rek, Afra  and
      Pan, Eric  and
      Kuwanto, Garry  and
      Wijaya, Derry},
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.114",
    doi = "10.18653/v1/2023.emnlp-main.114",
    pages = "1847--1861",
    abstract = "Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model{'}s knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. {``}Messi plays for Inter Miami{''} confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model{'}s outputs. We are introducing DUnE, an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We argue that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark.",
}
@inproceedings{zhong-etal-2023-mquake,
    title = "{MQ}u{AKE}: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
    author = "Zhong, Zexuan  and
      Wu, Zhengxuan  and
      Manning, Christopher  and
      Potts, Christopher  and
      Chen, Danqi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.971",
    doi = "10.18653/v1/2023.emnlp-main.971",
    pages = "15686--15702",
    abstract = "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model{'}s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.",
}
@article{cohen-etal-2024-evaluating,
    title = "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
    author = "Cohen, Roi  and
      Biran, Eden  and
      Yoran, Ori  and
      Globerson, Amir  and
      Geva, Mor",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.16",
    doi = "10.1162/tacl_a_00644",
    pages = "283--298",
    abstract = "Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., {``}Jack Depp is the son of Johnny Depp{''}) introduces a {``}ripple effect{''} in the form of additional facts that the model needs to update (e.g., {``}Jack Depp is the sibling of Lily-Rose Depp{''}). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model{'}s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.1",
}
@article{zagzebski2017knowledge,
  title={What is knowledge?},
  author={Zagzebski, Linda},
  journal={The Blackwell guide to epistemology},
  pages={92--116},
  year={2017},
  publisher={Wiley Online Library}
}
@misc{liu2024codeupdatearenabenchmarkingknowledgeediting,
      title={CodeUpdateArena: Benchmarking Knowledge Editing on API Updates}, 
      author={Zeyu Leo Liu and Shrey Pandit and Xi Ye and Eunsol Choi and Greg Durrett},
      year={2024},
      eprint={2407.06249},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.06249}, 
}
@misc{li2024unveilingpitfallsknowledgeediting,
      title={Unveiling the Pitfalls of Knowledge Editing for Large Language Models}, 
      author={Zhoubo Li and Ningyu Zhang and Yunzhi Yao and Mengru Wang and Xi Chen and Huajun Chen},
      year={2024},
      eprint={2310.02129},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.02129}, 
}
@misc{li2024mikenewbenchmarkfinegrained,
      title={MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing}, 
      author={Jiaqi Li and Miaozeng Du and Chuanyi Zhang and Yongrui Chen and Nan Hu and Guilin Qi and Haiyun Jiang and Siyuan Cheng and Bozhong Tian},
      year={2024},
      eprint={2402.14835},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14835}, 
}
@inproceedings{akyurek2023dune,
  title={DUnE: Dataset for Unified Editing},
  author={Aky{\"u}rek, Afra and Pan, Eric and Kuwanto, Garry and Wijaya, Derry},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={1847--1861},
  year={2023}
}
@inproceedings{zhong2023mquake,
  title={MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions},
  author={Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D and Potts, Christopher and Chen, Danqi},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={15686--15702},
  year={2023}
}
@inproceedings{maas2013rectifier,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
  booktitle={Proc. icml},
  volume={30},
  number={1},
  pages={3},
  year={2013},
  organization={Atlanta, GA}
}
@inproceedings{hadsell2006dimensionality,
  title={Dimensionality reduction by learning an invariant mapping},
  author={Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  booktitle={2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)},
  volume={2},
  pages={1735--1742},
  year={2006},
  organization={IEEE}
}
@inproceedings{wan2018improving,
  title={Improving automatic source code summarization via deep reinforcement learning},
  author={Wan, Yao and Zhao, Zhou and Yang, Min and Xu, Guandong and Ying, Haochao and Wu, Jian and Yu, Philip S},
  booktitle={Proceedings of the 33rd ACM/IEEE international conference on automated software engineering},
  pages={397--407},
  year={2018}
}
@article{khosla2020supervised,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}
@misc{huh2024platonicrepresentationhypothesis,
      title={The Platonic Representation Hypothesis}, 
      author={Minyoung Huh and Brian Cheung and Tongzhou Wang and Phillip Isola},
      year={2024},
      eprint={2405.07987},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07987}, 
}
@inproceedings{bansal2021project,
  title={Project-level encoding for neural source code summarization of subroutines},
  author={Bansal, Aakash and Haque, Sakib and McMillan, Collin},
  booktitle={2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC)},
  pages={253--264},
  year={2021},
  organization={IEEE}
}
@misc{liu2023meaningrepresentationstrajectoriesautoregressive,
      title={Meaning Representations from Trajectories in Autoregressive Models}, 
      author={Tian Yu Liu and Matthew Trager and Alessandro Achille and Pramuditha Perera and Luca Zancato and Stefano Soatto},
      year={2023},
      eprint={2310.18348},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.18348}, 
}
@misc{wang2024wiserethinkingknowledgememory,
      title={WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models}, 
      author={Peng Wang and Zexi Li and Ningyu Zhang and Ziwen Xu and Yunzhi Yao and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen},
      year={2024},
      eprint={2405.14768},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14768}, 
}
@misc{dou2024whatswrongcodegenerated,
      title={What's Wrong with Your Code Generated by Large Language Models? An Extensive Study}, 
      author={Shihan Dou and Haoxiang Jia and Shenxi Wu and Huiyuan Zheng and Weikang Zhou and Muling Wu and Mingxu Chai and Jessica Fan and Caishuang Huang and Yunbo Tao and Yan Liu and Enyu Zhou and Ming Zhang and Yuhao Zhou and Yueming Wu and Rui Zheng and Ming Wen and Rongxiang Weng and Jingang Wang and Xunliang Cai and Tao Gui and Xipeng Qiu and Qi Zhang and Xuanjing Huang},
      year={2024},
      eprint={2407.06153},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.06153}, 
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186345},
  year={2024},
  publisher={Springer}
}
@article{zhang2023large,
  title={How do large language models capture the ever-changing world knowledge? a review of recent advances},
  author={Zhang, Zihan and Fang, Meng and Chen, Ling and Namazi-Rad, Mohammad-Reza and Wang, Jun},
  journal={arXiv preprint arXiv:2310.07343},
  year={2023}
}
@article{thirunavukarasu2023large,
  title={Large language models in medicine},
  author={Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
  journal={Nature medicine},
  volume={29},
  number={8},
  pages={1930--1940},
  year={2023},
  publisher={Nature Publishing Group US New York}
}
@article{jiang2023self,
  title={Self-planning code generation with large language model},
  author={Jiang, Xue and Dong, Yihong and Wang, Lecheng and Shang, Qiwei and Li, Ge},
  journal={arXiv preprint arXiv:2303.06689},
  year={2023}
}
@article{jiang2024survey,
  title={A Survey on Large Language Models for Code Generation},
  author={Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}
@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  pages={1--10},
  year={2022}
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{meta2024introducing,
  title={Introducing meta llama 3: The most capable openly available llm to date},
  author={Meta, AI},
  journal={Meta AI},
  year={2024}
}
@inproceedings{gu2024model,
  title={Model editing harms general abilities of large language models: Regularization to the rescue},
  author={Gu, Jia-Chen and Xu, Hao-Xiang and Ma, Jun-Yu and Lu, Pan and Ling, Zhen-Hua and Chang, Kai-Wei and Peng, Nanyun},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={16801--16819},
  year={2024}
}
@article{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{levy2017zero,
  title={Zero-shot relation extraction via reading comprehension},
  author={Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1706.04115},
  year={2017}
}
@article{li2024pmet, title={PMET: Precise Model Editing in a Transformer}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29818}, DOI={10.1609/aaai.v38i17.29818}, number={17}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Xiaopeng and Li, Shasha and Song, Shezheng and Yang, Jing and Ma, Jun and Yu, Jie}, year={2024}, month={Mar.}, pages={18564-18572} }
@misc{zhang2024comprehensive,
      title={A Comprehensive Study of Knowledge Editing for Large Language Models}, 
      author={Ningyu Zhang and Yunzhi Yao and Bozhong Tian and Peng Wang and Shumin Deng and Mengru Wang and Zekun Xi and Shengyu Mao and Jintian Zhang and Yuansheng Ni and Siyuan Cheng and Ziwen Xu and Xin Xu and Jia-Chen Gu and Yong Jiang and Pengjun Xie and Fei Huang and Lei Liang and Zhiqiang Zhang and Xiaowei Zhu and Jun Zhou and Huajun Chen},
      year={2024},
      eprint={2401.01286},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01286}, 
}
@misc{tambon2024bugslargelanguagemodels,
      title={Bugs in Large Language Models Generated Code: An Empirical Study}, 
      author={Florian Tambon and Arghavan Moradi Dakhel and Amin Nikanjam and Foutse Khomh and Michel C. Desmarais and Giuliano Antoniol},
      year={2024},
      eprint={2403.08937},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2403.08937}, 
}
@inproceedings{mitchellfast,
  title={Fast Model Editing at Scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  booktitle={International Conference on Learning Representations}
}
@misc{li2024sweaupdatingfactualknowledge,
      title={SWEA: Updating Factual Knowledge in Large Language Models via Subject Word Embedding Altering},
      author={Xiaopeng Li and Shasha Li and Shezheng Song and Huijun Liu and Bin Ji and Xi Wang and Jun Ma and Jie Yu and Xiaodong Liu and Jing Wang and Weimin Zhang},
      year={2024},
      eprint={2401.17809},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.17809},
}
@inproceedings{NEURIPS2023_3927bbdc,
 author = {Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {17643--17668},
 publisher = {Curran Associates, Inc.},
 title = {Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},
 volume = {36},
 year = {2023}
}
@article{zhu2020modifying,
  title={Modifying memories in transformer models},
  author={Zhu, Chen and Rawat, Ankit Singh and Zaheer, Manzil and Bhojanapalli, Srinadh and Li, Daliang and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2012.00363},
  year={2020}
}
@article{hartvigsen2024aging,
  title={Aging with grace: Lifelong model editing with discrete key-value adaptors},
  author={Hartvigsen, Tom and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}
@inproceedings{zheng-etal-2023-edit,
    title = "Can We Edit Factual Knowledge by In-Context Learning?",
    author = "Zheng, Ce  and
      Li, Lei  and
      Dong, Qingxiu  and
      Fan, Yuxuan  and
      Wu, Zhiyong  and
      Xu, Jingjing  and
      Chang, Baobao",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.296",
    doi = "10.18653/v1/2023.emnlp-main.296",
    pages = "4862--4876"}
@misc{wang2024easyediteasytouseknowledgeediting,
      title={EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models},
      author={Peng Wang and Ningyu Zhang and Bozhong Tian and Zekun Xi and Yunzhi Yao and Ziwen Xu and Mengru Wang and Shengyu Mao and Xiaohan Wang and Siyuan Cheng and Kangwei Liu and Yuansheng Ni and Guozhou Zheng and Huajun Chen},
      year={2024},
      eprint={2308.07269},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.07269},
}
@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}
@article{kohonen1972correlation,
  title={Correlation matrix memories},
  author={Kohonen, Teuvo},
  journal={IEEE transactions on computers},
  volume={100},
  number={4},
  pages={353--359},
  year={1972},
  publisher={IEEE}
}
@article{anderson1972simple,
  title={A simple neural network generating an interactive memory},
  author={Anderson, James A},
  journal={Mathematical biosciences},
  volume={14},
  number={3-4},
  pages={197--220},
  year={1972},
  publisher={Elsevier}
}
@inproceedings{geva-etal-2021-transformer,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.446",
    doi = "10.18653/v1/2021.emnlp-main.446",
    pages = "5484--5495",
    abstract = "Feed-forward layers constitute two-thirds of a transformer model{'}s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys{'} input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model{'}s layers via residual connections to produce the final output distribution.",
}
@inproceedings{NEURIPS2020_92650b2e,
 author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {12388--12401},
 publisher = {Curran Associates, Inc.},
 title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
 volume = {33},
 year = {2020}
}

@misc{zhu2020modifyingmemoriestransformermodels,
      title={Modifying Memories in Transformer Models}, 
      author={Chen Zhu and Ankit Singh Rawat and Manzil Zaheer and Srinadh Bhojanapalli and Daliang Li and Felix Yu and Sanjiv Kumar},
      year={2020},
      eprint={2012.00363},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.00363}, 
}
@misc{meng2022massediting,
      title={Mass-Editing Memory in a Transformer},
      author={Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},
      year={2022},
      url={http://arxiv.org/abs/2210.07229},
      journal={arXiv preprint arXiv:2210.07229},
}
@misc{gu2024neuronlevel,
      title={Neuron-level LLM Patching for Code Generation}, 
      author={Jian Gu and Aldeida Aleti and Chunyang Chen and Hongyu Zhang},
      year={2024},
      eprint={2312.05356},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2312.05356}, 
}
@inproceedings{mitchell2022fast,
    title={Fast Model Editing at Scale},
    author={Eric Mitchell and Charles Lin and Antoine Bosselut and Chelsea Finn and Christopher D Manning},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/pdf?id=0DcZxeWfOPt}
}
@inproceedings{conala,
author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
title = {Learning to mine aligned code and natural language pairs from stack overflow},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196408},
doi = {10.1145/3196398.3196408},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {476–486},
numpages = {11},
location = {Gothenburg, Sweden},
series = {MSR '18}
}
@article{husain2019codesearchnet,
title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},
author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
journal={arXiv preprint arXiv:1909.09436},
year={2019}
}
@inproceedings{yao-etal-2023-editing,
    title = "Editing Large Language Models: Problems, Methods, and Opportunities",
    author = "Yao, Yunzhi  and
      Wang, Peng  and
      Tian, Bozhong  and
      Cheng, Siyuan  and
      Li, Zhoubo  and
      Deng, Shumin  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.632",
    doi = "10.18653/v1/2023.emnlp-main.632",
    pages = "10222--10240",
}
@inproceedings{Meng2022Locating,
 author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17359--17372},
 publisher = {Curran Associates, Inc.},
 title = {Locating and Editing Factual Associations in GPT},
 volume = {35},
 year = {2022}
}
@article{fang2024alphaedit,
  title={Alphaedit: Null-space constrained knowledge editing for language models},
  author={Fang, Junfeng and Jiang, Houcheng and Wang, Kun and Ma, Yunshan and Wang, Xiang and He, Xiangnan and Chua, Tat-seng},
  journal={arXiv preprint arXiv:2410.02355},
  year={2024}
}
@inproceedings{gu-etal-2024-model,
    title = "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
    author = "Gu, Jia-Chen  and
      Xu, Hao-Xiang  and
      Ma, Jun-Yu  and
      Lu, Pan  and
      Ling, Zhen-Hua  and
      Chang, Kai-Wei  and
      Peng, Nanyun",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.934",
    pages = "16801--16819",
}
@article{mazzia2023survey,
  title={A survey on knowledge editing of neural networks},
  author={Mazzia, Vittorio and Pedrani, Alessandro and Caciolai, Andrea and Rottmann, Kay and Bernardi, Davide},
  journal={arXiv preprint arXiv:2310.19704},
  year={2023}
}
@article{ma2024perturbation,
  title={Perturbation-Restrained Sequential Model Editing},
  author={Ma, Jun-Yu and Wang, Hong and Xu, Hao-Xiang and Ling, Zhen-Hua and Gu, Jia-Chen},
  journal={arXiv preprint arXiv:2405.16821},
  year={2024}
}
@misc{wang2023knowledge,
      title={Knowledge Editing for Large Language Models: A Survey}, 
      author={Song Wang and Yaochen Zhu and Haochen Liu and Zaiyi Zheng and Chen Chen and Jundong Li},
      year={2023},
      eprint={2310.16218},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.16218}, 
}
@misc{codellama,
      title={Code Llama: Open Foundation Models for Code},
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
}
@inproceedings{Acharya23,
author = {Acharya, Arkadeep and Singh, Brijraj and Onoe, Naoyuki},
title = {LLM Based Generation of Item-Description for Recommendation System},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3610647},
doi = {10.1145/3604915.3610647},
abstract = {The description of an item plays a pivotal role in providing concise and informative summaries to captivate potential viewers and is essential for recommendation systems. Traditionally, such descriptions were obtained through manual web scraping techniques, which are time-consuming and susceptible to data inconsistencies. In recent years, Large Language Models (LLMs), such as GPT-3.5, and open source LLMs like Alpaca have emerged as powerful tools for natural language processing tasks. In this paper, we have explored how we can use LLMs to generate detailed descriptions of the items. To conduct the study, we have used the MovieLens 1M dataset comprising movie titles and the Goodreads Dataset consisting of names of books and subsequently, an open-sourced LLM, Alpaca, was prompted with few-shot prompting on this dataset to generate detailed movie descriptions considering multiple features like the names of the cast and directors for the ML dataset and the names of the author and publisher for the Goodreads dataset. The generated description was then compared with the scraped descriptions using a combination of Top Hits, MRR, and NDCG as evaluation metrics. The results demonstrated that LLM-based movie description generation exhibits significant promise, with results comparable to the ones obtained by web-scraped descriptions.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1204–1207},
numpages = {4},
keywords = {Large Language Models (LLMs), NLP, automated content generation., web scraping},
location = {Singapore, Singapore},
series = {RecSys '23}
}
@article{rosati2024long,
  title={Long-form evaluation of model editing},
  author={Rosati, Domenic and Gonzales, Robie and Chen, Jinkun and Yu, Xuemin and Erkan, Melis and Kayani, Yahya and Chavatapalli, Satya Deepika and Rudzicz, Frank and Sajjad, Hassan},
  journal={arXiv preprint arXiv:2402.09394},
  year={2024}
}
@article{wang2024editing,
  title={Editing conceptual knowledge for large language models},
  author={Wang, Xiaohan and Mao, Shengyu and Zhang, Ningyu and Deng, Shumin and Yao, Yunzhi and Shen, Yue and Liang, Lei and Gu, Jinjie and Chen, Huajun},
  journal={arXiv preprint arXiv:2403.06259},
  year={2024}
}
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@misc{stable-code-3b,
      url={[https://huggingface.co/stabilityai/stable-code-3b](https://huggingface.co/stabilityai/stable-code-3b)},
      title={Stable Code 3B},
      author={Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and Cooper, Nathan}
}
@inproceedings{NEURIPS2023_95b6e2ff,
 author = {Hartvigsen, Tom and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {47934--47959},
 publisher = {Curran Associates, Inc.},
 title = {Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors},
volume = {36},
 year = {2023}
}
@article{deng2024unke,
  title={UnKE: Unstructured Knowledge Editing in Large Language Models},
  author={Deng, Jingcheng and Wei, Zihao and Pang, Liang and Ding, Hanxing and Shen, Huawei and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2405.15349},
  year={2024}
}
@article{wang2024detoxifying,
  title={Detoxifying Large Language Models via Knowledge Editing},
  author={Wang, Mengru and Zhang, Ningyu and Xu, Ziwen and Xi, Zekun and Deng, Shumin and Yao, Yunzhi and Zhang, Qishen and Yang, Linyi and Wang, Jindong and Chen, Huajun},
  journal={arXiv preprint arXiv:2403.14472},
  year={2024}
}
@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}
@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}
@article{hochreiter1997long,
  title={Long Short-term Memory},
  author={Hochreiter, S},
  journal={Neural Computation MIT-Press},
  year={1997}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@book{hosmer2013applied,
  title={Applied logistic regression},
  author={Hosmer Jr, David W and Lemeshow, Stanley and Sturdivant, Rodney X},
  year={2013},
  publisher={John Wiley \& Sons}
}
@inproceedings{geva-etal-2023-dissecting,
    title = "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    author = "Geva, Mor  and
      Bastings, Jasmijn  and
      Filippova, Katja  and
      Globerson, Amir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.751",
    doi = "10.18653/v1/2023.emnlp-main.751",
    pages = "12216--12235",
    abstract = "Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation {``}queries{''} the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.",
}
@InProceedings{pmlr-v162-mitchell22a,
  title = 	 {Memory-Based Model Editing at Scale},
  author =       {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15817--15831},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  abstract = 	 {Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit’s intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model’s predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.}
}

@article{Chang2024,
author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
title = {A Survey on Evaluation of Large Language Models},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3641289},
doi = {10.1145/3641289},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {mar},
articleno = {39},
numpages = {45},
keywords = {Large language models, evaluation, model assessment, benchmark}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@inproceedings{NEURIPS2022_8bb0d291,
 author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22199--22213},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models are Zero-Shot Reasoners},
 volume = {35},
 year = {2022}
}
@inproceedings{zhang-etal-2023-large,
    title = "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
    author = "Zhang, Zihan  and
      Fang, Meng  and
      Chen, Ling  and
      Namazi-Rad, Mohammad-Reza  and
      Wang, Jun",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.516",
    doi = "10.18653/v1/2023.emnlp-main.516",
    pages = "8289--8311",
}

@inproceedings{lin2023cct5,
  title={Cct5: A code-change-oriented pre-trained model},
  author={Lin, Bo and Wang, Shangwen and Liu, Zhongxin and Liu, Yepang and Xia, Xin and Mao, Xiaoguang},
  booktitle={Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1509--1521},
  year={2023}
}

@inproceedings{geng2024large,
  title={Large language models are few-shot summarizers: Multi-intent comment generation via in-context learning},
  author={Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Wang, Haotian and Li, Ge and Jin, Zhi and Mao, Xiaoguang and Liao, Xiangke},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@inproceedings{du2024evaluating,
  title={Evaluating large language models in class-level code generation},
  author={Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}
@inproceedings{BLEU,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}
@misc{tan2024massiveeditinglargelanguage,
      title={Massive Editing for Large Language Models via Meta Learning},
      author={Chenmien Tan and Ge Zhang and Jie Fu},
      year={2024},
      eprint={2311.04661},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.04661},
}
@article{chen2024can,
  title={Can Editing LLMs Inject Harm?},
  author={Chen, Canyu and Huang, Baixiang and Li, Zekun and Chen, Zhaorun and Lai, Shiyang and Xu, Xiongxiao and Gu, Jia-Chen and Gu, Jindong and Yao, Huaxiu and Xiao, Chaowei and others},
  journal={arXiv preprint arXiv:2407.20224},
  year={2024}
}
@inproceedings{libadedit,
  title={BadEdit: Backdooring Large Language Models by Model Editing},
  author={Li, Yanzhou and Li, Tianlin and Chen, Kangjie and Zhang, Jian and Liu, Shangqing and Wang, Wenhan and Zhang, Tianwei and Liu, Yang},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}
@article{youssef2024detecting,
  title={Detecting Edited Knowledge in Language Models},
  author={Youssef, Paul and Zhao, Zhixue and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin},
  journal={arXiv preprint arXiv:2405.02765},
  year={2024}
}
@inproceedings{ROUGE,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{tao2012software,
  title={How do software engineers understand code changes? An exploratory study in industry},
  author={Tao, Yida and Dang, Yingnong and Xie, Tao and Zhang, Dongmei and Kim, Sunghun},
  booktitle={Proceedings of the ACM SIGSOFT 20th International symposium on the foundations of software engineering},
  pages={1--11},
  year={2012}
}

@inproceedings{ma2024compositional,
  title={Compositional API Recommendation for Library-Oriented Code Generation},
  author={Ma, Zexiong and An, Shengnan and Xie, Bing and Lin, Zeqi},
  booktitle={Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
  pages={87--98},
  year={2024}
}

@inproceedings{sun2022importance,
  title={On the importance of building high-quality training datasets for neural code search},
  author={Sun, Zhensu and Li, Li and Liu, Yan and Du, Xiaoning and Li, Li},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1609--1620},
  year={2022}
}

@article{cohen1960coefficient,
  title={A coefficient of agreement for nominal scales},
  author={Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={20},
  number={1},
  pages={37--46},
  year={1960},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@inproceedings{guo2024exploring,
  title={Exploring the potential of chatgpt in automated code refinement: An empirical study},
  author={Guo, Qi and Cao, Junming and Xie, Xiaofei and Liu, Shangqing and Li, Xiaohong and Chen, Bihuan and Peng, Xin},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@inproceedings{yang2022natural,
  title={Natural attack for pre-trained models of code},
  author={Yang, Zhou and Shi, Jieke and He, Junda and Lo, David},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1482--1493},
  year={2022}
}

@inproceedings{geng2022fine,
  title={Fine-grained code-comment semantic interaction analysis},
  author={Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Gu, Shanzhi and Peng, Fang and Ruan, Weijian and Liao, Xiangke},
  booktitle={Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
  pages={585--596},
  year={2022}
}

@inproceedings{cheng2022path,
  title={Path-sensitive code embedding via contrastive learning for software vulnerability detection},
  author={Cheng, Xiao and Zhang, Guanqin and Wang, Haoyu and Sui, Yulei},
  booktitle={Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages={519--531},
  year={2022}
}

@inproceedings{shi2023cocosoda,
  title={Cocosoda: Effective contrastive learning for code search},
  author={Shi, Ensheng and Wang, Yanlin and Gu, Wenchao and Du, Lun and Zhang, Hongyu and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={2198--2210},
  year={2023},
  organization={IEEE}
}
