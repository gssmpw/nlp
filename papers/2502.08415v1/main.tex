% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve text aesthetics in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{setspace}  %use this package to set line spacing as desired
\usepackage{times}  %set Times New Roman as the font

% \usepackage{biblatex}  %reference manager
\usepackage{hyperref}
\usepackage{rotating}  %for rotated, landscape images
\usepackage{linguex}
\usepackage{tcolorbox}
\usepackage{lmodern}

\usepackage{graphicx}

% \usepackage[table,xcdraw]{xcolor}
% \setlength\topmargin{-0.5cm}    
% \setlength\textheight{24.7cm} 

\usepackage{lscape}
%% for semantics
\usepackage{stmaryrd}
\usepackage{tabto}
\usepackage{tikz-qtree,tikz-qtree-compat}
%\usepackage{geometry}
% For linguistic examples with gloss
\usepackage{linguex}

\usepackage{ragged2e} 
\usepackage{mathptmx}  % this is for nicer looking lambdas
\usepackage{amsmath}  % this is for \operatorname
\usepackage{ccg}  % you can find the necessary style file ccg.sty in the repository of the parser
% currently available at https://raw.githubusercontent.com/stanojevic/ccgtools/main/ccg/ccg.sty
\usepackage{amsmath}
\usepackage{ccg-latex}
\usepackage{tablefootnote}
% New commands{its name}[arg #]{identification}
\newcommand{\sembrac}[1]{$\llbracket\textrm{#1}\rrbracket$}
\newcommand{\tset}[1]{$\langle\textrm{\textit{#1}}\rangle$}
\newcommand{\tss}[1]{\textsubscript{#1}}

\usepackage{multirow}

\usepackage{geometry}
\usepackage{mathtools, nccmath}
\usepackage{enumitem}

\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}

\definecolor{grey}{rgb}{0.5,0.5,0.5}

\newcommand{\bto}[1]{\textcolor{blue}{[#1 -BTO]}}


% \title{Natural Language Logic Inference System \\ for Linear Order Problems}

\title{A Semantic Parsing Algorithm to\\ 
Solve Linear Ordering Problems}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Maha Alkhairy, Vincent Homer, Brendan O'Connor \\
  University of Massachusetts Amherst, Amherst, MA, USA \\
  \texttt{malkhairy@umass.edu} 
}

\begin{document}
\maketitle
\begin{abstract}

We develop an algorithm to semantically parse linear ordering problems, which require a model to arrange entities using deductive reasoning.
Our method takes as input a number of premises and candidate statements, parsing them to a first-order logic of an ordering domain, and then utilizes constraint logic programming to infer the truth of proposed statements about the ordering.

% For the semantic parser, we enhance a segment of \citet{Heim1998SemanticsIG}'s syntax-based compositional formal semantic rules, making them more conducive to computational algorithms. This enhancement introduces abstract types and templates while maintaining Heim's foundational rules, along with a dynamic component for interpreting entities within a contextual framework.

Our semantic parser transforms \citet{Heim1998SemanticsIG}'s syntax-based compositional formal semantic rules to a computational algorithm. This transformation involves introducing abstract types and templates based on their rules, and introduces a dynamic component to interpret entities within a contextual framework.

% making them more conducive to computational algorithms. This enhancement introduces abstract types and templates while maintaining Heim's foundational rules, along with a dynamic component for interpreting entities within a contextual framework.

% Our semantic parser transforms \citet{Heim1998SemanticsIG}'s syntax-based compositional formal semantic rules, into a dynamic parser to extract entities into a discourse context, sequentially interpreting sentences.

%%%%% 
% For the semantic parser, we implement a version of \citet{Heim1998SemanticsIG}'s syntax-based compositional formal semantic rules, extending them with a dynamic component to extract entities into a discourse context, sequentially interpreting sentences.
%%%%%

% The algorithm operates at four levels: word, sentence, paragraph, and problem. At the sentence level, the algorithm combines the logical representations of words using the rules in the syntactic tree and dynamically extracts entities. The paragraph-level semantic parser passes the entities of a sentence to the succeeding sentences. 
% The problem-level semantic parser passes the entities introduced in the premises to the statements to extract their semantic denotations. 
Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to answer multiple choice questions in BIG-bench's logical\_deduction multiple choice problems, achieving perfect accuracy, compared to 67.06\% for the best-performing LLM (GPT-4) and 87.63\% for the hybrid system Logic-LM. 

These promising results demonstrate the benefit of developing a semantic parsing algorithm driven by first-order logic constructs. 

\end{abstract}


\section{Introduction}

Natural language processing methods have made impressive recent advances in various tasks. However, they continue to struggle with inferential and logical reasoning, which often requires the incorporation of computational logic.

A class of inferential and logical reasoning is the problem of linear ordering. BIG-bench \cite{srivastava2023beyond}\footnote{\url{https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/logical_deduction}} is an LLM benchmark that includes a `logical\_deduction' task that focuses on linear ordering. It asks a model to choose the correct multiple choice statement about an ordering of items, given a description of ordering conditions among them. 
An example, further explained in Section \ref{chap:scope-preprocessing}, is: 

\begin{tcolorbox}[
    colback=white!10,    % Background of the main box (light gray)
    colframe=gray!75!black, % Border color (dark gray)
    % title=\small\textbf{\textcolor{black}{Example: BIG-Bench logical\_deduction}}, % Title in black
    % colbacktitle=white,  % Title background color (white)
    sharp corners=all,   % Sharp corners for a rectangular box
    boxrule=0.1mm      % Thin border
]
 \small \textbf{Premises:} \\ 
 
In an antique car show, there are three vehicles: a sedan, a convertible, and a truck.
The truck is the newest. 
The sedan is older than the convertible. \\

\textbf{Candidate Statements:}
\begin{itemize}[leftmargin=0.5cm] \item The sedan is the second-newest.
\item The convertible is the second-newest.
\item The truck is the second-newest.
\end{itemize} \end{tcolorbox}

These problems are constructed so that there is exactly one unambiguously correct response, which can be derived through logical deduction. This task, as part of BIG-bench's evaluation suite, is widely used for model evaluation \cite{PanLogicLM23, Schaeffer2023AreEA, Suzgun2022ChallengingBT}. 

Several NLP approaches have been proposed to solve logic deduction problems, including methods based on large language models \cite{openai2023gpt4, touvron2023llama, anil2023palm2}, hybrid approaches that combine logic inference engines with LLMs \cite{PanLogicLM23}, and those based on Combinatory Categorial Grammar (CCG) syntactic-semantic parsing \cite{Steedman2019Combinatory}.  While these methods show promise, they also have limitations (detailed in Section \ref{sec:relevantlit}): 
\begin{itemize}
    \item LLMs are pattern-based and prone to logical inconsistencies.
    \item Hybrid systems  improve accuracy but add complexity and reduce flexibility.
    \item CCG parsing suffers from high computational cost and ambiguity.
\end{itemize}

% A conventional method to solving logic deduction problems is to use a symbolic approach \bto{`approach' repeated} that incorporates a logic inference module \cite{cooper1993prolog, pereira2002prolog}. \bto{`This motivates' in next sentence is unsupported. why does the existence of a conventional approach motivate anything?} This motivates us to build a symbolic system that can provide a reasonable benchmark for solving logical deduction problems. A symbolic system would require semantic parsing to process natural language, and to our knowledge, prior work in semantic parsing, including CCG literature, does not focus on analytical reasoning tasks \cite{Abzianidze2017TowardsUS, lewis2014ccg, misra2016neural, liang2013learning, krishnamurthy2012weakly, cai2013large, artzi-etal-2015-broad, Bos2004WideCoverageSR, Xu2016LSTMSC, stanojevic-etal-2021-modeling}.

% A conventional method for solving logic deduction problems is to adopt a symbolic approach that incorporates a logic inference module \cite{cooper1993prolog, pereira2002prolog}. This approach has been widely explored in traditional logic systems, which motivates our decision to build a symbolic system that serves as a benchmark for solving logical deduction problems. A key requirement for such a system is semantic parsing to process natural language inputs. To our knowledge, prior work in semantic parsing, including the CCG literature, has not focused extensively on analytical reasoning tasks \cite{Abzianidze2017TowardsUS, lewis2014ccg, misra2016neural, liang2013learning, krishnamurthy2012weakly, cai2013large, artzi-etal-2015-broad, Bos2004WideCoverageSR, Xu2016LSTMSC, stanojevic-etal-2021-modeling}.

Given the challenges of existing approaches, it is essential to explore structured alternatives that prioritize logical consistency. A conventional approach for solving logical deduction problems is to adopt symbolic systems that incorporate dedicated logic inference modules \cite{cooper1993prolog, pereira2002prolog}. These systems offer interpretability and generate traceable logical representations, making them suitable foundations for models which solve logical deduction problems. This motivates our decision to build a symbolic system that serves as a benchmark for solving logical deduction problems. 

A key requirement for such systems is semantic parsing, which transforms natural language into formal logical representations. While this area has enjoyed much research attention (e.g.\ \citealp{Abzianidze2017TowardsUS, lewis2014ccg, misra2016neural, liang2013learning, krishnamurthy2012weakly, cai2013large, artzi-etal-2015-broad, Bos2004WideCoverageSR, Xu2016LSTMSC, stanojevic-etal-2021-modeling}), most prior work in semantic parsing has focused on general language understanding, rather than the specific representational and parsing challenges  necessary to perform analytical reasoning in tasks like logical deduction.

To overcome these limitations, we build the Formal Semantic Logic Inferer (FSLI) -- a symbolic system that combines semantic parsing with a dedicated logic inference module. Unlike hybrid methods, FSLI focuses on precise semantic representation and logical consistency without relying on LLM-generated outputs, through a series of preprocessing, semantic parsing, and logical inference components  (Sections \ref{sec:fsli}--\ref{chap:logic}).

Finally, we evaluate FSLI on the BIG-bench `logical\_deduction' benchmark dataset \cite{srivastava2023beyond} to assess its semantic parsing proficiency and to ensure the soundness of its logical inferences.  Since each problem has a single valid sequence for the arrangement of entities, perfect accuracy is possible, and is in fact achieved by our system (Section \ref{chap:performance}).

\vspace{-0.05em}
\section{Current Approaches}
\label{sec:relevantlit}
We investigate methodologies relevant to solving the problem of logical deduction in Sections \ref{llm_related}, \ref{llm_related-hybrid} and \ref{subsec:semanticparsing-ll}. 

\subsection{Large Language Models}
\label{llm_related}
The general purpose large language models (LLMs) may naturally be applied to our set of problems, as it can conduct many tasks such as summarization, information extraction,  generation of code, content generation, question answering, and solving arithmetic problems. 

\citet{marcus2019rebooting} and  \citet{bender-koller-2020-climbing} argue that LLMs have no intrinsic ability for rigorous logical deduction, rigidity, and error propagation.
Since LLMs perform pattern matching based on patterns in training data and extrapolate based on those patterns, they may exhibit some rigidity. 
This rigidity might negatively affect LLMs' ability to solve reasoning tasks, where the ability to extrapolate and adapt learned patterns to novel problems is essential \cite{marcus2019rebooting, bender-koller-2020-climbing, lake2017building, zhang2022paradox}.  

% LLMs currently don't seem to have a mechanism to check if the solution is self-consistent at all stages. So a ``misunderstanding" at a stage gets accumulated further down the road, resulting in error propagation.

% We illustrate the following example using ChatGPT \footnote{\url{https://chat.openai.com/} ChatGPT3.5 and  ChatGPT4 in February 2024} to test these phenomena. 
% We asked: `abiding by the following constraints, what is the ordering? There are three desks. The red desk is first. The blue desk is last. The green desk cannot be next to the red desk.', and it gave the following response: ``Given the constraints: The red desk is first. The blue desk is last. The green desk cannot be next to the red desk. Let's denote the desks as R for red, G for green, and B for blue. Since the red desk must be first and the blue desk last, the only possible order is: R \_ \_ B. Now, since the green desk cannot be next to the red desk, the only remaining option is to place the green desk in the middle: R G \_ B. So, the ordering is: Red (R) Green (G) Blue (B)'' 
% %%% also when testing again in Feb 10 2025 it still gives an answer 

% The given example has no valid solution: if the red desk has to be first, the blue desk has to be last, and the green desk cannot be next to the red desk then there is no valid ordering. This response demonstrates the possible inability to catch the error in the constraints. 


We illustrate the following example using ChatGPT\footnote{\url{https://chat.openai.com/}, ChatGPT3.5 and ChatGPT4 in February 2024 and again on ChatGPT4o in February 2025} to test these phenomena.

\begin{tcolorbox}[
    colback=white!10,    % Background of the main box (light gray)
    colframe=gray!75!black, % Border color (dark gray)
    title=\small\textbf{\textcolor{black}{Our Question}}, % Title in black
    colbacktitle=white,  % Title background color (white)
    sharp corners=all,   % Sharp corners for a rectangular box
    boxrule=0.1mm      % Thin border
]


\small
\justifying
\texttt{Abiding by the following constraints, what is the ordering? There are three desks. The red desk is first. The blue desk is last. The green desk cannot be next to the red desk.}
\end{tcolorbox}


\begin{tcolorbox}[
    colback=white!10,    % Background of the main box (light gray)
    colframe=gray!75!black, % Border color (dark gray)
    title=\small\textbf{\textcolor{black}{ChatGPT's Response}}, % Title in black
    colbacktitle=white,  % Title background color (white)
    sharp corners=all,   % Sharp corners for a rectangular box
    boxrule=0.1mm      % Thin border
]

\small
\justifying
\texttt{Given the constraints: The red desk is first. The blue desk is last. The green desk cannot be next to the red desk. Let's denote the desks as R for red, G for green, and B for blue. Since the red desk must be first and the blue desk last, the only possible order is: R \_ \_ B. Now, since the green desk cannot be next to the red desk, the only remaining option is to place the green desk in the middle: R G \_ B. So, the ordering is: Red (R), Green (G), Blue (B).}
\end{tcolorbox}

The given example has no valid solution: if the red desk has to be first, the blue desk has to be last, and the green desk cannot be next to the red desk, then there is no valid ordering. This response demonstrates the possible inability to catch the error in the constraints.


\subsection{Hybrid LLM approach}
\label{llm_related-hybrid}
\citet{PanLogicLM23} integrate logic inference models with LLM logic representation of input problems. This approach uses LLMs to produce the domain, variables, and constraints in first-order logic. For each type of problem, the approach uses a specific solver such as Pyke, and python-constraint\footnote{\url{https://github.com/python-constraint/python-constraint}}. This approach (Logic-LM) outperforms LLMs because of offloading the logical deduction to the solver \cite{PanLogicLM23}. 

For tasks that are hard for LLMs such as logical\_deduction \cite{Suzgun2022ChallengingBT}, chain of thought prompting (CoT) has been used. In CoT, the user provides the reasoning to be used in solving a problem that is like the one asked.\footnote{\url{https://www.promptingguide.ai/techniques/cot}} Logic-LM also presents results using CoT prompting and demonstrates that this manual support increases the accuracy of LLMs. 

\subsection{Combinatory Categorial Grammar}
\label{subsec:semanticparsing-ll}
A core prerequisite to logic reasoning is semantic parsing, with combinatory categorial grammar (CCG) \cite{steedman2000syntactic} being a prominent methodology to convert a sentence into its logical representation based on lexicalized syntactic labels \cite{Bos2004WideCoverageSR, Abzianidze2017TowardsUS, MartinezGomez2016ccg2lambda}. CCG encodes both syntactic and semantic information through lexical annotations. Since CCG models both natural language syntax and semantics in a single representation, it is complex for the task at hand. 

In addition, the number of categories in CCG (425) is tenfold that of PoS, thus exploding ambiguity at the outset  \cite{hockenmaier2007ccgbank}. Combination rules that could potentially reduce ambiguity are lax \cite{Moortgat1997-MOOCTL}. The complexity of CCG development is highlighted in \citet{steedman2000syntactic}'s foundational work . 

We believe that separating the semantic representation from the syntactic not only manages the two complex layers rather than creating a single layer, but also provides a more amenable representation for logic deduction tasks. 


\section{Our Approach: Formal Semantic Logic Inferer (FSLI)}  
\label{sec:fsli}

Our goal is to develop a single integrated system that uses a first-order logic solver using logical programming. With this goal in mind and the purpose of attaining high accuracy, we develop a formal semantic parsing algorithm that is driven by computational logic to transform the premises and candidate statements into first-order logic constructs. Another benefit of our methodology is the ability to answer queries related to the premises without access to the answer choices.

We build a computational linguistic system -- Formal Semantic Logic Inferer (FSLI), which embodies a semantic parsing algorithm that tightly integrates with a logic inference engine. FSLI is illustrated in Figure \ref{fig:figure-FSLI}. 

\begin{figure}[!htb] % Use figure instead of figure* for smaller size
    \centering
    \includegraphics[width=\columnwidth]{Problem-Level-Parser.png}
    \caption{System Overview: FSLI consists of a problem-level semantic parser and a logic inference module. $C_{in}$ is the input context (initially empty). $C_{choices}$ represents the dynamically populated contexts containing entities from premises, questions, and answer choices. $R_i$ is the logical representation (denotation) of a sentence. $B_i \in \{0, 1\}$ (for $i \leq$ number of choices) indicates whether the $i$-th answer choice matches the output of the query to the logic inference model. If the question is “what is true?”, $B_i$ indicates whether the $i$-th statement is true.}
    \label{fig:figure-FSLI}
\end{figure}



We model the semantics of a sentence using compositional formal semantic rules as introduced by \citet{Heim1998SemanticsIG} built upon \citet{partee1975toward}'s contributions to \citet{montague1970} Grammar. The result of applying formal semantic rules using word denotations is a logical formula for the sentence. In compositional formal semantics, a context $\mathcal{C}$ is employed to interpret context-dependent expressions such as who the speaker is `I', the place `here',  and the time `now'. In our system, the context $\mathcal{C}$ is represented as mapping between the entity label and the entity description an is dynamically populated. Context is not to be confused with coreference that is used to handle pronouns -- which do not exist in the dataset \cite{sukthanker2020anaphora}. 

We algorithmize the compositional formal semantic rules, producing a procedure that converts a sentence into its logical representation using lambda expressions.  Furthermore, we develop an approach to semantic parsing that populates the context with the entities in a dynamic manner. This is needed for logic deduction tasks as the entities are at the core of the problems and need to be extracted as the parser parses each sentence. 

Our formal semantic parsing algorithm operates at four levels: word, sentence, paragraph, and problem. The word denotation extraction algorithm uses templates based on part of speech and named entity extraction to deduce the word's logical representation (denotation). 

The sentence-level semantic parser applies the formal semantic rules, uses the sentence's syntax tree representation to combine the word denotations to the sentence-level denotation, and dynamically extracts the entities in the process. 

The paragraph-level parser keeps track of the entities introduced in each sentence and passes them along to the following sentence in the paragraph. The problem-level semantic parser passes the entities introduced in the premises to the statements when arriving at their semantic denotation.

\section{Dataset and Text Processing}
\label{chap:scope-preprocessing}

    \citet{srivastava2023beyond}'s BIG-bench is a comprehensive benchmark comprised of various tasks designed to assess the performance and limitations of large language models. This includes a logical-deduction task\footnote{\footnotesize \url{https://github.com/google/BIG-bench}}
 which employs linear arrangement as the relationship among entities.

    The logical\_deduction benchmark is comprised of three parts: sequences with three entities, five entities, and seven entities. The gradual increase in the number of entities introduces three difficulty levels to test since more entities introduce more relationships to keep track of. 

The premises introduce the rules of entity ordering, while the candidate statements contain assertions that could be either true or false, along with their corresponding truth values (true = 1, false = 0). 

\begin{tcolorbox}[
    colback=white!10,    % Background of the main box (light gray)
    colframe=gray!75!black, % Border color (dark gray)
    title=\small\textbf{\textcolor{black}{Example: BIG-Bench logical\_deduction}}, % Title in black
    colbacktitle=white,  % Title background color (white)
    sharp corners=all,   % Sharp corners for a rectangular box
    boxrule=0.1mm      % Thin border
]

\small % Smaller font for the box content
\textbf{Premises:} \\
In an antique car show, there are three vehicles: a sedan, a convertible, and a truck. The truck is the newest. The sedan is older than the convertible. \\[0.5em]

\textbf{Candidate Statements:}
\begin{itemize}[leftmargin=0.5cm]
    \item The sedan is the second-newest.
    \item The convertible is the second-newest.
    \item The truck is the second-newest.
\end{itemize}
\end{tcolorbox}

    Based on the programmatic construction of the dataset, the rules constrain the ordering entirely, so there is only one statement that is correct. These benchmark properties help test the semantic parsing algorithm, assuming that the logic model is sound. If the parsing is correct, we assume that the system should give the correct answer.  

\vspace{-0.03em}
\subsection{Text Preprocessing}

We use the Stanford Stanza pipeline \cite{qi2020stanza} to conduct text preprocessing and constituency syntactic parsing. Text preprocessing consists of part of speech (PoS) tagging, lemmatization, morphosyntactic feature tagging, and named entity recognition. 

Lemmatization \cite{jurafskyspeech} is the process of extracting the base form of a word (lemma). Dealing with base forms of words is sufficient for our purposes as the tense and other morphosyntactic attributes are of no consequence.  

We center parsing on constituency parse for use in compositional formal semantics and syntax. The PoS tagging informs the semantic denotations of the leaves, and the daughters of a node combine to give the denotation of the node. In compositional formal semantics \cite{Heim1998SemanticsIG}, the syntax is represented by a phrase structure grammar parse (constituency parse). We use the NLTK python library \cite{bird2009natural} to binarize the constituency parse to Chomsky normal form (CNF) \cite{chomsky1959formal}, which requires each constituent to be comprised of one or two parts.

Coreference, which is generally used for pronouns, is not applicable to our case. We handle contextual information pertaining to entities by dynamically populating a context that contains the entity description and its label as described in Section \ref{subsec:sentence_level}. 

In order to map linear ordering-related phrasing in natural language text to the defined logic predicates, we create a translation table of the most common phrasing for linear ordering to the predicates. The keyword translations are related to direction, speed, expense, time, popularity, quantity, and performance. This listing is extrapolated from 10\% of the data and extended using world knowledge to encompass as many phrases related to linear ordering as possible. Table \ref{tab:translations} in Appendix \ref{appen:translations} enumerates some of the phrases.


\section{Semantic Parsing Algorithm}
\label{chap:semantics}

Compositional formal semantics \cite{Heim1998SemanticsIG} is a logical approach to semantics that labels words according to their first-order lambda logic notation. The logic representation of a sentence is determined by combining the logic representations of its constituents using the structure of the syntactic parse.

Logic representation of a word, phrase, or sentence is referred to as the denotation in compositional formal semantics. A denotation consists of two parts: first-order lambda logic notation and the semantic type. \sembrac{Y} represents a denotation of Y where Y could be a word, phrase, or sentence. 

Semantic types indicate whether the sentence, phrase, or word represents an entity, a truth value, or a functional type. An entity is represented with $e$, and a truth value is represented with $t$. $e$ and $t$ are the basic semantic types that can be combined to create functional types. A functional type \tset{$\sigma$,$\tau$} where $\sigma$ and $\tau$ can be a basic type or a functional type.  
An entity is an object such as a noun. Examples of entities include a green apple, a blue banana, and Harry.  

We generate denotations for a problem at three levels: word level, sentence level, and paragraph level. Lower-level denotations inform the denotations at the higher level. 

\subsection{Word Level Semantic Denotation}

In order to reduce the ambiguity of word denotation, we have one denotation per word/word category based on the most probable representation given the domain. For example, in our case, verbs are likely to be transitive verbs. We use predicates in our representations to allow semantic parsing output to be inputted to the logic inference system with little to no modifications. A predicate is a function that can take in one or more inputs. For example, $before(a, b)$ means a is before b. 

We extend the denotation types to include two cross-categorial types: \tset{$\mu$,$\mu$} and \tset{$f$,$\mu$}. $\mu$ is a placeholder type that could be of any semantic type, and $f$ is a placeholder type of a functional semantic type.  
They are used to allow a variety of input types rather than restricting it to a single type for word denotation or introducing ambiguity by having more than one possible denotation for a word. 

\subsubsection{Open Category Words}

Focusing on the part of speech categories: noun, adjective, verb, and propernoun; we use templates for the denotations. We use the lemma of a word as part of the logic representation.  The templates are: 
\begin{enumerate}
    \item  \textbf{noun / adjective}  : ($\lambda x$: $lemma(x)$, \tset{e,t}) \\
    which means that a noun is a function of type \tset{e,t} which takes as input an entity and returns a truth value. 
    \item  \textbf{verb} : ($\lambda x$: $\lambda y$: $lemma(y, x)$, \tset{e,\tset{e,t}}) \\ 
    which means that a verb is a function of type \tset{e,\tset{e,t}} which takes as input an entity and returns a function which takes as input an entity and returns a truth value. 
    \item \textbf{propernoun}  : ($\lambda x$: $lemma(x)$, \tset{\tset{e,t}, ec}) \\ 
    which means that a propernoun is a function of type \tset{\tset{e,t}, ec}, which takes as input a function of type \tset{e,t} and creates an entity using the input function. 
\end{enumerate}

We have introduced a special denotation type for propernouns \tset{\tset{e,t},ec}. This follows the syntactic representation that a propernoun could be part of a determiner phrase with a hidden determiner \cite{abney1987english}. This representation makes the system robust against errors in NER and allows for handling two-part propernouns like `New York'. \tset{\tset{e,t},ec} means that an entity is created and placed in the context $\mathcal{C}$ if not previously placed in it. 

\subsubsection{Key Words and Closed Category Words}

Keywords (kw) which represent sequential rules such as `before', `after' and `next' are represented as : (``$\lambda x$: $\lambda y$: kw(y,x)", \tset{e,\tset{e,t}}).   Closed category words have specific denotations, an example is  \sembrac{is} =  (``$\lambda f$: $f$",  \tset{$\mu$,$\mu$}) which is an identity function so the input is the same as the output in terms of denotation and type. 

Numeric words denote a position (ordinal). They are denoted using the template
(``$\lambda x$ : $position(x, \textbf{first}, n)$", \tset{e,t}). Where $n$ is the number which the numeric word represents such as `one' = 1, `third' = 3.

$position$ is a predicate that starts counting from 1 with special keywords \textbf{first} and \textbf{last}.  The position predicate assumes that the entities are in a unidirectional line, which is represented as a list that starts indexing at 1, and index(x) represents the index of entity x in the list.  $position(x, \textbf{first}, n)$ means $index(x)$ = $n$ with \textbf{first} referring to the start of the entity list. $position(x, \textbf{last}, n)$ means $index(x)$ = $N$ - $n$ where $N$ is the number of entities introduced in the text; with \textbf{last} referring to the end of the entity list. 

\subsection{Sentence Level Semantic Parser}
\label{subsec:sentence_level}

\begin{figure}[!hbt]
    \center
    \includegraphics[width=.60\columnwidth]{example-sentence-1.jpeg}\hfill
    \includegraphics[width=.40\columnwidth]{example-sentence-2.jpeg}
    \\[\smallskipamount]
    \caption{The binarized syntactic parses of the sentences in `A pink monkey is eating an apple. The apple is tasty.'}
    \label{fig:illustrative_example}
\end{figure}

We address semantic parsing in a rule-based manner, which models the rules introduced in Compositional Formal Semantics.  The denotations of the words are derived using the PoS and NER tags informed using the combination rules. 

A context $\mathcal{C}$  maps between the entity description and its label. The context is updated when the semantic parser encounters a new entity. In the context $\mathcal{C}$  = \{$h$ : `harry(h)', $r$: `ron(r)'\},  `Harry' has the label $h$ and `Ron' has label $r$. 

In order to identify the entities and use them dynamically across the text, we have a special denotation for the definite article (the) and the indefinite articles (a, an) -- described in Section \ref{subsub:def_indef}. This implementation allows us to populate the context ($\mathcal{C}$) as we come across a new entity in each sentence in the text.  In addition, by populating the context dynamically with the entities, we are introducing a novel approach to extracting entities that might not be captured using NER.  We are also operating under the assumption that no two distinct entities have the same description. 

Section \ref{appen:sentence_example} provides an illustrative example of how to populate the context and extract the denotation of each sentence in a paragraph using the formal semantic rules. Let $\mathcal{D}$ be the denotation dictionary, which contains the denotation of the words in the sentence: 

\begin{enumerate}
    \item Function Application (FA): If X is a branching node that has two daughters - Y and Z - and if \sembrac{Y} is a function whose domain contains \sembrac{Z}, then \sembrac{$X$} = \sembrac{$Y$}(\sembrac{$Z$})
    \item Non-branching Node (NN): If X is a non-branching node and Y is its sole daughter then \sembrac{$X$} = \sembrac{$Y$} 
    \item Terminal Node (TN): If X is a terminal node (leaf) then \sembrac{$X$} = $\mathcal{D}$[X] if X is not an entity otherwise  \sembrac{$X$} = $\mathcal{C}$[X] if $X$ is an entity
    \item Predicate Modification (PM): If X is a branching node that has two daughters - Y and Z - and if \sembrac{Y} and \sembrac{Z} are of type \tset{e, t}, then \sembrac{X} = ([$\lambda x. $ \sembrac{Y}($x$) and \sembrac{Z}($x$)], \tset{e, t})
\end{enumerate}

\subsubsection{Definite and Indefinite Article}
\label{subsub:def_indef}

The indefinite articles `a' and `an' introduce a novelty condition, and definite `the' introduces a familiarity condition \cite{abney1987english}. This motivates the introduction of special predicates $createEntity$  and $getEntity$ for the determiners `a', `an', and `the'. Consequently, entities with the same description have the same label.  Let $\mathcal{C}$ be the context:  

 \begin{itemize}[leftmargin=0cm,rightmargin=0cm]
 % \resizebox{\columnwidth}{!}{
    \item[] \sembrac{a} = \sembrac{an} = (`$\lambda f: createEntity(f, \mathcal{C})$', \tset{\tset{e,t}, e})
    % } 
    \item[]  \sembrac{the} = (`$\lambda f: getEntity(f, \mathcal{C})$', \tset{\tset{e,t}, e})
\end{itemize}


$createEntity$ is a predicate that takes in a description, creates a label for the new entity, places it in the context $\mathcal{C}$, and returns the label as the denotation of the entity. 

$getEntity$ is a predicate that takes in a description and retrieves the label of the entity from the context $\mathcal{C}$.
        

\subsubsection{Sentence Level Parsing Example}
\label{appen:sentence_example} 

Using the sentences in the following paragraph, we illustrate how to populate the context and extract the denotation of each sentence during tree traversal bottom-up right to left. The paragraph is: `A pink monkey is eating an apple. The apple is tasty.'.

The syntax tree of each sentence is traversed from right to left to derive the denotation of the sentence and populate the context. This direction is advantageous because nouns are encountered before their adjectives and related verbs. The syntax trees are in Figure \ref{fig:illustrative_example}. 

Let $\mathcal{C}$ be the context which starts empty.
The denotation of the first sentence `A pink monkey is eating an apple' is derived as follows:

\begin{enumerate}

\item $\mathcal{C}$ = \{\}, \\
    \sembrac{apple} = ($\lambda x: apple(x)$, \tset{e,t}) \hfill (by TN) \\ 
    \sembrac{an} = ($\lambda f: createEntity(f,\mathcal{C})$, \tset{\tset{e,t}, e}) \hfill (by TN) \\ 
    \sembrac{an apple} = \sembrac{an}(\sembrac{apple}) = a , \\ $\mathcal{C}$ = \{a: apple(a)\} \hfill (by FA) 
    \item $\mathcal{C}$ = \{a: apple(a)\}, \\
    \sembrac{is} = ($\lambda f: f$, \tset{$\mu$,$\mu$})  \hfill (by TN) \\ 
    \sembrac{eating} = ($\lambda x: \lambda y: eat(y, x)$, \tset{e,\tset{e,t}})  \hfill (by TN) \\ 
    \sembrac{is eating} = \sembrac{is}(\sembrac{eating}) = \\  
    ($\lambda x: \lambda y: sing(y, x)$, \tset{e,\tset{e,t}}) \hfill (by FA and $\beta$-reduction) 
    \item $\mathcal{C}$ = \{a: apple(a)\}, \\
    \sembrac{monkey} = ($\lambda x: monkey(x)$, \tset{e,t}) \hfill (by TN) \\ 
    \sembrac{pink} = ($\lambda x: pink(x)$, \tset{e,t}) \hfill (by TN) \\ 
    \sembrac{pink monkey} = ($\lambda x$: $pink(x)$ and $monkey(x)$, \tset{e,t}) \hfill (by PM) 
    \item $\mathcal{C}$ = \{a: apple(a)\}, \\
    \sembrac{a} = ($\lambda f: createEntity(f,\mathcal{C})$, \tset{\tset{e,t}, e}) \hfill (by TN) \\ 
    \sembrac{a pink monkey} = \sembrac{a}(\sembrac{pink monkey}) = p , \\ $\mathcal{C}$ = \{a: apple(a), p: pink(p) and monkey(p)\} \hfill (by FA, step (c)) 
    \item $\mathcal{C}$ = \{a: apple(a), p: pink(p) and monkey(p)\}, \\
    \sembrac{is eating} = ($\lambda x: \lambda y: eat(y, x)$, \tset{e,\tset{e,t}}) \hfill (by step (b))  \\ 
     \sembrac{an apple} = a \hfill (by step (a)) \\
    \sembrac{is eating an apple} = ($\lambda y: eat(y, a)$, \tset{e,t}) \hfill (by FA and $\beta$-reduction) 
    \item $\mathcal{C}$ = \{a: apple(a), p: pink(p) and monkey(p)\}, \\
    \sembrac{is eating an apple} = ($\lambda y: eat(y, a)$, \tset{e,t}) \hfill (by step 5)  \\ 
     \sembrac{a pink monkey} = p \hfill (by step (d)) \\
    \sembrac{a pink monkey is eating an apple} = \sembrac{is eating an apple}(\sembrac{a pink monkey}) = (($\lambda y: eat(y, a)$, \tset{e,t})(p) =  ($eat(p, a)$, t) \hfill (by FA and $\beta$-reduction) 
    
\end{enumerate}

The denotation of the second sentence `The apple is tasty' is derived as follows. The context $\mathcal{C}$  will be the final context derived from the first sentence $\mathcal{C}$ = \{a: apple(a), p: pink(p) and monkey(p)\}:  

\begin{enumerate}

    \item $\mathcal{C}$ = \{a: apple(a), p: pink(p) and monkey(p)\}, \\
    \sembrac{apple} = ($\lambda x: apple(x)$, \tset{e,t}) \hfill (by TN) \\ 
    \sembrac{the} = ($\lambda f$: $getEntity(f, \mathcal{C})$, \tset{\tset{e,t}, e}) \hfill (by TN) \\ 
    \sembrac{the apple} = \sembrac{the}(\sembrac{apple}) = a  \hfill (by FA)  
    \item $\mathcal{C}$  = \{a: apple(a), p: pink(p) and monkey(p)\}, \\
    \sembrac{is} = ($\lambda f: f$, \tset{$\mu$,$\mu$})  \hfill (by TN) \\ 
    \sembrac{tasty} = ($\lambda x:  tasty(x)$, \tset{e,t})   \hfill (by TN) \\ 
    \sembrac{is tasty} = \sembrac{is}(\sembrac{tasty}) = \\  
    ($\lambda x:  tasty(x)$, \tset{e,t})  \hfill (by FA and $\beta$-reduction) 
    \item $\mathcal{C}$ =  \{a: apple(a), p: pink(p) and monkey(p)\}, \\
    \sembrac{the apple} =  a  \hfill (by step (a)) \\ 
    \sembrac{is tasty} = ($\lambda x:  tasty(x)$, \tset{e,t})  \hfill (by step (b)) \\
     \sembrac{the apple is tasty} = \\ 
     \sembrac{is tasty}(\sembrac{the apple}) = \\ 
     ($\lambda x:  tasty(x)$, \tset{e,t})(a) = (tasty(a), t)  \hfill (by FA and $\beta$-reduction) 
\end{enumerate}

\subsection{Paragraph Level Semantic Parser}
\label{subsec:paragraph_level}

A paragraph is the input to the semantic parser, and the output is the denotation of each of the sentences and the context. A context is a dictionary that contains the label and description of each entity. 


Let $\mathcal{C}_{in}$ = \{\} be the input context. Let the paragraph $P$ = [$X_1$, $X_2$ ..., $X_n$] where $n$ is the number of sentences in the paragraph. 

The context outputted after getting the denotation of sentence $i$ is $\mathcal{C}_i$ and the denotation of the sentence \sembrac{X$_i$} = $R_i$ using the input context $\mathcal{C}_{i-1}$ which is the output context of \sembrac{X$_{i-1}$}. 

\sembrac{$P$} = [$R_1$, $R_2$ ..., $R_n$] with output context $\mathcal{C}_{out}$ = $\mathcal{C}_n$.  

Using the example in Section \ref{appen:sentence_example}, we illustrate the denotation of a paragraph. $P$ = [`A pink monkey is eating an apple.', `The apple is tasty.']. $X_1$ = `A pink monkey is eating an apple.' $X_2$ =  `The apple is tasty.'

\sembrac{$P$} = [\sembrac{$X_1$}, \sembrac{$X_2$}] = [$R_1$, $R_2$] = [(eat(p, a), t), (tasty(a), t)]

$\mathcal{C}_{out}$ = $\mathcal{C}_2$ =   \{a: apple(a), p: pink(p) and monkey(p)\}

\subsection{Problem Level Semantic Parser}
\label{subsec:problem_level}

A BIG-bench logic deduction problem contains the premise and the candidate statements, with the underlying question being `what is true?'.  

Let $\mathcal{C}_{in}$ = \{\} be the input context. Let $\mathcal{L}$ = ($P$, $Q$, $A$) be the problem where $P$ is the premise that introduces the set of linear ordering rules, $Q$ is the related question `what is true?', and $A$ is the candidate statements related to the premise. Let $n$ be the number of sentences in the premise, $m$ the number of sentences in the question, and $w$ the number of sentences of the candidate statements. The denotations of the premise, question, and candidate statements are computed using the paragraph-level semantic parser detailed in Section \ref{subsec:paragraph_level}: 

\begin{itemize}[leftmargin=0cm,rightmargin=0cm]
    \item[] \sembrac{$P$} = [$R_{p1}$, ..., $R_{pn}$] with output context $\mathcal{C}_{premise}$ and input context  $\mathcal{C}_{in}$. 

    \item[] \sembrac{$Q$} = $R_{q}$ with output context $\mathcal{C}_{question}$ and input context $\mathcal{C}_{premise}$. 

    \item[] \sembrac{$A$} = [$R_{a1}$, ..., $R_{aw}$] with output context $\mathcal{C}_{choices}$ and input context $\mathcal{C}_{question}$. 
    
    \item[] \sembrac{$\mathcal{L}$}  = (\sembrac{$P$}, \sembrac{$Q$}, \sembrac{$A$}), Entities = labels of the entities in $\mathcal{C}_{choices}$. 

\end{itemize}


We illustrate the problem-level semantic parser on the BIG-bench logic\_deduction problem introduced in Section \ref{chap:scope-preprocessing}. The problem contains the premises $P_{BB}$ = \textbf{premises}, and candidate statements  $A_{{1P}_{BB}}$ = \textbf{candidate statements} with the underlying question $Q_{{1P}_{BB}}$  = `What is true?': 

The problem related to $P_{BB}$ is: $\mathcal{L}_{P_{BB}}$ = ($P_{BB}$, $Q_{{1P}_{BB}}$, $A_{{1P}_{BB}}$). \sembrac{$P_{BB}$} is in Table \ref{table:denotation_P_BB}. Note that the first sentence before the colon is not parsed as it does not provide any needed information for the logic inference.  

The denotation \sembrac{$\mathcal{L}_{P_{BB}}$} = [\sembrac{$P_{BB}$}, [(true(Z), \tset{e,t})], [(position(s, \textbf{last}, 2), t), (position(c, \textbf{last}, 2), t), (position(t, \textbf{last}, 2), t)]] with the output context $\mathcal{C}_{P_{BB}}$ =  \{t: truck(t), c: convertible(c), s: sedan(s)\}


\begin{table}[htb]
\caption{\label{table:denotation_P_BB} \sembrac{$P_{BB}$} with output context $\mathcal{C}_{P_{BB}}$ =  \{t: truck(t), c: convertible(c), s: sedan(s)\}}

\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|}
\hline
\rowcolor[HTML]{D3D3D3} \textbf{Sentence} & \textbf{Denotation} \\ \hline
A sedan, a convertible, and a truck. & $([t, c, s], e)$ \\ \hline
The truck is the newest. & $(last(t), t)$ \\ \hline
The sedan is older than the convertible. & $(before(s, c), t)$ \\ \hline
\end{tabular}
}
\end{table}


%%%% http://www.amzi.com/articles/prolog_under_the_hood.htm
\section{Logic Inference}
\label{chap:logic}

We utilize SWI-Prolog \cite{wielemaker2012swiprolog}, a declarative programming language, in order to specify the conditions to infer the solution. We define the main SWI-Prolog predicate ($linearOrderInf$), which takes in the constraints and the entities of a problem and returns a model $\mathcal{M}$ and can take in a query to infer the correct answer.  A model $\mathcal{M}$ is : 

\begin{tcolorbox}[
    colback=white!10,    % Background of the main box (light gray)
    colframe=gray!75!black, % Border color (dark gray)
    % title=\small\textbf{\textcolor{black}{Model Definition}}, % Title in black
    % colbacktitle=white,  % Title background color (white)
    sharp corners=all,   % Sharp corners for a rectangular box
    boxrule=0.1mm      % Thin border
]

\begin{fleqn}
\begin{equation}
\begin{aligned}
\label{eq:model}
  \mathcal{M} = model(&Query, Pairs) : \\ 
    linearOrderInf(&Entities, Constraints, \\
                  &Query, Pairs).
\end{aligned}
\end{equation}
\end{fleqn}
\small
\begin{itemize}[leftmargin=0.03cm,rightmargin=0cm]
    \item[] \textit{Entities}:  list of entities. 
    \item[] \textit{Constraints}: constraints introduced by the premises. 
    \item[] \textit{Query}:  logical representation of the question. 
    \item[] \textit{Pairs}:  output ordering.  
\end{itemize}

\end{tcolorbox}




The main predicate constrains the entities to positions or relative positions in the list based on the constraints, allowing the resulting model to query based on these constraints. 

The semantic denotation of each candidate statement is passed into the model $\mathcal{M}$ as a query to infer whether the statement is true or false. 
The set of rules is extracted based on the semantic parse of the premises. The listed entities are the labels of entities in the output context of a problem $\mathcal{L}$: $\mathcal{C}_\mathbf{L}$. 
%ordering-related Prolog

We model a linear ordering problem by defining predicates for the Constraint Logic Programming over Finite Domains (CLP(FD)) SWI-Prolog library \cite{Triska2012FD}.
The representation utilizes integer indexing as well as transitivity and other typical axioms for an ordering domain.
For entities A and B, and number n, the ordering predicates are:
\textit{before(A, B)},  \textit{succeed(A, B)}, \textit{not\_(constraint)}, \textit{first(A)}, \textit{last(A)}, 
\textit{position(A, \textbf{first}, n)}, \textit{position(A, \textbf{last}, n)}.

% The predicates are defined and used in the main predicate, which ensures that the index of each entity is unique and within the range of the number of entities. 

% \begin{figure}[!hbt]
%     \centering
%     \includegraphics[scale=0.45]{./figures/BB_example.png}
%     \caption{Automated visualization of the premise : A fruit stand sells five fruits: watermelons, apples, kiwis, cantaloupes, and mangoes. The cantaloupes are the second-cheapest. The mangoes are more expensive than the watermelons. The kiwis are less expensive than the apples. The kiwis are more expensive than the mangoes. \\ with the context : \{m: mango(m), c: cantaloup(c), k: kiwis(k), a: apple(a), w: watermelon(w)\}}
%     \label{fig:example_BB}
% \end{figure}

To solve the problem $\mathcal{L}_{P_{BB}}$ introduced in Section \ref{subsec:problem_level}, the system creates the logic inference model $\mathcal{M}_{BB}$ based on Equation (1) and semantic parsing algorithm output on $\mathcal{L}_{P_{BB}}$ shown in Section \ref{subsec:problem_level}.
The output of FSLI using the model $\mathcal{M}_{BB}$ on $\mathcal{L}_{P_{BB}}$ is [0, 1, 0]; which means the second choice is true and the others are false.

\begin{tcolorbox}[
    colback=white!10,    % Background of the main box (light gray)
    colframe=gray!75!black, % Border color (dark gray)
    % title=\small\textbf{\textcolor{black}{Logic Inference Model for Example BB}}, % Title in black
    % colbacktitle=white,  % Title background color (white)
    sharp corners=all,   % Sharp corners for a rectangular box
    boxrule=0.1mm      % Thin border
]

\begin{fleqn}
\begin{equation*}
\begin{aligned}
\label{eq:model}
  \mathcal{M}_{BB}  = model(&Query, Pairs) :  \\
             linearOrderInf(& [t, c, s],  (last(t), before(s, c) ), \\
                            & Query, Pairs).  
\end{aligned}
\end{equation*}
\end{fleqn}
\end{tcolorbox}

 

\section{Performance}
\label{chap:performance} 




We use the BIG-bench benchmark \cite{srivastava2023beyond} to asses FSLI's semantic parsing proficiency and ensure the soundness of its logical inferences since is only one valid sequence for the arrangement of entities. 


%%%%%%%%%% how I prompted the LLM models %%%%%%%%%%%%%% 

We tested the following LLMs: GPT-4, PALM-2 and LLAMA-2-13b-chat \cite{openai2023gpt4, anil2023palm2, touvron2023llama}. Appendix \ref{appen:LLM-Running} presents examples of LLM responses to BBLD problems and illustrates the prompt used. 
% In order to test the performance of the LLMs on the dataset, we prompted them with the format: ```please give me the answer to a multiple choice question by giving the index of the correct answer:  premise :   \{ the premise \}  question :  \{ the question \}   answer choices :  \{ the candidate statements \} ''' 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \begin{table}[!htb]
% \caption{Accuracy of models on the benchmark BIG-bench logical\_deduction (BBLD) subtasks, which contain a total of 1500 examples: 300 for three entities, 500 for five entities, and 700 for seven entities. BBLD refers to the weighted average.}

% \label{tab:evaluation}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|l|lllll|}
% \hline

% \rowcolor[HTML]{D3D3D3} 
% \textbf{Approach} & \textbf{Model} & \multicolumn{5}{c|}{\textbf{Accuracy}} \\ \hline
%                   &                & \multicolumn{1}{c|}{\textbf{Three}} & \multicolumn{1}{c|}{\textbf{Five}} & \multicolumn{1}{c|}{\textbf{Seven}} & \multicolumn{1}{c|}{\textbf{BBLD}} & \textbf{BBLD-300\tablefootnote{BBLD-300 refers to accuracy on 300 examples.}} \\ \hline
% \multirow{3}{*}{LLM} 
%                   & GPT-4          & \multicolumn{1}{c|}{93.00\%} & \multicolumn{1}{c|}{65.60\%} & \multicolumn{1}{c|}{57.00\%} & \multicolumn{1}{c|}{67.06\%} & - \\ \cline{2-7} 
%                   & PaLM-2         & \multicolumn{1}{c|}{58.67\%} & \multicolumn{1}{c|}{37.00\%} & \multicolumn{1}{c|}{24.14\%} & \multicolumn{1}{c|}{35.33\%} & - \\ \cline{2-7} 
%                   & LLAMA-2        & \multicolumn{1}{c|}{42.67\%} & \multicolumn{1}{c|}{23.60\%} & \multicolumn{1}{c|}{20.43\%} & \multicolumn{1}{c|}{25.93\%} & - \\ \hline
% LLM+CoT           & CoT (GPT-4)\tablefootnote{\url{https://github.com/teacherpeterpan/Logic-LLM}} 
%                   & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & 75.25\% \\ \hline
% LLM+Logic         & Logic-LM (GPT-4)\tablefootnote{\url{https://github.com/teacherpeterpan/Logic-LLM}} 
%                   & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{-} & 87.63\% \\ \hline
% \textbf{Symbolic} & \textbf{FSLI}  & \multicolumn{1}{c|}{\textbf{100\%}} & \multicolumn{1}{c|}{\textbf{100\%}} & \multicolumn{1}{c|}{\textbf{100\%}} & \multicolumn{1}{c|}{\textbf{100\%}} & - \\ \hline
% \end{tabular}%
% }
% \end{table}
\begin{table}[!htb]
\caption{Accuracy of models on the benchmark BIG-bench logical\_deduction (BBLD) subtasks, which contain a total of 1500 examples: 300 for three entities, 500 for five entities, and 700 for seven entities. BBLD refers to the weighted average, while BBLD-300 refers to accuracy on 300 specific examples reported in related works.}
\label{tab:evaluation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\rowcolor[HTML]{D3D3D3} 
\textbf{Approach} & \textbf{Model} & \textbf{Three} & \textbf{Five} & \textbf{Seven} & \textbf{BBLD} \\ \hline
\multirow{3}{*}{LLM} 
                  & GPT-4          & 93.00\% & 65.60\% & 57.00\% & 67.06\%  \\ \cline{2-6} 
                  & PaLM-2         & 58.67\% & 37.00\% & 24.14\% & 35.33\%  \\ \cline{2-6} 
                  & LLAMA-2        & 42.67\% & 23.60\% & 20.43\% & 25.93\%  \\ \hline
\multirow{2}{*}{LLM+CoT}          & CoT   & - & - & - & \cellcolor[HTML]{ADD8E6}75.25\%\footnotemark[5] \\ 
    &  (GPT-4)\footnotemark[4]  &  &  &  & \cellcolor[HTML]{ADD8E6} \\ \hline
\multirow{2}{*}{LLM+Logic}         & Logic-LM & - & - & - & \cellcolor[HTML]{ADD8E6}87.63\%\footnotemark[5] \\
      & (GPT-4)\footnotemark[4] &  &  &  & \cellcolor[HTML]{ADD8E6} \\ \hline
\textbf{Symbolic} & \textbf{FSLI}  & \textbf{100\%} & \textbf{100\%} & \textbf{100\%} & \textbf{100\%}  \\ \hline
\end{tabular}%
}
\end{table}

\footnotetext[4]{\url{https://github.com/teacherpeterpan/Logic-LLM}}
\footnotetext[5]{on 300 examples from BBLD as reported by \citet{PanLogicLM23}}

% \subsection{Comparison to LLMs}
GPT-4 gets the highest score among LLMs, scoring an accuracy of 67.06\% on the BIG-bench logical\_deduction (BBLD) \footnote{\url{https://github.com/google/BIG-bench/tree/main/BIG-bench/}}, but fails to scale nicely with large decreases in accuracy across the subtasks of BBLD on three entities (93\%), five entities (65.6\%), and seven entities (57\%). The large difference in accuracy (93\% vs 65.6\%) indicates the challenge GPT-4 faces in logical deduction.

% % \subsection{Comparison to Logic-LM}
% The combination of a logical deduction module and LLMs could help improve the accuracy of vanilla LLMs. Logic-LM introduced in \cite{PanLogicLM23}, combines the LLM structured output with various logical deduction modules based on the tasks. The reported scores of Logic-LM (GPT-4) of 87.63\% on BIG-bench logical\_deduction (BBLD) compared to that of GPT-4 of 67.06\% indicates an around 20\% improvement of accuracy due to the logical deduction modules.

% \subsection{FSLI results}
The accuracy scores reported in Table \ref{tab:evaluation} indicate that our model (FSLI) -- with a score of 100\% -- outperforms LLMs and Logic-LM \cite{PanLogicLM23} on BBLD. Table \ref{tab:evaluation} reports the score of Logic-LM of 87.63\% on a subset of 300 examples in BIG-bench logical\_deduction as presented in their paper \cite{PanLogicLM23}. The reported score of GPT-4 using chain of thought prompting on the 300 examples is 75.25\% \cite{PanLogicLM23}. 

The accuracy, interpretability, explainability and reproducibility of FSLI are all evidences in favour of adopting our methodology for natural language logical deduction tasks. 


\section{Conclusion}
\label{chap:conclusion}

We have developed a Formal Semantic Logic Inferer (FSLI) system that consists of a semantic parsing algorithm tightly integrated with the logic inference engine. 

We use the BIG-bench logical\_deduction benchmark to assess FSLI's semantic parsing proficiency and ensure the soundness of its logical inferences since there is only one valid sequence for the arrangement of entities.

FSLI attains 100\% accuracy in solving BIG-bench logical\_deduction problems compared to performance of systems based on general-purpose large language models that rely on patterns.

This suggests the feasibility of linguistic-based algorithms for deterministic problems that require logic deduction to build a systematic model of relationships that are consistent among entities. 

FSLI can be expanded to solve more complex logical deduction problems related to linear ordering that require the determination of multiple valid orderings and those that do not provide choices. 

The logic-driven semantic layer developed offers the future possibility of 
utilizing such a lambda calculus-based layer to handle problems that may be framed as constraint satisfaction problems, such as the N-Queens problem posed in natural language. 

This research illustrates the promise and important future steps toward more rich and powerful logical-based reasoning for natural language problems.


\section*{Acknowledgements}
We would like to acknowledge the contributions made by Professor Neil Immerman and Professor Katrin Erk. Both of whom have been instrumental in enriching the research. 

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\newpage
\bibliography{main}

\newpage
\appendix

\section{Translations}
\label{appen:translations}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}

Table \ref{tab:translations} enumerates some of the phrases that are used to indicate linear ordering. 
\begin{table*}[!hbt]
\caption{Selected translations}
\label{tab:translations}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|}
\hline
\rowcolor[HTML]{D3D3D3} 
\textbf{Category} & \textbf{Possible Preceding Phrase} & \textbf{Keyword} & \textbf{Possible Succeeding Phrase} & \textbf{Translation} \\ \hline
\multirow{4}{*}{Direction} 
    & to the           & right       & of           & after  \\ \cline{2-5} 
    & to the           & left        & of           & before \\ \cline{2-5} 
    &                  & rightmost   &              & last   \\ \cline{2-5} 
    &                  & leftmost    &              & first  \\ \hline
\multirow{4}{*}{Relative Position} 
    & the              & first from the left  &          & first \\ \cline{2-5} 
    & the              & third from the left  &          & third \\ \cline{2-5} 
    & the              & first from the right &          & last  \\ \cline{2-5} 
    & the              & third from the right &          & third \\ \hline
\multirow{6}{*}{Age} 
    &                  & newer       & than         & after  \\ \cline{2-5} 
    &                  & older       & than         & before \\ \cline{2-5} 
    & the              & newest      &              & last   \\ \cline{2-5} 
    & the              & oldest      &              & first  \\ \cline{2-5} 
    &                  & younger     & than         & after  \\ \cline{2-5} 
    & the              & youngest    &              & last   \\ \hline
\multirow{4}{*}{Weight} 
    &                  & heavier     & than         & after  \\ \cline{2-5} 
    &                  & lighter     & than         & before \\ \cline{2-5} 
    & the              & heaviest    &              & last   \\ \cline{2-5} 
    & the              & lightest    &              & first  \\ \hline
\multirow{5}{*}{Performance} 
    &                  & finished above  &             & before \\ \cline{2-5} 
    &                  & finished below  &             & after  \\ \cline{2-5} 
    &                  & finished third  &             & third  \\ \cline{2-5} 
    &                  & finished first  &             & first  \\ \cline{2-5} 
    &                  & finished last   &             & last   \\ \hline
\multirow{4}{*}{Speed} 
    &                  & slower      & than         & after  \\ \cline{2-5} 
    &                  & faster      & than         & before \\ \cline{2-5} 
    & the              & slowest     &              & last   \\ \cline{2-5} 
    & the              & fastest     &              & first  \\ \hline
\multirow{5}{*}{Expense} 
    &                  & less expensive & than       & after  \\ \cline{2-5} 
    &                  & more expensive & than       & before \\ \cline{2-5} 
    & the              & least expensive &           & last   \\ \cline{2-5} 
    & the              & cheapest    &              & last   \\ \cline{2-5} 
    & the              & most expensive &            & first  \\ \hline
\end{tabular}%
}
\end{table*}



\section{Sample LLM responses on BIG-bench logical\_deduction problems}
\label{appen:LLM-Running}

 Tables \ref{tab:eval_gpt4}, \ref{tab:eval_palm2}, and \ref{tab:eval_llama2} show responses of GPT-4, PALM-2, and LLAMA-2-13b-chat to some BIG-bench logical\_deduction problems. We post-process the responses and extract the indices of the candidate statements that the LLM determined to be the answer to the question.  PALM-2 and GPT-4 have succinct responses mostly outputting the index of the candidate statements selected. LLAMA-2-13b-chat is more expressive and requires some post-processing to get the direct answer to the list of selected candidate statements. LLAMA-2-13b-chat outputs followed a certain pattern and pattern matching was used to extract the direct answer. 


% % Please add the following required packages to your document preamble:
% % \usepackage{graphicx}
% \begin{table*}[]
% \caption{Sample of Prompt and Responses of GPT-4 on BIG-bench logical\_deduction}
% \label{tab:eval_gpt4}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|l|l|l|l|}
% \hline
% \textbf{Prompt} & \multicolumn{1}{l|}{\textbf{GPT-4 response}} & \textbf{GPT-4 label} & \multicolumn{1}{l|}{\textbf{true label}} \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             A fruit stand sells seven fruits: plums, kiwis, pears, mangoes, apples, oranges, and loquats. The pears are less expensive than the oranges. \\             The mangoes are less expensive than the kiwis. The plums are the second-most expensive. The loquats are more expensive than the apples. \\               The kiwis are less expensive than the apples. The loquats are the fourth-most expensive.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The plums are the most expensive.', 'The kiwis are the most expensive.', 'The pears are the most expensive.',\\             'The mangoes are the most expensive.', 'The apples are the most expensive.',\\           'The oranges are the most expensive.', 'The loquats are the most expensive.'{]}\end{tabular} & 6 & {[}6{]} & 5 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             On a shelf, there are five books: a green book, a blue book, a brown book, a gray book, and a red book. The green book is to the left of the brown book. \\             The gray book is to the left of the blue book. The green book is to the right of the blue book. The red book is the leftmost.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The green book is the rightmost.', 'The blue book is the rightmost.', 'The brown book is the rightmost.', 'The gray book is the rightmost.', 'The red book is the rightmost.'{]}\end{tabular} & 2 & {[}2{]} & 2 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             On a branch, there are three birds: a falcon, an owl, and a raven. The raven is to the left of the owl. The falcon is the leftmost.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The falcon is the leftmost.', 'The owl is the leftmost.', 'The raven is the leftmost.'{]}\end{tabular} & 0 & {[}0{]} & 0 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             In an antique car show, there are five vehicles: a minivan, a hatchback, a bus, a convertible, and a motorcyle. The hatchback is newer than the convertible. \\          The bus is newer than the hatchback. The bus is older than the motorcyle. The minivan is the newest.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The minivan is the second-oldest.', 'The hatchback is the second-oldest.', 'The bus is the second-oldest.', 'The convertible is the second-oldest.', 'The motorcyle is the second-oldest.'{]}\end{tabular} & \multicolumn{1}{l|}{The convertible is the second-oldest.'} & {[}3{]} & 1 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             A fruit stand sells five fruits: watermelons, oranges, loquats, plums, and kiwis. The plums are less expensive than the kiwis. The plums are more expensive than the watermelons. \\           The loquats are more expensive than the kiwis. The oranges are the most expensive.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The watermelons are the second-most expensive.', 'The oranges are the second-most expensive.',\\            'The loquats are the second-most expensive.', 'The plums are the second-most expensive.', 'The kiwis are the second-most expensive.'{]}\end{tabular} & 4 & {[}4{]} & 2 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             In an antique car show, there are three vehicles: a truck, a minivan, and a tractor. The truck is newer than the minivan. The minivan is newer than the tractor.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The truck is the second-newest.', 'The minivan is the second-newest.', 'The tractor is the second-newest.'{]}\end{tabular} & 1 & {[}1{]} & 1 \\ \hline
% \end{tabular}%
% }
% \end{table*}
\begin{table*}[]
\caption{Sample of Prompt and Responses of GPT-4 on BIG-bench logical\_deduction}
\label{tab:eval_gpt4}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|}
\hline
\rowcolor[HTML]{D3D3D3} 
\textbf{Prompt} & \textbf{GPT-4 Response} & \textbf{GPT-4 Label} & \textbf{True Label} \\ \hline
\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ premise: \\ A fruit stand sells seven fruits: plums, kiwis, pears, mangoes, apples, oranges, and loquats. The pears are less expensive \\ than the oranges. The mangoes are less expensive than the kiwis. The plums are the second-most expensive. The loquats are \\ more expensive than the apples. The kiwis are less expensive than the apples. The loquats are the fourth-most expensive. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The plums are the most expensive.', 'The kiwis are the most expensive.', 'The pears are the most expensive.', \\ 'The mangoes are the most expensive.', 'The apples are the most expensive.', 'The oranges are the most expensive.', \\ 'The loquats are the most expensive.'{]}\end{tabular} & 6 & {[}6{]} & 5 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ premise: \\ On a shelf, there are five books: a green book, a blue book, a brown book, a gray book, and a red book. The green book is \\ to the left of the brown book. The gray book is to the left of the blue book. The green book is to the right of the blue book. \\ The red book is the leftmost. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The green book is the rightmost.', 'The blue book is the rightmost.', 'The brown book is the rightmost.', \\ 'The gray book is the rightmost.', 'The red book is the rightmost.'{]}\end{tabular} & 2 & {[}2{]} & 2 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ premise: \\ On a branch, there are three birds: a falcon, an owl, and a raven. The raven is to the left of the owl. The falcon is \\ the leftmost. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The falcon is the leftmost.', 'The owl is the leftmost.', 'The raven is the leftmost.'{]}\end{tabular} & 0 & {[}0{]} & 0 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ premise: \\ In an antique car show, there are five vehicles: a minivan, a hatchback, a bus, a convertible, and a motorcycle. The \\ hatchback is newer than the convertible. The bus is newer than the hatchback. The bus is older than the motorcycle. The \\ minivan is the newest. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The minivan is the second-oldest.', 'The hatchback is the second-oldest.', 'The bus is the second-oldest.', \\ 'The convertible is the second-oldest.', 'The motorcycle is the second-oldest.'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}The convertible is \\ the second-oldest.\end{tabular} & {[}3{]} & 1 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ premise: \\ A fruit stand sells five fruits: watermelons, oranges, loquats, plums, and kiwis. The plums are less expensive than \\ the kiwis. The plums are more expensive than the watermelons. The loquats are more expensive than the kiwis. \\ The oranges are the most expensive. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The watermelons are the second-most expensive.', 'The oranges are the second-most expensive.', \\ 'The loquats are the second-most expensive.', 'The plums are the second-most expensive.', \\ 'The kiwis are the second-most expensive.'{]}\end{tabular} & 4 & {[}4{]} & 2 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ premise: \\ In an antique car show, there are three vehicles: a truck, a minivan, and a tractor. The truck is newer than \\ the minivan. The minivan is newer than the tractor. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The truck is the second-newest.', 'The minivan is the second-newest.', 'The tractor is the second-newest.'{]}\end{tabular} & 1 & {[}1{]} & 1 \\ \hline
\end{tabular}%
}
\end{table*}


\begin{table*}[]
\caption{Sample of Prompt and Responses of PaLM-2 on BIG-bench logical\_deduction}
\label{tab:eval_palm2}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|}
\hline
\rowcolor[HTML]{D3D3D3} 
\textbf{Prompt} & \textbf{PaLM-2 Response} & \textbf{PaLM-2 Label} & \textbf{True Label} \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ A fruit stand sells five fruits: watermelons, oranges, loquats, plums, and kiwis. \\ The plums are less expensive than the kiwis. The plums are more expensive than the watermelons. \\ The loquats are more expensive than the kiwis. The oranges are the most expensive. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The watermelons are the cheapest.', 'The oranges are the cheapest.', \\ 'The loquats are the cheapest.', 'The plums are the cheapest.', 'The kiwis are the cheapest.'{]}\end{tabular} 
& 0 & {[}0{]} & 0 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ On a shelf, there are three books: a red book, a blue book, and an orange book. \\ The red book is to the left of the orange book. The orange book is the second from the left. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The red book is the rightmost.', 'The blue book is the rightmost.', \\ 'The orange book is the rightmost.'{]}\end{tabular} 
& 2 & {[}2{]} & 1 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ In a golf tournament, there were three golfers: Ana, Mel, and Ada. \\ Ada finished above Ana. Mel finished below Ana. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'Ana finished second.', 'Mel finished second.', 'Ada finished second.'{]}\end{tabular} 
& 1 & {[}1{]} & 0 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ A fruit stand sells five fruits: kiwis, pears, peaches, loquats, and apples. \\ The loquats are more expensive than the kiwis. The pears are more expensive than the peaches. \\ The apples are more expensive than the loquats. The apples are less expensive than the peaches. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The kiwis are the second-most expensive.', 'The pears are the second-most expensive.', \\ 'The peaches are the second-most expensive.', 'The loquats are the second-most expensive.', \\ 'The apples are the second-most expensive.'{]}\end{tabular} 
& 3 & {[}3{]} & 2 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ On a shelf, there are seven books: a green book, a brown book, a white book, a black book, \\ an orange book, a purple book, and a yellow book. The purple book is to the left of the yellow book. \\ The green book is to the left of the white book. The brown book is to the right of the yellow book. \\ The white book is the fourth from the left. The green book is to the right of the orange book. \\ The black book is the second from the left. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The green book is the second from the left.', 'The brown book is the second from the left.', \\ 'The white book is the second from the left.', 'The black book is the second from the left.', \\ 'The orange book is the second from the left.', 'The purple book is the second from the left.', \\ 'The yellow book is the second from the left.'{]}\end{tabular} 
& 3 & {[}3{]} & 3 \\ \hline


\end{tabular}%
}
\end{table*}




% % Please add the following required packages to your document preamble:
% % \usepackage{graphicx}
% \begin{table*}[]
% \caption{Sample of Prompt and Responses of LLAMA-2-13b-chat on BIG-bench logical\_deduction}
% \label{tab:eval_llama2}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|l|l|l|l|}
% \hline
% \textbf{Prompt} & \textbf{LLAMA2 response} & \textbf{LLAMA2 label} & \multicolumn{1}{l|}{\textbf{true label}} \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             On a branch, there are three birds: a quail, an owl, and a hummingbird. The quail is to the left of the owl. \\              The owl is to the left of the hummingbird.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The quail is the rightmost.', 'The owl is the rightmost.', 'The hummingbird is the rightmost.'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question:\\  \\ \\  The correct answer is:\\  \\ \\  2. The owl is the rightmost.\end{tabular} & {[}1{]} & 2 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             In a golf tournament, there were five golfers: Dan, Amy, Eve, Ana, and Mya. Dan finished above Eve. \\          Dan finished below Mya. Amy finished third. Ana finished second-to-last.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'Dan finished last.', 'Amy finished last.', 'Eve finished last.', 'Ana finished last.', 'Mya finished last.'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sure thing! Based on the information provided, the correct answer is:\\  \\ \\  Index 2: Eve finished last.\end{tabular} & {[}2{]} & 2 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             In a golf tournament, there were three golfers: Joe, Eve, and Amy. Amy finished above Eve. Joe finished first.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'Joe finished first.', 'Eve finished first.', 'Amy finished first.'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question:\\  \\ \\  The correct answer is:\\  \\ \\  1 (Joe finished first.)\end{tabular} & {[}0{]} & 0 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             In an antique car show, there are three vehicles: a station wagon, a minivan, and a truck. \\        The minivan is newer than the truck. The station wagon is newer than the minivan.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The station wagon is the second-newest.', 'The minivan is the second-newest.', 'The truck is the second-newest.'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question:\\  \\ \\  The correct answer is:\\  \\ \\  {[}'The station wagon is the second-newest.{]}\\  \\ \\  Explanation:\\  \\ \\  Since the minivan is newer than the truck, and the station wagon is newer than the minivan, \\ it follows that the station wagon is the second-newest vehicle at the antique car show.\end{tabular} & {[}0{]} & 1 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             A fruit stand sells seven fruits: watermelons, cantaloupes, pears, peaches, mangoes, kiwis, and oranges. \\ The oranges are more expensive than the cantaloupes. \\ The pears are less expensive than the kiwis. The kiwis are the fourth-most expensive.\\  The mangoes are less expensive than the pears. The peaches are the cheapest. \\ The cantaloupes are more expensive than the watermelons.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The watermelons are the second-most expensive.', 'The cantaloupes are the second-most expensive.',\\              'The pears are the second-most expensive.', \\             'The peaches are the second-most expensive.', 'The mangoes are the second-most expensive.', \\             'The kiwis are the second-most expensive.',\\            'The oranges are the second-most expensive.'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question:\\  \\ \\  The correct answer is: B) The cantaloupes are the second-most expensive.\end{tabular} & {[}1{]} & 1 \\ \hline
% \begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving the index of the correct answer: \\ \\ \\             premise : \\             \\             On a branch, there are seven birds: a quail, a raven, a hummingbird, a robin, a falcon, an owl, and a blue jay.\\            The blue jay is to the left of the falcon. The blue jay is the second from the right. The raven is to the left of the robin. \\            The owl is the third from the right. The hummingbird is to the left of the quail. The raven is the third from the left.\\             \\             question : \\             \\             what is true? \\             \\             answer choices : \\             \\             {[}'The quail is the second from the right.', 'The raven is the second from the right.',\\              'The hummingbird is the second from the right.',\\          'The robin is the second from the right.', 'The falcon is the second from the right.',\\           'The owl is the second from the right.',\\          'The blue jay is the second from the right.'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sure! Here's the answer to your question:\\  \\ \\  The correct answer is:\\  \\ \\  B) The raven is the second from the right.\\  \\ \\  Explanation:\\  \\ \\  1. The blue jay is the second from the right, so it is not the correct answer.\\  2. The falcon is the third from the right, so it is not the correct answer.\\  3. The owl is the third from the right, so it is not the correct answer.\\  4. The hummingbird is to the left of the quail, so it is not the correct answer.\\  5. The robin is to the left of the raven, so it is not the correct answer.\\  6. The quail is the second from the left, so it is not the correct answer.\\  \\ \\  Therefore, the correct answer is B) The raven is the second from the right.\end{tabular} & {[}1{]} & 6 \\ \hline
% \end{tabular}%
% }
% \end{table*}

\begin{table*}[]
\caption{Sample of Prompt and Responses of LLAMA-2-13b-chat on BIG-bench logical\_deduction}
\label{tab:eval_llama2}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|c|c|}
\hline
\rowcolor[HTML]{D3D3D3} 
\textbf{Prompt} & \textbf{LLAMA2 Response} & \textbf{LLAMA2 Label} & \textbf{True Label} \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ On a branch, there are three birds: a quail, an owl, and a hummingbird. \\ The quail is to the left of the owl. The owl is to the left of the hummingbird. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The quail is the rightmost.', 'The owl is the rightmost.', \\ 'The hummingbird is the rightmost.'{]}\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question: \\ \\ The correct answer is: \\ \\ 2. The owl is the rightmost.\end{tabular} 
& {[}1{]} 
& 2 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ In a golf tournament, there were five golfers: Dan, Amy, Eve, Ana, and Mya. \\ Dan finished above Eve. Dan finished below Mya. Amy finished third. \\ Ana finished second-to-last. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'Dan finished last.', 'Amy finished last.', 'Eve finished last.', \\ 'Ana finished last.', 'Mya finished last.'{]}\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Sure thing! Based on the information provided, the correct answer is: \\ \\ Index 2: Eve finished last.\end{tabular} 
& {[}2{]} 
& 2 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ In a golf tournament, there were three golfers: Joe, Eve, and Amy. \\ Amy finished above Eve. Joe finished first. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'Joe finished first.', 'Eve finished first.', 'Amy finished first.'{]}\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question: \\ \\ The correct answer is: \\ \\ 1 (Joe finished first.)\end{tabular} 
& {[}0{]} 
& 0 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ In an antique car show, there are three vehicles: a station wagon, \\ a minivan, and a truck. The minivan is newer than the truck. \\ The station wagon is newer than the minivan. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The station wagon is the second-newest.', \\ 'The minivan is the second-newest.', 'The truck is the second-newest.'{]}\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question: \\ \\ The correct answer is: \\ \\ {[}'The station wagon is the second-newest.'{]} \\ \\ Explanation: \\ \\ Since the minivan is newer than the truck, and the station \\ wagon is newer than the minivan, it follows that the \\ station wagon is the second-newest vehicle at the show.\end{tabular} 
& {[}0{]} 
& 1 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ A fruit stand sells seven fruits: watermelons, cantaloupes, pears, peaches, \\ mangoes, kiwis, and oranges. The oranges are more expensive than \\ the cantaloupes. The pears are less expensive than the kiwis. \\ The kiwis are the fourth-most expensive. The mangoes are less \\ expensive than the pears. The peaches are the cheapest. \\ The cantaloupes are more expensive than the watermelons. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The watermelons are the second-most expensive.', \\ 'The cantaloupes are the second-most expensive.', 'The pears are the second-most expensive.', \\ 'The peaches are the second-most expensive.', 'The mangoes are the second-most expensive.', \\ 'The kiwis are the second-most expensive.', 'The oranges are the second-most expensive.'{]}\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Sure thing! Here's the answer to your question: \\ \\ The correct answer is: B) The cantaloupes are the second-most expensive.\end{tabular} 
& {[}1{]} 
& 1 \\ \hline

\begin{tabular}[c]{@{}l@{}}please give me the answer to a multiple choice question by giving \\ the index of the correct answer: \\ \\ premise: \\ On a branch, there are seven birds: a quail, a raven, a hummingbird, \\ a robin, a falcon, an owl, and a blue jay. \\ The blue jay is to the left of the falcon. The blue jay is the second from the right. \\ The raven is to the left of the robin. The owl is the third from the right. \\ The hummingbird is to the left of the quail. The raven is the third from the left. \\ \\ question: \\ what is true? \\ \\ answer choices: \\ {[}'The quail is the second from the right.', \\ 'The raven is the second from the right.', 'The hummingbird is the second from the right.', \\ 'The robin is the second from the right.', 'The falcon is the second from the right.', \\ 'The owl is the second from the right.', 'The blue jay is the second from the right.'{]}\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Sure! Here's the answer to your question: \\ \\ The correct answer is: \\ \\ B) The raven is the second from the right. \\ \\ Explanation: \\ 1. The blue jay is the second from the right, so it is not the correct answer. \\ 2. The falcon is the third from the right, so it is not the correct answer. \\ 3. The owl is the third from the right, so it is not the correct answer. \\ 4. The hummingbird is to the left of the quail, so it is not the correct answer. \\ 5. The robin is to the left of the raven, so it is not the correct answer. \\ 6. The quail is the second from the left, so it is not the correct answer. \\ \\ Therefore, the correct answer is B) The raven is the second from the right.\end{tabular} 
& {[}1{]} 
& 6 \\ \hline

\end{tabular}%
}
\end{table*}


% \section{}
 

 


\end{document}
