\section{Related work}
% \subsection{Music generation?}

% \subsection{Video analysis?}

% \subsection{Video-based audio generation?}

In this section, we introduce the video emotion classifier and the emotion-based MIDI generator used in our work. We then review existing video-based MIDI generation methods and discuss their evaluation approaches.


\subsection{Video emotion classification}

% We use the pretrained VEMOCLAP (Video EMOtion Classifier using Pretrained features) model to analyze the emotions of the input video**Ekman, "Facial Action Coding System"**. 
% The model extracts features relevant to the video's underlying emotions using multiple publicly available pretrained models for automatic speech recognition (ASR)**Sermanet et al., "OverFeat: Integrated Classification, Localization and Detection using Deep Exhaustive Searching"**, optical character recognition (OCR)**Jaderberg et al., "Reading Text in the Wild with Convolutional Neural Networks"**, facial expression classification**Kollias et al., "Convolutional Neural Networks for Facial Expression Recognition"**, audio classification**Tran et al., "Convolutional Neural Networks for Audio Classification"**, and image understanding**Long et al., "Deep Residual Learning for Image Recognition"**. 
The VEMOCLAP model exploits publicly available pretrained models for a multimodal video analysis and uses the resulting pretrained features to classify the emotions of arbitrary videos**Zhou et al., "Learning Deep Features for Multimodal Video Analysis"**. It employs pretrained models for automatic speech recognition (ASR), optical character recognition (OCR), facial expression classification, audio classification, and image understanding.
Textual features obtained from ASR and OCR are further processed using a text sentiment classifier. 
% To integrate and process these multimodal features, we train cross-attention layers**Chiu et al., "State-of-the-Art Speech Recognition with a Sequence-to-Sequence Model"**, followed by a linear layer, which produces the final emotion probabilities. 
Cross-attention layers**Wang et al., "Attention Is All You Need"** integrate and process these multimodal features, and a linear layer produces the final emotion probabilities. 
The model is trained on the Ekman-6 dataset**Ekman, "Facial Action Coding System"**, consisting of 1637 videos labeled with the six basic emotions derived from the original work of Ekman: anger, disgust, fear, joy, sadness, and surprise~\shortcite{ekman}. 
% The architecture of VEMOCLAP is illustrated in Figure \ref{fig:vemoclap}. 
% For additional details, we refer readers to our original publications**Kumar et al., "Video Emotion Classification using Pretrained Features"**.

\subsection{Emotion-based MIDI generation}
\label{sec:emotion_based}

Sulun et al.~\shortcite{sighttosound} collected valence and arousal values for songs in the Lakh Pianoroll Dataset **Lad, "Automatic Music Transcription of Polyphonic Music"**, and trained a conditional transformer using the resulting labeled dataset. To integrate continuous-valued valence and arousal features with discrete musical note tokens, they project valence and arousal into a vector space using separate linear layers. 
% If valence or arousal is unspecified, they assign it a NaN value and use a learned vector instead of a projection. 
Musical input tokens are projected into vectors of the same dimensionality using an embedding layer. These vectors are then concatenated with the projected valence and arousal vectors along the sequence dimension and fed into the transformer body with relative global attention**Vaswani et al., "Attention Is All You Need"**. 

% We opted for this simpler input modification approach—rather than injecting valence and arousal values directly into the transformer body—to prevent an increase in model size and to allow users to fine-tune the model seamlessly, whether with or without valence/arousal conditioning or using alternative conditions. The emotion-conditioning mechanism can be seen in the upper part of Figure \ref{fig:music}.

\subsection{Video-based MIDI generation}

While our method applies to arbitrary videos, several works focus on generating symbolic music for specific types of videos, such as those featuring human movements like dancing or instrumental performances. The Foley Music model**Srivastava et al., "Learning Representations for Multi-Modal Sequence Data"** generates MIDI from videos of musicians by processing body keypoint movements using a Graph Convolutional Network**Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"** and a Transformer**Vaswani et al., "Attention Is All You Need"**. Similarly, Koepke et al.~\shortcite{sighttosound} and Su et al.~\shortcite{audeo} utilize use deep neural networks with residual connections to generate symbolic music from videos of finger movements on piano keyboards. Due to their specialized nature, these approaches rely on datasets containing video-MIDI pairs, though these datasets typically contain fewer than 1k samples**Huang et al., "Automatic Music Transcription of Polyphonic Music"**. The RhythmicNet model employs a multi-stage process to generate music from dance videos by predicting beats and style, generating a drum track, and subsequently creating multitrack music**Srivastava et al., "Learning Representations for Multi-Modal Sequence Data"**.

Some studies explore the more general task of generating symbolic music for arbitrary videos. The most similar to our approach, the Controllable Music Transformer (CMT), generates music based on video features such as motion speed, motion saliency, and timing**Huang et al., "Learning Representations for Multi-Modal Sequence Data"**. CMT employs the Lakh Pianoroll Dataset **Lad, "Automatic Music Transcription of Polyphonic Music"**, the same dataset we use, and processes music using an extended Compound Word representation, where each token encodes type, beat/bar marking, note strength, note density, instrument, pitch, and duration**Srivastava et al., "Learning Representations for Multi-Modal Sequence Data"**. While this representation reduces sequence length, it significantly increases input dimensionality. Consequently, CMT uses a low temporal resolution of four ticks per beat, translating to a time resolution of 125 milliseconds for a song at 120 beats per minute. Furthermore, note events align precisely with beat subdivisions, whereas human musicians introduce expressive timing deviations. In contrast, our model utilizes an event-based representation with independent time shift tokens at an 8-millisecond resolution. This resolution is sufficiently fine to capture musical nuance while maintaining absolute time rather than rigid beat subdivisions**Huang et al., "Learning Representations for Multi-Modal Sequence Data"**. Additionally, unlike CMT, our model integrates high-level emotional conditioning alongside low-level temporal features.

In a follow-up to CMT, Zhuo et al. introduced the Symbolic Music Videos dataset, containing video-MIDI pairs~\shortcite{zhuo}. They sourced MIDI data from piano tutorial videos, automatically transcribing audio using the Onsets and Frames model, resulting in 1140 samples**Huang et al., "Automatic Music Transcription of Polyphonic Music"**. Video features such as color histograms, RGB frame differences for motion, and high-level CLIP features**Radford et al., "Learning Transferable Visual-Semantic Representations from Large Pre-Training Text to Small Finetuning Tasks"** were extracted to train three models sequentially: one generating chord sequences, another generating the melody, and a third generating the accompaniment.

The Video2Music model shares similarities with our approach by utilizing both low-level video features and high-level emotional conditioning but differs in application**Huang et al., "Learning Representations for Multi-Modal Sequence Data"**. The authors compiled the MuVi-Sync dataset, consisting of 748 music videos labeled with musical attributes like note density, loudness, chords, and key**Kumar et al., "Multi-Task Learning for Music Generation from Video"**. Their encoder-decoder transformer takes low- and high-level video features, along with user-provided primer chord and key, to generate chord sequences. These sequences are then arpeggiated using fixed patterns to create the final MIDI output. However, this method's reliance on a fixed time grid omits the subtle expressive timing found in human performances. Additionally, its use of fixed arpeggiation patterns limits musical diversity, and requiring user-defined chord and key inputs restricts accessibility for non-musicians.

\subsubsection{Evaluation methods}
\label{sec:related_eval}

Evaluation of video-based music generators is non-trivial. For objective evaluation, most works measure the difference between generated and ground-truth samples using metrics such as Pitch Class Histogram Entropy**Bello et al., "Automatic Music Structure Analysis"**, Grooving Pattern Similarity**Raffel et al., "Learning Representations for Multi-Modal Sequence Data"**, and Structureness Indicator**Srivastava et al., "Learning Representations for Multi-Modal Sequence Data"**.
% , and Number of Statistically-Different Bins**Huang et al., "Automatic Music Transcription of Polyphonic Music"**, 
% or functions like contrastive loss**Kumar et al., "Contrastive Loss for Music Generation from Video"**
When the dataset contains paired video and MIDI samples, this method is straightforward**Srivastava et al., "Learning Representations for Multi-Modal Sequence Data"**. For unpaired video and MIDI samples, objectively evaluating a model that generates MIDI from arbitrary videos is not feasible. 
% Su et al. evaluate individual modules in their multi-stage architecture using ground-truth data from training individual stages~\shortcite{rhythmicnet}. 

The authors of the Controllable Music Transformer, using unpaired datasets, evaluate output MIDIs generated unconditionally, without input video**Huang et al., "Learning Representations for Multi-Modal Sequence Data"**. 
They empirically show that unconditionally generated MIDI samples exhibit metrics closer to the unpaired MIDI dataset compared to video-conditioned output samples. This occurs because constraining the model with video structure causes deviations from intrinsic MIDI structures, and the best way to match a specific MIDI dataset’s metrics is to train exclusively on that dataset.
Subjective evaluation through user studies remains the most common, and often the only, viable method for assessing video-based MIDI generators**Kumar et al., "User Study for Video-Based Music Generation"**.