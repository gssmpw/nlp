%%%% ijcai25.tex

% \typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years
\usepackage{ijcai25}

% Use the pPostscript Timesfont!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}


% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% The following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% % Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Serkan Sulun$^1$
\and
Paula Viana$^1$$^2$\And
Matthew E. P. Davies
\affiliations
$^1$INESC TEC\\
$^2$ISEP, Polytechnic of Porto\\
\emails
\{serkan.sulun, paula.viana\}@inesctec.pt
}

% \author{
% Anonymous$^1$
% \and
% Anonymous$^1$$^2$\And
% Anonymous
% \affiliations
% $^1$Hidden\\
% $^2$Hidden\\
% \emails
% \{hidden, hidden\}@hidden
% }

% \fi

\begin{document}

\maketitle

\begin{abstract}
We introduce \mbox{EMSYNC}, a video-based symbolic music generation model that aligns music with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate and align musical chords with scene cuts. Unlike existing models, our approach retains event-based encoding, ensuring fine-grained timing control and expressive musical nuances. We also propose a mapping scheme to bridge the video emotion classifier, which produces discrete emotion categories, with the emotion-conditioned MIDI generator, which operates on continuous-valued valence-arousal inputs. In subjective listening tests, \mbox{EMSYNC} outperforms state-of-the-art models across all subjective metrics, for music theory-aware participants as well as the general listeners. 
% A demo of our model and output samples are available at \mbox{\href{https://em-sync.github.io/}{em-sync.github.io}}.

\end{abstract}

\section{Introduction}

The online distribution of user-generated multimedia content is expanding at an exponential rate thanks to affordable, high-quality recording equipment and video editing software~\cite{multimedia}. A key challenge in content creation is providing suitable soundtracks to enhance viewer engagement~\cite{soundtrack}. However, unauthorized use of commercially published music infringes copyright, preventing monetization for creators on platforms like YouTube. Alternatives such as purchasing music, hiring composers, or searching for royalty-free tracks are often costly, time-consuming, or fail to ensure proper synchronization with video content. Automatic video-based music generation offers a promising solution to this problem~\cite{di,kang,zhuo}. 

Existing approaches that generate music as audio waveforms lack editability~\cite{musicgen1,musicgen2}. This method compresses all stages of music production—composition, performance, editing, mixing, and mastering—into a single process, limiting creative control for professionals. By contrast, generating music in a symbolic format like MIDI (Musical Instrument Digital Interface) offers greater flexibility. MIDI functions as a digital music score, encoding instrument names, note pitches, durations, and velocities. Professionals can edit compositions, synthesize audio using digital audio workstations (DAWs), or record and refine performances.

This work focuses on generating music in MIDI format from arbitrary videos using deep neural networks (DNNs). We use ``MIDI" and ``music" interchangeably, as MIDI is our exclusive format. A major challenge in training video-to-MIDI DNNs is the absence of large-scale paired video-MIDI datasets. Existing datasets are either domain-specific, such as pianist hand videos~\cite{sighttosound}, or contain limited samples, around 1k~\cite{di,zhuo}. To address this, we develop a model that generates music for any video type, leveraging the Lakh MIDI Dataset—the largest available MIDI dataset with 176,581 samples—ensuring diverse and high-quality outputs~\cite{lmd}.

Since the Lakh MIDI dataset lacks corresponding videos, we adopt a two-stage approach: extracting video features relevant to music generation and using them as conditioning inputs. We name our model \textit{EMSYNC}, as it aligns music with video by matching their \textbf{em}otions, and \textbf{sync}hronizing their temporal boundaries. 
While our method is applicable to any selection of temporal boundary and emotion representation, we define musical boundaries using long-duration chords, video boundaries using scene cuts, and represent emotions with the valence-arousal model~\cite{valence_arousal}. Specifically, we extract scene cut locations from the input video and guide the music generator to produce long-duration chords near those locations that are rhythmically and harmonically compatible with the rest of the generated music. We also use a pretrained video emotion classifier to estimate discrete emotion probabilities~\cite{vemoclap}, map them to valence-arousal values, and condition our music generator accordingly.

Temporal conditioning in MIDI-generating models presents unique challenges. While deep transformer models can handle time-based data, such as videos, their sequence dimension correlates linearly with time due to a fixed frame rate~\cite{swin,vivit}. In contrast, MIDI is typically processed using an event-based representation, where sequence and time dimensions are not linearly correlated~\cite{event_encoding}. In event-based MIDI encoding, two primary token types exist: ``note" and ``time shift". Note tokens represent pitch, while time shift tokens advance the time axis, capturing both note durations and silences. Each time shift token specifies a time increment, forming a one-dimensional sequence where the position in the sequence does not directly correspond to the position in time. The key advantage of event-based encoding is the absence of a fixed time grid, allowing for subtle, expressive timing variations that reflect human musicianship. Unlike state-of-the-art video-based music generators that rely on fixed, coarse time grids~\cite{di,kang}, we introduce a novel temporal conditioning method that preserves event-based encoding, enabling fine-grained temporal control.

We also tackle the challenge of using emotions as an intermediary to link music generation to video. Sulun et. al~\shortcite{access} previously assigned valence-arousal emotion labels to samples in the Lakh MIDI dataset~\cite{lmd}, creating a training dataset two orders of magnitude larger than existing emotion-labeled MIDI datasets~\cite{midiemotion1,midiemotion2,midiemotion3}. However, the largest emotion-labeled video datasets primarily provide categorical labels~\cite{ekman6,eev}. 
To bridge this gap, we employ the VEMOCLAP model (Video EMOtion Classifier using Pretrained features)~\cite{vemoclap}, trained on the Ekman-$6$ dataset~\cite{ekman6}. Since VEMOCLAP outputs probabilities of categorical emotions, we map these to valence-arousal values using prior user study results on emotion~\cite{mapping}.

We compare our method to state-of-the-art video-based MIDI generators through subjective listening tests, where it outperforms all competing methods across all metrics for both music theory-aware participants and general listeners while also being the fastest. 
% Upon acceptance, we will also publicly release our code and trained models. 
Our key contributions include the following:

\begin{itemize}

    \item We develop a video-based MIDI generator that surpasses state-of-the-art models across all subjective metrics and participant demographics.
    \item We introduce boundary offsets for temporal conditioning in transformers, ensuring precise alignment between the input video and the output MIDI.
    \item We leverage insights from psychological studies to map discrete emotion categories to continuous valence-arousal values, enabling an emotional connection between the input video and the generated music.
    \item We design our model architecture to modify only the inputs to the transformer body, allowing future users to fine-tune it for other tasks and data structures seamlessly.

\end{itemize}


\section{Related work}

% \subsection{Music generation?}

% \subsection{Video analysis?}

% \subsection{Video-based audio generation?}

In this section, we introduce the video emotion classifier and the emotion-based MIDI generator used in our work. We then review existing video-based MIDI generation methods and discuss their evaluation approaches.


\subsection{Video emotion classification}

% We use the pretrained VEMOCLAP (Video EMOtion Classifier using Pretrained features) model to analyze the emotions of the input video~\cite{vemoclap}. 
% The model extracts features relevant to the video's underlying emotions using multiple publicly available pretrained models for automatic speech recognition (ASR)\cite{whisper}, optical character recognition (OCR)\cite{paddle}, facial expression classification~\cite{expression_classifier}, audio classification~\cite{beats}, and image understanding~\cite{clip}. 
The VEMOCLAP model exploits publicly available pretrained models for a multimodal video analysis and uses the resulting pretrained features to classify the emotions of arbitrary videos~\cite{vemoclap,trailer}. It employs pretrained models for automatic speech recognition (ASR), optical character recognition (OCR), facial expression classification, audio classification, and image understanding.
Textual features obtained from ASR and OCR are further processed using a text sentiment classifier. 
% To integrate and process these multimodal features, we train cross-attention layers~\cite{transformer} followed by a linear layer, which produces the final emotion probabilities. 
Cross-attention layers~\cite{transformer} integrate and process these multimodal features, and a linear layer produces the final emotion probabilities. 
The model is trained on the Ekman-6 dataset~\cite{ekman6}, consisting of 1637 videos labeled with the six basic emotions derived from the original work of Ekman: anger, disgust, fear, joy, sadness, and surprise~\shortcite{ekman}. 
% The architecture of VEMOCLAP is illustrated in Figure \ref{fig:vemoclap}. 
% For additional details, we refer readers to our original publications~\cite{vemoclap,trailer}.

\subsection{Emotion-based MIDI generation}
\label{sec:emotion_based}

Sulun et al.~\shortcite{access} collected valence and arousal values for songs in the Lakh Pianoroll Dataset \cite{lpd} and trained a conditional transformer using the resulting labeled dataset. To integrate continuous-valued valence and arousal features with discrete musical note tokens, they project valence and arousal into a vector space using separate linear layers. 
% If valence or arousal is unspecified, they assign it a NaN value and use a learned vector instead of a projection. 
Musical input tokens are projected into vectors of the same dimensionality using an embedding layer. These vectors are then concatenated with the projected valence and arousal vectors along the sequence dimension and fed into the transformer body with relative global attention~\cite{musictransformer}. 

% We opted for this simpler input modification approach—rather than injecting valence and arousal values directly into the transformer body—to prevent an increase in model size and to allow users to fine-tune the model seamlessly, whether with or without valence/arousal conditioning or using alternative conditions. The emotion-conditioning mechanism can be seen in the upper part of Figure \ref{fig:music}.

\subsection{Video-based MIDI generation}

While our method applies to arbitrary videos, several works focus on generating symbolic music for specific types of videos, such as those featuring human movements like dancing or instrumental performances. The Foley Music model~\cite{foley} generates MIDI from videos of musicians by processing body keypoint movements using a Graph Convolutional Network~\cite{gcn} and a Transformer~\cite{transformer}. Similarly, Koepke et al.~\shortcite{sighttosound} and Su et al.~\shortcite{audeo} utilize use deep neural networks with residual connections to generate symbolic music from videos of finger movements on piano keyboards. Due to their specialized nature, these approaches rely on datasets containing video-MIDI pairs, though these datasets typically contain fewer than 1k samples~\cite{music_dataset,video_midi_dataset,sighttosound,audeo}. The RhythmicNet model employs a multi-stage process to generate music from dance videos by predicting beats and style, generating a drum track, and subsequently creating multitrack music~\cite{rhythmicnet}.

Some studies explore the more general task of generating symbolic music for arbitrary videos. The most similar to our approach, the Controllable Music Transformer (CMT), generates music based on video features such as motion speed, motion saliency, and timing~\cite{di}. CMT employs the Lakh Pianoroll Dataset \cite{lpd}, the same dataset we use, and processes music using an extended Compound Word representation, where each token encodes type, beat/bar marking, note strength, note density, instrument, pitch, and duration~\cite{compound_words}. While this representation reduces sequence length, it significantly increases input dimensionality. Consequently, CMT uses a low temporal resolution of four ticks per beat, translating to a time resolution of 125 milliseconds for a song at 120 beats per minute. Furthermore, note events align precisely with beat subdivisions, whereas human musicians introduce expressive timing deviations. In contrast, our model utilizes an event-based representation with independent time shift tokens at an 8-millisecond resolution. This resolution is sufficiently fine to capture musical nuance while maintaining absolute time rather than rigid beat subdivisions~\cite{event_encoding}. Additionally, unlike CMT, our model integrates high-level emotional conditioning alongside low-level temporal features.

In a follow-up to CMT, Zhuo et al. introduced the Symbolic Music Videos dataset, containing video-MIDI pairs~\shortcite{zhuo}. They sourced MIDI data from piano tutorial videos, automatically transcribing audio using the Onsets and Frames model, resulting in 1140 samples~\cite{onset_and_frames}. Video features such as color histograms, RGB frame differences for motion, and high-level CLIP features~\cite{clip} were extracted to train three models sequentially: one generating chord sequences, another generating the melody, and a third generating the accompaniment.

The Video2Music model shares similarities with our approach by utilizing both low-level video features and high-level emotional conditioning but differs in application~\cite{kang}. The authors compiled the MuVi-Sync dataset, consisting of 748 music videos labeled with musical attributes like note density, loudness, chords, and key. Their encoder-decoder transformer takes low- and high-level video features, along with user-provided primer chord and key, to generate chord sequences. These sequences are then arpeggiated using fixed patterns to create the final MIDI output. However, this method's reliance on a fixed time grid omits the subtle expressive timing found in human performances. Additionally, its use of fixed arpeggiation patterns limits musical diversity, and requiring user-defined chord and key inputs restricts accessibility for non-musicians.

\subsubsection{Evaluation methods}
\label{sec:related_eval}

Evaluation of video-based music generators is non-trivial. For objective evaluation, most works measure the difference between generated and ground-truth samples using metrics such as Pitch Class Histogram Entropy, Grooving Pattern Similarity, and Structureness Indicator~\cite{metrics}.
% , and Number of Statistically-Different Bins~\cite{ndb},
% or functions like contrastive loss~\cite{contrastive}
When the dataset contains paired video and MIDI samples, this method is straightforward~\cite{foley,zhuo}. For unpaired video and MIDI samples, objectively evaluating a model that generates MIDI from arbitrary videos is not feasible. 
% Su et al. evaluate individual modules in their multi-stage architecture using ground-truth data from training individual stages~\shortcite{rhythmicnet}. 

The authors of the Controllable Music Transformer, using unpaired datasets, evaluate output MIDIs generated unconditionally, without input video~\cite{di}. 
They empirically show that unconditionally generated MIDI samples exhibit metrics closer to the unpaired MIDI dataset compared to video-conditioned output samples. This occurs because constraining the model with video structure causes deviations from intrinsic MIDI structures, and the best way to match a specific MIDI dataset’s metrics is to train exclusively on that dataset.
Subjective evaluation through user studies remains the most common, and often the only, viable method for assessing video-based MIDI generators~\cite{di,kang,zhuo,foley,rhythmicnet}.


\section{Methodology}

In this section, we present our video-based symbolic music generator. We match the output music to the input video based on emotions and temporal boundaries. Since there are no datasets where videos are paired with symbolic music, we employ a two-stage approach, i.e., use independent modules for video analysis and conditional music generation, and then combine them. Our pipeline is shown in Figure \ref{fig:model}. We first highlight our music generator and how it is conditioned on emotions and temporal boundaries. We then describe how we incorporate temporal and emotional video features into our conditional music generator.

\subsection{Conditional music generator}

Our music generator is conditioned on emotions as valence-arousal values and temporal boundaries. Figure \ref{fig:music} illustrates the model, with the emotion-conditioning mechanism in the upper part and the boundary-conditioning mechanism in the lower part.

\begin{figure}[t]
    \centering    \includegraphics[width=0.8\columnwidth]{_fig_model.pdf}
    \caption{Video-based music generation pipeline. The text and the image next to the arrows demonstrate sample values.}
    \label{fig:model}
\end{figure}

\subsubsection{Training dataset and preprocessing}

We train our music generator on the Lakh Pianoroll Dataset (LPD)~\cite{lpd}, which contains 174,154 pianorolls derived from the Lakh MIDI Dataset~\cite{lmd}. We tokenize the pianorolls using an event-based symbolic music representation~\cite{event_encoding}. Specifically, an \texttt{ON} (note on) token marks the start of a note, and an \texttt{OFF} (note off) token marks its end. These tokens also encode pitch and instrument information. For example, a piano note with a MIDI pitch of 60 (C4) is denoted as \texttt{PIANO\_ON\_60}. Since a larger number of instruments increases vocabulary size, we use the Lakh Pianoroll Dataset-5 variant, where all instrument tracks are merged into five predefined categories: bass, drums, guitar, piano, and strings \cite{lpd}. However, our method is adaptable to datasets with different instrument groupings.

\texttt{TIMESHIFT} tokens are used to move along the time axis, representing both note durations and the silences between them. Each token specifies a time increment in milliseconds. For example, an 800-millisecond shift is encoded as \texttt{TIMESHIFT\_800}. We use a temporal resolution of 8 milliseconds with a maximum shift of 1000 milliseconds. Longer durations are represented using multiple consecutive \texttt{TIMESHIFT} tokens. 
We also use the \texttt{START} tokens to mark the beginning of the songs, the \texttt{BAR} tokens to indicate the musical bars, and the \texttt{PAD} tokens to standardize input sequence lengths in minibatches.
We use the \texttt{CHORD} token to mark long-duration chords as temporal boundaries, as they often serve as anchor points in musical compositions~\cite{chords}. However, this is a design choice, and our method can accommodate any selection of temporal boundaries.

% Since we aim to generate multi-instrument compositions, we prioritize pieces with three or more instruments. However, the dataset contains many songs with only one or two instruments. Rather than filtering them out, we prepend special tokens: \texttt{FEWER\_INSTRUMENTS} for songs with two or fewer instruments and \texttt{MORE\_INSTRUMENTS} for those with three or more. These tokens allow users to specify instrumentation preferences at inference time while leveraging the entire dataset during training.

For emotion-conditioned generation, we use the model architecture and valence-arousal labeled dataset of Sulun et al., detailed in Section \ref{sec:emotion_based}, which includes 23,661 samples~\shortcite{access}. To utilize the remaining unlabeled samples, we further modify the model to accept NaN (Not A Number) values for valence or arousal, enabling both conditional and unconditional generation. If valence or arousal is unspecified, we assign it a NaN value and use a learned vector in place of its projection vector. 



\begin{figure}[t] 
    \centering
    \includegraphics[width=0.9\columnwidth]{_fig_music.pdf}
    \caption{Our music generator. Numbers underneath valence and arousal are sample values.}
    \label{fig:music}
\end{figure}



\subsubsection{Temporal boundary matching}
\label{sec:boundary}
In addition to matching emotions, we also temporally synchronize the input video and output music by aligning their temporal boundaries. We define temporal boundaries as scene cut locations in video and chord locations in music. Since we employ a hybrid approach, we first train a music generator capable of incorporating boundary locations as input and producing music with chords near these boundaries. To achieve this, we label the chords in the Lakh Pianoroll Dataset. During training, we consider only guitar and piano chords with at least three simultaneous notes that last for a minimum of two beats. These chords are labeled by inserting a \texttt{CHORD} token before the first \texttt{ON} token of each chord. 
% To prevent the model from becoming overly reliant on the \texttt{CHORD} token for chord generation, we randomly remove 20\% of these tokens during training. 
To allow the model to generate chords not only at video scene cut locations but also independently where musically appropriate, we randomly remove 20\% of \texttt{CHORD} tokens during training.
An example of a labeled chord is shown in the top row of Figure \ref{fig:boundary}. 
In training, the temporal locations of chords from the ground-truth MIDI serve as input boundaries, whereas during video-based inference, we replace these boundaries with video scene cut locations.

\begin{figure*}[t]  % 't' places the figure at the top of the page
    \centering
    \includegraphics[width=0.9\textwidth]{_fig_boundary.pdf}  % Adjust the width as necessary
    \caption{Graphical illustration of boundaries. Top: In symbolic music, a chord with three or more simultaneous notes and a duration exceeding a set threshold defines a musical boundary and is used during training. Middle: In video, scene cuts serve as video boundaries and are used during inference. Bottom: Boundary offset represents the temporal distance to the next boundary. These figures are illustrative; offsets do not perfectly align with the music except at the boundary.}
    \label{fig:boundary}
\end{figure*}

Chords are integral to a melody, providing harmonic and rhythmic support to surrounding notes, both preceding and following~\cite{chords}. During inference, forcing a \texttt{CHORD} token into the sequence at a specific location may cause the chord to sound off-beat or overly abrupt. This occurs because the model, having no prior knowledge of the upcoming chord, may generate preceding notes that do not align naturally with it. To address this, we develop a method that enables the model to ``anticipate" upcoming chords and generate preceding notes and time shifts accordingly. Additionally, we train the model to generate the \texttt{CHORD} token itself, ensuring rhythmic consistency in the generated music. To achieve this, we define \textit{boundary offsets} for each input token, representing the remaining time until the next boundary. These offsets are capped at a maximum value and, since our music generator is autoregressive, they are computed based on future chords rather than past ones. Furthermore, we amplify the loss of the \texttt{CHORD} token by a factor of 10, as it influences multiple preceding chord notes and plays a critical role in structuring the generated music. Sample offsets are illustrated in the bottom row of Figure \ref{fig:boundary}.

We process boundary offsets through a feed-forward network, generating boundary offset encodings, which are then concatenated with learned positional encodings \cite{learned_position} along the feature dimension, as shown in the lower part of Figure \ref{fig:music}. This architectural choice is motivated by several key factors. First, similar to the emotion-conditioning mechanism, we inject boundary offset encodings at the input level rather than within the transformer body, ensuring that the core model remains unchanged. This allows the model to process inputs with or without boundary offsets, enabling seamless fine-tuning by repeating the learned positional encodings along the feature dimension when boundary offsets are absent. Second, we avoid adding boundary offset encodings directly to learned positional encodings to maintain a distinction between the two. Finally, recent studies suggest that decoder-only transformers can implicitly learn positional encodings through their internal weights, even without explicitly adding them~\cite{position1,position2}. Based on this insight, we halve the feature length of learned positional encodings and allocate the remaining feature space to boundary offset encodings. The resulting vector sequence consists of positional encodings augmented with boundary offset encodings. Following the standard transformer model, we add this sequence to the token embeddings~\cite{transformer} before passing it to the transformer body with relative global attention~\cite{musictransformer}. The transformer's input is represented as:

\begin{equation*}
\begin{split}
    \mathbf{X}_{\text{trans}} &= \text{Concat}_s\left( \text{FFN}_v(x_v), \text{FFN}_a(x_a), \text{Embedding}(\mathbf{x}_t) \right) \\
    &
    % \quad 
    + \text{Concat}_f(\text{FFN}_b(\mathbf{b}) + \mathbf{W_{pe}})
\end{split}
\end{equation*}



where \( x_v \) and \( x_a \) are the valence and arousal inputs, \( \mathbf{x}_t \) is the input token sequence, \( \mathbf{b} \) represents the input boundary offsets, and \( \mathbf{W_{pe}} \) is the learned positional encoding. The feed-forward networks \( \text{FFN}_v \), \( \text{FFN}_a \), and \( \text{FFN}_b \) process the valence, arousal, and boundary offset inputs, respectively. The operations \( \text{Concat}_s \) and \( \text{Concat}_f \) denote concatenation along the sequence and feature dimensions, respectively. Our music generator architecture is illustrated in Figure \ref{fig:music}. 
% For further details, readers may refer to our previous work~\cite{access}.


Algorithm \ref{alg:offset} outlines the process of computing boundary offsets during inference, i.e., music generation. The model generates music autoregressively, producing one token per forward pass. By tracking the generated \texttt{TIMESHIFT} tokens, we maintain a time cursor and compute the boundary offset, i.e., the time remaining until the next boundary, for each generated token. If the model generates a \texttt{CHORD} token, we calculate the absolute difference between the time cursor and each input boundary. The boundary with a difference smaller than the predefined sensitivity threshold of 1 second is considered successfully generated. We then remove these boundaries from future offset calculations by replacing them with infinity. Next, we compute the boundary offset for the generated token, regardless of its type. This offset is determined as the distance to the next closest boundary, capped by a maximum offset value. For simplicity, Algorithm \ref{alg:offset} is presented for a single sample, but in practice, this operation is performed in minibatches. During training, we preprocess the entire input sequence at once by constructing a time grid instead of a time cursor, allowing us to calculate boundary offsets for all tokens simultaneously. For clarity, the initial list of generated tokens is shown as empty; however, in practice, we begin with a \texttt{START} token.

The \texttt{ON} tokens that appear after a \texttt{CHORD} token and before the next \texttt{TIMESHIFT} token are considered notes of the generated chord. To make the generated chord more distinctive, we increase the velocity of these notes. Music generation continues until the time cursor reaches the duration of the video. After synthesizing the generated MIDI into an audio waveform, we apply a 3-second fade-out effect to prevent an abrupt ending and ensure the music matches the duration of the input video.

\begin{algorithm}[tb]
    \caption{Creating boundary offsets during music generation in inference.}
    \label{alg:offset}
    \textbf{Input}: List of input boundaries (in seconds), $\mathbf{b}$; video duration, $d$; valence, $x_v$; arousal $x_a$\\
    \textbf{Parameter}: Sensitivity, $\xi$; maximum offset, $\delta_{max}$; generated boundary mask, $\mathbf{m_b}$; token type $t_t$; token value, $t_v$; time cursor, $c$; generation function including forward-pass and sampling, g\\
    \textbf{Output}: List of generated tokens $\mathbf{t}$; list of boundary offsets, $\boldsymbol{\delta}$
    \begin{algorithmic}
        % \STATE $\mathbf{b}$.append($+\infty$)
        \STATE Let $c=0$; $\boldsymbol{\delta}=\left[ \hspace{2pt} \right]$; $\mathbf{t}=\left[ \hspace{2pt} \right]$.
        \WHILE{$c < d$}
            \STATE \texttt{\# generate token as (type, value):} 
            \STATE $(t_t, t_v) = g(\mathbf{t}, \boldsymbol{\delta}, x_v, x_a)$
            \IF {$t_t$ == \texttt{TIMESHIFT}}
                \STATE $c = c + t_v $
            \ELSIF {$t_t$ == \texttt{CHORD}}
                \STATE $\mathbf{m_b} = \left| t - \mathbf{b} \right| < \xi$.
                \STATE $\mathbf{b}\left[\mathbf{m_b}\right] = +\infty$.
            \ENDIF
            \STATE $\boldsymbol{\delta}$.append$(\max (\min (\mathbf{b} - c ), \delta_{max}))$
            \STATE $\mathbf{t}$.append$((t_t, t_v))$

        \ENDWHILE
    \end{algorithmic}
\end{algorithm}



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{_vemoclap.pdf}
%     \caption{Our video emotion classifier. Figure taken from Sulun et al. ~\protect\shortcite{vemoclap}.}
%     \label{fig:vemoclap}
% \end{figure}

\subsection{Scene detection}

During video-based inference, we first extract the video's temporal boundaries, specifically its scene cut locations, using the FFmpeg software\footnote{https://www.ffmpeg.org/}. We then apply a difference filter to the extracted scene cuts and remove those occurring less than 4 seconds apart. The remaining timestamps serve as the input boundaries for the music generator.


\subsection{Emotion matching}

We use the VEMOCLAP emotion classifier to extract emotions from input videos as probability distributions over discrete emotion categories~\cite{vemoclap}. 
The music generator is conditioned on emotions represented as valence and arousal values ranging from -1 to 1, rather than discrete categories~\cite{access}. Since the VEMOCLAP emotion classifier outputs discrete emotion categories, we first map them to the valence-arousal plane~\cite{valence_arousal} before feeding them into our music generator.

% \subsubsection{Mapping emotions to valence-arousal plane}

Russell and Mehrabian conducted user studies to find the corresponding valence and arousal values of discrete emotion categories~\shortcite{mapping}. They presented their findings in a table that contains the mean and standard deviation values of valence and arousal for each categorical emotion. We extracted and used the values corresponding to Ekman's six basic emotions, which are presented in Table \ref{tab:mapping}~\cite{ekman}.

% \begin{table}[h!]
% \centering
% \begin{tabular}{|l|cc|cc|}
% \hline
%  & \multicolumn{2}{c|}{\textbf{Valence}} & \multicolumn{2}{c|}{\textbf{Arousal}} \\ 
%    \textbf{Emotion} & \textbf{Mean} & \textbf{SD}         & \textbf{Mean} & \textbf{SD}         \\ \hline
% anger         & -0.51             & 0.20   & \phantom{-}0.59          & 0.29                \\ \hline
% disgust       & -0.60             & 0.20   & \phantom{-}0.35          & 0.41                \\ \hline
% fear          & -0.64             & 0.20   & \phantom{-}0.60           & 0.32                \\ \hline
% joy           & \phantom{-}0.76   & 0.22   & \phantom{-}0.48          & 0.26                \\ \hline
% sadness       & -0.63             & 0.23   & -0.27                    & 0.34                \\ \hline
% surprise      & \phantom{-}0.40   & 0.30   & \phantom{-}0.67          & 0.27                \\ \hline
% \end{tabular}
% \caption{Means and standard deviations of valence and arousal values corresponding to Ekman's categorical emotions~\protect\cite{ekman}, obtained from the user studies of Russell and Mehrabian~\protect\shortcite{mapping}.}
% \label{tab:mapping}
% \end{table}

\begin{table}[t]
\centering
\begin{tabular}{l cc cc}
\toprule
 & \multicolumn{2}{c}{\textbf{Valence}} & \multicolumn{2}{c}{\textbf{Arousal}} \\ 
 \cmidrule(lr){2-3} \cmidrule(lr){4-5}
   \textbf{Emotion} & \textbf{Mean} & \textbf{SD}         & \textbf{Mean} & \textbf{SD}         \\ 
   \midrule
anger         & -0.51             & 0.20   & \phantom{-}0.59          & 0.29                \\ 
disgust       & -0.60             & 0.20   & \phantom{-}0.35          & 0.41                \\ 
fear          & -0.64             & 0.20   & \phantom{-}0.60           & 0.32                \\ 
joy           & \phantom{-}0.76   & 0.22   & \phantom{-}0.48          & 0.26                \\ 
sadness       & -0.63             & 0.23   & -0.27                    & 0.34                \\ 
surprise      & \phantom{-}0.40   & 0.30   & \phantom{-}0.67          & 0.27                \\ 
\bottomrule
\end{tabular}
\caption{Means and standard deviations of valence and arousal values corresponding to Ekman's categorical emotions~\protect\cite{ekman}, obtained from the user studies of Russell and Mehrabian~\protect\shortcite{mapping}.}
\label{tab:mapping}
\end{table}

Using the output probabilities of the video classifier along with the means and standard deviations of each emotion, we construct a mixture of Gaussian distributions. Users can either sample from the mixture—first selecting an emotion category based on the output probabilities and then sampling from the corresponding Gaussian distribution—or use the mean of the mixture, which is computed as the weighted average of the emotion category means, with weights determined by the output probabilities. Additionally, we introduce a parameter that represents the maximum absolute value of means across all emotion categories. Using this parameter, we compute a single scaling coefficient applied to all valence-arousal distributions. This approach allows users to adjust the conditioning of the music generator, choosing between a wider or narrower range of emotions.

\begin{table*}[t]
    \centering
    \begin{tabular}{lccccc ccccc c}
        \toprule
        & \multicolumn{5}{c}{Group 1} & \multicolumn{5}{c}{Group 2} & \\
        \cmidrule(lr){2-6} \cmidrule(lr){7-11}
        & MR & MQ & EM & TM & OM & MR & MQ & EM & TM & OM & RT \\
        \midrule
        Video2Music   & 2.521 & 1.854 & 2.125 & 1.958 & 2.167 & 2.708 & 1.771 & 1.854 & 1.979 & 2.083 & 1.607 \\
        CMT           & 1.854 & 2.438 & 2.000 & 2.125 & 2.083 & 1.792 & 2.562 & 2.458 & 2.312 & 2.354 & 3.554 \\
        EMSYNC (Ours) & \textbf{1.625} & \textbf{1.708} & \textbf{1.875} & \textbf{1.917} & \textbf{1.750} & \textbf{1.500} & \textbf{1.667} & \textbf{1.688} & \textbf{1.708} & \textbf{1.562} & \textbf{1.417} \\
        \bottomrule
    \end{tabular}
    \caption{Group 1 consists of participants with a self-reported understanding of music theory, while Group 2 includes those without it. We use the following abbreviations: MR (music richness), MQ (music quality), EM (emotion match), TM (timing match), OM (overall match), 
    % O (originality), 
    and RT (runtime). Runtime measures the minutes required to generate music for a one-minute video. All other metrics represent average rankings, where 1 is the best possible and 3 is the worst possible ranking. Across all metrics, lower is better.}
    \label{tab:formatted}
\end{table*}




\section{Experimental setup}

As discussed in Section \ref{sec:related_eval}, conducting an objective evaluation is challenging when using unmatched video and MIDI datasets. We therefore focus on the subjective evaluation of our overall model. We conduct a user study to compare our results with two open-source state-of-the-art models that generate MIDI from arbitrary videos: Video2Music~\cite{kang} and CMT~\cite{di}.

\subsection{Dataset}

We train our music generator on the Lakh Pianoroll-5 dataset~\cite{lpd} with valence-arousal labels~\cite{access}. For evaluation, we perform inference on videos from the Pittsburgh Advertisements Dataset (Ads)~\cite{ads}. We select this dataset because advertisements often rely on music to maximize viewer engagement~\cite{ads_music}. Additionally, since none of the compared methods have used this dataset, it ensures a fair comparison. Finally, the Ads dataset includes sentiment labels, allowing us to construct a well-balanced test set in terms of emotion.

We first filter the dataset by selecting videos associated with the four basic emotions commonly used in music emotion classification: \textit{cheerful}, \textit{calm}, \textit{angry}, and \textit{sad}~\cite{music_emotion}. We then remove videos shorter than 1 minute. To ensure an unbiased selection, we use YouTube IDs, which are generated randomly by YouTube. We sort these IDs alphabetically and select the first six videos from each emotion category, resulting in a total of 24 test videos. Finally, we trim all videos to a uniform duration of 1 minute.

\subsection{Implementation details}

We use the Music Transformer model with relative global attention in our music generator~\cite{musictransformer}. The model consists of 11 layers, 8 attention heads, and a dimensionality of 512. We train it with a context length of 1216 and a batch size of 64. Training is performed using cross-entropy loss with a learning rate of 2e-4 for the first 300k steps, followed by 5e-5 for the next 300k steps. We use the Adam optimizer with gradient clipping to a norm of 1~\cite{adam}. We do not apply any regularization methods and did not observe overfitting, as the model is trained on large-scale, densely labeled data where it predicts each token of its input sequence~\cite{masters}.

For data augmentation, we transpose the pitches of all instruments, except drums, by a randomly chosen integer between -3 and 3, inclusive. We set our model size to be comparable to the baseline methods, with our model containing 37M parameters, compared to 39M in CMT~\cite{di} and 33M in Video2Music~\cite{kang}. We set the maximum absolute value of means across all emotion categories to 0.8.
For all models, we synthesize MIDI files into audio waveforms using Fluidsynth\footnote{https://www.fluidsynth.com} and apply peak audio normalization up to -3 dB.

\subsection{Subjective evaluation}

Using the 24 test videos, we generate accompanying music with the three models and create 12 survey pages, each containing two videos. For each video, the three music versions, generated by the compared models, are presented side by side with anonymized model names, with each model's output appearing equally in the left, center, and right positions. We enroll 36 participants: 12 with self-reported knowledge of music theory, forming Group 1, and 24 without, forming Group 2. Each survey page and each video are viewed by one participant from Group 1 and two participants from Group 2. 
% For readers, we also provide output samples from eight videos online, with model names deanonymized and each model's output appearing in the same position for clarity.\footnote{https://em-sync.github.io/}

% Across the 12 survey pages, 

% Within a single survey page, the positions of each model's outputs remain consistent across both videos.

We ask participants to rank the three models based on the standard criteria used in previous works~\cite{di,kang}: \textit{Music richness (MR)}: The richness and diversity of the music, independent of the video content. \textit{Music quality (MQ)}: The overall quality of the music, independent of the video content. \textit{Emotion match (EM)}: How well the music matches the video in terms of emotion. \textit{Timing match (TM)}: How well the music synchronizes with the video in terms of rhythm and timing. \textit{Overall match (OM)}: How well the music matches the video as a whole. Each model is assigned a unique ranking of 1 (best), 2, or 3 (worst), ensuring no ties. Finally, we report \textit{runtimes (RT)} as the average time (in minutes) required to generate music for a one-minute video, including model initialization.


\section{Results and conclusion}

Table \ref{tab:formatted} compares EMSYNC, Video2Music~\cite{kang}, and CMT~\cite{di} across all subjective criteria based on average rankings. A lower score represents a better ranking, with 1 being the best and 3 the worst possible.

EMSYNC consistently outperforms both baselines across all metrics for both Group 1 (music theory-aware) and Group 2. Group 2 rated EMSYNC even higher, particularly in music richness and emotion match, indicating that its strengths are even more apparent to general listeners. EMSYNC is also the fastest, with a runtime of 1.417 minutes, compared to 1.607 minutes for Video2Music and 3.554 minutes for CMT. 

These results confirm that EMSYNC produces the most diverse and high-quality music with the best emotional alignment, rhythmic synchronization, and overall video compatibility while also being the most computationally efficient model. 

By automatically generating soundtracks for user-provided videos, our work has the potential to streamline video production and enhance viewer engagement while offering a valuable framework for both machine learning researchers and multimedia content creators.

% In future work, we will leverage labeled audio samples and use multi-instrument transcription models to obtain corresponding MIDI files, expanding our labeled MIDI dataset~\cite{transcription}. 
% Additionally, we will explore video-based music generation in the audio domain.



\clearpage

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{_references}

\end{document}

