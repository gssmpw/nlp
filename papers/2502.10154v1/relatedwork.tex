\section{Related work}
% \subsection{Music generation?}

% \subsection{Video analysis?}

% \subsection{Video-based audio generation?}

In this section, we introduce the video emotion classifier and the emotion-based MIDI generator used in our work. We then review existing video-based MIDI generation methods and discuss their evaluation approaches.


\subsection{Video emotion classification}

% We use the pretrained VEMOCLAP (Video EMOtion Classifier using Pretrained features) model to analyze the emotions of the input video~\cite{vemoclap}. 
% The model extracts features relevant to the video's underlying emotions using multiple publicly available pretrained models for automatic speech recognition (ASR)\cite{whisper}, optical character recognition (OCR)\cite{paddle}, facial expression classification~\cite{expression_classifier}, audio classification~\cite{beats}, and image understanding~\cite{clip}. 
The VEMOCLAP model exploits publicly available pretrained models for a multimodal video analysis and uses the resulting pretrained features to classify the emotions of arbitrary videos~\cite{vemoclap,trailer}. It employs pretrained models for automatic speech recognition (ASR), optical character recognition (OCR), facial expression classification, audio classification, and image understanding.
Textual features obtained from ASR and OCR are further processed using a text sentiment classifier. 
% To integrate and process these multimodal features, we train cross-attention layers~\cite{transformer} followed by a linear layer, which produces the final emotion probabilities. 
Cross-attention layers~\cite{transformer} integrate and process these multimodal features, and a linear layer produces the final emotion probabilities. 
The model is trained on the Ekman-6 dataset~\cite{ekman6}, consisting of 1637 videos labeled with the six basic emotions derived from the original work of Ekman: anger, disgust, fear, joy, sadness, and surprise~\shortcite{ekman}. 
% The architecture of VEMOCLAP is illustrated in Figure \ref{fig:vemoclap}. 
% For additional details, we refer readers to our original publications~\cite{vemoclap,trailer}.

\subsection{Emotion-based MIDI generation}
\label{sec:emotion_based}

Sulun et al.~\shortcite{access} collected valence and arousal values for songs in the Lakh Pianoroll Dataset \cite{lpd} and trained a conditional transformer using the resulting labeled dataset. To integrate continuous-valued valence and arousal features with discrete musical note tokens, they project valence and arousal into a vector space using separate linear layers. 
% If valence or arousal is unspecified, they assign it a NaN value and use a learned vector instead of a projection. 
Musical input tokens are projected into vectors of the same dimensionality using an embedding layer. These vectors are then concatenated with the projected valence and arousal vectors along the sequence dimension and fed into the transformer body with relative global attention~\cite{musictransformer}. 

% We opted for this simpler input modification approach—rather than injecting valence and arousal values directly into the transformer body—to prevent an increase in model size and to allow users to fine-tune the model seamlessly, whether with or without valence/arousal conditioning or using alternative conditions. The emotion-conditioning mechanism can be seen in the upper part of Figure \ref{fig:music}.

\subsection{Video-based MIDI generation}

While our method applies to arbitrary videos, several works focus on generating symbolic music for specific types of videos, such as those featuring human movements like dancing or instrumental performances. The Foley Music model~\cite{foley} generates MIDI from videos of musicians by processing body keypoint movements using a Graph Convolutional Network~\cite{gcn} and a Transformer~\cite{transformer}. Similarly, Koepke et al.~\shortcite{sighttosound} and Su et al.~\shortcite{audeo} utilize use deep neural networks with residual connections to generate symbolic music from videos of finger movements on piano keyboards. Due to their specialized nature, these approaches rely on datasets containing video-MIDI pairs, though these datasets typically contain fewer than 1k samples~\cite{music_dataset,video_midi_dataset,sighttosound,audeo}. The RhythmicNet model employs a multi-stage process to generate music from dance videos by predicting beats and style, generating a drum track, and subsequently creating multitrack music~\cite{rhythmicnet}.

Some studies explore the more general task of generating symbolic music for arbitrary videos. The most similar to our approach, the Controllable Music Transformer (CMT), generates music based on video features such as motion speed, motion saliency, and timing~\cite{di}. CMT employs the Lakh Pianoroll Dataset \cite{lpd}, the same dataset we use, and processes music using an extended Compound Word representation, where each token encodes type, beat/bar marking, note strength, note density, instrument, pitch, and duration~\cite{compound_words}. While this representation reduces sequence length, it significantly increases input dimensionality. Consequently, CMT uses a low temporal resolution of four ticks per beat, translating to a time resolution of 125 milliseconds for a song at 120 beats per minute. Furthermore, note events align precisely with beat subdivisions, whereas human musicians introduce expressive timing deviations. In contrast, our model utilizes an event-based representation with independent time shift tokens at an 8-millisecond resolution. This resolution is sufficiently fine to capture musical nuance while maintaining absolute time rather than rigid beat subdivisions~\cite{event_encoding}. Additionally, unlike CMT, our model integrates high-level emotional conditioning alongside low-level temporal features.

In a follow-up to CMT, Zhuo et al. introduced the Symbolic Music Videos dataset, containing video-MIDI pairs~\shortcite{zhuo}. They sourced MIDI data from piano tutorial videos, automatically transcribing audio using the Onsets and Frames model, resulting in 1140 samples~\cite{onset_and_frames}. Video features such as color histograms, RGB frame differences for motion, and high-level CLIP features~\cite{clip} were extracted to train three models sequentially: one generating chord sequences, another generating the melody, and a third generating the accompaniment.

The Video2Music model shares similarities with our approach by utilizing both low-level video features and high-level emotional conditioning but differs in application~\cite{kang}. The authors compiled the MuVi-Sync dataset, consisting of 748 music videos labeled with musical attributes like note density, loudness, chords, and key. Their encoder-decoder transformer takes low- and high-level video features, along with user-provided primer chord and key, to generate chord sequences. These sequences are then arpeggiated using fixed patterns to create the final MIDI output. However, this method's reliance on a fixed time grid omits the subtle expressive timing found in human performances. Additionally, its use of fixed arpeggiation patterns limits musical diversity, and requiring user-defined chord and key inputs restricts accessibility for non-musicians.

\subsubsection{Evaluation methods}
\label{sec:related_eval}

Evaluation of video-based music generators is non-trivial. For objective evaluation, most works measure the difference between generated and ground-truth samples using metrics such as Pitch Class Histogram Entropy, Grooving Pattern Similarity, and Structureness Indicator~\cite{metrics}.
% , and Number of Statistically-Different Bins~\cite{ndb},
% or functions like contrastive loss~\cite{contrastive}
When the dataset contains paired video and MIDI samples, this method is straightforward~\cite{foley,zhuo}. For unpaired video and MIDI samples, objectively evaluating a model that generates MIDI from arbitrary videos is not feasible. 
% Su et al. evaluate individual modules in their multi-stage architecture using ground-truth data from training individual stages~\shortcite{rhythmicnet}. 

The authors of the Controllable Music Transformer, using unpaired datasets, evaluate output MIDIs generated unconditionally, without input video~\cite{di}. 
They empirically show that unconditionally generated MIDI samples exhibit metrics closer to the unpaired MIDI dataset compared to video-conditioned output samples. This occurs because constraining the model with video structure causes deviations from intrinsic MIDI structures, and the best way to match a specific MIDI dataset’s metrics is to train exclusively on that dataset.
Subjective evaluation through user studies remains the most common, and often the only, viable method for assessing video-based MIDI generators~\cite{di,kang,zhuo,foley,rhythmicnet}.