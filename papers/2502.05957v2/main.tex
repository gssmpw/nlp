\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}

\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[preprint]{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green
            ]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bbm}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{footnote}
\usepackage{wrapfig}
\usepackage{fontawesome}
\usepackage{float}
\usepackage{tabularray}
% \usepackage{caption}
% Tableau colors
\definecolor{tblue}{RGB}{31,119,180}
\definecolor{torange}{RGB}{255,127,14}
\definecolor{tgreen}{RGB}{44,160,44}
\definecolor{tred}{RGB}{214,39,40}
\definecolor{tpurple}{RGB}{148,103,189}
\definecolor{lightblue}{RGB}{173, 216, 230}
\definecolor{lightpink}{RGB}{255, 182, 193}
\definecolor{lightgreen}{RGB}{144, 238, 144}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{array}

\newcommand{\hide}[1]{} %hide
\newcommand{\vpara}[1]{\vspace{0.05in}\noindent\textbf{#1 }}
\newcommand{\noind}[1]{\hspace{-0.05in}\noindent{#1}}
\newcommand{\etal}{\textit{et al}.}
\newcommand{\beq}[1]{\vspace{-0.03in}\begin{equation}#1\end{equation}\vspace{-0.02in}}
\newcommand{\beqn}[1]{\vspace{-0.03in}\begin{eqnarray}#1\end{eqnarray}\vspace{-0.03in}}
\newcommand{\besp}[1]{\begin{split}#1\end{split}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.} 
\newcommand{\wrt}{\textit{w}.\textit{r}.\textit{t}} 

\def\model{AutoAgent}

\title{\model: A Fully-Automated and Zero-Code Framework for LLM Agents}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Jiabin Tang~~~
  Tianyu Fan~~~
  Chao Huang\thanks{Chao Huang is the Corresponding Author.} \\
  The University of Hong Kong \\
  \texttt{\{jiabintang77, tianyufan0504, chaohuang75\}@gmail.com} \\
  \faGithub~\textbf{Source Code:} \textcolor{blue}{\url{https://github.com/HKUDS/\model}}
}
\usepackage{graphicx}
% \usepackage{subcaption}
\usepackage{longtable}
\usepackage{bookmark} 

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{adjustbox}

\usepackage{xcolor} % 用于颜色支持

% 定义 XML 高亮样式
\lstdefinelanguage{XML}{
  moredelim=[s][\color{violet}\bfseries]{<}{>}, % 高亮 < 和 > 包围的内容
  morecomment=[s]{<!--}{-->}, % 注释
  morestring=[b]", % 字符串
  stringstyle=\color{blue!60}, % 字符串样式
  commentstyle=\color{green}\itshape, % 注释样式
}

\lstdefinelanguage{Prompt}{
  moredelim=[s][\color{violet}\bfseries]{`}{`}, % 高亮 < 和 > 包围的内容
  moredelim=[s][\color{violet}\bfseries]{<}{>}, % 高亮 < 和 > 包围的内容
  morecomment=[s]{<!--}{-->}, % 注释
  morestring=[b]", % 字符串
  stringstyle=\color{blue!60}, % 字符串样式
  commentstyle=\color{green}\itshape, % 注释样式
}

\lstdefinelanguage{Tools}{
literate={[}{{\color{violet}\bfseries[}}1
           {]}{{\color{violet}\bfseries]}}1, % 将 [ 和 ] 设置为紫色
  morecomment=[s]{<!--}{-->},  % 注释
  morestring=[b]",  % 字符串
  stringstyle=\color{cyan!60},  % 字符串样式
  commentstyle=\color{green}\itshape,  % 注释样式
}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

% \icmltitlerunning{MetaChain: A Generalist Agent System with Language Programming}

\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother

\begin{document}

\maketitle

\begin{figure*}[h]
\vspace{-0.15in}
    \begin{center}
    \includegraphics[width=1.\textwidth]{figs/autoagent-intro-final.pdf}
    \end{center}
    \vspace{-0.15in}
    \caption{\model\ stands out as a new LLM Agent Framework that enables fully automated, zero-code development for complex task automation. Ranking \#1 among open-source solutions on the GAIA benchmark, it delivers state-of-the-art RAG performance as a general AI assistant. Its revolutionary approach democratizes AI development - allowing anyone, regardless of coding experience, to create and customize their own agents, tools, and workflows with ease.}
    \label{fig:intro}
    % \vspace{-1.0em}
\end{figure*}

\begin{abstract}
Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise—a significant limitation considering that only 0.03\% of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: \emph{Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone?} To address this challenge, we introduce \model\ - a \textbf{Fully-Automated} and highly \textbf{Self-Developing} framework that enables users to create and deploy LLM agents through \textbf{Natural Language Alone}. Operating as an autonomous Agent Operating System, \model\ comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, \model\ also serves as a versatile multi-agent system for \textbf{General AI Assistants}. Comprehensive evaluations on the GAIA benchmark demonstrate \model's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, \model's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions. 
% The \model\ framework is publicly open-sourced at: \textcolor{blue}{\url{https://github.com/HKUDS/MetaChain}}.
\end{abstract}

% \clearpage

\input{intro}
\input{preliminary}
\input{solution}
\input{evaluation}
% \input{relate} 
\input{conclusion}

\clearpage

\bibliographystyle{unsrtnat}
\bibliography{neurips_2024}

\clearpage

\input{appendix}

\end{document}