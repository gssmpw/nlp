\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%\usepackage{hyperref}
\usepackage{url}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage[colorlinks=true,citecolor=apolloblue,linkcolor=apolloblue,urlcolor=apolloblue]{hyperref}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{float}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithmic}


\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}


\graphicspath{ {./figures/} }

\title{Forecasting Frontier Language Model Agent Capabilities}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{% core contributors
Govind Pimpale\thanks{Equal contribution}\\\textmd{MATS}
\And
Axel H{\o}jmark\footnotemark[1]\\\textmd{MATS \& Apollo Research}
\AND
% senior authors
J\'er\'emy Scheurer\thanks{Equal contribution}\\\textmd{Apollo Research}
\And
Marius Hobbhahn\footnotemark[2]\\\textmd{Apollo Research}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\vspace{0.3in} 
\begin{abstract}
    As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. 
    We evaluate six forecasting methods that predict downstream capabilities of LM agents. 
    We use ``one-step'' approaches that predict benchmark scores from input metrics like compute or model release date directly or ``two-step'' approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings.
    We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. 
    We then use the validated two-step approach (Release Date$\to$Elo$\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks:
    SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). 
    Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54\% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87\% success rate.
    Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.
\end{abstract}

\input{sections/introduction}

\input{sections/methods}

\input{sections/backtesting}

\input{sections/agent_predictions}

\input{sections/related_work}

\input{sections/discussion}


\section{Conclusion}
There are three primary novel contributions from this paper. First, we focus on predicting frontier performance instead of average performance. Second, we use different data types than previous work, e.g. using Elo as an intermediate variable and using release date as an input. One advantage of using multiple techniques is that we can choose methods based on the availability of data, e.g. for frontier models, release date is known, while training compute isn't. Third, we focus on benchmarks specifically designed for LM agents while previous work has often focused on QA benchmarks. 

\subsubsection*{Author Contributions}
% List detailed author contributions here either sorted by author or by topic

Govind Pimpale and Axel H{\o}jmark ran all experiments and analyses. They also contributed substantially to the conceptual efforts. For example, AH came up with the idea of using Elo as an intermediate variable.
Jérémy Scheurer and Marius Hobbhahn co-supervised the project.
JS had the original idea for the project and developed the first roadmap.
GP and MH wrote the paper, supported by AH and JS.

% \textbf{Alexander Meinke}: Led the second half of the project, including the writing of the paper. Was responsible for the evaluations: Goal-guarding, covertly re-ranking emails, instrumental alignment faking, and sandbagging. 

% \textbf{Bronson Schoen}: Provided engineering support throughout the second half of the project. Major contributions to running the experiments, to writing and analysis.

% \textbf{Jérémy Scheurer}: Was responsible for the evaluations: oversight subversion, self-exfiltration. Contributed to writing and analysis.

% \textbf{Mikita Balesni}: Led the first half of the project. Led all efforts on scheming evaluations based on pre-cursor capabilities, such as instrumental self-reasoning (omitted from this paper).

% \textbf{Rusheb Shah}: Provided engineering support throughout the entirety of the project.

% \textbf{Marius Hobbhahn}: Provided supervision and strategic advice throughout the entirety of the project. Contributed to writing and analysis.

%\section*{Impact Statement}
%This paper aims to improve the forecasting capabilities of frontier LM agent capabilities. We think this has important implications for the field of Machine Learning. First, better forecasting generally improves the evidence that important AI-related decisions are based on. Second, many voluntary and regulatory requirements are based on the capability of frontier LMs, e.g. AI company safety frameworks or the EU AI Act. Better forecasting ability reduces uncertainty both for the developer as well as the regulator. Finally, this methodology could be used to forecast agentic performance for safety-critical domains such as AI R\&D, biology, cyber security, and AI scheming, which can increase preparedness for companies and governments alike.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{bibliography}
\bibliographystyle{iclr2025_conference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\include{appendix/scaled_compute_calculations}

\include{appendix/elo_pc1_correlation}


% \section{Pathway Prediction Ablations}
% \label{app:pathway_prediction_ablations}

% Some ablations would go here

\include{appendix/capability_metric_backtesting_details}

\include{appendix/scaffold_details}


\end{document}