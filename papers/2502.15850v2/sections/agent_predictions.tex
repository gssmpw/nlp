\section{Predictions for Agentic Benchmarks}
\label{sec:agent_predictions}

Informed by our backtesting, we now want to apply the most suitable methodology to predict the performance of three LM agent benchmarks.  

\subsection{Choice of benchmarks}

First and foremost, we want the benchmarks to capture important, economically valuable skills such that our forecasts have meaningful real-world implications.
Second, we want to use benchmarks that have a high option space and require repeated interaction with the environment in order to measure agent capabilities rather than pure knowledge. 
Third, we want the benchmarks to be difficult but have easily verifiable solutions. 
Finally, we want them to be popular for general validation and to compare performance against other implementations.

As such, we use SWE-Bench Verified~\citep{jimenez2024swebenchlanguagemodelsresolve, openai2024swebenchverified}, where all problems have been human-verified and a public leaderboard exists, Cybench~\citep{zhang2024cybenchframeworkevaluatingcybersecurity}, which aims to be representative of real-world cybersecurity work, and RE-Bench~\citep{wijk2024rebenchevaluatingfrontierai}, which attempts to measure the AI R\&D capabilities of LM agents. METR has kindly shared scores for eight frontier models with us \cite{scorescore}.

\subsection{Forecasting methodology}
We only use release date as the input variable since training FLOP count is no longer publicly known for most frontier models.
Furthermore, we only use Elo as our capability metric since almost all publicly available frontier models are available on Chatbot Arena, but not necessarily all benchmark scores. 
%However, as shown in Section \ref{subsec:backtest_capability_metrics}, Elo is competitive with PC-1, and we thus expect the predictions to be similarly accurate. 
In Section \ref{subsec:backtest_full_approaches}, we show that the release~Date$\to$Elo$\to$Benchmark~score path performs second-best in backtesting. Thus, it seems like a sufficiently good choice.

\subsection{Scaffolding}
We use the same scaffold for both SWE-Bench Verified and Cybench. For RE-Bench, we rely on METR's data, and thus don't have detailed knowledge of which scaffold was used.
Our scaffold attempts to be as simple as possible while avoiding simple known pitfalls.

% The scaffold prompts the model with a single instruction and resolves tool calls (if any). 
% For tool calls, we use native function calling if the model supports it. 
% Otherwise, we ask the model to format its tool calls in XML.
% We provide a second instruction with a worked example for the model if it uses non-native function calling.

We provide the model with three tools: a) A Bash shell, b) a Python shell, and c) a file editing tool that enables the model to view, create, and edit files by searching and replacing text and allows it to undo changes (similar to \citet{anthropic2024raisingbarswebench}).

All runs have a message cap of 50 messages and 2 million tokens. If the model runs out of context, we delete the earliest non-instruction messages.
Prompts for our scaffold are provided in Appendix \ref{app:scaffold}.

\subsection{Elicitation}
\label{subsec:elicitation}

The highest-performing scaffolds for each benchmark typically give more affordances to the model, or provide more inference-time compute. Furthermore, they often integrate prior knowledge about the benchmark into the scaffold, e.g. different prompts for isolating the bug, writing test cases, and retrying for SWE-Bench Verified.

Since our simple scaffold makes no use of additional inference compute, such as ``best-of-n'' or o1-style inference techniques \citep{openai2024learningreasonllms}, or highly task-specific prompts, we achieve a score of around 33\% on SWE-Bench Verified with Claude-Sonnet-3.5, while the best public scaffold known to be using Claude-Sonnet-3.5 on the SWE-Bench Verified leaderboard achieves 62.2\% \citep{pani2024sotaswebench}. 

Thus, we differentiate between a ``low-elicitation'' estimate, which should be seen as a general conservative estimate, and a ``high-elicitation'' estimate, which represents the best publicly known scaffolds at the time taken from publicly available leaderboards. The ``high-elicitation'' forecast has the advantage that it predicts the real public frontier, but the disadvantage that the scaffolds are almost always different between data points. 

We fit the low-elicitation forecast on only data gathered from our simple uniform scaffold. For the high-elicitation forecast, we combine \emph{all} data points, including both our own scaffold, and data from public leaderboards.  

%These scaffolds are essentially single-purpose and don't generalize to other benchmarks. Since we want to predict general performance rather than overfitting a benchmark, we decide to not use specialized scaffolding.

%Therefore, our estimates should be seen as \emph{conservative} estimates of general scaffolding. However, we're also interested in forecasting the maximal performance on a benchmark using specialized scaffolding and high inference-time compute techniques. We call this the \emph{high-elicitation} forecast.

%Thus, we compute an ``elicitation-correction factor''. 
%This is a single per-benchmark number that aims to account for the difference between our our scaffold and the best possible elicitation in terms of the capability score. It makes the assumption that models of all capability levels will be equally boosted by better elicitation.
%To compute it, we infer the necessary capability score that would be needed to achieve this level of performance, and subtract the unelicited capability score of that model. If we have multiple highly elicited models, we take the average difference in terms of capability score.

%To produce the elicitation-adjusted forecast, we add the factor to the extrapolated capability score at the given time before applying the S-curve.
%\todo{double-check this section after fixing high-elicitation performance}

\subsection{Results}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth]{all_dist.png}
    \vskip -0.1in
    \caption{
        Predictions for a 0.9 success rate on SWE-Bench Verified and Cybench and a score of 1 on RE-Bench for low and high elicitation, respectively. We compute the distribution using bootstrapping with 10,000 samples.
        Note that the medians (50th percentile) of these histograms do not necessarily equal the forecasts made with all data points in Figure \ref{fig:scaling-graph}.
    }
    \label{fig:distributions}
\end{figure*}




Figure \ref{fig:scaling-graph} shows the results of our forecasts until early 2027.
%We show our forecast for the upcoming 2 years, until approximately 2027.
%We find that our model is able to account for the rapid increases in agentic capabilities over the past year, and indeed predicted the o3 SWE-bench result, once we account for the impact of elicitation.

For SWE-Bench, we have access to all 17 models tested with our simple scaffold for the low elicitation effort and access to strong elicitation efforts of other groups from the public leaderboard.
Our model indicates that by January 2026, models with weak elicitation will achieve 54\% on SWE-Bench, and with better elicitation may achieve 87\%. However, our model does not take into account the potential for heavily increased test-time scaling, which may further increase performance.

Our forecast suggests that Cybench scores will be 55\% and 66\% in January 2026 for low and high-elicitation efforts, respectively. We observe that there is much less difference between the non-elicited and elicited cases, likely because far less effort has gone into eliciting Cybench performance to date.

On RE-Bench, we forecast a score of 0.73 by January 2026. Note that METR reported that they did not spend a lot of effort on elicitation, which suggests our estimates might be too conservative. Consequently, we exclude a high-elicitation scenario from our forecasts on this benchmark.

%We forecast Cybench scores will increase rapidly, with a mean estimate of 50\% in January 2026, significantly up from the current SOTA of 20\%. Since we don't find many scaffolds attempting to strongly elicit Cybench performance, we expect that the true SOTA in 2026 could be even higher. 

In Figure \ref{fig:distributions} we show the conditional distributions for a fixed benchmark score. We chose a score of 0.9 for SWE-Bench and Cybench as an arbitrary marker of strong performance and a score of 1 for RE-Bench, which is the expert baseline. 

With high elicitation, we expect SWE-Bench Verified to reach 90\% around March 2026, with a 95\% CI spanning from October 2025 to September 2027. With standard elicitation, we expect 90\% to be reached about two years later, in January 2028.

For Cybench, our best guess for high elicitation is December 2026, with a 95\% CI from April 2026 to April 2029. Standard elicitation predicts June 2027. 

Our forecast suggests that agent performance on RE-Bench may reach a score of 1—equivalent to the expert baseline reported by \citet{wijk2024rebenchevaluatingfrontierai}—around December 2026. We have much more uncertainty about this forecast, and our 95\% CI reflects this. It has a span of over 8 years, from August 2025 to May 2033. 

Across all three benchmarks and elicitation types, we observe that the probability distributions are asymmetric, with a longer right tail. This indicates greater uncertainty about potential delays compared to early achievements.

