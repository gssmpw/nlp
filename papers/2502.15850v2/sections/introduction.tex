\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig1.png}
    \vskip -0.1in
    \caption{Low-Elicitation and High-Elicitation forecasts for LM agent performance on SWE-Bench, Cybench, and RE-Bench. Elicitation level refers to performance improvements from optimizing agent scaffolds, tools, and prompts to achieve better results. Forecasts are generated by predicting Chatbot Arena Elo-scores from release date and then benchmark score from Elo. The low-elicitation (blue) forecasts serve as a conservative estimate, as the agent has not been optimized and does not leverage additional inference compute. 
    The high-elicitation (orange) forecasts use the highest publicly reported performance scores. Because RE-Bench has no public high-elicitation data, it is excluded from these forecasts.
    } 
    \label{fig:scaling-graph}
\end{figure*}

\section{Introduction}
Large language models are increasingly trained and deployed as autonomous agents capable of independently pursuing goals and executing longer-form real-world tasks. This rapid advancement creates a need to forecast when these systems will reach critical capability thresholds in order to understand and prepare for their effects on society and the economy.

While there are several approaches that forecast LM capabilities, they face key limitations. Methods like Observational Scaling Laws \citep{ruan2024observational} and Sloth \citep{polo2024sloth} can only make predictions after evaluating models from a new model family on multiple benchmarks to derive their predictive metrics, such as PC-1 scores (which correspond directly to general capabilities) \citep{ruan2024observational} or family-specific efficiency parameters (which can be used to convert FLOP to general capabilities) \citep{polo2024sloth}\footnote{While Observational Scaling Laws \citet{ruan2024observational} allows direct FLOP-based predictions for specific model families, this approach fails to account for algorithmic advances across families and would therefore underestimate future benchmark performance.}. Other approaches that predict directly from compute \citep{owen2024predictablelanguagemodelbenchmark} or release date are either less accurate \citep{ruan2024observational} or have not been systematically evaluated. 


Backtesting these six methods, we find that the two-step approach with a linear relationship between date and Elo and a sigmoidal relationship from Elo to benchmark performs competitively and has publicly available data for frontier models. 
%This two-step methodology has the advantage of superior data efficiency: our backtesting show it enables robuster predictions when benchmark performance is scarce, but you still have plenty of data from the input metric to the intermediate capability metric.
Our forecasts focus on \emph{frontier} performance, the performance of the best-known model at a given time or compute level, unlike previous work that focuses on predicting average model performance \citep{ruan2024observational, polo2024sloth}. This is relevant because frontier performance determines what capabilities can be automated at a given time and because frontier models are most likely to present novel risks and societal impacts \citep{hendrycks2023overviewcatastrophicairisks}. Finally, our method only requires model release dates and Elo Ratings, information that is typically public even for proprietary models, rather than metrics like FLOP count, parameter count, or dataset size, which are often unavailable for frontier models.
%In order to understand and prepare for increasingly agentic systems and their effects on the economy and society, we not only want to track their performance but also forecast them ahead of time. For example, we might want to estimate when AI systems reach critical capability thresholds in specific domains, e.g. software engineering and AI research.

% Prior work has already described techniques to forecast the performance of future models. For example, \citet{ruan2024observational} estimate an ``underlying general capability''  (PC-1) from cross-benchmark performance to predict downstream task performance. Furthermore, \citet{phuong2024} developed methods to predict future performance of LM agents on agentic benchmarks that current models have never been observed to solve. However, these approaches have known limitations - OSL requires evaluating models on multiple benchmarks to calculate PC-1, while \citet{phuong2024}'s methods are known to strongly underestimate performance \citep{højmark2024analyzingprobabilisticmethodsevaluating}. 

%Existing forecasting approaches have a range of known shortcomings. For instance, OSL requires actually evaluating the model on various benchmarks to calculate PC-1. \citet{phuong2024} are known to strongly underestimate performance \citep{højmark2024analyzingprobabilisticmethodsevaluating}.

%Formally, we say a model is on the frontier if there is no model that \emph{dominates} it, i.e. no previously released model has a higher benchmark score. 
%Our definition of what counts as a frontier model is closely linked to the skyline operator and the maximal vector problem \citep{borzsony914855}. 
%In Figure \ref{fig:scaling-graph}, points on the frontier are highlighted as stars.

%In this paper, we compare different methodologies to forecast frontier benchmark performance. We evaluate methods that directly estimate performance from release date or pretraining FLOP, as well as methods that utilize intermediate measures of capability like OSL's PC-1 and \citet{chatbot_arena}'s Chatbot Arena Elo. After identifying the best-performing applicable approach through comprehensive backtesting, we make concrete predictions for three commonly used agentic benchmarks. Our predictions are based on evaluating models with basic agent scaffolds, providing conservative estimates of capability. We extend these predictions by introducing a novel elicitation adjustment that estimates how much better models would perform with optimal prompting and scaffolding, derived from comparing basic and state-of-the-art scaffolds. Our conservative forecast suggests that N\% on SWEBench will be achieved by 2026, M\% on Cybench, and K\% on RE-Bench. \todo{fill these values in after trying various ablations} 

Our \textbf{contributions} are as follows:
\begin{itemize}
    \item We show that Chatbot Arena Elo \citep{chatbot_arena} is a good proxy for a model's underlying performance and correlates strongly with FLOP count.%and \citet{ruan2024observational}'s PC-1. 
    %\todo{this is consistent with findings TODO reference; Jeremy, you had a reference here to another paper but I cannot find it anymore.}
    \item We compare 6 methodologies that predict performance based on combinations of FLOP count, model release date, Elo, and PC-1. We backtest all methods on 38 models from Open LLM leaderboard 2 \citep{open-llm-leaderboard-v2}, which have Elo scores available, and find that Release Date$\to$PC-1$\to$Benchmark performs best closely followed by Release Date$\to$Elo$\to$Benchmark.
    \item We evaluate 17 models on SWEBench and Cybench with a simple agent scaffold and use publicly available results of RE-Bench \citep{wijk2024rebenchevaluatingfrontierai} to enable direct performance comparisons across models. 
    %Based on this data, we develop two forecasts for future model capabilities on these three benchmarks. 
    Our low-elicitation prediction projects that non-specialized LM agents will achieve a 55\% success rate on SWE-Bench Verified tasks by early 2026, while our high-elicitation forecast suggests 85\%. \footnote{All code for the analysis, scaffold, and evaluation is available at \href{https://github.com/pimpale/forecasting-frontier-language-model-agent-capabilities}{https://github.com/pimpale/forecasting-frontier-language-model-agent-capabilities}}
%    \item We introduce a method to estimate the impact of elicitation on model performance by learning an elicitation adjustment factor from the performance gap between basic and optimal scaffolds. This enables us to provide both conservative and best-case forecasts of frontier capabilities.

\end{itemize}


% As artificial intelligence capabilities continue to advance rapidly, benchmarks play a crucial role in measuring and understanding model performance.
% Recent benchmarks have moved away from simple multiple-choice and cloze tests to longer-form tasks that are more representative of real-world use cases.
% These include domain-specific evaluations such as SWE-Bench \citep{jimenez2024swebenchlanguagemodelsresolve} for software engineering, CyBench \citep{zhang2024cybenchframeworkevaluatingcybersecurity} for cybersecurity, and RE-Bench \citep{wijk2024rebenchevaluatingfrontierai} for ML capabilities.

% We want to:
% \begin{enumerate}
%     \item Understand how model capabilities relate to performance on these benchmarks.
%     \item Use our understanding to forecast frontier model performance on these benchmarks over the next few years.
% \end{enumerate}

% Being able to forecast when AI systems might reach significant capability thresholds in specific domains enables us to better prepare for their economic and societal impacts. 

% In this work, we address these challenges by analyzing the relationship between model capabilities and performance on several advanced benchmarks. We develop and validate a methodology for predicting benchmark performance based on model capabilities, enabling more accurate forecasting of when AI systems might reach important capability thresholds.

% \subsection{Contributions}
% \begin{itemize}
%     \item We show that Chatbot Arena Elo \citep{chatbot_arena} is a good proxy for a model's underlying performance, and correlates strongly with effective FLOP count and \citet{ruan2024observational}'s PC-1.
%     \item We propose 6 potential methodologies for predicting frontier benchmark performance from FLOP count and release date.
%     \item We back-test all proposed methodologies on the past 2 years of data. ... we do this to forcast lm agent capabilities ...
%     \item We provide experimental data for how performance on SWEBench, CyBench, and RE-Bench changes with respect to model capabilities using a uniform scaffold and evaluation conditions.
%     \item We forecast what the SOTA performance will be on these benchmarks over the next few years, assuming models continue to improve at the same rate that they have over the past 2 years, attempting to answer:
%         \begin{enumerate}
%               \item When could AI models generate substantial economic value?
%               \item When could AI models potentially pose significant risks?
%         \end{enumerate}
% \end{itemize}

% The recent paper Observational Scaling laws by \citet{ruan2024observational} introduces forecasting techniques for benchmark scores which show to be remarkebly accurate. They show that one can find scaling laws for benchmark performance where a language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, they find the surprising predictability of complex scaling phenomena and show that several "emergent" phenomena follow a smooth, sigmoidal behavior and are predictable from small models. This includes agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks.

% Or work builds on the direction from Observational Scaling Laws, investigating how elo scores might be a usefull proxy for agent capabilities like the low-dimensional capability scores they used. Furthermore we extend this into the dimension of time, showing that frontier model Elo scores have had a consistent and highly linear trend over the past year and a half. Combining these estimates gives us the possibility of forecasting the agentic capability of future models based on their respective winrates against current models on Chatbot Arena, and allows us to extrapolate current trends and give an estimate of when models with a certain agent capability might be released. There are many assumptions in the extrapolation which might turn out to make the predictions inaccurate, but we hope this work can help inform regulators and people working in AI governance, and hopefully open up new directions in predicting capabilities of future AI models.  

% These questions are very broad, and so we narrow these questions down in two major ways. Firstly we choose to focus specifically on LM agents. In this paper, we define LM agents as composite systems that combine an LM with \emph{scaffolding}, software that repeatedly prompts the LM and lets it interact with the environment \citep{nakano2021webgpt, ahn2022can, yao2023react, schick2023toolformer, shen2023hugginggpt, park2023generative, shinn2023reflexion}.
% These agents could have significant economic utility, e.g. through replacing knowledge workers or automating R\&D efforts \citep{liu2023agentbenchevaluatingllmsagents,phuong2024,shevlane2023modelevaluationextremerisks,anthropic2023responsiblesccalingpolicy,apollo2024weneedscienceevals}. Secondly, in terms of risks from AI systems, the evaluation we perform in the paper mainly give insight into offensive cybersecurity capabilities of models. Altough advanced agents can also pose significant risks, such as the potential to construct bioweapons \citep{li2024wmdpbenchmarkmeasuringreducing},
% conduct cyber attacks \citep{xu2024autoattackerlargelanguagemodel},
% strategically deceive humans \citep{scheurer2024largelanguagemodelsstrategically},
% or replicate autonomously \citep{kinniment2024evaluatinglanguagemodelagentsrealistic}, we will mainly be forecasting AI risk through a lens of cybersecurity.