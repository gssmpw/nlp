\section{Related Work}
\label{sec:related_work}

\paragraph{Pre-training scaling laws}
\citet{hestness2017deeplearningscalingpredictable} discuss power-law scaling of the error with respect to dataset size and parameter count across multiple domains.
\citet{kaplan2020scalinglawsneurallanguage} discusses specific power law exponents for training transformers optimally given a fixed compute budget, and demonstrate that these scaling laws hold over many orders of magnitude. These parameters are improved by \citet{hoffmann2022trainingcomputeoptimallargelanguage}, who introduce ``Chinchilla'' scaling laws. \citet{besiroglu2024chinchilla} repeat their experiment and further refine the parameters.

\paragraph{Predicting downstream performance}
\citet{Finnveden_2020} uses data from GPT-3 \citep{brown2020languagemodelsfewshotlearners} to extrapolate the performance of future models on downstream tasks, including Winograd, SuperGLUE, and ANLI.
\citet{owen2024predictablelanguagemodelbenchmark}, models BIG-Bench performance with respect to Scaled FLOP and finds that we can predict aggregated performance but not individual task performance.
\citet{ruan2024observational}, use a PCA on benchmark scores to decompose scores into a low-dimensional capability space. They find that performance across benchmarks can largely be explained by a single principle component which they interpret as a ``general capability factor''. \citet{polo2024sloth} extend this idea via a low-dimensional “skill” model that can predict across families if smaller models from the same family are available.  
\citet{zhang2024collaborativeperformancepredictionlarge}, applies collaborative filtering to handle sparse benchmark scores, i.e. cases where not all models have the same benchmark scores available. 

Less attention has been paid to directly predicting frontier performance instead of average performance. 
\citet{steinhardt2022ForecastingMLBenchmarks} discusses factors relevant to forecasting LLM performance, including training data availability and algorithmic progress. They attempt to predict frontier models' scores on MATH and MMLU. 
\citet{villalobos2024run} discusses how much training data future models are likely to have access to in further detail. 
\citet{ho2024algorithmic} discusses the rate of algorithmic progress, or how much improvement we are likely to see due to changes in architecture and improvements in the training process. 
\citet{cottier2024rising} discusses the increasing cost of training the underlying foundation models.
\citet{erdil2023power} discusses the base rate of breakthrough developments, and how it contributes to algorithmic progress.
%
\paragraph{Emergent capabilities}
Sudden jumps in capabilities on benchmark performance are often attributed to ``Emergent capabilities.'' 
\citet{srivastava2023imitationgamequantifyingextrapolating}, noticed that certain tasks in BIG-Bench experienced sudden jumps in capabilities.
\citet{wei2022emergentabilitieslargelanguage} discusses these emergent abilities in further detail.
\citet{schaeffer2023emergent} shows that emergent capabilities heavily depend on the choice of grading criterion or metric and that non-linear jumps in one metric can be smooth in another. 