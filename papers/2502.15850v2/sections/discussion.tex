\section{Discussion}
\label{sec:discussion}

\subsection{Limitations}
\label{sec:limitations}
%
\paragraph{Paradigm changes} 
While this paper does not make any explicit assumptions about the training paradigm of any particular model, we fit almost all predictions on models that were trained with the ``pre-training scaling'' paradigm, where the primary driver for downstream performance was improvements of pre-training. However, with OpenAI's o1 \citep{openai2024learningreasonllms}, we may start to see a new ``inference scaling'' paradigm where models are trained to utilize inference compute much more effectively through reasoning. This might invalidate our predictions and thus provide a reason to assume faster progress than our forecasts would suggest, even for high-elicitation predictions.
%
\paragraph{Underelicitation} 
As discussed in Section \ref{subsec:elicitation}, we did not put a lot of effort into elicitation.
%We did not put a lot of effort into elicitation, i.e. to find a procedure that yields maximal performance on the task. For example, we did not optimize the prompts or scaffolding for each task (which is known to be a major performance increase). We also did not make use of additional inference compute such as ``best-of-n'' or o1-style inference techniques. We made these choices for various reasons, including a) we want to forecast general rather than specialized agents, b) comparability between tasks, and c) high costs of iterating on agent scaffolding. 

As a consequence, we know that our results are significantly below frontier performance and that our ``low-elicitation'' predictions are conservative. Even the ``max-current-elicitation'' forecast might underestimate performance due to paradigm changes (see above) or later breakthroughs in agent scaffolding and elicitation.
%
\paragraph{Small sample size} 
Unfortunately, almost by definition, there are only a small number of frontier models. Therefore, our predictions have a small sample size. This is partially mitigated by making use of the two-step methodology and predicting the intermediate variable independently. However, we think the small sample size should imply large uncertainty about our forecasts. 

This limitation also affects our backtesting. Because our available test data is limited, we must rely on small evaluation windows, some as brief as two-month intervals. As a result, we have little empirical evidence regarding how our predictions might perform over longer periods.
%We welcome replications with more datapoints or methodological contributions specifically assuming low sample sizes, such as Bayesian data analysis with a clever choice of priors. 

\paragraph{Limited Scope of Evaluations}
The benchmarks we consider focus primarily on software engineering, cyber capabilities, and machine learning engineering. 
%However, agent capabilities extend far beyond these areas, covering a much broader range of skills. Consequently, our evaluations may not fully capture the range of competencies expected from an advanced agent. 
Noteworthy other agent benchmarks include GAIA \citep{mialon2023gaiabenchmarkgeneralai} and OS-World \citep{xie2024osworldbenchmarkingmultimodalagents} for browsing and tool use, as well as MLE-Bench \citep{chan2024mlebenchevaluatingmachinelearning} for additional machine learning capabilities.

%
% \paragraph{Interpretation} 
% As discussed in Section \ref{subsec:elicitation} and \ref{sec:limitations}, we know that our results are an underestimate. Therefore, all of our results should be treated as conservative estimates. Our low- and high-elicitation forecasts predict a success rate of 0.55 and 0.85 on SWE-Bench Verified by the beginning of 2026, respectively.
%
\subsection{Future work}
The most straightforward way to extend this paper is by adding more agentic benchmarks and more models. 
Secondly, our ``high-elicitation'' estimates are done with very different scaffolds and techniques. We think there is a lot of room for improvement in choosing datapoints and designing forecasting techniques here. 
Thirdly, extending any forecasting technique with a factor for inference-time compute scaling might yield more accurate results. 
%We encourage improved techniques for elicitation correction, especially in controlled settings where only one variable is varied at a time. 