\section{Evaluating approaches through backtesting}

To compare all six of our approaches, we backtest them on existing data from the Open LLM Leaderboard v2 \citep{open-llm-leaderboard-v2} with six benchmarks: IFEval, BBH, MATH Lvl 5, GPQA, MUSR, and MMLU-PRO.
We only use the subset of Open LLM Leaderboard v2 that has Elo scores available, resulting in 38 models (see Appendix~\ref{app:models_on_both_leaderboards}).

However, before we determine which pathway is the most accurate overall, we want to compare the predictive power of input and intermediate variables.
%understand how predicting from our intermediate capability metrics compares to predicting from the input variables. 
In Section \ref{subsec:backtest_capability_metrics}, we backtest individual capability metrics to validate Elo as a potential candidate and compare it to PC-1, scaled log-FLOP, and release date.
Then, we move towards testing the entire pathway. In Section \ref{subsec:backtest_full_approaches}, we backtest all six full approaches.

\subsection{Backtesting capability metrics}
\label{subsec:backtest_capability_metrics}

To backtest capability metrics, we use expanding window cross-validation \cite{expanding_window_backtest} with 3 splits based on release date.
We first split our data up into 4 divisions with approximately equal model count.
Then, we train a statistical model only on the first split and evaluate it on the second split, another statistical model on the first and second split, and evaluate it on the third, and so on.
Our cross-validation methodology is displayed for predictions of MMLU-PRO with the full approach in Figure \ref{fig:path_backtest} (see also Section \ref{subsec:backtest_full_approaches}).

We are computing the error of only the capability metric, so we train just the sigmoid from the capability metric to the target benchmark. (Subplot 2 in Figure~\ref{fig:paths}).
For PC-1, we avoid testing on the training data and thus omit the benchmark we're predicting when fitting the PCA. Furthermore, we only use the data available up to that point when fitting the principal component vectors.  
To compare the overall performance of our four capability metrics, we compute the RMSE over our three splits and six benchmarks, for each approach.

\begin{table}[H]
    \begin{center}
        \begin{scriptsize}
            \setlength{\tabcolsep}{8.5pt}
            \begin{tabular}{lcccr}
                 \toprule
                 Capability Metric & PC-1 & Elo & log-FLOP & Release Date \\
                 \midrule
                 Test RMSE & 0.068 & 0.080 & 0.102 & 0.146 \\
                \bottomrule                     
            \end{tabular}
        \end{scriptsize}
    \end{center}
    \vskip -0.1in
    \caption{Average all-model test-split back-prediction RMSE for prediction of target benchmark from capability metrics. Intermediate metrics (PC-1 and Elo), outperform raw input variables (log-FLOP and Release Date).}
    \label{table:rmse_capability_metrics}
\end{table}

Table \ref{table:rmse_capability_metrics} displays the aggregated results. 
PC-1 performs best, followed by Elo, log-FLOP, and date as intermediate metrics. 
The full results of our capability metric backtesting can be found in Appendix~\ref{app:capability_metric_backtesting}.

\subsection{Backtesting full approaches}
\label{subsec:backtest_full_approaches}

\begin{figure*}[!htb]
    \centering
    \makebox[\columnwidth][c]{
    \includegraphics[width=1\textwidth]{path_backtesting.png}
    }
    \vskip -0.1in
    \caption{
        Visualization of backtesting forecasts for MMLU-PRO using the full method.
        %TODO: explain more.
        We split the data into 4 parts with an equal number of models. We then fit a full path on split 1 and test on split 2, fit on 1 \& 2, and predict on 3, and so forth. 
        \textbf{Top:} Comparing predicted to actual performance. Frontier models are marked with stars. 
        \textbf{Bottom:} Average RMSE over frontier models. Bars are colored by the split they predict.
    }
    \label{fig:path_backtest}
\end{figure*}

To backtest the full paths, we use the same expanding window cross-validation procedure. 
However, there are two important differences. 
First, we are testing the complete path from input variable to benchmark score, ignoring the internal loss of the S-curve or linear regression subcomponents.
Second, we only compute error for data points on the frontier. If there are no frontier points in a split, that split is ignored.
We then aggregate the error in each split as usual. Since there are far fewer data points, the error is likely to be noisier.

\begin{table}[h]
   \begin{center}
       \begin{scriptsize}
           \setlength{\tabcolsep}{10pt}
           \begin{tabular}{l|cr}
                \toprule
                \textbf{Intermediate Variable} & \multicolumn{2}{c}{\textbf{Input Variable}}\\
                \cmidrule(r){1-1} \cmidrule(l){2-3}
                & log-FLOP & Date  \\
                \midrule
                \emph{None} (One-Step) &  0.119 & 0.125 \\
                Elo & 0.197 & 0.095 \\
                PC-1 & 0.105 & 0.082 \\
               \bottomrule
           \end{tabular}
       \end{scriptsize}
   \end{center}
   \vskip -0.1in
   \caption{Average frontier model test-split back-prediction RMSE for full approach. The path Date$\to$PC-1$\to$Benchmark performs best, followed by Date$\to$Elo$\to$Benchmark.}
   \label{table:rmse_paths}
\end{table}

Our results (see Table \ref{table:rmse_paths}) show that the best overall path is going from Release Date$\to$PC-1$\to$Benchmark, with an overall RMSE of 0.082, followed by Date$\to$Elo$\to$Benchmark with an RMSE of 0.095. Overall, using release date as the input variable outperforms log-FLOP.