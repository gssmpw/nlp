\section{Methods}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{./figures/paths.pdf}
    \vskip -0.1in
    \caption{
     Six approaches for predicting frontier LM capabilities. Two direct methods (blue pathways) model benchmark performance as a sigmoid function of either release date or compute (log-FLOP). Four two-step methods (red and purple pathways) first use a linear function to predict intermediate capability metrics (PC-1 or Chatbot Arena Elo) from input variables, then map these metrics to benchmark scores using a sigmoid function.
    }
    \label{fig:paths}
\end{figure*}

We evaluate six different approaches to predict frontier LM capabilities.
The simplest approaches try to directly predict frontier benchmark performance from an \emph{input variable} such as training log-FLOP or release date. 

We contrast such one-step approaches with two-step approaches where we first use the input variable to predict an intermediate \emph{capability metric} (e.g. PC-1 or Elo), and then use that capability metric to forecast the downstream \emph{benchmark performance}. These variables can be combined in four different ways. When combined with the one-step approaches, this results in six different methods to forecast the performance on a target benchmark (see Figure \ref{fig:paths}).

\subsection{Prediction Tasks}
In the following, we describe which input variables and downstream benchmarks we use.

\subsubsection{Input Variables}

Input variables are broad and general quantities that we expect to have predictive power for downstream performance. They don't require knowing any specifics about the model, e.g. architecture or exact training procedure. 

\textbf{Scaled training log-FLOP}: The amount of compute used to train the model measured in FLOP. Previous literature has found an approximately log-linear relationship between input compute and model performance \citep[e.g.][]{Finnveden_2020, owen2024predictablelanguagemodelbenchmark}. \citet{sevilla2022computetrends} observes that the amount of compute utilized by large scale pre-training runs tends to double approximately once every 9 months, providing us with a broad reference class for the compute requirements of frontier model training.

In this paper, we always use \emph{scaled} FLOP as described in \citet{owen2024predictablelanguagemodelbenchmark}. It is a common practice to ``overtrain'' models \citep{shafkat2023, dubey2024llama3herdmodels} by reducing parameter count and increasing dataset size beyond what would be optimal under Hoffman scaling laws \citep{hoffmann2022trainingcomputeoptimallargelanguage} to reduce cost at inference time. Therefore, raw (unscaled) FLOP estimates are less comparable. We can overcome this by normalizing all models to the lowest possible FLOP count that would achieve the same loss using Hoffman scaling laws. See Appendix~\ref{app:scaled_compute} for details.

Note that we only focus on pre-training FLOP and do not take into account post-training such as RLHF/RLAIF \citep{ouyang2022traininglanguagemodelsfollow, bai2022constitutionalaiharmlessnessai}, nor inference time compute. The ``inference compute'' paradigm started by OpenAI's o1 \citep{openai2024learningreasonllms} is not accounted for in our methodology. Column 1 of Figure \ref{fig:methods} depicts the relationship between scaled log-FLOP and intermediate capability metrics.

\textbf{Release date}: The date that the model was officially released for public use. While there is no inherent reason to assume that the release date influences the performance of the model, we speculate that it is a good aggregate measure of both algorithmic improvements \citep{ho2024algorithmic, xiao2024densinglawllms, erdil2023algorithmicprogresscomputervision} and increased training compute \citep{sevilla2022computetrends}, especially for frontier models. 
%Column 2 of Figure \ref{fig:methods} depicts the relationship between release date and capability metrics.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{2x2_methods.png}
    \vskip -0.1in
    \caption{
        Forecasting intermediate capability metrics from input variables for frontier models.  
        We find that both PC-1 and Elo are surprisingly linear when predicted from FLOP and release date, with all combinations having a $R^2$ $\ge$ 0.91.
        %The top row displays the relationship between 3 different input variables and OSL's PC-1. The bottom row displays the relationship between the input variables and Chatbot Arena Elo. Graphs in the same column have the same input variables.
    }
    \label{fig:methods}
\end{figure*}

\subsubsection{Target Benchmarks}
\label{subsec:target_benchmark}

Target benchmarks are measures of downstream performance that compare the results of different models. In our specific case, we use benchmarks on the OpenLLM v2 leaderboard \citep{open-llm-leaderboard-v2} for backtesting. They include IFEval \citep{zhou2023instructionfollowingevaluationlargelanguage}, BBH \citep{suzgun2022challengingbigbenchtaskschainofthought}, MATH LVL 5 \citep{hendrycks2021measuringmathematicalproblemsolving}, GPQA \citep{rein2023gpqagraduatelevelgoogleproofqa}, MUSR \citep{sprague2024musrtestinglimitschainofthought}, and MMLU-PRO \citep{wang2024mmluprorobustchallengingmultitask}. We then use the method that we judge to be best in our comparisons to forecast future agent capabilities on three benchmarks: SWE-Bench Verified \cite{openai2024swebenchverified}, Cybench \cite{zhang2024cybenchframeworkevaluatingcybersecurity}, and RE-Bench \cite{wijk2024rebenchevaluatingfrontierai}.

% \begin{algorithm}
%    \caption{Compute Frontier Set of Models}
%    \label{alg:frontier}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} list of models $m_i$,\\ corresponding list of input variable values $x_i$,\\ corresponding list of benchmark values $b_i$,\\ number of models $N$ \\
%    \STATE {\bfseries Result:} set of frontier models
%    \STATE Sort $m_i$ and $b_i$ by key $x_i$ ascending
%    \STATE Initialize $frontierScore = 0$
%    \STATE Initialize $frontierSet = \{\}$.
%    \FOR{$i=0$ {\bfseries to} $N-1$}
%    \IF{$b_i >= frontierScore$} 
%    \STATE $frontierSet \gets frontierSet \cap m_i$
%    \STATE $frontierScore \gets b_i$
%    \ENDIF
%    \ENDFOR
%    \STATE \bfseries{return} $frontierSet$
% \end{algorithmic}
% \end{algorithm}

\subsection{One-step Forecasting Approach}
``One-step'' approaches predict the benchmark score from an input variable directly.
%Two of the approaches we consider are ``one-step'' models, because they don't involve converting the input variable to a separate capability metric before predicting the target benchmark.

First, we identify which models lie on the frontier for each choice of input variable and target benchmark. A point \((x_i, y_i)\) is on the frontier if there is no other point \((x_j, y_j)\) with \(x_j < x_i\) and \(y_j > y_i\). Visually, this corresponds to the upper boundary of points in Figure  \ref{fig:methods}.
%, where \(x\) might represent log-FLOPs or release date.

%Our algorithm is described in detail in Algorithm~\ref{alg:frontier}.

Following \citet{ruan2024observational}, we assume a sigmoidal relationship between capability metrics and the eventual benchmark scores. 
Per input variable $x$ with frontier datapoints $x_i$, and corresponding benchmark scores $y_i$, we fit $a, b$.\footnote{We assume the ceiling is 1 for all benchmarks except RE-Bench. For RE-Bench, the sigmoid is scaled to asymptote at 1.67 instead of 1. This choice reflects the authors' estimate of the maximum achievable performance across their tasks  \cite{wijk2024rebenchevaluatingfrontierai}. Averaging those maximum performances yields 1.67, motivating our selection of this asymptote.}
\begin{equation}\label{eq:sigmoid}
    y_i = \sigma(a x_i + b)
\end{equation}
%Thus, we fit a sigmoid curve attempting to predict the benchmark of choice from the input variable values using Algorithm~\ref{alg:sigmoid}.
% Concretely, we compare the following two one-step approaches:
% \begin{enumerate}
%     \item Release Date $\to$ Target (One-step)
%     \item log-FLOP $\to$ Target (One-step)
% \end{enumerate}

% \begin{algorithm}
%     \caption{Fitting Sigmoid Curve}
%     \label{alg:sigmoid}
%     \begin{algorithmic}
%         \STATE {\bfseries Input:} list of capability scores $x_i$,\\ corresponding list of benchmark scores $y_i$
%         \STATE {\bfseries Result:} optimal parameters for a sigmoid between 
%         \STATE $\beta^*, \alpha^*, h^* \gets \text{Fit}(y_i = h\sigma(\beta x_i + \alpha))$
%         \STATE \bfseries{return} $\beta^*, \alpha^*, h^*$
%     \end{algorithmic}
% \end{algorithm}

\subsection{Two-step Forecasting Approach}
For ``two-step'' approaches, we first predict an ``intermediate capability metric'' and then use that to predict the downstream performance.

There are a few reasons in favor of a two-step approach. First, it is conceptually plausible that there is some underlying ``general capability'' that is highly predictive of the output variables. 
Second, the intermediate capability metric might function as a ``regularization'' step, e.g. by projecting all training compute onto GPT-2-equivalent FLOP as shown in \citet{ruan2024observational} or onto Hoffmann scaling laws as shown in \citep{owen2024predictablelanguagemodelbenchmark}.
Third, we might have more data available for either of the two intermediate forecasts, thus improving the overall prediction.

\subsubsection{Intermediate Capability Metrics}
\label{subsec:capability_metrics}

We aim to reduce the complex behaviors of the model down to a single number as an underlying measure of the latent \emph{general capabilities}. We have three main criteria for selecting such a metric. First, we need the metric to be easily computable. Second, it should be available for a large range of models. Third, the metric has to have high predictive power for a large range of downstream benchmarks.

\textbf{PC-1}: \citet{ruan2024observational} show that around 80\% of the variance in benchmark scores is explained by the first principal component of a PCA on benchmarks. The PCA is applied to a model~$\times$~benchmark table. The authors show that as a result, PC-1 can be used as a measure of general capabilities and is predictive across a large variety of tasks. Note that \citet{ruan2024observational} use the top 3 components, whereas we limit ourselves to only the top one.

\textbf{Elo}: Chatbot Arena \citep{chatbot_arena} is an open platform for evaluating LLMs based on human preferences. Users are asked to compare responses from two models and select the one they prefer. The Elo rating system is then used to assign a score to each model based on the outcomes of these comparisons.

There are various advantages and disadvantages to each metric. Elo does not require extensive evaluation on benchmarks, which can be time-consuming, expensive, or use inconsistent methodologies. Elo ratings do not saturate, while PC-1 scores saturate when all of its component benchmarks are saturated.
However, Elo ratings are only meaningful in relationship to the exact rating pool in which they were calculated. As a result, any of our forecasts expressed in Elo is best interpreted as: “If a model had an Elo of X relative to today’s models, how good would it be?”—recognizing that, by the time the model is actually released, the entire rating landscape may have shifted.
%Therefore, we think it is important to have multiple possible approaches available to be more flexible depending on data availability. 


\subsubsection{Fitting Two-step Approaches}
% As we have two input variables and two intermediate capability metrics, this results in 4 two-step models, which are depicted in Figure~\ref{fig:paths}.

% The motivation for two-step models is that we can split the problem into (a) predicting the frontier rate of change of a more predictable and easy to measure capability metric, and (b) finding the relationship between the capability metric and the target benchmark. 

To fit the relationship between the input variable and the capability metric, we first determine which models are on the frontier as in the ``one-step'' approaches. Then, we fit a linear regression between the frontier points' input variable $x$ and the intermediate capability metric values $z$.
\begin{equation}\label{eq:linear}
    z_i = c x_i + d
\end{equation}
% This linear regression represents our hypothesis on how the latent abilities of the best public LM agent will change in the future or at greater FLOP levels. 
Figure~\ref{fig:methods} shows that a linear relationship between the input variable and the capability metric is usually a good fit.

In the second step, we fit a sigmoid between the capability metric and benchmark score.  
\begin{equation}\label{eq:sigmoid}
    y_j = \sigma(e z_j + f)
\end{equation}
We fit the sigmoid on \emph{all} available models instead of just the frontier since we expect that frontier models will have the same relationship between the underlying capability and benchmark score as other models. 
%Similarly to one-step methods, we use Algorithm~\ref{alg:sigmoid} to fit the sigmoid. 
% Subplots 1 and 2 in Figure \ref{fig:path_backtest} show the calibration between predicted and true scores between intermediate variables and MMLU-PRO.