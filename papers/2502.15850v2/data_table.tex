\begin{tiny}
    \begin{tabular}{llrrrrrrrrrr}
    \toprule
     & model & Elo & IFEval & BBH & MATH Lvl 5 & GPQA & MUSR & MMLU-PRO & release\_date & N (10e9) & D (10e12) \\
    \midrule
    0 & Qwen2.5-72B-Instruct & 1259 & 0.86 & 0.73 & 0.01 & 0.38 & 0.42 & 0.56 & 2024.72 & 72.00 & 18.00 \\
    1 & Meta-Llama-3.1-70B-Instruct & 1247 & 0.87 & 0.69 & 0.31 & 0.36 & 0.46 & 0.53 & 2024.56 & 70.00 & 15.00 \\
    2 & Gemma-2-27B-it & 1219 & 0.80 & 0.65 & 0.01 & 0.38 & 0.40 & 0.45 & 2024.49 & 27.00 & 13.00 \\
    3 & Command R+ (08-2024) & 1215 & 0.75 & 0.60 & 0.12 & 0.35 & 0.48 & 0.44 & 2024.65 & 104.00 & NaN \\
    4 & Llama-3-70B-Instruct & 1206 & 0.81 & 0.65 & 0.25 & 0.29 & 0.42 & 0.52 & 2024.30 & 70.00 & 15.00 \\
    5 & Gemma-2-9B-it & 1190 & 0.74 & 0.60 & 0.00 & 0.36 & 0.41 & 0.39 & 2024.49 & 9.00 & 8.00 \\
    6 & Qwen2-72B-Instruct & 1187 & 0.80 & 0.70 & 0.38 & 0.37 & 0.46 & 0.54 & 2024.44 & 72.00 & 7.00 \\
    7 & Meta-Llama-3.1-8B-Instruct & 1175 & 0.79 & 0.51 & 0.19 & 0.27 & 0.39 & 0.38 & 2024.56 & 8.00 & 15.00 \\
    8 & Qwen1.5-110B-Chat & 1162 & 0.59 & 0.62 & 0.00 & 0.34 & 0.45 & 0.48 & 2024.32 & 110.00 & 3.00 \\
    9 & 01-ai/Yi-1.5-34B-Chat & 1157 & 0.61 & 0.61 & 0.25 & 0.36 & 0.43 & 0.45 & 2024.37 & 34.00 & 3.60 \\
    10 & Llama-3-8B-Instruct & 1152 & 0.48 & 0.49 & 0.09 & 0.29 & 0.38 & 0.36 & 2024.30 & 8.00 & 15.00 \\
    11 & internlm/internlm2\_5-20b-chat & 1149 & 0.70 & 0.75 & 0.00 & 0.32 & 0.46 & 0.40 & 2024.30 & 20.00 & NaN \\
    12 & Mixtral-8x22b-Instruct-v0.1 & 1148 & 0.72 & 0.61 & 0.19 & 0.37 & 0.43 & 0.45 & 2024.30 & 141.00 & NaN \\
    13 & Gemma-2-2b-it & 1140 & 0.57 & 0.42 & 0.00 & 0.27 & 0.39 & 0.25 & 2024.49 & 2.00 & 2.00 \\
    14 & HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1 & 1127 & 0.65 & 0.63 & 0.20 & 0.38 & 0.45 & 0.46 & 2024.28 & 141.00 & NaN \\
    15 & Qwen1.5-32B-Chat & 1125 & 0.55 & 0.61 & 0.07 & 0.31 & 0.42 & 0.45 & 2024.10 & 32.00 & 3.00 \\
    16 & microsoft/Phi-3-medium-4k-instruct & 1123 & 0.64 & 0.64 & 0.18 & 0.34 & 0.43 & 0.47 & 2024.31 & 14.00 & 4.90 \\
    17 & Mixtral-8x7B-Instruct-v0.1 & 1114 & 0.56 & 0.50 & 0.09 & 0.30 & 0.42 & 0.37 & 2023.95 & 47.00 & NaN \\
    18 & 01-ai/Yi-34B-Chat & 1111 & 0.47 & 0.56 & 0.05 & 0.34 & 0.40 & 0.41 & 2023.84 & 34.00 & 3.10 \\
    19 & Qwen1.5-14B-Chat & 1109 & 0.48 & 0.52 & 0.00 & 0.27 & 0.44 & 0.36 & 2024.10 & 14.00 & 3.00 \\
    20 & WizardLM/WizardLM-70B-V1.0 & 1106 & 0.50 & 0.56 & 0.04 & 0.27 & 0.44 & 0.34 & 2023.61 & 70.00 & 2.00 \\
    21 & DBRX-Instruct-Preview & 1103 & 0.54 & 0.54 & 0.07 & 0.34 & 0.43 & 0.37 & 2024.24 & 132.00 & 12.00 \\
    22 & Meta-Llama-3.2-3B-Instruct & 1102 & 0.74 & 0.46 & 0.17 & 0.28 & 0.35 & 0.32 & 2024.74 & 3.00 & 9.00 \\
    23 & microsoft/Phi-3-small-8k-instruct & 1102 & 0.65 & 0.62 & 0.03 & 0.31 & 0.46 & 0.45 & 2024.31 & 7.00 & 4.90 \\
    24 & meta-llama/Llama-2-70b-chat-hf & 1093 & 0.50 & 0.30 & 0.01 & 0.26 & 0.37 & 0.24 & 2023.55 & 70.00 & 2.00 \\
    25 & openchat/openchat-3.5-0106 & 1091 & 0.60 & 0.46 & 0.07 & 0.31 & 0.43 & 0.33 & 2024.02 & 7.00 & 2.00 \\
    26 & berkeley-nest/Starling-LM-7B-alpha & 1088 & 0.55 & 0.44 & 0.08 & 0.30 & 0.41 & 0.32 & 2023.87 & 7.00 & NaN \\
    27 & google/gemma-1.1-7b-it & 1084 & 0.50 & 0.39 & 0.04 & 0.29 & 0.42 & 0.26 & 2024.26 & 7.00 & 6.00 \\
    28 & NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO & 1084 & 0.59 & 0.55 & 0.12 & 0.32 & 0.46 & 0.37 & 2024.04 & 47.00 & NaN \\
    29 & deepseek-ai/deepseek-llm-67b-chat & 1077 & 0.56 & 0.52 & 0.07 & 0.32 & 0.51 & 0.39 & 2024.01 & 67.00 & 2.00 \\
    30 & openchat/openchat\_3.5 & 1076 & 0.59 & 0.44 & 0.07 & 0.30 & 0.42 & 0.32 & 2023.84 & 7.00 & 2.00 \\
    31 & teknium/OpenHermes-2.5-Mistral-7B & 1074 & 0.56 & 0.49 & 0.05 & 0.28 & 0.42 & 0.31 & 2023.83 & 7.00 & NaN \\
    32 & mistralai/Mistral-7B-Instruct-v0.2 & 1072 & 0.55 & 0.45 & 0.03 & 0.28 & 0.40 & 0.27 & 2023.95 & 7.00 & NaN \\
    33 & microsoft/Phi-3-mini-4k-instruct & 1071 & 0.55 & 0.55 & 0.15 & 0.33 & 0.43 & 0.40 & 2024.49 & 3.80 & 4.90 \\
    34 & Qwen1.5-7B-Chat & 1070 & 0.44 & 0.45 & 0.00 & 0.30 & 0.38 & 0.30 & 2024.10 & 7.00 & 3.00 \\
    35 & meta-llama/Llama-2-13b-chat-hf & 1063 & 0.40 & 0.33 & 0.01 & 0.23 & 0.40 & 0.19 & 2023.55 & 13.00 & 2.00 \\
    36 & upstage/SOLAR-10.7B-Instruct-v1.0 & 1062 & 0.47 & 0.52 & 0.00 & 0.31 & 0.39 & 0.31 & 2023.95 & 10.70 & NaN \\
    37 & WizardLM/WizardLM-13B-V1.2 & 1058 & 0.34 & 0.45 & 0.02 & 0.26 & 0.44 & 0.25 & 2023.56 & 13.00 & 2.00 \\
    38 & Meta-Llama-3.2-1B-Instruct & 1054 & 0.57 & 0.35 & 0.03 & 0.28 & 0.33 & 0.17 & 2024.73 & 1.00 & 9.00 \\
    39 & HuggingFaceH4/zephyr-7b-beta & 1053 & 0.50 & 0.43 & 0.03 & 0.29 & 0.39 & 0.28 & 2023.82 & 7.00 & NaN \\
    40 & HuggingFaceH4/zephyr-7b-alpha & 1041 & 0.52 & 0.46 & 0.02 & 0.30 & 0.39 & 0.28 & 2023.79 & 7.00 & NaN \\
    41 & google/gemma-7b-it & 1037 & 0.39 & 0.36 & 0.02 & 0.28 & 0.43 & 0.17 & 2024.14 & 7.00 & 6.00 \\
    42 & Phi-3-Mini-128k-Instruct & 1037 & 0.60 & 0.56 & 0.10 & 0.32 & 0.39 & 0.37 & 2024.31 & 3.80 & 4.90 \\
    43 & meta-llama/Llama-2-7b-chat-hf & 1037 & 0.40 & 0.31 & 0.01 & 0.25 & 0.37 & 0.17 & 2023.55 & 7.00 & 2.00 \\
    44 & google/gemma-1.1-2b-it & 1021 & 0.31 & 0.32 & 0.00 & 0.27 & 0.34 & 0.15 & 2024.26 & 2.00 & 3.00 \\
    45 & allenai/OLMo-7B-Instruct & 1015 & 0.35 & 0.37 & 0.01 & 0.27 & 0.38 & 0.18 & 2024.09 & 7.00 & 2.00 \\
    46 & mistralai/Mistral-7B-Instruct-v0.1 & 1008 & 0.45 & 0.34 & 0.02 & 0.25 & 0.38 & 0.24 & 2023.74 & 7.00 & NaN \\
    47 & lmsys/vicuna-7b-v1.5 & 1005 & 0.24 & 0.39 & 0.01 & 0.26 & 0.42 & 0.21 & 2023.22 & 7.00 & 2.00 \\
    48 & google/gemma-2b-it & 990 & 0.27 & 0.32 & 0.00 & 0.28 & 0.33 & 0.14 & 2024.14 & 2.00 & 3.00 \\
    49 & Qwen1.5-4B-Chat & 988 & 0.32 & 0.40 & 0.01 & 0.27 & 0.40 & 0.24 & 2024.10 & 4.00 & 3.00 \\
    50 & databricks/dolly-v2-12b & 822 & 0.24 & 0.33 & 0.01 & 0.24 & 0.37 & 0.11 & 2023.50 & 12.00 & 0.30 \\
    \bottomrule
    \end{tabular}
    \end{tiny}
    