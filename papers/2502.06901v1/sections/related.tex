\section{Related Works}
\subsection*{Discrete Diffusion}
Discrete diffusion models have emerged as a promising alternative to traditional autoregressive models for text generation and, notably, text infilling. Inspired by the success of diffusion models in continuous domains like image generation \cite{ho2020denoisingdiffusionprobabilisticmodels}, these models adapt the diffusion framework to operate on discrete sequences of tokens. In the context of text infilling, discrete diffusion offers several advantages. Its iterative refinement process allows for fine-grained control over the generated text and the ability to tradeoff quality for efficiency. However, as mentioned in the introduction, these models can be computationally expensive during inference due to the multiple refinement steps and the lack of KV caching.  They also face challenges in scaling up training compared to autoregressive models. In this paper, we will primarily focus on the work of Scaling Masked Diffusion Model (SMDM) \cite{nie2024scalingmaskeddiffusionmodels} and DiffuLlama \cite{gong2024scalingdiffusionlanguagemodels}, but the space includes many promising works \cite{sahoo2024simpleeffectivemaskeddiffusion, liu2024discrete, liu2024thinkgeneratediscretediffusion, hoogeboomardiffusion, ou2024absorbingdiscretediffusionsecretly}
\subsection*{FIM}
AR models can be adapted to perform infilling through a special training process called Fill-in-the-Middle (FIM) \cite{donahue2020enabling}, in which the order of the original sequence is changed such that the middle of the sequence is moved to the end and marked with a special FIM token. These FIM models are particularly useful for coding applications \cite{fried2023incodergenerativemodelcode}. We make a distinction between FIM and masked infilling. FIM necessitates that the infilled text is a contiguous block, while masked infilling can fill in arbitrary sequences of tokens.
\subsection*{MLM and AR Unification}
Notable works to unify MLM and AR modelling include BART \cite{lewis2019bart}. Besides architectural differences, the main distinction between MARIA and BART is that MARIA is applied to existing pretrained MLM and AR models, while BART must be trained end-to-end. Other notable works incorporate together MLM and AR modeling techniques for improved training \cite{du2022glmgenerallanguagemodel, nguyen2023meetmiddlenewpretraining, yu2024antlmbridgingcausalmasked}, but none are expressly targeting masked infilling as an application.