\section{Conclusion}
The introduction of MARIA (Masked and Autoregressive Infilling Architecture) addresses a long-standing gap in the field of natural language processing by seamlessly combining the strengths of autoregressive (AR) and masked language models (MLM). This hybrid approach has demonstrated significant improvements in masked token infilling, achieving lower perplexity scores across diverse datasets and outperforming existing methods like discrete diffusion models in both quality and efficiency. Furthermore, MARIA's integration of KV caching ensures it retains the computational advantages of AR models during inference. 

Future directions include further optimizing the inference algorithm to support modern AR inference techniques. For example, incorporating Paged Attention \cite{kwon2023efficient} would provide tremendous gains in throughput beyond the gains demonstrated in this paper. Also, in this paper, we utilize a pretrained base AR and MLM model. For future work, it is possible to use fine-tuned versions of these models for domain-specific tasks. For example, combining an AR and MLM model specialized for infilling DNA sequences or code blocks could yield strong, highly specialized infilling models.

