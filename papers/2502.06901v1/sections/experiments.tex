\section{Experiments}

\begin{table*}[t]
    \centering
\begin{tabular}{l r r r r r r r}
 \toprule & \\[-2ex]
 Model & Size & Type  & Masking Rate & & & &  \\[0.5ex]
 \hline & \\[-2ex]
 & &  & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 \\[0.5ex]
 \hline & \\[-2ex]

ModernBert & 0.395 B & MLM (AR Decode) & 2.92 & 5.79 & 19.73 & 136.2 & 1468 \\
OLMo 1B & 1.18 B & AR & 22.28 & 22.13 & 22.20 & 22.17 & 22.62 \\
OLMo 7B & 7.3 B & AR & 14.93 & 15.01 & 14.96 & 15.00 & 15.046 \\
SMDM &1.1B & DD & $\leq$ 14.44 & $\leq$ 46.36 & $\leq$ 118.7 & $\leq$ 363.7 & $\leq$ 1391 \\
DiffuLlama & 6.74 B & DD & $\leq$ 10.36 & $\leq$ 30.04 & $\leq$ 68.38 & $\leq$ 180.5 & $\leq$ 599.5\\[1ex]
\hline & \\[-1.5ex]
MARIA 1B (ours) & 1.575 B &MLM + AR& 3.10 & 4.45 & 7.41 & 13.80 & 23.99 \\
MARIA 7B (ours) & 7.695 B & MLM + AR & \textbf{2.82} & \textbf{3.85} & \textbf{5.94} & \textbf{10.11} & \textbf{16.30} \\[1ex]
\bottomrule & \\[-1.5ex]
\end{tabular}
\caption{\textbf{Downstream perplexity for various masking ratios.} We evaluate the downstream perplexity, averaging over 5 standard evaluation sets. ModernBERT is computed autoregressively, and we estimate the upper bound perplexity in the discrete diffusion models. MARIA performs the best by inheriting the strengths of its components: OLMo (AR) and ModernBERT (MLM). Based on parameter counts, MARIA presents the most effective way to scale models for masked token infilling.}
\end{table*}

\label{sec:experiments}
In this section, we evaluate MARIA in a variety of settings against strong baselines. Our key findings include:
\begin{itemize}
    \item \textbf{Superior Perplexity.} MARIA achieves lower perplexity across various masking rates and datasets compared to ModernBERT, SMDM, and DiffuLlama.
    \item \textbf{Efficient Inference.} MARIA offers high throughput by KV caching at inference time. AR decoding with ModernBERT does not scale.
    \item \textbf{High-Quality Samples.} Evaluation using LLM judge based ELO demonstrates that MARIA's generated text is of higher quality than baselines.
    \item \textbf{Better Representations.} MARIA exhibits better representations for a downstream part-of-speech tagging task.
\end{itemize}
\section*{Baselines}
We consider three primary baselines to compare our method against. First, we consider ModernBERT. Although ModernBERT is an MLM model, in practice MLM models can be used autoregressively by progressively filling in masks from left to right. Surprisingly, MLM models demonstrate considerable in-context learning capabilities when used in this manner \cite{samuel2024bertsgenerativeincontextlearners}. Another necessary baseline for masked infilling are discrete diffusion models, of which we select Scaling Masked Diffusion Model (SMDM) \cite{nie2024scalingmaskeddiffusionmodels} and DiffuLlama \cite{gong2024scalingdiffusionlanguagemodels}. These works execute interesting approaches to for scaling MLM models for discrete diffusion. SMDM analyzes MLM scaling laws and is trained in a compute-optimal manner. DiffuLlama distills an MLM model from an existing AR model, namely LLaMA 7B \cite{touvron2023llama}. While these approaches are viable and worthwhile, in the following experiments we shall argue that MARIA is the most pragmatic approach for scaling masked infilling models.
\section*{Downstream Perplexity}
Generative models optimize maximum likelihood objectives, and a common way to compare modeling performance is with likelihood on a test set. Here, we compare a similar notion of perplexity, which is defined as the exponentiated average negative log likelihood on some corpus of tokens. We select five standard datasets to evaluate downstream perplexity: WikiText \cite{merity2016pointer}, LM1B \cite{chelba2014billion}, Lambada \cite{paperno2016lambadadatasetwordprediction}, AG News \cite{zhang2016characterlevelconvolutionalnetworkstext}, and ArXiv papers \cite{clement2019arxiv}. Some of the datasets are tokenized for an MLM word level tokenizer, so we detokenize them following standard procedure \cite{sahoo2024simpleeffectivemaskeddiffusion}. Because the context lengths of models differ, we also compute fixed length perplexity on a rolling basis, that is partitioning corpuses of tokens as necessary to fit within a context and summing over the negative log likelihoods for each partition. We compute the perplexity given 5 different masking rates: 0.1, 0.3, 0.5, 0.7, 0.9 (least to most masked); specifically, the goal is to model the randomly masked tokens given the surrounding unmasked context. From the downstream datasets, we subsample 500 examples from each.

Importantly, discrete diffusion models do not admit an exact perplexity. Instead, we compute the negative evidence lower bound (NELBO) though sampling. While it may seem unintuitive to compare exact perplexities with upper bounds, in practice these bounds are tight \cite{kingma2023variationaldiffusionmodels}, and these comparisons are widespread in the literature \cite{ho2020denoisingdiffusionprobabilisticmodels, gulrajani2023likelihoodbaseddiffusionlanguagemodels}.

We report the average perplexities for seven models. ModernBERT perplexity is computed using the left to right autoregressive distribution that an MLM model admits by successively unmasking and computing the likelihood from left to right. We also compute the perplexities for regular AR models that cannot condition on future tokens. These results show that MLM models poorly model heavily noised text. We speculate that for ModernBERT, which was trained at a fixed mask ratio of 0.3 \cite{warner2024smarter}, performs poorly with higher noise ratios because they are out of distribution. Meanwhile, AR models cannot condition on future context and therefore demonstrate surprisingly strong performance independent of noising rate. MARIA, which is a mixture of OLMo and ModernBERT, achieves the upside of both models with strong performance in low noise settings, and it stays strong as the noise level increases, similar to the AR models. Of note, performance scales with model size, indicating a straightforward way to scale masked infilling capabilities more efficiently than scaling MLM models.


\section*{Throughput}
Efficiency is a crucial reason why AR models are more widely adopted than MLM models. We profile the throughput of each model to better understand how these approaches compare. In light of this, we fix the generation parameters such as number of diffusion steps to the same parameters that will be later used in infilling experiments. Thus, we can analyze these efficiency results with sample quality results in tandem. In Figure \ref{fig:throughput}, we measure the throughput in tokens per second on different length inputs with 50 percent masking. We average the throughput over 10 runs, with 2 warm-up runs in the beginning for each model to ensure the GPU is operating maximally. From the results, we observe that MARIA 1B has the best throughput. Surprisingly, SMDM has worse throughput than larger 7B models. This can be attributed to an expensive classifier free guidance method (which we apply for later results) and miscellaneous implementation details. From Figure \ref{fig:throughput}, it is also critical to observe the performance of ModernBERT. Because ModernBERT is an MLM model incapable of KV caching, it is impractical to use for inference. KV caching models will have an inference runtime $O(n^2)$ in the sequence length, and without caching this runtime is $O(n^3)$. Though we include decoding ModernBERT autoregressively in the experimental benchmarks, poor efficiency at scale makes it severely impractical. Though discrete diffusion models cannot KV cache, they can unmask multiple tokens at each iteration. Thus, we see that DiffuLlama and MARIA 7B have similar throughputs. However, we shall show in the following section that MARIA achieves much better quality for similar efficiency.
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/throughput_plot.png}
    \caption{\textbf{Throughput over input length.} We show the throughput in tokens per second for sequences of given lengths at 0.5 masking rate. MARIA 1B exhibits the best performance, and MARIA-7B is comparable to DiffuLlama 7B. Decoding ModernBERT autoregressively is extremely inefficient at scale, and therefore is impractical in many circumstances.}
    \label{fig:throughput}
\end{figure}
\section*{Sample Quality}
To evaluate sample quality, we adopted the same setting as before using 1000 samples total from the downstream datasets previously described (200 samples each). The task is to infill a random 50 percent of the text for each. However, to ensure comparable masked sequences in light of different tokenizers, we mask 50 percent words by replacing them with the mask string (i.e. \texttt{[MASK]}), ensuring that every model is given the same task. We define a word to be an alphanumeric string with spaces at the beginning and end.

We set the inference time hyperparameters to the respective values that achieved the best results for DiffuLlama and SMDM. For DiffuLlama, it uses nucleus sampling \cite{holtzman2020curiouscaseneuraltext} and temperature scaling of 0.9 each. For SMDM, it applies classifier guidance scaling of 2 with greedy sampling. In all of the following experiments, we use 256 denoising steps. For ModernBERT and MARIA models, we decode greedily.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/elo_plot.png}
    \caption{\textbf{ELO scores for masked infilling.} We perform infilling on downstream data with words masked 50 percent. Using GPT4o-mini as a judge we compute the ELO scores for each model respectively. MARIA 7B and 1B have the highest rating ELO rating under the Bradley-Terry model.}
    \label{fig:elo}
\end{figure}

We assess sample quality using an ELO system judged by GPT-4o mini \cite{achiam2023gpt}. We create 1000 random fixtures and prompt GPT to give a score for each text ``based on coherence, fluency, and style". For ELO scoring, a higher score is a win (1), lower score is a loss (0), and even score is a tie (0.5). We then calculate the ELO through logistic regression using the Bradley-Terry model, the same method as ChatBot Arena \cite{chiang2024chatbotarenaopenplatform}. This method ensures that match order does not influence the final score, which is a problem with iteratively computing online ELO. We employ standard hyperparameters of scale 400, base 10, and inital rating of 1000.

As shown in Figure \ref{fig:elo}, the MARIA models score the highest ELO ratings, with MARIA 7B and 1B attaining the top scores. In the ELO rating system, every difference of 400 corresponds to a 10x improvement in winning odds. From these results, we infer that the win probability of MARIA 7B against SMDM and DiffuLlama are 53.1\% and 57.4\%. Though these differences are not drastic, in practice it is difficult to achieve large differences in win rate if the LLM judge is insufficient to adequately differentiate between texts. Interestingly, the LLM judges the generated texts of four models as higher quality than the ground truth unnoised text. This may be a consequence of greedy decoding producing more likely text than the source text.

\section*{Test Time Scaling}

Discrete diffusion admits a desirable property that more FLOPs can be spent at test time to produce higher quality text. We discuss an alternative method for test time scaling in Section \ref{sec:method}, namely simulated annealing. We apply simulated annealing in MARIA by remasking 30 percent of tokens at each iteration and sampling with MARIA with a progressively lower temperature using a linear schedule. In Figure \ref{fig:inference_scale}, we measure the generative perplexity of 200 unconditional samples according to Llama3 8B \cite{grattafiori2024llama3herdmodels}. We show that for MARIA 1B, simulated annealing is an effective and efficient way to generate higher quality samples, converging faster than both DiffuLlama and SMDM. MARIA 7B with simulated annealing is far slower to converge than MARIA 1B, and it is omitted to avoid plot scaling issues.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/inference_scaling_plot.png}
    \caption{\textbf{Scaling test time compute for unconditional generation.} We compare our simulated annealing inference approach for MARIA to our baseline discrete diffusion methods. MARIA 1B using simulated annealing effectively trades-off quality (as measured by generative perplexity) and with compute (measured in wall clock time).}
    \label{fig:inference_scale}
\end{figure}

\section*{Representations}

Representation learning is a key motivation behind training Transformers with an MLM objective. We aim to analyze MARIA through a representation learning perspective to offer insight into why combining MLM and AR models can improve performance. Specifically, we study the token level representations by measuring performance on part-of-speech tagging. The part-of-speech tagging task has a history in NLP \cite{manning2011part}, and we use the CoNLL-2003 dataset \cite{sang2003introductionconll2003sharedtask}. We train a linear classifier on representations from ModernBERT, MARIA 1B, and MARIA 7B on 10000 sentence examples with POS labels that can belong to 48 different classes. We train for 10 epochs with a learning rate of 1e-4. As Table \ref{tab:representation} shows, part-of-speech tagging accuracy increases with MARIA 1B and further increases with MARIA 7B. These results are somewhat expected because MARIA hidden states are much larger in dimension: ModernBERT has dimension 1024, MARIA 1B has dimension 3072, and MARIA 7B has dimension 5120. These results confirm that AR representations contain information that MLM representations do not due to scale.

\begin{table}[t]
\centering
\vspace{0.5cm}
\begin{tabular}{l l}
\toprule
Representation & Accuracy  \\
\midrule
ModernBERT & $0.642 \pm 0.002$ \\
MARIA 1B & $0.714 \pm 0.002$ \\
MARIA 7B & $0.735 \pm 0.002$ \\
\bottomrule

\end{tabular}
\vspace{0.5cm}
\caption{\textbf{Representation learning for part-of-speech tagging.} We demonstrate that MARIA representations produce higher accuracy when used to predict parts-of-speech. This indicates that the concatenated AR and MLM hidden states of MARIA contain more information than MLM alone.}
\label{tab:representation}

\end{table}
