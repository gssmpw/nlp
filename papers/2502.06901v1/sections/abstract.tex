\begin{abstract}
% Abstract Notes:
% Text infilling is an important but neglected application of language models
% Large Language Models have traditionally been trained using one of the two following objectives: Autoregressive (AR) or Masked Language Modelling (MLM)
% In recent years, due to scalability, AR models have become the dominant paradigm
% AR models cannot do infilling. MLM models can but they are less compute efficient. 
% During training they are suboptimal because they do not utilize the full context for gradient updates
% During inference, they are very slow to sample from because they do not support KV caching.
% In this work, we show that through minimal modifications, AR decoder only transformers can become state of the art at text infilling, while preserving the faster inference improvements inherent to AR models.
% Specifically we train a linear decoder on the concatenated hidden states of an MLM and AR model.
% We call this method MARIA, for Masked and Autoregressive Infilling Architecture.
% Maria demonstrates substantive improvment over existing methods such as discrete diffusion models for text infiling tasks.

Historically, LLMs have been trained using either autoregressive (AR) or masked language modeling (MLM) objectives, with AR models gaining dominance in recent years. However, AR models are inherently incapable of masked infilling, which is the ability to predict masked tokens between past and future context. In contrast, MLM models suffer from intrinsic computational inefficiencies during both training and inference that hinder their scalability. This work introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that leverages the strengths of both paradigms to achieve state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM and AR model by training a linear decoder that takes their concatenated hidden states as input. This minimal modification enables the AR model to perform infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our results demonstrate that MARIA significantly outperforms existing methods, namely discrete diffusion models, on masked infilling tasks.
\end{abstract}