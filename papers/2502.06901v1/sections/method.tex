\section{Method}
\label{sec:method}
\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/maria_diagram.png}
    \caption{\textbf{MARIA architecture and training pipeline.} MARIA takes two frozen pretained models: one MLM and one AR. As input, the MLM recieves the masked inputs and the AR model recieves the denoised inputs. We compute the hidden states under each model and perform truncating and shifting operations to ensure both hidden states model the same tokens. MARIA trains a linear layer to predict the logits of each masked input on the concatenated hidden states. This training scheme models an autoregressive distribution conditioned on unmasked tokens.}
    \label{fig:maria_diagram}
\end{figure*}
\section*{Background}
Consider an autoregressive model $\pi_{\mathrm{AR}}$ and masked language model $\pi_{\mathrm{MLM}}$. Given access to a dataset $\mathcal{D} = \{x_1, x_2, ...\}$, autoregressive models are trained to maximize the joint likelihood given by

% \[L_{\mathrm{AR}} = -E_{x\sim D} \left( \sum_{i} \log \pi_{\mathrm{AR}}(x_i \mid x_{<i}) \right)\]
% \[L_{\mathrm{MLM}} = -E_{x \sim D, m \sim M}\left( \sum_{i} \log \pi_{\mathrm{MLM}}(x_m \mid x_{\setminus m}) \right)\]

\begin{equation}
    \mathcal{L}_{\mathrm{AR}} = -\mathbb{E}_{x\sim \mathcal{D}} \left[ \sum_{i} \log \pi_{\mathrm{AR}}(x_i \mid x_{<i}) \right]
\end{equation}
Masked language models employ a masking objective  that assumes a distribution over masks $\mathcal{M}$, where $m \in \mathcal{M}$ is selection of indices $m = \{i_1, i_2, ...\}$.
\begin{equation}
    \mathcal{L}_{\mathrm{MLM}} = -\mathbb{E}_{\substack{x \sim \mathcal{D}\\m \sim \mathcal{M}}}\left[ \sum_{i \in m} \log \pi_{\mathrm{MLM}}(x_i \mid x_{\setminus m}) \right]
\end{equation}
Also observe that each language model is composed of a function $h$ that embeds inputs into hidden state vectors and a linear weight matrix $W$ used to decode the hidden states into logits.
\begin{equation}
    \pi_{\mathrm{AR}}(x \mid \cdot\,) = \sigma\left(W_1 h_1(x)\right) 
\end{equation}
\begin{equation}
    \pi_{\mathrm{MLM}}(x \mid \cdot\,) = \sigma\left(W_2 h_2(x)\right)
\end{equation}
where $\sigma(z_i) = e^{z_i}/ \Sigma_j e^{z_j}$ is the softmax function. We define $W_1 \in \mathbb{R}^{d_1 \times v}$ and $W_2 \in \mathbb{R}^{d_2 \times v}$ such that their hidden dimensions $d$ can be different but vocabulary size $v$ are the same.
\section*{MARIA}
\subsection*{Objective}
The MARIA architecture can be defined very straightforwardly with a linear layer on the concatenated hidden states of an AR and MLM model.
\begin{equation}
    \pi_{\mathrm{MARIA}}(x \mid \cdot\,) = \sigma(W_3 \, [h_1(x); h_2(x)])
\end{equation}
where $W_3 \in \mathbb{R}^{(d_1+d_2) \times v}$. Finally, we may now define an objective that is both autoregressive and masked. Let $c(i, m) = \{x_{<i}, x_{>i \, \cap \, \setminus m }\}$ define the union of tokens before the index $i$  and all unmasked tokens after $i$.
\begin{equation*}
\mathcal{L}_{\mathrm{MARIA}} = -\mathbb{E}_{\substack{x \sim \mathcal{D}\\m \sim \mathcal{M}}}\left[ \sum_{i \in m} \log \pi_{\mathrm{MARIA}}(x_i \mid c(i, m) \right]
\end{equation*}
This objective defines the expected negative log likelihood of an autoregressive distribution conditioned on unmasked tokens.
\subsection*{Training Procedure}
MARIA training can be parallelized in a similar manner as a typical autoregressive Transformer. For a clean input sequence $X_{1:n}$, we consider its masked counter part $M_{1:n}$. The AR model receives the clean inputs and the MLM model receives the masked inputs such that we compute the hidden state $[h_1(X); h_2(M)]_{1:n}$ concatenated on the sequence dimension. These are then decoded with $W_3$ to next token logits. Thus, the autoregressive loss is computed over the entire sequence in parallel. This training procedure is best depicted by Figure \ref{fig:maria_diagram}. 
\subsection*{Initialization}
As part of our method, we also provide a way to initialize the newly defined MARIA weights $W_3$. Because we have access to existing weights of pretrained models, namely an autoregressive weights $W_1$ and masked weights $W_2$, we can initialize $W_3$
\begin{equation}
    W_3 \leftarrow [W_1/2; W_2/2]
\end{equation}
Observe that this will output the average of the logits of $\pi_{\mathrm{AR}}$ and $\pi_{\mathrm{MLM}}$
\begin{align}
    \pi_{\mathrm{MARIA}}(x \mid \cdot\,) &= \sigma([W_1/2; W_2 / 2] \, [h_1(x); h_2(x)]) \\
    &= \sigma((\pi_{\mathrm{AR}}(x \mid \cdot\,)+\pi_{\mathrm{MLM}}(x \mid \cdot\,))/2)
\end{align}
This is a good initialization because the average of logits corresponds to a multiplicative mixture of the two original distributions. This ensemble, known as product of experts \cite{hinton2002training}, has proven effective in the context of LLMs \cite{liu2021dexperts}. Smart weight initialization leads to faster and better convergence \cite{samragh2024scaling}, and we demonstrate this with ``product initialization" for MARIA in Figure \ref{fig:init}.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/init_plot.png}
    \caption{\textbf{Comparing evaluation loss curves for two different weight initializations.} Product initialization (ours) is a far better weight initialization than random weight initialization, leading to faster training and better convergence.}
    \label{fig:init}
\end{figure}
\subsection*{Unconditional Generative Model}
While MARIA is trained to sample conditionally, we propose a method to sample unconditionally. The desideratum of this generative model is to make possible iterative refinement of text such that more compute leads to better samples. Discrete diffusion has this property for the number of denoising steps, and while it is possible to use MARIA directly as a discrete diffusion model, it is undesirable because autoregressive sampling at each times step is slow, and discrete diffusion only unmasks a small number of tokens at a time, remasking most samples at every iteration. Thus, we propose using MARIA as a generative model with an inference strategy inspired by simulated annealing \cite{bertsimas1993simulated}. We can describe the process as follows:
\begin{enumerate}
    \item Sample from the base AR model at temperature 1.
    \item Using MARIA, resample a fixed percentage of tokens autoregressively at temperature $T$.
    \item Repeat the process for some number of iterations, annealing $T$ from 1 to 0.
\end{enumerate}
This inference strategy is a way to optimize over the joint likelihood of a sequence, and it is an improvement over standard greedy sampling because it is non-myopic \cite{shih2023long}. More formally, we are sampling from the following distribution:
\begin{align*}
    % p_S(x) =  \prod_{t\in S} \underset{m\in \mathcal{M}}{\mathbb{E}} \pi_{\mathrm{MARIA}}(x_m \mid x_{\setminus m} ; t \,)
    &p\left(x^{i}\right)\propto\\
    &\sum_{\subalign{x^{1:i-1}\\m^{1:i-1}}}\prod_{j=1}^i  \pi_{\mathrm{MARIA}}\left(x^{j}\,\Big|\, x^{j-1},m^{j-1} ; t_{j-1} \,\right)
\end{align*}

where $x^{i}$ is the sequence at step $i$, $t_j$ is a temperature at step $j$, $m^{k}$ is a mask at step $k$, and $\pi_{\mathrm{MARIA}}(\,\cdot \,; t)$ denotes the autoregressive MARIA distribution temperature scaled by $t$.

\section*{Implementation}
\subsection*{Models}
A key constraint of MARIA is that the combined AR and MLM models must be trained with the same tokenizer. We make use of two important open-source works that are both trained with a GPT2 \cite{radford2019language} based tokenizer: ModernBERT \cite{warner2024smarter} and OLMo \cite{groeneveld2024olmo}. We train two models: 
\begin{itemize}
    \item MARIA 1B: a model composed of ModernBERT-Large and pretrained OLMo 1B
    \item MARIA 7B: a model composed of ModernBERT-Large and pretrained OLMo 7B
\end{itemize}
We will refer to these models in this manner throughout the course of the paper.

% \begin{itemize}[leftmargin=*, labelsep=5pt]
%   \item MARIA 1B, composed of ModernBERT-Large and pretrained OLMo 1B
%   \item MARIA 7B, composed of ModernBERT-Large and pretrained OLMo 7B
% \end{itemize}


\subsection*{Training}
Our training data is composed of high quality tokens from FineWebEdu \cite{penedo2024finewebdatasetsdecantingweb}, a standard pretraining corpus curated for fast convergence and good downstream performance. We randomly mask the data by sampling masking rates from a Beta(2.5, 2.5) distribution, which is more effective than a uniform rate \cite{shen2023filmfillinlanguagemodels}.
To train the MARIA Linear Layer, we initialize the weights as previously described. For MARIA 1B and MARIA 7B respectively, we train for 90000 steps (approximately 30 billion tokens) and 25000 steps (approximately 7 billion tokens). Given the size of FineWebEdu, we complete less than a single epoch, and we evaluate test loss on ten thousand holdout examples. We train at batch size 32 using gradient accumulation with a learning rate of 5-e5 and cosine learning rate schedule. Our training hardware is comprised of 8 NVIDIA 48GB A6000 GPUs connected to a Colfax CX41060s-EK9 4U Rackmount Server with AMD EPYC (Genoa) 9124 processors.

\subsection*{Inference}

\begin{algorithm}[t]
\caption{MARIA KV Cached Inference}
\label{alg:maria_inference}
\begin{algorithmic}[1]
% texttt
    \STATE \textbf{Input:} \texttt{input\_ids}, \texttt{masked\_indices}
    \STATE \textbf{Output:} \texttt{input\_ids} with infilled \texttt{[MASK]} tokens\\\COMMENT{Get MLM hidden states once}
    % \STATE 
% \PROCEDURE{Infill}{$input\_ids$, $greedy$, $temperature$}
    %\STATE \texttt{mlm\_hidden\_states} $\gets$ \text{MLM\_Model}(\texttt{input\_ids})
    \STATE \texttt{mlm\_hidden\_states} $\gets$ \text{MLM\_Model}\texttt{(input\_ids)}
    \STATE \texttt{past\_kv} $\gets$ \texttt{None}
    \STATE \texttt{prev\_idx} $\gets$ 0
    \FOR{\texttt{curr\_idx} $\in$ \texttt{masked\_indices}}
        \STATE \texttt{ar\_input} $\gets$ \texttt{input\_ids[prev\_idx:curr\_idx]}\\\COMMENT{Run AR model with caching}
        \STATE \texttt{ar\_output} $\gets$ \texttt{\text{AR\_Model}(ar\_input, past\_kv)}\\\COMMENT{Update cache}
        \STATE \texttt{past\_kv} $\gets$ \texttt{ar\_output.\text{past\_kv}}
        \STATE \texttt{ar\_hidden\_state} $\gets$ \texttt{ar\_output.\text{hidden\_states}}
        \STATE \texttt{maria\_hidden\_states} $\gets$ \texttt{\text{Concat}(}\\ \qquad\texttt{ar\_hidden\_state,}\\ \qquad \texttt{mlm\_hidden\_states[curr\_idx]} \\)
        \STATE \texttt{logits} $\gets$ \texttt{\text{MARIA\_Linear}(maria\_hidden\_states)}
        \STATE \texttt{sampled\_token} $\gets$ \texttt{\text{Sample}(logits)}\\\COMMENT{Fill in the mask}
        \STATE \texttt{input\_ids[curr\_idx]} $\gets$ \texttt{sampled\_token} 
        \STATE \texttt{prev\_idx} $\gets$ \texttt{curr\_idx}
    \ENDFOR
    \STATE \textbf{return} \texttt{input\_ids}
\end{algorithmic}
\end{algorithm}

As we will further argue in Section \ref{sec:experiments}, AR models have an advantage at inference time over MLM models with the ability to reuse previous computations through KV caching. Transformers with bidirectional masking cannot cache the computations from previous samples because sampling a new token will change the representations of all existing future tokens. We present a simple KV caching inference algorithm with MARIA in Algorithm \ref{alg:maria_inference}. This algorithm computes a single forward pass on the MLM model to compute hidden states. After this negligible overhead, we perform standard KV caching just the same as a standard AR model.
