\section{Introduction}

% Introduction Notes
% Talk about LLMs, Transformer architecture
% Talk about AR vs MLM objectives, also differ in causal vs bidirectional masking
% NonAR paradigms that support infilling, discrete diffusion
% AR paradigms scale better. Why?
% How to combine AR and MLM

The field of natural language processing (NLP) has witnessed remarkable advancements in recent years, largely driven by the advent of large language models (LLMs) \cite{zhao2023survey} built upon the Transformer architecture \cite{vaswani2017attention}. These models, characterized by their self-attention mechanisms and vast parameter counts, have demonstrated unprecedented capabilities in understanding and generating human-like text.

A critical aspect of LLM training lies in the choice of pre-training objective. Traditionally, two dominant paradigms have emerged: autoregressive (AR) and masked language modeling (MLM). AR models, such as GPT \cite{achiam2023gpt}, are trained to predict the next token in a sequence, given the preceding context. This left-to-right approach, coupled with causal masking that prevents the model from ``seeing" future tokens, enables efficient training and inference. MLM models, exemplified by BERT \cite{devlin2019bertpretrainingdeepbidirectional}, are trained to predict masked-out tokens in a sequence, leveraging bidirectional context from both past and future tokens.

One notable capability where AR models typically fall short is text infilling \cite{donahue2020enabling}, the task of predicting missing tokens within a given text span, surrounded by both preceding and subsequent context. While MLM models inherently support infilling due to their bidirectional nature, AR models, with their unidirectional processing, cannot leverage future context for this task.  This limitation restricts the applicability of AR models in scenarios where infilling is essential, such as interactive text editing \cite{lee2022coauthor}, code completion \cite{liu2020multi}, and structured generation \cite{xia2024fofo}. 

\begin{table*}[t]
\label{tab:maria_summary}
    \centering
\begin{tabular}{l c c c}
\toprule
 Model & Scalable Training & KV Cached Inference  & Supports Mask Infilling   \\
 \midrule
 AR & \cmark & \cmark & \xmark \\
 MLM & \xmark & \xmark &  \cmark \\
 MARIA & \cmark & \cmark & \cmark \\

\bottomrule
\end{tabular}
\caption{\textbf{Comparison of different modeling approaches.} We compare the three modelling approaches: Autoregressive (AR), Masked Language Modelling, and our method Masked and Autoregressive Infilling Architecture (MARIA). While AR enjoys more scalable training and computationally efficient inference, it cannot perform masked infilling. Contrarily, MLM can but is less scalable. We argue that our method MARIA inherits the benefits from both approaches.}
\end{table*}

Despite the limitations of AR models in handling text infilling, they remain the dominant paradigm for large-scale language modeling due to their superior scalability. AR models benefit from several key advantages that make them more efficient during both training and inference. First, AR models can exploit causal masking to parallelize every next token prediction, enabling faster training on massive datasets across multiple GPUs. This differs from MLM models, which only make predictions for a fixed ratio of masked tokens during training, such as 15 percent in BERT. Second, the sequential nature of AR models allows for the use of KV caching at inference time, which significantly reduces the computational cost of attention operations by reusing previously computed embeddings. Significant effort has been dedicated to optimizing the memory and speed of KV caching \cite{kwon2023efficient, zhao2024prepacking, liu2024scissorhands}. Thus, AR models are better suited for real-time applications, such as chatbots and virtual assistants, where low-latency responses are critical. These factors contribute to the widespread adoption of AR models in industry and academia, despite their inherent limitations for infilling.

Researchers have explored non-autoregressive paradigms that support text infilling. One such approach is discrete diffusion \cite{lou2023discrete}, which iteratively refines a noisy input sequence. Discrete diffusion models have shown promise in tasks like text generation and infilling. However, discrete diffusion models are built on the MLM modeling paradigm, making it difficult to scale their training in the same manner as AR models. Furthermore, these models often require numerous refinement steps and do not support KV caching, which can make them less efficient for inference. 

Given the complementary strengths and weaknesses of AR and MLM models, there is a clear need for a hybrid approach that leverages the best of both paradigms. In this work, we introduce MARIA (Masked and Autoregressive Infilling Architecture), a novel framework that combines the benefits of AR and MLM models to achieve state-of-the-art performance in text infilling. MARIA integrates a pre-trained MLM and AR model by training a linear decoder that takes the concatenated hidden states of both models as input. This minimal modification enables the AR model to perform effective infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our experiments demonstrate that MARIA significantly outperforms existing methods, including discrete diffusion models, on a variety of text infilling benchmarks. By bridging the gap between AR and MLM paradigms, MARIA offers a new technique for scaling infilling language models. We summarize the advantages of MARIA in Table \ref{tab:maria_summary}.
