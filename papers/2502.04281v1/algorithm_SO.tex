
\begin{algorithm}[t]
\caption{Update for Split Optimization}
\label{alg:SOUpdate}
\begin{algorithmic}[1]
\Function{Update}{$Q_\theta$, $Q_{\theta'}$, $\mathcal{D}$, env}
    \State Sample a mini-batch of $n$ experiences from replay buffer $\mathcal{D}$
    \State Unpack $Q_\theta$ into $U_\theta$ and $F_\theta$
    \State Unpack $Q_{\theta'}$ into $U_{\theta'}$ and $F_{\theta'}$
    
    \For{each experience $\langle \mathbf{o}, \mathcal{A}, \mathbf{r}_u, \mathbf{r}_f, \mathbf{o}' \rangle$ in the mini-batch}
        
        \State Compute the combined Q-values for the successor observation:
        \State \hspace{1em} $Q_\theta(\mathbf{o}') = (1 - \beta)  U_\theta(\mathbf{o}') + \beta  F_\theta(\mathbf{o}')$
        
        \State Solve ILP to get the optimal action $\mathcal{A}^*$ for the next observation $\mathbf{o}'$:
        \State \hspace{1em} $\mathcal{A}^* = \text{solve\_ILP}(Q_\theta(\mathbf{o}'), \text{env.constraints})$
        
        \For{model in \{$U, F$\}}
            \If{model is \( U \)}
                \State Set $M_\theta = U_\theta$, $M_{\theta'} = U_{\theta'}$, and $r = \mathbf{r}_u$
            \Else
                \State Set $M_\theta = F_\theta$, $M_{\theta'} = F_{\theta'}$, and $r = \mathbf{r}_f$
            \EndIf
            
            \State Compute the target for the TD update:
            \State \hspace{1em} $\text{target} = r + \gamma  M_{\theta'}(\mathbf{o}', \mathcal{A}^*)$
            
            \State Compute the TD loss:
            \State \hspace{1em} $\text{loss} = \left( M_\theta(\mathbf{o}, \mathcal{A}) - \text{target} \right)^2$
            
            \State Perform gradient descent on the TD loss to update $M_\theta$
        \EndFor
    \EndFor
    
\EndFunction
\end{algorithmic}
\end{algorithm}