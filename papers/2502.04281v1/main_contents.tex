
\section{Introduction}

\label{sec:intro}

AI has become an essential component of many modern systems, playing a crucial role in automating complex decision-making processes. Increasingly, AI algorithms are used to make decisions that impact millions of people. In multi-agent settings, these decisions are often optimized for overall system utility. However, this utilitarian approach can introduce biases, making fairness an important consideration in AI-driven decision-making.

We introduce a class of problems called \emph{Distributed Evaluation, Centralized Allocation (DECA)}. To the best of our knowledge, prior work has addressed these problems separately using domain-specific solutions~\citep{shah2020neural, qin2022RLforRides,kube2019allocating, kube2023community}. In this paper, we take a first step toward unifying them under a single DECA framework and propose fairness-oriented methods that apply broadly across DECA problems.

In DECA, multiple agents act within an environment while a central controller coordinates their behavior to ensure resource constraints and environmental requirements are met. Each agent evaluates its own actions (Distributed Evaluation, DE), and the central controller aggregates these evaluations, optimizes for system-wide utility, and assigns actions accordingly (Centralized Allocation, CA). These problems are dynamic in nature, 
with time-varying resources and agents with variable action spaces. 
This makes DECA both computationally challenging and practically significant.\footnote{
Note that DECA is an execution paradigm, not a learning paradigm like Centralized Training/Decentralized Execution (CTDE). CTDE can be used to train agents that operate in a DECA environment, as we do in this work.
}

Fairness in DECA-based decision-making is critical, as algorithmic biases can lead to disparities and decreased trust in automated systems~\citep{mehrabi2021MLBiassurvey}. Beyond ethical considerations, fair resource allocation may also be desirable from the perspective of the central controller. Standard DECA solutions rely on estimating agent utilities and solving a constrained optimization to compute the best action for each agent. To improve fairness, we propose a framework that integrates fairness estimation into DECA, allowing for more balanced allocations. Specifically, we introduce three optimization strategies:

\squishlist
\item \textbf{Joint Optimization (JO)}: A scalarized multi-objective learning approach that jointly optimizes for fairness and utility.
\item \textbf{Split Optimization (SO)}: A method that learns separate fairness and utility estimators, enabling online trade-off adjustments for fairness and utility.
\item \textbf{Fair-Only Optimization (FO)}: A fairness-focused approach that modifies an existing black-box utility function to incorporate fairness considerations.
\squishend

Our approach is broadly applicable across different domains and fairness metrics, as demonstrated through empirical evaluations. We compare these methods and show how each offers unique advantages in different scenarios, using variance in agent utilities as a fairness metric. Notably, the Split and Fair-Only Optimization approaches enable real-time tuning of the fairness-utility trade-off, an important consideration for real-world applications that has been overlooked in prior work on fairness in multi-agent RL.

This work addresses a critical gap in the literature by providing a unified framework for fairness in DECA problems, which encompass a wide range of multi-agent decision-making scenarios. Through our proposed optimization strategies (JO, SO, and FO), we offer a flexible framework for balancing fairness and efficiency in real time. Furthermore, we present the first general approach for integrating fairness into multi-agent resource allocation using Q-learning, paving the way for future advancements in fair AI decision-making.


\section{Related Work}
\label{sec:related}

While DECA has not been formalized in prior work, it has seen application in many domains. From optimizing passenger-driver matches in ridesharing~\citep{shah2020neural, qin2022RLforRides} to efficient allocation of homelessness resources~\citep{kube2019allocating, kube2023community}, many real-world applications follow this general structure. 
It is also analogous to the predict-then-optimize (P+O) approach \citep{wang2021PtO, elmachtoub2022smartPtO}, where a predictive model estimates unknown parameters that are subsequently used in optimization. However, unlike P+O, which may not specifically address resource allocation or multi-agent systems, DECA explicitly focuses on these complexities. This distinction is crucial as it allows us to restrict the problem space and tailor our research towards enhancing fairness within multi-agent resource allocation.

Significant research has addressed algorithmic bias, where ML models, such as those used in hiring decisions \citep{raghavan2020hiringbias}, can exhibit harmful biases. 
We refer readers to an extensive survey by \citet{mehrabi2021MLBiassurvey} for a review of recent work.
These studies typically focus on debiasing the outputs of predictive models to meet fairness criteria such as equalized odds~\citep{hardt2016equality} or demographic parity~\citep{DP_dwork2012}. However, our work diverges from this approach. Instead of correcting biases in predictions, we aim to develop algorithms that inherently promote fair decision-making via the actions they optimize.

Our focus in this paper is on designing ways to learn fair policies in a multi-agent RL setting. \citet{FairRLSurvey} present a survey on RL methods used to improve fairness. We now highlight a few papers that are the closest to our work: FEN~\citep{jiang2019FEN} uses a hierarchical network to learn a fair-efficient policy for multi-agent coordination, learning to optimize the coefficient of variation, with a meta network that selects when each agent behaves greedily or fairly. However, the model does not allow for resource constraints, instead opting for a first-come-first-serve approach. Further, this approach needs communication between agents to allow agents to choose between acting fairly and efficiently. Some methods, on the other hand, propose to optimize fairness in a multi-objective MDP, where each agent's utility is treated as a separate objective, and the goal is to optimize the a social welfare function over agent rewards~\citep{zimmer2021MOMDP,siddique2020MOMDP}. 
This means the learning agent has to predict the utility over the joint action space~\citep{siddique2020MOMDP}, or, as done by SOTO~\citep{zimmer2021MOMDP}, use a decentralized policy gradient based approach, which prevents use of global constraints. 
Finally, SI~\citep{SI_kumar2023} is an approach for improving fairness in rideshare-matching that attempts to improve fairness through myopic fairness post-processing of black-box utility estimates. However, it does not attempt to learn long-term fairness, and is specially designed for the ridesharing domain.

The DECA approach allows us to consider global constraints while allocating resources, opening up the scope for better global solutions, which none of the prior approaches allow. The distributed evaluation allows each agent to only learn a local value function, which reduces the complexity when compared to learning a joint policy. Further, our Split and Fair-Only approaches allow changing the trade-offs between utility and fairness post-training, which provides additional flexibility that previous approaches lack. 


\input{problem_formulation}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/variance.png}
    \caption{Change in system utility and fairness as $\beta$ is increased, with $\beta=0$ at the top left $\beta=1$ at the bottom-right. For all domains, we can see that split and joint optimization perform similarly, while learning only fairness can sometimes be slightly worse. All our methods Pareto-dominate SOTO and FEN. Each point depicts the average performance over five different models trained at that $\beta$ value, and the lines show the Pareto front for each method.}
    \label{fig:main_results}
\end{figure*}

\section{Experimental Setup}

We conduct experiments for maximizing the objective in Eq.~\ref{eq:objective}, where the system utility is the sum of all agent utilities at the end of an episode, and the fairness is measured as the negative of the variance of agent resources at the end of the episode. We perform experiments for a variety of $\beta$ values, repeating each configuration 5 times for each of our three settings: \textbf{Joint Optimization (JO)}, \textbf{Split Optimization (SO)} and \textbf{Fair-Only Optimization (FO)}. 
% , and the code will be made publicly available after acceptance 
We were unable to use off-the-shelf multi-agent RL libraries because of their lack of support for constrained central decision making. Thus, we also implemented our own versions of the learning algorithm (DDQN with $\epsilon$-greedy TD(0) learning), as described in Section~\ref{sec:fairness_in_decas}.
Each model uses the same network architecture, with two hidden layers of dimension 20, and  output of dimension 1.
The utility model used for FO is randomly selected from the JO models trained with $\beta=0$. We included features indicating the relative advantage of each agent as a signal for fairness, in addition to the features describing the local observation of each agent.

\subsection{Environments}
We adapt the environments from \citet{jiang2019FEN} to align with the DECA framework, reformulating them as resource allocation problems with constraints. Additionally, we introduce a new environment, BiasedDM, featuring a biased decision-maker with differing utilities for agents. Below is a description of each environment.

\noindent \textbf{Matthew:} This environment showcases the Matthew effect~\citep{rigney2010matthew, gao2023matthew}, where the rich get richer. Ten agents move on a continuous unit grid with three resources available at a time. Consuming resources grants agents speed and size boosts, allowing faster access to future resources. Some agents start with inherent advantages. Actions involve assigning resources to agents, preventing others from accessing them, or taking a null action to move randomly. Agents provide utility estimates for each action, and the decision-maker allocates resources, ensuring no two agents share the same resource. Agents always move in straight lines toward their targets and are unable to target new resources while moving to collect a previously allocated resource.%, abstracting motion planning.

\noindent \textbf{Job:} Four agents operate on a discretized grid with a fixed square containing a job. Agents receive rewards for occupying the job's location. Grid locations act as resources, with only one agent allowed per location. Agents move in cardinal directions and communicate directional preferences to the decision-maker, who assigns final moves.

\noindent \textbf{JobAlloc:} This simplified version of the Job environment removes the grid. Agents directly compete to occupy the job, with actions limited to occupying or leaving it. The job can only be claimed if it is unoccupied. This domain's challenge is overcoming the single-step suboptimality when no agent occupies the job.

\noindent \textbf{Plant:} Five agents operate on a discretized grid containing eight resources of three types. Agents must collect specific resource combinations to construct a `unit' and earn rewards. Requirements vary in difficulty across agents. The decision-maker assigns resources based on agents' preferences, ensuring exclusivity. Agents deterministically move toward assigned resources.

\noindent \textbf{BiasedDM:} Unlike the other environments, this environment introduces an explicit bias in decision-making. Five agents compete for a single resource per timestep, where utility to the decision-maker increases with agent index ($0.2 \times i$ for agent $i$). Optimal utility is achieved by always allocating resources to agent 5. Fairness is assessed based on resource distribution over time, highlighting a disconnect between fairness and utility.



\subsection{Baselines}

As we described in our Related Work section, FEN~\cite{jiang2019FEN} and SOTO~\cite{zimmer2021MOMDP} are two most relevant approaches that are generalizable to different domains. We thus compare against them in our experimental evaluations. 
However, they both operate in environments where agents can independently take actions without explicitly accounting for resource constraints, making them incompatible with the DECA framework. We attempt two methods of making constrained decisions with per-agent policies to adapt them:
\squishlist
    \item \textbf{Policy as Q-values:} We treat the action probabilities as Q-values and use them for the central allocation. This is denoted by the \textbf{`\_ILP'} suffix in the experiments.
    \item \textbf{Masked sequential action selection:} We go through agents sequentially and let them sample an action from their policy and assign it to them. Any invalid actions are masked as the resources get consumed. We randomize the order of agents every step to prevent ordering bias. This is denoted by the \textbf{`\_Mask'} suffix in the experiments.
\squishend
We add the extra features that SOTO requires (only for SOTO), and train SOTO with both the $\alpha$-fair and $GGF$ objective described in their paper~\cite{zimmer2021MOMDP}. We also use shared weights across agents in our experiments. 



\input{restable}
\section{Results}

Figure \ref{fig:main_results} shows the performance of all three DECAF methods (JO, SO, FO) on the five domains discussed above. For each method, we varied the hyperparameter $\beta$ controlling the fairness-utility trade-off (Eqs.~\ref{eq:JO}, \ref{eq:SO}, \ref{eq:FO}), starting with $\beta=0$ (top-left) and increasing to $\beta=1$ (bottom-right).  As mentioned in Section \ref{sec:fairness_in_decas}, we present results where we optimize for variance as the fairness function here. Additional results on learning with different fairness functions are included in the supplement.

\subsection{Efficacy of the Fairness-Utility Optimization}
For all domains, all three methods are able to learn expressive policies which lie at various points close to the Pareto front. This shows that the optimization allows the model to trade off utility and fairness to show diverse behaviors as required by the user. This also confirms that the fairness reward proposed for minimizing variance is a good signal.

\subsection{Comparison Against Baselines}

As seen in Figure~\ref{fig:main_results}, our methods Pareto-dominate both FEN and SOTO in all experiments, with SOTO\_Mask with GGF being the most competitive. 
Since the baselines were not trained on variance, we also compare the performance of DECAF on other metrics of interest.
For a more granular comparison, we select one $\beta$ value for JO, SO, and FO each, and compare it to the other methods across a variety of metrics. 
Table~\ref{tab:BaselineResults} shows the results for all domains, where we see that our methods provide better results all across the board.
Between the masked and ILP versions of the baselines, the masked versions perform better. This is to be expected, as using the ILP to select the best actions results in trajectories that are not on-policy for each agent, which breaks the requirements for policy gradient methods. The masked approach, on the other hand, cannot benefit from the centralized decision-making, and the random order of agents can lead to suboptimal behavior and missed opportunities. Further, it is difficult to extend policy gradient methods to variable or combinatorial action spaces, while our approach allows for arbitrary action spaces, as long as reasonable post-decision states can be approximated. Thus, in the DECA setting, Q-learning based methods like DECAF have the upper edge.


\begin{figure}[t]
    \centering
    \subfloat[System Utility]{\includegraphics[width=0.475\linewidth]{Figures/Generalization/Matthew/Split_system_utility.png}
    \label{fig:matthew-a}}
    \subfloat[Variance]{\includegraphics[width=0.475\linewidth]{Figures/Generalization/Matthew/Split_fairness.png}
    \label{fig:matthew-b}}
    \caption{Evaluation of SO models trained on $\beta_{train}$ and evaluated on $\beta_{test}$ for the Matthew environment. Brighter colors indicate better outcomes.}
    \label{fig:matthew}
\end{figure}

\begin{figure}[t]
    \centering
    \subfloat[System Utility]{    
    \includegraphics[width=0.475\linewidth]{Figures/Generalization/Matthew/SplitNoUtility_system_utility.png}
    \label{fig:matthew_FO-a}}
    \subfloat[Variance]{    \includegraphics[width=0.475\linewidth]{Figures/Generalization/Matthew/SplitNoUtility_fairness.png}
    \label{fig:matthew_FO-b}}
    \caption{Evaluation of FO models trained on $\beta_{train}$ and evaluated on $\beta_{test}$ for the Matthew environment. Brighter colors indicate better outcomes.}
    \label{fig:matthew_FO}
\end{figure}
\begin{figure}[t]
    \centering
    \subfloat[Generalization of SO]{
    \includegraphics[width=0.45\linewidth]{Figures/Generalization/Approx_pareto/Matthew/Split_ApproxPareto.png}
    \label{fig:matthew-apprx-Pareto-SO}}
    \subfloat[Generalization of FO]{    \includegraphics[width=0.45\linewidth]{Figures/Generalization/Approx_pareto/Matthew/SplitNoUtility_ApproxPareto.png}
    \label{fig:matthew-apprx-Pareto-FO}}
    \caption{Approximated Pareto fronts using sparse $\beta_{train}$ evaluated on other $\beta$ values for the Matthew domain.}
    \label{fig:matthew_apprx_Pareto}
\end{figure}

\subsection{Comparison of DECAF Methods}
In our results, JO and SO generally exhibit similar performance characteristics, suggesting that simultaneous evolution of utility and fairness estimates is beneficial. FO is also very competetive, but in some cases, it falls below the Pareto front.
This underperformance is likely due to out-of-distribution transitions for the fixed utility model, which are more problematic in FO when a large fairness weight $\beta$ is used, causing a larger shift in the state distribution and resulting in degraded utility estimates. We expect this to be a bigger issue in more complicated environments, or with poor black-box utility functions.

\subsection{Generalization Using Split Optimization (SO)}
Figure~\ref{fig:matthew} shows detailed results for the Matthew environment, when using SO. We evaluate each model trained on a particular $\beta_{train}$ on all other $\beta_{test}$. This allows us to see how well the trained fairness and utility models generalize when the operating $\beta$ is changed. Note that for all these models, $\beta$ is not provided as a feature to the Q-function. 

The diagonal elements show the behavior when training and testing is done on the same $\beta$ value.  
From the plots for system utility (Figure \ref{fig:matthew-a}) and variance (Figure \ref{fig:matthew-b}), we can see that as $\beta_{test}$ increases, variance improves, and as $\beta_{test}$ decreases, utility improves. This is the expected behavior, and the major advantage of SO over JO. With JO, the model only predicts a single value, so we are unable to change the trade-off weight during evaluation, and we require a unique model for each $\beta$ that we want the model to work for. However, with SO, selecting just a few spread out $\beta$ values can allow us to extrapolate between them, providing online adaptability.
This shows that SO has the flexibility to function well at operating points away from the $\beta_{train}$ that it is trained for.

Figure~\ref{fig:matthew-apprx-Pareto-SO} shows how well a few selected models can generalize to the Pareto front. We pick $\beta_{train}$ values evenly spaced across the search space, and evaluate the model on all $\beta_{test}$, picking the closest $\beta_{train}$ in order of the search space. We can see that the approximated Pareto front closely matches the actual Pareto front, even with just 3 models, further demonstrating the strength of SO.
These observations hold for other domains as well, and the results are included in the Supplement.

\subsection{Effectiveness of Fair-Only Optimization (FO)}
Like SO, FO is also able to generalize well when different $\beta_{test}$ are used to evaluate the learned models (Figure~\ref{fig:matthew_FO}). Because the utility model is fixed, all models achieve high utility as $\beta_{test}\xrightarrow{}0$ (Figure~\ref{fig:matthew_FO-a}). Further, all models also improve fairness as $\beta_{test}$ grows larger (Figure~\ref{fig:matthew_FO-b}). The behavior change from utility-oriented to fairness-oriented is much sharper in FO when compared to SO. 
Looking at Figure~\ref{fig:matthew-apprx-Pareto-FO}, we can again see that even FO has the ability to generalize from only a few models to cover the entire Pareto front. 
Despite being Pareto-dominated by SO and JO at intermediate $\beta$ values in some domains, FO has the advantage of reliability: A trusted black-box utility model can be used in conjunction with a possibly smaller fairness model, with the guarantee to behave optimally as $\beta_{test}$ is reduced. When such a model is available, FO is the best choice, given its competent performance and lower computational load.

\section{Conclusions and Future Work}
We proposed DECAF, a framework for learning long-term utility and fairness estimates in multi-agent resource allocation. DECAF is among the first approaches to optimize fair resource allocation under resource constraints, supporting diverse problem settings by decoupling fairness and utility metrics. Split and Fair-Only optimization enable online trade-offs between utility and fairness without retraining, enhancing interpretability. Our results demonstrate the flexibility and effectiveness of our approaches across various scenarios.
 
Our framework currently relies on Q-Learning, as deriving a policy gradient approach for DECA problems is challenging due to the dynamic state-action space and the indirect relationship between agent `policies' and actions resulting from ILP optimization. Addressing this challenge is a promising direction for future research.
Finally, our methods are not the only way to decompose the fairness reward across agents. Techniques like VDN~\citep{sunehag2017VDN} or QMIX~\citep{rashid2020QMIX} could be integrated with our framework to learn credit assignment for fair rewards.