
\section{Theoretical Results}
\label{sec:appendixTheoretical}

For this section, we use an alternate notation, replacing $\beta$ with $\eta= \frac{\beta}{1-\beta_o}$ to make the equations easier to read, such that:
\begin{align*}
    (1-\beta)U + \beta F \Leftrightarrow U + \eta F
\end{align*}

This does not affect the allocation made by the ILP, as it only scales all Q-values by $1/(1-\beta)$. This would only be undefined when $\beta=1$, but we avoid that condition in our proofs. As $\beta\rightarrow1, \eta\rightarrow\infty$, and for any $\beta'>\beta$, $\eta'>\eta$. Note that in the theorem statements, we replace $\beta$ with $\eta$, but the proofs are equivalent.

The following results hold for any fairness function used in the DECAF formulation.

\begin{proposition}
As $\eta_{test}\xrightarrow{} 0$, all fair-only models behave in a utility-maximizing manner.
\end{proposition}
We state this without proof. It is easy to follow how this holds, as at $\eta=0$, the fairness model does not play any role in the decision making.

\setcounter{theorem}{0}
\begin{theorem}
Given perfect estimates for utility and fairness, increasing $\eta$ always improves the one-step fairness gain for SO with $\gamma=0$.
\label{th:theorem_fair_app}
\end{theorem}
\begin{proof}
We assume that the utility and fairness estimators are converged, i.e., the estimates of fairness and utility are correct. For the following discussion, assume the environment has evolved over some time $t$ and is at a state $s_t$. We consider what changes when we change $\eta$ at this state. Variables used henceforth are conditioned on $s_t$, wherever reasonable. We make the conditioning on $s_t$ implicit and do not notate it, to make it easier to read.

With $\gamma=0$, the optimal utility and fairness estimates equal the one-step return, i.e. the change in utility and fairness because of the resulting joint action. Note that these values are not known to the agents prior to the allocation as they depend on the joint actions of all agents, so computing these estimates is not trivial.

Let $U_{tot}(\mathcal{A})$ and $F_{tot}(\mathcal{A})$ be defined as follows, given an allocation $\mathcal{A}$:
\begin{align}
    U_{tot}(\mathcal{A}) &= \sum_{i\in\alpha}U(\mathcal{A}_i)\\
    F_{tot}(\mathcal{A}) &= \sum_{i\in\alpha}F(\mathcal{A}_i)
\end{align}
We remind the reader that $\mathcal{A}_i$ refers to the action assigned to agent $i$ in the allocation $\mathcal{A}$.

Let $\textbf{Z}_t$ represent the agent metrics at time $t$. Further, let $\mathcal{A}^*$ represent the optimal allocation from the ILP with $\eta$ as the trade-off weight. Since $\mathcal{A}^*$ is optimal, it follows that for all other possible allocations $\mathcal{A}_o$: 
\begin{align}
    U_{tot}(\mathcal{A}^*) + \eta F_{tot}(\mathcal{A}^*) &\ge U_{tot}(\mathcal{A}_o) + \eta F_{tot}(\mathcal{A}_o)  \\
    U_{tot}(\mathcal{A}^*) - U_{tot}(\mathcal{A}_o)&\ge  \eta (F_{tot}(\mathcal{A}_o) - F_{tot}(\mathcal{A}^*)) \label{eq:base_comp} 
\end{align}

We are interested in finding what happens to the allocation when we increase $\eta$.
For $\eta'>\eta$, note that the left side of Eq.~\ref{eq:base_comp} remains the same. Since utility estimates are not affected by changing $\eta$, any other allocation $\mathcal{A}_o$ can only be selected over $\mathcal{A}^*$ if the following condition holds:
\begin{align}
    U_{tot}(\mathcal{A}^*) + \eta' F_{tot}(\mathcal{A}^*) &\le U_{tot}(\mathcal{A}_o) + \eta' F_{tot}(\mathcal{A}_o)  \\
    U_{tot}(\mathcal{A}^*) - U_{tot}(\mathcal{A}_o)&\le  \eta' (F_{tot}(\mathcal{A}_o) - F_{tot}(\mathcal{A}^*)) \label{eq:base_comp2} 
\end{align}

Combining Eqs.\ref{eq:base_comp} and \ref{eq:base_comp2}, we get the following:
\begin{align}
    \eta (F_{tot}(\mathcal{A}_o) - F_{tot}(\mathcal{A}^*)) &\le  \eta' (F_{tot}(\mathcal{A}_o) - F_{tot}(\mathcal{A}^*)) \\
    F_{tot}(\mathcal{A}^*)(\eta'-\eta) &\le F_{tot}(\mathcal{A}_o)(\eta'-\eta) \label{eq:base_comp3} 
\end{align}
Since $\eta\ge0$ and $\eta'>\eta$, Eq.~\ref{eq:base_comp3} can only be true if $F_{tot}(\mathcal{A}_o)>F_{tot}(\mathcal{A}_o)$.

Thus, any allocation $\mathcal{A}_o$ that is optimal (and thus selected by the ILP) for $\eta'>\eta$ is guaranteed to have equal or better fairness than the allocation $\mathcal{A}^*$ at $\eta$.
\end{proof}

We also state the corollary to Theorem~\ref{th:theorem_fair_app}.
\begin{corollary}
Given perfect estimates for utility and fairness, decreasing $\eta$ always improves the one-step utility gain for SO with $\gamma=0$.
\label{th:theorem_util_app}
\end{corollary}
The proof follows a similar structure to Theorem~\ref{th:theorem_fair_app}.

While we only prove the behavior for $\gamma=0$, our empirical results show that we can expect similar behavior for long-horizon estimates. For any state, we will select actions that improve fairness in the long run starting from that state as $\eta$ is increased.

We also show the following useful property:
\begin{theorem}
    For a large enough $\eta$, the fairest allocation will be selected with perfect utility and fairness estimators for SO with $\gamma=0$.
\end{theorem}
\begin{proof}
Let $\mathcal{A}_f$ denote the allocation with the largest fairness gain:
\begin{align*}
    \mathcal{A}_f = \argmax_\mathcal{A} F_{tot}(\mathcal{A}) 
\end{align*}

For simplicity, let us assume no two allocations have the same $F_{tot}(\mathcal{A})$. For any other allocation $\mathcal{A}_o$, we have:
\begin{align}
    F_{tot}(\mathcal{A}_f) > F_{tot}(\mathcal{A}_o)  
\end{align}

Then, $\mathcal{A}_f$ will be optimal and selected by the ILP if the following condition holds:
\begin{align}
  U_{tot}(\mathcal{A}_f) + \eta_f F_{tot}(\mathcal{A}_f) &\ge U_{tot}(\mathcal{A}_o) + \eta_f F_{tot}(\mathcal{A}_o)  \\
  \eta_f &\ge \frac{U_{tot}(\mathcal{A}_o) - U_{tot}(\mathcal{A}_f)}{F_{tot}(\mathcal{A}_f)- F_{tot}(\mathcal{A}_o)} 
\end{align}

We can compute an upper bound for $\eta_f$ by considering the range of values that $U_{tot}$ and $F_{tot}$ can take. Let $U_{max}=\max_\mathcal{A}U_{tot}(\mathcal{A})$, and $F_{max}=\max_{\mathcal{A}, \mathcal{A}\ne \mathcal{A}_f }F_{tot}(\mathcal{A})$.

Then, we have the following:
\begin{align}
    \eta_f &\ge \frac{U_{tot}(\mathcal{A}_o) - U_{tot}(\mathcal{A}_f)}{F_{tot}(\mathcal{A}_f)- F_{tot}(\mathcal{A}_o)}\\
    &\le \frac{U_{max} - U_{tot}(\mathcal{A}_f)}{F_{tot}(\mathcal{A}_f)- F_{tot}(\mathcal{A}_o)}\\
    &\le \frac{U_{max} - U_{tot}(\mathcal{A}_f)}{F_{tot}(\mathcal{A}_f)- F_{max}} = \eta_f^u \label{eq:beta_upper}
\end{align}
Eq.~\ref{eq:beta_upper} gives us an upper bound for $\eta_f$. Thus, for all $\eta>\eta_f^u$, $\mathcal{A}_f$ will be the optimal allocation.
\end{proof}
\begin{corollary}
    For a small enough $\eta$, the most utilitarian allocation will be selected with perfect utility and fairness estimators for SO with $\gamma=0$.
\end{corollary}
The proof follows a similar structure to the proof for the previous theorem.
% \memo{
% This proof would be so much easier if we learn beta as well. The estimate for u and f would be conditioned on beta, so the algorithm is guaranteed to predict the correct long-term utility for a given beta
% }
% Possible other theorems:
% 1. For large enough beta, even after discounting, the action selected is the fairest immediate action? No, that is false.
