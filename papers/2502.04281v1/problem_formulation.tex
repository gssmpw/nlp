



\section{Problem Formulation}
\label{sec:problem_formulation}
In the context of Distributed Evaluation, Centralized Allocation (DECA), our primary goal is to integrate fairness into the decision-making process of resource allocation in multi-agent systems. Formally, we seek to maximize a combined measure of system utility and fairness, represented as:
\begin{align}
    \max \, (1-\beta) \mathcal{U}_T + \beta \mathcal{F}_T \label{eq:objective}
\end{align}
where $\mathcal{U}_T$ denotes the total utility at time $T$ and $\mathcal{F}_T$ represents the fairness measure, weighted by~$\beta$. 

In this section, we describe the DECA optimization framework and how it can be used for resource allocation. In the next section, we will describe DECAF, our solution to learn to improve fairness in this framework.

\subsection{Distributed Evaluation, Centralized Allocation (DECA)}

We define the DECA framework through a temporal resource allocation problem formulated as a Constrained Multi-agent MDP~\citep{CMMDP_de2021}. Our model is described by the tuple $\mathcal{M}$ with the following components:
\begin{align}
\mathcal{M} = \langle \alpha, S, \mathcal{O}, \{A_i\}_{i \in \alpha}, T, \{R_u\}_{i \in \alpha}, \gamma, c \rangle %incorporates the following components:
\end{align}
\squishlist
    \item $\alpha$ is the set of agents indexed by $i$ ($n$ agents).
    \item $S$ is the global state space.
    \item $\mathcal{O}: S \rightarrow O_1 \times O_2 \times \ldots \times O_n$ is the observation function that maps the true state to agent observations.
    \item $A_i$ is the action space for agent $i$.
    \item $T: S \times A_1 \times A_2 \times \ldots \times A_n \times S \rightarrow [0,1]$ represents the joint transition probabilities.
    \item $R_u: S \times A_1 \times A_2 \times \ldots \times A_n  \rightarrow \mathbb{R}^n$ denotes the (utility) reward function, which returns a vector of rewards, one for each agent.% Note that the reward of an agent depends on the action of other agents.
    \item $\gamma$ is the discount factor for future rewards.
    \item $c: A_1 \cup A_2 \cup \ldots \cup A_n \rightarrow \mathbb{R}^K$ maps each action to its resource consumption for $K$ types of resources.
\squishend



In a DECA problem, illustrated by Figure~\ref{fig:deca}, agents independently evaluate actions based on their local observations (DE), while a central controller aggregates these evaluations and optimizes resource allocation subject to constraints (CA). 
Agent actions include the null action and may have other effects apart from allocation of resources (e.g. moving in an environment), but only actions which consume resources are constrained. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/DECA/DECA.png}
    \smallskip
    \caption{An outline of the DECA pipeline. Each agent evaluates its available actions in a decentralized manner (DE), and the ILP finds the best joint action $\mathcal{A}$ using these evaluations and resource constraints (CA). }
    \label{fig:deca}
\end{figure}


\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[height=1.5in]{Figures/DECA/JO.png}
        \caption{Joint Optimization (JO)}
    \end{subfigure}
    ~~
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[height=1.5in]{Figures/DECA/SO.png}
        \caption{Split Optimization (SO)}
    \end{subfigure}
    ~~
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[height=1.5in]{Figures/DECA/FO.png}
        \caption{Fair-Only (FO)}
    \end{subfigure}%    
    \caption{Illustration of our three DECAF methods to learn fairness. Each subfigure shows how the values propagate for a single agent. The red lines and text denote the actual reward to the learning model, which is used to update weights using TD learning. (a) Joint Optimization learns to predict a single combined value. (b) Split Optimization learns two separate estimators for utility and fairness, and combines their output. (c) Fair-Only assumes a black-box utility model $U^*$, and learns a fairness estimator only, combining their outputs to make decisions.}
    \label{fig:decaf}
\end{figure*}


\subsection{Optimization Framework}
The distributed evaluation (DE) step involves agents learning to predict the utilities of observation-action pairs using approaches such as Deep Q-learning~\citep{mnih2013playing,van2016DDQN}. The utility estimates are computed using partially-observable post-decision states, which are estimated locally, ignoring other agents' actions due to the infeasibility of exploring the joint state space.


The central allocation (CA) step solves a integer linear program that combines predicted utilities and resource constraints. 
Let $\mathcal{A}$ denote the allocation of actions decided by the central allocator such that $\mathcal{A}_i$ is the action assigned to agent $i$,
and let there be $K$ types of resources, each represented by $k\in\{1,2,\dots,K\}$, such that the number of resources available can be written as $\mathcal{R}\in\mathbb{R}^K$. 
We thus say that each resource type $k$ has an availability $\mathcal{R}_k$. This gives us the following optimization with decision variables $x_i(a)$:
\begin{align}
    \max_{x_i(a) \in \{0,1\}} \sum_{i \in \alpha} \sum_{a \in A_i} x_i(a) Q(o_i,a) \label{eq:Opt_DECAF}
\end{align}
subject to:
\begin{align}
    \sum_{a \in A_i, x_i(a) \in \{0,1\}} x_i(a) &= 1, \quad \forall i \in \alpha \label{eq:action_constraint}\\
    \sum_{a \in \mathcal{A}} c(a)_k \le \mathcal{R}_k, & \quad \forall k \in \{1, \ldots, K\} \label{eq:resource_constraint}
\end{align}
These constraints ensure that each agent is assigned exactly one action and that total resource usage does not exceed available supplies.
The ILP described above forms the central controller, and the Q-value estimator controls distributed evaluation. This offers benefits over completely distributed approaches by encapsulating resource constraints, and over completely centralized approaches by reducing the complexity of the learning objective. 
This setup is seen in many resource allocation problems~\citep{kube2019allocating, ride_alonso, shah2020neural}.

\section{DECAF: Fairness in DECAs}
\label{sec:fairness_in_decas}


In this section, we describe DECAF, our framework for incorporating fairness into DECA problems using Q-Learning. Specifically, we detail how we can specify and learn the fairness objective in Eq.~\ref{eq:objective} through the use of decomposed fairness rewards, and a modified Q-Learning algorithm to learn from these rewards using centralized training.

\subsection{Fairness Reward}


Previous work has considered learning to optimize a single social welfare function (SWF) that captures the notions of fairness and utility together, like the Coefficient of Variation used by FEN~\cite{jiang2019FEN} or $\alpha$-fairness and the Generalized Gini Function (GGF) used by SOTO~\cite{zimmer2021MOMDP}. With DECAF, we try to learn a class of objectives that trade off between system utility and fairness, characterized by a trade-off variable $\beta$ (Eq.~\ref{eq:objective}), with the aim of enabling flexible trade-offs between the two. 

Let $\textbf{Z}=\{z_i\}_{i\in \alpha}$ denote the vector of accumulated agent utilities (averaged or total). We interpret this utility as `accumulated wealth,' and look to make allocations that can result in a fairer distribution of this wealth. Let $\textbf{Z}_t^\pi$ represent the distribution of agent utility metrics at time $t$ following policy $\pi$. Then, we define a fairness function $\mathbb{F}:\mathbb{R}^{n}\rightarrow \mathbb{R}$ as a mapping of vector $\textbf{Z}$ to a real value, and our fairness objective is maximizing $\mathcal{F}_T = \mathbb{F}(\textbf{Z}_{t=T}^\pi)$.  Most popular SWFs and fairness functions can be cast into this form. 

In our work, we consider optimizing this objective by computing a fairness reward $R_f(\mathbf{s},\mathcal{A})$ that provides a vector of rewards, one for each agent, that captures how their current action contributed to the system fairness. We do this by using the per-step fairness change because of an allocation:
\begin{align}
    \Delta \mathcal{F}|\mathcal{A}^t 
    &= \mathcal{F}_{t+1} - \mathcal{F}_t \\
    &= \mathbb{F}(\textbf{Z}_{t+1}) - \mathbb{F}(\textbf{Z}_t) 
\end{align}
A naive way to decompose this reward is to evenly divide it among agents. This is commonly done in collaborative multi-agent RL when agents are optimizing a shared goal.
\begin{align}
    R_f(\mathbf{s}_t, \mathcal{A}^t) = \left[ \frac{\Delta \mathcal{F}|\mathcal{A}^t}{n} \right]_{i\in\alpha}
\end{align}
Alternatively, specialized decompositions can be designed to give a more informative signal to each agent. For example, if variance is used as the fairness function ($\mathcal{F}_t = -\var(\textbf{Z}_t)$):
\begin{align}
    \Delta \mathcal{F}|\mathcal{A}^t 
    &= -\var(\textbf{Z}_{t+1}) + \var(\textbf{Z}_t) \\
    &= -\frac{1}{n}\sum_{i\in\alpha}\left( z^{t+1}_i - \bar{z}_{t+1} \right)^2 + \frac{1}{n}\sum_{i\in\alpha}\left( z^{t}_i - \bar{z}_{t} \right)^2 \\
    R_f(\mathbf{s}, \mathcal{A}) &= \left[ -\frac{1}{n} \left( z_i^{t+1} - \bar{z}_{t+1} \right)^2 + \frac{1}{n}\left( z_i^{t} - \bar{z}_{t} \right)^2  \right]_{i \in \alpha} \label{eq:var_fair_reward}
\end{align}
Observe that this reward only depends on the agent's own metric value and the average metric. Thus, each iteration, apart from the local observation, each agent only needs to be communicated information about the average utility of all agents to reliably predict this value. This could be done by the central agent, or by message passing between the agents. 

For the main experiments of this paper, we use variance as our fairness function, with the reward function in Eq.~\ref{eq:var_fair_reward} as the fair reward. However, our methods are not limited to using variance. We also provide reward decompositions and experiments with other fairness functions including $\alpha$-fair, GGF, and maximin functions in the supplement, showing the generality of our approach. 

\subsection{Algorithms}


Given the fair reward $R_f$ described above, our approach targets the DE step to improve fairness, by changing the Q-values used in the ILP (Eq.~\ref{eq:Opt_DECAF}) to also account for fairness. We do this modifying $Q$ to be an estimator of the combined fair-efficient objective, with a weight $\beta\in [0,1]$ used to regulate relative value of fairness and utility. 

We use experience replay with centralized training to learn the Q-function, where an experience $\tau=\langle \mathbf{o},\mathcal{A},\mathbf{r}_u,\mathbf{r}_f,\mathbf{o}'\rangle$ stores a joint transition across all agents, with utility rewards $\mathbf{r}_u$ and fair rewards $\mathbf{r}_f$. Let $\theta$ denote the parameters of the Q-function. Given a replay buffer $\mathcal{D}$, we want to minimize the loss function $J_\theta=\mathbb{E}_{\tau\sim \mathcal{D}}L(\delta(\tau))$, where $\delta(\tau)$ is the Bellman error of the transition $\tau$, and $L$ is the MSE loss.
% For this discussion, let  $L(\cdot)$ denote mean-squared loss function.
We propose three approaches for integrating fairness, illustrated in Figure~\ref{fig:decaf}. \footnote{Unless stated, we use bold terms to denote vectors, and overload Q-functions to also operate on vectors to compute a vector of outputs. Further, we use $Q(\mathbf{o})$ as a shorthand for computing Q-values for all possible actions for each observation in $\mathbf{o}$.}

\squishlist
    \item \textbf{Joint Optimization (JO):} A single estimator optimizes a weighted combination of fairness and utility.
    \begin{align}
    \delta(\tau) = (1-\beta) \mathbf{r}_u + \beta  \mathbf{r}_f + \gamma Q_\theta(\mathbf{o}') - Q_\theta(\mathbf{o}, \mathcal{A}) \label{eq:JO}
\end{align}
    \item \textbf{Split Optimization (SO):} Separate estimators for fairness ($F_\theta(\cdot)$) and utility ($U_\theta(\cdot)$) allow dynamic adjustment of their trade-off during policy execution.
    \begin{align}
    \delta^f(\tau) &= \mathbf{r}_f + \gamma F_\theta(\mathbf{o}') - F_\theta(\mathbf{o}, \mathcal{A}) \\
    \delta^u(\tau) &= \mathbf{r}_u + \gamma U_\theta(\mathbf{o}') - U_\theta(\mathbf{o}, \mathcal{A}) \\
    Q(\mathbf{o}, \mathcal{A}) &= (1-\beta) U_\theta(\mathbf{o}, \mathcal{A}) + \beta F_\theta(\mathbf{o}, \mathcal{A}) \label{eq:SO}
\end{align}
    \item \textbf{Fair-Only Optimization (FO):} A fairness estimator ($F_\theta(\cdot)$) adjusts a pre-existing utility function $U^*(\cdot)$ to incorporate fairness, useful when utility functions are provided externally.
    \begin{align}
    \delta^f(\tau) &= \mathbf{r}_f(s,a) + \gamma F_\theta(\mathbf{o}') - F_\theta(\mathbf{o}, \mathcal{A}) \\
    Q(\mathbf{o}, A) &= (1-\beta) U^*(\mathbf{o}, \mathcal{A}) + \beta F_\theta(\mathbf{o}, \mathcal{A}) \label{eq:FO}
\end{align}
\squishend


Our learning algorithm is based on Double Deep Q-Learning~\cite{van2016DDQN}, which uses a target network to stabilize updates. The key differentiating factor is in how the target values are computed. When learning from an experience, we compute the optimal action $\mathcal{A}^*$ in the successor state by solving the ILP~(Eq.~\ref{eq:Opt_DECAF}) using the online Q-network, and then compute the Q-value of the selected actions using the target network. The models are updated using the rewards (stored in the experience) obtained after the ILP allocation of the previous state, as shown in the red text and arrows in Figure~\ref{fig:decaf}. 
For SO and FO, we package the utility and fairness estimators into the Q-function, and compute the optimal successor action using both. Then, we independently update each estimator using the TD error of their respective objectives. We provide the pseudocode for the learning algorithms in Appendix \ref{sec:appendixAlgo}.
For FO, we skip training the utility estimator.
SO and FO offer the additional benefits of interpretability, as during execution, we are able to discern how much of the decision was based on the utility gain and fairness improvement respectively.

SO also provides some useful properties described below.
\begin{theorem}
\label{th:theorem_fair}
Given perfect estimates for utility and fairness, increasing $\beta$ always improves the one-step fairness gain for SO with $\gamma=0$.
\end{theorem}
\noindent \textbf{Proof Sketch:} We show this using the property of the ILP. The only way another action is selected when $\beta$ is increased is if the objective value of that action is higher. Since $\beta\ge0$, comparing the objective value of two allocations shows us that for $\beta'>\beta$, this only happens when the fairness gain for the new action is higher. $\hfill \Box$

\begin{theorem}
\label{th:theorem_fairest}
For a large enough $\beta$, the fairest allocation will be selected with perfect utility and fairness estimators for SO with $\gamma=0$.
\end{theorem}
\noindent \textbf{Proof Sketch:}
Since the fairest allocation has the largest fairness gain, we show that there exists a $\beta_f$ such that the fairness gain's contribution to the objective outweighs any utility loss, making it the optimal allocation. $\hfill \Box$

\medskip
We also provide the corollaries to these theorems for improving utility as $\beta$ is reduced in Appendix \ref{sec:appendixTheoretical}, along with full proofs for these theorems. 
These properties also empirically hold when $\gamma\ne0$, as our experiments demonstrate. This adaptability is a major strength of SO: It allows a degree of flexibility that other methods do not possess. Specifically, SO allows users to vary the trade-off weight $\beta$ during runtime, and the behavior can be expected to be monotonic in the direction of change. For $\gamma=0$, Theorem~\ref{th:theorem_fair} and its corollary guarantee that the space of selected allocations is Pareto-efficient with changing $\beta$ at each time step.
