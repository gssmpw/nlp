\begin{algorithm}[t]
\caption{Update for Joint Optimization}
\label{alg:JOUpdate}
\begin{algorithmic}[1]
\Function{Update}{$Q_{\theta}$, $Q_{\theta'}$, $\mathcal{D}$, env}

    \State Sample a mini-batch of $n$ experiences from replay buffer $\mathcal{D}$
    
    \For{each experience $\langle \mathbf{o}, \mathcal{A}, \mathbf{r}_u, \mathbf{r}_f, \mathbf{o}' \rangle$ in the mini-batch}
        \State Compute Q-values for the successor observation $Q_\theta(\mathbf{o}')$
        
        \State Solve ILP to get the optimal allocation $\mathcal{A}^*$ for the next observation $\mathbf{o}'$:
        \State \hspace{1em} $\mathcal{A}^* = \text{solve\_ILP}(Q_\theta(\mathbf{o}'), \text{env.constraints})$
        
        \State Compute Q-values for $\mathcal{A}^*$ using the target network:
        \State \hspace{1em} $Q_{\theta'}(\mathbf{o}', \mathcal{A}^*)$
        
        \State Compute the target for the TD update:
        \State \hspace{1em} $\text{target} = (1-\beta)  \mathbf{r}_u + \beta  \mathbf{r}_f + \gamma  Q_{\theta'}(\mathbf{o}', \mathcal{A}^*)$
        
        \State Compute the TD loss:
        \State \hspace{1em} $\text{loss} = \left( Q_\theta(\mathbf{o}, \mathcal{A}) - \text{target} \right)^2$
        
        \State Perform gradient descent on the TD loss to update $Q_\theta$
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}