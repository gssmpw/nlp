
\input{theoretical_results}

\section{Learning Algorithm}
\label{sec:appendixAlgo}

\begin{algorithm}
\caption{DECAF Algorithm}
\label{alg:main_loop}
\begin{algorithmic}[1]
\State \textbf{Initialize:} agent network $Q_{\theta}$, target network $Q_{\theta'}$
\State \textbf{Initialize:} $\epsilon$ (exploration rate)
\State \textbf{Initialize:} replay buffer $\mathcal{D}$

\For{episode = 1 to $N_{eps}$}
    \State Decay $\epsilon$ according to decay schedule    
    \State RunEpisode($Q_{\theta}$, $\epsilon$, $T$, env, $\mathcal{D}$)

    \If{episode \% k == 0}
        \Comment Run validation with $\epsilon = 0$
        \State RunEpisode($Q_{\theta}$, 0, $\infty$, env, $\mathcal{D}$)
        \Comment Save validation objective
    \EndIf
    \If{episode \% $\tau$ == 0}
        \State $Q_{\theta'}\leftarrow Q_{\theta}$
        \Comment Update target weights
    \EndIf
    
\EndFor

\State \textbf{Load} model with best validation objective value
\State Run $50$ validation episodes using RunEpisode($Q_{\theta}$, $ 0$, $\infty$, env, $\mathcal{D}$)
\State Save validation results

\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{RunEpisode (Executes a single episode )}
\label{alg:episode_loop}
\begin{algorithmic}[1]
\Function{RunEpisode}{$Q_{\theta}$, $\epsilon$, $T$, env, $\mathcal{D}$}
    \State Reset environment, get initial observation $\mathbf{o}_0$
    \For{t = 1 to $N_{steps}$}
        \State Sample a random number $r \in [0, 1]$
        
        \If{$r < \epsilon$} 
            % \Comment{Exploration: Select random Q-values (greedy)}
            \State $\mathbf{Q}_t = \mathbf{Q}_\text{random}$ \Comment{Random Q-values for exploration}
        \Else
            % \Comment{Exploitation: Select Q-values from agent's network}
            \State $\mathbf{Q}_t = Q_\theta(\mathbf{o}_t)$ \Comment{Q-values from the agent}
        \EndIf
        
        \State Use ILP to compute optimal action $\mathbf{a}_t$:
        \State \hspace{1em} $\mathbf{a}_t = \text{solve\_ILP}(\mathbf{Q}_t, \text{env.constraints})$
                
        \State Take step in environment: 
        \State \hspace{1em} $(\mathbf{R}_{f,t}, \mathbf{R}_{u,t}, \mathbf{o}_{t+1}) = \text{env.step}(\mathbf{a}_t)$
        
        \State Store transition $(\mathbf{o}_t, \mathbf{a}_t, \mathbf{R}_{u,t}, \mathbf{R}_{f,t}, \mathbf{o}_{t+1})$ in replay buffer $\mathcal{D}$
        
        \If{$t \% T == 0$}
            % \State Update agent network $Q_\theta$ using replay buffer $\mathcal{D}$
            \State update($Q_\theta$, $Q_\theta'$, $\mathcal{D}, env)$
        \EndIf
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\input{algorithm_JO}
\input{algorithm_SO}

Algorithm~\ref{alg:main_loop} shows the overall training loop used for our experiments, with Algorithm~\ref{alg:episode_loop} showing how each episode is executed. We decay epsilon to 0.05 over half of the total number of episodes. $T$, $k$, $\tau$ decide how frequently we learn, validate and update the target model respectively. 
Algorithm~\ref{alg:JOUpdate} and Algorithm~\ref{alg:SOUpdate} detail how the update step is performed for joint and split models. The update for FO is identical to SO, except omitting the update for the utility model.

\subsection{Model Architecture and Training}
All our models use a learning rate of 0.0003 with the Adam optimizer. For all environments except BiasedDM, we train for 1000 episodes, and run validation every 50 steps for model selection. For BiasedDM, we train for 200 episodes, and validate every 20 episodes.

The neural network architecture for all models is the same, with two fully connected hidden layers, of dimension 20, with ReLU activations. The output (1-dimensional) does not have any activation function. We implement our networks using pytorch. 

We use a replay buffer of size 250000, where one experience is a joint transition across all agents. During training, we sample experiences from the replay buffer, and for each experience, we evaluate actions for all agents using the current online network, solving the ILP to get the best joint action. Then, we score the post-decision state for each agent using the target network, and compute the MSE loss between the target value and value estimates of the selected action from the online network.

We ran all our main experiments on a university compute cluster, with each experiment running on a single CPU node with 12GB RAM. Experiment runtime varied with environment choice. Training a single model with one $\beta$ value took between 30 minutes (BiasedDM) and 2 hours (Matthew). Evaluation, as for the generalization experiments, was performed on a 2019 MacBook Pro, where one episode took 2-5 seconds to run, and we bootstrap over 5 runs. 

\section{Environment Details}
\label{sec:appendix1}
Here, we provide further details about the environments for reproducibility. We will also make the environment and training code available upon acceptance.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/Env_illustration/Matthew.png}
        \caption*{Matthew}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/Env_illustration/JobAlloc.png}
        \caption*{JobAlloc}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/Env_illustration/Job.png}
        \caption*{Job}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/Env_illustration/Plant.png}
        \caption*{Plant}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/Env_illustration/BiasedDM.png}
        \caption*{BiasedDM}
    \end{subfigure}
    \caption{Illustration of all five environments}
    \label{fig:env_illustration}
\end{figure*}

\subsection{Matthew}
One episode for this environment lasts 200 steps. At each step, 10 agents and 3 resources are available on the map. Agent speeds grow proportionally to their size, and a size ceiling exists to prevent agents from growing too large for the environment bounds. Agent and resource positions are 2-D coordinates in $[0,1]$. At the beginning of each episode, agent and resource positions are randomly initialized.
To show the matthew effect, 4 agents are initialized to have a larger initial size than other agents. This allows them to reach resources faster. Agents receive a unit reward when they collect a resource, in addition to a small increase in size and speed. Resources allocated to agents are reserved, and other agents cannot pick them up. A new resource only spawns when an agent reaches its allocated resource, not when it is allocated.

\subsection{Job}
One episode for this environment lasts 100 steps. There are 4 agents on a $7\times 7$ grid, and agents can move in any cardinal direction or stay in their location. Agents cannot occupy the same location as any other agent, and they receive a unit reward when occupying the resource location. Attempting to move out of the grid results in a no-op. The agents start off at the corners of the grid. The job's location is fixed at the center of the grid, and its position remains fixed for the entire episode.
The objective in this environment is for agents to learn to share the job instead of occupying it alone.

\subsection{JobAlloc}
This is a simplified version of the Job environment, after removing the grid and casting it more directly as a resource allocation problem. We still maintain the core challenge of agents learning to give up resources at some point so that other agents can also occupy the job, by adding a constraint that agents can only occupy a resource if the resource is free at the beginning of the timestep. This requires a joint action where all agents decide to abstain, in order to change the agent occupying the resource. One episode lasts 100 steps, with 4 agents.

\subsection{Plant}
One episode for this environment lasts 200 steps. There are 5 agents on a $8 \times 8$ grid, where agents can move in cardinal directions.  The grid also contains 8 resources, which can be of three different types. Each agent gets a reward when they construct a `unit'. Each agent has a requirement of a set of resources it must collect  so that it can construct this unit. The requirements are: 
\[
\{ (2, 1, 0), (1, 0, 1), (1, 0, 0), (1, 3, 0), (0, 1, 2) \}
\]
For example, agent 1 requires two resources of type 1 and one resource of type 2 following which it can get a reward. Some agents are given easier requirements to fulfill, which creates a bias in the number of units agents produce. The resources and agent locations are randomly initialized at the beginning of each episode. The allocation follows a similar process to the Matthew environment, where actions are allocations of agents to resources, and other agents cannot pick up resources already allocated to other agents. When a resource is picked up, another resource of the same type appears in a random location on the map.

\subsection{BiasedDM}
One episode for this environment lasts 100 steps. At each time step, the decision maker allocates one resource to one of five agents. The utility of assigning the resource to each agent is different for the decision maker. In other environments, fairness is computed as the variance over the accumulated rewards for each agent. In this environment, however, fairness is computed over the resource rate, which is the fraction of steps in which an agent received the resource ($z_i\in[0,1]$).
\begin{align}
    z_i = \frac{\text{Num. resources}}{time}
\end{align}

This also results in much smaller variances, thus our hyperparameter search for this domain explores the higher range of $\beta$ values more.


\section{Learning with Other Fairness Functions}
\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/alpha_fair.png}
    \includegraphics[width=0.98\linewidth]{Figures/ggf.png}
    \includegraphics[width=0.98\linewidth]{Figures/maximin.png}
    \caption{Results training DECAF with $\alpha$-fairness, GGF and maximin fairness functions. The lines show the Pareto fronts for each model type.}
    \label{fig:other_metrics}
\end{figure*}

As mentioned in the main text, we are not restricted to using variance and the given decomposition. We show here results for three more functions: $\alpha$-fairness, $GGF$, and  maximin.

\paragraph{1. \textbf{$\alpha$-fairness}}
The $\alpha$-fair function can be stated as:
\begin{align}
    F_\alpha(\textbf{Z}) =& \sum_{z_i\in\textbf{Z}}\begin{cases}
        \frac{z_i^{1-\alpha}}{1-\alpha} & \alpha\neq 1 \\
        \log z_i & \alpha=1
    \end{cases}
\end{align}
With $\alpha=1$, this is equivalent to proportional fairness or log Nash Welfare, both popular notions of fairness, while at $\alpha=0$ it represents the utilitarian objective. In our experiments, we use $\alpha=1$.

\paragraph{2. \textbf{Generalized Gini Function (GGF)}}
Given a sequence of positive, fixed, strictly decreasing weights $\mathbf{w}$, the GGF function can be stated as:
\begin{align}
    G_w(\textbf{Z}) = \sum_{i\in\alpha} \mathbf{w}_i z_i^{\uparrow}
\end{align}
Here, $\textbf{z}^{\uparrow}$ represents the vector obtained by sorting the $\textbf{Z}$ vector. This function can also represent a diverse set of SWFs including utilitarian and maximin fairness. In our experiments, we use decreasing negative powers of 2 as the weights (i.e. $\mathbf{w}=[1, 2^{-1}, 2^{-2}, \dots 2^{-(n-1)}]$). 

\medskip\noindent For both of the above objectives, we use the equal decomposition, where the fairness reward is computed as an equal division of the change in the metric value across all agents.
\begin{align}
    R_f(\mathbf{s}_t, \mathcal{A}^t) = \left[ \frac{\Delta \mathcal{F}|\mathcal{A}^t}{n} \right]_{i\in\alpha}
\end{align}

\paragraph{3. \textbf{Maximin Fairness}}
This function captures the worst off utility of any agent:
\begin{align}
    F_{MMF}(\textbf{Z}) = \min(\textbf{Z})
\end{align}
This is a hard objective to learn, as the maximin objective changes only when the worst-off agent is improved. 
We decompose this reward by combining the global signal with the per-agent contribution towards improving the minimum. Intuitively, each agent receives a reward for a joint action that improves the minimum, but the agents that were at the minimum (and improved) receive a larger reward. 
\begin{align}
    r_{f,i} &= \frac{\min(\textbf{Z}') - \min(\textbf{Z})}{n} \\
    r_{f,i} &= r_{f,i} +
    \begin{cases}
        z'_i - z_i & \text{if } z_i =\min(\textbf{Z}) \\
        0 & \text{otherwise}
    \end{cases}\\
    r_{f,i} &= r_{f,i} + 
    \begin{cases}
        z'_i - z_i & \text{if } z'_i =\min(\textbf{Z}') \\
        0 & \text{otherwise}
    \end{cases}\\
    R_{f,i} &= \frac{r_{f,i}}{\sum_j r_{f,j}} \left(\min(\textbf{Z}') - \min(\textbf{Z})\right)
\end{align}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/Matthew_var_fairness.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/JobAlloc_var_fairness.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/Job_var_fairness.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/Plant_var_fairness.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/BiasedDM_var_fairness.png}
    
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/Matthew_var_util.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/JobAlloc_var_util.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/Job_var_util.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/Plant_var_util.png}
    \includegraphics[width=0.19\linewidth]{Figures/ErrorBars/BiasedDM_var_util.png}
    \caption{Effect of changing $\beta$ on variance (top row) and utility (bottom row) for all three methods on all five environments. 
    The shaded area shows the 1-$\sigma$ error bar. 
    We observe that the performance of FO has large variation for intermediate $\beta$ values.}
    \label{fig:error-bars}
\end{figure*}

\subsection{DECAF Results with Other Fairness Metrics}
Figure~\ref{fig:other_metrics} shows the results of our approaches when using different fairness functions, with the decompositions as described above.  In general, we observe our methods offer a good range of trade-offs in all environments, often Pareto-dominating SOTO and FEN. We describe some additional details about these results in this section.

In almost all environments, it is possible to get significant fairness improvement starting from the utilitarian model. The only exception is the Plant environment for $\alpha$-fair and GGF, where the utilitarian solution is almost optimal for fairness as well. In Job and JobAlloc, SOTO masked models are competitive, especially for the $\alpha$-fair fairness function, while some DECAF models are not as performant. This may suggest the existence of a more informative decomposition for this function which can lead to better learning. In BiasedDM, SOTO and FEN face a significant disadvantage, as they can not contend with the misaligned fairness signal. Thus, they can either know the system utility, or the payoffs on which fairness is based. In the prior case, the learned fair policy performs poorly for both utility and fairness, as it tries to equalize the accumulated value from the decision maker's perspective, or, as in the latter case (selected for the results shown), only looks at the resource distribution and has no idea of the utility to the decision maker.

$\alpha$-fair and GGF could also be decomposed to compute per-agent contributions, but since the metrics treat all agents indepependently, the interaction between agent utilities is harder to learn, especially for the Job and JobAlloc environment, where being fair requires a globally suboptimal decision in terms of utility. In other environments, the pressure for fairness is better captured in the ILP optimization, by the valuations of other agents that could benefit more from getting certain resources. 


\begin{figure*}[t]
    \centering
    % Row 1: Split
    \subfloat[Matthew]{
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/Matthew/Split_ApproxPareto.png}
    \label{fig:matthew_split}}
    \subfloat[JobAlloc]{
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/JobAlloc/Split_ApproxPareto.png}
    \label{fig:joballoc_split}}
    \subfloat[Job]{
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/Job/Split_ApproxPareto.png}
    \label{fig:job_split}}
    \subfloat[Plant]{
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/Plant/Split_ApproxPareto.png}
    \label{fig:plant_split}}
    \subfloat[BiasedDM]{
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/BiasedDM/Split_ApproxPareto.png}
    \label{fig:biaseddm_split}}
    \\
    % Row 2: SplitNoUtility
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/Matthew/SplitNoUtility_ApproxPareto.png}
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/JobAlloc/SplitNoUtility_ApproxPareto.png}
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/Job/SplitNoUtility_ApproxPareto.png}
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/Plant/SplitNoUtility_ApproxPareto.png}
    \includegraphics[width=0.19\linewidth]{Figures/Generalization/Approx_pareto/BiasedDM/SplitNoUtility_ApproxPareto.png}
    \caption{Approximate Pareto fronts for Split Optimization and Fair-Only Optimization models for variance across different environments. The top row represents SO models, while the bottom row represents FO models.}
    \label{fig:approx_pareto_fronts}
\end{figure*}

\subsection{On the Importance of Past Discounting and Warm Starts}
%new
We also highlight two departures from prior work in our implementation of DECAF for learning fairness: (1) We discount the agent metrics $\textbf{Z}$ over the past, and (2) we implement warm starts for initializing $\textbf{Z}$ instead of initializing at 0. The past discounts are necessary to account for the time dependence of various normalized fairness metrics, like the Gini coefficient and coefficient of variation, or the variance over normalized agent metrics. In these cases, actions taken early on can cause larger changes to the fairness metric while actions taken after a sufficient history has been established have an imperceptible effect. This is undesirable in long-horizon settings, but can be remedied by past discounts, effectively `forgetting' events that happened far in the past. The warm starts function to counteract the other pitfall in some fairness metrics: The zero vector is perfectly fair, and any changes can incur a large fairness penalty. Adding randomized warm starts helps in preventing the algorithm from converging to this trivial solution. In practice, we keep the warm start values small, so that the past discounts effectively scale them down to zero over the course of an episode.

\subsubsection{Past Discounts and Warm Starts}
Past discounts and warm starts significantly helped in improving the stability of learning in our experiments, especially with variance as the fairness function. Small initial perturbations (that are smoothed out over time using past discounts) help in exploration of different states, as well as in avoiding the cold start problem where any action would lead to a worse state and hence agents learn to avoid taking beneficial actions. For variance, we used warm starts based on the size of the maximum rewards possible in an episode in each environment. For BiasedDM, the warm starts are used to create an initial 'resource rate' by normalizing based on the number of total warm start resources, as this environment uses resource rates as the payoff vector $\textbf{Z}$ . 

For $\alpha$-fair, we used $\alpha=1$ for the experiments, and hence we avoided using warm starts for this metric, as the log function is sensitive to small perturbations especially with near-zero utilities. For GGF, we used a warm start of 0.1 for each environment. For both of these functions, we used no past discounting, as the metrics are additive functions over agent utilities, so any past discounting would cause negative fairness gains. 
For maximin, we used the same past discounting and warm start values as variance. The warm start and past discount values for all environments are given in Table~\ref{tab:warm-past-disc}.

\begin{table}[]
\caption{Warm start ($w$) and past discount ($\gamma_p$) values for different environments and fairness functions used for DECAF.}
\label{tab:warm-past-disc}
\begin{minipage}{\linewidth}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{}                  &                     & \textbf{Matthew} & \textbf{Plant} & \textbf{Job} & \textbf{JobAlloc} & \textbf{BiasedDM} \\ \hline
\multirow{2}{*}{\textbf{$\alpha$-fair}} & \textbf{$w$}        & 0                & 0              & 0            & 0                 & 0                 \\ \cline{2-7} 
                                        & \textbf{$\gamma_p$} & 1                & 1              & 1            & 1                 & 1                 \\ \hline
\multirow{2}{*}{\textbf{GGF}}           & \textbf{$w$}        & 0.1              & 0.1            & 0.1          & 0.1               & 0.1               \\ \cline{2-7} 
                                        & \textbf{$\gamma_p$} & 1                & 1              & 1            & 1                 & 1                 \\ \hline
\multirow{2}{*}{\textbf{Maximin}}       & \textbf{$w$}        & 5                & 1              & 3            & 3                 & 2                 \\ \cline{2-7} 
                                        & \textbf{$\gamma_p$} & 0.995            & 0.995          & 0.995        & 0.995             & 0.999             \\ \hline
\multirow{2}{*}{\textbf{Variance}}      & \textbf{$w$}        & 5                & 1              & 3            & 3                 & 2                 \\ \cline{2-7} 
                                        & \textbf{$\gamma_p$} & 0.995            & 0.995          & 0.995        & 0.995             & 0.999             \\ \hline
\end{tabular}%
}
\end{minipage}
\end{table}




Given a warm start value of $w$, we compute an initial distribution of pseudo-resources by uniformly sampling from a region of width $w/4$ centered around $w$. For additive utilities, we use past discounts to decay the accumulated payoffs in $\textbf{Z}$ before adding the current time-step's reward. If $\gamma_p$ is the past discount factor, and $R_{t,i}$ is the resource value allocated to agent $i$ at time $t$, then we compute:
\begin{align*}
    z_i^{t+1} = \gamma_p z_i + R_{t,i}
\end{align*}

For averaged rewards (as in BiasedDM), where the payoff $z_i= {\#resources}/{\#timesteps}$, we compute the discount by reweighting both the numerator and denominator. Note that this can be easily extended to act as a `resource rate' where the denominator is the number of potential resources the agent could have received, instead of the time.
\begin{align*}
    z_i^t &= \frac{res_i}{t_i}\\
    res_i &= \gamma_p res_i + R_{t,i} \\
    t_i &= \gamma_p t_i + 1 \\
    z_i^{t+1} &= \frac{res_i}{t_i}
\end{align*}


\begin{figure*}[t]
    \centering
    \subfloat[Matthew]{
    \includegraphics[width=0.32\linewidth]{Figures/Barplots/Matthew.png}
    }
    \subfloat[JobAlloc]{
    \includegraphics[width=0.32\linewidth]{Figures/Barplots/JobAlloc.png}
    }
    \subfloat[Job]{
    \includegraphics[width=0.32\linewidth]{Figures/Barplots/Job.png}
    }
    
    \subfloat[Plant]{
    \includegraphics[width=0.32\linewidth]{Figures/Barplots/Plant.png}
    }
    \subfloat[BiasedDM]{
    \includegraphics[width=0.32\linewidth]{Figures/Barplots/BiasedDM.png}
    }
    \caption{Comparisons of selected DECAF models against the three baselines, scaled to fit on the same axes. We omit results for ILP versions of FEN and SOTO due to their poor performance. We selected DECAF models (trained on variance) that maximized $0.1 U - 0.9\var(\textbf{Z})$. The numbers in brackets denote the selected $\beta$ value for our models.}
    \label{fig:all_barplots}
\end{figure*}

\section{FEN and SOTO Implementation Details}
% \memo{TODO This section}
For FEN and SOTO, we use 5 times the number of training steps as used for DECAF, to allow the PPO based approaches sufficient trajectories to learn from, and to better match the experiments in the respective papers. We do not use past discounts and warm starts.
For BiasedDM, we chose the reward vector to be the number of resources each agent received (1 for each agent), instead of the biased utility to the decision-maker ($0.2\cdot i$ for agent $i$). Since our fairness metrics operate on the vector of resources as well, and since FEN and SOTO aim to learn fairness, this was the obvious choice. We also ran experiments where the reward vector was based on the decision-maker's biased utility, and found the solutions to be poor for both utility and fairness based on resources, so we omit them in our results. However, the greedy self-oriented model in SOTO was trained using the decision-maker's reward.

We used FEN without gossip, where the agent is directly communicated the distribution information, without need for inter-agent communication rounds. This is expected to be the stronger version of FEN. 
For all models, we used the same features from the DECA environments, containing the agent's relative advantage as a feature. In addition to this, SOTO also required the entire payoff distribution $\textbf{Z}$ and information about nearby neighbors for the tiered team-oriented network.  

The Job environment uses a shaping reward for the distance to the job as a scaled penalty. For the Job environment for SOTO, we reduced the weight of the shaping reward to 0.01 to minimize its effect on the optimization. We found this to be the best setting for learning in this environment. The ILP version of SOTO was not able to learn at all in this setting, even when we completely removed the shaping reward. Again, the inability to use shaping rewards is a significant handicap for methods like SOTO, since they optimize fairness over the rewards. Our methods explicitly decouple learning utility and fairness, so they are more expressive, and able to learn better especially at intermediate values of $\beta$.


\section{Extended Results for variance}

\begin{figure*}[t]
    \centering
    % Subfloat for Split (left two columns)
    \subfloat[SO models]{
        \begin{minipage}{0.48\linewidth}
            \centering
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Matthew/Split_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Matthew/Split_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/JobAlloc/Split_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/JobAlloc/Split_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Job/Split_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Job/Split_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Plant/Split_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Plant/Split_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/BiasedDM/Split_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/BiasedDM/Split_fairness.png}
        \end{minipage}
        \label{fig:all_results_gen_SO}
    }
    % \hspace{0.05\linewidth}
    % Subfloat for FO (right two columns)
    \subfloat[FO models]{
        \begin{minipage}{0.48\linewidth}
            \centering
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Matthew/SplitNoUtility_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Matthew/SplitNoUtility_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/JobAlloc/SplitNoUtility_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/JobAlloc/SplitNoUtility_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Job/SplitNoUtility_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Job/SplitNoUtility_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Plant/SplitNoUtility_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/Plant/SplitNoUtility_fairness.png}
            \\
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/BiasedDM/SplitNoUtility_system_utility.png}
            \includegraphics[width=0.48\linewidth]{Figures/Generalization/BiasedDM/SplitNoUtility_fairness.png}
        \end{minipage}
        \label{fig:all_results_gen_FO}
    }
    \caption{Evaluation of Split Optimization (left) and Fair-Only Optimization (right) models trained on $\beta_{train}$ (for variance) and evaluated on $\beta_{test}$ across different environments. Brighter colors indicate better outcomes.}
    \label{fig:combined_results}
\end{figure*}



Here we provide additional results for our methods, including providing confidence intervals for our main results, additional results for generalization, and more comparison to baselines. 

\subsection{Confidence Intervals for Results}
We provide confidence intervals for system utility and fairness separately as we vary $\beta$, as plotting this on a Pareto plot (as in the main results) would be hard to read (Figure~\ref{fig:error-bars}). An interesting thing to note here is that we see a phase lag between when variance starts to reduce, and when utility starts to drop. This shows there is a range of solutions where fairness can be improved without significantly harming the utility.

\subsection{Approximating the Pareto Front}
We also show generalization results by generating approximate Pareto fronts for all methods (Figure~\ref{fig:approx_pareto_fronts})   based on a limited set of $\beta_{train}$ models, showing that the SO and FO methods generalize very well even without training for all intermediate $\beta$ values. One interesting thing to note here is that FO falls under the Pareto front with intermediate $\beta$ values, showing how a tuned utility model also helps in guiding agents towards better decisions.

\subsection{Comparison of Selected Models}
Figure~\ref{fig:all_barplots} shows the performance of selected DECAF models trained on variance when evaluated on different metrics on all environments, compared to the baselines. We can see DECAF models perform much better on all metrics. For BiasedDM, all methods were able to converge to approximately the same fairest policy, so the differences in the figure are small. The variance for SO might appear large, but we point out that this variance is scaled 10000 times, and the actual values are all very close to zero.

\subsection{Generalization Heatmaps}
We also provide the generalization results for varying $\beta_{test}$ for both Split (Figure~\ref{fig:all_results_gen_SO}) and Fair Only (Figure~\ref{fig:all_results_gen_FO}) models for all environments. We note that the general trends noted in the main experimental results hold, with each model able to improve fairness as $\beta_{test}$ is increased. Further, in all cases, FO maximizes utility at $\beta_{test}=0$, and has a sharper transition between utility-maximizing and fairness-maximizing behavior.