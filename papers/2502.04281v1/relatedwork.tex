\section{Related Work}
\label{sec:related}

While DECA has not been formalized in prior work, it has seen application in many domains. From optimizing passenger-driver matches in ridesharing~\citep{shah2020neural, qin2022RLforRides} to efficient allocation of homelessness resources~\citep{kube2019allocating, kube2023community}, many real-world applications follow this general structure. 
It is also analogous to the predict-then-optimize (P+O) approach \citep{wang2021PtO, elmachtoub2022smartPtO}, where a predictive model estimates unknown parameters that are subsequently used in optimization. However, unlike P+O, which may not specifically address resource allocation or multi-agent systems, DECA explicitly focuses on these complexities. This distinction is crucial as it allows us to restrict the problem space and tailor our research towards enhancing fairness within multi-agent resource allocation.

Significant research has addressed algorithmic bias, where ML models, such as those used in hiring decisions \citep{raghavan2020hiringbias}, can exhibit harmful biases. 
We refer readers to an extensive survey by \citet{mehrabi2021MLBiassurvey} for a review of recent work.
These studies typically focus on debiasing the outputs of predictive models to meet fairness criteria such as equalized odds~\citep{hardt2016equality} or demographic parity~\citep{DP_dwork2012}. However, our work diverges from this approach. Instead of correcting biases in predictions, we aim to develop algorithms that inherently promote fair decision-making via the actions they optimize.

Our focus in this paper is on designing ways to learn fair policies in a multi-agent RL setting. \citet{FairRLSurvey} present a survey on RL methods used to improve fairness. We now highlight a few papers that are the closest to our work: FEN~\citep{jiang2019FEN} uses a hierarchical network to learn a fair-efficient policy for multi-agent coordination, learning to optimize the coefficient of variation, with a meta network that selects when each agent behaves greedily or fairly. However, the model does not allow for resource constraints, instead opting for a first-come-first-serve approach. Further, this approach needs communication between agents to allow agents to choose between acting fairly and efficiently. Some methods, on the other hand, propose to optimize fairness in a multi-objective MDP, where each agent's utility is treated as a separate objective, and the goal is to optimize the a social welfare function over agent rewards~\citep{zimmer2021MOMDP,siddique2020MOMDP}. 
This means the learning agent has to predict the utility over the joint action space~\citep{siddique2020MOMDP}, or, as done by SOTO~\citep{zimmer2021MOMDP}, use a decentralized policy gradient based approach, which prevents use of global constraints. 
Finally, SI~\citep{SI_kumar2023} is an approach for improving fairness in rideshare-matching that attempts to improve fairness through myopic fairness post-processing of black-box utility estimates. However, it does not attempt to learn long-term fairness, and is specially designed for the ridesharing domain.

The DECA approach allows us to consider global constraints while allocating resources, opening up the scope for better global solutions, which none of the prior approaches allow. The distributed evaluation allows each agent to only learn a local value function, which reduces the complexity when compared to learning a joint policy. Further, our Split and Fair-Only approaches allow changing the trade-offs between utility and fairness post-training, which provides additional flexibility that previous approaches lack. 


\input{problem_formulation}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/variance.png}
    \caption{Change in system utility and fairness as $\beta$ is increased, with $\beta=0$ at the top left $\beta=1$ at the bottom-right. For all domains, we can see that split and joint optimization perform similarly, while learning only fairness can sometimes be slightly worse. All our methods Pareto-dominate SOTO and FEN. Each point depicts the average performance over five different models trained at that $\beta$ value, and the lines show the Pareto front for each method.}
    \label{fig:main_results}
\end{figure*}