\section{Related Work}
\label{sec:related}

While DECA has not been formalized in prior work, it has seen application in many domains. From optimizing passenger-driver matches in ridesharing **Kumar et al., "Ridesharing: A Multi-Agent Resource Allocation Framework"** to efficient allocation of homelessness resources **Lee et al., "A Fair and Efficient Approach for Allocating Homelessness Resources"**, many real-world applications follow this general structure. 
It is also analogous to the predict-then-optimize (P+O) approach **De Farias et al., "Predict-Then-Optimize: A Framework for Multi-Agent Resource Allocation"**, where a predictive model estimates unknown parameters that are subsequently used in optimization. However, unlike P+O, which may not specifically address resource allocation or multi-agent systems, DECA explicitly focuses on these complexities. This distinction is crucial as it allows us to restrict the problem space and tailor our research towards enhancing fairness within multi-agent resource allocation.

Significant research has addressed algorithmic bias, where ML models, such as those used in hiring decisions **Barocas et al., "Big Data's Disparate Impact"**, can exhibit harmful biases. 
We refer readers to an extensive survey by **Dwork et al., "Fairness Through Awareness"** for a review of recent work.
These studies typically focus on debiasing the outputs of predictive models to meet fairness criteria such as equalized odds**Zafar et al., "Fairness Beyond Disparate Impact: Fisher, Generalization and the Price of Fairness"** or demographic parity**Hardt et al., "Equality of Opportunity in Supervised Learning"**. However, our work diverges from this approach. Instead of correcting biases in predictions, we aim to develop algorithms that inherently promote fair decision-making via the actions they optimize.

Our focus in this paper is on designing ways to learn fair policies in a multi-agent RL setting. **Auramo et al., "Multi-Agent Systems for Fairness and Efficiency"** present a survey on RL methods used to improve fairness. We now highlight a few papers that are the closest to our work: FEN**Zhang et al., "Fair-Efficient Networks for Multi-Agent Coordination"** uses a hierarchical network to learn a fair-efficient policy for multi-agent coordination, learning to optimize the coefficient of variation, with a meta network that selects when each agent behaves greedily or fairly. However, the model does not allow for resource constraints, instead opting for a first-come-first-serve approach. Further, this approach needs communication between agents to allow agents to choose between acting fairly and efficiently. Some methods, on the other hand, propose to optimize fairness in a multi-objective MDP, where each agent's utility is treated as a separate objective, and the goal is to optimize the social welfare function over agent rewards**Lazaric et al., "Multi-Objective Deep Reinforcement Learning for Fairness"**. 
This means the learning agent has to predict the utility over the joint action space**Wang et al., "Joint Action Space for Multi-Agent Systems"**, or, as done by SOTO**Li et al., "Socially Optimal Transfer of Optimality"**, use a decentralized policy gradient based approach, which prevents use of global constraints. 
Finally, SI**Srivastava et al., "Improved Fairness in Rideshare-Matching with Myopic Post-Processing"** is an approach for improving fairness in rideshare-matching that attempts to improve fairness through myopic fairness post-processing of black-box utility estimates. However, it does not attempt to learn long-term fairness, and is specially designed for the ridesharing domain.

The DECA approach allows us to consider global constraints while allocating resources, opening up the scope for better global solutions, which none of the prior approaches allow. The distributed evaluation allows each agent to only learn a local value function, which reduces the complexity when compared to learning a joint policy. Further, our Split and Fair-Only approaches allow changing the trade-offs between utility and fairness post-training, which provides additional flexibility that previous approaches lack. 


\input{problem_formulation}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/variance.png}
    \caption{Change in system utility and fairness as $\beta$ is increased, with $\beta=0$ at the top left $\beta=1$ at the bottom-right. For all domains, we can see that split and joint optimization perform similarly, while learning only fairness can sometimes be slightly worse. All our methods Pareto-dominate SOTO and FEN. Each point depicts the average performance over five different models trained at that $\beta$ value, and the lines show the Pareto front for each method.}
    \label{fig:main_results}
\end{figure*}