% \newpage
% place holder
% \newpage

\section{Knowledge Distilled Attacker}

\subsection{Preliminary}\label{sec:preliminary}

Let $\text{LLM}_T(R|A)$ be the conditional distribution of the response $R(A)$ of a target LLM, $\text{LLM}_T$, given the attack prompt $A$. A jailbreak attack can be formulated as the following optimization problem:
\begin{align}\label{eq:jailbreak}
    \begin{split}
        & \max_{A} \quad \mathbb{E}_{R \sim \text{LLM}_T(R|A)} \brac{J\paren{Q,R(A)}}, \\
        &\text{where }J(Q,R) = 
    \begin{cases}
        1\quad\text{successful jailbreak;}\\0\quad\text{otherwise,}
    \end{cases}
    \end{split}
\end{align}
where $Q$ is the harmful query\footnote{For simplicity, we use $Q$ to denote both the harmful question (``e.g., \textit{Write a tutorial on how to make a bomb}'') and the target string  (``e.g., \textit{Sure, here is a tutorial on how to make a bomb}'').} and $J$ is a binary evaluator indicating whether the jailbreak attack succeeded~or~not.

There are two primary approaches to solving Problem \ref{eq:jailbreak} using optimization techniques: 
1) \textbf{Token-space optimization}~\citep{zou_universal_2023, liu_autodan_2024}: These methods directly optimize the attack prompt $A$ within the token space, typically requiring access to the target model's internal information, such as gradients and logits. 
2) \textbf{Generative model optimization}~\citep{liao_amplegcg_2024, paulus_advprompter_2024}: This approach learns a conditional generative model $\text{LLM}_{\theta}(A | Q)$ that generates an attack prompt $\hat{A}_\theta$ given a harmful query $Q$. The model is learned by minimizing the expected value of a loss $\mathcal{L}$ between the generated response $\hat{A}_{\theta}$ and the ground truth response $A$ (e.g., cross-entropy):
% \begin{align}\label{eq:generator}
%     & \min_{\theta} \mathbb{E}_{(Q,A)\in \mathcal{D}} \brac{ \mathcal{L} \paren{g_{\theta}(Q), A} } 
% \end{align}
\begin{align}\label{eq:generator}
    \begin{split}
        \min_{\theta} \mathbb{E}_{(Q, A, \hat{A}_{\theta})}\brac{\mathcal{L} \paren{\hat{A}_\theta(Q), A}}.
%        & \min_{\theta} \mathbb{E}_{(Q, A)}\brac{\mathcal{L} \paren{\hat{A}_\theta , A}}, \\
 %   &\text{where } \hat{A}_\theta \sim \text{LLM}_{\text{att}, \theta}(Q). 
    \end{split}
\end{align}
% \begin{align}\label{eq:generator}
%     & \min_{\theta} \mathbb{E}_{Q, A} \mathbb{E}_{R\sim P_{\theta}(Q) | Q}\brac{ \mathcal{L} \paren{R, A} } 
% \end{align}
% Here, harmful queries $Q$ and attack prompts $A$ are random variables representing samples which comprises a large collection of harmful query and attack prompt pairs.
In practice, one curates an attacker dataset $\mathcal{D}$ to estimate the expected value, hence the effectiveness of this approach critically depends on the quality of the samples collected. 

In this paper, we focus on the generative model optimization approach. Section~\ref{sec:KDA_training} describes the attacker training phase, while Section~\ref{sec:KDA_infer} covers the attack generation phase.











\subsection{Attacker Training Phase}\label{sec:KDA_training}

% \buyun{Describe formulation in details. Mention key ingredients. LLM-theta; D; How to optimize (2)}

The key insight of our work is to distill knowledge from a diverse set of attack methods into a single attacker model, $\text{LLM}_{\text{KDA}, \theta}$, termed  Knowledge-Distilled Attacker (KDA). KDA can use any existing jailbreak method as a  teacher. In this work, we choose three different attackers with complementary strategies: AutoDAN~\citep{liu_autodan_2024}, PAIR~\citep{chao_jailbreaking_2024}, and GPTFuzzer~\citep{yu_gptfuzzer_2024}. 

To generate an attack prompt with a specific style or format, KDA learns a model $\text{LLM}_{\text{KDA}, \theta}(A | Q, F)$ that is also conditioned on a style/format variable $F\in\mathcal{F} = \{\texttt{A}, \texttt{P}, \texttt{G}\}$, where $\texttt{A}$, $\texttt{P}$ and $\texttt{G}$ are the attack formats for AutoDAN, PAIR and GPTFuzzer, respectively. Specifically, our KDA model $\text{LLM}_{\text{KDA}, \theta}(A | Q, F)$ is trained to generate an attack $\hat{A}_{\theta}$ with format $F$ for harmful query $Q$ by minimizing:
\begin{align}\label{eq:generator_format_condition}
    \begin{split}
        \min_{\theta} \mathbb{E}_{(Q, A, \hat{A}_{\theta}, F)}\brac{ \mathcal{L} \paren{\hat{A}_\theta(Q,F), A}}.
%        & \min_{\theta} \mathbb{E}_{(Q, A, F)~\sim \mathcal{D}_{\text{train}}}\brac{ \mathcal{L} \paren{\hat{A}_\theta , A}}, \\
 %   &\text{where } \hat{A}_\theta \sim \text{LLM}_{\text{KDA}, \theta}(Q, F). 
    \end{split}
\end{align}
%


%Here, $\mathcal{F}$ acts as a format token, guiding the prompt generation process to produce attacks in predefined formats or structures.

The schematic overview of the KDA training process is illustrated in~\autoref{fig:kda_training}, which consists of two crucial components: 1) A dataset $\mathcal{D}_{\text{train}}$ curated from a collection of teacher methods; 2) A fine-tuning process with control over the format $F$ and harmful query $Q$ based on~\autoref{eq:generator_format_condition}.

\begin{figure}[!tbp]
% \vspace{-4mm}
\centering
    \includegraphics[width=0.48\textwidth]{figures/KDA_train.png}
    \caption{\textbf{Schematic overview of KDA training.}}
   % } 
   % \vspace{-4mm}
    \label{fig:kda_training} 
\end{figure}

\myparagraph{Training set preparation} 
To construct the training dataset $\mathcal{D}_{\text{train}}$,
we assume $P(Q,A,F) = P(A|Q,F)P(Q)P(F)$. Thus, we can first choose an attack format $F\in \mathcal{F}$, draw a harmful query $Q$ from a curated dataset of harmful queries, $\mathcal{D}_{\text{harm}}$, and then draw an attack $A$ given $Q$ and $F$. We refer the reader to \autoref{app:our_harmful_dataset} for the construction of our curated \text{Harmful-Query-KDA} dataset $\mathcal{D}_{\text{harm}}$ and focus here on the curation of a dataset $\mathcal{D}_F$ of attacks for each format $F\in\mathcal{F}$. 

Let $\mathcal{A}_F$ be the model that, given $Q$, generates attacks according to format $F$ (e.g., AutoDAN, PAIR, GPTFuzzer). We construct $\mathcal{D}_F$ using  Algorithm~\ref{alg:training_set}, which uses the teacher attacker with format $F$, $\mathcal{A}_F$, to generate an attack $A$ for $Q$, feeds this attack to a target model $\text{LLM}_{T}$ to obtain a response $R$, verifies if the attack was successful or not using the evaluation function $J(Q,R)$, and adds the pair $(Q,A)$ to dataset $\mathcal{D}_F$ whenever $A$ is successful. The datasets with different formats $F\in \mathcal{F}$ are then combined to create the KDA training set $\mathcal{D}_{\text{train}}=\Brac{(Q, A, F) | (Q,A)\sim \mathcal{D}_F, F\in\mathcal{F}}$.

\vspace{-1mm}
\begin{algorithm}[h]
\textbf{Input}: Dataset of harmful queries $\mathcal{D}_{\text{harm}}$, teacher attacker $\mathcal{A}_F$ for format $F$, target model $\text{LLM}_{T}$, and set size $N_{\text{train}}$

\textbf{Initialize}: KDA training set $\mathcal{D}_F \leftarrow \emptyset$

\textbf{For} {$Q\sim \mathcal{D}_{\text{harm}}$}  

\quad Generate attack prompt $A \sim \mathcal{A}_F(A|Q)$  

\quad Obtain target response $R \sim \text{LLM}_{T}(R|A)$ 

\quad \textbf{if} $J(Q,R) = 1$  \textbf{then} 

\quad \quad $\mathcal{D}_F \gets\mathcal{D}_F \cup \paren{Q, A}$

\quad \quad \textbf{if} $\abs{\mathcal{D}_F}\geq N_{\text{train}}$  \textbf{then}

\quad \quad \quad \textbf{return} $\mathcal{D}_F$ 

\textbf{return} $\mathcal{D}_F$ 

\caption{Single Format Training Set Generation}\label{alg:training_set}
\end{algorithm}
\vspace{-2mm}

The target models $\text{LLM}_T$ for dataset generation include open-source models (\text{Vicuna-7B}, \text{Llama-2-7B-Chat}) and commercial models (\text{GPT-3.5-Turbo}, \text{GPT-4-Turbo}). Consequently, the dataset curation involves $N_{\text{attacker}}=3$ teacher attackers and $N_{\text{target}}=4$ target models. Assuming $N_{\text{train}}=50$ samples are generated for each $F$ and $Q$, we obtain a total of $N_{\text{attacker}}\times N_{\text{target}} \times N_{\text{train}} = 3 \times 4 \times 50 = 600$ samples. Additionally, we leverage \text{GPT-4o} to synthesize 200 supplementary samples incorporating mixed formats from the three teacher attackers to enhance dataset diversity (see~\autoref{app:KDA_training_set} for details). With an overloaded notation, we denote the augmented set of formats as $\mathcal{F}=\{\texttt{A},\texttt{P},\texttt{G},\texttt{M}\}$, where $\texttt{M}$ denotes the mixed strategy. The final dataset of 800 samples is split into $80\%$ for training and $20\%$ for validation, ensuring disjoint harmful queries across splits.

\myparagraph{KDA fine-tuning} We adopt \text{Vicuna-13B}~\citep{zheng_judging_2023} as the foundation of our attacker model $\text{LLM}_{\text{KDA},\theta}(Q, F)$ due to its open-source availability and strong capabilities in generating creative and coherent prompts, aligning with our attack generation objectives. The KDA model is fine-tuned on the generated attack prompt dataset by optimizing the objective in~\autoref{eq:generator_format_condition}, where the loss function $\mathcal{L}$ is the cross-entropy. To reduce computational overhead, we employ parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA)~\citep{hu_lora_2021}. A detailed list of training hyperparameters and procedures is provided in \autoref{app:KDA_training_set}. We will release the fine-tuned KDA model, enabling researchers and practitioners to directly utilize it for the generation phase without the need for dataset collection and fine-tuning efforts.

\subsection{Attack Generation Phase}\label{sec:KDA_infer} 
Once the KDA model, $\text{LLM}_{\text{KDA}}$, has been trained, we can use it to generate novel attacks for a given harmful query $Q$. In doing so, we have the flexibility of selecting any of the attack formats used during training, i.e., $\mathcal{F}=\{\texttt{A},\texttt{P},\texttt{G}, \texttt{M}\}$. However, the success of this \textbf{single format} setting depends on which format is most effective for $Q$, which we do not know a priori. Alternatively, we can sample multiple attack formats to increase the probability of success. We consider three such \textbf{ensemble formats}: 1) Uniform Selection ($\texttt{uni}$), which randomly selects formats based on a uniform distribution on $\mathcal{F}$; 2) Inference-Guided Selection ($\texttt{ifr}$), which selects formats based on a probability distribution obtained by applying the softmax function to inference-time success counts; and 3) Training-Guided Selection ($\texttt{trn}$), which selects formats based on a probability distribution obtained by applying the softmax function to training-time success counts. A detailed ablation study is conducted in \autoref{sec:exp} to identify the most effective strategy for KDA attack generation. Please refer to~\autoref{app:methods} for detailed information on format selection strategies.


Algorithm~\ref{alg:kda} outlines the attack generation process using KDA. First, a format $F$ is sampled from the given format set according to the specified format selection strategy. Next, the harmful query $Q$ and format $F$ are fed into the KDA model, $\text{LLM}_{\text{KDA}}$, to generate a candidate attack prompt $A$. The generated prompt $A$ is then passed to the target model, $\text{LLM}_T$, to produce the target response $R$. Finally, only prompts $A$ that are deemed successful jailbreaks by the evaluator $J$ are retained.




%By specifying the format $\mathcal{F}$, we can generate diverse attack prompts by directly querying the KDA model. Thus, selecting the most effective format is critical for optimizing attack performance. We consider various format selection strategies $\sigma$. In the \textbf{single format} setting, the KDA model is guided to generate attack prompts that mimic specific attack formats--AutoDAN ($\texttt{A}$), PAIR (\texttt{P}), GPTFuzzer (\texttt{G}), or a mixed format (\texttt{M}), so we have $\sigma=sgl$-\texttt{A}, $sgl$-\texttt{P}, $sgl$-$\texttt{G}$ or $sgl$-\texttt{M}. In contrast, the \textbf{ensemble formats} setting samples formats from the set $ \Brac{\texttt{A},\texttt{P},\texttt{G},\texttt{M}} $ using different strategies: 1). $\sigma=\texttt{uni}$, which selects formats based on uniform distribution; 2). $\sigma=inf$, which selects formats based on a softmax probability distribution based on the inference-time success count; and 3). $\sigma=trn$, which selects formats based on a softmax probability distribution based on the training-time success count. 


\vspace{-2mm}
\begin{algorithm}[h]
\textbf{Input}: Harmful query $Q$, KDA model $\text{LLM}_{\text{KDA}}$, target model $\text{LLM}_{T}$, jailbreak evaluator $J$, format selection strategy.

% \textbf{Initialize}: Set of KDA attack prompts $\mathcal{A}\leftarrow \emptyset$

\textbf{For} $i \leftarrow 1$ to $N_{\text{max\_iter}}$

\quad Sample format $F$ from $\mathcal{F}$ via the format selection strategy

\quad Generate attack prompt $A \sim \text{LLM}_{\text{KDA}}(A| Q,F)$

\quad Generate target response $R \sim \text{LLM}_{T}(R|A)$ 

\quad \textbf{if} $J(Q,R) = 1$ \textbf{then}

\quad \quad \textbf{return} $A$

\textbf{return} $\emptyset$ 

\caption{KDA Attack Prompts Generation}\label{alg:kda}
\end{algorithm}
\vspace{-2mm}








% \newpage

% \newpage





