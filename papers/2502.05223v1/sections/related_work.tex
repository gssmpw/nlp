


% \newpage

% placeholder

% \newpage

\section{Related Work}

% \myparagraph{Properties essential for Jailbreaking}

Several key attributes mentioned in  Section~\ref{sec:intro} are the focus of ongoing research in jailbreak studies.

\myparagraph{Automation} Early jailbreak attacks, such as the Do-Anything-Now (DAN) prompt~\citep{walkerspider_dan_2022, shen_anything_2024} and MJP~\citep{li_multi-step_2023},  were performed predominantly by manually crafting attack prompts through trial-and-error. However, due to the limited scalability of manual methods, recent research like GCG~\citep{zou_universal_2023} shifted to automated jailbreak techniques, which leverage algorithmic approaches to systematically generate attack prompts, providing a more scalable solution.
    
    % While being highly effective against state-of-the-art (SOTA) LLMs, handcrafted prompts are not practical for comprehensive risk assessment due to their poor scalability and adaptability across various scenarios. \textit{To minimize the effort of manual prompting, the attack method should be automatic.}

    
\myparagraph{Coherence} Certain methods \citep{zou_universal_2023} generate nonsensical prompts that are unlikely to occur in real-world scenarios, limiting their utility for assessing LLM risks. Furthermore, these prompts can be easily mitigated by defensive techniques such as perplexity-based detection~\citep{alon_detecting_2023} or randomized smoothing~\citep{robey_smoothllm_2024}. In contrast, more recent approaches~\citep{chao_jailbreaking_2024, liu_autodan_2024} leverage LLMs to generate or rephrase attack prompts, resulting in coherent and more effective attacks.


    
\myparagraph{Open-source dependency} Many frameworks~\citep{yu_gptfuzzer_2024} rely on commercial LLMs, such as GPT-4~\citep{openai_gpt-4_2024}, for critical steps in attack generation (e.g., mutation, rephrasing). This reliance increases costs and reduces reproducibility, especially when model versions change. In contrast, recent methods~\citep{chao_jailbreaking_2024} only require open-source dependency, providing more cost-effective and consistent alternatives.

\autoref{tab:jailbreaks} summarizes the attributes of various jailbreak methods in terms of automation, coherence, and open-source dependency, emphasizing that many attack methods fail to meet these criteria. As discussed in Section~\ref{sec:intro}, KDA distinguishes itself from other automated, coherent, and open-source methods by eliminating the need for meticulous prompt engineering and large query volumes. 



\input{tables/jailbreak_list}


\myparagraph{Comparison to prior generative model optimization-based methods}
As discussed in Section~\ref{sec:preliminary}, our KDA training leverages generative model optimization by solving~\autoref{eq:generator}. This methodology is also employed by prior works such as AmpleGCG~\citep{liao_amplegcg_2024}, which uses GCG-generated data for training, and AdvPrompter~\citep{paulus_advprompter_2024}, which fine-tunes its model on adversarial suffixes obtained through token-level optimization. The key distinction of our approach lies in distilling knowledge from an ensemble of teacher attackers into a single model while incorporating control over attack formats. This format control enables KDA to generate significantly more diverse prompts than these methods, contributing to its superior performance.

% \myparagraph{Additional comparison to prior generative model optimization-based methods} As mentioned in Section~\ref{sec:preliminary}, our KDA training is based on generative model optimization by solving~\autoref{eq:generator}, where AmpleGCG~\citep{liao_amplegcg_2024} and AdvPrompter~\citep{paulus_advprompter_2024} also focuses on this method. 

% AmpleGCG utilizes GCG data as training data, while AdvPrompter is finetuned on their adversarial suffix found by a token-level optimization. The key difference of our KDA lying on distilling the knowledge from an ensemble of teacher attackers into a single model by introducing a control on attack format, which is the source of KDA's superior performance. By doing so, KDA can generate much more diverse prompts than these two methods.




% To address the five challenges outlined above, the attack method must possess the following characteristics: 1). To minimize the effort of manual prompting, the method should be \textbf{A}utomatic. 2). To successfully jailbreak commercial LLMs, it should operate in a \textbf{B}lack-box setting, without requiring access to model internals. 3). To reflect real-world user scenarios, the generated prompts must be \textbf{C}oherent. 4). To challenge the defense mechanisms, the method should produce attacks with \textbf{D}iverse patterns. 5). For reproducibility and cost efficiency, it must be \textbf{E}xempt from dependency on commercial LLMs during the attack generation process. 

% \vspace{-3mm}



% \vspace{-2mm}

% As summarized in \autoref{tab:jailbreaks}, to the best of our knowledge, PAIR~\citep{chao_jailbreaking_2024} is the only existing method with these attributes. However, it struggles with a low attack success rate (ASR) when targeting many SOTA models, such as \texttt{Llama-2} and \texttt{GPT-3.5}, limiting its utility in red-teaming advanced LLMs.  In addition to the ABCDE features, computational efficiency is another crucial factor. While methods like AutoDAN~\citep{liu_autodan_2024} and GPTFuzzer~\citep{yu_gptfuzzer_2024} perform well, they are computationally intensive, taking 5-10 minutes per successful attack\footnote{All experiments were conducted on 8 A5000 GPUs}. This makes them unsuitable for large-scale red-teaming and adversarial assessments, which require high volumes of attack prompts to expose vulnerabilities \citep{dubey_llama_2024}. 


































% Jailbreak attacks can be classified into five distinct categories based on their unique characteristics and the methodologies employed in their generation. This classification is also shown in \autoref{tab:jailbreaks}.

% % \vspace{-3mm}

% \paragraph{Automatic vs. manual.} Early attempts at jailbreaking LLMs involved manually crafted prompts, such as those in DAN~\citep{walkerspider_dan_2022}, which use carefully designed phrasing to provoke unethical responses from safety-aligned LLMs. \citet{dan_chat_2023} gives an overview of manual jailbreak efforts by compiling prompts from sources like Reddit, Discord, JailbreakChat.com, and other web platforms. Both \citet{wei_jailbroken_2023} and \citet{li_multi-step_2023} base their evaluations heavily on these handcrafted prompts. However, due to the limited scalability of manual methods, recent research like GCG~\citep{zou_universal_2023} shifted to automated jailbreak techniques, which leverage algorithmic approaches to systematically generate attack prompts, providing a more scalable solution.

% % \vspace{-3mm}

% \paragraph{Black-box vs. white-box.} GCG was the first to automate attack generation through token-level optimization, requiring white-box access to gradient information for attack generation. Similarly, AutoDAN2~\citep{zhu_autodan_2023} and ASETF~\citep{wang_asetf_2024} use gradient-based approaches for discrete optimization, while COLD~\citep{guo_cold-attack_2024} leverages gradients in an energy-based method. Although AutoDAN~\citep{liu_autodan_2024} and SMJ~\citep{li_semantic_2024} employ gradient-free optimization techniques, such as genetic algorithms, they still require white-box access to compute the log-likelihood of token sequences to evaluate fitness scores. In this paper, we categorize any method that relies on internal model information as a white-box attack, even when it exhibits strong transferability to black-box models. In contrast, black-box attacks do not require access to model internals, offering greater versatility and being more suitable for jailbreaking commercial LLMs. For instance, PAIR~\citep{chao_jailbreaking_2024}, GPTFuzzer~\citep{yu_gptfuzzer_2024}, and TAP~\citep{mehrotra_tree_2024} rely solely on the target LLMâ€™s responses to refine their attack prompts.

% % \vspace{-3mm}
    
% \paragraph{Coherent vs. nonsensical.} Methods like  GCG, PAL~\citep{sitawarin_pal_2024}, and Opensesame~\citep{lapid_open_2023} often generate nonsensical prompts due to token-level optimization. Similarly, Adaptive Attack~\citep{andriushchenko_jailbreaking_2024} relies on random search, producing gibberish, while AmpleGCG~\citep{liao_amplegcg_2024} uses non-sensical suffixes for training, leading to incoherent outputs. Such prompts cannot resemble real-world attack scenarios and are easily mitigated by existing defenses~\citep{alon_detecting_2023, robey_smoothllm_2024}. In contrast, newer approaches generate coherent prompts. PAIR employs in-context learning, AutoDAN optimizes prompts at the sentence level, and methods like DeepInception~\citep{li_deepinception_2024} and CodeChameleon~\citep{lv_codechameleon_2024} use structured templates to ensure coherence.

% % \vspace{-3mm}

% \paragraph{Diverse vs. monotonous.} Existing methods like DeepInception, LRL~\citep{yong_low-resource_2024}, CodeChameleon,  DRA~\citep{liu_making_2024}, ArtPrompt~\citep{jiang_artprompt_2024}, and DrAttack~\citep{li_drattack_2024} rely on templates, fixed functions, or static obfuscation strategies to elicit harmful responses from LLMs. Although effective, these approaches tend to produce repetitive attack patterns, making them easier to detect and counter. Unlike manually crafted techniques, which offer more variety, these methods often use encryption and decryption mechanisms to conceal malicious intent. In contrast, diverse prompt-generation methods like GPTFuzzer, AutoDAN, and PAIR create varied attack prompts through mutation, genetic algorithms, or in-context learning. Such variability presents a greater challenge for LLM safety systems, complicating detection and mitigation.


% % \vspace{-3mm}

% \paragraph{Exempt from commercial LLMs dependency.} Many frameworks rely on commercial LLMs for attack generation. For example, AutoDAN and GPTFuzzer use \texttt{GPT-3.5} for mutation, while DrAttack requires \texttt{GPT-4} for decomposition. Rainbow Teaming~\citep{samvelyan_rainbow_2024} and TAP rely on \texttt{GPT-4} as the Judge LLM, and PAIR relies on the public API of \texttt{Mixtral-8x7B}, which incurs charges. ArtPrompt uses \texttt{GPT-3.5} for paraphrasing and \texttt{GPT-4} for font generation, ReneLLM~\citep{ding_wolf_2024} uses \texttt{GPT-3.5} for prompt rewriting, and PAP~\citep{zeng_how_2024} fine-tunes \texttt{GPT-3.5} for persuasive paraphrasing. Puzzler~\citep{chang_play_2024} engages \texttt{GPT-4} to extract malicious content and \texttt{GPT-3.5} for offensive responses. This reliance on commercial LLMs raises cost and reproducibility issues, particularly with model updates. As a result, attack methods that avoid dependence on commercial LLMs are generally preferred for their affordability and consistency. In contrast, methods like AutoDAN and GCG avoid this dependency, offering more affordable and consistent solutions. We distinguish between ``commercial LLM dependency'', which involves relying on commercial models for attack generation, and ``black-box attacks'', which target commercial models without relying on them.

% \paragraph{Jailbreak evaluators.} Existing evaluation methods fall into four categories:  human evaluation, text matching, LLM-prompted evaluation, and safety classifiers. Human evaluation, while reliable, lacks scalability. Text matching~\citep{zou_universal_2023}, based on predefined refusal phrases (e.g., \autoref{app:tm}), has limited coverage, allowing harmful content to bypass detection. The LLM-prompted evaluation depends heavily on the prompt quality~\citet{chao_jailbreaking_2024}. Safety classifiers, such as \texttt{Llama-Guard-2}~\citep{llama_team_meta_2024}, classify conversations by safety. In this paper, We fine-tune \texttt{Llama-Guard-2} to better detect diverse malicious queries.


% % Lastly, safety classifiers such as \texttt{Llama-Guard-2-8b}~\citep{llama_team_meta_2024} is an LLM safeguard model fine-tuned on pretrained LLMs to classify the conversation into different safety categories. Our approach finetunes the \texttt{Llama-Guard-2-8b} to classify better the diverse set of malicious queries seen in practice. 

% \paragraph{Comparison with PAIR.} Among all the related jailbreak methods in~\autoref{tab:jailbreaks}, PAIR~\citep{chao_jailbreaking_2024} is the most similar to our work. The key differences are as follows: (1) PAIR generates attacks using few-shot prompting with hand-crafted examples, while our method, KDA, employs knowledge distillation and is fine-tuned on a curated dataset of successful jailbreak prompts in a supervised manner. This allows KDA to produce successful attacks with shorter system instructions and reduced inference time.  (2) For evaluation, PAIR relies solely on pre-trained LLMs such as \texttt{GPT-4} or \texttt{Llama-Guard}, whereas KDA uses a fine-tuned evaluator based on human annotations, ensuring more reliable assessments. Further empirical comparisons are presented in Section~\ref{sec:eval_performance_comparison}. 


% % Second, for evaluation, PAIR uses only pre-trained LLM such as \texttt{GPT-4} or \texttt{Llama Guard} for assessing jailbreak quality, whereas KDA utilizes a finetuned evaluator based on human annotation that ensures reliable evaluation. We provide further empirical comparisons in Section~\ref{sec:eval_performance_comparison}. 
% % \vspace{-2mm}









