\section*{Impact Statement}\label{sec:broader_impact}

The development of the Knowledge-Distilled Attacker (KDA) enhances automated red-teaming for large language models (LLMs) by providing an efficient and scalable tool for evaluating and stress-testing safety mechanisms. By distilling knowledge from diverse jailbreak attacks, KDA enables researchers and practitioners to systematically identify vulnerabilities, refine defenses, and improve the robustness of LLM safeguards. The open-source release of KDA also fosters transparency and reproducibility in adversarial robustness research, promoting collaboration across academia, industry, and policy sectors to strengthen AI security.

However, KDA, like any attack framework, carries the risk of misuse. While designed for security evaluation, malicious actors could attempt to exploit it to generate harmful content or evade safety controls. To mitigate this risk, responsible disclosure practices—such as usage restrictions, ethical guidelines, and controlled access—should be considered. Future research should focus on balancing the need for robust red-teaming with safeguards against abuse, ensuring that advancements in adversarial testing contribute to, rather than compromise, the safety of AI systems.

\newpage