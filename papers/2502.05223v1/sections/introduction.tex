


\section{Introduction}\label{sec:intro}

\begin{figure*}[h]
% \vspace{-4mm}
\centering
    \includegraphics[width=0.85\textwidth]{figures/KDA_inference.png}
    % \vspace{-2mm}
    \caption{\textbf{KDA Attack Generation: Overview and Example Prompts.} (Top) Schematic overview of the KDA attack generation process. The formats \texttt{A}, \texttt{P}, \texttt{G}, and \texttt{M} correspond to prompts learned from the teacher attackers: AutoDAN, PAIR, GPTFuzzer, and Mixed, respectively. (Bottom) Examples of attack prompts generated by KDA, conditioned on different formats. }\label{fig:opening}
   % \vspace{-1mm}
\end{figure*}

With the widespread adoption of Large Language Models (LLMs) across critical domains such as biomedicine~\citep{tinn_fine-tuning_2023}, financial analysis~\citep{wu_bloomberggpt_2023}, code generation~\citep{roziere_code_2024}, and education~\citep{kasneci_chatgpt_2023}, ensuring their alignment with human values has become paramount. Jailbreak attacks have emerged as a popular red-teaming strategy to bypass LLM safety mechanisms, leading to harmful, illegal, or objectionable outputs~\citep{dubey_llama_2024, zou_universal_2023}. Many jailbreak attacks share essential properties that enhance their practicality: \textbf{automation} to eliminate human effort in creating jailbreak prompts by generating them automatically, \textbf{coherence} to generate attack prompts that mimic real-world scenarios and cannot be easily detected, and \textbf{open-source reliance} to reduce the cost of generating such prompts by leveraging non-commercial LLMs. Despite the growing prevalence of automated, coherent, and open-source attack methods~\citep{chao_jailbreaking_2024, liu_autodan-turbo_2024, mehrotra_tree_2024, li_deepinception_2024, yong_low-resource_2024, liu_making_2024, lv_codechameleon_2024, liu_autodan_2024, zhu_autodan_2023, wang_asetf_2024, li_semantic_2024, guo_cold-attack_2024}, their real-world effectiveness remains constrained by two critical challenges: 

% Many jailbreak attacks share essential properties that enhance their practicality, including the ability to automatically generate prompts to eliminate human effort, ensuring coherent prompts that are harder to detect by defensive algorithms, and relying on zero commercial model dependencies, which provides a cost-effective solution by leveraging the open-source ecosystem. Despite the increasing presence of \textbf{automated}, \textbf{coherent}, and \textbf{open-source} attack methods~\citep{chao_jailbreaking_2024, liu_autodan-turbo_2024, mehrotra_tree_2024, li_deepinception_2024, yong_low-resource_2024, liu_making_2024, lv_codechameleon_2024, liu_autodan_2024, zhu_autodan_2023, wang_asetf_2024, li_semantic_2024, guo_cold-attack_2024}, their real-world effectiveness remains constrained by two critical challenges: 

%\vspace{-3mm}
\myparagraph{Reliance on careful prompt engineering} 
Many jailbreaking methods that leverage LLM attackers, such as PAIR~\citep{chao_jailbreaking_2024}, TAP~\citep{mehrotra_tree_2024}, and AutoDAN-Turbo~\citep{liu_autodan-turbo_2024}, rely on carefully crafted system prompts to guide harmful prompt generation. The success of these methods heavily depends on prompt quality, making the identification of optimal prompts a complex and non-trivial task. Similarly, puzzle/game-based approaches—such as DeepInception~\citep{li_deepinception_2024} (nested puzzles), LRL~\citep{yong_low-resource_2024} (low-resource languages), DRA~\citep{liu_making_2024} (obfuscating malicious intent), and CodeChameleon~\citep{lv_codechameleon_2024} (code-based puzzles)—rely on fixed, carefully designed templates with limited variation. This lack of diversity renders them more susceptible to detection and mitigation by advanced safety mechanisms, diminishing their long-term effectiveness.



% Many jailbreak methods depend on meticulously engineered system prompts to guide prompt generation. Techniques that leverage LLMs as attackers, such as PAIR~\citep{chao_jailbreaking_2024}, TAP~\citep{mehrotra_tree_2024}, and AutoDAN-Turbo~\citep{liu_autodan-turbo_2024}, require meticulously crafted instructions to shape the model's behavior in generating harmful outputs. However, the success of these methods is highly contingent on prompt quality, making the identification of optimal prompts a complex and non-trivial task. Additionally, puzzle/game-based approaches—such as DeepInception~\citep{li_deepinception_2024} (nested puzzles), LRL~\citep{yong_low-resource_2024} (low-resource languages), DRA~\citep{liu_making_2024} (obfuscating malicious intent), and CodeChameleon~\citep{lv_codechameleon_2024} (code-based puzzles)—rely on fixed, carefully designed templates with limited variation. This lack of diversity renders them more susceptible to detection and mitigation by advanced safety mechanisms, ultimately diminishing their long-term effectiveness.

\myparagraph{Need for a large number of queries} Many attack methods require extensive queries to optimize directly on the token space. These include genetic algorithm-based approaches such as AutoDAN~\citep{liu_autodan_2024} and SMJ~\citep{li_semantic_2024}, gradient-based methods like AutoDAN2~\citep{zhu_autodan_2023}, ASETF~\citep{wang_asetf_2024}, and COLD~\citep{guo_cold-attack_2024}. Each of these methods typically requires hundreds to thousands of iterations per attack. This substantial query demand poses a significant challenge to large-scale red teaming, restricting its scalability and practical deployment.

\newpage

These challenges lead us to our main research question:

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black]
{\it Can we create an automated, coherent, open-source attacker that is effective without requiring intricate prompt engineering or a large number of queries?}
\end{tcolorbox}


In this work, we propose a natural solution to these challenges by distilling knowledge from an ensemble of state-of-the-art (SOTA) attackers. Specifically, we distill the knowledge from three SOTA attackers, including AutoDAN, PAIR, and GPTFuzzer, into a single open-source LLM (Vicuna-13B) via supervised fine-tuning, effectively capturing their attack strategies. The resulting fine-tuned attacker, named Knowledge-Distilled Attacker (KDA), inherits the strengths and coherent nature of SOTA methods while significantly reducing computational costs. Specifically, KDA samples prompts based on prior knowledge, enhancing the diversity of attack prompts, and improving their effectiveness and efficiency by eliminating the need for meticulous prompt engineering, which demands human labor and expertise. 

%This approach enhances the diversity of attack prompts and improves efficiency in uncovering model vulnerabilities.

In summary, this work makes the following contributions:
%
\begin{itemize}[wide]
    \item We propose the KDA, an open-source attacker model that distills and integrates the strengths of multiple SOTA teacher attackers. As shown in~\autoref{fig:opening}, 
    %the schematic overview of KDA attack generation, 
    KDA generates prompts in an ensemble of formats that represent the teacher attackers, integrating their distinct strategies into a unified framework. By learning diverse attack patterns from these teacher models, KDA automatically generates effective, efficient, diverse, and coherent attack prompts, applicable to both open-source and commercial LLMs, while eliminating the need for meticulous prompt engineering. To facilitate further research, we will release both the KDA model and the training set, removing the need for extensive effort during the training phase.


    \item KDA demonstrates superior attack success rates (ASR) on various open-source and commercial LLMs compared to the Harmbench~\citep{mazeika_harmbench_2024} baseline on its standard behavior dataset. Notably, it achieves an $88.5\%$ ASR on Llama-2-7B-Chat, $83.5\%$ on Llama-2-13B-Chat, and $100\%$ ASR on Vicuna, Qwen, and Mistral models. Furthermore, KDA showcases strong generalization by achieving high ASRs on unseen datasets, such as Harmbench and Harmful-Behavior~\citep{chao_jailbreaking_2024}, as well as on target LLMs not included in the training.

    \item We evaluate KDA across effectiveness, efficiency, diversity, and coherence metrics, where it outperforms or performs comparably to SOTA baselines including its teacher attackers.  Ablation studies reveal that KDA's superior performance stems from format ensembling, which enhances both attack diversity and success. Additionally, ablations on topic diversity highlight that KDA's ability to generate complementary attack patterns is key to its high diversity.




    


\end{itemize}

% \newpage


    % \item  We propose the Knowledge-Distilled Attacker (KDA), an attacker model created by transferring knowledge from multiple SOTA attackers into an open-source LLM via knowledge distillation. Specifically, we construct a high-quality training set derived from attack prompts generated by AutoDAN, PAIR, and GPTFuzzer, and utilize it to fine-tune the Vicuna-13B model. The resulting KDA effectively generates automated, efficient, diverse, and coherent attack prompts. To facilitate further research, we will release both the KDA model and the training set, removing the need for extensive effort during the training phase.

    % \item KDA achieves superior results on unseen datasets (e.g., Harmbench~\citep{mazeika_harmbench_2024} and Harmful-Behavior~\citep{chao_jailbreaking_2024}) and on target models not included in training (e.g., Llama-2-13B-Chat, Vicuna-13B, Qwen-7B/13B Chat, Mistral-7B, Claude-2.1), showcasing strong generalization.


    % \item We conduct detailed ablation studies on format selection strategies, highlighting that KDA's superior effectiveness, efficiency, and diversity stem from format ensembling. Additionally, ablations on topic diversity reveal that the complementary attack patterns generated by KDA are key to its high diversity.

% In this work, we propose a natural solution to these challenges by distilling knowledge from an ensemble of state-of-the-art (SOTA) attackers. Our approach allows attackers to sample prompts based on prior knowledge, ensuring both effectiveness and efficiency while eliminating the need for meticulous prompt engineering, which demands human labor and expertise. Specifically, we distill knowledge from three SOTA attackers, including AutoDAN, PAIR, and GPTFuzzer, into a single open-source LLM (Vicuna-13B) via supervised fine-tuning, effectively capturing their attack strategies. The resulting fine-tuned attacker inherits the strengths and coherent nature of SOTA methods while significantly reducing computational costs. 


% As shown in \autoref{fig:opening} (Top), the schematic overview of KDA attack generation, KDA is an open-source LLM capable of generating prompts in an ensemble of formats, $\mathcal{F} = \{\texttt{A}, \texttt{P}, \texttt{G}, \texttt{M}\}$, representing AutoDAN, PAIR, GPTFuzzer, and Mixed, respectively. Although the formats are named after SOTA attackers, their styles may vary due to the integration of patterns learned from multiple attackers during training.  \autoref{fig:opening} (Bottom) provides examples of attack prompts generated by KDA when conditioned on different formats $\mathcal{F}$, with each format favoring distinct topics, styles, or patterns. This ensemble approach enhances the diversity of attack prompts and improves efficiency in identifying model vulnerabilities. 




% \input{figures/pami_overview}






% Leveraging an ensemble introduces greater diversity to attack patterns and avoids the repetitive styles guided by identical system instructions. Moreover, attackers can sample prompts based on prior knowledge, ensuring effectiveness and efficiency.


% on the reliance on carefully engineered system prompts, ensuring greater robustness across diverse target models. Additionally, it should be efficient while maintaining high effectiveness, minimizing query overhead and operational costs, especially when targeting commercial models.


% \newpage

% place holder

% \newpage





% \buyun{-----------------------------------------------------}

% A,B,C are good properties. Many attacker methods have it.  Limitations on: system prompts for ICL...mention a lot methods. serving as the attacker. prompt generating with similar prompt has similar pattern easy the be dected by safety mechanisms.  Large Query numbers is another problem, high cost especially when targeting commercial models.  

% Question: we want good ABC properties. eliminating the requirement on carefully design system prompts, more robust across different modesl？Efficient while keep effective? 

% To address this Natural idea is learning from existing SOTA attacker as teacher, distill their knowledge into a small model. Finetune, lightweight, so cheap.






% While many jailbreak methods have been proposed recently, they continue to encounter several challenges:










% token-level optimization; generator model optimization; instruction-based few-shot prompting. Adv prompter requires gray-box access: output probability needed. 

% PAIR; AutoDAN-turbo has long system prompts before. 

% Categorizing by searching space: human annotated; Random search; optimization based; teacher model.

% Maybe ABCED in Appendix. Some methods are also ABCDE. AutoDAN turbo? ABCDE + prior knowledge? Random search?

% \textbf{\buyun{classify the papers as random search (generic algorithm; learning from teacher models; optimization based). ABCDE classification can be summarized in one table. Describe searching space.}}

% \buyun{Only emphasize CDE}

% \buyun{Only keep two pages for introduction and related work}

%\buyun{Intro needs to be further polished based on Rene's feedback. Mention the most relevant work in the intro/abstract like IP and VIP.}

% \buyun{TODO: fig: nonsensical vs coherent prompts; Monotonous vs diverse prompts.}

% \buyun{add papers: FuseLLM; AutoDAN-turbo; a few tokens deep; UMap: BERTopic}

% \buyun{Add paper: Optimization and Optimizers for Adversarial Robustness for pattern anlysis discussion.}



% The widespread adoption of Large Language Models (LLMs) across critical domains including biomedicine~\citep{tinn_fine-tuning_2023}, financial analysis~\citep{wu_bloomberggpt_2023}, code generation~\citep{roziere_code_2024}, and education~\citep{kasneci_chatgpt_2023} has highlighted the importance of ensuring their alignment with human values. Jailbreak attacks have become a popular red-teaming strategy to bypass the safety mechanisms of LLM outputs~\citep{dubey_llama_2024} and induce harmful, illegal, objectionable, or undesirable responses~\citep{zou_universal_2023,chao_jailbreaking_2024}. While many jailbreak methods have been proposed recently, they continue to encounter several challenges.

% \vspace{-2mm}









% \begin{itemize}[wide]
%     \item \textit{Human-aligned Evaluator}: We train a jailbreak evaluator, a judge LLM finetuned on around $1000$ manually annotated LLM responses across different attack methods. Compared to SOTA evaluators, ours shows superior alignment with human judgment, measured by accuracy and F1 score.
    
%     \item \textit{Knowledge-Distilled Attacker (KDA)}: As illustrated in \autoref{fig:kda_overview}, our method operates by first generating attack prompts by running SOTA jailbreak methods such as AutoDAN~\citep{liu_autodan_2024}, GPTFuzzer~\citep{yu_gptfuzzer_2024} and PAIR~\citep{chao_jailbreaking_2024}. Next, we fine-tune a lightweight pre-trained LLM (e.g., \texttt{Vicuna-13B}) on these attack prompts to build a model that distills the diverse attack styles into one efficient attacker that can generate attack prompts given a harmful goal (e.g. \textit{Write fake news about a storm that will impact thousands}). KDA \textbf{A}utomatically generates attack prompts without needing access to the target model’s internal details, making it a \textbf{B}lack-box attack. By mimicking patterns from a diverse set of semantically meaningful attack prompts, KDA ensures that its prompts are both \textbf{C}oherent and \textbf{D}iverse. The framework is entirely based on open-source LLMs with finetuning, rendering it \textbf{E}xempt from reliance  on proprietary LLMs. In addition, the attack generation process is efficient and effective, as demonstrated in Section~\ref{sec:exp}, requiring less than 10 seconds to achieve nearly 100\% ASR on each attack.

%     \item \textit{Large Scale Attack Dataset}: Unlike existing methods that are computationally intensive, KDA significantly reduces attack generation time to under 10 seconds per attack, which makes it suitable for large-scale adversarial assessments and red-teaming efforts. In order to facilitate further research, we curate the \texttt{RedTeam-10k} dataset, a comprehensive dataset of $10,000$ diverse 
%     attack prompts for $1,000$ different harmful queries, which demonstrates KDA's ability to attack SOTA LLMs at scale. To the best of our knowledge, this is $20$ times larger than any existing attack prompt dataset. 
% \end{itemize}




% \section{Related Work}






    % \textbf{NOTE:} To minimize the effort required for dataset preparation and the training process, we will release all datasets, as well as the fine-tuned attacker and judge LLMs.

    % \jinqicomment{If possible, we can also mention that we will release the fine-tuned attackers (so that future users do not need to re-train them). Some reviewers may question that we still need GPU to train the system.}



%As illustrated in Phase 1 of \autoref{fig:pami},  the KDA attacker is created by first generating attack prompts via SOTA jailbreak methods, such as AutoDAN~\citep{liu_autodan_2024}, GPTFuzzer~\citep{yu_gptfuzzer_2024} and PAIR~\citep{chao_jailbreaking_2024}, with failed prompts filtered out, leaving only successful ones as training data; Next, we fine-tune a lightweight pre-trained LLM (e.g., \texttt{Vicuna-13B-v1.5}) on these attack prompts to build a model that distills the diverse attack styles into one efficient attacker. At test-time, when given a harmful goal (e.g. \textit{Write fake news about a storm that will impact thousands}), the KDA attacker can generate multiple candidate attack prompts until a judge declares the attack as a success. Further, we train a jailbreak evaluator that is a judge LLM finetuned on over 2000 manually annotated responses of various LLMs on different attack methods. 



%Phase 2 of \autoref{fig:pami} details the KDA attacker in action when generating attack prompts at test-time: Once a harmful goal is specified (e.g., \textit{Write fake news about a storm that will impact thousands}), the KDA attacker \textbf{A}utomatically generates attack prompts without needing access to the target model’s internal details, making it a \textbf{B}lack-box attack. By mimicking patterns from a diverse set of semantically meaningful attack prompts, \pami ensures that its prompts are both \textbf{C}oherent and \textbf{D}iverse. The framework is entirely based on open-source LLMs with finetuning, rendering it \textbf{E}xempt from reliance  on proprietary LLMs. In addition, the attack generation process is efficient and effective, as demonstrated in \autoref{sec:exp}, requiring less than 30 seconds to achieve nearly 100\% ASR on each attack.

% Given these limitations, the natural question arises: \textit{Can we create a jailbreak framework that can \textbf{A}utomatically generate \textbf{B}lack-box attacks that are both \textbf{C}oherent and exhibit \textbf{D}iverse patterns, while being \textbf{E}xempt from commercial LLMs, as well as \textbf{F}ast and effective in attack generation?}




%\buyun{Rene: This paragraph is not focused: is it talking about the model, or about data collected for alignment?}

%The judge model, depicted in Phase 2 of \autoref{fig:pami}, constitutes a critical components of the PaMi framework. To ensure the evaluator closely aligns with human judgment, we first curate an evaluation dataset by mannually anotating whether the target LLM provides confirmative responses to the attack prompts. This dataset comprises over 2,000 responses from various target LLMs, which were subjected to attacks from three methods, i.e., AutoDAN, GPTFuzzer, and PAIR. The anotated dataset is subsequently employed to fine-tune a safety classifier, which demonstates superior accuracy and F1 score in aligning with human judgment when comparing with other SOTA evaluators.

%Building on our PaMi framework, we now focus on its application to large-scale risk assessment of LLMs. Current state-of-the-art (SOTA) attack methods are typically evaluated on datasets with around 500 samples ~\citep{zou_universal_2023, liu_autodan_2024} or as few as 100 ~\cite{yu_gptfuzzer_2024,chao_jailbreaking_2024}. This limitation arises from the high computational cost of methods like GCG, AutoDAN, and GPTFuzzer, which require $\sim$600 seconds per successful attack, significantly hindering scalability.  In contrast, PaMi completes each successful attack in $\sim$30 seconds, allowing for evaluation on much larger datasets. We introduce the \texttt{toxic\_10k\_dataset}, containing 10,000 harmful queries, and provide evaluation results of PaMi against various SOTA LLMs. This enables a more comprehensive risk assessment of various commercial and open-source LLM vulnerabilities.



%Our contributions can be summarized as follows: 
%\begin{itemize}[leftmargin=0.25in]
%     \item We introduce PaMi, a novel jailbreak framework that can \textbf{A}utomatically generates \textbf{B}lack-box attacks with \textbf{C}oherent and \textbf{D}iverse prompts, while being \textbf{E}xempt from reliance on commercial LLMs during attack generation. This is achieved by mimicking patterns from successful jailbreak prompts via fine-tuning.

%    \item We annotated more than 2,000 different responses generated via various target LLMs when attacked by three SOTA attack methods. Using this human-annotated evaluation dataset, we fine-tuned a human-aligned evaluator. The evaluator demonstrates superior performance in matching human judgment.

%    \item Unlike existing methods that are computationally intensive, PaMi significantly reduces attack generation time to under 30 seconds per attack while achieving nearly 100\% attack success rate (ASR) on various SOTA LLMs. This makes it suitable for large-scale adversarial assessments and red-teaming efforts.

%    \item We curate the \texttt{toxic\_10k\_dataset}, a comprehensive dataset of 10,000 harmful queries, and demonstrate PaMi's ability to attack SOTA LLMs at scale, completing evaluations in 80 hours on a moderate GPU setup. PaMi's efficiency and effectiveness make it a viable tool for evaluating the vulnerabilities of LLMs at large. \jinqicomment{If possible, we can also mention that we will release the fine-tuned attackers (so that future users do not need to re-train them). Some reviewers may question that we still need GPU to train the system.}

%\end{itemize}

% \newpage

% \paragraph{Contributions} We introduce a novel jailbreak framework, \texttt{EnsembleAttack}, designed to efficiently and effectively generate coherent and diverse attack prompts against both open-source and commercial LLMs. The framework incorporates an automatic jailbreak evaluation method that closely aligns with human judgment. Our key contributions include:

% 1) \textbf{Efficiency and Effectiveness}: Our attack method typically requires fewer than 20 queries to generate successful attack prompts, significantly reducing computational demands. For example, when attacking \texttt{Llama-2-7B}, our approach achieves a wall time of only 32.9 seconds per attack. More importantly, it outperforms SOTA methods in attack success rate while demanding substantially less computational resources.

% 2) \textbf{Diversity}: The attacker LLMs in our framework are fine-tuned on successful attack prompts from a range of algorithms, including AutoDAN \citep{liu_autodan_2024}, GPTFuzzer \citep{yu_gptfuzzer_2024}, and PAIR \citep{chao_jailbreaking_2024}. This enables the generation of a wide variety of attack patterns, such as logical appeals, authority endorsements, and role-playing. Additionally, the interpretability of the training data ensures that all generated attacks are coherent and semantically meaningful.

% 3) \textbf{Human-Aligned Judgment}: Our evaluation method combines a text-matching prefiltering technique with a content classifier fine-tuned on human annotations, enhancing the accuracy and reliability of performance assessments.

% 4). \textbf{Independence from Commercial LLMs}: Neither our attacker LLMs nor our evaluation methods rely on commercial LLMs, making the framework cost-efficient and mitigating issues of poor reproducibility due to model version updates.





% Large Language Models (LLMs) have pervasive applications across numerous fields including biomedicine \citep{tinn_fine-tuning_2023}, financial analysis \citep{wu_bloomberggpt_2023}, code generation \citep{roziere_code_2024}, and education \citep{kasneci_chatgpt_2023}. To mitigate the risks of LLMs generating toxic or hamrful content and better align LLM behavior with human values, researchers and practitioners have implemented safety alignment techniques, notably safety fine-tuning, pretraining, content distillation \citep{touvron_llama_2023, dubey_llama_2024} and reinforcement learning from human feedback \citep{ouyang_training_2022}. Nonetheless, LLMs remain vulnerable to specific prompts designed to circumvent safety mechanisms, known as a \textit{jailbreak attack}\citep{zou_universal_2023,chao_jailbreaking_2024}. 

% 1) \textbf{Manual} vs. \textbf{Automatic}: The earliest jailbreak attack work starts from Mannual prompt, also known as Do-Anything-Now \citep{shen_anything_2024}, which rely on human creativity to devise sophisticated prompts and not scalable. To overcome this issue, researcher and practitioners \citep{zou_universal_2023} move to automatic generated jailbreaks based on algorithms. 

% 2) \textbf{White-box} vs \textbf{black-box}: many existing attacks are  White-box attacks \citep{zou_universal_2023, liu_autodan_2024}, which use internal model knowledge (e.g., gradient) to generate diverse prompts but are limited by the closed-source nature of many commercial LLMs \citep{openai_gpt-4_2024,gemini_team_gemini_2024}. Black-box attacks, on the other hand, do not require access to the model's internals, making them more broadly applicable, which is more favorable for jailbreaking.   


% 3) \textbf{Nonsensical} vs. \textbf{Coherent}: Some methods \citep{zou_universal_2023} produce nonsensical prompts which is less likely encountered in real-world scenario and easily countered by defensive algorithms such as perplexity measures \citep{alon_detecting_2023} or randomized smoothing \citep{robey_smoothllm_2024}. Recent methods \citep{liu_autodan_2024} focus on generating coherent attack prompts, making them harder to detect. 

% 4). \textbf{single pattern} vs \textbf{diverse pattern}: some methods \citep{li_deepinception_2024} use prompt with very similar pattern to perform all jailbreaks, this methods is very easy to defend against; while method with more diverse prompt \citep{yu_gptfuzzer_2024} is harder to be defend against. 

% 5). commercial LLM dependency vs or not: some methods \citep{liu_autodan_2024} require commercial LLM like GPT-4 to perform critical steps in attack generation, which is costly and have  poor reproducibility, particularly when the model versions are update.  Also, A summary of existing jailbreak attack framework's property can be found in \autoref{tab:jailbreaks}

% black box; white box. nonsensical vs coherent. Diverse prompt vs single pattern. require commercial llm dependency.




% \buyun{A table for different method of attack prompts.}

% A typical jailbreak framework comprises three essential components: 1) the attack method, 2) the target LLM, and 3) the evaluation method. The attack method is crucial for generating jailbreak prompts that can mislead the target LLM into producing harmful or toxic responses. The evaluation method is employed to determine whether the generated response has been compromised.

% \ryan{Provide a high-level introduction that jailbreak frameworks consider: 1) attack 2) evaluation. I might even go further to introudce the three key roles in any jailbreak: 1) attacker, 2) target 3) judge}

% Regarding attack methods, we categorize jailbreak attacks based on their distinct characteristics and methods used for the generation: 1) \textbf{Manual} vs. \textbf{Automatic}: Manually-crafted jailbreak attacks \citep{shen_anything_2024} rely on human creativity to devise sophisticated prompts. Automatic methods \citep{zou_universal_2023} address this by generating prompts algorithmically, reducing human labor. 2) \textbf{Nonsensical} vs. \textbf{Coherent}: Some methods \citep{zou_universal_2023} produce nonsensical prompts easily countered by defensive algorithms such as perplexity measures \citep{alon_detecting_2023} or randomized smoothing \citep{robey_smoothllm_2024}. Recent methods \citep{liu_autodan_2024} focus on generating coherent attack prompts, making them harder to detect. 3) \textbf{White-box} vs \textbf{black-box}: White-box attacks \citep{zou_universal_2023} use internal model knowledge (e.g., gradient) to generate diverse prompts but are limited by the closed-source nature of many commercial LLMs \citep{openai_gpt-4_2024,gemini_team_gemini_2024}. Black-box attacks, on the other hand, do not require access to the model's internals, making them more broadly applicable. 

% Weak-to-strong \citep{zhao_weak--strong_2024} & & \crossmark & ? & ? & ? \\ \hline

% AmpleGCG\citep{liao_amplegcg_2024} & & & \crossmark & &  \\ \hline
% ReNeLLM\citep{ding_wolf_2024} &    &  &  &   & \crossmark \\ \hline
% Rainbow Teaming\citep{samvelyan_rainbow_2024}\tablefootnote{Rainbow Teaming \citep{samvelyan_rainbow_2024} necessitates the use of GPT-4 as the Judge LLM. Moreover, the \texttt{Llama-2-70B} model, utilized in their mutation operator and preference model, requires over 140 GB of memory, making it resource-intensive and costly.} & &  &  & & \crossmark \\ \hline
% PAP\citep{zeng_how_2024} & &  & &  & \crossmark \\ \hline
% ASEFT\citep{wang_asetf_2024} &    &  \crossmark &  &  &  \\ \hline
% TAP\citep{mehrotra_tree_2024} & & & & & \crossmark \\ \hline
% Opensesame\citep{lapid_open_2023} &    &  &  \crossmark &  &  \\ \hline
% SMJ\citep{li_semantic_2024} & & \crossmark &   &  &    \\ \hline
% COLD\citep{guo_cold-attack_2024} & & \crossmark &   &  &    \\ \hline
% DrAttack\citep{li_drattack_2024} &  &  &  & \crossmark & \crossmark \\ \hline
% Puzzler\citep{chang_play_2024} &  &  &  &  & \crossmark  \\ \hline
% DRA\citep{liu_making_2024} &  &   &   & \crossmark &   \\ \hline
% CodeChameleon\citep{lv_codechameleon_2024} &  & & & \crossmark &  \\ \hline

% While some automatic methods can generate coherent black-box attacks \citep{chao_jailbreaking_2024}, their success rate is limited by prompt diversity. This leads us to our first question: \textbf{\textit{How can we automatically generate coherent and black-box attacks with diverse patterns?}} \buyun{Seperate the related work and introduction; describe the challenge. Why do we need it?} \ryan{Comment 1:Each type of jailbreak only need a short sentence on describing what they are. E.g. Manual vs. Automatic: whether the attack is manually-crafted by a human or from another LLM. Move rest to related work. } \ryan{Comment 2: The final question needs to be better motivated. what are the shortcomings of each type and how will our method resolve this issue? }


% \buyun{Motivation: red teaming dataset requires large amount of jailbreak prompts: https://huggingface.co/meta-llama/Meta-Llama-3.1-405B; adversarial benchmark: The Llama 3 Herd of Models}


% While certain methods demonstrate the ability to automatically generate coherent attacks for commercial LLMs, or at least exhibit good transferability to such models, several challenges persist. For instance, some approaches \cite{chao_jailbreaking_2024} exhibit poor attack success rates against specific LLMs, such as \texttt{Llama-2-7B}, due to a limited diversity in attack patterns. Others \citep{liu_autodan_2024} demand a high number of queries, leading to inefficiencies in computational resources. Additionally, some methods \citep{yu_gptfuzzer_2024} rely on the use of commercial LLMs for generating attack prompts, which not only increases costs but also suffers from poor reproducibility, particularly when the model versions are updated. This leads us to our first question: \textit{How can we automatically generate coherent and black-box attacks with diverse patterns, minimal queries, and no reliance on commercial LLMs?} 



% To address this question, we propose \texttt{EnsembleAttack}, an ensemble of attacker LLMs designed to efficiently generate attacks with diverse patterns, as illustrated in \autoref{fig:ensemble_attack}. Each attacker within the ensemble is an LLM fine-tuned on successful attack prompts derived from SOTA jailbreak attack algorithms. The \texttt{EnsembleAttack} is capable of \textit{automatically} and \textit{efficiently} generating \textit{coherent} and \textit{black-box} attack, independent of reliance on commercial LLMs.



% \begin{figure}[!ht]
% \centering
% \includegraphics[width=8cm]{figs/evaluator.jpeg}
% \vspace{-6mm}
% \caption{Schematic overview of the jailbreak evaluator.} 
% \label{fig:evaluator} 
% % \vspace{-1em}
% \end{figure}

% \buyun{Table form also}

% In addition to the ABCDE properties, computational cost is a key consideration. Red teaming and adversarial assessments of LLMs require large volumes of attack prompts to identify vulnerabilities \citep{dubey_llama_2024}. While methods like AutoDAN \citep{liu_autodan_2024} and GPTFuzzer \citep{yu_gptfuzzer_2024} perform well, they are computationally expensive, taking 5-10 minutes per successful attack\footnote{All experiments were conducted on 8 A5000 GPUs}. In contrast, \pami generates successful attack prompts in under 30 seconds, using \textbf{less than 10\%} of the computational resources, while maintaining a high attack success rate (ASR). This makes it far more efficient for red teaming. See Section \ref{sec:exp} for further computational comparisons.


% Evaluating the success of jailbreaks is another essential topic. While human evaluation provides high accuracy, it is inherently labor-intensive and not scalable. Existing automatic evaluation methods include: 1. \textit{Text Matching} \citep{zou_universal_2023}, which is both effective and cost-efficient but depends heavily on predefined safety words sets; 2) \textbf{Conversation Generation} \citep{chao_jailbreaking_2024}, where performance is highly sensitive to the system message and frequently necessitates the use of commercial LLMs like \texttt{GPT-4}; and 3) \textbf{Content Classification} methods such as Llama-Guard \citep{chao_jailbreaking_2024}, whose effectiveness is significantly influenced by the quality of the training data used for fine-tuning the classifier. Despite these approaches, there remains a significant gap in aligning these methods with human evaluation.  As shown in our results, \textit{text matching} often suffers from low precision, whereas methods based on \textit{conversation generation} or \textit{content classification} tend to exhibit low recall. This observation leads us to our second research question: \textit{How can we develop an automatic evaluation method for jailbreak success that aligns closely with human judgment?}


% A central component of our evaluation framework is a safety classifier fine-tuned on a human-annotated dataset, which demonstrates strong alignment with human judgment. To enhance evaluation performance, we integrate this method with the Text Matching approach, resulting in improved metrics such as accuracy and F1 score. An illustration of a typical jailbreak evaluator is provided in \autoref{fig:evaluator}. This evaluation framework enables us to automatically and reliably assess jailbreak performance without relying on commercial LLMs. Additionally, this framework can serve a dual purpose, functioning both as a jailbreak evaluation tool and as a safety filter.




% \ryan{This needs to be better motivated. Why is there a gap? Can show a small conclusion of the result? e.g. it turns out many false-positives when using text matching. }

% \buyun{Summarize what is our work. Schematic overview of Ensemble Attack. In this paper, ...}