\section{Preliminary}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{Image/ff.pdf}
    \caption{We demonstrate our hypothesis by using three models (MLPs, Fourier features with one linear layer and their combination (i.e. MLPs with Fourier features embeddings)) to fit two kinds of functions. The result demonstrate that combining MLPs with Fourier Features can actually combine their representation capability. These highlighted red boxes demonstrate that MLPs with Fourier features also involve the representation capability of the Fourier features where there are high-frequency fluctuations in the flat regions due to the non-differentiable point in spike function.}
    \label{fig:fourier_series} 
\end{figure*}
\subsection{Fourier Features Embeddings}
\label{sec:FF}
Fourier features embeddings are common embedding methods to alleviate spectral bias. As a type of embedding that maps inputs into the frequency domain, they can be expressed by the function $\gamma(\mathbf{v}): \mathbb{R}^d\rightarrow\mathbb{R}^N$, where \(d\) is the input coordinate dimension and \(N\) is the embedding dimension. The two most common types are Random Fourier Features (RFF) and Positional Encoding (PE), which can both be represented by a single formula with slight variations in their implementation.
\begin{definition}
[$\mathbf{Fourier\, features}$] $\mathbf{Fourier\, features}$ can be generally defined as a function such that $\gamma(\mathbf{v}): \mathbb{R}^d\rightarrow\mathbb{R}^N$
\begin{equation}
\begin{split}
\gamma(\mathbf{v}) = &[sin(2\pi\mathbf{b}_i^{\top}\mathbf{v}), cos(2\pi \mathbf{b}_i^{\top}\mathbf{v})]_{i\in[N]}\\
&[N] = \{1, 2, 3,\cdots, N\},\,\mathbf{b}_i \in\mathbb{R}^{d\times 1}\\
\end{split}
\end{equation}
\,\,\,\,\,\,\,$\mathbf{Positional\, Encoding}$: $\mathbf{b}_i=\mathbf{s}^{\frac{i}{N}}, \, for\, i\in[N]$. It applies log-linearly spaced frequencies for each dimension, with the scale $\mathbf{s}$ and size of embedding \(N\) as hyperparameters, and includes only on-axis frequencies.

\,\,\,\,\,\,\,$\mathbf{Random\, Fourier\, Features}$: $\mathbf{b}_i\sim\mathcal{N}(0, \Sigma)$. Typically, this is an isotropic Gaussian distribution, meaning that $\Sigma$ has only diagonal entries. Other distributions, such as the Uniform distribution, can also be used, though the Gaussian distribution remains the most common choice.
\label{def:ff}
\end{definition}
\subsection{Bias-free ReLU MLPs}
\label{section:BF}
For tasks with identical input and output dimensions, additive term-free ReLU MLPs (also referred to as bias-free MLPs) can, in contrast to standard MLPs, be regarded as locally strictly linear operators (Lemma~\ref{lemma:BF1}) with a scale-invariant property (Lemma~\ref{lemma:BF2}), as demonstrated in the following lemmas.

\begin{lemma}
    (\citet{mohan2019robust}) For a Bias-free ReLU activation function ($\sigma(\cdot)$) MLPs $f_{BF}(\cdot): \mathbb{R}^N\rightarrow\mathbb{R}^N$ with L layers, matrix at each layer is denoted as $W^l$ for $l=1,\cdots,L$. Then, the MLPs can be written as $f_{BF}(\mathbf{x})=\mathbf{A}_\mathbf{x}\mathbf{x}$.
\label{lemma:BF1}
\end{lemma}
% \begin{proof}
% \begin{equation*}
% \begin{split}
% &f_{BF}(\mathbf{x}) = W_L\sigma(W_{L-1}(\sigma(\cdots\sigma(W_1\mathbf{x}))))\\
%              &\text{since $\sigma(x)=x\,\,if\, x > 0$ and $\sigma(x)=0\,\,if\, x \leq 0\,$}\\
%              &f_{BF}(\mathbf{x}) = W_L\sigma(W_{L-1}(\sigma(\cdots\sigma(W_1\mathbf{x}))))=\mathbf{A}_\mathbf{x}\mathbf{x}\\
%              &\text{where $\mathbf{A}_\mathbf{x}\in\mathbb{R}^{N\times N}$ is the Jacobian matrix evaluated at $\mathbf{x}$}
% \end{split}
% \end{equation*}
% \end{proof}

\begin{lemma}
(\citet{mohan2019robust}) For a Bias-free ReLU activation function ($\sigma(\cdot)$) MLPs $f_{BF}(\cdot): \mathbb{R}^N\rightarrow\mathbb{R}^N$ with L layers, matrix at each layer is denoted as $W^l$ for $l=1,\cdots,L$.  For any input $\mathbf{x}$ and any nonnegative constant $\alpha$,
\begin{equation}
f_{BF}(\alpha \mathbf{x}) = \alpha f_{BF}(\mathbf{x}).
\end{equation}
\label{lemma:BF2}
\vspace{-15pt}
\end{lemma}

% \begin{proof}
% To prove the result, we need to firstly proof the linearity of the activation function $\sigma(\cdot)$. Since $\sigma(x)=x\,\,if\, x > 0$ and $\sigma(x)=0\,\,if\, x \leq 0\,$, we can easily have $\sigma(\alpha x)=\alpha x\,\,if\, x > 0$ and $\sigma(\alpha x)=0\,\,if\, x \leq 0\,$, which then implies:
% \begin{equation*}
% \begin{split}
%     &f_{BF}(\alpha \mathbf{x}) = W_L\sigma(W_{L-1}(\sigma(\cdots\sigma(\alpha W_1\mathbf{x}))))\\
%     &= \alpha W_L\sigma(W_{L-1}(\sigma(\cdots\sigma(W_1\mathbf{x})))) = \alpha f_{BF}(\mathbf{x}).
% \end{split}
% \end{equation*}
% \end{proof}

To connect these properties with our method, intuitively, scaling the input by a constant should preserve the input's frequency. Since the goal for MLPs is to filter based on frequency pattern rather than amplitude of a signal for a singe coordinate, the scale-invariant property ensures that scaling does not affect the result. By incorporating an additive term in the MLPs, the network function is modified to \( f_{BF}(\mathbf{x}) = \mathbf{A}_\mathbf{x} \mathbf{x} + \mathbf{b}_\mathbf{x} \), can be shown in a similar proof to Lemma \ref{lemma:BF1}. However, this adjustment disrupts the scale-invariant property and alters the activation pattern, which is undesirable.
