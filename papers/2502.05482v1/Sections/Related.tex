\section{Related works}
Implicit Neural Representations are designed to learn continuous representations of target functions by taking advantages of the approximation power of neural networks.
%
Their inherent continuous property can beneficial in many cases like video compression~\citep{chen2021nerv,strumpler2022implicit}, 3D modeling~\citep{park2019deepsdf,atzmon2020sal,9010266,gropp2020implicit,sitzmann2019scene} and volume rendering~\citep{pumarola2021d, barron2021mip,martin2021nerf,barron2023zip}.
%
However, simply employing MLPs may result in spectral bias, where oversmoothed outputs are generated due to the inherent tendency of MLPs to prioritize learning low-frequency components first. Consequently, many studies have focused on these drawbacks and explored various methods to address this issue.
%
The most straightforward way to address this issue is by projecting the coordinates into the higher dimension~\citep{tancik2020fourier, wang2021spline}.
%
However, these methods can lead to noisy outputs if there is a mismatch in the embeddings variance.
%
To address this, \citet{landgraf2022pins} propose dividing the Random Fourier Features into multiple levels of detail, allowing the MLPs to disregard unnecessary high-frequency components. Another type of approach to mitigating the spectral bias introduced by the ReLU activation function, as proposed by \citet{sitzmann2020implicit}, \citet{ramasinghe2022beyond}, \citet{saragadam2023wire}, and \citet{shenouda2024relus}, is to modify the activation function itself by using alternatives such as the Sine function, Wavelets, or a combination of ReLU with other functions. There are also efforts to modify network structures to mitigate spectral bias~\citep{mujkanovic2024neural}. 
%
\citet{lindell2022bacon} introduce a network design that treats MLPs as filters applied to the input of the next layer, known as Multiplicative Filter Networks (MFNs). 
%
Additionally, based on the discrete nature of signals like images and videos, grid-based approaches (e.g., Grid Tangent Kernel~\citep{zhao2024grounding}, DINER~\citep{xie2023diner}, and Fourier Filter Bank~\citep{wu2023neural}) have been proposed to address spectral bias, as the grid property allows for sharp changes in features, which facilitates learning fine details.
Even though, there are some prior works trying to solve the inherent problems of Fourier features embeddings ~\citep{landgraf2022pins, yuce2022structured, hertz2021sape, saratchandran2024sampling}, limited research has addressed both the underlying causes of high-frequency noise and provides a non-heuristic solution even if these embeddings are widely employed into many downstream tasks.