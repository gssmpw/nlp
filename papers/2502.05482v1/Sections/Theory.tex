\section{Theoretical Analysis of Fourier Features}
In this section, we examine why Fourier features can introduce high-frequency noise, from the persepctive of the Neural Tangent Kernel (NTK) derived from two-layer MLPs with a frozen second layer for simplicity as in \citet{arora2019fine}. Our experiments in \autoref{figure:combined} and \autoref{fig:varinace} confirm that the conclusion also stands in multi-layer MLPs.
%
This analysis also helps explain why Positional Encoding might be more stable than Random Fourier Features in certain cases. By decomposing the target function as its Fourier series, we observe that MLPs primarily learn the given frequency components globally, leaving high-frequency components remain in smoother regions. Proofs for all theorems can be found in Appendix \ref{section:proof}.

The two following theorems, based on NTK (please check Appendix \ref{sectionï¼šNTK} for the detailed formula), demonstrate that two-layer MLPs incorporated with Fourier Features essentially fit the target function by leveraging sampled frequencies and their combinations.
%
If the sampled frequencies are integers, the unsampled frequencies impose a lower bound on the minimum achievable loss, meaning that finite sampling introduces noise primarily driven by these unsampled frequencies.
\newtheorem{thm:1}{Theorem}
\begin{thm:1}
    For a two-layer Multilayer-perceptrons (MLPs) denoted as $f(\mathbf{x};\mathbf{W})$, where $\mathbf{x}\in\mathbb{R}^d$ as input and $\mathbf{W}$ as the parameters of the MLPs. 
    %
    Then the order-N approximation of eigenvectors of the Neural Tangent Kernel (Eq.\ref{Eq:NTK}) when using Fourier features embedding, as defined in Def.\ref{def:ff}, to project the input to the frequency space can be presented as,
    \begin{equation}
        k(\gamma(\mathbf{x}),\gamma(\mathbf{z})) = \sum^{N^{\dag}}_{i=1} \lambda^2_i cos(\mathbf{b}^*\mathbf{x})cos(\mathbf{b}^*\mathbf{z}) + \sum^{N^{\dag}}_{i=1} \lambda^2_i sin(\mathbf{b}^*\mathbf{x})sin(\mathbf{b}^*\mathbf{z})\text{, where $N^\dag\leq 4Nk^mkm^2$}
    \end{equation}
    where 
    \begin{equation}
        \mathbf{b}^*\in \mathcal{L}_{Span\{b_j\}} \equiv \left\{   \mathbf{b}^*=\sum^n_{j=1} c_j\mathbf{b}_j\,\vline\, \sum^{\infty}_{j=1}|c_j|<N+k^mkm+m  \right\}
    \end{equation}
    and $\lambda_i$s are eigenvalues for each eigenfunctions $sin(\mathbf{b}^*\mathbf{x})$ and $cos(\mathbf{b}^*\mathbf{x})$.
\end{thm:1}
\begin{figure}[!ht]
    \centering\includegraphics[page=1, width=1.\textwidth]{Image/teaser_cropped (3).pdf}
    \caption{The pipeline of our method introduces two additional modules compared to the original approach. The first module, an adaptive linear filter, removes unnecessary frequency components at the pixel level, reducing high-frequency noise during regression. The second module dynamically adjusts the learning rate during training to optimize the approximated loss for the next step, achieving Pareto efficiency. Together, these modules result in cleaner and more detailed images.}
    \label{fig:T}
   \vspace{-15pt} 
\end{figure}
\begin{thm:1}
   For a d-dimensional target function $\mathbf{y}(\mathbf{x}) = \sum_{\mathbf{n}\in\mathbb{Z}^d}\hat{y}_{\mathbf{n}}e^{i\mathbf{n}^{\top}\mathbf{x}}$, where $\hat{y}_{\mathbf{n}}$ are the corresponding coefficients of the Fourier series expansion of the $\mathbf{y}(\mathbf{x})$. Given a pre-sampled frequency set $\mathbf{B}_n = \{\mathbf{b}_i\in\mathbb{Z}^d\}_{i\in[N]}$ and the $L_2$ loss function as $\phi(\mathbf{y}, f(\mathbf{x};\mathbf{W})) = ||f(\mathbf{x};\mathbf{W})-\mathbf{y}||_2$.Let the projection of $\mathbf{y}(\mathbf{x})$ onto the spanned space of frequency set $\mathbf{B}_n$ be denoted by $\mathbf{y}_\mathbf{B}$ and the projection onto the orthogonal complement of this spanned space by $\mathbf{y}^{\dag}_\mathbf{B}$ such that $\mathbf{y} =\mathbf{y}^{\dag}_\mathbf{B}+\mathbf{y}_\mathbf{B} $. Then, with probability at least $1-\delta$, for all $k=0,1,2,\cdots$ (iteration numbers), the lower bound of the loss function can be represented as:
    \begin{equation}
        ||\mathbf{y}^{\dag}_\mathbf{B}||_2 -  \sqrt{\sum(1-\eta\lambda_i)^{2k}\langle\mathbf{v}_i,\,\mathbf{y}_\mathbf{B}\rangle^2}\pm\epsilon\, \leq\, \phi(\mathbf{y}, f(\mathbf{x};\mathbf{W}))
    \end{equation}
\end{thm:1}
To extend the analysis, we also examine the continuous frequencies sampled from $\mathbb{R}^d$. 
%
The next result demonstrates that the decay rate for integer frequencies close to the eigenfunctions is larger, aligning with and extending Theorem 2.
\begin{thm:1}
    For a d-dimensional target function $\mathbf{y}(\mathbf{x}) = \sum_{\mathbf{n}\in\mathbb{Z}^d}\hat{\mathbf{y}}_{\mathbf{n}}e^{i\mathbf{n}^{\top}\mathbf{x}}$, where $\hat{\mathbf{y}}_{\mathbf{n}}$ are the corresponding coefficients of the Fourier series expansion of the $\mathbf{y}(\mathbf{x})$. 
    %
    Given a pre-sampled frequency set $\mathbf{B}_n = \{\mathbf{b}_i\in\mathbb{R}^d\}_{i\in[N]}$ and the $L_2$ loss function as $\phi(\mathbf{y}, f(\mathbf{x};\mathbf{W})) = ||f(\mathbf{x};\mathbf{W})-\mathbf{y}||_2$. 
    %
    Then, for the frequency component $\mathbf{n}\in\mathbb{Z}^d$, and a sampled frequency $\mathbf{b}\in\mathbf{B}_n$ and its decomposition into the integer, $\mathbf{b}_z$, and residual part, $\mathbf{b}_r\in [0,1) $, the decreasing rate of the loss function for specific frequency $\mathbf{n}$ from the target function using two-layers MLPs with second layer frozen is $\mathcal{O}( \frac{1}{\prod_i[(\mathbf{n} + \mathbf{b}_z)+\mathbf{b} _r]_i}+ \frac{1}{\prod_i[(\mathbf{n} - \mathbf{b}_z)-\mathbf{b}_r]_i})$.
\end{thm:1}
The last lemma explains why Positional Encoding is more stable than Random Fourier Features in 2D case. 
%
Positional Encoding struggles with tilted high-frequency components, while Random Fourier Features, due to their high variance, mix high- and low-frequency signals, making it similarly difficult to capture low-frequency components.
%
Intuitively, with Random Fourier Features, if the closest sampled frequency to a target's low-frequency component contains high-frequency elements, the noise gets introduced into the fitting result.
\begin{lemma}
    Considering two different Fourier features, Positional Encoding, and Random Fourier Features as in Def.~\ref{def:ff}.
    %
    For two sampled frequencies using two embedding, $\mathbf{b}_{pe}$ from Positional Encoding and $\mathbf{b}_{rff}$ from Random Fourier Features, assume $\mathbf{b}_{rff}$ has two components with $[\mathbf{b}_{rff}]_1 \gg [\mathbf{b}_{rff}]_2$, and $\mathbf{b}_{pe}$ has only one non-zero component,  $[\mathbf{b}_{pe}]_2$, equal to $[\mathbf{b}_{rff}]_2$. 
    %
    Let $\mathbf{b}_z$ be the closest integer frequency to $\mathbf{b}_{rff}$ and $[\mathbf{b}_{rff}]_2$ = $[\mathbf{b}_z]_2$.
    %
    Then the decay rate of $\mathbf{b}_z$ for Positional Encoding, $\mathbf{b}_{pe}$, is equal to $[\mathbf{b}_z]_2$ for Random Fourier Features.
\end{lemma}
