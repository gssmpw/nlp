\section{Analysis and Motivations}
\begin{figure*}[!ht]
    \centering\includegraphics[page=1, width=1\textwidth]{Image/teaser_cropped.pdf}
    \caption{The pipeline of our method introduces two additional modules compared to the original approach. The first module, an adaptive linear filter, removes unnecessary frequency components at the pixel level, reducing high-frequency noise during regression. The second module dynamically adjusts the learning rate during training to optimize the approximated loss for the next step, achieving dynamical balance. Together, these modules result in cleaner and more detailed images.}
    \label{fig:T}
   \vspace{-10pt} 
\end{figure*}
It is widely known that the ReLU-activated function can form a spike function as shown in \autoref{fig:fourier_series} using two neurons in the hidden layer. This spike function forms a Shauder basis for the $\mathcal{C}[0,1]$ space, which means that that every continuous function can be uniquely decompose into a linear combination of countable infinitely many spike functions. In contrast, using Fourier features embeddings with a single linear layer can be recognized as the Fourier series to fit the target function which can also be considered as a Shauder basis in [0, 1] (since the function can be periodic with T=1).

Therefore, using three layers of MLPs (one input layer, one hidden layer, and one output layer) requires infinitely many neurons in hidden layers to fit a sinusoidal function. Since if we consider each two neurons as a spike function times a constant and fitting a sinusoidal function using spike function requires countable infinity of those functions, this is equivalent to using infinitely many neurons to perfectly fit a sinusoidal function. On the other hand, using the sinusoidal function to fit the spike function also requires infinitely many sinusoidal functions. This is because the Fourier transform of the spike function has infinite support, and we need infinitely many delta functions (the Fourier transform of sinusoidal functions) to fit the Fourier transform of the spike function.

By the above simple analysis, to understand the behaviour of the noisy output, we propose a hypothesis that combining both MLPs and Fourier features can be considered as combining their representation capacity as shown \autoref{fig:fourier_series} (also the convergence plot in \autoref{sec:spike}). This provides an explanation for the noisy output that discontinuous or non-differentiable property (usually from change of objects or change of color) of the image will require infinity many sinusoidal functions to suppress the high-frequencies components on the continuous regions. This can also be partially proved by considering the Neural Tangent Kernel theory. If MLPs plus Fourier features embeddings can be interpreted as the linear combination of sinusoidal functions (proved by the Lemma. \ref{lemma1}), then by simple deduction based on triangular inequality and Orthogonal Decomposition Theorem, the unselected frequencies by the Fourier features embeddings can form an lower bound for the theoretical performance (proved by Lemma. \ref{lemma:1}).
To avoid noises, one idea is to expand the frequency band and suppress the high-frequencies components on the flat regions which leads to our adaptive linear filter.

%

\section{Methods}

In this section, we present our solution grounded in the previous analysis. The proposed method has two main components: (i) an adaptive linear filter that automatically adjust the input embeddings which also potentially introduces more frequencies components, and (ii) a learning-rate adjustor that uses the line-search method during back-propagation to dynamically adjust the filter's learning rate. The full pipeline is illustrated in \autoref{fig:T}.
\subsection{Bias-Free MLPs as Adaptive Linear Filter}
% Building on this analysis, MLPs can be viewed as a linear combination of eigenfunction frequencies, where MLPs utilize these frequencies as a prior to fit the Fourier series expansion of the target function.
% %
% However, since the eigenfunctions' frequencies cannot represent all \( m \in \mathbb{Z}^d \), noise may arise in the low-frequency regions. This is demonstrated in a 1-dimensional toy example (\autoref{fig:fourier_series}) that with a limited number of frequency components, the fitted function struggles to suppress high-frequency components in low-frequency regions.
% %
% Inspired by this observation, constraining the low-frequency regions to contain only low-frequency elements can significantly mitigate the issue.

% Therefore, we propose using bias-free MLPs as an adaptive band-limited coordinate-level linear filter for continuous representations as shown in \autoref{figure:alf}.
% %
% Bias-free MLPs act as a linear filter that their output matches the size of the input Fourier features embedding and is then used to perform a coordinate-wise Hadamard product to filter the embedding. 
% %
% The bias-free network is chosen for its scale-invariance~\citep{mohan2019robust}, which preserves input frequency patterns when using ReLU activation.
% %
% This ensures that if scaling the embedding by a constant, MLPs maintain the same amplitude and keep it at 0 for 0 inputs.
% %
% Additionally, its local linearity enables the network to function as an adaptive linear filter, applying different linear terms to each coordinate, to selectively attenuate unnecessary components.
% %
% Furthermore, This approach can also be extended to continuous-space tasks, such as 3D shape regression and inverse graphics, where the input of INRs is continuous rather than discrete, like image coordinates, benefiting from the continuity of MLPs. 
% %
% To verify the performance of this filter, we also visualized the filtered results in Appendix \ref{sec:visual_output}. This visualization confirms that the proposed module effectively filters high-frequency inputs, preventing noisy outputs.
In section \ref{section:BF}, we have introduced the properties of bias-free MLPs. In this section, we will further explore the advantages of using bias-free MLPs as adaptive linear filters. Let $\gamma(\mathbf{v})$ represent the Fourier feature embeddings of the input $\mathbf{v}$, and let \( f_{a}(\cdot) \) denote the adaptive linear filter, implemented as a bias-free ReLU-activated MLP. Our adaptive linear filter, applied to the input embeddings $\gamma(\mathbf{v})$, can be defined as $f_{a}(\gamma(\mathbf{v}))\otimes\gamma(\mathbf{v})$, where $\otimes$ denotes the Hadamard product. In other words, we apply \( f_{a}(\cdot) \) to the embedding to obtain a linear filter, which is then applied to the original embeddings to modify them. The filtered result, \( f_{a}(\gamma(\mathbf{v})) \otimes \gamma(\mathbf{v}) \), will subsequently serve as the final filtered embeddings to the INRs.

Moreover, these bias-free MLPs not only preserve the input pattern after the activation function but also extend the range of embeddings' frequencies, which could theoretically lower the performance bound discussed in  Lemma. \ref{lemma:1}, in contrast to using a simple mask as the linear filter. Note that \( f_{a}(\cdot) \) is bias-free and can be expressed as \( f_{a}(\mathbf{x}) = \mathbf{A}_{\mathbf{x}} \mathbf{x} \), as established in Lemma \ref{lemma:BF1}. Substituting this into \( f_{a}(\gamma(\mathbf{v})) \otimes \gamma(\mathbf{v}) \) yields \( \mathbf{A}_{\gamma(\mathbf{v})} \gamma(\mathbf{v}) \otimes \gamma(\mathbf{v}) \). 
\begin{figure}[!b]
 \vspace{-5pt}
    \centering
    \includegraphics[width=0.5\textwidth]{Image/Freq-Band.pdf}
    \vspace{-10pt}
    \caption{The black boxes highlight frequency bands with varying phases or frequencies, supporting our derivation that the adaptive linear filter can enrich the embeddings.
}
    \label{figure:fb}
   
\end{figure}
% \begin{figure*}[!ht]
%     \centering
%     \includegraphics[width=.75\textwidth]{Image/motivation_cropped.pdf}
%     \vspace{-10pt} 
%     \caption{Both high- and low-learning rates result in unsatisfactory outcomes, where high-frequency components aren't suppressed in low-frequency regions (LR in this figure stands for learning rate).}
%     \vspace{-15pt}
%     \label{figure:llr}
% \end{figure*}
For simplicity, we consider the one-dimensional case, which can easily be extended to higher dimensions by following a similar deduction. Let \( a_{ij} \) represent an entry of \( \mathbf{A}_{\gamma(\mathbf{v})} \), and \( \sin(b_i v) \) denote the \( i^{\text{th}} \) entry of \( \gamma(v) \) without loss of generality. Since the Hadamard product performs entry-wise multiplication, the output for a channel with frequency \( b_i \) is given by \( o_i = \sum_j a_{ij} \sin(b_j v) \sin(b_i v) \). Using the trigonometric identity \( \sin(b_j v) \sin(b_i v) = \frac{1}{2} (\sin((b_j + b_i) v) + \sin((b_j - b_i) v)) \), \( f_a(\cdot) \) can adjust input frequencies for the INRs through \( a_{ij} \), enabling enriched frequency choices. It can also preserve the original embeddings by setting \( a_{ij} = \frac{1}{2N \sin(b_j v)} \), where \( N \) is the embedding length, or block specific frequencies by setting \( a_{ij} = 0 \) for those entries. This simple analysis supports the implementation of the adaptive linear filter. Notably, the above derivation remains valid for MLPs with bias terms; however, they may introduce more complex, non-sinusoidal patterns (Comparison of the performance will be demonstrated in Section \ref{section:BFExp}).
 The illustration (\autoref{figure:fb}) from our experiments with mixing frequencies in a single frequency channel verifying our deduction.

\subsection{Line-searched based optimization}

During experiments, we observed that varying initial learning rates for the adaptive linear filter and INRs resulted in different performance outcomes. Balancing their learning rates is crucial: if INRs learn too quickly, the system risks local minima, hindering the filter's performance. Conversely, a high learning rate for the filter can cause excessive input fluctuations, preventing INRs from converging. Inspired by \citet{hao2021adaptive}, we aim to optimize the learning rate of the adaptive linear filter. By optimizing the loss function $f(\theta_{fa}, \theta_{MLP})$ as $\phi(lr_a) = f(\theta_{fa},  \theta_{MLP})$ during training (where $\theta_{fa}$ represents the parameters of the adaptive linear filter, $\theta_{MLP}$ represents the parameters of the INRs, $lr_a$ and $lr_I$ represent the learning rates for the adaptive filter and INR, respectively), we calculate the learning rate $lr_a$ for the adaptive linear filter at each iteration.
%
By applying the Taylor expansion of the loss function, this optimization problem can be approximated as a linear optimization problem, which can be evaluated efficiently without extensive computation. The complete algorithm and derivation are provided in the supplementary material. 


