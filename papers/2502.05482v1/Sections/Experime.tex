\section{Experiments}
To validate the proposed method, we test it across various tasks, including image regression, 3D shape regression, and inverse graphics. All experiments are performed on a single RTX 4090 GPU, using an adaptive linear filter with 3 layers, each with the same width as the number of channels in the Fourier features embedding.
\subsection{Image Regression on Kodak Dataset}
In this section, we evaluate the performance of our proposed method on the high resolution (512$\times$768 or 768$\times$512) Kodak Dataset~\citep{article}. All baselines are trained using the mean squared error (MSE) loss function.

We benchmark our approach against several state-of-the-art baselines, including Multi-Layer Perceptrons (MLP) with Positional Encoding, MLP with Random Fourier Features, SIREN~\citep{sitzmann2020implicit}, GAUSS~\citep{ramasinghe2022beyond}, and WIRE~\citep{saragadam2023wire}. Each model is trained for 20,000 iterations to ensure convergence and taken the highest performance as the final result, with all hyperparameters including learning rate, layers, $\omega$ and s for other activation functions, aligned with the official implementations of the baseline methods in WIRE~\footnote{\url{https://github.com/vishwa91/wire.git}}. 

For the custom line-search algorithm employed in our method, we configure the maximum learning rate to $1 \times 10^{-3}$, with a minimum threshold of 0. To ensure a comprehensive comparison, we assess performance across three standard metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS)~\citep{zhang2018unreasonable}.

\begin{table}[!ht]
\vspace{-10pt}
    \centering
    \caption{MLP+PE+Ours achieves the best performance across all metrics, demonstrating superior reconstruction quality and visual fidelity. WIRE ranks second, excelling in SSIM and LPIPS. RFF-based methods perform poorly, likely due to their inherent limitation in fitting non-square images, as diagonal frequency components near the edges are harder to cover. Overall, our proposed method shows significant effectiveness, achieving approximately 10 PSNR improvement.}
     \vspace{9pt}
    \begin{tabular}{lccc}
        \toprule
        Methods & PSNR↑ & SSIM↑ & LPIPS↓ \\
        \midrule
        MLP+PE        & 25.67 &0.7001 &  0.2674\\
        MLP+RFF       & 26.58 & 0.7180 & 0.2307 \\
        
        SIREN         & 30.90 & 0.8505& 0.1621\\
        GAUSS         & 33.34 & 0.8950 & 0.0693 \\
        WIRE          & 35.38 &  0.9247 &0.0386\\
        MLP+PE+Ours& \textbf{40.96} & \textbf{0.9719} & \textbf{0.0126} \\
        MLP+RFF+Ours&\underline{36.63} & \underline{0.9545}& \underline{0.0220} \\
        \bottomrule
    \end{tabular}
    \label{tab:odak}
   \vspace{-5pt}
\end{table}
And the performance can be view at \autoref{figure:odak} where it can be found that our methods successfully reconstruct the windows with clarity, demonstrating their effectiveness.
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1.\textwidth]{Image/Image.pdf}
    \caption{For the image regression task, our method can reach SOTA performance. It can be observed for the reconstruction quality of the window part in the image is the best without much noise and clear structure.}
    \vspace{-10pt} 
    \label{figure:odak}
\end{figure*}
Overall, by employing our method on the Fourier features embeddings, the overall performance can even surpass SOTA methods which validates the effectiveness of our proposed method that can not only reduce the noise level of the fitted result but also improve the fitting accuracy in different metrics.

\subsection{3D-Shape Regression}
\begin{table*}[!ht]
    \centering
    \caption{We highlight the best results in bold and underline the second-best results. Since the space is normalized, the detail difference will be extremely small in scale ($10^{-6}$) and the main structure is fitted well. However, the actual fitted result is quite different in details. Therefore, we also provide figures (\autoref{figure:exp_sdf}) of the performance to have a better understanding of the performance.}
    \resizebox{1\linewidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule
            Metric & MLP+PE & MLP+RFF & BACON & SIREN & GAUSS & WIRE & MLP+PE+Ours & MLP+RFF+Ours \\
            \midrule
            Chamfer Distance (↓) & 1.8413e-06 & 1.8525e-06 & 1.9535e-06 & 1.8313e-06 &  2.1593e-06&2.7243e-06&\textbf{1.7919e-06} & \underline{1.7947e-06} \\
            \bottomrule
        \end{tabular}}
    \label{table:metrics_comparison}
 \vspace{-10pt}
\end{table*}
 We evaluate our method on the Signed-Distance-Function (SDF) regression task, aiming to learn a function that maps 3D coordinates to their signed distance values. Positive values indicate points outside an object, and negative values are inside.
%
The objective is precise 3D shape reconstruction. We follow the experimental setup from \citet{lindell2022bacon}, training each model for 200,000 iterations with other hyperparameters the same as baselines provided. The learning rate for line-search was capped at $1 \times 10^{-3}$.
%
Performance is evaluated on four Stanford 3D Scanning Repository scenes~\footnote{\url{http://graphics.stanford.edu/data/3Dscanrep/}}: Armadillo, Dragon, Lucy, and Thai, each with 10,000 sampled points which is relatively sparse in 3D space.
%
To evaluate the performance of each model, we employ Chamfer distance instead of IOU. The absence of an official IOU implementation can lead to inconsistent results, whereas Chamfer Distance is computed by measuring the nearest vertex distances between the ground truth and predicted mesh vertices which can be implemented without much controversial and hyperparameters. Moreover, Chamfer distance reflects more details about the surface of object compared using sampling occupancy to calculate IOU. As the entire scene is rescaled to a 0–1 range, the Chamfer Distance is relatively small, with a magnitude on the order of \(10^{-6}\).
%
Our comparisons include baselines consistent with those used in the image regression task.

From quantification results shown in the \autoref{table:metrics_comparison}, Fourier features embeddings+our method achieves the lowest Chamfer Distance, demonstrating superior accuracy in shape reconstruction. Illustrations of results can be found at \autoref{figure:exp_sdf}, where it can be observed that the proposed method, to some extent, smoothed the surface while reconstructing more details compared with other baselines. However, GAUSS and WIRE tend to overfit the training set due to the sparsity of training set in 3D space, resulting in uneven surfaces, whereas SIREN exhibits smoother results but underfits the training dataset.

\begin{figure}[!ht]
    \includegraphics[width=.5\textwidth]{Image/sdf.pdf} % 可根据需要调整宽度
    \caption{Visualization of the 3D shape regression task shows that our method can smooth the surface while maintain detail structures.}
     % 可根据需要调整空白
     \vspace{-10pt}
    \label{figure:exp_sdf}
\end{figure}

\subsection{Neural Radiance Field Experiments}
\begin{table}[!ht]
    \centering
    \caption{The quantitative results demonstrate our method can produce the best reconstruction in the dense input situations.}
    \vspace{10pt}
    \begin{tabular}{lccc}
        \toprule
        Methods & PSNR↑ & SSIM↑ & LPIPS↓ \\
        \midrule
        MLP+PE        & \underline{31.06} & \underline{0.9542} & \underline{0.0202} \\
        MLP+RFF        & 30.18 & 0.9476 & 0.0292 \\
        SIREN         & 25.52 & 0.8659 & 0.1500 \\
        GAUSS         & 27.87 & 0.9079 & 0.0707 \\
        WIRE          & 28.53 & 0.9198 & 0.0523 \\
        MLP+PE+Ours   & \textbf{31.45} & \textbf{0.9596} & \textbf{0.0172} \\
        MLP+RFF+Ours   & 30.70 & 0.9542 & 0.0232\\
        \bottomrule
        \vspace{-25pt}
    \end{tabular}
    \label{tab:nerf200}
    
\end{table}
This section explores the application of Neural Radiance Fields (NeRF) for fitting 3D scenes, focusing on reconstructing scenes by predicting color and density from 3D coordinates and viewing directions. The models are trained with MSE loss for 200,000 iterations, using the same hyperparameters as in the official implementation. Performance is evaluated using PSNR, SSIM, and LPIPS metrics. 

For the adaptive linear filer, we still employ 3 layers to maximize the performance. To minimize overfitting, we applied the line-search method (from $1 \times 10^{-3}$ to 0) and evaluated the models on the NeRF Blender dataset \citep{martin2021nerf} with 100 training images, which includes diverse synthetic scenes. Training utilized cropped 200$\times$200 images with a white background for consistency. Comparisons were conducted against a baseline MLP with Positional Encoding~\citep{mildenhall2021nerf}, SIREN~\citep{sitzmann2020implicit}, GAUSS~\citep{ramasinghe2022beyond} and WIRE~\citep{saragadam2023wire}. We use 8 layers for all models except for GAUSS and WIRE where we found 6 layers can achieve better performance. Since no official implementation of SIREN, GAUSS and WIRE exists for the NeRF task, we adapted the network structures implemented by WIRE to the nerf-pytorch codebase~\footnote{\url{https://github.com/yenchenlin/nerf-pytorch.git}} without altering hyperparameters that author suggested (notice that WIRE chooses sparse input setting but ours is dense setting).
The results in \autoref{tab:nerf200} show that our proposed method surpasses all baselines including WIRE and vanilla NeRF. As shown in \autoref{figure:nerflego}, our approach enables NeRF to capture finer details, such as the Lego's bucket.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.45\textwidth]{Image/nerf.pdf}
    \caption{It can be observed that the reconstruction quality of the lego bucket remains high even at low resolutions when using our method.}
    \vspace{-15pt} 
    \label{figure:nerflego}
\end{figure}
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.475\textwidth]{Image/ship.pdf}
%     \caption{From this figure, it can be seen that our method successfully reconstructs high-frequency details, such as the ropes on the ship's mast and the complex variations of light on the water surface.}
%     \vspace{-10pt} 
%     \label{figure:nerfship}
% \end{figure}



\subsection{Ablation Study}
We validate the effectiveness of bias-free MLPs as adaptive linear filters still on the Kodak dataset. As even using our method learning rate scheduler is still necessary for the INRs part, therefore, we use Lambda Learning rate scheduler which is the most commonly used scheduler in all above tasks. The result is shown in the \autoref{tab:comparison}, where using our line-search based learning rate adjustor can further improve the performance.
\begin{table}[!ht]
    \centering
    \caption{Performance comparison of various methods for Image Regression. "w/o" stands for "without," "w/" stands for "with," and "L" refers to our custom line-search algorithm. Notice that our method is not contradict to the learning rate scheduler.}

    \small % 调整字体大小
    \begin{tabular}{lccc}
        \toprule
        & \text{PSNR} $\uparrow$ & \text{SSIM} $\uparrow$ & \text{LPIPS} $\downarrow$ \\
        \midrule
        MLP + PE + Ours w/o L & 40.50 & 0.9691 &  0.0143 \\
        MLP + PE + Ours w/L  & \textbf{40.96} & \textbf{0.9719} & \textbf{0.0126} \\
        MLP + RFF + Ours w/o L & 36.08 & 0.9506 & 0.0247 \\
        MLP + RFF + Ours w/L  & \textbf{36.63} & \textbf{0.9545} & \textbf{0.0220}\\
        \bottomrule
    \end{tabular}
 
    \label{tab:comparison}
    \vspace{-10pt} % 减小表格下方的垂直间距
\end{table}



