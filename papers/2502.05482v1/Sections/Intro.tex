\section{Introduction}
Implicit Neural Representations (INRs), which fit the target function using only input coordinates, have recently gained significant attention.
%
By leveraging the powerful fitting capability of Multilayer Perceptrons (MLPs), INRs can implicitly represent the target function without requiring their analytical expressions. 
%
The versatility of MLPs allows INRs to be applied in various fields, including inverse graphics~\citep{mildenhall2021nerf, barron2023zip, martin2021nerf}, image super-resolution~\citep{chen2021learning, yuan2022sobolev, gao2023implicit}, 
image generation~\citep{skorokhodov2021adversarial}, and more~\citep{chen2021nerv, strumpler2022implicit, shue20233d}.
%
\begin{figure}
    \includegraphics[width=0.5\textwidth]{Image/Fig2.pdf}
    \caption{As illustrated at the circled blue regions and green regions, it can be observed that even with well-chosen standard deviation/scale, as experimented in \autoref{figure:combined}, the results are still unsatisfactory. However, using our proposed method, the noise is significantly alleviated while further enhancing the high-frequency details.}
    \label{fig:var}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[!ht]
    \centering
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{Image/fig_cropped.pdf} % 替换为你的小图文件
        \label{figure:small_image}
        \vspace{-20pt}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.75\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{Image/psnr_trends_rff_pe_simplified.pdf} % 替换为你的大图文件
        \vspace{-20pt}
        \label{figure:large_image}
        
    \end{minipage}
    \caption{We test the performance of MLPs with Random Fourier Features (RFF) and MLPs with Positional Encoding (PE) on a 1024-resolution image to better distinguish between high- and low-frequency regions, as demonstrated on the left-hand side of this figure. We find that the performance of MLPs+RFF degrades rapidly with increasing standard deviation compared with MLPs+PE. Since positional encoding is deterministic, scale=512 can be considered to have standard deviation around 121.}
    \label{figure:combined}
    \vspace{-10pt}
\end{figure*}
Varying the sampling standard deviation/scale may lead to degradation results, as shown in \autoref{figure:combined}.
%
However, MLPs face a significant challenge known as the spectral bias, where low-frequency signals are typically favored during training~\citep{rahaman2019spectral}. 
A common solution is to map coordinates into the frequency domain using Fourier features, such as Random Fourier Features and Positional Encoding, which can be understood as manually set high-frequency correspondence prior to accelerating the learning of high-frequency targets.~\citep{tancik2020fourier}. 
This embeddings widely applied to the INRs for novel view synthesis~\citep{mildenhall2021nerf,barron2021mip}, dynamic scene reconstruction~\citep{pumarola2021d}, object tracking~\citep{wang2023tracking}, and medical imaging~\citep{corona2022mednerf}.
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1.\textwidth]{Image/psnr_trends_rff_pe_simplified.pdf}
%     \caption{This figure shows the change of PSNR on the whole, low-frequency region, and high-frequency region of the image fitting by using two Fourier Features Embedding with varying scale of variance: (Right) Positional Encoding (PE) (Left) Random Fourier Features (RFF). Both PE and RFF will degrade the low-frequency regions of the target image when variance increases.}
%     \vspace{-20pt} 
%     \label{figure:stats}
% \end{figure}


Although many INRs' downstream application scenarios use this encoding type, it has certain limitations when applied to specific tasks.
%
It depends heavily on two key hyperparameters: the sampling standard deviation/scale (available sampling range of frequencies) and the number of samples.
%
Even with a proper choice of sampling standard deviation/scale, the output remains unsatisfactory, as shown in \autoref{fig:var}: Noisy low-frequency regions and degraded high-frequency regions persist with well chosen sampling standard deviation/scale with the grid-searched standard deviation/scale, which may potentially affect the performance of the downstream applications resulting in noisy or coarse output.
%
However, limited research has contributed to explaining the reason and finding a proper frequency embeddings for input~\citep{landgraf2022pins, yuce2022structured}.

In this paper, we aim to offer a potential explanation for the high-frequency noise and propose an effective solution to the inherent drawbacks of Fourier feature embeddings for INRs.
%
Firstly, we hypothesize that the noisy output arises from the interaction between Fourier feature embeddings and multi-layer perceptrons (MLPs). We argue that these two elements can enhance each other's representation capabilities when combined. However, this combination also introduces the inherent properties of the Fourier series into the MLPs.
%
To support our hypothesis, we propose a simple theorem stating that the unsampled frequency components of the embeddings establish a lower bound on the expected performance. This underpins our hypothesis, as the primary fitting error in finitely sampled Fourier series originates from these unsampled frequencies.

Inspired by the analysis of noisy output and the properties of Fourier series expansion, we propose an approach to address this issue by enabling INRs to adaptively filter out unnecessary high-frequency components in low-frequency regions while enriching the input frequencies of the embeddings if possible.
%
To achieve this, we employ bias-free (additive term-free) MLPs. These MLPs function as adaptive linear filters due to their strictly linear and scale-invariant properties~\citep{mohan2019robust}, which preserves the input pattern through each activation layer and potentially enhances the expressive capability of the embeddings.
%
Moreover, by viewing the learning rate of the proposed filter and INRs as a dynamically balancing problem, we introduce a custom line-search algorithm to adjust the learning rate during training. This algorithm tackles an optimization problem to approximate a global minimum solution. Integrating these approaches leads to significant performance improvements in both low-frequency and high-frequency regions, as demonstrated in the comparison shown in \autoref{fig:var}.
%
Finally, to evaluate the performance of the proposed method, we test it on various INRs tasks and compare it with state-of-the-art models, including BACON~\citep{lindell2022bacon}, SIREN~\citep{sitzmann2020implicit}, GAUSS~\citep{ramasinghe2022beyond} and WIRE~\citep{saragadam2023wire}. 
The experimental results prove that our approach enables MLPs to capture finer details via Fourier Features while effectively reducing high-frequency noise without causing oversmoothness.
%
To summarize, the following are the main contributions of this work:
\begin{itemize}
    \item From the perspective of Fourier features embeddings and MLPs, we hypothesize that the representation capacity of their combination is also the combination of their strengths and limitations. A simple lemma offers partial validation of this hypothesis.

    
    \item  We propose a method that employs a bias-free MLP as an adaptive linear filter to suppress unnecessary high frequencies. Additionally, a custom line-search algorithm is introduced to dynamically optimize the learning rate, achieving a balance between the filter and INRs modules.

    \item To validate our approach, we conduct extensive experiments across a variety of tasks, including image regression, 3D shape regression, and inverse graphics. These experiments demonstrate the effectiveness of our method in significantly reducing noisy outputs while avoiding the common issue of excessive smoothing.
\end{itemize}
