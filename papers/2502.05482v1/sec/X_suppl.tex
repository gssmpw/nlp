\clearpage
\section{Comparison with SAPE}
There might be arguing that our proposed method is similar to SAPE \citep{hertz2021sape}. However, from our perspective, we differ from this work in the following points:
\begin{itemize}
    \item Our proposed method can extend the frequency band of embeddings, i.e. $A_y*y$, where SAPE can be considered a simple mask that applied on the embeddings, i.e. $w*y$. This difference makes our method can reach better lower bound of the loss compared to SAPE. The performance on image regression task is demonstrate on \autoref{tab:SAPE}.
    
    \item  Our method using Bias-free MLPs as the filter which can be applied not only on grid-based structure, but also on continuous space like Neural Radiance Field. This is benefited from the continuous property of the prediction from MLPs.
\end{itemize}
\begin{table}[ht]
    \centering
        \caption{Performance comparison with SAPE and Our proposed method for both Positional Encoding and Random Fourier Features.}

    \small % 调整字体大小
    \vspace{10pt}
    \begin{tabular}{lcccccc}
        \toprule
         &
        \multicolumn{3}{c}{Positional Encoding} &
        \multicolumn{3}{c}{Random Fourier Features} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\
        \midrule
        SAPE& 33.06 & 0.8996 & 0.0681 & 36.24 & 0.9455 & 0.0356  \\
        Ours  & \textbf{40.96} & \textbf{0.9719} & \textbf{0.0126} & \textbf{36.63} & \textbf{0.9545} & \textbf{0.0220}  \\
 
        \bottomrule
    \end{tabular}

    \label{tab:SAPE}
    \vspace{-10pt} % 减小表格下方的垂直间距
\end{table}
\section{Convergence Comparison For Spike Function and Sinusoidal Function}
\label{sec:spike}
In this section, we compare the convergence speed and loss to provide a more profound understanding of how Fourier features, MLPs and their combination's representation capability for spike function and sinusoidal function (spatial compact and spectral compact) as illustrated in \autoref{figure:loss}.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{Image/loss.pdf}
    \caption{By comparing the value of loss function for Fourier features plus one linear layer, MLPs and their combination, it can be found that MLPs more good at fitting spatial compact function like spike function and Fourier features can fit sinusoidal function well. Through combining two models, the representation capability is even better for both situations.}
    \vspace{-10pt} 
    \label{figure:loss}
\end{figure}
\section{Convergence of Modified Line-search algorithm}
To address potential divergence concerns, we validate the convergence of the modified line-search algorithm on the DIV2K validation split with 256$\times$256 resolutions (for the fast inference speed and the large scale of the dataset(100 images)) for both RFF and PE embeddings. Results show consistent convergence for both, as illustrated in \autoref{figure:converg}. The learning rates of the adaptive linear filter steadily decrease throughout training, ultimately converging to 0, confirming the algorithm’s stability and convergence by the end of training.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{Image/psnr_lr_plot.pdf}
    \caption{We demonstrate the convergence of the modified line-search algorithm through image regression experiments for RFF and PE. During training, both the PSNR and the learning rate consistently converge, confirming the effectiveness of our proposed line-search-based approach.}
    \vspace{-10pt} 
    \label{figure:converg}
\end{figure}
\section{Robustness under Varying Standard Deviation}
We also evaluate the impact of varying standard deviation on the same image regression task as in \autoref{figure:combined}. As shown in \autoref{figure:stats2}, unlike the results presented in \autoref{figure:combined}, performance remains stable even with high sampling standard deviation when our method is applied. This highlights the robustness of our approach under high sampling standard deviation.

\begin{figure}[!h]
    \centering
    \includegraphics[width=.8\textwidth]{Image/psnr_trends_rff_pe_changed.pdf}
    \caption{We evaluated our method's ability to mitigate high-frequency artifacts in two Fourier feature embedding methods. Results show that our approach effectively prevents model degradation under high standard deviation conditions for RFF, where traditional embeddings struggle.
}
    \vspace{-15pt} 
    \label{figure:stats2}
\end{figure}

\section{Varying the Number of Layers}
\label{section:layer}
In this section, we investigate the impact of the number of layers in bias-free MLPs on overall performance and demonstrate that simply increasing the number of layers of MLP plus Fourier features embeddings cannot significantly enhance performance as our methods. To evaluate this, we firstly conducted experiments on images from the DIV2K dataset at a resolution of 256$\times$256 for larger dataset and fast training speed. Filters with 1, 2, 3, 4, and 5 layers were tested with the same 3 layers INRs part, and the results are presented in \autoref{figure:bfMLPlayers}. The performance initially improves as the number of layers increases to 3 but starts to decline beyond that. This may be due to over-parameterization, which can reduce smoothness and make it more challenging for the filter to preserve the input signals.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Image/psnr_pe_vs_rff_layers.pdf}
    \caption{As the number of filter layers increases to 3, performance reaches its peak with around 52.7 PSNR and 47.5 PSNR respectively, but begins to decline with further increases in layers.}
    \vspace{-10pt} 
    \label{figure:bfMLPlayers}
\end{figure}
 We also test the impact of the number of layers on MLPs with Fourier features using the same sampled images. As shown in \autoref{figure:layers}, performance improves with more layers up to 12, after which it begins to decline. Despite the substantially larger number of parameters at 12 layers, the performance still fails to surpass that of our methods as previously illustrated in \autoref{figure:bfMLPlayers}.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Image/psnr_vs_layers_PEL_RFF.pdf}
    \caption{Performance increases with additional layers up to 12 with around 44 PSNR and 41 PSNR respectively, but starts to diminish beyond that point.}
    \vspace{-10pt} 
    \label{figure:layers}
\end{figure}

\section{Bias-free MLPs vs Bias MLPs}
We also compare the influence of additive term to the performance of the filter for the NeRF task. This is because that the NeRF task involves the interpolation task where the frequency pattern of the embeddings should not be disrupted by the filter. For the image regression task where it requires overfitting, the difference for the bias and bias-free MLPs may not be so obvious. The result is shown in \autoref{table:BIAS} where it shows that additive term indeed can worsen the performance.
\label{section:BFExp}
\begin{table}[!h]
\vspace{-10pt}
    \centering
    \caption{The result of adaptive linear filter w/w.o. bias term tested on the NeRF task.}
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
      &  Bias MLP+PE &  Bias-free MLP+PE  & Bias MLP+RFF  &  Bias-free MLP+RFF \\
        \midrule
        PSNR↑  & 31.17 & \textbf{31.45}& 30.18&\textbf{30.70}  \\
        SSIM↑  &0.9563  &\textbf{0.9596}& 0.9476&\textbf{0.9542} \\
        LPIPS↓ & 0.0201 &\textbf{0.0172}& 0.0293&\textbf{0.0232}\\
        \bottomrule
    \end{tabular}
    }
    \label{table:BIAS}
    \vspace{-15pt}
\end{table}
\section{Definition of High-dimensional Fourier Series}
For a d-dimensional periodic function $f(\mathbf{x})$ with input $\mathbf{x} = [x_1, x_2,\cdots,x_d]^{\top}$ be a 2$\pi$ period function with respect to each components. Then the function $f(\mathbf{x})$ can be expanded as:
$$
f(\mathbf{x}) = \sum_{\mathbf{m}\in\mathbb{Z}^d}\hat{f}_m\mathbf{e}^{i\mathbf{m}^{\top}\mathbf{x}}
$$
where $\hat{f}_m$ is the coefficient of different frequency component.

\section{Definition of Neural Tangent Kernel}
\label{section：NTK}
The Neural Tangent Kernel (NTK), a prominent tool for neural network analysis, has attracted considerable attention since its introduction. To simplify the analysis, this section will focus specifically on the NTK for two-layer MLPs, as the subsequent analysis also relies on the two-layer assumption. The two-layer MLP, $f(\mathbf{x}; \mathbf{w})$, with activation function $\sigma(\cdot)$ and input $\mathbf{x} \in \mathbb{R}^d$, can be expressed as follows:
$$
    f(\mathbf{x}; \mathbf{w}) = \frac{1}{\sqrt{m}}\sum^{m}_{r=1}a_r\sigma(\mathbf{w}^{\top}_r\mathbf{x}+\mathbf{b}_r)
$$
where m is the width of the layer and $\Arrowvert\mathbf{x}\Arrowvert = 1$ (also can be written as $\mathbf{x}\in\mathbb{S}^{d-1}$, where $\mathbb{S}^{d-1}\equiv\{\mathbf{x}\in\mathbb{R}^d:\Arrowvert\mathbf{x}\Arrowvert = 1\}$). The term $\frac{1}{\sqrt{m}}$ is used to assist the analysis of the network. Based on this MLP, the kernel is defined as the following:
$$
k(\mathbf{x_i}, \mathbf{x_j}) = \mathbb{E}_{\mathbf{w}\sim\mathcal{I}}\left\lbrace\left\langle\frac{\partial f(\mathbf{x_i};\mathbf{w})}{\partial \mathbf{w}}, \frac{\partial f(\mathbf{x_j};\mathbf{w})}{\partial \mathbf{w}}\right\rangle\right\rbrace
$$
This formula enables the exact expression of the NTK to better analyze the behavior and dynamics of MLP. 
%
For a two-layer MLP with a rectified linear unit (ReLU) activation function where only the first layer weights are trained and the second layer is frozen, the NTK of this network can be written as the following \citep{xie2017diverse}:
\begin{equation*}
k(\mathbf{x_i}, \mathbf{x_j}) = \frac{1}{4\pi}(\langle\mathbf{x_i}, \mathbf{x_j}\rangle+1)(\pi-arccos(\langle\mathbf{x_i}, \mathbf{x_j}\rangle))    
\label{Eq:NTK}
\end{equation*}
This expression can help us to determine the eigenfunction and eigenvalue of kernel and therefore provide a more insightful analysis of the network.
\section{Proof of the Proposed Proposition}
\label{section:proof}
In this section, we will introduce why the unselected frequencies of the Fourier features will form a lower bound for the theoretical performance. Compared to \citep{yuce2022structured} where they show that the INRs with embeddings can be decomposed into the Fourier basis, we view the INRs with Fourier features embeddings from the perspective of Neural Tangent Kernels and derive a similar result about the Harmonic expansion of the INRs.

\begin{lemma}
    (\citet{yuce2022structured}) Let $\{\mathbf{b}^{(1)}_i\in\mathbb{R}^d\}_{i\in[N]}$ and $\{\mathbf{b}^{(2)}_j\in\mathbb{R}^d\}_{j\in[M]}$ be two sets of frequency vectors and N and M are integers that represent the size for each set, $\mathbf{x}\in\mathbb{R}^d$ is the coordinates in d-dimensional space. Then,
    \begin{equation*}
    \begin{split}
            &\left(\sum^N_{i=1}c^{(1)}_i cos(\mathbf{b}^{(1)\top}_i\mathbf{x})\right)\left(\sum^M_{j=1}c^{(2)}_jcos(\mathbf{b}^{(2)\top}_j\mathbf{x})\right) \\&= \left(\sum^{T}_{k=1}c^{*}_kcos(\mathbf{b}^{*\top}_k\mathbf{x})\right), where\, T\leq 2NM
    \end{split}
    \end{equation*}
    where,
    \begin{equation*}
    \mathbf{b}^*\in \left\{ \mathbf{b}^*=\mathbf{b}^{(1)}_i\pm\mathbf{b}^{(2)}_j\,\vline\,i\in[N],\, j\in[M]\right\}
    \end{equation*}
\label{lemma:2}
\end{lemma}
% \begin{proof}
% \begingroup
% \scriptsize
%     \begin{align*}
%         &\left(\sum^N_{i=1}c^{(1)}_i cos(\mathbf{b}^{(1)\top}_i\mathbf{x})\right)\left(\sum^M_{j=1}c^{(2)}_jcos(\mathbf{b}^{(2)\top}_j\mathbf{x})\right)\\
%         &= \left(\sum^N_{i=1}\sum^M_{j=1}c^{(1)}_ic^{(2)}_j cos(\mathbf{b}^{(1)\top}_i\mathbf{x})cos(\mathbf{b}^{(2)\top}_j\mathbf{x})\right)\\
%         &= \left(\sum^N_{i=1}\sum^M_{j=1}\frac{1}{2}c^{(1)}_ic^{(2)}_j\left(cos((\mathbf{b}^{(1)}_i+\mathbf{b}^{(2)}_j)^{\top}\mathbf{x})+cos((\mathbf{b}^{(1)}_i-\mathbf{b}^{(2)}_j)^{\top}\mathbf{x})\right)\right)\\
%         &= \sum^{T}_{k=1}c^{*}_kcos(\mathbf{b}^{*\top}_k\mathbf{x}), where\, T\leq 2NM
%     \end{align*}
%     \endgroup
% \end{proof}
\begin{lemma}
    (\citet{yuce2022structured}) $\{\mathbf{b}_i\in\mathbb{R}^d\}_{i\in[n]}$ be a set of frequency vectors and N is an integer that represents the size, $\mathbf{x}\in\mathbb{R}^d$ is the coordinates in d-dimensional space. Then,
    \begin{equation*}
    \left(\sum^n_{i=1}cos(\mathbf{b}^{\top}_i\mathbf{x})\right)^k = \left(\sum^{N}_{k=1}cos(\mathbf{b}^{*\top}_k\mathbf{x})\right), where\, N \leq k^nnk
    \end{equation*}
    where,
    \begin{equation*}
    \mathbf{b}^*\in \left\{ \mathbf{b}^*=\sum_i^n c_i\mathbf{b_i}\,\vline\, c_i\in\mathbb{Z},\,\sum_i^n|c_i|\leq k\right\}
    \end{equation*}
\label{lemma:3}
\end{lemma}

% \begin{proof}

%     \textbf{Proof by induction:}\\
%     \textbf{when k=1}\\
%     \textbf{This is a special case proved by Lemma \ref{lemma:2}.}\\
%     \textbf{Assuming the claim of this Lemma is true for k=m, then when k=m+1}\\
%     \begin{align*}
%     &\left(\sum^n_{i=1}cos(\mathbf{b}^{\top}_i\mathbf{x})\right)^{m+1}\\
%     &=\left(\sum^n_{i=1}cos(\mathbf{b}^{\top}_i\mathbf{x})\right)^{m}\left(\sum^n_{i=1}cos(\mathbf{b}^{\top}_i\mathbf{x})\right)\\
%     \end{align*}
%     \textbf{By the assumption on k=m}\\
%     \begingroup
%     \small 
%     \begin{align*}
%     &=\left(\sum^{n'}_{k=1}cos(\mathbf{b}^{\dag\top}_k\mathbf{x})\right)\left(\sum^n_{i=1}cos(\mathbf{b}^{\top}_i\mathbf{x})\right), where\, n' \leq m^nnm\\
%     &\textbf{where}\,\mathbf{b}^\dag\in \left\{ \mathbf{b}^\dag=\sum_i^n c_i\mathbf{b_i}\,\vline\, c_i\in\mathbb{Z},\,\sum_i^n|c_i|\leq m\right\}\\  
%     \end{align*}
%     \textbf{By Lemma \ref{lemma:2}}\\
%     \begin{align*}
%     &=\left(\sum^{N}_{k=1}cos(\mathbf{b}^{*\top}_k\mathbf{x})\right), where\, N \leq (m+1)^nn(m+1)\\
%     &\textbf{where}\,\mathbf{b}^{*\top}_k\in \left\{ \mathbf{b}^*=\sum_i^n c_i\mathbf{b_i}\pm\mathbf{b_j}\,\vline\, c_i\in\mathbb{Z},\,\sum_i^n|c_i|\leq m,\,\forall i,\,j\right\}\\
%     &\Rightarrow \mathbf{b}^{*\top}_k\in \left\{ \mathbf{b}^*=\sum_i^n c^*_i\mathbf{b_i}\,\vline\, c_i\in\mathbb{Z},\,\sum_i^n|c_i|\leq m+1\right\}
%     \end{align*}
%     \endgroup
% \end{proof}
\begin{lemma}
      Given a pre-sampled frequency set $\mathbf{B}_n = \{\mathbf{b}_i\in\mathbb{N}^d\}_{i\in[N]}$ and the Fourier features projection, $\gamma(\cdot)$, as $\gamma(\mathbf{x}) = [sin(2\pi \mathbf{b}_i^{\top}\mathbf{x}), cos(2\pi \mathbf{b}_i^{\top}\mathbf{x})]_{i\in[N]}, [N] = {1, 2, 3,\cdots, N}$. Then, $\gamma(\mathbf{x})^\top\gamma(\mathbf{
      z}) = sum(\gamma(\mathbf{x-z}))$.
\label{lemma:4}
\end{lemma}
\begin{proof}
    \begin{align*}
        &\gamma(\mathbf{x})^\top\gamma(\mathbf{z})\\ &= \sum^N_{i=1} cos(2\pi \mathbf{b}_i^{\top}\mathbf{x})cos(2\pi \mathbf{b}_i^{\top}\mathbf{z}) + sin(2\pi \mathbf{b}_i^{\top}\mathbf{x})sin(2\pi \mathbf{b}_i^{\top}\mathbf{z})\\
        &=\sum^N_{i=1}cos(2\pi \mathbf{b}_i^{\top}(\mathbf{x}-\mathbf{z})) = sum(\gamma(\mathbf{x}-\mathbf{z}))
    \end{align*}
\end{proof}
\begin{lemma}
    For a two-layer Multilayer-perceptrons (MLPs) denoted as $f(\mathbf{x};\mathbf{W})$, where $\mathbf{x}\in\mathbb{R}^d$ as input and $\mathbf{W}$ as the parameters of the MLPs. 
    %
    Then the order-N approximation of eigenvectors of the Neural Tangent Kernel (Eq.\ref{Eq:NTK}) when using Fourier features embedding, as defined in Def.\ref{def:ff}, to project the input to the frequency space can be presented as,
    \begin{equation*}
    \begin{split}
               &k(\gamma(\mathbf{x}),\gamma(\mathbf{z}))= \sum^{N^{\dag}}_{i=1} \lambda^2_i cos(\mathbf{b}^*\mathbf{x})cos(\mathbf{b}^*\mathbf{z})\\ &+ \sum^{N^{\dag}}_{i=1} \lambda^2_i sin(\mathbf{b}^*\mathbf{x})sin(\mathbf{b}^*\mathbf{z})\text{, where $N^\dag\leq 4Nk^mkm^2$}
    \end{split}
    \end{equation*}
    where 
\begingroup
\small 
\begin{equation*}
    \mathbf{b}^*\in \mathcal{L}_{Span\{b_j\}} \equiv \left\{   \mathbf{b}^*=\sum^n_{j=1} c_j\mathbf{b}_j\,\vline\, \sum^{\infty}_{j=1}|c_j|<N+k^mkm+m  \right\}
\end{equation*}
\endgroup
    and $\lambda_i$s are eigenvalues for each eigenfunctions $sin(\mathbf{b}^*\mathbf{x})$ and $cos(\mathbf{b}^*\mathbf{x})$.
\label{lemma1}
\end{lemma}
\begin{proof}
    By ~\citet{xie2017diverse}, the two-layer MLP's NTK has the form as the following:
    \[
    k(x,z)= \frac{\langle\mathbf{x},\mathbf{z}\rangle(\pi - arccos(\langle\mathbf{x},\mathbf{z}\rangle)}{2\pi}
    \]
    If we use Fourier features mapping, $\gamma(\mathbf{x})$, before inputting to the Neural Network with a randomly sampled frequency set $\{\mathbf{b}_i\}^m_{i=1}$.
    
    By the Lemma \ref{lemma:4}, in order to ensure that the vector dot product still be a valid dot product in $S^{d-1}$, the dot product of two embedded input can be written as $\gamma(\mathbf{x})^{\top}\gamma(\mathbf{z})=\frac{1}{||\gamma(\mathbf{x})||||\gamma(\mathbf{z})||}\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))$ to make sure the dot product is bounded by 1.

    \begin{align*}
         k(&\gamma(\mathbf{x}),\gamma(\mathbf{z}))= \frac{\langle\gamma(\mathbf{x}),\gamma(\mathbf{z})\rangle(\pi - arccos(\langle\gamma(\mathbf{x}),\gamma(\mathbf{z})\rangle)}{2\pi}\\
         &\textbf{Denoting $||\gamma(\mathbf{x})||||\gamma(\mathbf{z})||$ as $\aleph$}\\
         &=\frac{\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))(\pi - arccos(\frac{1}{\aleph}\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))))}{2\pi\aleph}\\
         &\textbf{By N-order approximation Taylor Expansion of arccos($\cdot$)}\\
         &=\frac{1}{2\pi\aleph}(\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))\times\\&(\frac{\pi}{2}+\sum^{N}_{k=1}\frac{(2n)!}{2^{2n}} (n!)^2
(\sum^m_{i=1}\frac{1}{\aleph}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x})))^k))\\
         &\textbf{By Lemma \ref{lemma:4}}\\
         &=\frac{\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))(\frac{\pi}{2}+\sum^{N}_{k=1}\sum^M_{i=1}\beta^*_icos(2\pi\mathbf{b}^*_i(\mathbf{z}-\mathbf{x})))}{2\pi\aleph}\\& \text{where M$\leq k^mkm$ }\\
         &\text{and}\,\,\mathbf{b}^*_i\in \left\{ \mathbf{b}^*=\sum_i^m c_i\mathbf{b_i}\,\vline\, c_i\in\mathbb{Z},\,\sum_i^n|c_i|\leq k\right\}\\
          &=\frac{\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))(\frac{\pi}{2}+\sum^{N^*}_{i=1}\beta^*_icos(2\pi\mathbf{b}^*_i(\mathbf{z}-\mathbf{x})))}{2\pi\aleph}\\ &\text{where $N^*\leq 2NM$ }\\
         &\text{and}\,\,\mathbf{b}^*_i\in \left\{ \mathbf{b}^*=\sum_i^m c_i\mathbf{b_i}\,\vline\, c_i\in\mathbb{Z},\,\sum_i^
         n|c_i|\leq N+M\right\}\\
         &=\frac{1}{2\pi\aleph}(\frac{\pi}{2}\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))+\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))\times\\ &\sum^{N^*}_{i=1}
         \beta^*_icos(2\pi\mathbf{b}^*_i(\mathbf{z}-\mathbf{x}))))\\
         &\textbf{By Lemma \ref{lemma:3}}\\
         &=\frac{\frac{\pi}{2}\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))+\sum^{N^{\dag}}_{i=1}
         \beta^\dag_icos(2\pi\mathbf{b}^\dag_i(\mathbf{z}-\mathbf{x})))}{2\pi\aleph}\\ &\text{where $N^\dag\leq 2mN^*$ }\\
         &\text{and}\,\,\mathbf{b}^\dag_i\in \left\{ \mathbf{b}^\dag=\sum_i^m c_i\mathbf{b_i}\,\vline\, c_i\in\mathbb{Z},\,\sum_i^n|c_i|\leq N+M+m\right\}\\
      &=\frac{1}{4\aleph}\sum^m_{i=1}cos(2\pi\mathbf{b}_i(\mathbf{z}-\mathbf{x}))
      \\&+\frac{1}{2\pi\aleph}\sum^{N^{\dag}}_{i=1}
\beta^\dag_icos(2\pi\mathbf{b}^\dag_i(\mathbf{z}-\mathbf{x})))\text{, where $N^\dag\leq 4Nk^mkm^2$}\\
    \end{align*}
 
    Furthermore, to do the eigendecomposition, we need further to split this into the product of two orthogonal functions by $cos(a-b)=cos(a)cos(b) + sin(a)sin(b)$\\
    
    \begin{align*}
                  =&\frac{1}{4\aleph}\sum^m_{i=1}cos(2\pi\mathbf{b}_i\mathbf{x}))cos(2\pi\mathbf{b}_i\mathbf{z}))+sin(2\pi\mathbf{b}_i\mathbf{x})sin(2\pi\mathbf{b}_i\mathbf{z})
          \\
          &+\frac{1}{2\pi\aleph}\sum^{N^{\dag}}_{i=1}
\beta^\dag_icos(2\pi\mathbf{b}^\dag_i\mathbf{x}))cos(2\pi\mathbf{b}^\dag_i\mathbf{z}))+sin(2\pi\mathbf{b}^\dag_i\mathbf{x})sin(2\pi\mathbf{b}^\dag_i\mathbf{z})
    \end{align*}

    
\end{proof}
This lemma explains why MLPs plus Fourier features embeddings can be interpreted as the linear combination of sinusoidal functions which further provide the evidence of the following lemma \ref{lemma1}.
\begin{theorem}
[Theorem 4.1 in \citet{arora2019fine}] Denoting $\mathbf{u}^{(k)}$ as the prediction of MLPs at iteration $k$, $\lambda$ as the eigenvalue of MLPs, $\mathbf{v}_i$ as the eigenfunction of MLPs, m as the number of neurons in a single layer and $\eta$ as the learning rate. Suppose $\lambda_0 = \lambda_{\min}(\mathbf{H}^\infty) > 0$, 
$\kappa = O\left(\frac{\epsilon \delta}{\sqrt{n}}\right)$, 
$m = \Omega\left(\frac{n^7}{\lambda_0^6 \delta^2 \epsilon^2}\right)$, 
and $\eta = O\left(\frac{\lambda_0}{m}\right)$. Then with probability 
at least $1 - \delta$ over the random initialization, for all $k = 0, 1, 2, \dots$, we have:
\[
\|\mathbf{y} - \mathbf{u}^{(k)}\|_2 = 
\sqrt{\sum_{i=1}^n (1 - \eta \lambda_i)^{2k} (\mathbf{v}_i^\top \mathbf{y})^2} \pm \epsilon.
\]
\label{Thmeorem:brought}
\end{theorem}
\begin{proof}
    Check the proof of Theorem 4.1 in \citet{arora2019fine}
\end{proof}
Notice that the above theorem is based on the full-batch training assumption which is widely used for the image regression task where there is no interpolation requirement.
\begin{lemma}
Let $\mathbf{y}(\mathbf{x}) = \sum_{\mathbf{n}\in\mathbb{Z}^d} \hat{y}_{\mathbf{n}} e^{i \mathbf{n}^\top \mathbf{x}}$ be a $d$-dimensional target function, where $\hat{y}_{\mathbf{n}}$ denotes the Fourier coefficients of $\mathbf{y}(\mathbf{x})$. Consider a pre-sampled frequency set $\mathbf{B}_n = \{\mathbf{b}_i \in \mathbb{Z}^d\}_{i \in [N]}$ and the $L_2$ loss function defined as $\phi(\mathbf{y}, f(\mathbf{x}; \mathbf{W})) = \|f(\mathbf{x}; \mathbf{W}) - \mathbf{y}\|_2$. Let $f(\mathbf{x}; \mathbf{W})$ denotes the MLPs that can be written in the form of sum of sinusoidal functions with frequencies in subspace spanned by $\mathbf{B}_n$ using Neural Tangent Kernel expansion, $\mathbf{y}_\mathbf{B}$ denote the mapping of $\mathbf{y}(\mathbf{x})$ onto the subspace spanned by $\mathbf{B}_n$ which is the same as Neural Tangent Kernel expansion of MLPs, and $\mathbf{y}^\dag_\mathbf{B}$ denote the mapping onto the orthogonal complement, such that $\mathbf{y} = \mathbf{y}_\mathbf{B} + \mathbf{y}^\dag_\mathbf{B}$. Then, with probability at least $1 - \delta$, for all iterations $k = 0, 1, 2, \dots$, the lower bound of the loss function $\phi(\mathbf{y}, f(\mathbf{x}; \mathbf{W}))$ can be expressed as:
\begin{equation*}
    \phi(\mathbf{y}, f(\mathbf{x}; \mathbf{W})) \geq \|\mathbf{y}^\dag_\mathbf{B}\|_2 - \sqrt{\sum_{i}(1 - \eta \lambda_i)^{2k} \langle \mathbf{v}_i, \mathbf{y}_\mathbf{B} \rangle^2} \pm \epsilon,
\end{equation*}
where $\eta$ is the learning rate, $\lambda_i$ are eigenvalues, $\mathbf{v}_i$ are the corresponding eigenvectors, and $\epsilon$ stands for a constant.
\label{lemma:1}
\end{lemma}
\begin{proof}
    Given $\mathbf{B}_n = \{\mathbf{b}_i\in\mathbb{N}^d\}_{i\in[N]}$, this can be spanned as a subspace, $\{cos(2\pi\mathbf{b}^\dag\mathbf{x}), sin(2\pi\mathbf{b}^\dag\mathbf{x})|\mathbf{b}^\dag_i\in\{ \mathbf{b}^\dag=\sum_i^m c_i\mathbf{b_i}\,\vline\, c_i\in\mathbb{Z}$, $\,\sum_i^n|c_i|\leq N^\dag\}\}$, in $\mathcal{L}_d$ space, where each item are orthogonal with each other. Therefore, by Orthogonal Decomposition Theorem, $\mathcal{L}_d$ can be decomposed into the space spanned by $\mathbf{B}_n$ and the space orthogonal to this spanned space (or the space spanned by the rest frequencies component).
    
    % By theorem 4.1 in ??, $\phi(y, f(\mathbf{x};\mathbf{W})) = \sqrt{\sum(1-\eta\lambda_i)^{2k}\langle\mathbf{v}_i,\,y\rangle^2}\pm\epsilon$. 
    By using the Fourier series to expand the $y$, this can be decomposed into $y =y^{\dag}_\mathbf{B}+y_\mathbf{B} $ by orthogonal decomposition theorem mentioned before. And each $\mathbf{v}_i$, the eigenfunction of $f(\mathbf{x}; \mathbf{W})$, belongs to the the spanned subspace by $\mathbf{B}_n$ (by Lemma \ref{lemma1}). Therefore, $f(\mathbf{x}; \mathbf{W})$ can be written as a linear combination of Fourier bases and, therefore, cannot fit signals in the orthogonal space of the spanned subspace of $\mathbf{B}_n$ due to orthogonality.
    \begin{align*}
        &\phi(\mathbf{y}, f(\mathbf{x};\mathbf{W})) = ||\mathbf{y}- f(\mathbf{x};\mathbf{W})||_2\\
        &= ||\mathbf{y}^{\dag}_\mathbf{B}+\mathbf{y}_\mathbf{B}- f(\mathbf{x};\mathbf{W})||_2\\
        &\textbf{By using triangular inequality:$||x+y|| - ||x||\leq||\mathbf{y}||$}\\
        &\geq ||\mathbf{y}^{\dag}_\mathbf{B}||_2 -||f(\mathbf{x};\mathbf{W})-\mathbf{y}_\mathbf{B}||_2 
    \end{align*}
    By Theorem \ref{Thmeorem:brought}, with probability $1-\delta$, $\phi(\mathbf{y}, f(\mathbf{x};\mathbf{W})) = \sqrt{\sum(1-\eta\lambda_i)^{2k}\langle\mathbf{v}_i,\,\mathbf{y}\rangle^2}\pm\epsilon$. We can only decompose the latter part and obtain the proposed result.
\end{proof}


\section{Line-search method}
In this section, we firstly make some definition of notations:

\begin{table}[!ht]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Symbol} & \textbf{Explanation} \\ 
\midrule
$\mathbf{X}$     & Dataset pair ($\mathbf{x}$, $\mathbf{y}$) \\
$\theta^t\in\Theta$     &Parameters of MLPs at iteration t\\
$\theta^t_I\in\Theta$     &Parameters of INRs at iteration t \\
$\theta^t_A\in\Theta$     &Parameters of filters at iteration t \\
$\theta^*\in\Theta$     &The optimal parameters of MLPs \\
$\alpha$         & Learning rate \\ 
$\alpha_I$         & Learning rate of INRs\\ 
$\alpha_A$         & Learning rate of filters\\ 
$\phi(\cdot)$  & Loss function that can depend on $\alpha$, \\&$\mathbf{X}$ and $\theta$\\
$\nabla $       & Gradient operator \\ 
$\mathbf{p}^t$ & The update direction at iteration t\\
$\mathbf{p}^t_I$ & The update direction of INRs at iteration t\\
$\mathbf{p}^t_A$ & The update direction of filters at iteration t\\
\bottomrule
\end{tabular}
\label{tab:notations}
\end{table}

Considering a minimization problem as the following:
\begin{equation*}
\begin{aligned} \label{P}
&\theta^* = \min_{\theta\in\Theta}\phi(\theta) \\
\end{aligned}
\end{equation*}
 One common method to find the $\theta^*\in\Theta$ that minimizes $\phi(\cdot)$ is to use the gradient descent method as shown in the Algorithm \ref{GD}.

\begin{algorithm}
\caption{Gradient Descent Algorithm}
\begin{algorithmic}[1] % [1]表示行号从1开始
    \STATE \textbf{Initialize:} variables $\theta^0$, max iteration $N$, learning rate $\alpha$
    \FOR{$i \gets 1$ to $N$}
        \STATE Calculate the derivative of $\phi(\theta^{t-1})$ about $\theta^{t-1}$ as direction denotes as $\mathbf{p}^t$
        \STATE $\theta^t\gets\theta^{t-1}+\alpha\mathbf{p}^t$
    \ENDFOR
\end{algorithmic}
\label{GD}
\end{algorithm}

Based on this Gradient Descent method, the line-search method is to find the proper learning rate $\alpha_t$ at each iteration by optimization to solve the approximate learning rate or exact learning rate if possible. The algorithm can be shown as the Algorithm \ref{LS}.
\begin{algorithm}
\caption{Line-search Method}
\begin{algorithmic}[1] % [1]表示行号从1开始
    \STATE \textbf{Initialize:} variables $\theta^0$, max iteration $N$
    \FOR{$i \gets 1$ to $N$}
        \STATE Calculate the derivative of $\phi(\theta^{t-1})$ about $\theta_{t-1}$ as direction denotes as $\mathbf{p}^t$
        \STATE $\alpha=arg\,min_{\alpha}\phi(\theta^{t-1}+\alpha \mathbf{p}^t)$ 
        \STATE $\theta^t\gets\theta^{t-1}+\alpha\mathbf{p}^t$
    \ENDFOR
\end{algorithmic}
\label{LS}
\end{algorithm}
\subsection{Details of Custorm Line-Search Algorithm}
\begin{figure*}[!ht]
    \centering
    \includegraphics[page=1, width=.95\textwidth]{Image/www.pdf}
    \caption{The blue line is the optimization target, while the orange lines indicate the predefined learning rate bounds, denoted as $\alpha_{\text{min}}$ and $\alpha_{\text{max}}$. $\mathbf{p}^t_A$ and $\mathbf{p}^t_I$ are the update directions for the filter and INRs, respectively. $\alpha^*$ is the optimal value and $\epsilon$ is a constant for robustness, usually set to $1 \times 10^{-6}$.
}
    \label{figure:line-search}
\end{figure*}
\label{section:LSA}
In this section, we will explain the derivation of the modified line-search algorithm used to determine the learning rate of the adaptive filter.

Let $\phi(\theta^t_A,\theta^t_I)$ denote the loss function at iteration $t$, $\mathbf{p}^t_A$ as the update direction for the adaptive filter, and $\mathbf{p}^t_I$ as the update direction for the INRs. We can then perform a Taylor expansion around the parameters $(\theta^{t-1}_A, \theta^{t-1}_I)$, expressed as follows:
\begingroup
\scriptsize
\[
\phi(\theta^{t}_A, \theta^{t}_I) = \phi(\theta_A^{t-1}, \theta_I^{t-1}) - \nabla_{\theta_A^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top (\theta_A^t - \theta_A^{t-1})\]\[ - \nabla_{\theta_I^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top (\theta_I^t - \theta_I^{t-1})
\]
\[
+ \frac{1}{2} \left[ (\theta_A^t - \theta_A^{t-1})^2 \triangle_{\theta_A^t} \phi(\theta_A^{t-1}, \theta_I^{t-1}) + (\theta_I^t - \theta_I^{t-1})^2\triangle_{\theta_I^t} \phi(\theta_A^{t-1}, \theta_I^{t-1}) \right]\]
\[+\mathcal{O}((\theta_I^t - \theta_I^{t-1})^2, (\theta_A^t - \theta_A^{t-1})^2)
\]
\endgroup
Using the gradient descent method, we have $\theta^{t} = \theta^{t-1} + \alpha p^{t-1}$. Since the ReLU activation function results in the second and higher-order derivatives being zero, the equation simplifies to:
\begingroup
\scriptsize
\[
\phi(\theta^{t}_A, \theta^{t}_I) \approx \phi(\theta_A^{t-1}, \theta_I^{t-1}) - \nabla_{\theta_A^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top (\alpha_{A}\mathbf{p}^{t-1}_A)\]\[ - \nabla_{\theta_I^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top (\alpha_{I}\mathbf{p}^{t-1}_I)
\]
\[
\Rightarrow\phi(\alpha_A) = \phi(\theta^{t}_A, \theta^{t}_I) \approx k\alpha_A+b
\]
\[\mathbf{where}\,\, k = -\nabla_{\theta_A^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top \mathbf{p}^{t-1}_A\]\[\mathbf{and}\, b =  \phi(\theta_A^{t-1}, \theta_I^{t-1}) - \nabla_{\theta_I^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top (\alpha_{I}\mathbf{p}^{t-1}_I)\]
\endgroup
Since the learning rate of the INRs part is known, this can be simplified as a linear optimization problem with only an order 1 unknown parameter $\alpha^t_A$.
\begingroup
\scriptsize
\begin{equation*}
    \begin{split}
&\arg\min_{\alpha_A} \phi(\theta^{t}_A, \theta^{t}_I) \approx\\ &\arg\min_{\alpha_A} \phi(\theta_A^{t-1}, \theta_I^{t-1}) - \nabla_{\theta_A^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top (\alpha_{A}\mathbf{p}^{t-1}_A)\\
&- \nabla_{\theta_I^t} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top (\alpha_{I}\mathbf{p}^{t-1}_I)
    \end{split}
\end{equation*}

\endgroup
To mitigate the impact of a small denominator, a constant $\epsilon=1\times10^{-6}$ is introduced, ensuring the robustness of the algorithm. The solution to this optimization problem is derived through a case-based analysis by treating the loss function as a function of $\alpha_A$, since $\theta^{t}_A$ is expressed as a function of $\theta^{t-1}_A$ and $\alpha_A$. Specifically, the loss function, denoted as $\phi(\alpha_A) = \phi(\theta^{t}_A, \theta^{t}_I)$. The analysis involves examining the sign of the slope and intercept of the loss function, as depicted in \autoref{figure:line-search}.

The overall algorithm pipeline is shown in the following Algorithm \ref{alg:1}
 % 使用较小字体，您也可以尝试 \small 或 \footnotesize

\begin{algorithm}[!ht]
\caption{Line-search Method-Relative Learning Rate}
\begin{algorithmic}[1] 
\label{alg:1}% [1]表示行号从1开始
    \STATE \textbf{Initialize:} variables $\theta_0$, max iteration $N$, $\alpha_A$, $\alpha_I$, $\alpha_{max}$, $\alpha_{min}$, $\epsilon$, $c_1$
    \FOR{$i \gets 1$ to $N$}
        \STATE Calculate the update direction as $\mathbf{p}^t_A$ and $\mathbf{p}^t_I$
        \STATE Calculate the partial derivative of $\phi(\theta^{t-1}_A)$ about $\theta^{t-1}_A$ as direction denotes as $\nabla_{\theta^{t-1}_A} \phi$
        \STATE Calculate the partial derivative of $\phi(\theta^{t-1}_I)$ about $\theta^{t-1}_I$ as direction denotes as $\nabla_{\theta^{t-1}_I} \phi$
        \STATE $k\gets-\nabla_{\theta_A^{t-1}} \phi(\theta_A^{t-1}, \theta_I^{t-1})^\top \mathbf{p}^{t-1}_A+\epsilon$
        \STATE $b\gets \phi(\theta_A^{t-1}, \theta_I^{t-1}) -\alpha_I \nabla_{\theta^{t-1}_I} \phi^{\top}\mathbf{p}^t_I$
        \IF{$a\ge0, b\leq0$}
        \STATE $\alpha_A\gets\alpha_{min} $
        \STATE $\mathbf{Armijo}$($c_1, \nabla_{\theta^{t-1}_A} \phi^{\top}\mathbf{p}^t_A,\nabla_{\theta^{t-1}_I} \phi^{\top}\mathbf{p}^t_I$)
        \ELSIF{$a\ge0, b\geq0$ OR $a\le0, b\leq0$}
        \STATE $\alpha_A\gets Clip\left[|\frac{-b}{k}|, \alpha_{min}, \alpha_{max}\right] $
        \STATE $\mathbf{Armijo}$($c_1, \nabla_{\theta^{t-1}_A} \phi^{\top}\mathbf{p}^t_A,\nabla_{\theta^{t-1}_I} \phi^{\top}\mathbf{p}^t_I$)
        \ELSE
        \STATE $\alpha_A\gets\alpha_{min} $
        \STATE $\mathbf{Armijo}$($c_1, \nabla_{\theta^{t-1}_A} \phi^{\top}\mathbf{p}^t_A,\nabla_{\theta^{t-1}_I} \phi^{\top}\mathbf{p}^t_I$)
        \ENDIF
        \STATE $\theta^t_A\gets\theta^{t-1}_A+\alpha_A \mathbf{p}^t_A$
        \STATE $\theta^t_I\gets\theta^{t-1}_I+\alpha_I \mathbf{p}^t_I$
    \ENDFOR
\end{algorithmic}
\end{algorithm}



To ensure sufficient decrease, we also apply a similar derivation based on the Armijo condition, which is commonly used in line-search algorithms to guarantee sufficient decrease. The Armijo condition is typically expressed as follows:
\[
\phi(\theta^{t} + \alpha \mathbf{p}^{t}) \leq \phi(\theta^t) + c_1 \alpha \mathbf{p}^{t\top} \nabla \phi(\theta^{t}),
\]
Where $c_1$ typically takes the value $1 \times 10^{-3}$. By assuming the current step is $t$ (which is equal to the previous derivation's \( t-1 \), but we denote it as \( t \) for simplicity), this can be written as:
\begingroup
\scriptsize
\[
\phi(\theta^{t}_A +\alpha_A \mathbf{p}^t_A, \theta^{t}_I+\alpha_I\mathbf{p}^t_I)\]
\[\leq \phi(\theta^{t}_A, \theta^{t}_I) + c_1 \alpha_A \nabla_{\theta^{t}_A} \phi^{\top}\mathbf{p}^t_A + c_1\alpha_I \nabla_{\theta^{t}_I} \phi^{\top}\mathbf{p}^t_I
\]
\endgroup
Therefore, using a similar Taylor expansion on the loss function with respect to the parameters and algorithm is shown in Algorithm.\ref{alg:armijo}:
\begingroup
\scriptsize
\[
 \phi(\theta_A^{t}, \theta_I^{t}) - \nabla_{\theta_A^t} \phi(\theta_A^{t}, \theta_I^{t})^\top (\alpha_{A}\mathbf{p}^t_A) - \nabla_{\theta_I^t} \phi(\theta_A^{t}, \theta_I^{t})^\top (\alpha_{I}\mathbf{p}^t_I)
 \]
 \[
 \leq \phi(\theta^{t}_A, \theta^{t}_I) + c_1 \alpha_A p_k^\top \nabla_{\theta^{t}_A} \phi^{\top}\mathbf{p}^t_A + c_1\alpha_I \nabla_{\theta^{t}_I} \phi^{\top}\mathbf{p}^t_I
\]
\[\Rightarrow
(c_1-1)\alpha_I\nabla_{\theta^{t}_A}\phi^{\top}\mathbf{p}^t_A \geq (1-c_1)\nabla_{\theta^{t}_I} \phi^{\top}\mathbf{p}^t_I
\]
\endgroup
\begin{algorithm}[!ht]
    \caption{Armijo Condition Check (refered as Armijo)}
\begin{algorithmic}
    \STATE \textbf{Initialize:} variables $c_1$,$\nabla_{\theta^{t-1}_A} \phi^{\top}\mathbf{p}^t_A$, $\nabla_{\theta^{t}_I} \phi^{\top}\mathbf{p}^t_I$, current step t
    \IF{$(c_1-1)\nabla_{\theta^{t}_A} \phi^{\top}\mathbf{p}^t_A > (1-c_1)\nabla_{\theta^{t}_I} \phi^{\top}\mathbf{p}^t_I$}
        \IF{$\nabla_{\theta^{t}_I} \phi^{\top}\mathbf{p}^t_I\times\nabla_{\theta^{t-1}_A} \phi^{\top}\mathbf{p}^t_A>0$}
        \STATE \textbf{return} $\alpha_A\gets\frac{\nabla_{\theta^{t}_I} \phi^{\top}\mathbf{p}^t_I}{\nabla_{\theta^{t}_A} \phi^{\top}\mathbf{p}^t_A}$
        \ELSE
        \STATE \textbf{return} $\alpha_A\gets\alpha_{min}$
        \ENDIF 
    \ELSE        
    \STATE \textbf{return} $\mathbf{None}$
    \ENDIF
\end{algorithmic}
\label{alg:armijo}
\end{algorithm}

\clearpage
\subsection{Visualization of the Final Output of Filters}
\label{sec:visual_output}
In this section, we further present the results of the filtered Positional Encoding embedding as shown in \autoref{fig:sup1} and \autoref{fig:sup2}. Compared to Random Fourier Features, which involve more complex combinations of frequency components, Positional Encoding displays more regular frequency patterns, making it better suited for visualization. These visualizations demonstrate that in low-frequency regions, the high-frequency embeddings are effectively suppressed by the filter, in line with our expectations of the adaptive linear filter's behavior. Additionally, for low-frequency embeddings, the filter can also emphasize high-frequency components, enabling more fine-grained outputs.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.475\textwidth]{Image/filtered_input1_cropped.pdf}
    \caption{Visualization of the filtered embedding for image 804 in the DIV2K validation split.}
    \label{fig:sup1}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.475\textwidth]{Image/filtered_input2_cropped.pdf}
    \caption{Visualization of the filtered embedding for image 814 in the DIV2K validation split.}
     \label{fig:sup2}
\end{figure}
