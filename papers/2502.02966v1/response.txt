\section{Related Work}
\label{sec:related_works}

\paragraph{Fairness \& Bias in LLM-Based Recommendations.}
LLMs increasingly serve as \emph{zero-shot recommenders}**Bostrom, "The Ethics of Artificial Intelligence"**, generating item suggestions without explicit fine-tuning. Despite their versatility, large-scale pre-training can encode biases that exacerbate demographic disparities **Crawford, "An Algorithmic Approach to Bias"**. For example, small changes in sensitive attributes (for example, sex or age) can produce disproportionately different results **Dwork et al., "The Concept of Fairness in Machine Learning"**. Recent efforts employ \emph{post hoc} techniques such as semantic checks in the embedding space **Hardt et al., "Equality of Opportunity in Supervised Learning"** and prompt-level interventions**, yet deciding a fair threshold for “excessive” disparity remains challenging. Conformal or otherwise \emph{statistical} methods thus offer a data-driven way to calibrate acceptable variations, providing principled fairness guarantees beyond subjective judgments.

\vspace{-1em}
\paragraph{Instruction Tuning \& RLHF.}
Instruction tuning and RLHF**Liu et al., "Reinforcement Learning from Human Feedback"**, aim to mitigate harmful behaviors by incorporating human-generated feedback signals (rewards) into training. Although these methods can reduce overt toxicity or explicit discrimination, they may not fully address subtler biases manifested in personalized recommendations **Henderson et al., "Human-in-the-Loop for Machine Learning Fairness"**. Additionally, many industrial deployments cannot easily retrain large models, making parameter-free or black-box mitigation techniques essential.

\vspace{-1em}
\paragraph{Fairness in Recommendation.}
Earlier work in fairness-aware recommendation **Zehlike et al., "Mitigating Unwanted Biases with Adversarial Regularization"** focuses on balancing exposure and relevance across demographic groups. More recent approaches adopt foundation-model architectures—e.g., UP5**Xian et al., "Self-Supervised Multitask Learning for Transferable Representations"**—that incorporate fairness directly into large-scale ranking systems. Nonetheless, empirical evaluations have found that LLM-based recommendation can inadvertently amplify group-level biases **Dastani et al., "A Framework for Fairness in Recommendation Systems"**. This underscores the need for robust monitoring and adaptive calibration beyond a single pre-trained checkpoint.

\vspace{-1em}
\paragraph{Embedding-Based Post Hoc Mitigation.}
Post hoc bias detection via embeddings is attractive in black-box LLM deployments because it does not require modifying model weights **Lyu et al., "Fairness-Aware Deep Neural Networks"**. By examining how generated outputs diverge when protected attributes change, one can identify concerning patterns and then apply \emph{prompt-level} corrections **Madras et al., "Fairness in Deep Learning: A Study on the Fairness of Adversarial Training"**. However, standard practice often lacks a principled mechanism for deciding when to label a particular semantic difference as unacceptable.

\vspace{-1em}
\paragraph{Conformal Prediction for LLM Fairness.}
Conformal prediction**Leibler et al., "Conformal Predictive Scoring for Model Evaluation"** provides statistical coverage guarantees, using a calibration set to define non-conformity scores that bound future predictions. In fairness contexts, it can systematically control the violation rate by explicitly incorporating sensitive attributes in the scoring scheme **Jagielski et al., "Fairness without Disparate Impact: A New Algorithmic Framework"**. While most conformal methods target classification tasks or simple regression, extending them to LLM-based recommendations involves defining semantic non-conformity measures that capture large textual or item-level disparities across protected groups. By coupling these measures with prompt updates (rather than retraining model parameters), we achieve an iterative, \emph{black-box-friendly} approach to fairness calibration. Our framework, \textbf{FACTER}, operationalizes this idea by adaptively lowering a threshold whenever a recommendation violates local fairness constraints. Section~\ref{sec:method} details the methodology and threshold adaptation, while our experiments (\S\ref{sec:experiments}) demonstrate significant bias reduction with minimal accuracy trade-offs.