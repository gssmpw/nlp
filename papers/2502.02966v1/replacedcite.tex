\section{Related Work}
\label{sec:related_works}

\paragraph{Fairness \& Bias in LLM-Based Recommendations.}
LLMs increasingly serve as \emph{zero-shot recommenders}____, generating item suggestions without explicit fine-tuning. Despite their versatility, large-scale pre-training can encode biases that exacerbate demographic disparities ____. For example, small changes in sensitive attributes (for example, sex or age) can produce disproportionately different results ____. Recent efforts employ \emph{post hoc} techniques such as semantic checks in the embedding space ____ and prompt-level interventions____, yet deciding a fair threshold for “excessive” disparity remains challenging. Conformal or otherwise \emph{statistical} methods thus offer a data-driven way to calibrate acceptable variations, providing principled fairness guarantees beyond subjective judgments.

\vspace{-1em}
\paragraph{Instruction Tuning \& RLHF.}
Instruction tuning and RLHF____ aim to mitigate harmful behaviors by incorporating human-generated feedback signals (rewards) into training. Although these methods can reduce overt toxicity or explicit discrimination, they may not fully address subtler biases manifested in personalized recommendations ____. Additionally, many industrial deployments cannot easily retrain large models, making parameter-free or black-box mitigation techniques essential.

\vspace{-1em}
\paragraph{Fairness in Recommendation.}
Earlier work in fairness-aware recommendation ____ focuses on balancing exposure and relevance across demographic groups. More recent approaches adopt foundation-model architectures—e.g., UP5____—that incorporate fairness directly into large-scale ranking systems. Nonetheless, empirical evaluations have found that LLM-based recommendation can inadvertently amplify group-level biases ____. This underscores the need for robust monitoring and adaptive calibration beyond a single pre-trained checkpoint.

\vspace{-1em}
\paragraph{Embedding-Based Post Hoc Mitigation.}
Post hoc bias detection via embeddings is attractive in black-box LLM deployments because it does not require modifying model weights ____. By examining how generated outputs diverge when protected attributes change, one can identify concerning patterns and then apply \emph{prompt-level} corrections ____. However, standard practice often lacks a principled mechanism for deciding when to label a particular semantic difference as unacceptable.

\vspace{-1em}
\paragraph{Conformal Prediction for LLM Fairness.}
Conformal prediction____ provides statistical coverage guarantees, using a calibration set to define non-conformity scores that bound future predictions. In fairness contexts, it can systematically control the violation rate by explicitly incorporating sensitive attributes in the scoring scheme ____. While most conformal methods target classification tasks or simple regression, extending them to LLM-based recommendations involves defining semantic non-conformity measures that capture large textual or item-level disparities across protected groups. By coupling these measures with prompt updates (rather than retraining model parameters), we achieve an iterative, \emph{black-box-friendly} approach to fairness calibration. Our framework, \textbf{FACTER}, operationalizes this idea by adaptively lowering a threshold whenever a recommendation violates local fairness constraints. Section~\ref{sec:method} details the methodology and threshold adaptation, while our experiments (\S\ref{sec:experiments}) demonstrate significant bias reduction with minimal accuracy trade-offs.