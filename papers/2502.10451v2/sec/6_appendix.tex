\NewDocumentCommand{\csq}{om}{%
  % colored square
  % #1 = color model (optional)
  % #2 = color spec
  \IfNoValueTF{#1}{%
    \textcolor{#2}{\rule{1.2ex}{1.2ex}}%
  }{%
    \textcolor[#1]{#2}{\rule{1.2ex}{1.2ex}}%
  }%
}

\clearpage

\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0}
\renewcommand{\thesection}{A\arabic{section}}
\setcounter{section}{0}
\maketitlesupplementary

%\onecolumn
The supplementary material presents the following sections to strengthen the main manuscript:

\begin{itemize}
\item Pseudocode of our algorithm.
\item Implementation details.
\item Dynamic route exploration.
\item More visualization
\end{itemize}

\section{Pseudocode of Our Algorithm}
\label{app:pseudo}

\begin{algorithm}[!ht]
    \caption{Inference procedure}
    \label{alg:inference}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    %\renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE conditional image $\mathbf{c}_{s}$, text prompt $\mathbf{c}_{t}$, timestep $T$. \\ Fully-trained FlexControl, pre-trained SD model.%%input
        %\ENSURE out    %%output
        \FOR{each $i \in [T,1]$}
            \FOR{each $l \in Blocks$}
               \STATE \textcolor{magenta}{/$^{*}$ The value of $\mathcal{M}_{l}$ is adjusted with input $\mathbf{h}_{l-1}$ $^{*}$/}
               \STATE Compute $\mathcal{M}_{l}$ though router unit
                \IF{$\mathcal{M}_{l}=1$}
                   \STATE \textcolor{magenta}{/$^{*}$ Extract latent features from conditions $^{*}$/}
                   \STATE Compute $\mathbf{h}_{l}$ though \cref{eq:forward_backward_1} 
                   \STATE \textcolor{magenta}{/$^{*}$ Feature transformation by zero modules $^{*}$/}
                   \STATE Transform $\mathbf{h}_{l}$ to $\mathbf{y}_{c}^{l}$ though \cref{eq:forward_backward_2}
                   \STATE \textcolor{magenta}{/$^{*}$ Inject modal information into feature space $^{*}$/}
                   \STATE Inject $\mathbf{y}_{c}^{l}$ to SD model though \cref{eq:inject_conditional_controls}
                \ELSE
                   \STATE \textcolor{magenta}{/$^{*}$ Align the dimension of feature mapping $^{*}$/}
                   \STATE Bypass $\mathcal{F}_{l}$ and $\mathcal{Z}_{l}$ though $\mathrm{skip}_{l}\left(\cdot\right)$  %\Comment{This is a comment}
                \ENDIF
            \ENDFOR
            \STATE \textcolor{magenta}{/$^{*}$ DDIM sampler for SD1.5-based models $^{*}$/}
            \STATE \textcolor{magenta}{/$^{*}$ RFlow sampler for SD3.0-based models $^{*}$/}
            \STATE Predict denoised image with $T$-step sampling
        \ENDFOR
        \STATE \textbf{return} $\mathbf{x}_{0}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \caption{Training procedure}
    \label{alg:training}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE Dataset $\mathcal{D}\left(x,\mathbf{c}_{s},\mathbf{c}_{t}\right)$, hyperparameters $\left(\tau,\gamma,\lambda_{\mathbf{C}}\right)$. \\ Initialized FlexControl model, pre-trained SD model.%%input
        %\ENSURE out    %%output
        \STATE Turn off the router units for warm-up training
        \STATE Turn on the router units for end-to-end training
        \WHILE{not converged}
            \STATE Sample timestep $t\sim Uniform\left(\mathbf{0},\mathbf{1}\right)$
            \STATE Sample nosie $\epsilon\sim\mathcal{N}\left(\mathbf{0},\boldsymbol{\mathit{I}}\right)$
            \STATE \textcolor{magenta}{/$^{*}$ \cref{eq:ddpm_forward} is used for SD1.5-based models $^{*}$/}
            \STATE \textcolor{magenta}{/$^{*}$ \cref{eq:fm_forward} is used for SD3.0-based models $^{*}$/}
            \STATE Transfer image $\mathbf{x}_{0}$ to noisy image $\mathbf{x}_{t}$
            \STATE \textcolor{magenta}{/$^{*}$ Based on \cref{eq:redefined_forward_backward_1,eq:redefined_forward_backward_2,eq:inject_conditional_controls} $^{*}$/}
            \STATE $\mathbf{y}_{pred},\mathcal{M}=\hat{\epsilon}_{\theta}\left(\mathbf{x}_{t},\mathbf{c}_{t},\mathbf{c}_{s},t\right)$
            \STATE \textcolor{magenta}{/$^{*}$ $\mathcal{L}_{\mathbf{SD}}$ is used to optimize generation effect $^{*}$/}
            \STATE Compute MSE loss $\mathcal{L}_{\mathbf{SD}}$ though \cref{eq:losssd}
            \STATE \textcolor{magenta}{/$^{*}$ $\mathcal{L}_{\mathbf{C}}$ is used to control sparsity $^{*}$/}
            \STATE Compute cost loss $\mathcal{L}_{\mathbf{C}}$ though \cref{eq:losscost}
            \STATE \textcolor{magenta}{/$^{*}$ $\mathcal{L}_{\theta}$ is used as final optimization goal $^{*}$/}
            \STATE Compute final loss $\mathcal{L}_{\theta}$ though \cref{eq:losstheta}
            \STATE \textcolor{magenta}{/$^{*}$ Freeze the weight parameters of the backbone $^{*}$/}
            \STATE $\theta=\theta-lr\nabla_{\theta}\mathcal{L}_{\theta}\left(\mathbf{x}_{t},\mathbf{y}_{pred},\mathcal{M}\right)$
        \ENDWHILE
        \STATE \textbf{return} fully-trained FlexControl
    \end{algorithmic}
\end{algorithm}

\section{Implementation Details}
We implement FlexControl based on SD1.5 \cite{SD1.5} and SD3.0 \cite{SD3}. The experiments are carried out under various conditions, mainly including depth map, canny edge and segmentation mask. The following is a description of the experimental details.

\paragraph{Training dateset.}
The experiment involves three types of conditional maps:
\begin{itemize}
\item[$\bullet$] \textit{Depth map.} In this application, we use MultiGen-20M proposed by \cite{zhao2024uni} as training data, which is a subset of LAION-Aesthetics \cite{schuhmann2022laion} and contains over 2 million depth-image-caption pairs, and 5K test samples.
\item[$\bullet$] \textit{Canny edge.} For the condition of the canny edge, we use the LLAVA-558K \cite{liu2024visual} dataset to verify the model, which contains 558K image-caption pairs. A canny edge detector \cite{canny1986computational} is used to convert RGB images to edge images, and the low and high threshold of hysteresis procedure in this process are set to 100 and 200, respectively. 
\item[$\bullet$] \textit{Segmentation mask.} For the segmentation mask, we use the ADE20K \cite{zhou2017scene} dataset for model training. This dataset contains a total of 27K segmentation image pairs, 25K for training and 2K for testing. InternVL2-2B \cite{chen2024internvl} is used to generate captions for RGB images with instruction ``Please use a brief sentence with as few words as possible to summarize the picture‚Äù.
\end{itemize}

\paragraph{Training settings.}
During the training procedure, we uniformly use the AdamW optimizer with a learning rate of 1$\times$10$^{-5}$. For SD1.5-based models, half-precision floating-point (Float16) is used for mixed precision training, original images and conditional images are resized to 512$\times$512, and batchsize and gradient accumulation steps are set to 4 and 32, respectively. When turning to SD3.0, we further use DeepSpeed \cite{rajbhandari2020zero} Zero-2 to accelerate the training process, the resolution of 1024$\times$1024 is used, and the batchsize and gradient accumulation steps are set to 4 and 8. We set the maximum training iterations to 50k and 25k for the models based on SD1.5 and SD3.0, respectively. For the threshold parameter $\tau$ required by the Gumbel-Sigmoid activation function in the router unit, we set it to 0.5, and the hyperparameter $\lambda_{\mathbf{C}}$ in the loss function $\mathcal{L}_{\theta}$ is set to 0.5, the value of $\gamma$ depends on the target sparsity. When training UNet-based ControlNet-large and FlexControl, we remove the residual connection between the encoder blocks and the decoder blocks of the control network. For the problem of the weight dimension cannot be aligned when initializing the decoder blocks of the control network using SD1.5's pre-trained weights caused by this operation, we solve it by reinitializing these weights. The models based on SD1.5 and SD3.0 are trained with 2 and 8 Nvidia-A100 (40G) GPUs, respectively.

Our FlexControl follows the core design philosophy of \cite{zhang2023adding}, the trainable blocks are initialized with the pre-trained weight parameters of the SD model, and zero modules are added at the same time, which leads to the conditional mappings generated at the early training stage do not have the ability to control generation effectively. Therefore, we first fix mask $\mathcal{M}$ to 1 for warm-up training in the early training stage, \textit{e.g.}, 10K steps for SD1.5-based FlexControl and 5K steps for SD3-based FlexControl in our implementation, and then turn on the router unit to train together with the copy blocks. This helps the whole training procedure move in the right direction.

\paragraph{Benchmark and metrics.}
For quantitative comparison, we present the Frechet Inception Distance (FID) \cite{heusel2017gans} and CLIP\_score \cite{radford2021learning} to assess the quality of the generated images. In addition, we calculate RMSE, SSIM and mIoU on depth map, canny edge and segmentation mask respectively, to evaluate the controllability of image generation. Finally, we emphatically compare the computational complexity. The results of depth map are tested on MultiGen-20M test set and the results of canny edge and segmentation mask are tested on COCO validation set, which contains 5,000 samples and each sample contains five text descriptions, we randomly choose one text of each sample as input during testing. For sampling, we employ DDIM \cite{song2020denoising} and RFlow \cite{SD3} sampler, implementing 20 denoising steps to generate images without incorporating any negative prompts. We generate five groups of images, and the average results are reported.
%RFlow with dynamic rectification strategy

%\section{More Experiments about Dynamic Route}
\section{Dynamic Route Exploration}
\label{app:exploration}

In order to improve the parameter utilization of ControlNet in the application, we explore how the router unit activates the control block to generate conditional controls.

It can be seen from \cref{fig1}(c), both SD1.5 based on UNet and SD3.0 based on DiT, the activation of control blocks presents a sparse distribution in the early denoising stage and a dense distribution in the late stage. This means that the late denoising stage plays a more important role in controllable image generation. Since the early sampling is mainly responsible for generating the global structure and low-frequency information of the image (\textit{e.g.}, the approximate shape of the object, the distribution of the components), while the late sampling is mainly responsible for generating high-frequency information and correcting complex details (\textit{e.g.}, edge, texture). For this, more conditional control signals are necessary. Moreover, the generation deviation in the early stage is relatively small and can be rectified by subsequent sampling. If the sampling error in the later stage is large, the conditional consistency will be destroyed, resulting in loss of control effect. 

Based on the above findings, we can conclude that using unified control scheme in any case is an inefficient control mode, which leads to most of the conditional controls added in the early stage not playing the ideal role, and there will be insufficient conditional controls added in the late stage. Therefore, our dynamic control method can further release the performance of the controllable generation model by solving this problem. Next, we do a more detailed analysis of the different settings.

First, we test the activation time and position of the control block under different number of timestep settings with $\gamma$ set to 0.5 (\textit{i.e.}, approximately 50\% sparsity). We set the timesteps to 10, 20 and 50 respectively. As shown in \cref{inf_steps}, it can be found that the pattern under different settings is basically the same as above. In the early stage, only a few blocks are activated, mainly concentrated in the head and tail. As the number of sampling steps increases, more blocks are activated. Until the middle stage of sampling, most of the blocks are activated.

Next, we test the activation of control blocks under different sparsity. We approximate 30\%, 50\%, and 70\% sparsity by setting $\gamma$ to 0.3, 0.5, and 0.7. It can be seen from \cref{act_inf_steps}, when 30\% sparsity is used, fewer blocks are activated at the late stage, and even some control blocks are not activated at all. When the sparsity increased to 70\%, more middle blocks are activated in the early sampling period, and almost all blocks are activated in the late sampling period.

In addition, we discuss the activation of various spatial conditions at each timestep. As shown in \cref{con_inf_steps}, similar trend is found across different types of conditional maps, which proves the generalization of the above findings. In addition, there are some differences in activation details, which means that the router unit makes independent judgments on different conditional samples and plans specific activation routes for them. Due to the differences in feature distribution and information in the samples, this fine-grained control is particularly important for striking a balance between performance and efficiency.

Relying on the above findings, when we apply ControlNet or similar architectures in practice, only activating the head and tail blocks in the early stage, or even activating ControlNet only in the late stage, can simply improve the inference efficiency, and no retraining is involved.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{pic/inf_various_ts.png}
    \caption{\textbf{The distribution of activated control blocks under different timesteps.} The hyperparameter $\gamma$ is set to 0.5 to approximate 50\% sparsity, and timestep is set to 10, 20 and 50 repectively. The first line shows the results of the model based on SD1.5, and the second line shows the results of the model based on SD3.0. $\large\csq[HTML]{665E9A}$ and $\large\csq[HTML]{BBE0E3}$ denotes activated and inactivated blocks, respectively.}
    \label{inf_steps}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{pic/act_inf_steps.png}
    \caption{\textbf{The distribution of activated control blocks under different sparsity.} The hyperparameter $\gamma$ is set to 0.3, 0.5 and 0.7 to approximate 30\%, 50\%, and 70\% sparsity, and the timestep is set to 20. The first line shows the results of the model based on SD1.5, and the second line shows the results of the model based on SD3.0.}
    \label{act_inf_steps}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{pic/con_inf_steps.png}
    \caption{\textbf{The distribution of activated control blocks under various conditional controls.} The hyperparameter $\gamma$ is set to 0.5 to approximate 50\% sparsity, and the timestep is set to 20.}
    \label{con_inf_steps}
\end{figure*}

\section{More Visualization}

\begin{figure*}[!t]
    \centering
	\includegraphics[width=1.0\textwidth]{pic/app_all_conditions.png}
	\caption{\textbf{Visualization comparison} with state-of-the-art controllable generation methods on various conditions. Except for the last two columns, the other models use SD1.5 as the backbone. \textit{Captions: A white stallion horse galloping furiously kicking up the dust behind it. Ingredients of curry, including onions, garlic, chili, and tomatoes. A group of people are observing an aquarium filled with colorful fish.}}
    \label{app_all_conditions}
\end{figure*}

\begin{figure*}[!t]
    \centering
	\includegraphics[width=1.0\textwidth]{pic/app_seg_mask.png}
	\caption{\textbf{Visualization comparison} of FlexControl and existing methods on SD1.5 for semantic consistency. \textit{Captions: A reddish rose in a vase filled with water on the table.}}
    \label{app_seg_mask}
\end{figure*}

\begin{figure*}[!t]
    \centering
	\includegraphics[width=1.0\textwidth]{pic/app_canny.png}
	\caption{\textbf{Visualization comparison} of FlexControl and existing methods on SD3.0 for edge preservation. \textit{Captions: A wooden chair with a striped cushion, a navy blue tote bag with anchors, and a potted plant are arranged on a white floor against a white wooden wall.}}
    \label{app_canny}
\end{figure*}