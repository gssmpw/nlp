\section{Related Work}
\label{sec:formatting}
%-------------------------------------------------------------------------
\paragraph{Text-to-image diffusion models.}
The diffusion probabilistic model was originally introduced by Sohl-Dickstein \textit{et al.} \cite{sohl2015deep}, which has been successfully applied in the field of image synthesis and achieved impressive results \cite{dhariwal2021diffusion, kingma2021variational, huang2023composer, huang2023reversion, jiang2023avatarcraft, ren2022image}. The Latent Diffusion Models (LDMs) \cite{rombach2022high}, reduce computational demands by transferring the diffusion process from the pixel space to the latent feature space. Such diffusion models \cite{SD1.5, nichol2021glide, podell2023sdxl, rombach2022high, saharia2022photorealistic} typically encode text prompts as potential vectors through pre-trained language models \cite{radford2021learning, raffel2020exploring}, combined with UNet \cite{ronneberger2015u} to predict noise to remove at each timestep. Recent studies explore Transformer-based architectures, which have yielded state-of-the-art results for large-scale text-to-image generation tasks \cite{bao2023all, bao2023one, peebles2023scalable, tu2022maxvit, esser2024scaling}, These frameworks leverage Transformers’ capacity for modeling long-range dependencies and scaling to massive multimodal datasets, enabling breakthroughs in compositional reasoning, dynamic resolution adaptation, and high-fidelity synthesis. However, their reliance on purely textual input—despite advances in cross-modal alignment—still poses challenges for precise spatial or stylistic control.

\paragraph{Controllable diffusion models.}
While state-of-the-art text-to-image models achieve remarkable photorealism, their reliance on inherently low-bandwidth, abstract textual input limits their ability to meet the nuanced and complex demands of real-world artistic and design applications. This underscores the growing need for frameworks like ControlNet \cite{zhang2023adding} and T2I-Adapter \cite{mou2024t2i}, which augment text prompts with spatial or structural constraints (\textit{e.g.}, sketches, depth maps, or poses), enabling finer-grained control over generation to bridge the gap between creative intent and algorithmic output. Recent advancements in controllable text-to-image generation have diversified across methodological approaches. Instance-based methods, such as those by \cite{wang2024instancediffusion, zhou2024migc} enable zero-shot generation of stylized images from a single reference input, prioritizing speed and flexibility. Meanwhile, an improvement in cross-attention constraint, proposed by \cite{chen2024training}, guides generation along desired trajectories by refining latent space interactions. Prompt engineering has also emerged as a lightweight strategy for enhancing controllability, with works like \cite{ju2023humansd, zhang2023controllable, yang2023reco, li2023gligen} optimizing textual or hybrid prompts for fine-grained guidance. Additionally, multi-condition frameworks  \cite{hu2023cocktail, qin2023unicontrol, zhao2024uni, li2025controlnet} integrate auxiliary inputs—such as segmentation maps or depth cues—to complement text prompts, improving alignment with complex user intent. However, while these methods expand generative versatility, many overlook the computational overhead introduced by auxiliary networks, limiting their scalability for real-time applications.

\paragraph{Improving ControlNet efficiency.}
Efforts to enhance ControlNet’s efficacy and efficiency have focused on architectural redesigns, training optimizations, and inference acceleration. ControlNeXt \cite{peng2024controlnext} 
replaces ControlNet’s bulky auxiliary branches with a streamlined architecture and substitutes zero modules with Cross Normalization, slashing learnable parameters by 90\% while maintaining stable convergence. Beyond this, multi-expert diffusion frameworks \cite{lee2024multi, zhang2023improving} tailor denoising operations to specific timesteps, though their computational demands hinder practicality. To reduce inference costs, pruning techniques\cite{fang2023spdm, kim2023architectural, ganjdanesh2024not} trim redundant parameters from pre-trained denoising models, while distillation methods\cite{hsiao2024plug} train lightweight guide models to minimize denoising steps. Inspired by RepVGG  \cite{ding2021repvgg}, RepControlNet \cite{deng2024repcontrolnet} introduces a novel reparameterization strategy: modal-specific adapters modulate features during training, and their weights are later merged with the base network, eliminating auxiliary computations at inference. Unlike prior methods that rely on fixed heuristics, post-hoc pruning, or static architectural modifications, our FlexControl introduces a dynamic, end-to-end trainable framework where block activation is both task-aware and computation-aware. By integrating a gating mechanism with a computational efficiency objective, our approach uniquely balances precision and resource usage, enabling adaptive control across diverse architectures (UNet, DiT) without manual intervention — a paradigm shift from rigid, task-specific designs to flexible, generalizable control.
