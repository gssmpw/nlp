\section{Methodology}

% width=17cm
\begin{figure*}[!t]
\centering
	\includegraphics[width=1.0\textwidth]{pic/controller.png}
	\caption{\textbf{Overview of dynamic routing guided by the router unit.} (a) In the training stage, Gumbel noise is added to the discrete mask to assist the gradient backpropagation. (b) In the inference stage, the router unit controls whether to activate the control block and whether to inject conditional control into the frozen block of the backbone according to the input latent variable. Once output the instruction of inactive, the corresponding control block and zero module will be skipped.}
    \label{fig2}
\end{figure*}

\subsection{Preliminaries}

Denoising diffusion probabilistic model (DDPM) \cite{ho2020denoising} aims to approximate the real data distribution $q\left(\mathbf{x}_{0}\right) $ with the learned model distribution $p\left(\mathbf{x}_{0}\right) $ \cite{ho2020denoising}. It contains a forward diffusion process that progressively adds noise to the image and a reverse generation process that synthesizes the image by progressively eliminating noise. Formulaly, the forward process is a $T$-step Markov chain:
\begin{equation}
\begin{split}                   
    &q\left(\mathbf{x}_{1:T}|\mathbf{x}_{0}\right):=\prod_{t=1}^{T}q\left(\mathbf{x}_{t}|\mathbf{x}_{t-1}\right), \\
    &q\left(\mathbf{x}_{t}|\mathbf{x}_{t-1}\right):=\mathcal{N}\left(\mathbf{x}_{t};\sqrt{1-\beta_{t}}\mathbf{x}_{t-1},\beta_{t}\boldsymbol{\mathit{I}}\right),
\end{split}
\end{equation}
where $\left\{\beta_{t}\right\}_{t=0}^{T}$ are the noise schedule, and $\left\{\mathbf{x}_{t}\right\}_{t=0}^{T}$ are latent variables. Let $\alpha_{t}=1-\beta_{t}$, the distribution of $\mathbf{x}_{t}$ for a given $\mathbf{x}_{0}$ can be expressed as:
\begin{equation}
    q\left(\mathbf{x}_{t}|\mathbf{x}_{0}\right):=\mathcal{N}\left(\mathbf{x}_{t};\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{t-1},\left(1-\bar{\alpha}_{t}\right)\boldsymbol{\mathit{I}}\right).
\label{eq:ddpm_forward}
\end{equation}
Here, $\bar{\alpha}_{t}=\prod_{i=0}^{t}\alpha_{i}$ is a differentiable function of timestep $t$, which is determined by the denoising sampler. Therefore, the diffusion training loss can be formulated as:
\begin{equation}
    \mathcal{L}_{\theta}=\mathbb{E}_{\mathbf{x}_{0},t\sim\mathcal{U}\left(t\right),\epsilon\sim\mathcal{N}\left(\mathbf{0},\boldsymbol{\mathit{I}}\right)}\left[w\left(\lambda_t\right)\left\|\hat{\epsilon}_{\theta}\left(\mathbf{x}_t,t\right)-\epsilon\right\|_{2}^{2}\right],
\label{eq:optimization_object}
\end{equation}
where $\epsilon$ denotes a noise vector drawn from a Gaussian distribution, and $\hat{\epsilon}_{\theta}$ refers to the predicted noise at timestep $t$ by denoising model with parameters $\theta$. $w\left(\lambda_t\right)$ is a pre-defined weighted function that takes into the signal-to-noise ratio $\lambda_t$. The reverse process first sample a Gaussian noise $p\left(\mathbf{x}_{T}\right)=\mathcal{N}\left(\mathbf{x}_{T};\mathbf{0},\boldsymbol{\mathit{I}}\right)$, and then proceeding with the transition probability density step by step:
\begin{equation}
\begin{split}
    p_{\theta}\left(\mathbf{x}_{t-1}|\mathbf{x}_{t}\right)&\approx q\left(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0}\right) \\
    &=\mathcal{N}\left(\mathbf{x}_{t-1};\mu_{\theta}\left(\mathbf{x}_{t},\mathbf{x}_{0}\right),\sigma_{t}^{2}\boldsymbol{\mathit{I}}\right),
\end{split}
\end{equation}
where $\mu_{\theta}\left(\mathbf{x}_{t},\mathbf{x}_{0}\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}\left(\mathbf{x}_{t},t\right)\right)$ and $\sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_{t}$ are the mean and variance of posterior Gaussian distribution $p_{\theta}\left(\mathbf{x}_{0}\right)$.

In order to improve the efficiency of diffusion model, flow-based optimization strategy \cite{lipman2022flow,liu2022flow,liu2023instaflow} is introduced, which defines the forward process as a straight path between the real data distribution and the standard normal distribution:
\begin{equation}
    \mathbf{x} _t=a_t\mathbf{x} _0+b_t\epsilon. 
\label{eq:fm_forward}
\end{equation}
With \cref{eq:fm_forward}, a vector field $u_t$ is constructed to generate a path $p_t$ between the noise distribution and the data distribution. Meanwhile, the velocity $v$ is parameterized by the weight $\theta$ of a neural network to approximate $u_t$. After variable recombination, the flow matching object can also be formulated as \cref{eq:optimization_object} \cite{SD3}. In the reverse stage, flow matching uses ODE solver for fast sampling:
\begin{equation}
    \mathbf{x}\left(t\right)=\mathbf{x}\left(0\right)+\int_{0}^{t}v_{\theta}\left(\mathbf{x\left(\tau\right),\tau}\right)\mathrm{d}\tau.   
\end{equation}

\subsection{Structure}
\label{sec:structure}

Building on the core design philosophy of ControlNet, we first fix the powerful diffusion model backbone, fine-tune a trainable copy with zero module to learn conditional controls, and then inject the acquired knowledge into the frozen backbone:
\begin{equation}
    \mathbf{y}=\mathcal{F}\left(\mathbf{x};\Theta \right) + \mathcal{Z}\left(\mathcal{F}\left(\mathbf{x}+\mathcal{Z}\left(\mathbf{c};\Theta_{z1}\right);\Theta_{c}\right);\Theta_{z2}\right),
\label{eq:inject_conditional_controls}
\end{equation}
where $\Theta$ and $\Theta_{c}$ are the weight parameters of the original block and the trainable copy respectively, $\mathcal{Z}$ represents zero module and $\mathbf{c}$ is the control element. Instead of just cloning the encoder and adding conditional controls only in the decoder blocks \cite{zhang2023adding}, as shown in \cref{fig1}(a), we copy all blocks of the original diffusion model to generate conditional controls and inject them into the corresponding blocks of the backbone in turn, as shown in \cref{fig1}(b), which is similar to the strategy used in BrushNet \cite{ju2024brushnet}, and we call this structure ControlNet-Large. Although the double branch structure improves the quality of the generated image, it leads to huge redundant computation and multiplies the inference delay. 

To reduce computational redundancy and enhance image generation quality, we propose FlexControl, which introduces a lightweight router unit before each conditional control generation block in the trainable branch.  The router generates a binary mask $\mathcal{M}\in\left\{0,1\right\}^{N}$ from the input latent feature, determining whether the underlining control block needs to be activated.  Specifically, ``0'' indicates inactive, ``1'' indicates activate, and $N$ represents the number of control blocks in the trainable branch. 

The mask generation process of the router is data-driven, enabling independent path planning and adaptive decision-making based on the input latent representation. As shown in \cref{fig2}, during inference, if the router outputs a mask value of ``0'', the conditional mapping skips the next control block until activation is deemed necessary. Taking the $l$-th control block as an example, the computation process can be formulated as:
\begin{equation}
\mathbf{h}_{l}=
\begin{cases}
    \mathcal{F}_{l}\left(\mathbf{h}_{l-1},\mathbf{c},t;\Theta_{c}^{l} \right)
    &\text{if} \ \mathcal{M}_{l}=1 \\
    \mathrm{skip}_{l}\left(\mathbf{h}_{l-1}\right)
    &\text{if} \ \mathcal{M}_{l}=0,
\end{cases}
\label{eq:forward_backward_1}
\end{equation}
where $\mathcal{F}_{l}\left(\cdot\right)$ indicates the $l$-th control block operation with parameter $\Theta_{c}^{l}$, $\mathbf{h}_{l}$ is the output of it at timestep $t$, and $\mathrm{skip}_{l}\left(\cdot\right)$ is used to bypass the current block. Following the design of \cite{zhang2023adding}, we utilize the zero module to transform the latent feature $\mathbf{h}_{l}$ into conditional control:
\begin{equation}
\mathbf{y}_{c}^{l}=
\begin{cases}
    \mathcal{Z}_{l}\left(\mathbf{h}_{l};\Theta_{z}^{l}\right)
    &\text{if} \ \mathcal{M}_{l}=1 \\
    \mathrm{N/A}
    &\text{if} \ \mathcal{M}_{l}=0.
\end{cases}
\label{eq:forward_backward_2}
\end{equation}
Here, $\mathbf{y}_{c}^{l}$ denotes the conditional control incorporated into the feature space of the backbone.

{\bf Remark:} The above designed router is in fact lightweight, accounting for less than 1\% of the parameters of the overall model. Since the skipped parameters are excluded from tensor computation during inference, FlexControl barely introduces computational burden by adaptively adjusting the number of active control blocks. See the detailed inference process in \cref{alg:inference} in \cref{app:pseudo}.

\subsection{Router unit design}

As illustrated earlier, the router unit is lightweight and plug-and-play to any diffusion architecture. However, given the differences between UNet and DiT, we will discuss the implementation of the router on these two commonly used architectures separately.

\paragraph{Router for UNet-based architecture.}
The output of UNet block is a multi-channel spatial feature. Given input $\mathbf{h}\in\mathbb{R}^{C\times H\times W}$, the router unit first transforms the spatial feature into linear feature $\mathbf{h}^{'}\in\mathbb{R}^{C}$ through the downsampling layer, we use global average pooling (GAP) in implementation, and then the MLP layer with weight $\mathbf{W}\in \mathbb{R}^{C\times 1}$ maps the linear feature into a scalar $\mathcal{K}$:
\begin{equation}
   \mathcal{K}=\mathrm{MLP}\left(\mathrm{GAP}\left(\mathbf{h}\right)\right).  
\end{equation}
Henceforth, we compute a new scalar $\mathcal{K}^{'}$ by restricting the value of $\mathcal{K}$ to the interval $\left(0,1\right)$ through the Sigmoid function. In order to convert $\mathcal{K}^{'}$ into a binary coding, we introduce a threshold discriminator to control the generation of the mask $\mathcal{M}$ by a preset threshold $\mathcal{T}$ (0.5 by default):
\begin{equation}
\mathcal{M}=
\begin{cases}
    1 & \text{if} \ \mathcal{K}^{'}> \mathcal{T} \\
    0 & \text{if} \ \mathcal{K}^{'}\le \mathcal{T}.
\end{cases}
\label{eq:discriminator}
\end{equation}
We multiply the mask $\mathcal{M}$ and the output latent feature to zero out the corresponding control block and zero module. It can be seen from the above description that the mask $\mathcal{M}$ is learned from the latent variable $\mathbf{h}$. Since timestep embedding is introduced into the blocks of the diffusion model during the generation of $\mathbf{h}$, the output of the router is also affected by the sampled timesteps.

\paragraph{Router for DiT-based architecture.}

For the router applied in DiT, we conduct feature analysis from multiple perspectives. Specifically, we perform both global and local feature encoding on the latent variable $\mathbf{h}\in\mathbb{R}^{N\times C}$ output by the Transformer block \cite{rao2021dynamicvit, rao2023dynamic}. The detailed encoding process is as follows:
\begin{equation}
    \mathbf{h}^{global}=\mathrm{MLP}^{global}\left(\mathrm{AVG}_{dim=1}\left(\mathbf{h}\right)\right),
\label{eq:mlp_global}
\end{equation}
\begin{equation}
    \mathbf{h}^{local}=\mathrm{MLP}^{local}\left(\mathrm{AVG}_{dim=2}\left(\mathbf{h}\right)\right).
\label{eq:mlp_local}
\end{equation}
From \cref{eq:mlp_global,eq:mlp_local}, the encoding process for global and local features primarily consists of two steps. First, feature fusion is performed across all tokens and hidden channels using the function $\mathrm{AVG\left(\cdot\right)}$, which is implemented via average pooling along different dimensions of latent variable. This yields the global feature $\mathbf{z}^{global}\in \mathbb{R}^{C}$ and local feature $\mathbf{z}^{local}\in \mathbb{R}^{N}$. Second, the embedding dimensions of $\mathbf{z}^{global}$ and $\mathbf{z}^{local}$ are aligned through an MLP layer and reduced to $\mathcal{O}$, which is set to $C/64$ by default. Intuitively, the local feature captures token-specific information, while the global feature encodes potential relationships between tokens. We then merge these global and local features to form a new feature representation:
\begin{equation}
    \mathbf{h}^{mix}=\alpha_{1}\cdot \mathbf{h}^{global}+\alpha_{2}\cdot \mathbf{h}^{local}.
\end{equation}
In the above equation, $\alpha_{1}$ and $\alpha_{2}$ are weight factors that balance the influence of global and local features, both set to 0.5 by default. The fused feature variable $\mathbf{h}_{l-1}^{mix}\in \mathbb{R}^{\mathcal{O}}$ is then passed through an MLP layer to produce $\mathcal{K}$. At the end, $\mathcal{K}$ is processed through a Sigmoid layer followed by the threshold discriminator described in \cref{eq:discriminator}, resulting in the router mask $\mathcal{M}$.

\subsection{End-to-end training}

\paragraph{Differentiable learning of router.}

To enable end-to-end training via gradient descent, we address the discrete, non-differentiable nature of the mask by incorporating Gumbel noise into the Sigmoid activation function. This allows the discrete mask $\mathcal{M}$ to be approximated by the differentiable Gumbel-Sigmoid version $\widetilde{\mathcal{M}}$ during training:
\begin{equation}    
    \widetilde{\mathcal{M}}_{l}=\mathrm{Sigmoid}\bigg(\frac{\mathcal{R}_{l}\left(\mathbf{h}_{l-1};\Theta_{\mathcal{R}}^{l}\right)+G_{1}-G_{2}}{\mathcal{TP}}\bigg),
\end{equation}
where $G_{1}$, $G_{2}$ $\sim$ Gumbel$(0, 1)$, $\mathcal{TP}$ denotes the temperature hyperparameter (5 by default), $\mathcal{R}\left(\cdot\right)$ denotes tensor computations in the router unit parametered by $\Theta_{\mathcal{R}}$.

To this end, we employ different mask schemes during the forward and backward passes:
\begin{equation}
\mathbf{h}_{l}=
\begin{cases}
    \mathcal{F}_{l}\cdot\mathcal{M}_{l}+\mathrm{skip}_{l}\left(\mathbf{h}_{l-1}\right)\cdot\left(1-\mathcal{M}_{l}\right)
    &\text{if Forward} \\
    \mathcal{F}_{l}\cdot\widetilde{\mathcal{M}}_{l}+\mathrm{skip}_{l}\left(\mathbf{h}_{l-1}\right)\cdot\big(1-\widetilde{\mathcal{M}}_{l}\big)
    &\text{if Backward}.
\end{cases}
\label{eq:redefined_forward_backward_1}
\end{equation}
Meanwhile, the computation process of the zero module is adjusted accordingly:
\begin{equation}
\mathbf{y}_{c}^{l}=
\begin{cases}
    \mathcal{Z}_{l}\left(\mathbf{h}_{l};\Theta_{z}^{l}\right)\cdot \mathcal{M}_{l}
    &\text{if Forward} \\
    \mathcal{Z}_{l}\left(\mathbf{h}_{l};\Theta_{z}^{l}\right)\cdot \widetilde{\mathcal{M}}_{l}
    &\text{if Backward}.
\end{cases}
\label{eq:redefined_forward_backward_2}
\end{equation}

{\bf Remark:} As can be seen in \cref{eq:redefined_forward_backward_1,eq:redefined_forward_backward_2} during training, the blockwise routing differs from the inference process displayed in \cref{eq:forward_backward_1,eq:forward_backward_2}: during training, we retain all blocks to ensure proper back-propagation, rather than skipping blocks as done during inference. %slightly 

\paragraph{Computation-aware training loss.}

\begin{figure*}[!t]
    \centering
	\includegraphics[width=1.0\textwidth]{pic/all_conditions.png}
    \caption{
    \textbf{Qualitative comparison of controllable generation methods.} FlexControl achieves higher fidelity and structure preservation across Depth Map, Canny Edge, and Segmentation Mask conditions, reducing distortions (boxes) seen in other methods. It better aligns with input conditions while maintaining visual quality.
    }
    \label{diode}
\end{figure*}

\begin{table*}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|cc|cc|cc|cc} 
        \toprule[1.0px]
        \textbf{Method}&\textbf{Base Model}& \multicolumn{2}{c|}{\textbf{Depth Map}}&\multicolumn{2}{c|}{\textbf{Canny Edge}}& \multicolumn{2}{c|}{\textbf{Seg. Mask}}& \multicolumn{2}{c}{\textbf{\#Average}}\\ & & \textbf{FID} $\downarrow$& \textbf{CLIP\_score} $\uparrow$& \textbf{FID} $\downarrow$& \textbf{CLIP\_score} $\uparrow$& \textbf{FID} $\downarrow$&\textbf{CLIP\_score} $\uparrow$& \textbf{FID} $\downarrow$&\textbf{CLIP\_score} $\uparrow$\\
        \midrule
        ControlNet \cite{zhang2023adding} &SDXL& 19.90& 0.3224& 22.07& 0.2657& 26.95& 0.2495& 22.97& 0.2792\\
        T2I-Adapter \cite{mou2024t2i} &SDXL& 19.74& 0.3197& 22.91& 0.2614& 27.54& 0.2501& 23.40& 0.2771\\
        \midrule
        GLIGEN \cite{li2023gligen} &SD1.4& 18.36& 0.3175& 19.01& 0.2520& 23.79& 0.2490& 20.39& 0.2728\\
        T2I-Adapter \cite{mou2024t2i} &SD1.5& 22.52& 0.3146& 16.74& 0.2598& 24.65& 0.2494& 21.30& 0.2728\\
        ControlNet \cite{zhang2023adding} & SD1.5& 17.76& 0.3245& 15.23& 0.2613& 21.33& 0.2531& 18.11& 0.2796\\ 
        ControlNet++ \cite{li2025controlnet} &SD1.5& 16.66& 0.3209& 17.23& 0.2598& 19.89& 0.2640& 17.93& 0.2816\\
        ControlNet-Large &SD1.5& \textcolor{blue}{12.45}& \textcolor{blue}{0.3492}&
        \textcolor{blue}{12.92}& \textcolor{red}{0.2789}& \textcolor{blue}{16.78}& \textcolor{blue}{0.2796}& \textcolor{blue}{14.05}& \textcolor{blue}{0.3026}\\
        \midrule
        \rowcolor{gray!20}
        \textbf{FlexControl}$_{\gamma=0.5} $&SD1.5& \textcolor{red}{11.65}& \textcolor{red}{0.3498}& \textcolor{red}{11.37}& \textcolor{blue}{0.2778}& \textcolor{red}{14.80}& \textcolor{red}{0.2842}& \textcolor{red}{12.61}& \textcolor{red}{0.3039}\\ 
        \bottomrule[1.0px]
    \end{tabular}
    }
    \caption{ 
    \textbf{Quantitative comparison of FlexControl with state-of-the-art methods.} We report FID ($\downarrow$) and CLIP score ($\uparrow$) on different conditioning types: Depth Map, Canny Edge, and Segmentation Mask. Lower FID indicates better image quality, while higher CLIP score reflects better alignment with textual prompts. The best results are highlighted in \textcolor{red}{red}, while the second-best results are shown in \textcolor{blue}{blue}. FlexControl achieves the best overall performance, demonstrating superior fidelity and semantic alignment.
    }
    \label{tab:table1}
\end{table*}

Following standard controllable generation methods, our training dataset $\mathcal{D}$ contains triples of the original image $x$, spatial conditioning control $\mathbf{c}_{s}$, and text prompt $\mathbf{c}_{t}$. The diffusion loss of FlexControl is formulated as:
\begin{equation}
    \mathcal{L}_{\mathbf{SD}}=\mathbb{E}_{\mathbf{x}_{0},\mathbf{c}_{t},\mathbf{c}_{s},t,\epsilon\sim \mathcal{N}\left(\mathbf{0},\boldsymbol{\mathit{I}}\right)}\left[\left\|\hat{\epsilon}_{\theta}\left(\mathbf{x}_{t},\mathbf{c}_{t},\mathbf{c}_{s},t\right)-\epsilon \right\|_{2}^{2}\right].
\label{eq:losssd}
\end{equation}
FlexControl aims to activate the optimal control blocks and inject conditional mapping into the backbone network for efficient image generation. In addition to the regular diffusion loss $\mathcal{L}_{\mathbf{SD}}$, we introduce a cost loss $\mathcal{L}_{\mathbf{C}}$ to regulate resource consumption to the desired sparsity $\gamma$, which measures the proportion of floating-point operations (FLOPs):
\begin{equation} 
\mathcal{L}_{\mathbf{C}}=\frac{1}{\left|\mathcal{D}_{\mathrm{bs}}\right|}\sum_{d\in\mathcal{D}_{\mathrm{bs}}}\left(\frac{F^{\mathrm{Flex}}_{t_d}\left(d\right)}{F^{\mathrm{Large}}_{t_d}\left(d\right)}-\gamma\right)^{2},
\label{eq:losscost}
\end{equation}
where $\mathcal{D}_{\mathrm{bs}}$ represents the current batch samples, $t_d\in \left[0,T\right]$ is the uniformly sampled timestep for sample $d$. $F_{t}\left(d\right)$ denotes FLOPs of the trainable branch at sampled timestep, and superscripts $\mathrm{Flex}$ and $\mathrm{Large}$ respectively denote FlexControl and ControlNet-Large. We combine $\mathcal{L}_{\mathbf{SD}}$ and $\mathcal{L}_{\mathbf{C}}$ to bring out the final optimization goal,
\begin{equation} 
 \mathcal{L}_{\theta}= \mathcal{L}_{\mathbf{SD}}+\lambda_{\mathbf{C}}\cdot\mathcal{L}_{\mathbf{C}},
 \label{eq:losstheta}
\end{equation}
where $\lambda_{\mathbf{C}}$ is the hyperparameter that controls the influence of loss $\mathcal{L}_{\mathbf{C}}$. See the detailed training process in \cref{alg:training} in \cref{app:pseudo}.
