\section{Introduction}
\label{sec:intro}

Diffusion-based image generation models have recently gained widespread acceptance in the art and design community, not only for their high-quality, photo-realistic image generation but also due to the transformative capabilities of from controllable unit like ControlNet \cite{zhang2023adding}, T2I-Adapter \cite{mou2024t2i}, \textit{etc}, which enables users to create images under diverse conditions(\textit{e.g.}, layout, pose, shape, and form), allows generated image that satisfied various real-world demand.

Despite its growing popularity, ControlNet methods typically rely on multiple design hyperparameters—such as choosing which network block to control for improved fidelity and adherence to input conditions—without a systematic investigation of their effects. For example, the ControlNet variant based on SD1.5 \cite{SD1.5} replicates encoder blocks and injects control information into the decoder, whereas T2I-Adapter applies control in the encoder. It remains unclear which block‐level configuration is most effective, especially since optimal designs can vary by task. Complicating matters further, ControlNet is often trained on significantly smaller datasets than those used for the diffusion model’s pre‐training, implying that adding too much control could disrupt the pretrained representations and degrade image quality, since insufficient control may fail to deliver the desired guidance. As an evidence, a recent study \cite{ju2024brushnet} highlights that the number and placement of control blocks might significantly affect image quality in tasks such as inpainting. Moreover, in practice, ControlNet pipeline relies on heuristic strategies to decide which timesteps should receive control signals at inference, yet evidence is scarce regarding which approach consistently yields the best results. Collectively, these gaps emphasize the need for a more principled, comprehensive analysis of ControlNet design and inference strategies.

To dynamically adjust control blocks based on timestep and conditional information while maintaining (or even improving) generation quality, we propose FlexControl, a data-driven dynamic control method. Similar to conventional controllable generation methods, as shown in \cref{fig1}(a), we freeze the original diffusion model and copy its parameters to process task-specific conditional images. FlexControl is equipped with a router unit within the control block (see \cref{fig1}(b)) to plan forward routes, activating control blocks only when necessary based on the current latent variable.  In contrast to other controllable generation models, FlexControl customizes the inference path for each input, minimizing potential redundant computations. In summary, our main contributions are as follows:

\paragraph{1. Data-driven dynamic control configuration:
} We introduce an automated router unit that dynamically selects control blocks at each timestep, eliminating the need for exhaustive architecture searches and retraining. Our approach enables: (1) task-adaptive control configurations through end-to-end training, (2) temporally adaptive inference via per-timestep activation decisions, and (3) faster configuration design compared to manual search baselines by removing the need for configuration search and repeated training.
\paragraph{2. Computation-aware controllable generation:} Our approach significantly enhances controllability and image quality while maintaining a similar computational cost to the original ControlNet by introducing a novel computation-aware training loss. Specifically, in the depth-map control task, our method achieves a 6.11 FID improvement and a 6.30\% RMSE reduction. Furthermore, this strategic allocation of computation to control units outperforms brute-force doubling, establishing new Pareto frontiers in the control-quality v.s. compute trade-off.
\paragraph{3. Universal plug-and-play integration:} Our method seamlessly integrates with any dual-stream control-model, introducing minimal additional parameters and zero architectural modifications to host models. It enables flexible switching between full control and efficiency-optimized modes, depending on computational requirements.
