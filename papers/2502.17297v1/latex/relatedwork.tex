\section{Related Work}
Existing RAG models~\cite{shi2023replug, asai2023self, yu2023augmentation, yan2024corrective} typically rely on dense retrievers~\cite{karpukhin2020dense, xiongapproximate, ren-etal-2021-rocketqav2, xiong2021answering, gao-callan-2022-unsupervised} or sparse retrievers like BM25~\cite{robertson2009probabilistic} for text document retrieval. More recent efforts have integrated multi-modal retrieval methods, allowing the inclusion of rich external knowledge from different modalities within RAG frameworks. For example, some works~\cite{liu2022universal, zhou2024marvel} have introduced unified multi-modal retrieval systems that map images and texts into a shared semantic space. This approach allows for single-modal matching, cross-modal matching, and modality routing within the embedding space. VISTA~\cite{zhou2024vista} further enhances multi-modal retrieval by optimizing synthetic training data and refining training strategies. These advancements enable the retrieval of multi-modal knowledge, providing a way for evaluating the effectiveness of MLLMs in multi-modal contexts.


Multi-modal Large Language Models (MLLMs)~\cite{achiam2023gpt,team2023gemini,sun2023emu,sun2024generative, aghajanyan2022cm3, lu2024deepseekvl} have proven their effectiveness in understanding, integrating, and utilizing both visual and textual knowledge in generation tasks. Models like BLIP~\cite{li2022blipbootstrappinglanguageimagepretraining, li2023blip}, LLaVA~\cite{liu2024visual}, and Flamingo~\cite{alayrac2022flamingo} build the MLLMs by combining pre-trained vision encoders with Large Language Models (LLMs) to process multi-modal inputs for generation. Thriving on the advancements in MLLMs, researchers pay more attention to extending the advantages of Retrieval-Augmented Generation (RAG) to these MLLMs, enhancing their generation capability. 

\input{latex/figure/benchmark_case} 
Multi-modal RAG has demonstrated its potential to enhance knowledge-intensive and information-seeking tasks, such as question answering~\cite{chang2022webqa, marino2019ok} and fact verification~\cite{mishra2022factify}. These models utilize retrieval-based multi-modal documents to provide richer and contextually relevant information. Additionally, other works have applied multi-modal RAG to improve the performance of MLLMs on the tasks like image captioning~\cite{lin2014microsoft, young2014image} and generation~\cite{yasunaga2023retrieval, yu2023scaling, sharifymoghaddam2024unirag}. However, existing multi-modal benchmarks~\cite{johnson2017clevr, schuhmann2021laion, lin2014microsoft, young2014image, marino2019ok} are typically tailored to specific tasks and lack a comprehensive framework for evaluating multi-modal RAG systems. 
% Further analysis reveals that MM-RAIT enables MLLMs to filter out irrelevant information from the context, while selectively extracting key information across different modalities. 
%老师，这句话是不是放错位置了？en

