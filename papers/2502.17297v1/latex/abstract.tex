\begin{abstract}
This paper introduces \textbf{M}ulti-\textbf{M}odal \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration (M$^2$RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce \textbf{M}ulti-\textbf{M}odal \textbf{R}etrieval-\textbf{A}ugmented \textbf{I}nstruction \textbf{T}uning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at \url{https://github.com/NEUIR/M2RAG}.
\end{abstract}

% Our experiments demonstrate the effectiveness of MM-RAIT by significantly improving the quality of MLLM generated responses, with MiniCPM-V 2.6 and Qwen2-VL achieving relative performance gains of 27\% and 34\%, respectively. All data and code will be released via GitHub.

%with 27\% and 34\% gains, respectively

