\section{Introduction}
With the rapid development of Large Language Models (LLMs), such as GPT-4~\cite{openai2023gpt} and LLaMA~\cite{touvron2023llama}, they have demonstrated strong emergent abilities in many NLP tasks~\cite{wei2022emergent, zhao2023survey}. However, LLMs often face the issue of hallucinations, making them produce unreliable responses~\cite{ji2023survey, huang2023survey, shuster2021retrieval}. Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval, asai2024reliable, shi2023replug, yao2022react} has proven effective in mitigating this hallucination problem by integrating external knowledge with LLMs.

\input{latex/figure/mmrag}

To enhance LLMs with retrieved knowledge, existing approaches typically feed retrieved documents as input contexts, prompting LLMs to generate responses based on this in-context information~\cite{ram2023context}. Existing RAG approaches~\cite{petroni2021kilt, linra} usually focus on retrieving textual knowledge from corpora to aid LLMs in answering queries. Recent studies~\cite{hu2024mrag, sharifymoghaddam2024unirag} have extended RAG to Multi-modal Large Language Models (MLLMs), enabling them to address knowledge-intensive and information-seeking tasks that involve visual queries. However, these approaches largely rely on text or images as the sole sources of external knowledge, often overlooking the critical role of multi-modal data in providing richer and more comprehensive information that leads to producing more accurate answers~\cite{hu2024mrag, liu2022universal}.

To advance RAG modeling in multi-modal scenarios, we introduce the Multi-Modal RAG (M$^2$RAG) benchmark, designed to explore the effectiveness of MLLMs by feeding multi-modal retrieved documents as the input contexts to answer the question. As shown in Figure~\ref{fig:mmrag}, we can use images or text as queries to retrieve multi-modal documents via multi-modal dense retrievers~\cite{liu2022universal, zhou2024marvel, zhou2024vista}. These multi-modal documents are then used as the input contexts to assist MLLMs during generation. Specifically, our M$^2$RAG benchmark includes four distinct tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. Different from existing works~\cite{aghajanyan2022cm3, sharifymoghaddam2024unirag}, M$^2$RAG is built upon high-quality datasets~\cite{chang2022webqa, mishra2022factify} and designs four evaluation tasks in the open-domain setting, aiming to assess the effectiveness of MLLMs in leveraging the knowledge from multi-modal contexts.

In this paper, we also propose the \textbf{M}ulti-\textbf{M}odal \textbf{R}etrieval \textbf{A}ugmented \textbf{I}nstruction \textbf{T}uning (MM-RAIT) method to adapt MLLMs to the multi-modal in-context learning scenario, enhancing the effectiveness of MLLMs in utilizing the knowledge from these multi-modal retrieval documents. Specifically, we design task-specific prompt templates and fine-tune MLLMs on the M$^2$RAG benchmark, making MLLMs maintain contextual awareness during generation. Our experimental results demonstrate that using retrieved knowledge significantly enhances MLLMs' performance, achieving significant improvements in both zero-shot and few-shot settings. After training with MM-RAIT, MiniCPM-V and Qwen2-VL show an average improvement of 27\% and 34\% over vanilla RAG modeling methods, showing the effectiveness of MM-RAIT.

%MiniCPM-V 2.6 achieves an average improvement of over 27% across all tasks in M2RAG, while Qwen2-VL shows an even greater improvement of 34%. 
% respectively