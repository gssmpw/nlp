% -- this is limitations --
Although our M$^2$RAG benchmark includes four common multi-modal tasks, incorporating additional tasks can provide a more comprehensive evaluation of the capabilities of MLLMs. Furthermore, while MLLMs perform satisfactorily within retrieved multi-modal contexts, they still rely predominantly on textual data for some tasks. Finding ways to enable MLLMs to more effectively leverage multi-modal contexts remains a critical challenge that requires further exploration. Additionally, due to the performance limitations of multi-modal retrieval models, the quality of the retrieved multi-modal documents directly impacts the overall performance of MLLMs. Improving the accuracy of multi-modal retrieval remains a vital area for future research.







% Although our M$^2$RAG benchmark includes four common multi-modal tasks, incorporating additional tasks could provide a more comprehensive evaluation of the capabilities of MLLMs. Furthermore, while MLLMs perform satisfactorily within retrieved multi-modal contexts, effectively utilizing multi-modal contextual knowledge remains a significant challenge, as current MLLMs struggle to extract and leverage crucial semantics from these image features to enhance RAG performance. Additionally, due to the performance limitations of multi-modal retrieval models, the quality of the retrieved multi-modal documents directly impacts the overall performance of MLLMs. While MM-RAIT can alleviate the performance decreases from bad retrieval documents, it also shows limited improvements. Improving both the accuracy of multi-modal retrieval and the modelâ€™s ability to integrate and utilize multi-modal information effectively remains a vital area for future research.


