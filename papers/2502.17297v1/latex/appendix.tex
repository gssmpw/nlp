
\subsection{License}
We show the licenses of the datasets that we use. WebQA uses CC0-1.0 license, while Factify uses MIT license.
All these licenses and agreements permit the use of their data for academic purposes.

\subsection{More Details of M$^2$RAG Benchmark}
In this section, we provide additional details regarding the data used in the M$^2$RAG benchmark. The data statistics are shown in Table~\ref{tab:data_statistic}.
\input{latex/table/data_statistic}

The M$^2$RAG benchmark incorporates tasks, such as image captioning, multi-modal question answering, and image reranking, all of which are built upon the WebQA~\cite{chang2022webqa} benchmark. For both the multi-modal question answering and image reranking tasks, the same multi-modal retrieval corpus is used. For the image captioning task, we filter out any image documents that are selected for the construction of both the training and evaluation sets. The image-caption pairs used in the image reranking test set are the same as those in the image captioning test set.


For the multi-modal fact verification task, since the test set labels are not publicly available in the Factify~\cite{mishra2022factify} dataset, we follow the approach of~\citet{tahmasebi2024multimodal} and sample data from the validation set to create the evaluation set. All text and image documents from the training and validation sets of the Factify dataset are then collected to construct the retrieval corpus. The original Factify dataset consists of five categories: ``Support\_Text'', ``Support\_Multimodal'', ``Insufficient\_Text'', ``Insufficient\_Multimodal'', and ``Refute''.When constructing the training and evaluation datasets for M$^2$RAG, we select an equal number of samples from each of these five categories. Since our RAG scenario involves both text and image information, we consolidate these modality-based categories into three: ``Support'', ``Refute'', and ``Insufficient'', in order to better evaluate the effectiveness of LLMs in the multi-modal fact verification task.

\subsection{Prompt Templates Used in M$^2$RAG}\label{app:prompt} 
In Figure~\ref{fig:prompts}, we present the prompt templates designed for various task scenarios in M$^2$RAG. Additionally, we describe the input format for the image reranking task. In terms of image placement, we use the placeholder \emph{\{image\}} for the image, following the method proposed by~\citet{hu2024mrag}.

\input{latex/table/image_cap_compare}
\input{latex/figure/img_cap_compare}
\input{latex/figure/appendix_prompt}
\input{latex/figure/appendix_case_mmqa}
\subsection{Comparison of Different Image Captioning Tasks}\label{app:image_cap}
In this section, we compare the performance of MiniCPM-V 2.6 and Qwen2-VL on the image captioning task using the MSCOCO~\cite{lin2014microsoft} and M$^2$RAG datasets. For the MSCOCO dataset, we use the version employed in the image captioning task of UniRAG~\cite{sharifymoghaddam2024unirag} and follow the same processing method described in their paper.

As shown in Table~\ref{tab:img_cap_compare}, both MiniCPM-V 2.6 and Qwen2-VL exhibit lower performance on M$^2$RAG compared to MSCOCO, with average declines of over 9\% and 19\%, respectively. This suggests that the image captioning task in M$^2$RAG is more challenging for multi-modal large language models (MLLMs) than that in MSCOCO. As shown in Figure~\ref{fig:img_cap_compare}, the length of image captions in M$^2$RAG is not fixed and varies depending on the scene, with diverse entities and detailed scene descriptions. This indicates the complexity of M$^2$RAG that requires MLLMs to leverage external knowledge to generate accurate captions. In contrast, the image captions in MSCOCO are more straightforward and regular, enabling MLLMs to perform better by relying primarily on internal knowledge. These observations further underscore the need for M$^2$RAG to serve as a more challenging benchmark for multi-modal RAG tasks.



\subsection{Additional Case Studies}\label{app:case}
As shown in Figure~\ref{fig:mmqa_case}, we present additional case studies from various tasks to assess the effectiveness of vanilla RAG MM-RAIT models in utilizing multi-modal context. In the RAG setting, we use the top-5 retrieved multi-modal documents for inference.

In the image captioning task, MLLMs initially provide generic descriptions based on internal knowledge. However, when multi-modal context is incorporated, they are able to extract richer and more specific information, such as identifying landmarks like the ``Library of Congress''. After MM-RAIT training, both MiniCPM-V 2.6 and Qwen2-VL generate more accurate captions. A similar improvement is observed in the image reranking task, where vanilla MLLMs initially struggle to align the semantics of the image and caption. After MM-RAIT training, fine-grained alignments between images and captions are achieved, allowing Qwen2-VL to rank the image of ``Bellagio and Caesars Palace from the left side of Bellagio's fountains'' first, even though the reranking task is not involved during training.

In multi-modal question answering, the retrieved image, along with the caption ``1929 Cadillac Sports Phaeton'', aids Qwen2-VL in producing accurate answers within the RAG framework. However, MiniCPM-V initially struggles to provide a correct answer. After MM-RAIT training, both models consistently provide stable and accurate answers. Similarly, in multi-modal fact verification, vanilla RAG models struggle to extract useful information from noisy documents while MM-RAIT enables the models to better extract and utilize relevant evidence, thereby improving their fact verification performance.
