\section{Experimental Methodology} This section outlines the datasets, evaluation metrics, baselines, and implementation details used in our experiments.

\textbf{Dataset.}
We use the M$^2$RAG dataset to evaluate the performance of different MLLMs in the multi-modal RAG scenario. The dataset consists of four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. For multi-modal retrieval, we adopt VISTA~\cite{zhou2024vista}, a universal embedding model to search for query-related documents. VISTA integrates image token embeddings into the BGE Text Embedding~\cite{xiao2024c} framework, enabling flexible processing of the inputs of both text and image data.

\input{latex/table/overall}
\textbf{Evaluation Metrics.}
For image captioning and multi-modal QA tasks, we use BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, and CIDEr~\cite{vedantam2015cider} scores to assess performance. In the multi-modal fact verification task, we evaluate the performance of different RAG models using accuracy (ACC) and F1 score. For the image reranking task, we use the Fréchet Inception Distance (FID↓)~\cite{heusel2017gans}\footnote{\url{https://github.com/mseitzer/pytorch-fid}}. 

\textbf{Baselines.}
We compare our models with two multi-modal baselines: MiniCPM-V 2.6~\cite{yao2024minicpm} and Qwen2-VL~\cite{Qwen2VL}. These models are evaluated in a zero-shot setting to assess their effectiveness in leveraging multi-modal knowledge from the input context. We feed the top-1, top-3, and top-5 ranked documents into these MLLMs to evaluate their RAG performance.

\textbf{Implementation Details.}
We apply Low-Rank Adaptation (LoRA)~\cite{hu2021lora} to fine-tune both MiniCPM-V 2.6 and Qwen2-VL using the top-5 retrieved multi-modal documents for 2 epochs. The batch size is 4, with a maximum token limit of 4,096. A cosine learning rate scheduler is used, with the learning rate set to $1e-6$ for MiniCPM-V and $1e-4$ for Qwen2-VL. We fine-tune Qwen2-VL using LLaMA-Factory~\cite{zheng2024llamafactory} and set \texttt{max\_pixels}=$512 \times 512$ for training and inference.

% For training, we apply Low-Rank Adaptation (LoRA)~\cite{hu2021lora} to fine-tune both MiniCPM-V 2.6 and Qwen2-VL using the top-5 retrieved multi-modal documents for 2 epochs. The batch size is set to 4, with a maximum token limit of 4,096. A cosine learning rate scheduler is used, with the learning rate set to $1e-6$ for MiniCPM-V 2.6 and $1e-4$ for Qwen2-VL.
% During training and inference, we set \texttt{max\_pixels} = $512 \times 512$ for Qwen2-VL.