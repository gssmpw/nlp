\section{M$^2$RAG Benchmark for Multi-Modal Retrieval-Augmented Generation} In this section, we describe our Multi-Modal Retrieval-Augmented Generation (M$^2$RAG) benchmark. We first introduce the RAG tasks in M$^2$RAG, followed by a detailed explanation of the construction process. Finally, we present a comparison of existing multi-modal benchmarks with M$^2$RAG.

\textbf{Task Definition.}\label{bench:task} As shown in Figure~\ref{fig:case}, M$^2$RAG defines four tasks to evaluate the capabilities of MLLMs in open-domain RAG scenarios: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. For each task, MLLMs are required to retrieve knowledge from the multi-modal document collection $\mathcal{D}$ and generate responses to answer the question $q$. Details of the prompt templates of different tasks are shown in Appendix~\ref{app:prompt}.

\textit{Image Captioning Task.} Image Captioning is a widely used task for evaluating the performance of multi-modal RAG models~\cite{aghajanyan2022cm3, sharifymoghaddam2024unirag}. In this task, an image is provided as the query $q$, and the document collection $\mathcal{D}$ is constructed using image documents that contain captions. The goal of image captioning is to generate concise and semantically coherent captions that accurately describe the image content. Unlike previous works~\cite{aghajanyan2022cm3, sharifymoghaddam2024unirag}, we source image captions from WebQA~\cite{chang2022webqa}, where all image documents are collected from Wikimedia Commons. These captions often include important details, such as named entities, which make the task more challenging and provide crucial information for query matching~\cite{liu2022universal}. More comparison details of the image captioning tasks in different benchmarks are shown in Appendix~\ref{app:image_cap}.

\input{latex/table/task_compare}
\textit{Multi-Modal Question Answering Task.} Following the WebQA benchmark~\cite{chang2022webqa}, the Multi-Modal QA task involves answering text-based queries $q$ by leveraging both text and image documents. The document collection $\mathcal{D}$ consists of both text and image documents containing captions. Additionally, we extend WebQA to an open-domain setting by instructing the retriever to return query-relevant documents from the collection $\mathcal{D}$, as demonstrated by~\citet{liu2022universal}.

\textit{Multi-Modal Fact Verification Task.} The Multi-Modal Fact Verification task challenges MLLMs to verify the accuracy of claims using multi-modal evidence. In this task, the query $q$ can be a multi-modal claim, and the document collection $\mathcal{D}$ consists of both text and image documents, where the image documents do not contain captions. The relationship between the claim and the evidence is categorized into three possible outcomes: ``Support'', ``Refute'', or ``Insufficient'', indicating whether the evidence supports, refutes, or lacks sufficient information to verify the claim. We build this task on the Factify dataset~\cite{mishra2022factify}, but we focus on open-domain fact verification by retrieving evidence from a multi-modal document collection~\cite{thorne2018fever}.



\textit{Image Reranking Task.} In the Image Reranking task, the objective is to identify the most relevant images based on a given image description. Here, the image description serves as the query $q$, and the document collection $\mathcal{D}$ consists of image documents that do not include captions. For each description, we first use a multi-modal retriever to retrieve image candidates based solely on their image features and then rerank the images using MLLMs. To adapt MLLMs for this task, we follow prior work~\cite{muennighoff2022sgpt} and compute the Perplexity (PPL) score for reranking image candidates based on their image features. This approach models the relevance between queries and images in a manner similar to image captioning, where a lower PPL score indicates greater relevance between the candidate image and the given query.

% \input{latex/table/data_statistic}
\textbf{Details of Data Construction.} To build the M$^2$RAG benchmark, we collect data from two widely used datasets, WebQA~\cite{chang2022webqa} and Factify~\cite{mishra2022factify}, constructing 3,000 instances for both training and evaluation processes for each task.

First, we select WebQA to build the tasks for image captioning, multi-modal QA, and image reranking. For the multi-modal QA task, we select an equal number of text-based and image-based QA pairs from WebQA to construct the dataset. For image captioning and image reranking tasks, we randomly select image-text pairs with a similarity score greater than 0.65, which are then split into training and evaluation sets. The retrieval corpus for these tasks consists of all image-text pairs except the selected ones. Then, for the multi-modal fact verification task, we use the Factify dataset and the entire set of image-text documents in the dataset is used as the retrieval corpus. Additionally, we consolidate the modality based categories into three: ``Support'', ``Refute'', and ``Insufficient.''

\textbf{Benchmark Comparison.} The comparison of different benchmarks is presented in Table~\ref{tab:benchmarks_comparison}.

Existing multi-modal benchmarks primarily evaluate the effectiveness of MLLMs on single tasks, such as image captioning~\cite{lin2014microsoft, young2014image} or question answering~\cite{chang2022webqa}, limiting their ability to conduct comprehensive evaluations of MLLMs in multi-modal RAG scenarios. In contrast, M$^2$RAG offers several unique features for a more thorough evaluation: 1) M$^2$RAG defines four tasks that assess an MLLM's ability to effectively understand and utilize retrieved knowledge. These tasks require MLLMs to perform reasoning and information matching based on both queries and contextual knowledge. 2) M$^2$RAG incorporates the retrieval results as the multi-modal contexts for model inputs, avoiding the need for separate processing of the retrieval documents of different modalities. 3) M$^2$RAG adapts these tasks to an open-domain setting, requiring MLLMs to retrieve knowledge from a comprehensive multi-modal document collection, offering a more realistic RAG scenario.

