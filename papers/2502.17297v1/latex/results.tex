
\section{Evaluation Result}
In this section, we first evaluate the performance of MLLMs on the M$^2$RAG benchmark. We then conduct ablation studies to assess the impact of varying numbers of retrieved documents of different modalities. Following that, we analyze the role of different retrieval modalities in RAG models. Finally, case studies are shown.

\subsection{Overall Performance}
As shown in Table~\ref{tab:overall}, we report the performance of various RAG models on the M$^2$RAG benchmark. The vanilla RAG models directly use retrieved documents to augment LLMs, while MM-RAIT models fine-tune MLLMs within the RAG framework.

For these vanilla RAG models, performance generally improves as the number of retrieved documents increases. However, when retrieving the top-5 ranked documents, the overall performance of vanilla RAG models on most tasks is lower compared to using top-1 or top-3 documents. This suggests that vanilla LLMs still struggle to fully leverage multi-modal knowledge to enhance MLLMs. Although some related works also use image captioning tasks to evaluate RAG performance~\cite{sharifymoghaddam2024unirag}, the performance of these MLLMs on M$^2$RAG is considerably worse, indicating that M$^2$RAG offers a more challenging dataset for image captioning.
In contrast to vanilla RAG models, both MiniCPM-V 2.6 and Qwen2-VL demonstrate strong performance across all tasks on the M$^2$RAG benchmark after training with MM-RAIT. Specifically, MiniCPM-V 2.6 achieves an average improvement of over 27\% across all tasks in M$^2$RAG, while Qwen2-VL shows an even greater improvement of 34\%. These results highlight the effectiveness of MM-RAIT, showcasing its ability to help MLLMs better utilize multi-modal contexts to enhance their performance.

\input{latex/table/ablation}
\input{latex/figure/retrieval_unimodality_analysis}
\input{latex/figure/appendix_case_different_k}
\subsection{Ablation Study}
As shown in Table~\ref{tab:ablation}, we conduct ablation studies to evaluate RAG effectiveness with retrieved documents of different modalities and numbers.
Specifically, we conduct two evaluation settings to evaluate the roles of different modalities: Only Text and Only Image. Only Text indicates removing all image features from multi-modal input contexts to enhance the MLLM, while Only Image removes all texts from top-ranked multi-modal documents.

Compared with the RAG models using top-3 ranked multi-modal documents for augmentation, the performance of vanilla RAG models usually decreases with top-5 ranked documents, while MM-RAIT alleviates the performance decreases but also shows limited improvements. It illustrates that effectively using the multi-modal contextual knowledge is still challenging for existing MLLMs. Moreover, we further remove all texts or image features to show the roles of different modalities in RAG modeling. For all tasks, the RAG performance of the Only Text model slightly decreases, showing that these texts contribute to the primary knowledge source for these RAG models. After adding the image features, the RAG performance usually increases, showing that these image features can improve the performance of RAG models. Even though different modalities show the effectiveness in multi-modal RAG modeling, it is still hard to effectively learn more crucial semantics from these image features to improve the RAG performance within multi-modal contexts.

\subsection{Effectiveness of MLLMs in Different Modality-based RAG Scenarios}
In this experiment, we investigate the impact of retrieved documents from different modalities on the effectiveness of RAG models.

As shown in Figure~\ref{fig:retrieval_modality_comparison}, we divide the multi-modal QA dataset of M$^2$RAG into two groups: image-answerable queries and text-answerable queries. These categories represent queries that can be answered by image or text documents, respectively. We compare both vanilla RAG and MM-RAIT, implemented using MiniCPM-V and Qwen2-VL. Top-5 ranked documents from texts, images, and both modalities are fed to the different RAG models to evaluate the QA performance.

Figures~\ref{fig:text_qa_cpmv} and~\ref{fig:text_qa_qwen} present the RAG performance on text-answerable queries. Overall, the RAG models using multi-modal retrieved documents exhibit comparable performance to those using only text-based documents, indicating that MLLMs can effectively learn from text documents to answer queries. Notably, vanilla RAG models show minimal differences in performance when using text, image, or both types of documents, whereas MM-RAIT significantly improves performance when leveraging documents from multiple modalities. This highlights the effectiveness of MM-RAIT in enabling MLLMs to learn from multi-modal contexts. Interestingly, vanilla MLLMs appear insensitive to the retrieved contexts, likely because they rely heavily on internal knowledge when processing text-answerable queries.

Next, we evaluate the RAG performance on image-answerable queries, shown in Figures~\ref{fig:img_qa_cpmv} and~\ref{fig:img_qa_qwen}. The results indicate that RAG models using multi-modal documents generally outperform those using only text documents, confirming that incorporating image documents during retrieval enhances the ability of MLLMs to answer questions. The performance gap narrows for Qwen2-VL, suggesting that different MLLMs exhibit varying levels of reliance on multi-modal documents.


\subsection{Case Study}
In this section, we show two cases from Qwen2-VL in the Multi-Modal QA task of M$^2$RAG to evaluate the effectiveness of the MM-RAIT method within the multi-modal retrieval contexts. More cases are shown in Appendix~\ref{app:case}.

As illustrated in Figure~\ref{fig:case_different_k}, in the first case, the question asks, ``What animal is included in the painting of John Campbell, 1st Baron Cawdor?''. This requires the MLLM to match the ``1st Baron Cawdor'' and extract information about animals in the painting. Due to limited internal knowledge, the model encounters hallucination issues and generates an incorrect answer, ``a lion''. When the retrieved multi-modal document of ``1st Baron Cawdor'' is fed into the MLLM, the vanilla RAG model can directly extract ``dog'' from the painting, thus providing the correct response. This highlights the importance of multi-modal information in offering more intuitive and richer semantic insights to answer the question, underscoring the effectiveness of constructing the M$^2$RAG benchmark.

In the second case, the question asks that, ``What weapon is the man in Daniel Maclis's A Scene from `Undine' (detail) holding?'' Based on retrieved documents, the vanilla RAG model focuses on the fifth document, which depicts a ``Scottish dirk''. This leads the vanilla RAG model to generate an incorrect response, ``holding a dirk''. After MM-RAIT training, the model can accurately identify the relevant document describing the man holding a sword and extract pertinent information from it, thereby generating the correct response.
