\section{Conclusion}
This paper introduces \textbf{M}ulti-\textbf{M}odal \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration (M$^2$RAG), a benchmark designed to evaluate MLLMs with retrieved multi-modal contexts across four tasks. To further enhance the utilization of retrieved information, we also propose a \textbf{M}ulti-\textbf{M}odal \textbf{R}etrieval-\textbf{A}ugmented \textbf{I}nstruction \textbf{T}uning (MM-RAIT) method, which optimizes MLLMs with multi-modal contexts as inputs, thereby improving their ability to effectively utilize retrieved information.

% In this paper, we introduce M$^2$RAG, a comprehensive multi-task and multi-perspective benchmark designed for evaluating the generation capability of MLLMs in multi-modal context scenarios. Our experiments reveal that current MLLMs still have deficiencies when dealing with multi-modal contexts. To address these shortcomings, we introduced our method MM-RAIT. Experiments demonstrate that our method enables the model to obtain a better capability to utilize multi-modal context information.