\begin{table*}[t]
\small
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccc|ccc|cc|c}
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Image Captioning}} & \multicolumn{3}{c|}{\textbf{Multi-Modal QA}} & \multicolumn{2}{c|}{\textbf{MM Fact Verification}} & \textbf{Image Reranking} \\ \cline{2-10} 
& \textbf{BLEU-4} & \textbf{ROUGE-L}& \textbf{CIDEr}& \textbf{BLEU-4} & \textbf{ROUGE-L}& \textbf{CIDEr}& \textbf{ ACC }& \textbf{F1}& \textbf{FIDâ†“} \\ \hline
\rowcolor{gray!8} \multicolumn{10}{l}{\textbf{MiniCPM-V 2.6 (8B)}} \\ \hline
\textbf{Vanilla RAG} & 1.91	& 17.58 & 18.39 & 13.84 & 32.78	& 82.65 & 43.03 & 41.13 & - \\ 
w/ top1 & 3.82 & 24.28 & 43.76 & 17.18	& 37.89	& 119.92 & 54.83 & 53.49 & 12.17 \\ 
w/ top3& 3.46 & 23.13	& 37.89 & 17.56 & 38.46	& 124.16 & 56.33 & 54.01 & 10.77 \\ 
w/ top5 & 3.29 & 22.82 & 36.09 & 17.15 & 37.99 & 114.21 & 55.33 & 52.69 & 11.15 \\ 
\textbf{MM-RAIT} & \textbf{6.25} & \textbf{32.77} & \textbf{77.08} & \textbf{26.37} & \textbf{53.21} & \textbf{266.47} & \textbf{60.17} & \textbf{60.22} & \textbf{10.32} \\ \hline

\rowcolor{gray!8}\multicolumn{10}{l}{\textbf{Qwen2-VL (7B)}} \\ \hline
\textbf{Vanilla RAG}& 2.24 & 19.48	& 26.01 & 18.77	& 39.05 & 153.40 & 45.43 & 34.23 & - \\ 
w/ top1& 3.79	& 25.43	& 46.32 & 21.21 & 42.14 & 187.99 & 51.60 & 41.05 & 12.17 \\ 
w/ top3& 3.62 & 25.70	& 45.08 & 20.98	& 41.90	&178.28 & 52.43 & 41.94 & 9.88 \\ 
w/ top5& 3.62	& 25.31	& 44.45 & 21.26	& 42.41	& 181.56 & 52.00 & 41.64 & 9.71 \\ 
\textbf{MM-RAIT} & \textbf{10.53}	& \textbf{39.79}	& \textbf{123.97} & \textbf{32.00} & \textbf{62.49} & \textbf{329.05} & \textbf{65.13}	& \textbf{62.97} & \textbf{9.15} \\ \hline
\end{tabular}%
}
\caption{\label{tab:overall}Overall Performance. We evaluate the performance of different RAG models implemented with MiniCPM-V 2.6 and Qwen2-VL on our M$^2$RAG benchmark. MM-RAIT uses the top-5 multi-modal documents for inference.}
\end{table*}