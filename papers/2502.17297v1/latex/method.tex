\section{Instruction Tuning for Multi-Modal Retrieval-Augmented Generation}
In this section, we present our Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT) method. First, we describe the framework for multi-modal Retrieval-Augmented Generation (RAG) (Sec.~\ref{method:rag}). Then, we introduce multi-task instruction tuning to enhance the performance of MLLMs in multi-modal RAG tasks (Sec.~\ref{method:mmtuning}).

\subsection{The Framework of Multi-Modal Retrieval-Augmented Generation}
\label{method:rag}

Given a query $q$, multi-modal RAG models first employ a retriever to search for query-relevant multi-modal documents $\mathcal{D}$ and then feed these documents to MLLMs to assist them in answering the query $q$. Each document $d \in \mathcal{D}$ can be either an image document or a text document. The multi-modal RAG framework consists of two main components: the multi-modal retrieval module and the retrieval-augmented generation module.

\textbf{Multi-Modal Retrieval.}
To retrieve documents from the multi-modal document collection $\mathcal{D}$, existing methods typically rely on multi-modal dense retrieval models~\cite{zhou2024marvel,zhou2024vista}.

Given a query $q$ and a multi-modal document $d$, multi-modal dense retrieval models, such as VISTA~\cite{zhou2024vista}, encode both as representations $h_q$ and $h_d$, respectively, and map them into an embedding space for retrieval: 
\begin{equation}
\small
h_q = \text{Enc} (q); h_d = \text{Enc}(d), 
\end{equation} 
where $\text{Enc}$ denotes the encoder model. The query $q$ can be either text or an image, and the multi-modal document $d$ can be a text document or an image document. For documents containing captions, both image features and image captions are fed into the encoder model.

Next, we compute the similarity score $S(q,d)$ between the representations $h_q$ and $h_d$ of the query and document:
\begin{equation} 
\small
S(q,d) = \text{Sim}(h_q,h_d), 
\end{equation} 
where $\text{Sim}$ denotes cosine similarity. We then perform a KNN search~\cite{johnson2019billion} to retrieve the top-$k$ most relevant multi-modal documents $\Tilde{\mathcal{D}}=\{d_1,...,d_k\}$ to the query $q$. During retrieval, the multi-modal retriever needs to conduct single-modality matching, cross-modality matching and modality routing in the embedding space~\cite{liu2022universal}.

\textbf{Multi-Modal RAG Module.}
After retrieval, we input the retrieved documents $\Tilde{\mathcal{D}}$ and query $q$ into the MLLM ($\mathcal{M}$), such as MiniCPM-V~\cite{yao2024minicpm} or Qwen2-VL~\cite{Qwen2VL}, to generate the output $y$:
\begin{equation}\label{eq:rag_module}
\small
y = \mathcal{M}(\Tilde{\mathcal{D}},q). 
\end{equation} 
These retrieved documents provide external knowledge, which helps to update the parametric memory of the MLLM, enabling it to generate more accurate responses to the query $q$.

\subsection{MM-RAIT: Multi-Task Multi-Modal Instruction Tuning for MLLMs}
\label{method:mmtuning}
To adapt MLLMs to the multi-modal RAG scenario, we propose the \textbf{M}ulti-\textbf{M}odal \textbf{R}etrieval-\textbf{A}ugmented \textbf{I}nstruction \textbf{T}uning (MM-RAIT) method, designed to further enhance the performance of MLLMs across various RAG tasks.

To improve the MLLM generation process, we incorporate external knowledge to assist in answering the query (Eq.\ref{eq:rag_module}). Specifically, we follow previous work~\cite{ram2023context} and concatenate the representations of the retrieved documents $\Tilde{\mathcal{D}}$ along with the query $q$ as the input for the MLLM ($\mathcal{M}$) to generate the output $y$: 
\begin{equation} 
\small
y = \mathcal{M}(\text{Instruct}_p,X(\Tilde{\mathcal{D}}),q), 
\end{equation} 
where $\text{Instruct}_p$ is the instruction for the task $p$, and $X(\Tilde{\mathcal{D}})$ denotes the concatenation of the representations of the retrieved documents: 
\begin{equation}\small
X(\Tilde{\mathcal{D}}) = X(d_1) \oplus \dots \oplus X(d_k). 
\end{equation} 
For the $i$-th retrieved document $d_i$, its representation can be the text sequence for a text document, the image features for an image document, or the concatenation of both image features and caption for an image document that contains a caption.

Next, we gather queries from three tasks to form the query set $Q$: image captioning, multi-modal question answering, and multi-modal fact verification. For each query $q$ in these tasks, the training objective for the model is to minimize the negative log-likelihood of generating the target sequence $y^*$: 
\begin{equation} 
\small
\mathcal{L} = - \sum_{q \in Q} \sum_{t=1}^T \log P(y^*_{t} \mid y^*_{<t}, \Tilde{\mathcal{D}}, q; \theta),
\end{equation} 
where $T$ is the length of the ground truth response, $y^*_{t}$ is the $t$-th token of the ground truth response, and $\theta$ represents the parameters of MLLM ($\mathcal{M}$).