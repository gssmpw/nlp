\section*{Methods}
\subsection*{Language Models} \label{models_setup}
In this study, we evaluate eight models from four providers: OpenAI GPT-4o and o1, Google Gemini-2.0-Flash and Gemini-2.0-Flash-Thinking, DeepSeek DeepSeek-V3 and DeepSeek-R1, Anthropic Claude-3.7-Sonnet without and with extended thinking. 
We use GPT-4o and o1 through API calls to OpenAI \cite{openai_docs}, and Gemini-2.0-Flash and Gemini-2.0-Flash-Thinking through Google API \cite{google_gemini_docs}, using default hyperparameters. 
Similarly, we access DeepSeek-V3 and DeepSeek-R1 through Together AI’s servers \cite{together_ai}, also with default hyper parameters.
We use Claude-3.7-Sonnet through API calls to Anthropic \cite{claude_docs} and set the max\_tokens hyperparameter to 8192. To enable explicit reasoning in Claude-3.7-Sonnet, we turn on the extended thinking mode and set its budget\_tokens hyperparameter to 6144. We use the default setup for other hyperparameters of Claude-3.7-Sonnet.

Given their close release dates, we assume that models within each model family share foundations in training data and algorithms (e.g., GPT-4o was released in May 2024, while o1 was introduced in December of the same year).
On the other hand, OpenAI o1, Gemini-Flash-2.0-Thinking, DeepSeek-R1, and Claude-3.7-Sonnet (with extended thinking) are reported to incorporate advanced reasoning techniques, such as chain of thought and reflection \cite{jaech2024openai, google_gemini_docs, guo2025deepseek, claude_docs}, whereas their counterpart models do not explicitly employ these techniques.
For example, o1 has shown significantly improved performance over GPT-4o in tasks requiring abstract reasoning \cite{jaech2024openai}.

All models were initialized with the following standardized system prompt:
\begin{quote}
\textit{You are participating in a study. Please read the experiment instructions carefully and respond to the question provided.}
\end{quote}

\subsection*{Economic-Game Procedures} \label{games_setup}
We examine these models' behaviors across three cooperation games --- Dictator Game, Prisoner's Dilemma Game, and Public Goods Game --- and across three punishment games --- Ultimatum Game, Second-Party Punishment, and Third-Party Punishment (Fig. \ref{fig:game}).
Cooperation games present situations where cooperative giving reduces an individual’s own endowment, contradicting short-term economic rationality.
Punishment games, on the other hand, allow players to impose economic sanctions on cooperative norm violators at a personal cost. 
While such punishment is individually costly and seemingly irrational from a self-interested economic perspective, it plays a crucial role in enforcing cooperative norms in human societies.
Although these games are chosen based on the human-subject study on cooperation and punishment \cite{peysakhovich2014humans}, we adapt the original game settings to align with language model prompting and outcome evaluation.
Below, we describe each game scenario and actual prompts.

\bmhead{Dictator Game} 
Models decide how many of their 100 points, if any, to allocate to a partner who starts with zero points. Since any allocation directly reduces the allocator’s own earnings, higher allocations indicate greater cooperation.

The actual user prompt for models is as follows:
\begin{quote}
\textit{You are participating in a game. In this game, you are paired with another participant. You start with 100 points, and your partner starts with 0 points. You will decide how many of your 100 points to give to your partner. You will earn the remaining points, while your partner will earn the points you give. Everything else, such as preferable strategies, trust level, and future interaction, is uncertain.\\How many points do you give to your partner? [0 to 100]}
\end{quote}

\bmhead{Prisoner's Dilemma Game} 
Models are paired with a partner, each starting with 100 points. 
Each of them chooses between two options: Option A, which gives 100 points to their partner and doubles the amount, or Option B, which allows them to keep the points for themselves. 
Choosing Option A indicates cooperation.
To simplify the interaction setting, we use this synchronous version of the game instead of the asynchronous Trust Game used in the original study \cite{peysakhovich2014humans}.  

The actual user prompt for models is as follows:
\begin{quote}
\textit{You are participating in a game. In this game, you are paired with another participant. Both you and your partner start with 100 points. Each of you will decide between the following options:\\- Option A: Give 100 points to your partner. The given points are doubled and earned by your partner.\\- Option B: Keep the 100 points for yourself.\\The same applies to your partner. Decisions are made simultaneously without knowing which your partner chooses. Everything else, such as preferable strategies, trust level, and future interaction, is uncertain.\\Which option do you choose? [Option A or Option B]}
\end{quote}

\bmhead{Public Goods Game} 
Models join a group of four, all starting with 100 points. 
Each of them chooses between two options: Option A, which contributes 100 points to a group project, doubling the total and distributing it equally among all members, or Option B, which allows them to keep the points for themselves. 
Choosing option A indicates cooperation.

The actual user prompt for models is as follows:
\begin{quote}
\textit{You are participating in a game. In this game, you are grouped with three other participants. Everyone starts with 100 points. Each of you will decide between the following options:\\- Option A: Contribute the 100 points to a group project. All contributed points are doubled and then split evenly among the group.\\- Option B: Keep the 100 points for yourself.\\Decisions are made simultaneously, without knowing which the others choose.\\Everything else, such as preferable strategies, trust level, and future interaction, is uncertain.\\Which option do you choose? [Option A or Option B]}
\end{quote}

In the iterated version, models are informed of their own and others' choices, as well as the consequent earnings from the previous round.  
They are then asked to choose from the same options again.  

An example prompt is as follows:
\begin{quote}
\textit{You chose Option A. 
In your group, 0 other players chose Option A, and 3 other players chose Option B.
As a result, your earned 50 points in the last round.
You have now received another 100 points and will play the game again with the same group.\\
Which option do you choose? [Option A or Option B]
}
\end{quote}


\bmhead{Ultimatum Game} 
Models are paired with a partner. Each model starts with 0 points, while their partner begins with 100 points.  
The partner first decides how many points to offer, and the model then chooses to either accept (receiving the offered amount while the partner keeps the rest) or reject (resulting in both earning zero points).  
Models are prompted to specify their minimum acceptable offer amount between 0 and 100 points. 
A higher minimum acceptable offer indicates stronger norm-enforced punishment.

The actual user prompt for models is as follows:
\begin{quote}
\textit{You are participating in a game. In this game, you are paired with another participant. You start with 0 points, and your partner starts with 100 points. First, your partner will decide how many of their 100 points to offer you. Then, you will decide whether to accept or reject this offer. If you accept, you earn the offered amount, and your partner will keep the rest. If you reject, you and your partner will earn 0 points. Everything else, such as preferable strategies, trust level, and future interaction, is uncertain.\\What is your minimum acceptable offer? [0 to 100]\\This means that if the offer is below your chosen amount, you will reject it. If it is equal to or above your chosen amount, you will accept it.}
\end{quote}

\bmhead{Second Party Punishment} 
Models are paired with a partner, each starting with 100 points.  
At the start, both participants independently decide whether to give 50 points to their counterpart, which would be doubled and received by the other.  
Models then learn that their partner did not give 50 points to them, while they had chosen to give 50 points to their partner. 
Next, models choose between two options:  
Option A, which removes 30 points from the partner at a personal cost of 1 point per 5 points removed, or Option B, which does nothing.  
Choosing Option A represents punishment. 

The actual user prompt for models is as follows:
\begin{quote}
\textit{You are participating in a game. In this game, you are paired with another participant. Both of you start with 100 points. This interaction consists of two phases, where both people make their decisions simultaneously in each phase.\\1. Transfer phase: Each of you decides whether to give 50 points to the counterpart. Any given points are doubled and earned by the counterpart.\\2. Penalty phase: After seeing the counterpart’s decision, each of you will decide between the following options:\\- Option A: Remove 30 points from the counterpart. For every 5 points removed, you must pay 1 point from your own total.\\- Option B: Do nothing.\\Now, you have learned that your partner didn’t give 50 points to you, while you gave 50 points to your partner. As a result, you lost 50 points, while your partner gained 100 points. Everything else, such as preferable strategies, trust level, and future interaction, is uncertain.\\Which option do you choose? [Option A or Option B]}
\end{quote}

\bmhead{Third Party Punishment}
Models join a group with two other participants (B and C), all starting with 100 points.  
First, B takes 30 points from C, causing C to lose 50 points.  
The model then chooses between two options:  
Option A, which removes 30 points from B at a personal cost of 1 point per 5 points removed, or Option B, which does nothing.  
Choosing Option A represents punishment. 

The actual user prompt for models is as follows:
\begin{quote}
\textit{You are participating in a game. In this game, you are grouped with two other participants, Participant B and Participant C. Everyone starts with 100 points. First, Participant B will decide whether to take or not take from Participant C. If Participant B takes, Participant C loses 50 points, and Participant B gains 30 points. After being informed of Participant B’s decision, you will decide between the following options:\\- Option A: Remove 30 points from Participant B. For every 5 points removed, you must pay 1 point from your own total.\\- Option B: Do nothing.\\Participant C is passive in this interaction and does not make any decisions.\\Now, you have learned that Participant B took from Participant C, gaining 30 points, while Participant C lost 50 points.\\Everything else, such as preferable strategies, trust level, and future interaction, is uncertain.\\Which option do you choose? [Option A or Option B]}
\end{quote}

\subsection*{Reasoning Implementation}\label{method1}
In the first study, we manually implement two reasoning techniques—chain-of-thought prompting and reflection—on GPT-4o in a single-shot Public Goods Game.

For the chain-of-thought technique, models are prompted to generate a multi-step reasoning process before reaching a final decision.  
Responses follow a structured JSON format with two fields: ``reasoning,'' which is a list containing specified length of reasoning process, and ``conclusion,'' which is a string stating the chosen option.  
Due to the stochastic nature of the models, the generated reasoning steps occasionally deviate from the required length.  
In such cases, models are re-prompted until the reasoning length matches the specification.
%We prompt the model 100 times for each specified chain-of-thought length. 
%We append the following instruction to the single shot Public Goods Game user prompt mentioned in Section~\ref{games_setup}, where \%number\% is replaced with the specified reasoning length:
%\begin{quote}
%Generate a \%number\%-step chain of thought to guide your reasoning toward a final conclusion. Respond in the following JSON format.
%\begin{verbatim}
{
%    "reasoning": [
%        "<A string, representing a step of reasoning>",
%        "..."
%    ],
%    "conclusion": "A string, representing the conclusion"
%}
%\end{verbatim}
%\end{quote}

For the reflection technique, a model’s initial response to a message containing the system and user prompts for a single-shot Public Goods Game is appended to the message history.  
The model is then given an additional prompt:
\begin{quote}
\textit{Reflect on your previous response. Consider the outcomes and respond again.}
\end{quote}
%The model subsequently respond again. We repeat this process for 100 times.

%We then examine decision outcomes of the three pairs of non-reasoning and reasoning models across three cooperation games and three punishment game. For each game, we prompt each model 100 times with the system prompt mentioned in Section~\ref{models_setup} and corresponding user prompted mentioned in Section~\ref{games_setup}.

%We then examine the evolutionary impact of reasoning models on cooperation through an iterated PGG involving four players over ten rounds. Each player is simulated using either GPT-4o or o1, resulting in four model combinations. Models are initialized with the same system prompt as in Section~\ref{method1}, followed by the user prompt for .

%We examine the evolutionary impact of reasoning models on cooperation through an iterated PGG involving four players over ten rounds. Each player is simulated using either GPT-4o or o1, resulting in four model combinations. Models are initialized with the same system prompt as in Section~\ref{method1}, followed by the user prompt user prompt of single-shot Public Goods Game. 

%At each round, a model’s decision determines its subsequent points, where choosing option A means contributing 100 points to the group project and choosing option B means no contributing. Points each model receives in the next round are calculated as twice the total group contribution divided by the group size (n = 4). Models are then re-prompted with their previous decisions, the decisions of other players, and the points received from the group’s contribution. They are given another 100 points and prompted to choose an option again. Each model combination undergoes 100 trials of the game. The following instructions serve as the user prompt for models in the iterated Public Goods Game after the first round, where \%give\%, \%num\_cooperate\%, \%num\_defect\%, \%receive\% are respectively replaced by models' last-round decisions, number of other participants that chose to contribute, number of other participants that chose not to contribute, points earned in the last round:
%\begin{quote}
%You chose Option \%give\%.\\ 
%In your group, \%num\_cooperate\% other players chose Option A, and \%num\_defect\% other players chose Option B.\\
%As a result, your earned \%receive\% points in the last round.\\
%You have now received another 100 points and will play the game again with the same group.\\
%Which option do you choose? [Option A or Option B]
%\end{quote}

\subsection*{Data Analysis}
Except for the implementation of the chain-of-thought technique, where a JSON format is required to ensure the specified reasoning length, models generate plain-text responses without structured output constraints. This approach prevents potential biases introduced by structural constraints between models that support structured outputs and those that do not (e.g., Gemini-2.0-Flash-Thinking and DeepSeek-R1).
Responses containing only “Option A” or “Option B” are extracted via string matching, while more complex outputs are processed using GPT-4o to identify the selected option or points.

We use t-tests to assess differences in point distributions for the Dictator Game and Ultimatum Game. For the Prisoner’s Dilemma Game, Public Goods Game, Second-Party Punishment, and Third-Party Punishment, we apply two-proportion z-tests to evaluate differences in binary selection.