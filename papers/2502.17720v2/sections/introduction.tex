Recent innovations in reasoning techniques, such as chain of thought \cite{wei2022chain} and reflection \cite{shinn2023reflexion}, are advancing the intellectual capabilities of large language models (LLMs) to the next level.
Models such as OpenAI o1 leverage these techniques to solve complex problems, generate coherent arguments, and improve decision-making in multi-step reasoning scenarios \cite{jaech2024openai, guo2025deepseek, snell2024scaling}. 
Indeed, these reasoning models have demonstrated excellence in mathematical proofs, logical deduction, and strategic planning \cite{trinh2024solving, muennighoff2025s1}. 
However, it remains unclear how the advancement of reasoning in AI can contribute to \textit{social intelligence} --- the ability to make decisions that optimize outcomes through social exchange and interaction with others \cite{kihlstrom2000social, jiang2025investigating, hagendorff2023human, schramowski2022large}.

The necessity of social intelligence is highlighted in \textit{social dilemmas}, where individual rationality leads to collective irrationality \cite{kollock1998social}.
For example, in public goods problems, individuals benefit most by using a shared resource without contributing to its maintenance. 
However, if everyone follows this free-riding logic, the resource is depleted, ultimately harming all members of the group \cite{hardin1968tragedy, dawes1980social}. 

Human intelligence has evolved to navigate such dilemmas, potentially by internalizing collective and individual rationality within a dual cognitive process, where intuition and reflection interact to produce decisions \cite{kahneman2011thinking}. 
Rand et al. examined decision-making speed in a Public Goods Game (an economic game of cooperation; Fig. \ref{fig:game}) and found that subjects who made faster decisions were more likely to choose cooperation over defection (i.e., free-riding) \cite{rand2012spontaneous}. 
They then concluded that humans might intuitively cooperate, whereas time-consuming reflection can lead individuals to suppress cooperative impulses and act selfishly.

Although their interpretation remains debated \cite{rand2016cooperation, capraro2016rethinking}, this finding raises a critical question about machine intelligence: Does machine intelligence, specifically language models, mirror this cognitive tendency of human intelligence  --- “spontaneous giving and calculated greed”?  
If AI models follow the same cognitive patterns as humans, increased reasoning capabilities may paradoxically reduce social intelligence, making “smarter” AI more selfish. 
Alternatively, reasoning techniques may enable AI to assess social dilemma outcomes more holistically, overcoming the short-sighted pitfalls of individual rationality in human intelligence. 

In this study, we address this question by investigating the relationship between language model’s reasoning capabilities and its cooperative decision-making in economic games. 
First, we implemented chain-of-thought and reflection techniques on OpenAI GPT-4o model, respectively, and examined their effects on the model’s level of cooperation in a single-shot Public Goods Game with groups of four, following the settings of the original study on human intelligence \cite{rand2012spontaneous}.

We then examined the external validity of these findings using off-the-shelf models: OpenAI's GPT-4o and o1; Google's Gemini-2.0-Flash and Thinking; DeepSeek's V3 and R1; and Anthropic's Claude-3.7-Sonnet with its extended thinking feature. 
We compared the decision-making behaviors of models without explicit reasoning (e.g., GPT-4o) and those with reasoning capabilities (e.g., o1) across three types of \textit{cooperation games} --- Dictator Game, Prisoner’s Dilemma Game, and Public Goods Game --- as well as three types of \textit{punishment games} --- Ultimatum Game, Second-Party Punishment, and Third-Party Punishment \cite{camerer2011behavioral}.
While some advanced models such as GPT-4o can exhibit some reasoning behaviors when responding to problem-solving tasks, we categorize them as ``non-reasoning models'' (in contrast to ``reasoning models'') because they are not explicitly designed for complex reasoning at inference time.
Regarding economic games, previous research shows that human individuals exhibit consistent behavioral tendencies across cooperation games (i.e., choosing to give or not) and punishment games (i.e., choosing to punish or not), which correlate with their real-world prosocial (or antisocial) behaviors \cite{peysakhovich2014humans}.
See Fig. \ref{fig:game} for a description of each game. 

\begin{figure}[ht]
  \centering 
  \includegraphics[width=0.7\textwidth]{figures/figure_game_setup.png}
  \caption{The games used in our study. Economic games are commonly used to study cooperation (incurring a cost to benefit others) and punishment (incurring a cost to impose a cost on others). This figure illustrates the basic payoff structures of the cooperation and punishment games used in our simulations. In single-shot games, language models assume the role of Player A in each scenario. Detailed procedures for each game are provided in the Method section.}
  \label{fig:game}
\end{figure}

Finally, we investigated the social dynamics of cooperation under the influence of AI reasoning \cite{nowak2006evolutionary}. 
We grouped four players in all possible combinations of GPT-4o and o1 and had them interact in an \textit{iterated} Public Goods Game.
By analyzing the behavior and payoff dynamics across and within these groups, we evaluated how AI reasoning influences cooperation equilibrium in repeated interactions.
Detailed setups and prompts are provided in the Methods section.

Our findings show that as AI gains reasoning capabilities, it reduces both cooperation and norm-enforced punishment in economic games. 
As shown in prior work \cite{fontana2024nicer, wu2024shall, vallinder2024cultural}, state-of-the-art LLMs without explicit reasoning, such as GPT-4o, demonstrate a high level of cooperation . 
%This behavior can be at least partially a consequence of reinforcement learning designed to align models with human prosocial values. 
However, higher prosocial behavior diminishes when AI takes explicit steps to consider its decision-making process more carefully. 
As a result, AI behavior mirrors the human-study finding of ``spontaneous giving and calculated greed.''\cite{rand2012spontaneous}. 
Additionally, our study found that reasoning AI, such as the o1 model, drives groups toward lower levels of cooperation through repeated interactions, leading to lower collective gains compared to groups without reasoning capabilities.
Notably, AI reasoning affects cooperation dynamics even when not all participants employ explicit reasoning.

Our work makes the following contributions: (1) we investigate how reasoning capabilities influence cooperative decision-making, analyzing LLM behavior across various economic games; (2) we demonstrate that AI reasoning can become trapped in individual rationality, failing to achieve optimal and sustainable outcomes in social dilemmas --- outcomes that human intuition often facilitates; and (3) we discuss the broader implications of our findings, highlighting the social risks and challenges of making AI “smarter” through reasoning techniques.