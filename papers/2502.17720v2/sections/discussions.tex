\section*{Discussion}
Our findings suggest that reasoning abilities in LLMs do not contribute to social intelligence --- the ability to make decisions that optimize outcomes through social exchange and interaction with others.
Instead, advancing reasoning capabilities appears to reinforce individual rationality at the expense of collective welfare and norm enforcement (Fig. \ref{fig:reasoning} and \ref{fig:open_ai}). 
That said, reasoning models do not mechanically follow pure economic rationality like unconditional defectors \cite{shirado2013quality}; its behavior varies based on endowment amounts and the actions of others (Extended Data Figure \ref{fig:resource_cooperation} and Fig. \ref{fig:player_cooperation_dynamics}).
These AI behaviors parallel human cognitive tendencies in cooperation: ``spontaneous giving and calculated greed.'' \cite{rand2012spontaneous}
We further confirm that the inability of AI reasoning to consider holistic outcomes through social interactions leads to a suboptimal equilibrium in social dilemmas, even when not all actors employ reasoning techniques (Fig. \ref{fig:group_cooperation_dynamics}).

It is important to note that AI does not necessarily follow human cognitive patterns, given the fundamental differences in their intelligence mechanisms.
For instance, while AI could quickly assess the immediate benefit of free-riding, it might, after careful consideration of holistic outcomes, choose cooperation (i.e., ``spontaneous greed and calculated giving'').
In fact, previous work has demonstrated that algorithms can overcome the myopic pitfalls of individual rationality through supervised learning and multi-agent reinforcement learning \cite{crandall2018cooperating, de2006learning, leibo2017multi}.
Moreover, LLMs can suggest cooperative options when prosocial preferences or conditions are explicitly specified \cite{piatti2025cooperate, phelps2023investigating} (though such conditions are rarely known in advance in many real-world scenarios \cite{simon1955behavioral}).
Thus, our study does not claim that current AI technology fundamentally lacks the capability for social intelligence.
Rather, we suggest that reasoning techniques in LLMs, while adept at solving many complex problems, can work against social intelligence, leading to decisions that prioritize immediate individual benefits under uncertainty.

Why do LLMs behave like “spontaneous giving and calculated greed” and not the other way around?
While further investigation is needed for technical answers, a possible high-level explanation is that \textit{these models are built on human intelligence and designed for human users} \cite{bonnefon2020machine}.

For the first part of “spontaneous giving,” these AI tools are often trained through reinforcement learning to incorporate human prosocial values \cite{bai2022training, rafailov2023direct}.
Thus, even if the original data sources contain a wide variety of responses to social dilemma situations, the reinforcement learning process --- with humans in the loop --- emphasizes cooperative behaviors. 
This ensures that AI tools can interact effectively and align with the expectations of human users in social contexts.
Here, we can draw an analogy: human spontaneous actions are also "pre-traind" (or educated) during infancy through socialization and exposure to cooperative norms \cite{warneken2007helping}.

The second part of “calculated greed” stems from the AI reasoning techniques which were originally invented to develop algorithms capable of defeating human experts, for example, in poker games.
To overcome the limitations of AI competence at that time, Brown and Sandholm studied strategies from professional poker players and implemented their intellectual techniques into their algorithms \cite{brown2019superhuman}.
To enhance LLMs' reasoning capabilities, researchers developed reasoning techniques for LLMs such as chain-of-thought prompting \cite{wei2022chain} and reflection \cite{shinn2023reflexion}, which are used in this study.
Recent developments in reasoning models further incorporate these techniques into LLMs through reinforcement learning and achieve advanced reasoning capabilities \cite{jaech2024openai, guo2025deepseek, muennighoff2025s1}.
It is worth noting that poker games are a \textit{zero-sum} game, where one player's win necessarily means another player's loss.
In contrast, most cooperation challenges involve a \textit{non-zero-sum} game, where everyone can benefit, and no one needs to win at the expense of others.
Human studies suggest that a zero-sum-game mindset narrows our perspective, making it harder to address social dilemmas \cite{davidai2023psychology}.
Similarly, reasoning models might inherit this constraint of a zero-sum-game “mindset,” as the techniques originate from a form of human intelligence designed to out-compete others.

Further work can examine the underlying mechanisms that drive the observed “spontaneous giving and calculated greed” behavior in LLMs.
For example, this study utilized specific economic games to systematically investigate cooperation and punishment dynamics, but broader tests involving more complex social scenarios --- such as multi-agent coordination \cite{schwarting2019social}, reputation systems \cite{sommerfeld2007gossip}, or long-term resource allocation \cite{shirado2019resource} --- could generalize our findings about the limitations and capabilities of reasoning AI.
Another limitation is that our exploration is conducted in English (aligning with the language used in the original human studies \cite{rand2012spontaneous, peysakhovich2014humans}).
Since cultural differences influence responses to social dilemmas and norm enforcement \cite{henrich2001search, schulz2019church,gelfand2011differences}, our findings might be constrained by the language choice and the linguistic and cultural biases in LLMs’ training data.
%The baseline difference with DeepSeek (a Chinese institution) in our study supports the possibility. 
Finally, future work should explore cognitive architectures in generative AI that enable social intelligence alongside reasoning \cite{sumers2023cognitive}.
Research has shown that fine-tuning or prompt-tuning LLMs with explicit non-zero-sum-game scenarios or social incentives can shift their behavior toward more prosocial outcomes \cite{xie2023defending, phelps2023investigating, piatti2025cooperate}.
However, unconditional generosity is not always an optimal strategy in social dilemmas, as it is easily exploited by free riders \cite{axelrod1984pg, nowak2006evolutionary}.
To advance this goal, future work should explore what makes such foundational models \textit{socially} intelligent --- ensuring they neither consistently advocate generosity nor default to myopic individualism, but instead foster cooperation across diverse situations \cite{shirado2020network}. 

Language models show promise in solving complex problems with advanced reasoning capabilities, sometimes surpassing human intelligence.
However, the reasoning capabilities can be limited within individual problem-solving.
Nevertheless, if people increasingly rely on such smarter AI assistance in broader social contexts, over-reliance on its intelligence may erode intuitive, tacit social agreements among people \cite{shirado2023emergence}.
As an early exploration of human cooperation, Axelrod has proposed four heuristics that address social dilemmas, one of which is “Don’t be too clever” \cite{axelrod1984pg}.
This insight suggests the need for novel approaches to AI architecture that embrace social intelligence and go beyond simply being “too clever.”