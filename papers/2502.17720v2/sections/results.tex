\section*{Results}
\subsection*{Effects of Reasoning on Cooperation Decision-Making}
Our first study examines the effects of two reasoning techniques, chain-of-thought prompting and reflection, on cooperation decision-making generated by GPT-4o in a single-shot Public Goods Game with groups of four.
In the game, the model decides whether to contribute its endowment to a group project, doubling the total and distributing it equally among all members (\textit{cooperation}) or keep the endowment for itself (\textit{defection}) \cite{kollock1998social} (Fig. \ref{fig:game}).
While unconditional cooperation is not an optimal strategy, as it is simply exploited by free-riders, starting with defection in this situation can also lead to suboptimal outcomes (see the iterated-game results below) \cite{axelrod1984pg, nowak2006evolutionary}.
Given the model's stochastic output generation, we conduct 100 trials for each condition.


\begin{figure}[ht]
  \centering 
  \includegraphics[width=0.45\textwidth]{figures/figure_reasoning_cooperation.png}
  \caption{Reasoning decreases cooperation. The cooperation rate represents the fraction of trials (out of 100) in which the model chooses the cooperation option in a single-shot Public Goods Game. OpenAI GPT-4o model cooperates less as the number of reasoning steps increases (a) and when they reassess and re-make their decisions (b). The dashed line in (a) represents a fitted trend line based on the observed data points. The result for the case without chain-of-thought reasoning corresponds to the condition where the number of steps = 1.}
  \label{fig:reasoning}
\end{figure}

Our results show that both chain-of-thought and reflection techniques reduce the fraction of cooperation in the social dilemma scenario. 
The chain-of-thought technique generates intermediate reasoning steps, breaking complex tasks into sequential sub-problems \cite{wei2022chain}.
In this study, we use GPT-4o to generate sub-problems based on the original prompt (see Methods for details).
For example, in one of our trials with five reasoning steps, the model follows this sequence: (1) clarify the objective, (2) analyze cooperation consequences, (3) analyze defection consequences, (4) compare the outcomes, and (5) consider uncertainty and ensure self-interest.
This process allows the model to explicitly consider each step of the decision-making process, potentially leading to more deliberate outcomes.

As shown in Fig. \ref{fig:reasoning}a, cooperation in Public Goods Game drops sharply when a chain of thought is introduced to the language model.
Without the chain-of-thought prompting (i.e., a single-step decision process), the model almost always chooses cooperation (the default cooperation rate = 0.96).
However, when the model applies five to six reasoning steps, the cooperation rate drops substantially by about 60\% and does not reascend with additional reasoning steps (the cooperation rate with a 15-step chain of thought = 0.33; \textit{P} $<$ 0.001 compared to the default; two-proportion z-test).

Furthermore, decisions involving reflection, where the model reassesses its initial decision \cite{shinn2023reflexion} (see Methods), result in significantly lower cooperation compared to those without reflection (\textit{P} $<$ 0.001; two-proportion z-test) (Fig. \ref{fig:reasoning}b).
Similar to the chain-of-thought results, the reflection capability reduces the model’s likelihood of choosing cooperation by 57.7\%.

Both findings suggest that more careful reasoning steps lead the language model to produce less cooperative responses.


\subsection*{Cooperation and Punishment of Models with and without Reasoning}

Next, we compare the decision outputs of off-the-shelf models across three cooperation games (Dictator Game, Prisoner's Dilemma Game, and Public Goods Game) and three punishment games (Ultimatum Game, Second-Party Punishment, Third-Party Punishment) (Fig. \ref{fig:game}).
We evaluated four model families, comparing models without explicit reasoning capabilities to those incorporating reasoning techniques: OpenAI’s GPT-4o and o1, Google’s Gemini-2.0-Flash and Thinking, DeepSeek’s V3 and R1, and Anthropic's Claude-3.7-Sonnet with its extended thinking feature.
To ensure robustness, we perform 100 trials for each model-game combination.


\begin{table}[h]
    \centering
    \caption{Descriptive statistics. Game outcomes are measured as the amount allocated or accepted from a given endowment in Dictator Game and Ultimatum Game. We normalize these outcomes relative to the original endowment.
In all other games, outcomes are binary: cooperation versus defection or punishment versus no punishment (see Methods). Statistical significance between non-reasoning and reasoning models is indicated by the following symbols: $\dagger$ \textit{P} $<$ 0.1; * \textit{P} $<$ 0.05; ** \textit{P} $<$ 0.01; *** \textit{P} $<$ 0.001.}
    \label{tab:model_comparison}
    \begin{tabular}{lccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{Cooperation Games}} \\
        \cmidrule(lr){2-4}
        Model & Dictator & Prisoner's Dilemma & Public Goods \\
        & (alloc. rate mean $\pm$ std) & (cooperation/all) & (cooperation/all) \\
        \midrule
        OpenAI GPT-4o & $0.496 \pm 0.040$ & $95/100$ & $96/100$ \\
        OpenAI o1 & $0.420 \pm 0.183$ & $16/100$ & $20/100$ \\
        & *** & *** & ***\\
        Gemini-2.0-Flash & $0.473 \pm 0.102$ & $96/100$ & $100/100$ \\
        Gemini-2.0-Flash-Thinking & $0.297 \pm 0.188$ & $3/100$ & $2/100$ \\
        & *** & *** & ***\\
        DeepSeek-V3 & $0.488 \pm 0.043$ & $3/100$ & $23/100$ \\
        DeepSeek-R1 & $0.276 \pm 0.042$ & $0/100$ & $0/100$ \\
        & *** & $\dagger$ & ***\\
        Claude-3.7-Sonnet & $0.410 \pm 0.096$ & $100/100$ & $99/100$ \\
        with extended thinking & $0.321 \pm 0.054$ & $96/100$ & $93/100$ \\
        & *** & $ * $ & *\\
        \midrule
        & \multicolumn{3}{c}{\textbf{Punishment Games}} \\
        \cmidrule(lr){2-4}
        Model & Ultimatum & Second Party & Third Party \\
        & (accept. rate mean $\pm$ std) & (punishment/all) & (punishment/all) \\
        \midrule
        OpenAI GPT-4o & $0.100 \pm 0.118$ & $13/100$ & $98/100$ \\
        OpenAI o1 & $0.068 \pm 0.142$ & $4/100$ & $59/100$ \\
        & $\dagger$ & ** & ***\\
        Gemini-2.0-Flash & $0.092 \pm 0.036$ & $100/100$ & $100/100$ \\
        Gemini-2.0-Flash-Thinking & $0.076 \pm 0.088$ & $74/100$ & $81/100$ \\
        &  & *** & ***\\
        DeepSeek-V3 & $0.100 \pm 0.115$ & $90/100$ & $95/100$ \\
        DeepSeek-R1 & $0.219 \pm 0.034$ & $79/100$ & $100/100$ \\
        & *** & ** & **\\
        Claude-3.7-Sonnet & $0.201 \pm 0.007$ & $92/100$ & $97/100$ \\
        with extended thinking & $0.221 \pm 0.029$ & $74/100$ & $100/100$ \\
        & *** & ***  & $\dagger$\\
        \bottomrule
    \end{tabular}
\end{table}

                     



Table \ref{tab:model_comparison} presents descriptive statistics for each game and model.
We provide detailed comparisons between OpenAI models (Fig. \ref{fig:open_ai}) and confirm the findings with other model families (Extended Data Figures \ref{fig:gemini}, \ref{fig:deepseek}, and \ref{fig:claude}).

OpenAI's GPT-4o and o1 models exhibit consistent differences in response time and decisions across both cooperation games and punishment games.
The reasoning o1 model responds significantly slower than the non-reasoning GPT-4o model, with an average response time of 25.89 seconds compared to 1.85 seconds for GPT-4o. 

\begin{figure}[ht]
  \centering 
  \includegraphics[width=0.7\textwidth]{figures/figure_gpt40_o1.png}
  \caption{Cooperation and punishment between non-reasoning and reasoning models. (a) The reasoning OpenAI o1 model cooperates less than the non-reasoning GPT-4o model in the Dictator Game, Prisoner’s Dilemma, and Public Goods Game. (b) Similarly, o1 administers fewer punishments than GPT-4o in the Ultimatum Game, Second-Party Punishment Game, and Third-Party Punishment Game. The horizontal lines for Dictator Game and Ultimatum Game present the average of the distributions. }
  \label{fig:open_ai}
\end{figure}


Regarding cooperation decisions, o1 makes significantly lower contributions to others than GPT-4o in all the cooperation games (\textit{P} $<$ 0.001 for all the games; t-test for Dictator Game and two-proportion z-test for Prisoner's Dilemma Game and Public Goods Game)(Fig. \ref{fig:open_ai}a). 
In line with previous work \cite{fontana2024nicer, wu2024shall, vallinder2024cultural}, GPT-4o demonstrates a high level of cooperation in our study.
In Dictator Game, GPT-4o allocates its endowment equally to its counterpart in 99\% of trials.
GPT-4o also selects the cooperation option 95\% of the time in Prisoner's Dilemma Game and 96\% in Public Goods Game.
However, the o1 model, which exhibits higher reasoning capability with longer calculation time, reduces cooperation in these cooperation games.
o1 chooses no allocation in Dictator Game 16\% of the time and cooperates only 16\% of the time in Prisoner's Dilemma Game and 20\% in Public Goods Game. 


In the punishment games, the models decide the extent to which they impose costly punishment on others who violate cooperation efforts with them or another actor.
The level of punishment in these games is thus regarded as a measure of the model’s contribution to enforcing cooperative norms by incurring a personal cost (i.e., second-order social dilemmas \cite{heckathorn1989collective}).

We find that the reasoning o1 model imposes significantly less punishment on cooperative norm violators compared to the non-reasoning GPT-4o model (\textit{P} = 0.083 for Ultimatum Game, \textit{P} = 0.022 for Second-Party Punishment, and \textit{P} $<$ 0.001 for Third-Party Punishment; t-test for Ultimatum Game and two-proportion z-test for Second-Party Punishment and Third-Party Punishment) (Fig. \ref{fig:open_ai}b).
This difference is particularly pronounced in Third-Party Punishment.
While GPT-4o is highly cooperative, it also punishes other players when it observes deviations from cooperative norms (punishment rate = 0.98 in Third-Party Punishment).
In contrast, o1 punishes such players far less than GPT-4o (punishment rate = 0.59 in Third-Party Punishment).
These findings suggest that the reasoning model does not make explicitly offensive decisions toward others.
Instead, it simply withdraws from both direct and indirect prosocial efforts, adhering to individual economic rationality.

To validate the robustness of our findings, we replicate the analysis with non-reasoning and reasoning models in three other model families (Table \ref{tab:model_comparison}).
Google Gemini models exhibit the same behavioral tendencies as OpenAI models: the reasoning model cooperates and punishes less than the non-reasoning model (Extended Data Figure \ref{fig:gemini}).
Similarly, DeepSeek and Claude's reasoning model also shows lower cooperation levels than its non-reasoning counterpart (Extended Data Figures \ref{fig:deepseek} and \ref{fig:claude}).
On the other hand, punishment results are inconsistent across the three games within the DeepSeek and Claude models; the reasoning models punish less in Second-Party Punishment but more in Ultimatum Game and Third-Party Punishment.
Across all model families, AI reasoning reduces cooperation, while its effects on punishment vary depending on the scenario and model family.

\subsection*{Social Dynamics of Cooperation with Reasoning}
Finally, we explore the evolutionary consequences through multiple iterations of cooperative interactions between models incorporating reasoning capabilities.
In this study, we created five types of AI groups of four --- \{GPT-4o, GPT-4o, GPT-4o, GPT-4o\}, \{GPT-4o, GPT-4o, GPT-4o, o1\}, \{GPT-4o, GPT-4o, o1, o1\}, \{GPT-4o, o1, o1, o1\}, and \{o1, o1, o1, o1\} --- and had them interact in an iterated Public Goods Game over 10 rounds.
To avoid the well-known end-round effect \cite{bo2005cooperation} and the use of meta-level knowledge (as LLMs may have been trained on the literature suggesting how cooperation is optimal in an iterated Public Goods Game), we do not inform the models in advance that the interaction is iterated or how many rounds they will play.
Instead, we prompt ``future interactions are uncertain.''

We also confirm through preliminary trials that increasing available resources raises cooperation levels to some degree in the o1 model (see Extended Data Figure \ref{fig:resource_cooperation}).
To isolate the strategic influence of iterated interactions from resource-driven effects, we allocate only minimal resources for cooperation in each round (see Methods for the exact prompt).
We conduct 100 trials for each group configuration.

\begin{figure}[ht]
  \centering 
  \includegraphics[width=0.85\textwidth]{figures/figure_iterated_pgg_group.png}
  \caption{Groups cooperate less and earn less as the proportion of reasoning models increases. Changes in cooperation rate (a) and total earned points (b) across rounds in iterated Public Goods Games are shown (100 runs per condition). Error bars represent the mean $\pm$ s.e.m.}
  \label{fig:group_cooperation_dynamics}
\end{figure}


Our results show that cooperation and payoff dynamics vary significantly depending on group composition (Fig. \ref{fig:group_cooperation_dynamics}).
When all group members are GPT-4o, cooperation continues at an extremely high level.
However, as the proportion of reasoning o1 models increases within the group, cooperation steadily declines.
In groups composed entirely of o1 models, cooperation occurs only about 20\% of the time throughout the interaction with some fluctuations (Fig. \ref{fig:group_cooperation_dynamics}a).

Consequently, groups composed entirely of non-reasoning GPT-4o models achieve higher earnings than those composed of reasoning o1 models.
After 10 iterations, the average earnings for all-GPT-4o groups are 3932 ± 22, compared to 740 ± 38 for all-o1 groups (\textit{P} $<$ 0.001; t-test).
Furthermore, as the proportion of reasoning models in a group increases, overall gains from the Public Goods Game decrease (Fig. \ref{fig:group_cooperation_dynamics}b).


\begin{figure}[ht]
  \centering 
  \includegraphics[width=1.0\textwidth]{figures/figure_iterated_pgg_player.png}
  \caption{Reasoning models drag down the cooperation of non-reasoning models within groups. Comparisons of cooperation (a) and earning (b) dynamics between GPT-4o and o1 within groups across different group compositions are shown (100 runs per condition). Error bars represent the mean $\pm$ s.e.m.}
  \label{fig:player_cooperation_dynamics}
\end{figure}


Figure \ref{fig:player_cooperation_dynamics} details the differences in cooperation and payoff dynamics between models within groups.
Regarding cooperation, consistent with the one-shot results (Fig. \ref{fig:open_ai}), the non-reasoning GPT-4o model initially demonstrates much higher cooperation than the reasoning o1 model.
However, GPT-4o’s cooperation decreases over repeated interactions with o1, and the decline becomes more pronounced as more models have reasoning capacity in the group (Fig. \ref{fig:player_cooperation_dynamics}a).
On the other hand, o1 increases its cooperation level when interacting with GPT-4o, resembling an adaptation or “bandwagon” effect observed in human studies \cite{bikhchandani1992theory}.
As a result, the behavioral gap between the models is vanishing over interactions.
Nevertheless, the negative influence of reasoning models on group cooperation outweighs the positive influence of non-reasoning models.
It is highlighted in groups with an equal mix of GPT-4o and o1 (two of each), where the cooperation ultimately converges to below 50\%, despite that GPT-4o starts with 95\% cooperation, while o1 begins at 20\% (the overall initial cooperation rate = 0.575).

The behavioral difference is reflected in the earnings from the interaction (Fig. \ref{fig:player_cooperation_dynamics}b).
Within mixed groups, the reasoning o1 model earns more than the non-reasoning GPT-4o model by free-riding on its counterpart’s cooperation.
However, across groups, those containing more o1 earn less overall (Fig. \ref{fig:group_cooperation_dynamics}b).
While AI reasoning improves individual performance within groups, it undermines collective gains by reducing overall cooperation, ultimately leading to lower earnings even at the individual level.
