\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{tcolorbox}
\usepackage{todonotes}
\usepackage{url}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{hyperref}


\newcommand{\change}[1]{\textcolor{red}{#1}\xspace}

\lstdefinestyle{mystyle}{
%    backgroundcolor=\color{backcolour},   
%    commentstyle=\color{codegreen},
 %   keywordstyle=\color{magenta},
 %   numberstyle=\tiny\color{codegray},
 %   stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    language=Java,
    %basicstyle=\ttfamily\tiny,
 %   breakatwhitespace=false,         
 %   breaklines=true,                 
    captionpos=b,                    
 %   keepspaces=true,                 
    %numbers=left,                    
    %numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
 %   showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\newcommand{\urlRepo}[0]{\url{https://osf.io/yh6v2/?view_only=a069f5a56f3640abaf235eed1320b46f}\xspace} 

%Erano presenti
\usepackage[inline]{enumitem}
\setlist[enumerate,1]{label=\textit{\alph*)}}
\definecolor{review}{RGB}{0,0,0}

\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{booktabs}






\begin{document}

\title{On the Possibility of Breaking Copyleft Licenses When Reusing Code Generated by ChatGPT
%\title{Investigating Possible IP Rights Infringement in AI-Generated Code: A Case Study on ChatGPT\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Gaia Colombo}
\IEEEauthorblockA{
\textit{University of Milano-Bicocca}\\
Milano, Italy \\
g.colombo147@campus.unimib.it}
\and
\IEEEauthorblockN{Leonardo Mariani}
\IEEEauthorblockA{
\textit{University of Milano-Bicocca}\\
Milano, Italy \\
leonardo.mariani@unimib.it}
 \and
%\linebreakand
%\hspace{-1.0cm} % Adjust spacing between authors if needed
\IEEEauthorblockN{Daniela Micucci}
\IEEEauthorblockA{
\textit{University of Milano-Bicocca}\\
Milano, Italy \\
daniela.micucci@unimib.it}
\and
\IEEEauthorblockN{Oliviero Riganelli}
\IEEEauthorblockA{
\textit{University of Milano-Bicocca}\\
Milano, Italy \\
oliviero.riganelli@unimib.it}

}

%\author{\IEEEauthorblockN{Anonymous Authors}}
%\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%}

\maketitle


\begin{abstract}
AI assistants can help developers by recommending code to be included in their implementations (e.g., suggesting the implementation of a method from its signature). Although useful, these recommendations may mirror copyleft code available in public repositories, exposing developers to the risk of reusing code that they are allowed to reuse only under certain constraints (e.g., a specific license for the derivative software). 

This paper presents a large-scale study about the frequency and magnitude of this phenomenon in ChatGPT. In particular, we generate more than 70,000 method implementations using a range of configurations and prompts, revealing that a larger context increases the likelihood of reproducing copyleft code, but higher temperature settings can mitigate this issue.
 %Our findings reveal that a larger matching context increases the likelihood of GPT-4 reproducing copyleft code, though higher temperature settings can help mitigate this issue.

\end{abstract}

\begin{IEEEkeywords}
AI-assisted coding, code generation, copyleft licenses, intellectual property\end{IEEEkeywords}

\section{Introduction}

Several AI assistants, such as ChatGPT\footnote{https://openai.com/chatgpt/}, GitHub Copilot\footnote{https://github.com/features/copilot}, and Google Gemini\footnote{https://gemini.google.com/}, are available to help developers complete their development tasks. Multiple studies reveal that, although far from perfect, these tools may produce useful results with the potential of accelerating code development~\cite{Hou:LLMforSE:SLR:TOSEM:2024,Fan:LLMChallenges:ICSEFOSE:2023,Mastropaolo:RobustnessGenCode:ICSE:2023,Corso:EmpiricalAssessment:ICPC:2024,Fagadau:EmpiricalStudy:ICPC:2024}. 

AI assistants exploit LLMs trained on a huge corpus of data, including code, to provide recommendations. This raises concerns in terms of the \emph{ownership and rights to reuse} the code generated by these tools when the recommended code matches the code in the training set. This issue is particularly severe if we consider that training code could be protected by restrictive licenses that forbid or limit reuse\footnote{For instance, open source code protected by copyleft licenses can be reused only under specific constraints on how the derivative code is licensed.}. That is, a developer may inadvertently break the license terms associated with the reused code by simply accepting a recommendation. 


A key question about the recommendations produced by AI assistants is thus: \emph{``Is the recommended code original, or is it a copy of existing code protected by a restrictive license?"} Answering this question can be extremely important to prevent potentially unethical behavior and possible legal issues. 

Previous studies based on the now-obsolete GPT-2 models suggest that LLMs can memorize and reproduce code present in their training data. In particular, Al-Kaswan et al.~\cite{AlKaswan:TracesMemorisation:ICSE:2024}  reported that LLMs may generate completions for code prefixes encountered during training by appending the corresponding code suffix from the training data.
%OLD
%Previous studies based on, now obsolete, GPT-2 models show that LLMs may memorize and reproduce the code that is part of the training data. In particular, Al-Kaswan et al.~\cite{AlKaswan:TracesMemorisation:ICSE:2024} reported that LLMs may suggest to complete a code-prefix that occurred in the training data, with the suffix-code present in the training data. 
Similarly, Yang et al.~\cite{Yang:UnveilingMemorization:ICSE:2024} found that training code can be observed in the output for a range of diverse prompts. 

A more recent study~\cite{Yu:codeipprompt:ICML:2023} investigated the concern of code memorization in GPT-4 models, considering code protected by restrictive licenses. The investigation reported a high probability (more than 50\%) for models to return code protected by copyleft licenses if queried for the generation of a function's implementation whose signature is present in the training set. 

%Such a high percentage of potential violations is, also, due to the configuration of the plagiarism detection tools used in the study. In fact, the code returned by AI-assistants is considered as a potential evidence of plagiarism if its similarity is above 0.5 for at least one of the two plagiarism detection tools used: JPlag and Devos. 

%This setup is in contrast with previous studies~\cite{AlKaswan:TracesMemorisation:ICSE:2024,Yang:UnveilingMemorization:ICSE:2024} that considered nearly identical code as an evidence of plagiarism (e.g., Type I clones), and reported a definitely lower probability (\textbf{LEO quanto?}) for AI-assistants to generate code that plagiarizes code in the training data. 

This study aims to \emph{broaden existing evidence and derive further insights} about the risks related to the reuse of the code originated by GPT models. In particular, we investigate how GPT-4 performs in the generation of Java methods, studying the \emph{similarity} between the recommended code and the corresponding open source code available under restrictive licenses using the JPlag plagiarism detection tool\footnote{https://helmholtz.software/software/jplag}. Further, our investigation considers multiple dimensions not studied in the context of GPT-4 models, so far. 

First, we study if and to what extent the \emph{context} may influence the generation of code protected by restrictive licenses. In particular, %we consider the case of developers who have already accepted some code recommendations that include licensed code, and we study if this may increase the likelihood of receiving further recommendations of code still protected by restrictive licenses. 
we investigate cases in which developers, having already accepted recommendations containing licensed code, may face an increased likelihood of receiving additional recommendations of code protected by restrictive licenses. We specifically consider the case of classes that already contain code that mirrors open-source code protected by copyleft licenses, and study its impact on recommendations.
%
We investigate two paradigmatic cases: one considering the presence of \emph{all the methods in the class but the one that has to be generated} matching an open-source implementation protected by a copyleft license, and another one only considering the \emph{presence of access methods} matching an open-source implementation protected by a copyleft license. The first case captures the situation where the context is the largest possible, encompassing every other method within the class. Instead, the second case investigates to what extent the presence of simple and commonly implemented methods with minimal semantic content (i.e., the access methods) may influence the generation of licensed code.

Second, we consider the impact of the \emph{temperature} on the likelihood of generating code protected by restrictive licenses. The temperature is a parameter that influences the creativity of the model, and thus it may have a role in the likelihood that the model replicates licensed code observed during training.

%Last, we investigate if GPT-4 shows any level of \emph{awareness} in the generation of code distributed under restrictive licenses, by studying if explicitly asking for original code may decrease the change code that cannot be freely reused is recommended. 

Finally, we examine if GPT-4 demonstrates any level of \emph{awareness} regarding the generation of code subject to restrictive licenses. Specifically, we investigate whether explicitly requesting code that is not a copy of any known implementation may influence the likelihood of receiving recommendations for code that cannot be freely reused.


We performed this investigation on a body of 7,347 methods, assessing more than 70,000 method implementations. Results show a low probability of receiving recommendations of copyleft code for individual requests, but an increasingly large matching context may increase the likelihood that GPT-4 returns copies of code protected by copyleft licenses. 
%That is, the 
This suggests that the early acceptance of copyleft code may lead developers to incidentally accumulate more copyleft code in their implementations. High-temperature values may help mitigate this problem. Finally, GPT-4 has not demonstrated any awareness of this phenomenon.

In a nutshell, this paper contributes as follows:
\begin{enumerate*}[leftmargin=*,label=\textit{\alph*)}]
\item Reporting a large-scale study on the generation of licensed code with GPT-4,

\item Presenting an analysis of how the context and the temperature may influence the results,

\item Investigating if any trace of awareness is present in GPT-4 models concerning the generation of code under restrictive licenses,

\item Producing recommendations for developers who use GPT-4 to generate code,

\item Publicly releasing the experimental material\footnote{\urlRepo} for replication and extension of the study.

\end{enumerate*}


The paper is organized as follows. Section~\ref{sec:RW} discusses related work. Section~\ref{sec:Met} introduces the five research questions investigated in this paper and presents the methodology used to answer them. Section~\ref{sec:Res} reports the empirical results collected to answer the five research questions. Finally, Section~\ref{sec:con} provides final remarks and suggests directions for future research.


\section{Related Work}
\input{related}

\section{Methodology}
\input{methodology}

\section{Results}
\input{results}

\section{Conclusions}
\input{conclusions}

\section*{Acknowledgment}
This work was partially supported by the MUR under the grant “Dipartimenti di Eccellenza 2023-2027" of the DISCo Department of the University of Milano - Bicocca; and by the Engineered MachinE Learning-intensive IoT systems (EMELIOT) national research project, funded by the MUR under the PRIN 2020 program (Contract 2020W3A5FY).

\bibliographystyle{IEEEtran}
  \bibliography{main}

%\vspace{12pt}


\end{document}
