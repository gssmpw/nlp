\label{sec:con}
Developers exploit AI assistants to speed up their work, interactively asking for code recommendations, such as the initial implementation of methods. However, AI assistants may inadvertently generate code that mirrors existing code protected by copyleft licenses, exposing developers who accept recommendations to the risk of reusing code without being aware of the legal implications of the reuse. 

In this paper, we systematically studied this phenomenon for ChatGPT and discovered that, although the risk of receiving such recommendations is mild %(3,35\% of the cases when ChatGPT is prompted with the method signature and Javadoc comment only) 
for individual requests, it might increase when a larger context is used. In particular, a matching set of access methods or a matching class code increases the chance of receiving the recommendation of copyleft code by a 2X and 5X factor, respectively. 
We also found that the temperature can be used to control this phenomenon, with high values reducing the likelihood of receiving recommendations with copyleft code. This recommendation has to be balanced with studies reporting that high-temperature values may decrease correctness. % of the recommendations. 


The results reported in this study open to several possible future works. For instance, the design of mechanisms to detect and control recommendations that expose developers to the risk of reusing copyleft code should be studied. Although it is hard to define a threshold that can define when reuse should be considered legally relevant, developers must be aware of the nature of the code they include in their software. Further, the study could be extended to other models, languages, and interaction modalities. %, to investigate if, how, and when LLMs may produce undesirable recommendations. 