

For instance, a study of Ren et al. \cite{} provides a comprehensive analysis of copyright protection in the context of LLMs. It highlights the emergent capabilities of LLMs in text generation while addressing significant copyright concerns, including the risks of content theft and the challenges of distinguishing between original and generated content. The authors categorize protection methods into watermarking and model parameter protection, discussing their effectiveness and trade-offs. Additionally, the study emphasizes the need for improved training algorithms and alignment strategies to mitigate memorization and potential copyright infringement, ultimately advocating for robust and performance-preserving protection mechanisms in the evolving landscape of generative AI.

Additionally, Franceschelli and Musolesi \cite{} provide a comprehensive analysis of the legal challenges surrounding generative deep learning, particularly focusing on copyright issues related to the storage of protected works for training models, the risks of plagiarism, and the ownership of generated artifacts. It highlights the complexities of copyright law as it applies to generative models, emphasizing the need for clear guidelines to navigate fair use and the implications of using copyrighted materials in model training. 



In \cite{Ciniselli:CloningCode:ICPC:2022}, the authors investigated the extent to which Deep Learning (DL) code recommenders, specifically a T5 model trained on over 2 million Java methods, generate code snippets that are clones of their training data. Using the Simian clone detector, the researchers found that approximately 10\% of the generated predictions represented Type-1 clones, while around 80\% were Type-2 clones. However, the likelihood of generating clones decreased significantly when the model produced more complex predictions, with virtually no clones identified in outputs consisting of at least four lines of code. These findings highlight the potential for DL-based code recommenders to suggest original code while also raising important considerations regarding licensing and the originality of generated outputs.

\begin{tcolorbox}[title=Approach, colback=white, colframe=black, sharp corners=southwest, boxrule=0.8mm]
\begin{itemize}
\item Model Training: The researchers trained a Text-To-Text-Transfer-Transformer (T5) model on over 2 million Java methods. The model was tasked with recommending code blocks to complete method implementations.
\item Clone Detection: A clone detector, specifically the Simian clone detector, was utilized to identify Type-1 and Type-2 clones in the generated code. 
\item Data Analysis: The study involved comparing the generated code snippets against the training set to determine the percentage of clones. The analysis focused on different characteristics of the clones and the predictions, particularly examining how the likelihood of cloning varied with the length and complexity of the generated code. Shorter snippets were found to have a higher probability of being clones compared to longer ones. More complex code is generally less likely to be a direct copy of simpler, more common patterns found in the training data.

\end{itemize}
\end{tcolorbox}



The study in \cite{Yang:UnveilingMemorization:ICSE:2024} investigates the extent to which large pre-trained code models memorize their training data, revealing that a significant number of outputs can reproduce memorized code snippets. The findings indicate a strong correlation between the frequency of code snippets in the training data and their occurrence in model outputs, with larger models and longer outputs exhibiting greater memorization. Additionally, the study highlights potential privacy and security risks associated with this memorization, emphasizing the need for strategies to mitigate these issues.

\begin{tcolorbox}[title=Approach, colback=white, colframe=black, sharp corners=southwest, boxrule=0.8mm]
\begin{itemize}
    \item Model Selection: The researchers selected two open-source models, CodeParrot (1.5 billion parameters) and CodeParrot-small (110 million parameters), due to their accessible training data and the feasibility of conducting memorization analysis.
\item Memorization Detection: The researchers employed clone detection techniques (Simian) to identify over 40,125 unique memorized code snippets from the generated outputs. They then conducted an open card sorting study on a statistically representative sample of 381 memorized snippets to build a taxonomy of the memorized contents.
\item Analysis of Factors: The study analyzed various factors affecting memorization, such as Top-k sampling, Number of generated outputs, occurrences in the training data, model size, and output length. It was found that larger models tend to memorize more content, and allowing models to generate longer outputs increases the likelihood of memorization.
\item Metrics for Memorization Inference: Four metrics were evaluated to infer whether an output contained memorization: (1) perplexity, (2) ratio of perplexity between models, (3) ratio of perplexity to zlib compression, and (4) average perplexity of sliding windows. These metrics were used to rank the outputs and assess the effectiveness of memorization detection.
\item Comparison with Other Models: The methodology also included an analysis of outputs from other deployed code models, such as Incoder and StarCoder, to demonstrate that memorization is a common phenomenon across different models.
\end{itemize}
RQs:
\begin{enumerate}
    \item RQ1: What do code models memorize?
    \item RQ2: What factors affect memorization in code models?
    \item RQ3: How to infer whether an output contains memorized information?
\end{enumerate}
\end{tcolorbox}


In \cite{AlKaswan:TracesMemorisation:ICSE:2024}, the authors investigated the memorisation capabilities of large language models (LLMs) specifically for code, revealing that these models can memorize their training data, albeit at a lower rate compared to natural language models. The study found that the memorisation rate increases with model size, and different architectures exhibit distinct memorisation patterns. Notably, the largest GPT-NEO model achieved a data extraction rate of 56\%, while a medium-sized model reached 46\%. The research underscores the vulnerability of pre-training data to extraction, even after multiple tuning rounds, and emphasizes the need for stronger safeguards against data leakage in LLMs for code

\begin{tcolorbox}[title=Approach, colback=white, colframe=black, sharp corners=southwest, boxrule=0.8mm]

\begin{enumerate}
    \item \textbf{Data Extraction Security Game}: A security game was defined based on membership inference attacks and k-extractability to quantify memorisation rates in both code and natural language models.
    
    \item \textbf{Benchmark Construction}: A benchmark dataset was created by mining data from the Google BigQuery GitHub dataset and using a CodeGen code generation model to generate samples that could be extracted from the models under analysis. This is challenging because, for some code models, the training data is not published.
    
    \item \textbf{Experiments with Various Models}: The study tested multiple models against the constructed benchmarks to assess their memorisation capabilities, focusing on the rate of exact matches.
    
    \item \textbf{Analysis of Memorisation Characteristics}: The researchers analyzed the memorisation patterns of different model architectures and how model size influenced the rate of memorisation.
\end{enumerate}


RQs:

\begin{enumerate}
    \item \textbf{RQ1}: To what extent do large language models for code memorise their training data?
    
    \item \textbf{RQ2}: What type of data are memorised by code-trained LLMs? Is there a specific code pattern that is memorised?
    
    \item \textbf{RQ3}: How much overlap is there between the memorised samples in different code-trained LLMs? Do some models memorise different samples than others?
    
    \item \textbf{RQ4}: To what extent do LLMs trained in code leak their pre-training data?
\end{enumerate}
\end{tcolorbox}

In contrast to the studies by \cite{AlKaswan:TracesMemorisation:ICSE:2024}, which focuses on the memorization rates of large language models for code and their vulnerability to data extraction, and \cite{Yang:UnveilingMemorization:ICSE:2024}, which explores the correlation between training data frequency and output reproduction in pre-trained code models, as well as \cite{Ciniselli:CloningCode:ICPC:2022}, which investigates the cloning behavior of code recommendation systems, our work emphasizes the legal implications of code generation by ChatGPT, particularly regarding the risks of plagiarism from restrictive licenses and the effectiveness of explicit requests for original content generation.

CODEIPPROMPT \cite{} is a framework designed to evaluate the potential intellectual property (IP) infringement in code generated by large language models (LMs). The analysis of over 4 million licensed code repositories revealed that many models, including both open-source and commercial products, frequently generate code that closely resembles copyrighted material, with similarity scores indicating potential plagiarism. The findings highlight significant risks of IP violations due to the presence of copyrighted code in training datasets, underscoring the need for improved data management and mitigation strategies to address these concerns effectively. Overall, the work emphasizes the importance of reconsidering training data and developing more intelligent models to enhance IP protection in code generation.

\begin{tcolorbox}[title=Approach, colback=white, colframe=black, sharp corners=southwest, boxrule=0.8mm]


The empirical study utilized an automated testing framework called CODEIPPROMPT, which comprises two key methodologies:
\begin{itemize}
\item Prompt Construction: The framework extracts function signatures and comments from licensed code to create prompts that elicit code generation from language models.
\item Plagiarism Measurement: It measures the extent of IP violations by calculating code plagiarism similarity scores, allowing for a quantitative assessment of how closely generated code resembles licensed code.
\end{itemize}
RQs:
\begin{enumerate}

\item To what extent do code language models generate code that violates intellectual property rights?
\item What are the characteristics of the training datasets that contribute to these violations?
\end{enumerate}

The models evaluated in the experimentation include both state-of-the-art open-source models and commercial products, specifically:
\begin{itemize}
\item Open-source models: CodeRL, CodeGen, CodeParrot
\item Commercial products: Codex, Copilot, ChatGPT, and GPT-4
\end{itemize}

The study analyzed a total of 10 models across five programming languages, using a dataset of over 4 million licensed repositories for evaluation

OLIVIERO: CRITERI DI SIMILARITA DIFFERENTI E SELZIONE DEI METODI

\end{tcolorbox}
