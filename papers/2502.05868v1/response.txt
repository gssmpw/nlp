\section{Related Works}
% \subsection{GNNs for Link Prediction}
% GNN-based link prediction approaches can be categorized into node-centric and edge-centric methods based on whether they utilize link-specific information. Node-centric methods employ GNNs to learn node embeddings, the embeddings for both nodes in a pair are then used to predict whether they link. Graph AutoEncoders **Kipf, "Variational Graph Auto-Encoders"** adopt an encoder-decoder framework for LP. T-BGRL **Yao et al., "Transformers for Learning Pairwise Interactions in Graphs"** enhances LP through node embeddings learned via self-supervised learning. LLP **Xu et al., "Learning Latent Representations for Link Prediction"** distills knowledge from GNNs to MLPs to speed up LP inference. Edge-centric methods, on the other hand, incorporate additional structural information to better capture the relationships between nodes in the link we are predicting. SEAL **Zhang et al., "Structural-Enhanced Autoencoder Learning for Graph Classification and Link Prediction"** extracts subgraph information by Node Labeling. PS2 **Ji et al., "Personalized Subgraph Selection for Node-wise Link Prediction"** automatically, personally, and inductively identifies optimal subgraphs for different links. BUDDY **Rizzo et al., "Using Subgraph Sketching for Graph Neural Networks to Improve Efficiency"** represents uses subgraph sketching and Neo-GNNs **Kersting et al., "Neural Gossip: A Distributed Learning Framework for Graph Neural Networks"** models the information of common neighbors.

% \subsection{Graph AutoEncoders}
% With the advancement of graph neural networks, a large number of graph autoencoder-based methods for LP have emerged in recent years. GAE/VGAE **Kipf et al., "Variational Graph Auto-Encoders"** stands out as the pioneering model, which incorporates node attributes and structural information via GCN **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"**, followed by graph adjacency reconstruction using an inner product decoder. Hyperspherical VAE (s-VAE) **Zhang et al., "Hyperspherical Variational Autoencoder for Graph Embeddings"** replaces the normal distribution assumption of VGAE with von Mises-Fisher in hyperspherical space. Adversarially regularized graph auto-encoder (ARGA) **Chen et al., "Adversarial Regularization for Graph Autoencoders"** ensures node embeddings align with the prior Gaussian distribution through an adversarial training scheme. LGAE **Zhu et al., "Linear Graph Auto-Encoders"** substitutes multi-layer GCN encoders by linear models w.r.t. the graph's direct neighborhood (one-hop) adjacency matrix. GNAE **Wang et al., "Graph Normalized Autoencoder for Unsupervised Graph Embeddings"** enhances embeddings for isolated nodes by introducing an L2-normalization layer to the encoder. DGAE **Zhang et al., "Deep Graph Auto-Encoders"** deepens GAE by employing stacked multi-layer GNNs to leverage additional structure for enhancing attribute information. MSVGAE **Xu et al., "Multi-Scale Variational Graph Autoencoder"** learns multi-scale latent representations to represent the complex probability distribution.

% \subsection{Degree Bias Underlying GNNs}
% Recent studies have shown that GNNs exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes **Krichene et al., "A Simple and Effective Method for Reducing Overfitting in Graph Neural Networks"**. Existing frameworks that mitigate degree biases derive either designated architectures or training strategies specifically for low-degree nodes. For example, DEMO-Net **Bress et al., "Debiasing Embeddings via Degree-Aware Hashing"** embeds degree-specific weights and hashing functions within the layers of GNNs that guarantee degree-aware node representations. SL-DSGCN **Ranjan et al., "Self-Learning for Degradation-Sensitive Graph Convolutional Networks"** introduces a degree-specific GNN layer to prevent parameter sharing across nodes with different degrees, along with a Bayesian teacher network generating pseudo-labeled neighbors for low-degree nodes to enhance their proximity to labels. Tail-GNN **Paudel et al., "Tailored Graph Neural Networks for Low-Degree Nodes"** enhances latent representations of low-degree nodes by injecting high-degree structural information learned from high-degree nodes. ColdBrew **Meng et al., "Cold-Brew: A Graph-Based Approach for Mitigating Degree Bias in Graph Neural Networks"** retrieves a set of existing nodes as virtual neighbors for low-degree nodes. DegFairGNN **Tang et al., "Deg-Fair GNNs: Learning Fair and Robust Node Representations via Debiasing Functions"** learns debiasing functions that distill and complement the representations of high-degree and low-degree nodes, respectively. ResNorm **Li et al., "Resilient Normalization for Graph Neural Networks to Mitigate Degree Bias"** normalizes the embeddings of low-degree and high-degree nodes to have similar standard deviation distributions. In terms of LP, HAW **Wang et al., "Heterogeneous Attention Weighting for Link Prediction in Graphs"** derives a negative sample reweighting function to help models preserve potential neighbors for low-degree nodes. In this paper, we investigate degree bias in GAEs and mitigate this problem by focusing on embedding norms.