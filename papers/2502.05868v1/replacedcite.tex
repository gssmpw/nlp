\section{Related Works}
% \subsection{GNNs for Link Prediction}
% GNN-based link prediction approaches can be categorized into node-centric and edge-centric methods based on whether they utilize link-specific information. Node-centric methods employ GNNs to learn node embeddings, the embeddings for both nodes in a pair are then used to predict whether they link. Graph AutoEncoders ____ adopt an encoder-decoder framework for LP. T-BGRL ____ enhances LP through node embeddings learned via self-supervised learning. LLP ____ distills knowledge from GNNs to MLPs to speed up LP inference. Edge-centric methods, on the other hand, incorporate additional structural information to better capture the relationships between nodes in the link we are predicting. SEAL ____ extracts subgraph information by Node Labeling. PS2 ____ automatically, personally, and inductively identifies optimal subgraphs for different links. BUDDY ____ represents uses subgraph sketching and Neo-GNNs ____ models the information of common neighbors.

% \subsection{Graph AutoEncoders}
% With the advancement of graph neural networks, a large number of graph autoencoder-based methods for LP have emerged in recent years. GAE/VGAE ____ stands out as the pioneering model, which incorporates node attributes and structural information via GCN ____, followed by graph adjacency reconstruction using an inner product decoder. Hyperspherical VAE (s-VAE) ____ replaces the normal distribution assumption of VGAE with von Mises-Fisher in hyperspherical space. Adversarially regularized graph auto-encoder (ARGA) ____ ensures node embeddings align with the prior Gaussian distribution through an adversarial training scheme. LGAE ____ substitutes multi-layer GCN encoders by linear models w.r.t. the graph's direct neighborhood (one-hop) adjacency matrix. GNAE ____ enhances embeddings for isolated nodes by introducing an L2-normalization layer to the encoder. DGAE ____ deepens GAE by employing stacked multi-layer GNNs to leverage additional structure for enhancing attribute information. MSVGAE ____ learns multi-scale latent representations to represent the complex probability distribution.

% \subsection{Degree Bias Underlying GNNs}
% Recent studies have shown that GNNs exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes ____. Existing frameworks that mitigate degree biases derive either designated architectures or training strategies specifically for low-degree nodes. For example, DEMO-Net ____ embeds degree-specific weights and hashing functions within the layers of GNNs that guarantee degree-aware node representations. SL-DSGCN ____ introduces a degree-specific GNN layer to prevent parameter sharing across nodes with different degrees, along with a Bayesian teacher network generating pseudo-labeled neighbors for low-degree nodes to enhance their proximity to labels. Tail-GNN ____ enhances latent representations of low-degree nodes by injecting high-degree structural information learned from high-degree nodes. ColdBrew ____ retrieves a set of existing nodes as virtual neighbors for low-degree nodes. DegFairGNN ____ learns debiasing functions that distill and complement the representations of high-degree and low-degree nodes, respectively. ResNorm ____ normalizes the embeddings of low-degree and high-degree nodes to have similar standard deviation distributions. In terms of LP, HAW ____ derives a negative sample reweighting function to help models preserve potential neighbors for low-degree nodes. In this paper, we investigate degree bias in GAEs and mitigate this problem by focusing on embedding norms.