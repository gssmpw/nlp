\section{Related Works}
% \subsection{GNNs for Link Prediction}
% GNN-based link prediction approaches can be categorized into node-centric and edge-centric methods based on whether they utilize link-specific information. Node-centric methods employ GNNs to learn node embeddings, the embeddings for both nodes in a pair are then used to predict whether they link. Graph AutoEncoders \cite{GAE, ARGA, S-VAE, LGAE, GNAE, DGAE, MSVGAE} adopt an encoder-decoder framework for LP. T-BGRL \cite{T-BGRL} enhances LP through node embeddings learned via self-supervised learning. LLP \cite{LLP} distills knowledge from GNNs to MLPs to speed up LP inference. Edge-centric methods, on the other hand, incorporate additional structural information to better capture the relationships between nodes in the link we are predicting. SEAL \cite{SEAL} extracts subgraph information by Node Labeling. PS2 \cite{PS2} automatically, personally, and inductively identifies optimal subgraphs for different links. BUDDY \cite{BUDDY} represents uses subgraph sketching and Neo-GNNs \cite{Neo-GNNs} models the information of common neighbors.

% \subsection{Graph AutoEncoders}
% With the advancement of graph neural networks, a large number of graph autoencoder-based methods for LP have emerged in recent years. GAE/VGAE \cite{GAE} stands out as the pioneering model, which incorporates node attributes and structural information via GCN \cite{GCN}, followed by graph adjacency reconstruction using an inner product decoder. Hyperspherical VAE (s-VAE) \cite{S-VAE} replaces the normal distribution assumption of VGAE with von Mises-Fisher in hyperspherical space. Adversarially regularized graph auto-encoder (ARGA) \cite{ARGA} ensures node embeddings align with the prior Gaussian distribution through an adversarial training scheme. LGAE \cite{LGAE} substitutes multi-layer GCN encoders by linear models w.r.t. the graph's direct neighborhood (one-hop) adjacency matrix. GNAE \cite{GNAE} enhances embeddings for isolated nodes by introducing an L2-normalization layer to the encoder. DGAE \cite{DGAE} deepens GAE by employing stacked multi-layer GNNs to leverage additional structure for enhancing attribute information. MSVGAE \cite{MSVGAE} learns multi-scale latent representations to represent the complex probability distribution.

% \subsection{Degree Bias Underlying GNNs}
% Recent studies have shown that GNNs exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes \cite{DEMO-Net, SL-DSGCN, Tail-GNN, ColdBrew, DegFairGNN, ResNorm}. Existing frameworks that mitigate degree biases derive either designated architectures or training strategies specifically for low-degree nodes. For example, DEMO-Net \cite{DEMO-Net} embeds degree-specific weights and hashing functions within the layers of GNNs that guarantee degree-aware node representations. SL-DSGCN \cite{SL-DSGCN} introduces a degree-specific GNN layer to prevent parameter sharing across nodes with different degrees, along with a Bayesian teacher network generating pseudo-labeled neighbors for low-degree nodes to enhance their proximity to labels. Tail-GNN \cite{Tail-GNN} enhances latent representations of low-degree nodes by injecting high-degree structural information learned from high-degree nodes. ColdBrew \cite{ColdBrew} retrieves a set of existing nodes as virtual neighbors for low-degree nodes. DegFairGNN \cite{DegFairGNN} learns debiasing functions that distill and complement the representations of high-degree and low-degree nodes, respectively. ResNorm \cite{ResNorm} normalizes the embeddings of low-degree and high-degree nodes to have similar standard deviation distributions. In terms of LP, HAW \cite{HAW} derives a negative sample reweighting function to help models preserve potential neighbors for low-degree nodes. In this paper, we investigate degree bias in GAEs and mitigate this problem by focusing on embedding norms.