\section{Experimental Results}

\subsection{Experimental Setup}


\textbf{Computation platform and implementation:}
%
We conduct all experiments on a GPU server (Intel i9-10980XE) equipped with a NVIDIA-4090 GPU (with 24GB memory).
%
%
We implement \alg{} (Ours), Learned CM sketch (LCMS) \cite{lcmsketch}, meta-sketch (MS) \cite{metasketch}, ElasticSketch (ES) \cite{elasticsketch}, and CocoSketch (CocoS) \cite{cocosketch} in Python.
%
For heavy hitter query, we set the threshold \(T = 0.01\% \cdot \|\mathbf{f_l}\|_1\) as many papers do.
%
For \alg{}, we use Roberta \cite{roberta} as the base model, and set the soft label to.
%
\[
    \text{label} = \sigma \Bigl(2.298((\log_2(n) - \log_2(64))\Bigr)
\]
%
Hence, for flows whose sizes exceed 256, the label is above 0.99; while for flows whose sizes are below 16, the label is below 0.01.
%
We apply LoRA (Low-Rank Adaptation) \cite{lora} to fine-tune our model. We limit the training process to 1 epoch due to fast convergence.
%
For other algorithms, we set their parameters as the original paper recommended.




\bbb{Datasets:} The datasets used for the evaluation are listed as follows.
%
\begin{itemize}[leftmargin=*]
    %
    \item \textit{CAIDA dataset} \footnote{\url{https://www.caida.org/catalog/datasets/passive_dataset}} is a passive traces dataset collected from high-speed monitors on a commercial backbone link.
    %
    We split the dataset into sub-datasets with time window of 5s.
    %
    Each sub-dataset consists of approximate 190k flows, 2.1M packets.
    %
    We use 12 adjacent sub-dataset as the training set, and 1 sub-dataset as the test set.
    %
    %
    \item \textit{MAWI dataset} \footnote{\url{https://mawi.wide.ad.jp/mawi}} is a traffic dataset maintained by the MAWI Working Group of the WIDE Project, collected at the transit link of WIDE to the upstream ISP.
    %
    We split the dataset into sub-datasets with time window of 5s.
    %
    Each sub-dataset consists of approximate 670k flows, 1.2M packets.
    %
    We use 12 adjacent sub-dataset as the training set, and 1 sub-dataset as the test set.
    %
    %
    \item \textit{IMC DC dataset} \footnote{\url{https://pages.cs.wisc.edu/~tbenson/IMC10_Data.html}} contains data from the data centers studied in \cite{imcdc}.
    %
    The dataset consists of multible sub-datasets, each consists of approximate 63k flows, 0.9M packets.
    %
    We use 10 sub-dataset as the training set, and 1 sub-dataset as the test set.
    %
\end{itemize}


\bbb{Metrics:}
%
The metrics used for the evaluation are listed as follows.
\begin{itemize}[leftmargin=*]
    \item \textit{Average Relative Error (ARE):}
    %
    \(\frac{1}{|\mathcal{U}|} \sum_{f\in\mathcal{U}} \frac{|n(f)-\hat{n(f)}|}{n(f)}\),
    %
    where \(\mathcal{U}\) is the universe, \(n(f)\) and \(\hat{n(f)}\) are the actual and estimated flow size of flow \(f\), respectively.
    %
    \item \textit{Average Absolute Error (AAE):}
    %
    \(\frac{1}{|\mathcal{U}|} \sum_{f\in\mathcal{U}} |n(f)-\hat{n(f)}|\).
    %
    \item \textit{F1 Score:} \(\frac{2\cdot PR\cdot RR}{PR+RR}\), where PR (Precision Rate) is the ratio of true positive flows to all reported flows, and RR (Recall Rate) is the ratio of true positive flows to all actual flows.
    %
    % \item \textit{Weighted Mean Relative Error (WMRE):} $\frac{\sum_{i=1}^{z}|m_i-\hat{m_i}|}{\sum_{i=1}^{z} \left(\frac{m_i+\hat{m_i}}{2}\right)}$, where $m_i$ and $\hat{m_i}$ are the true and estimated numbers of the items of frequency $i$, respectively, and $z$ is the maximum frequency.
    %
    % \item \textit{Related Error (RE):} \(\frac{|True-Est|}{True}\), where \(True\) and \(Est\) are the true and estimated values, respectively.
    %
    % \item \textit{Throughput:} The number of packets per second, in million packets per second (Mpps).
\end{itemize}




\subsection{Experiment on Parameter Settings}


\input{Experiments/params}


\bbb{\# bucket size (Figure~\ref{eva:bucket_size}):}
%
We vary the bucket size used in the heavy part of \alg{}. We find that when the bucket size is 1, the error is highest because two large flows may collide in the same bucket. As the bucket size increases, accuracy improves. When the bucket size is at least 2, even if collisions occur, the two large flows can still be recorded in different cells within the same bucket, thus avoiding collision-induced errors. Once the bucket size reaches 8, further increases lead to only marginal improvements.
%
Therefore, we set the bucket size in the heavy part to 8.




\bbb{Heavy ratio (Figure~\ref{eva:heavy_ratio}):}
%
We adjust the proportion of total memory allocated to the heavy part (referred to as the heavy ratio) and measure the accuracy. We find that a heavy ratio of 10\% consistently yields the lowest ARE, while the heavy ratio that achieves the lowest AAE varies with the total memory size. This is because increasing the heavy ratio improves the accuracy of large flows but lowers the accuracy of small flows.
When the total memory is small, the accuracy of small flows has a greater impact on overall accuracy, so a smaller heavy ratio yields better results. Conversely, when the total memory is large, the accuracy of large flows has a bigger influence, and AAE becomes more sensitive to large-flow accuracy, so a larger heavy ratio leads to better overall performance.
%
Therefore, for flow size query, we set the heavy ratio to 20\% as a balance between the accuracy of large flows and small flows.
%
Note that for heavy hitter query, we only use the heavy part, because heavy hitter query focuses solely on large-flow accuracy.




\bbb{\# hash functions (Figure~\ref{eva:hash_num}):}
%
We vary the number of hash functions used in the light part of \alg{} and examine its accuracy. We observe that when there is only 1 hash function, the error is highest due to the lack of multi-hash error reduction. When the number of hash functions is at least two, the best-performing number of hashes depends on the memory size. Nevertheless, with 3 hash functions, \alg{} consistently achieves near-optimal accuracy.
Therefore, we set the number of hash functions in the light part to 3.




\subsection{End-to-end Performance}


\input{Experiments/flow_size}
\input{Experiments/heavy_hitter}
\input{Experiments/hhh}
\input{Experiments/other_datasets}


\bbb{Accuracy of flow size query (Figure~\ref{eva:fs}):}
%
We compare \alg{} with LCMS, MS, and ES. \textit{We find that \alg{} achieves the highest accuracy among all 4 algorithms.} On average, the ARE of \alg{} is 11.8 and 18.8 times lower than that of LCMS and ES, respectively, while its AAE is also 8.1 and 12.1 lower than those of LCMS and ES, respectively.
%
It is worth noting that meta-sketch is not practical at our current data scale for two main reasons:
%
\begin{enumerate}[leftmargin=*]
    \item When the memory budget is set to 200 KB, training a 500k-step model fails to converge, leading to poor accuracy.

    \item When the memory is increased to 400 KB, meta-sketch requires more than 24 GB of GPU memory for training.
\end{enumerate}




\bbb{Accuracy of heavy hitter query (Figure~\ref{eva:hh}):}
%
We compare \alg{} with ES. \textit{We find that \alg{} achieves higher accuracy among the 2 algorithms.} Under a memory budget of 50KB, \alg{} reaches an F1 score of 0.94, whereas the F1 score of ES is 0.74. In addition, the ARE of \alg{} is also on average 2.6 times lower than that of ES.




\bbb{Accuracy of HHH query (Figure~\ref{eva:hhh}):}
%
We compare \alg{} with CocoS. \textit{We find that \alg{} achieves higher accuracy among the 2 algorithms.} Under a memory budget of 50KB, \alg{} reaches an F1 score of 0.94, whereas the F1 score of CocoS is 0.82. In addition, the ARE of \alg{} is also on average 1.9 times lower than that of ES.




\bbb{Accuracy on other datasets (Figure~\ref{eva:fs_other}-\ref{eva:hh_other}:)}
%
Apart from the CAIDA dataset, we evaluate the accuracy on flow size query and heavy hitter query on two additional datasets and find that \alg{} achieves high accuracy.
%
For flow size query, on average, \alg{}’s ARE is 5.3 and 13.2 times lower than those of LCMS and ES, respectively, while its AAE is also 3.9 and 8.2 lower than those of LCMS and ES, respectively.
%
For heavy hitter query, under a memory budget of 50KB, \alg{} reaches an F1 score of 0.99 and 0.98 on the two datasets, respectively, whereas the F1 score of ES is 0.94 and 0.89, respectively.
%
It is worth noting that, on the IMC DC dataset, when the memory size is large, ElasticSketch achieves higher accuracy. This is because, with ample memory, ElasticSketch’s heavy part has sufficient cells to store and identify potential large flows, thus avoiding errors from misclassifying large flows. In this scenario, \alg{} also maintains high accuracy (F1 score > 0.99, ARE < 0.05).




\subsection{Micro Benchmark}


\input{Experiments/model_acc}
\input{Experiments/time_vs_acc}


\bbb{Model accuracy (Figure~\ref{eva:model}):} We evaluate the classification accuracy for flows of different sizes and observe that \alg{} accurately classifies both very large and very small flows, which meets our expectations. For flows smaller than 16 packets and those larger than 256 packets, the model’s classification accuracy exceeds 90\%. Although accuracy for flows within the [32,64) range is relatively low, it has little impact on the overall performance and is thus acceptable.




\bbb{Enhancement by Using Full Packet Headers (Figure~\ref{eva:time}).}
%
We compare the accuracy of \alg{} with a baseline algorithm that uses only flow IDs for classification. We train the classifier on a training set and test it on 4 test sets, each collected at a different time interval from the training set. We also evaluate the accuracy of heavy hitter query under 50KB. \textit{We find that when using full packet headers, the classifier can accurately infer flow sizes over an extended period, thereby preserving the sketch’s high accuracy.}
%
As shown in Figure~\ref{eva:time}, although at minute 0 the baseline and \alg{} yield essentially the same results, the baseline’s classifier accuracy and its sketch accuracy decline rapidly over time, whereas \alg{}’s accuracy shows only a slight decrease. At the 20-minute mark, the classifier’s F1 scores for \alg{} and the baseline drop by 0.059 and 0.113, respectively, while the sketch’s end-to-end F1 scores also drop by 0.017 and 0.069, respectively.
