\section{Background}


\subsection{Problem Definition}


\begin{definition}
    \textit{Data Stream:} A data stream \(\mathcal{S} = \{p_1, p_2, \cdots, p_i, \cdots\}\) is a sequence of packets, where each packet \(p_i\) has a flow ID \(f\), drawn from a universe \(\mathcal{U}\). In this paper, we focus on flow measurement within a fixed time window, so the data stream is finite. Note that each packet can only be proceeded once.
\end{definition}




\begin{definition}
    \textit{Flow size query:} Given a data stream \(\mathcal{S}\), reporting the size of every flow \(f \in \mathcal{U}\), where flow size is defined as the number of packets of ID \(f\), i.e., \(n(f) = |\{i:p_i=f\}|\).
\end{definition}




\begin{definition}
    \textit{Heavy hitter query:} Given a data stream \(\mathcal{S}\) and a threshold \(T\), reporting the ID of every flow with size larger than \(T\).
\end{definition}




\begin{definition}
    \textit{Hierarchical Heavy Hitters (HHH) Query:}
    Consider a hierarchy \(H\) imposed on the flow IDs in \(\mathcal{U}\). For example, each IP address can be subdivided into multiple levels of prefixes (e.g., /8, /16, /24). Given a data stream \(\mathcal{S}\) and a threshold \(T\), an HHH query aims to find all nodes in the hierarchy whose aggregated flow size exceeds \(T\). \cite{rhhh} 
    % Formally, a node \(v\) in the hierarchy is a Hierarchical Heavy Hitter if 
    % \[
    %     \sum_{f \in \mathcal{F}(v)} n(f) > T,
    % \]
    % where \(\mathcal{F}(v)\) denotes the set of flows that map to node \(v\) (i.e., all flow IDs covered by \(v\) in the hierarchy), and \(n(f)\) is the size of flow \(f\). 
\end{definition}







\subsection{Large Language Model}

A large language model (LLM) is a neural network designed to process and generate natural language using vast amounts of training data. In recent years, it also finds success in other fields, such as biomedical research, code generation, and data analysis. Typical LLMs include BERT \cite{bert}, GPT \cite{gpt4}, and Llama \cite{llama2}. Among them, BERT (Bidirectional Encoder Representations from Transformers) introduces masked language model (MLM), enabling the model to learn context from both directions. It also leverages next sentence prediction to capture relationships between sentences. Variations of BERT include DistilBERT \cite{distilbert}, RoBERTa \cite{roberta}, and ALBERT \cite{albert}, each offering improvements in efficiency or performance.




\subsection{The Count-Min Sketch}

As shown in Figure \ref{fig:cms}, the data structure of the Count-Min sketch (CMS) consists of \(d\) counter array. The \(i\)-th counter array \(A_i\) consists of \(w\) counters, and is also associated with a hash function \(h_i(.) (1\leqslant h_i(.) \leqslant w)\).
%
When inserting a packet of flow \(f\), CMS computes the \(d\) hash functions to locate the \(d\) mapped counters, \(A_1[h_1(f)], \cdots, A_d[h_d(f)]\), and increments each mapped counter by 1.
%
When querying the size of flow \(f\), CMS reports the minimum value of each mapped counter as the estimated flow size, i.e., \(\min\{A_i[h_i(f)]\} (1\leqslant i \leqslant d)\).


\begin{figure}[!ht]
    \centering  
    \includegraphics[width=0.5\linewidth]{Figures/cmsketch.pdf}
    \caption{The Count-Min sketch.}
    \label{fig:cms}
\end{figure}
