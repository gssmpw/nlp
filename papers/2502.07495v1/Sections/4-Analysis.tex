\section{Theoretical Analysis}


\subsection{Accuracy and Error Bound}


We make the following assumptions in our analysis.


\begin{itemize}[leftmargin=*]
    \item \textbf{Assumption 1 (Classification Consistency):} The predicted label of a flow does not change from large to small (or vice versa) during its lifetime.

    \item \textbf{Assumption 2 (Sufficient Heavy Part):} The heavy part is large enough so that any large flow correctly classified as large is never evicted once inserted.
\end{itemize}




\begin{theorem}
\label{theorem:full_accurate}
%
The probability that a large flow is fully accurate (i.e., tracked with zero error) in \alg{} is
%
\[
  P_{\mathrm{LLMS}} = A + (1 - A) \cdot P_{\mathrm{CMS}} \bigl(w_{light}, d_{light}, N_{light}\bigr),
\]
%
where \(A\) is the classifier's accuracy for large flows, i.e., the probability that a large flow is correctly identified as large.
%
\(w_{light}\), \(d_{light}\) is the width and depth of the light part (CMS), and \(N_{light}\) is the number of flows that end up in the light part.
%
\(P_{\mathrm{CMS}}(w, d, N)\) is the probability that a single flow is fully accurate in a CMS of width \(w\) and depth \(d\), when there are \(N\) total flows in that sketch. One common formula, based on a Poisson approximation of collisions, is:
\[
  P_{\mathrm{CMS}}(w, d, N) = 1 - \bigl(1 - e^{-(N-1)/w}\bigr)^{d}.
\]
\end{theorem}


\begin{proof}
Consider a large flow \(f\):

\bbb{Case 1: Correct Classification (probability \(A\)).}
%
Under Assumption 2, once inserted into the heavy part, \(f\) is never evicted. Consequently, \(f\) is fully accurate, so the probability in this scenario is \(1\).

\bbb{Case 2: Misclassification (probability \(1 - A\)).}
%
If \(f\) is misclassified as small, it goes into the light part.
The probability that \(f\) is fully accurate in CMS is \(P_{\mathrm{CMS}}(w_{light}, d_{light}, N_{light})\).

By the law of total probability, we sum over these two disjoint cases:
%
\begin{align*}
    P_{\mathrm{LLMS}}
    &= A \times 1 + (1 - A) \times P_{\mathrm{CMS}}(w_{light}, d_{light}, N_{light}) \\
    &= A + (1 - A) \cdot P_{\mathrm{CMS}} \bigl(w_{light}, d_{light}, N_{light}\bigr).
\end{align*}
\end{proof}






\begin{theorem}
\label{theorem:error_bound}
%
Let \(\hat{n(f)}\) and \(n(f)\) be the estimated and actual size of a flow \(f\).
%
Let \(\|\mathbf{f_{light}}\|_1\) be the total number of packets sent to the light part. We have
%
    \[\|\mathbf{f_{light}}\|_1 \leqslant \|\mathbf{f}\|_1 - A \times N_{large} \times T,
\]
%
where \(\|\mathbf{f}\|_1\) is the total number of packets.
%
Let \(\delta = (1 - A) e^{-d_{light}}\), \(\epsilon = \frac{e}{w_{light}}\).
%
With probability at least \(1 - \delta\), 
%
\[
    \hat{n(f)} \leqslant n(f) + \epsilon \|\mathbf{f_{light}}\|_1.
\]
%
\end{theorem}


\begin{proof}
We split into two cases according to whether the flow $f$ is correctly or incorrectly classified:

\textbf{Case 1: Correct Classification (probability \(A\)).}
%
If $f$ is a large flow and the classifier labels it as large, $f$ is inserted into the heavy part. 
Under our assumptions, once in the heavy part, $f$ is fully accurate. 
Thus, in this scenario, we trivially have
\[
  \hat{n(f)} = n(f) \leqslant n(f) + \epsilon \|\mathbf{f_{light}}\|_1.
\]


\textbf{Case 2: Misclassification (probability $1 - A$).}
If $f$ is a large flow but is incorrectly labeled as small, it goes into the light part. 
Standard CMS analysis shows that with probability $1 - e^{-d_{light}}$,
\[
  \hat{n(f)} \leqslant n(f) + \epsilon \|\mathbf{f_{light}}\|_1.
\]
Equivalently,
\[
  P\Bigl(\hat{n(f)} > n(f) + \epsilon \|\mathbf{f_{light}}\|_1 \bigm| \text{misclassified}\Bigr)
  \leqslant e^{-d_{light}}.
\]


Hence, unconditionally, the probability that \(f\) ends up in CMS and is over-counted by more than \(\epsilon \|\mathbf{f_{light}}\|_1\) is at most
\[
  (1 - A) e^{-d_{light}}.
\]


Therefore, with probability at least \(1 - (1 - A) e^{-d_{light}}\),
\[
    \hat{n(f)} \leqslant n(f) + \epsilon \|\mathbf{f_{light}}\|_1.
\]

Combining both cases, we set
\[
  \delta = (1 - A) e^{-d_{light}},
\]
which gives
\[
    P\Bigl(\hat{n(f)} \leqslant n(f) + \epsilon \|\mathbf{f_{light}}\|_1\Bigr) \geqslant 1 - \delta.
\]

Finally, observe that
\[
    \|\mathbf{f_{light}}\|_1 \leqslant \|\mathbf{f}\|_1 - A \times N_{large} \times T
\]
because at least $A\times N_{large}$ large flows are correctly classified and stored in the heavy part, each contributing at least $T$ packets. 
Hence, the light part contains at most $\|\mathbf{f}\|_1 - A\times N_{large} \times T$ packets in total, ensuring that 
\[
    \hat{n(f)} \leqslant n(f) + \epsilon \Bigl(\|\mathbf{f}\|_1 - A\times N_{large} \times T\Bigr).
\]

\end{proof}




\subsection{Lock Flag Estimation}


\begin{theorem}
\label{theorem:flag}
The lock flag \(L\) is an unbiased estimator of the flow’s predicted labels.
\end{theorem}


\begin{proof}
Let \(\hat{n}_t\) be the flow size for a given flow up to the \(t\)-th packet, and \(\hat{y}_t\in\{0,1\}\) be the classifier’s predicted label at the \(t\)-th insertion. Let \(L_t\) be the value of the lock flag after the \(t\)-th insertion. The lock flag is updated as follows:

\[
L_{t+1} = \begin{cases}
    1, & \text{w.p.}\, \frac{L_t \cdot \hat{n}_{t} + \hat{y}_{t+1}}{\hat{n}_t + 1} \\
    0, & \text{otherwise}
\end{cases}
\]

We aim to show that
\[
    \mathbb{E}[L_t] = \frac{1}{t} \sum_{i=1}^t \mathbb{E}[\hat{y}_i]
\]


\textbf{Base Case (\(t=1\)).}
When \(t=1\), the flow has been inserted exactly once. Thus,
\[
L_1 = \begin{cases}
    1, & \text{w.p.}\, \hat{y}_1 \\
    0, & \text{otherwise}
\end{cases}
\]

Hence,
\[
\mathbb{E}[L_1] = 1 \times \hat{y}_1 + 0 \times (1 - \hat{y}_1) = \hat{y}_1
\]

Since the expectation of the predicted labels for just one insertion is \(\hat{y}_1\), we have
\[
    \mathbb{E}[L_1] = \frac{1}{1}\sum_{i=1}^1 \mathbb{E}[\hat{y}_i] = \mathbb{E}[\hat{y}_1]
\]

Thus, the base case holds.


\textbf{Inductive Step.}
Assume that after \(t\) insertions,
\[
\mathbb{E}[L_t] = \frac{1}{t}\sum_{i=1}^t \mathbb{E}[\hat{y}_i]
\]

We want to show that the statement also holds for \(t+1\), i.e.,
\[
\mathbb{E}[L_{t+1}] = \frac{1}{t+1}\sum_{i=1}^{t+1} \mathbb{E}[\hat{y}_i]
\]

By the law of total expectation and the update rule for \(L\), we have
\begin{align*}
\mathbb{E}[L_{t+1}]
    &= \mathbb{E} \Bigl[\mathbb{E}\bigl[L_{t+1} \big\vert L_t, \hat{n}_t, \hat{y}_{t+1} \bigr]\Bigr] \\
    &= \mathbb{E} \Bigl[\frac{L_t \cdot \hat{n}_t + \hat{y}_{t+1}}{\hat{n}_t + 1}\Bigr]
\end{align*}

Since \(\hat{n}_t = t\) at the \((t+1)\)-th insertion, we have
\begin{align*}
\mathbb{E}[L_{t+1}]
    &= \mathbb{E}\Bigl[\frac{L_t \cdot t + \hat{y}_{t+1}}{t + 1} \Bigr]\\
    &= \frac{1}{t+1} \mathbb{E}\bigl[L_t \cdot t + \hat{y}_{t+1} \bigr]\\
    &= \frac{1}{t+1} \Bigl[t \cdot \mathbb{E}[L_t] + \mathbb{E}[\hat{y}_{t+1}] \Bigr]
\end{align*}

Using the inductive hypothesis
\(\mathbb{E}[L_t] = \frac{1}{t}\sum_{i=1}^t \mathbb{E}[\hat{y}_i]\),
we have
\begin{align*}
\mathbb{E}[L_{t+1}]
    &= \frac{1}{t+1}\Bigl[t \cdot \frac{1}{t}\sum_{i=1}^t \mathbb{E}[\hat{y}_i] + \mathbb{E}[\hat{y}_{t+1}]\Bigr]\\
    % &= \frac{1}{t+1}\Bigl[\sum_{i=1}^t \mathbb{E}[\hat{y}_i] + \mathbb{E}[\hat{y}_{t+1}]\Bigr]\\
    &= \frac{1}{t+1} \sum_{i=1}^{t+1} \mathbb{E}[\hat{y}_i].
\end{align*}

This completes the inductive step.


\textbf{Conclusion.}
By mathematical induction, we have
\[
\mathbb{E}[L_t] = \frac{1}{t}\sum_{i=1}^t \mathbb{E}[\hat{y}_i]
\]
for all \(t\). Therefore, \(L_t\) is an unbiased estimator of the average predicted label up to the \(t\)-th insertion.
\end{proof}
