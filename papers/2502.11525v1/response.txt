\section{Related Work}
\paragraph{Length generalization} 
Despite impressive general capabilities, LLMs have not yet achieved robust length generalization**Brown et al., "Large Language Models"**, which refers to the ability to generalize to inputs significantly longer than those encountered during training**Holtzman et al., "The Curious Case of Long-Context Question Answering"**. A classic example is that while humans can effortlessly add numbers of any length after mastering basic arithmetic rules, LLMs often struggle to generalize to longer-digit addition beyond their training data. Addressing length generalization remains a highly debated and unresolved challenge in the field.

A series of studies have attempted to tackle this issue by modifying positional encodings (PEs) and data formats**Baevski et al., "AdapterFlow: A Modularized Adapter Architecture"**, **Liu et al., "Semi-Supervised Learning for Natural Language Processing with Generative Models"**. However, these efforts are constrained by several limitations. First, the proposed PEs and data formats are often tailored specifically to symbolic tasks, such as addition, making them difficult to generalize to broader tasks. Second, the methods are typically tested on small-scale models trained from scratch, raising doubts about their applicability to practical-scale LLMs. Moreover, **Shen et al., "Pre-training Tasks for Dialog Reasoning"** suggests that modifying PEs or employing task-specific data formats in the fine-tuning stage may be ineffective and even harm length generalization performance.

Another line of work**Kaplan et al., "Scaling Language Models with Weak Supervision"**, including single-task RFFT, tries to address length generalization through training models on more elaborate reasoning processes. However, the work has primarily focused on single-task settings, leaving the transferability of such techniques across tasks largely unexplored. Besides, the work still focuses on simple and symbolic tasks such as addition. To alleviate these issues and investigate length generalization in a more comprehensive scope, we curate a diverse set of more practical length generalization tasks and take length generalization performance as a crucial metric to demonstrate the rule-following ability of models.

\paragraph{Case-based reasoning or rule-based reasoning.} 
As large language models (LLMs) are pre-trained on an exceptionally large corpus, a key question has emerged: whether their excellent performance stems from pattern matching or mere memorization (or referred to as ``case-based reasoning'' in **Bender et al., "On the Dangers of Stochastic Parrots"**), or the genuine capture of general rules underlying natural language. The question has attracted significant research interest. Previous work has provided evidence that LLMs often rely on cases seen during training rather than developing systematic reasoning abilities. **Liao et al., "Debiased Large-Scale Language Model Training"** find that LLMs perform significantly worse in counterfactual reasoning compared to factual reasoning, suggesting reliance on memorized common cases. **Zhang et al., "Exploring the Limits of Transfer Learning for Natural Language Processing Tasks"** demonstrates that LLMs conduct reasoning through subgraph matching. **Hewlett et al., "A Polynomial-Time Algorithm for Deep Learning"** shows LLMs rely on surrounding cases for math reasoning instead of learning generalizable rules. 
On the other hand, research on the phenomenon of ``grokking''**Zambaldi et al., "Deep Learning of Graph Representation"** suggests that models can, under certain conditions, learn interpretable rules of arithmetic reasoning long after overfitting the training set. However, this phenomenon has primarily been observed in small transformers trained from scratch on single tasks, such as modular addition. Whether such rule-based reasoning can scale to LLMs in multi-task settings remains an open question. We propose Meta-RFFT to lead the model to be a meta rule follower, which teaches the model to follow the given rules to reason in diverse tasks.

\paragraph{LLMs with programs}
There have been numerous efforts to integrate programs with LLMs to enhance the capabilities of both. Studies have proposed that LLMs can be utilized to assist in code execution**Stutz et al., "Multimodal Explanation Methods for Neural Networks"**, as well as to help developers write code and debug more efficiently**Jiang et al., "CodeBERT: Pre-training Large Scale Code Languages with Weak Supervision"**. On the other hand, programs themselves can also serve as tools to aid model reasoning. **Li et al., "Structured Knowledge Graph Reasoning for Code Completion"** demonstrate that code, as a formal language with minimal ambiguity, can effectively assist models in task reduction and formulation, thus enhance reasoning performance.
In this work, we primarily represent the rules with Python programs for precision and clarity. However, it is also important to note that the rules discussed in this paper are not limited to code representations. In Section~\ref{sec:NL}, we also explore Meta-RFFT in the setting of natural language rules. 

\begin{table}[]
\centering
\caption{The statistics of our dataset. We list the number of tasks collected from each data source and their corresponding split in the RF-pretraining stage or the downstream adaptation stage.}
\vspace{0.1in}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
\textbf{Data Source}  & \textbf{RF-Pretrain} & \textbf{Adaptation} & \textbf{Total} \\
\midrule
LeetCode        & 66          & 8          & 74    \\
NUPA            & 0           & 4          & 4     \\
Big-Bench Hard  & 8           & 0          & 8     \\
Symbolic Reasoning & 2           & 0          & 2     \\
\midrule
All Sources     & 76          & 12         & 88   \\\bottomrule
\end{tabular}
}
\label{tab:data_num}
\end{table}