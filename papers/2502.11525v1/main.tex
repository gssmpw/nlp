%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% \usepackage{minted}
\usepackage[frozencache,cachedir=minted-cache]{minted}
% \usepackage[finalizecache,cachedir=minted-cache]{minted}
\usepackage{listings}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{float}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{multirow}

\usepackage{enumitem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Training Large Language Models to be Better Rule Followers}

\begin{document}

\twocolumn[
\icmltitle{Training Large Language Models to be Better Rule Followers}
% learn meta rules?
% 

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yi Hu}{equal,pku}
\icmlauthor{Shijia Kang}{equal,pku}
\icmlauthor{Haotong Yang}{pku}
\icmlauthor{Haotian Xu}{xhs}
\icmlauthor{Muhan Zhang}{pku}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
\icmlaffiliation{pku}{Institute for Artificial Intelligence, Peking University}
\icmlaffiliation{xhs}{Xiaohongshu Inc}

\icmlcorrespondingauthor{Muhan Zhang}{muhan@pku.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large language models (LLMs) have shown impressive performance across a wide range of tasks. However, they often exhibit unexpected failures in seemingly straightforward tasks, suggesting a reliance on case-based reasoning rather than rule-based reasoning. While the vast training corpus of LLMs contains numerous textual ``rules'', current training methods fail to leverage these rules effectively. Crucially, the relationships between these ``rules'' and their corresponding ``instances'' are not explicitly modeled. As a result, while LLMs can often recall rules with ease, they fail to apply these rules strictly and consistently in relevant reasoning scenarios.
In this paper, we investigate the rule-following capabilities of LLMs and propose \textbf{Meta Rule-Following Fine-Tuning (Meta-RFFT)} to enhance the cross-task transferability of rule-following abilities. We first construct a dataset of 88 tasks requiring following rules, encompassing diverse reasoning domains. We demonstrate through extensive experiments that models trained on large-scale rule-following tasks are better rule followers, outperforming the baselines in both downstream fine-tuning and few-shot prompting scenarios.  This highlights the cross-task transferability of models with the aid of Meta-RFFT. Furthermore, we examine the influence of factors such as dataset size, rule formulation, and in-context learning.
\end{abstract}

\section{Introduction}
Large language models (LLMs) have achieved revolutionary performance across various tasks, from natural language understanding and generation to complex reasoning~\citep{chatgpt, gpt4, grattafiori2024llama3herdmodels,touvron2023llama2openfoundation,qwen2025qwen25technicalreport, yang2024qwen2technicalreport, bai2023qwentechnicalreport,deepseekai2024deepseekv3technicalreport,deepseekai2024deepseekllmscalingopensource}. Notably, the recent release of o1~\citep{openaio1} has demonstrated that post-training scaling laws significantly enhance the complex reasoning capabilities of LLMs, particularly in mathematics and programming, which further ignites our expectations for LLMs to serve as a general-purpose reasoning engine~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,kimiteam2025kimik15scalingreinforcement,wang2024openropensourceframework,zhao2024marcoo1openreasoningmodels,xu2025redstardoesscalinglongcot,xu2025llavacotletvisionlanguage,wei2023skyworkopenbilingualfoundation}. 

Despite the seemingly exceptional performance of LLMs, their unique failure modes motivate deeper investigation into the underlying reasoning mechanisms. More specifically, why are these powerful models capable of solving Olympic-level math problems while struggling with length generalization in basic calculations? Why do LLMs readily recall and even explain complex principles yet fail to apply them systematically, resulting in ``out-of-distribution'' errors? This indicates that LLMs tend to mimic patterns from similar training samples rather than capturing and applying underlying rules, as demonstrated in previous work that LLMs are performing ``case-based reasoning'' rather than ``rule-based reasoning''~\citep{hu2024casebasedrulebasedtransformersmath, dziri2023faithfatelimitstransformers, wu2023reasoning-or-reciting, zhang2023counterfactual}.

Expecting LLMs to naturally discover the ground-truth rules and apply them has been shown to be hard~\citep{gendron2024largelanguagemodelsstrong, bang2023multitaskmultilingualmultimodalevaluation, mitchell2023comparinghumansgpt4gpt4v, yang2024languagemodelsinductivereasoners, moskvichev2023conceptarcbenchmarkevaluatingunderstanding}. Although previous work shows that LLMs can learn certain rules given merely question-answer pairs~\citep{lee2023teaching, zhou2024transformers,cho2024positioncouplingimprovinglength, mcleish2024transformersarithmeticrightembeddings, cho2024arithmetictransformerslengthgeneralizeoperand,nanda2023progress, zhong2023clock, liu2022understanding}, the techniques typically demand task-specific model architecture designs and thus are hard to transfer across different tasks. Consequently, transforming LLMs to perform robust ``rule-based reasoning'' across general tasks remains a crucial research question.

Humans have developed an extensive array of rules throughout history, encompassing both natural laws and regulatory frameworks. For instance, the rules for integer addition are clearly outlined in textbooks, on wikis, and within computer programs. Although these rules may be present within the large training corpus during pretraining for LLMs, current training methods fail to fully leverage them. The relationships between the ``rules'' and their corresponding ``instances'' remain inadequately modeled. As a result, while recalling rules is relatively straightforward for models, they struggle to apply these rules to specific instances. This results in failures to generalize in length on simple problems, e.g., state-of-the-art LLMs perform poorly on basic addition when the digit length is more than 20~\citep{yang2024numbercookbooknumberunderstanding}. %Besides, LLMs exhibit stronger deductive reasoning abilities than inductive reasoning abilities, making it easier for models to apply a given rule to a corresponding instance than to derive an underlying principle from a set of instances. % delete because we are not talking about induction in this paper

%We follow~\citet{hu2024casebasedrulebasedtransformersmath} and further study the rule-following abilities of LLMs. 
Recently, \citet{hu2024casebasedrulebasedtransformersmath} proposed \textbf{R}ule-\textbf{F}ollowing \textbf{F}ine-\textbf{T}uning (RFFT), suggesting that explicitly informing the model of the rules for each task in the input could reduce the model's reliance on case-based reasoning and encourage a shift towards rule-based reasoning, thereby offering advantages in the scenarios of length generalization. We recognize that fine-tuning models on pairs of rules in the input and detailed rule-execution process in the output offers a potential strategy to model the relationships between the ``rules'' and ``instances'', which could foster the model's true comprehension of ``rules'' rather than mere memorization.

 
However, \citet{hu2024casebasedrulebasedtransformersmath} focuses on single-task settings, which necessitates crafting the task-specific rules and costly single-task training. This is extremely inconvenient for users and does not align with our expectation of LLMs to be a general reasoning engine. More crucially, single-task RFFT only models the relationship between one specific rule and the relevant instances, neglecting the broader commonalities between different rules and their potential for mutual generalization. In this paper, we extend RFFT and study the meta-rule-following capabilities across various tasks through \textbf{Meta Rule-Following Fine-Tuning (Meta-RFFT)}. Meta-RFFT jointly trains an LLM on a large number of rule-instance pairs from diverse tasks to model the relationships between multiple rules and their corresponding instances. We hypothesize that training LLMs on extensive, diverse rule-instance pairs can make them better rule followers, so that they can grasp new rules for unseen tasks much easier and more quickly. More specifically, Meta-RFFT trains an LLM not just to follow individual rules but to learn the patterns and structures underlying rules across different tasks, making it better at adapting to unseen rules with minimal fine-tuning.

Our hypothesis is based on research on human learners---experts tend to adapt more effectively to new tasks. The key distinction between experts and novices is that experts can abstract principles, enabling them to develop high-level task representations, while novices base their representations on shallow or literal features~\citep{categorization,Matsuda2009ACM}. This insight inspires us to hypothesize that experienced rule followers might perform better when adapting to new rules. Our experiments validate this hypothesis, showing that models trained on a large set of rule-following tasks and subsequently fine-tuned on a new task significantly outperform models fine-tuned solely on the target task. Furthermore, we show that the performance gain arises from the meta-rule-following model's comprehension of the rule-following nature, analogous to how experienced humans can extract high-level principles from prior experience and apply them to new tasks.

We summarize our contributions as follows:
\begin{itemize}[left=0pt, itemsep=0pt, topsep=0pt]
 \item First, we \textbf{construct a dataset for length generalization across 88 tasks from several task domains}, significantly broadening the previous length generalization tasks which mainly focus on addition or other basic calculations~\citep{zhou2023length, shen2023positional, kazemnejad2023impact, lee2023teaching, zhou2024transformers,cho2024positioncouplingimprovinglength, mcleish2024transformersarithmeticrightembeddings, cho2024arithmetictransformerslengthgeneralizeoperand}. Our tasks span various categories such as code execution, symbolic reasoning, math reasoning, etc. The data sources include LeetCode\footnote{https://leetcode.com}, Big-Bench Hard~\citep{suzgunChallengingBIGBenchTasks2022}, NUPA~\citep{yang2024numbercookbooknumberunderstanding}, and classic symbolic reasoning tasks. Meanwhile, we generate extensive rule-following instances for these tasks.
    
 \item Second, we demonstrate that \textbf{rule-following capabilities can be transferred across task domains through Meta-RFFT}. Specifically, models trained on large-scale length generalization tasks with RFFT in the first stage
only need fine-tuning with a relatively small dataset or prompting with a few exemplars in the second
stage to make significant progress over the baselines in length generalization performance on new tasks. Our results imply great potential for training a universal rule reasoner, or in other words, a foundation model for rules.
    
 \item Finally, we conduct comprehensive ablation studies of this transferability, conducting \textbf{detailed error analysis} and discussing \textbf{the impact of the data size} in both the first and the second stage, as well as studying the adaptability of models to \textbf{various rule formats} after the first stage of large-scale RFFT. Our results reveal what is being transferred and what the commonalities are across tasks.
\end{itemize}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{input_output_new.pdf}
    \vskip -0.03in
    \caption{We illustrate the data construction and training process in this figure. The input-output sequences of \textit{direct answer}, \textit{scratchpad} (upper left) and \textit{RFFT} (right) are shown. Besides, we show the difference between the training process of direct answer / scratchpad / vanilla RFFT and that of Meta-RFFT in the bottom left corner, where direct answer / scratchpad / vanilla RFFT training only contains fine-tuning on the target task, while Meta-RFFT requires large-scale fine-tuning on diverse rule-following tasks and then subsequently fine-tuning on the target task.}
    \label{fig:input_output}
\end{figure*}

\section{Related Work}
\paragraph{Length generalization} 
Despite impressive general capabilities, LLMs have not yet achieved robust length generalization~\citep{zhou2024transformers}, which refers to the ability to generalize to inputs significantly longer than those encountered during training~\citep{abbe2023generalization, anil2022exploring, zhou2023length}. A classic example is that while humans can effortlessly add numbers of any length after mastering basic arithmetic rules, LLMs often struggle to generalize to longer-digit addition beyond their training data. Addressing length generalization remains a highly debated and unresolved challenge in the field.

A series of studies have attempted to tackle this issue by modifying positional encodings (PEs) and data formats~\citep{zhou2023length, shen2023positional, kazemnejad2023impact, lee2023teaching, zhou2024transformers,cho2024positioncouplingimprovinglength, mcleish2024transformersarithmeticrightembeddings, cho2024arithmetictransformerslengthgeneralizeoperand}. However, these efforts are constrained by several limitations. First, the proposed PEs and data formats are often tailored specifically to symbolic tasks, such as addition, making them difficult to generalize to broader tasks. Second, the methods are typically tested on small-scale models trained from scratch, raising doubts about their applicability to practical-scale LLMs. Moreover, \citet{yang2024numbercookbooknumberunderstanding} suggests that modifying PEs or employing task-specific data formats in the fine-tuning stage may be ineffective and even harm length generalization performance.

Another line of work~\citep{hu2024casebasedrulebasedtransformersmath,hou2024universallengthgeneralizationturing, yang2024numbercookbooknumberunderstanding}, including single-task RFFT, tries to address length generalization through training models on more elaborate reasoning processes. However, the work has primarily focused on single-task settings, leaving the transferability of such techniques across tasks largely unexplored. Besides, the work still focuses on simple and symbolic tasks such as addition. To alleviate these issues and investigate length generalization in a more comprehensive scope, we curate a diverse set of more practical length generalization tasks and take length generalization performance as a crucial metric to demonstrate the rule-following ability of models.

\paragraph{Case-based reasoning or rule-based reasoning.} 
As large language models (LLMs) are pre-trained on an exceptionally large corpus, a key question has emerged: whether their excellent performance stems from pattern matching or mere memorization (or referred to as ``case-based reasoning'' in ~\citet{hu2024casebasedrulebasedtransformersmath}), or the genuine capture of general rules underlying natural language. The question has attracted significant research interest. Previous work has provided evidence that LLMs often rely on cases seen during training rather than developing systematic reasoning abilities.~\citet{wu2023reasoning-or-reciting, zhang2023counterfactual} find that LLMs perform significantly worse in counterfactual reasoning compared to factual reasoning, suggesting reliance on memorized common cases.~\citet{dziri2023faith} demonstrates that LLMs conduct reasoning through subgraph matching.~\citet{hu2024casebasedrulebasedtransformersmath} shows LLMs rely on surrounding cases for math reasoning instead of learning generalizable rules. 
On the other hand, research on the phenomenon of ``grokking''~\citep{power2022grokking, liu2022understanding, nanda2023progress, zhong2023clock} suggests that models can, under certain conditions, learn interpretable rules of arithmetic reasoning long after overfitting the training set. However, this phenomenon has primarily been observed in small transformers trained from scratch on single tasks, such as modular addition. Whether such rule-based reasoning can scale to LLMs in multi-task settings remains an open question. We propose Meta-RFFT to lead the model to be a meta rule follower, which teaches the model to follow the given rules to reason in diverse tasks.

\paragraph{LLMs with programs}
There have been numerous efforts to integrate programs with LLMs to enhance the capabilities of both. Studies have proposed that LLMs can be utilized to assist in code execution~\citep{li2024chaincodereasoninglanguage}, as well as to help developers write code and debug more efficiently~\citep{jimenez2024swebenchlanguagemodelsresolve, yang2024swebenchmultimodalaisystems, wang2024openhandsopenplatformai}. On the other hand, programs themselves can also serve as tools to aid model reasoning.~\citet{gao2023palprogramaidedlanguagemodels, chen2023programthoughtspromptingdisentangling, hu2023codepromptingneuralsymbolic, li2024chaincodereasoninglanguage, nye2021workscratchpadsintermediatecomputation} demonstrate that code, as a formal language with minimal ambiguity, can effectively assist models in task reduction and formulation, thus enhance reasoning performance.
In this work, we primarily represent the rules with Python programs for precision and clarity. However, it is also important to note that the rules discussed in this paper are not limited to code representations. In Section~\ref{sec:NL}, we also explore Meta-RFFT in the setting of natural language rules. 

\begin{table}[]
\centering
\caption{The statistics of our dataset. We list the number of tasks collected from each data source and their corresponding split in the RF-pretraining stage or the downstream adaptation stage.}
\vspace{0.1in}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
\textbf{Data Source}  & \textbf{RF-Pretrain} & \textbf{Adaptation} & \textbf{Total} \\
\midrule
LeetCode        & 66          & 8          & 74    \\
NUPA            & 0           & 4          & 4     \\
Big-Bench Hard  & 8           & 0          & 8     \\
Symbolic Reasoning & 2           & 0          & 2     \\
\midrule
All Sources     & 76          & 12         & 88   \\\bottomrule
\end{tabular}
}
\label{tab:data_num}
\end{table}

\section{Methods}
\subsection{Meta Rule-Following Fine-Tuning (Meta-RFFT)}

In this section, we will first review the vanilla RFFT proposed by~\citet{hu2024casebasedrulebasedtransformersmath}, and then introduce Meta-RFFT. 
% We illustrate the workflow of Meta in Figure~\ref{fig:input_output}.

\paragraph{Vanilla RFFT} \citet{hu2024casebasedrulebasedtransformersmath} proposes RFFT, which as shown in Figure~\ref{fig:input_output}, includes the rules to solve the task in the input and detailed rule-execution process in the output. We refer to the method from~\citet{hu2024casebasedrulebasedtransformersmath} as vanilla RFFT in this paper and identify three key components as follows:
(1) the \textit{rules} required to solve a task must be \textit{explicitly provided in the input}; (2) before executing an action, the model is required to \textit{recite the corresponding rule} to ensure precise adherence; and (3) the model must \textit{describe the variables modified by the action}, detailing their states before and after execution.
While we primarily use programmatic representations of rules to ensure clarity and precision, the rules discussed in this paper are not limited to code. In Section \ref{sec:NL}, we also explore natural language rules and investigate the model's ability to transfer rules from code-based representations to other formats.

\paragraph{Meta-RFFT} In this paper, we use length generalization performance as a key metric to evaluate the rule-following ability of models. We note that tasks requiring length generalization frequently exhibit common structural elements, such as loops, conditional branches, and other recurring patterns. However, the standard RFFT approach necessitates independent fine-tuning for each specific task, which is computationally costly and fails to leverage the structural similarities across tasks. To mitigate such limitations, we introduce Meta-RFFT, a framework designed to leverage the structural commonalities among tasks. As illustrated in Figure~\ref{fig:input_output} (lower left corner), Meta-RFFT adopts a two-stage training process: we first train the model on a large number of rule-following tasks, and refer to this stage as ``RF-pretraining'', following which we fine-tune the model on the target tasks using a relatively small dataset. During RF-pretraining, the model is expected to grasp the shared commonalities and fundamental rule concepts, such as loops. By leveraging these shared structures, Meta-RFFT enables models to quickly adapt to new target tasks with minimal fine-tuning or even solve them through few-shot prompts. Additionally, training across multiple tasks reduces the risk of overfitting to a single task's text, encouraging the model to transform from case-based reasoning to more robust rule-based reasoning. 
% Experimental results demonstrate that this two-stage training strategy achieves significant improvements over the vanilla RFFT.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.9\linewidth]{ft_qwen7b_lc_nupa_d.pdf}
    \vskip -0.05in
    \caption{Length generalization performance of \textit{direct answer}, \textit{scratchpad}, \textit{vanilla RFFT} and \textit{Meta-RFFT} on LeetCode and NUPA tasks. The shaded region represents the in-distribution test results (length $\leq 5$), while the unshaded background corresponds to out-of-distribution lengths (length $\geq 6$). Here the base model is Qwen2.5-7B-Instruct.}
    \label{fig:qwen-7b-ft-lc}
\end{figure*}

\subsection{Data Construction}

To extend the horizon of length generalization in rule-following tasks, and to facilitate large-scale multi-task training as well as comprehensive evaluation, it is essential to construct a dataset that spans a diverse range of tasks. When selecting these tasks, we follow these guiding principles:
\begin{itemize}[left=0pt, itemsep=0pt, topsep=0pt]
    \item \textbf{Tasks must exhibit the property of length generalization.}  Specifically, solving a task should require iterative reasoning. For example, the coin flip task requires enumerating each participant's actions to determine whether to change the state of the coin. Here we use the number of participants to denote the ``length'' of each question.
    % \item \textbf{Tasks should not be overly complex within a single iteration.} In each iteration, the task should remain relatively simple, ensuring that LLMs can reliably perform each step. This is because our primary objective is to evaluate the model's ability to generalize across different sequence lengths rather than to achieve perfect accuracy in every atomic operation. Consequently, we do not require the model to perform complex single-step operations, such as sorting an entire list with high accuracy. Specifically, we exclude tasks with complex input formats—such as graphs or multi-dimensional data—as well as those involving intricate math operations, like square roots or large integer multiplications, since LLMs currently struggle with these operations.
    \item \textbf{Tasks should avoid excessive complexity within a single iteration.} Each step of a task should not be too hard for the LLM, as the primary goal is to assess the model's ability to generalize across varying sequence lengths, rather than achieving flawless accuracy in every atomic operation. Consequently, we avoid tasks requiring complex single-step operations, such as sorting an entire list with high precision. Specifically, we exclude tasks with complex input formats (e.g., graphs or multi-dimensional data) and those involving advanced mathematical operations (e.g., square roots or large integer multiplications), as LLMs still struggle with such complexities.
\end{itemize}

Following these principles, we construct a dataset covering diverse domains, including code execution, number processing, logical reasoning, and symbolic reasoning. Our data sources are as follows:

\begin{itemize}[left=0pt, itemsep=0pt, topsep=0pt]
    \item \textbf{LeetCode Problems.} Since most problems on LeetCode can be scaled with varying input sizes---primarily in terms of length---many of them are naturally suited for evaluating length generalization. For instance, in the task \textit{LC Add Digits} (``repeatedly sum all digits until the result is a single digit''), we use the number of digits in the input to denote the inherent ``length'' of the task. Based on this criterion, we selected 74 tasks from the LeetCode platform.
    
    \item \textbf{NUPA.} NUPA is a benchmark designed to assess basic number processing capabilities of LLMs~\citep{yang2024numbercookbooknumberunderstanding}. While many tasks in NUPA are still overly challenging for current LLMs, we select four practical tasks feasible within the context length in terms of RFFT.
    
    \item \textbf{Big-Bench Hard.} The benchmark includes reasoning tasks considered challenging for LLMs~\citep{suzgunChallengingBIGBenchTasks2022}. We select 8 tasks that are suitable for length generalization evaluation.
    
    \item \textbf{Symbolic Reasoning.} We select ``coin flip'' and ``last letter concatenation'' from~\citet{weiChainofThoughtPromptingElicits2023a}.
\end{itemize}

Our dataset includes 88 tasks in total, with examples of each domain presented in Appendix \ref{app:input_examples}. To transform the tasks to be suitable for evaluating models' length generalization capabilities, we define the concept of ``length'' for each problem and develop data generation scripts that can produce task instances of any specified length. Then, we annotate RFFT-style input-output sequences for each task, as shown in Figure~\ref{fig:input_output}, with detailed procedures in Appendix~\ref{app:data_annotation}.

\section{Results}

\subsection{Experimental Setup}
Our experiments involve two training stages: \textit{RF-pretraining stage} and \textit{downstream adaptation stage}. In the RF-pretraining stage, we fine-tune the model on 76 tasks, aiming to develop a model that can strictly follow rules across multiple tasks and potentially transfer this capability to new tasks. In the adaptation stage, we select 4 NUPA tasks and 8 LeetCode tasks of appropriate difficulty and practical significance to assess the task transferability of the rule-following capability of the RF-pretrained model. The description of each downstream task is provided in Appendix~\ref{app:taks_description}, while the dataset statistics are listed in Table~\ref{tab:data_num}, and the training hyperparameters are detailed in Appendix~\ref{app:training_details}.

\paragraph{RF-pretraining} For each task, 300 rule-following samples are generated for each length from 1 to 15, resulting in approximately 310k samples in total. We train models of two different sizes: Qwen-7B and Qwen-32B~\citep{qwenQwen25TechnicalReport2025}. The 7B model is fine-tuned using full-parameter tuning, while the 32B model is fine-tuned with PiSSA~\citep{mengPiSSAPrincipalSingular2024b}.

\paragraph{Downstream adaptation} In the adaptation stage, we fine-tune the RF-pretrained model on the target task. To evaluate the model's rule-following ability on the target task, we train the models on data of lengths from 1 to 5 and test their performance on out-of-distribution (OOD) lengths 6 to 30. Length generalization performance works as a metric of rule-following capability here, and the OOD test data can assess whether the model truly understands the rules for solving the test task or is merely fitting the training data. For each task, 1,000 samples are generated for each length from 1 to 5, resulting in a total of 5k training samples. Both the 7B and 32B models are fine-tuned using PiSSA. For test lengths, 100 samples are generated for each length to evaluate performance.

\subsection{Rule-Following is a Task-Transferable Ability}
We compare Meta-RFFT with three baseline methods: direct answer, scratchpad, and vanilla RFFT. As shown in Figure~\ref{fig:input_output}, the base model is fine-tuned directly on the target task for the direct answer, scratchpad, and vanilla RFFT approaches. In contrast, Meta-RFFT employs a two-stage training process. First, the base model is RF-pretrained on a diverse set of rule-following tasks with sequence lengths ranging from 1 to 15, explicitly excluding the target tasks. The RF-pretrained model is then fine-tuned on the target task, which involves sequences of lengths 1 to 5, with longer sequences reserved for OOD testing. To ensure fairness, all methods use identical settings during fine-tuning, including training samples and the number of training steps.

The test results of Qwen-2.5-7B-Instruct trained with direct answer, scratchpad, vanilla RFFT and Meta-RFFT are shown in Figure~\ref{fig:qwen-7b-ft-lc}. Additionally, experiments are conducted on a larger model, Qwen-2.5-32B-Instruct, with the results detailed in Appendix~\ref{sec:32b_ft}. Overall, Meta-RFFT demonstrates superior performance in terms of length generalization compared to the other methods.
For the 7B model, the performance of Meta-RFFT declines relatively slowly with increasing sequence length, whereas the direct answer, scratchpad, and vanilla RFFT methods suffer from more significant performance drops when extrapolating to longer sequences. For the 32B model, surprisingly, Meta-RFFT exhibits minimal performance degradation even up to sequences of length 30, achieving a length extension ratio of $6\times$ compared to training data. This suggests that the advantages of Meta-RFFT can be further developed with stronger base models. In contrast, the other methods suffer performance losses as sequence lengths increase across many tasks. 
\begin{figure}[ht]
    \centering
    \subfigure[Error distribution of vanilla RFFT models and Meta-RFFT models. Errors are classified into Loop Errors and Non-loop Errors. Error bars denote 95\% confidence interval.]{
        \includegraphics[width=.75\linewidth]{error_distribution.pdf}
        \label{fig:error_distribution}
    }
    \subfigure[Relative error of the loop count of vanilla RFFT models and Meta-RFFT models on LeetCode tasks.]{
        \includegraphics[width=.75\linewidth]{relative_length_error.pdf}
        \label{fig:relative_length_error}
    }
    \vskip -0.05in
    \caption{Error analysis of vanilla RFFT and Meta-RFFT models.}
    \label{fig:combined}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.9\linewidth]{icl_qwen7b.pdf}
    \vskip -0.1in
    \caption{We show the 1-shot performance of the base model (Qwen-2.5-7B-Instruct), and the RF-pretrained model (RF-pretrained Qwen-2.5-7B-Instruct). RF-pretrain also enhances the in-context learning performance in the downstream tasks. }
    \label{fig:icl_7b}
\end{figure*}

\paragraph{Error Analysis: How does Meta-RFFT help length generalization?}
We analyze the errors of both vanilla RFFT and Meta-RFFT models and find out that a major cause of errors in length generalization tasks lies in the model's inability to maintain the correct number of loops corresponding to the intended ``length''. Models tend to either prematurely terminate loops or fail to exit loops, leading to repetitive outputs until the token count reaches the context length limit. We illustrate the proportion of error cases caused by incorrect iteration counts in Figure~\ref{fig:error_distribution}, indicating that the errors attributing to incorrect loop counts indeed constitute a significant portion of the overall errors. We hypothesize that during the RF-pretraining stage, the model learns to correctly maintain loop counts as a sub-skill of rule-following, as it is exposed to a large number of examples involving following rules to maintain loops. Consequently, in downstream tasks, the RF-pretrained models demonstrate better loop maintenance. 

To validate this hypothesis, we compare the relative error between the model's predicted iteration counts and the ground truth iteration counts, as shown in Figure~\ref{fig:relative_length_error}. Meta-RFFT models exhibit significantly reduced errors in iteration counts across tasks compared to vanilla RFFT, thereby verifying our hypothesis. This reduction in error highlights the effectiveness of RF-pretraining in enhancing the model's ability to handle iterative tasks, contributing to its improved performance in length generalization.

\paragraph{Comparison to vanilla RFFT.}
\label{sec:comparison_vanilla_loss}
To further validate the reasons behind the performance improvement of Meta-RFFT over vanilla RFFT are that Meta-RFFT helps models develop rule-following capabilities transferable across tasks, we analyze the model performance and training curves of both methods. The training curves for the adaptation stage of the 7B and 32B models are shown respectively in Figure~\ref{fig:training_curve-7b_1vs2} and Figure~\ref{fig:training_curve-32b_1vs2}. Models trained with Meta-RFFT exhibit lower initial training loss compared to vanilla RFFT, as the former is already familiar with the rule-following paradigm due to the first-stage pretraining. This allows Meta-RFFT models to fit the training samples more quickly during the adaptation stage. As training progresses, the training loss of vanilla RFFT and Meta-RFFT models converges to similar levels in most tasks. This indicates that the gap in length generalization performance between Meta-RFFT and vanilla RFFT is not due to the latter's inability to fit the training data. To further validate this, we evaluate the performance of Meta-RFFT at the 60-step checkpoints (Meta-RFFT-ckpt60 models) during the adaptation stage, where the loss has not yet converged, as shown in Figure~\ref{fig:datasize_7b_2-stage}. We assure that the training losses of the 60-step checkpoints are consistently higher than those of the vanilla RFFT models across all target tasks, with the performance of the latter shown in Figure~\ref{fig:qwen-7b-ft-lc}. However, the Meta-RFFT-ckpt60 models, which have not fully converged, still outperform the vanilla RFFT models in terms of performance. This further confirms that the superior length generalization of Meta-RFFT is not attributable to the better fitting of the training data but rather to its enhanced ability to generalize to longer sequences.

\paragraph{Direct answer, scratchpad vs RFFT.}
On some tasks, scratchpad or even direct answer outperforms vanilla RFFT, despite using fewer tokens and thus being more cost-effective. This indicates that, for certain tasks, the format of direct answer or scratchpad is more friendly for models and can lead models to successfully generalize to longer sequences. However, the superior performance of direct answer or scratchpads is not consistently observed across all tasks. On the remaining tasks, direct answer or scratchpad formatting is insufficient to enable robust length generalization. While direct answer and scratchpad formatting features lower costs and prove effective for a subset of tasks, it does not consistently aid models in achieving length generalization. In contrast, Meta-RFFT consistently outperforms the baselines in terms of length generalization for the 7B model. For the 32B model, as the basic capabilities of the base model improve, Meta-RFFT successfully enables robust length generalization across all tasks.

\subsection{In-Context Learning}

\paragraph{Experimental Settings} To enable the model to adapt to the in-context learning (ICL) paradigm, where few-shot examples are provided within the input, we include a 1-shot exemplar in each training sample. In-context learning requires the model to establish stronger correspondences between rules and execution traces, as it must learn to follow a new rule from just a single example. To improve this, we augment the training data with synthetic data, assigning a unique rule to each sample. This approach increases task diversity and encourages the model to rely on the provided rules during training. Specifically, we manually design 22 code snippets and their corresponding rule-following outputs (see Appendix \ref{app:synthetic_data} for details). For each sample, we randomly select several snippets to define its rule, allowing us to generate an arbitrary number of tasks with diverse rules and outputs. Using this method, we create 100k synthetic samples and combine them with 310k samples from the RF-pretraining dataset to form the ICL training dataset.


\paragraph{Results} The ICL performance of 7B models are shown in Figure~\ref{fig:icl_7b}. We show the results of 32B models in Appendix~\ref{sec:32b_icl}. The RF-pretrained model with synthetic data significantly outperforms the base model on LeetCode tasks in the 1-shot learning setting. This demonstrates that the model not only acquires the potential for transferable rule-following ability during the RF-pretraining stage but also that this capability does not necessarily require fine-tuning on downstream tasks to be activated. The strong in-context learning performance suggests that the model can accurately apply rules based solely on the examples provided in the input during testing. Consequently, an RF-pretrained model can demonstrate robust length generalization on a given task without requiring additional task-specific fine-tuning, making it particularly advantageous for real-world applications.

\subsection{Ablations}
\paragraph{What about following natural language rules?} 
~\label{sec:NL}
% use natural language rule instead of code or pseudocode
% + discussion of whether the model is overfitting to the form of formal "code" 

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{ft_qwen7b_nl_ablation.pdf}
    \vskip -0.2in
    \caption{Vanilla RFFT and Meta-RFFT results on LC Add Digits. In the downstream adaptation stage, we use natural language rules instead of code representations.}
    \label{fig:qwen7b_nl}
\end{figure}


We use rules represented by Python programs in the previous sections due to their clarity, conciseness, and low ambiguity. However, rules can be expressed in various forms, and in everyday life, natural language is another primary medium for representing rules. We also explore the adaptability of models trained with code-based rules during the RF-pretraining stage to tasks involving natural language-based rules in downstream scenarios.

More specifically, to investigate whether the superior performance of Meta-RFFT on target tasks truly stems from the better mastering and understanding of general rules, rather than merely overfitting to specific code statements like \verb|pop()| and \verb|insert()| during the RF-pretraining stage, we represent the rules in forms of natural language instead of Python code and evaluated the performance of Meta-RFFT and vanilla RFFT . Notably, the rules in the inputs of the target tasks no longer share specific statements with the rules seen during the RF-pretraining stage. We list an example pair of input-output sequences in Appendix \ref{app:NL}. 
% To investigate whether the superior performance of Meta-RFFT on target tasks truly stems from the better mastering and understanding of general rules, rather than merely overfitting to specific code statements like \verb|pop()| and \verb|insert()| during the RF-pretraining stage, we rewrite the rules of LC258 from code format to natural language (details in Appendix \ref{app:NL}) and evaluated the performance of Meta-RFFT and vanilla RFFT. Notably, this means that the rule of this target task no longer share specific statements with the rules during the RF-pretraining stage.

The results of task~\textit{LC Add Digits} are shown in Figure~\ref{fig:qwen7b_nl}, showing that Meta-RFFT still significantly outperforms vanilla RFFT on lengths ranging from 12-30. This suggests that the superior performance of Meta-RFFT is not simply due to fitting specific code statements. Instead, the RF-pretrained model has acquired a meta rule-following ability, enhancing its capacity for length generalization.

\paragraph{What is the effect of the data size in RF-pretraining?}
% 我们挑选了rf-pretraining阶段training loss收敛后的一些checkpoints进行target-task fine-tuning。我们把这些ckpt经过target-task finetune后在对应task上的length generalization performance展现在Figure~\ref{fig:datasize_7b_rf_pretrain}。在前期，随着训练进行，模型最终的performance会缓慢上升，而到达了一定的step数过后模型的表现区域稳定，不再有明显的提升。
We select several checkpoints from the RF-pretraining stage after the training loss has converged and perform downstream adaptation on these checkpoints. The performance of these checkpoints on the corresponding tasks after fine-tuning is presented in Figure~\ref{fig:datasize_7b_rf_pretrain}. In the early stages, as training progresses, the model's final performance gradually improves. However, after reaching a certain number of steps, the model's performance stabilizes and no longer shows significant improvement. The models do not show signs of ``grokking'' during the RF-pretraining stage.



\paragraph{What is the effect of the data size in downstream adaptation?} 
For the downstream adaptation stage, we also analyze the effects of data size on performance. We select several checkpoints after the training loss has converged. The results of these checkpoints are presented in Figure~\ref{fig:datasize_7b_rf_pretrain}. Similar to the RF-pretraining stage, in the early phases, the model's performance improves as the data size increases. However, after reaching a certain number of steps, the model's performance stabilizes and no longer shows significant improvement, with no evidence of grokking observed.



% \paragraph{More practical and complex settings? (law reasoning)} 


\section{Conclusion}
We aim to investigate the rule-following capabilities of LLMs. We first construct a dataset including 88 tasks requiring models' length generalization abilities, involving diverse reasoning domains. We use length generalization performance as a key metric to evaluate the rule-following ability of models. Second, to enhance the rule-following capabilities of LLMs, we propose the Meta-RFFT. We train the base model on a large-scale set of rule-following tasks to transform it into a better rule follower. During fine-tuning on downstream tasks, models after RF-pretrained demonstrate superior length generalization performance compared to baseline models. We further verify that the advantage of Meta-RFFT lies in enabling the model to learn meta rule-following capabilities that are transferable across tasks. Additionally, we show that RF-pretrained models exhibit improved 1-shot rule-following capabilities compared to the base model, reducing the cost of adapting to new tasks. Furthermore, we conduct a comprehensive investigation into this transferability, including error analysis, discussions on training curves, the impact of data size in each training stage, and the model's adaptability to various rule representations.
% In this paper, we explore the rule-following capabilities of Large Language Models (LLMs). We first introduce a dataset comprising 88 tasks designed to evaluate length generalization across diverse reasoning domains, using this as a key metric to assess rule-following performance. To enhance LLMs' rule-following abilities, we propose Meta-RFFT, a method that pre-trains base models on a large-scale set of rule-following tasks, transforming them into more effective rule followers. Fine-tuned models demonstrate superior length generalization compared to baselines, with Meta-RFFT enabling the acquisition of transferable meta rule-following skills. Additionally, RF-pretrained models exhibit improved 1-shot rule-following, reducing adaptation costs for new tasks. We further analyze the transferability of these capabilities, including error patterns, training dynamics, data size effects, and adaptability to different rule representations.

\paragraph{Discussion} We believe that the transferability of rule-following capabilities in LLMs demonstrates the potential for achieving more robust rule-following across general tasks. This opens up opportunities for more practical applications, such as adjudicating legal cases in accordance with statutory provisions or executing tasks by adhering to predefined workflows.

% We believe that the transferability of rule-following capabilities in LLMs demonstrates the potential for building a foundation model for rules, and our work takes an initial step toward enhancing models into a universal rule reasoner.

\newpage
\section*{Impact Statement}

Our work focuses on establishing a relationship between rules and their corresponding instances within LLMs. We aim to enhance model performance on downstream tasks by training the base model on a wide range of rule-following tasks. Current training strategies fall short in enabling models to fully grasp the rules that humans have summarized or proposed that exist in the pre-training corpus. As a result, while LLMs can easily recall rules, they often struggle to apply these rules strictly to specific instances. Our proposed Meta-RFFT takes an initial step towards strengthening models into meta rule followers, a development that is crucial for improving both the reasoning capabilities and learning efficiency of these models. 

Teaching LLMs to follow rules also aligns with societal demands. By ensuring that LLMs can reliably adhere to rules, we contribute to the development of AI systems that are more aligned with human values, ethical standards, and practical applications, ultimately fostering trust and safety in their deployment.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Dataset Overview}

\lstset{
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{lightgray!20},
    frame=none,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}


\subsection{Rule-Following Input Examples}
\label{app:input_examples}
We present a question example for each reasoning domain in Table~\ref{tab:input_examples}.

\begin{table}[H]
\centering
\caption{Input example in rule-following format for each category.}
\vskip 0.15in
\begin{tabular}{|>{\raggedright\arraybackslash}p{0.5\textwidth}|>{\raggedright\arraybackslash}p{0.5\textwidth}|}
\hline
\textbf{LeetCode} & \textbf{NUPA} \\ \hline
\makecell[l]{Follow the given rule to solve the question. \\ rule:}
\begin{lstlisting}
def moveZeros(nums):
    num_zero = 0
    result = []
    while nums:
        number = nums.pop(0)
        if number != 0:
            result.append(number)
        else:
            num_zero += 1
    i = 0
    while i < num_zero:
        result.append(0)
        i += 1
    return result
\end{lstlisting}
Q: Given an integer array [0, 16], move all zeros to the end while preserving the relative order of the non-zero elements.
&
\makecell[l]{Follow the given rule to solve the question. \\ rule:}
\begin{lstlisting}
def add(num1, num2):
    result = ''
    carry = 0
    # Main Loop
    while num1 or num2:
        digit1 = int(num1[-1]) if num1 else 0
        digit2 = int(num2[-1]) if num2 else 0
        total = digit1 + digit2 + carry
        result = str(total%10) + result
        carry = total//10
        num1 = num1[:-1] if num1 else num1
        num2 = num2[:-1] if num2 else num2
    if carry:
        result = str(carry) + result
    result = result.lstrip('0') or '0'
    return result
\end{lstlisting}
\makecell[l]{Q: Add two numbers: 123 + 4567. \\ ~}
\\ \hline
\textbf{Big-Bench Hard} & \textbf{Symbolic Reasoning}\\ \hline
\makecell[l]{Follow the given rule to solve the question. \\ rule:}
\begin{lstlisting}
def navigate(moves):
    # Initialize Location
    loc = [0, 0]
    # Main Loop
    while moves:
        move = moves.pop(0)
        if move[0] == "left":
            loc[0] -= move[1]
        elif move[0] == "right":
            loc[0] += move[1]
        elif move[0] == "forward":
            loc[1] += move[1]
        elif move[0] == "backward":
            loc[1] -= move[1]
    return loc == [0, 0]
\end{lstlisting}
Q: If you follow these instructions, do you return to the starting point? Always face forward. Take 2 steps left. Take 2 steps forward.
In short, the moves are as follows: [(`left', 2), (`forward', 2)].
&
\makecell[l]{Follow the given rule to solve the question. \\ rule:}

\begin{lstlisting}
def coin_flip(flips):
    # Initialize Coin State
    heads_up = True
    # Main Loop
    while flips:
        flip = flips.pop(0)
        if flip:
            heads_up = not heads_up
        else:
            pass
    return heads_up
\end{lstlisting}
Q: A coin is heads up. Carrillo does not flip the coin. Cunningham flips the coin. Is the coin still heads up?
In short, the situation of 2 people flipping coins is as follows: [False, True].
\\ \hline
\end{tabular}
\label{tab:input_examples}
\end{table}


\subsection{Downstream Tasks Description}
\label{app:taks_description}

The descriptions of 12 selected downstream tasks are listed as follows:
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item \textbf{LC Add Digits:} Given an integer, repeatedly sum its digits until the result is a single digit.
    \item \textbf{LC Move Zeroes:} Given a list of integers, move all zeros to the end while preserving the relative order of the non-zero elements.
    \item \textbf{LC Hamming Distance:} The Hamming distance between two integers is the number of positions at which the corresponding bits are different. Given two integers in binary representation, return their Hamming distance.
    \item \textbf{LC Crawler Log Folder:} Determine the final folder after performing the operations in the given list, where \verb|../| moves up one level, \verb|./| stays in the current folder, and \verb|x/| moves into folder \verb|x|.
    \item \textbf{LC Alternate Digit Sum:} Given a positive integer where the most significant digit has a positive sign, and each subsequent digit has the opposite sign of its adjacent digit, return the sum of these signed digits.
    \item \textbf{LC Chunk Array:} Given array and chunk size, split the array into subarrays of a given size.
    \item \textbf{LC String Sequence:} Given a target string, return a list of all strings that appear on the screen in order, using the minimum key presses. Key 1 appends "a" to the string, and Key 2 changes the last character to its next letter in the alphabet.
    \item \textbf{LC Valid Palindrome:} Given a string s, return true if it is a palindrome after removing all non-alphanumeric characters and converting it to lowercase; otherwise, return false.
    \item \textbf{NUPA Get Digit Integer:} Get the digit at the given position (from left to right, starting from 0).
    \item \textbf{NUPA Add Integer:} Add two integers.
    \item \textbf{NUPA Digit Max Integer:} Compare two numbers digit by digit and return the larger digit at each position, treating any missing digits as 0.
    \item \textbf{NUPA Length Integer:} Find total number of digits of the given integer.
    
\end{itemize}




\section{Training Details}
\label{app:training_details}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}


Table~\ref{tab:hyperparameters} shows the training parameters for the RF-pretraining stage and the adaptation stage of the Qwen-7B and Qwen-32B models. In the RF-pretraining stage, we use data samples with a length of 31 from the training set as the validation set and the early stop strategy is applied based on the validation loss, which results in different training steps. Considering early stopping, the number of training data samples for the 7B and 32B models in the RF-pretraining stage is 179k and 205k, respectively.

Since the RF-pretraining stage involves fine-tuning across numerous tasks, we train more model parameters during this stage. The 7B model uses full parameter fine-tuning, while due to computational resource constraints, the 32B model employs PiSSA with a large rank of 32. In the adaptation stage, where fine-tuning is performed on different target tasks, we use PiSSA with a relatively small rank of 8.

\begin{table}[H]
\centering
\caption{The training hyperparameters for the RF-pretraining stage and the adaptation stage.}
\vspace{0.1in}
\resizebox{0.55\textwidth}{!}{
\begin{tabular}{c|cccc}
\toprule
\multirow{2}{*}{\textbf{Hyperparameters}} & \multicolumn{2}{c|}{\textbf{RF-Pretrain}}                    & \multicolumn{2}{c}{\textbf{Adaptation}} \\ \cline{2-5} 
   & \multicolumn{1}{c|}{Qwen-7B} & \multicolumn{1}{c|}{Qwen-32B} & \multicolumn{1}{c|}{Qwen-7B} & Qwen-32B \\ \midrule
Training Steps   & \multicolumn{1}{c|}{800}     & \multicolumn{1}{c|}{700}      & \multicolumn{1}{c|}{156}     & 156      \\ \midrule
Num of Epoch    & \multicolumn{4}{c}{1}               \\ \midrule
Learning Rate    & \multicolumn{4}{c}{1e-5}               \\ \midrule
Batch Size       & \multicolumn{2}{c|}{256}            & \multicolumn{2}{c}{32}                  \\ \midrule
Fine-Tuning Method & \multicolumn{1}{c|}{full fine-tune}    & \multicolumn{3}{c}{PiSSA}        \\ \midrule
PiSSA Rank       & \multicolumn{1}{c|}{/}       & \multicolumn{1}{c|}{32}       & \multicolumn{2}{c}{8}                   \\ \bottomrule
\end{tabular}
}
\label{tab:hyperparameters}
\end{table}

\section{Additional Results}
\subsection{Results of 32B Model}
\paragraph{Fine-tuning results.}
\label{sec:32b_ft}
In Figure~\ref{fig:qwen32b_ft_lc_np}, we list the length generalization performance of Qwen-2.5-32B-Instruct fine-tuned through the following methods: \textit{direct answer}, \textit{scratchpad}, \textit{vanilla RFFT} and \textit{Meta-RFFT}. \textit{Meta-RFFT} significantly outperforms the rest of the methods, showing that rule-following is a meta ability that can be mastered by large-scale RF-pretrain and can benefit length generalization greatly.

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{ft_qwen32b_lc_nupa.pdf}
    \caption{Length generalization performance of \textit{direct answer}, \textit{scratchpad}, \textit{vanilla RFFT} and \textit{Meta-RFFT} on LeetCode and NUPA tasks. Here the experiments are all conducted on Qwen-2.5-32B-Instruct.}
    \label{fig:qwen32b_ft_lc_np}
\end{figure}


\paragraph{In-context learning results.}
\label{sec:32b_icl}
In Figure~\ref{fig:qwen32b_icl}, we show the in-context learning performance of both the base model and the RF-pretrained model. Here the base model we use is Qwen-2.5-32B-Instruct. The RF-pretrained model outperforms the base model by a large margin in the context of 1-shot learning on downstream tasks. RF-pretraining shows a positive transfer to the in-context rule-following capabilities in downstream tasks.

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{icl_qwen32b.pdf}
    \caption{The figure shows the 1-shot performance of the base model (Qwen-2.5-32B-Instruct), and the RF-pretrained model (RF-pretrained
Qwen-2.5-32B-Instruct).}
    \label{fig:qwen32b_icl}
\end{figure}


\subsection{Training Curves}
We show the training loss curves of the downstream adaptation stage of the 7B base model and the 32B base model respectively in Figure~\ref{fig:training_curve-7b_1vs2}, Figure~\ref{fig:training_curve-32b_1vs2}. The figures show that models trained with Meta-RFFT
exhibit lower initial training loss compared to vanilla RFFT,
as the former is already familiar with the rule-following
paradigm due to the first-stage pretraining. This allows Meta-RFFT models to fit the training samples more quickly
during the adaptation stage. As training progresses,
the training loss of vanilla RFFT and Meta-RFFT models
converges to similar levels in most tasks. This indicates
that the gap in length generalization performance between
Meta-RFFT and vanilla RFFT is not due to the latter’s in-
ability to fit the training data. More detailed discussions are put in Section~\ref{sec:comparison_vanilla_loss}.

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{training_curve_qwen7b_2-stage-vs-1-stage.pdf}
    \caption{Training curves of Qwen2.5-7B-Instruct in the downstream adaptation stage on LeetCode tasks and NUPA tasks. The figure shows both the training curves of vanilla RFFT and the second training stage of Meta-RFFT.}
    \label{fig:training_curve-7b_1vs2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{training_curve_qwen32b_2-stage-vs-1-stage.pdf}
    \caption{Training curves of Qwen2.5-32B-Instruct in the downstream adaptation stage on LeetCode tasks and NUPA tasks. The figure shows both the training curves of vanilla RFFT and the second training stage of Meta-RFFT.}
    \label{fig:training_curve-32b_1vs2}
\end{figure}

Besides, we conduct repeated experiments in the stage of downstream fine-tuning. We show the training curves of different random seeds of 7B RF-pretrained models in the adaptation stage training in Figure~\ref{fig:training_curve_qwen7b_random_seed}, indicating that the adaptation stage training is stable across different seeds.

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{training_curve_qwen7b_2-stage-seed.pdf}
    \caption{Training curves of the adaptation stage of Meta-RFFT using different random seeds. Here we show the results of Qwen2.5-7B-Instruct.}
    \label{fig:training_curve_qwen7b_random_seed}
\end{figure}



\subsection{Effects of Data Dize on RF-pretraining and Downstream Adaptation}
% 这个图比较大，而且没有特别surprising的结论，可以放附录
For RF-pretraining stage, we select several checkpoints after the training loss has converged and
perform downstream adaptation on these checkpoints. The
length generalization performance of these checkpoints on
the corresponding tasks after fine-tuning are presented in Figure~\ref{fig:datasize_7b_rf_pretrain}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{ft_qwen7b_datasize_pretrain.pdf}
    \caption{Effects of training steps in the RF-pretraining stage.}
    \label{fig:datasize_7b_rf_pretrain}
\end{figure}

For downstream adaptation stage, we also select several checkpoints after the training loss has converged and show the results in Figure~\ref{fig:datasize_7b_2-stage}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{ft_qwen7b_datasize_2-stage.pdf}
    \caption{Effects of training steps in the downstream adaptation stage of Meta-RFFT.}
    \label{fig:datasize_7b_2-stage}
\end{figure}

\section{Dataset Construction}
\newmdenv[
  backgroundcolor=gray!10,
  linecolor=gray!70,
  linewidth=0.5mm,
  roundcorner=0pt,
  skipabove=10pt,
  skipbelow=10pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  innerleftmargin=10pt,
  innerrightmargin=10pt
]{promptbox}

\subsection{Rule-following Dataset Annotation}
\label{app:data_annotation}
To generate the rule-following output for each task, the rules necessary to accomplish the task first need to be provided in the input (examples are shown in Table \ref{tab:input_examples}), which can be generated by an LLM. Subsequently, a data generation script is manually written for each task to produce the intermediate execution steps of the rule for any given input. With the rule-following output generation script and the input data generator, an arbitrary number of data samples can be generated. Here is an example of one data sample in the rule-following format.


Prompt:
\begin{promptbox}[]
Follow the given rule to solve the question.\\
Rule:
\begin{minted}{python}
def add_digits(self, num: int) -> int:
    while num > 9:
        sum = 0
        while num:
            sum += num % 10
            num //= 10
        num = sum
    return num
\end{minted}
Q: Given an integer number 15, repeatedly add up all its digits until the result has only one digit.
\end{promptbox}
Response:
\begin{promptbox}[]
1. Initialize \\
num = 15 \\
2. Outer loop

\begin{minted}{python}
while num > 9:
\end{minted}

num = 15 \\
enter the loop \\
2.1 One iteration \\
sum = 0 \\
3. Inner loop

\begin{minted}{python}
while num:
\end{minted}

num = 15 \\
enter the loop \\
3.1 One iteration

\begin{minted}{python}
sum += num % 10
num //= 10
\end{minted}

num = 15 \\
num \% 10 = 5 \\
sum = 0 \\
now, \\
sum = 0 + 5 = 5 \\
num = 15 // 10 = 1

\begin{minted}{python}
while num:
\end{minted}

num = 1 \\
enter the loop \\
3.1 One iteration

\begin{minted}{python}
sum += num % 10
num //= 10
\end{minted}

num = 1 \\
num \% 10 = 1 \\
sum = 5 \\
now, \\
sum = 5 + 1 = 6 \\
num = 1 // 10 = 0

\begin{minted}{python}
while num:
\end{minted}

num = 0 \\
do not enter

\begin{minted}{python}
num = sum
\end{minted}

sum = 6 \\
now, \\
num = 6

\begin{minted}{python}
while num > 9:
\end{minted}

num = 6 \\
do not enter \\
4. Return

\begin{minted}{python}
return num
\end{minted}

num = 6 \\
So the answer is 6

\end{promptbox}
\subsection{Synthetic Data Generation}
\label{app:synthetic_data}
22 manually-written code snippets are as below:
\begin{promptbox}
\begin{minted}{python}
0: "if list1:\n    list1[-1] += {}",
1: "if list1:\n    list1[0] += {}",
2: "if list1:\n    var = list1.pop(0)\n    list2.append(var)",
3: "if list1:\n    var = list1.pop()\n    list2.append(var)",
4: "list1.insert(0, {})",
5: "list1.sort()",
6: "list1.reverse()",
7: "list1 = list1[1:] if list1 else list1",
8: "if list2:\n    list1.insert(0, list2[0])\nelse:\n    list1.insert(0, {})",
9: "val = list2[-1] if list2 else {}\nlist1.append(val)",
10: "if list1 and list2 and list1[0] > list2[0]:\n    list1.pop(0)",
11: "if list1 and list2 and list1[-1] < list2[-1]:\n    list1.pop()",
12: "if list1:\n    list1.pop(0)",
13: "if list1 and list2:\n    list1.append(list2.pop())",
14: "if list1 and list1[0] % 2 == 0:\n    list1.pop(0)",
15: "if list1 and list1[0] % 2 == 1:\n    list1.pop(0)",
16: "if len(list1) > len(list2):\n    list2.insert(0, list1.pop())",
17: "if list1 and list1[0] > {}:\n    list1.pop(0)",
18: "if list1 and list1[0] < {}:\n    list1.pop(0)",
19: "if list2:\n    list1.append(list2.pop(0))",
20: "if list1:\n    list1.pop()",
21: "if list2:\n    list2.pop()"
\end{minted}
\end{promptbox}

To further enhance rule diversity, we replace \verb|list1| and \verb|list2| with meaningless random strings in each sampled snippet. Here is a prompt of synthetic data sample with one-shot example:

\begin{promptbox}[]

Here is 1 example:
 
Follow the given rule to solve the question. \\
rule:
\begin{minted}{python}
def process_list(ywhm, erep):
    while ywhm and erep:
        if ywhm:
            ywhm.pop()
        erep = erep[1:] if erep else erep
        if erep and erep[0] % 2 == 0:
            erep.pop(0)
        if erep and erep[0] % 2 == 1:
            erep.pop(0)
        if erep:
            erep.pop(0)
        if ywhm:
            ywhm.pop()
        val = erep[-1] if erep else 53
        ywhm.append(val)
        if erep:
            var = erep.pop(0)
            ywhm.append(var)
    return ywhm
\end{minted}

Q: Given two lists, ywhm = [3] and erep = [50, 31], what is the final value of ywhm? \\
\\
1 Initialize \\
ywhm = [3] \\
erep = [50, 31] \\
2 Main loop

\begin{minted}{python}
while ywhm and erep:
\end{minted}

ywhm = [3] \\
erep = [50, 31] \\
enter the loop \\
2.1 One iteration:

\begin{minted}{python}
if ywhm:
    ywhm.pop()
\end{minted}

ywhm = [3] \\
enter if \\
now, \\
ywhm = []

\begin{minted}{python}
erep = erep[1:] if erep else erep
\end{minted}

erep = [50, 31] \\
now, \\
erep = [31]

\begin{minted}{python}
if erep and erep[0] % 2 == 0:
    erep.pop(0)
\end{minted}

erep = [31] \\
erep[0] \% 2 = 31 \% 2 != 0 \\
do not enter if

\begin{minted}{python}
if erep and erep[0] % 2 == 1:
    erep.pop(0)
\end{minted}

erep = [31] \\
erep[0] \% 2 = 31 \% 2 == 1 \\
enter if \\
now, \\
erep = []

\begin{minted}{python}
if erep:
    erep.pop(0)
\end{minted}

erep = [] \\
do not enter if

\begin{minted}{python}
if ywhm:
    ywhm.pop()
\end{minted}

ywhm = [] \\
do not enter if

\begin{minted}{python}
val = erep[-1] if erep else 53
ywhm.append(val)
\end{minted}

erep = [] \\
val = 53 \\
ywhm = [] \\
now, \\
ywhm = [53]

\begin{minted}{python}
if erep:
    var = erep.pop(0)
    ywhm.append(var)
\end{minted}

erep = [] \\
do not enter if

\begin{minted}{python}
while ywhm and erep:
\end{minted}

ywhm = [53] \\
erep = [] \\
do not enter

\begin{minted}{python}
return ywhm
\end{minted}

So the answer is [53] \\
 
Follow the above examples to answer the following question: \\
rule:

\begin{minted}{python}
def process_list(cybez, eonx):
    while cybez and eonx:
        eonx.reverse()
        if eonx:
            cybez.insert(0, eonx[0])
        else:
            cybez.insert(0, 63)
        if cybez and cybez[0] < 9:
            cybez.pop(0)
        if eonx:
            eonx[-1] += 9
        if cybez and cybez[0] % 2 == 1:
            cybez.pop(0)
        if eonx:
            eonx[0] += 96
        if cybez and cybez[0] % 2 == 0:
            cybez.pop(0)
        if cybez:
            cybez.pop()
    return cybez
 \end{minted}

Q: Given two lists, cybez = [31, 22, 95, 74, 70, 55, 39, 22, 77] and eonx = [53, 70, 50, 28, 15, 81, 92, 6], what is the final value of cybez?

\end{promptbox}

\subsection{Natural Language Rules}
\label{app:NL}
To generate rule-following output in natural language, we first convert code-formatted rules into natural language descriptions. The remaining process is the same as generating the output for the code rule. To ensure clarity and avoid ambiguity, each line is prefixed with a number. An example of a rule-following output in natural language is provided below.

Prompt:
\begin{promptbox}[]
Follow the given rule to solve the question.\\
Rule:
\begin{minted}[breaklines=true]{c}
1. Begin outer loop:  
    1.1 Check whether the number is a single-digit number. If it is not, enter the outer loop; otherwise, the loop is over, go to step (2).  
    1.2 One iteration:  
        1.2.1 Initialize the sum of the digits of the number in this iteration as sum equals 0.  
        1.2.2 Begin inner loop to calculate the digit sum:  
            1.2.2.1 Check whether the number is 0. If the number is not 0, enter the inner loop; otherwise, the loop is over, go to step (1.2.3).  
            1.2.2.2 One iteration:  
                1.2.2.2.1 Find the last digit of the number and add it to sum.  
                1.2.2.2.2 Remove the last digit from the number.  
            1.2.2.3 Return to the start of the inner loop.  
        1.2.3 After calculating the sum of all digits, update the number with the value of sum.  
    1.3 Return to the start of the outer loop.  
2. Return the remaining single-digit number.  
\end{minted}
Q: Given an integer number 32, repeatedly add up all its digits until the result has only one digit.
\end{promptbox}
Response:
\begin{promptbox}[]
The number is 32. Begin the process.

\begin{minted}[breaklines=true]{c}
1. Begin outer loop:
1.1 Check whether the number is a single-digit number. If it is not, enter the outer loop; otherwise, the loop is over, go to step (2).
\end{minted}

The number is 32, which is not a single-digit number. Enter the outer loop.

\begin{minted}[breaklines=true]{c}
1.2 One iteration:
1.2.1 Initialize the sum of the digits of the number in this iteration as sum equals 0.
\end{minted}

Initialize sum = 0.

\begin{minted}[breaklines=true]{c}
1.2.2 Begin inner loop to calculate the digit sum:
1.2.2.1 Check whether the number is 0. If the number is not 0, enter the inner loop; otherwise, the loop is over, go to step (1.2.3).
\end{minted}

The number is 32, which is not 0. Enter the inner loop.

\begin{minted}[breaklines=true]{c}
1.2.2.2 One iteration:
1.2.2.2.1 Find the last digit of the number and add it to sum.
\end{minted}

number = 32, the last digit of the number is 2. Add it to sum, sum = 0 + 2 = 2.

\begin{minted}[breaklines=true]{c}
1.2.2.2.2 Remove the last digit from the number.
\end{minted}

Remove the last digit from the number, number = 32 // 10 = 3.

\begin{minted}[breaklines=true]{c}
1.2.2.3 Return to the start of the inner loop.
\end{minted}

Back to the start of the inner loop.

\begin{minted}[breaklines=true]{c}
1.2.2 Begin inner loop to calculate the digit sum:
1.2.2.1 Check whether the number is 0. If the number is not 0, enter the inner loop; otherwise, the loop is over, go to step (1.2.3).
\end{minted}

The number is 3, which is not 0. Enter the inner loop.

\begin{minted}[breaklines=true]{c}
1.2.2.2 One iteration:
1.2.2.2.1 Find the last digit of the number and add it to sum.
\end{minted}

number = 3, the last digit of the number is 3. Add it to sum, sum = 2 + 3 = 5.

\begin{minted}[breaklines=true]{c}
1.2.2.2.2 Remove the last digit from the number.
\end{minted}

Remove the last digit from the number, number = 3 // 10 = 0.

\begin{minted}[breaklines=true]{c}
1.2.2.3 Return to the start of the inner loop.
\end{minted}

Back to the start of the inner loop.

\begin{minted}[breaklines=true]{c}
1.2.2 Begin inner loop to calculate the digit sum:
1.2.2.1 Check whether the number is 0. If the number is not 0, enter the inner loop; otherwise, the loop is over, go to step (1.2.3).
\end{minted}

The number is 0. The loop is over. Go to step (1.2.3).

\begin{minted}[breaklines=true]{c}
1.2.3 After calculating the sum of all digits, update the number with the value of sum.
\end{minted}

sum = 5. Update the number with the value of sum, so number is 5 now.

\begin{minted}[breaklines=true]{c}
1.3 Return to the start of the outer loop.
\end{minted}

Back to the start of the outer loop.

\begin{minted}[breaklines=true]{c}
1. Begin outer loop:
1.1 Check whether the number is a single-digit number. If it is not, enter the outer loop; otherwise, the loop is over, go to step (2).
\end{minted}

The number is 5, which is a single-digit number. The loop is over. Go to step (2).

\begin{minted}[breaklines=true]{c}
2. Return the remaining single-digit number.
\end{minted}

The number is 5. So the answer is 5.

\end{promptbox}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
