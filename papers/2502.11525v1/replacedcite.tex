\section{Related Work}
\paragraph{Length generalization} 
Despite impressive general capabilities, LLMs have not yet achieved robust length generalization____, which refers to the ability to generalize to inputs significantly longer than those encountered during training____. A classic example is that while humans can effortlessly add numbers of any length after mastering basic arithmetic rules, LLMs often struggle to generalize to longer-digit addition beyond their training data. Addressing length generalization remains a highly debated and unresolved challenge in the field.

A series of studies have attempted to tackle this issue by modifying positional encodings (PEs) and data formats____. However, these efforts are constrained by several limitations. First, the proposed PEs and data formats are often tailored specifically to symbolic tasks, such as addition, making them difficult to generalize to broader tasks. Second, the methods are typically tested on small-scale models trained from scratch, raising doubts about their applicability to practical-scale LLMs. Moreover, ____ suggests that modifying PEs or employing task-specific data formats in the fine-tuning stage may be ineffective and even harm length generalization performance.

Another line of work____, including single-task RFFT, tries to address length generalization through training models on more elaborate reasoning processes. However, the work has primarily focused on single-task settings, leaving the transferability of such techniques across tasks largely unexplored. Besides, the work still focuses on simple and symbolic tasks such as addition. To alleviate these issues and investigate length generalization in a more comprehensive scope, we curate a diverse set of more practical length generalization tasks and take length generalization performance as a crucial metric to demonstrate the rule-following ability of models.

\paragraph{Case-based reasoning or rule-based reasoning.} 
As large language models (LLMs) are pre-trained on an exceptionally large corpus, a key question has emerged: whether their excellent performance stems from pattern matching or mere memorization (or referred to as ``case-based reasoning'' in ____), or the genuine capture of general rules underlying natural language. The question has attracted significant research interest. Previous work has provided evidence that LLMs often rely on cases seen during training rather than developing systematic reasoning abilities.____ find that LLMs perform significantly worse in counterfactual reasoning compared to factual reasoning, suggesting reliance on memorized common cases.____ demonstrates that LLMs conduct reasoning through subgraph matching.____ shows LLMs rely on surrounding cases for math reasoning instead of learning generalizable rules. 
On the other hand, research on the phenomenon of ``grokking''____ suggests that models can, under certain conditions, learn interpretable rules of arithmetic reasoning long after overfitting the training set. However, this phenomenon has primarily been observed in small transformers trained from scratch on single tasks, such as modular addition. Whether such rule-based reasoning can scale to LLMs in multi-task settings remains an open question. We propose Meta-RFFT to lead the model to be a meta rule follower, which teaches the model to follow the given rules to reason in diverse tasks.

\paragraph{LLMs with programs}
There have been numerous efforts to integrate programs with LLMs to enhance the capabilities of both. Studies have proposed that LLMs can be utilized to assist in code execution____, as well as to help developers write code and debug more efficiently____. On the other hand, programs themselves can also serve as tools to aid model reasoning.____ demonstrate that code, as a formal language with minimal ambiguity, can effectively assist models in task reduction and formulation, thus enhance reasoning performance.
In this work, we primarily represent the rules with Python programs for precision and clarity. However, it is also important to note that the rules discussed in this paper are not limited to code representations. In Section~\ref{sec:NL}, we also explore Meta-RFFT in the setting of natural language rules. 

\begin{table}[]
\centering
\caption{The statistics of our dataset. We list the number of tasks collected from each data source and their corresponding split in the RF-pretraining stage or the downstream adaptation stage.}
\vspace{0.1in}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
\textbf{Data Source}  & \textbf{RF-Pretrain} & \textbf{Adaptation} & \textbf{Total} \\
\midrule
LeetCode        & 66          & 8          & 74    \\
NUPA            & 0           & 4          & 4     \\
Big-Bench Hard  & 8           & 0          & 8     \\
Symbolic Reasoning & 2           & 0          & 2     \\
\midrule
All Sources     & 76          & 12         & 88   \\\bottomrule
\end{tabular}
}
\label{tab:data_num}
\end{table}