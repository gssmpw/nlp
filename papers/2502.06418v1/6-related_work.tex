\section{Related Work}
\subsection{Image Watermarking Methods}
\emph{Non-learning-based watermarking methods} have developed for decades. Invisible-watermark~\cite{invisible-watermark}, a representative method deployed by Stable Diffusion, encodes watermark into frequency sub-bands. \emph{Learning-based watermarking methods} are gaining dominance due to their superior performance. Zhu et al. \cite{zhu2018hidden} propose the first end-to-end learning architecture for robust watermarking. Following this trend, a series of studies continue to enhance robustness against real-world interferences~\cite{stegastamp, Liu2019TwoStage}. 

% enhancing the robustness of watermarks against image degradation factors and even physical-world disturbances. It is essential to ensure that the watermark can still fulfill its security role during transmission through various forms of channels in the image.

% StegaStamp~\cite{stegastamp} and Liu~et~al.~\cite{Liu2019TwoStage} enrich the types of distortions induced by the middle layer to include operations such as JPEG compression, Gaussian noise, and Blurring, %\hl{concludes the goals of these approaches. What these methods are developed for? To improve robustness level? Or to integrate new capability?}

% applies Discrete Wavelet Transform (DWT) to decompose an image into several frequency sub-bands, encodes watermark as the coefficients change of the Discrete Cosine Transform (DCT) results of some sub-bands, and generates the watermarked image with reverse transform. 

\subsection{Detection Evasion Attacks}
\textbf{Destruction and Reconstruction}. The watermarked image firstly undergoes a certain level of degradation, followed by reconstruction to obtain a purified image. The mainstream approach for this method involves adding noise to the image and then using generative models, such as Diffusion Models (DM)~\cite{ho2020denoising}, for reconstruction and generation~\cite{an2024benchmarking, saberi2023robustness, zhao2023invisible}. In contrast, UnMarker ~\cite{Kassis2024Unmarker} employs a learnable filter to process the watermarked image, supplemented by visual loss functions to ensure and enhance the visual quality of the attack results. 

\textbf{Adversarial Attacks}. Transferring classic adversarial attack methods to the watermarking domain primarily targets the decoder of watermark models. WEVADE~\cite{jiang2023evading} incorporates both black-box and white-box adversarial attack methods. Lukas et al.~\cite{lukas2024leveraging} employ a surrogate model closely resembling the target model to perform transfer attacks. WmRobust~\cite{saberi2023robustness} requires a dataset containing both watermarked and non-watermarked images to train a feature classifier subjected to adversarial attacks. The attacks are transferred to the target watermark detection module. Similarly, WAVES~\cite{an2024benchmarking} relies on a relevant watermark dataset for surrogate attacks but introduces a more detailed classification of watermark data. Hu et al.~\cite{hu2024transfer} explore the feasibility of large-scale ensemble surrogate models for transfer attacks against target watermark models.

\subsection{Watermark Forgery Attacks}
CopyAttack~\cite{kutter2000watermark} was the first to introduce the concept of spoof attacks and proposed a method for predicting watermarks in unknown watermarking algorithm scenarios, embedding them into other images to achieve forgery. WmRobust~\cite{saberi2023robustness} proposes an attack leveraging the encoder of the watermark model to embed noise with a watermark and applies fine-tuning to generate forged watermarks. Steganalysis~\cite{yang2024steganalysis} computes a residual by statistically analyzing a dataset of watermarked images and unpaired clean images. This residual is then used to facilitate watermark forgery.

