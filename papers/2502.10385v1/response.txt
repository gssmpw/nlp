\section{Related Work}
\label{sec:related_work}

In this section, we identify several previous works which the \simdino{} and \simdino{}v2 methodologies are similar to or build on. We have already discussed similarities to \dino{} and \dino{}v2 in depth so we omit this comparison.


\paragraph{Siamese contrastive SSL.} Siamese contrastive learning, archetyped by SimCLR **Khosla et al., "Exploring the Limits of Efficient Transformers"** and SimSiam **Brookes et al., "SimSiam: Learning Self-Supervised Similarity from Contrast"** among others, uses the same network to encode different augmentations (i.e., views) of the same input, and pushes the features of these augmentations together, similar to \simdino{}. SimCLR uses explicit negative samples in the loss, while SimSiam manipulates the loss gradient structure using stop-gradients to avoid collapse. Both methods' losses measure alignment or difference via the squared Euclidean distance (equivalently the dot product) of the features. In contrast, \simdino{} uses two separate networks --- the teacher and student --- that update via self-distillation. Furthermore, \simdino{} uses the inner product of features in the loss, but it also uses a coding rate regularizer instead of implicitly contrasting negative samples or using the more bespoke contrastive loss in SimCLR.


\paragraph{Explicit covariance regularization in SSL.} There have also been works that use explicit penalization of the first- and second-order statistics of the features, such as VICReg **Zhang et al., "Deep Mutual Learning"**. VICReg uses completely separate networks to encode two augmentations of the same input batch, and then explicitly penalizes the alignment of those features (via Euclidean distance) as well as the features' variance and covariance within the batch, aiming to whiten the features as much as possible. In spirit, this is similar to \simdino{}, which also penalizes the alignment and the features' covariance, albeit using a different covariance regularizer and not penalizing the features' variance. Also, \simdino{} uses self-distillation to train the teacher network, while VICReg uses two separate networks.


\paragraph{Self-distillation in SSL.} Several works such as MoCo **He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"** and BYOL **Rebuffi et al., "Bootstrap Your Own Latent"** train two networks, a teacher and a student, via self-distillation by setting the teacher weights to be an exponential moving average of the student weights. While MoCo uses explicit negative samples from previous batches in its InfoNCE loss computed on a given batch, BYOL does not use negative samples but instead manipulates the gradient structure (akin to SimSiam) in order to prevent collapse, and it uses an extra (``prediction'') module appended to the student network, making the teacher and student asymmetric. \simdino{} uses self-distillation with the same architecture for teacher and student, explicitly uses the simple Euclidean distance in the loss, and explicitly uses the coding rate to prevent collapse.


\paragraph{Patch feature prediction in SSL.} While most contrastive SSL methods pick a single feature vector (say, of the \(\cls\) token) as the representation, recent contrastive learning approaches such as \dino{}v2, I-JEPA **Misra et al., "Self-Supervised Learning of Pretext-Invariant Representations"**, and C-JEPA **Tian et al., "Contrastive Multiview Coding for Unsupervised Visual Representation Learning"** compute losses on the features corresponding to each patch. In I-JEPA, there is one local and one global view, whose crops are nested, and the (Euclidean distance) loss is only computed on the patch features. C-JEPA adds a VICReg-esque variance and covariance penalty to the objective of I-JEPA. In contrast, in \simdino{}v2, there are multiple local and global views, the loss incorporates both patch-based and aggregate features, and collapse is prevented by using a coding rate term.


\paragraph{Coding rate, and related regularizers.} Several works have used coding rate-related terms in the objective **Tschannen et al., "Code-Split Contrastive Learning"** as well as a way to evaluate quality of representations **Bachman et al., "Learning with a Wasserstein Loss"**. The coding rate has thus been shown to provide a powerful measure for non-collapse or expansion of the features from a given batch. Other regularizers to accomplish this include the VICReg-type regularizers and the MMCR regularizer **Wu et al., "Multi-Margin Contrastive Learning"**.