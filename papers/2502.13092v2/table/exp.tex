\begin{table*}[htbp]
\centering
\small
\caption{Performance comparison of different LLMs on PDDL generation. \textit{Exec.} represents executability, \textit{Sim.} represents structural similarity, and F1 scores are reported for different PDDL components (predicates, parameters, preconditions, and effects). Results are averaged across all domains.}
\label{tab:main-results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|ccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Size}} & \textbf{Exec.} & \textbf{Sim.} & \textbf{F1\textsubscript{pred}} & \textbf{F1\textsubscript{param}} & \textbf{F1\textsubscript{precond}} & \textbf{F1\textsubscript{eff}} \\
& & (\%) & (\%) & (\%) & (\%) & (\%) & (\%) \\
\midrule
\multicolumn{8}{l}{\textit{Zero-shot Performance}} \\
\midrule
GPT-4 & - & 87.5 & 85.3 & 89.2 & 88.7 & 84.5 & 83.1 \\
Claude-3.5 & - & 82.3 & 81.9 & 85.6 & 84.2 & 80.3 & 79.8 \\
OpenAI-o1 & - & 80.1 & 79.5 & 83.4 & 82.8 & 78.9 & 77.5 \\
\midrule
LLaMA-3.1 & 70B & 75.6 & 74.2 & 78.9 & 77.5 & 73.4 & 72.1 \\
LLaMA-3.1 & 34B & 71.2 & 70.8 & 75.3 & 74.1 & 69.8 & 68.5 \\
LLaMA-3.1 & 13B & 65.4 & 64.9 & 69.7 & 68.3 & 63.5 & 62.1 \\
\midrule
\multicolumn{8}{l}{\textit{With Error Correction (3 attempts)}} \\
\midrule
GPT-4 & - & 94.2 & 88.7 & 91.5 & 90.8 & 87.3 & 86.2 \\
Claude-3.5 & - & 90.1 & 85.4 & 88.9 & 87.6 & 84.2 & 83.5 \\
OpenAI-o1 & - & 88.5 & 83.2 & 86.7 & 85.9 & 82.1 & 81.4 \\
\midrule
LLaMA-3.1 & 70B & 84.3 & 78.9 & 82.4 & 81.5 & 77.8 & 76.5 \\
LLaMA-3.1 & 34B & 80.5 & 75.6 & 79.8 & 78.4 & 74.5 & 73.2 \\
LLaMA-3.1 & 13B & 75.2 & 70.3 & 74.5 & 73.2 & 69.1 & 67.8 \\
\bottomrule
\end{tabular}%
}
\end{table*}