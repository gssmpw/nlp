%%%%%%%%%%%%%%%%%%%% Evaluation of World Modeling of LLMs %%%%%%%%%%%%%%%%%%%%%%%%%

@article{yang2024evaluating,
  title={Evaluating World Models with LLM for Decision Making},
  author={Yang, Chang and Wang, Xinrun and Jiang, Junzhe and Zhang, Qinggang and Huang, Xiao},
  journal={arXiv preprint arXiv:2411.08794},
  year={2024}
}

@article{wang2024can,
  title={Can Language Models Serve as Text-Based World Simulators?},
  author={Wang, Ruoyao and Todd, Graham and Xiao, Ziang and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Clark, Peter and Jansen, Peter},
  journal={arXiv preprint arXiv:2406.06485},
  year={2024}
}

@article{dainese2024generating,
  title={Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search},
  author={Dainese, Nicola and Merler, Matteo and Alakuijala, Minttu and Marttinen, Pekka},
  journal={arXiv preprint arXiv:2405.15383},
  year={2024}
}

@article{tang2024worldcoder,
  title={Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment},
  author={Tang, Hao and Key, Darren and Ellis, Kevin},
  journal={arXiv preprint arXiv:2402.12275},
  year={2024}
}

@article{vafa2024evaluating,
  title={Evaluating the World Model Implicit in a Generative Model},
  author={Vafa, Keyon and Chen, Justin Y and Kleinberg, Jon and Mullainathan, Sendhil and Rambachan, Ashesh},
  journal={arXiv preprint arXiv:2406.03689},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%% Evaluation of World Modeling of LLMs [End] %%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%% World Models %%%%%%%%%%%%%%%%%%%%%%%%%

@article{ha2018world,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

@article{ha2018recurrent,
  title={Recurrent world models facilitate policy evolution},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{wu2023daydreamer,
  title={Daydreamer: World models for physical robot learning},
  author={Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Abbeel, Pieter and Goldberg, Ken},
  booktitle={Conference on robot learning},
  pages={2226--2240},
  year={2023},
  organization={PMLR}
}

@article{guan2024world,
  title={World models for autonomous driving: An initial survey},
  author={Guan, Yanchen and Liao, Haicheng and Li, Zhenning and Hu, Jia and Yuan, Runze and Li, Yunjian and Zhang, Guohui and Xu, Chengzhong},
  journal={IEEE Transactions on Intelligent Vehicles},
  year={2024},
  publisher={IEEE}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

%%%%%%%%%%%%%%%%%%%% World Models [END] %%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%% PDDL Generation with LLMs %%%%%%%%%%%%%%%%%%%%%%%%%

@article{hu2024agentgen,
  title={AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation},
  author={Hu, Mengkang and Zhao, Pu and Xu, Can and Sun, Qingfeng and Lou, Jianguang and Lin, Qingwei and Luo, Ping and Rajmohan, Saravan and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2408.00764},
  year={2024}
}

@article{guan2023leveraging,
  title={Leveraging pre-trained large language models to construct and utilize world models for model-based task planning},
  author={Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={79081--79094},
  year={2023}
}

@inproceedings{oswald2024large,
  title={Large Language Models as Planning Domain Generators},
  author={Oswald, James and Srinivas, Kavitha and Kokel, Harsha and Lee, Junkyu and Katz, Michael and Sohrabi, Shirin},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={34},
  pages={423--431},
  year={2024}
}

@inproceedings{silver2024generalized,
  title={Generalized planning in pddl domains with pretrained large language models},
  author={Silver, Tom and Dan, Soham and Srinivas, Kavitha and Tenenbaum, Joshua B and Kaelbling, Leslie and Katz, Michael},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={18},
  pages={20256--20264},
  year={2024}
}


@article{smirnov2024generating,
  title={Generating consistent PDDL domains with Large Language Models},
  author={Smirnov, Pavel and Joublin, Frank and Ceravola, Antonello and Gienger, Michael},
  journal={arXiv preprint arXiv:2404.07751},
  year={2024}
}

@article{zhu2024language,
  title={Language Models can Infer Action Semantics for Classical Planners from Environment Feedback},
  author={Zhu, Wang and Singh, Ishika and Jia, Robin and Thomason, Jesse},
  journal={arXiv preprint arXiv:2406.02791},
  year={2024}
}

@article{wang2023bytesized32,
  title={ByteSized32: A corpus and challenge task for generating task-specific world models expressed as text games},
  author={Wang, Ruoyao and Todd, Graham and Yuan, Eric and Xiao, Ziang and C{\^o}t{\'e}, Marc-Alexandre and Jansen, Peter},
  journal={arXiv preprint arXiv:2305.14879},
  year={2023}
}

@article{wong2023word,
  title={From word models to world models: Translating from natural language to the probabilistic language of thought},
  author={Wong, Lionel and Grand, Gabriel and Lew, Alexander K and Goodman, Noah D and Mansinghka, Vikash K and Andreas, Jacob and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2306.12672},
  year={2023}
}

%%%%%%%%%%%%%%%%%%%% PDDL Generation with LLMs [END] %%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%% LLM as world model %%%%%%%%%%%%%%%%%%%%%%%%%

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@article{xiang2024language,
  title={Language models meet world models: Embodied experiences enhance language models},
  author={Xiang, Jiannan and Tao, Tianhua and Gu, Yi and Shu, Tianmin and Wang, Zirui and Yang, Zichao and Hu, Zhiting},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{xie2024making,
  title={Making large language models into world models with precondition and effect knowledge},
  author={Xie, Kaige and Yang, Ian and Gunerli, John and Riedl, Mark},
  journal={arXiv preprint arXiv:2409.12278},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%% LLM as world model [End] %%%%%%%%%%%%%%%%%%%%%%%%%

@article{wang2023promptagent,
  title={Promptagent: Strategic planning with language models enables expert-level prompt optimization},
  author={Wang, Xinyuan and Li, Chenxi and Wang, Zhen and Bai, Fan and Luo, Haotian and Zhang, Jiayou and Jojic, Nebojsa and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2310.16427},
  year={2023}
}

@article{banerjee2020can,
  title={Can Transformers Reason About Effects of Actions?},
  author={Banerjee, Pratyay and Baral, Chitta and Luo, Man and Mitra, Arindam and Pal, Kuntal and Son, Tran C and Varshney, Neeraj},
  journal={arXiv preprint arXiv:2012.09938},
  year={2020}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{luo2023towards,
  title={Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models},
  author={Luo, Man and Kumbhar, Shrinidhi and Parmar, Mihir and Varshney, Neeraj and Banerjee, Pratyay and Aditya, Somak and Baral, Chitta and others},
  journal={arXiv preprint arXiv:2310.00836},
  year={2023}
}

@article{elazar2018adversarial,
  title={Adversarial removal of demographic attributes from text data},
  author={Elazar, Yanai and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1808.06640},
  year={2018}
}
@misc{si2024design2code,
      title={Design2Code: How Far Are We From Automating Front-End Engineering?},
      author={Chenglei Si and Yanzhe Zhang and Zhengyuan Yang and Ruibo Liu and Diyi Yang},
      year={2024},
      eprint={2403.03163},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
  }
@article{chowdhury2021adversarial,
  title={Adversarial scrubbing of demographic information for text classification},
  author={Chowdhury, Somnath Basu Roy and Ghosh, Sayan and Li, Yiyuan and Oliva, Junier B and Srivastava, Shashank and Chaturvedi, Snigdha},
  journal={arXiv preprint arXiv:2109.08613},
  year={2021}
}
@article{claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  journal={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf},
  year={2024}
}
@article{han2021diverse,
  title={Diverse adversaries for mitigating bias in training},
  author={Han, Xudong and Baldwin, Timothy and Cohn, Trevor},
  journal={arXiv preprint arXiv:2101.10001},
  year={2021}
}
@article{coavoux2018privacy,
  title={Privacy-preserving neural representations of text},
  author={Coavoux, Maximin and Narayan, Shashi and Cohen, Shay B},
  journal={arXiv preprint arXiv:1808.09408},
  year={2018}
}
@article{li2018towards,
  title={Towards robust and privacy-preserving text representations},
  author={Li, Yitong and Baldwin, Timothy and Cohn, Trevor},
  journal={arXiv preprint arXiv:1805.06093},
  year={2018}
}
@inproceedings{ganin2015unsupervised,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={International conference on machine learning},
  pages={1180--1189},
  year={2015},
  organization={PMLR}
}
@InProceedings{pmlr-v139-yu21f,
  title = 	 {Large Scale Private Learning via Low-rank Reparametrization},
  author =       {Yu, Da and Zhang, Huishuai and Chen, Wei and Yin, Jian and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12208--12218},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yu21f/yu21f.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yu21f.html},
  abstract = 	 {We propose a reparametrization scheme to address the challenges of applying differentially private SGD on large neural networks, which are 1) the huge memory cost of storing individual gradients, 2) the added noise suffering notorious dimensional dependence. Specifically, we reparametrize each weight matrix with two \emph{gradient-carrier} matrices of small dimension and a \emph{residual weight} matrix. We argue that such reparametrization keeps the forward/backward process unchanged while enabling us to compute the projected gradient without computing the gradient itself. To learn with differential privacy, we design \emph{reparametrized gradient perturbation (RGP)} that perturbs the gradients on gradient-carrier matrices and reconstructs an update for the original weight from the noisy gradients. Importantly, we use historical updates to find the gradient-carrier matrices, whose optimality is rigorously justified under linear regression and empirically verified with deep learning tasks. RGP significantly reduces the memory cost and improves the utility. For example, we are the first able to apply differential privacy on the BERT model and achieve an average accuracy of $83.9%$ on four downstream tasks with $\epsilon=8$, which is within $5%$ loss compared to the non-private baseline but enjoys much lower privacy leakage risk.}
}


@inproceedings{Anil2021LargeScaleDP,
  title={Large-Scale Differentially Private BERT},
  author={Rohan Anil and Badih Ghazi and Vineet Gupta and Ravi Kumar and Pasin Manurangsi},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

@article{maheshwari2022fair,
  title={Fair NLP Models with Differentially Private Text Encoders},
  author={Maheshwari, Gaurav and Denis, Pascal and Keller, Mikaela and Bellet, Aur{\'e}lien},
  journal={arXiv preprint arXiv:2205.06135},
  year={2022}
}

@article{Ginart2022SubmixPP,
  title={Submix: Practical Private Prediction for Large-Scale Language Models},
  author={Antonio A. Ginart and Laurens van der Maaten and James Y. Zou and Chuan Guo},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.00971}
}

@article{Weggenmann2022DPVAEHT,
  title={DP-VAE: Human-Readable Text Anonymization for Online Reviews with Differentially Private Variational Autoencoders},
  author={Benjamin Weggenmann and Valentin Rublack and Michael Andrejczuk and Justus Mattern and Florian Kerschbaum},
  journal={Proceedings of the ACM Web Conference 2022},
  year={2022}
}


@InProceedings{pmlr-v75-dwork18a,
  title = 	 {Privacy-preserving Prediction},
  author =       {Dwork, Cynthia and Feldman, Vitaly},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {1693--1702},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/dwork18a/dwork18a.pdf},
  url = 	 {https://proceedings.mlr.press/v75/dwork18a.html},
  abstract = 	 {Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression. We first describe a simple baseline approach based on training several models on disjoint subsets of data and using standard private aggregation techniques to predict. We show that this approach has nearly optimal sample complexity for (realizable) PAC learning of any class of Boolean functions. At the same time, without strong assumptions on the data distribution, the aggregation step introduces a substantial overhead. We demonstrate that this overhead can be avoided for the well-studied class of thresholds on a line and for a number of standard settings of convex regression. The analysis of our algorithm for learning thresholds relies crucially on strong generalization guarantees that we establish for all differentially private prediction algorithms.}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{shi2021selective,
  title={Selective differential privacy for language modeling},
  author={Shi, Weiyan and Cui, Aiqi and Li, Evan and Jia, Ruoxi and Yu, Zhou},
  journal={arXiv preprint arXiv:2108.12944},
  year={2021}
}

@inproceedings{Dwork2006CalibratingNT,
  title={Calibrating Noise to Sensitivity in Private Data Analysis},
  author={Cynthia Dwork and Frank McSherry and Kobbi Nissim and Adam D. Smith},
  booktitle={Theory of Cryptography Conference},
  year={2006}
}

@inproceedings{garcia2020sensitive,
    title = "Sensitive Data Detection and Classification in {S}panish Clinical Text: Experiments with {BERT}",
    author = "Garc{\'\i}a Pablos, Aitor  and
      Perez, Naiara  and
      Cuadros, Montse",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.552",
    pages = "4486--4494",
    abstract = "Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT with other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{tovino2016hipaa,
  title={The HIPAA privacy rule and the EU GDPR: illustrative comparisons},
  author={Tovino, Stacey A},
  journal={Seton Hall L. Rev.},
  volume={47},
  pages={973},
  year={2016},
  publisher={HeinOnline}
}

@inproceedings{babakov-etal-2021-detecting,
    title = "Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation",
    author = "Babakov, Nikolay  and
      Logacheva, Varvara  and
      Kozlova, Olga  and
      Semenov, Nikita  and
      Panchenko, Alexander",
    booktitle = "Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing",
    month = apr,
    year = "2021",
    address = "Kiyv, Ukraine",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.bsnlp-1.4",
    pages = "26--36",
    abstract = "Not all topics are equally {``}flammable{''} in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness. While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness. The core of inappropriateness is that it can harm the reputation of a speaker. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this data.",
}

@article{feyisetan2019privacypreserving,
  title={Privacy- and Utility-Preserving Textual Analysis via Calibrated Multivariate Perturbations},
  author={Oluwaseyi Feyisetan and Borja Balle and Thomas Drake and Tom Diethe},
  journal={Proceedings of the 13th International Conference on Web Search and Data Mining},
  year={2019}
}

@article{carvalho2021privacypreserving,
  title={TEM: High Utility Metric Differential Privacy on Text},
  author={Ricardo Silva Carvalho and Theodore Vasiloudis and Oluwaseyi Feyisetan},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.07928}
}

@article{Xu2021DensityAwareDP,
  title={Density-Aware Differentially Private Textual Perturbations Using Truncated Gumbel Noise},
  author={Nan Xu and Oluwaseyi Feyisetan and Abhinav Aggarwal and Zekun Xu and Nathanael Teissier},
  journal={The International FLAIRS Conference Proceedings},
  year={2021}
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@article{alsentzer2019publicly,
  title={Publicly Available Clinical BERT Embeddings},
  author={Emily Alsentzer and John R. Murphy and Willie Boag and Wei-Hung Weng and Di Jin and Tristan Naumann and Matthew B. A. McDermott},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.03323}
}

@article{Feyisetan2021PrivateRO,
  title={Private Release of Text Embedding Vectors},
  author={Oluwaseyi Feyisetan and Shiva Prasad Kasiviswanathan},
  journal={Proceedings of the First Workshop on Trustworthy Natural Language Processing},
  year={2021}
}

@inproceedings{Meehan2022SentencelevelPF,
  title={Sentence-level Privacy for Document Embeddings},
  author={Casey Meehan and Khalil Mrini and Kamalika Chaudhuri},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022}
}

@inproceedings{Subramani2020EnablingFD,
  title={Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization},
  author={Pranav Subramani and Nicholas Vadivelu and Gautam Kamath},
  booktitle={Neural Information Processing Systems},
  year={2020}
}

@article{Goodfellow2015EfficientPG,
  title={Efficient Per-Example Gradient Computations},
  author={Ian J. Goodfellow},
  journal={ArXiv},
  year={2015},
  volume={abs/1510.01799}
}

@article{Tetko1995NeuralNS,
  title={Neural network studies, 1. Comparison of overfitting and overtraining},
  author={Igor V. Tetko and David J. Livingstone and Alexander I. Luik},
  journal={J. Chem. Inf. Comput. Sci.},
  year={1995},
  volume={35},
  pages={826-833}
}

@article{zhou2023webarena,
  title={Webarena: A realistic web environment for building autonomous agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},
  journal={arXiv preprint arXiv:2307.13854},
  year={2023}
}

@article{zhang2023mobile,
  title={Mobile-Env: A Universal Platform for Training and Evaluation of Mobile Interaction},
  author={Zhang, Danyang and Chen, Lu and Yu, Kai},
  journal={arXiv preprint arXiv:2305.08144},
  year={2023}
}

@article{li2023sheetcopilot,
  title={SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models},
  author={Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and Zhang, Zhaoxiang},
  journal={arXiv preprint arXiv:2305.19308},
  year={2023}
}

@article{deng2023mind2web,
  title={Mind2Web: Towards a Generalist Agent for the Web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={arXiv preprint arXiv:2306.06070},
  year={2023}
}

@article{rawles2023android,
  title={Android in the wild: A large-scale dataset for android device control},
  author={Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2307.10088},
  year={2023}
}

@inproceedings{shi2017world,
  title={World of bits: An open-domain platform for web-based agents},
  author={Shi, Tianlin and Karpathy, Andrej and Fan, Linxi and Hernandez, Jonathan and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={3135--3144},
  year={2017},
  organization={PMLR}
}

@article{liu2018reinforcement,
  title={Reinforcement learning on web interfaces using workflow-guided exploration},
  author={Liu, Evan Zheran and Guu, Kelvin and Pasupat, Panupong and Shi, Tianlin and Liang, Percy},
  journal={arXiv preprint arXiv:1802.08802},
  year={2018}
}

@article{yao2022webshop,
  title={Webshop: Towards scalable real-world web interaction with grounded language agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20744--20757},
  year={2022}
}

@article{toyama2021androidenv,
  title={Androidenv: A reinforcement learning platform for android},
  author={Toyama, Daniel and Hamel, Philippe and Gergely, Anita and Comanici, Gheorghe and Glaese, Amelia and Ahmed, Zafarali and Jackson, Tyler and Mourad, Shibl and Precup, Doina},
  journal={arXiv preprint arXiv:2105.13231},
  year={2021}
}

@article{wen2023empowering,
  title={Empowering llm to use smartphone for intelligent task automation},
  author={Wen, Hao and Li, Yuanchun and Liu, Guohong and Zhao, Shanhui and Yu, Tao and Li, Toby Jia-Jun and Jiang, Shiqi and Liu, Yunhao and Zhang, Yaqin and Liu, Yunxin},
  journal={arXiv preprint arXiv:2308.15272},
  year={2023}
}

@article{zhang2023appagent,
  title={AppAgent: Multimodal Agents as Smartphone Users},
  author={Zhang, Chi and Yang, Zhao and Liu, Jiaxuan and Han, Yucheng and Chen, Xin and Huang, Zebiao and Fu, Bin and Yu, Gang},
  journal={arXiv e-prints},
  pages={arXiv--2312},
  year={2023}
}
@article{li2020mapping,
  title={Mapping natural language instructions to mobile UI action sequences},
  author={Li, Yang and He, Jiacong and Zhou, Xin and Zhang, Yuan and Baldridge, Jason},
  journal={arXiv preprint arXiv:2005.03776},
  year={2020}
}

@article{sun2022meta,
  title={META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI},
  author={Sun, Liangtai and Chen, Xingyu and Chen, Lu and Dai, Tianle and Zhu, Zichen and Yu, Kai},
  journal={arXiv preprint arXiv:2205.11029},
  year={2022}
}

@article{venkatesh2022ugif,
  title={UGIF: UI Grounded Instruction Following},
  author={Venkatesh, Sagar Gubbi and Talukdar, Partha and Narayanan, Srini},
  journal={arXiv preprint arXiv:2211.07615},
  year={2022}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={39--48},
  year={2016}
}


@inproceedings{gupta2023visual,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}

@article{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  journal={arXiv preprint arXiv:2303.08128},
  year={2023}
}

@article{wu2023visual,
  title={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}

@article{furuta2023multimodal,
  title={Multimodal Web Navigation with Instruction-Finetuned Foundation Models},
  author={Furuta, Hiroki and Nachum, Ofir and Lee, Kuang-Huei and Matsuo, Yutaka and Gu, Shixiang Shane and Gur, Izzeddin},
  journal={arXiv preprint arXiv:2305.11854},
  year={2023}
}

@inproceedings{lee2023pix2struct,
  title={Pix2struct: Screenshot parsing as pretraining for visual language understanding},
  author={Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  booktitle={International Conference on Machine Learning},
  pages={18893--18912},
  year={2023},
  organization={PMLR}
}

@article{shaw2023pixels,
  title={From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces},
  author={Shaw, Peter and Joshi, Mandar and Cohan, James and Berant, Jonathan and Pasupat, Panupong and Hu, Hexiang and Khandelwal, Urvashi and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:2306.00245},
  year={2023}
}

@article{gur2023real,
  title={A real-world webagent with planning, long context understanding, and program synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2307.12856},
  year={2023}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@inproceedings{wang2023enabling,
  title={Enabling conversational interaction with mobile ui using large language models},
  author={Wang, Bryan and Li, Gang and Li, Yang},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2023}
}

@inproceedings{humphreys2022data,
  title={A data-driven approach for learning to control computers},
  author={Humphreys, Peter C and Raposo, David and Pohlen, Tobias and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Santoro, Adam and Lillicrap, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={9466--9482},
  year={2022},
  organization={PMLR}
}

@article{sridhar2023hierarchical,
  title={Hierarchical Prompting Assists Large Language Model on Web Navigation},
  author={Sridhar, Abishek and Lo, Robert and Xu, Frank F and Zhu, Hao and Zhou, Shuyan},
  journal={arXiv preprint arXiv:2305.14257},
  year={2023}
}

@inproceedings{MirnaNachouki2019ISSPIT_RPAAcademicAdvising,
  author       = {Mirna Nachouki and
                  Mahmoud Abou Naaj},
  title        = {Process Automation Tool for Academic Advising},
  booktitle    = {{IEEE} International Symposium on Signal Processing and Information
                  Technology, {ISSPIT} 2019, Ajman, United Arab Emirates, December 10-12,
                  2019},
  pages        = {1--6},
  publisher    = {{IEEE}},
  year         = {2019},
  url          = {https://doi.org/10.1109/ISSPIT47144.2019.9001864},
  doi          = {10.1109/ISSPIT47144.2019.9001864},
  timestamp    = {Tue, 25 Feb 2020 17:16:38 +0100},
  biburl       = {https://dblp.org/rec/conf/isspit/NachoukiN19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{JulianKoch2020ECIS_MirrorOnTheWall,
  author       = {Julian Koch and
                  Michael Trampler and
                  Ingo Kregel and
                  Andr{\'{e}} Coners},
  editor       = {Frantz Rowe and
                  Redouane El Amrani and
                  Moez Limayem and
                  Sue Newell and
                  Nancy Pouloudi and
                  Eric van Heck and
                  Ali El Quammah},
  title        = {'mirror, mirror, on the Wall': robotic Process Automation in the Public
                  Sector using a Digital twin},
  booktitle    = {28th European Conference on Information Systems - Liberty, Equality,
                  and Fraternity in a Digitizing World, {ECIS} 2020, Marrakech, Morocco,
                  June 15-17, 2020},
  year         = {2020},
  url          = {https://aisel.aisnet.org/ecis2020\_rip/2},
  timestamp    = {Wed, 10 Jun 2020 12:03:24 +0200},
  biburl       = {https://dblp.org/rec/conf/ecis/KochTKC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ZelleM96,
  author    = {John M. Zelle and
               Raymond J. Mooney},
  title     = {Learning to Parse Database Queries Using Inductive Logic Programming},
  booktitle = {{AAAI} 1996},
  pages     = {1050--1055},
  year      = {1996},
  timestamp = {Wed, 10 Feb 2021 08:44:51 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zettlemoyer05,
  author    = {Luke S. Zettlemoyer and Michael Collins},
  title     = {Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars},
  journal   = {UAI},
  year      = {2005},
}

@inproceedings{berant-etal-2013-semantic,
    title = "Semantic Parsing on {F}reebase from Question-Answer Pairs",
    author = "Berant, Jonathan  and
      Chou, Andrew  and
      Frostig, Roy  and
      Liang, Percy",
    booktitle = "Proc.\ of EMNLP",
    year = "2013",
    url = "https://aclanthology.org/D13-1160",
}

@inproceedings{yin-neubig-2017-syntactic,
    title = "A Syntactic Neural Model for General-Purpose Code Generation",
    author = "Yin, Pengcheng  and
      Neubig, Graham",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1041",
    doi = "10.18653/v1/P17-1041",
    pages = "440--450",
    abstract = "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",
}

@inproceedings{TobyJiaJunLI2018Mobisys_Kite,
  author       = {Toby Jia{-}Jun Li and
                  Oriana Riva},
  editor       = {J{\"{o}}rg Ott and
                  Falko Dressler and
                  Stefan Saroiu and
                  Prabal Dutta},
  title        = {Kite: Building Conversational Bots from Mobile Apps},
  booktitle    = {Proceedings of the 16th Annual International Conference on Mobile
                  Systems, Applications, and Services, MobiSys 2018, Munich, Germany,
                  June 10-15, 2018},
  pages        = {96--109},
  publisher    = {{ACM}},
  year         = {2018},
  url          = {https://doi.org/10.1145/3210240.3210339},
  doi          = {10.1145/3210240.3210339},
  timestamp    = {Thu, 14 Oct 2021 09:56:16 +0200},
  biburl       = {https://dblp.org/rec/conf/mobisys/LiR18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{TobyJiaJunLi2019_PUMICE,
  author       = {Toby Jia{-}Jun Li and
                  Marissa Radensky and
                  Justin Jia and
                  Kirielle Singarajah and
                  Tom M. Mitchell and
                  Brad A. Myers},
  editor       = {Fran{\c{c}}ois Guimbreti{\`{e}}re and
                  Michael S. Bernstein and
                  Katharina Reinecke},
  title        = {{PUMICE:} {A} Multi-Modal Agent that Learns Concepts and Conditionals
                  from Natural Language and Demonstrations},
  booktitle    = {Proceedings of the 32nd Annual {ACM} Symposium on User Interface Software
                  and Technology, {UIST} 2019, New Orleans, LA, USA, October 20-23,
                  2019},
  pages        = {577--589},
  publisher    = {{ACM}},
  year         = {2019},
  url          = {https://doi.org/10.1145/3332165.3347899},
  doi          = {10.1145/3332165.3347899},
  timestamp    = {Fri, 04 Feb 2022 08:27:42 +0100},
  biburl       = {https://dblp.org/rec/conf/uist/LiRJSMM19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{TobyJiaJunLi2020ACLDemo_SUGILITE,
  author       = {Toby Jia{-}Jun Li and
                  Tom M. Mitchell and
                  Brad A. Myers},
  editor       = {Asli Celikyilmaz and
                  Tsung{-}Hsien Wen},
  title        = {Interactive Task Learning from GUI-Grounded Natural Language Instructions
                  and Demonstrations},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
                  Linguistics: System Demonstrations, {ACL} 2020, Online, July 5-10,
                  2020},
  pages        = {215--223},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.acl-demos.25},
  doi          = {10.18653/V1/2020.ACL-DEMOS.25},
  timestamp    = {Fri, 06 Aug 2021 00:41:00 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/LiMM20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yu2018spider,
  title={Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task},
  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={3911--3921},
  year={2018}
}

@article{zhong2018seq2sql,
  title={Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning},
  author={Zhong, Victor and Xiong, Caiming and Socher, Richard},
  year={2018}
}

@inproceedings{scholak2021picard,
    title = "{PICARD}: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
    author = "Scholak, Torsten  and
      Schucher, Nathan  and
      Bahdanau, Dzmitry",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.779",
    doi = "10.18653/v1/2021.emnlp-main.779",
    pages = "9895--9901",
    abstract = "Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.",
}

@inproceedings{shaw-etal-2021-compositional,
    title = "Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?",
    author = "Shaw, Peter  and
      Chang, Ming-Wei  and
      Pasupat, Panupong  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
}

@article{zhang2023you,
  title={You Only Look at Screens: Multimodal Chain-of-Action Agents},
  author={Zhang, Zhuosheng and Zhang, Aston},
  journal={arXiv e-prints},
  pages={arXiv--2309},
  year={2023}
}

@article{hong2023cogagent,
  title={CogAgent: A Visual Language Model for GUI Agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv preprint arXiv:2312.08914},
  year={2023}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{DeborahFerreira2020_RealWoB,
  author       = {Deborah Ferreira and
                  Julia Rozanova and
                  Krishna Dubba and
                  Dell Zhang and
                  Andr{\'{e}} Freitas},
  title        = {On the Evaluation of Intelligence Process Automation},
  journal      = {CoRR},
  volume       = {abs/2001.02639},
  year         = {2020},
  url          = {http://arxiv.org/abs/2001.02639},
  eprinttype    = {arXiv},
  eprint       = {2001.02639},
  timestamp    = {Thu, 28 May 2020 10:01:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2001-02639.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{BryanWang2023ACMCHI_LLMMobileInteraction,
  author       = {Bryan Wang and
                  Gang Li and
                  Yang Li},
  editor       = {Albrecht Schmidt and
                  Kaisa V{\"{a}}{\"{a}}n{\"{a}}nen and
                  Tesh Goyal and
                  Per Ola Kristensson and
                  Anicia Peters and
                  Stefanie Mueller and
                  Julie R. Williamson and
                  Max L. Wilson},
  title        = {Enabling Conversational Interaction with Mobile {UI} using Large Language
                  Models},
  booktitle    = {Proceedings of the 2023 {CHI} Conference on Human Factors in Computing
                  Systems, {CHI} 2023, Hamburg, Germany, April 23-28, 2023},
  pages        = {432:1--432:17},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3544548.3580895},
  doi          = {10.1145/3544548.3580895},
  timestamp    = {Thu, 01 Jun 2023 13:14:33 +0200},
  biburl       = {https://dblp.org/rec/conf/chi/WangLL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ShunyuYao2023ICLR_ReAct,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=WE\_vluYUL-X},
  timestamp    = {Wed, 16 Aug 2023 16:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/YaoZYDSN023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{IzzeddinGur2023_WebAgent,
  author       = {Izzeddin Gur and
                  Hiroki Furuta and
                  Austin Huang and
                  Mustafa Safdari and
                  Yutaka Matsuo and
                  Douglas Eck and
                  Aleksandra Faust},
  title        = {A Real-World WebAgent with Planning, Long Context Understanding, and
                  Program Synthesis},
  journal      = {CoRR},
  volume       = {abs/2307.12856},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.12856},
  doi          = {10.48550/ARXIV.2307.12856},
  eprinttype    = {arXiv},
  eprint       = {2307.12856},
  timestamp    = {Tue, 01 Aug 2023 14:49:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-12856.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{AmanMadaan2022EMNLP_MemPrompt,
  author       = {Aman Madaan and
                  Niket Tandon and
                  Peter Clark and
                  Yiming Yang},
  editor       = {Yoav Goldberg and
                  Zornitsa Kozareva and
                  Yue Zhang},
  title        = {Memory-assisted prompt editing to improve {GPT-3} after deployment},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
                  December 7-11, 2022},
  pages        = {2833--2861},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.emnlp-main.183},
  doi          = {10.18653/V1/2022.EMNLP-MAIN.183},
  timestamp    = {Thu, 10 Aug 2023 12:35:25 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/MadaanTCY22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DanyangZhang2023_Rememberer,
  author       = {Danyang Zhang and
                  Lu Chen and
                  Situo Zhang and
                  Hongshen Xu and
                  Zihan Zhao and
                  Kai Yu},
  title        = {Large Language Model Is Semi-Parametric Reinforcement Learning Agent},
  journal      = {CoRR},
  volume       = {abs/2306.07929},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.07929},
  doi          = {10.48550/ARXIV.2306.07929},
  eprinttype    = {arXiv},
  eprint       = {2306.07929},
  timestamp    = {Sat, 17 Jun 2023 18:52:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-07929.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{AndrewZhao2023_ExpeL,
  author       = {Andrew Zhao and
                  Daniel Huang and
                  Quentin Xu and
                  Matthieu Lin and
                  Yong{-}Jin Liu and
                  Gao Huang},
  title        = {ExpeL: {LLM} Agents Are Experiential Learners},
  journal      = {CoRR},
  volume       = {abs/2308.10144},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.10144},
  doi          = {10.48550/ARXIV.2308.10144},
  eprinttype    = {arXiv},
  eprint       = {2308.10144},
  timestamp    = {Wed, 30 Aug 2023 15:34:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-10144.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ReiichiroNakano2021_WebGPT,
  author       = {Reiichiro Nakano and
                  Jacob Hilton and
                  Suchir Balaji and
                  Jeff Wu and
                  Long Ouyang and
                  Christina Kim and
                  Christopher Hesse and
                  Shantanu Jain and
                  Vineet Kosaraju and
                  William Saunders and
                  Xu Jiang and
                  Karl Cobbe and
                  Tyna Eloundou and
                  Gretchen Krueger and
                  Kevin Button and
                  Matthew Knight and
                  Benjamin Chess and
                  John Schulman},
  title        = {WebGPT: Browser-assisted question-answering with human feedback},
  journal      = {CoRR},
  volume       = {abs/2112.09332},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.09332},
  eprinttype    = {arXiv},
  eprint       = {2112.09332},
  timestamp    = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-09332.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{YongliangShen2023_HuggingGPT,
  author       = {Yongliang Shen and
                  Kaitao Song and
                  Xu Tan and
                  Dongsheng Li and
                  Weiming Lu and
                  Yueting Zhuang},
  title        = {HuggingGPT: Solving {AI} Tasks with ChatGPT and its Friends in HuggingFace},
  journal      = {CoRR},
  volume       = {abs/2303.17580},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.17580},
  doi          = {10.48550/ARXIV.2303.17580},
  eprinttype    = {arXiv},
  eprint       = {2303.17580},
  timestamp    = {Thu, 04 May 2023 17:10:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-17580.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{TimoSchick2023_Toolformer,
  author       = {Timo Schick and
                  Jane Dwivedi{-}Yu and
                  Roberto Dess{\`{\i}} and
                  Roberta Raileanu and
                  Maria Lomeli and
                  Luke Zettlemoyer and
                  Nicola Cancedda and
                  Thomas Scialom},
  title        = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  journal      = {CoRR},
  volume       = {abs/2302.04761},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.04761},
  doi          = {10.48550/ARXIV.2302.04761},
  eprinttype    = {arXiv},
  eprint       = {2302.04761},
  timestamp    = {Mon, 13 Feb 2023 14:23:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-04761.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{TianbaoXie2023_OpenAgents,
  author       = {Tianbao Xie and
                  Fan Zhou and
                  Zhoujun Cheng and
                  Peng Shi and
                  Luoxuan Weng and
                  Yitao Liu and
                  Toh Jing Hua and
                  Junning Zhao and
                  Qian Liu and
                  Che Liu and
                  Leo Z. Liu and
                  Yiheng Xu and
                  Hongjin Su and
                  Dongchan Shin and
                  Caiming Xiong and
                  Tao Yu},
  title        = {OpenAgents: An Open Platform for Language Agents in the Wild},
  journal      = {CoRR},
  volume       = {abs/2310.10634},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.10634},
  doi          = {10.48550/ARXIV.2310.10634},
  eprinttype    = {arXiv},
  eprint       = {2310.10634},
  timestamp    = {Wed, 06 Dec 2023 16:22:07 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-10634.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{li2023silkie,
  title={Silkie: Preference Distillation for Large Visual Language Models},
  author={Li, Lei and Xie, Zhihui and Li, Mukai and Chen, Shunian and Wang, Peiyi and Chen, Liang and Yang, Yazheng and Wang, Benyou and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2312.10665},
  year={2023}
}

@article{mackay2019state,
  title={The State of Work Life Balance in 2019: What we learned from studying 185 million hours of working time},
  author={MacKay, J},
  journal={RescueTime: blog},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2023-09-28},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ranpox/Zotero/storage/AUQI42X4/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf;/Users/ranpox/Zotero/storage/WGJ69RTM/2307.html}
}

@incollection{engelbart2023augmenting,
  title={Augmenting human intellect: A conceptual framework},
  author={Engelbart, Douglas C},
  booktitle={Augmented Education in the Global Age},
  pages={13--29},
  year={2023},
  publisher={Routledge}
}

@misc{adept2022act1,
    title = {{ACT-1: Transformer for Actions}},
    howpublished = {\url{https://www.adept.ai/act}},
    year = {2022},
    author = {{Adept}}
}

@ARTICLE{licklider1960man,
  author={Licklider, J. C. R.},
  journal={IRE Transactions on Human Factors in Electronics}, 
  title={Man-Computer Symbiosis}, 
  year={1960},
  volume={HFE-1},
  number={1},
  pages={4-11},
  keywords={Symbiosis;Insects;Time sharing computer systems;Performance evaluation;Performance analysis;Computer languages},
  doi={10.1109/THFE2.1960.4503259}}


@inproceedings{sarsenbayeva2018situational,
  title={Situational impairments during mobile interaction},
  author={Sarsenbayeva, Zhanna},
  booktitle={Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
  pages={498--503},
  year={2018}
}

@misc{stallbaumer2023introducing,
  title={Introducing Microsoft 365 Copilot—a whole new way to work},
  author={Stallbaumer, Colette},
  year={2023},
  publisher={Microsoft}
}

@article{liang2023taskmatrix,
  title={Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis},
  author={Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji, Lei and Mao, Shaoguang and others},
  journal={arXiv preprint arXiv:2303.16434},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{yang2023set,
  title={Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v},
  author={Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.11441},
  year={2023}
}

@inproceedings{song2023llm,
  title={Llm-planner: Few-shot grounded planning for embodied agents with large language models},
  author={Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M and Chao, Wei-Lun and Su, Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2998--3009},
  year={2023}
}

@article{zheng2024gpt,
  title={GPT-4V (ision) is a Generalist Web Agent, if Grounded},
  author={Zheng, Boyuan and Gou, Boyu and Kil, Jihyung and Sun, Huan and Su, Yu},
  journal={arXiv preprint arXiv:2401.01614},
  year={2024}
}

@misc{gpt4vact,
  title = {{GPT-4V-Act: GPT-4 Variant for Active Learning}},
  author = {Dupont, D.},
  howpublished = {GitHub repository},
  year = {2023},
  url = {https://github.com/ddupont808/GPT-4V-Act}
}

@article{lin2018nl2bash,
  title={Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system},
  author={Lin, Xi Victoria and Wang, Chenglong and Zettlemoyer, Luke and Ernst, Michael D},
  journal={arXiv preprint arXiv:1802.08979},
  year={2018}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, intercode and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{server2008using,
  title={Using vmrun to Control Virtual Machines},
  author={Server, VMware},
  year={2008}
}

@book{grinberg2018flask,
  title={Flask web development: developing web applications with python},
  author={Grinberg, Miguel},
  year={2018},
  publisher={" O'Reilly Media, Inc."}
}


@article{yang2023intercode,
  title={InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback},
  author={Yang, John and Prabhakar, Akshara and Narasimhan, Karthik and Yao, Shunyu},
  journal={arXiv preprint arXiv:2306.14898},
  year={2023}
}

@article{Shridhar2020ALFWorldAT,
  title={ALFWorld: Aligning Text and Embodied Environments for Interactive Learning},
  author={Mohit Shridhar and Xingdi Yuan and Marc-Alexandre C{\^o}t{\'e} and Yonatan Bisk and Adam Trischler and Matthew J. Hausknecht},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.03768},
  url={https://api.semanticscholar.org/CorpusID:222208810}
}

@article{fan2022minedojo,
  title={Minedojo: Building open-ended embodied agents with internet-scale knowledge},
  author={Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18343--18362},
  year={2022}
}

@inproceedings{shridhar2020alfred,
  title={Alfred: A benchmark for interpreting grounded instructions for everyday tasks},
  author={Shridhar, Mohit and Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan and Han, Winson and Mottaghi, Roozbeh and Zettlemoyer, Luke and Fox, Dieter},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10740--10749},
  year={2020}
}

@inproceedings{searles2023empirical,
  title={An Empirical Study \& Evaluation of Modern $\{$CAPTCHAs$\}$},
  author={Searles, Andrew and Nakatsuka, Yoshimichi and Ozturk, Ercan and Paverd, Andrew and Tsudik, Gene and Enkoji, Ai},
  booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
  pages={3081--3097},
  year={2023}
}

@article{lin2023vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Lu, Yao and Molchanov, Pavlo and Tao, Andrew and Mao, Huizi and Kautz, Jan and Shoeybi, Mohammad and Han, Song},
  journal={arXiv preprint arXiv:2312.07533},
  year={2023}
}

@article{yue2023mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  journal={arXiv preprint arXiv:2311.16502},
  year={2023}
}

@article{mialon2023gaia,
  title={Gaia: a benchmark for general ai assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  journal={arXiv preprint arXiv:2311.12983},
  year={2023}
}

@article{he2024webvoyager,
  title={WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models},
  author={He, Hongliang and Yao, Wenlin and Ma, Kaixin and Yu, Wenhao and Dai, Yong and Zhang, Hongming and Lan, Zhenzhong and Yu, Dong},
  journal={arXiv preprint arXiv:2401.13919},
  year={2024}
}

@article{gao2023assistgui,
  title={ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation},
  author={Gao, Difei and Ji, Lei and Bai, Zechen and Ouyang, Mingyu and Li, Peiran and Mao, Dongxing and Wu, Qinchen and Zhang, Weichen and Wang, Peiyi and Guo, Xiangwu and others},
  journal={arXiv preprint arXiv:2312.13108},
  year={2023}
}


@article{wu2023smartplay,
  title={SmartPlay: A Benchmark for LLMs as Intelligent Agents},
  author={Wu, Yue and Tang, Xuan and Mitchell, Tom M and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2310.01557},
  year={2023}
}

@article{wang2022scienceworld,
  title={ScienceWorld: Is your Agent Smarter than a 5th Grader?},
  author={Wang, Ruoyao and Jansen, Peter and C{\^o}t{\'e}, Marc-Alexandre and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2203.07540},
  year={2022}
}

@inproceedings{li2023behavior,
  title={Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation},
  author={Li, Chengshu and Zhang, Ruohan and Wong, Josiah and Gokmen, Cem and Srivastava, Sanjana and Mart{\'\i}n-Mart{\'\i}n, Roberto and Wang, Chen and Levine, Gabrael and Lingelbach, Michael and Sun, Jiankai and others},
  booktitle={Conference on Robot Learning},
  pages={80--93},
  year={2023},
  organization={PMLR}
}

@article{guo2023pptc,
  title={PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion},
  author={Guo, Yiduo and Zhang, Zekai and Liang, Yaobo and Zhao, Dongyan and Nan, Duan},
  journal={arXiv preprint arXiv:2311.01767},
  year={2023}
}

@article{Cheng2024SeeClickHG,
  title={Seeclick: Harnessing gui grounding for advanced visual gui agents},
  author={Cheng, Kanzhi and Sun, Qiushi and Chu, Yougang and Xu, Fangzhi and Li, Yantao and Zhang, Jianbing and Wu, Zhiyong},
  journal={arXiv preprint arXiv:2401.10935},
  year={2024}
}

@article{Zheng2024GPT4VisionIA,
  title={GPT-4V(ision) is a Generalist Web Agent, if Grounded},
  author={Boyuan Zheng and Boyu Gou and Jihyung Kil and Huan Sun and Yu Su},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.01614},
  url={https://api.semanticscholar.org/CorpusID:266741821}
}

@article{He2024WebVoyagerBA,
  title={WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models},
  author={Hongliang He and Wenlin Yao and Kaixin Ma and Wenhao Yu and Yong Dai and Hongming Zhang and Zhenzhong Lan and Dong Yu},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.13919},
  url={https://api.semanticscholar.org/CorpusID:267211622}
}

@article{Baechler2024ScreenAIAV,
  title={ScreenAI: A Vision-Language Model for UI and Infographics Understanding},
  author={Baechler, Gilles and Sunkara, Srinivas and Wang, Maria and Zubach, Fedir and Mansoor, Hassan and Etter, Vincent and C{\u{a}}rbune, Victor and Lin, Jason and Chen, Jindong and Sharma, Abhanshu},
  journal={arXiv preprint arXiv:2402.04615},
  year={2024}
}

@article{Kapoor2024OmniACTAD,
  title={OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web},
  author={Kapoor, Raghav and Butala, Yash Parag and Russak, Melisa and Koh, Jing Yu and Kamble, Kiran and Alshikh, Waseem and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2402.17553},
  year={2024}
}

@article{Koh2024VisualWebArenaEM,
  title={Visualwebarena: Evaluating multimodal agents on realistic visual web tasks},
  author={Koh, Jing Yu and Lo, Robert and Jang, Lawrence and Duvvur, Vikram and Lim, Ming Chong and Huang, Po-Yu and Neubig, Graham and Zhou, Shuyan and Salakhutdinov, Ruslan and Fried, Daniel},
  journal={arXiv preprint arXiv:2401.13649},
  year={2024}
}

@article{Zhang2024UFOAU,
  title={UFO: A UI-Focused Agent for Windows OS Interaction},
  author={Zhang, Chaoyun and Li, Liqun and He, Shilin and Zhang, Xu and Qiao, Bo and Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and others},
  journal={arXiv preprint arXiv:2402.07939},
  year={2024}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yan2023gpt,
  title={Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation},
  author={Yan, An and Yang, Zhengyuan and Zhu, Wanrong and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yang, Jianwei and Zhong, Yiwu and McAuley, Julian and Gao, Jianfeng and others},
  journal={arXiv preprint arXiv:2311.07562},
  year={2023}
}

@article{lu2024weblinx,
  title={Weblinx: Real-world website navigation with multi-turn dialogue},
  author={L{\`u}, Xing Han and Kasner, Zden{\v{e}}k and Reddy, Siva},
  journal={arXiv preprint arXiv:2402.05930},
  year={2024}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@misc{Anthropic2023Claude,
  author = {{Anthropic}},
  title = {Introducing the next generation of Claude},
  year = {2023},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-family}},
  note = {Accessed: 2024-03-26}
}

@article{niu2024screenagent,
  title={ScreenAgent: A Vision Language Model-driven Computer Control Agent},
  author={Niu, Runliang and Li, Jindong and Wang, Shiqi and Fu, Yali and Hu, Xiyu and Leng, Xueyuan and Kong, He and Chang, Yi and Wang, Qi},
  journal={arXiv preprint arXiv:2402.07945},
  year={2024}
}

@article{zhang2024large,
  title={Large Language Models Are Semi-Parametric Reinforcement Learning Agents},
  author={Zhang, Danyang and Chen, Lu and Zhang, Situo and Xu, Hongshen and Zhao, Zihan and Yu, Kai},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{zhao2022tie,
  title={TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages},
  author={Zhao, Zihan and Chen, Lu and Cao, Ruisheng and Xu, Hongshen and Chen, Xingyu and Yu, Kai},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1808--1821},
  year={2022}
}

@article{brohan2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2307.15818},
  year={2023}
}

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@article{jimenez2023swe,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{li2024devbench,
  title={DevBench: A Comprehensive Benchmark for Software Development},
  author={Li, Bowen and Wu, Wenhan and Tang, Ziwei and Shi, Lin and Yang, John and Li, Jinyang and Yao, Shunyu and Qian, Chen and Hui, Binyuan and Zhang, Qicheng and others},
  journal={arXiv preprint arXiv:2403.08604},
  year={2024}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous multi-modal mobile device agent with visual perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}

@article{drouin2024workarena,
  title={WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?},
  author={Drouin, Alexandre and Gasse, Maxime and Caccia, Massimo and Laradji, Issam H and Del Verme, Manuel and Marty, Tom and Boisvert, L{\'e}o and Thakkar, Megh and Cappart, Quentin and Vazquez, David and others},
  journal={arXiv preprint arXiv:2403.07718},
  year={2024}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{tan2024towards,
  title={Towards general computer control: A multimodal agent for red dead redemption ii as a case study},
  author={Tan, Weihao and Ding, Ziluo and Zhang, Wentao and Li, Boyu and Zhou, Bohan and Yue, Junpeng and Xia, Haochong and Jiang, Jiechuan and Zheng, Longtao and Xu, Xinrun and others},
  journal={arXiv preprint arXiv:2403.03186},
  year={2024}
}


@inproceedings{wang2022scienceworld,
  title={ScienceWorld: Is your Agent Smarter than a 5th Grader?},
  author={Wang, Ruoyao and Jansen, Peter and C{\^o}t{\'e}, Marc-Alexandre and Ammanabrolu, Prithviraj},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11279--11298},
  year={2022}
}

@article{zhou2023webarena,
  title={Webarena: A realistic web environment for building autonomous agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},
  journal={arXiv preprint arXiv:2307.13854},
  year={2023}
}

@article{liu2023agentbench,
  title={{AgentBench}: Evaluating llms as agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and others},
  journal={arXiv preprint arXiv:2308.03688},
  year={2023}
}

@article{xu2023rewoo,
  title={ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models},
  author={Xu, Binfeng and Peng, Zhiyuan and Lei, Bowen and Mukherjee, Subhabrata and Liu, Yuchen and Xu, Dongkuan},
  journal={arXiv preprint arXiv:2305.18323},
  year={2023}
}

@article{shridhar2020alfworld,
  title={Alfworld: Aligning text and embodied environments for interactive learning},
  author={Shridhar, Mohit and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and Hausknecht, Matthew},
  journal={arXiv preprint arXiv:2010.03768},
  year={2020}
}

@article{yao2022webshop,
  title={Webshop: Towards scalable real-world web interaction with grounded language agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20744--20757},
  year={2022}
}

@article{deng2023mind2web,
  title={Mind2Web: Towards a Generalist Agent for the Web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={arXiv preprint arXiv:2306.06070},
  year={2023}
}

@article{qin2023toolllm,
  title={{ToolLLM}: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@inproceedings{yang2018hotpotqa,
  title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2369--2380},
  year={2018}
}

@article{gou2023tora,
  title={{ToRA}: A tool-integrated reasoning agent for mathematical problem solving},
  author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Yang, Yujiu and Huang, Minlie and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2309.17452},
  year={2023}
}


% ---- Part: AgentBoard里面引用和比较过的benchmarks ----

% AgentBoard
@article{ma2024agentboard,
  title={AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents},
  author={Ma, Chang and Zhang, Junlei and Zhu, Zhihao and Yang, Cheng and Yang, Yujiu and Jin, Yaohui and Lan, Zhenzhong and Kong, Lingpeng and He, Junxian},
  journal={arXiv preprint arXiv:2401.13178},
  year={2024}
}

% AgentBench

% GAIA
@article{mialon2023gaia,
  title={Gaia: a benchmark for general ai assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  journal={arXiv preprint arXiv:2311.12983},
  year={2023}
}

% MINT
@article{wang2023mint,
  title={Mint: Evaluating llms in multi-turn interaction with tools and language feedback},
  author={Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen, Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2309.10691},
  year={2023}
}

% API-Bank
@article{li2023api,
  title={Api-bank: A comprehensive benchmark for tool-augmented llms},
  author={Li, Minghao and Zhao, Yingxiu and Yu, Bowen and Song, Feifan and Li, Hangyu and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2304.08244},
  year={2023}
}

% ToolLLM / ToolEval

% LLM-Eval
@article{lin2023llm,
  title={Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models},
  author={Lin, Yen-Ting and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:2305.13711},
  year={2023}
}

% ---- Part: AgentBoard子任务对应的dataset ----

% ALF: ALFWorld

% SW: ScienceWorld
@article{wang2022scienceworld,
  title={Scienceworld: Is your agent smarter than a 5th grader?},
  author={Wang, Ruoyao and Jansen, Peter and C{\^o}t{\'e}, Marc-Alexandre and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2203.07540},
  year={2022}
}

% BA: BabyAI
@article{chevalier2018babyai,
  title={Babyai: A platform to study the sample efficiency of grounded language learning},
  author={Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1810.08272},
  year={2018}
}

% JC: Jericho
@article{hausknecht19,
  title={Interactive Fiction Games: A Colossal Adventure},
  author={Hausknecht, Matthew and Ammanabrolu, Prithviraj and C\^ot\'{e} Marc-Alexandre and Yuan Xingdi},
  journal={CoRR},
  year={2019},
  url={http://arxiv.org/abs/1909.05398},
  volume={abs/1909.05398}
}

% PDDL
@article{vallati20152014,
  title={The 2014 international planning competition: Progress and trends},
  author={Vallati, Mauro and Chrpa, Lukas and Grze{\'s}, Marek and McCluskey, Thomas Leo and Roberts, Mark and Sanner, Scott and others},
  journal={Ai Magazine},
  volume={36},
  number={3},
  pages={90--98},
  year={2015}
}

% WS: WebShop
@article{yao2022webshop,
  title={Webshop: Towards scalable real-world web interaction with grounded language agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20744--20757},
  year={2022}
}

% WA: WebArena
@article{zhou2023webarena,
  title={Webarena: A realistic web environment for building autonomous agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},
  journal={arXiv preprint arXiv:2307.13854},
  year={2023}
}

% Tool-Query and Tool-Operation ?


% ---- Part: AgentBoard里面引用和比较过的benchmarks ----

% AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents
@article{debenedetti2024agentdojo,
  title={AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents},
  author={Debenedetti, Edoardo and Zhang, Jie and Balunovi{\'c}, Mislav and Beurer-Kellner, Luca and Fischer, Marc and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2406.13352},
  year={2024}
}

% WebCanvas: Benchmarking Web Agents in Online Environments
@article{pan2024webcanvas,
  title={WebCanvas: Benchmarking Web Agents in Online Environments},
  author={Pan, Yichen and Kong, Dehan and Zhou, Sida and Cui, Cheng and Leng, Yifei and Jiang, Bing and Liu, Hangyu and Shang, Yanyi and Zhou, Shuyan and Wu, Tongshuang and others},
  journal={arXiv preprint arXiv:2406.12373},
  year={2024}
}

% Autonomous Human Computer Interaction System in Windows Environment Using YOLO and LLM
@inproceedings{muralikrishna2024autonomous,
  title={Autonomous Human Computer Interaction System in Windows Environment Using YOLO and LLM},
  author={Muralikrishna, V and Vijayalakshmi, M},
  booktitle={International Conference on Artificial Intelligence and Smart Energy},
  pages={157--169},
  year={2024},
  organization={Springer}
}

% Towards Unified Alignment Between Agents, Humans, and Environment
@article{yang2024towards,
  title={Towards Unified Alignment Between Agents, Humans, and Environment},
  author={Yang, Zonghan and Liu, An and Liu, Zijun and Liu, Kaiming and Xiong, Fangzhou and Wang, Yile and Yang, Zeyuan and Hu, Qingyuan and Chen, Xinrui and Zhang, Zhenhe and others},
  journal={arXiv preprint arXiv:2402.07744},
  year={2024}
}

% Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark
@article{wu2024seal,
  title={Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark},
  author={Wu, Mengsong and Zhu, Tong and Han, Han and Tan, Chuanyuan and Zhang, Xiang and Chen, Wenliang},
  journal={arXiv preprint arXiv:2405.08355},
  year={2024}
}

% Datasets for large language models: A comprehensive survey
@article{liu2024datasets,
  title={Datasets for large language models: A comprehensive survey},
  author={Liu, Yang and Cao, Jiahuan and Liu, Chongyu and Ding, Kai and Jin, Lianwen},
  journal={arXiv preprint arXiv:2402.18041},
  year={2024}
}

% ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents
@article{kang2024researcharena,
  title={ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents},
  author={Kang, Hao and Xiong, Chenyan},
  journal={arXiv preprint arXiv:2406.10291},
  year={2024}
}

% Bells: A framework towards future proof benchmarks for the evaluation of llm safeguards
@article{dorn2024bells,
  title={Bells: A framework towards future proof benchmarks for the evaluation of llm safeguards},
  author={Dorn, Diego and Variengien, Alexandre and Segerie, Charbel-Rapha{\"e}l and Corruble, Vincent},
  journal={arXiv preprint arXiv:2406.01364},
  year={2024}
}

% WebSuite: Systematically Evaluating Why Web Agents Fail
@article{li2024websuite,
  title={WebSuite: Systematically Evaluating Why Web Agents Fail},
  author={Li, Eric and Waldo, Jim},
  journal={arXiv preprint arXiv:2406.01623},
  year={2024}
}

% Inadequacies of large language model benchmarks in the era of generative artificial intelligence
@article{mcintosh2024inadequacies,
  title={Inadequacies of large language model benchmarks in the era of generative artificial intelligence},
  author={McIntosh, Timothy R and Susnjak, Teo and Liu, Tong and Watters, Paul and Halgamuge, Malka N},
  journal={arXiv preprint arXiv:2402.09880},
  year={2024}
}

% AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability
@article{yang2024aqa,
  title={AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability},
  author={Yang, Siwei and Zhao, Bingchen and Xie, Cihang},
  journal={arXiv preprint arXiv:2402.09404},
  year={2024}
}

% Travelplanner: A benchmark for real-world planning with language agents
@article{xie2024travelplanner,
  title={Travelplanner: A benchmark for real-world planning with language agents},
  author={Xie, Jian and Zhang, Kai and Chen, Jiangjie and Zhu, Tinghui and Lou, Renze and Tian, Yuandong and Xiao, Yanghua and Su, Yu},
  journal={arXiv preprint arXiv:2402.01622},
  year={2024}
}

% Adapting standard retrieval benchmarks to evaluate generated answers
@inproceedings{arabzadeh2024adapting,
  title={Adapting standard retrieval benchmarks to evaluate generated answers},
  author={Arabzadeh, Negar and Bigdeli, Amin and Clarke, Charles LA},
  booktitle={European Conference on Information Retrieval},
  pages={399--414},
  year={2024},
  organization={Springer}
}

% ---- Part: AgentBench数据集 ----

% OS?
% Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system
@article{lin2018nl2bash,
  title={Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system},
  author={Lin, Xi Victoria and Wang, Chenglong and Zettlemoyer, Luke and Ernst, Michael D},
  journal={arXiv preprint arXiv:1802.08979},
  year={2018}
}

% Docker: lightweight linux containers for consistent development and deployment
@article{merkel2014docker,
  title={Docker: lightweight linux containers for consistent development and deployment},
  author={Merkel, Dirk and others},
  journal={Linux j},
  volume={239},
  number={2},
  pages={2},
  year={2014}
}


% DB
@article{zhong2017seq2sql,
  title={Seq2sql: Generating structured queries from natural language using reinforcement learning},
  author={Zhong, Victor and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1709.00103},
  year={2017}
}

@article{pasupat2015compositional,
  title={Compositional semantic parsing on semi-structured tables},
  author={Pasupat, Panupong and Liang, Percy},
  journal={arXiv preprint arXiv:1508.00305},
  year={2015}
}

@inproceedings{iyyer2017search,
  title={Search-based neural structured learning for sequential question answering},
  author={Iyyer, Mohit and Yih, Wen-tau and Chang, Ming-Wei},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1821--1831},
  year={2017}
}

@article{chen2020hybridqa,
  title={Hybridqa: A dataset of multi-hop question answering over tabular and textual data},
  author={Chen, Wenhu and Zha, Hanwen and Chen, Zhiyu and Xiong, Wenhan and Wang, Hong and Wang, William},
  journal={arXiv preprint arXiv:2004.07347},
  year={2020}
}

@article{nan2022fetaqa,
  title={FeTaQA: Free-form table question answering},
  author={Nan, Linyong and Hsieh, Chiachun and Mao, Ziming and Lin, Xi Victoria and Verma, Neha and Zhang, Rui and Kry{\'s}ci{\'n}ski, Wojciech and Schoelkopf, Hailey and Kong, Riley and Tang, Xiangru and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={35--49},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

% KG? : Anonymous. Knowledge base question answering as tool learning. under review, 2023.
% Freebase: a collaboratively created graph database for structuring human knowledge
@inproceedings{bollacker2008freebase,
  title={Freebase: a collaboratively created graph database for structuring human knowledge},
  author={Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  booktitle={Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
  pages={1247--1250},
  year={2008}
}

% Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments
@article{gu2022don,
  title={Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments},
  author={Gu, Yu and Deng, Xiang and Su, Yu},
  journal={arXiv preprint arXiv:2212.09736},
  year={2022}
}

% Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases
@inproceedings{gu2021beyond,
  title={Beyond IID: three levels of generalization for question answering on knowledge bases},
  author={Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
  booktitle={Proceedings of the Web Conference 2021},
  pages={3477--3488},
  year={2021}
}

% The Web as a Knowledge-Base for Answering Complex Questions
@article{talmor2018web,
  title={The web as a knowledge-base for answering complex questions},
  author={Talmor, Alon and Berant, Jonathan},
  journal={arXiv preprint arXiv:1803.06643},
  year={2018}
}

% On Generating Characteristic-rich Question Sets for QA Evaluation
@inproceedings{su2016generating,
  title={On generating characteristic-rich question sets for qa evaluation},
  author={Su, Yu and Sun, Huan and Sadler, Brian and Srivatsa, Mudhakar and G{\"u}r, Izzeddin and Yan, Zenghui and Yan, Xifeng},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={562--572},
  year={2016}
}

% ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering
@article{gu2022arcaneqa,
  title={ArcaneQA: Dynamic program induction and contextualized encoding for knowledge base question answering},
  author={Gu, Yu and Su, Yu},
  journal={arXiv preprint arXiv:2204.08109},
  year={2022}
}


% DCG
%  The many ai challenges of hearthstone.
@article{hoover2020many,
  title={The many ai challenges of hearthstone},
  author={Hoover, Amy K and Togelius, Julian and Lee, Scott and de Mesentier Silva, Fernando},
  journal={KI-K{\"u}nstliche Intelligenz},
  volume={34},
  pages={33--43},
  year={2020},
  publisher={Springer}
}

% LTP
% Super Lateral Thinking Puzzles
@book{sloane2000super,
  title={Super Lateral Thinking Puzzles},
  author={Sloane, Paul and MacHale, Des},
  year={2000},
  publisher={Sterling Publishing Company, Inc.}
}
% Lateral thinking
@book{de1970lateral,
  title={Lateral thinking},
  author={De Bono, Edward and Zimbalist, Efrem},
  year={1970},
  publisher={Penguin London}
}


% HH
% ALFWorld: 已有
% Textworld: A learning environment for text-based games
@inproceedings{cote2019textworld,
  title={Textworld: A learning environment for text-based games},
  author={C{\^o}t{\'e}, Marc-Alexandre and K{\'a}d{\'a}r, Akos and Yuan, Xingdi and Kybartas, Ben and Barnes, Tavian and Fine, Emery and Moore, James and Hausknecht, Matthew and El Asri, Layla and Adada, Mahmoud and others},
  booktitle={Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7},
  pages={41--75},
  year={2019},
  organization={Springer}
}

% React: Synergizing reasoning and acting in language models
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}


% WS已有

% WB
% Mind2web: Towards a generalist agent for the web
@article{deng2024mind2web,
  title={Mind2web: Towards a generalist agent for the web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


% ---- Part: AgentBench Related Work ----


@article{deng2024mind2web,
  title={Mind2web: Towards a generalist agent for the web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


% ---- Part: AgentBench Related Work ----
% ALFWorld已有

% Measuring coding challenge competence with apps
@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

% Evaluating large language models trained on code
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

% Program synthesis with large language models
@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

% Competition-level code generation with alphacode
@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

% Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x
@article{zheng2023codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},
  journal={arXiv preprint arXiv:2303.17568},
  year={2023}
}

%  Codegen: An open large language model for code with multi-turn program synthesis
@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

% Intercode: Standardizing and benchmarking interactive coding with execution feedback
@article{yang2024intercode,
  title={Intercode: Standardizing and benchmarking interactive coding with execution feedback},
  author={Yang, John and Prabhakar, Akshara and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@article{chen2023fireact,
  title={Fireact: Toward language agent fine-tuning},
  author={Chen, Baian and Shu, Chang and Shareghi, Ehsan and Collier, Nigel and Narasimhan, Karthik and Yao, Shunyu},
  journal={arXiv preprint arXiv:2310.05915},
  year={2023}
}

@article{zeng2023agenttuning,
  title={Agenttuning: Enabling generalized agent abilities for llms},
  author={Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2310.12823},
  year={2023}
}

@article{wang2024learning,
  title={Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents},
  author={Wang, Renxi and Li, Haonan and Han, Xudong and Zhang, Yixuan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2402.11651},
  year={2024}
}


@article{chen2024agent,
  title={Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models},
  author={Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Zhang, Wenwei and Liu, Jiangning and Lin, Dahua and Chen, Kai and Zhao, Feng},
  journal={arXiv preprint arXiv:2403.12881},
  year={2024}
}

@article{zhang2024agentohana,
  title={AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning},
  author={Zhang, Jianguo and Lan, Tian and Murthy, Rithesh and Liu, Zhiwei and Yao, Weiran and Tan, Juntao and Hoang, Thai and Yang, Liangwei and Feng, Yihao and Liu, Zuxin and others},
  journal={arXiv preprint arXiv:2402.15506},
  year={2024}
}

@article{yin2023lumos,
  title={Lumos: Learning agents with unified data, modular design, and open-source llms},
  author={Yin, Da and Brahman, Faeze and Ravichander, Abhilasha and Chandu, Khyathi and Chang, Kai-Wei and Choi, Yejin and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2311.05657},
  year={2023}
}

@article{wang2024llms,
  title={LLMs in the Imaginarium: tool learning through simulated trial and error},
  author={Wang, Boshi and Fang, Hao and Eisner, Jason and Van Durme, Benjamin and Su, Yu},
  journal={arXiv preprint arXiv:2403.04746},
  year={2024}
}

@article{song2024trial,
  title={Trial and error: Exploration-based trajectory optimization for llm agents},
  author={Song, Yifan and Yin, Da and Yue, Xiang and Huang, Jie and Li, Sujian and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2403.02502},
  year={2024}
}

@article{kambhampati2024llms,
  title={LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks},
  author={Kambhampati, Subbarao and Valmeekam, Karthik and Guan, Lin and Stechly, Kaya and Verma, Mudit and Bhambri, Siddhant and Saldyt, Lucas and Murthy, Anil},
  journal={arXiv preprint arXiv:2402.01817},
  year={2024}
}


@article{arora2023learning,
  title={Learning and leveraging verifiers to improve planning capabilities of pre-trained language models},
  author={Arora, Daman and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2305.17077},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{besta2023graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  journal={arXiv preprint arXiv:2308.09687},
  year={2023}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@inproceedings{shinn2023reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik R and Yao, Shunyu},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@article{wu2023autogen,
  title={{AutoGen}: Enabling next-gen llm applications via multi-agent conversation framework},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}

@article{chen2023autoagents,
  title={{AutoAgents}: A Framework for Automatic Agent Generation},
  author={Chen, Guangyao and Dong, Siwei and Shu, Yu and Zhang, Ge and Sesay, Jaward and Karlsson, B{\"o}rje F and Fu, Jie and Shi, Yemin},
  journal={arXiv preprint arXiv:2309.17288},
  year={2023}
}

@article{talebirad2023multi,
  title={Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents},
  author={Talebirad, Yashar and Nadiri, Amirhossein},
  journal={arXiv preprint arXiv:2306.03314},
  year={2023}
}

@article{chen2023agentverse,
  title={{AgentVerse}: Facilitating multi-agent collaboration and exploring emergent behaviors in agents},
  author={Chen, Weize and Su, Yusheng and Zuo, Jingwei and Yang, Cheng and Yuan, Chenfei and Qian, Chen and Chan, Chi-Min and Qin, Yujia and Lu, Yaxi and Xie, Ruobing and others},
  journal={arXiv preprint arXiv:2308.10848},
  year={2023}
}

@article{hong2023metagpt,
  title={{MetaGPT}: Meta programming for multi-agent collaborative framework},
  author={Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and others},
  journal={arXiv preprint arXiv:2308.00352},
  year={2023}
}

@article{sumers2023cognitive,
  title={Cognitive architectures for language agents},
  author={Sumers, Theodore R and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2309.02427},
  year={2023}
}

@inproceedings{huang2023inner,
  title={Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author={Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
  booktitle={Conference on Robot Learning},
  pages={1769--1782},
  year={2023},
  organization={PMLR}
}



@article{wang2023survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2308.11432},
  year={2023}
}

@article{xi2023rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal={arXiv preprint arXiv:2309.07864},
  year={2023}
}

@misc{kim2023language,
      title={Language Models can Solve Computer Tasks}, 
      author={Geunwoo Kim and Pierre Baldi and Stephen McAleer},
      year={2023},
      eprint={2303.17491},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{mao2023editing,
  title={Editing personality for llms},
  author={Mao, Shengyu and Zhang, Ningyu and Wang, Xiaohan and Wang, Mengru and Yao, Yunzhi and Jiang, Yong and Xie, Pengjun and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2310.02168},
  year={2023}
}

@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--22},
  year={2023}
}

@misc{zhang2023exploring,
      title={Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View}, 
      author={Jintian Zhang and Xin Xu and Shumin Deng},
      year={2023},
      eprint={2310.02124},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023bolaa,
      title={BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents}, 
      author={Zhiwei Liu and Weiran Yao and Jianguo Zhang and Le Xue and Shelby Heinecke and Rithesh Murthy and Yihao Feng and Zeyuan Chen and Juan Carlos Niebles and Devansh Arpit and Ran Xu and Phil Mui and Huan Wang and Caiming Xiong and Silvio Savarese},
      year={2023},
      eprint={2308.05960},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{liang2023encouraging,
      title={Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate}, 
      author={Tian Liang and Zhiwei He and Wenxiang Jiao and Xing Wang and Yan Wang and Rui Wang and Yujiu Yang and Zhaopeng Tu and Shuming Shi},
      year={2023},
      eprint={2305.19118},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2023instruction,
      title={Instruction Tuning for Large Language Models: A Survey}, 
      author={Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
      year={2023},
      eprint={2308.10792},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2021generated,
  title={Generated knowledge prompting for commonsense reasoning},
  author={Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.08387},
  year={2021}
}

@article{fu2023specializing,
  title={Specializing Smaller Language Models towards Multi-Step Reasoning},
  author={Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  journal={arXiv preprint arXiv:2301.12726},
  year={2023}
}


@article{patil2023gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{qiao2023making,
  title={Making Language Models Better Tool Learners with Execution Feedback},
  author={Qiao, Shuofei and Gui, Honghao and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13068},
  year={2023}
}

@article{wang2023instructuie,
  title={InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction},
  author={Wang, Xiao and Zhou, Weikang and Zu, Can and Xia, Han and Chen, Tianze and Zhang, Yuansen and Zheng, Rui and Ye, Junjie and Zhang, Qi and Gui, Tao and others},
  journal={arXiv preprint arXiv:2304.08085},
  year={2023}
}


@article{ivison2022hint,
  title={HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation},
  author={Ivison, Hamish and Bhagia, Akshita and Wang, Yizhong and Hajishirzi, Hannaneh and Peters, Matthew},
  journal={arXiv preprint arXiv:2212.10315},
  year={2022}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{lv2023full,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Kai and Yang, Yuqing and Liu, Tengxiao and Gao, Qinghui and Guo, Qipeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2306.09782},
  year={2023}
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}


% --------- Memory --------------

@article{wang2023robogen,
  title={Robogen: Towards unleashing infinite data for automated robot learning via generative simulation},
  author={Wang, Yufei and Xian, Zhou and Chen, Feng and Wang, Tsun-Hsuan and Wang, Yian and Fragkiadaki, Katerina and Erickson, Zackory and Held, David and Gan, Chuang},
  journal={arXiv preprint arXiv:2311.01455},
  year={2023}
}


@inproceedings{yang2024holodeck,
  title={Holodeck: Language guided generation of 3d embodied ai environments},
  author={Yang, Yue and Sun, Fan-Yun and Weihs, Luca and VanderBilt, Eli and Herrasti, Alvaro and Han, Winson and Wu, Jiajun and Haber, Nick and Krishna, Ranjay and Liu, Lingjie and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16227--16237},
  year={2024}
}

@article{wang2023gensim,
  title={Gensim: Generating robotic simulation tasks via large language models},
  author={Wang, Lirui and Ling, Yiyang and Yuan, Zhecheng and Shridhar, Mohit and Bao, Chen and Qin, Yuzhe and Wang, Bailin and Xu, Huazhe and Wang, Xiaolong},
  journal={arXiv preprint arXiv:2310.01361},
  year={2023}
}

% ---------- SOTA LLMs -------------------

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{meta2024llama3,
  title = {Introducing Meta {Llama 3}: The most capable openly available {LLM} to date},
  author = {{Meta AI}},
  year = {2024},
  month = apr,
  url = {https://ai.meta.com/blog/meta-llama-3/},
  note = {Accessed: 2024-04-18}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}



@misc{team2023internlm,
  title={Internlm: A multilingual language model with progressively enhanced capabilities},
  author={Team, InternLM},
  journal={2023-01-06)[2023-09-27]. https://github. com/InternLM/InternLM},
  year={2023}
}

@article{jiang2024mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{openai2019gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@online{opeiai2022gpt,
  author   = {OpenAI},
  title     = {OpenAI: Introducing ChatGPT},
  year      = {2022},
  url       = {https://openai.com/blog/chatgpt},
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@misc{openai2024o1,
  title = {Learning to reason with LLMs},
  author = {OpenAI},
  year = {2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/}
}

@misc{openai2025o3,
  title = {OpenAI o3-mini},
  author = {OpenAI},
  year = {2025},
  url = {https://openai.com/index/openai-o3-mini/}
}

@misc{meta_llama_3_1,
  author = {{Meta AI}},
  title = {Introducing Llama 3.1: Our most capable models to date},
  url = {https://ai.meta.com/blog/meta-llama-3-1/}
}

@misc{claude_3.5_sonnet,
  author = {Anthropic},
  title = {Introducing Claude 3.5 Sonnet},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@book{girden1992anova,
  title={ANOVA: Repeated measures},
  author={Girden, Ellen R},
  number={84},
  year={1992},
  publisher={Sage}
}

@article{hu2024hiagent,
  title={Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model},
  author={Hu, Mengkang and Chen, Tianxing and Chen, Qiguang and Mu, Yao and Shao, Wenqi and Luo, Ping},
  journal={arXiv preprint arXiv:2408.09559},
  year={2024}
}

@article{lai2024analogcoder,
  title={AnalogCoder: Analog Circuit Design via Training-Free Code Generation},
  author={Lai, Yao and Lee, Sungyoung and Chen, Guojin and Poddar, Souradip and Hu, Mengkang and Pan, David Z and Luo, Ping},
  journal={arXiv preprint arXiv:2405.14918},
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

% ---------- SOTA LLMs [END] -------------------



% ---------- Instruction Tuning ------------ 


@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


% ----------- Misc ---------------

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}


@article{hussein2017imitation,
  title={Imitation learning: A survey of learning methods},
  author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={1--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}


% -------- Wizard Series -------

@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}

% ------------ LLM data annotation --------------

@article{tan2024large,
  title={Large language models for data annotation: A survey},
  author={Tan, Zhen and Beigi, Alimohammad and Wang, Song and Guo, Ruocheng and Bhattacharjee, Amrita and Jiang, Bohan and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan},
  journal={arXiv preprint arXiv:2402.13446},
  year={2024}
}


% -------- Memory ------------


@inproceedings{zhong2024memorybank,
  title={Memorybank: Enhancing large language models with long-term memory},
  author={Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={19724--19731},
  year={2024}
}

@article{liu2023think,
  title={Think-in-memory: Recalling and post-thinking enable llms with long-term memory},
  author={Liu, Lei and Yang, Xiaoyan and Shen, Yue and Hu, Binbin and Zhang, Zhiqiang and Gu, Jinjie and Zhang, Guannan},
  journal={arXiv preprint arXiv:2311.08719},
  year={2023}
}

@article{liang2023unleashing,
  title={Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system},
  author={Liang, Xinnian and Wang, Bing and Huang, Hui and Wu, Shuangzhi and Wu, Peihao and Lu, Lu and Ma, Zejun and Li, Zhoujun},
  journal={arXiv e-prints},
  pages={arXiv--2304},
  year={2023}
}

@article{yao2023retroformer,
  title={Retroformer: Retrospective large language agents with policy gradient optimization},
  author={Yao, Weiran and Heinecke, Shelby and Niebles, Juan Carlos and Liu, Zhiwei and Feng, Yihao and Xue, Le and Murthy, Rithesh and Chen, Zeyuan and Zhang, Jianguo and Arpit, Devansh and others},
  journal={arXiv preprint arXiv:2308.02151},
  year={2023}
}

@inproceedings{zhao2024expel,
  title={Expel: Llm agents are experiential learners},
  author={Zhao, Andrew and Huang, Daniel and Xu, Quentin and Lin, Matthieu and Liu, Yong-Jin and Huang, Gao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={19632--19642},
  year={2024}
}



@article{zhu2023ghost,
  title={Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory},
  author={Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and others},
  journal={arXiv preprint arXiv:2305.17144},
  year={2023}
}


@article{wang2023user,
  title={User behavior simulation with large language model based agents},
  author={Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and others},
  journal={arXiv preprint arXiv:2306.02552},
  year={2023}
}


@article{tack2024online,
  title={Online adaptation of language models with a memory of amortized contexts},
  author={Tack, Jihoon and Kim, Jaehyung and Mitchell, Eric and Shin, Jinwoo and Teh, Yee Whye and Schwarz, Jonathan Richard},
  journal={arXiv preprint arXiv:2403.04317},
  year={2024}
}


@article{huang2023recommender,
  title={Recommender ai agent: Integrating large language models for interactive recommendations},
  author={Huang, Xu and Lian, Jianxun and Lei, Yuxuan and Yao, Jing and Lian, Defu and Xie, Xing},
  journal={arXiv preprint arXiv:2308.16505},
  year={2023}
}


% ---- Tool Use ----


@article{cheng2022binding,
  title={Binding language models in symbolic languages},
  author={Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and others},
  journal={arXiv preprint arXiv:2210.02875},
  year={2022}
}

@article{mu2024embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mu2024robocodex,
  title={RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis},
  author={Mu, Yao and Chen, Junting and Zhang, Qinglong and Chen, Shoufa and Yu, Qiaojun and Ge, Chongjian and Chen, Runjian and Liang, Zhixuan and Hu, Mengkang and Tao, Chaofan and others},
  journal={arXiv preprint arXiv:2402.16117},
  year={2024}
}

@article{parisi2022talm,
  title={Talm: Tool augmented language models},
  author={Parisi, Aaron and Zhao, Yao and Fiedel, Noah},
  journal={arXiv preprint arXiv:2205.12255},
  year={2022}
}

@article{shen2024hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


% Code Generation Dataset
@inproceedings{lai2023ds,
  title={DS-1000: A natural and reliable benchmark for data science code generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle={International Conference on Machine Learning},
  pages={18319--18345},
  year={2023},
  organization={PMLR}
}


% ----- LLM Agent Survey ------


@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186345},
  year={2024},
  publisher={Springer}
}


% ----- Planning --------

@article{zheng2024natural,
  title={NATURAL PLAN: Benchmarking LLMs on Natural Language Planning},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Zhang, Hugh and Chen, Xinyun and Chen, Minmin and Nova, Azade and Hou, Le and Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and others},
  journal={arXiv preprint arXiv:2406.04520},
  year={2024}
}

@inproceedings{puig2018virtualhome,
  title={Virtualhome: Simulating household activities via programs},
  author={Puig, Xavier and Ra, Kevin and Boben, Marko and Li, Jiaman and Wang, Tingwu and Fidler, Sanja and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8494--8502},
  year={2018}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

@article{valmeekam2024planbench,
  title={Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change},
  author={Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hu2023tree,
  title={Tree-planner: Efficient close-loop task planning with large language models},
  author={Hu, Mengkang and Mu, Yao and Yu, Xinmiao and Ding, Mingyu and Wu, Shiguang and Shao, Wenqi and Chen, Qiguang and Wang, Bin and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2310.08582},
  year={2023}
}


@article{gao2024dag,
  title={DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning},
  author={Gao, Zeyu and Mu, Yao and Qu, Jinye and Hu, Mengkang and Guo, Lingyue and Luo, Ping and Lu, Yanfeng},
  journal={arXiv preprint arXiv:2406.09953},
  year={2024}
}


@inproceedings{singh2023progprompt,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={11523--11530},
  year={2023},
  organization={IEEE}
}

@inproceedings{brohan2023can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others},
  booktitle={Conference on robot learning},
  pages={287--318},
  year={2023},
  organization={PMLR}
}

@article{sun2023adaplanner,
  title={AdaPlanner: Adaptive Planning from Feedback with Language Models},
  author={Sun, Haotian and Zhuang, Yuchen and Kong, Lingkai and Dai, Bo and Zhang, Chao},
  journal={arXiv preprint arXiv:2305.16653},
  year={2023}
}


@article{sun2023pearl,
  title={PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents},
  author={Sun, Simeng and Liu, Yang and Wang, Shuohang and Zhu, Chenguang and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2305.14564},
  year={2023}
}

@misc{ding2023task,
      title={Task and Motion Planning with Large Language Models for Object Rearrangement}, 
      author={Yan Ding and Xiaohan Zhang and Chris Paxton and Shiqi Zhang},
      year={2023},
      eprint={2303.06247},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{lin2023grounded,
  title={On grounded planning for embodied tasks with language models},
  author={Lin, Bill Yuchen and Huang, Chengsong and Liu, Qian and Gu, Wenda and Sommerer, Sam and Ren, Xiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={13192--13200},
  year={2023}
}

@misc{wu2023embodied,
      title={Embodied Task Planning with Large Language Models}, 
      author={Zhenyu Wu and Ziwei Wang and Xiuwei Xu and Jiwen Lu and Haibin Yan},
      year={2023},
      eprint={2307.01848},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{song2023llmplanner,
      title={LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models}, 
      author={Chan Hee Song and Jiaman Wu and Clayton Washington and Brian M. Sadler and Wei-Lun Chao and Yu Su},
      year={2023},
      eprint={2212.04088},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{wang2023describe,
      title={Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents}, 
      author={Zihao Wang and Shaofei Cai and Anji Liu and Xiaojian Ma and Yitao Liang},
      year={2023},
      eprint={2302.01560},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{huang2023grounded,
      title={Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control}, 
      author={Wenlong Huang and Fei Xia and Dhruv Shah and Danny Driess and Andy Zeng and Yao Lu and Pete Florence and Igor Mordatch and Sergey Levine and Karol Hausman and Brian Ichter},
      year={2023},
      eprint={2303.00855},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{huang2022inner,
      title={Inner Monologue: Embodied Reasoning through Planning with Language Models}, 
      author={Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Noah Brown and Tomas Jackson and Linda Luu and Sergey Levine and Karol Hausman and Brian Ichter},
      year={2022},
      eprint={2207.05608},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@article{liu2024tool,
  title={Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering},
  author={Liu, Yanming and Peng, Xinyue and Zhang, Yuwei and Cao, Jiannan and Zhang, Xuhong and Cheng, Sheng and Wang, Xun and Yin, Jianwei and Du, Tianyu},
  journal={arXiv preprint arXiv:2406.03807},
  year={2024}
}



@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{wang2023plan,
  title={Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models},
  author={Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
  journal={arXiv preprint arXiv:2305.04091},
  year={2023}
}


@article{zhao2024large,
  title={Large language models as commonsense knowledge for large-scale task planning},
  author={Zhao, Zirui and Lee, Wee Sun and Hsu, David},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2023llm+,
  title={Llm+ p: Empowering large language models with optimal planning proficiency},
  author={Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter},
  journal={arXiv preprint arXiv:2304.11477},
  year={2023}
}


@article{ruan2023tptu,
  title={Tptu: Task planning and tool usage of large language model-based ai agents},
  author={Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and Mao, Hangyu and Zeng, Xingyu and Zhao, Rui},
  journal={arXiv preprint arXiv:2308.03427},
  year={2023}
}

@article{bairi2024codeplan,
  title={Codeplan: Repository-level coding using llms and planning},
  author={Bairi, Ramakrishna and Sonwane, Atharv and Kanade, Aditya and Iyer, Arun and Parthasarathy, Suresh and Rajamani, Sriram and Ashok, B and Shet, Shashank},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={675--698},
  year={2024},
  publisher={ACM New York, NY, USA}
}



@article{zhou2023language,
  title={Language agent tree search unifies reasoning acting and planning in language models},
  author={Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
  journal={arXiv preprint arXiv:2310.04406},
  year={2023}
}

@article{aghzal2023can,
  title={Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning},
  author={Aghzal, Mohamed and Plaku, Erion and Yao, Ziyu},
  journal={arXiv preprint arXiv:2310.03249},
  year={2023}
}

@article{ajay2024compositional,
  title={Compositional foundation models for hierarchical planning},
  author={Ajay, Anurag and Han, Seungwook and Du, Yilun and Li, Shuang and Gupta, Abhi and Jaakkola, Tommi and Tenenbaum, Josh and Kaelbling, Leslie and Srivastava, Akash and Agrawal, Pulkit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@INPROCEEDINGS{kaelblingTAMP,
  author={Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
  booktitle={2011 IEEE International Conference on Robotics and Automation}, 
  title={Hierarchical task and motion planning in the now}, 
  year={2011},
  volume={},
  number={},
  pages={1470-1477},
  doi={10.1109/ICRA.2011.5980391}}


@inproceedings{McDermott1998PDDLthePD,
  title={PDDL-the planning domain definition language},
  author={Drew McDermott and Malik Ghallab and Adele E. Howe and Craig A. Knoblock and Ashwin Ram and Manuela M. Veloso and Daniel S. Weld and David E. Wilkins},
  year={1998},
  url={https://api.semanticscholar.org/CorpusID:59656859}
}

@book{russell2016artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2016},
  publisher={Pearson}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

%EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
@article{zala2024envgen,
  title={EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents},
  author={Zala, Abhay and Cho, Jaemin and Lin, Han and Yoon, Jaehong and Bansal, Mohit},
  journal={arXiv preprint arXiv:2403.12014},
  year={2024}
}



%%%%%%%%%%%%%%%%%%%% Misc %%%%%%%%%%%%%%%%%%%%%%

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  pages={1--62},
  year={2022}
}


@book{craik1967nature,
  title={The nature of explanation},
  author={Craik, Kenneth James Williams},
  volume={445},
  year={1967},
  publisher={CUP Archive}
}

@article{10.2307/2529310,
    abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
    author = {J. Richard Landis and Gary G. Koch},
    issn = {0006341X, 15410420},
    journal = {Biometrics},
    number = {1},
    pages = {159--174},
    publisher = {[Wiley, International Biometric Society]},
    title = {The Measurement of Observer Agreement for Categorical Data},
    url = {http://www.jstor.org/stable/2529310},
    urldate = {2023-05-25},
    volume = {33},
    year = {1977}
}


@article{chen2025ecm,
  title={ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model},
  author={Chen, Qiguang and Qin, Libo and Liu, Jinhao and Peng, Dengyun and Wang, Jiaqi and Hu, Mengkang and Chen, Zhi and Che, Wanxiang and Liu, Ting},
  journal={arXiv preprint arXiv:2502.03325},
  year={2025}
}

@misc{chen2024textbfemostextbfembodimentawareheterogeneoustextbfmultirobot,
      title={$\textbf{EMOS}$: $\textbf{E}$mbodiment-aware Heterogeneous $\textbf{M}$ulti-robot $\textbf{O}$perating $\textbf{S}$ystem with LLM Agents}, 
      author={Junting Chen and Checheng Yu and Xunzhe Zhou and Tianqi Xu and Yao Mu and Mengkang Hu and Wenqi Shao and Yikai Wang and Guohao Li and Lin Shao},
      year={2024},
      eprint={2410.22662},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.22662}, 
}

@inproceedings{chen-etal-2024-m3cot,
    title = "{M}$^3${C}o{T}: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought",
    author = "Chen, Qiguang  and
      Qin, Libo  and
      Zhang, Jin  and
      Chen, Zhi  and
      Xu, Xiao  and
      Che, Wanxiang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.446/",
    doi = "10.18653/v1/2024.acl-long.446",
    pages = "8199--8221",
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{qin2024large,
  title={Large language models meet nlp: A survey},
  author={Qin, Libo and Chen, Qiguang and Feng, Xiachong and Wu, Yang and Zhang, Yongheng and Li, Yinghui and Li, Min and Che, Wanxiang and Yu, Philip S},
  journal={arXiv preprint arXiv:2405.12819},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%% Misc [END] %%%%%%%%%%%%%%%%%%%%%%
