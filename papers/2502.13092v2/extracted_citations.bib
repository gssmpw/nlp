@article{banerjee2020can,
  title={Can Transformers Reason About Effects of Actions?},
  author={Banerjee, Pratyay and Baral, Chitta and Luo, Man and Mitra, Arindam and Pal, Kuntal and Son, Tran C and Varshney, Neeraj},
  journal={arXiv preprint arXiv:2012.09938},
  year={2020}
}

@misc{chen2024textbfemostextbfembodimentawareheterogeneoustextbfmultirobot,
      title={$\textbf{EMOS}$: $\textbf{E}$mbodiment-aware Heterogeneous $\textbf{M}$ulti-robot $\textbf{O}$perating $\textbf{S}$ystem with LLM Agents}, 
      author={Junting Chen and Checheng Yu and Xunzhe Zhou and Tianqi Xu and Yao Mu and Mengkang Hu and Wenqi Shao and Yikai Wang and Guohao Li and Lin Shao},
      year={2024},
      eprint={2410.22662},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.22662}, 
}

@article{guan2023leveraging,
  title={Leveraging pre-trained large language models to construct and utilize world models for model-based task planning},
  author={Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={79081--79094},
  year={2023}
}

@article{guan2024world,
  title={World models for autonomous driving: An initial survey},
  author={Guan, Yanchen and Liao, Haicheng and Li, Zhenning and Hu, Jia and Yuan, Runze and Li, Yunjian and Zhang, Guohui and Xu, Chengzhong},
  journal={IEEE Transactions on Intelligent Vehicles},
  year={2024},
  publisher={IEEE}
}

@article{ha2018recurrent,
  title={Recurrent world models facilitate policy evolution},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{ha2018world,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@article{hu2023tree,
  title={Tree-planner: Efficient close-loop task planning with large language models},
  author={Hu, Mengkang and Mu, Yao and Yu, Xinmiao and Ding, Mingyu and Wu, Shiguang and Shao, Wenqi and Chen, Qiguang and Wang, Bin and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2310.08582},
  year={2023}
}

@article{hu2024agentgen,
  title={AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation},
  author={Hu, Mengkang and Zhao, Pu and Xu, Can and Sun, Qingfeng and Lou, Jianguang and Lin, Qingwei and Luo, Ping and Rajmohan, Saravan and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2408.00764},
  year={2024}
}

@article{hu2024hiagent,
  title={Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model},
  author={Hu, Mengkang and Chen, Tianxing and Chen, Qiguang and Mu, Yao and Shao, Wenqi and Luo, Ping},
  journal={arXiv preprint arXiv:2408.09559},
  year={2024}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

@article{lai2024analogcoder,
  title={AnalogCoder: Analog Circuit Design via Training-Free Code Generation},
  author={Lai, Yao and Lee, Sungyoung and Chen, Guojin and Poddar, Souradip and Hu, Mengkang and Pan, David Z and Luo, Ping},
  journal={arXiv preprint arXiv:2405.14918},
  year={2024}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{luo2023towards,
  title={Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models},
  author={Luo, Man and Kumbhar, Shrinidhi and Parmar, Mihir and Varshney, Neeraj and Banerjee, Pratyay and Aditya, Somak and Baral, Chitta and others},
  journal={arXiv preprint arXiv:2310.00836},
  year={2023}
}

@article{mu2024robocodex,
  title={RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis},
  author={Mu, Yao and Chen, Junting and Zhang, Qinglong and Chen, Shoufa and Yu, Qiaojun and Ge, Chongjian and Chen, Runjian and Liang, Zhixuan and Hu, Mengkang and Tao, Chaofan and others},
  journal={arXiv preprint arXiv:2402.16117},
  year={2024}
}

@inproceedings{oswald2024large,
  title={Large Language Models as Planning Domain Generators},
  author={Oswald, James and Srinivas, Kavitha and Kokel, Harsha and Lee, Junkyu and Katz, Michael and Sohrabi, Shirin},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={34},
  pages={423--431},
  year={2024}
}

@article{qin2024large,
  title={Large language models meet nlp: A survey},
  author={Qin, Libo and Chen, Qiguang and Feng, Xiachong and Wu, Yang and Zhang, Yongheng and Li, Yinghui and Li, Min and Che, Wanxiang and Yu, Philip S},
  journal={arXiv preprint arXiv:2405.12819},
  year={2024}
}

@inproceedings{silver2024generalized,
  title={Generalized planning in pddl domains with pretrained large language models},
  author={Silver, Tom and Dan, Soham and Srinivas, Kavitha and Tenenbaum, Joshua B and Kaelbling, Leslie and Katz, Michael},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={18},
  pages={20256--20264},
  year={2024}
}

@article{smirnov2024generating,
  title={Generating consistent PDDL domains with Large Language Models},
  author={Smirnov, Pavel and Joublin, Frank and Ceravola, Antonello and Gienger, Michael},
  journal={arXiv preprint arXiv:2404.07751},
  year={2024}
}

@article{vafa2024evaluating,
  title={Evaluating the World Model Implicit in a Generative Model},
  author={Vafa, Keyon and Chen, Justin Y and Kleinberg, Jon and Mullainathan, Sendhil and Rambachan, Ashesh},
  journal={arXiv preprint arXiv:2406.03689},
  year={2024}
}

@article{wang2023bytesized32,
  title={ByteSized32: A corpus and challenge task for generating task-specific world models expressed as text games},
  author={Wang, Ruoyao and Todd, Graham and Yuan, Eric and Xiao, Ziang and C{\^o}t{\'e}, Marc-Alexandre and Jansen, Peter},
  journal={arXiv preprint arXiv:2305.14879},
  year={2023}
}

@article{wang2023promptagent,
  title={Promptagent: Strategic planning with language models enables expert-level prompt optimization},
  author={Wang, Xinyuan and Li, Chenxi and Wang, Zhen and Bai, Fan and Luo, Haotian and Zhang, Jiayou and Jojic, Nebojsa and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2310.16427},
  year={2023}
}

@article{wang2024can,
  title={Can Language Models Serve as Text-Based World Simulators?},
  author={Wang, Ruoyao and Todd, Graham and Xiao, Ziang and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Clark, Peter and Jansen, Peter},
  journal={arXiv preprint arXiv:2406.06485},
  year={2024}
}

@article{wong2023word,
  title={From word models to world models: Translating from natural language to the probabilistic language of thought},
  author={Wong, Lionel and Grand, Gabriel and Lew, Alexander K and Goodman, Noah D and Mansinghka, Vikash K and Andreas, Jacob and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2306.12672},
  year={2023}
}

@inproceedings{wu2023daydreamer,
  title={Daydreamer: World models for physical robot learning},
  author={Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Abbeel, Pieter and Goldberg, Ken},
  booktitle={Conference on robot learning},
  pages={2226--2240},
  year={2023},
  organization={PMLR}
}

@article{xie2024making,
  title={Making large language models into world models with precondition and effect knowledge},
  author={Xie, Kaige and Yang, Ian and Gunerli, John and Riedl, Mark},
  journal={arXiv preprint arXiv:2409.12278},
  year={2024}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{zhu2024language,
  title={Language Models can Infer Action Semantics for Classical Planners from Environment Feedback},
  author={Zhu, Wang and Singh, Ishika and Jia, Robin and Thomason, Jesse},
  journal={arXiv preprint arXiv:2406.02791},
  year={2024}
}

