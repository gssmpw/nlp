\section{Related Work}
% \paragraph{World Modeling with LLMs.}
% Neural World Modeling
% https://arxiv.org/pdf/2412.03572
% merler2024generating 基于mcts生成code world model
% wong2023word
% li2022emergent leverage transformer model to predict next legal step on a board game...
% tang2024worldcoder construct a code world model via interacting with the environments
% direct application of LLMs for state prediction during planning~\cite{hao2023reasoning,wang2023promptagent}, where benchmarks like ByteSized32-State-Prediction~\cite{wang2024can} have been developed to evaluate LLMs' predictive capabilities. However, this evaluation approach faces significant challenges: exhaustively evaluating all possible state transitions becomes intractable in complex environments, and limited interpretability and high inference latency constrain deeper analysis of LLMs' world modeling capabilities.
Neural world modeling is a long-standing research topic with widespread applications across various fields, including reinforcement learning~\citep{ha2018world, ha2018recurrent}, robotics~\citep{wu2023daydreamer}, and autonomous driving~\citep{guan2024world}, among others.
In recent years, LLMs trained on massive datasets have demonstrated zero-shot capabilities across a variety of tasks, including planning~\cite{zhao2023survey,qin2024large,huang2022language,hu2024hiagent}, robotics~\cite{mu2024robocodex,chen2024textbfemostextbfembodimentawareheterogeneoustextbfmultirobot}, analog design~\cite{lai2024analogcoder}, and more.
% Preliminary studies propose directly using LLMs as world models~\citep{hao2023reasoning,wang2024can,wang2023promptagent,li2022emergent}, where the input consists of the state and action, and the LLM outputs the predicted next state. 
% However, due to the unreliability of LLM outputs and their limited interpretability, this approach can lead to the problem of accumulating errors. 
Preliminary studies propose directly using LLMs as world models~\citep{hao2023reasoning,wang2024can,wang2023promptagent,li2022emergent}, by taking the state and action as input and predicting the next state, but the unreliability and limited interpretability of LLM outputs can lead to accumulating errors.
Moreover, some studies have shown that autoregressive models perform poorly in predicting action effects~\cite{banerjee2020can,luo2023towards}.
Tree-planner~\cite{hu2023tree} instead proposes to constructing the possible action space using LLMs before executing.
Another line of work focuses on leveraging LLMs to construct symbolic world models~\citep{oswald2024large,silver2024generalized,smirnov2024generating,zhu2024language,wang2023bytesized32,wong2023word,vafa2024evaluating}. 
For example, ~\citet{guan2023leveraging} uses LLMs to generate a PDDL domain model and relies on human feedback to correct errors. 
AgentGen~\citep{hu2024agentgen} synthesizes diverse PDDL domains, aiming to create high-quality planning data. 
~\citet{xie2024making} propose to finetune LLMs for predicting precondition and effect of actions.
% ~\citet{vafa2024evaluating} proposes novel evaluation metrics inspired by the Myhill-Nerode theorem to assess the implicit world models within generative models.
% 
Despite the growing interest in this research direction, there is currently a lack of a comprehensive benchmark in this area. 


% \vspace{-10pt}