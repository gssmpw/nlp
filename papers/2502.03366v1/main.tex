%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{titletoc}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}
\input{math_commands.tex}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\definecolor{GoogleGreen}{RGB}{52,168,83}
\definecolor{GoogleYellow}{RGB}{251,188,4}
\definecolor{GoogleBlue}{RGB}{66, 133, 244}
\DeclareRobustCommand{\colorrectangle}[1]{%
    \begin{tikzpicture}[baseline=0ex]
        \draw[#1, very thick, fill=#1] (0, 0.02) rectangle (0.3, 0.17);
    \end{tikzpicture}%
}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[caption=false]{subfig}
\usepackage{wrapfig}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{nicefrac}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Rethinking Approximate Gaussian Inference in Classification}

\begin{document}

\twocolumn[
\icmltitle{Rethinking Approximate Gaussian Inference in Classification}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bálint Mucsányi}{equal,tue}
\icmlauthor{Nathaël Da Costa}{equal,tue}
\icmlauthor{Philipp Hennig}{tue}
\end{icmlauthorlist}

\icmlaffiliation{tue}{Tübingen AI Center, University of Tübingen, Germany}

\icmlcorrespondingauthor{Bálint Mucsányi}{balint.mucsanyi@uni-tuebingen.de}
\icmlcorrespondingauthor{Nathaël Da Costa}{nathael.da-costa@uni-tuebingen.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Deep learning, Uncertainty quantification, Probabilistic modeling, Bayesian inference}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In classification tasks, softmax functions are ubiquitously used as output activations to produce predictive probabilities. Such outputs only capture aleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian inference methods have been proposed, which output Gaussian distributions over the logit space. Predictives are then obtained as the expectations of the Gaussian distributions pushed forward through the softmax. However, such softmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC) approximations can be costly and noisy. We propose a simple change in the learning objective which allows the \emph{exact} computation of predictives and enjoys improved training dynamics, with no runtime or memory overhead. This framework is compatible with a family of output activation functions that includes the softmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for approximating the Gaussian pushforwards with Dirichlet distributions by analytic moment matching. We evaluate our approach combined with several approximate Gaussian inference methods (Laplace, HET, SNGP) on large- and small-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty quantification capabilities compared to softmax MC sampling. Code is available at \href{https://github.com/bmucsanyi/probit}{\texttt{github.com/bmucsanyi/probit}}.
\end{abstract}

\section{Introduction}
\begin{figure}[ht!]
    \centering
    \includegraphics{gfx/synthetic_exp_kl.pdf}
    \caption{Mean KL divergence between the `true' predictive (approximated with a $10000$ sample MC approximation) and different predictive approximations on a synthetic data set. This synthetic dataset consists of 100 logit means $\bm \mu$ and standard deviations $\bm\sigma$, whose components are i.i.d.~uniformly distributed. For a fair comparison we scale the logit dataset for the different activations to match the activations to first order, so that $\mu_c \in [-1,1]$, $\sigma_c\in[0,1]$ for sigmoid, $\mu_c \in [-1/2-\log 2,1/2-\log 2]$, $\sigma_c\in[0,1/2]$ for softmax, $\mu_c\in [-\sqrt{\pi/8},\sqrt{\pi/8}]$, $\sigma_c\in [0,\pi/8]$ for normCDF. The MC approximations are capped at a fixed computational budget of $10000$ class samples, i.e.~$\lceil 10000/C\rceil$ samples. We see that, even without enforcing the constraint \cref{eq:constraint}, our approximations are the only ones that do not become worse with the number of classes. See \cref{app:quality} for theoretical analyses of the quality of the different approximations.}
    \label{fig:synthetic_exp}
\end{figure}
Given an input space $\mathcal X$, $C$ classes and training data $(x_n, c_n)_{n=1}^N \subset \mathcal X\times \{1,\dots, C\}$, the goal of probabilistic classification is to learn a function $\bm h\colon \mathcal X \to \Delta^{C-1}$, where
\begin{equation}
    \Delta^{C-1} = \{(p_1,\dots,p_C)\in [0,1]^C : p_1+\dots+p_C=1\}
\end{equation}
is the $(C-1)$-dimensional probability simplex. The model $\bm h$ outputs the probability of inputs $x\in \mathcal X$ belonging to each class, $h_c(x) = p(c\mid x)$. To learn such a map to the simplex, $\bm h$ is typically defined as a composition
\begin{equation}\label{eq:vanilla_model}
    \bm h\colon \mathcal X \xrightarrow{\quad \bm f \quad} \R^C \xrightarrow{\softmax} \Delta^{C-1}
\end{equation}
where a function $\bm f$, learned from the training data, is mapped through the softmax activation function. In this case, $\R^C$ is called the \emph{logit} space, and, for $x\in\mathcal X$, $\bm f(x)$ is a logit.

With the usual assumption of i.i.d.~data, the likelihood under the model $\bm h$ is given by ($\delta$ is the Kronecker delta)
\begin{equation}
    \begin{aligned}
        p((c_n)_{n=1}^N\mid (x_n)_{n=1}^N) &= \prod_{n=1}^N p(c_n\mid x_n) \\
        &= \prod_{n=1}^N \prod_{c=1}^C h_c(x_n)^{\delta_{c_n,c}}.
    \end{aligned}
\end{equation}
This yields a natural loss function for classification problems, the cross-entropy (CE) loss
\begin{equation}
    \begin{aligned}
        \mathcal L((x_n, c_n)_{n=1}^N) &= -\log(p((c_n)_{n=1}^N\mid (x_n)_{n=1}^N)) \\
        &= -\sum_{n=1}^N\sum_{c=1}^C \delta_{c_n,c} \log(h_c(x_n)).
    \end{aligned}
\end{equation}
For typical models $\bm h$ trained through such approximate maximum likelihood estimation, an output probability vector $\bm h(x)\in\Delta^{C-1}$ should be interpreted as an estimate of $p_{\text{gen}}(\bm y\mid  x)$, the probability under the generative model. Notably, the output probability does not take into account the uncertainty of the model due to the finite nature of the data. In the uncertainty quantification formalism, such models can only estimate \emph{aleatoric uncertainty} and disregard \emph{epistemic uncertainty}~\cite{hullermeier2021aleatoric}.

The probabilistic way to capture epistemic uncertainty is to require the model to output a \emph{second-order distribution} (as in ``a distribution over probability distributions''). That is, for each input $x\in \mathcal X$, the model should output a probability measure over the simplex $\mathrm h(x)\in \mathcal P(\Delta^{C-1})$, as opposed to a point estimate in the simplex $\bm h(x)\in\Delta^{C-1}$~\cite{sensoy2018evidential,charpentier2020posterior}.

A number of methods for such distributional uncertainty quantification in classification rely on approximate Gaussian inference in logit space. That is, $\mathrm h$ is written as a composition
\begin{equation}\label{eq:gaussian_inference_softmax_model}
    \mathrm h\colon \mathcal X \xrightarrow{\quad \mathrm f \quad} \mathcal G(\R^C) \xrightarrow{\softmax_*} \mathcal P(\Delta^{C-1})
\end{equation}
where $f$ is learned from the training data, $\mathcal G(\R^C)$ is the set of Gaussian measures on the logit space $\R^C$, and $\softmax_*$ pushes forward the Gaussian probability measures through the softmax. For example, Heteroscedastic Classifiers (HET) \cite{collier2021correlated} learn the output means and covariances of $\mathrm f$ explicitly with a neural network. In linearised Laplace approximations \cite{daxberger2021laplace}, only the means are learned explicitly with a neural network, while the covariances are obtained post-hoc from that neural network by approximating its parameter posterior distribution with a Gaussian, and then pushing it forward to logit space by linearising the network in the parameters. Last-layer Laplace approximations \cite{kristiadi_being_2020} are a variant of linearised Laplace approximations where only the posterior over the last-layer parameters is Gaussian-approximated, requiring no linearisation of the network. Spectral-Normalized Gaussian processes (SNGP) \cite{liu2020simple} are neural networks with last layers that are approximately Gaussian processes, through random feature expansions and Laplace approximations.

To obtain predictive probabilities from a distributional classifier as in \cref{eq:gaussian_inference_softmax_model}, we marginalise it out by the measure-theoretic change of variables, using $\mathrm h(x) = \softmax_*\mathrm f(x)$:
\begin{equation}\label{eq:gaussian_softmax_predictive}
    \begin{aligned}
        \E_{\bm P\sim \mathrm h(x)}[\bm P] & = \int_{\Delta^{C-1}} p\;\mathrm h(x)(dp) \\
        &= \int_{\R^C} \softmax(y)\; \mathrm f(x)(dy).
    \end{aligned}
\end{equation}
In addition to predictive probabilities, the distributional framework allows for the acquisition of other quantities of interest, such as variances, entropy, and information-theoretic decompositions of the predictive uncertainty into aleatoric and epistemic parts~\cite{depeweg2018decomposition,mukhoti2023deep,wimmer2023quantifying}. These are usually obtained by Monte Carlo (MC) sampling from the Gaussian measure $\mathrm f(x)$. Indeed, while the probability density of $\mathrm h(x) = \softmax_*\mathrm f(x)$ can be obtained in closed form using a change of variables formula \cite{atchison_logistic-normal_1980}, this does not seem of practical use. Even predictives cannot be computed analytically. On the other hand, given a variance, the runtime and memory cost required for an MC approximation to achieve an estimator of said variance grows linearly with the number of classes. Crucially, this computational cost is not limited to \emph{training time}, but also to \emph{inference time}, when the computational budget may be much more limited. This limits the application of such Gaussian inference methods in classification tasks with large number of classes, such as those common in computer vision and natural language processing.
\subsection{Proposed Recipe}
We can summarise our framework as a 4-step recipe:
\begin{enumerate}
    \item Choose an approximate Gaussian inference method: a method that outputs logit covariances.
    \item Choose an output activation for which the corresponding Gaussian integral can be solved (\cref{sec:predictives}).
    \item Train with an objective that enforces a certain normalisation constraint (\cref{sec:learning_the_constraint}).
    \item At inference time, use our analytic formulas (see \cref{sec:predictives} for the predictives, \cref{sec:distributions} for other quantities of interest).
\end{enumerate}
\section{Gaussian Inference with Analytic Predictives}\label{sec:predictives}
\subsection{Motivation from Softmax Models}\label{sec:motivation}
A key difficulty in obtaining exact or closed-form approximate predictive probabilities of $\mathrm h(x) = \softmax_*\mathrm f(x)$ is that pushforwards of Gaussian distributions through the softmax are intractable. The softmax function is the composition $\bm \softmax=\bm n \circ \bm \exp$, where $\bm n$ is the normalisation function given by $\bm n(\bm q):= \bm q/(q_1+\dots+q_C)$ and $\bm{\exp}$ is applied element-wise. Thus, $\mathrm h$ can be written as the composition (c.f.~\cref{eq:gaussian_inference_softmax_model})
\begin{equation}\label{eq:gaussian_inference_softmax_model decomposed}
    \mathrm h\colon \mathcal X \xrightarrow{\quad \mathrm f \quad} \mathcal G(\R^C) \xrightarrow{\bm\exp_*} \mathcal P(\R_{>0}^C) \xrightarrow{\bm n_*} \mathcal P(\Delta^{C-1}).
\end{equation}
Now it is easy to see that (\cref{app:gaussian_exp_integral})
\begin{equation}\label{eq:gaussian_exp_integral}
    \int_{\R}\exp(y)\;\mathcal N(\mu,\sigma^2)(dy) = \exp\left(\mu+\frac{\sigma^2}{2}\right)
\end{equation}
and thus
\begin{equation}\label{eq:exact_exp_predictive}
    \begin{aligned}
        \E_{\bm Q\sim \bm \exp_*\mathrm f(x)}[\bm Q] &= \int_{\R^C}\bm \exp(\bm y) \; \mathrm f(x)(d\bm y) \\
        &= \bm \exp\left(\bm\mu(x)+\frac{\bm\sigma^2(x)}{2}\right)
    \end{aligned}
\end{equation}
where $\bm \mu(x)$ and $\bm \sigma^2(x)$ are, respectively, the mean and the diagonal of the covariance of $\mathrm f(x)$, and the vector division is element-wise. Note that no approximations are made so far.

We can use this to approximate the predictive of the model in closed form:
\begin{equation}\label{eq:approximate_exp_predictive}
    \begin{aligned}
        \E_{\bm P\sim \mathrm h(x)}[\bm P] &=  \E_{\bm Q\sim \bm \exp_*\mathrm f(x)}\left[\frac{\bm Q} {\sum_{c=1}^CQ_c}\right] \\
        &\approx \frac{\E_{\bm Q\sim \bm \exp_*\mathrm f(x)}[\bm Q]}{\E_{\bm Q\sim \bm \exp_*\mathrm f(x)}\left[\sum_{c=1}^CQ_c\right]} \\
        &= \frac{\E_{\bm Q\sim \bm \exp_*\mathrm f(x)}[\bm Q]} {\sum_{c=1}^C\E_{Q_c\sim  \exp_*\mathrm f_c(x)}\left[Q_c\right]} \\
        &= \frac{\bm \exp\left(\bm\mu(x)+\frac{\bm\sigma^2(x)}{2}\right)}{\sum_{c=1}^C\exp\left(\mu_c(x)+\frac{\sigma^2_c(x)}{2}\right)}.
    \end{aligned}
\end{equation}

\subsection{The General Framework}\label{sec:general_framework}
In fact, we can apply the previous argumentation to a general family of output activation functions: suppose we can write the output activation as $\bm n\circ\bm\varphi$, where $\bm\varphi$ is the element-wise application of some activation function $\varphi\colon \R\to \R_{>0}$. So we have (c.f.~\cref{eq:gaussian_inference_softmax_model decomposed})
\begin{equation}\label{eq:gaussian_inference_model}
    \mathrm h\colon \mathcal X \xrightarrow{\quad \mathrm f \quad} \mathcal G(\R^C) \xrightarrow{\bm\varphi_*} \mathcal P(\R_{>0}^C) \xrightarrow{\bm n_*} \mathcal P(\Delta^{C-1}).
\end{equation}
If we can solve the one-dimensional integrals (c.f.~\cref{eq:gaussian_exp_integral})
\begin{equation}\label{eq:gaussian_integral}
    \int_{\R}\varphi(y)\;\mathcal N(\mu,\sigma^2)(dy)
\end{equation}
then we can compute the approximate predictive (c.f.~\cref{eq:approximate_exp_predictive})
\begin{equation}\label{eq:approximate_predictive}
    \begin{aligned}
        \E_{\bm P\sim \mathrm h(x)}[\bm P]\approx \frac{\E_{\bm Q\sim \bm \varphi_*\mathrm f(x)}[\bm Q]}{\sum_{c=1}^C\E_{Q_c\sim  \varphi_*\mathrm f_c(x)}\left[Q_c\right]}.
    \end{aligned}
\end{equation}
For instance, taking $\varphi =\Phi$, the standard Normal cumulative distribution function (normCDF)\footnote{The resulting model is distinct from a multinomial probit model, see \cref{app:probit_model}.}, we have by a classical result (\cref{app:gaussian_normcdf_integral}),
\begin{equation}\label{eq:gaussian_normcdf_integral}
    \int_{\R}\Phi(y)\;\mathcal N(\mu,\sigma^2)(dy) = \Phi\left(\frac{\mu}{\sqrt{1+\sigma^2}}\right)
\end{equation}
and thus
\begin{equation}\label{eq:approximate_normcdf_predictive}
    \begin{aligned}
        &\E_{\bm P\sim \mathrm h(x)}[\bm P] \approx \frac{\bm \Phi\left(\frac{\bm \mu(x)}{\sqrt{1+\bm \sigma^2(x)}}\right)} {\sum_{c=1}^C\Phi\left(\frac{\mu_c(x)}{\sqrt{1+\sigma^2_c(x)}}\right)}.
    \end{aligned}
\end{equation}

Taking instead $\varphi =\rho$, the logistic sigmoid, we can approximate \cref{eq:gaussian_integral} with the probit approximation (\cref{app:gaussian_sigmoid_integral})
\begin{equation}\label{eq:gaussian_sigmoid_integral}
    \int_{\R}\rho(y)\;\mathcal N(\mu,\sigma^2)(dy) \approx \rho\left(\frac{\mu}{\sqrt{1+\frac{\pi}{8}\sigma^2}}\right)
\end{equation}
yielding in this case
\begin{equation}\label{eq:approximate_sigmoid_predictive}
    \begin{aligned}
        \E_{\bm P\sim \mathrm h(x)}[\bm P]\approx \frac{\bm \rho\left(\frac{\bm \mu(x)}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2(x)}}\right)}{ \sum_{c=1}^C\rho\left(\frac{\mu_c(x)}{\sqrt{1+\frac{\pi}{8}\sigma^2_c(x)}}\right)}.
    \end{aligned}
\end{equation}

\subsection{Making the Approximations Exact}\label{sec:making_approx_exact}
At this point, the approximate \cref{eq:approximate_predictive} may seem ad-hoc. However, note that \cref{eq:approximate_predictive} is exact when $\sum_{c=1}^C Q_c$ is a constant random variable, as can be seen from the derivation in \cref{eq:approximate_exp_predictive}. Thus, we would like to enforce a constraint in the model of the form
\begin{equation}\label{eq:constraint}
    \begin{aligned}
            \sum_{c=1}^C Q_c&=s(x) &\text{where }\bm Q\sim \bm\varphi_*\mathrm f(x) 
    \end{aligned}
\end{equation}
where $s\colon \mathcal X \to \R$ is a deterministic function. In other words, the law of $\sum_{c=1}^C Q_c$ is a Dirac measure for all $x$.

In neural networks, constraints can be learned through appropriate choices of learning objectives during training, ensuring that they are satisfied within the training data distribution. In \cref{sec:learning_the_constraint}, we describe how this can be achieved depending on $\bm\varphi$ and the method that is used to obtain the mapping to Gaussian distributions $\mathrm f$.

Given the constraint of~\cref{eq:constraint} is satisfied, \cref{eq:approximate_exp_predictive,eq:approximate_normcdf_predictive} ($\varphi = \exp\text{ or } \Phi$) both become \emph{exact}. For \cref{eq:approximate_sigmoid_predictive} ($\varphi = \rho$), the only error incurred is the one from the probit approximation  (\cref{eq:gaussian_sigmoid_integral}) per each class. In the synthetic experiment (\cref{fig:synthetic_exp}), we see that this induces no significant effect on the quality of the predictive. We justify this observation with the following result:
\begin{theorem}\label{}
    Let $\varphi = \rho$. Suppose that the logit means and variances $(\mu_c,\sigma^2_c)$ lie in some compact set $\mathcal K \subset \R\times\R_{>0}$ for each class $c$. Then, assuming the constraint \cref{eq:constraint} is perfectly satisfied, the KL divergence between the true and approximate predictive \cref{eq:approximate_sigmoid_predictive} is bounded by some constant $M(\mathcal K)$ only depending on $\mathcal K$, not on $C$.
\end{theorem}
\cref{app:probit_quality} provides a more precise statement, a formula for $M(\mathcal K)$, and a proof. Importantly, this result states that the approximation error does not increase with the number of classes.

Finally, observe that, when enforcing \cref{eq:constraint}, we obtain exact predictives with \cref{eq:approximate_predictive}, which only uses the diagonal of $\mathrm f(x)$'s covariance. Enforcing this constraint thus allows one to ignore non-diagonal terms in the logit covariances. In \cref{sec:computational_gains}, we discuss how this can be leveraged for computational gains.
\section{Distributional Approximations of the Gaussian Pushforwards}\label{sec:distributions}

\subsection{Dirichlet Matching}\label{sec:dirichlet_matching}
While in many applications, predictive probabilities are all that is needed from a model, we set out to build a model that outputs a tractable probability distribution over the simplex $\mathrm h(x)\in \mathcal P(\Delta^{C-1})$. As previously discussed, such a distribution allows, for instance, closed-form decompositions of aleatoric and epistemic uncertainties.

The formulation of \cref{eq:gaussian_inference_model} does not yet allow this. However, we will now see that we can obtain good Dirichlet approximations by constructing a tractable approximation of \cref{eq:gaussian_inference_model} of the form
\begin{equation}\label{eq:dirichlet_model}
    \mathrm h\colon \mathcal X \xrightarrow{\quad \mathrm f \quad} \mathcal G(\R^C) \xrightarrow{\quad \mathrm a \quad} \mathcal D(\Delta^{C-1}),
\end{equation}
where $\mathcal D(\Delta^{C-1})$ is the space of Dirichlet distributions on $\Delta^{C-1}$. The family of Dirichlets presents an ideal choice of distributions over the simplex as, for such distributions, all quantities of interest can be computed analytically (c.f.~\cref{app:list_estimators}). The map $\mathrm a$ is constructed by moment matching: we match the first two moments of the Dirichlet distribution to the moments of the Gaussian pushforward $\bm\varphi_*\mathrm f(x)$.

Conveniently, just as the first moments described in \cref{sec:predictives}, we can obtain second moments for $\exp$
\begin{equation}\label{eq:gaussian_exp2_integral}
    \int_{\R}\exp(y)^2\;\mathcal N(\mu,\sigma^2)(dy) = \exp(2\mu+2\sigma^2)
\end{equation}
and for $\Phi$ (\citet[Eq. 20,010.4]{owen_table_1980})
\begin{equation}\label{eq:gaussian_normcdf2_integral}
    \begin{aligned}
        &\int_{\R}\Phi(y)^2\;\mathcal N(\mu,\sigma^2)(dy) \\
        &\quad = \Phi\left(\frac{\mu}{\sqrt{1+\sigma^2}}\right) - 2 T\left(\frac{\mu}{\sqrt{1+\sigma^2}}, \frac{1}{\sqrt{1+2\sigma^2}}\right),
    \end{aligned}
\end{equation}
where $T$ is Owen's T function. For $\rho$, we have the following approximation (\citet[Equation 23]{daunizeau_semi-analytical_2017})
\begin{equation}\label{eq:gaussian_sigmoid2_integral}
    \begin{aligned}
        &\int_{\R}\rho(y)^2\;\mathcal N(\mu,\sigma^2)(dy) \approx \rho\left(\frac{\mu}{\sqrt{1+\frac{\pi}{8}\sigma^2}}\right) \\
        &- \frac{\rho\left(\frac{\mu}{\sqrt{1+\frac{\pi}{8}\sigma^2}}\right)\left(1-\rho\left(\frac{\mu}{\sqrt{1+\frac{\pi}{8}\sigma^2}}\right)\right)}{\sqrt{1+\frac{\pi}{8}\sigma^2}}.
    \end{aligned}
\end{equation}

This enables us to compute the second moment of the pushforward distribution via
\begin{equation}\label{eq:approximate_second_moment}
    \begin{aligned}
        \E_{\bm P\sim \bm n_*\bm \varphi_* \mathrm f(x)}[\bm P^2] &= \E_{\bm Q\sim \bm \varphi_*\mathrm f(x)}\left[\frac{\bm Q^2} {\left(\sum_{c=1}^CQ_c\right)^2}\right] \\
        &\approx \frac{\E_{\bm Q\sim \bm \varphi_*\mathrm f(x)}[\bm Q^2]}{\E_{Q_c\sim \varphi_*\mathrm f_c(x)}\left[\sum_{c=1}^CQ_c\right]^2}.
    \end{aligned}
\end{equation}
As in \cref{eq:approximate_predictive}, the approximation in \cref{eq:approximate_second_moment} is exact when $\sum_{c=1}^C Q_c$ is a constant random variable, as under the constraint of \cref{eq:constraint}.

If we were to directly attempt to infer the parameters of the Dirichlet $\mathrm h(x)$ from \cref{eq:approximate_predictive,eq:approximate_second_moment}, we would obtain an overparametrised system of equations with $2C$ equations and $C$ unknowns. Thus, we instead use a classical Dirichlet method of moments \citep[Eqs. 19 \& 23]{minka_2000}, which reduces these equations, as we will now describe.

The sum of the Dirichlet parameters $\sum_{c=1}^C\gamma_c$ may be estimated by
\begin{equation}\label{eq:estimate_sum_dirichlet_parameters}
    \frac{\E[P_c]-\E[P^2_c]}{\E[P^2_c]-\E[P_c]^2}
\end{equation}
for any $1\leq c\leq C$. So, a natural estimate of $\sum_{c=1}^C\gamma_c$ that uses all $P_c$ is the geometric mean of \cref{eq:estimate_sum_dirichlet_parameters}. Since the mean of the Dirichlet is $\bm\gamma / \sum_{c=1}^C \gamma_c$, we obtain using \cref{eq:estimate_sum_dirichlet_parameters} an expression for the Dirichlet parameters
\begin{equation}
    \left(\prod_{c=1}^C \frac{\E[P_c]-\E[P^2_c]}{\E[P^2_c]-\E[P_c]^2}\right)^{1/C}\E[\bm P].
\end{equation}
Hence, using \cref{eq:approximate_predictive,eq:approximate_second_moment}, we obtain the computationally feasible Dirichlet parameters
\begin{equation}\label{eq:dirichlet_parameters}
    \bm\gamma := \left(\prod_{c=1}^C\frac{\E[Q_c]\cdot S-\E[Q^2_c]}{\E[Q^2_c]-\E[Q_c]^2}\right)^{1/C}\left(\frac{\E[\bm Q]}{\sum_{c=1}^C\E[Q_c]}\right)
\end{equation}
for $\mathrm h(x)$, where $S = \max(\sum_{c=1}^C\E[Q_c], 1)$. Taking the maximum with $1$ in the expression for $S$ is an additional approximation to ensure $\bm \gamma \geq \bm 0$, and is justified when taking $s(x) = 1$ for all $x$ in \cref{eq:constraint}.
Note that performing such moment matching to obtain the Dirichlet parameters in \cref{eq:dirichlet_parameters}, the mean of this Dirichlet matches precisely the approximate predictive \cref{eq:approximate_predictive}. That is, \emph{the approximate predictive of the exact distributional model }(\cref{eq:gaussian_inference_model})\emph{ matches the exact predictive of the approximate distributional model} (\cref{eq:dirichlet_model}).

\subsection{Intermediate Beta Matching}\label{sec:beta_matching}
\begin{figure}[t]
    \centering
    \includegraphics{gfx/beta_approx.pdf}
    \caption{Approximating Gaussian pushforwards $\Phi_*\mathrm f(x)$ and $\rho_*\mathrm f(x)$ through normCDF and sigmoid respectively by Beta distributions $\mathrm p(\mathrm f(x))$ with moment matching. With normCDF, we can match exact moments, whereas with sigmoid, we match approximate moments. Moreover, when $\mathrm f(x)$ is the standard Normal $\mathcal N(0,1)$ (on the left in blue), the approximation for the pushforward through normCDF is exact (\cref{app:information_geometry}).}
    \label{fig:beta_approx}
\end{figure}
When $\varphi = \Phi \text{ or } \rho$, noting that their range is $(0,1)$, we can decompose the approximate distributional model (\cref{eq:dirichlet_model}) further:
\begin{equation}\label{eq:beta_dirichlet_model}
    \mathrm h\colon \mathcal X \xrightarrow{\quad \mathrm f \quad} \mathcal G(\R^C) \xrightarrow{\mathrm p} \mathcal B((0,1))^C \xrightarrow{\mathrm n} \mathcal D(\Delta^{C-1}),
\end{equation}
where $\mathcal B((0,1))$ is the space of Beta distributions on $(0,1)$. Like the output Dirichlets, we obtain the intermediate Beta distributions by moment matching (\cref{app:beta_matching}). \Cref{fig:beta_approx} indicates that such Beta approximations tend to be of high quality. In \cref{app:information_geometry}, we give an information-geometric motivation for this, which shows that the choice of activation $\Phi$ is ideal for approximating the Gaussian pushforwards with Beta distributions.

The quality of such Beta approximations is encouraging for that of the output Dirichlet approximations: observe that taking $s(x)=1$ for all $x$ in \cref{eq:constraint}, $\bm n$ becomes the identity (as it normalises by 1), and thus all $\mathrm n$ does is combining the $C$ Beta approximations into one Dirichlet.

\section{Learning the Constraint}\label{sec:learning_the_constraint}

As discussed in \cref{sec:making_approx_exact,sec:distributions}, the quality of our approximations relies on the enforcement of the constraint $\sum_{c=1}^CQ_c=s(x)$ for some deterministic function $s\colon \mathcal X \to \R$ (\cref{eq:constraint}). In this work, we simply choose $s(x)=1$ for all $x$. We thus make use of the degree of freedom in the logits to obtain exact predictives in \cref{eq:approximate_predictive}. In \cref{app:energy_model}, we make the connection with the energy-based model view of classifiers \cite{grathwohl_your_2019}, which instead makes use of this degree of freedom to turn discriminative models into generative models. Future work could combine both frameworks by learning $s(x)\propto p_{\operatorname{gen}}(x)$.

\subsection{Explicit Constraint}
\label{sec:through_regularisation}

Up to this point, we have treated the model $\mathrm f$ as a black box. However, to discuss how to impose~\cref{eq:constraint}, we need to think about the specifics of how such models are trained and how Gaussian distributions are then obtained.

In linearised Laplace approximations and SNGP, a neural network $\bm f\colon \mathcal X \to \R^C$ is trained for the mean of $\mathrm f$, while the covariance is obtained post-hoc. Therefore, to enforce \cref{eq:constraint} on the training data distribution, we can train for $\sum_{c=1}^C \varphi(f_c(x_n)) = 1$ for all $1\leq n\leq N$.

This can be achieved, for example, by adding an appropriate regulariser to the loss. Namely,
\begin{equation}\label{eq:regularised_loss}
    \begin{aligned}
        &\mathcal L((x_n, c_n)_{n=1}^N) \\
        &\quad =\tilde{\mathcal L}((x_n, c_n)_{n=1}^N)+ \lambda\sum_{n=1}^N\left(\sum_{c=1}^C \varphi(f_c(x_n))-1\right)^2
    \end{aligned}
\end{equation}
for some tuned regularisation constant $\lambda$, where $\tilde{\mathcal L}$ is the original unregularised loss.

In contrast to the previously mentioned methods, HET directly trains not only for the mean $\bm f$ but also for the covariance matrix $\bm\Sigma$ of $\mathrm f$ through the predictives. Thus, in this case, we can regularise with the means of the Gaussian pushforwards through $\varphi$:
\begin{equation}
    \begin{aligned}
        \mathcal L((x_n, c_n)_{n=1}^N) &=\tilde{\mathcal L}((x_n, c_n)_{n=1}^N) \\
        &+ \lambda\sum_{n=1}^N\left(\sum_{c=1}^C \E_{Q_c\sim\varphi_*\mathrm f_c(x)}[Q_c]-1\right)^2.
    \end{aligned}
\end{equation}
In \cref{sec:experiments}, we will see that training with such a learning objective does not always outperform vanilla CE training. For $\varphi=\exp$, for instance, we observe that the regulariser can take on extreme values, dominating the data term, slowing down training.

We shall now see that, when $\varphi$ has range $(0,1)$, we can enforce the constraint with the learning objective in a more natural, implicit way.

\subsection{Implicit Constraint}
\label{sec:through_classwise_ce_losses}

In the case of $\varphi=\Phi \text{ or } \rho$, $\sum_{c=1}^C \varphi(f_c(x_n)) = 1$ can be achieved more advantageously: since the range of $\Phi$ and $\rho$ is $(0,1)$, under the condition $\sum_{c=1}^C \varphi(f_c(x_n)) = 1$, we can view $\varphi(f_c(x_n))$ as estimates of $p(c\mid x_n)$. This motivates the use of \emph{class-wise cross-entropy losses} (class-wise CE).

For linearised Laplace approximations and SNGP, which solely train a mean $\bm f$, the class-wise CE is given by
\begin{equation}\label{eq:classwise_loss}
    \begin{aligned}
        \mathcal L((x_n, c_n)_{n=1}^N) &= -\sum_{n=1}^N\sum_{c=1}^C \big(\delta_{c_n,c}\log(\varphi(f_c(x))) \\
        &\quad+ (1-\delta_{c_n,c})\log(1-\varphi(f_c(x))\big).
    \end{aligned}
\end{equation}
The labels being one-hot vectors and the binary cross-entropy loss being a (negative) strictly proper scoring rule (c.f.~\cref{app:benchmark_tasks}) together ensure that \cref{eq:classwise_loss} enforces the soft constraint $\sum_{c=1}^C \varphi\circ \mu_c=1$ on the training data, and hence in the training data distribution (\cref{fig:constraint}).

We note that the loss \cref{eq:classwise_loss} is only used for training and not for inference or evaluation. Indeed, methods such as Laplace approximations and SNGP -- which rely on negative log-likelihood losses to obtain an approximate Gaussian distribution -- can then use the actual log-likelihood loss of the model, that is, the cross-entropy loss of $\bm n\circ \bm \varphi\circ \bm f$.

Similarly, for HET, we use the class-wise CE with the pushforward means
\begin{equation}\label{eq:het_classwise_loss}
    \begin{aligned}
        \mathcal L((x_n, c_n)_{n=1}^N) &= -\sum_{n=1}^N\sum_{c=1}^C \big(\delta_{c_n,c}\log(\E_{Q_c\sim\varphi_*\mathrm f_c(x)}[Q_c]) \\
        &+ (1-\delta_{c_n,c})\log(1-\E_{Q_c\sim\varphi_*\mathrm f_c(x)}[Q_c])\big).
    \end{aligned}
\end{equation}
Enforcing the constraint in this implicit way is powerful since, in contrast to the regularised CE objective (\cref{eq:regularised_loss}), the class-wise CE (\cref{eq:classwise_loss}) fully aligns with the task. In \cref{sec:experiments} we shall see that training on class-wise CE generally outperforms softmax CE training.
\begin{figure}[t]
    \centering
    \includegraphics{gfx/constraint.pdf}
    \caption{Evolution of the constraint on validation data for normCDF models trained on ImageNet.}
    \label{fig:constraint}
\end{figure}

\section{Computational Gains}\label{sec:computational_gains}
The runtime cost of the MC predictive approximation is $\mathcal O(S\cdot C+C^3)$, and the memory cost is $\mathcal O(S\cdot C+C^2)$, where $S$ is the number of samples. The $C^2$ term in memory comes from the logit covariance matrix; the $C^3$ term in runtime comes from the need to Cholesky-decompose such covariance matrix before sampling. For example, on a ResNet-50, sampling one thousand logit vectors from the logit-space Gaussian means an approximately 7\% overhead on the forward pass. In embedded systems with much smaller models, this overhead is even larger. In contrast, the runtime and memory cost of calculating a predictive through our framework is $\mathcal O(C)$, which is negligible in comparison.

Indeed, as noted at the end of \cref{sec:making_approx_exact}, the constraint (\cref{eq:constraint}) ensures that only the mean and \emph{the diagonal} of the logit covariance are needed in order to calculate the predictive exactly. That is, the logit quantities required to compute a predictive scale as $\mathcal O(C)$ instead of $\mathcal O(C^2)$. This provides further opportunities for computational savings in the methods used to obtain the Gaussian distributions.

In HET, for example, the logit covariance is trained with both a diagonal term and a low-rank term. Our framework thus allows one to drop the low-rank term without losing any expressivity in the model.

In SNGP, the off-diagonal terms of the logit covariance are ignored~\citep[p.~47]{JMLR:v24:22-0479}. For unconstrained models, this incurs an approximation error as the inter-class covariance structure can encode vital information for inference. When \cref{eq:constraint} is satisfied, however, we are \emph{guaranteed} not to lose expressivity for the predictive computation by discarding the off-diagonal covariance terms, theoretically backing the approximation in SNGP.

% In last layer Laplace approximations, the last layer parameters are equipped with Gaussian distributions. Explicitly, the logits are obtained by $\bm f(x) = \bm W \bm g(x)+\bm b$, where $\bm g(x) \in \R^D$ are the penultimate layer features, $\bm W\in \R^{C\times D}$ and $\bm b \in \R^C$. Last layer Laplace approximations yield $\bm w\sim \mathcal N(\bm\mu_{\bm w},\bm\Sigma_{\bm w})$, $\bm b\sim \mathcal N(\bm\mu_{\bm b},\bm\Sigma_{\bm b})$, where $\bm w\in \R^{C\cdot D}$ is the row-wise vectorisation of $\bm W$. So for $x\in \mathcal X$ we obtain $\mathrm f(x) = \mathcal N(\bm R_{\bm g(x)}\bm\mu_{\bm w} +\bm\mu_{\bm b}, \bm R_{\bm g(x)}\bm\Sigma_{\bm w}\bm R_{\bm g(x)}^\top +\bm\Sigma_{\bm b})$ where $\bm R_{\bm g(x)} = \bm g(x)^\top\otimes I \in \R^{C\times C\cdot D}$ is the matrix corresponding to right multiplication by $\bm g(x)$.
\section{Experiments}\label{sec:experiments}

We now investigate our two research questions:
\begin{itemize}
    \item Do we have to sacrifice performance for sample-free predictives?
    \item What are the effects of changing the learning objective?
\end{itemize}
To answer the first question, we consider fixed (method, activation) pairs and verify that our methods perform on par with the Monte Carlo sampled predictives in~\cref{sec:quality_of_sample_free_predictives}.

We train four classes of models: \textbf{Softmax} ($\varphi = \exp$, trained with vanilla   CE), \textbf{Exp} ($\varphi=\exp$, trained with the explicit constraint (\cref{sec:through_regularisation})), \textbf{NormCDF} ($\varphi=\Phi$, trained with the implicit constraint, (\cref{sec:through_classwise_ce_losses})) and \textbf{Sigmoid} ($\varphi=\rho$, trained with the implicit constraint, (\cref{sec:through_classwise_ce_losses})).

We consider \textbf{Heteroscedastic Classifiers (HET)}~\cite{collier2021correlated}, \textbf{Spectral-Normalized Gaussian Processes (SNGP)}~\cite{liu2020simple}, and last-layer \textbf{Laplace approximation} methods~\cite{daxberger2021laplace} as backbones. The resulting 18 (method, activation, predictive) triplets are evaluated on ImageNet-1k \citep{deng2009imagenet} and CIFAR-10 \citep{krizhevsky2009learning} on five metrics aligning with practical needs from uncertainty estimates~\cite{mucsanyi2023trustworthy}. Further, we test the Out-of-Distribution (OOD) detection capabilities of the models on balanced mixtures of In-Distribution (ID) and OOD samples. For ImageNet, we treat ImageNet-C~\citep{hendrycks2019benchmarking} samples with 15 corruption types and 5 severity levels as OOD samples. For CIFAR-10, we use the CIFAR-10C corruptions.

For the second question, we evaluate our analytic predictives and moment-matched Dirichlets against softmax models with approximate inference tools -- Laplace Bridge~\cite{pmlr-v180-hobbhahn22a} and Mean Field~\cite{lu_mean-field_2021} approximations, as well as MC sampling, in \cref{sec:changing_the_learning_objective}.

To provide a fair comparison, we reimplement each method as simple-to-use wrappers around deterministic backbones. For ImageNet evaluation, we use a ResNet-50 backbone pretrained with the softmax activation function, and train each (method, activation) pair for 50 ImageNet-1k epochs following~\citet{mucsányi2024benchmarking}. On CIFAR-10, we train ResNet-28 models from scratch for 100 epochs. We search for ideal hyperparameters and checkpoints with a ten-step Bayesian Optimization scheme~\cite{shahriari2015taking} in Weights \& Biases~\cite{wandb}.

The main paper focuses on ImageNet results to highlight the scalability of our framework and only shows proper scoring results for CIFAR-10. For a complete overview of CIFAR-10 results, refer to~\cref{app:cifar_10}.


In all plots of the main paper and the Appendix, the error bars are calculated over five independent runs with different seeds and show the minimum and maximum performance.

\begin{wraptable}{r}{0.4\columnwidth}
\tiny
\caption{Comparison of ECE results for different predictives using a fixed Laplace backbone.}
\label{tab:imagenet_pred_probs}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Mean} & \textbf{Std} \\
\midrule
\multicolumn{3}{c}{\textbf{Sigmoid Laplace}} \\
\midrule
Analytic & 0.0101 & 0.0006 \\
MC 1000 & 0.0153 & 0.0011 \\
MC 100 & 0.0153 & 0.0012 \\
MC 10 & 0.0165 & 0.0014 \\
\midrule
\multicolumn{3}{c}{\textbf{NormCDF Laplace}} \\
\midrule
MC 10 & 0.0112 & 0.0005 \\
Analytic & 0.0114 & 0.0007 \\
MC 100 & 0.0115 & 0.0006 \\
MC 1000 & 0.0118 & 0.0007 \\
\bottomrule
\end{tabular}
\end{wraptable}

\subsection{Quality of Sample-Free Predictives}
\label{sec:quality_of_sample_free_predictives}

In this section, we investigate our first research question: whether there is a price to pay for sample-free predictives and second-order distributions. To this end, we take the two best-performing methods on the Expected Calibration Error (ECE) metric~\cite{naeini2015obtaining}, NormCDF and Sigmoid Laplace (c.f.~\cref{fig:imagenet_ece}) and evaluate their per-predictive performance. As \cref{tab:imagenet_pred_probs} shows, the analytic predictive is always on par with the (unbiased) MC estimates on ImageNet while being cheaper. This empirical observation supports our theoretical claim that when the constraint is satisfied, the analytic predictives are exact. Refer to~\cref{app:dirichlet} for an evaluation of the moment-matched Dirichlet distributions.

% \begin{table}[h]
% \tiny
% \caption{Comparison of ECE results for different predictives using a fixed Laplace backbone.}
% \label{tab:imagenet_pred_probs}
% \centering
% \begin{tabular}{lcc}
% \toprule
% \textbf{Method} & \textbf{Mean} & \textbf{Std} \\
% \midrule
% \multicolumn{3}{c}{\textbf{Sigmoid Laplace}} \\
% \midrule
% Analytic & 0.0101 & 0.0006 \\
% MC 1000 & 0.0153 & 0.0011 \\
% MC 100 & 0.0153 & 0.0012 \\
% MC 10 & 0.0165 & 0.0014 \\
% \midrule
% \multicolumn{3}{c}{\textbf{NormCDF Laplace}} \\
% \midrule
% MC 10 & 0.0112 & 0.0005 \\
% Analytic & 0.0114 & 0.0007 \\
% MC 100 & 0.0115 & 0.0006 \\
% MC 1000 & 0.0118 & 0.0007 \\
% \bottomrule
% \end{tabular}
% \end{table}

\subsection{Effects of Changing the Learning Objective}
\label{sec:changing_the_learning_objective}

The previous section shows that for an already trained model that satisfies the constraint (\cref{eq:constraint}), our analytic predictives always perform on par with MC sampling while being more efficient. However, our proposed objectives (\cref{sec:through_regularisation,sec:through_classwise_ce_losses}) change the training dynamics of models. Therefore, in this section, we showcase the performance of methods equipped with our analytic (i.e., sample-free) predictives and learning objectives against softmax models. For the latter, we use the \emph{best-performing} predictive and estimator (excluding ours). See~\cref{app:list_predictive,app:list_estimators} for an overview of predictives and estimators. We employ our methods with the analytic predictives and second-order Dirichlet distributions to demonstrate their competitive nature while being more efficient than MC sampling.

% \begin{figure*}[t]
% \centering
% \begin{subfigure}{0.48\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{gfx/imagenet/log_prob_score_hard_bma_aleatoric_original.pdf}
%     \caption{ImageNet results. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) constraints are either on par with or outperform Softmax (\colorrectangle{GoogleYellow}).}
%     \label{fig:imagenet_log_prob}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{0.48\textwidth}
%     \includegraphics[width=\textwidth]{gfx/cifar10/log_prob_score_hard_bma_aleatoric_original.pdf}
%     \caption{CIFAR-10 results. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) constraints consistently outperform Softmax (\colorrectangle{GoogleYellow}) on all methods.}
%     \label{fig:cifar10_log_prob}
% \end{subfigure}
% \caption{Log probability proper scoring (negative NLL) results on the ImageNet and CIFAR-10 test datasets.}
% \label{fig:log_prob}
% \end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics{gfx/imagenet/log_prob_score_hard_bma_aleatoric_original.pdf}
    \caption{ImageNet log probability proper scoring results. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) constraints are either on par with or outperform Softmax (\colorrectangle{GoogleYellow}) on all methods.}
    \label{fig:imagenet_log_prob}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics{gfx/cifar10/log_prob_score_hard_bma_aleatoric_original.pdf}
    \caption{CIFAR-10 log probability proper scoring results. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) constraints consistently outperform Softmax (\colorrectangle{GoogleYellow}) on all methods.}
    \label{fig:cifar10_log_prob}
\end{figure}

We first evaluate how calibrated the models are using the log probability scoring rule~\cite{gneiting2007strictly} and the Expected Calibration Error (ECE) metric~\cite{naeini2015obtaining}. \cref{fig:cifar10_log_prob} shows that on CIFAR-10, the score of our analytic predictives with implicit constraints (Sigmoid, NormCDF) are consistently better than the corresponding Softmax results for all methods. On the large-scale ImageNet dataset, Sigmoid and NormCDF either outperform or are on par with Softmax.

\begin{figure}[t]
    \centering
    \includegraphics{gfx/imagenet/ece_hard_bma_correctness_original.pdf}
    \caption{ImageNet ECE results. Analytic predictives with implicit constraints (\colorrectangle{GoogleGreen}) outperform Softmax (\colorrectangle{GoogleYellow}) across all methods. Explicit constraints (\colorrectangle{GoogleBlue}) work best on HET but do not benefit the other two methods. Note the restricted $y$-limits for readability.}
    \label{fig:imagenet_ece}
\end{figure}

The ECE requires the models' confidence to match their accuracy. \cref{fig:imagenet_ece} shows that on ImageNet, our analytic predictives with implicit constraints (Sigmoid, NormCDF) have a clear advantage over Softmax across all methods. Explicit constraints benefit SNGP and HET but not the post-hoc Laplace method.

\begin{figure}[t]
    \centering
    \includegraphics{gfx/imagenet/log_prob_score_hard_bma_correctness_original.pdf}
    \caption{ImageNet log probability proper scoring results for the binary correctness prediction task. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) constraints consistently outperform Softmax (\colorrectangle{GoogleYellow}).}
    \label{fig:imagenet_log_prob_corr_pred}
\end{figure}

Next, we turn to the correctness prediction task: whether the models can predict the correctness of their own predictions. We consider \emph{correctness estimators} $\tilde c(x) \in [0, 1]$ for inputs $x \in \mathcal{X}$ derived from the predictives. Framed as a binary prediction task, the goal of these estimators is to predict the probability of the predicted class' correctness. We then measure the binary log probability score of $\tilde c(x)$. \cref{fig:imagenet_log_prob_corr_pred} shows that our analytic predictives with implicit constraints outperform all Softmax predictives across all methods.

\begin{figure}[t]
    \centering
    \includegraphics{gfx/imagenet/hard_bma_accuracy_original.pdf}
    \caption{\textbf{Analytic predictives do not sacrifice accuracy.} ImageNet validation accuracies. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) or explicit (\colorrectangle{GoogleBlue}) constraints either outperform or are on par with Softmax (\colorrectangle{GoogleYellow}) across all methods.}
    \label{fig:imagenet_accuracy}
\end{figure}

\paragraph{Analytic predictives do not sacrifice accuracy.} \cref{fig:imagenet_accuracy} evidences this claim on ImageNet: our analytic predictives either outperform or are on par with Softmax predictives. The most accurate methods are NormCDF and Sigmoid SNGP. These results are in line with the findings of~\citet{wightman_resnet_2021}, who also recommend training with a per-class binary cross-entropy loss using the sigmoid activation function.

\begin{figure}[t]
    \centering
    \includegraphics{gfx/imagenet/auroc_oodness.pdf}
    \caption{ImageNet OOD detection AUROC results for severity level one. Across all methods, the best-performing predictive is analytic with an implicit constraint (\colorrectangle{GoogleGreen}).}
    \label{fig:imagenet_ood_detection}
\end{figure}

Finally, we consider another binary prediction task, where a general uncertainty estimator $u(x) \in \mathbb{R}$ (derived from predictives or second-order Dirichlet distributions) is tasked to separate ID and OOD samples from a balanced mixture. As the uncertainty estimator can take on any real value, we measure the Area Under the Receiver Operating Characteristic curve (AUROC), which quantifies the separability of ID and OOD samples w.r.t.~the uncertainty estimator. As OOD inputs, we consider corrupted ImageNet-C~\cite{hendrycks2019benchmarking} samples. \cref{fig:imagenet_ood_detection} shows that our analytic predictives with implicit constraints (Sigmoid, NormCDF) outperform Softmax across all methods, even though separating ID and OOD samples does not require a fine-grained representation of uncertainty, unlike the ECE or proper scoring rules. Importantly, these analytic predictives and second-order Dirichlet distributions are considerably cheaper to calculate than Softmax MC predictions (see \cref{sec:computational_gains}). See~\cref{app:ood} for the other severity levels.

\section{Conclusion and Limitations}
We developed a framework that allows to obtain predictives and other quantities of interest from logit space Gaussian distributions analytically. Our experimental results suggest the ubiquitous softmax activation should be replaced by normCDF or sigmoid.

We proposed to approximate the Gaussian pushforwards by Dirichlet distributions which cannot encode correlations between classes. Leveraging a more expressive yet tractable family of distributions on the simplex that can achieve this presents an interesting area of future research.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be
specifically highlighted here.

\section*{Acknowledgments}

The authors gratefully acknowledge co-funding by the European Union (ERC, ANUBIS, 101123955). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. BM \& PH are supported by the DFG through Project HE 7114/6-1 in SPP2298/2. NDC is supported by the Fonds National de la Recherche, Luxembourg, Project 17917615. PH is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy – EXC number 2064/1 – Project number 390727645.
The authors also gratefully acknowledge the German Federal Ministry of Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039A); and funds from the Ministry of Science, Research and Arts of the State of Baden-Württemberg.

\bibliographystyle{icml2024}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Gaussian Integral Derivations}
In this appendix, we derive the closed-form formula for the mean of Gaussian pushforwards through exp (\cref{eq:gaussian_exp_integral}) and normCDF (\cref{eq:gaussian_normcdf_integral}), as well as the approximations for pushforwards through sigmoid (\cref{eq:gaussian_sigmoid_integral}) and softmax.
\subsection{Gaussian Exp Integral}\label{app:gaussian_exp_integral}
By absorbing the exponential into the Gaussian probability density function, we get
\begin{equation}
    \begin{aligned}
        \int_{\R}\exp(y)\;\mathcal N(\mu,\sigma^2)(dy) &= \int_{\R}\exp(y)\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)\;dy \\
        &=\int_{\R}\exp\left(\mu+\frac{\sigma^2}2\right)\exp\left(-\frac{1}{2\sigma^2}(y-\mu-\sigma^2)^2\right)\;dy \\
        &=\int_{\R}\exp\left(\mu+\frac{\sigma^2}2\right)\;\mathcal N(\mu+\sigma^2,\sigma^2)(dy) \\
        &=\exp\left(\mu+\frac{\sigma^2}2\right).
    \end{aligned}
\end{equation}
\subsection{Gaussian NormCDF Integral}\label{app:gaussian_normcdf_integral}
Here, we derive the classical normCDF Gaussian integration formula \citep[Eq. 10,010.8]{owen_table_1980}.

For $\lambda>0$, $Z\sim\mathcal N(0,1)$ and $Y\sim \mathcal N(\mu,\sigma^2)$ with $Y$ and $Z$ independent,
\begin{equation}\label{eq:normcdf_gaussian_integral}
    \begin{aligned}
        \int_\R \Phi(\lambda y)\;\mathcal N(\mu, \sigma^2)(dy) &= \int_\R p(Z\leq y/\lambda)\;\mathcal N(\mu, \sigma^2)(dy) \\
        &= p\left(Z \leq \frac{Y}{\lambda}\right) \\
        &= p\left(\frac{Z/\lambda - Y+ \mu}{\sqrt{\lambda^{-2}+\sigma^2}}\leq \frac{\mu}{\sqrt{\lambda^{-2}+\sigma^2}}\right) \\
        &= \Phi\left(\frac{\mu}{\sqrt{\lambda^{-2}+\sigma^2}}\right)
    \end{aligned}
\end{equation}
where we used $\frac{Z/\lambda - X+ \mu}{\sqrt{\lambda^{-2}+\sigma^2}} \sim \mathcal N(0,1)$ for the last equality. Taking $\lambda = 1$, this gives the formula for the exact predictive with normCDF.

\subsection{Probit Approximation for Gaussian Sigmoid Integral}\label{app:gaussian_sigmoid_integral}Taylor expanding $\rho$ to first order about $0$,
\begin{equation}
    \begin{aligned}
        \rho(y) &= \frac{1}{2} + \frac{1}{4}y + o(y) \\
        \Phi(y) &= \frac{1}{2} + \frac{1}{\sqrt{2\pi}}y + o(y).
    \end{aligned}
\end{equation}
Hence matching $\rho$ and $\Phi$ to first order we get the approximation $\rho(y) \approx \Phi\left(\sqrt\frac{\pi}{8}y\right)$. So using \cref{eq:normcdf_gaussian_integral} we derive the \emph{probit approximation} \cite{spiegelhalter_sequential_1990,mackay_evidence_1992}
\begin{equation}\label{eq:probit_approx_derivation}
    \int_\R \rho(y)\;\mathcal N(\mu, \sigma^2)(dy) \stackrel{(1)}\approx \int_\R \Phi\left(\sqrt\frac{\pi}{8} y\right)\;\mathcal N(\mu, \sigma^2)(dy) = \Phi\left(\frac{\mu}{\sqrt{\frac{8}{\pi}+\sigma^2}}\right) \stackrel{(2)}\approx \rho\left(\frac{\mu}{\sqrt{1+\frac{\pi}{8}\sigma^2}}\right).
\end{equation}
Note that the approximation (2) is not strictly needed, as $\Phi$ is computationally tractable. However, adding (2) empirically improves the quality of the overall approximation. This may be due to the fact the thicker tails of $\rho$ in the integrand of the left-hand side are better captured by $\rho$ than $\Phi$ on the right-hand side.
\subsection{Mean Field Approximation for Gaussian Softmax Integral}\label{app:gaussian_softmax_integral}
For $\bm \mu\in \R^C$, $\bm \sigma^2 \in \R^C_{>0}$ and $\bm \Sigma = \diag(\bm \sigma^2)$, the \emph{mean field approximation} to the Gaussian softmax integral \cite{lu_mean-field_2021} is obtained as follows
\begin{equation}\label{eq:mean_field_approx_derivation}
    \begin{aligned}
        \E_{\bm Y\sim \mathcal N(\bm \mu,\bm \Sigma)}[\operatorname{softmax}_c \bm Y] &= \E\left[\left(2-C+\sum_{c'\neq c}\rho(Y_c-Y_{c'})\inv\right)\inv\right] \\
        &\stackrel{(1)}\approx \left(2-C+\sum_{c'\neq c}\E[\rho(Y_c-Y_{c'})]\inv\right)\inv \\
        &\stackrel{(2)}\approx \left(2-C+\sum_{c'\neq c}\E[\rho(Y_c-\mu_{c'})]\inv\right)\inv \\
        &\stackrel{(3)}\approx \left(2-C+\sum_{c'\neq c}\rho\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv\right)\inv \\
        &= \operatorname{softmax}_c\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm\sigma^2}}\right)
    \end{aligned}
\end{equation}
i.e.,
\begin{equation}
    \E_{\bm Y\sim \mathcal N(\bm \mu,\bm \Sigma)}[\softmax \bm Y] \approx \softmax\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm\sigma^2}}\right).
\end{equation}
(1) is the mean field approximation, and (3) uses the probit approximation \cref{eq:probit_approx_derivation}. \cite{lu_mean-field_2021} provides two other variants of this approximation with other choices of approximation (2).

\section{Comparison with the Multinomial Probit Model}\label{app:probit_model}
In this appendix, we show that a model whose output activation is a composition of an element-wise normCDF activation $\bm \Phi$ and a normalisation $\bm n$ is distinct from the classical multinomial probit model \cite{daganzo_chapter_1979}.

Given a logit $\bm y\in \R^C$, the multinomial probit model sets
\begin{equation}
    Z(\bm y)= \argmax_{1\leq c \leq C} Y_c
\end{equation}
where $Y_c = y_c +\epsilon_c$ and the $\epsilon_c$ are i.i.d.~standard Normal. So
\begin{equation}\label{eq:multinomial_probit_model}
    p(c\mid y) = p(y_c+\epsilon_c > y_{c'}+\epsilon_{c'}\all c'\neq c)
\end{equation}
which is generally not analytically tractable. On the other hand, a model that uses normCDF and normalisation as output activation yields
\begin{equation}\label{eq:our_model}
    p(c\mid y) = \frac{p(y_c+\epsilon_c>0)}{\sum_{c'=1}^Cp(y_{c'}+\epsilon_{c'}>0)} = \frac{\Phi(y_c)}{\sum_{c'=1}^C\Phi(y_c')}.
\end{equation}
In the case $C=2$, the multinomial probit model \cref{eq:multinomial_probit_model} outputs closed form probabilities. This allows us to construct an explicit counterexample to the equivalence of the two models \cref{eq:multinomial_probit_model} and \cref{eq:our_model}:
\begin{equation}
    \begin{aligned}
        p(y_1+\epsilon_1> y_2+\epsilon_2)&= p\left(\frac{y_1-y_2}{2} +\frac{\epsilon_1-\epsilon_2}{2}> 0\right)= \Phi\left(\frac{y_1-y_2}{2}\right) \neq \frac{\Phi(y_1)}{\Phi(y_1)+\Phi(y_2)}.
    \end{aligned}
\end{equation}

\section{Analysis of the Predictive Approximations' Quality}\label{app:quality}
In this appendix, we provide theoretical analyses of the quality of various predictive approximations. This complements the empirical analyses taken, for instance, in the synthetic experiment (\cref{fig:synthetic_exp}) or in \cite{daunizeau_semi-analytical_2017}.

Due to its information-theoretic interpretation, a natural metric to equip the probability simplex $\Delta^{C-1}$ with is the KL divergence
\begin{equation}\label{eq:kl}
    \KL(\bm p, \bm q) = \sum_{c=1}^Cp_c(\log p_c - \log q_c)
\end{equation}
which is well defined if $\bm p$, $\bm q$ lie in the interior of the simplex ($p_i,q_i\neq 0,1$ for all $i$). So, for a predictive approximation, $\bm {\hat p}$, we would like to analyse $\KL(\bm p, \bm {\hat p})$, where $\bm p := \E_{\bm P \sim\bm a_*\mathcal N(\bm \mu, \bm\Sigma)}[\bm P]$ is the true predictive, $\bm \mu$ and $\bm\Sigma$ are some logit space mean and covariance and $\bm a$ is an output activation (e.g.~$\bm a = \bm n\circ \bm\varphi$).
\subsection{Analysis of Monte Carlo Approximations}\label{app:mc_quality}
An $N$ sample Monte Carlo estimate is defined as
\begin{equation}\label{eq:mc_estimate}
    \bm{\hat P}^{S}:= \frac{1}{S}\sum_{s=1}^S \bm{\hat P}^{(s)}
\end{equation}
where the $\bm{\hat P}^{(s)}$ are i.i.d.~$\bm a_*\mathcal N(\bm \mu, \bm\Sigma)$. The computational cost of MC integration is $\mathcal O(S\cdot C)$. This becomes prohibitive for large $S$ and $C$. Thus, for a fair assessment of the quality of such an estimate in terms of the number of classes, one should consider MC estimates $\bm{\hat P}^{\lceil S/C\rceil}$.

We now give an informal theoretical argument for the linear growth of the KL divergence between $\bm p$ and $\bm{\hat P}^{\lceil S/C\rceil}$ in terms of $C$, under the distributional conditions of the synthetic experiment (\cref{fig:synthetic_exp}).

Taylor expanding \cref{eq:kl} about $\bm p$ to second order we obtain
\begin{equation}\label{eq:kl_expansion}
    \begin{aligned}
        \KL(\bm p, \bm q) &= \sum_{c=1}^Cp_c(\log p_c - \log q_c) \\
        &= \sum_{c=1}^Cp_c\sum_{k=1}^\infty \frac{(p_c-q_c)^k}{kp_c^k} \\
        &= \underbrace{\sum_{c=1}^C p_c}_{=1} -\underbrace{\sum_{c=1}^C q_c}_{=1} + \sum_{c=1}^C\sum_{k=2}^\infty \frac{(p_c-q_c)^k}{kp_c^{k-1}} \\
        &\stackrel{(1)}\approx \sum_{c=1}^C \frac{(p_c-q_c)^2}{2p_c} \\
        &\stackrel{(2)}\approx \frac{C}{2}\|\bm p-\bm q\|_2^2
    \end{aligned}
\end{equation}
where approximation (1) assumes $\|\bm p-\bm q\|_2$ is small, and (2) assumes $p_c\approx 1/C$.

In the synthetic experiment (\cref{fig:synthetic_exp}), the logit class-wise means $\mu_c$ and variances $\sigma_c^2$ are sampled in an i.i.d.~way. Let $\bm Y\sim N(\bm \mu, \bm \Sigma)$ with $\bm\Sigma :=\diag(\bm \sigma^2)$, $\bm Q=\bm\varphi(\bm Y)$ the unnormalised `probabilities' and $\bm P:= \bm Q/\sum_{c=1}^C Q_c$ the probabilities. We have
\begin{equation}
    \Var [\bm P] = \E\left[\frac{\bm Q^2} {\left(\sum_{c=1}^CQ_c\right)^2}\right]-\E\left[\frac{\bm Q}{\sum_{c=1}^CQ_c}\right]^2 \approx \frac{\E[\bm Q^2]-\E[\bm Q]^2}{\left(\sum_{c=1}^C\E[Q_c]\right)^2} = \frac{\Var[\bm Q]}{C^2\E[Q_1]^2}
\end{equation}
where all operations are taken element-wise. Thus
\begin{equation}
    \E\left[\|\bm p-\bm P\|^2_2\right] = \sum_{c=1}^C \Var[P_c] \approx \sum_{c=1}^C \frac{\Var[Q_c]}{C^2\E[Q_1]^2} = \frac{\Var[Q_1]}{C\E[Q_1]^2}.
\end{equation}
Now the MC samples $\bm{\hat P}^{(s)}$ are i.i.d.~copies of $\bm P$. So we have
\begin{equation}
    \E\big[\|\bm p - \bm{\hat P}^{\lceil S/C\rceil}\|^2_2\big] = \frac{1}{\lceil S/C \rceil}\E\left[\|\bm p-\bm P\|^2_2\right] \approx \frac{\Var[Q_1]}{S\E[Q_1]^2}.
\end{equation}
Plugging this into \cref{eq:kl_expansion} we get
\begin{equation}
    \E[\KL(\bm p, \bm{\hat P}^{\lceil S/C\rceil})] \approx \frac{\Var[Q_1]}{2\E[Q_1]^2}\cdot\frac{C}{S}
\end{equation}
which grows linearly with the number of classes, as observed in \cref{fig:synthetic_exp}.
\subsection{Analysis of the Probit Approximation for Sigmoid Models}\label{app:probit_quality}
Recall the probit approximation to the Gaussian sigmoid integral (\cref{app:gaussian_sigmoid_integral})
\begin{equation}\label{eq:probit_approximation}
    q:= \int_{\R}\rho(y)\;\mathcal N(\mu,\sigma^2)(dy) \approx \rho\left(\frac{\mu}{\sqrt{1+\frac{\pi}{8}\sigma^2}}\right) =: \hat q.
\end{equation}

In the multiclass setting, for $\bm \mu\in \R^C$, $\bm \sigma^2 \in \R^C_{>0}$ and $\bm \Sigma = \diag(\bm \sigma^2)$, define
\begin{equation}
    \begin{aligned}
        \bm q := \E&_{\bm Q \sim\bm \rho_*\mathcal N(\bm \mu, \bm\Sigma)}[\bm Q] \\
        \bm {\hat q} := \bm\rho&\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2}}\right) \\
        \bm p := \E_{\bm P\sim \bm n_*\bm \rho_* \mathcal N(\bm \mu, \bm \Sigma)}&[\bm P] = \E_{\bm Q \sim \bm \rho_*\mathcal N(\bm \mu, \bm\Sigma)}\left[\frac{\bm Q} {\sum_{c=1}^CQ_c} \right] \\
        \bm{\hat p} := \frac{\bm {\hat q}}{\sum_{c=1}^C \hat q_c}& =  \frac{\bm\rho\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2}}\right)}{\sum_{c=1}^C \rho\left(\frac{\mu_c}{\sqrt{1+\frac{\pi}{8}\sigma^2_c}}\right)}
    \end{aligned}
\end{equation}
We assume that, e.g.~through training, the constraint \cref{eq:constraint} is perfectly satisfied, such that $\bm p = \bm q / \sum_{c=1}^C q_c$. Then we obtain
\begin{theorem}
    Let $\mathcal K\subset \R\times \R_{>0}$ compact. Using the compactness of $\mathcal K$ define $\delta(\mathcal K) := \max_{(\mu, \sigma^2)\in \mathcal K}|q-\hat q|$ and $u(\mathcal K):= \min_{(\mu, \sigma^2)\in \mathcal K} q>0$, where $q$ and $\hat q$ are as in \cref{eq:probit_approximation}.
    Then if $(\mu_c, \sigma_c^2)\in \mathcal K$ for all $c$ we have 
    \begin{equation}\label{eq:probit_kl_bound}
        \KL(\bm p,\bm{\hat p}) \leq \log\left(\frac{u+\delta}{u-\delta}\right).
    \end{equation}
\end{theorem}
\begin{remark}
    The bound \cref{eq:probit_kl_bound} is of practical value as
    \begin{enumerate}
        \item it is independent of $C$,
        \item it tends to 0 as $\delta$ tends to 0.
    \end{enumerate}
    In other words, given knowledge of the worst case error in the approximation \cref{eq:probit_approximation} on the compact set $\mathcal K$, we can bound the KL divergence in terms of that error independently of the number of classes. Due to the simplicity of our assumptions, the bound remains quite raw and could be strengthened with further distributional assumptions on the means and variances.
\end{remark}
\begin{proof}
    \begin{equation}
    \begin{aligned}
        \KL(\bm p, \bm{\hat p}) &= \sum_{c=1}^Cp_c(\log p_c -\log \hat p_c) \\
        &= \sum_{c=1}^C\frac{q_c}{\sum_{c'=1}^C q_{c'}}\left(\log \left(\frac{q_c}{\sum_{c'=1}^C q_{c'}}\right) -\log \left(\frac{\hat q_c}{\sum_{c'=1}^C \hat q_{c'}}\right)\right) \\
        &= \frac{1}{\sum_{c=1}^Cq_{c}}\sum_{c=1}^Cq_c\left(-\log\left(\frac{\hat q_c}{q_c}\right)+\log\left(\frac{\sum_{c'=1}^C \hat q_{c'}}{\sum_{c'=1}^C q_{c'}}\right)\right) \\
        &\leq \frac{1}{\sum_{c=1}^Cq_{c}}\sum_{c=1}^Cq_c\left(-\log\left(\frac{q_c-\delta}{q_c}\right)+\log\left(\frac{\sum_{c'=1}^C q_{c'}+C\delta}{\sum_{c'=1}^C q_{c'}}\right)\right) \\
        &= \frac{1}{\sum_{c=1}^Cq_{c}}\sum_{c=1}^Cq_c\left(-\log\left(1-\frac{\delta}{q_c}\right)+\log\left(1+\frac{C\delta}{\sum_{c'=1}^C q_{c'}}\right)\right) \\
        &\leq \frac{1}{\sum_{c=1}^Cq_{c}}\sum_{c=1}^Cq_c\left(-\log\left(1-\frac{\delta}{u}\right)+\log\left(1+\frac{C\delta}{Cu}\right)\right) \\
        &= \log\left(\frac{u+\delta}{u-\delta}\right).
    \end{aligned}
\end{equation}
\end{proof}
\subsection{Analysis of the Mean Field Approximation for Softmax Models}
Recall the mean field approximation to the Gaussian softmax integral (\cref{app:gaussian_softmax_integral})
\begin{equation}
    \bm p := \int_{\R^C}\softmax(\bm x)\;\mathcal N(\bm \mu, \bm \Sigma) (d\bm x) \approx \softmax\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm\sigma^2}}\right)=: \bm {\hat p}.
\end{equation}
This approximation is obtained through three subsequent approximations, that is, (1), (2), and (3) in \cref{eq:mean_field_approx_derivation}. Approximation (2) is not actually necessary, as it can be replaced by an exact computation, as shown in \cite{lu_mean-field_2021}. However, this empirically does not appear to improve the quality of the overall approximation. We focus our attention on the main approximation (1), namely
\begin{equation}
    \E\left[\left(2-C+\sum_{c'\neq c}\rho(Y_c-Y_{c'})\inv\right)\inv\right] \stackrel{(1)}\approx\left(2-C+\sum_{c'\neq c}\E[\rho(Y_c-Y_{c'})]\inv\right)\inv.
\end{equation}
This is exact if $\rho(Y_c-Y_{c'})$ is a constant random variable for all $Y_c$ and $Y_{c'}$. For $C>2$, this would imply that $Y_c$ is distributed according to a Dirac delta. However, this does not allow for any second-order distribution over the logits. We thus see that we cannot train for exactness in the mean field approximation as we do for \cref{eq:approximate_predictive}. This is a fundamental limitation of this mean field approximation.

% We will informally show that that approximation (3) is not of critical concern as we should not expect its error to grow with the number of classes. The main bottleneck in the mean field approximation seems to result from approximation (1). We will then show that we cannot train for exactness in (1) like we do for our approximation \cref{eq:approximate_predictive}; this is a fundamental limitation of the mean field approximation.

% Recall approximation (3):
% \begin{equation}
%      \left(2-C+\sum_{c'\neq c}\E[\sigma(X_c-\mu_{c'})]\inv\right)\inv \stackrel{(3)}\approx \left(2-C+\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv\right)\inv.
% \end{equation}
% Define
% \begin{equation}
%     \delta_{c'}:= \E[\sigma(X_c-\mu_{c'})]- \sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)
% \end{equation}
% the error from the probit approximation. Then by applying two subsequent Taylor expansions of the function $x\mapsto x\inv$,
% \begin{equation}
%     \begin{aligned}
%         &\left(2-C+\sum_{c'\neq c}\E[\sigma(X_c-\mu_{c'})]\inv\right)\inv \\
%         &= \left(2-C+\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv + \delta_{c'}\right)\inv \\
%         &=\left(2-C+\sum_{c'\neq c}\left(\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv - \sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)^{-2}\delta_{c'} + o(\delta_{c'})\right)\right)\inv \\
%         &= \left(2-C+\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv\right)\inv \\
%         &\quad+\left(2-C+\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv\right)^{-2}\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)^{-2}\delta_{c'}+\sum_{c'\neq c}o(\delta_{c'})
%     \end{aligned}
% \end{equation}
% for small $\delta_{c'}$. Letting $\delta :=\max_{c'}|\delta_{c'}|$, we have
% \begin{equation}
%     \begin{aligned}
%         &\left|\left(2-C+\sum_{c'\neq c}\E[\sigma(X_c-\mu_{c'})]\inv\right)\inv-\left(2-C+\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv\right)\inv\right| \\
%         &\leq \left(2-C+\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv\right)^{-2}\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)^{-2}\delta+o(\delta) \\
%         &\leq \underbrace{\left(2-C+\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)\inv\right)^{-2}\left(\sum_{c'\neq c}\sigma\left(\frac{\mu_c-\mu_{c'}}{\sqrt{1+\frac{\pi}{8}\sigma_c^2}}\right)^{-1}\right)^2}_{\leq 1}\delta+o(\delta) \\
%         &\leq \delta + o(\delta)
%     \end{aligned}
% \end{equation}
% for small $\delta$. This is independent of $C$, thus the approximation (3) does not explain the behaviour of the mean field approximation observed in \nathael{ref}.

% Therefore the main bottleneck of the mean field approximation results from approximation (1), where recall,
% \begin{equation}
%     \E\left[\left(2-C+\sum_{c'\neq c}\sigma(X_c-X_{c'})\inv\right)\inv\right] \stackrel{(1)}\approx\left(2-C+\sum_{c'\neq c}\E[\sigma(X_c-X_{c'})]\inv\right)\inv.
% \end{equation}
\section{Moment Matching Beta distributions}\label{app:beta_matching}
As noted in \cref{sec:beta_matching}, when $\varphi = \Phi\text{ or } \rho$, we can construct a mapping
\begin{equation}
    \mathrm p\colon \mathcal G(\R^C) \to \mathcal B((0,1))^C
\end{equation}
by moment matching. Specifically, the parameters $\bm\alpha,\bm\beta\in \R_{>0}^C$ that match the moments of $\bm Q \sim \bm\varphi_* \mathrm f$ for some $\mathrm f\in \mathcal G(\R^C)$ must satisfy
\begin{equation}\label{eq:beta_moments}
    \begin{aligned}
        \E[\bm Q]   & = \frac{\bm\alpha}{\bm\alpha+\bm\beta},                                     \\
        \E[\bm Q^2] & = \frac{\bm\alpha(\bm\alpha+1)}{(\bm\alpha+\bm\beta)(\bm\alpha+\bm\beta+1)},
    \end{aligned}
\end{equation}
where all vector operations are element-wise. Multiplying out the denominators on the right-hand side of the equations of \cref{eq:beta_moments}, we obtain a system of two linear equations with two unknowns (for each $c$), which can be solved uniquely, yielding
\begin{equation}\label{eq:beta_parameters}
    \begin{aligned}
        \bm\alpha & := \frac{\E[\bm Q] - \E[\bm Q^2]}{\E[\bm Q^2] - \E[\bm Q]^2}\E[\bm Q],                \\
        \bm\beta  & := \frac{\E[\bm Q] - \E[\bm Q^2]}{\E[\bm Q^2] - \E[\bm Q]^2}\left(1-\E[\bm Q]\right).
    \end{aligned}
\end{equation}
which give us the parameters of the Beta distributions $\mathrm p(\mathrm f)$.
\subsection{Information Geometric Interpretation of the Pushforward through NormCDF}\label{app:information_geometry}
\begin{figure}
        \centering
        \includegraphics[trim={0cm 1.2cm 0cm 0.6cm},clip]{gfx/normcdf.pdf}
        \includegraphics[trim={0cm 1.2cm 0cm 0.6cm},clip]{gfx/sigmoid.pdf}
        \caption{Illustration of the statistical manifolds of the pushforward Gaussian distributions $\mathcal G(\R)$ through the normCDF and the sigmoid respectively compared to the statistical manifold of Beta distributions $\mathcal B((0,1))$. NormCDF, unlike the sigmoid, makes the manifolds intersect at the point $\Phi_*\mathcal N(0,1) = B(1,1)$.}
        \label{fig:information_geometry}
\end{figure}
Here, we argue that the normCDF activation is an ideal choice for approximating Gaussian pushforwards with Beta distributions by interpreting \cref{fig:information_geometry}. This extends the work from \cite{mackay_choice_1998}, as it shows how one can make sense of the `right' basis for performing Laplace approximations in the classification setting.

$\Phi_*\mathcal G(\R)$, $\rho_*\mathcal G(\R)$ the space of pushforwards of Gaussian distributions by normCDF and sigmoid respectively, and $\mathcal B((0,1))$, the space of Beta distributions, are statistical manifolds naturally equipped with Riemannian metrics, that is their respective Fisher information metrics. We would like to visualise these manifolds. However, two difficulties arise.

The first difficulty is that, while these manifolds all lie in the infinite-dimensional vector space of signed measures on the open unit interval $\mathcal M((0,1))$, there is \emph{no} subspace $\mathbb V\subset \mathcal M((0,1))$ which is 3-dimensional ($\mathbb V \cong \R^3$) and contains any two of these statistical manifolds ($\Phi_*\mathcal G(\R), \mathcal B((0,1)) \subset \mathbb V$ or $\rho_*\mathcal G(\R), \mathcal B((0,1)) \subset \mathbb V$). We will work around this by building distinct isometric embeddings $\Phi_*\mathcal G(\R) \hookrightarrow \mathbb V$, $\rho_*\mathcal G(\R) \hookrightarrow \mathbb V$ and $\mathcal B((0,1))\hookrightarrow \mathbb V$ for some 3-dimensional vector space $\mathbb V$. This means that while the shape of the manifold illustrations is meaningful, the positioning of a manifold with respect to another is not, apart from some design choices that we describe below.

The second difficulty is that some--if not all--of these manifolds cannot be embedded isometrically into Euclidean space. As a workaround, we instead embed them into the 3-dimensional Minkowski space $\R^{2,1}$, that is $\R^3$ equipped with the pseudo-Riemannian metric $dx_1^2+dx_2^2-dx_3^2$.

The key observation is that $\Phi_*\mathcal G(\R)$ and $\rho_*\mathcal G(\R)$ are isometric to $\mathcal G(\R)$. This is because $\Phi$ and $\rho$ are diffeomorphisms, so in particular sufficient statistics, and the Fisher information metric is invariant under sufficient statistics \citep[Section 5.1.3]{ay_information_2017}. Visually, this means that $\varphi_*\mathcal G(\R)$ has the same shape irrespectively of the diffeomorphism activation function $\varphi$. One can thus observe that, given that $\mathcal B((0,1))$ is not isometric to $\mathcal G(\R)$, there exists no activation $\varphi$ such that $\varphi_*\mathcal G(\R) = \mathcal B((0,1))$. To design an activation $\varphi$ that maps Gaussians to Betas, the best one can hope to do is to map one specific Gaussian distribution $\mathcal N(\mu,\sigma^2)$ to a specific Beta distribution $B(\alpha, \beta)$. This is done with the map $F_{\alpha,\beta}\inv\circ\Phi_{\mu,\sigma^2}$ where $\Phi_{\mu,\sigma^2}$ and $F_{\alpha,\beta}$ are the cumulative distribution functions of $\mathcal N(\mu,\sigma^2)$ and $B(\alpha, \beta)$ respectively. Taking $\mu=0$, $\sigma^2=1$, $\alpha = \beta = 1$ we get $\Phi_{0,1} = \Phi$ and $F_{1,1} = \operatorname{id}_{(0,1)}$, yielding $F_{\alpha,\beta}\inv\circ\Phi_{\mu,\sigma^2} = \Phi$.

Now $\mathcal G(\R)$, and hence $\Phi_*\mathcal G(\R)$ and $\rho_*\mathcal G(\R)$, is isometric to the hyperbolic plane \citet[Example 3.1]{ay_information_2017}. We can embed this isometrically into Minkowski space with the classical hyperboloid model of the hyperbolic plane \cite{reynolds_hyperbolic_1993},
\begin{equation}\label{eq:gaussian_embedding}
    \begin{aligned}
        \mathcal G(\R) \hookrightarrow \R^{2,1}.
    \end{aligned}
\end{equation}
For $\mathcal B((0,1))$, we use the isometric embedding from \citet[Proposition 2]{le_brigant_fisher-rao_2021}:
\begin{equation}\label{eq:beta_embedding}
    \begin{aligned}
        \mathcal B((0,1)) & \hookrightarrow \R^{2,1}                               \\
        (\alpha, \beta)   & \mapsto (\eta(\alpha), \eta(\beta),\eta(\alpha+\beta))
    \end{aligned}
\end{equation}
where $\eta(a):=\int_1^a \sqrt{\psi'(r)}\;dr$ and $\psi$ is the digamma function.

Finally, we choose our embedding \cref{eq:gaussian_embedding} for $\Phi_*\mathcal G(\R)$ such that it intersects the embedding \cref{eq:beta_embedding} at a point, to highlight that the statistical manifolds $\Phi_*\mathcal G(\R)$ and $\mathcal B((0,1))$ intersect at a point in the infinite-dimensional ambient space $\mathcal M((0,1))$, while $\rho_*\mathcal G(\R)$ and $\mathcal B((0,1))$ do not.

Moreover, since $\Phi_*\mathcal N(0,1) = B(1,1)$, our moment matching approximation \cref{eq:beta_moments} is exact when $\mathrm f(x)$ is the standard Normal distribution.

\section{Comparison with the Energy Model View of Classifiers}\label{app:energy_model}
\citet{grathwohl_your_2019} propose to view classifiers of the form \cref{eq:vanilla_model} as energy-based models by
\begin{equation}\label{eq:energy_model}
    \begin{aligned}
        p(c\mid x) &= \frac{\exp( f_c(x))}{\sum_{c'=1}^C\exp(f_{c'}(x))}, \\
        p(x, c) &= \frac{\exp(f_c(x))}{Z}, \\
        p(x) &= \frac{\sum_{c=1}^C \exp(f_c(x))}{Z}
    \end{aligned}
\end{equation}
where $Z = \int_{\mathcal X} \sum_{c=1}^C \exp(f_c(x))\; dx$.
Note the consistency
\begin{equation}\label{eq:energy_model_consistency}
    p(c\mid x) = \frac{p(x,c)}{p(x)}.
\end{equation}
Observe that the choice of $p(x,c)$ and $p(x)$ in \cref{eq:energy_model} is not canonical: we could have multiplied $p(x, c)$ and $p(x)$ by $\frac{h(x)}{Z'}$ for any non-vanishing function $h$ and appropriate normalisation constant $Z'$, and we would still obtain the consistency \cref{eq:energy_model_consistency}. \citet{grathwohl_your_2019} propose to use the degree of freedom in the logits to train for \cref{eq:energy_model}, turning discriminative models into generative models.

Note that $\exp$ in \cref{eq:energy_model} can be replaced by any positive activation $\varphi$. In our framework, we train for $\sum_{c=1}^C \varphi(f_c(x)) = s(x)$. More specifically, in the present work, we train for $\sum_{c=1}^C \varphi(f_c(x)) = 1$ (\cref{sec:learning_the_constraint}), making use of the degree of freedom in the logits to obtain exact predictives in \cref{eq:approximate_predictive}. In the formalism \cref{eq:energy_model}, this means we are setting $x$ to be uniformly distributed. An interesting avenue of future research would be to combine our framework with that of \citet{grathwohl_your_2019}, by e.g.~training for $\sum_{c=1}^C \varphi(f_c(x)) \propto p(x)$, and extending the framework of \citet{grathwohl_your_2019} to second-order distributions.

\section{List of Predictive and Dirichlet Parameters Formulas}\label{app:list_predictive}
\subsection{Predictive Formulas}
We gather formulas for all predictive estimators $\bm{\hat p}$ of the true predictive $\E_{\bm P\sim \bm a_* \mathcal N(\bm\mu,\bm\Sigma)}[\bm P]$, $\bm\Sigma = \bm\diag(\bm\sigma^2)$, used in our experiments (\cref{sec:experiments}).

\subsubsection{Softmax}

\paragraph{Monte Carlo} One can Monte Carlo estimate the true predictive as follows:
\begin{equation}
    \begin{aligned}
        \bm{\hat p}:= \frac{1}{S}\sum_{s=1}^S \bm{\hat p}^{(s)}
    \end{aligned}
\end{equation}
where the $\bm{\hat p}^{(s)}$ are sampled i.i.d.~from $\mathcal N(\bm \mu, \bm\Sigma)$.
\paragraph{Mean Field (\cref{app:gaussian_softmax_integral})} The Mean Field predictive~\cite{lu_mean-field_2021} uses the following approximation for the true predictive:
\begin{equation}
    \bm{\hat p} := \bm\softmax\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm\sigma^2}}\right).
\end{equation}

\paragraph{Laplace Bridge} The Laplace Bridge predictive~\cite{pmlr-v180-hobbhahn22a} approximates the true predictive as follows:
\begin{equation}
    \bm{\hat p} := \frac{\frac{1}{\bm {\tilde\sigma}^2}\left(1-\frac{2}{C}+\frac{e^{\bm{\tilde\mu}}}{C^2}\sum_{c=1}^Ce^{-\tilde\mu_c}\right)}{\sum_{c=1}^C\frac{1}{\tilde\sigma^2_c}\left(1-\frac{2}{C}+\frac{e^{\bm{\tilde\mu}}}{C^2}\sum_{c'=1}^Ce^{-\tilde\mu_{c'}}\right)}
\end{equation}
where
\begin{equation}
    \bm{\tilde\mu}^2 := \sqrt{\frac{\sqrt{C/2}} {\sum_{c=1}^C\sigma^2_c}}\bm\mu,\; \bm{\tilde\sigma^2} := \frac{\sqrt{C/2}} {\sum_{c=1}^C\sigma^2_c}\bm\sigma^2.
\end{equation}

\subsubsection{Exp}
Our analytic predictive for the exp activation function~(\cref{sec:motivation}) is given by
\begin{equation}
    \bm{\hat p} := \frac{\bm \exp\left(\bm\mu+\frac{\bm\sigma^2}{2}\right)}{ \sum_{c=1}^C\exp\left(\mu_c+\frac{\sigma^2_c}{2}\right)}.
\end{equation}

\subsubsection{NormCDF}
The analytic predictive for the normCDF activation function~(\cref{sec:general_framework}) is computed as
\begin{equation}
    \bm{\hat p} := \frac{\bm\Phi\left(\frac{\bm \mu}{\sqrt{1+\bm \sigma^2}}\right)}{\sum_{c=1}^C \Phi\left(\frac{\mu_c}{\sqrt{1+\sigma^2_c}}\right)}.
\end{equation}

\subsubsection{Sigmoid}
For the sigmoid activation function~(\cref{sec:general_framework}), the analytic predictive can be computed as
\begin{equation}
    \bm{\hat p} := \frac{\bm\rho\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2}}\right)} {\sum_{c=1}^C \rho\left(\frac{\mu_c}{\sqrt{1+\frac{\pi}{8}\sigma^2_c}}\right)}.
\end{equation}

\subsection{Dirichlet Parameters Formulas}
We now gather the formulas of the parameters $\bm\gamma$ for the Dirichlet approximations to the Gaussian pushforwards.

\subsubsection{Softmax}

The Laplace Bridge method~\cite{pmlr-v180-hobbhahn22a} uses the following Dirichlet parameters:
\begin{equation}
        \bm{\gamma} := \frac{1}{\bm {\tilde\sigma}^2}\left(1-\frac{2}{C}+\frac{e^{\bm{\tilde\mu}}}{C^2}\sum_{c=1}^Ce^{-\tilde\mu_c}\right)
\end{equation}
where
\begin{equation}
    \bm{\tilde\mu}^2 := \sqrt{\frac{\sqrt{C/2}} {\sum_{c=1}^C\sigma^2_c}}\bm\mu,\; \bm{\tilde\sigma^2} := \frac{\sqrt{C/2}} {\sum_{c=1}^C\sigma^2_c}\bm\sigma^2.
\end{equation}

\subsubsection{Exp}
Exp uses the analytic parameters derived from Moment Matching the Gaussian pushforwards (\cref{sec:dirichlet_matching}):
\begin{equation}
        \bm\gamma := \left(\prod_{c=1}^C\frac{a_c\cdot \max(\sum_{c'=1}^Ca_{c'},1)-b_c}{b_c-a_c^2}\right)^{1/C}\frac{\bm a}{\sum_{c=1}^C a_c}
\end{equation}
where
\begin{equation}
    \bm a = \exp\left(\bm\mu+\frac{\bm\sigma^2}2\right),\; \bm b = \exp(2\bm\mu+2\bm\sigma^2).
\end{equation}
\subsubsection{NormCDF}
NormCDF uses the analytic parameters derived from Moment Matching the Gaussian pushforwards (\cref{sec:dirichlet_matching}):
\begin{equation}
        \bm\gamma := \left(\prod_{c=1}^C\frac{a_c\cdot \max(\sum_{c'=1}^Ca_{c'},1)-b_c}{b_c-a_c^2}\right)^{1/C}\frac{\bm a}{\sum_{c=1}^C a_c}
\end{equation}
where
\begin{equation}
    \bm a = \bm\Phi\left(\frac{\bm \mu}{\sqrt{1+\bm \sigma^2}}\right),\; \bm b = \bm\Phi\left(\frac{\bm\mu}{\sqrt{1+\bm\sigma^2}}\right) - 2 \bm T\left(\frac{\bm\mu}{\sqrt{1+\bm\sigma^2}}, \frac{1}{\sqrt{1+2\bm\sigma^2}}\right).
\end{equation}
\subsubsection{Sigmoid}
Sigmoid uses the analytic parameters derived from Moment Matching the Gaussian pushforwards (\cref{sec:dirichlet_matching}):
\begin{equation}
        \bm\gamma := \left(\prod_{c=1}^C\frac{a_c\cdot \max(\sum_{c'=1}^Ca_{c'},1)-b_c}{b_c-a_c^2}\right)^{1/C}\frac{\bm a}{\sum_{c=1}^C a_c}
\end{equation}
where
\begin{equation}
    \bm a = \bm\rho\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2}}\right),\; \bm b =  \bm\rho\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2}}\right) - \frac{1}{\sqrt{1+\frac{\pi}{8}\bm\sigma^2}}\bm\rho\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2}}\right)\left(1-\bm\rho\left(\frac{\bm \mu}{\sqrt{1+\frac{\pi}{8}\bm \sigma^2}}\right)\right).
\end{equation}

\section{List of Uncertainty Estimators}\label{app:list_estimators}
In this section, we list the uncertainty estimators used in our experiments (\cref{sec:experiments}).

\subsection{Predictive}
\label{app:predictive}
Given a predictive $\bm p$, we consider two uncertainty estimators.

\textbf{Maximum Probability}
    \begin{equation}
        \argmax_{c\in \{1,\dots C\}} p_c.
    \end{equation}
\textbf{Entropy}
    \begin{equation}
        -\sum_{c=1}^C p_c\log p_c.
    \end{equation}

\subsection{Monte Carlo}
Given $S$ Monte Carlo samples $\bm {\hat p}^{(1)},\dots,\bm {\hat p}^{(S)}$ with mean $\bm {\hat p}$, one can calculate a predictive as their average and derive the estimators in~\cref{app:predictive}. However, Monte Carlo samples allow one to calculate two additional estimators detailed below.

\textbf{Expected Entropy}
\begin{equation}
    -\frac{1}{S}\sum_{s=1}^S\sum_{c=1}^C \hat p^{(s)}_c \log \hat p^{(s)}_c.
\end{equation}

\textbf{Mutual Information/Jensen-Shannon Divergence}
    \begin{equation}
        -\sum_{c=1}^C {\hat p}_c\log {\hat p}_c +\frac{1}{S}\sum_{s=1}^S\sum_{c=1}^C \hat p^{(s)}_c \log \hat p^{(s)}_c.
    \end{equation}

\subsection{Dirichlet}
Given a second-order Dirichlet distribution with parameters $\bm \gamma$, one can obtain the expected entropy and mutual information estimators without the need for Monte Carlo samples.

\textbf{Expected Entropy}
\begin{equation}
    -\sum_{c=1}^C\frac{\gamma_c}{\sum_{c'=1}^C\gamma_{c'}}\left(\psi(\gamma_c+1) -\psi\left(\sum_{c'=1}^C \gamma_{c'}+1\right)\right)
\end{equation}
where $\psi$ is the digamma function.

\textbf{Mutual Information}
\begin{equation}
    \sum_{c=1}^C\frac{\gamma_c}{\sum_{c'=1}^C\gamma_{c'}}\left(-\log \gamma_c +\log \left(\sum_{c'=1}^C \gamma_{c'}\right)+\psi(\gamma_c+1) -\psi\left(\sum_{c'=1}^C \gamma_{c'}+1\right)\right).
\end{equation}

\section{Experimental Setup}
\label{app:experimental_setup}

This section describes our experimental setup in detail.

We have two main research questions:
\begin{itemize}
    \item What are the effects of changing the learning objective?
    \item Do we have to sacrifice performance for sample-free predictives?
\end{itemize}

To answer the first question, we evaluate our analytic predictives (Sigmoid, NormCDF, Exp) and moment-matched Dirichlet distributions against softmax models equipped with approximate inference tools (Laplace Bridge~\cite{pmlr-v180-hobbhahn22a}, Mean Field~\cite{lu_mean-field_2021}, Monte Carlo sampling). We consider \textbf{Heteroscedastic Classifiers (HET)}~\cite{collier2021correlated}, \textbf{Spectral-Normalized Gaussian Processes (SNGP)}~\cite{liu2020simple}, and last-layer \textbf{Laplace approximation} methods~\cite{daxberger2021laplace} as backbones (see~\cref{app:benchmarked_methods} for details).

The resulting 18 (method, activation, predictive) triplets are evaluated on ImageNet-1k \citep{deng2009imagenet} and CIFAR-10 \citep{krizhevsky2009learning} on five metrics aligning with practical needs from uncertainty estimates~\cite{mucsanyi2023trustworthy}:
\begin{enumerate}
    \item Log probability proper scoring rule for the predictive,
    \item Expected calibration error of the predictive's maximum-probability confidence,
    \item Binary log probability proper scoring rule for the correctness prediction task,
    \item Accuracy of the predictive's argmax,
    \item AUROC for the out-of-distribution (OOD) detection task.
\end{enumerate}
See~\cref{app:benchmark_tasks} for details.

For ImageNet, we treat ImageNet-C~\citep{hendrycks2019benchmarking} samples with 15 corruption types and 5 severity levels as OOD samples. For CIFAR-10, we use the CIFAR-10C corruptions.

For the second question, we consider fixed (method, activation) pairs and test whether our methods perform on par with the Monte Carlo sampled predictives.

To provide a fair comparison, we reimplement each method as simple-to-use wrappers around deterministic backbones.

For ImageNet evaluation, we use a ResNet-50 backbone pretrained with the softmax activation function, and train each (method, activation) pair for 50 ImageNet-1k epochs following~\citet{mucsányi2024benchmarking}. We train with the LAMB optimizer~\cite{you2019large} using a batch size of $128$ and gradient accumulation across $16$ batches, resulting in an effective batch size of $2048$, following \citet{tran2022plex}. We further use a cosine learning rate schedule with a single warmup epoch using a warmup learning rate of $0.0001$. The learning rate is treated as a hyperparameter and selected from the interval $[0.0005, 0.05]$ based on the validation performance. The weight decay is selected from the set $\{0.01, 0.02\}$. During training, we keep track of the best-performing checkpoint on the validation set and load it before testing. We search for ideal hyperparameters with a ten-step Bayesian Optimization scheme~\cite{shahriari2015taking} in Weights \& Biases~\cite{wandb} based on the negative log-likelihood.

On CIFAR-10, we train ResNet-28 models from scratch for 100 epochs. The only exceptions are the SNGP models that are trained for 125 epochs~\cite{liu2020simple}. We train with Momentum SGD using a batch size of $128$ and no gradient accumulation. Similarly to ImageNet, we use a cosine learning rate schedule but with five warmup epochs and warmup learning rate $1\mathrm{e}{-5}$. The learning rate is also treated as a hyperparameter on CIFAR-10. We use the interval $[0.05, 1]$ for Sigmoid and NormCDF, and $[0.01, 0.15]$ for Softmax and Exp. The optimal learning rates for Sigmoid and NormCDF are generally larger, as the class-wise binary cross-entropies are averaged instead of summed. The weight decay is selected from the interval $[1\mathrm{e}{-6}, 1\mathrm{e}{-4}]$. Similarly to ImageNet, we use the best-performing checkpoint in the tests and use a ten-step Bayesian Optimization scheme to select performant hyperparameters.

\section{Benchmark Metrics}
\label{app:benchmark_tasks}

Our experiments use five tasks/metrics:
\begin{enumerate}
    \item Log probability proper scoring rule for the predictive,
    \item Expected calibration error of the predictive's maximum-probability confidence,
    \item Binary log probability proper scoring rule for the correctness prediction task,
    \item Accuracy of the predictive's argmax,
    \item AUROC for the out-of-distribution detection task.
\end{enumerate}

Below, we describe these metrics and their respective tasks.

\subsection{Log Probability Proper Scoring Rule for the Predictive}

First, we briefly discuss proper and strictly proper scoring rules over general probability measures based on~\cite{mucsanyi2023trustworthy}.

Consider a function $S\colon \mathcal{Q} \times \mathcal{Y} \to \mathbb{R}$ where $\mathcal{Q}$ is a family of probability distributions over the space $\mathcal{Y}$, called the label space.

$S$ is called a proper scoring rule if and only if
\begin{equation}
\max_{q \in \mathcal{Q}} \mathbb{E}_{Y\sim p} S(q, Y) = \mathbb{E}_{Y\sim p} S(p, Y),
\end{equation}
i.e., $p$ is \emph{one of} the maximisers of $S$ in $q$ in expectation. $S$ is further \emph{strictly} proper if $\argmax_{q \in \mathcal{Q}} \mathbb{E}_{Y\sim p} S(q, Y) = p$ is the \emph{unique} maximiser of $S$ in $q$ in expectation.

The log probability scoring rule for categorical distributions is defined as
\begin{equation}
S(q, c) = \sum_{c' = 1}^C \delta_{c,c'} \log q_{c'}(x) = \log q_c(x),
\end{equation}
where $c \in \{1, \dotsc, C\}$ is the true class and $\delta$ is the Kronecker delta. $S$ defined this way is a strictly proper scoring rule, i.e.~$\mathbb{E}_{Y\sim p}S(q, Y)$
is maximal if and only if
\begin{equation}
q(Y=c\mid x) = p(Y = c \mid x)\; \forall c \in \{1, \dotsc, C\}.
\end{equation}

The score above is equivalent to the negative cross-entropy loss.

\subsection{Expected Calibration Error}

To set up the required quantities for the Expected Calibration Error (ECE) metric~\cite{naeini2015obtaining}, we follow the steps below, based on~\cite{mucsanyi2023trustworthy}.
\begin{enumerate}
    \item Train a neural network on the training dataset.
    \item Create predictions and confidence estimates on the test data.
    \item Group the predictions into \(M\) bins based on the confidences estimates. Define bin \(B_m\) to be the set of all indices $n$ of predictions \((\hat{y}_n, \tilde c_n)\) for which
    \begin{equation}
    \tilde c_n \in \left(\frac{m - 1}{M}, \frac{m}{M}\right].
    \end{equation}
\end{enumerate}

The Expected Calibration Error (ECE) metric~\cite{naeini2015obtaining} is then defined as
\begin{equation}
\mathrm{ECE} = \sum_{m = 1}^M \frac{|B_m|}{n} \left|\mathrm{acc}(B_m) - \mathrm{conf}(B_m)\right|
\end{equation}
where
\begin{align}
\mathrm{acc}(B_m) &= \frac{1}{|B_m|} \sum_{n \in B_m} 1\left(\hat{y}_n = c_n\right),\\
\mathrm{conf}(B_m) &= \frac{1}{|B_m|} \sum_{n \in B_m} \max_{c \in \{1, \dotsc, C\}}f_c(x_n).
\end{align}
Intuitively, the ECE is high when the model's per-bin confidences match its accuracy on the bin. We use $M = 15$ bins in this paper.

\subsection{Binary Log Probability Proper Scoring Rule for Correctness Prediction}

The correctness prediction task measures the models' ability to predict the correctness of their own predictions. We consider \emph{correctness estimators} $\tilde c(x) \in [0, 1]$ for inputs $x \in \mathcal{X}$ derived from the predictives. Framed as a binary prediction task, the goal of these estimators is to predict the probability of the predicted class' correctness. In particular, for an (input, target) pair $(x, y)$ with $x \in \mathcal{X}, y \in \mathcal{Y}$, we set the correctness target to
\begin{equation}
\ell \equiv \ell(x, y) = 1\left(\max_{c \in \{1, \dotsc, C\}}h_c(x) = y\right).
\end{equation}

Dropping the dependency on $x \in \mathcal{X}$ for brevity, the log probability score for binary targets $\ell \in \{0, 1\}$ and estimators $\tilde c \in [0, 1]$ is defined as
\begin{equation}
S(\tilde c, \ell) = \begin{cases}\log c &\mathrm{if}\  \ell = 1 \\ \log(1 - \tilde c) &\mathrm{if}\  \ell = 0\end{cases} = \ell\log \tilde c + (1 - \ell)\log (1 - \tilde c).
\end{equation}
One can show that this is indeed a strictly proper scoring rule~\cite{mucsanyi2023trustworthy}.

\subsection{Accuracy}

For completeness, the accuracy of a predictive $h$ on a dataset $(x_n, c_n)_{n=1}^N$ is
\begin{equation}
\operatorname{acc}(h; (x_n, c_n)_{n=1}^N) = \frac{1}{N}\sum_{n=1}^N 1\left(\argmax_{c \in \{1, \dotsc, C\}}h_c(x_n) = \tilde c_n\right).
\end{equation}

\subsection{Area Under the Receiver Operating Characteristic Curve for Out-of-Distribution Detection}

Out-of-distribution detection is another binary prediction task where a general uncertainty estimator $u(x) \in \mathbb{R}$ (derived from predictives or second-order Dirichlet distributions) is tasked to separate ID and OOD samples from a balanced mixture. The target OOD indicator variable $o(x)$ is, therefore, binary. As the uncertainty estimator can take on any real value, we measure the Area Under the Receiver Operating Characteristic curve (AUROC), which quantifies the separability of ID and OOD samples w.r.t. the uncertainty estimator.

Given uncertainty estimates \(u_n \equiv u(x_n)\) and target binary labels \(o_i\) on a balanced dataset $(x_n, o_n)_{n=1}^N$, as well as a threshold \(t \in \mathbb{R}\), we predict $1$ (out-of-distribution) when \(u_n \ge t\) and $0$ (in-distribution) when \(u_n < t\). This lets us define the following index sets:
\begin{align*}
\text{True positives: }\mathrm{TP}(t) &= \left\{n: o_n = 1 \land u_n \ge t\right\}\\
\text{False positives: }\mathrm{FP}(t) &= \left\{n: o_n = 0 \land u_n \ge t\right\}\\
\text{False negatives: }\mathrm{FN}(t) &= \left\{n: o_n = 1 \land u_n < t\right\}\\
\text{True negatives: }\mathrm{TN}(t) &= \left\{n: o_n = 0 \land u_n < t\right\}.
\end{align*}

The Receiver Operating Characteristic (ROC) curve compares the following quantities:
\begin{align*}
\mathrm{TPR}(t) &= \frac{|\mathrm{TP}(t)|}{|\mathrm{TP}(t)| + |\mathrm{FN}(t)|} = \frac{|\mathrm{TP}(t)|}{|\mathrm{P}|}\\
\mathrm{FPR}(t) &= \frac{|\mathrm{FP}(t)|}{|\mathrm{FP}(t)| + |\mathrm{TN}(t)|} = \frac{|\mathrm{FP}(t)|}{|\mathrm{N}|}.
\end{align*}
Here, FPR tells us how many of the actual negative samples in the dataset are recalled (predicted positive) at threshold \(t\).

One can draw a curve of \((\mathrm{FPR}(t), \mathrm{TPR}(t))\) for all \(t\) from \(-\infty\) to \(+\infty\). This is the \emph{ROC curve}. The area under this curve quantifies how well the uncertainty estimator $u(x)$ can separate in-distribution and out-of-distribution inputs.

\section{Benchmarked Methods}
\label{app:benchmarked_methods}

This section describes our benchmarked methods and provides further implementation details.

\subsection{Spectral Normalized Gaussian Process}
\label{app:sngp}

Spectral normalized Gaussian processes (SNGP)~\cite{liu2020simple} use spectral normalization of the parameter tensors for distance-awareness and a last-layer Gaussian process approximated by Fourier features to capture uncertainty. For an input $x \in \mathcal{X}$ and number of classes $C$, they predict a $C$-variate Gaussian distribution
\begin{equation}
\mathcal{N}\left(\bm{B}\bm{\phi}(x), \bm{\phi}(x)^\top\left(\bm{\Psi}^\top\bm{\Psi} + \bm{I}\right)^{-1}\bm{\phi}(x)\bm{I}_C\right),
\end{equation}
in logit space.
\begin{itemize}
    \item $\bm{B} \in \mathbb{R}^{C \times D}$ is a \emph{learned} parameter matrix that maps pre-logits to logits.
    \item $\bm{\phi}(x) = \bm\cos\left(\bm{W}\bm{f}^{L-1}(x) + \bm{b}\right) \in \mathbb{R}^D$ is a random pre-logit embedding of the input $x \in \mathcal{X}$. $\bm{f}^{L-1}(x)$ denotes the pre-logit embedding. $\bm{W}$ is a \emph{fixed} semi-orthogonal random matrix, and $\bm{b}$ is also a \emph{fixed} random vector but sampled from $\text{Uniform}(0, 2\pi)$.
    \item $\bm{\Psi}^\top\bm{\Psi}$ is the (unnormalised) empirical covariance matrix of the pre-logits of the training set. This is calculated via accumulating the mini-batch estimates during the last epoch.\footnote{As we use a cosine learning rate decay in all experiments, the model makes negligible changes in its pre-logit feature space in the last epoch. Thus, the empirical covariance matrix is approximately consistent.}
\end{itemize}

The method applies spectral normalization to the hidden weights in each layer using a power iteration scheme with a single iteration per batch to obtain the largest singular value. \citet{liu2020simple} claim this helps with input distance awareness.

\subsection{Heteroscedastic Classifier}

Heteroscedastic classifiers (HET)~\cite{collier2021correlated} construct a Gaussian distribution in the logit space to model per-input uncertainties:
\begin{equation}
\mathcal{N}(\bm{f}(x), \bm{\Sigma}(x)),
\end{equation}
where $\bm{f}(x) \in \mathbb{R}^D$ is the logit mean for input $x \in \mathcal{X}$ and
\begin{equation}
\bm{\Sigma}(x) = \bm{V}(x)^\top \bm{V}(x) + \bm\diag(\bm{d}(x))
\end{equation}
is a (positive definite) covariance matrix. Both the low-rank term $\bm{V}(x)$ and the diagonal term $\bm{d}(x)$ are calculated as a linear function of the pre-logit layer's output.

To learn the per-input covariance matrices from the training set, one has to construct a predictive estimate from $\mathcal{N}(\bm{f}(x), \bm{\Sigma}(x))$ using any of the methods in~\cref{app:list_predictive}. This predictive estimate is then trained using a standard cross-entropy (NLL) loss.

HET uses a temperature parameter to scale the logits before calculating the BMA. This is chosen using a validation set.

In~\cref{sec:making_approx_exact}, we show that when our constraint is satisfied, the off-diagonal terms of the covariance matrix do not matter for the predictive. This means that, in our framework, one can discard the low-rank term $\bm{V}(x)$ and only model the diagonal term $\bm{d}(x)$ without a decrease in expressivity. To keep comparisons fair and use the same backbone with the same number of parameters, we also only model $\bm{d}(x)$ for softmax-based predictives.

\subsection{Laplace Approximation}

The Laplace approximation~\cite{daxberger2021laplace} approximates the posterior $p(\bm{\theta} \mid \mathcal{D})$ over the network parameters $\bm{\theta}$ for a Gaussian prior $p(\bm{\theta})$ and likelihood defined by the network architecture by a Gaussian. In its simplest form, it uses the maximum a posteriori (MAP) weights $\bm{\theta}_\text{MAP} \in \mathbb{R}^P$ as the mean and the inverse Hessian of the \emph{regularized} loss over the training set $\tilde{\mathcal{L}}(\bm{\theta}; \mathcal{D}) = \mathcal{L}(\bm{\theta}; \mathcal{D}) + \lambda \Vert\bm{\theta}\Vert_2^2$ evaluated at the MAP as the covariance matrix:
\begin{equation}
\mathcal{N}\left(\bm{\theta}_\text{MAP}, \left(\left.\frac{\partial^2 \tilde{\mathcal{L}}(\bm{\theta}; \mathcal{D})}{\partial \theta_i \partial \theta_j}\right|_{\bm{\theta}_\text{MAP}}\right)^{-1}\right) = \mathcal{N}\left(\bm{\theta}_\text{MAP}, \left(\left.\frac{\partial^2 \mathcal{L}(\bm{\theta}; \mathcal{D})}{\partial \theta_i \partial \theta_j}\right|_{\bm{\theta}_\text{MAP}} + \lambda \bm{I}_P\right)^{-1}\right).
\end{equation}
This is a \emph{locally optimal} post-hoc Gaussian approximation of the true posterior $p(\bm{\theta} \mid \mathcal{D})$ based on a second-order Taylor approximation. For details, see~\cite{tatzel2025debiasing}.

For deep neural networks, the Hessian matrix is often replaced with the Generalized Gauss-Newton (GGN) matrix. The GGN is guaranteed to be positive semidefinite even for suboptimal weights and has efficient approximation schemes, such as Kronecker-Factored Approximate Curvature~\cite{martens2015optimizing} or low-rank approximations.

Denoting our curvature estimate of choice by $\bm{G}$, the logit-space Gaussian is obtained by pushing forward the weight-space Gaussian measure through the \emph{linearised} model around $\bm{\theta}_\text{MAP}$. For an input $x \in \mathcal{X}$, this results in
\begin{equation}
\mathcal{N}\left(\left(\bm{J}_{\bm{\theta}_\text{MAP}} \bm{f}(x)\right)\bm{\theta}_\text{MAP}, \left(\bm{J}_{\bm{\theta}_\text{MAP}} \bm{f}(x)\right)\left(\bm{G} + \lambda \bm{I}_P\right)^{-1}\left(\bm{J}_{\bm{\theta}_\text{MAP}} \bm{f}(x)\right)^\top\right),
\end{equation}
where $\bm{J}_{\bm{\theta}_\text{MAP}} \bm{f}(x) \in \mathbb{R}^{C \times P}$ is the model Jacobian matrix.

We use a last-layer KFAC Laplace variant in our experiments and use the \emph{full} training set for calculating the GGN instead of a mini-batch based on recent works on the bias in mini-batch estimates~\cite{tatzel2025debiasing}.

\section{CIFAR-10 Experiments}
\label{app:cifar_10}

This appendix section repeats the experiments presented in the main paper on the CIFAR-10 dataset. For a detailed description of the experimental setup, refer to \cref{app:experimental_setup}. \cref{app:benchmark_tasks} describes the used tasks and metrics.

As stated in the main paper, our two research questions are:
\begin{itemize}
    \item What are the effects of changing the learning objective? (\cref{app:changing_the_learning_objective})
    \item Do we have to sacrifice performance for sample-free predictives? (\cref{app:quality_of_sample_free_predictives})
\end{itemize}

\subsection{Quality of Sample-Free Predictives}
\label{app:quality_of_sample_free_predictives}

\begin{table}[t]
\footnotesize
\caption{Comparison of ECE results for different predictives using a fixed Laplace backbone.}
\label{tab:cifar10_pred_probs}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Mean} & \textbf{Std} \\
\midrule
\multicolumn{3}{c}{\textbf{Softmax Laplace}} \\
\midrule
MC 100 & 0.0096 & 0.0013 \\
MC 1000 & 0.0102 & 0.0016 \\
MC 10 & 0.0120 & 0.0013 \\
Mean Field & 0.0121 & 0.0029 \\
Laplace Bridge Predictive & 0.5933 & 0.0105 \\
\midrule
\multicolumn{3}{c}{\textbf{NormCDF Laplace}} \\
\midrule
Analytic & 0.0074 & 0.0012 \\
MC 1000 & 0.0092 & 0.0022 \\
MC 100 & 0.0095 & 0.0015 \\
MC 10 & 0.0100 & 0.0020 \\
\bottomrule
\end{tabular}
\end{table}

Similarly to the main paper, in this section, we investigate our first research question: whether there is a price to pay for sample-free predictives. \cref{tab:cifar10_pred_probs} showcases the two best-performing (activation, method) pairs on the ECE metric and the CIFAR-10 dataset: Softmax and NormCDF Laplace. Mean Field (MF) is a strong alternative for sample-free predictives, but it has no guarantees and can fall behind MC sampling (see also \cref{fig:synthetic_exp}). When the constraint is satisfied, the analytic NormCDF predictive is \emph{exact}. Empirically, our analytic predictives always perform on par with MC sampling.

\subsection{Effects of Changing the Learning Objective}
\label{app:changing_the_learning_objective}

As in the main paper, in this section, we use the best-performing predictive and estimator (see~\cref{app:list_predictive}) for softmax models and employ our methods with the analytic predictives.

\subsubsection{Calibration and Proper Scoring}

We first evaluate calibration using the log probability scoring rule~\cite{gneiting2007strictly} and the Expected Calibration Error (ECE) metric~\cite{naeini2015obtaining}. \cref{fig:cifar10_log_prob} shows that on CIFAR-10, the score of our analytic predictives with implicit constraints (Sigmoid, NormCDF) are consistently better than the corresponding softmax results for all methods.

\begin{figure}[t]
    \centering
    \includegraphics{gfx/cifar10/ece_hard_bma_correctness_original.pdf}
    \caption{CIFAR-10 ECE results. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) or explicit (\colorrectangle{GoogleBlue}) constraints outperform Softmax (\colorrectangle{GoogleYellow}) on HET and SNGP. Laplace tunes its hyperparameters based on the ECE metric -- NormCDF Laplace is the overall best method. Note the restricted $y$-limits for readability.}
    \label{fig:cifar10_ece}
\end{figure}

\cref{fig:cifar10_ece} shows that on CIFAR-10, our analytic predictives have a clear advantage on HET and SNGP. Laplace is a post-hoc method that tunes its hyperparameters on the ECE metric, hence its enhanced performance. Our NormCDF predictive is on par with Softmax.

\subsubsection{Correctness Prediction}

\begin{figure}[t]
    \centering
    \includegraphics{gfx/cifar10/log_prob_score_hard_bma_correctness_original.pdf}
    \caption{CIFAR-10 log probability proper scoring results for the binary correctness prediction task. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) constraints consistently outperform Softmax (\colorrectangle{GoogleYellow}) on all methods.}
    \label{fig:cifar10_log_prob_corr_pred}
\end{figure}

\cref{fig:cifar10_log_prob_corr_pred} shows that on the correctness prediction task, our analytic predictives with implicit constraints outperform all Softmax predictives across all methods, as measured by the log probability proper scoring rule.

\subsubsection{Accuracy}

\begin{figure}[t]
    \centering
    \includegraphics{gfx/cifar10/hard_bma_accuracy_original.pdf}
    \caption{\textbf{Analytic predictives do not sacrifice accuracy.} CIFAR-10 accuracies. Analytic predictives with implicit (\colorrectangle{GoogleGreen}) or explicit (\colorrectangle{GoogleBlue}) constraints either outperform or are on par with Softmax (\colorrectangle{GoogleYellow}) across all methods.}
    \label{fig:cifar10_accuracy}
\end{figure}

Analytic predictives do not sacrifice accuracy. \cref{fig:cifar10_accuracy} evidences this claim on CIFAR-10: our analytic predictives either outperform or are on par with Softmax predictives. The most accurate method is Sigmoid HET. These results support the findings of~\citet{wightman_resnet_2021} that showcase desirable training dynamics of the class-wise cross-entropy loss.

\subsubsection{Out-of-Distribution Detection}

\begin{figure}[t]
    \centering
    \includegraphics{gfx/cifar10/auroc_oodness.pdf}
    \caption{CIFAR-10C OOD detection AUROC results for severity level one. Across all methods, the best-performing predictive is analytic with an implicit constraint (\colorrectangle{GoogleGreen}).}
    \label{fig:cifar10_ood_detection}
\end{figure}

Finally, we consider the OOD detection task on a balanced mixture of ID (CIFAR-10) and OOD inputs. As OOD inputs, we consider corrupted CIFAR-10C samples. We use the AUROC metric to evaluate the methods' performance. As shown in \cref{fig:cifar10_ood_detection}, the best-performing method is Sigmoid SNGP, an analytic method. Generally, Softmax performs on par with our analytic predictives. Intuitively, separating ID and OOD samples does not require a fine-grained representation of uncertainty, unlike the ECE or proper scoring rules. Nevertheless, the analytic predictives and second-order Dirichlet distributions are considerably cheaper to calculate than Softmax MC predictions (see \cref{sec:computational_gains}).

\section{ImageNet Dirichlet Evaluation}
\label{app:dirichlet}

This appendix section evaluates our moment-matched Dirichlet distributions on the OOD detection task~(\cref{app:benchmark_tasks}). \cref{tab:imagenet_dirichlet} shows the per-estimator performances of the three best-performing (method, activation) pairs on ImageNet: Sigmoid Laplace, NormCDF HET, and NormCDF Laplace. In all three cases, the best estimator is derived from the moment-matched Dirichlet distributions.

\begin{table}[t]
\footnotesize
\caption{Comparison of OOD AUROC results for different uncertainty estimators on the three best-performing (method, activation) pairs: Sigmoid Laplace, NormCDF HET, and NormCDF Laplace. The best estimator is derived from the moment-matched Dirichlet distributions in all three cases.}
\label{tab:imagenet_dirichlet}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Mean} & \textbf{Std} \\
\midrule
\multicolumn{3}{c}{\textbf{Sigmoid Laplace}} \\
\midrule
Dirichlet Mutual Information & 0.6388 & 0.0005 \\
Analytic Predictive Entropy & 0.6353 & 0.0003 \\
MC 1000 Predictive Entropy & 0.6323 & 0.0001 \\
MC 100 Predictive Entropy & 0.6323 & 0.0001 \\
MC 1000 Expected Entropy & 0.6321 & 0.0001 \\
\midrule
\multicolumn{3}{c}{\textbf{NormCDF HET}} \\
\midrule
Dirichlet Expected Entropy & 0.6353 & 0.0021 \\
MC 1000 Expected Entropy & 0.6281 & 0.0020 \\
MC 100 Expected Entropy & 0.6281 & 0.0020 \\
MC 10 Expected Entropy & 0.6281 & 0.0020 \\
Analytic Predictive Entropy & 0.6277 & 0.0021 \\
\midrule
\multicolumn{3}{c}{\textbf{NormCDF Laplace}} \\
\midrule
Dirichlet Expected Entropy & 0.6321 & 0.0009 \\
MC 100 Predictive Entropy & 0.6296 & 0.0012 \\
MC 1000 Predictive Entropy & 0.6296 & 0.0012 \\
Closed-Form Predictive Entropy & 0.6296 & 0.0012 \\
MC 10 Predictive Entropy & 0.6294 & 0.0012 \\
\bottomrule
\end{tabular}
\end{table}

\section{Further Out-of-Distribution Detection Results}
\label{app:ood}

\cref{fig:all_ood} shows OOD detection results across all ImageNet-C severity levels. Our analytic predictives with implicit constraints consistently outperform Softmax.

\begin{figure}[t]
\centering
\subfloat[OOD detection AUROC with severity level one.]{\includegraphics[width=0.48\linewidth]{gfx/imagenet/auroc_oodness.pdf}}\\
\subfloat[OOD detection AUROC with severity level two.]{\includegraphics[width=0.48\linewidth]{gfx/imagenet/ood/s2.pdf}}\hfill
\subfloat[OOD detection AUROC with severity level three.]{\includegraphics[width=0.48\linewidth]{gfx/imagenet/ood/s3.pdf}}\\
\subfloat[OOD detection AUROC with severity level four.]{\includegraphics[width=0.48\linewidth]{gfx/imagenet/ood/s4.pdf}}\hfill
\subfloat[OOD detection AUROC with severity level five.]{\includegraphics[width=0.48\linewidth]{gfx/imagenet/ood/s5.pdf}}\\
\caption{The OOD detection performance of all methods increases steadily as we increase the severity of the perturbed half of the mixed dataset on the ImageNet validation dataset. Our analytic predictives consistently outperform Softmax.}
\label{fig:all_ood}
\end{figure}


\end{document}



% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
