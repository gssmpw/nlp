\documentclass[pre, twocolumn, amsmath, amssymb, superscriptaddress]{revtex4}


\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on the decimal point
\usepackage{bm}% bold math
\usepackage{braket}
% color can be used to apply background shading to table cells only
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
%\usepackage{caption}
\usepackage{siunitx}
\usepackage{xspace}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{{\bf\textcolor{blue}{#1}}}

\newcommand{\rv}{\textbf{r}}
\newcommand{\qv}{\textbf{q}}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\DeclareMathOperator{\sign}{sign}

\newcommand{\eqname}{{Eq.}\xspace}

% Definire un nuovo contatore per le figure di supporto
\newcounter{supportingfigure}
\renewcommand{\thesupportingfigure}{S\arabic{supportingfigure}}

% Definire un nuovo comando per le figure di supporto
\newcommand{\supportingfigure}[2][]{%
    \refstepcounter{supportingfigure}%
    \begin{figure}[ht]%
        \centering
        \includegraphics[width=0.5\textwidth]{#2}%
        \caption[#1]{\textbf{Fig. \thesupportingfigure}: #1}%
        \label{fig:support\thesupportingfigure}%
    \end{figure}%
}


\begin{document}

\title{Breaking the bonds of generative artificial intelligence\\ by minimizing the maximum entropy}
%Min-max entropy algorithm: an entropic approach to generative artificial intelligence

\author{Mattia Miotto}
\affiliation{Center for Life Nanoscience, Istituto Italiano di Tecnologia, Viale Regina Elena 291,  00161, Rome, Italy}

\author{Lorenzo Monacelli}
\affiliation{Department of Physics, Sapienza University, Piazzale Aldo Moro 5, 00185, Rome, Italy}





\begin{abstract}
The emergence of generative artificial intelligence (GenAI), comprising large language models, text-to-image generators, and AI algorithms for medical drug and material design, had a transformative impact on society. However, despite an initial exponential growth surpassing Moore's law, progress is now plateauing, suggesting we are approaching the limits of current technology. Indeed, these models are notoriously data-hungry, prone to overfitting, and challenging to direct during the generative process, hampering their effective professional employment. % for professional music, image, and video editing.

To cope with these limitations, we propose a paradigm shift in GenAI by introducing an \emph{ab initio} method based on the minimal maximum entropy principle.
Our approach does not fit the data. Instead, it compresses information in the training set by finding a latent representation parameterized by arbitrary nonlinear functions, such as neural networks.
%, where the maximum entropy is minimized.
%mathematically grounded approach that guarantees convergence to the exact distribution, even with finite datasets. 
%Our method leverages a deep neural network of arbitrary topology to capture underlying features and patterns based on a novel minimal maximum entropy alsatz. 
%The result is a general physical model, based on an effective Hamiltonian, through which novel samples can be generated via standard simulation algorithm, like Metropolis or Newtonian dynamics. 
The result is a general physics-driven model, which is data-efficient, resistant to overfitting, and flexible, permitting to control and influence the generative process. % without the need for fine-tuning or retraining.

Benchmarking shows that our method outperforms variational autoencoders (VAEs) with similar neural architectures, particularly on undersampled datasets. We demonstrate the method’s effectiveness in generating images, even with  limited training data, and its 
%on the MNIST dataset, 
%excelling with limited training data. The method offer 
unprecedented capability to customize the generation process \emph{a posteriori} without the need of any fine-tuning or retraining. %This advancement brings us closer to a general model of reality, a critical step toward achieving Artificial General Intelligence.
\end{abstract}


\maketitle


%\blue{MAIN. Introduction to the problem, the existing models, and the problems: we basically are finishing the trainable data.\\}

%\section{Main}
Generative artificial intelligence (GenAI) refers to models capable of algorithmically producing -- novel -- data resembling those from the training set. %sampling a probability distribution that resembles the training set.
Text generative models, for instance, predict the probability of each possible next token, \textit{i.e.} clusters of characters, of a sequence to generate a plausible continuation of an initial prompt~\cite{Lv2023}. %GenAI is being successfully applied also for designing novel materials~\cite{Zeni2025}, medical drugs~\cite{Wu2024,Gangwal2024}, and/or functional proteins~\cite{Trinquier2021}.

Multiple algorithms have been developed for such tasks, each offering distinct advantages depending on the data type. For instance, transformers are particularly effective for sequence generation, as seen in large language models \cite{vaswani_attention_2023}, while Generative Adversarial Networks (GANs) \cite{Goodfellow2020}, Variational Autoencoders (VAEs) \cite{kingma_auto-encoding_2022}, and Diffusion models \cite{diffmodel} are well-suited for handling multidimensional data, such as images.
Thanks to these models/architectures, GenAI is being used to address a wide range of complex problems~\cite{Gupta2024}, from designing drugs~\cite{Wu2024, Gangwal2024} and functional proteins\cite{Trinquier2021} to the discovery of novel materials~\cite{Zeni2025}, from advertising and entertainment~\cite{Totlani2023} to  education~\cite{Law2024} and communication~\cite{Roumeliotis2023}. 
% train the generative model against a discriminator that attempts to identify whether a sample belongs to the training set or is produced by the algorithm~\cite{Goodfellow2020}. %Transformers are an evolution of recursive neural networks where the recursion is replaced by multiple attention layers, allowing for highly parallelizable training~\cite{vaswani_attention_2023}. 
%Variational Autoencoders (VAEs) are composed by an encoder that compresses the original data into a low dimensional latent space where the training set is distributed according to a normal distribution and a decoder capable of reverting points from the latent space back into their corresponding form~\cite{kingma_auto-encoding_2022}. Diffusion models operate a denoising process, where the final data is reconstructed from random noise~\cite{diffmodel}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figure1-crop.pdf}
    \caption{ \textbf{Illustration of the minimal maximum entropy principle.} \textbf{a)} Two dimensional representation of a possible  entropy landscape as a function of two reaction coordinates, with a dark blue line marking the values of the entropy subject to a given constraint/observable. Colors shift from red to blue as the entropy increases.   Green and red dots identify the real and maximum entropies values, respectively.  
    \textbf{b)} The observables/constrains can be defined in an unsupervised way using a neural network to extract the relevant features from the input data. Starting from a fully connected network with equal weights, the minimal maximum entropy algorithm adjusts the network parameters to obtain the constrain whose associated maximum entropy is minimum, i.e. closer to the real entropy. }
    \label{fig:1}
\end{figure}


Parallel to the enormous spreading of GenAI, serious ethical concerns regarding the generated content are being formulated~\cite{carlini2023}, and the feasibility of further improving by just scaling existing architectures is questioned~\cite{Jones2024}. Indeed, the extremely data-greedy nature of most GenAI models is leading to a saturation of the available data~\cite{Jones2024}, while training with generated samples is demonstrated to poison the models~\cite{Shumailov2024, poison_data}.  
In addition, GenAI models tend to overfit~\cite{carlini2023}, verbatim memorizing entire parts of the training set~\cite{ippolito2022preventing}, with consequential ethical concerns about the generated content closely resembling copyright-protected items or personal data~\cite{copyrightTimes,dzuong2024uncertain}. %In particular, in cases when it is not trivial to assess if the final parametrization of a model embeds a compressed copy of some of the training data that can be generated with a specific prompt, thus representing a real copyright violation upon distribution. 

With some remarkable exceptions, most GenAI solutions struggle to find a transformative impact. While these models can rapidly generate novel samples, customizing the results remains challenging, often requiring multiple, supervised random attempts to steer the outcome \cite{li_controlnet_2024,huang2023composer}. Considerable efforts have been revolved to mitigate this issue~\cite{zhang_adding_2023,li_controlnet_2024,hu_lora_2021,xie2023smartbrush,huang2023composer,rombach2022high}, that still relies on model-specific solutions, requiring \textit{ad hoc} retraining each time the underlying model is updated. 

%Moreover, the inter\red{Black box \cite{Rai2019, Hassija2023}}


Our work introduces a paradigm shift to GenAI that addresses these limitations by proposing a novel method based on the 
`minimal maximum entropy' principle. 
%Different from all other generative processes, this approach is proven to converge to the exact probability distribution even in the presence of a finite data set.

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{Figure2-crop.pdf}
    \caption{\textbf{Principal component analysis as a special Min-MaxEnt solution.} \textbf{a)} Example of 2D normal distributed data. Red and green bars represent the squared roots of the variances in two different basis. \textbf{b)} Entropy as a function of the rotation angle of the basis in which data are represented.}
    \label{fig:2}
\end{figure*}


\section{The minimal maximum entropy principle}

Maximum entropy is a guiding principle to assign probabilities to events~\cite{Bialek2012-lp}. Indeed,  maximizing entropy selects the most unbiased probability distribution consistent with given constraints, ensuring no unwarranted assumptions are made beyond the available information. Given a set of measures $f_i(x)$, the probability distribution that (i) maximizes entropy and (ii) ensures that the expected values of $f_i$ match those observed in the training set, is given by:
\begin{equation}
    \tilde P_{f_i,\lambda_i}(x) = \frac{1}{Z}\exp\left[-\sum_i \lambda_i f_i(x)\right],
    \label{eq:maxent}
\end{equation}
with $Z = \int dx \exp\left[-\sum_i \lambda_i f_i(x)\right]$. Here, the integral marginalize over all possible configurations $x$ and $\lambda_i$ are Lagrange multipliers that constrain the average values of the measures $f_i(x)$ to match those of the training set.
\begin{equation}
    \int dx f_i(x) \frac{1}{Z}\exp\left[-\sum_i \lambda_i f_i(x)\right] = \frac{1}{N}\sum_{\{x\}_{\text{train set}}} f_i(x) = \mu_i.
    \label{eq:constrain}
\end{equation}

%Starting from the maximum entropy principle, one can define the MaxEnt machine-learning algorithm to model the probability distribution $P(x)$ underlying a dataset, where $x$ is a particular configuration that could represent, e.g., a text string, an image, an audio track, or a chemical compound. The knowledge of the probability distribution allows for the generation of random samples via, e.g., a Metropolis Markov's chain. 
Operationally, training a maximum entropy model consists of the following steps: i) define a set of measures $f_i(x)$, ii) compute the average values $\mu_i$ of such measures on the training dataset, iii) solve iteratively \eqname~\eqref{eq:constrain} to find the values of $\lambda_i$.
Once we got the converged $\lambda_i$ values, the maximum entropy principle ensures that the entropy of the target $\tilde P_{f_i, \lambda_i}(x)$ is always above the exact entropy~\cite{ecolat}. Since both the real probability distribution, $P(x)$, and the MaxEnt one $\tilde P_{f_i, \lambda_i}(x)$ satisfy the constraints 
$ \int dx f_i(x) P(x) = \mu_i$, they belong to the same manifold of all the distributions satisfying $\left<f_i(x)\right> = \mu_i$, where $\braket{\cdot}$ indicates the expected value (sketched in Figure~\ref{fig:1}\textbf{a}). $\tilde P_{f_i,\lambda_i}(x)$ maximizes the entropy within this manifold, thus, $S[\tilde P_{f_i,\lambda_i}] $ is always higher or equal to the real distribution Shannon's entropy $S[P]$: $S[\tilde P_{f_i,\lambda_i}] \ge S[P]$. 
%gives a graphical representation of the process: the constraints \eqname~\eqref{eq:constrain} define a manifold (blue line in the heat map) in the space of probability distributions where both the MaxEnt and the real distributions lie. 
%, defined as
%\begin{equation}
%    S[P] = - \int d\sigma P(\sigma)\ln P(\sigma).
%    \label{eq:entropy}
%\end{equation}


The inequality between real and MaxEnt entropy sets up a new variational principle: for a fixed set of measures $f_i$, the entropy of the corresponding MaxEnt distribution is always above the real one. The residual entropy difference quantifies how much information can be further extracted from the data by improving the choice of $f_i(x)$.
%how well a specific set of measures captures correlations in the real probability distribution. 

From the minimal maximum entropy principle, we can introduce a Min-MaxEnt algorithm to optimize the set of measures $f_i$ by minimizing the entropy of the MaxEnt distribution $S[\tilde P{f_i, \lambda_i}]$:
\begin{equation}
    S[P] = \min_{f_i} S[\tilde P_{f_i,\lambda}]
    \label{eq:minmaxent:opt}
\end{equation}
The equality in \eqname~\eqref{eq:minmaxent:opt} holds as it always exists a set of measures $f_i$ that uniquely define a probability distribution~\cite{ecolat}.
In practice, the measures $f_i$ can be parametrized as generic nonlinear functions of the configurations, depending on a vector of parameters $\theta_1, \cdots, \theta_n$ (see Figure~\ref{fig:1}), and the minimization of the entropy can be performed directly optimizing $\theta_1, \cdots, \theta_n$:
\begin{equation}
    \tilde P\left[\{\theta_i\}_1^n, \{\lambda_i\}_1^n\right](x) = \frac{1}{Z} \exp\left[-\sum_i \lambda_i f_i[\theta_1, \cdots, \theta_n](x)\right]
\end{equation}

\begin{figure*}
    \centering
    \includegraphics[width=1.\linewidth]{Figure3-crop.pdf}
    \caption{\textbf{Inference of a bimodal normal distribution.} \textbf{a)} Inferred Min-MaxEnt distribution as a function of the training epochs for a dataset of 1000  bimodal normal variables. In the top panel, the best-inferred MaxEnt and Min-MaxEnt distributions are reported in red and blue, respectively. Real data distribution is shown in gray. \textbf{b)} Values of the Lagrangian multipliers as a function of the training epochs. \textbf{c)} Modulus of the gradient of the observables and \textbf{d)} Lagrangian multipliers as a function of the training epochs. \textbf{e)} Best inferred  Min-MaxEnt distributions and real data distribution for a training performed with a dataset composed of 10 samples. }
    \label{fig:3}
\end{figure*}



Therefore, we must simultaneously train the $\lambda_i$ parameters to constrain the averages of $f_i$ (\eqname~\ref{eq:constrain}), and the $\theta_i$ parameters to minimize the maximum entropy (\eqname~\ref{eq:minmaxent:opt}).
To optimize $\lambda_i$, we define a cost function quantifying the displacement of the $f_i$ averages between training and generated samples~\cite{ecolat, miotto_tolomeo_2021}: 
%Multiple functions have been employed in this case~\cite{ecolat, miotto_tolomeo_2021}; here, we employ a slight modification of the standard mean squared error
\begin{equation}
    \chi^2 = \sum_i\frac{(\braket{f_i} - \mu_i)^2}{\sigma_i^2} 
    %\sigma_i^2 = \int d\sigma P(\sigma) \left[f_i[\theta_1, \cdots, \theta_n](\sigma) - \mu_i\right]^2
    \label{eq:chi2}
\end{equation}
where $\sigma_i^2$ is the variance of the $i$-th observable on the training dataset. The minimization of \eqname~\eqref{eq:chi2} can be pre-conditioned as discussed in Ref. \cite{ecolat}. The parameters $\lambda_i$ are updated with a gradient descend algorithm according to
%\begin{multline}
%    \mu_i \approx \frac 1 N \sum_{i = 1}^N f_i[\theta_1, \cdots,\theta_n](\sigma_i)
%    \\
%    \sigma_i^2 \approx \frac{1}{N(N-1)}\sum_{i = 1}^N \left[f_i[\theta_1, \cdots,\theta_n](\sigma_i) - \mu_i\right]^2    
\begin{equation}
    \lambda_i \longrightarrow \lambda_i - \eta \frac{\partial \chi^2}{\partial\lambda_i}
    \label{eq:update:lambda}
\end{equation}
(In this work, we employed the ADAM algorithm~\cite{ADAM}).
%One of the advantages of the MSE cost function in \eqname~\eqref{eq:chi2} is that it automatically encodes information about when the minimization should be stopped. If the $f_i$ are uncorrelated, \eqname~\eqref{eq:chi2} represent a valid $\chi^2$ stochastic variable which quantifies statistical significance: when it becomes lower than the number of observables, we are fitting the random noise arising from the finite summation in the expression of the average values $\mu_i$. The case of correlated $f_i$ can be appropriately handled with a generalization of \eqname~\eqref{eq:chi2} as introduced in Ref.~\cite{ecolat}.
%The gradient of $\chi^2$ provides an update rule for $\lambda_i$ as a standard MaxEnt algorithm. 
Next, we introduce an update rule also for the $\theta_i$ parameters that decrease the maximum entropy. 
As sketched in Figure~\ref{fig:1}b, updating $\theta_1\cdots\theta_n$  progressively modifies the constraint manifold, minimizing the entropy of the corresponding MaxEnt distribution.
This is achieved by computing the gradient of the MaxEnt distribution's entropy
\begin{equation}
    \theta_i \longrightarrow \theta_i - \eta \frac{dS}{d\theta_i} 
    \label{eq:update:theta}.
\end{equation}

Notably, the entropy itself is practically incomputable. However, the entropy gradient is a standard observable and can be evaluated efficiently as:
\begin{equation}
    \frac{dS}{d\theta_i} = - \sum_j \lambda_j \left(\left<\frac{d f_j}{d\theta_j}\right>_{\tilde P[\theta_1, \cdots,\theta_n,\lambda_1, \cdots,\lambda_n]} - \left<\frac{d f_j}{d\theta_j}\right>_{P}\right),
    \label{eq:minim:entropy}
\end{equation}
where $\left<\frac{d f_j}{d\theta_j}\right>_{\tilde P[\theta_1, \cdots,\theta_n,\lambda_1, \cdots,\lambda_n]}$ is obtained by averaging an ensemble of configurations generated with the current MaxEnt distribution, while $\left<\frac{d f_j}{d\theta_j}\right>_{P}$ is evaluated on the real distribution, i.e., the training set. The formal proof of \eqname~\eqref{eq:minim:entropy} is reported in Supplementary Materials~\ref{sec:entropy:derivative}.
\eqname~\eqref{eq:minim:entropy} can be implemented with the usual backpropagation by defining an auxiliary cost function $\tilde S$
\begin{multline}
    \tilde S(\theta_1,\cdots,\theta_n) = \frac{1}{N_1}\sum_{i = 1}^{N_1} \lambda_i f_i[\theta_1,\cdots,\theta_n](x_i) - \\ -  \frac{1}{N_2}\sum_{i = 1}^{N_2} \lambda_i f_i[\theta_1,\cdots,\theta_n](\tilde x_i), 
\end{multline}
\begin{equation}
    \frac{dS}{d\theta_i} = \frac{\partial\tilde S}{\partial\theta_i},
\end{equation}
where the configurations $\tilde x_i$ are sampled through the MaxEnt probability distribution. 
The gradient of $\tilde S$ does not depend explicitly on the probability distribution, therefore, the backpropagation is fast as it does not require running through the ensemble generation.  %However, the auxiliary function $\tilde S$ is not the real entropy, and it is not ensured to decrease through the optimization.
Note that the double optimization of all the $\theta_i$ and $\lambda_i$ parameters works like an adversary competition: the $\lambda_i$ optimization aims at maximizing entropy with the given set of constraints, while the $\theta_i$ optimization alters the set of constraints to minimize the entropy of the distribution, extracting order from disorder. 
Unlike most machine-learning approaches, the optimization rule in \eqname~\eqref{eq:minim:entropy} does not evaluate a distance between the generated data and the training, thus mitigating the risks of overfitting.


In the following sections, we discuss different applications of our approach. First, we focus on a special case where analytical insight can be gained.  In particular, we show that Principal component analysis (PCA) can be formally derived from the Min-MaxEnt principle. Next, we probe the capability of the Min-MaxEnt to infer different kinds of 1D bimodal distributions against the predictions of standard MaxEnt 
%constraining mean values and variance, , where the constraints are optimized via a
%neural network,
and variational autoencoders (VAE). Finally, we apply the method to the contest of image generation,
%on the MNIST data set,
 demonstrating (i) its
capabilities when trained on a small subset of data,  (ii) how it can be refined via adversary network training, and (iii) how controlled generation can be easily enforced on the trained model. 


\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{Figure4-crop.pdf}
    \caption{\textbf{Min-MaxEnt vs variational auto-encoder.} \textbf{a)} Inferred Min-MaxEnt distribution as a function of the training epochs for a dataset of 1000  bimodal Lorentians variables. In the top panel, the best-inferred Min-MaxEnt distribution is reported blue, while real data distribution is shown in gray. \textbf{b)} Inferred VAE distribution as a function of the training epochs for a dataset of 1000  bimodal Lorentians variables. In the top panel, 
    the best-inferred VAE distribution is reported red, while real data distribution is shown in gray. \textbf{c)} Kullback–Leibler divergence between real and inferred distributions as a function of the training epochs for the Min-MaxEnt and VAE methods.}
    \label{fig:4}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{Figure5-crop.pdf}
    \caption{ \textbf{Image generation.} Example of the generative power of the proposed Min-MaxEnt principle in the case of images. Starting from the 8x8 MNIST dataset, a Min-MaxEnt is trained using a deep neural network with 16 output observable. The results can be further refined using a discriminator network to bias the generation process. }
    \label{fig:5}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{Figure_GAN.pdf}
    \caption{ \textbf{Discriminator procedure.} \textbf{a)} Probability distribution of the post-training discrimination score for MNIST data (real) and Min-MaxEnt data (generated). 
    \textbf{b)} Probability distribution of the post-training discrimination score for data generated via the Min-MaxEnt algorithm with the addition of the discriminator bias.}
    \label{fig:gan}
\end{figure}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figure6-crop.pdf}
    \caption{\textbf{Example of preferential-generation process.} \textbf{a)} Pictorial representation of the Min-MaxEnt process in MNIST dataset: pseudo-hamiltonian, $H_0$, can be interpreted as a pseudo-potential whose minima correspond to different kinds of configurations of the system, i.e., numbers.  Adding a classifier neural network as a perturbation $H'$ to $H_0$  alters the probability of sampling certain kinds of configurations. \textbf{b)} Example of biased generations of 0 (top row) and 2 (bottom row). \textbf{c)} Classifier capacity of assigning a given configuration to numbers from 0 to 9.
    \textbf{d)} Comparison between a configuration obtained via a Monte Carlo process using only $H_0$ vs using only $H'$.}
    \label{fig:6}
\end{figure*}

\subsection{Principal component analysis as a special Min-MaxEnt solution}

Principal component analysis is a widely used statistical approach to represent high-dimensional data in its essential features or principal components. Such components are obtained by the linear combinations of the original variables that diagonalize the covariance matrix. % the explained variance of all the variables. 
Due to its capability to reduce dimensionality by retaining only the components with the highest variances, PCA finds wide applications, especially in fields characterized by the presence of vast amounts of data, from bioinformatics~\cite{Ma2011} to particle physics~\cite{Altsybeev2020}, in tasks like estimating missing values in huge data matrices, sparse component estimation, and the analysis of images, shapes, and functions~\cite{Greenacre2022}. 

%To get analytical insight on our theoretical framework, we consider the case in which the chosen observables are squares of linear combination of the system variables. 
In the following, we demonstrate that PCA can be retrieved as a particular solution of the minimal maximum entropy principle by constraining the variance of an arbitrary linear combination of the system variables.     
Figure~\ref{fig:2}a) shows a straightforward 2D case in which data (gray dots) are drawn from a general probability distribution with a covariance matrix $\Sigma_{ij} = \left<x_ix_j\right>_P$ of elements $\Sigma_{11}=3$, $\Sigma_{22}=2$, and $\Sigma_{12}=1$. 

To constrain the variances of a linear combination of the variables, we define the $f_i[\theta](x, y)$ observables as
\begin{equation}
\left\{\begin{array}{l}
f_1(x) = (\cos\theta x + \sin\theta y)^2 \\ 
f_2(x) = (-\sin\theta x + \cos\theta y)^2
\end{array}\right. .
\label{eq:pca2:obs}
\end{equation}
The corresponding MaxEnt solution is a normal distribution of the form
\begin{align}
    \tilde P[\theta](x,y) \propto \exp\bigg[&\lambda_1 \left(\cos\theta x + \sin\theta y\right)^2 + \nonumber \\ 
    &\lambda_2\left(-\sin\theta x + \cos\theta y\right)^2\bigg],
\end{align}
where the $\lambda_i$ can be found analytically imposing that the averages of $f_i(x)$ matches with the exact distribution:
\begin{eqnarray}
    \tilde\Sigma_{11} = \frac{1}{2\lambda_1} = \Sigma_{11}\cos^2\theta + \sin^2\theta \Sigma_{22} + \Sigma_{12}\sin2\theta,\\
    \tilde\Sigma_{22} = \frac{1}{2\lambda_2} = \Sigma_{11}\sin^2\theta + \cos^2\theta \Sigma_{22} - \Sigma_{12}\sin2\theta.
\end{eqnarray}
The entropy of this MaxEnt distribution (Gaussian) is analytical and only depends on the determinant of its covariance matrix, i.e., the product $\tilde\Sigma_{11}\tilde\Sigma_{22}$
\begin{equation}
    S  = \frac{1}{2} \ln\left(4\pi^2 e^2\tilde\Sigma_{11}\tilde\Sigma_{22} \right) .
\end{equation}
Figure~\ref{fig:2}b displays the entropy as a function of the rotation angle $\theta$. 
Minimizing $S$ is equivalent to minimizing the product $\tilde\Sigma_{11}\tilde\Sigma_{22}$. It can be shown that

\begin{multline}
\tilde\Sigma_{11}\tilde\Sigma_{22} = \left(\frac{\sin 2\theta}{2}[\Sigma_{11} -\Sigma_{22}] -  \cos 2\theta  \Sigma_{12}\right)^2 +  \text{const},
\label{eq:}
\end{multline}
which is trivially minimized for
\begin{equation}
\theta^\star = \frac{1}{2} \arctan\left(\frac{2~\Sigma_{12}}{\Sigma_{11}-\Sigma_{22}}\right)   \label{eq:angle}
\end{equation}

The angle in \eqname~\eqref{eq:angle} also identifies the rotation diagonalizing the real covariance matrix $\Sigma$, i.e. the PCA solution. In fact, diagonalizing $\sigma$, requires that:
\begin{equation}
\left<(\cos\theta^\star x + \sin\theta^\star y)(\cos\theta^\star y - \sin\theta^\star x)\right>_P = 0.
\end{equation}
This establishes that the Min-MaxEnt distribution coincides with the PCA solution when the observables $f_i$ are defined as in \eqname~\eqref{eq:pca2:obs}. In the Supplementary Materials, we generalize this result to arbitrary dimensions, proving that the optimal PCA rotation emerges naturally from minimizing the MaxEnt entropy while constraining variances along an arbitrary basis.

%For normally distributed data, the entropy of the system can be computed analytically and is found to be proportional to $\ln{\text{det}\Sigma}$, i.e. to the logarithm of the determinant of the covariance matrix. For a 2D distribution, the latter is given by $\Sigma_{11}\Sigma_{22} - \Sigma_{12}^2$. Thus, the presence of covariance across the data reduces the entropy. The maximum entropy distribution associated to constrained variances is, in fact, a 2D normal distribution with zero covariance. To get the exact probability distribution, one has to add covariance as a third observable. However, it is still possible to get the exact distribution constraining only variances if we perform a rotation of the data.
%In fact, we can define the rotated  system variables as $y_i = \left(\sum_j^2 \theta_j x_j \right)$, choose as observables the variances,  $f_i = y_i^2$ and compute the MaxEnt distribution associate to each possible rotation. 
%, we see that the PCA rotation corresponds to the case in which the maximum entropy computed on the variable variances is the minimum one (for such a simple system it also corresponds to the real entropy).
%Indeed, performing a PCA on the data corresponds to diagonalizing the covariance matrix, i.e., finding the rotation of the basis for which the covariance matrix is diagonal. 
%Such a simple example indicates given a certain number of observables, it is possible to improve the accuracy of the inferred distribution introducing parametrized-observables and optimizing the parameters on which the observables depends. 
%Let us assume that the system real probability distribution is of the form:
%\begin{equation}
%    P(\vec{x}) = \frac{1}{ \left(2\pi~\text{det}\Sigma\right)^{\frac{n}{2}}} e^{- \frac{1}{2} (\vec{x} - \vec{\mu})^T \Sigma^{-1}(\vec{x} - \vec{\mu})}
%    %\frac{1}{2\sqrt{a}\sqrt{b}} e^{-(a x_1 + b x_2)^2 - (a x_1 - b x_2)^2}
%\end{equation}
%i.e. it is a multivariate normal distribution where $\Sigma$ is the covariance matrix, $n$ is the number of dimension of the phase space and $\vec\mu$ is the n-dimensional vector of the mean values.
%We will show that the well known Principal Component Analysis (PCA) is the exact solution of the proposed MinMax Entropy theory for this system choosing as observables the squares of linear combinations of the variables. 


\subsection{Neural network as Min-MaxEnt observables}

It is generally challenging to capture complex data patterns with an \emph{ad hoc} parametrization of the observables $f_i(x)$. In this section, we implement the $f_i(x)$ measures as the output layer of a neural network, which parametrizes a general nonlinear function. Specifically, we test the method's performance on a dataset generated from one-dimensional bimodal distributions.
First, we draw a set of 1000 training data (see the gray histogram in the top panel Figure~\ref{fig:3}a) from a bimodal distribution $P = \frac{1}{2}\mathcal{N}(x, \bar{x}_1, \sigma_1) + \frac{1}{2}\mathcal{N}(x, \bar{x}_2, \sigma_2)$, where $\mathcal{N}$ is a normal distribution with mean $\bar x$ and variance $\sigma^2$. 
Figure~\ref{fig:3} shows the results for a Min-MaxEnt run using two observables obtained as output nodes of a multilayer perceptron. Figure~\ref{fig:3}a displays the evolution of the predicted Min-MaxEnt probability distribution as the training epochs increase. The top panel compares the real data, the Min-MaxEnt distribution after 1000 epochs, and the MaxEnt distribution constraining mean and variance of $x$. After a few hundred epochs, the Min-MaxEnt distribution perfectly captures both the bimodality and the variances of the single peaks. % that two observables are the minimum number of parameters required to describe the shape of this distribution; in fact, absolute and square value (or combinations thereof) analytically define the distribution. 
Notably, the Min-MaxEnt algorithm can infer the distribution without overfitting the data also in the case of scarce input data. Figure~\ref{fig:3}e displays the inferred Min-MaxEnt distribution (blue) trained only on 10 data points (gray). 

A more challenging test is to replace the normal distributions with a Cauchy (Lorentzian) distribution, which allows for rare events to occur far from the distribution peaks. In this case, we compare the Min-MaxEnt with a Variational Autoencoder where the Encoder network has the same architecture as the $f_i$ parametrization (see Figure~\ref{fig:4}).
The 1000 training data are drawn from the distribution $P(x) = \frac{1}{2}\mathcal{L}(x, {x}_0^1, \lambda_1) + \frac{1}{2}\mathcal{L}(x, {x}^2_0, \lambda_2)$, where $\mathcal{L}$ is a Cauchy distribution with median $x_0$ and half width at half maximum (HWHM) $\lambda$. Figure~\ref{fig:4}a and Figure~\ref{fig:4}b) display the training of the Min-MaxEnt and VAE, respectively. 
%Details on the networks architectures are reported in  \red{Methods}. 
%from the evolution of the probability distributions as a function of the training epoch, Min-MaxEnt at first rapidly converges to a bimodal distribution and then starts adjusting their shapes. VAE instead requires more training steps to acquire bimodality. 
The final distributions are shown in the top panels with the real data. %While the Min-MaxEnt distribution slightly overestimates the widths of the two peaks, it correctly reproduces the relative spectral weights of the peaks and the presence of fat tails. VAE distribution fails to grasp these features instead. 
To quantitatively measure the difference between predicted and real distributions, in Figure~\ref{fig:4}c, we reported the Kullback-Leibler divergence between real and inferred distributions as a function of the training epochs. The better result of the Min-MaxEnt reflects the tendency of the VAE to overfit rare events, producing a noisy background and overestimating the probability in the regions far from the distribution peaks. This accounts for the notorious issue of VAE, which suffers from blurry generated samples compared to the data they have been trained on~\cite{Bredell2023}.


\subsection{Min-MaxEnt for image generation}

Next, we applied the Min-MaxEnt algorithm to the case of image generation,  using the MNIST dataset~\cite{mnist}, a collection of 1797 images of greyscale labeled handwritten digits, that are represented as 8x8 pixel matrices. We selected a training set of 200 images, discarding the labels to train the model. The observables are defined through a convolutional neural network (CNN) made by two convolutional layers, tailed by a last fully connected ending in 16 output nodes. The images are generated according to the MaxEnt probability distribution using a Metropolis-Monte Carlo Markov's chain~\cite{Hastings1970,ecolat,miotto_tolomeo_2021}.
%where $f_i(x|\theta_1\cdots \theta_n)$ are the value of the output nodes of the CNN, $\theta_j$ are the parameters of the CNN, and $\lambda_i$ the Lagrange multiplier. $\lambda_i$ and $\theta_j$ are optimized following the update rules in \eqname~\eqref{eq:update:lambda} and \eqref{eq:update:theta}. Images are generated in parallel batches via a Metropolis-Monte Carlo algorithm on the probability distribution \eqname~\eqref{eq:prob:image} starting from a random configuration\cite{Hastings1970,ecolat,miotto_tolomeo_2021}.

%As the optimization proceeds, the $\lambda$ parameters constrain the average CNN output of the generated images to the average output of the training set (\eqname~\eqref{eq:chi2}), while the $\theta$ optimization reduces the entropy of the generated data (\eqname~\ref{eq:dSdtheta}). 
The entropy reduction is evident from \figurename~\ref{fig:5}, where the generative process evolves from noisy images at the first epochs (high entropy) to more defined outputs. 
Unlike all other generative algorithms, the training procedures never enforce the model's output to replicate the training set. Therefore, no memory of specific images are stored within the network, preventing overfit and helping with generalization.

Once the model is trained, we have a final probability distribution from which images can be extracted. Consequently, it is possible to define an effective energy landscape for images, which can be further used to direct the generation process:
\begin{equation}
H_0(x) = \sum_{i=1}^{16} f_i[\theta_1\cdots\theta_n](x)\lambda_i.
\end{equation}
Similarly to physical systems, generation can be directed by adding an \emph{external field} $H'(x)$. 
For instance, we can train an independent network $g(x)$ to recognize generated data like  
\begin{equation}
    g(x) = \left\{\begin{array}{lr}
    1 & \text{generated image} \\
    0 & \text{real image}
    \end{array}\right.
\end{equation}
By altering the energy landscape as
\begin{equation}
    H(x) = H_0(x) + \underbrace{\alpha g(x)}_{H'(x)},
    \label{eq:gan}
\end{equation}
we can extract biased samples that are guaranteed to be indistinguishable from the original training set according to the network $g(x)$ (see Figure~\ref{fig:gan}). This process can be repeated by updating the training set of the discriminator to include some of the data generated with $H(x)$. 
Such an approach resembles the adversary network training, which can be applied efficiently to the Min-MaxEnt. In fact, the only network that needs to be re-trained is the discriminator since \eqname~\eqref{eq:gan} automatically generates indistinguishable samples for the respective classifier.

The effective energy landscape framework is also helpful for biasing the generation process toward specific targets. In most GenAI models, this involves some retraining of the network. In contrast, the generation through the Min-MaxEnt algorithm can be conditioned by introducing an external field modeled via a simple CNN classifier. For example, we trained a CNN classifier $h_i(x)$ to guess the labels encoding the written digit of the MNIST dataset (a task extremely easy for networks) like 
\begin{equation}
    h_i(x) = \left\{\begin{array}{lr}
         1 & \text{if } x\text{ represents the number }i \\
         0&\text{otherwise} 
    \end{array}\right.
\end{equation}
The biased generation is performed with the new energy landscape as
\begin{equation}
    H(x) = H_0(x) \underbrace{-\alpha  h_j(x) + \alpha \sum_{i\neq j}  h_j(x)}_{H'(x)}.
\end{equation}
The external field $H'(x)$ favors the generation of images that $h(x)$ classifies as the $j$ number. The result is shown in \figurename~\ref{fig:6}.
The classifier increases the potential energy around numbers different from the target and decreases it around the target. Interestingly, numbers generated via this method appear more readable, as images that cannot be clearly classified as one of the digits are un-favored. The training of $h(x)$ is completely independent of the Min-MaxEnt training, as the only training set employed is the original dataset of real images.
Panel d of \figurename~\ref{fig:6} shows what happens if we turn off the Min-Max-Ent Hamiltonian, and generate only according to the classifier. In this case, the generative process explores random and noisy configurations where the CNN has no training data, thus entering fake energy minima due to extrapolation.


\section{Discussion}

%has transcended the realm of science fiction and 
Generative artificial intelligence is now permeating everyday life at a pace that has surpassed Mooore's law. However, while it is being applied to an ever-increasing spectrum of applications, its spread is coupled with some intrinsic flaws, both at the technical and conceptual levels.
These comprise, among others, the enormous amount of data needed for the training and the possibility of over-fitting to increase performances~\cite{ippolito2022preventing, carlini2023, Jones2024}. %, and the difficulty/impossibility of effectively getting insight into the learning process (resulting in black-box tools).
Therefore, concerns arise on the persistence of significant progresses in GenAI capabilities, on the ethics/legitimacy of using the generated data, and ultimately, the effective long-term impact of GenAI on society.  

The Min-MaxEnt model is a novel generative AI algorithm that differs from most competitors by two significant features: (i) the approach stems from a solid theoretical apparatus rooted in fundamental physics and information theory, (ii) the cost function at the basis of its training never accounts for any metric distance between the generated samples and the training set.
%The `distance' function in \eqname~\eqref{eq:chi2}, employed to train the $\lambda_i$ parameters, is not a distance between samples, but on average values computed on the samples.
As a consequence, the training process is never directly exposed to the training set, thus making it extremely hard for the model to overfit. %In principle, it would still be possible to overfit in cases where the number of observables is big enough to generate an energy landscape with funnels located at each data point of the training set. However, such a Hamiltonian is highly unstable, with a variance of the observables over the training set going to zero. To prevent this occurrence, we divided the $\chi^2$-like distance in the \eqname~\ref{eq:chi2} by the variance. 

The robustness of the Min-MaxEnt to overfit has the promise to lift the concerns regarding copyright violation and fair use of publicly distributed data to train generative models~\cite{copyrightTimes,dzuong2024uncertain}. Indeed, 
 Min-MaxEnt models learn only generalized features/patterns across all the training sets. %, so no specific copyright-protected features are trained if the training set is not limited to one specific copyright-protected target. 
The approach also proved efficient, compared to state-of-the-art methods, when using small datasets (see \figurename~\ref{fig:3}) and/or in the presence of rare events (\figurename~\ref{fig:4}). Moreover, the ability to control the output via discriminator networks trained \emph{a priori} promotes more effective interaction between the user and the generative process (see \figurename~\ref{fig:5} and \figurename~\ref{fig:6}), currently a significant limitation of most GenAI algorithms.
As shown in \figurename~\ref{fig:gan}, the Min-MaxEnt can bypass GenAI detection mechanisms without retraining the model by adding the detection function as a bias in the generative process. While this provides a systematic approach to enhancing the quality of synthetic data, in turn it may raise ethical concerns about the capability of  algorithmically distinguishing real from  generated content, which could have significant social implications. Therefore, it will be crucial for production applications built on this approach to address these concerns by incorporating watermarks or implementing mechanisms for identifying generated content.
%\red{However, including the discriminator as a bias still requires multiple evaluations during the generative process, which could be hampered if the discriminator is not publicly available or there is a significant cooldown time between allowed requests, if available via web service.}

%Requiring a dynamical, challenging to parallelize, process for generation and training, the method is computationally more expensive with respect to generate data from standard networks of the same size. However, we proved that training requires smaller networks to achieve competitive performances thanks to the physical insight gained by the minimal maximum entropy principle.

In conclusion, Min-MaxEnt stands out as a first principles approach to GenAI, offering a fundamentally different perspective to the field and enabling to overcome current limitations of state-of-the-art approaches. %We anticipate that 
%Further developing the algorithmic efficiency and engineering novel network architectures for the constraints
% will enable overcoming the current limitations of state-of-the-art approaches.



%will serve as a guiding principle for GenAI as much as maximum entropy allowed for the interpretation of thermodynamics. %\section{Online content}
%Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at: XXX


\subsection*{Author contributions statement}
M.M and L.M. conceived the research, developed the model, wrote and revised the manuscript. 

\subsection*{Competing Interests statement}
The authors declare no competing financial or non-financial interests.


%\subsection*{Acknowledgments}
%L. M. acknowledges the European Union under the program H2020 for the MSCA individual fellowship, grant number 101018714.


\bibliographystyle{unsrtnat}
\bibliography{mybib}

\onecolumngrid
\renewcommand{\appendixname}{Supplementary Materials}
\appendix
\section{Methods}

\subsection{Total derivative of the entropy}
\label{sec:entropy:derivative}
Consider a probability distribution of the form $P = \frac{e^{-\beta H}}{Z}$ with $Z = \int dx e^{-\beta H}$, where is given by $H = \sum_i \lambda_i f_i$, i.e. is the sum of N observable, $\{f_i\}_N$ each multiplied by its Lagrangian multiplier, $\{\lambda_i\}_N$ and assume that the observables (and in turn the multipliers) are function of a set of parameters, $\{\theta_\rho\}_M$. 
The entropy of such system is given by $S(\theta, \lambda(\theta) = - \int dx P \text{ln}(P)$ 

The total derivative of the entropy with respect to the parameters of the observables is given by:

\begin{equation}
\label{eq:totalderivative}
    \frac{dS}{d\theta_\rho} = \frac{\partial S}{\partial\theta_\rho} + \sum_i \frac{\partial S}{\partial\lambda_i} \frac{d \lambda_i}{d\theta_\rho}  
\end{equation}



where we have that:

\begin{equation}
\frac{\partial S}{\partial\theta_\rho} = \frac{\partial}{\partial\theta\rho} \left[ -\int dx P ln(P) \right]  = \frac{\partial}{\partial\theta\rho} \left[ -\int dx P  [ -\beta H - ln(Z)  \right] = \beta \frac{\partial<H>_P}{\partial\theta\rho} + \frac{\partial ln(Z)}{\partial\theta\rho} 
\end{equation}

Using the relation: $\frac{\partial lnZ}{\partial\theta\rho} = \frac{\partial Z}{\partial\theta\rho}/Z = \int dx P \frac{\partial H}{\partial\theta_\rho} = -\beta \braket{\frac{\partial H}{\partial\theta_\rho}}$, we get to:

\begin{equation}
\label{eq:dSdtheta}
\frac{\partial S}{\partial\theta_\rho} =  {-\beta} \left( \braket{\frac{\partial H}{\partial\theta_\rho}}_P -  \frac{\partial<H>_P}{\partial\theta_\rho}
\right)
\end{equation}
where $\braket{\cdot}_P$, indicates the average over the  probability distribution, $P$.

This equation in turn can be written as:
\begin{equation}
    \frac{\partial S}{\partial\theta_\rho} =  {-\beta} \left( \braket{\frac{\partial H}{\partial\theta_\rho}}_P - \int dx \frac{\partial H}{\partial \theta_\rho}  P - \int dx \frac{\partial P}{\partial\theta_\rho} H  \right)
\end{equation}
that reduces to:

\begin{equation}
    \frac{\partial S}{\partial\theta_\rho} = {+\beta} \braket{\sum_i\lambda_i f_i \frac{\partial \ln P}{\partial\theta_\rho}}
\end{equation}

The above equation can be further solved to yield:

\begin{equation}
    \frac{\partial S}{\partial\theta_\rho} = - {\beta}\braket{\sum_i\lambda_i f_i} \frac{\partial \ln Z}{\partial\theta_\rho}  {-\beta^2} \sum_{ij}\lambda_i\lambda_j \braket{f_i \frac{df_j}{d\theta_\rho}}
\end{equation}

where 

\begin{equation}
    \frac{\partial \ln Z}{\partial\theta_\rho} = {-}\beta \braket{\sum_i \lambda_i \frac{df_i}{d\theta_\rho}}
\end{equation}

Plugging all terms together, the final expression of $\frac{\partial S}{\partial\theta_\rho}$ becomes:

\begin{equation}
    \frac{\partial S}{\partial\theta_\rho} = {-\beta^2} \sum_{ij}\lambda_i\lambda_j\left(\braket{f_i \frac{df_j}{d\theta_\rho}} - \braket{f_i}\braket{\frac{df_j}{d\theta_\rho}}\right)
\end{equation}

The second term of Eq.~\ref{eq:totalderivative} instead is given by two contributions. In particular, the term $\frac{\partial S}{\partial \lambda}$ assume the same formal expression of Eq.~\ref{eq:dSdtheta}:

\begin{equation}
    \frac{\partial S}{\partial \lambda_i} = {-\beta} \left( \braket{\frac{\partial H}{\partial\lambda_i}}_P -  \frac{\partial<H>_P}{\partial\lambda_i} \right) = {-\beta}\left(\braket{f_i} - \frac{\partial}{\partial\lambda_i} \braket{H}\right)
\end{equation}

This can be written as

\begin{equation}
    \frac{\partial S}{\partial \lambda_i} = {-\beta} \left[ \braket{f_i} - \frac{\partial}{\partial\lambda_i} \int dx \left(\sum_j \lambda_j f_j\right) P(\lambda, x) \right] = {-\beta} \left[ \braket{f_i} - \int dx \left(\sum_j \lambda_j f_j\right)  \frac{\partial P}{\partial\lambda_i} - \int dx f_i  P\right]  
\end{equation}

which simplifies in:

\begin{equation}
    \frac{\partial S}{\partial \lambda_i} = {+\beta} \braket{\left(\sum_j \lambda_j f_j\right) \frac{\partial \ln P}{\partial\lambda_i}}
\end{equation}

The logarithm of the probability distribution is given by:

$$
\ln P = -\beta\sum_i\lambda_if_i - \ln\left(\int dx e^{-\beta\sum_i\lambda_i f_i}\right)
$$

Thus the derivative of this term with respect to the i-th Lagrangian multiplier assumes the form:

\begin{equation}
    \frac{\partial \ln P}{\partial\lambda_i} = -\beta f_i - \int dx \frac{-\beta  f_i e^{-\sum_k\beta\lambda_k f_k}}{Z} =  - \beta (f_i -  \braket{f_i})
\end{equation}

Plugging all terms together, we obtain:

\begin{equation}
    \frac{\partial S}{\partial \lambda_j} =  {-\beta^2}  \left< \left(\sum_i \lambda_i f_i\right)\left(f_j - \braket{f_j}\right) \right>
\end{equation}

Noting that:
$$
\braket{ f_i\left(f_j - \braket{f_j}\right) }-\underbrace{\braket{ \braket{f_i}\left(f_j - \braket{f_j}\right) }}_{ = 0} = \braket{ \left(f_i - \braket{f_i}\right)\left(f_j - \braket{f_j}\right) }
$$

we get to the final expression:

\begin{equation}
    \frac{\partial S}{\partial \lambda_j} =  {-\beta^2} \sum_i \lambda_i \left(\Sigma_{\text{MC}}\right)_{ij}
\end{equation}

with $\Sigma_{\text{MC}}$ being the covariance matrix computed on the configurations.

Finally, to get an expression for the derivative of the Lagrangian multiplier with respect to the observable,  we need to adopt a perturbation approach ($H \rightarrow H + \delta H$).  In this framework, we have that  $H' = H_0 + \sum_i d\lambda_i f_i + \lambda_i df_i$
and  assume that $\braket{f_i}_{H'} = \braket{f_i}_P$.

Computing the average of an observable over configurations generated by $H'$ equals to: 

$$
\braket{f_i}_H = \braket{f_i + df_i}_{H_0 + \delta H}
$$

The probability distribution terms become:
\begin{equation}
e^{-\beta H'} = e^{-\beta H_0} e^{-\beta \sum_i d\lambda_i f_i} e^{-\beta \sum_i \lambda_i df_i}
\end{equation}

and since the perturbation is small and linear, we have:
\begin{equation}
e^{-\beta H'} \simeq e^{-\beta H_0} \left(1 -\beta \sum_i \left( d\lambda_i f_i + \lambda_i df_i\right) \right)
\end{equation}

The partition function is given by:
\begin{equation}
Z' = \int dx e^{-\beta H'} = \int dx e^{-\beta H_0} \left(1 	-\beta \sum_i \left( d\lambda_i f_i + \lambda_i df_i\right) \right)
\end{equation}

which can be recast into

\begin{equation}
\frac{Z'}{Z_0} = 1 - \beta \sum_i \left( d\lambda_i \braket{f_i} + \lambda_i \braket{d f_i}\right)
\end{equation}

It follows that:

\begin{equation}
\braket{f_i + df_i}_{H'} = \int \left[ f_i + df_i \right]\frac{e^{-\beta H'}Z_0}{Z' Z_0} =
\int \left[ f_i + df_i \right] \frac{1 - \beta \sum_i \left( d\lambda_i f_i + \lambda_i df_i \right) }{1 - \beta \sum_i \left( d\lambda_i \braket{f_i} + \lambda_i \braket{df_i} \right)}\frac{e^{-\beta H_o}}{Z_0} 
\end{equation}


Keeping again only the first-order terms,  we have:

\begin{equation}
\braket{f_i + df_i}_{H'} = \braket{df_i}_{H_0} -  \beta \sum_{j} \left( d\lambda_j \braket{f_i f_j} + \lambda_j \braket{ df_j f_i} \right) + \beta \braket{f_i} \sum_j \left( d\lambda_j \braket{f_j} + \lambda_j \braket{df_j} \right)
\end{equation}
We must now impose that
\begin{equation}
    \braket{f_i + df_i}_{H'} = \braket{f_i + df_i}_P
\end{equation}
Keeping only the first order term in both, we have
\begin{equation}
    \braket{df_i}_P = \braket{df_i}_{H_0} -  \beta \sum_{j} \left( d\lambda_j \braket{f_i f_j} + \lambda_j \braket{ df_j f_i} \right) + \beta \braket{f_i} \sum_j \left( d\lambda_j \braket{f_j} + \lambda_j \braket{df_j} \right)
\end{equation}
\begin{equation}
\label{eq:mid}
\braket{df_j}_P - \braket{df_j}_{H_0} = 
-\beta \sum_i \lambda_i \left( \braket{f_j df_i} - \braket{f_j} \braket{df_i} \right) -
\beta \sum_i d\lambda_i \left( \braket{f_j f_i} - \braket{f_j} \braket{f_i}
\right)
\end{equation}

We can define the covariance matrix with respect to the $H_o$ distribution as
\begin{equation}
\Sigma_{ij}^{\text{MC}} = \braket{f_i f_j}_{H_0} - \braket{f_i}_{H_0}\braket{f_j}_{H_0}
\end{equation}

and reorganize terms in equation~\ref{eq:mid} to get

\begin{equation}
\braket{df_j}_P - \braket{df_j}_{H_0} = 
-\beta \sum_i \lambda_i \left( \braket{f_j df_i} - \braket{f_j} \braket{df_i} \right) -
\beta \sum_i d\lambda_i \Sigma^\text{MC}_{ij}
\end{equation}

and

\begin{equation}
\sum_i d\lambda_i \Sigma_{ij}^{\text{MC}} = \frac{\braket{df_j}_{H_0} - \braket{df_j}_{P}}{\beta} - 
\sum_i \lambda_i\left(  \braket{f_j df_i}_{H_0} - \braket{f_j}_{H_0}\braket{df_i}_{H_0}  \right)
\end{equation}

Inverting the relation, we get the response function

\begin{equation}
 d\lambda_i  =  \frac{1}{\beta}  \sum_j \left( \Sigma^{-1}_{\text{MC}} \right)_{ij}   \left( \braket{df_j}_{H_0} - \braket{df_j}_{P} \right)  -  
\sum_{jk}   \lambda_k \left( \Sigma^{-1}_{\text{MC}} \right)_{ij} 
 \left(  \braket{f_j df_k}_{H_0} - \braket{f_j}_{H_0}\braket{df_k}_{H_0}  \right)
\end{equation}



%Setting $\beta = 1$ for notation simplicity, 
We can differentiate with respect to the parameter:


\begin{equation}
 \frac{d\lambda_i}{d\theta_\rho}  =  {\frac{1}{\beta}} \sum_j \left( \Sigma^{-1}_{\text{MC}} \right)_{ij}   \left( \braket{  \frac{df_j}{d\theta_\rho} }_{H_0} - \braket{  \frac{df_j}{d\theta_\rho}  }_{P} \right)  - 
\sum_{jk}   \lambda_k \left( \Sigma^{-1}_{\text{MC}} \right)_{ij} 
 \left(  \braket{f_j \frac{df_k}{d\theta_\rho}}_{H_0} - \braket{f_j}_{H_0}\braket{ \frac{df_k}{d\theta_\rho}}_{H_0}  \right)
\end{equation}


%We can for convenience rewrite the last part as
%\begin{equation}
% \sum_{jk}   \lambda_k \left( \Sigma^{-1}_{\text{MC}} \right)_{ij} \left(  \braket{f_j \frac{df_k}{d\theta_\rho}}_{H_0} - \braket{f_j}_{H_0}\braket{ \frac{df_k}{d\theta_\rho}}_{H_0}  \right) =  \left< \left[\sum_j \left(\Sigma_\text{MC}^{-1}\right)_{ij} f_j\right] \left(\sum_k \lambda_k \frac{df_k}{d\theta_\rho}\right)\right>_c
%\end{equation}

The second term of Eq.~\ref{eq:totalderivative}
can now be composed as
\begin{equation}
    \sum_{i}\frac{\partial S}{\partial\lambda_i}\frac{d\lambda_i}{d\theta_\rho} = {-\beta} \sum_h \lambda_h \sum_j\delta_{hj}\left( \braket{  \frac{df_j}{d\theta_\rho} }_{H_0} - \braket{  \frac{df_j}{d\theta_\rho}  }_{P} \right)  {+\beta^2} \sum_h \lambda_h \sum_{jk}\delta_{hj}\lambda_k \left(  \braket{f_j \frac{df_k}{d\theta_\rho}}_{H_0} - \braket{f_j}_{H_0}\braket{ \frac{df_k}{d\theta_\rho}}_{H_0}  \right)
\end{equation}

where the Kronecker $\delta$s arises from the multiplication of the covariance matrix with its inverse. Simplifying the indexes, the expression assumes the form:

\begin{equation}
    \sum_{i}\frac{\partial S}{\partial\lambda_i}\frac{d\lambda_i}{d\theta_\rho} = {-\beta} \sum_h \lambda_h \left( \braket{  \frac{df_h}{d\theta_\rho} }_{H_0} - \braket{  \frac{df_h}{d\theta_\rho}  }_{P} \right) \underbrace{{+\beta^2} \sum_h \lambda_h \sum_{k}\lambda_k \left(  \braket{f_h \frac{df_k}{d\theta_\rho}}_{H_0} - \braket{f_h}_{H_0}\braket{ \frac{df_k}{d\theta_\rho}}_{H_0}  \right)}_{ = {- \frac{\partial S}{\partial\theta_\rho}}}
\end{equation}

The final expression for the total derivative of the entropy is

\begin{equation}
    \frac{dS}{d\theta_\rho} = {-\beta} \sum_h \lambda_h \left( \braket{  \frac{df_h}{d\theta_\rho} }_{H_0} - \braket{  \frac{df_h}{d\theta_\rho}  }_{P} \right)
\end{equation}


%\subsection{Minimal maximum entropy  of the 2D normal distribution}


 
\subsection{Principal component analysis as MinMax Ent solution of variable linear combinations}

Here, we provide the detailed proof that PCA is a special solution of the minimal maximum entropy principle using as constraints/observables  the squares of linear combinations of the system variables. 

Let us assume that the system real probability distribution is 
%of the form:
%\begin{equation}
%    P(\vec{x}) = \frac{1}{ \left(2\pi~\text{det}\Sigma\right)^{\frac{n}{2}}} e^{- \frac{1}{2} (\vec{x} - \vec{\mu})^T \Sigma^{-1}(\vec{x} - \vec{\mu})}
    %\frac{1}{2\sqrt{a}\sqrt{b}} e^{-(a x_1 + b x_2)^2 - (a x_1 - b x_2)^2}
%\end{equation}
%i.e. it is a multivariate normal distribution where 
characterized by a  covariance matrix, $\Sigma$,  and null  n-dimensional vector of the mean values, $\vec\mu=0$.

%We will show that the well known Principal Component Analysis (PCA) is the exact solution of the proposed MinMax Entropy theory for this system choosing as observables the squares of linear combinations of the variables. 

\subsubsection{Two dimensional case}

To begin with, we start considering the two-dimensional case assuming that $\vec\mu = 0$ (this condition can always be achieved simply translating the system variables). Next, we define the observables as $f_i = y_i^2 = \left(\sum_j^n \theta_j x_j \right)^2$, i.e. as the variance of the linear combination of the system variables.

%= \theta x_1 + \beta x_2$ and $y_2 = \theta x_1 - \beta x_2$. Note that we can assume for simplicity that the means of $x_1$ and $x_2$ are nil. 

In this case, it is easy to show that the maximum entropy distribution is a normal distribution of the form:
\begin{equation}
    \tilde{P} = \frac{e^{ \sum_i \lambda_i y_i^2}}{Z} = \frac{e^H_0}{Z}
\end{equation}

with $\lambda_i = -\frac{1}{2<y_i^2>}_P$, which depend on the set of $\{\theta_i\}$ parameters. We have now restrained the probability space to the region that maximizes entropy with respect to the constraints. Now one could ask what are the values of the parameters that yield the minimum entropy in this region. 
In other words, we want to solve the system:
\begin{align}
 \frac{dS}{d\theta_i}  = 0 
\end{align}

To begin with, let us start with the 2D case, where data can be easily represented as the scatter plot in Figure~\ref{fig:2}. 

%Using the canonical expression of the Shannon entropy one finds that the entropy of a multivariate normal distribution is given by:
%\begin{equation}
%    S =  - \int P ln P  = \frac{1}{2} \ln\left(\text{det} \Sigma \right) + \frac{n}{2}(1+\ln{2\pi})
%\end{equation}
%which tells us that the entropy is proportional to the volume the data occupied in the covariance space (see Figure~\ref{fig:1}b). 


%Since the obtained MaxEnt probability distribution has the form of a multivariate normal distribution with a covariance given by the equality $\sum_i \lambda_i (\sum_j a_j x_j)^2 = \vec{x}^T \cdot \Sigma_{ME}^{-1} \cdot \vec{x}$, we can see how finding the Min-MaxEnt distribution maps in the problem of determining the set of parameters that rotates the system in such a way the volume of the data is minimum.Figure~\ref{fig:1}c gives a schematic representation of the problem.

In the 2D case, we have that:
\begin{equation}
\bar{y} = \begin{bmatrix}
y_1 \\
y_2 
\end{bmatrix} = \Theta \begin{bmatrix}
x_1 \\
x_2 
\end{bmatrix} 
\quad\text{with} \quad \Theta = \begin{bmatrix}
\theta_{11} & \theta_{12} \\
\theta_{21} & \theta_{22} 
\end{bmatrix} 
\label{eq:lincomb}
\end{equation}

Imposing, without loosing generality, that the linear combination is a rotation, equals to requiring that $\Theta \Theta^{T} = I$. The relationship is satisfied by a matrix of the form: 
\begin{equation}
\Theta = \frac{1}{\sqrt{\alpha^2 + \beta^2}}\begin{bmatrix}
\alpha & -\beta \\
\beta & \alpha 
\end{bmatrix}
\end{equation}



 We can now use  expression \ref{eq:minim:entropy} to find the values of the parameters:
\begin{align}
\frac{dS}{d\alpha} = \lambda_1 \left( \left< 2 y_1\frac{\partial y_1}{\partial \alpha}\right>_{H_0} -  \left< 2 y_1\frac{\partial y_1}{\partial \alpha}\right>_{P} \right) + \lambda_2 \left( \left< 2 y_2\frac{\partial y_2}{\partial \alpha}\right>_{H_0} -  \left< 2 y_2\frac{\partial y_2}{\partial \alpha}\right>_{P} \right) = 0
\label{eq:dS1=0}
\end{align}


while the second term is given by:

\begin{align}
\frac{dS}{d\beta} = \lambda_1 \left( \left< 2 y_1\frac{\partial y_1}{\partial \beta}\right>_{H_0} -  \left< 2 y_1\frac{\partial y_1}{\partial  \beta}\right>_{P} \right) + \lambda_2 \left( \left< 2 y_2\frac{\partial y_2}{\partial  \beta}\right>_{H_0} -  \left< 2 y_2\frac{\partial y_2}{\partial  \beta}\right>_{P} \right) = 0
\label{eq:dS2=0}
\end{align}

From Eq.~\ref{eq:lincomb}, we can compute the term $\partial y_i/\partial \alpha$ and $\partial y_i/\partial \beta$ as:
\begin{equation}
\begin{cases}
\frac{\partial y_1}{\partial \alpha} = \frac{x_1}{\sqrt{\alpha^2 + \beta^2}} - y_1\frac{\alpha}{\alpha^2 + \beta^2} = \frac{\beta y_2}{\alpha^2 + \beta^2}\\
\frac{\partial y_1}{\partial \beta} = -\frac{x_2}{\sqrt{\alpha^2 + \beta^2}} - y_1\frac{\beta}{\alpha^2 + \beta^2}  =  -\frac{\alpha y_2}{\alpha^2 + \beta^2}\\
\frac{\partial y_2}{\partial \alpha} = \frac{x_2}{\sqrt{\alpha^2 + \beta^2}} - y_2\frac{\alpha}{\alpha^2 + \beta^2} = -\frac{\beta y_1}{\alpha^2 + \beta^2}\\
\frac{\partial y_2}{\partial \beta} = \frac{x_1}{\sqrt{\alpha^2 + \beta^2}} - y_2\frac{\beta}{\alpha^2 + \beta^2} = \frac{\alpha y_1}{\alpha^2 + \beta^2}\\
\end{cases}
\end{equation}

where we used the inverse relation:

\begin{equation}
\bar{x} = \begin{bmatrix}
x_1 \\
x_2 
\end{bmatrix} = \frac{1}{\sqrt{\alpha^2 + \beta^2}}\begin{bmatrix}
\alpha & \beta \\
-\beta & \alpha 
\end{bmatrix} \bar{y}
\end{equation}

Substituting the expressions for the derivatives in Eq.~\ref{eq:dS1=0}, one gets to:

\begin{align}
\frac{dS}{d\alpha} = \lambda_1 \frac{2\beta}{\alpha^2 + \beta^2} \left( \left< y_1y_2\right>_{H_0} -  \left< y_1y_2\right>_{P} \right) - \lambda_2 \frac{2\beta}{\alpha^2 + \beta^2} \left( \left<y_1y_2\right>_{H_0} -  \left< y_1y_2\right>_{P} \right) = 0
\end{align}

Simplifying the notation and noting that $<y1 y2>_{H_0}=0$, one get to the final expression:
\begin{equation}
(\lambda_1 - \lambda_2)\left<  y_1 y_2 \right>_P = 0
\end{equation}
The previous expression can be satisfied in the trivial case in which the two Lagrangian multipliers are the same or by a   linear combination of the variables such that:
\begin{equation}
<y_1y_2>_P =  \alpha\beta(<x_1x_1>_P- <x_2x_2>_P) + <x_1x_2>_P(\alpha^2+\beta^2)= 0    
\label{eq:pca_rel}
\end{equation}

\subsubsection{Principal component transformation}
To get the meaning of Eq.~\ref{eq:pca_rel}, we can go through the simple calculation of the 2D PCA transformation. Given a 2D covariance matrix, the PCA transformation equals to  find the basis that diagonalizes the covariance matrix, i.e.

\begin{equation}
U \Sigma U^t = \frac{1}{\sqrt{\alpha^2 + \beta^2}}\begin{bmatrix}
\alpha & -\beta \\
\beta & \alpha 
\end{bmatrix}
\begin{bmatrix}
\left<x_1 x_1\right>_P & \left<x_1 x_2\right>_P \\
\left<x_1 x_2\right>_P & \left<x_2 x_2\right>_P 
\end{bmatrix}
\frac{1}{\sqrt{\alpha^2 + \beta^2}}\begin{bmatrix}
\alpha & \beta \\
-\beta & \alpha 
\end{bmatrix}
= \begin{bmatrix}
\tilde{\lambda_1} & 0 \\
0 & \tilde{\lambda_1} 
\end{bmatrix}
\label{eq:pca}
\end{equation}

where the $U$ matrix is written in such a way that the two columns are linearly independent, while each column correspond to a normalized vector. Note that this results in the same format Eq.~\ref{eq:lincomb}.
A straightforward calculation leads to recast Eq.~\ref{eq:pca} as:
\begin{equation}
\begin{bmatrix}
\left<y_1 y_1\right>_P & \left<y_1 y_2\right>_P \\
\left<y_1 y_2\right>_P & \left<y_2 y_2\right>_P 
\end{bmatrix}
= \begin{bmatrix}
\tilde{\lambda_1} & 0 \\
0 & \tilde{\lambda_1} 
\end{bmatrix}
\end{equation}

which proves that fulfilling relation Eq.~\ref{eq:pca_rel} equals to finding the PCA transformation.


\subsubsection{N-dimensional case}

To demonstrate the validity of the previous result in a n-dimensional case, we proceed by induction: we assume that it is true in n-1 dimension and show it works in n dimension.
If the relation holds for n-1, it means we can rotate the system in such a way that the observable linear combination matrix has the form:

\begin{equation}
\bar{y} = 
\underbrace{
\begin{pmatrix}
\alpha_1 & 0 & .. & .. &0 & \delta_1\\
0 & \alpha_2 & 0 & .. & 0& \delta_2\\
0 & 0 & \alpha_i & 0 & 0&\delta_i\\
0 & .. & 0 & &\alpha_{n-1} & \delta_{n-1}\\
\beta_1 & .. & \beta_i & .. & \beta_{n-1} & \gamma\\
\end{pmatrix}}_{\Theta}
\bar{x}
\end{equation}

Imposing that $\Theta\Theta^t = I$, as done in the 2D case, equals to:
\begin{multline}
\begin{pmatrix}
\alpha_1 & 0 & .. & .. &0 & \delta_1\\
0 & \alpha_2 & 0 & .. & 0& \delta_2\\
0 & 0 & \alpha_i & 0 & 0&\delta_i\\
0 & .. & 0 & &\alpha_{n-1} & \delta_{n-1}\\
\beta_1 & .. & \beta_i & .. & \beta_{n-1} & \gamma\\
\end{pmatrix}
\begin{pmatrix}
\alpha_1 & 0 & .. & .. &0 & \beta_1\\
0 & \alpha_2 & 0 & .. & 0& \beta_2\\
0 & 0 & \alpha_i & 0 & 0&\beta_i\\
0 & .. & 0 & &\alpha_{n-1} & \beta_{n-1}\\
\delta_1 & .. & \delta_i & .. & \delta_{n-1} & \gamma\\
\end{pmatrix}
=  \\ = 
\begin{pmatrix}
\alpha_1^2 + \delta_1^2 & \delta_1 \delta_2 & .. & \delta_1 \delta_j & .. & \beta_1\alpha_1 + \delta_1\gamma\\
\delta_1 \delta_2 & \alpha_2^2 + \delta_2^2 & .. & .. & ..& ..\\
.. & .. & \alpha_i^2 + \delta_i^2 & .. & ..&..\\
.. & .. & .. & &\alpha_{n-1}^2 + \delta_{n-1}^2 & \beta_{n-1}\alpha_{n-1}  + \delta_{n-1}\gamma\\
\beta_1\alpha_1 + \delta_1\gamma & .. & .. & .. & \beta_{n-1}\alpha_{n-1} + \delta_{n-1}\gamma & \sum_i \beta_i^2 + \gamma^2\\
\end{pmatrix}
= I
\end{multline}

Since the $\alpha_i$ can not be null, in order for this expression to be satisfied, $\delta_i = 0$ for $i \in (1,n-2)$,  $\beta_i = 0$ for $i \in (1,n-2)$, and  $\alpha_i = 1$ for $i \in (1,n-2)$. Thus, the problem reduces to a two dimensional case,  as only the $\alpha_{n-1}$, $\delta_{n-1}$, $\beta_{n-1}$ and $\gamma$ parameters remain to be determined.    
But we already proved in the   previous section that the PCA rotation is equivalent to the minimal maximum entropy one in a two dimensional case, so it follows that the result is true for n-dimensional data.  

%\red{frase conclusive che convinca della dimostrazione.}


\end{document}


