% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% manually added packages
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage{afterpage}
\usepackage{stfloats}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{cleveref}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{listings} 

\usepackage{todonotes}
\newcommand{\SideNote}[2]{\todo[color=#1,size=\small]{#2}}
\newcommand{\kw}[1]{\SideNote{blue!40}{#1 --kw}}
\newcommand{\violet}[1]{\SideNote{purple!40}{#1 --violet}}


\newcommand{\model}{\textbf{\textcolor{darkgray}{\textsc{METAL}}}}
\newcommand{\gpt}{\textsc{GPT-4o}\xspace}
\newcommand{\llama}{\textsc{LLaMA 3.2-11b}\xspace}


\title{\model{}: A Multi-Agent Framework for Chart Generation \\ with Test-Time Scaling}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
Bingxuan Li{$^{\dagger}$} \ \ \ Yiwei Wang{$^{\dagger\mathsection}$} \ \ \  Jiuxiang Gu{$^{\ddagger}$} \ \ \  Kai-Wei Chang{$^{\dagger}$} \ \ \  Nanyun Peng{$^{\dagger}$}\\
$^\dagger$  University of California, Los Angeles \quad 
$^\mathsection$ University of California, Merced  \quad 
$^\ddagger$ Adobe Research\\
\texttt{bingxuan@ucla.edu} \\\\
\href{https://metal-chart-generation.github.io}{\textcolor{magenta}{\texttt{https://metal-chart-generation.github.io}}}
}


\begin{document}

\maketitle
\begin{abstract}


% Chart generation aims to generate code to produce the charts that satisfy the desired visual properties, e.g., texts, layout, color, type. 
% It is an important task across diverse professional fields, playing crucial roles in financial analysis, research presentation, education, and healthcare.
% In this work, we empower visual language models (VLMs) to function as intelligent AI \textit{agents} that assist users with limited coding expertise to create high-quality charts.
% \kw{This sentence is weird. Should be "we build AI agents empowered by VLMs to assist ..."}
% We propose \model{} (\textbf{M}ulti-ag\textbf{E}n\textbf{T} fr\textbf{A}mework with vision \textbf{L}anguage models for chart generation), a multi-agent framework that decomposes the complex, multi-modal reasoning process into an iterative collaboration among specialized agents. 
% Our approach improves more than 5.2\% the existing best results in the ChartMIMIC \cite{shi2024chartmimic} dataset, demonstrating that our modality-specific iterative feedback \kw{Need to first define what is modality-specific iterative feedback } can improve chart generation accuracy. 
% Empirical results highlight two key findings in the experiments: (1) There is a near-linear relationship \kw{but the curve will saturate eventually with high budget? I think we need some conditions here. } between the logarithm computational budget and \model{}'s performance, suggesting promising test-time scaling in a multi-agent system. (2) Disentangled self-critique across different modalities significantly enhances the multimodal reasoning capabilities of VLMs.



%%%%%% new version %%%%%%

Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare.
In this work, we build a vision-language model (VLM) based \textit{multi-agent} framework for effective automatic chart generation.
Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code.
Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs.
To resolve these challenges, we propose \model{} (\textbf{M}ulti-ag\textbf{E}n\textbf{T} fr\textbf{A}mework with vision \textbf{L}anguage models for chart generation), a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. 
\model{} achieves a 5.2\% improvement in the F1 score over the current best result in the chart generation task.
Additionally, \model{} improves chart generation performance by 11.33\% over Direct Prompting with \llama.
Furthermore, the \model{} framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithm of computational budget grows from $2^9$ to $2^{13}$ tokens.

% \violet{is "logarithmic computational budget" a well-established concept? If not, I'll avoid using it. Also, what does logarithmic computational budget = 512 mean?}

% \violet{this final sentence is not very important. can probably drop from the abstract.} In addition, we find that separating different modalities during the critique process of \model{} boosts the self-correction capability of VLMs in the multimodal context.



\end{abstract}

\section{Introduction}
\input{content/01-introduction}

\section{Related Works}
\input{content/02-related_works}

\section{Method}
\label{sec:method}
\input{content/03-method}

\section{Experiments}
\label{sec:experiments}
\input{content/04-experiments}

\section{Analysis and Discussion}
\label{sec:discussion}
\input{content/05-discussion}

\section{Case Study}
\label{sec:case_study}
\input{content/06-case_study}

\section{Conclusion}
\input{content/07-conclusion}

\section*{Limitation}
\input{content/08-limitation}

\bibliography{custom}
\bibliographystyle{acl_natbib}

\clearpage
\section*{Appendix}
\label{sec:appendix}
\appendix
\input{content/appendix}





\end{document}
