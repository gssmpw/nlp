In this section, we systematically evaluate \model{}. Section~\ref{sec:exp_setup} details the experimental setup, Section~\ref{sec:exp_res} presents the results, and Section~\ref{sec:ablation_study} provides an ablation study to further elucidate the model's performance.


\input{tables/main_results}

\subsection{Experiment Setup}
\label{sec:exp_setup}

\paragraph{Dataset} We select the ChartMIMIC dataset to evaluate \model{}. It is a benchmark that includes 1,000 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains. These charts span 18 regular types and 4 advanced types, diversifying into 191 subcategories \cite{shi2024chartmimic} .

\paragraph{Automatic Evaluation Metric}
Following the approach in \cite{shi2024chartmimic}, we assess four key low-level chart elements: text, layout, type, and color. During code execution, relevant information for each element is logged for evaluation. We then compare each element in the generated chart to its counterpart in the reference chart and calculate the F1 score. Note that the evaluation metric used here \textit{differs} from the multi-criteria verification metric described in the section \ref{sec:method}.

\paragraph{Base Model}
We assess the effectiveness of \model{} on both open-source and closed-source vision-language models. Specifically, we evaluate \gpt ~\cite{gpt4o} and \llama ~\cite{llama3modelcard}. The details of model size and computation budget are introduced in Appendix \ref{app:model_size}.

\paragraph{Implementation Details}  The implementation details of each component of \model{} and the baselines are described in Appendix \ref{app:implementation}. We include the prompt templates for each VLM-driven agent and the baselines in Appendix \ref{app:prompts}.



\paragraph{Baselines}
We compare \model{} against three baseline methods, detailed as follows:
\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \item \textit{Direct Prompting}: This baseline generates charts directly from the input prompt without any modifications or explicit guidance. It relies solely on the model's inherent ability.
    \item \textit{Hint-Enhanced Prompting}: In this approach, the input prompt is augmented with additional hints or structured guidance to help the model better understand the desired chart components \cite{wang2024hint}. Specifically, we augment the generation with a self-generated short description of a chart that provides context for elements such as layout, text, type, and color.
    \item \textit{Best-of-N}: This baseline generates multiple candidate charts in parallel, and the one that best meets a predefined verification metric is selected. We compare against Best-of-N by matching the number of iterations used in our approach.
    % \kw{this sounds more like Best-of-N. Rejection sampling usually mean that when generating output, if it violate certain constraints, then it backtracks to the previous state and regenerates. }
\end{enumerate}



\subsection{Experiment Results}
\label{sec:exp_res}

The primary research question of the experiment was to assess whether our proposed method could improve the performance of the base model in the chart generation task. Table~\ref{tbl:main_results} presents the experiment result.

For the LLaMA base model, the results indicate that the performance of baseline methods varied moderately. Direct Prompting and Hint-Enhanced Prompting achieved average F1 scores of 40.45\% and 41.33\%, respectively, while Best-of-N reached 43.13\%. In contrast, \model{} yielded an average F1 score of 51.78\% with improvements observed in each metric. The average F1 score improves by 11.33\% over Direct Prompting with 5 test-time compute recurrences, which is a significant improvement.

Similarly, for the GPT base model, the baseline methods demonstrated high performance with average F1 scores ranging from 81.12\% to 82.32\%. However, \model{} outperformed these methods by a considerable margin, achieving an average F1 score of 86.46\%. Specifically, our method improved 5.2\% in average over Direct Prompting.

These results clearly demonstrate that our approach consistently improves performance across both base models and all evaluation metrics. In particular, the significant gains observed in the Text and Layout metric, along with the overall increase in average F1 scores, indicate that our model effectively captures both the structural attributes and finer details of visual data. This enhancement not only boosts the performance of both open-source and closed-source vision-language models but also maintains a high level of consistency across all metrics, underscoring the robustness and generalizability of our method.



\subsection{Ablation Study}
\label{sec:ablation_study}
To further analyze the impact of different components, we performed an ablation study by selectively removing key elements of \model{} to assess the influence of each component on the overall performance.


\paragraph{Variations Setup}
We evaluate the following variants of \model{} to assess the contribution of each agent:

\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \item \model{}$_V$: Uses only the visual critique agent, omitting the code critique component.
    \item \model{}$_C$: Uses only the code critique agent, omitting the visual critique component.
    \item \model{}$_S$: Combines the visual and code critique agents into a single, unified critique agent.
    % \item \model{}\_r: Removes both critique agents, allowing the revision agent to directly refine the chart based on the two generated charts.
\end{itemize}

We implement each variation with \gpt as the base model. The prompt templates of each variation are attached to Appendix \ref{app:prompts}.

\paragraph{Results}

As shown in Table~\ref{tbl:ablation_study}, the full \model{} achieves the highest average F1 score, outperforming all ablated variants. Specifically, when only the visual critique agent is used (\model{}$_V$), the system obtains an average F1 score of 84.31\%, while the variant using only the code critique agent (\model{}$_C$) achieves 82.96\%. The unified agent variant (\model{}$_S$) yields the lowest average performance. 

These results indicate that both the visual and code critique agents play crucial roles in enhancing the model’s performance. The degradation observed in the ablated variants highlights that removing either component, or merging them into a single unit, compromises the system’s ability to effectively capture and refine the critical attributes of the visual data.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figs/accuracy_by_iteration.pdf}
    \caption{The performance of \model{} demonstrates an near-linear relationship with the log of compute budget.}
    \vspace{-0.1in}
    \label{fig:acc_by_iter}
\end{figure*}


\input{tables/ablation}
