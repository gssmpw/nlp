\input{appendix/code_style}


\section{Implementation}
\label{app:implementation}

In this section, we present the detailed implementation of our approach.

\subsection{VLMs driven agent Agents}
\label{app:agents}

In our implementation, we leverage VLMS to drive agents. The agents are designed to process and generate multimodal information as follows:

\paragraph{\textbf{Generation Agent and Visual Critique Agent:}}  
Both the Generation Agent and the Visual Critique Agent are designed to handle multimodal inputs. Specifically, they take as input a combination of visual data (e.g., the reference chart image or rendered chart) and textual descriptions. These agents are implemented using VLM architectures that can effectively integrate and reason over both image and text modalities. Their outputs are generated in the form of text, which provides either the initial code (in the case of the Generation Agent) or detailed visual discrepancy feedback (in the case of the Visual Critique Agent).

\paragraph{\textbf{Code Critique Agent and Revision Agent:}}  
In contrast, the Code Critique Agent and the Revision Agent are fully text-based. They accept textual inputs—either the generated code or the code accompanied by critique feedback—and produce textual outputs. Both agents are configured to generate responses up to approximately 600 tokens. 

\paragraph{\textbf{Integration of Agents:}}  
The agents interact in an iterative pipeline, where the Generation Agent first produces an initial code snippet. The Visual Critique Agent then examines the rendered output for any discrepancies relative to the reference chart, while the Code Critique Agent inspects the code for logical or syntactic issues. Finally, the Revision Agent integrates the feedback from both critique agents to modify the code. We have a Multi-Criteria Verifier (described in Appendix \ref{app:verifier} to verify the output of each iteration.


\subsection{Multi-Criteria Verifier}
\label{app:verifier}

We design three heuristic-based criteria—color, text, and overall—to assess the similarity between two images. The process begins by using EasyOCR to extract text from both the golden and generated images, and then computing a text similarity score based on the Jaccard index of the extracted text sets. In parallel, a verification from the color aspect is performed by converting the images into the HSV color space and applying predefined color ranges to count the pixels corresponding to specific colors; the resulting color histograms are compared using cosine similarity. Finally, an overall similarity measure is obtained by resizing the grayscale versions of the images and calculating the Structural Similarity Index (SSIM). The final verification result is a combination of these three metrics, providing a comprehensive assessment of image equivalence.

During the inference, the iteration will stop if the average of verification results exceeds the predefined threshold. The complete implementation code is attached as follows.
\lstinputlisting[language=python]{appendix/verifier.py}



\section{Model Size and Computational Requirement}
\label{app:model_size}

We have developed two versions of \model{}, each built upon a different foundational model to cater to varying operational needs.

For the version using the \gpt base model, we integrate the model via the OPENAI API. In this setup, each of the four agents makes one API call per action. One single iteration—where each agent acts once—results in 4 API calls in total. In our main experiments, we perform up to 5 iterations per trial.

Alternatively, the \llama-based version of \model{} is hosted locally on two NVIDIA A100 Tensor Core GPUs, each with 40 GB of GPU memory. Each of the four agents runs its own instance of the \llama model, leading to an overall GPU memory requirement of approximately 70 GB.

\section{Prompt Templates}
\label{app:prompts}

\subsection{\model{}}

This section lists all prompt templates used in \model{}.

\lstinputlisting[language=python]{appendix/metal_prompts.py}

\subsection{Variations}

This section lists all prompt templates used in variations from the ablation study.
\lstinputlisting[language=python]{appendix/variation_prompts.py}


\subsection{Baselines}

This section lists all prompt templates used in baselines.
\lstinputlisting[language=python]{appendix/baseline_prompts.py}