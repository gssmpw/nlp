Data visualization through charts is an important part of the communication and research life cycle. Well-designed visualizations help distill complex data into digestible insights, allowing researchers, analysts, and stakeholders to identify relationships that might remain hidden in raw data \citep{qin2020making, xu2023chartbench, yang2024matplotagent}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/teaser.pdf}
    \caption{Direct prompting of \gpt often fails to generate charts that accurately replicate reference charts, resulting in errors in structure, color, and text alignment. To address this limitation, we propose \model{}, a Multi-Agent system that iterative refinement through generation, critique, and revision. Our experiments demonstrate that increasing test-time compute recurrence and token usage leads to improved accuracy.}
    \label{fig:teaser}
    \vspace{-0.1in}
\end{figure}


Recent advancements in vision language models (VLMs), such as GPT-4V \cite{GPT4V} and LLaVa \cite{li2024llavanext-strong}, have expanded the capabilities of language models in tackling complex multimodal problem-solving tasks. These breakthroughs have sparked growing interest in designing \textit{intelligent AI assistants} to help users with limited coding expertise create compelling charts, leading to the emergence of the chart-to-code generation task \citep {wu2024plot2code, han2023chartllama,  shi2024chartmimic}. 

The chart-to-code generation task focuses on automatically generating visualization code based on visual references. 
%It has \textbf{huge practical value} because human usually invest a substantial amount of time and effort into crafting high-quality charts for their publications.\kw{This reads weird as the practical value emphasized here is only to help people to publish. I would probably change the tone of this sentence or just remove it. }
%However, replicating these visually appealing charts without access to the original implementation code remains a challenging endeavor.\kw{this sentence is a bit redundant. } An automated solution capable of generating charts from visual cues would bridge this gap, significantly enhancing efficiency and productivity in professional settings.
%In the meanwhile, 
This task embodies a \textbf{highly challenging visually-grounded code generation problem} that demands robust visual understanding and advanced reasoning. The system must interpret complex visual elements—such as layouts, color schemes, and data relationships—and translate them into syntactically correct, semantically meaningful code. Successfully addressing this challenge not only improves chart replication but also paves the way for advancing the general capabilities of vision-language models, pushing the boundaries of multimodal learning and program synthesis.

Current state-of-the-art vision language models (VLMs), such as GPT-4o, often fail to accurately interpret and reproduce the intricate visual elements and relationships embedded in reference charts, as illustrated in Figure \ref{fig:teaser}.\kw{In the caption of Fig 1 should mention the direct prompting is using GPT-4o} A core challenge in leveraging VLMs for this task lies in the effective integration of visual comprehension with code synthesis. Existing approaches to automated chart creation either focus solely on code generation without incorporating visual feedback or lack an iterative mechanism to refine outputs dynamically. This results in suboptimal chart quality, requiring manual corrections. These limitations motivated us to explore the effective visually grounded code generation paradigm for chart creation.

%Test-time scaling captures significant research attention \cite{narangself, li2023making, teller2000speech}. However, most existing work primarily focuses on single-modality tasks \cite{graves2012sequence, snell2024scaling, wu2024inference}. Test-time scaling for multimodal reasoning remains largely under-explored, despite its potential to enhance adaptability across diverse modalities. Addressing this gap is important for improving generalization and robustness in real-world scenarios where models process and integrate information from multiple sources under varying computational constraints.

In this paper, we present \model{}, a multi-agent framework designed specifically for multimodal reasoning in chart-to-code generation. Our system decomposes the complex task into four specialized roles, each handled by a specialized agent: (1) \textit{Generation Agent}: Responsible for the initial translation of chart images into the corresponding code. (2) \textit{Visual Critique Agent}: Analyzes and identifies visual differences between the reference chart and the generated output. (3) \textit{Code Critique Agent}: Reviews the generated code and suggests improvements to better match the reference chart. (4)  \textit{Revision Agent}: Implements code modifications based on the combined feedback from both critique agents. \looseness=-1
\kw{need to add a sentence to say we iterate among these four agents?}
\kw{Also, I would suggest swapping this and the previous paragraph. The current writing kind of like reader to infer the benefit of your method by criticizing the prior works. but it would be more direct to just first talk about your method and then say in contrast to the previous one, what are the benefits. }

Our framework improves chart generation accuracy by over 10\%, demonstrating its potential to significantly enhance VLMs' reasoning performance. \kw{not clear to me what's reasoning performance means here.}
In our experiments, we uncovered a near-linear relationship between performance and the logarithm of the computational budget, suggesting that with sufficient iterative multi-agent collaboration, the final performance could eventually approach near perfection. \kw{this might be over-claimed as we don't know if the performance would saturate by only observing 5 points. }
Moreover, our results indicate that disentangling self-critique by modality yields superior outcomes compared to a unified critique approach. \kw{need to elaborate, reader won't know what is disentangling self-critique by modality... }
These insights not only highlight the promise of a multi-agent framework but also pave the way for innovative strategies to boost the reasoning and adaptability of VLMs in complex multimodal tasks.
We envision that our approach will serve as a robust foundation for expanding the applications of VLMs in data visualization and related fields. \kw{I feel the last two sentences make a vague claim and to be honest the wording looks like generating by GPT. I would either make the claims more concrete or just remove them. }







% Many recent researchers suggest a paradigm shift from single monolithic models toward compound systems comprising multiple specialized components. Multi-agent systems exemplify this approach, offering enhanced collaborative reasoning capabilities that are particularly promising for tasks requiring sophisticated cross-modal understanding. These systems can potentially overcome the limitations faced by single models in visually grounded code generation tasks.

% These findings provide valuable insights into the role of structured feedback in enhancing multimodal reasoning capabilities.