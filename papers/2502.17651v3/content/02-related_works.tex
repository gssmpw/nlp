\vspace{-0.02in}
We discuss three lines of related work: chart-to-code generation, multi-agent framework, and test-time scaling research.
\vspace{-0.01in}
\subsection{Chart Generation with VLMs}

Chart generation, or chart-to-code generation,  is an emerging task aimed at automatically translating visual representations of charts into corresponding visualization code \cite{shi2024chartmimic, wu2024plot2code}. This task is inherently challenging as it requires both visual understanding and precise code synthesis, often demanding complex reasoning over visual elements.

Recent advances in Vision-Language Models (VLMs) have expanded the capabilities of language models in tackling complex multimodal problem-solving tasks, such as visually-grounded code generation.
% \kw{It's a bit unclear to me what do you mean by multimodal reasoning and how it is related to code generation}
Leading proprietary models, such as GPT-4V~\cite{GPT4V}, Gemini~\cite{Gemini}, and Claude-3~\cite{Claude}, have demonstrated impressive capabilities in understanding complex visual patterns.
% \kw{what do you mean by structured outputs here?} 
The open-source community has contributed models like LLaVA~\cite{xu2024llava-uhd, li2024llavanext-strong}, Qwen-VL~\cite{Qwen-VL}, and DeepSeek-VL~\cite{lu2024deepseekvl}, which provide researchers with greater flexibility for specific applications like chart generation.

Despite these advancements, current VLMs often struggle with accurately interpreting chart structures and faithfully reproducing visualization code. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figs/method.pdf}
    \caption{Overview of \model{}: A multi-agents system that consists of four specialized agents working in an iterative pipeline: (1) Generation Agent creates initial Python code to reproduce the reference chart, (2) Visual Critique Agent identifies visual discrepancies between the generated and reference charts, (3) Code Critique Agent analyzes the code and provides specific improvement guidelines, and (4) Revision Agent modifies the code based on the critiques. The process iterates until either reaching the verification score or maximum attempts limit.} %Each agent has a clearly defined objective and operates on specific inputs and outputs, enabling systematic improvement of the generated visualization.}
    \vspace{-0.1in}
    \label{fig:method}
\end{figure*}


\subsection{Multi-Agents Framework}

Many researchers have suggested a paradigm shift from single monolithic models to compound systems comprising multiple specialized components~\cite{compound-ai-blog, du2024compositional}. One prominent example is the multi-agent framework.

LLMs-driven multi-agent framework  has been widely explored in various domains, including narrative generation~\cite{huot2024agents}, financial trading~\cite{xiao2024tradingagents}, and cooperative problem-solving~\cite{du2023improving}.

Our work investigates the application of multi-agent framework to the visually-grounded code generation task.



\subsection{Test-Time Scaling} 
% \kw{I feel this subsection is a bit irrelevant }

Inference strategies have been a long-studied topic in the field of language processing. Traditional approaches include greedy decoding \cite{teller2000speech}, beam search \cite{graves2012sequence}, and Best-of-N.

Recent research has explored test-time scaling law for language model inference. For example, \citet{wu2024inference} empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while \citet{zhang2024scaling} and \citet{snell2024scaling} highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation,  remains under-explored. 
