Data visualization through charts is an important part of the communication and research life cycle. Well-designed visualizations help distill complex data into digestible insights, allowing researchers, analysts, and stakeholders to identify relationships that might remain hidden in raw data \citep{qin2020making, xu2023chartbench, yang2024matplotagent}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/teaser.pdf}
    \caption{Direct prompting of current VLMs (e.g. \gpt ) often fails to generate charts that accurately replicate reference charts, resulting in errors in structure, color, and text alignment. Our proposed approach, \model{}, tackle this challenge with iterative refinement through generation, critique, and revision. Our experiments show that increasing the logarithm of test-time compute recurrence and token usage leads to improved performance.}
    \label{fig:teaser}
    \vspace{-0.1in}
\end{figure}


Recent advancements in vision language models (VLMs), such as GPT-4V \cite{GPT4V} and LLaVA \cite{li2024llavanext-strong}, have expanded the capabilities of language models in tackling complex multimodal problem-solving tasks. These breakthroughs have sparked growing interests in designing \textit{intelligent AI assistants} to help humans with limited coding expertise create compelling charts, leading to the emergence of a complex multi-modal generation task with crucial practical value - Chart to code generation task \citep {wu2024plot2code, han2023chartllama,  shi2024chartmimic}. 

The chart to code generation task focuses on automatically generating visualization code based on visual references. This task embodies a \textbf{highly challenging visually-grounded code generation problem} that demands robust visual understanding and advanced reasoning. The model must interpret complex visual elements—such as layouts, color schemes, and data relationships—and translate them into syntactically correct, semantically meaningful code. Successfully addressing this challenge not only improves chart replication but also paves the way for advancing the general capabilities of VLMs in multimodal learning and program synthesis.

As illustrated in Figure \ref{fig:teaser}, current state-of-the-art VLMs, such as GPT-4o, often fail to accurately interpret and reproduce the intricate visual elements and relationships embedded in reference charts.  Existing solutions, such as Best-of-N and Hint-enhanced \cite{wang2024hint}, have not effectively improved upon direct prompting of VLMs. The core challenge in leveraging VLMs for chart generation lies in effectively integrating visual comprehension with code synthesis. This complex task exceeds the capabilities of the single model or single agent.

In this paper, we present \model{}, a multi-agent framework designed for chart generation. Our framework decomposes this complex multimodal reasoning task into four specialized roles, each handled by a specialized agent: (1) \textit{Generation Agent}: Responsible for the initial translation of chart images into the corresponding code. (2) \textit{Visual Critique Agent}: Analyzes and identifies visual differences between the reference chart and the generated output. (3) \textit{Code Critique Agent}: Reviews the generated code and suggests improvements to better match the reference chart. (4)  \textit{Revision Agent}: Implements code modifications based on the combined feedback from both critique agents. During inference, these agents collaborate iteratively, critiquing and refining the code until the rendered chart achieves the desired quality. 

% \looseness=-1

% \kw{need to add a sentence to say we iterate among these four agents?}
% \kw{Also, I would suggest swapping this and the previous paragraph. The current writing kind of like reader to infer the benefit of your method by criticizing the prior works. but it would be more direct to just first talk about your method and then say in contrast to the previous one, what are the benefits. }

%\violet{I'll swap these previous sentence in this paragraph will the previous paragraph. Leave the following sentence as is.}
In contrast to existing methods, our approach delivers more concrete and targeted feedback, and iteratively refines outputs through the multi-agentic framework, leading to enhanced chart generation performance. Experiment results show that our framework achieves a 5.2\% improvement over the current best result in the chart generation task. Additionally, \model{} improves chart generation performance by 11.33\% over Direct Prompting with \llama,
%\violet{inconsistency with the abstract. } 
demonstrating its potential to significantly enhance VLMs' ability to integrate visual understanding with code synthesis.

Furthermore, we have two key findings through in-depth analysis: (1) \textbf{Test-time scaling in the \model{} framework}: Recent works suggest the potential of scaling the number of tokens (computational budget) to enhance the reasoning performance of LLMs \citep{snell2024scaling, muennighoff2025s1}. Specifically, utilizing more tokens during inference may lead to improved performance. In this work, we found that there is a near-linear relationship between the performance and the logarithm of the computational budget in experiments. Specifically, the performance of \model{} increases monotonically as the logarithm of the computational budget grows from $2^9$ to $2^{13}$ tokens. 
%\violet{better to show the power format (e.g., $2^9$) to show linearity}. 
(2) \textbf{Modality-tailored critiques enhance self-correction}: We observe that explicitly separating different modalities during the critique process—such as visual evaluation and code analysis—substantially enhances the multimodal self-correction capabilities of VLMs. Ablation study shows that \model{} with the separate-critique design achieves a 5.6\% improvement over the single-critique baseline.

% In our experiments, we uncovered a near-linear relationship between performance and the logarithm of the computational budget, suggesting that with sufficient iterative multi-agent collaboration, the final performance could eventually approach near perfection. \kw{this might be over-claimed as we don't know if the performance would saturate by only observing 5 points. }
% Moreover, our results indicate that disentangling self-critique by modality yields superior outcomes compared to a unified critique approach. \kw{need to elaborate, reader won't know what is disentangling self-critique by modality... }

In summary, we present \model{}, a VLMs-based multi-agent framework, which achieves significant improvements over the current best methods for chart generation, and our insights into test-time scaling and multi-modal critique offer a promising pathway for enhanced visually-grounded code generation with VLMs.