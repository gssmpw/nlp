We analyze the experimental results in this section. We highlight two interesting findings: Test-time scaling in Multi-Agent system (Section \ref{sec:result_test_time_scaling}), and modality-tailored critiques enhance the self-correction ability (Section \ref{sec:result_multimodal_self_critique}). Additionally, we discuss the advantage of \model{} (Section \ref{sec:why_metal}), and the benefit of agentic design (Section \ref{sec:result_agentic_vs_modular})


\subsection{Test-Time Scaling} 
\label{sec:result_test_time_scaling}

We investigate the relationship between the test-time computational budget and model performance. As illustrated in Figure~\ref{fig:acc_by_iter}, our analysis reveals an interesting trend:  increasing the logarithm of the computational budget leads to continuous performance improvements. This near-linear relationship indicates the test-time scaling phenomenon, demonstrating that allowing more iterations during inference could potentially enhance performance.

One potential reason for this phenomenon is the strong self-improvement capability of \model{}. Our framework is designed so that specialized agents iteratively collaborate, allowing each agent to refine its output based on feedback from others. With each iteration, errors are corrected and insights from different modalities are integrated, leading to incremental performance gains. This continual refinement process leverages the strengths of individual agents, resulting in the self-improvement capability that drives the observed performance enhancements as computational resources increase. 

Due to limited resources, we have not extended the experiment range further. However, the observed scaling implies that the framework can benefit from more iterations of collaborative self-improvement. We leave a more comprehensive exploration of this potential to the future work.

% Figure~\ref{fig:acc_by_iter} reveals a near-linear relationship between performance and the logarithm of the computational budget. \kw{one question might be why don't we further increase the budget and stop at 4096 tokens.} As more computations are performed, performance improves continuously, suggesting that prolonged multi-agent collaboration could eventually achieve near-perfect results. This finding confirms that the test-time scaling law applies to the chart generation task with the multi-agent framework. \kw{I would not use "confirm" as it might be too strong. We can say we observe this trend in our experiments. }


\subsection{Modality-Tailored Critiques} \label{sec:result_multimodal_self_critique}

\begin{figure} [tbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/average_improvement_by_difficulty.pdf}
    \caption{Performance gain after 5 compute recurrences of \model{} over different  difficulty.}
    \vspace{-0.2in}
    \label{fig:improv_by_diff}
\end{figure}

From the ablation study result shown in Table \ref{tbl:ablation_study}, we observed that separating visual and code critiques enhances the model's self-correction capabilities. In contrast, \model{}$_S$ struggles to effectively self-improve in the chart-to-code generation task.

We identify two potential reasons for this observation. First, combining both visual and code inputs results in an extended context that can overwhelm the model, leading to information loss. This dilution makes it difficult to capture key details from each modality, resulting in less accurate critiques and a reduction in overall self-correction effectiveness. Second, the self-critique process for chart generation involves distinct requirements: visual data demands spatial understanding, color analysis, and fine detail recognition, while code data requires strict adherence to syntax and logical consistency. A unified critique approach is ill-suited to address these differing needs. Without modality-specific feedback, the model struggles to detect and correct errors unique to each data type.

These findings suggest that self-correction in the multimodal context can be enhanced by leveraging tailored critique strategies for each modality.

\subsection{Why \model{}} 
\label{sec:why_metal}

We believe \model{} provides three advantages. 
First, by assigning specialized tasks to individual agents, the system effectively reduces error propagation. During inference, each agent evaluates whether to take action based on the available information and insights from other agents. This process enables each agent to serve as a safeguard, detecting and correcting mistakes before they escalate. 

Second, the modular design of \model{} enables easy modification and adaptation. For instance, one can integrate different base models tailored for specific tasks—such as employing a critique-trained model for critique agents and a generation-trained model for generation agents—to maximize overall performance. 

Third, \model{} is robust with the strong base model. Figure~\ref{fig:improv_by_diff} compares the performance of \model{} to that of Direct Prompting over five iterations across varying chart difficulty levels. \model{} with the \gpt base model achieved consistent improvements regardless of difficulty. When using \llama as the base model, the performance gains tend to diminish with increasing reference chart complexity, but the improvements remain substantial. This drop might be due to the limited critique capabilities of the \llama base model. Nonetheless, the flexibility of \model{} to replace the base model for different agents allows us to tailor the system optimally—using, for example, a critique-optimized model for critique agents and a generation-focused model for generation agents—to maximize overall performance.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figs/case_study.pdf}
    \caption{Case study of \model{}'s progressive refinement from initial generation to perfect. Starting from Round 0's initial generation (60\% color score , 84\% text score), the system iteratively improves the output. In Round 1, the system identifies and corrects Y-axis scale issues and missing annotations, achieving 100\% text score. Round 2 refines the color representations of distributions, achieving perfect F1 score across all metrics.}
    \label{fig:case_study}
    \vspace{-0.08in}
\end{figure*}

\subsection{Multi-Agent System vs. Modular System}
\label{sec:result_agentic_vs_modular}

We further investigate the impact of agentic behavior of \model{} on final performance. We think self-decision-making and code execution abilities are key features that distinguish the multi-agent system from a modular system. We implement a self-revision modular system without these two key abilities,  and conduct an additional ablation study on a subset of 50 data points to examine the impact of these agentic behaviors on final performance.

The results show that, compared to \model{}, there is a 4.51\% reduction in average performance gain over direct prompting. The absence of decision-making and code execution abilities in the modular system hinders its capacity to refine generated charts effectively. Specifically, the inability to execute code for chart rendering significantly diminishes the quality of the critique, and the absence of self-decision-making ability potentially leads to error propagation that further negatively impacts the self-correction process.

This comparison underscores the critical role of the agentic approach.

