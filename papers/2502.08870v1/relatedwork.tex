\section{Related work}
Lower bounds for the linear bandit problem depend on the structure of specific action spaces \citep[for example,][]{dani2008stochastic,rusmevichientong2010linearly,lattimore2017end}. 
Theorem 2.1 of \citet{rusmevichientong2010linearly} shows that there exists a problem instance where the action space is the $d$-dimensional unit sphere in which any policy must incur $\Omega(d\sqrt{n})$ regret. Optimistic algorithms have frequentist regret nearly matching the lower bound for linear bandits \citep{auer2002using,dani2008stochastic,abbasi2011improved}.
Specifically, \citet{abbasi2011improved} show that by constructing confidence sets using self-normalized bounds for vector-valued martingales, and taking actions optimistically within these, the resulting regret is $O(d \sqrt{n} \log(n/\delta))$ with probability at least $1-\delta$. Despite the strong theoretical performance of optimistic algorithms, randomised algorithms, such as Thompson sampling, have been shown to perform better in practice \citep{chapelle2011empirical,may2012optimistic}. In the simpler multi-armed bandit setting, randomised algorithms achieve optimal regret 
\citep{agrawal2012analysis,kaufmann2012thompson,korda2013thompson,honda2014optimality}. 
Under Bayesian assumptions, where regret is defined by taking an expectation over the unknown parameter, \citet{russo2014learning,russo2016information} show that Thompson sampling is near-optimal in many structured and unstructured settings. In particular, for the linear bandit setting, they show a Bayesian regret bound of $\widetilde O(d\sqrt{n})$ \citep{russo2014learning}.

In this paper, our focus is on the regret of randomised exploration algorithms in linear bandits. While this setting has been studied extensively by, amongst others, previous approaches rely on modifying the algorithm to force it to be more optimistic. The main line of analysis, by \citet{agrawal2013thompson,abeille2017linear,xu2023noise}, inflates the variance of the posterior over models in round $t$ by a factor of $\Theta(\sqrt{d \log(t/\delta)})$ to show that the algorithm is optimistic with constant probability---this leads $O((d\log(n))^{3/2}\sqrt{n})$ regret, where the increased dependence on $d$ is due to the inflation of the posterior. Further variants of randomised exploration algorithms include modifying the algorithms to only sample parameters with reward greater than the mean \citep{may2012optimistic,vaswani2020old} and modifying the likelihood used in the Bayesian update of Thompson sampling to force the algorithm to be more optimistic \citep{zhang2021feel,huix2023tight}. The analysis of Thompson sampling in other structured settings, such as generalised linear bandits, relies on these same modifications \citet{kveton2020randomized,janz2023exploration}.

We remark that the results presented in this paper do not contradict the lower bounds by \citet{hamidi2020worst,zhang2021feel} where examples were provided for which linear Thompson sampling incurs linear regret if the posterior distribution is not inflated. The action spaces constructed in those examples fail to satisfy our assumptions.






%%%% problem setting