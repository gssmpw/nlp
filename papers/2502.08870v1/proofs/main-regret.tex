\section{Proof of main result} \label{sec:proof}
We now prove our main result, \cref{thm:main-regret}. Here and in the appendices, we will write $J$ in place of~$J_\cX$, and we will work throughout on the $1-\delta$ probability event where $\thetaopt \in \cap_{t \geq 1} \Theta_{t-1}$. 

We start by moving from $R_n$ to $\bar R_n := \sum_{t=1}^n \Et r_t$. This can be done by noting that $\xi_t = r_t - \Et r_t$, $t \geq 1$, is a martingale difference sequence 
satisfying $|\xi_t| \lesssim \sqrt{d}$ for all $t \geq 1$, and applying a standard concentration inequality (included here as \cref{lem:time-unif-bound}, \cref{appendix:useful}). From this, conclude that with probability $1-\delta$, for all $n \geq 1$,
$$
  R_n \lesssim \bar R_n  +\sqrt{dn \log(dn/\delta)}\,.
$$
We now outline the three main results we use in bounding $\bar R_n$, and then show how they come together.

\newcommand{\RTS}{\bar{R}^{\text{TS}}_n}
\newcommand{\ROPT}{\bar{R}^{\text{OPT}}_n}
\newcommand{\tbet}{\tilde{\beta}_{t-1}}

\subsection{Regret decomposition \& upper bound} Denote by $p_{t-1}$ the conditional probability of optimism $\Pt{J(\thetat) \geq J(\thetaopt)}$ at time-step $t \geq 1$.  Letting $\chip = \1{p_{t-1} \leq p}$ for a threshold $p \in (0,1)$, we now decompose the regret into that incurred in time-steps where $p_{t-1}$ is high, and those where it is low (we take $p = 1/( 16 K^4)$, where $K$ is the constant appearing in \cref{ass:perturb}):
\begin{align}
  \bar R_n &= \sum_{t=1}^n  \chip \Et r_t  + \sum_{t=1}^n (1-\chip) \Et r_t \nonumber  \\
    &\lesssim \underbrace{ \frac{M (\beta_n^2 \vee K^2d)}{J(\thetaopt)}\sum_{t=1}^n \chip \sup_{u \in \Bd} \snorm{\Vt^{-1/2}u}^2}_{=: \RTS} + \underbrace{K^4 (\beta_n \vee K\sqrt{d})  \sum_{t=1}^n \Et \vinvnorm{X_t} }_{=: \ROPT}\,.\label{eq:regret-decomp}
\end{align}
The derivations of the bound is presented in \cref{sec:per-step-bounds}.  It is based on repeatedly applying properties of Bregman divergences and convex functions. At a high level, we introduce $\thetat'$, which is, conditionally on $\FF_{t-1}$, an independent copy of $\thetat$; then  conditioning condition on the event $\{J(\theta'_t) \leq J(\thetaopt) \}$ (the converse for the second term), and integrating the $\thetat'$ out.

Examining the two terms, $\ROPT$ is a term that appears in the standard regret analysis of optimistic algorithms, and is easily handled using a concentration argument (\cref{lem:nonneg-concentration}) and the elliptical potential lemma (\cref{lem:epl}); this yields
$$
  \ROPT \lesssim (\beta_n \vee K\sqrt{d}) K^4 \sqrt{n (d\log(1+n/(d\lambda)) + \log(1/\delta))}\,,
$$
a term featuring in our overall regret bound. The term $\RTS$ is a cost associated with randomised exploration: it is the sum of the sizes of the parameter sampling distributions (or confidence sets, as these are the same up to scaling), where size is measured in the geometry induced by $\snorm{\cdot}$.

\subsection{Relating confidence widths to the amount of exploration} 

The challenge is now to show that $V_t$ grows sufficiently fast, measured with respect to the geometry induced by $\snorm{\cdot}$, such that $\RTS$ is small. First, we relate the width $\snorm{\Vt^{-1/2}u}$ to the expected amount of exploration in the direction of $u \in \Bd$ at step $t$, with the latter measured in the $\ell_2$ norm, $\norm{\cdot}$, at a cost of $1/m$ from $m$-strong convexity. This is a \emph{change of geometry} lemma:

\begin{lemma}\label{lem:geometry}
  For all $t \geq 1$ with $p_{t-1} \leq 1/(16K^4)$, for any $u \in \Bd$,
  $$ % Real constant is 6\sqrt{2}
  \frac{1}{J(\thetaopt)}\snorm{\Vt^{-1/2}u}^2 \precsim \frac{K}{m} \norm{\Et[\Xt\Xt\tran]^{1/2} \Vt^{-1/2} u }\,.
  $$
\end{lemma}

\begin{remark}
  When $\cX = \Bd$, we have $m=1$ for $\snorm{\cdot} = \norm{\cdot}$, and thus no change of geometry is needed. In that case $X_t = \thetat/\norm{\thetat}$ almost surely, and $J(\theta) = \norm{\theta}$ for all $\theta \in \Rd$, and so
$$
  \Et \Xt\Xt\tran = \Et[\thetat\thetat\tran/\norm{\thetat}^2] \approx \frac{1}{\norm{\thetaopt}^2}\Et \thetat\thetat\tran \succeq \frac{1}{\norm{\thetaopt}^2}\Var_{t-1}\, \thetat = \frac{K^2}{J^2(\thetaopt)}\Vt^{-1}\,,
$$
where we for the sake of exposition, allowed ourselves the simplifying assumption our the confidence sets and perturbations are concentrated sufficiently to ensure that $1/\norm{\thetat} \approx 1/\norm{\thetaopt}$.
\end{remark}

\noindent We present the proof of \cref{lem:geometry} in \cref{sec:change-of-geometry}. Once again, we proceed by introducing a random variable $\thetat'$ with the same $\FF_{t-1}$-conditional law as $\thetat$; however, this time, we couple $\thetat$ and $\thetat'$ closely coupled, in that they differ only in the $u$ marginal (along which they are independent). We then proceed with a convex Poincar\'e inequality-style argument along the $u$ direction, which relates $\Xt = \nabla J(\thetat)$ and the $\Vt^{-1}$ matrix, with the latter being essentially the conditional variance of $\theta_t$.

\subsection{Establishing the growth of the design matrices} 
The final ingredient is the following relation between the sum $\sum_{t=1}^n \Et \Xt\Xt\tran$ of the conditional expected increments in the design matrices and their realisation $\sum_{t=1}^n \Xt\Xt$.

\begin{lemma}\label{claim:pointwise-concentration}
  For any $r \in (0, 1]$ and $\delta \in (0,1)$, with probability at least $1-\delta$, for all $n \geq 1$ and all $u \in r\Bd$,
  $$
      u\tran \sum_{i=1}^n \Xt\Xt\tran u + J^2(u) \omega_n  + 5 \geq \frac{1}{2}\sum_{t=1}^n u\tran \Et[\Xt\Xt\tran] u \,,
  $$
  where $\omega_n = d\log(20d^3n^2r/\delta^2)$.
\end{lemma}

\begin{remark}
  A standard matrix Chernoff inequality\footnote{See \citet{tropp2012user}. This exact inequality is not stated there, but all the tools needed to derive it are.} gives that with probability $1-\delta$, for all $n \geq 1$,
  \begin{equation}\label{eq:tropp}
    \sum_{t=1}^n \Xt\Xt + \log(d/\delta) \succeq \frac{1}{2} \sum_{t=1}^n \Et \Xt\Xt\tran\,,
  \end{equation}
  where $\succeq$ denotes the usual ordering on positive-semidefinite matrices. For the $\ell_2$ ball, \cref{eq:tropp} serves the same role as \cref{claim:pointwise-concentration}, but is tighter.  However, in the general setting where $J(u) \neq \norm{u}$, it is crucial that we obtain the $J^2(u)$ dependence seen in \cref{claim:pointwise-concentration}. 
\end{remark}

\noindent The proof of \cref{claim:pointwise-concentration} is presented in \cref{sec:proof-covering}. It uses \cref{lem:nonneg-concentration}, a one-dimensional version of the inequality given in \cref{eq:tropp}, and applies it to the process $(\langle u, X_t \rangle^2 \colon t \geq 1)$ for all $u$ in a time-dependent cover of $r\Bd$. A union bound over the size of the cover is responsible for the $\omega_n$ term, and the discretisation error involved in the covering argument yields the additive $5$.

\subsection{Putting everything together}

Let $N_n = \sum_{t=1}^n \chip$ be the number of steps up to $n$ on which the conditional probability of optimism was below the threshold $p$. We will shortly show that \cref{lem:geometry} and \cref{claim:pointwise-concentration}, together with the assumed smoothness, yield the following bound:

\begin{claim}\label{claim:curvature-per-step}
  For all $t \geq 2$ and $u \in \Bd$,
  $$
      \frac{1}{J(\thetaopt)} \snorm{\Vt^{-1/2} u}^2 \lesssim \frac{ M\omega_{t-1} K^2 J(\thetaopt)}{m^2 N_{t-1}} + \frac{K}{m\sqrt{N_{t-1}}}\,.
  $$
\end{claim}

\noindent First though, note that using \cref{claim:curvature-per-step} within the regret decomposition of \cref{eq:regret-decomp} completes the proof. Indeed, using that the expected per-step regret is bounded by $2\norm{\thetaopt}\lesssim \sqrt{d}$ (to handle step the first step, which is not covered by \cref{claim:curvature-per-step}), and the usual integral for monotonic integrands, we have
\begin{align*}
  \RTS
  &\lesssim \sqrt{d} + (\beta_n^2 \vee K^2 d) \int_1^{n}  \left[\frac{ M^2\omega_t K^2 J(\thetaopt)}{m^2 N_t} + \frac{K}{m\sqrt{N_t}}\right] dt \\
  &\lesssim \sqrt{d} + d\sqrt{d} (\beta_n^2 \vee K^2 d) K^2 \frac{M^2}{m^2}\log(d n /(\delta\lambda))\log n + \frac{M}{m} K (\beta_n^2 \vee K^2 d) \sqrt{n}\,,
\end{align*}
which completes our bound (observe that the first two terms are lower order).  


\begin{proof}[Proof of \cref{claim:curvature-per-step}]
We work on the $1-\delta$ probability event resulting from applying \cref{claim:pointwise-concentration} with $r = 1/\sqrt{\lambda}$. Since $u \in \Bd$, we have $V_{n}^{-1/2} u \in \Bd/\sqrt{\lambda}$, and thus 
for all $n \geq 1$,
\begin{align}
     J^2(V^{-1/2}_n u) \omega_n + 6  \geq \frac{1}{2}\sum_{t=1}^n \norm{\Et[ \Xt\Xt\tran]^{1/2} V^{-1/2}_n u}^2\,. \label{eq:ub}
\end{align}
Now we proceed to in turn upper and lower-bounding the above expression.

For the upper-bound, note that by $M$-smoothness, $J^2(V^{-1/2}_n u) \leq \frac{M}{2} \snorm{V^{-1/2}_n u}^2$.

For the lower bound of the right-hand side of \cref{eq:ub}, we will use \cref{lem:geometry}. Let $v_{t-1} = V^{1/2}_{t-1} V^{-1/2}_{n} u$, and note that since $V_{t-1} \preceq V_n$, we have that $\norm{v_{t-1}} \leq 1$. Now,
\begin{align*}
    \sum_{t=1}^n \chip \norm{\Et[ \Xt\Xt\tran]^{1/2} V^{-1/2}_n u}^2  
    &= \sum_{t=1}^n \chip \norm{v_{t-1}}^2\norm{\Et[ \Xt\Xt\tran]^{1/2} \Vt^{-1/2} \frac{v_{t-1}}{\norm{v_{t-1}}}}^2 \\ 
    &\gtrsim \frac{m^2}{K^2 J^2(\thetaopt)} \sum_{t=1}^n \chip \frac{\snorm{\Vt^{-1/2} v_{t-1} }^4}{\norm{v_{t-1}}^2} \tag{\cref{lem:geometry}} \\ 
    &\geq \frac{m^2}{K^2 J^2(\thetaopt)} N_n \snorm{V^{-1/2}_n u}^4  \tag{$\norm{v_{t-1}} \leq 1$}\, 
\end{align*}

Combining our lower and upper bounds on \cref{eq:ub}, writing $\alpha_n = C m^2 N_n / K^2$ for a numerical constant $C > 0$ and letting $y = \frac{1}{J(\thetaopt)}\snorm{V^{-1/2}_n u}^2$, we obtain the quadratic
$$
    - \alpha_n y^2 + M \omega_n J(\thetaopt)y + 6 \geq 0\,.
$$
Solving for $y$, we have
\begin{equation*}
    \snorm{V^{-1/2}_n u}^2 \leq \frac{M\omega_n J(\thetaopt) + \sqrt{M^2\omega_n^2 J^2(\thetaopt) + 24\alpha_n}}{2\alpha_n} \leq \frac{M\omega_n J(\thetaopt)}{\alpha_n} + \sqrt{\frac{6}{\alpha_n}}\,,
\end{equation*}
whence relabelling $n \mapsto t-1$ concludes the proof.
\end{proof}
