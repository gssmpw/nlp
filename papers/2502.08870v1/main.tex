\pdfoutput=1
\documentclass[final,12pt]{alt2025} % Anonymized submission
% \documentclass[final,12pt]{alt2025} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[When and why randomised exploration works]{When and why randomised exploration works (in linear bandits)}
\usepackage{times}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{xspace}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{backgrounds}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{shapes}
%\usepackage{caption}
\pgfplotsset{width=7cm,compat=1.8}
\usepackage{adjustbox}
\usepackage{enumerate}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{mathtools} % MoveEqLeft
\usepackage[shortcuts]{extdash}

\usepackage{algorithm,algpseudocode}
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFor}% Remove "end for" text

\newtheorem{theorem}[]{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[]{Example}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}{Corollary}
\crefname{claim}{Claim}{Claims}
\newtheorem{assumption}{Assumption}
\crefname{assumption}{Assumption}{Assumptions}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}{Remark}

\usepackage{custom_citations}
\bibliography{references}

\input{custom_notation.tex}
\newcommand{\todoc}[1]{\textcolor{red!60!white}{ [[Ciara: #1]]}}

\newcommand{\todod}[1]{\textcolor{red!60!green}{ [[David: #1]]}}

\newcommand{\todom}[1]{\textcolor{red!60!blue}{ [[Marc: #1]]}}

\definecolor{darkgreen}{RGB}{10,160,10}
\definecolor{darkblue}{RGB}{0,128,255}

\altauthor{%
 \Name{Marc Abeille} \Email{m.abeille@criteo.com}\\
 \addr Criteo AI Lab, Paris, France
 \AND
 \Name{David Janz} \Email{david.janz93@gmail.com}\\
 \addr University of Oxford, UK
 \AND
 \Name{Ciara Pike-Burke} \Email{c.pike-burke@imperial.ac.uk}\\
 \addr Imperial College London, UK%
}

\begin{document}

\maketitle

\begin{abstract}%
  We provide an approach for the analysis of randomised exploration algorithms like Thompson sampling that does not rely on forced optimism or posterior inflation. With this, we demonstrate that in the $d$-dimensional linear bandit setting, when the action space is smooth and strongly convex, randomised exploration algorithms enjoy an $n$-step regret bound of the order $O(d\sqrt{n} \log(n))$. 
  Notably, this shows for the first time that there exist non-trivial linear bandit settings where Thompson sampling can achieve optimal dimension dependence in the regret.%
\end{abstract}

\begin{keywords}%
  linear bandits, randomised exploration, Thompson sampling%
\end{keywords}


%%%% Introduction
\section{Introduction}

To achieve low regret in sequential decision-making problems, it is necessary to balance exploration (selecting uncertain actions) and exploitation (selecting previously successful actions).
One method of balancing this exploration-vs-exploitation trade-off that is particularly well-understood is through optimism:  
optimistic algorithms maintain a set of statistically plausible models of the environment and select actions that maximize the reward in the best plausible model---note however that this entails solving a bi-level optimization problem in each round.
Randomised exploration is an alternative approach where algorithms select a model of the problem randomly from a set of plausible models and act optimally with respect to that randomly sampled model---bypassing the need to solve the bi-level optimization problem associated with optimism. 
Notable examples of randomised decision-making algorithms include Bayesian algorithms such as posterior sampling \citep[also known as Thompson sampling]{thompson1933likelihood}, ensemble sampling \citep{lu2017ensemble,janz2023ensemble} and perturbed history exploration \citep{kveton2020randomized,janz2023exploration}. 
However, while randomisation-based algorithms are often preferred in practice, our theoretical understanding of when and why randomised exploration works in structured sequential decision-making problems is limited.

In this paper, we analyse randomised sequential decision-making algorithms in the classic linear bandit problem---but the techniques that we introduce should carry over to other structured settings. 
In this setting, previous frequentist analyses \citep[e.g.][]{agrawal2013thompson,abeille2017linear,kveton2020randomized,janz2023exploration} are not sufficient to explain the practical effectiveness of randomised exploration, nor do they identify a mechanism through which randomised exploration works. 
Indeed, existing proofs rely on modifying randomised exploration algorithms so that they can be analysed using the optimism framework. These modifications often lead to suboptimal regret. 
Our analysis does away with such modifications; it holds under the assumption that the action space is smooth and strongly convex (see \cref{sec:ass-arm} for formal definitions), which allows for perturbation in the model parameter space to translate to perturbations in the action space, while also guaranteeing that small changes in the action space only lead to small changes in the incurred regret.

For such smooth, strongly convex action sets, which include $\ell_p$-balls for $p \in (1,\infty)$, we a regret of the order $O(d\sqrt{n} \log(n))$ where $d$ is the dimension of the action space and $n$ is the number of rounds. 
Notably, this shows for the first time that (unmodified) linear Thompson sampling can enjoy regret with the optimal dependence on the dimension in a structured linear bandit settings, thus partially resolving an important open question \citep{russo2018tutorial}.

%%%% Related work
\section{Related work}
Lower bounds for the linear bandit problem depend on the structure of specific action spaces \citep[for example,][]{dani2008stochastic,rusmevichientong2010linearly,lattimore2017end}. 
Theorem 2.1 of \citet{rusmevichientong2010linearly} shows that there exists a problem instance where the action space is the $d$-dimensional unit sphere in which any policy must incur $\Omega(d\sqrt{n})$ regret. Optimistic algorithms have frequentist regret nearly matching the lower bound for linear bandits \citep{auer2002using,dani2008stochastic,abbasi2011improved}.
Specifically, \citet{abbasi2011improved} show that by constructing confidence sets using self-normalized bounds for vector-valued martingales, and taking actions optimistically within these, the resulting regret is $O(d \sqrt{n} \log(n/\delta))$ with probability at least $1-\delta$. Despite the strong theoretical performance of optimistic algorithms, randomised algorithms, such as Thompson sampling, have been shown to perform better in practice \citep{chapelle2011empirical,may2012optimistic}. In the simpler multi-armed bandit setting, randomised algorithms achieve optimal regret 
\citep{agrawal2012analysis,kaufmann2012thompson,korda2013thompson,honda2014optimality}. 
Under Bayesian assumptions, where regret is defined by taking an expectation over the unknown parameter, \citet{russo2014learning,russo2016information} show that Thompson sampling is near-optimal in many structured and unstructured settings. In particular, for the linear bandit setting, they show a Bayesian regret bound of $\widetilde O(d\sqrt{n})$ \citep{russo2014learning}.

In this paper, our focus is on the regret of randomised exploration algorithms in linear bandits. While this setting has been studied extensively by, amongst others, previous approaches rely on modifying the algorithm to force it to be more optimistic. The main line of analysis, by \citet{agrawal2013thompson,abeille2017linear,xu2023noise}, inflates the variance of the posterior over models in round $t$ by a factor of $\Theta(\sqrt{d \log(t/\delta)})$ to show that the algorithm is optimistic with constant probability---this leads $O((d\log(n))^{3/2}\sqrt{n})$ regret, where the increased dependence on $d$ is due to the inflation of the posterior. Further variants of randomised exploration algorithms include modifying the algorithms to only sample parameters with reward greater than the mean \citep{may2012optimistic,vaswani2020old} and modifying the likelihood used in the Bayesian update of Thompson sampling to force the algorithm to be more optimistic \citep{zhang2021feel,huix2023tight}. The analysis of Thompson sampling in other structured settings, such as generalised linear bandits, relies on these same modifications \citet{kveton2020randomized,janz2023exploration}.

We remark that the results presented in this paper do not contradict the lower bounds by \citet{hamidi2020worst,zhang2021feel} where examples were provided for which linear Thompson sampling incurs linear regret if the posterior distribution is not inflated. The action spaces constructed in those examples fail to satisfy our assumptions.






%%%% problem setting
\section{Problem setting, notation and basic definitions} \label{sec:problem-setting}

We study the linear bandit problem, where each bandit instance is parameterised by an unknown $\thetaopt \in R \Bd$ ($R>0$ known), and an action set $\cX$, a closed subset of $\Bd$ (the closed unit $\ell_2$-ball in $\Rd$). Then, at each time-step $t=1,2,\dots$ an agent selects an action $X_t\in \cX$, allowed to depend on observations from previous time-steps, and receives a real-valued reward $Y_t$. We assume that the reward $Y_t$ is $S$-subgaussian given $X_t$ and the past ($S>0$ known),  with mean given by $\langle X_t, \thetaopt \rangle$. The goal of the agent is to select actions to minimize the $n$-step regret ($n \geq 1$), defined by
$$
  R_n = \sum_{t=1}^n r_t \spaced{for} r_t = \langle \xopt - X_t, \thetaopt \rangle\,,
$$
where $\xopt \in \argmax_{x\in \cX} \langle x, \thetaopt\rangle$ is any optimal arm and the horizon $n$ needs not be known. 

\paragraph{Confidence set construction} The algorithms and analysis in this work are based on the standard regularised least-squares-based confidence ellipsoids for~$\thetaopt$ \parencite{abbasi2011improved}. To construct these, fix a regularisation parameter $\lambda > 0$ and a confidence parameter $\delta \in (0,1)$. Define the regularised design matrices and least-squares estimates as $V_0 = \lambda I$, $\quad \hat\theta_0 = 0$ and then
$$
   V_t = X_tX_t\tran + V_{t-1} \spaced{and} \hat\theta_t = V_t^{-1} \sum_{i=1}^t Y_i X_i \quad \spaced{for} t\geq 1\,,
$$
Also, define the sequence of nondecreasing, nonnegative confidence widths
$$
  \beta_t = \textstyle{R\sqrt{\lambda} + S\sqrt{2 \log(1/\delta) + \log(\det (V_t)/\lambda^d)}}, \quad t \geq 0\,.
$$
Then, \citet{abbasi2011improved} show that, with probability $1-\delta$, $\theta_\star \in \cap_{t\geq 1} \Theta_{t-1}$ for the ellipsoids given by
$$
  \Theta_{t-1} = \{\theta \in \R^d \colon \vnorm{\theta-\hat\theta_{t-1}}\leq \beta_{t-1} \}\,, \quad t \geq 1\,,
$$
where for $a \in \R^d$ and a $d\times d$ positive-definite matrix $B$, we denote by $\norm{a}_B$ the $B$-weighted Euclidean norm of $a$ given by $\sqrt{\langle Ba, a \rangle}$. 

\paragraph{Optimistic algorithms} Optimistic algorithms select actions $X_t$ by solving the bi-level optimization problem
 $ (X_t, \theta_t) \in \argmax_{(x, \theta) \in \cX \times \Theta_{t-1}}\langle x, \theta \rangle $ in each round $t\leq n$. We instead consider randomised algorithms which randomise over $\Theta_{t-1}$. These methods are formally defined in \cref{sec:assumptions}

\paragraph{Bregman divergence} Our analysis will make use of a generalised Bregman divergence, defined convex function $f \colon \R^d \to \R$ as
$$
  D_{f}(x,y) = f(x) - f(y) - \langle \nabla f(y), x-y \rangle\,,
$$
for almost every $y \in \Rd$, where $\nabla f$ denotes the gradient of $f$. We recall that convex functions are almost everywhere differentiable \citep[][Theorem 25.5]{rockafellar1970convex}.

\paragraph{Probabilistic formalism} Let $\F = (\F_t)_{t \geq 0}$ be a filtration where $\F_0$ is the trivial $\sigma$-algebra and $\F_t = \sigma( \sigma(X_t, Y_t), \mathbb{A}_t)$, where $\mathbb{A}_t$ is the $\sigma$-algebra generated by any additional random variables the algorithm uses in selecting $X_t$. 
Note that this means that $X_t$ is $\F_t$-measurable.
We will write $\P_t$ for the $\F_t$-conditional probability measure and $\E_t$ for the corresponding expectation. With this, we formalise the assumption that for all $t \geq 1$, $Y_t$ is conditionally $S$-subgaussian as that
\begin{equation}\label{eq:subgauss-assumption}
  \E_{t-1} \exp \{s Y_t\} \leq \exp \{s^2 S^2/2\} \spaced{for all} s \in \R\,,\ t \geq 1\,.
\end{equation}

\paragraph{Asymptotic notation} We will write $f(x) \lesssim g(x)$ if $f(x) = O(g(x))$, and use $\gtrsim$ for the converse.

\paragraph{Vectors, norms, balls \& spheres} We will write $\norm{\cdot}$ to denote the $\ell_2$-norm. We recall that for a positive-definite matrix $B$ and a vector $a$ of compatible dimensions, $\norm{a}_B = \sqrt{\langle B a, a \rangle}$ denotes the $B$-weighted $\ell_2$ norm. We write $\Bd$ for the closed unit Euclidean ball in $\Rd$, and $\Sd$ for its surface~$\partial \Bd$, the $(d-1)$-sphere.


%%%% Main result
%\section{Main result}
\section{A frequentist regret bound for randomised algorithms in linear bandits}

In this section, we state our main result that provides conditions under which randomised exploration algorithms can achieve frequentist regret of $\widetilde{O}(d\sqrt{n})$ in the linear bandit setting. We begin by describing the algorithmic framework and assumptions for the action set under which it holds.

\subsection{Randomised algorithms: definition and assumptions} \label{sec:assumptions}
We consider algorithms that at each time-step $t \geq 1$ sample a parameter of the form
$$
  \theta_t = \hat\theta_{t-1} + V^{-1/2}_{t-1} \eta_t\,,
$$
where $(\eta_t)_{t\geq 1}$ is a sequence of independent random variables (perturbations), and select action
$$
  X_t \in \argmax_{x \in \cX} \langle x, \theta_t \rangle\,.
$$
Our result will require the following assumptions to hold for the perturbations $(\eta_t)_{t\geq 1}$.

\begin{assumption}\label{ass:perturb}
  The perturbations $(\eta_t)_{t\geq 1}$ are independent rotationally-invariant random variables for which there exists a constant $K>0$ such that
    $$
      1 \leq \E \langle u, \eta_t \rangle^2 \leq K^2 \spaced{and} \E \langle u, \eta_t \rangle^4 \leq K^4 \spaced{for all} u \in \Sd\,,\ t \geq 1\,.
    $$    
\end{assumption}
\noindent These assumptions hold for many common distributions, such as standard Gaussian (with $K^4=3$), and the uniform distribution on $\sqrt{d}\Sd$ (with $K=1$). 


% Therefore, the per-step regret $r_t$ of randomised algorithms, is almost surely equal to the Bregman divergence under $J$ between $\thetaopt$ and $\thetat$:
% 

% These are smoothness, which ensures that perturbations in the parameter $\theta_t$ translate to at least some change in the corresponding arm $X_t$, and strong convexity, which ensures that small changes in $X_t$ only lead to small changes in the incurred regret. 


\subsection{Action set assumptions: smoothness and strong convexity}\label{sec:ass-arm}
A core part of our contribution is in identifying the properties of action sets that allow randomised exploration to succeed. Our assumptions will be expressed in terms of the support function of~$\cX$,
$$
  J_\cX(\theta) = \max_{x \in \cX} \langle x, \theta \rangle\,.
$$
Crucially, for randomised algorithms where for each $t \geq 1$, the $\FF_{t-1}$-conditional law of $\thetat$ is diffuse (implied by rotational invariance), we have that
$$
   X_t = \nabla J_\cX(\theta_t) \quad \text{almost surely for all $t \geq 1$\,.}
$$
Our upcoming assumptions ensure that $\nabla J_\cX$ is a suitably regular function. Note that the above relation means the per-step regret of randomised algorithms is given by the divergence
$$
  r_t = J_\cX(\thetaopt) - \langle X_t, \thetaopt \rangle = J_\cX(\thetaopt) - J_\cX(\thetat) - \langle \nabla J_\cX(\theta_t), \thetaopt - \thetat \rangle = D_{J_\cX}(\thetaopt, \thetat)\,,
$$
again, almost surely with respect to the $\FF_{t-1}$-conditional law of $\theta_t$.

Our assumptions will be based on the following three definitions:
\begin{definition}[Absorbing set]\label{def:absorbing}
  We call a set $\cX \subset \Rd$ absorbing if it is a neighbourhood of the origin.
\end{definition}

\begin{definition}[Strong convexity]\label{def:smooth}
  We say $J_\cX^2$ is $m$-strongly convex with respect to a norm $\snorm{\cdot}$ if
  $$
    \frac{m}{2} \snorm{\theta-\theta'}^2 \leq D_{J_\cX^2}(\theta,\theta')\spaced{for all} \theta, \theta'\in \Rd\,.
  $$
\end{definition}

\begin{definition}[Smoothness]\label{def:convex}
  We say that $J_\cX^2$ is $M$-smooth with respect to a norm $\snorm{\cdot}$ if 
  $$
  D_{J_\cX^2}(\theta,\theta') \leq \frac{M}{2} \snorm{\theta-\theta'}^2  \spaced{for all} \theta, \theta'\in \Rd\,.
  $$
\end{definition}


\noindent With these definitions in place, the conditions we will ask for on the arm set $\cX$ are captured thus.

\begin{assumption}\label{ass:convex}
  The action set $\cX$ is a closed absorbing subset of $\Bd$, and there exists a norm $\snorm{\cdot}$ and constants $M,m > 0$ such that $J_\cX^2$ is $m$-strongly convex and $M$-smooth.
\end{assumption}

The motivation for asking for strong convexity and smoothness for the square $J^2_\cX$, rather than directly for $J_\cX$, is that the quantity
\begin{equation}\label{eq:J-squared}
  \nabla J^2_\cX(\theta) = 2 J(\theta) \nabla J(\theta)
\end{equation}
does not explode as $\theta \to 0$, whereas $\nabla J_\cX(\theta)$ does. That $\cX$ is absorbing ensures that the multiplier $J(\theta)$ in the above is positive, which will come in useful in our proofs---we do not believe this assumption to be essential, but we have thus far been unable to eliminate it.

\begin{remark}
  \cref{def:convex} generalises the notion of $M$-strong convexity used in \citet{rusmevichientong2010linearly}, where this was defined by the requirement that $$\norm{\nabla J_{\cX}(\theta) - \nabla J_{\cX}(\theta')} \leq M\norm{\theta-\theta'} \spaced{for all} \theta,\theta' \in \Sd\,.$$
  Our definition will be vital to getting the right rate for randomised algorithms outside the $\ell_2$-ball case, and specifically to avoid incurring an extra factor of $\norm{\thetaopt} / J(\thetaopt)$ in the regret, which may be large. We note also that their definition is for the strong convexity of the arm-set, whereas our definition is for the smoothness of $J^2_\cX$. There is a duality between the (indicator function of) the set and the corresponding support function, which explains the inversion in the nomenclature.
\end{remark}

\begin{remark}
  If $\cX$ is absorbing and balanced (symmetric about the origin), $J_\cX$ is a norm; if it is just absorbing, $\tilde{J}(\theta) = J_\cX(\theta) \vee J_\cX(-\theta)$ is a norm. In these cases, it may be productive to try taking $\snorm{\cdot} = J_{\cX}(\cdot)$ (or $\tilde J(\cdot)$), as in our above examples. Of course, $\snorm{\cdot}, m,M$ do not need to be known to run the algorithm, and the regret implicitly scales with the best $M/m$ over all norms $\snorm{\cdot}$.
\end{remark}

An example of an action sets that satisfy \cref{ass:convex} are $\ell_p$ balls with $p \in (1,\infty)$:
\begin{example}\label{example:ellp-ball}
  Let $p,q > 1$ be conjugate indices ($\frac{1}{p} + \frac{1}{q} = 1$), $\cX = \mathbb{B}^d_q$ and $\snorm{\cdot} = \norm{\cdot}_p$. Then, \cref{ass:convex} holds with $m=1$, $M= (p-1)$ for $q \in (1,2)$ and $m = p-1$, $M=1$ for $q \in [2,\infty)$.
\end{example}

\noindent \cref{ass:convex} is unaffected by linear transformations, extending the above examples to ellipsoids:
\begin{example}\label{example:transformation.invariance}
Let $\cX$ be any arm set satisfying \cref{ass:convex} for some $\snorm{\cdot}$, $M$ and $m$. Then, for any $A \in \R^{d\times d}$, $A \cX := \{ x \in \R^d \colon Ax \in \cX \}$ satisfies \cref{ass:convex} for norm $x \mapsto \snorm{A x}$, $M$ and $m$.
\end{example}

\subsection{Main result and discussion}
We are now ready to state our main result which shows that any randomised algorithm satisfying  \cref{ass:perturb} with an action set satisfying \cref{ass:convex} achieves at most $\widetilde{O}(d\sqrt{n})$ regret in the linear bandit problem. This matches the lower bound of \citet{rusmevichientong2010linearly} up to logarithmic factors (based on $\cX = \Bd$, a set that satisfies our assumptions). 

\begin{theorem}\label{thm:main-regret}
  Fix $\lambda \geq 1$ and $\delta \in (0,1)$. Suppose that a learner uses a randomised algorithm with perturbations satisfying \cref{ass:perturb} on a linear bandit instance with an arm-set that satisfies \cref{ass:convex}. Then, for any $\thetaopt \in \sqrt{d}\Bd$, with probability $1-\delta$, for all $n \geq 1$, the $n$-step regret incurred by the learner is bounded as
  $$
    R_n \lesssim \frac{M}{m} K (\beta_n^2 \vee K^2d) \sqrt{n} + K^4 \beta_n \sqrt{n(d \log(1+n/(d\lambda)) + \log(1/\delta))} \,, %+ d \beta_n^2 K^2 \frac{M^2}{m^2} \log(dn/(\delta \lambda)) \log(n+1) \,.
  $$
\end{theorem}

\noindent The proof of this result is presented in \cref{sec:proof}, with much of the details deferred to the appendices. We now discuss some aspects of our result, its proof and its relation to previous works.

\paragraph{On the regret of Thompson sampling}
If the noise in the responses $(Y_t)_{t\geq 1}$ is Gaussian with a known variance $\sigma^2$, and if for all $t\geq 1$ the perturbations are given by $\eta_t\sim \cN_d(0, \sigma^2I)$, then our randomised exploration algorithm is equivalent to the linear Thompson sampling algorithms of \citet{russo2014learning,agrawal2012analysis,abeille2017linear}. Thus, for action spaces satisfying \cref{ass:convex}, \cref{thm:main-regret} shows that Thompson sampling can enjoy regret of $O(d\sqrt{n} \log(n))$, leaving at most an $O(\log n)$ gap between this frequentist regret and the corresponding Bayesian regret \citep[see][for Bayesian analyses]{russo2014learning,russo2016information}.

\paragraph{On the lower bound for randomised algorithms}
We remark that \cref{thm:main-regret} holds for any randomised algorithm without any modification; in particular there is no need to inflate any variance proxies. 
This is in contrast to lower bounds by \citet{hamidi2020worst,zhang2021feel} which show that there exist problem instances on which linear Thompson sampling suffers linear regret. These instances are specifically designed so that there is a bad `trap' arm, where pulling that arm yields regret, but no information, so that Thompson sampling gets stuck. They are the polar opposite of what \cref{ass:convex} asks for: not absorbing, strongly convex, or smooth.


\begin{figure}[t]
\begin{center}
  \input{figure.tex}
\end{center}
\caption{Illustration of the update to the confidence sets during non-optimistic exploration, and the impact this has on the per-step worst case regret, when $\mathcal{X} = \Bd$. In red, we have an initial confidence set $\Theta$; the corresponding worst-case optimal action over $\Theta$ is given by $x = \arg\min_{\theta \in \Theta} \langle \nabla(\theta), \theta_\star \rangle$ and the associated per-step worst case regret is $\Delta = \|\theta_\star\|_2 - \langle x, \theta_\star\rangle$. In blue, we illustrate the average of the respective quantities after randomised structured exploration with $\theta \sim \Theta$. That is, taking $V^\prime = V + \mathbb{E}_{\theta \sim \Theta} \big(\nabla(\theta) \nabla(\theta)\tran\big)$. While the actions sampled by this strategy are unlikely to be optimistic, this randomised strategy does in fact explore---the confidence set shrinks---and this reduces the per-step regret.}\label{fig:ts-non-optimistic-behavior}
\end{figure}


\paragraph{Limitation of optimism-based proofs}
Existing proofs of frequentist regret bounds for randomised algorithms in linear bandit, including those of \citet{agrawal2013thompson,abeille2017linear,kveton2020randomized,janz2023exploration}, leverage that with high probability, 
\begin{equation*}
r_t = J_\cX(\thetaopt) - \langle X_t, \thetaopt \rangle  = D_{J_\cX}(\theta_\star,\theta_t) \leq \sup_{\theta \in \Theta_t} D_{J_\cX}(\theta_\star,\theta)\,,
\end{equation*}
and then show that $\sup_{\theta \in \Theta_t} D_{J_\cX}(\theta_\star,\theta)$ can be suitably controlled when randomised sampling guarantees sufficient optimism---that is, when the algorithm is optimistic with a fixed probability. Unfortunately, as illustrated in~\citet[][Fig.~2]{abeille2017linear}, guaranteeing optimism with a fixed probability requires inflating the variance of the sampling distributions, and this results in an extra $\sqrt{d}$ factor in the regret bound. 
Moreover, these proofs implicitly suggest that non-optimistic samples do not help in controlling the upper bound on the per-step regret, $\sup_{\theta \in \Theta_t} D_{J_\cX}(\theta_\star,\theta)$.

This approach is overly conservative in two ways: first, while a particular sample may provide very little information---measured through the design matrix update $X_tX_t\tran = V_{t} - V_{t-1}$---the sample may still provide useful information \emph{on average}, that is, by considering $\Et X_tX_t\tran $. Second, while the information acquired at a time $t$ might not significantly reduce the per-step regret bound $\sup_{\theta \in \Theta_{t+1}} D_{J_\cX}(\theta_\star,\theta)$ for the step immediately following it, it may prove useful at later steps. Figure.~\ref{fig:ts-non-optimistic-behavior} illustrates how non-optimistic samples provide useful information that is ignored by the optimistic proof approaches.

\paragraph{Our proof techniques} The key challenge in developing a non-optimistic proof for randomised algorithms in linear bandits is to directly analyse the \emph{dynamic of the exploration}, that is, of the process $\{\Theta_t\}_{t\geq 0}$, and relating this to the upper bound of the per-step regret process, $\sup_{\theta^\prime,\theta \in \Theta_t} D_{J_\cX}(\theta^\prime,\theta)$. Interestingly, such approach is closer to the analysis of Thompson sampling in the $K$-armed bandit setting, for which it is shown to be optimal \citep{kaufmann2012thompson,agrawal2012analysis}. Within the proof of our regret bound, \cref{thm:main-regret}, we address the above points by: 
\begin{enumerate}[(i)]
  \item Providing a new bound on $\sup_{\theta^\prime,\theta \in \Theta_t} D_{J_\cX}(\theta^\prime,\theta)$, $t \geq 0$ by leveraging strong convexity and smoothness;
  \item Characterising the minimum amount of information acquired during interaction through a lower bound on $V_t$, where $V_t$ acts as a proxy for $\Theta_t$;
\end{enumerate}
and connecting (i) and (ii) by studying the properties of the \emph{average per-step} information $\Et X_tX_t\tran$.

\paragraph{Comparison with forced exploration} \citet{rusmevichientong2010linearly} proposes a phased explore-then-commit algorithm that interleaves rounds of playing $d$ linearly independent actions with increasingly long exploitation phases, where the estimated best action is selected. They show prove a regret bound on the order of $O((\|\thetaopt\| + 1/ \|\thetaopt\|) d\sqrt{n})$ for their approach, which notably behaves poorly as $\theta \to 0$. This behaviour is because their exploration is isotropic---equal in all directions---and not directed by an estimate of $\thetaopt$. In contrast, randomised exploration algorithms account for structure by (i)~taking $X_t = \nabla J(\theta_t)$ (almost surely), which accounts for the geometry of the action-set, and (ii)~sampling $\theta_t$ from a distribution concentrated on a scaled version of $\Theta_{t-1}$, which accounts for the current estimate of $\thetaopt$. One might interpret randomised algorithms as blending together the exploration and exploitation stages with a more careful balance between the two.

\input{proofs/main-regret}

\section{Conclusion}
In this paper, we have presented a new analysis of randomised exploration algorithms for the linear bandit setting, which establishes that, given a nice-enough action set, randomised algorithms can obtain the optimal dependence on the dimension of the problem without need for any algorithmic modifications. Our improved regret bounds requires that the action space satisfies a smoothness and strong convexity condition, \cref{ass:convex}, which ensures that small perturbations in the parameter space are translate directly to at least some perturbations in the action space, while also guaranteeing that these do not lead to large changes in the instantaneous regret. 

Our results complement the lower bounds by \citet{hamidi2020worst,zhang2021feel} which show that linear Thompson sampling can suffer linear regret in particular settings where the connection between randomness in the parameter and action spaces is broken. 
However, these results together still do not give a complete characterisation of when randomised exploration algorithms can and cannot achieve the optimal rate of regret in the linear bandit setting: it remains an important open problem to understand exactly which action spaces permit an optimal dependence on the dimension.

\section*{Acknowledgements} This project started in earnest as a result of discussions taking place at the 2023 Workshop on the Theory of Reinforcement Learning in Edmonton. We thank Csaba Szepesvári for organising this workshop, and for feedback on early versions of this work. DJ \& MA  thank Gergely Neu for putting them in touch with CP-B, who was working contemporaneously on the same problem.

\printbibliography

\appendix

\clearpage
\input{proofs/appendix}
\input{proofs/per-step}
% \input{proofs/geometry-change}
\input{proofs/epl}

\end{document}
