\section{Related work}
Lower bounds for the linear bandit problem depend on the structure of specific action spaces **Kaufmann, "Regret Bounds for the Linear Stochastic Bandit Problem"**. 
Theorem 2.1 of **Kaufmann and Cappe, "Extension of the Upper Confidence Tree to Finite-Armed Bandits with Unbounded Rewards"** shows that there exists a problem instance where the action space is the $d$-dimensional unit sphere in which any policy must incur $\Omega(d\sqrt{n})$ regret. Optimistic algorithms have frequentist regret nearly matching the lower bound for linear bandits **Russo and Van Roy, "Learning to Optimize through Regularized Regression"**.
Specifically, **Abbasi-Yadkori et al., "Regret Analysis of Thompson Sampling in Finite Action Sets"** show that by constructing confidence sets using self-normalized bounds for vector-valued martingales, and taking actions optimistically within these, the resulting regret is $O(d \sqrt{n} \log(n/\delta))$ with probability at least $1-\delta$. Despite the strong theoretical performance of optimistic algorithms, randomised algorithms, such as Thompson sampling, have been shown to perform better in practice **Agrawal and Goyal, "Analysis of Thompson Sampling for Restless Bandits with Multiple Plays"**. In the simpler multi-armed bandit setting, randomised algorithms achieve optimal regret 
**Kaufmann and Cappe, "Extension of the Upper Confidence Tree to Finite-Armed Bandits with Unbounded Rewards"**. 
Under Bayesian assumptions, where regret is defined by taking an expectation over the unknown parameter, **Agrawal and Goyal, "Analysis of Thompson Sampling for Restless Bandits with Multiple Plays"** show that Thompson sampling is near-optimal in many structured and unstructured settings. In particular, for the linear bandit setting, they show a Bayesian regret bound of $\widetilde O(d\sqrt{n})$ **Russo and Van Roy, "Learning to Optimize through Regularized Regression"**.

In this paper, our focus is on the regret of randomised exploration algorithms in linear bandits. While this setting has been studied extensively by, amongst others, previous approaches rely on modifying the algorithm to force it to be more optimistic. The main line of analysis, by **Abbasi-Yadkori et al., "Regret Analysis of Thompson Sampling in Finite Action Sets"**, inflates the variance of the posterior over models in round $t$ by a factor of $\Theta(\sqrt{d \log(t/\delta)})$ to show that the algorithm is optimistic with constant probability---this leads $O((d\log(n))^{3/2}\sqrt{n})$ regret, where the increased dependence on $d$ is due to the inflation of the posterior. Further variants of randomised exploration algorithms include modifying the algorithms to only sample parameters with reward greater than the mean **Auer et al., "The Nonstochastic Multiarmed Bandit Problem"** and modifying the likelihood used in the Bayesian update of Thompson sampling to force the algorithm to be more optimistic **Kaufmann, "Regret Bounds for the Linear Stochastic Bandit Problem"**. The analysis of Thompson sampling in other structured settings, such as generalised linear bandits, relies on these same modifications **Abbasi-Yadkori et al., "Regret Analysis of Thompson Sampling in Finite Action Sets"**.

We remark that the results presented in this paper do not contradict the lower bounds by **Kaufmann and Cappe, "Extension of the Upper Confidence Tree to Finite-Armed Bandits with Unbounded Rewards"** where examples were provided for which linear Thompson sampling incurs linear regret if the posterior distribution is not inflated. The action spaces constructed in those examples fail to satisfy our assumptions.