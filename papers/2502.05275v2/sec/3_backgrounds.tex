\section{Backgrounds}

\textbf{Overview on Failure Detection. } We consider failure detection on the multi-class classification task. 
Let $\mathcal{X} \in \mathbb{R}^d$ be the input space and $\mathcal{Y} = \{ 1, 2, \dots, C \}$ be the label space, where $d$ is the dimension of the input vector.
Given a data set $\{ (\mathbf{x}_i, y_i) \}_{i=1}^N$ with $N$ data points independently sampled from the joint probability distribution $\mathcal{X} \times \mathcal{Y}$, a standard neural network $f: \mathcal{X} \rightarrow \mathcal{Y}$ outputs a probability distribution over the $C$ categories.
For an input $\mathbf{x}$, $f$ outputs $\boldsymbol{\hat{p}} = \hat{P}(y|\mathbf{x};\theta)$ as the class probabilities, where $\theta$ denotes the network's parameters.
In the context of failure detection, we consider a pair of functions $(f, g)$, where $g: \mathcal{F} \times \mathcal{X} \rightarrow \mathbb{R}$ is the confidence scoring function, and $f \in \mathcal{F}$.
With a predefined threshold $\tau \in \mathbb{R}^+$, the failure detection output is defined as:
\begin{equation}
    (f,g)(\mathbf{x}) = 
    \begin{cases}
   \hat{P}(y|\mathbf{x};\theta), & \text{if } g(f, \mathbf{x}) \geq \tau \\
    \text{detect},              & \text{otherwise.}
\end{cases}
\end{equation}

Failure detection is initiated when $g(f, \mathbf{x})$ falls below a threshold $\tau$. 
Ideally, a confidence scoring function should output higher confidence scores for correct predictions and lower confidence scores for incorrect predictions.
Despite efforts in designing CSFs, Jaeger et al.~\cite{jaeger2023a} has shown that the standard Maximum Softmax Prediction remains the best CSF across a wide range of datasets and network architectures. 
Mathematically, MSP is defined as:
\begin{equation}
    g(f, \mathbf{x}) = \max_{c\in\mathcal{Y}} \hat{P}(y=c|\mathbf{x};\theta)
\end{equation}
which returns the maximum output signal after the softmax activation function on the network output layer.

\noindent\textbf{Failure Detection with VLM.} \label{subsec:zs}
CLIP~\cite{Radford2021LearningTV}, a vision-language model, is pre-trained on a large-scale dataset comprising of 400 million image-text pairs. 
CLIP uses contrastive learning to align the image and text pairs. 
During inference, we calculate the model's logits as the cosine similarity score between the input image embedding and the corresponding text embeddings. 
Given an input image $\mathbf{x}$, the embedding is denoted as $f_{\text{img}}(\mathbf{x}) \in \mathbb{R}^m$. 
In addition, $C$ text labels represent the category names $\{ \mathbf{t}_c \}_{c=1}^C$, where $f_{\text{txt}}(\mathbf{t}_c) \in \mathbb{R}^m$ are the embeddings and $m \ll d$. 
For each category, we calculate the corresponding logit as:
\begin{equation} \label{eq:simscore}
    s_c = 100 \times \frac{f_{\text{img}}(\mathbf{x}) \cdot f_{\text{txt}}(\mathbf{t}_c)}{\lVert f_{\text{img}}(\mathbf{x}) \rVert \lVert f_{\text{txt}}(\mathbf{t}_c) \rVert}
\end{equation}
where $\lVert \cdot \rVert$ is the $L_2$ norm. 
The softmax function then converts the logits into probabilities:
\begin{equation} \label{eq:softmax}
    \hat{p}_c = \frac{\exp({s_c})}{\sum_{j=1}^{C} \exp({s_j})}
\end{equation}
where $\hat{p}_c \in \boldsymbol{\hat{p}}$. $f(\mathbf{x}) = \operatorname{argmax}_{c \in \mathcal{Y}} \hat{p}_c$ is the prediction, and $g(f, \mathbf{x}) = \max_{c \in \mathcal{Y}} \hat{p}_c$ can be regarded as the model confidence for a given input $\mathbf{x}$ using MSP. 