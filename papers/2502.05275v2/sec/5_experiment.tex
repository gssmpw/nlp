% Natural
\begin{table*}[!htb]
    \centering
    \resizebox{0.8\linewidth}{!}{
    
    \begin{tabular}{llcccccc}
    \toprule
          \multirow{2}{*}{\textbf{Dataset}}&\multirow{2}{*}{\textbf{Method}}& \multicolumn{3}{c}{ResNet-101} & \multicolumn{3}{c}{ViT-B/32}\\
          \cmidrule(l{3pt}r{3pt}){3-5} \cmidrule(l{3pt}r{3pt}){6-8}
          &&\textbf{AUROC} $\uparrow$ & \textbf{FPR95} $\downarrow$ & \textbf{ACC} $\uparrow$ & \textbf{AUROC} $\uparrow$ & \textbf{FPR95} $\downarrow$ &\textbf{ACC} $\uparrow$  \\
    
    \cmidrule(l{3pt}r{3pt}){1-5} \cmidrule(l{3pt}r{3pt}){6-8}% CIFAR10
          \multirow{7}{2cm}{CIFAR10 ($K = 10$)}&Zero-shot $+$ MSP & 85.98 & 62.98 & 78.01 & 88.92&  58.66 & 88.92\\
          &\hspace{1.35cm} $+$ ODIN & 83.65 & 65.50 & 78.01 & 84.49&   65.36& 88.92\\
          &\hspace{1.35cm} $+$ DOCTOR & 86.56 & 63.76 & 78.01 & 88.58&   62.32& 88.92\\
          & Ensemble $+$ MSP & \textbf{86.35} & 63.53 & 80.97 & \textbf{89.25} & 57.03 & 89.70\\
          & \hspace{1.4cm} $+$ ODIN &83.39 & 67.95& 80.97 &  83.66& 63.34& 89.70\\
          & \hspace{1.4cm} $+$ DOCTOR & 85.67 & 66.53 & 80.97 &  88.68& 58.87& 89.70\\
          & DescCLIP $+$ MSP & 85.84 & 64.68 & 80.70 &  89.28 &  58.77 & 88.80\\
          & \hspace{1.45cm} $+$ ODIN & 80.92 & 68.34 & 80.70 & 82.61 & 66.83 & 88.80 \\
         & \hspace{1.45cm} $+$ DOCTOR & 84.99 & 67.92 & 80.70 & 88.80 & 61.64 & 88.80 \\
          \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-8}
          &ORCA-B & 84.90 & 66.09 & \underline{80.98} &  87.34 &  \textbf{50.52}& \underline{89.34}\\
          &ORCA-R & \underline{85.93} & \textbf{62.68} & 80.60 & \underline{89.00} &  \underline{52.70} & \textbf{90.00}\\
    
    \cmidrule(l{3pt}r{3pt}){1-5} \cmidrule(l{3pt}r{3pt}){6-8}% CIFAR100
         \multirow{7}{2cm}{CIFAR100 ($K = 20$)}& Zero-shot $+$ MSP & 80.72 & 73.40 & 48.50 & 81.15 & 71.09 & 58.42\\
         & \hspace{1.35cm} $+$ ODIN & 77.21& 75.13& 48.50 & 76.93& 71.08& 58.42\\
         & \hspace{1.35cm} $+$ DOCTOR & 79.68& 75.36& 48.50 & 81.57& 69.40& 58.42\\
         & Ensemble $+$ MSP & 79.22 & 73.43 & 48.66 & 81.44 & 70.88 & 63.91 \\
         & \hspace{1.4cm} $+$ ODIN & 75.59& 76.00& 48.66 & 75.73& 73.87& 63.91 \\
         & \hspace{1.4cm} $+$ DOCTOR & 77.96& 76.47& 48.66 & 80.02&74.06& 63.91 \\
         & DescCLIP $+$ MSP & 80.22 & 73.39 & 52.90 & 82.54 &  67.38 & 66.70 \\
         & \hspace{1.45cm} $+$ ODIN & 75.86 & 75.35 & 52.90 & 75.72 & 73.11 & 66.70 \\
         & \hspace{1.45cm} $+$ DOCTOR & 79.09 & 74.96 & 52.90 & 81.30 & 70.83 & 66.70 \\
         \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-8}
         & ORCA-B & 80.35 & \textbf{70.46} & 52.16 & \underline{83.35} & \underline{67.35} & 66.00\\
         & ORCA-R & \textbf{80.46} & \underline{72.38} & \textbf{53.11} & \textbf{83.40} & \textbf{67.00} & \underline{66.50}\\

    \cmidrule(l{3pt}r{3pt}){1-5} \cmidrule(l{3pt}r{3pt}){6-8}% ImageNet
         \multirow{7}{2cm}{ImageNet ($K = 25$)}& Zero-shot $+$ MSP & 78.93 & 74.05 & 56.67 & 79.44 & 72.91 & 58.37 \\
         & \hspace{1.35cm} $+$ ODIN & 70.59& 80.75& 56.67 & 70.48& 80.07& 58.37 \\
         & \hspace{1.35cm} $+$ DOCTOR & 78.38&  75.90& 56.67 & 79.01& 74.17& 58.37 \\
         & Ensemble $+$ MSP & 78.58 & 74.37 & 56.73 & 79.66 & 72.89 & 59.22 \\
         & \hspace{1.4cm} $+$ ODIN & 70.29&  80.98& 56.73 &  70.61& 80.55 & 59.22 \\
         & \hspace{1.4cm} $+$ DOCTOR & 77.98 & 76.25 & 56.73 & 78.34 & 76.24 & 59.22 \\
         & DescCLIP $+$ MSP & 80.09 & 72.99 & 61.94 & 80.77 & 71.34 & 63.20 \\
         & \hspace{1.45cm} $+$ ODIN & 69.92 & 81.53 & 61.94 & 70.80 & 80.14 & 63.20 \\
         & \hspace{1.45cm} $+$ DOCTOR & 79.68 & 73.95 & 61.94 & 80.50 & 71.96 & 63.20 \\
         \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-8}
         & ORCA-B & \underline{80.24} & \textbf{71.13} & 62.11 & \underline{80.77} & \textbf{69.19} & 63.02 \\
         & ORCA-R & \textbf{80.57} & \underline{72.41} & \textbf{62.29} & \textbf{80.91} & 71.70 & \textbf{63.20} \\

    \bottomrule
    \end{tabular}
    }
    \caption{Performance on \textit{CIFAR-10/100} and \textit{ImageNet}. AUROC, FPR@95TPR (FPR95), and ACC are percentages. With ACC taken into account, \textbf{bold} indicate the best results, \underline{underlined} denote ours with the second best results.}
    \label{tab:cifar}
\end{table*}

\section{Experiment} \label{sec:experiment}
\vspace{5pt}
\textbf{Datasets.}
We evaluate ORCA on a wide variety of datasets:

\noindent\textbf{1. Natural Image Benchmark}
(1) \textit{CIFAR-10/100} \cite{Krizhevsky2009LearningML} is a popular image recognition benchmark spanning across 10/100 categories.
(2) \textit{ImageNet-1K}~\cite{imagenet} a well-known benchmark in computer vision, containing 1000 fine-grained categories, with 1,281,167 training and 50,000 validation samples. This benchmark contains fine-grained categories that are visually similar, making the failure detection task more challenging.

\noindent\textbf{2. Satellite Image Benchmark}
(3) \textit{EuroSAT}~\cite{Helber2017EuroSATAN} is a satellite RGB image dataset, containing $10$ categories of land usage, such as forest, river, residential buildings, industrial buildings, \textit{etc}. The dataset comprises of 27,000 geo-referenced samples.
(4) \textit{RESISC45}~\cite{Cheng_2017} is a public benchmark for Remote Sensing Image Scene Classification. It contains 31,500 images, covering 45 scene categories with 700 images in each categories.

\vspace{5pt}
\noindent\textbf{Baselines.}
We compare ORCA to $3$ models in combination with $3$ CSFs, yielding a total of $9$ baselines. Note that we only compare with post-hoc CSFs because our methods do not require any training.

\noindent\textbf{1. Models} 
(1) \textit{Zero-shot}~\cite{Radford2021LearningTV}: The prediction of zero-shot CLIP relies on the text category name as introduced in the original paper. We compute the logits using Eq.~\ref{eq:simscore} and apply CSFs to calculate the model's confidence.
(2) \textit{Ensemble}~\cite{Radford2021LearningTV}: This model ensembles multiple templates into zero-shot classification, effectively acting as an ensemble method. We average the similarity scores from multiple templates for each category before extracting the softmax logits.
(3) \textit{DescCLIP}~\cite{Menon2022VisualCV}: As described in Sec.~\ref{sec:desc-clip}, DescCLIP averages the similarity scores of all the concepts for each category; we then apply CSFs to estimate the confidence score.

\noindent\textbf{2. CSFs}
(1) \textit{MSP}~\cite{Hendrycks2016ABF}: The confidence score is measured by taking the maximum value of the softmax responses.
(2) \textit{ODIN}~\cite{liang2020enhancing}: This CSF is a temperature-scaled version of MSP. We use the default temperature $T=1000$ and do not use perturbation for a fair comparison.
(3) \textit{DOCTOR}~\cite{Granese2021DOCTORAS}: Unlike MSP, DOCTOR fully exploits all available information contained in the soft-probabilities of the predictions to estimate the confidence.

\vspace{5pt}
\noindent\textbf{Implementation Details.} 
We utilize CLIP's ResNet-101 and ViT-B/32 backbones to perform zero-shot prediction on the benchmarks and calculate the performance metrics.
For dataset with few categories, such as \textit{CIFAR-10} and \textit{EuroSAT}, we use different prompts to retrieve diverse collections of concepts from the large language model GPT-3.5~\cite{Brown2020LanguageMA,peng2023gpt35turbo} and manually select the top $10$ visual concepts that are the most distinctive among categories. An example of our prompt is as follows, with more details in the Supplementary:
\begin{lstlisting}[breakatwhitespace=true]
Q: What are some distinctive visual concepts of [CATEGORY]?
A: Some distinctive visual concepts of [CATEGORY] are:
\end{lstlisting}
For datasets with a larger number of categories, we use the concept collection provided by Yang et al.~\cite{Yang2022LanguageIA}. 
This collection contains up to $500$ concept candidates per category; we then select the top concepts that yield the highest average similarity score with the images within each category to form $\mathcal{A}$.
We include the number of concepts used for each dataset in Table~\ref{tab:cifar} and~\ref{tab:scientific}.

\subsection{Evaluation Metrics} 
\textbf{Failure detection accuracy (AUROC).} This evaluation protocol, a threshold-independent performance evaluation, measures the area under the receiver operating characteristic curve as CSFs inherently perform binary classification between correct and incorrect predictions. A higher value denotes better ability to predict failures.

\noindent\textbf{False positive rate (FPR@95TPR).} This metric denotes the false positive rate or the probability that a misclassified sample is predicted as a correct one when the true positive rate is at 95\%.
It is a fraction that the model falsely assigns higher confidence values to incorrect samples, reflecting the tendency to be overly confident in incorrect predictions.

\noindent\textbf{Classification accuracy (ACC).} A classifier with low accuracy might produce easy-to-detect failures~\cite{jaeger2023a} and benefit from a high AUROC. Ideally, we wish a model to yield a high AUROC and ACC, and a low FPR simultaneously. 

\subsection{Results on Natural Image Benchmarks}\label{subsec:natural}
We report the performance of all methods on the three evaluation metrics on the natural image benchmarks on ResNet-101 and ViT-B/32 and provide the following \textit{observations}:
\begin{tcolorbox}[width=\linewidth,colback={verylightgray}, colframe={lightgray},left=0pt, right=0pt, top=0pt, bottom=0pt]
{\bf Observation 1:} {Concept-based methods demonstrate better failure detection.}
\end{tcolorbox}
\noindent Table~\ref{tab:cifar} shows DescCLIP and ORCA consistently achieves higher AUROC compared to Zero-shot and Ensemble, especially on datasets with a large number of categories, such as \textit{CIFAR-100} and \textit{ImageNet}. 
% For instance, ORCA-R prominently achieves the best performance on both AUROC and ACC on \textit{ImageNet} for both backbones.
The augmentation to multiple signals per category helps concept-based methods obtain a finer-grained analysis for better failure detection.
On a different note, Ensemble boosts the Zero-shot's ACC but still results in a lower AUROC and higher FPR on the large-scale datasets for both backbones.
% On \textit{CIFAR-100}, this approach exhibits an increase of $5.49\%$ on Zero-shot's ACC on ViT-B/32 but still suffers an inferior performance on AUROC and FPR compared to concept-based methods.
Ensemble, in the same principles as concept-based methods, augments the number of signals; however, we hypothesize the \textit{lack of diversity} in those signals deteriorates the separability between correct and incorrect samples.
\begin{tcolorbox}[width=\linewidth,colback={verylightgray}, colframe={lightgray},left=0pt, right=0pt, top=0pt, bottom=0pt]
{\bf Observation 2:} {Our method reduces overconfident but incorrect predictions.}
\end{tcolorbox}
\noindent In Table~\ref{tab:cifar}, we observe that our methods consistently reduce the false positive rate across datasets and for both backbones.
Both variants of ORCA decrease the FPR@95TPR substantially while keeping AUROC and ACC competitive.
On \textit{ImageNet}, ORCA-B achieves the best performance on this metric, outperforming the zero-shot model and DescCLIP by $3.72\%$ and $2.15\%$ respectively using ViT-B/32.
We hypothesize that allowing the model to recognize an object from different angles provides more reliable confidence assessment, enabling faithful failure detection while also achieving superior predictive accuracy.

\subsection{Results on Satellite Image Benchmarks}
We report the performance on \textit{EuroSAT} and \textit{RESISC45} on ResNet-101 and ViT-B/32. Note that all results are zero-shot performance. We discuss the following \textit{observation}:
\begin{tcolorbox}[width=\linewidth,colback={verylightgray}, colframe={lightgray},left=0pt, right=0pt, top=0pt, bottom=0pt]
{\bf Observation 3:} {Our method boosts both predictive and failure detection accuracy on remote sensing benchmarks.}
\end{tcolorbox}
\noindent Table~\ref{tab:scientific} shows that ORCA-R consistently outperforms all baselines on all evaluation metrics.
Compared to DescCLIP $+$ MSP on \textit{EuroSAT}, ORCA-R enjoys a $3.6\%$ improvement in AUROC and $6.25\%$ in FPR while boosting the overall accuracy by $1.49\%$. On \textit{RESISC45}, while ORCA-R's improvement on AUROC and ACC is marginal, it significantly reduces FPR. Additionally, these datasets represent out-of-distribution data for CLIP, underscoring ORCA's enhanced reliability and robustness against such distributional variations.

% EuroSAT
\begin{table*}[!htb]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{llcccccc}
    \toprule
          \multirow{2}{*}{\textbf{Dataset}}&\multirow{2}{*}{\textbf{Method}}& \multicolumn{3}{c}{ResNet-101} & \multicolumn{3}{c}{ViT-B/32}\\
          \cmidrule(l{3pt}r{3pt}){3-5} \cmidrule(l{3pt}r{3pt}){6-8}
          &&\textbf{AUROC} $\uparrow$ & \textbf{FPR95} $\downarrow$ & \textbf{ACC} $\uparrow$ & \textbf{AUROC} $\uparrow$ & \textbf{FPR95} $\downarrow$ &\textbf{ACC} $\uparrow$  \\
          
    \cmidrule(l{3pt}r{3pt}){1-5} \cmidrule(l{3pt}r{3pt}){6-8}% EuroSAT
         \multirow{7}{2cm}{EuroSAT ($K = 10$)}& Zero-shot $+$ MSP & 61.73& 88.98 & 30.30 & 76.42& 80.24& 41.11\\
         & \hspace{1.35cm} $+$ ODIN & 61.35 & 89.38 & 30.30 & 75.54 & 79.28 & 41.11\\
         & \hspace{1.35cm} $+$ DOCTOR & 60.76 & 89.85 & 30.30 & 76.67 & 79.30 & 41.11\\
         & Ensemble $+$ MSP & 54.69 & 92.21 & 31.90 & 66.83 & 89.19 & 48.73\\
         & \hspace{1.4cm} $+$ ODIN & 55.10 & 93.09 & 31.90 & 65.73 & 90.09 & 48.73\\
         & \hspace{1.4cm} $+$ DOCTOR & 53.73 & 94.09 & 31.90 & 61.14 & 90.63 & 48.73\\
         & DescCLIP $+$ MSP & 64.89 & 86.39 & 33.13 & 73.93&  77.54 & 48.51\\
         & \hspace{1.45cm} $+$ ODIN & 64.16 & 87.16 & 33.13 & 71.74 & 78.34 & 48.51 \\
         & \hspace{1.45cm} $+$ DOCTOR & 62.79 & 89.05 & 33.13 & 72.74 & 79.85 & 48.51 \\
         \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-8}
         & ORCA-B & \underline{67.86} & \textbf{86.43} & \underline{34.11} &  76.20&  77.80 & \underline{49.74}\\
         & ORCA-R & \textbf{69.01} & \textbf{86.43} & \textbf{34.76} &  \textbf{77.55}&  \textbf{71.29}& \textbf{50.00}\\
         

    \cmidrule(l{3pt}r{3pt}){1-5} \cmidrule(l{3pt}r{3pt}){6-8}% RESISC45
         \multirow{7}{2cm}{RESISC45 ($K = 10$)}& Zero-shot $+$ MSP & 68.13 & 87.04 & 37.66 & 77.92 & 80.35 & 55.57 \\
         & \hspace{1.35cm} $+$ ODIN & 62.60 & 89.48 & 37.66 & 71.66 & 84.85 & 55.57 \\
         & \hspace{1.35cm} $+$ DOCTOR & 67.57 & 87.19 & 37.66 & 76.95 & 82.17 & 55.57 \\
         & Ensemble $+$ MSP & 68.87 & 85.39 & 39.79 & 78.40 & 80.14 & 56.68 \\
         & \hspace{1.4cm} $+$ ODIN & 62.67 & 89.57 & 39.79 & 71.99 & 85.31 & 56.68 \\
         & \hspace{1.4cm} $+$ DOCTOR & 67.88 & 87.29 & 39.79 & 77.54 & 82.34 & 56.68 \\
         & DescCLIP $+$ MSP & 73.44 & 79.78 & 43.16 & 77.47 & 82.25 & 58.33 \\
         & \hspace{1.45cm} $+$ ODIN & 69.47 & 84.61 & 43.16 & 71.49 & 86.21 & 58.33 \\
         & \hspace{1.45cm} $+$ DOCTOR & 72.95 & 79.89 & 43.16 & 76.81 & 84.88 & 58.33 \\
         \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-8}
         & ORCA-B & 71.88 & 90.41 & \textbf{46.22} & \underline{77.71} & 86.31 & 59.10 \\
         & ORCA-R & \textbf{74.28} & \textbf{80.31} & \underline{45.13} & \textbf{78.24} & \textbf{76.52} & \textbf{59.10}\\
    
    \bottomrule
    \end{tabular}
    }
    \caption{Performance on \textit{EuroSAT} and \textit{RESICS45}. AUROC, FPR@95TPR (FPR95), and ACC are percentages. With ACC taken into account, \textbf{bold} indicate the best results, \underline{underlined} denote ours with the second best results.}
    \label{tab:scientific}
\end{table*}

\subsection{Ablation Studies}~\label{ablate}
We conduct two ablation studies on the effect of the number of concepts and the choice of the weighting function used for ORCA-R in this section.
\begin{figure}[!ht]
  \centering
  
     \includegraphics[width=.75\linewidth]{fig/no_concepts_cifar100.pdf}
     \caption{Failure detection accuracy (AUROC) and false positive rate (FPR@95TPR) across different numbers of concepts on \textit{CIFAR-100}. Overall, we can an increase in the number of concepts boosts the performance in both metrics.}
    \label{fig:no-concepts}
\end{figure}

\begin{figure}[!ht]
  \centering
         \includegraphics[width=0.65\linewidth]{fig/wf-b.pdf}
         \caption{
         % Illustration of different weighting functions. 
         % (\textbf{Left}) Visualization of the weight distribution on the top-$10$ concepts among different weighting functions: \texttt{Logarithmic}, \texttt{Exponential}, \texttt{Linear} and \texttt{Polynomial}.
         Failure detection capabilities of each weighting function on \textit{EuroSAT}, where \texttt{Logarithmic} consistently outperforms others.}
         \label{fig:wf}
\end{figure}

% \begin{figure*}
% \centering
% \begin{subfigure}{.6\textwidth}
%   \centering
%   \includegraphics[width=0.8\linewidth]{CameraReady/LaTeX/fig/wf-a.pdf}
%   \caption{A subfigure}
%   \label{fig:sub1}
% \end{subfigure}%
% \begin{subfigure}{.4\textwidth}
%   \centering
%   \includegraphics[width=0.8\linewidth]{CameraReady/LaTeX/fig/wf-b.pdf}
%   \caption{A subfigure}
%   \label{fig:sub2}
% \end{subfigure}
% \caption{A figure with two subfigures}
% \label{fig:wf}
% \end{figure*}

\noindent\textbf{Ablation on number of concepts.}
We use the ViT-B/32 backbone on \textit{CIFAR-100} and $K=\{5, 10, 15, 20\}$ for this experiment. We study the effect of the number of concepts on the performance on AUROC and FPR@95TPR of DescCLIP $+$ MSP, ODIN, DOCTOR and ORCA-R.
Fig.~\ref{fig:no-concepts} shows that the FPR of ORCA-R is consistently lower than those of the other baselines across various $K$.
We also see an increasing (decreasing) trend in AUROC (FPR) as the number of concepts rises. This signifies a finer-grained assessment both enables better failure detection and alleviates the problem of assigning high confidence to incorrect predictions.

\noindent\textbf{Ablation on choice of weighting function.}
We examine how various weighting functions influence the failure detection efficacy of ORCA-R. Fig.~\ref{fig:wf} (left) visualizes the weight distribution on the top-$10$ concepts among the weighting functions. 
In Figure~\ref{fig:wf} (right), \texttt{Logarithmic} outperforms others, contrasting with \texttt{Exponential}, which exhibits the least effectiveness.
\texttt{Logarithmic} ensures a balanced distribution of weights, recognizing the importance of higher-ranked concepts while also accounting for lower-ranked ones.
Conversely, \texttt{Exponential} significantly overweighs the highest-ranked concept, neglecting the contributions of those ranked lower.