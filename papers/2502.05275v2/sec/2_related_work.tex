\section{Related Work}
% Talk about failure detection first
\textbf{Failure Detection.}
Failure detection, or misclassification detection, is a burgeoning area of research within the realm of artificial intelligence. 
Detecting when machine learning models produce incorrect or misleading predictions has significant implications for safety, reliability, and transparency in various domains.
Existing research in this field falls into two main categories: (1) retraining or fine-tuning of neural networks~\cite{Moon2020ConfidenceAwareLF,Zhu2023OpenMixEO,Zhu2023RethinkingCC}, and (2) the design of novel confidence score functions~\cite{Granese2021DOCTORAS,Hendrycks2016ABF}. 
The former approach involves retraining or fine-tuning neural networks with specific objectives aimed at improving the model's capability to recognize its own failures.
Zhu et al.~\cite{Zhu2023RethinkingCC} employs a training objective that seeks flat minima to mitigate overconfident predictions. While these approaches have shown promise, they often require extensive computational resources and access to the entire model, which may not be feasible for large VLMs.
Researchers have also turned their attention to the design of new CSFs~\cite{Granese2021DOCTORAS}.
% DOCTOR~\cite{Granese2021DOCTORAS} introduces a rejection-based method to identify model failures. 
Despite these efforts, the most robust CSF remains the MSP~\cite{jaeger2023a}. 
However, a downside of category-level CSFs is their inability to detect overconfident but incorrect predictions, which is problematic. 
In this work, we deconstruct category-level into concept-level signals to achieve a more nuanced estimate of the model's confidence.

A closely related sub-field is \textit{confidence calibration}~\cite{minderer2021revisiting,levine2023enabling,Mukhoti_2020_NIPS,pereyra2017regularizing}, where the goal is to adjust a model's predicted probabilities to ensure that they accurately reflect the true likelihood of those predictions being correct.
However, Zhu et al.~\cite{Zhu2023RethinkingCC} has empirically shown that calibration methods frequently yield no benefits or even detrimentally affect failure prediction.
Similarly, some works~\cite{jaeger2023a,Bernhardt2022FailureDI} also emphasizes the importance of \textit{confidence ranking} over \textit{confidence calibration} in failure detection. 
Some other related sub-fields are \textit{predictive uncertainty estimation}~\cite{pmlr-v48-gal16, BlundellJMLR15, LakshminarayananNIPS17, Mukhoti_2023_CVPR}, \textit{out-of-distribution detection}~\cite{Zhu2023OpenMixEO, liang2020enhancing, pmlr-v180-dinari22a,lee2018simple}, \textit{open-set recognition}~\cite{vaze2022openset, Geng_2021} and \textit{selective classification}~\cite{geifman2017selective,fisch2022calibrated}.

\vspace{5pt}
\noindent\textbf{Human-level Concepts in Vision-Language Models.}
Concept-based models (CBMs) aim to open the black box of neural networks. 
Concept bottleneck networks are pioneers for interpretable neural networks, with each neuron in the concept bottleneck layer representing a concept at the human level~\cite{Koh2020ConceptBM, Yuksekgonul2022PosthocCB}.
With the flexibility to employ free language in vision language models, such as CLIP~\cite{Radford2021LearningTV}, ALIGN~\cite{Jia2021ScalingUV}, FLAVA~\cite{Singh2021FLAVAAF}, and BLIP~\cite{Li2022BLIPBL, Li2023BLIP2BL}, concepts of human level can be naturally integrated into the prediction mechanism~\cite{Menon2022VisualCV, Yang2022LanguageIA, Oikarinen2023LabelFreeCB}.
This work can be viewed as a variant of CBMs for failure detection, which has never been considered before.
We show the approach better predicts failures and, as a byproduct, helps interpret \textit{why} a model fails.