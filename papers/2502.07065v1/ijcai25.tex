%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{bm}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{acronym}
\usepackage{amssymb}
\usepackage{changes}
\usepackage{mathtools}
\usepackage{natbib}
\input{defs}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.


% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Active Inference through Incentive Design in Markov Decision Processes}

% \author{
% }

\author{
Xinyi Wei\thanks{Equal contribution.}$^1$\and
Chongyang Shi\footnotemark[1]$^1$\and
Shuo Han$^2$\and
Ahmed H. Hemida$^3$\and
Charles A. Kamhoua$^3$\and 
Jie Fu$^1$ \\
\affiliations
$^1$University of Florida, Gainesville, FL \\
$^2$University of Illinois, Chicago.
$^3$DEVCOM Army Research Laboratory\\  
\emails
\{weixinyi, c.shi, fujie\}@ufl.edu,
hanshuo@uic.edu
\{ahmed.h.hemida.ctr,
charles.a.kamhoua.civ\}@army.mil
}




\begin{document}

\maketitle

\begin{abstract}
   We present a method for active inference with partial observations in stochastic systems through incentive design, also known as the leader-follower game. Consider a leader agent who aims to infer a follower agent's type given a finite set of possible types. Different types of followers differ in either the 
dynamical model, the reward function, or both. We assume the leader can partially observe a follower's behavior in the stochastic system modeled as a Markov decision process, in which the follower takes an optimal policy to maximize a total reward. To improve inference accuracy and efficiency, the leader can offer side payments (incentives) to the followers such that different types of them, under the incentive design, can exhibit diverging behaviors that facilitate the leader's inference task. We show the problem of active inference through incentive design can be formulated as a special class of leader-follower games, where the leader's objective is to balance the information gain and cost of incentive design. The information gain is measured by the entropy of the estimated follower's type given partial observations. Furthermore, we demonstrate that this problem can be solved by reducing a single-level optimization through softmax temporal consistency between followers' policies and value functions. This reduction allows us to develop an efficient gradient-based algorithm. We utilize observable operators in the \ac{hmm} to compute the necessary gradients and demonstrate the effectiveness of our approach through experiments in stochastic grid world environments.
\end{abstract}

\section{Introduction}
Incentive design~\citep{boltonContractTheory2005}, is also referred to as the \textit{principal-agent} or \textit{leader-follower} game. 
~\cite{ho1981information} address applications where a planner or leader aims to optimize system performance while anticipating and accounting for the active interactions of multiple users or followers. It is applied in various domains such as economic market~\citep{myerson1981optimal,williams2011persistent, easley2015behavioral}, smart city~\citep{mei2017game,kazhamiakin2015using}, and smart grid~\citep{braithwait2006incentives,alquthami2021incentive}.
 
 % Incentive design has a rich history  in the context of Stackelberg games ~\citep{ho1981information}.
% From a game-theoretic viewpoint \citep{ho1981information}, incentive design can be interpreted as a non-cooperative Stackelberg game, often referred to as a leader-follower game ~\citep{simaan1973stackelberg}. 
  % In a leader-follower game, % the follower's active response policy can be shaped by an incentive policy employed by the leader.  
  % the leader selects an incentive policy, in anticipation that  the follower will implement a best response policy to optimize his/her planning  objective. 
  
In the classical problem formulation, the leader and followers have respective payoff/reward functions and aim to optimize their own rewards.  We focus on a specific class of incentive design where the leader is to infer the followers’ \emph{type} (intentions, rewards, or dynamic model) by partially observing the follower’s active interactions with the system.  We refer to this as the problem of \emph{active inference through incentive design}.  
% Active inference offers a principled approach to modeling and explaining how autonomous agents make decisions in uncertain environments~\citep{friston2017active,friston2017deep}. The concept employs the free energy principle to describe the properties of random dynamical systems from an information-theoretic perspective. By minimizing expected free energy over time, we can obtain an agent's behavior through Bayesian inference~\citep{friston2014anatomy,friston2019free}.

In particular, consider a finite hypothesis set of possible followers, where each follower’s planning problem is modeled as a \ac{mdp} with the objective of maximizing total discounted rewards. Followers’ \ac{mdp}s may differ in their transition dynamics, discount factors, or reward functions. At each time step, the leader observes the activity of one follower, selected from the hypothesis set, through imperfect and noisy observations. The leader’s goal is to identify which follower is currently interacting with the system.
To improve inference accuracy within a finite time horizon, the leader strategically designs an incentive policy to offer side payments (additional rewards)  within the environment, influencing the followers' best responses. These incentives, however, incur a direct cost to the leader's own payoff. With only partial, noisy observations of the follower's state sequence, the leader calibrates the incentive policy to minimize uncertainty about the follower's type, while balancing the trade-off between inference accuracy and the cost of providing incentives.
\paragraph{Related work.}
 
Incentive design has been studied in different contexts, particularly in federated learning for IoT environments and mobile ad hoc networks (MANETs). Zhan et al.~\citep{zhan2020learning} used deep reinforcement learning for optimal pricing strategies in federated learning systems, while Li et al.~\citep{li2011game} applied game theory to study cooperation incentives in MANETs.
In addition, \cite{ratliff2020adaptive} addresses a principal-agent problem with multiple agents of unknown types. The principal optimizes an objective function which depends on the data from strategic decision makers (agents). The followers' decision-making processes are categorized into Nash equilibrium strategies and myopic update rules. They develop an algorithm that the principal can employ to learn the agents’ decision-making processes while simultaneously designing incentives to change their response to one that is more desirable. Adaptive incentive design in  \citep{ma2024adaptive} introduces gradient-ascent algorithms to compute the leader’s optimal incentive strategy, despite the lack of knowledge about the follower’s reward function. 
While adaptive incentive design focuses on dynamically adapting the leader’s policy based on the follower’s response, our work examines a different problem: incentive design that supports inference tasks under partial observability. Specifically, we assume the leader has a prior distribution over potential follower types and aims to reduce uncertainty in the posterior distribution of these types based on limited observations. To achieve this, we propose an information-theoretic objective for the leader, rather than defining the leader’s value in terms of cumulative discounted rewards.   Since information-theoretic measures cannot be directly expressed as cumulative rewards, we develop novel solutions  for this class of incentive design problems. % This change addresses the active inference problem by leveraging the incentive problem framework. 

% To solve the equilibrium in a 
%  leader-follower game, one can formulate a bi-level optimization (BLO) problem by viewing the leader's incentive design as the upper-level problem, and the followers' planing as the lower-level problem. Gradient-based bi-level optimization has recently drawn the attention in the field of deep neural networks for single-task and multi-task~\citep{chen2022gradient}. Building on this framework, \cite{zhang2022advancing} proposed a bi-level optimization-oriented pruning method (BIP) which a special class of BLO problems with a bi-linear structure, allowing it to be solved as easily as first-order optimization while maintaining computational efficiency.

% In planning problems, recent work \cite{molloy2023smoother} developed a method for obfuscating state trajectories in partially observable Markov decision processes (POMDPs). Another work defines opacity as entropy measures and uses it as the planning objective to reduce information leakage. \citep{shi_opacity_2024}.}

Information-theoretic metrics are also widely used in various active inference problems. \cite{egorovTargetSurveillanceAdversarial2016} and \cite{Araya2010Neurips} formulate a \ac{pomdp} with a reward function dependent on the agent's information state or belief. For instance, in target surveillance, a patrolling team is rewarded for reducing uncertainty in its belief about an intruder’s position or state. Similarly, in intent inference, \cite{Shendeep2019} utilized the negative entropy of the belief over an opponent’s intent as a reward, maximizing the total reward to enhance active inference. Also, \cite{Shi2024} use Shannon conditional entropy as an information leakage measure to solve the active inference problem in which an agent can actively query sensors to infer an unknown variable in a hidden Markov model.
To our knowledge, information-theoretic objectives have not been considered for the leader-follower games. Existing solutions to active inference are for motion planning of a single agent or a team of agents, and cannot be extended to solve the incentive design problems in the leader-follower game.

% \jf{add one paragraph on information-leakage from security community.}
Our problem formulation and methods are also applicable for intent inference, which is a key research topic of AI alignment. Inferring humans' intentions or capabilities can ensure that AI systems tailor the decisions for the specific user group and avoid unintended or harmful behaviors~\citep{leike2018scalable,soares2014aligning}.
%Additionally, \cite{kelley2012context} addresses intent recognition problem where robots involves observing agents, adopting their perspective to calculate parameter changes, and using these inputs with pre-trained \ac{hmm} and contextual knowledge to infer intentions.
\paragraph{Our contribution.}
Our research advances active inference through the following key contributions:
\begin{itemize}
    \item We introduce a novel incentive design framework for active inference, employing conditional entropy to quantify the leader's uncertainty about the follower's type from the leader's partial observations. %  all types of followers provide their best responses to given side payments.
    \item  We show the problem of incentive design can be formulated as a bi-level optimization problem, using an augmented-state hidden Markov model constructed from the set of policies of different potential follower types. 
    When each follower follows an optimal entropy-regularized policy, it is possible to transform the bi-level problem into a single-level optimization, enabling the solution with gradient descent methods.
    \item We consider that the leader has only partial and noisy observations of the followers’ trajectories, and we develop efficient methods for computing the gradient terms. These methods leverage observable operators \citep{jaeger2000observableoperator} within the framework of \ac{hmm}, enabling more accurate and computationally efficient gradient computation.
    \item Finally, we demonstrate the accuracy and effectiveness of our proposed methods through experimental validation. In this research, specifically, we minimize uncertainty when all types of followers provide their best responses to given side payments, using gradient descent methods.
\end{itemize}
 
\section{Preliminaries and Problem formulation}


\noindent \textbf{Notation.} The set of real numbers is denoted by $\reals$. Random variables will be denoted by capital letters and their realizations by lowercase letters ($X$ and $x$).  A sequence of random variables and their realizations with length $T$ are denoted as $X_{0:T}$ and $x_{0:T}$. The notation $x_i$ refers to the $i$-th component of a vector $x \in \reals^{n}$ or the $i$-th element of a sequence $x_0, x_1, \ldots$, which will be clarified by the context. 
Given a finite set $\mathcal{S}$, let $\dist{\mathcal{S}}$ be the set of all probability distributions over $\mathcal{S}$. The set $\mathcal{S}^{T}$ denotes the set of sequences with length $T$ composed of elements from $\mathcal{S}$, and $\mathcal{S}^\ast$ denotes the set of all finite sequences generated from $\mathcal{S}$.  
% The empty string in $\mathcal{S}^\ast$ is denoted by $\lambda$.

This active inference problem involves a leader-follower framework where a leader can offer side payments to supplement the followers' original rewards, and the followers always take the best responses. The leader can partially observe the followers' behavior and aims to minimize uncertainty about their types, balancing against the cost of side payments.

\paragraph{The follower's model.} A single-agent \ac{mdp} is defined as:

\[
    M = (S,A, P, \mu, \gamma, \bar R),
\]where $S$ is a set of states, $A$ is a set of actions, $P\colon S \times A \rightarrow \dist{S}$ is a probabilistic transition function such that $P({s}'|{s}, {a})$ is the probability of reaching state ${s}'$ given action ${a}$ being taken at state ${s}$, $\mu \in \dist{S}$ is an initial state distribution.
The original reward function for the agent without any side payment is $\bar R \colon S \times A \rightarrow \reals$ where $\bar R(s, a)$ is the reward received by the follower for taking action $a$ in state $s$.

% \paragraph{Incentives as side payments} The leader's incentive to an agent is represented as a function $x \colon S \times A \to \reals$, hereafter referred to as the \emph{side payments}. Specifically, $x(s,a)$ is the additional reward that the leader offers to the agent when he takes action $a$ in state $s$. We can view $x$ as a vector with the entry to be $x(s,a)$ for each $s\in S$, $a \in A$. 
% Given a  side payment $x$ and an \ac{mdp} $M$, an agent's modified reward function $R(x)$  is defined as follows: For all $(s,a)\in S\times A$, 
% 		\begin{equation}
% 			\label{eq:sidepayment-reward}
% 			R(s, a; 
% 			x ) = \bar R(s,a) + x(s,a).  
% 		\end{equation}

% \paragraph{The follower's best response}
% We assume the follower is fully rational, meaning they will always provide the optimal response in the \ac{mdp}. Given a well-defined value function $V(\mu,R(x),\pi) = \mathbb{E}_{s\sim\mu}[V(s,x,\pi)]$, where $\mu$ represents the initial state distribution for the follower, we denote the follower's optimal policy as $\pi^\ast(x)$, such that $\pi^\ast(x) \in \argmax_\pi V(\mu,R(x),\pi)$. The corresponding optimal value is denoted as $V^\ast(\mu,R(x))$.
% \xw{do I need to point out the value function is entropy regularized value function in this part?}
   

\paragraph{Different types of followers.}
We consider the interaction between a leader and a finite set $\cal T$ of followers with different types. 
For each type $i\in \mathcal{T}$,    follower $i$'s planning problem is modeled by an \ac{mdp}  $M_i = (S,A, P_i, \mu_i, \gamma_i, \Ro_i)  $. Different types have different initial state distributions, reward functions, discounting factors, or transition dynamics. Without loss of generality, we assume the followers have the same state and action spaces. 


\paragraph{Incentives as side payments.} 
The leader can allocate side payments to the environment. A side payment allocation is a function: $x \colon S \times A \to \reals_+$, hereafter referred to as the \emph{side payment}. Specifically, $x(s, a)$ is the additional non-negative reward that the leader offers to follower $i$ when follower $i$ takes action $a$ in state $s$, for each $i\in \mathcal{T}$. Let $x$ be a vector of dimension $S \times A$ representing side payments for each state-action pair, with its domain denoted as $\mathcal{X}$.

Due to the independent dynamics and independent reward functions with side payments, for each follower $i\in \mathcal{T}$, 
given a  side payment   $x$, follower $i$'s modified  reward function $R_i( x)$  is defined as follows: For all $(s,a)\in S\times A$, 
    \begin{equation}
        \label{eq:sidepayment-reward-i}
        {R}_i(s, a; 
        x ) = \Ro_i(s,a)+  x (s,a) .  
    \end{equation}
For each $i \in \mathcal{T}$,  follower $i$'s planning problem with side payment $x$ is thus modeled as the following \ac{mdp}:
\[M_i(x) = (S,A, P_i, \mu_i, \gamma_i, R_i( x)).\]

Given the \ac{mdp} $M_i(x)$ and a Markov policy $\pi: S\rightarrow \dist{A}$, let $V_i(\mu_i, R_i(x),\pi)$ be the value function evaluated for  policy $\pi$. 
%Let $\Pi$ be a set of Markov policies.

\paragraph{Leader's partial observation}
Given a follower's \ac{mdp} $M_i(x)$ and a set $\mathcal{O}$ of observations,  the leader's observation function is defined by $E_i: S\rightarrow \dist{\mathcal{O}}$. That is, the leader only partially observes the state of the follower. Note that it is easy to extend this observation function to allow partial observations of both states and actions, by augmenting the follower's states with actions taken.

% \jf{since all followers subject to the same leader's side payment. there is no need to introduce vector of $x$, or $x$.}
% For notational clarity, we denote by $x = [x]$ the collection of side payments to all followers, and denote $M_i(x) \coloneqq M_i(x)$ the follower $i$'s planning problem.

An informal problem statement is given  below.
\begin{problem}
Consider that the leader's objective is to estimate the type of a follower based on a finite observation sequence. The leader can incentivize diverging behaviors from different followers by providing side payments, which come at a cost to the leader. How should the leader design a side payment strategy to balance the estimation-utility trade-off? That is, how can the leader maximize the estimation accuracy while minimizing the total cost of side payments?
\end{problem}

\begin{remark}
   This problem formulation is motivated by AI-alignment problems, where the AI agent is to tailor some decision recommendations to the user based on the inferred intention or capability of the user. For example, ride-sharing platforms continuously monitor driver behaviors to estimate driver preferences and capabilities. By designing strategic incentives such as surge pricing and completion bonuses, these platforms can improve the accuracy and efficiency of the inference, and thus provide more personalized services, such as route recommendations and rider-driver matching. Other applications are related to AI-based security and intrusion detection. By strategically allocating sensors and decoys, a security system can manipulate users' perceptual reward functions to elicit divergent behaviors between attackers and normal users.
\end{remark}


\section{The incentive design problem formulation}
We first study how to measure the leader's uncertainty about the current follower's type after a finite sequence of observations, when the followers' policies are known and the leader observes a follower's behavior through imperfect observations. % In this case, let $\mathcal{T}$ be a finite set of possible follwers' types and $M_i$ is the \ac{mdp} for the $i$-th type, for $i\in \mathcal{T}$.
% \jf{lacking transition sentence. "First, we consider the case when the followers' policies are determined and the leader  observes a follower's activity with imperfect observations, how   uncertain the leader is about the follower's policy after a finite sequence of observation."}


\begin{definition}
 Given a collection of policies, referred to as a \emph{policy profile}, $\bm{\pi} = [\pi_i]_{i\in \mathcal{T}}$ of followers, and the leader's observation functions for followers $\{E_i, i \in \mathcal{T}\}$. The following \ac{hmm} can be constructed:
\[
\mathcal{M}(\bm{\pi})= \langle  
S\times \mathcal{T}, \mathbf{P}, {\cal{O}}, {\cal{E}}, \mu_0 \rangle
\]

\begin{itemize}
    \item $S\times \mathcal{T} $ is the augmented state space. Each state $(s,i)$ includes a state in the follower's \ac{mdp} and a type of the follower.
    % \item $A$ is the set of actions.
    \item $\mathbf{P}_{\bm{\pi}}: S\times \mathcal{T}\rightarrow \Delta(S\times \mathcal{T})$ is defined by 
    \begin{multline*}
    \mathbf{P}_{\bm{\pi}}((s', j)| (s,i)) \\
    =\begin{cases}
        \sum_{a\in A} P_i(s'|s, a)\pi_i ( s, a ) &\text{if} \ i=j\\
        0& \text{otherwise.}
    \end{cases}
    \end{multline*}
    In other words, at the state $(s,i)$, follower $i$ will take an action by following his policy $\pi_i ( s  ) $, and the type does not change.
    \item $\mu_0 \in \Delta(S\times \mathcal{T})$ is the initial state distribution. For all $(s, i) \in S\times \mathcal{T}$, $$\mu_0(s,i) =  \mu_i(s)\probs(\mathbf{T}=i).$$
    where $\bm{T}$ is a random variable representing the estimated type of the follower. $\probs(\bm{T})$ is the prior distribution over possible types.
    \item $\cal O$ is a finite set of observations.
    \item ${\cal{E}}: S\times \mathcal{T}\rightarrow \dist{\cal O}$ is the observation function, defined by 
    $
    {\cal E}(o|(s,i)) = E_i(o|s) $ is the probability of observing $o$ when agent $i$ at the state $s$.
 \end{itemize}
\end{definition}

Let $O_t$ denote a random variable representing the observation at time $t$, and let $o_t$ be a specific realization of this random variable. We denote the posterior estimate of the type $\bm{T}$ given an observation $o_{0:T}$ as $\probs_{\bm{\pi}}(\bm{T}|O_{0:T}= o_{0:T})$.

Next, we define the planning objective—Shannon conditional entropy. Information-theoretic metrics are widely used as planning objectives, with entropy measures being particularly common for quantifying information leakage. These entropy measures have been extensively studied in channel design problems, which can be interpreted as one-step decision-making problems \citep{khouzani2017leakage}.

\begin{definition} 
Let $Y\coloneqq O_{0:T}$.
The conditional Shannon entropy of the agent's type given the observations is defined by,
\begin{equation}
\begin{aligned}
H (\bm{T}\mid Y, 
\mathcal{M}(\bm{\pi}) ) &= \sum_{y\in \mathcal{Y}} \probs_{\bm{\pi}}(y)H(\bm{T}|Y=y, 
\mathcal{M}(\bm{\pi})) \\ & = -  \sum_{i\in \mathcal{T} } \sum_{y\in \mathcal{Y}} \probs_{\bm{\pi}}(i,y)\log \probs_{\bm{\pi}}(i|y),
\end{aligned}
\end{equation} where $y$ is a sample observation sequence, and $\mathcal{Y}$ is a set of all finite observation sequences of length $T$.
\end{definition}
% \jf{motivate the use of entropy using minimal information leakge literature.}

Given different policy profiles $\bm{\pi}$ and $\bm{\pi}'$, the entropies $H (\bm{T}\mid O_{0:T}, \bm{\pi} ) $ and   $H (\bm{T}\mid O_{0:T}, \bm{\pi}') $ can be different. 

For the leader's goal of inferring the follower's type, a policy profile  makes the leader's task easier   if it has a lower conditional entropy.
Based on this relation, we formulate the following incentive design problem where
the objective of the leader is to allocate the side payments ${x}$ to improve the accuracy and efficiency of inferring the type of the follower.


\begin{problem}[Incentive design for intention inference] 
   \label{prob:bilevel}
  Assuming the followers always provide the best responses, the leader's incentive design for intention inference problem is the following bi-level optimization problem:
  \begin{equation}
   \label{eq:bi-level}
	\begin{alignedat}{2}
      	& \optmin_{{x} \in\mathcal{X}} &  &  H(\bm{T}| O_{0:T}, \mathcal{M}(\bm{\pi}^\star({x}))) + h({x}) \\\
		& \optst_{\phantom{x\in \reals_{+}^{\abs{S\times A}},\ \pi^\star}} &\quad  & \pi_i^{\star} (x)\in \argmax_{\pi\in \Pi}    V_i( \mu_{ i}, R_i(x) ,\pi ), \forall i \in \mathcal{T}.
 	\end{alignedat}
  \end{equation}
where $\pi^\star(x ) = [\pi_i^\star(x)]_{i \in \mathcal{T}}$ is a policy profile consisting of the best responses of followers, and $h \colon \cal X \to \reals_{+}$ is a side payment cost function, which is differentiable.

\end{problem}
% One particular choice of  $h$ is given by  $h(x) = c \norm{x}_1$ for some constant $c >0$.  \xw{this is not lipschitz continous unless x>0} 

% \jf{Change the following to a remark. it is a bit difficult to write this remark because the initial state opacity is unknown to most people. we can go ahead with the solution without referring to the initial state opacity.}
% If we augment the type $T$ as the initial state to the hidden markov chain  shown in the , we can also view the problem as a initial state opacity problem \cite{shi2024information}.


\subsection{Reduction to single-level optimization}
 We show how to leverage entropy-regularized MDP ~\citep{nachum2017bridging} to reduce the bi-level optimization problem equation~\eqref{eq:bi-level} to a single-level optimization problem. The following reviews the entropy-regularized MDP solution for a single agent, which is the same for all followers. We omit the index of a follower for clarity.

% Given initial state $s$, side payment $x$ and policy $\pi$, the value function of the entropy-regularized \ac{mdp} is defined by %\sh{Let's determine what symbols to use for indices. How about using $t$ for time in the MDP and $k$ for iteration in gradient-based optimization?}

      
\paragraph{Entropy-regularized optimal value/policy.}
The optimal value function $V^\star$ of the entropy-regularized \ac{mdp} with respect to the reward function $R$ satisfies the following entropy-regularized Bellman equation~\citep{nachum2017bridging}:  
\begin{multline}
\label{eq:soft-max}
    V ^\star(s, R ) =
    \tau \log \sum_{a \in A}\exp \{(R (s,a ) \\ +  
     \gamma  \Expect_{s'\sim P(\cdot\mid s,a)} V ^\star(s', R  ))/\tau\}, \ \forall s\in S.
\end{multline}Note that, as   $\tau$ approaches $0$, equation~\eqref{eq:soft-max} recovers the standard  optimal Bellman equation.
Let $\Qstar(R) \colon S\times A\rightarrow \reals$ 
    be the optimal state-action value function (also called Q-function) of the entropy-regularized \ac{mdp} under reward $R$: 
\[
\Qstar(s,a, R )= R (s,a)+\Expect_{s'\sim P(\cdot |s,a)} V_2^\star(s', R ).
\]
For a fixed temperature parameter $\tau$, the optimal policy of the entropy-regularized \ac{mdp}  is uniquely defined by
    \begin{equation}
        \label{eq:policy-softmax}
    \pi^\star(s,a) = \frac{\exp(\Qstar(s,a, R)/\tau) }{\sum_{a'\in A} \exp(\Qstar(s,a')/\tau)}.
    \end{equation}

Then, the optimal policy of the entropy-regularized \ac{mdp} can be written succinctly as $\pi_{\Qstar(R )}$, where $\Qstar(R )$ is viewed as a vector in $\reals^{|S \times A|}$. 

 In the following, we use $V^\star (R_i)$,  $Q^\star (R_i)$,  $\pi_i^\star$, $\bm{\pi}^\star$ (resp. $V^\star (R_i(x))$,  $Q^\star (R_i(x))$,  $\pi_i^\star(x)$, $\bm{\pi}^\star(x)$)  for optimal entropy-regularized value function, Q-value function, policy, and policy profile with respect to follower $i$'s reward $R_i$ (resp. modified reward $R_i(x)$ with side payment $x$). 




Due to the relation between the  optimal policy and the optimal state-action value function of the entropy-regularized \ac{mdp} given by equation~\eqref{eq:policy-softmax}, the lower-level problem in the bilevel optimization problem in~equation~\eqref{eq:bi-level} has a unique solution $\pi_{\Qstar(R_i(x))}$.
   We can reduce the problem \ref{prob:bilevel} to the following single-level optimization problem.

 % \begin{problem}
 % We assume that the followers takes the entropy regularized optimal policy, the leader’s incentive design for intention inference problem is reduced to the following bi-level optimization problem
     \begin{equation}
     \label{eq:single-level}
         \optmin_{{x} \in\mathcal{X}}   H(\bm{T}| O_{0:T}, \mathcal{M}(\bm{\pi}^\star(x) ) + h({x}).
     \end{equation}
     where 
$\bm{\pi}^\star(x) =[\pi_{\Qstar(R_i(x))}]_{i \in \mathcal{T}}$ is the policy profile and each $\pi_{\Qstar(R_i(x))}$ is the entropy-regularized optimal policy:
\[\pi_{\Qstar(R_i(x))} =  \arg\max_{\pi_i }V(\mu_i, R_i(x), \pi_i), \forall i \in \mathcal{T}.
\]
% \end{problem}

\subsection{Incentive design with gradient descent}

 For convenience, define the \emph{softmax policy} $\pi_\theta$ as 
\begin{equation}
\label{eq:policy-softmax-generic}
\pi_\theta(s,a) = \frac{\exp(\theta_{s,a}/\tau)}{\sum_{a' \in A} \exp(\theta_{s,a'}/\tau)}, \quad \theta \in \reals^{|S \times A|}.
\end{equation}

% \paragraph{Clarify the relation between $\bm{\pi}$ and $\bm{\theta}$, use $\mathcal{M}(\bm{\theta})$ in place of $\mathcal{M}(\bm{\pi}_{\bm{\theta}})$. explain $P_{\bm{\theta}}$ in $\mathcal{M}(\bm{\pi}_{\bm{\theta}})$}
\paragraph{Notation.} Given this parametrization, we will replace the notation of policy $\pi$ (or policy profile $\bm{\pi}$) with policy parameter $\theta$ (or policy parameter profile $\bm{\theta}\triangleq [\theta_i]_{i \in \mathcal{T}}$). Thus, $\mathcal{M}(\bm{\pi}_{\bm{\theta}}) \triangleq \mathcal{M}(\bm{\theta})$ and  $\probs_{\bm{\pi}_{\bm{\theta}}}\triangleq \probs_{\bm{\theta}}$. Similar to the notation of $\bm{\theta}$, We denote $\bm{\Qstar}(\bm{R}(x) )  \triangleq [\Qstar(R_i(x))]_{i\in \mathcal{T}}$, and $\bm{R}(x)  \triangleq [R_i(x)]_{i\in \mathcal{T}}$.


Because the entropy-regularized optimal policy $\pi^\star_i(x)$ is an implicit function of $x$, we consider using the gradient-descent method to find a stationary point    for the objective function in equation~\eqref{eq:single-level}. 

Let's define $J_1(\bm{\theta}) = H(\bm{T}|Y;\mathcal{M}(\bm{\theta}))$ (recall $Y=O_{0:T}$), note that $\bm{\theta}$ should be $\bm{\Qstar}(\bm{R}(x) ) $ in  the equation~\eqref{eq:single-level}. Let $$J(x) \coloneqq J_1(\bm{\Qstar}(\bm{R}(x) ) ) + h(x).$$ 
Following the chain rule, the derivative of $J$ with respect to $x$ is given by
     \begin{equation}
     \label{eq:total-derivative}
D J(x) = DJ_1(\bm{\Qstar}(\bm{R}(x) )) \cdot D\bm{\Qstar}(\bm{R}(x) )  \cdot D \bm{R}(x)  + D h(x).
     \end{equation}
    Because $h$ is given, $Dh(x)$ can be computed analytically. Similarly, $D\bm{R}(x)$ can be computed analytically given the function $R_i(x)$, for each $i \in \mathcal{T}$. 
   % Let $M \triangleq |\mathcal{T}||S||A|$ denote the dimension of $\bm{\theta}$, and $N \triangleq |S||A|$ denote the dimension of $x$. According to equation~\eqref{eq:sidepayment-reward-i}, $D\bm{R}(x)$ is an $M \times N$ matrix of ones. 
    
    In the following, we show how to compute $DJ_1(\bm{\Qstar}(\bm{R}(x)))$ and $D {\bm{\Qstar}(\bm{R}(x))  }$ respectively.
    
% \paragraph{Computing $DJ_1(\bm{\Qstar}(\bm{R}(x))  )$:}
\paragraph{Computing $DJ_1(\bm{\Qstar}(\bm{R}(x)))$.} Let $\bm{\theta} = \bm{\Qstar}(\bm{R}(x))$. Computing  $DJ_1(\bm{\theta})$ involves computing  the gradient of the conditional entropy $H(\bm{T}|Y;\mathcal{M}(\bm{\theta}))$ w.r.t. the  parameter $\bm{\theta}$ of policy profile, i.e. $DJ_1(\bm{\theta})=\nabla_{\bm{\theta}}H(\bm{T}|Y;\bm{\theta})^\intercal$.
    
    
    By using a trick that $\nabla_{\bm{\theta}} \probs_{\bm{\theta}}(y) = \probs_{\bm{\theta}}(y) \nabla_{\bm{\theta}} \log \probs_{\bm{\theta}}(y)$ and the property of conditional probability, we have
\begin{equation}
\begin{aligned}
\label{eq:HMM_gradient_entropy}
 &\nabla_{\bm{\theta}} H(\bm{T}|Y;\bm{\theta}) \\
= & - \sum_{y \in \mathcal{O}^T} \sum_{i \in \mathcal{T}} \Big[\nabla_{\bm{\theta}} \probs_{\bm{\theta}}(i, y) \log_2 \probs_{\bm{\theta}}(i | y) \\
&+  \probs_{\bm{\theta}}(i, y) \nabla_{\bm{\theta}}  \log_2 \probs_{\bm{\theta}}(i | y)\Big] \\
= & - \sum_{y \in \mathcal{O}^T} \sum_{i \in \mathcal{T}} \Big[\probs_{\bm{\theta}}(y) \nabla_{\bm{\theta}} \probs_{\bm{\theta}}(i| y) \log_2 \probs_{\bm{\theta}}(i | y) + \\ 
& \probs_{\bm{\theta}}(i|y) \nabla_{\bm{\theta}} \probs_{\bm{\theta}}(y) \log_2 \probs_{\bm{\theta}}(i | y) 
+   \probs_{\bm{\theta}}(y)\frac{\nabla_{\bm{\theta}} \probs_{\bm{\theta}}(i | y)}{\ln 2}\Big].
\end{aligned}
\end{equation}

% \begin{aligned}
%     \sum_{i \in \mathcal{T}} \Big[ \log_2 P_{\bm{\theta}}(i | y) \nabla_{\bm{\theta}} P_{\bm{\theta}}(\bm{T}| y)  + P_{\bm{\theta}}(\bm{T}| y) \log_2 P_{\bm{\theta}}(i | y) \nabla_{\bm{\theta}} \log P_{\bm{\theta}}(y) + \frac{\nabla_{\bm{\theta}} P_{\bm{\theta}}(i | y)}{\log 2} \Big] \\ =
% \sum_{i \in \mathcal{T}}  \log_2 P_{\bm{\theta}}(i | y) \nabla_{\bm{\theta}} P_{\bm{\theta}}(\bm{T}| y) + \sum_{i \in \mathcal{T}} P_{\bm{\theta}}(\bm{T}| y) \log_2 P_{\bm{\theta}}(i | y) \nabla_{\bm{\theta}} \log P_{\bm{\theta}}(y) + \sum_{i \in \mathcal{T}} \frac{\nabla_{\bm{\theta}} P_{\bm{\theta}}(i | y)}{\log 2} \\ =
% \sum_{i \in \mathcal{T}}  \log_2 P_{\bm{\theta}}(i | y) \nabla_{\bm{\theta}} P_{\bm{\theta}}(\bm{T}| y) + \sum_{i \in \mathcal{T}} P_{\bm{\theta}}(\bm{T}| y) \log_2 P_{\bm{\theta}}(i | y) \nabla_{\bm{\theta}} \log P_{\bm{\theta}}(y) + \bm{0} \\ =
% [\log_2 P_{\theta_1}(\bm{T}=1 | y) \nabla_{\theta_1} P_{\theta_1}(\bm{T}=1| y), \cdots,\log_2 P_{\theta_n}(\bm{T}=n | y) \nabla_{\theta_n} P_{\theta_n}(\bm{T}=n| y)] +
% \end{aligned}


% In the case of full state-action observation, the set of observations $O = S\times A$ and the emission function is defined by 
% \[
% E((s,a)|(s,i),a)=1, \forall (s,i,a)\in S\times \mathcal{T}\times A.
% \]
Given a prior distribution $\probs(\bm{T})$, we have $$\probs_{\bm{\theta}}(y) = \sum_{i \in \mathcal{T}}\probs(\bm{T}=i)\probs_{\bm{\theta}}(y|\bm{T}=i),$$
where $\probs_{\bm{\theta}}(y|\bm{T}=i) = \sum_{s\in S}\probs_{\bm{\theta}}(y|s,i)\mu_0(s,i) $ is the probability of generating the observation  sequence $y$ given the follower $i$ following a policy parameterized by ${\theta}_i$---the $i$-th component of policy profile parameters $\bm{\theta}$.

We can calculate the gradient $\nabla_{\bm{\theta}} \probs_{\bm{\theta}}(\bm{T}=i|y)$ 
as
\begin{equation}
\begin{aligned}
\label{eq:HMM_gradient_P_zT_y}
&\nabla_{\bm{\theta}} \probs_{\bm{\theta}}(\bm{T} = i |y)\\
= &  P(\bm{T}=i) \nabla_{\bm{\theta}} \frac{ \probs_{\bm{\theta}}(y|\bm{T}=i) }{\probs_{\bm{\theta}}(y)}\\
=  & P(\bm{T}=i)\Big[ \frac{\nabla_{\bm{\theta}}  \probs_{\bm{\theta}}(y|\bm{T}=i)}{\probs_{\bm{\theta}}(y)}  - \frac{ \probs_{\bm{\theta}}(y|\bm{T}=i)}{ \probs_{\bm{\theta}}^2(y)} \nabla_{\bm{\theta}} \probs_{\bm{\theta}}(y) \Big],
\end{aligned}
\end{equation} 
where $\nabla_{\bm{\theta}} \probs_{\bm{\theta}}(y) = \sum_{i \in \mathcal{T}}P(\bm{T}=i) \nabla_{\bm{\theta}}\probs_{\bm{\theta}}(y|\bm{T}=i)$.

To complete the gradient computation, we only need to determine $\nabla_{\bm{\theta}}\probs_{\bm{\theta}}(y|\bm{T}=i)$ for each type of follower. We can utilize the observable operators~\citep{jaeger2000observableoperator} to compute this gradient for the partially observable system induced by follower $i$'s policy.

\paragraph{Observable operators for single-agent \ac{hmm}.} 
Consider a single-agent MDP $M= \langle S, A, P, \mu_0, R \rangle $, a parameterized Markov policy $\pi_\theta$ induces a hidden Markov model
\[
M(\theta) = \langle  S, A, P_\theta, \mu_0, \mathcal{O}, E\rangle
\]
where $S =\{1,\ldots, N\}$, $\mathcal{O}= \{1,\ldots, M\}$,  $P_\theta (s'|s)=\sum_{a\in \mathcal{A}} P(s'|s,a)\pi_\theta(a|s)
$ and   $E: S\rightarrow \dist{\mathcal{O}}$ is an observation function for the leader.    

% We propose a novel approach to compute % $\nabla_{\theta_i} P_{\theta_i}(y|\bm{T}=i)$  based on observable operators~\citep{jaeger2000observableoperator}. 

% We denote the cardinality of the state space $S$ as $n \triangleq |S|$, and the cardinality of the observation space $\mathcal{O}$ as $m \triangleq |\mathcal{O}|$. 

In this single-agent HMM, let the random variable of state, observation, and control action, at time point $t$ be denoted as $X_t, O_t, A_t$, respectively. Let $\matT^\theta \in \reals^{N \times N}$ be the transposed state transition matrix in the single-agent HMM $M_\theta$ with $$\matT^\theta_{i,j} = P_\theta(X_{t+1} = i|X_t = j).$$ 

Let $\matO \in \reals^{M \times N}$ be the observation probability matrix with $\matO_{o,j} =E( o|j)$ for each $o\in \mathcal{O}$ and $j\in S$.  
\begin{definition} 
Given the single-agent \ac{hmm} $M_\theta$, for any observation $o$,
the observable operator $\matA^{{\theta}}_{o}$ is a matrix of size $N \times N$ with its $ij$-th entry defined as $$
 \matA_{o}^{{\theta}}[i,j] =  \matT_{i, j}^\theta \matO_{o,j} \ ,$$
which is the probability of transitioning from state $j$ to state $i$ and at the state $j$, an observation $o$ is emitted.
In matrix form, 
\[
\matA_{o}^\theta = \matT^\theta \text{diag}(\matO_{o, 1}, \dots, \matO_{o, N}).
\]
\end{definition}


\begin{proposition}
\label{prop:initial_opacity_probability} 

Given the single-agent \ac{hmm} $M_\theta$, the probability of observing $y$ is \begin{equation}
\label{eq:matrix_operation_s0}
P_{\theta}(y) = \mathbf{1}_{n}^\top \matA_{o_t}^\theta \dots \matA_{o_0}^\theta \mu_{0}.
\end{equation}
where $\mathbf{1}_{N}$ is a vector of size $N$. An the derivative of $P_{\theta}(y)$ with respect to $\theta$ is

\begin{equation}
\nabla_{\theta } P_{\theta }(y ) = \sum_{i = 0}^t \mathbf{1}_N^\top \matA_{o_t}^\theta \dots \nabla_{\bm{\theta}} \matA_{o_i}^\theta \dots \matA_{o_0}^\theta \mu_{0 }.
\end{equation}
\end{proposition}
\begin{proof}
The proof is based on the property of observable operators \citep{jaeger2000observableoperator} and the derivative of a parameterized tensor product.
\end{proof}

Based on the above analysis for a general single-agent HMM $M(\theta)$ and observation function $E$, we can obtain $\nabla_{\theta_i}  \probs_{\bm{\theta}}(y|\bm{T}=i)$ and $\probs_{\bm{\theta}}(y|\bm{T}=i)$ by replacing the single-agent HMM with the follower $i$'s HMM $M_i(\theta_i)$ and the observation with $E_i$, and then compute $\nabla_{\theta_i} P_{\theta_i}(y)$ and $P_{\theta_i}(y)$.


It is observed that   $\nabla_{\theta_j}  \probs_{\bm{\theta}}(y|\bm{T}=i)=\bm{0}$ because the observation process of follower $i$ is not influenced by the follower $j$'s policy, when $i\ne j$. With these computation, we obtain $\nabla_{\bm{\theta}} \probs_{\bm{\theta}} (\bm{T} = i |y)$.


It is noted that, though $\mathcal{O}$ is a finite set of observations, it is combinatorial and may be too large to enumerate. To mitigate this issue, we can employ sample approximations to estimate   $\nabla_{\bm{\theta}} H(\bm{T}|Y;\bm{\theta})$: 
Given $K$ sequences of observations $\{y_1, \dots, y_K\}$, we can approximate $ H(\bm{T}|Y;\bm{\theta}) $ by
\begin{equation}
\label{eq:HMM_approx_entropy}
 H(\bm{T}|Y;\bm{\theta}) \approx - \frac{1}{K} \sum_{k=1}^K \sum_{i \in \mathcal{T}} \probs_{\bm{\theta}}(i| y_k) \log \probs_{\bm{\theta}}(i | y_k),
\end{equation}
and approximate $\nabla_{\bm{\theta}} H(\bm{T}|Y;\bm{\theta})$ by
\begin{equation}
\begin{aligned}
\label{eq:HMM_approx_gradient_entropy}
&\nabla_{\bm{\theta}} H(\bm{T}|Y;\bm{\theta}) \\
&\approx - \frac{1}{K} \sum_{k=1}^K \sum_{i \in \{0,1\}} \big[ \log \probs_{\bm{\theta}}(i | y_k) \nabla_{\bm{\theta}} \probs_{\bm{\theta}}(i| y_k) \\
& + \probs_{\bm{\theta}}(i| y_k) \log \probs_{\bm{\theta}}(i | y_k) \nabla_{\bm{\theta}} \log \probs_{\bm{\theta}}(y_k) + \frac{\nabla_{\bm{\theta}} \probs_{\bm{\theta}}(i | y_k)}{\log 2} \big].
\end{aligned}
\end{equation}
 

\paragraph{Computing $D\bm{\Qstar}(\bm{R}(x) )$.}
The derivative $D \bm{\Qstar}(\bm{R}(x) )$    is a block diagonal matrix. Each block along the main diagonal corresponds to $D \Qstar(R_i(x))$ for each follower $i\in \mathcal{T}$. 

The following proposition (proven in \cite{ma2024adaptive}) allows us to compute $D \Qstar(R_i(x))$ given each follower's MDP $M_i$ with reward $R_i(x)$.

    \begin{proposition}
    \label{thm:dtheta_dr}
    Consider an infinite-horizon \ac{mdp} $M=(S,A, P, s_0, \gamma, R)$ with discounting. Let $Q^\star(R) \colon S\times A\rightarrow \reals $ be the optimal state-action value function of the entropy-regularized  \ac{mdp} under the reward function $R$. For any $(s,a), (\tilde s, \tilde a)\in S\times A$, it holds that
    \begin{equation}
    \label{eq:dtheta_dr}
    \frac{\partial Q^\star_{s,a}}{\partial R_{\tilde{s},\tilde{a}}} = \mathbf{1}_{(\tilde{s},\tilde{a})}(s,a)+\gamma \mathbb{E}_{s'\sim P(\cdot\mid s,a)} \sum_{a'\in A} \pi_{Q^\star(R)}(s',a') \frac{\partial Q^\star_{s',a'} } {\partial R_{\tilde{s},\tilde{a}}},
    \end{equation}
    where
    \begin{equation}
    \mathbf{1}_{(\tilde{s},\tilde{a})}(s,a) =
        \begin{cases}
            1 & \text{if } (s,a) = (\tilde{s},\tilde{a}), \\
            0 & \text{otherwise.}
        \end{cases}
    \end{equation}
    \end{proposition} 

For any given $(s',a') \in S \times A$, equation~\eqref{eq:dtheta_dr} is in the form of the standard Bellman equation for state-action value function. This implies that the partial derivative can be computed using any method for solving the Bellman equation~\citep{ma2024adaptive}.

 Lastly, using the gradient computation methods described above, we can compute the total gradient $DJ(x)$ and employ a gradient-descent algorithm to find a stationary point of  the objective function. 
% \cs{Don't forget to change the $\theta_T$ and add the sample approximation formulas. I define the length of trajectories to be $M$ and time horizon to be $K$ for now.}

\begin{remark}
Though the method places no restriction on the side payment $x$---the leader can modify the follower's reward at any state-action pair. In practice, the side payment decision variables $x$ may be constrained to be within a set $\mathcal{X}$ of feasible allocation. In addition, let $\lambda$ be the number of decision variables in $X$, 
the time complexity of calculating $D \Qstar(R (x)) \cdot DR (x) $ in the single-agent case is   $\mathcal{O}(\lambda |S|^2  |A|^2   + |S|^3|A|^2)$.  Thus, the leader can select a subset of state-action pairs for side payment allocation to reduce the number $\lambda$ and subsequently the computations.
\end{remark}

\section{Experiments}

\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{images/gw1.png}
\caption{Fire rescue task in grid world environment.}
\label{fig:environment_1}
\end{figure}



\begin{example}[Fire rescue task]
\label{ex:1}
We demonstrate the effectiveness of the proposed policy-based algorithm in a fire rescue task (Figure~\ref{fig:environment_1}) \footnote{The code is available on \url{https://drive.google.com/drive/folders/1a33YT3gDoJxXNxU36dHfi3ae40E_BIc_?usp=sharing}}. The scenario involves two types of rescue robots (followers) that start from the same initial position, \((0,0)\), and navigate toward specific goal locations (the targets to be rescued) marked by flags. The robots can move in four compass directions: north, south, east, and west. However, their movement is stochastic. When a robot moves in a chosen direction, there is a probability \(\alpha\) that it will also shift  in one of the two adjacent directions. For example, if a robot moves east, there is a probability \(\alpha\) that it will also move  north and another probability \(\alpha\) that it will move  south. If a robot encounters an obstacle or boundary, it remains in place. 

Type 1 robots are equipped with superior hardware compared to Type 2 robots, resulting in lower movement uncertainty ($\alpha = 0.05$ for Type 1 vs. $ \alpha = 0.1$ for Type 2). Additionally, Type 1 robots have better fireproof armor. Thus, if a Type 1 robot enters a fire cell, only  a small negative reward of $-0.1$  is incurred. In contrast, Type 2 robots, which are more vulnerable to fire, suffer a much larger negative reward of $-20$. Both robot types receive a positive reward of $0.1$ upon reaching goal (flag) states, which are designated as sink states. Since the goal states are sink states, robots continue to accumulate discounted rewards after reaching them. The discount factor is set to $\gamma_i = 0.9$ for all i when computing the optimal policy for each robot type. 
% Type 1 robots are equipped with superior hardware compared to Type 2 robots. As a result, the movement uncertainty for Type 1 robots is lower, with \(\alpha = 0.05\), whereas for Type 2 robots, \(\alpha = 0.1\). Additionally, Type 1 robots have better fireproof armor. When a Type 1 robot enters a fire cell, it incurs a small negative reward of \(-0.1\), whereas a Type 2 robot, which is more vulnerable to fire, receives a significantly larger negative reward of \(-20\). Both robots receive a positive reward of \(0.1\) at the goal (flags) states.   All goal states (flags) are set to be sink states. Since the goal states are all sink states, the robots will continuously receive discounted rewards after reaching the goals. We set the discount factor $\gamma_i = 0.9$ for all $i$ when computing the optimal policy for each type of robot.

\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{images/gw2.png}
\caption{Behavior comparison task in grid world environment.}
\label{fig:environment_2}
\end{figure}

For perception, the environment is equipped with four sensors \(\{1, 2, 3, 4\}\), each with a distinct range. If a robot is within   sensor $i$'s range, the observer receives observation $i$, for $i \in \{ 1, 2, 3, 4\}$ with a $90$\% probability and a null observation ("n") with a $10$\% probability due to false negatives. If the robot is outside the sensor's range, the observer only receives a null observation.
\end{example}

In this environment, the leader can only allocate side payments in the cell \((5,0)\). When setting the side payment, we can assign a positive value to only one action at the target state, i.e., \( x((5,0), a) > 0 \) for a single action \( a \in A \), while setting \( x((5,0), a) = 0 \) for all other actions. Since the targets are sink states, the agent will always choose the action with the positive reward when computing the optimal policy. This approach will reduce the effective dimension of side payment. The cost function is defined as \( h(x) = \beta \|x\|_1 \) where \( \beta = 0.05 \), represents the cost of side payments in this specific cell. In optimization, we set the time horizon to \( T = 12 \) and use \( K = 2000 \) sampled trajectories for approximation of gradients. 

\begin{figure*}[ht!]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/fire_results.png}
  \caption{The result of the fire rescue.}
  \label{fig:result_1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/capbility_results.png}
  \caption{The result of the behavior comparison.}
  \label{fig:result_2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/posterior.png}
  \caption{Inference during the optimization.}
  \label{fig:posterior}
\end{subfigure}
\caption{The results of experiments.}
\label{fig:experiment}
\end{figure*}



Figure~\ref{fig:result_1} illustrates that as the algorithm converges. {The objective function $J(x)$ converges to $0.299$. The conditional entropy \( H(\bm{T} | Y, \mathcal{M}(\bm{\pi}^\star(x_t)) \) approaches $0.153$. And the side payment reaches $0.291$. The trend of objective function and entropy are similar because we set the weight $\beta = 0.05$. Then the cost function $h(x)$ has a smaller impact on the objective function.} Since the conditional entropy is close to $0$—the minimal value of entropy—the observations provide significant information about the robot's type.  

The conditional entropy is inversely proportional to the side payment, a behavior driven by the environmental configuration. When the initial side payment is $1$, both types of robots move toward the flag at cell \((5,5)\) due to the presence of fires around cell \((5,0)\). Hence, they are indistinguishable and the entropy is high as $0.793$. However, as the side payment increases, the Type 1 robot, which has a low fire penalty, chooses to move to \((5,0)\), while the Type 2 robot, facing a much higher fire penalty, still moves to \((5,5)\). This divergence in movement makes it easier to distinguish between the two robot types, thereby reducing entropy. Clearly, increasing the side payment to be higher than $0.291$ will improve the inference accuracy, but at too much cost for the leader. Our solution achieves a balance between the inference accuracy and side payment cost.

\begin{example}[Behavior comparison task]



We use the example shown in Figure~\ref{fig:environment_2} to further illustrate the application of our algorithm in active inference. This environment is similar to Example~\ref{ex:1}, but with additional obstacles.

There are also two types of robots (followers) with significantly different behaviors. The dynamic noise is set to \(\alpha = 0.05\) for Type 1 and \(\alpha = 0.25\) for Type 2, meaning the Type 2 robot moves more randomly than the Type 1 robot. Both robots receive a positive reward of \(0.1\) upon reaching their target (flags). Additionally, they incur a continuous negative reward of \(-0.01\) in other cells, encouraging them to move toward their targets more quickly.
The targets are sink states as well, which means that the robots can continuously collect rewards here.
\end{example}

In this environment, the leader can only allocate side payment in the cell \((5,5)\). All other settings remain the same as in Example~\ref{ex:1}.

% \begin{figure}[t!]
% \centering
% \includegraphics[width=0.35\textwidth]{images/fire_results.png}
% \caption{The result of the fire rescue task.}
% \label{fig:result_1}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.35\textwidth]{images/posterior.png}
% \caption{The results of inference during the optimization.}
% \label{fig:posterior}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.35\textwidth]{images/capbility_results.png}
% \caption{The result of the behavior comparison task.}
% \label{fig:result_2}
% \end{figure}




% \xw{I wonder if we should look at the figures about objective function, which is entropy + cost}
% \jf{I agree, at least include this for one of the problem. ideally with different initial condition.}
% \cs{I don't have time to try multiple initial conditions. But I'll add one graph for objective function.}
% \jf{I also don't get this result. the target has a reward of 0.1, when given initial reward 1, it is much higher than the target. why the robot does not reach it?}

% Figure~\ref{fig:result_2} shows that the conditional entropy eventually converges to $0.390$, while the initial conditional entropy is $0.945$. The side payment converges to $5$. As the side payment increases, the robots' behavior changes from indistinguishable to distinguishable given partial observations. It is observed that under an initial side payment $x((5,5), a)=1$ for an action $a \in A$, the cell \((5,5)\) is not attractive to either robot, as both can obtain sufficient rewards from cell \((5,0)\). However, as the side payment increases, the cell \((5,5)\) becomes more appealing.  

% \jf{is $(5,5)$ sink or not? clarity, type 1 can collect rewards at multiple locations.} \cs{Done.}
Figure~\ref{fig:result_2} shows that the conditional entropy eventually converges to $0.390$, while the initial conditional entropy is $0.945$. The side payment converges to $5$. As the side payment increases, the robots' behavior changes from indistinguishable to distinguishable given partial observations. It is observed that under an initial side payment $x((5,5), a)=1$ for a certain action $a \in A$, the cell \((5,5)\) is not attractive to either robot, as both can obtain sufficient rewards from cell \((5,0)\). However, as the side payment increases, the cell \((5,5)\) becomes more appealing.  
In this scenario, the Type 1 robot can ensure reaching \((5,5)\) with a high probability, whereas the Type 2 robot cannot because it is more likely to run into obstacles with its higher stochasticity. As a result, increasing the reward at $(5,5)$ makes it easier for the leader to distinguish between the two types of robots.

The following graph (Figure~\ref{fig:posterior}) shows the results of inference during the optimization process. We define the estimator of posterior distribution \( \hat{\probs}_{\bm \theta}(\bm{T} = i |Y ) \triangleq \Expect_Y[\probs_{\bm{\theta}}(\bm{T} = i |Y)] = \sum_y \probs_{\bm{\theta}}(y) \probs(\bm{T} = i |Y = y) \) given different side payments \( x \), which are sampled during the optimization. Given $y_k,k=1,\ldots, M$ of sampled observation sequences, the estimator can be approximated by

\begin{equation}
\hat{\probs}_{\bm{\theta}}(\bm{T} = i |Y) \approx 
\frac{1}{M} \sum_{k = 1}^M \probs_{\bm{\theta}}(\bm{T} = i |Y = y_k).
\end{equation}
The estimator \( \hat{\probs}_{\bm{\theta}}(\bm{T} = i |Y) \) represents the probability that the robot is of type \( i \). If the posterior is close to $1$, the leader can confidently infer that the true type is \( i \). In this experiment, we fix the true robot type to be Type 2.  
When the side payment is 1, the posteriors \( \hat{\probs}_{\bm{\theta}}(\bm{T} = 1 |Y) = 0.447 \) and \( \hat{\probs}_{\bm{\theta}}(\bm{T} = 2 |Y) = 0.553 \) are close to 0.5, making it difficult to determine which type is the true type. As the side payment increases and converges to the optimal value of $0.5$, the posterior \( \hat{\probs}_{\bm{\theta}}(\bm{T} = 1 |Y)\) decreases to about $0.15$, while \( \hat{\probs}_{\bm{\theta}}(\bm{T} = 2 |Y) \) increases to about $0.85$. At this point, the leader can confidently infer that the true type is Type 2.
% \jf{question about this result, when you calculate $P_\theta(\bm{T} = 1 | Y)$, do you use $P_\theta(\bm{T} = 1 | Y=y)$ for some sampled observation $y$? or $\Expect(P_\theta(\bm{T} = 1 | Y=y) P_\theta(y))$? If it is the later, then should change the notation. } \cs{I changed the definition.}

These experimental results from the stochastic grid world examples validate the accuracy and effectiveness of our proposed methods. These two case studies demonstrated the use of incentive design to distinguish users with different capabilities/dynamics and/or different reward functions.  % During iterations, the side payment converged, and the entropy reached a stationary point, demonstrating the accuracy of our proposed gradient descent algorithm. Moreover, the posterior distribution of the agent's type approached its ground truth from the prior distribution. 


\section{Conclusion}
In this paper, we introduced a class of active inference problems through  incentive design and developed a bi-level optimization method to solve a  locally optimal solution for the leader's incentive policy. Our approach uses  conditional entropy to measure uncertainty about followers' types from the leader's partial observations and consider an incentive design with an objective to balance information gain and incentive cost. 
%By formulating followers' behavior as a \ac{mdp} and employing an entropy-regularized optimal policy, we reduced the bi-level optimization to a single-level optimization problem, and   developed a gradient descent method  with a novel methods to compute the total gradient of the leader's objective function. We show how to leverage observable operators in hidden Markov models for efficient gradient calculation. 

Future work could explore several directions: 1) Integrating active inference with adaptive incentive design for personalized systems; 2) Extending the proposed method from model-based to model-free setting, where the leader only has access to collected data from different followers in the past interactions. This extension will be crucial for many practical applications.
% Future work could explore extending the framework to more complex multi-agent scenarios and investigating alternative entropy-based uncertainty quantification techniques.
\appendix



%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25, chongyang_refs}

\end{document}

