\section{Related Work}
\label{related}
The challenges of achieving robust and efficient model training in decentralized financial environments~\cite{isichenko2021quantitative, karatzas2021portfolio} have driven advancements in financial risk management and sensitivity analysis~\cite{shang2021capital}. FL has emerged as a powerful approach for decentralized model training, allowing multiple clients to collaborate while preserving data privacy. However, these methods often struggle with model divergence in financial applications, where client datasets exhibit significant variability due to diverse market conditions and operational priorities. Statistical heterogeneity remains a key challenge, as differences in local data distributions introduce non-IID effects that lead to training drift and performance degradation~\cite{zhao2018federated}.


To address these limitations, several approaches have been developed. Convergence analyses~\cite{ khaled2020tighter, yang2021achieving} and bounded gradient techniques~\cite{wang2019adaptive} provide theoretical insights into stabilizing training under heterogeneous conditions. FedProx~\cite{li2020federated} mitigates data heterogeneity by incorporating a proximal term in local objectives, while SCAFFOLD~\cite{karimireddy2020scaffold} employs variance reduction techniques to correct client drift. Communication efficiency has also been extensively studied, with dynamic client sampling strategies proposed to reduce overhead~\cite{gorbunov2021marina, yang2021achieving}. However, existing methods often overlook the impact of random client availability, which introduces variance, destabilizes convergence, and limits performance in real-world FL scenarios.

Sensitivity analysis has long been a fundamental tool for evaluating model robustness by quantifying how small perturbations in parameters influence outputs~\cite{borgonovo2016sensitivity, fissler2023sensitivity}. While gradients are widely used in deep learning for optimization, their role in robustness assessment and federated interpretability has been less explored~\cite{pesenti2024differential}. Recent advancements in differential sensitivity measures extend traditional gradient-based methods by capturing the evolution of training dynamics, providing deeper insights into stability, generalization, and risk exposure in model behavior~\cite{pesenti2021cascade}. These measures have shown promise in mitigating performance degradation under distribution shifts by enabling adaptive adjustments during optimization. However, existing FL frameworks rarely incorporate sensitivity-aware strategies, limiting their ability to handle heterogeneous client distributions effectively. Our work builds on these developments by integrating differential sensitivity estimation into federated learning, addressing both robustness and scalability challenges in decentralized financial systems.


Recent research in FL has focused on addressing challenges such as model heterogeneity, representation degeneration, and personalization. For instance, FedPAC~\cite{xu2023personalized} enhances feature alignment through a shared representation and personalized classifier heads but faces limitations due to its computational overhead and reliance on stable client participation. FedDBE~\cite{zhang2024eliminating} tackles domain discrepancies by employing a Domain Bias Eliminator to improve generalization and personalization, although it struggles with scalability under resource-constrained environments. Similarly, FedGH~\cite{yi2023fedgh} provides a communication-efficient approach to handle model heterogeneity by training a generalized global prediction header; however, its reliance on consistent client availability undermines its robustness in dynamic conditions. While these methods demonstrate effectiveness in specific contexts, they often fall short in addressing the combined demands of robustness, scalability, and data heterogeneity inherent in large-scale decentralized systems, which are critical for financial decision-making. 


Our work addresses these limitations by introducing a novel FL framework that integrates distortion risk measures with differential sensitivity analysis. Unlike traditional FL methods that rely on local updates and suffer from client drift under heterogeneous data distributions, FRAL-CSE introduces a sensitivity-aware optimization strategy. By leveraging aggregated sensitivity measures, our approach enables more precise global model updates, mitigating local inconsistency while preserving decentralized autonomy. This design improves the stability of federated training without compromising adaptability, allowing FRAL-CSE to scale efficiently in dynamic financial environments.