\section{Background} \label{sec:background}

\subsection{Large Language Models}

Language models are trained to operate across different languages. 
Table \ref{model-summary} summarizes the four LLMs we study, which differ in the number of languages they were trained on. 
\aya \ supports the widest range of languages, while \gemma \ covers the fewest.

We evaluate these models across five languages, selected based on their varying levels of representation during training. English, the predominant training language, serves as a baseline. French and German represent high-resource, non-English languages, while Dutch and Chinese are lower-resource languages. Dutch is only a high-resource language in \aya \ and therefore provides an interesting comparison to German due to their linguistic similarity.
This analysis allows us to understand the performance disparities across languages with varying levels of representation in training.

\subsection{Methods}

Our goal is to understand whether LLMs have a universal representation space. 
To address this question, we use three mechanistic interpretability methods. 
The logit lens (Section \ref{sec:logit_lens}) allows us to examine the internal representations, while causal tracing provides insight into where facts are encoded in the model across different languages (Section \ref{sec:patching}). 
Finally, steering vectors let us intervene on the models' internal representations (Section \ref{sec:steering_vec}), which allows us to verify that the representations influence the output.


\begin{table*}[t]
\caption{LLM-Insight dataset examples: sentences and prompts for the word  animal.}
\label{data-example}
\vskip -0.25in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ l p{8cm} p{6cm}  }
        \toprule
        {Language} & {Sentence Example} & {Prompt Example}   \\
        \midrule
        English & The zoo has a wide variety of animal species. & They adopted a    \\ 
        Dutch & De boerderij had elk type huisdier. & In de dierentuin zag ik een bijzonder  \\ 
        French & Le lion est un animal sauvage qui vit dans la savane. & Il a vu un \\ 
        Mandarin & \md{森林中生活着许多野生动物}& \md{每年都会有新的}     \\ 
        German & Der Zoo beherbergt viele faszinierende Tiere. & Sie liebt es Zeit mit ihrem    \\ 
        \bottomrule
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsubsection{Logit Lens} \label{sec:logit_lens}
The logit lens \citep{logitlens} decodes the internal representations of an LLM into tokens. LLMs take an input $x$ and output a probability distribution over the next token.
The logit lens decodes the intermediate representation $h_l(x)$ at layer $l$ into an output token, by applying the unembedding layer:
\begin{equation}
    \text{argmax}_t \  \text{softmax}(W_uh_l(\text{norm}(x)))
\end{equation}
where $x$ is the input, $W_u$ is the unembedding matrix of the model and the subscript $t$ corresponds to the token.
Figure \ref{fig:logit_lens} shows the logit lens applied to Llama when generating: 
\begin{displayquote}
``Le bateau naviguiait en douceur sur l'\textbf{eau au calme du lac. Le soleil ...}''. 
\end{displayquote}
For each layer (y-axis) and token position in the generation (x-axis), a token is decoded from the internal representation.
The decoded tokens from the middle layers onward are more interpretable, whereas early layers are less interpretable.

\subsubsection{Causal tracing} \label{sec:patching}
Causal tracing \citep{meng2022locating, vig2020investigating} uses causal mediation analysis to identify where facts are stored within a network. 
The method compares corrupted hidden states -- where the information necessary to retrieve the fact has been removed -- with clean hidden states --that successfully output the fact. This approach allows us to identify the part of the network that encodes the fact. 
Further details can be found in Appendix \ref{sec:causal_tracing}. 

\subsubsection{Steering Vectors} \label{sec:steering_vec}
Steering vectors \citep{subramani2022extracting, turner2023activation, panickssery2024steeringllama2contrastive} are used to nudge the behavior of the LLM in the desired direction. The main idea is to add activation vectors during the forward of a model to modify its behavior as follows:
\begin{equation}
    h_l(x) \leftarrow h_l(x) + \gamma v_l,
\end{equation}
where $v_l$ is the steering vector, and $\gamma \in \mathbb{R}^{+}$ is a scalar hyperparameter. Steering vectors are used to nudge the output of the LLM in the desired direction. For example, if we want the output to contain more `love', we can compute a steering vector as follows:
\begin{equation}
    v_l = h_l(\text{love}) -  h_l(\text{hate}).
\end{equation}
Further details can be found in \citet{subramani2022extracting} and  \citet{turner2023activation}.
