\section{Limitations}
Our work provides evidence suggesting that MLLMs primarily operate in English.
Below, we outline potential limitations and directions for future research.

\paragraph{Tokenization}
Sentences in different languages often vary in tokenization length \citep{rust-etal-2021-good, muller-etal-2021-unseen,petrov2024language}, which complicates cross-lingual comparisons. 
In this work, we provide heuristics (e.g., for causal tracing, which operates on a per-token level) to compare the results when tokenization lengths vary. 
However, tokenization remains an important consideration for the development of future interpretability methods designed to be used across multiple languages.


\paragraph{Language confidence and confusion}
Models often assign higher probabilities to outputs in certain languages, which can affect analyses such as causal tracing by requiring higher noise levels. Similarly, models often exhibit language confusion \citep{marchisio2024understandingmitigatinglanguageconfusion}, continuing to respond in English even when prompted in other languages. 
Both factors influence our analysis. We can mitigate some issues associated with the first problem -- e.g., in causal tracing, we ensure the probabilities all fall below a specified threshold when a prompt is noised. However, we do not actively address language confusion, as doing so could alter the natural behavior of the LLMs, which we aim to understand.


\paragraph{Factors affecting interpretability methods}
Interpretability methods are influenced by various factors. Steering performance, for example, depends on the intrinsic steerability of a prompt \citep{turner2023activation, tan2024analyzinggeneralizationreliabilitysteering}. To address this, we designed a custom dataset that, to the best of our knowledge, is equally steerable across all languages.
Another challenge is that steering could push activations outside the expected data distribution, leading to unintended outputs. 
To mitigate this, we checked for stuttering in the generated outputs. 
However, further work is needed to deepen our understanding of steering mechanisms and to develop more robust evaluation procedures.

\paragraph{Other Methods}
Exploring alternative methods could provide valuable insights. For example, sparse autoencoders (SAEs) \citep{olshausen1997sparse, hinton2006reducing, templeton2024scaling} are a popular interpretability tool. 
However, training SAEs for each layer is computationally expensive and beyond our computational budget. 
While some pre-trained SAEs are available, they are predominantly trained on English data, which introduces biases we aim to avoid \citep{lieberum2024gemmascopeopensparse}.
