\section{Experiments}

\begin{figure*}[t]
\begin{minipage}{0.49\textwidth}
    \centering
    \subcaption{\aya}
    \includegraphics[width=\textwidth]{figures/pos_results/pos_comparison_aya.png} 
     
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
    \centering
     \subcaption{\llama}
    \includegraphics[width=\textwidth]{figures/pos_results/pos_comparison_llama.png} 
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
    \centering
    \subcaption{\mistral}
    \includegraphics[width=\textwidth]{figures/pos_results/pos_comparison_mistral_large.png}      
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
    \centering
    \subcaption{\gemma}
    \includegraphics[width=\textwidth]{figures/pos_results/pos_comparison_gemma.png} 
    \end{minipage}
    \caption{Logit lens analysis of LLMs routing through English. Each plot shows the proportion of words routed through the English representation space for each model. The shaded bars indicate the portion explained by homographs -- words that are spelled the same in English and the specified language. Overall, the degree of English-routing depends on the model: less diverse pretraining leads to more English-routing. Similarly, most routing occurs for lexical words.}
    \label{fig:pos_plots}
\end{figure*}

We want to understand whether LLMs process prompts differently depending on the output language. 
First, we analyze the latent space to find that LLMs make semantic decisions that are more closely aligned with the English representation space (Section \ref{sec:logit_lens_latent}).
Next, we show that we can steer activations better when using English steering vectors (Section \ref{sec:steering}). Lastly, in Section \ref{sec:latent_space_geometry}, we show that the representations of facts are shared across languages, but have an English-centric bias when decoded. 


\subsection{Inspecting the latent space of LLMs using the Logit Lens} \label{sec:logit_lens_latent}


\paragraph{Qualitative Examples}
To build an intuition on how LLMs operate when prompted in different languages, we analyze their latent space using the logit lens, which decodes the internal representations. 
In Figure \ref{fig:logit_lens}, nouns and pronouns are routed through English, whereas the coordinating conjunction is not. 
Similarly, Figure \ref{fig:logit_lens_nl} shows the logit lens applied to \llama \ for the Dutch prompt \textit{Ze telen hun eigen}. 
The noun `fruit', verb `kweken', and pronoun `they' are all routed through the English words, whereas the coordinating conjunction `en' is not routed through the English word `and'. 
Interestingly, the word growing appears in the latent space several tokens before `kweken' is generated, suggesting that the LLM may plan words in advance in English, which builds on \citet{Pal_2023}'s finding that LLMs encode future tokens in the latent space.

\begin{figure}
    \begin{minipage}{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/logit_lens/fig2.png} 
    \end{minipage}
     \caption{Logit lens applied to the latent space of \llama, prompted in Dutch with \textit{Ze telen hun eigen}. Each row depicts decoded latent representations across layers, and each column corresponds to the token generated at a specific time step. Orange boxes highlight words selected in English, darker red boxes highlight related words, while gray boxes indicate explicit terms omitted from the figure (see Appendix \ref{sec:explicit_text}). The nouns `fruit' and pronoun `they' are selected in English.}
    \label{fig:logit_lens_nl}
\end{figure}

\begin{figure*}[t]
    \begin{minipage}{0.49\textwidth}
    \centering
    \subcaption{\aya}
    \includegraphics[width=\textwidth]{figures/steering/steering_Aya__max.png} 
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
    \centering
    \subcaption{\llama}
    \includegraphics[width=\textwidth]{figures/steering/steering_Llama__max.png} 
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
    \centering
    \subcaption{\mistral }
    \includegraphics[width=\textwidth]{figures/steering/steering_Mistral__max.png} 
    \end{minipage}
        \begin{minipage}{0.49\textwidth}
      \subcaption{\gemma }
    \includegraphics[width=\textwidth]{figures/steering/steering_Gemma__max.png} 
    \end{minipage}
    \caption{Cross-Lingual Steering LLMs: The language on the x-axis is the prompt and the desired output language, while the color of each bar indicates the language used to generate the topic steering vectors.}
    \label{fig:steering}
\end{figure*}

\paragraph{Quantitative Evaluation}
The qualitative examples shown in Figures \ref{fig:logit_lens} and \ref{fig:logit_lens_nl} suggest that the part of speech determines whether LLMs employ English routing.
To investigate this, we prompt each LLM to generate $720$ sentences. For each generated word, we evaluate whether the English equivalent of a word appears in the latent space. 
For example, in Figure \ref{fig:logit_lens_nl}, for the word groenten, we check whether the English equivalent, vegetables, appears in the decoded latent space.
We then aggregate the results across different parts of speech. Further implementation details are provided in Appendix \ref{sec:appendix_logit_lens_eval}.

Figure \ref{fig:pos_plots} shows the results for \aya, \llama, \mistral \ and \gemma. Each bar shows the percentage of words that route through the English representation space. The shaded part shows the proportion explained by cross-lingual homographs, words that are the same in English and the specified language (e.g., water in English and Dutch). For homographs, it is not possible to disambiguate whether the word routes through English. 

In general, lexical words -- nouns and verbs --  are often chosen in English. These parts of speech influence the semantic meaning of the sentence. 
Other parts of speech, such as adpositions, determiners and compositional conjugates are infrequently routed through English in \aya \ and \llama.

\begin{table*}[h]
\caption{English-routing in LLMs: percentage of generated words that are routed through English. \aya \ shows the least routing behavior, whereas \gemma \ shows the most routing behavior. }
\label{english-routing-model}
\vskip -0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l cccc|c}
        \toprule
        Model & Dutch & French & German & Mandarin & Average \\
        \midrule
        \gemma & 0.72 $\pm$ 0.01 & 0.67 $\pm$ 0.01 & 0.72 $\pm$ 0.01 & 0.71 $\pm$ 0.01 & 0.70 $\pm$ 0.00 \\
        \mistral & 0.69 $\pm$ 0.01 & 0.63 $\pm$ 0.01 & 0.71 $\pm$ 0.01 & 0.69 $\pm$ 0.01 & 0.68 $\pm$ 0.01 \\
        \llama & 0.51 $\pm$ 0.01 & 0.57 $\pm$ 0.01 & 0.58 $\pm$ 0.01 & 0.55 $\pm$ 0.01 & 0.55 $\pm$ 0.00 \\
        \aya & 0.58 $\pm$ 0.01 & 0.49 $\pm$ 0.01 & 0.50 $\pm$ 0.01 & 0.41 $\pm$ 0.01 & 0.50 $\pm$ 0.00 \\ 
        \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

The degree of English routing is model-dependent, as shown in Table \ref{english-routing-model}. 
One explanation is the degree of multilingualism in the pre-training data -- with more multilingual models, such as \aya, routing less through English, in contrast to the least multilingual model, \gemma, which routes the most through English.
However, this does not account for the differences observed between \mistral \ and \llama, for which French and German are both high-resource languages.
Another possible explanation is model size. Smaller models, such as \mistral\ and \gemma, route through English more frequently than larger models, potentially due to their more limited representation space.

\subsection{Cross-Lingual Steering} \label{sec:steering}

Our experiments in Section \ref{sec:logit_lens_latent} suggest that LLMs may first select topic words in an English representation space, before translating them into the output language in the later layers. 
To further investigate this hypothesis, we evaluate whether non-English model outputs can be modified using English steering vectors.

More concretely, we test whether we can steer models to generate a sentence in a specified output language using two types of steering vectors:
    \begin{itemize}
        \item \textbf{topic steering vector} -- encourages the LLM to generate a sentence with the given topic, such as animals.
        \item \textbf{language steering vector} -- encourages the model to generate text in the desired output language. 
    \end{itemize}
We evaluate the effectiveness of steering across various topics and prompts, using the LLM-Insight dataset (see Section \ref{sec:datasets}). 
We evaluate steering as successful if the generated sentence includes the target word associated with the steering vector while avoiding output collapse -- incoherent sentences or stuttering. 

Figure \ref{fig:steering} shows results when steering different LLMs.
In general, we observe that English steering vectors perform the best -- outperforming steering vectors generated using the desired output language. 
This suggests that the representation space is not universal -- if it were, we would expect the cross-lingual performance to be roughly equal across languages. Instead, this supports the hypothesis that these models select these words in English.


\paragraph{How similar are the steering vectors generated in different languages?}
The steering vectors for the same concepts generated in different languages have a relatively high cosine-similarity, particularly in the early middle layers (see Appendix \ref{sec:appendix_geo}). However, the steering vectors are not language-agnostic -- part of the dissimilarity of the steering vectors can be attributed to the difference in the language used to generate the vectors. This further supports the argument that the representation space is not universal. 

\subsection{Investigating the Representation Space} \label{sec:latent_space_geometry}

\begin{figure}
    \centering
    \includegraphics[trim={12.7cm 0cm 0cm 0cm},clip,width=0.48\textwidth]
    {figures/tracing/aya_10_5_mean.png}
    \caption{Causal traces for the City Facts dataset in \aya. The AIE scores are similar across different languages, suggesting that facts are localized in the same area of the model.  }
    \label{fig:aya_tracing}
\end{figure}

In this section, we study \textit{how} cross-lingual facts are encoded relative to each other using the city facts dataset (see Section \ref{sec:datasets}). 
First, we perform causal tracing to determine whether facts in different languages are encoded in the same part of the model. Figure \ref{fig:aya_tracing} shows the causal traces for \aya \ (see Appendix \ref{appendix:tracing} for other LLMs). We find that facts are generally localized in similar layers, regardless of the language.

Next, we want to understand if the representation of a fact is shared across different languages. In particular, if we have the same fact in two different languages, such as English and Dutch, can we decompose the representation as follows:
\begin{align} 
    h(\text{capital of Canada}) = h_{Ottawa} \ + h_{English} \\
    h(\text{hoofdstad van Canada}) = h_{Ottawa} \ + h_{Dutch},
\end{align}
where $h$ represents a vector in the latent space. If the above equations hold, we may be able to interpolate between the facts: 
\begin{align*}
    & \alpha h(\text{hoofdstad van Canada})  + (1-\alpha)h(\text{capital of Canada}),  \\ 
    & = h_{Ottawa} \ + \alpha h_{Dutch} + (1-\alpha)h_{English} 
\end{align*}
If we pushforward the interpolated hidden state, and the output is correct, then this suggests that we may be able to disentangle the language and semantic context. 

\begin{figure}[h]
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[trim={1cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/ENG_GER_llama_heatmap_interpolate_results.png} 
    \caption{Relative propensity of \llama \ to answer in German (red) vs English (blue). \llama \ is most likely to answer in English.}
\end{minipage}
\end{figure}

We find that we can interpolate between the hidden states without significant changes in accuracy; the accuracy generally interpolates between the accuracies of the two languages (see Appendix \ref{sec:hidden_state_interpolation}). Furthermore, we find that models have a propensity to answer in English.
This provides further evidence that the models likely operate in an English-centric space. 
