\section{Discussion} \label{sec:discussions}
There are currently two perspectives in interpretability research on concept representations in multilingual models: (1) concept representations are universal; and (2) concepts have language-centric representations, where the language is the training-dominant language. 
Our work aligns more closely with the second perspective, as well as a third perspective -- namely, that LLMs encode language-specific representations, where the language is the input/output language. Below, we discuss how the different theories may be reconciled. 

\citet{wendler2024llamasworkenglishlatent} and \citet{dumas2024llamas} argue that 
the concept space is universal, but likely more aligned with the English output space. 
However, our findings contest this conclusion, as we find that interventions in the latent space are more effective when using English text, even when the target language is not English. 
If the concept space were truly universal, we would expect interventions using all languages to perform equally well. Our findings are consistent with concurrent work by \citet{wu2024semantic}, who similarly find that steering using English performs comparably to, or slightly better than, the target language.

One possible way to reconcile the two theories is via the difference between concepts that are encoded and concepts that are used \citep[as discussed in][]{brinkmann2025large}. 
There may be multiple representations of any given concept \citep{hase, mcgrath2023hydraeffectemergentselfrepair}, or a concept may be represented in an LLM but not used during generation.
\citet{wendler2024llamasworkenglishlatent} focus more on the encoding of concepts, whereas our work focuses more on the generation of text. 

An alternative explanation is that different behavior is captured in the tasks. In \citet{wendler2024llamasworkenglishlatent, dumas2024llamas}, the tasks are designed to generate a single token. In this setting, the task is to select the correct token, and we expect a high probability mass on a single token. In contrast, we focus on a more open-ended setting where there are several different possible continuations. These two settings are inherently different, leading to different conclusions about the behavior of LLMs. Even within the same task of fact retrieval, prior work found that different components of the forward pass are language-specific and language-agnostic \citep{fierro2025multilinguallanguagemodelsremember}. 

More generally, the open-generation setting allows us to analyze different parts of speech. 
This leads to the second main difference in conclusions, which is that LLMs encode language-specific representations. 
For semantically loaded words, we find evidence that the latent space is English-centric (in LLMs where English is the dominant training language). 
This is consistent with one line of prior work, which generally focuses on nouns \citep{wu2024semantic, zhong2024beyond}.
However, we find that the same pattern does not hold for non-lexical words. 

This is in contrast to concurrent work by \citet{brinkmann2025large}, who showed that models share morpho-syntactic concept representations across languages in Llama-3-7b and Aya-8B. In line with their previous work \citep{wendler2024llamasworkenglishlatent}, they argue that the representations are universal. 
While our high-level conclusions differ, our findings also support the hypothesis that smaller models emit more shared representations than larger models, which permit more language-specific representations. 

In summary, our findings indicate that the extent to which representations are shared across languages is more nuanced than previously thought. 
Contrasting our work with previous work suggests that the task and model size likely influence the observed behavior. 
Fully understanding these nuances is important to ensure the fairness and robustness of LLMs.