\section{Appendix}

\subsection{Causal Tracing} \label{sec:causal_tracing}
Causal tracing \citep{meng2022locating, vig2020investigating} uses causal mediation analysis to identify where facts are stored within a network.
For example, imagine that we want to find where the fact ``The capital of Canada is Ottawa" is represented in an LLM. 
We could prompt the model with ``The capital of Canada is" to find where ``Ottawa'' is stored in the network.
There are two main steps in causal tracing:
\begin{enumerate}
    \item \textbf{corrupt the signal}: destroy the information so that the model no longer outputs the fact. 
    \item \textbf{restore the signal}: determine where in the network the representation needs to be restored so that the LLM can recover the correct output.
\end{enumerate}

Let $e^{\text{clean}} \in \mathbb{R}^{m,d}$ be the embedding of the prompt ``The capital of Canada is'', where $m$ is the number of tokens and $d$ is the embedding dimension. 
In the first step, the information is ``destroyed" by adding noise to the embedding of the subject token:
\begin{equation}
  e^{\text{corrupted}}_{j} =
    \begin{cases}
      e^{\text{clean}}_{j} + \varepsilon & \text{if token $j$ is a subject token}\\
      e^{\text{clean}}_{j} & \text{otherwise}
    \end{cases}       
\end{equation}
where $\varepsilon $ is noise sampled from an isotropic Gaussian distribution, and $e^{\text{corrupted}}$ is the corrupted embedding.
We pushforward corrupted embeddings $e^{\text{corrupted}}$ through the network to obtain the probability that the model outputs Ottawa, $p[\text{Ottawa}|e^{\text{corrupted}}]$.

Next, we want to find out which part of the hidden states encodes the relevant information to restore the correct output. 
At a given layer $l$ in the network, we `restore' part of the corrupt hidden state by copying back part of the clean hidden state $h$ at position $p$:
\begin{equation}
  h^{\text{restored}}_{j,l} =
    \begin{cases}
      h^{\text{clean}}_{j,l} & \text{if} \  j = p\\
      h^{\text{corrupted}}_{j,l} & \text{otherwise},
    \end{cases}       
\end{equation}
where the hidden state $h^{\text{clean}}_{l} = [h^{\text{clean}}_{1,l}, \dots, h^{\text{clean}}_{m,l}]$ is obtained by pushing the original embeddings, $e^{\text{clean}}$, through the network. 

Finally, we propagate $h^{\text{restored}}_l$ through the remaining layers produce the output probability $p[\text{Ottawa}|h^{{\text{restored}}}_{l,p}]$.
The difference $p[\text{Ottawa}|h^{{\text{restored}}}_{l,p}] - p[\text{Ottawa}|h_l^{\text{corrupted}}]$, measures the importance of layer $l$ and token position $p$ in encoding a fact. 
Through this approach, causal tracing helps identify which parts of the representation are sufficient to retrieve the correct output. 


\subsection{LLM Training Data Languages}

Table \ref{model} summarizes the languages the different models are trained on. 

\begin{table*}[h]
\caption{LLMs: High resource training languages}
\label{model}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{6cm} p{8cm}} 
\toprule
{\bf Model} & {\bf Languages} \\ \midrule 
Llama-3.1-70B \citep{dubey2024llama3herdmodels} & English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai \\  
Mixtral-8x22B-v0.1 \citep{jiang2024mixtralexperts} & English, French, Italian, German, and Spanish \\  
Aya-23-35B \citep{aryabumi2024aya} & Arabic, Chinese (simplified and traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese \\  
Gemma-2-27b  \citep{gemma_2024} & English \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\subsection{LLM-Insight Dataset} \label{sec:appendix_dataset}

Our goal is to generate a dataset that can be used for cross-lingual interpretability. We wanted a dataset that can be used to introspect LLM internal representations and analyze LLM behavior when the internal representations are intervened on.
Additionally, the dataset focuses on open-ended sentence generation rather than being restricted to specific tasks like fact recall or sentiment analysis, as text generation is an important real-world application. 

\subsubsection{Text generation}
We use GPT-4o to generate sentences and prompts. For each target word, we generate:
\begin{itemize}
    \item 10 unique sentences containing a version of the word -- for example, for the verb `(to) see', a suitable sentence is `She saw a bird in the sky.'
    \item a list containing the version of the word used in each sentence. In the previous example, the version of the word is `saw'. 
    \item 10 unique prompts, designed to be completed with the target word.
    \item a list containing a version of the word used in each sentence
\end{itemize}

We instruct GPT-4o to generate prompts that can be completed with the target word, as well as semantically distinct words. 
However, we observe that the model sometimes produces sentences and prompts that do not meet the criteria.

An example of a sentence that does not meet the criteria is:
\begin{displayquote}
\textbf{Target word}: bouquet (boeket in Dutch) \\
\textbf{Sentence}: Het boeket was gevuld met levendige rozen en lelies. \\
\textbf{Translation}: The bouquet was filled with live roses and lilies.
\end{displayquote}
The issue with this sentence is its unnatural phrasing—the word "live" is not typically used in this context.

An example of a prompt that does not meet the criteria is:
\begin{displayquote}
\textbf{Target word}: money \\
\textbf{Prompt}: He went to the bank to withdraw
\end{displayquote}
In this case, the only plausible continuation is "money." While the prompt is coherent, it lacks the open-endedness needed to analyze how interventions influence model behavior.

An example of a well-constructed prompt is:
\begin{displayquote}
\textbf{Target word}: bus \\
\textbf{Prompt}: She took a 
\end{displayquote}
This can be completed with the intended word "bus", as well as semantically different alternatives such as "walk" or "long road trip".

To ensure data quality, we asked native speakers to review and correct the data. The original version of the data and the corrections are provided in the dataset. 

\FloatBarrier

\subsubsection{Dataset summary}

We selected words that vary in the number of tokens (in non-English languages), whether the word is a homograph with the English version of the word, and the part of speech. Table \ref{word-translations} summarizes the words used. 


\begin{longtable}{%
    >{\raggedright\arraybackslash}p{2.5cm} 
    >{\raggedright\arraybackslash}p{2.5cm} 
    >{\raggedright\arraybackslash}p{2.5cm} 
    >{\raggedright\arraybackslash}p{2.5cm} 
    >{\raggedright\arraybackslash}p{2.5cm} 
    >{\raggedright\arraybackslash}p{2cm}}
\caption{Word Translations Across Languages} \label{word-translations} \\ 
\toprule
\textbf{Word} & \textbf{English} & \textbf{Dutch} & \textbf{French} & \textbf{German} & \textbf{Mandarin} \\ 
\midrule
\endfirsthead

\toprule
\textbf{Word} & \textbf{English} & \textbf{Dutch} & \textbf{French} & \textbf{German} & \textbf{Mandarin} \\ 
\midrule
\endhead

\bottomrule
\endfoot    

animal & animal, animals & dier, dieren & animal, animaux & Haustier, Tier, Tiere & \md{动物} \\
bad & bad & slecht, slechte & mal, mauvais, mauvaise, mauvaises & schlecht, schlechte, schlechten & \md{不好, 坏了} \\
ballet & ballet & ballet & ballet & Ballett & \md{芭蕾} \\
bank & bank & bank & banque & Bank & \md{银行} \\
beautiful & beautiful & mooi, mooie & beau, bel, belle, magnifique & schön, schöne, schönen & \md{美丽} \\
big & big, tall & groot, groots, grote & grand, grande, grands, gros & große, großen, großer, großes & \md{大} \\
bouquet & bouquet & boeket & bouquet & Strauß & \md{花束} \\
brother & brother & broer & frère & Bruder & \md{哥哥, 弟弟, 兄弟} \\
bus & bus & bus & bus & Bus & \md{公交车 / 巴士} \\
cat & cat & kat & chat & Katze, Katzen, Mutterkatze & \md{猫, 小猫, 流浪猫} \\
centre & centre & centrum & centre & Zentrum & \md{中心, 市中心, 研究中心, 社区中心, 艺术中心} \\
chair & chair & stoel & chaise & Stuhl, Stuhls, Stühlen & \md{椅子, 椅子, 摇椅} \\
chauffeur & chauffeur, driver & chauffeur & chauffeur, chauffeuse & Chauffeur & \md{司机} \\
child & child & kind & enfant & Kind & \md{孩子} \\
club & club & club, vereniging & club & Club & \md{俱乐部} \\
cold & cold & ijskoud, kou, koud, koude & froid, froide, froides & kalt & \md{冷, 寒冷} \\
computer & PC, computer, laptop, machine, rig, system & computer & ordinateur & Computer, Computern, Laptop & \md{电脑} \\
culture & culture, cultures & culturen, cultuur & culture, cultures, la culture & Kultur & \md{文化} \\
day & day & dag & jour, journée & Tag & \md{天, 日子} \\
dog & dog & hond & chien & Hund & \md{小狗, 狗} \\
drink & drink & drank, drankje, drinken & boire, boisson, drinken & trinken & \md{饮料} \\
eat & eat & eten & dîner, manger, repas & essen & \md{吃} \\
fast & fast & fast, snel, snelle & rapide, vite & schnell & \md{快} \\
film & film & film, films & film & Film, film & \md{电影, 影片, 纪录片} \\
food & cuisine, cuisines, dish, food, meals & gerecht, gerechten, voedsel & cuisine, nourriture & Essen, Futter, Nahrung & \md{食物} \\
fruit & fruit & fruit & fruit, fruits & Frucht, Früchte, Obst & \md{水果} \\
garage & garage & garage, parkeergarage & garage & Garage & \md{车库} \\
give & give & geven, helpen & dire, donnent, donner, partager & geben & \md{给} \\
goal & goal & doel, doelpunt & but, objectif & Tor, Ziel, Ziele & \md{球门, 目标} \\
gobbledygook & buzzwords, gibberish, jargon, nonsense & gebazel, geheimtaal, jargon, koeterwaals, onzin, retoriek, waanzin, wartaal & baragouin, bla-bla, charabia, galimatias, gargouilloux & Kauderwelsch & \md{废话, 难懂的术语} \\
good & delicious, excellent, fun, good, great, helpful & goed, goede & bien, bon, bonne, bons & gut & \md{好} \\
hand & hand & hand, handen & main & Hand, Hände & \md{手} \\
happy & happy & blij & content, contente, contents, enthousiaste, gais, heureux, joyeuse & froh, glücklich & \md{快乐, 高兴, 高兴, 快乐} \\
horse & horse, pony & paard, paarden & cheval & Pferd, Pferde & \md{马} \\
hot & hot & heet, hete & chaud, chaude, chaudes & heiß & \md{热} \\
incomprehen-sibility & incomprehen-sibility & onbegrijpe-lijkheid & incompréhen-sibilité & Unverständlichkeit & \md{不可理解, 难以理解} \\
information & information & informatie & information, informations & Information, Informationen & \md{信息} \\
land & land & land, landen & atterrir, campagne, nature, terrain, terrains, terre, territoire & Land & \md{土地, 土地, 国家} \\
machine & device, equipment, machine, maker, system & machine & machine & Maschine & \md{机器} \\
menu & menu & menu, menukaart & menu & Menü & \md{菜单} \\
money & money & geld & argent & Geld & \md{钱} \\
no & no & nee & non & nein & \md{不} \\
please & please & alsjeblieft & s'il te plaît, s'il vous plaît & bitte & \md{请} \\
police & police & politie & police & Polizei & \md{警察} \\
radio & radio & radio, radiozender & fréquence, radio & Radio & \md{广播, 收音机} \\
read & read & lezen & lire & lesen & \md{阅读} \\
room & room & kamer & chambre & Zimmer & \md{房间} \\
sea & sea & zee & mer, zoo & Meer & \md{大海, 海, {海洋}, 海浪, 海风} \\
see & see & zien & voir & sehen & \md{欣赏, 看, 观察} \\
serendipity & serendipity & toeval, toevalstreffer & chance, coïncidence, hasard, sérendipité, éventualité & glückliche Zufälle, glücklicher Zufall & \md{机缘巧合} \\
sister & sister & zus, zusje & soeur & Schwester & \md{姐妹} \\
sleep & asleep, sleep & slaap, slapen & coucher, dormir, s'assoupir, se coucher, se reposer, sommeil, somnoler & Schlaf, schlafen & \md{睡眠, 睡觉} \\
slow & slow, slowly & langzaam, langzame & lent, lente, lentement, lentes, lents & langsam & \md{慢} \\
small & compact, little, small, tiny & klein, kleine & petit, petite & klein & \md{小} \\
speak & speak & spreken & communiquer, parler, s'exprimer & sprechen & \md{发言, 表达, 讲话, 说, 说话} \\
supermarket & grocer, grocery, market, store, supermarket & supermarkt & supermarché & Supermarkt & \md{超市} \\
table & table & tafel & table & Tisch & \md{台, 桌, 桌子} \\
take & take & maken, neemt, nemen, zorgen & prendre & mitnehmen, nehmen & \md{取, 带, 拿} \\
taxi & taxi & taxi & taxi & Taxi & \md{出租车} \\
thermodynamics & thermodynamics & thermodynamica & thermodynamique & Thermodynamik & \md{热力学} \\
tour & tour & excursie, rondleiding, tour, tournee & Tour, tour, visite & Tour & \md{旅行, 游览} \\
water & water & water & eau & Wasser & \md{水} \\
write & write & schrijven & écrire & schreiben & \md{写, 写字} \\
yes & yes & ja & oui & ja & \md{对, 是} \\
\end{longtable}

We selected words that varied in the number of tokens used to represent the words and selected some that were the same as English words. Table \ref{word-overlap} summarizes the word overlap across the different languages.  Tables \ref{llama-token-mean}, \ref{gemma-token-mean}, \ref{mixtral-token-mean} and \ref{aya-token-mean} summarize the average tokenization lengths of the words in different languages and models. The average number of tokens per word in English is lower than in other languages. 

\begin{table*}[h]
\caption{Word overlap between languages}
\label{word-overlap}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ c c c c c c }
\toprule
 & English & Mandarin & German & Dutch & French \\ \midrule
English & 75 & 0 & 0 & 14 & 16 \\
Mandarin & 0 & 75 & 0 & 0 & 0 \\
German & 0 & 0 & 75 & 0 &0 \\
Dutch & 14 & 0& 0 & 75 & 8 \\
French &  15 & 0 & 0 & 8 & 75 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[h]
\caption{\llama \ tokenization statistics}
\label{llama-token-mean}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ l cccccc}
\toprule
Language & 1 & 2 & 3 & 4 & 5+ & Mean \\ \midrule
English & 55 & 15 & 2 & 2 & 1 & 1.39 \\
Mandarin & 35 & 18 & 5 & 3 & 3 & 1.80 \\
German & 17 & 30 & 13 & 2 & 2 & 2.12 \\
Dutch & 19 & 29 & 11 & 2 & 3 & 2.09 \\
French & 20 & 32 & 7 & 3 & 2 & 2.03 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[h]
\caption{\gemma \ tokenization statistics}
\label{gemma-token-mean}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l cccccc}
\toprule
Language & 1 & 2 & 3 & 4 & 5+ & Weighted Mean \\ \midrule
English & 66 & 6 & 1 & 1 & 1 & 1.20 \\
Mandarin & 50 & 7 & 3 & 3 & 1 & 1.41 \\
German & 33 & 24 & 5 & 2 & 0 & 1.62 \\
Dutch & 34 & 23 & 4 & 2 & 1 & 1.64 \\
French & 39 & 20 & 2 & 2 & 1 & 1.53 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[h]
\caption{\mistral \ tokenization statistics}
\label{mixtral-token-mean}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l cccccc}
\toprule
Language & 1 & 2 & 3 & 4 & 5+ &  Mean \\ \midrule
English & 65 & 3 & 3 & 2 & 2 & 1.31 \\
Mandarin & 0 & 22 & 22 & 4 & 16 & 3.52 \\
German & 15 & 30 & 13 & 2 & 4 & 2.25 \\
Dutch & 17 & 29 & 12 & 3 & 3 & 2.19 \\
French & 21 & 28 & 8 & 5 & 2 & 2.11 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[h]
\caption{\aya \ tokenization statistics}
\label{aya-token-mean}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l cccccc}
\toprule
Language & 1 & 2 & 3 & 4 & 5+ & Mean \\ \midrule
English & 59 & 11 & 3 & 2 & 0 & 1.31 \\
Mandarin & 47 & 11 & 1 & 3 & 2 & 1.47 \\
German & 20 & 35 & 7 & 1 & 1 & 1.88 \\
Dutch & 29 & 24 & 7 & 1 & 3 & 1.83 \\
French & 32 & 25 & 4 & 1 & 2 & 1.70 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\FloatBarrier
\newpage 

\subsection{Parts of Speech Analysis}

In this experiment, we analyze how often a word is first `selected' in English, for each part of speech. To identify the part of speech, we used \texttt{spacy} models \citep{spacy2}.
To identify English words, we use \texttt{enchant.Dict("en\_US")}.
We use \texttt{nl\_core\_news\_sm}, \texttt{de\_core\_news\_sm}, \texttt{fr\_core\_news\_sm} and \texttt{zh\_core\_web\_sm}.
In general, we can use spaces to identify words in sentences. For Mandarin, we use the package \texttt{jieba}.

\begin{table*}[h]
\caption{Part of Speech Abbreviations, Terms, and Examples}
\label{pos-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l l l} 
\toprule
 Abbreviation &  Term &  Examples \\ \midrule
ADJ & Adjective &  \\
ADP & Adposition & in, to, during \\
ADV & Adverb & very, everywhere \\
AUX & Auxiliary & has (done), was (done) \\
CCONJ & Coordinating Conjunction & and, or, but \\
DET & Determiner & their, her, some \\
INTJ & Interjection & ouch \\
NOUN & Noun & place, thing, idea \\
NUM & Number & 10, 200  \\
PRON & Pronoun & he, she, they \\
PROPN & Proper Noun & specific name, place \\
SCONJ & Subordinating Conjunction & that, if, while \\
SYM & Symbol &  \\
VERB & Verb & see, run  \\ \bottomrule 

\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\subsection{Logit Lens Quantitative Evaluation} \label{sec:appendix_logit_lens_eval}

To evaluate whether a word is chosen in English, we use GPT-4o. 
We considered alternative evaluation procedures. 
We tested various translation packages but found issues with both word- and sentence-level approaches. When used on a word level, this caused problems with colexification and did not allow for close synonyms often only providing a single translation per word. 
When using translation on a sentence level, it was difficult to map tokens to each word (due to changes in the sentence structure).
We also considered WordNet \citep{miller-1994-wordnet}, but it only covers nouns, verbs, adjectives, and adverbs, making it unsuitable for other parts of speech.
Ultimately, we chose GPT-4o and manually verified 100 samples to ensure the evaluation was accurate.

We ask GPT-4o to score words as follows:
\begin{itemize}
    \item 5: An exact translation.
    \item 4: A close synonym.
    \item 3: A word with a similar but distinct meaning.
    \item 2: A word whose meaning is at best weakly related.
    \item 1: A word whose meaning is not related.
\end{itemize}
When a word receives a score of 4 or higher, we evaluate the word as chosen in English. 


An example of the command we use is:
\begin{displayquote}
Below, you will be given a reference word in Dutch and a context (i.e., phrase or sentence) in which the word is used. You will then be given another list of English words or subparts of words/phrases. \\ 
You should respond with the word from the list that is most similar to the reference word, along with a grade for the degree of similarity. \\
Special Note on Contextual Translations: If an English word could form a common phrase or idiomatic expression that accurately translates the reference word, it should be rated highly. For example, if a phrase like “turned out” perfectly matches a Dutch verb, the word “turned” alone would receive a high score due to its idiomatic fit.  \\ 
Special Note on Tenses: Do not penalize for different tenses. For example, the word ‘want’ matches ‘wilde’ and should receive a 5.  \\ 

Degrees of Similarity: Similarity should be evaluated from 1 to 5, as follows: \\ 
5: An exact translation. \\ 
4: A close synonym. \\ 
3: A word with a similar but distinct meaning. \\ 
2: A word whose meaning is at best weakly related. \\ 
1: A word whose meaning is not related. \\ 

Consider the following examples: \\ 

**Example 1** \\ 
Reference Word in Dutch: 'waarop'
Context: ‘Ze had een hekel aan de manier waarop hij zijn’ \\ 
English Word List: ['hicks', 'mild', 'rut', 'sens', 'spiral', 'hometown', 'how', 'manner', 'van', '101', 'ward'] \\ 

Analysis: ‘waarop’ means "on which" in Dutch. The word ‘how’ is most similar to this in the list, while the other options are unrelated. \\ 
Answer Word: ‘how’ \\ 
Similarity Score: 4 - a close synonym \\ 

**Example 2** \\ 
Reference Word in Dutch: ‘bleek’ \\ 
Context: ‘Ze adopteerde een zwerfdier, maar het bleek een wolf te zijn’ \\ 
English Word List: ['cup', 'freed', 'freeman', 'laurent', 'turns', 'turned', 'van', '348', 'i', 'ken', 'oms'] \\ 

Analysis: ‘bleek’ means "turned out" in Dutch, making ‘turned’ the most similar option. \\ 
Answer Word: ‘turned’ \\ 
Similarity Score: 5 - an exact translation \\ 

**Example 3** \\ 
Reference Word in Dutch: ‘vaas’ \\ 
Context: ‘Ze schikte een prachtig boeket bloemen in een vaas.’ \\ 
English Word List: ['tucker', 'van', 'container', 'opp', 'van', 'vessel', '-g', '-t', '397', 'art', 'as', 'ed', 'ion', 'let'] \\ 

Analysis: ‘vaas’ means "vase" in Dutch. The word ‘vessel’ is somewhat similar, as vases are vessels for holding items like flowers. \\ 
Answer Word: ‘vessel’ \\ 
Similarity Score: 3 - a word with a similar but distinct meaning \\ 

**Example 4** \\ 
Reference Word in Dutch: ‘werd’ \\ 
Context: ‘Ze ging geld opnemen bij de bank en werd overvallen.’ \\ 
English Word List: ['dee', 'lafayette', 'bank', 'bu', 'herself', 'kw', 'met', 'ramp', 'return', 'returning', '113', '347'] \\ 

Analysis: ‘werd’ means `was' in Dutch. None of these words are related. \\ 
Answer Word: None \\ 
Similarity Score: 1 - a word whose meaning is not related \\ 

**Example 5** \\ 
Reference Word in Dutch: ‘vrienden’ \\ 
Context: ‘Ze bracht het weekend door met haar vrienden in een huisje in de Ardennen.’ \\
English Word List: ['sag', 'sat', 'tween', 'bro', 'families', 'family', 'her', 'herself', 'mo', 'own', 'parents', 'weekend', '666', 'elf'] \\ 

Analysis: ‘vrienden’ means "friends" in Dutch. The closest word here is ‘families’, which is weakly related but distinct. \\ 
Answer Word: ‘families’ \\ 
Similarity Score: 2 - a word whose meaning is at best weakly related \\ 


The examples are complete. Now it is your turn. The reference word will be in Dutch, and you must find the most similar English word and assess the degree of meaning similarity on a scale from 1 to 5.
\end{displayquote}

The commands for other languages are similar but adapted to provide examples in the language. 

\newpage 
\subsubsection{Explicit Text} \label{sec:explicit_text}

\subsection{Other language-specific phenomena} 
We observe explicit vocabulary in the latent space of LLMs (examples can be found in Appendix \ref{sec:explicit_text}). 
Table \ref{vulgar-table} shows the frequency of vulgar words in the latent space, with \llama \ showing the highest count. This model is safety-tuned in eight languages \citep{dubey2024llama3herdmodels}, including English, German, and French, but not Dutch.
This may suggest that explicit terminology is a language- and model-specific feature.

\begin{table}[h]
\caption{Frequency of explicit words decoded in the latent space across LLMs. \llama \ has the highest proportion of explicit terms.}
\label{vulgar-table}
\vskip -0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c c } \toprule 
 Model &\multicolumn{5}{c}{ Explicit words ($\%$)}  \\ 
& ENG & FR & NL  &DE & ZH \\ \midrule 
\llama & $6.25$ &  $11.25$ & $18.35$ & $8.13$ & $7.19$  \\
\mistral & $1.56$ & $3.91$ & $5.21$ & $4.53$ & $10.19$ \\
\aya   & $2.50$ & $2.97$ & $3.44$ & $2.89$ & $2.69$\\
\gemma  & $0.00$ & $0.31$& $0.31$ & $0.39$ & $0.38$\\ \bottomrule
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{figure}[h]
    \centering
\includegraphics[width=0.49\textwidth]{figures/logit_lens/inappropriate_words.png} 
\caption{Example 1: Logit Lens applied to \llama.}
\label{fig:ll_explicit1}    
\end{figure}

Terms such as kutje (pussy), pornofilm (porn film), lul (dick), and knull (fuck) appear in various contexts, including inappropriate sentences about children.
For example, in Figure \ref{fig:ll_explicit1} during the generation:
\begin{displayquote}
    Ze houdt ervan om met \textbf{haar vriendinnen te winkelen en te klets(en)}  ... \\
    English translation: She enjoys shopping and talking with her friends ...
\end{displayquote}
We find the explicit words `kutje' and `pornofil(m)' when decoding the latent space using the logit lens.
This behavior is consistent across other examples (see Figure \ref{fig:ll_explicit2}).


\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{figures/logit_lens/inappropriate_words_2.png} 
\includegraphics[width=0.49\textwidth]{figures/logit_lens/inappropriate_words_3.png} 
\caption{Logit Lens applied to \llama.}
\label{fig:ll_explicit2}
\end{figure}

\FloatBarrier
\newpage 

\subsection{Steering}
For the steering experiment, we use the LLM-Insight dataset. We compute two steering vectors:
    \begin{itemize}
        \item a topic steering vector: this is a steering vector that captures the intended topic. For example, for the topic `love, we create a steering vector that is $v_l^t = h_l(\text{love}) - h_l(\text{hate})$, where $h_l$ is the hidden state in layer $l$. 
        \item a language steering vector: we add a steering vector that captures the intended output language. For example, for the target language Dutch, we can create a steering vector $v_l^l = h_l(\text{Dutch}) - h_l(\text{English})$.
        \item For each steering vector, we take the difference between sets of sentences containing the topic. 
    \end{itemize}
Currently, we consider steering successful if (1) the generated sentence contains the target word and (2) does not lead to output collapse (stuttering). We set the steering vector weights by using a hold-out set of 5 words (50 prompts). We found that $5$ was optimal for the topic steering vector, and $10$ was optimal for the language steering vector. 
For the layers, we considered every $5$-th layer of the model for the topic steering vector. We considered every $2$nd layer for the language vector. 
We reported the results across the best layers. On average, we found that 20-40 $\%$ of layers allowed for successful steering, with English steering vectors being the least sensitive to layer selection. 

\vfill 

\subsection{Cosine Similarity of Steering Vectors } \label{sec:appendix_geo}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]
    {figures/Cosine_distance/Aya.png}
    \caption{Cosine distance between steering vectors in \aya.}
    \label{fig:cosine_distance_appendix_aya}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]
    {figures/Cosine_distance/Gemma.png}
    \caption{Cosine distance between steering vectors in \gemma.}
    \label{fig:cosine_distance_appendix_gemma}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]
    {figures/Cosine_distance/Mistral.png}
    \caption{Cosine distance between steering vectors in \mistral.}
    \label{fig:cosine_distance_appendix_mistral}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]
    {figures/Cosine_distance/Llama.png}
    \caption{Cosine distance between steering vectors in \llama.}
    \label{fig:cosine_distance_appendix_llama}
\end{figure}

To analyze the geometry of the latent space, we compute both topic vectors and language vectors for our dataset. 
We track the cosine similarity of these steering vectors across different layers and models, providing insights into how topics and languages are represented internally.
Specifically, we plot the cosine similarities of topic vectors derived from different languages.
In Figures \ref{fig:cosine_distance_appendix_aya}, \ref{fig:cosine_distance_appendix_gemma}, \ref{fig:cosine_distance_appendix_mistral} and \ref{fig:cosine_distance_appendix_llama} are the plots for \aya, \gemma, \mistral, and \llama, respectively.

We find that topic vectors maintain a high cosine similarity of approximately 0.8 across languages. The similarity can be increased by incorporating the corresponding language vector, suggesting an interaction between topic and language-specific representations.


\FloatBarrier

\subsection{Causal tracing} \label{appendix:tracing}

\begin{figure}[h]
    \centering
    \includegraphics[trim={12.7cm 0cm 0cm 0cm},clip,width=0.7\textwidth]
    {figures/tracing/aya_10_5_mean.png}
    \caption{The causal traces of the city facts in \aya. }
    \label{fig:aya_tracing_appendix}
\end{figure}

Figures \ref{fig:aya_tracing_appendix}, \ref{fig:llama_tracing_appendix}, \ref{fig:mistral_tracing_appendix} \ and \ref{fig:gemma_tracing_appendix} show the causal traces, averaged over different country-city pairs for \aya, \llama, \mistral \ and \gemma \ respectively. 
Across all models, we find that facts are generally localized in similar layers, regardless of the language. 
Two main traces emerge: a mid-layer trace on the subject token(s), which may correspond to entity resolution, and a later trace when attribute recollection occurs (as suggested by \citet{nanda2023factfinding}).
Overall, these plots suggest that facts are approximately stored in the same parts of the model.

\begin{figure}[h]
    \centering
    \includegraphics[trim={12.7cm 0cm 0cm 0cm},clip,width=0.7\textwidth]{figures/tracing/llama_10_5_mean.png}
    \caption{The causal traces of the city facts in \llama. }
    \label{fig:llama_tracing_appendix}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[trim={12.7cm 0cm 0cm 0cm},clip,width=0.7\textwidth]{figures/tracing/mistral_10_5_mean.png}
    \caption{The causal traces of the city facts in \mistral. }
    \label{fig:mistral_tracing_appendix}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[trim={12.7cm 0cm 0cm 0cm},clip,width=0.7\textwidth]{figures/tracing/gemma_10_5_mean.png}
    \caption{The causal traces of the city facts in \gemma. }
    \label{fig:gemma_tracing_appendix}
\end{figure}

\FloatBarrier
\newpage 

\subsection{Hidden state interpolation (with instructions)} \label{sec:hidden_state_interpolation}

We include instructions as otherwise the MLLMs often describe the city, rather than provide the city. E.g., ``The capital of Canada is beautiful". 
For most models, the accuracy when interpolating between the hidden states is between the performances in the two languages. Interestingly, we observe a propensity of models to answer in a specific language. All models are most likely to answer in English. 

\subsubsection{Aya}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_ENG_aya__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_ENG_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and English prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and French (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/GER_ENG_aya__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/GER_ENG_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and English prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and German (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_ENG_aya__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_ENG_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and English prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Dutch (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_FR_aya__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_FR_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and Dutch prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Dutch (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_GER_aya__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_GER_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and Dutch prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and Dutch (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_GER_aya__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_GER_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and German prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and French (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_ENG_aya_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_ENG_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Mandarin prompts, and English prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Mandarin (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_NL_aya_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_NL_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Mandarin prompts, and Dutch prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in Dutch (red) and Mandarin (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_FR_aya_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_FR_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Mandarin prompts, and French prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Mandarin (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_GER_aya_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_GER_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Mandarin prompts, and German prompts in \aya. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and Mandarin (blue). }
\end{figure}

\FloatBarrier
\newpage 

\subsubsection{Llama} 

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_ENG_aya__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_ENG_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and French prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and French (blue). }

\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/GER_ENG_llama__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/GER_ENG_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and German prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and German (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_ENG_llama__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_ENG_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and Dutch prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Dutch (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_FR_llama__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_FR_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and French prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Dutch (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_GER_llama__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_GER_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and Dutch prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Dutch (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_GER_llama__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_GER_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and French prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and French (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_ENG_llama_no_instructions__interpolate_results.png}
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_ENG_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and Mandarin prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Mandarin (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_NL_llama_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_NL_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and Mandarin prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in Dutch (red) and Mandarin (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_FR_llama_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_FR_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and Mandarin prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Mandarin (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_GER_llama_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_GER_llama_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and Mandarin prompts in \llama. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and Mandarin (blue). }
\end{figure}


\FloatBarrier
\newpage 

\subsubsection{Mistral}
\mistral \ is most likely to answer in English and least likely to answer in Dutch. It is roughly equally likely to answer in German and French.


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_ENG_mistral__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_ENG_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and French prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and French (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/GER_ENG_mistral__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/GER_ENG_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and German prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and German (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_ENG_mistral__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_ENG_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and Dutch prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Dutch (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_FR_mistral__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_FR_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and French prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Dutch (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_GER_mistral__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_GER_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and German prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and Dutch (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_GER_mistral__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_GER_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and German prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and French (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_GER_mistral__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_GER_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and German prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and French (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_ENG_mistral_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_ENG_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and Mandarin prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Mandarin (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_FR_mistral_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_FR_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and Mandarin prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Mandarin (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_GER_mistral_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_GER_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and Mandarin prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and Mandarin (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_NL_mistral_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_NL_mistral_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and Mandarin prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in Dutch (red) and Mandarin (blue). }
\end{figure}


\FloatBarrier
\newpage 
\subsubsection{Gemma} 

\gemma \ is most likely to answer in English, German, French and then Dutch. 


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_ENG_gemma__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_ENG_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and English prompts in \gemma. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and French (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/GER_ENG_gemma__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/GER_ENG_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and English prompts in \gemma. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and German (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_ENG_gemma__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_ENG_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and English prompts in \gemma. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Dutch (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_FR_gemma__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_FR_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and Dutch prompts in \gemma. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Dutch (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/NL_GER_gemma__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/NL_GER_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and Dutch prompts in \gemma. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and Dutch (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/FR_GER_gemma__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/FR_GER_aya_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and German prompts in \gemma. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and French (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_ENG_gemma_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_ENG_gemma_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between English prompts, and Mandarin prompts in \gemma. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in English (red) and Mandarin (blue). }
\end{figure}


\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_FR_gemma_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_FR_gemma_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between French prompts, and Mandarin prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in French (red) and Mandarin (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_GER_gemma_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_GER_gemma_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between German prompts, and Mandarin prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in German (red) and Mandarin (blue). }
\end{figure}

\begin{figure}[h]
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={0 0 5cm 0},clip, width=\textwidth]{figures/interpolate/MD_NL_gemma_no_instructions__interpolate_results.png} 
\end{minipage}
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[trim={1.5cm 0.5cm 1.5cm 1.5cm},clip,width=\textwidth]{figures/interpolate/MD_NL_gemma_heatmap_interpolate_results.png} 
\end{minipage}
\caption{Hidden state interpolation between Dutch prompts, and Mandarin prompts in \mistral. Left shows the accuracy (i.e., the proportion of times the model correctly outputs city in either language). Right shows the propensity of the model to answer in Dutch (red) and Mandarin (blue). }
\end{figure}