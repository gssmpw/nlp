\section{Conclusion}

Our results provide evidence that semantic decisions in LLMs are predominantly made in a representation space close to English, while non-lexical words are processed in the prompt language. 
However, we find that this behavior varies across models, likely due to differences in multilingual proficiency and model size. 
The English-centric behavior is further validated by our findings that steering non-English prompts using vectors derived from English sentences is more effective than those from the prompt language.

Exploring the structure of the latent space, we find that factual knowledge across languages is stored in roughly the same regions of the model. 
Interpolating between the latent representations of these facts in different languages preserves predictive accuracy, with the only change being the output language. 
This suggests that facts encoded in different languages likely share a common representation. 
However, when interpolating, we find that model output is most frequently in English, further underlining the English-centric bias of the latent space. 


The English-centricity of the latent space is consistent with prior observations about LLM behavior. 
In particular, \citet{etxaniz2023multilinguallanguagemodelsthink} found that instructing LLMs to first translate a non-English prompt into English improves model performance. 
However, this bias can be detrimental. If the latent space is English-centric, this may lead the LLMs toward exhibiting Western-centric biases \citep{naous2024havingbeerprayermeasuring, shafayat2024multifactassessingfactualitymultilingual}. 