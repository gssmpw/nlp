\section{Introduction}

\begin{figure}[t]    
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/logit_lens/fig1.png} 
    \end{minipage}
    \caption{Logit lens applied to \llama's latent space, when prompted with \textit{Le bateau naviguait en douceur sur l'}. Each row depicts the decoded latent representations for one layer and each column corresponds to the generated token. Dark red boxes highlight words selected in English. The nouns `eau', `lac', and `soleil' are selected in English, whereas other parts of speech are not.}
    \label{fig:logit_lens}
    
\end{figure}

Large Language Models (LLMs) are predominantly trained on English data, yet are deployed across various languages, including some that are rarely seen during training. 
This raises an important question: \textit{how} do LLMs operate across different languages?

LLMs are hypothesized to operate in an abstract concept space  \citep{olahfeaturesllm, nanda2023progress, wendler2024llamasworkenglishlatent, dumas2024llamas}.
From the multilingual perspective, one main question is whether the concept space is language-specific or language-agnostic. 
We consider three different hypotheses: 
\begin{enumerate}
    \item LLMs `operate' in a space that is English-centric (or centered on the main pretraining language)
    \item LLMs `operate' in a language-agnostic space
    \item LLMs `operate' in a language-specific space, which is determined by the input language.
\end{enumerate}

We present evidence that the first hypothesis is true: LLMs reason in an English-centric way. Our work studies open-ended multi-token language generation, contrasting with prior work \citep{wendler2024llamasworkenglishlatent} which found evidence for the second hypothesis in the single token context. 

\begin{table*}[t]
\caption{Summary of pre-training dataset languages of the open-source large language models studied in this work.}
\label{model-summary}
\vskip -0.2in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ p{6cm} p{2.3cm} ccccc}
\toprule
Model & Nr. Languages &\multicolumn{5}{c}{Trained on Language?} \\ 
& Trained On  & English & French & German  &Dutch & Mandarin \\ 
\midrule
Aya-23-35B {\citep{aryabumi2024aya}}  & 23 & $\surd$ & $\surd$ & $\surd$ & $\surd$  & $\surd$ \\
Llama-3.1-70B  \citep{dubey2024llama3herdmodels}  & 8 & $\surd$ & $\surd$ & $\surd$ & $\times$  & $\times$ \\
Mixtral-8x22B-v0.1 \citep{jiang2024mixtralexperts} & 5 & $\surd$ & $\surd$ & $\surd$ & $\times$  & $\times$ \\
Gemma-2-27b \citep{aryabumi2024aya}  & 1 & $\surd$ & $\times$ & $\times$ & $\times$  & $\times$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

We study three aspects of language generation. 
First, we study how representations progress within the model, showing that for lexical words, English-focused representations often appear first before being transformed into the target language.
However, non-lexical words do not route through the English representation space. 
Second, we show that steering the representations is more effective using vectors constructed in English than in the target language.
Third, we show that the latent representation structure is consistent with the language and semantic context being represented separately. In more detail, we:

\paragraph{Decode the Representation Space}
LLMs make semantic decisions in English, even when prompted in a non-English language. 
Figure \ref{fig:logit_lens} shows the logit lens to \llama \ as it generates the French text \textit{Le bateau naviguait en douceur sur l'\textbf{eau au calme du lac. Le soleil ...}}, with the bold text representing Llama's output. The English translation is \textit{The boat sailed smoothly on the \textbf{calm water in of the lake. The sun ...}}. Lexical words like ``water," ``lake," and ``sun" are selected in English, whereas grammatical elements such as ``du" and ``le" are not. We find that this trend holds more generally for other models (Section \ref{sec:logit_lens_latent}), with Aya being the least English-centric and Gemma the most English-centric.

\paragraph{Manipulating the representations}
Non-English sentences generated can be steered more effectively using English-derived steering vectors than those derived from the target language. This surprising result provides further evidence that LLMs rely on an English-centric conceptual space for semantic reasoning. 
Further, we find that the steering vectors have a relatively high cosine similarity, however, they do encode a language-specific component. We can increase the similarity of steering vectors found in different languages by nudging them toward each other using a language steering vector.

\paragraph{Structure of the latent space}
Fact representations are shared between languages, allowing interpolation between a fact expressed in two languages while maintaining the correct answerâ€”changing only the output language. 

We analyze four open source models  (\llama, \gemma, \aya \ and \mistral), which vary in architecture and language coverage.
In general, we find that LLMs make decisions in the representation space close to English, independent of the input or output language. 
This English-centric behavior of LLMs cause them to perform worse in other languages, whether in downstream tasks \citep{shafayat2024multifactassessingfactualitymultilingual, huang2023languagescreatedequalllms, bang2023multitaskmultilingualmultimodalevaluation, shi2022language}, or in fluency \cite{guo2024benchmarking}. Moreover, this impacts the fairness of these models -- which currently exhibit cultural biases \cite{shafayat2024multifactassessingfactualitymultilingual} -- and their robustness and reliability in diverse linguistic settings \cite{ marchisio2024understandingmitigatinglanguageconfusion, deng2024multilingualjailbreakchallengeslarge}.
