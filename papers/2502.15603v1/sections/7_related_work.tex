\section{Related Work}

We can think about understanding a model from two perspectives:
\begin{itemize}
    \item an \textbf{internal perspective}, focused on analyzing the model through the latent space and operations performed inside of the model. Examples of questions include: how do models represent knowledge across different languages? How does a model retrieve facts? 
    \item an \textbf{external perspective}, focused on analyzing the model output. For example, how well do models perform in different languages? 
\end{itemize}
Having a unifying theory that combines both perspectives is important -- the internal perspective helps us understand the mechanisms underlying behavior, while the external perspective examines the real-world impact of that behavior. Below, we summarize the research on multilingual language models from both perspectives.

\subsection{How do LLMs operate internally?}
The current main theory in mechanistic interpretability suggests that there are three general phases in the forward pass of an LLM \cite{olahfeaturesllm, nanda2023progress, wendler2024llamasworkenglishlatent, dumas2024llamas, fierro2025multilinguallanguagemodelsremember}:
\begin{enumerate}
    \item \textbf{Detokenization}:  In this phase, individual tokens are combined into abstract units that the model uses for analysis. These units can be referents -- for example, \cite{nanda2023progress} found evidence that the tokens [Michael] and [Jordan] are combined into a unit representing the basketball player Michael Jordan. Similarly, these units can encode instructions, as shown by \cite{dumas2024llamas}, where the model extracts the target language during translation tasks in these layers.
    \item \textbf{Processing}: In this phase, the model processes or reasons over abstract units. For instance, this stage may involve tasks like fact recall \cite{geva-etal-2023-dissecting, nanda2023progress}.
    \item {\textbf{Selecting the output}}: In this phase, the model selects the output. This may involve selecting the correct attribute \cite{nanda2023progress}, mapping an abstract concept to the corresponding word in the target language \cite{wendler2024llamasworkenglishlatent} and/or selecting the correct token for the intended word.  
\end{enumerate}

In the context of multilingual models, an important question is whether the concept space (in phase 2) is universal. 
Here, \textit{universal} means the representation is \textit{shared} across languages, i.e., the representation for `cat' (cat in English) and `kat' (cat in Dutch) is the same.

One stream of research argues that the concept space is \textit{universal}. When analyzing the latent space with the logit lens, \citet{wendler2024llamasworkenglishlatent} 
find that the concept space is language-agnostic, but more closely aligned with the English output space.
In their follow-up work, \citet{dumas2024llamas} used tracing to find further evidence of language-agnostic representations of concepts.\footnote{They used tracing to mitigate potential shortcomings of cosine-similarity \citep{steck2024cosine}}.
Concurrent to this work, \citet{brinkmann2025large} showed that models share morpho-syntactic concept representations across languages.

Other researchers found that the concept space is biased towards the training-dominant language. 
Using the logit lens, \citet{zhong2024beyond} focused on Japanese, showing that Swallow \citep{fujii2024continual}, which is fine-tuned on Japanese, and LLM-jap \cite{aizawa2024llm}, pre-trained on Japanese and English, are Japanese-centric. 
\cite{fierro2025multilinguallanguagemodelsremember} found that subject enrichment -- retrieving attributes about the subject -- is language-agnostic whereas object extraction is language-dependent, when studying EuroLLM \citep{martins2024eurollmmultilinguallanguagemodels}, XLGM \citep{lin2022fewshotlearningmultilinguallanguage} and mT5 \citep{xue-etal-2021-mt5}. 


Overall, our findings suggest the presence of a language-specific latent space. Specifically, our results indicate that the concept space is largely English-centric, consistent with prior work \citet{zhong2024beyond, wu2024semantic}, where English is the dominant training language. However, unlike previous studies, we uncover additional nuance: non-lexical words are not necessarily routed through the English representation space.
Moreover, we find that the behavior varies across models. One potential explanation is that we focus on open language generation -- which is different from prior work that predominantly focuses on single-token and translation tasks. 
We provide a more in-depth discussion in Section \ref{sec:discussions}.

\subsection{Multilingual LLM behavior}

The internal mechanisms of LLMs affect their performance in several different ways, which we summarize below.

\paragraph{Performance}
The performance of multilingual language models varies across languages \cite{shafayat2024multifactassessingfactualitymultilingual, huang2023languagescreatedequalllms, bang2023multitaskmultilingualmultimodalevaluation, shi2022language, ahuja2023megamultilingualevaluationgenerative}, often performing best in English.
This can even be leveraged to improve performance in other languages, for example, through cross-lingual chain-of-thought reasoning \cite{chai2024xcotcrosslingualinstructiontuning}, or by modifying prompts, such as using multilingual instructions or asking the LLM to translate the task into English before completing it \cite{zhu2023extrapolatinglargelanguagemodels, etxaniz2023multilinguallanguagemodelsthink}.

\paragraph{Fluency and language confusion}
\citet{marchisio2024understandingmitigatinglanguageconfusion} has shown that English-centric models are prone to language confusion, i.e., providing the answer in the incorrect language. 
Moreover, even when LLMs output text in the correct language, they can produce unnatural sentences in other languages, akin to an accent \cite{guo2024benchmarking}. 

\paragraph{Bias and culture}
 Moreover, LLMs tend to be biased toward certain cultures, with content performing better when dealing with facts originating from Western contexts \cite{naous2024havingbeerprayermeasuring, shafayat2024multifactassessingfactualitymultilingual}, falling short when answering questions on other cultures \cite{chiu2024culturalbenchrobustdiversechallenging}.
\citet{liu2024multilingualllmsculturallydiversereasoners} investigate the cultural diversity of LLMs using proverbs and find that these models often struggle to reason with or effectively use proverbs in conversation. Their understanding appears limited to memorization rather than true comprehension, creating a notable ``culture gap" when translating or reasoning with culturally specific content across languages.
