\section{Datasets} \label{sec:datasets}

\paragraph{LLM-Insight}
We created a dataset to analyze the behavior of LLMs, which we will release alongside this paper. The dataset is specifically designed to study steering in LLMs. 

It includes 72 target words, each paired with 10 prompts and 10 sentences in English, Dutch, French, German and Mandarin. Table \ref{data-example} shows a sample of the data. 
The prompts are designed so the word could appear as the next token, but the prompts are also sufficiently open-ended so that semantically unrelated words can be used to complete the sentence.
For example ``They adapted a" can be completed with the word `animal' as well as `daughter'. 

The sentences provided in the dataset can be used to find steering vectors.
Some words in the dataset naturally form pairs that can be used to create steering vectors, such as the words `good' and `bad'. For words without a natural pairing, such as `thermodynamics', we provide a general set of sentences as the counter set to create the steering vector. Further details on the dataset can be found in Appendix \ref{sec:appendix_dataset}.

\paragraph{City facts \cite{ghandeharioun2024patchscope}}
We use this dataset to investigate how facts in different languages are encoded in LLMs. The task is to provide the capital city of a given country.
For example, when prompted with “The capital of Canada is”, the model should output “Ottawa”. 
This allows us to identify where in the network Ottawa is encoded.
To analyze cross-lingual representations, we augment the dataset by translating these facts into German, Dutch, and French. 