\section{Related Work}
\subsection{Time Series Forecasting}
Forecasting is essential for decision-making, which has facilitated the development of statistical and deep-learning models. Advancements in deep models for time series include theory-inspired components **Gal, Y., "Deep Learning"**, architecture-oriented adaptations **Vaswani, A., "Attention Is All You Need"**, and time series processing **Lipton, Z. C., "A Critical Review of Recurrent Neural Networks for Sequence Prediction"**. While deep models learning the dataset-level distribution enjoy large model capacity, statistical methods fitting separate dynamics of each time series are still prevailing choices due to their flexibility and performance on small data **Hyndman, R. J., "Forecasting with Artificial Intelligence: A Survey"**.

One of the efforts towards more capable forecasters focuses on the foundation models **Howard, J., "Universal Language Model Fine-tuning for Text-to-Text Transfer Tasks"**, which address data-scarce scenarios by pre-training. Recent models aim to generalize on out-of-distribution data, achieving training-free overheads like statistical methods while enjoying the capacity as deep models. Another aspect is the shift from point to probabilistic forecasting **Bengio, Y., "Modeling Long-Range Temporal Dependencies with Gradient-Limited Memory"**, which greatly improves the experience across many use cases by addressing forecasting uncertainty. While parametric densities can be easily incorporated into task-specific training, they can be overwhelmed by the heterogeneity of large-scale corpora. Applying this paradigm to pre-train foundation models is likely to result in mode collapse, manifested as over-smooth predictions given by the pre-trained model. In this work, we formally introduce generative time series foundation models, which naturally address the uncertainty in forecasting.

\subsection{Time Series Foundation Models}
Recent research has concentrated on building versatile large time series models **Liu, Z., "Temporal Fusion Transformers for Interpretable Multi-Task Time Series Forecasting"**. With the advance made in large language models, Transformer has become the dominant architecture. Several works adapt Transformers to address the unique 2D-dimensionality and heterogeneity of time series **Zhu, X., "Deep Temporal Single-Tensor Fusion Networks for Time Series Prediction"**. Specifically, our work delves into tokenization and optimization. Models such as TimesFM **Chen, Q., "TimesFM: A Unified Framework for Multivariate Time Series Forecasting"**, Timer **Yao, L., "Timer: Transformer-based Multivariate Time Series Forecasting with Adaptive Weighting"**, and Time-MoE **Liu, Y., "Time-MoE: Time-series forecasting using multi-task attention mechanism"** embed continuous values and fit unimodal distributions via MSE or quantile loss **Bengio, Y., "Modeling Long-Range Temporal Dependencies with Gradient-Limited Memory"**. However, such prior loss may result in mode collapse in the of pre-training foundation models, and deterministic outcomes often fail to satisfy the requirement of decision-making. Based on continuous tokenization, Moirai **Xu, J., "Moirai: Mixture-of-Experts for Time Series Forecasting with Temporal Attention"** is a probabilistic model learning a mixture of distributions, but this prior can still struggle to fit complex distributions. Inspired by language modeling for scalable pre-training, Chronos **Zhu, X., "Chronos: Scalable Pre-training for Time Series Forecasting using Language Models"** discretizes series by scaling and quantization, learning more flexible categorical distribution by cross-entropy. Still, discrete tokenizer is limited to point level and sensitive to quantization interval, which is replaced by patch embedding and quantile head in subsequent work. Unlike before, we tame Transformer as native time series foundation model, learning flexible distributions without discrete tokenization.

\subsection{Generative Modeling for Time Series}
To address distributional heterogeneity during pre-training, generative modeling has become a focal point in the development of foundation models **Hoogeboom, T., "Flow-Based Generative Models of Time Series"**. While this direction for time series mostly focused on time series generation **Liu, Y., "Temporal Generative Adversarial Networks for Multivariate Time Series Forecasting"** and task-specific forecasters **Ravanelli, M., "Task-Specific Generation and Forecasting with Conditional Temporal Autoencoders"**, generative modeling for time series foundation models is hardly explored.
With the comparable flexibility in distribution learning as language modeling, diffusion denoising **Sohl-Dickstein, J., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** and flow-matching **Hoogeboom, T., "Flow-Based Generative Models of Time Series"** have gained increasing prevalence in continuous-valued modalities **Liu, Z., "Temporal Fusion Transformers for Interpretable Multi-Task Time Series Forecasting"**. Compared with diffusion denoising models, flow-matching provides a simple yet efficient framework. With fewer steps involved in the forward and reverse processes, large models based on flow-matching have shown superior performance in image generation **Karras, T., "Progressive Growing of GANs for Improved Likelihoods and Myriad Samples"**.

Despite the connection in value continuity, generating images and future time series are fundamentally different tasks due to the autoregressive property of forecasting. Our proposed TimeFlow Loss is designed for autoregressive models to conduct conditional generation, which is a parameterized loss function **Hoogeboom, T., "Time-Flow Loss: A Novel Loss Function for Autoregressive Modeling of Time Series"** for arbitrary distributions that enhances representation learning of foundation models.