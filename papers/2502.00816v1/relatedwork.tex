\section{Related Work}
\subsection{Time Series Forecasting}
Forecasting is essential for decision-making, which has facilitated the development of statistical and deep-learning models. Advancements in deep models for time series include theory-inspired components~\cite{wu2021autoformer, zeng2023transformers, wu2022timesnet}, architecture-oriented adaptations~\cite{bai2018empirical, salinas2020deepar, lim2021temporal}, and time series processing~\cite{kim2021reversible, nie2022time}. While deep models learning the dataset-level distribution enjoy large model capacity, statistical methods fitting separate dynamics of each time series are still prevailing choices due to their flexibility and performance on small data~\cite{ke2017lightgbm, hyndman2018forecasting}.

One of the efforts towards more capable forecasters focuses on the foundation models~\cite{bommasani2021opportunities}, which address data-scarce scenarios by pre-training. Recent models aim to generalize on out-of-distribution data, achieving training-free overheads like statistical methods while enjoying the capacity as deep models. Another aspect is the shift from point to probabilistic forecasting~\cite{woo2024unified, ansari2024chronos}, which greatly improves the experience across many use cases by addressing forecasting uncertainty. While parametric densities can be easily incorporated into task-specific training, they can be overwhelmed by the heterogeneity of large-scale corpora. Applying this paradigm to pre-train foundation models is likely to result in mode collapse, manifested as over-smooth predictions given by the pre-trained model. In this work, we formally introduce generative time series foundation models, which naturally address the uncertainty in forecasting.

\subsection{Time Series Foundation Models}
Recent research has concentrated on building versatile large time series models~\cite{liang2024foundation}. With the advance made in large language models, Transformer has become the dominant architecture. Several works adapt Transformers to address the unique 2D-dimensionality and heterogeneity of time series~\cite{woo2024unified, liu2024timer}. Specifically, our work delves into tokenization and optimization. Models such as TimesFM~\cite{das2023decoder}, Timer~\cite{liu2024timer, liutimer}, and Time-MoE~\cite{shi2024time} embed continuous values and fit unimodal distributions via MSE or quantile loss~\cite{wen2017multi}. However, such prior loss may result in mode collapse in the of pre-training foundation models, and deterministic outcomes often fail to satisfy the requirement of decision-making. Based on continuous tokenization, Moirai~\cite{woo2024unified} is a probabilistic model learning a mixture of distributions, but this prior can still struggle to fit complex distributions. Inspired by language modeling for scalable pre-training, Chronos~\cite{ansari2024chronos} discretizes series by scaling and quantization, learning more flexible categorical distribution by cross-entropy. Still, discrete tokenizer is limited to point level and sensitive to quantization interval, which is replaced by patch embedding and quantile head in subsequent work. Unlike before, we tame Transformer as native time series foundation model, learning flexible distributions without discrete tokenization.

\subsection{Generative Modeling for Time Series}
To address distributional heterogeneity during pre-training, generative modeling has become a focal point in the development of foundation models~\cite{zhao2023survey, liu2024sora}. While this direction for time series mostly focused on time series generation~\cite{tashiro2021csdi} and task-specific forecasters~\cite{rasul2021autoregressive,shen2023non,kollovieh2024flow}, generative modeling for time series foundation models is hardly explored.
With the comparable flexibility in distribution learning as language modeling, diffusion denoising~\cite{sohl2015deep} and flow-matching~\cite{lipman2022flow} have gained increasing prevalence in continuous-valued modalities~\cite{lipman2024flow}. Compared with diffusion denoising models, flow-matching provides a simple yet efficient framework. With fewer steps involved in the forward and reverse processes, large models based on flow-matching have shown superior performance in image generation~\cite{esser2024scaling}.

Despite the connection in value continuity, generating images and future time series are fundamentally different tasks due to the autoregressive property of forecasting. Our proposed TimeFlow Loss is designed for autoregressive models to conduct conditional generation, which is a parameterized loss function~\cite{zhang2018unreasonable} for arbitrary distributions that enhances representation learning of foundation models.