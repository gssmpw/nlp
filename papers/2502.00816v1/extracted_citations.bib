@article{ansari2024chronos,
  title={Chronos: Learning the language of time series},
  author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others},
  journal={arXiv preprint arXiv:2403.07815},
  year={2024}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{das2023decoder,
  title={A decoder-only foundation model for time-series forecasting},
  author={Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  journal={arXiv preprint arXiv:2310.10688},
  year={2023}
}

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, RJ},
  year={2018},
  publisher={OTexts}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{kim2021reversible,
  title={Reversible instance normalization for accurate time-series forecasting against distribution shift},
  author={Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{kollovieh2024flow,
  title={Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting},
  author={Kollovieh, Marcel and Lienen, Marten and L{\"u}dke, David and Schwinn, Leo and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2410.03024},
  year={2024}
}

@inproceedings{liang2024foundation,
  title={Foundation models for time series analysis: A tutorial and survey},
  author={Liang, Yuxuan and Wen, Haomin and Nie, Yuqi and Jiang, Yushan and Jin, Ming and Song, Dongjin and Pan, Shirui and Wen, Qingsong},
  booktitle={Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining},
  pages={6555--6565},
  year={2024}
}

@article{lim2021temporal,
  title={Temporal fusion transformers for interpretable multi-horizon time series forecasting},
  author={Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  journal={International Journal of Forecasting},
  volume={37},
  number={4},
  pages={1748--1764},
  year={2021},
  publisher={Elsevier}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{lipman2024flow,
  title={Flow Matching Guide and Code},
  author={Lipman, Yaron and Havasi, Marton and Holderrieth, Peter and Shaul, Neta and Le, Matt and Karrer, Brian and Chen, Ricky TQ and Lopez-Paz, David and Ben-Hamu, Heli and Gat, Itai},
  journal={arXiv preprint arXiv:2412.06264},
  year={2024}
}

@article{liu2024sora,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={arXiv preprint arXiv:2402.17177},
  year={2024}
}

@article{liu2024timer,
  title={Timer-XL: Long-Context Transformers for Unified Time Series Forecasting},
  author={Liu, Yong and Qin, Guo and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2410.04803},
  year={2024}
}

@inproceedings{liutimer,
  title={Timer: Generative Pre-trained Transformers Are Large Time Series Models},
  author={Liu, Yong and Zhang, Haoran and Li, Chenyu and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@article{nie2022time,
  title={A time series is worth 64 words: Long-term forecasting with transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={arXiv preprint arXiv:2211.14730},
  year={2022}
}

@inproceedings{rasul2021autoregressive,
  title={Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting},
  author={Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
  booktitle={International Conference on Machine Learning},
  pages={8857--8868},
  year={2021},
  organization={PMLR}
}

@article{salinas2020deepar,
  title={DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
  author={Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  journal={International journal of forecasting},
  volume={36},
  number={3},
  pages={1181--1191},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{shen2023non,
  title={Non-autoregressive conditional diffusion models for time series prediction},
  author={Shen, Lifeng and Kwok, James},
  booktitle={International Conference on Machine Learning},
  pages={31016--31029},
  year={2023},
  organization={PMLR}
}

@article{shi2024time,
  title={Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts},
  author={Shi, Xiaoming and Wang, Shiyu and Nie, Yuqi and Li, Dianqi and Ye, Zhou and Wen, Qingsong and Jin, Ming},
  journal={arXiv preprint arXiv:2409.16040},
  year={2024}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{tashiro2021csdi,
  title={Csdi: Conditional score-based diffusion models for probabilistic time series imputation},
  author={Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24804--24816},
  year={2021}
}

@article{wen2017multi,
  title={A multi-horizon quantile recurrent forecaster},
  author={Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
  journal={arXiv preprint arXiv:1711.11053},
  year={2017}
}

@article{woo2024unified,
  title={Unified training of universal time series forecasting transformers},
  author={Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2402.02592},
  year={2024}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@article{wu2022timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2210.02186},
  year={2022}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  pages={11121--11128},
  year={2023}
}

@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

