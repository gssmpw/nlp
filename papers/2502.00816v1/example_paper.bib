@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{jin2023time,
  title={Time-llm: Time series forecasting by reprogramming large language models},
  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others},
  journal={arXiv preprint arXiv:2310.01728},
  year={2023}
}

@article{chang2023llm4ts,
  title={Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms},
  author={Chang, Ching and Peng, Wen-Chih and Chen, Tien-Fu},
  journal={arXiv preprint arXiv:2308.08469},
  year={2023}
}

@article{zhou2023one,
  title={One Fits All: Power General Time Series Analysis by Pretrained LM},
  author={Zhou, Tian and Niu, Peisong and Wang, Xue and Sun, Liang and Jin, Rong},
  journal={arXiv preprint arXiv:2302.11939},
  year={2023}
}

@article{liu2023unitime,
  title={UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting},
  author={Liu, Xu and Hu, Junfeng and Li, Yuan and Diao, Shizhe and Liang, Yuxuan and Hooi, Bryan and Zimmermann, Roger},
  journal={arXiv preprint arXiv:2310.09751},
  year={2023}
}

@article{liu2023itransformer,
  title={itransformer: Inverted transformers are effective for time series forecasting},
  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  journal={arXiv preprint arXiv:2310.06625},
  year={2023}
}

@article{nie2022time,
  title={A time series is worth 64 words: Long-term forecasting with transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={arXiv preprint arXiv:2211.14730},
  year={2022}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  pages={11121--11128},
  year={2023}
}

@article{wu2022timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2210.02186},
  year={2022}
}

@article{liu2023koopa,
  title={Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors},
  author={Liu, Yong and Li, Chenyu and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2305.18803},
  year={2023}
}

@article{oreshkin2019n,
  title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},
  author={Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1905.10437},
  year={2019}
}

@inproceedings{challu2023nhits,
  title={Nhits: Neural hierarchical interpolation for time series forecasting},
  author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={6989--6997},
  year={2023}
}

@article{makridakis2020m4,
  title={The M4 Competition: 100,000 time series and 61 forecasting methods},
  author={Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={54--74},
  year={2020},
  publisher={Elsevier}
}

@article{wang2022koopman,
  title={Koopman neural forecaster for time series with temporal distribution shifts},
  author={Wang, Rui and Dong, Yihe and Arik, Sercan {\"O} and Yu, Rose},
  journal={arXiv preprint arXiv:2210.03675},
  year={2022}
}

@article{zhou2022film,
  title={Film: Frequency improved legendre memory model for long-term time series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Sun, Liang and Yao, Tao and Yin, Wotao and Jin, Rong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12677--12690},
  year={2022}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@article{gruver2023large,
  title={Large language models are zero-shot time series forecasters},
  author={Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2310.07820},
  year={2023}
}

@article{sun2023test,
  title={TEST: Text prototype aligned embedding to activate LLM's ability for time series},
  author={Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda},
  journal={arXiv preprint arXiv:2308.08241},
  year={2023}
}

@article{cao2023tempo,
  title={TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting},
  author={Cao, Defu and Jia, Furong and Arik, Sercan O and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
  journal={arXiv preprint arXiv:2310.04948},
  year={2023}
}

@article{xue2023promptcast,
  title={Promptcast: A new prompt-based learning paradigm for time series forecasting},
  author={Xue, Hao and Salim, Flora D},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{zhang2022crossformer,
  title={Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author={Zhang, Yunhao and Yan, Junchi},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  pages={13},
  year={2023}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{woo2023pushing,
  title={Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain},
  author={Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2310.05063},
  year={2023}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@incollection{box2013box,
  title={Box and Jenkins: time series analysis, forecasting and control},
  author={Box, George},
  booktitle={A Very British Affair: Six Britons and the Development of Time Series Analysis During the 20th Century},
  pages={161--215},
  year={2013},
  publisher={Springer}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI},
  journal={OpenAI}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={11106--11115},
  year={2021}
}

@inproceedings{wang2022language,
  title={What language model architecture and pretraining objective works best for zero-shot generalization?},
  author={Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={22964--22984},
  year={2022},
  organization={PMLR}
}

@article{dai2022can,
  title={Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{winters1960forecasting,
  title={Forecasting sales by exponentially weighted moving averages},
  author={Winters, Peter R},
  journal={Management science},
  volume={6},
  number={3},
  pages={324--342},
  year={1960},
  publisher={INFORMS}
}

@book{box2015time,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}

@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@book{durbin2012time,
  title={Time series analysis by state space methods},
  author={Durbin, James and Koopman, Siem Jan},
  volume={38},
  year={2012},
  publisher={OUP Oxford}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{das2023long,
  title={Long-term Forecasting with TiDE: Time-series Dense Encoder},
  author={Das, Abhimanyu and Kong, Weihao and Leach, Andrew and Sen, Rajat and Yu, Rose},
  journal={arXiv preprint arXiv:2304.08424},
  year={2023}
}

@article{liu2022non,
  title={Non-stationary transformers: Exploring the stationarity in time series forecasting},
  author={Liu, Yong and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9881--9893},
  year={2022}
}

@inproceedings{zhou2022fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International Conference on Machine Learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@inproceedings{wang2022micn,
  title={Micn: Multi-scale local and global context modeling for long-term series forecasting},
  author={Wang, Huiqiang and Peng, Jian and Huang, Feihu and Wang, Jince and Chen, Junhui and Xiao, Yifei},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{breunig2000lof,
  title={LOF: identifying density-based local outliers},
  author={Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  booktitle={Proceedings of the 2000 ACM SIGMOD international conference on Management of data},
  pages={93--104},
  year={2000}
}

@inproceedings{wang2017time,
  title={Time series classification from scratch with deep neural networks: A strong baseline},
  author={Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  booktitle={2017 International joint conference on neural networks (IJCNN)},
  pages={1578--1585},
  year={2017},
  organization={IEEE}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{kornblith2019better,
  title={Do better imagenet models transfer better?},
  author={Kornblith, Simon and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2661--2671},
  year={2019}
}

@article{dong2023simmtm,
  title={SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling},
  author={Dong, Jiaxiang and Wu, Haixu and Zhang, Haoran and Zhang, Li and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2302.00861},
  year={2023}
}

@article{woo2022cost,
  title={CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting},
  author={Woo, Gerald and Liu, Chenghao and Sahoo, Doyen and Kumar, Akshat and Hoi, Steven},
  journal={arXiv preprint arXiv:2202.01575},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{das2023decoder,
  title={A decoder-only foundation model for time-series forecasting},
  author={Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  journal={arXiv preprint arXiv:2310.10688},
  year={2023}
}

@inproceedings{rosenstein2005transfer,
  title={To transfer or not to transfer},
  author={Rosenstein, Michael T and Marx, Zvika and Kaelbling, Leslie Pack and Dietterich, Thomas G},
  booktitle={NIPS 2005 workshop on transfer learning},
  volume={898},
  year={2005}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@article{yan2021videogpt,
  title={Videogpt: Video generation using vq-vae and transformers},
  author={Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
  journal={arXiv preprint arXiv:2104.10157},
  year={2021}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{zerveas2021transformer,
  title={A transformer-based framework for multivariate time series representation learning},
  author={Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
  booktitle={Proceedings of the 27th ACM SIGKDD conference on knowledge discovery \& data mining},
  pages={2114--2124},
  year={2021}
}

@article{wang2022learning,
  title={Learning latent seasonal-trend representations for time series forecasting},
  author={Wang, Zhiyuan and Xu, Xovee and Zhang, Weifeng and Trajcevski, Goce and Zhong, Ting and Zhou, Fan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38775--38787},
  year={2022}
}

@inproceedings{yue2022ts2vec,
  title={Ts2vec: Towards universal representation of time series},
  author={Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={8980--8987},
  year={2022}
}

@article{zhang2022self,
  title={Self-supervised contrastive pre-training for time series via time-frequency consistency},
  author={Zhang, Xiang and Zhao, Ziyuan and Tsiligkaridis, Theodoros and Zitnik, Marinka},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3988--4003},
  year={2022}
}

@article{dooley2023forecastpfn,
  title={ForecastPFN: Synthetically-Trained Zero-Shot Forecasting},
  author={Dooley, Samuel and Khurana, Gurnoor Singh and Mohapatra, Chirag and Naidu, Siddartha and White, Colin},
  journal={arXiv preprint arXiv:2311.01933},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{adftest,
 author = {Graham Elliott and Thomas J. Rothenberg and James H. Stock},
 journal = {Econometrica},
 title = {Efficient Tests for an Autoregressive Unit Root},
 year = {1996}
}

@inproceedings{goerg2013forecastable,
  title={Forecastable component analysis},
  author={Goerg, Georg},
  booktitle={International conference on machine learning},
  pages={64--72},
  year={2013},
  organization={PMLR}
}

@article{godahewa2021monash,
  title={Monash time series forecasting archive},
  author={Godahewa, Rakshitha and Bergmeir, Christoph and Webb, Geoffrey I and Hyndman, Rob J and Montero-Manso, Pablo},
  journal={arXiv preprint arXiv:2105.06643},
  year={2021}
}

@article{tan2021time,
  title={Time series extrinsic regression: Predicting numeric values from time series data},
  author={Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Fran{\c{c}}ois and Webb, Geoffrey I},
  journal={Data Mining and Knowledge Discovery},
  volume={35},
  pages={1032--1060},
  year={2021},
  publisher={Springer}
}

@article{dau2019ucr,
  title={The UCR time series archive},
  author={Dau, Hoang Anh and Bagnall, Anthony and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Keogh, Eamonn},
  journal={IEEE/CAA Journal of Automatica Sinica},
  volume={6},
  number={6},
  pages={1293--1305},
  year={2019},
  publisher={IEEE}
}

@article{wang2023contrast,
  title={Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series},
  author={Wang, Yihe and Han, Yu and Wang, Haishuai and Zhang, Xiang},
  journal={arXiv preprint arXiv:2310.14017},
  year={2023}
}

@article{munoz2021era5,
  title={ERA5-Land: A state-of-the-art global reanalysis dataset for land applications},
  author={Mu{\~n}oz-Sabater, Joaqu{\'\i}n and Dutra, Emanuel and Agust{\'\i}-Panareda, Anna and Albergel, Cl{\'e}ment and Arduini, Gabriele and Balsamo, Gianpaolo and Boussetta, Souhail and Choulga, Margarita and Harrigan, Shaun and Hersbach, Hans and others},
  journal={Earth system science data},
  volume={13},
  number={9},
  pages={4349--4383},
  year={2021},
  publisher={Copernicus GmbH}
}

@article{friedman1962interpolation,
  title={The interpolation of time series by related series},
  author={Friedman, Milton},
  journal={Journal of the American Statistical Association},
  volume={57},
  number={300},
  pages={729--757},
  year={1962},
  publisher={Taylor \& Francis}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{Paszke2019PyTorchAI,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Adam Paszke and S. Gross and Francisco Massa and A. Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Z. Lin and N. Gimelshein and L. Antiga and Alban Desmaison and Andreas K{\"o}pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  booktitle={NeurIPS},
  year={2019}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{xu2021anomaly,
  title={Anomaly transformer: Time series anomaly detection with association discrepancy},
  author={Xu, Jiehui and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2110.02642},
  year={2021}
}

@inproceedings{lai2018modeling,
  title={Modeling long-and short-term temporal patterns with deep neural networks},
  author={Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={95--104},
  year={2018}
}

@article{liu2022scinet,
  title={Scinet: Time series modeling and forecasting with sample convolution and interaction},
  author={Liu, Minhao and Zeng, Ailing and Chen, Muxi and Xu, Zhijian and Lai, Qiuxia and Ma, Lingna and Xu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5816--5828},
  year={2022}
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {ICLR},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wu2021current,
  title={Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress},
  author={Wu, Renjie and Keogh, Eamonn},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  publisher={IEEE}
}

@article{garza2023timegpt,
  title={TimeGPT-1},
  author={Garza, Azul and Mergenthaler-Canseco, Max},
  journal={arXiv preprint arXiv:2310.03589},
  year={2023}
}

@article{ansari2024chronos,
  title={Chronos: Learning the language of time series},
  author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others},
  journal={arXiv preprint arXiv:2403.07815},
  year={2024}
}

@article{goldberger2000physiobank,
  title={PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals},
  author={Goldberger, Ary L and Amaral, Luis AN and Glass, Leon and Hausdorff, Jeffrey M and Ivanov, Plamen Ch and Mark, Roger G and Mietus, Joseph E and Moody, George B and Peng, Chung-Kang and Stanley, H Eugene},
  journal={circulation},
  volume={101},
  number={23},
  pages={e215--e220},
  year={2000},
  publisher={Am Heart Assoc}
}

@article{woo2024unified,
  title={Unified training of universal time series forecasting transformers},
  author={Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2402.02592},
  year={2024}
}

@article{rasul2023lag,
  title={Lag-llama: Towards foundation models for time series forecasting},
  author={Rasul, Kashif and Ashok, Arjun and Williams, Andrew Robert and Khorasani, Arian and Adamopoulos, George and Bhagwatkar, Rishika and Bilo{\v{s}}, Marin and Ghonia, Hena and Hassen, Nadhir Vincent and Schneider, Anderson and others},
  journal={arXiv preprint arXiv:2310.08278},
  year={2023}
}

@article{dooley2024forecastpfn,
  title={Forecastpfn: Synthetically-trained zero-shot forecasting},
  author={Dooley, Samuel and Khurana, Gurnoor Singh and Mohapatra, Chirag and Naidu, Siddartha V and White, Colin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{liu2023largest,
  title={LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting},
  author={Liu, Xu and Xia, Yutong and Liang, Yuxuan and Hu, Junfeng and Wang, Yiwei and Bai, Lei and Huang, Chao and Liu, Zhenguang and Hooi, Bryan and Zimmermann, Roger},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{emami2023buildingsbench,
  title={BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting},
  author={Emami, Patrick and Sahu, Abhijeet and Graf, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{alexandrov2020gluonts,
  title={Gluonts: Probabilistic and neural time series modeling in python},
  author={Alexandrov, Alexander and Benidis, Konstantinos and Bohlke-Schneider, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C and Rangapuram, Syama and Salinas, David and Schulz, Jasper and others},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={116},
  pages={1--6},
  year={2020}
}

@article{libcitylong,
  title={LibCity: A Unified Library Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction}, 
  author={Jiang, Jiawei and Han, Chengkai and Jiang, Wenjun and Zhao, Wayne Xin and Wang, Jingyuan},
  journal={arXiv preprint arXiv:2304.14343},
  year={2023}
}

@article{wang2023benchmarks,
  title={Benchmarks and Custom Package for Electrical Load Forecasting},
  author={Wang, Zhixian and Wen, Qingsong and Zhang, Chaoli and Sun, Liang and Von Krannichfeldt, Leandro and Wang, Yi},
  journal={arXiv preprint arXiv:2307.07191},
  year={2023}
}

@article{mouatadid2024subseasonalclimateusa,
  title={SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking},
  author={Mouatadid, Soukayna and Orenstein, Paulo and Flaspohler, Genevieve and Oprescu, Miruna and Cohen, Judah and Wang, Franklyn and Knight, Sean and Geogdzhayeva, Maria and Levang, Sam and Fraenkel, Ernest and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhou2022sdwpf,
  title={Sdwpf: A dataset for spatial dynamic wind power forecasting challenge at kdd cup 2022},
  author={Zhou, Jingbo and Lu, Xinjiang and Xiao, Yixiong and Su, Jiantao and Lyu, Junfu and Ma, Yanjun and Dou, Dejing},
  journal={arXiv preprint arXiv:2208.04360},
  year={2022}
}

@article{mancuso2021machine,
  title={A machine learning approach for forecasting hierarchical time series},
  author={Mancuso, Paolo and Piccialli, Veronica and Sudoso, Antonio M},
  journal={Expert Systems with Applications},
  volume={182},
  pages={115102},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{zheng2015forecasting,
  title={Forecasting fine-grained air quality based on big data},
  author={Zheng, Yu and Yi, Xiuwen and Li, Ming and Li, Ruiyuan and Shan, Zhangqing and Chang, Eric and Li, Tianrui},
  booktitle={Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={2267--2276},
  year={2015}
}

@misc{misc_beijing_multi-site_air_quality_501,
  author       = {Chen,Song},
  title        = {{Beijing Multi-Site Air Quality}},
  year         = {2019},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5RK5G}
}

@misc{Bergmeir_C_2023,
  author       = {Bergmeir, Christoph and
                  Bui, Quang and
                  de Nijs, Frederik and
                  Stuckey, Peter},
  title        = {{Residential Power and Battery Data}},
  year         = 2023,
  publisher    = {Zenodo},
  version      = {1},
  doi          = {10.5281/zenodo.8219786},
  url          = {https://doi.org/10.5281/zenodo.8219786}
}

@misc{cdc_2017,
  author       = {CDC},
  title        = {Flu Portal Dashboard},
  year         = 2017,
  url          = {https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html},
  note         = {Accessed: [insert date of access]}
}

@article{van2018project,
  title={Project Tycho 2.0: a repository to improve the integration and reuse of data for global population health},
  author={van Panhuis, Willem G and Cross, Anne and Burke, Donald S},
  journal={Journal of the American Medical Informatics Association},
  volume={25},
  number={12},
  pages={1608--1617},
  year={2018},
  publisher={Oxford University Press}
}

@article{nguyen2024climatelearn,
  title={Climatelearn: Benchmarking machine learning for weather and climate modeling},
  author={Nguyen, Tung and Jewik, Jason and Bansal, Hritik and Sharma, Prakhar and Grover, Aditya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{goswami2024moment,
  title={MOMENT: A Family of Open Time-series Foundation Models},
  author={Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur},
  journal={arXiv preprint arXiv:2402.03885},
  year={2024}
}

@article{liu2024autotimes,
  title={AutoTimes: Autoregressive Time Series Forecasters via Large Language Models},
  author={Liu, Yong and Qin, Guo and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2402.02370},
  year={2024}
}

@misc{ecldata,
    title={{UCI Electricity Load Time Series Dataset}},
    howpublished={\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}}
}

@misc{weatherdata,
    title={{Weather Dataset}},
    howpublished={\url{https://www.bgc-jena.mpg.de/wetter/}}
}

@article{wang2024timexer,
  title={Timexer: Empowering transformers for time series forecasting with exogenous variables},
  author={Wang, Yuxuan and Wu, Haixu and Dong, Jiaxiang and Liu, Yong and Qiu, Yunzhong and Zhang, Haoran and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2402.19072},
  year={2024}
}

@inproceedings{ding2024unireplknet,
  title={UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio Video Point Cloud Time-Series and Image Recognition},
  author={Ding, Xiaohan and Zhang, Yiyuan and Ge, Yixiao and Zhao, Sijie and Song, Lin and Yue, Xiangyu and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5513--5524},
  year={2024}
}

@article{wu2023interpretable,
  title={Interpretable weather forecasting for worldwide stations with a unified deep model},
  author={Wu, Haixu and Zhou, Hang and Long, Mingsheng and Wang, Jianmin},
  journal={Nature Machine Intelligence},
  volume={5},
  number={6},
  pages={602--611},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{liutimer,
  title={Timer: Generative Pre-trained Transformers Are Large Time Series Models},
  author={Liu, Yong and Zhang, Haoran and Li, Chenyu and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, RJ},
  year={2018},
  publisher={OTexts}
}

@article{taylor2018forecasting,
  title={Forecasting at scale},
  author={Taylor, Sean J and Letham, Benjamin},
  journal={The American Statistician},
  volume={72},
  number={1},
  pages={37--45},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{salinas2020deepar,
  title={DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
  author={Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  journal={International journal of forecasting},
  volume={36},
  number={3},
  pages={1181--1191},
  year={2020},
  publisher={Elsevier}
}

@article{cao2020spectral,
  title={Spectral temporal graph neural network for multivariate time-series forecasting},
  author={Cao, Defu and Wang, Yujing and Duan, Juanyong and Zhang, Ce and Zhu, Xia and Huang, Congrui and Tong, Yunhai and Xu, Bixiong and Bai, Jing and Tong, Jie and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17766--17778},
  year={2020}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}

@article{wang2024beyond,
  title={Beyond the limits: A survey of techniques to extend the context length in large language models},
  author={Wang, Xindi and Salmani, Mahsa and Omidi, Parsa and Ren, Xiangyu and Rezagholizadeh, Mehdi and Eshaghi, Armaghan},
  journal={arXiv preprint arXiv:2402.02244},
  year={2024}
}

@inproceedings{kim2021reversible,
  title={Reversible instance normalization for accurate time-series forecasting against distribution shift},
  author={Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@inproceedings{zhang2023crossformer,
  title={Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author={Zhang, Yunhao and Yan, Junchi},
  booktitle={The eleventh international conference on learning representations},
  year={2023}
}

@article{li2019enhancing,
  title={Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting},
  author={Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{lim2021temporal,
  title={Temporal fusion transformers for interpretable multi-horizon time series forecasting},
  author={Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  journal={International Journal of Forecasting},
  volume={37},
  number={4},
  pages={1748--1764},
  year={2021},
  publisher={Elsevier}
}

@article{liu2024unitst,
  title={UniTST: Effectively Modeling Inter-Series and Intra-Series Dependencies for Multivariate Time Series Forecasting},
  author={Liu, Juncheng and Liu, Chenghao and Woo, Gerald and Wang, Yiwei and Hooi, Bryan and Xiong, Caiming and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2406.04975},
  year={2024}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4015--4026},
  year={2023}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@article{hersbach2020era5,
  title={The ERA5 global reanalysis},
  author={Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Hor{\'a}nyi, Andr{\'a}s and Mu{\~n}oz-Sabater, Joaqu{\'\i}n and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and others},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={146},
  number={730},
  pages={1999--2049},
  year={2020},
  publisher={Wiley Online Library}
}

@article{lago2021forecasting,
  title={Forecasting day-ahead electricity prices: A review of state-of-the-art algorithms, best practices and an open-access benchmark},
  author={Lago, Jesus and Marcjasz, Grzegorz and De Schutter, Bart and Weron, Rafa{\l}},
  journal={Applied Energy},
  volume={293},
  pages={116983},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{liu2021pyraformer,
  title={Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting},
  author={Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X and Dustdar, Schahram},
  booktitle={International conference on learning representations},
  year={2021}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{shi2024time,
  title={Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts},
  author={Shi, Xiaoming and Wang, Shiyu and Nie, Yuqi and Li, Dianqi and Ye, Zhou and Wen, Qingsong and Jin, Ming},
  journal={arXiv preprint arXiv:2409.16040},
  year={2024}
}

@article{wang2024deep,
  title={Deep Time Series Models: A Comprehensive Survey and Benchmark.(2024)},
  author={Wang, Yuxuan and Wu, Haixu and Dong, Jiaxiang and Liu, Yong and Long, Mingsheng and Wang, Jianmin},
  journal={URL https://arxiv. org/abs/2407.13278},
  volume={18},
  year={2024}
}

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009},
  publisher={IEEE}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={8821--8831},
  year={2021},
  organization={Pmlr}
}

@article{liu2024sora,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={arXiv preprint arXiv:2402.17177},
  year={2024}
}

@inproceedings{aksugift,
  title={GIFT-Eval: A Benchmark for General Time Series Forecasting Model Evaluation},
  author={Aksu, Taha and Woo, Gerald and Liu, Juncheng and Liu, Xu and Liu, Chenghao and Savarese, Silvio and Xiong, Caiming and Sahoo, Doyen},
  booktitle={NeurIPS Workshop on Time Series in the Age of Large Models},
  year={2024}
}

@inproceedings{rasul2021autoregressive,
  title={Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting},
  author={Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
  booktitle={International Conference on Machine Learning},
  pages={8857--8868},
  year={2021},
  organization={PMLR}
}

@article{tashiro2021csdi,
  title={Csdi: Conditional score-based diffusion models for probabilistic time series imputation},
  author={Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24804--24816},
  year={2021}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{tong2023improving,
  title={Improving and generalizing flow-based generative models with minibatch optimal transport},
  author={Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2302.00482},
  year={2023}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{liu2024timer,
  title={Timer-XL: Long-Context Transformers for Unified Time Series Forecasting},
  author={Liu, Yong and Qin, Guo and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2410.04803},
  year={2024}
}

@article{wen2017multi,
  title={A multi-horizon quantile recurrent forecaster},
  author={Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
  journal={arXiv preprint arXiv:1711.11053},
  year={2017}
}

@article{kollovieh2024flow,
  title={Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting},
  author={Kollovieh, Marcel and Lienen, Marten and L{\"u}dke, David and Schwinn, Leo and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2410.03024},
  year={2024}
}

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{lipman2024flow,
  title={Flow Matching Guide and Code},
  author={Lipman, Yaron and Havasi, Marton and Holderrieth, Peter and Shaul, Neta and Le, Matt and Karrer, Brian and Chen, Ricky TQ and Lopez-Paz, David and Ben-Hamu, Heli and Gat, Itai},
  journal={arXiv preprint arXiv:2412.06264},
  year={2024}
}

@article{li2024autoregressive,
  title={Autoregressive Image Generation without Vector Quantization},
  author={Li, Tianhong and Tian, Yonglong and Li, He and Deng, Mingyang and He, Kaiming},
  journal={arXiv preprint arXiv:2406.11838},
  year={2024}
}

@inproceedings{shen2023non,
  title={Non-autoregressive conditional diffusion models for time series prediction},
  author={Shen, Lifeng and Kwok, James},
  booktitle={International Conference on Machine Learning},
  pages={31016--31029},
  year={2023},
  organization={PMLR}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={606--624},
  year={2023}
}

@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@inproceedings{liang2024foundation,
  title={Foundation models for time series analysis: A tutorial and survey},
  author={Liang, Yuxuan and Wen, Haomin and Nie, Yuqi and Jiang, Yushan and Jin, Ming and Song, Dongjin and Pan, Shirui and Wen, Qingsong},
  booktitle={Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining},
  pages={6555--6565},
  year={2024}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@article{shi2024scaling,
  title={Scaling Law for Time Series Forecasting},
  author={Shi, Jingzhe and Ma, Qinwei and Ma, Huan and Li, Lei},
  journal={arXiv preprint arXiv:2405.15124},
  year={2024}
}

@article{gruver2024large,
  title={Large language models are zero-shot time series forecasters},
  author={Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{angelopoulos2021gentle,
  title={A gentle introduction to conformal prediction and distribution-free uncertainty quantification},
  author={Angelopoulos, Anastasios N and Bates, Stephen},
  journal={arXiv preprint arXiv:2107.07511},
  year={2021}
}

@article{chen2024visionts,
  title={Visionts: Visual masked autoencoders are free-lunch zero-shot time series forecasters},
  author={Chen, Mouxiang and Shen, Lefei and Li, Zhuo and Wang, Xiaoyun Joy and Sun, Jianling and Liu, Chenghao},
  journal={arXiv preprint arXiv:2408.17253},
  year={2024}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@misc{trafficdata,
    author = {{PEMS}},
    title={{Traffic Dataset}},
    howpublished={\url{http://pems.dot.ca.gov/}}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}