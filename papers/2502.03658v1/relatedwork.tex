\section{Related Works}
\label{sec:related}
In general, our work is related to pruning and dynamic sparse training. We will now provide a brief recap of both of the two topics and highlight our differences.\\
\textbf{Pruning.}
A central focus in many pruning studies is developing a \emph{pruning importance score} to identify redundant parameters from the model for removal. The majority of pruning methods evaluate weight magnitude~\cite{thimm1995evaluating, strom1997sparse,han2015deep, narang2017exploring,zhu2017prune,gale2019state} on well-performed pretrained dense weights as importance scores. 
% They can be considered as a single exploitation step in that they fix the sparse architecture without further exploring potentially better architecture. 
There have also been other importance criterion proposed such as Hessian-based scores~\cite{lecun1990optimal, hassibi1992second} and probability-based scores~\cite{zhou2021effective, srinivas2017training, molchanov2017variational}. In this paper, we study our method with weight magnitude metric~\cite{han2015deep} for unstructured sparsity. 

Some works, targeting structured sparsity, aim to prune convolutional filters~\cite{li2017pruning} or attention heads~\cite{michel2019sixteen}, thus enjoy immediate memory and computation benefit without specialized hardware and library support~\cite{han2016eie}. Exemplary channel importance criterion relied on metrics like weight norm~\cite{li2017pruning, chin2020towards, he2020learning, he2018soft, yang2018netadapt, sun2024towards}, Taylor expansion~\cite{lin2018accelerating, molchanov2019importance, you2019gate}, geometric median~\cite{he2019filter}, and feature maps rank~\cite{lin2020hrank}. Other works~\cite{chen2018constraint, shen2021halp, shen2023hardware} including the very recent HALP~\cite{shen2021halp} consider channel pruning under a latency or FLOPs constraint, aiming for more hardware-friendly structured sparse architecture with practical speed-up. In our structured sparsity experiment, we study our method with Taylor score~\cite{molchanov2019importance} and leverage the hardware-aware pruning scheme developed in HALP to directly achieve a sparse model optimized in inference latency.

% with only a small number of data batches
\noindent\textbf{Dynamic Sparse Training.}
Albeit the decent performance of pruning on pretrained models, the dense model pretraining is usually computationally demanding and redundant. Moreover, once parameters are pruned, they cannot be recovered for reassessment and potential improvement of the sparse structure later in training. Towards addressing these goals,  a group of works~\cite{mocanu2018scalable, dettmers2019sparse, mostafa2019parameter, kusupati2020soft, wortsman2019discovering, dai2019nest, evci2020rigging, lin2020dynamic, ma2021effective, yuan2020growing, liusw2021sparse, yuan2021mest, liu2021we, yuan2022layer, hou2022chex, lasby2023dynamic}, usually referred to as \textit{Dynamic Sparse Training}, have considered the idea of repeated alternating prune-grow sessions to dynamically configure the sparsity structure through training from scratch, giving the model more flexibility. A key aspect of these approaches is the development of a \emph{growing importance score} to identify and grow back the prematurely pruned parameters. For instance, RigL, a pivotal study in this field, uses magnitude-based pruning but suggests regrowing parameters based on immediate gradients of zeroed and pruned weights, greedily optimizing their effectiveness for the subsequent data batch's gradient descent. Building on RigL's success, various adaptations have emerged, such as starting with lower sparsity~\cite{liusw2021sparse} or smaller batch size and longer update intervals~\cite{liu2021we}, or incorporating layer freezing and data sieving techniques\cite{yuan2021mest, yuan2022layer}, aiming to boost accuracy or reduce training costs. Despite advancements, the prevalent reliance on greedy exploration strategy undermines the long-term quality of the resulting model. Moreover, the discrepancy in the criteria for pruning and growing could lead to a suboptimal exploration of new architectures, where newly grown parameters are mostly pruned before being fully exploited.

Implementing structured sparsity in dynamic sparse training has been minimally addressed in prior work due to the challenge that gradients over pruned channels lead to zero values following chain-rule in backpropagation, making previous growth criteria infeasible. Existing solutions~\cite{he2018soft, kang2020operation, lym2019prunetrain, yuan2020growing, lasby2023dynamic} use alternatives to prune-grow, such as soft-masking~\cite{he2018soft, kang2020operation} and group-lasso regularization~\cite{lym2019prunetrain}. While the recent SCS~\cite{yuan2020growing} incorporates structured parameter exploration into optimization and uses continuous sparsification, it still relies on dense gradients throughout training, limiting its practicality and the possibility of a sparsified backward pass.

Like many dynamic sparse training approaches, our method also employs a prune-grow dynamics to update the structure. However, a key distinction is that we use the same given importance criterion for both pruning and growing parameters for either unstructured or structured sparsity without dense gradients reliance during the training process.