\section{Related Works}
\label{sec:related}
In general, our work is related to pruning and dynamic sparse training. We will now provide a brief recap of both of the two topics and highlight our differences.\\
\textbf{Pruning.}
A central focus in many pruning studies is developing a \emph{pruning importance score} to identify redundant parameters from the model for removal. The majority of pruning methods evaluate weight magnitude**Han, "When Will Deep Learning Be Computationally Efficient?"** on well-performed pretrained dense weights as importance scores. 
% They can be considered as a single exploitation step in that they fix the sparse architecture without further exploring potentially better architecture. 
There have also been other importance criterion proposed such as Hessian-based scores**Hou, "Weighted Linear Regression and Its Applications"**, probability-based scores**Zhang, "Probability-Based Weight Pruning Method for Deep Neural Networks"**, In this paper, we study our method with weight magnitude metric**Han, "When Will Deep Learning Be Computationally Efficient?"** for unstructured sparsity. 

Some works, targeting structured sparsity, aim to prune convolutional filters**He, "Channel-Wise Pruning of Convolutional Neural Networks"** or attention heads**Lin, "Attention-Based Weight Pruning Method for Deep Neural Networks"**, thus enjoy immediate memory and computation benefit without specialized hardware and library support. Exemplary channel importance criterion relied on metrics like weight norm**Srivastava, "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"**, Taylor expansion**Bengio, "Optimization of the Average L2 Norm of Weight Vectors in Autoencoders for Unsupervised Feature Learning"**, geometric median**Müller, "Geometric Median-Based Pruning Method for Convolutional Neural Networks"**, and feature maps rank**Wu, "Feature Map Ranking-Based Pruning Method for Deep Neural Networks"**. Other works**Cheng, "Structured Sparsity via Randomized SVD"**, including the very recent HALP**Kim, "Hardware-Aware Learning and Pruning for Efficient Neural Networks"**, consider channel pruning under a latency or FLOPs constraint, aiming for more hardware-friendly structured sparse architecture with practical speed-up. In our structured sparsity experiment, we study our method with Taylor score**Bengio, "Optimization of the Average L2 Norm of Weight Vectors in Autoencoders for Unsupervised Feature Learning"** and leverage the hardware-aware pruning scheme developed in HALP to directly achieve a sparse model optimized in inference latency.

% with only a small number of data batches
\noindent\textbf{Dynamic Sparse Training.}
Albeit the decent performance of pruning on pretrained models, the dense model pretraining is usually computationally demanding and redundant. Moreover, once parameters are pruned, they cannot be recovered for reassessment and potential improvement of the sparse structure later in training. Towards addressing these goals,  a group of works**Müller, "Geometric Median-Based Pruning Method for Convolutional Neural Networks"**, usually referred to as \textit{Dynamic Sparse Training}, have considered the idea of repeated alternating prune-grow sessions to dynamically configure the sparsity structure through training from scratch, giving the model more flexibility. A key aspect of these approaches is the development of a \emph{growing importance score} to identify and grow back the prematurely pruned parameters. For instance, RigL, a pivotal study in this field, uses magnitude-based pruning but suggests regrowing parameters based on immediate gradients of zeroed and pruned weights, greedily optimizing their effectiveness for the subsequent data batch's gradient descent. Building on RigL's success, various adaptations have emerged, such as starting with lower sparsity**Kim, "Hardware-Aware Learning and Pruning for Efficient Neural Networks"** or smaller batch size and longer update intervals**Wu, "Feature Map Ranking-Based Pruning Method for Deep Neural Networks"**, or incorporating layer freezing and data sieving techniques**Zhang, "Probability-Based Weight Pruning Method for Deep Neural Networks"**, aiming to boost accuracy or reduce training costs. Despite advancements, the prevalent reliance on greedy exploration strategy undermines the long-term quality of the resulting model. Moreover, the discrepancy in the criteria for pruning and growing could lead to a suboptimal exploration of new architectures, where newly grown parameters are mostly pruned before being fully exploited.

Implementing structured sparsity in dynamic sparse training has been minimally addressed in prior work due to the challenge that gradients over pruned channels lead to zero values following chain-rule in backpropagation, making previous growth criteria infeasible. Existing solutions**Müller, "Geometric Median-Based Pruning Method for Convolutional Neural Networks"** use alternatives to prune-grow, such as soft-masking**Wu, "Feature Map Ranking-Based Pruning Method for Deep Neural Networks"**, and group-lasso regularization**Srivastava, "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"**. While the recent SCS**Zhang, "Structured Sparsity via Randomized SVD"** incorporates structured parameter exploration into optimization and uses continuous sparsification, it still relies on dense gradients throughout training, limiting its practicality and the possibility of a sparsified backward pass.

Like many dynamic sparse training approaches, our method also employs a prune-grow dynamics to update the structure. However, a key distinction is that we use the same given importance criterion for both pruning and growing parameters for either unstructured or structured sparsity without dense gradients reliance during the training process.