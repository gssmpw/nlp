% Long form of conference & journal abbreviations -- especially for camera ready
@String(PAMI  = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV  = {Int. J. Comput. Vis.})
@String(CVPR  = {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV  = {Int. Conf. Comput. Vis.})
@String(ECCV  = {Eur. Conf. Comput. Vis.})
@String(NeurIPS = {Adv. Neural Inform. Process. Syst.})
@String(ICML  = {Int. Conf. Mach. Learn.})
@String(ICLR  = {Int. Conf. Learn. Represent.})
@String(ACCV  = {Asian Conf. Comput. Vis.})
@String(BMVC  = {Brit. Mach. Vis. Conf.})
@String(CVPRW = {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(AAAI  = {AAAI})
@String(IJCAI = {IJCAI})
@String(ICIP  = {IEEE Int. Conf. Image Process.})
@String(ICPR  = {Int. Conf. Pattern Recog.})
@String(ICASSP=	{ICASSP})
@String(ICME  = {Int. Conf. Multimedia and Expo})
@String(JMLR  = {J. Mach. Learn. Res.})
@String(TMLR  = {Trans. Mach. Learn Res.})
@String(TOG   = {ACM Trans. Graph.})
@String(TIP   = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TCSVT = {IEEE Trans. Circuit Syst. Video Technol.})
@String(TMM   = {IEEE Trans. Multimedia})
@String(ACMMM = {ACM Int. Conf. Multimedia})
@String(PR    = {Pattern Recognition})

@String(MNI	  = {Nature Mach. Intell.})
@String(SPL	  = {IEEE Sign. Process. Letters})
@String(VR    = {Vis. Res.})
@String(JOV	  = {J. Vis.})
@String(TVC   = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF   = {Comput. Graph. Forum})
@String(CVM   = {Computational Visual Media})


% Short form of conference & journal abbreviations -- especially for submission version
% if desired, remove these macros in favor of the above ones
@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NeurIPS = {NeurIPS})
@String(ICML  = {ICML})
@String(ICLR  = {ICLR})
@String(ACCV  = {ACCV})
@String(BMVC  =	{BMVC})
@String(CVPRW = {CVPRW})
@String(AAAI  = {AAAI})
@String(IJCAI = {IJCAI})
@String(ICIP  = {ICIP})
@String(ICPR  = {ICPR})
@String(ICASSP=	{ICASSP})
@String(ICME  =	{ICME})
@String(JMLR  = {JMLR})
@String(TMLR  = {TMLR})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(PR    = {PR})



@inproceedings{shen2021halp,
    title={Structural Pruning via Latency-Saliency Knapsack},
    author={Shen, Maying and Yin, Hongxu and Molchanov, Pavlo and Mao, Lei and Liu, Jianna and Alvarez, Jose},
    booktitle={Advances in Neural Information Processing Systems},
    year={2022}
}
@inproceedings{humble2022soft,
  title={Soft Masking for Cost-Constrained Channel Pruning},
  author={Humble, Ryan and Shen, Maying and Latorre, Jorge Albericio and Darve, Eric and Alvarez, Jose},
  booktitle={European Conference on Computer Vision},
  pages={641--657},
  year={2022},
  organization={Springer}
}
@inproceedings{alvarez2016learning,
  title={Learning the number of neurons in deep networks},
  author={Alvarez, Jose M and Salzmann, Mathieu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2270--2278},
  year={2016}
}
@article{shen2021when,
  title={When To Prune? A Policy Towards Early Structural Pruning},
  author={Shen, Maying and Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M},
  journal={CVPR},
  year={2022}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{han2015learning,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William J},
  booktitle={NeurIPS},
  year={2015}
}

@article{park2016holistic,
  title={Holistic sparsecnn: Forging the trident of accuracy, speed, and size},
  author={Park, Jongsoo and Li, Sheng R and Wen, Wei and Li, Hai and Chen, Yiran and Dubey, Pradeep},
  journal={arXiv preprint arXiv:1608.01409},
  volume={1},
  number={2},
  year={2016}
}

@inproceedings{elsen2020fast,
  title={Fast sparse convnets},
  author={Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  booktitle={CVPR},
  pages={14629--14638},
  year={2020}
}

@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={ICML},
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@inproceedings{thimm1995evaluating,
  title={Evaluating pruning methods},
  author={Thimm, Georg and Fiesler, Emile},
  booktitle={ICANN},
  pages={20--25},
  year={1995},
  organization={Citeseer}
}

@inproceedings{strom1997sparse,
  title={Sparse connection and pruning in large dynamic artificial neural networks},
  author={Str{\"o}m, Nikko},
  booktitle={Fifth European Conference on Speech Communication and Technology},
  year={1997},
  organization={Citeseer}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={ICLR},
  year={2015}
}

@article{han2016eie,
  title={EIE: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={SIGARCH},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={NeurIPS},
  pages={598--605},
  year={1990}
}

@article{hassibi1992second,
  title={Second order derivatives for network pruning: Optimal Brain Surgeon},
  author={Hassibi, Babak and Stork, David},
  journal={NeurIPS},
  volume={5},
  year={1992}
}

@inproceedings{mozer1989skeletonization,
  title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
  author={Mozer, Michael C and Smolensky, Paul},
  booktitle={NeurIPS},
  pages={107--115},
  year={1989}
}

@article{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  journal={NeurIPS},
  year={2016}
}

@inproceedings{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  booktitle={ICML},
  pages={4646--4655},
  year={2019},
  organization={PMLR}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@inproceedings{lee2018snip,
  title={SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{lubana2020gradient,
  title={A Gradient Flow Framework For Analyzing Network Pruning},
  author={Lubana, Ekdeep Singh and Dick, Robert},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{zhou2019deconstructing,
  title={Deconstructing lottery tickets: zeros, signs, and the supermask},
  author={Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  booktitle={ICNIP},
  pages={3597--3607},
  year={2019}
}

@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  year={2018},  
}

@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={CVPR},
  pages={11264--11272},
  year={2019}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}
@article{liu2021sparse,
  title={Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware},
  author={Liu et al.},
  journal={Neural Computing and Applications},
  volume={33},
  number={7},
  pages={2589--2604},
  year={2021},
  publisher={Springer}
}


@inproceedings{wang2019picking,
  title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={ICML},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={NeurIPS},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@inproceedings{liu2016ssd,
  title={Ssd: Single shot multibox detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  booktitle={ECCV},
  pages={21--37},
  year={2016},
  organization={Springer}
}

@article{chen2017deeplab,
  title={Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs},
  author={Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  journal={TPAMI},
  volume={40},
  number={4},
  pages={834--848},
  year={2017},
  publisher={IEEE}
}

@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={CVPR},
  pages={3431--3440},
  year={2015}
}

@article{theis2018faster,
  title={Faster gaze prediction with dense networks and fisher pruning},
  author={Theis, Lucas and Korshunova, Iryna and Tejani, Alykhan and Husz{\'a}r, Ferenc},
  journal={arXiv preprint arXiv:1801.05787},
  year={2018}
}

@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={ICCV},
  pages={2961--2969},
  year={2017}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{chen2015compressing,
  title={Compressing neural networks with the hashing trick},
  author={Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
  booktitle={ICML},
  pages={2285--2294},
  year={2015},
  organization={PMLR}
}

@inproceedings{liu2018bi,
  title={Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm},
  author={Liu, Zechun and Wu, Baoyuan and Luo, Wenhan and Yang, Xin and Liu, Wei and Cheng, Kwang-Ting},
  booktitle={ECCV},
  pages={722--737},
  year={2018}
}

@inproceedings{yu2019any,
  title={Any-precision deep neural networks},
  author={Yu, Haichao and Li, Haoxiang and Shi, Humphrey and Huang, Thomas S and Hua, Gang},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{denton2014exploiting,
  author = {Denton, Emily L. and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  booktitle = {NeurIPS},
  pages = {1269-1277},
  title = {Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation.},
  year = 2014
}

@inproceedings{zhang2015efficient,
  title={Efficient and accurate approximations of nonlinear convolutional networks},
  author={Zhang, Xiangyu and Zou, Jianhua and Ming, Xiang and He, Kaiming and Sun, Jian},
  booktitle={ICCV},
  pages={1984--1992},
  year={2015}
}

@article{lin2018holistic,
  title={Holistic cnn compression via low-rank decomposition with knowledge transfer},
  author={Lin, Shaohui and Ji, Rongrong and Chen, Chao and Tao, Dacheng and Luo, Jiebo},
  journal={TPAMI},
  volume={41},
  number={12},
  pages={2889--2905},
  year={2018},
  publisher={IEEE}
}

@article{deng2020survey,
  author={L. {Deng} and G. {Li} and S. {Han} and L. {Shi} and Y. {Xie}},
  journal={Proceedings of the IEEE}, 
  title={Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey}, 
  year={2020},
  volume={108},
  number={4},
  pages={485-532}
}

@inproceedings{li2017pruning,
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  booktitle = {ICLR},
  title = {Pruning Filters for Efficient ConvNets.},
  year = 2017
}

@article{hinton2015distilling,
  author = {Hinton, Geoffrey E. and Vinyals, Oriol and Dean, Jeffrey},
  journal = {NeurIPS},
  title = {Distilling the Knowledge in a Neural Network.},
  year = 2014
}

@inproceedings{jiao2019geometry,
  title={Geometry-aware distillation for indoor semantic segmentation},
  author={Jiao, Jianbo and Wei, Yunchao and Jie, Zequn and Shi, Honghui and Lau, Rynson WH and Huang, Thomas S},
  booktitle={CVPR},
  pages={2869--2878},
  year={2019}
}

@inproceedings{liu2019structured,
  title={Structured knowledge distillation for semantic segmentation},
  author={Liu, Yifan and Chen, Ke and Liu, Chris and Qin, Zengchang and Luo, Zhenbo and Wang, Jingdong},
  booktitle={CVPR},
  pages={2604--2613},
  year={2019}
}

@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={CVPR workshops},
  pages={138--145},
  year={2017}
}

@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@inproceedings{dvornik2017blitznet,
  title={Blitznet: A real-time deep network for scene understanding},
  author={Dvornik, Nikita and Shmelkov, Konstantin and Mairal, Julien and Schmid, Cordelia},
  booktitle={ICCV},
  pages={4154--4162},
  year={2017}
}

@inproceedings{jou2016deep,
  title={Deep cross residual learning for multitask visual recognition},
  author={Jou, Brendan and Chang, Shih-Fu},
  booktitle={ACM ICM},
  pages={998--1007},
  year={2016}
}

@article{ranjan2017hyperface,
  title={Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition},
  author={Ranjan, Rajeev and Patel, Vishal M and Chellappa, Rama},
  journal={TPAMI},
  volume={41},
  number={1},
  pages={121--135},
  year={2017},
  publisher={IEEE}
}

@article{bousmalis2016domain,
  title={Domain separation networks},
  author={Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
  journal={NeurIPS},
  volume={29},
  pages={343--351},
  year={2016}
}

@article{liu2017adversarial,
  title={Adversarial multi-task learning for text classification},
  author={Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1704.05742},
  year={2017}
}

@article{suteu2019regularizing,
  title={Regularizing deep multi-task networks using orthogonal gradients},
  author={Suteu, Mihai and Guo, Yike},
  journal={arXiv preprint arXiv:1912.06844},
  year={2019}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={CVPR},
  pages={4700--4708},
  year={2017}
}

@inproceedings{cordts2016cityscapes,
  title={The cityscapes dataset for semantic urban scene understanding},
  author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  booktitle={CVPR},
  pages={3213--3223},
  year={2016}
}

@inproceedings{zamir2018taskonomy,
  title={Taskonomy: Disentangling task transfer learning},
  author={Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas J and Malik, Jitendra and Savarese, Silvio},
  booktitle={CVPR},
  pages={3712--3722},
  year={2018}
}

@inproceedings{silbermanECCV12,
  author    = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},
  title     = {Indoor Segmentation and Support Inference from RGBD Images},
  booktitle = {ECCV},
  year      = {2012}
}

@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={NeurIPS},
  volume={32},
  pages={14014--14024},
  year={2019}
}

@inproceedings{ding2021resrep,
  title={ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting},
  author={Ding, Xiaohan and Hao, Tianxiang and Tan, Jianchao and Liu, Ji and Han, Jungong and Guo, Yuchen and Ding, Guiguang},
  booktitle={ICCV},
  pages={4510--4520},
  year={2021}
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={ICML},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2021pruning,
  title={Pruning-Aware Merging for Efficient Multitask Inference},
  author={He, Xiaoxi and Gao, Dawei and Zhou, Zimu and Tong, Yongxin and Thiele, Lothar},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={585--595},
  year={2021}
}

@article{cheng2021multi,
  title={Multi-task Pruning via Filter Index Sharing: A Many-Objective Optimization Approach},
  author={Cheng, Hanjing and Wang, Zidong and Ma, Lifeng and Liu, Xiaohui and Wei, Zhihui},
  journal={Cognitive Computation},
  volume={13},
  number={4},
  pages={1070--1084},
  year={2021},
  publisher={Springer}
}

@article{he2018multi,
  title={Multi-task zipping via layer-wise neuron sharing},
  author={He, Xiaoxi and Zhou, Zimu and Thiele, Lothar},
  journal={arXiv preprint arXiv:1805.09791},
  year={2018}
}

@inproceedings{misra2016cross,
  title={Cross-stitch networks for multi-task learning},
  author={Misra, Ishan and Shrivastava, Abhinav and Gupta, Abhinav and Hebert, Martial},
  booktitle={CVPR},
  pages={3994--4003},
  year={2016}
}

@inproceedings{gao2019nddr,
  title={Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction},
  author={Gao, Yuan and Ma, Jiayi and Zhao, Mingbo and Liu, Wei and Yuille, Alan L},
  booktitle={CVPR},
  pages={3205--3214},
  year={2019}
}

@article{sun2019adashare,
  title={Adashare: Learning what to share for efficient deep multi-task learning},
  author={Sun, Ximeng and Panda, Rameswar and Feris, Rogerio and Saenko, Kate},
  journal={arXiv preprint arXiv:1911.12423},
  year={2019}
}

@inproceedings{standley2020tasks,
  title={Which tasks should be learned together in multi-task learning?},
  author={Standley, Trevor and Zamir, Amir and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
  booktitle={ICML},
  pages={9120--9132},
  year={2020},
  organization={PMLR}
}

@inproceedings{eigen2015predicting,
  title={Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture},
  author={Eigen, David and Fergus, Rob},
  booktitle={ICCV},
  pages={2650--2658},
  year={2015}
}

@article{eigen2014depth,
  title={Depth map prediction from a single image using a multi-scale deep network},
  author={Eigen, David and Puhrsch, Christian and Fergus, Rob},
  journal={arXiv preprint arXiv:1406.2283},
  year={2014}
}

@article{morcos2019one,
  title={One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
  author={Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  journal={NeurIPS},
  volume={32},
  pages={4932--4942},
  year={2019}
}

@article{paganini2020bespoke,
  title={Bespoke vs. Pr$\backslash$\^{} et-$\backslash$a-Porter Lottery Tickets: Exploiting Mask Similarity for Trainable Sub-Network Finding},
  author={Paganini, Michela and Forde, Jessica Zosa},
  journal={arXiv preprint arXiv:2007.04091},
  year={2020}
}

@inproceedings{sabatelli2021transferability,
  title={On the Transferability of Winning Tickets in Non-Natural Image Datasets},
  author={Sabatelli, Matthia and Kestemont, Mike and Pierre, Geurts},
  booktitle={16th International Conference on Computer Vision Theory and Applications-VISAPP 2021},
  year={2021}
}

@inproceedings{ruder2019latent,
  title={Latent multi-task architecture learning},
  author={Ruder, Sebastian and Bingel, Joachim and Augenstein, Isabelle and S{\o}gaard, Anders},
  booktitle={AAAI},
  pages={4822--4829},
  year={2019}
}

@inproceedings{ahn2019deep,
  title={Deep elastic networks with model selection for multi-task learning},
  author={Ahn, Chanho and Kim, Eunwoo and Oh, Songhwai},
  booktitle={ICCV},
  pages={6529--6538},
  year={2019}
}

@inproceedings{sun2022disparse,
  title={DiSparse: Disentangled Sparsification for Multitask Model Compression},
  author={Sun, Xinglong and Hassani, Ali and Wang, Zhangyang and Huang, Gao and Shi, Humphrey},
  booktitle={CVPR},
  pages={12382--12392},
  year={2022}
}

@article{bellec2017deep,
  title={Deep rewiring: Training very sparse deep networks},
  author={Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  journal={ICLR},
  year={2018}
}

@article{lin2020dynamic,
  title={Dynamic model pruning with feedback},
  author={Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin},
  journal={ICLR},
  year={2020}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@inproceedings{li2020eagleeye,
  title={EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning},
  author={Li, Bailin and Wu, Bowen and Su, Jiang and Wang, Guangrun},
  booktitle={ECCV},
  pages={639--654},
  year={2020}
}

@inproceedings{zagoruyko2016wide,
  title={Wide Residual Networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={BMVC},
  year={2016},
  organization={British Machine Vision Association}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  pages={770--778},
  year={2016}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}


@article{narang2017exploring,
  title={Exploring sparsity in recurrent neural networks},
  author={Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  journal={ICLR},
  year={2017}
}

@inproceedings{zhou2021effective,
  title={Effective sparsification of neural networks with global sparsity constraint},
  author={Zhou, Xiao and Zhang, Weizhong and Xu, Hang and Zhang, Tong},
  booktitle={CVPR},
  pages={3599--3608},
  year={2021}
}

@inproceedings{louizos2018learning,
  title={Learning Sparse Neural Networks through L\_0 Regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{zhang2021unified,
  title={A unified dnn weight pruning framework using reweighted optimization methods},
  author={Zhang, Tianyun and Ma, Xiaolong and Zhan, Zheng and Zhou, Shanglin and Ding, Caiwen and Fardad, Makan and Wang, Yanzhi},
  booktitle={DAC},
  pages={493--498},
  year={2021},
  organization={IEEE}
}

@article{tartaglione2018learning,
  title={Learning sparse neural networks via sensitivity-driven regularization},
  author={Tartaglione, Enzo and Leps{\o}y, Skjalg and Fiandrotti, Attilio and Francini, Gianluca},
  journal={NeurIPS},
  volume={31},
  year={2018}
}

@inproceedings{wang2021neural,
  title={Neural Pruning via Growing Regularization},
  author={Wang, Huan and Qin, Can and Zhang, Yulun and Fu, Yun},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{ning2020dsa,
  title={Dsa: More efficient budgeted pruning via differentiable sparsity allocation},
  author={Ning, Xuefei and Zhao, Tianchen and Li, Wenshuo and Lei, Peng and Wang, Yu and Yang, Huazhong},
  booktitle={ECCV},
  pages={592--607},
  year={2020},
  organization={Springer}
}

@article{you2019gate,
  title={Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks},
  author={You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{tang2020scop,
  title={Scop: Scientific control for reliable neural network pruning},
  author={Tang, Yehui and Wang, Yunhe and Xu, Yixing and Tao, Dacheng and Xu, Chunjing and Xu, Chao and Xu, Chang},
  journal={NeurIPS},
  volume={33},
  pages={10936--10947},
  year={2020}
}

@article{dong2019network,
  title={Network pruning via transformable architecture search},
  author={Dong, Xuanyi and Yang, Yi},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@inproceedings{liu2019metapruning,
  title={Metapruning: Meta learning for automatic neural network channel pruning},
  author={Liu, Zechun and Mu, Haoyuan and Zhang, Xiangyu and Guo, Zichao and Yang, Xin and Cheng, Kwang-Ting and Sun, Jian},
  booktitle={ICCV},
  pages={3296--3305},
  year={2019}
}

@article{yuan2020growing,
  title={Growing efficient deep networks by structured continuous sparsification},
  author={Yuan, Xin and Savarese, Pedro and Maire, Michael},
  journal={ICLR},
  year={2021}
}

@inproceedings{li2021dynamic,
  title={Dynamic slimmable network},
  author={Li, Changlin and Wang, Guangrun and Wang, Bing and Liang, Xiaodan and Li, Zhihui and Chang, Xiaojun},
  booktitle={CVPR},
  pages={8607--8617},
  year={2021}
}

@article{zhuang2020neuron,
  title={Neuron-level structured pruning using polarization regularizer},
  author={Zhuang, Tao and Zhang, Zhixuan and Huang, Yuheng and Zeng, Xiaoyi and Shuang, Kai and Li, Xiang},
  journal={NeurIPS},
  volume={33},
  pages={9865--9877},
  year={2020}
}

@inproceedings{guo2020dmcp,
  title={Dmcp: Differentiable markov channel pruning for neural networks},
  author={Guo, Shaopeng and Wang, Yujie and Li, Quanquan and Yan, Junjie},
  booktitle={CVPR},
  pages={1539--1547},
  year={2020}
}

@inproceedings{chin2020towards,
  title={Towards efficient model compression via learned global ranking},
  author={Chin, Ting-Wu and Ding, Ruizhou and Zhang, Cha and Marculescu, Diana},
  booktitle={CVPR},
  pages={1518--1528},
  year={2020}
}

@inproceedings{he2020learning,
  title={Learning filter pruning criteria for deep convolutional neural networks acceleration},
  author={He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi},
  booktitle={CVPR},
  pages={2009--2018},
  year={2020}
}

@inproceedings{he2018soft,
  title={Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
  author={He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
  booktitle={IJCAI},
  year={2018}
}

@inproceedings{yang2018netadapt,
  title={Netadapt: Platform-aware neural network adaptation for mobile applications},
  author={Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
  booktitle={ECCV},
  pages={285--300},
  year={2018}
}

@inproceedings{lin2018accelerating,
  title={Accelerating Convolutional Networks via Global \& Dynamic Filter Pruning.},
  author={Lin, Shaohui and Ji, Rongrong and Li, Yuchao and Wu, Yongjian and Huang, Feiyue and Zhang, Baochang},
  booktitle={IJCAI},
  year={2018},
  organization={Stockholm}
}

@inproceedings{he2019filter,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle={CVPR},
  pages={4340--4349},
  year={2019}
}

@inproceedings{lin2020hrank,
  title={Hrank: Filter pruning using high-rank feature map},
  author={Lin, Mingbao and Ji, Rongrong and Wang, Yan and Zhang, Yichen and Zhang, Baochang and Tian, Yonghong and Shao, Ling},
  booktitle={CVPR},
  pages={1529--1538},
  year={2020}
}

@inproceedings{wu2020constraint,
  title={Constraint-aware importance estimation for global filter pruning under multiple resource constraints},
  author={Wu, Yu-Cheng and Liu, Chih-Ting and Chen, Bo-Ying and Chien, Shao-Yi},
  booktitle={CVPR Workshops},
  pages={686--687},
  year={2020}
}

@inproceedings{tan2019mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle={CVPR},
  pages={2820--2828},
  year={2019}
}

@inproceedings{chen2018constraint,
  title={Constraint-aware deep neural network compression},
  author={Chen, Changan and Tung, Frederick and Vedula, Naveen and Mori, Greg},
  booktitle={ECCV},
  pages={400--415},
  year={2018}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal={NeurIPS},
  volume={33},
  pages={6377--6389},
  year={2020}
}

@inproceedings{wimmer2020freezenet,
  title={Freezenet: Full performance by reduced storage costs},
  author={Wimmer, Paul and Mehnert, Jens and Condurache, Alexandru},
  booktitle={ACCV},
  year={2020}
}

@article{van2020single,
  title={Single shot structured pruning before training},
  author={van Amersfoort, Joost and Alizadeh, Milad and Farquhar, Sebastian and Lane, Nicholas and Gal, Yarin},
  journal={arXiv preprint arXiv:2007.00389},
  year={2020}
}

@inproceedings{liu2021lottery,
  title={Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?},
  author={Liu, Ning and Yuan, Geng and Che, Zhengping and Shen, Xuan and Ma, Xiaolong and Jin, Qing and Ren, Jian and Tang, Jian and Liu, Sijia and Wang, Yanzhi},
  booktitle={ICML},
  pages={7011--7020},
  year={2021},
  organization={PMLR}
}

@article{verdenius2020pruning,
  title={Pruning via iterative ranking of sensitivity statistics},
  author={Verdenius, Stijn and Stol, Maarten and Forr{\'e}, Patrick},
  journal={arXiv preprint arXiv:2006.00896},
  year={2020}
}

@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle={ICML},
  pages={5544--5555},
  year={2020},
  organization={PMLR}
}

@article{wortsman2019discovering,
  title={Discovering neural wirings},
  author={Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{dai2019nest,
  title={NeST: A neural network synthesis tool based on a grow-and-prune paradigm},
  author={Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K},
  journal={IEEE Transactions on Computers},
  volume={68},
  number={10},
  pages={1487--1497},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ma2021effective,
  title={Effective Model Sparsification by Scheduled Grow-and-Prune Methods},
  author={Ma, Xiaolong and Qin, Minghai and Sun, Fei and Hou, Zejiang and Yuan, Kun and Xu, Yi and Wang, Yanzhi and Chen, Yen-Kuang and Jin, Rong and Xie, Yuan},
  booktitle={ICLR},
  year={2021}
}

@misc{nvidia2020a100,
  title={A100 Tensor Core GPU architecture},
  author={Nvidia, Nvidia},
  year={2020},
  publisher={Accessed}
}

@article{mishra2021accelerating,
  title={Accelerating sparse deep neural networks},
  author={Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2104.08378},
  year={2021}
}

@article{zhou2021learning,
  title={Learning n: m fine-grained structured sparse neural networks from scratch},
  author={Zhou, Aojun and Ma, Yukun and Zhu, Junnan and Liu, Jianbo and Zhang, Zhijie and Yuan, Kun and Sun, Wenxiu and Li, Hongsheng},
  journal={ICLR},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{machado2015domain,
  title={Domain-independent optimistic initialization for reinforcement learning},
  author={Machado, Marlos C and Srinivasan, Sriram and Bowling, Michael},
  booktitle={AAAI},
  year={2015}
}

@inproceedings{lobel2022optimistic,
  title={Optimistic Initialization for Exploration in Continuous Control},
  author={Lobel, Sam and Gottesman, Omer and Allen, Cameron and Bagaria, Akhil and Konidaris, George},
  booktitle={AAAI},
  pages={7612--7619},
  year={2022}
}

@article{yu2019autoslim,
  title={Autoslim: Towards one-shot architecture search for channel numbers},
  author={Yu, Jiahui and Huang, Thomas},
  journal={NeurIPS Workshop},
  year={2019}
}

@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={ECCV},
  pages={784--800},
  year={2018}
}

@article{de2020progressive,
  title={Progressive skeletonization: Trimming more fat from a network at initialization},
  author={de Jorge, Pau and Sanyal, Amartya and Behl, Harkirat S and Torr, Philip HS and Rogez, Gregory and Dokania, Puneet K},
  journal={ICLR},
  year={2021}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={IJCV},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017},
  journal={NeurIPS Workshop}
}

@article{nvidia2020,
  title={Nvidia. Convolutional Networks for Image Classification in PyTorch.},
  author={NVIDIA}
}

@article{wei2020theoretical,
  title={Theoretical analysis of self-training with deep networks on unlabeled data},
  author={Wei, Colin and Shen, Kendrick and Chen, Yining and Ma, Tengyu},
  journal={arXiv preprint arXiv:2010.03622},
  year={2020}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{chetlur2014cudnn,
  title={cudnn: Efficient primitives for deep learning},
  author={Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal={arXiv preprint arXiv:1410.0759},
  year={2014}
}

@article{han2016dsd,
  title={Dsd: Dense-sparse-dense training for deep neural networks},
  author={Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Gong, Enhao and Tang, Shijian and Elsen, Erich and Vajda, Peter and Paluri, Manohar and Tran, John and others},
  journal={ICLR},
  year={2016}
}

@article{dai2020incremental,
  title={Incremental learning using a grow-and-prune paradigm with efficient neural networks},
  author={Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K},
  journal={IEEE Transactions on Emerging Topics in Computing},
  year={2020},
  publisher={IEEE}
}

@article{kim2021dynamic,
  title={Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights},
  author={Kim, Jangho and Yoo, Jayeon and Song, Yeji and Yoo, KiYoon and Kwak, Nojun},
  journal={arXiv preprint arXiv:2109.04660},
  year={2021}
}

@inproceedings{wimmer2022interspace,
  title={Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs},
  author={Wimmer, Paul and Mehnert, Jens and Condurache, Alexandru},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12527--12537},
  year={2022}
}

@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  number={2},
  pages={303--338},
  year={2010},
  publisher={Springer}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@inproceedings{huang2017speed,
  title={Speed/accuracy trade-offs for modern convolutional object detectors},
  author={Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7310--7311},
  year={2017}
}

@inproceedings{humble2022soft,
  title={Soft Masking for Cost-Constrained Channel Pruning},
  author={Humble, Ryan and Shen, Maying and Latorre, Jorge Albericio and Darve, Eric and Alvarez, Jose},
  booktitle={European Conference on Computer Vision},
  pages={641--657},
  year={2022},
  organization={Springer}
}
@article{liusw2021sparse2,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  author={Liu et. al},
  journal={NIPS},
  year={2021}
}
@article{liusw2021sparse,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={NIPS},
  year={2021}
}

@article{yuan2021mest2,
  title={Mest: Accurate and fast memory-economic sparse training framework on the edge},
  author={Yuan et al.},
  journal={NIPS},
  year={2021}
}
@article{yuan2021mest,
  title={Mest: Accurate and fast memory-economic sparse training framework on the edge},
  author={Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others},
  journal={NIPS},
  year={2021}
}

@inproceedings{liu2021we2,
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu et al.},
  booktitle={ICML},
  year={2021}
}
@inproceedings{liu2021we,
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  booktitle={ICML},
  year={2021}
}

@inproceedings{yuan2022layer,
  title={Layer Freezing \& Data Sieving: Missing Pieces of a Generic Framework for Sparse Training},
  author={Yuan, Geng and Li, Yanyu and Li, Sheng and Kong, Zhenglun and Tulyakov, Sergey and Tang, Xulong and Wang, Yanzhi and Ren, Jian},
  booktitle={NIPS},
  year={2022}
}
@inproceedings{yuan2022layer2,
  title={Layer Freezing \& Data Sieving: Missing Pieces of a Generic Framework for Sparse Training},
  author={Yuan et al.},
  booktitle={NIPS},
  year={2022}
}

@article{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}

@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@inproceedings{real2019regularized,
  title={Regularized evolution for image classifier architecture search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4780--4789},
  year={2019}
}

@inproceedings{kolesnikov2020big,
  title={Big transfer (bit): General visual representation learning},
  author={Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part V 16},
  pages={491--507},
  year={2020},
  organization={Springer}
}

@inproceedings{sun2017revisiting,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}

@article{chen2015net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}

@article{yuan2022accelerated,
  title={Accelerated Training via Principled Methods for Incrementally Growing Neural Networks},
  author={Yuan, Xin and Savarese, Pedro Henrique Pamplona and Maire, Michael},
  year={2022}
}

@inproceedings{ding2023network,
  title={Network Expansion for Practical Training Acceleration},
  author={Ding, Ning and Tang, Yehui and Han, Kai and Xu, Chao and Wang, Yunhe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20269--20279},
  year={2023}
}

@article{wu2020firefly,
  title={Firefly neural architecture descent: a general approach for growing neural networks},
  author={Wu, Lemeng and Liu, Bo and Stone, Peter and Liu, Qiang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22373--22383},
  year={2020}
}

@article{wang2023learning,
  title={Learning to grow pretrained models for efficient transformer training},
  author={Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon},
  journal={arXiv preprint arXiv:2303.00980},
  year={2023}
}

@article{zhou2021efficient,
  title={Efficient neural network training via forward and backward propagation sparsification},
  author={Zhou, Xiao and Zhang, Weizhong and Chen, Zonghao and Diao, Shizhe and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15216--15229},
  year={2021}
}

@inproceedings{lym2019prunetrain,
  title={Prunetrain: fast neural network training by dynamic sparse model reconfiguration},
  author={Lym, Sangkug and Choukse, Esha and Zangeneh, Siavash and Wen, Wei and Sanghavi, Sujay and Erez, Mattan},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--13},
  year={2019}
}

@article{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{kang2020operation,
  title={Operation-aware soft channel pruning using differentiable masks},
  author={Kang, Minsoo and Han, Bohyung},
  booktitle={International Conference on Machine Learning},
  pages={5122--5131},
  year={2020},
  organization={PMLR}
}

@article{louizos2017learning,
  title={Learning sparse neural networks through $ L\_0 $ regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1712.01312},
  year={2017}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{carion2020end,
  title={End-to-End Object Detection with Transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  journal={Computer Vision--ECCV 2020},
  pages={213--229},
  year={2020},
  publisher={Springer International Publishing}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12077--12090},
  year={2021}
}

@inproceedings{ding2019approximated,
  title={Approximated oracle filter pruning for destructive cnn width optimization},
  author={Ding, Xiaohan and Ding, Guiguang and Guo, Yuchen and Han, Jungong and Yan, Chenggang},
  booktitle={International Conference on Machine Learning},
  pages={1607--1616},
  year={2019},
  organization={PMLR}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}

@inproceedings{hou2022chex,
  title={Chex: Channel exploration for CNN model compression},
  author={Hou, Zejiang and Qin, Minghai and Sun, Fei and Ma, Xiaolong and Yuan, Kun and Xu, Yi and Chen, Yen-Kuang and Jin, Rong and Xie, Yuan and Kung, Sun-Yuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12287--12298},
  year={2022}
}

@inproceedings{shen2023hardware,
  title={Hardware-Aware Latency Pruning for Real-Time 3D Object Detection},
  author={Shen, Maying and Mao, Lei and Chen, Joshua and Hsu, Justin and Sun, Xinglong and Knieps, Oliver and Maxim, Carmen and Alvarez, Jose M},
  booktitle={2023 IEEE Intelligent Vehicles Symposium (IV)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@article{jayakumar2020top,
  title={Top-kast: Top-k always sparse training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20744--20754},
  year={2020}
}

@article{lasby2023dynamic,
  title={Dynamic Sparse Training with Structured Sparsity},
  author={Lasby, Mike and Golubeva, Anna and Evci, Utku and Nica, Mihai and Ioannou, Yani},
  journal={arXiv preprint arXiv:2305.02299},
  year={2023}
}

@article{sun2024refining,
  title={Refining Pre-Trained Motion Models},
  author={Sun, Xinglong and Harley, Adam W and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2401.00850},
  year={2024}
}

@inproceedings{sun2024towards,
  title={Towards Better Structured Pruning Saliency by Reorganizing Convolution},
  author={Sun, Xinglong and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2204--2214},
  year={2024}
}

@inproceedings{sun2023revisiting,
  title={Revisiting deformable convolution for depth completion},
  author={Sun, Xinglong and Ponce, Jean and Wang, Yu-Xiong},
  booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={1300--1306},
  year={2023},
  organization={IEEE}
}

@article{sun2023pruning,
  title={Pruning for better domain generalizability},
  author={Sun, Xinglong},
  journal={arXiv preprint arXiv:2306.13237},
  year={2023}
}

@article{sun2024multi,
  title={Multi-Dimensional Pruning: Joint Channel, Layer and Block Pruning with Latency Constraint},
  author={Sun, Xinglong and Lakshmanan, Barath and Shen, Maying and Lan, Shiyi and Chen, Jingde and Alvarez, Jose},
  journal={arXiv preprint arXiv:2406.12079},
  year={2024}
}
