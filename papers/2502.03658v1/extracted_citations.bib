@inproceedings{chen2018constraint,
  title={Constraint-aware deep neural network compression},
  author={Chen, Changan and Tung, Frederick and Vedula, Naveen and Mori, Greg},
  booktitle={ECCV},
  pages={400--415},
  year={2018}
}

@inproceedings{chin2020towards,
  title={Towards efficient model compression via learned global ranking},
  author={Chin, Ting-Wu and Ding, Ruizhou and Zhang, Cha and Marculescu, Diana},
  booktitle={CVPR},
  pages={1518--1528},
  year={2020}
}

@article{dai2019nest,
  title={NeST: A neural network synthesis tool based on a grow-and-prune paradigm},
  author={Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K},
  journal={IEEE Transactions on Computers},
  volume={68},
  number={10},
  pages={1487--1497},
  year={2019},
  publisher={IEEE}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={ICML},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={ICLR},
  year={2015}
}

@article{han2016eie,
  title={EIE: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={SIGARCH},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{hassibi1992second,
  title={Second order derivatives for network pruning: Optimal Brain Surgeon},
  author={Hassibi, Babak and Stork, David},
  journal={NeurIPS},
  volume={5},
  year={1992}
}

@inproceedings{he2018soft,
  title={Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
  author={He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
  booktitle={IJCAI},
  year={2018}
}

@inproceedings{he2019filter,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle={CVPR},
  pages={4340--4349},
  year={2019}
}

@inproceedings{he2020learning,
  title={Learning filter pruning criteria for deep convolutional neural networks acceleration},
  author={He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi},
  booktitle={CVPR},
  pages={2009--2018},
  year={2020}
}

@inproceedings{hou2022chex,
  title={Chex: Channel exploration for CNN model compression},
  author={Hou, Zejiang and Qin, Minghai and Sun, Fei and Ma, Xiaolong and Yuan, Kun and Xu, Yi and Chen, Yen-Kuang and Jin, Rong and Xie, Yuan and Kung, Sun-Yuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12287--12298},
  year={2022}
}

@inproceedings{kang2020operation,
  title={Operation-aware soft channel pruning using differentiable masks},
  author={Kang, Minsoo and Han, Bohyung},
  booktitle={International Conference on Machine Learning},
  pages={5122--5131},
  year={2020},
  organization={PMLR}
}

@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle={ICML},
  pages={5544--5555},
  year={2020},
  organization={PMLR}
}

@article{lasby2023dynamic,
  title={Dynamic Sparse Training with Structured Sparsity},
  author={Lasby, Mike and Golubeva, Anna and Evci, Utku and Nica, Mihai and Ioannou, Yani},
  journal={arXiv preprint arXiv:2305.02299},
  year={2023}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={NeurIPS},
  pages={598--605},
  year={1990}
}

@inproceedings{li2017pruning,
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  booktitle = {ICLR},
  title = {Pruning Filters for Efficient ConvNets.},
  year = 2017
}

@inproceedings{lin2018accelerating,
  title={Accelerating Convolutional Networks via Global \& Dynamic Filter Pruning.},
  author={Lin, Shaohui and Ji, Rongrong and Li, Yuchao and Wu, Yongjian and Huang, Feiyue and Zhang, Baochang},
  booktitle={IJCAI},
  year={2018},
  organization={Stockholm}
}

@article{lin2020dynamic,
  title={Dynamic model pruning with feedback},
  author={Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin},
  journal={ICLR},
  year={2020}
}

@inproceedings{lin2020hrank,
  title={Hrank: Filter pruning using high-rank feature map},
  author={Lin, Mingbao and Ji, Rongrong and Wang, Yan and Zhang, Yichen and Zhang, Baochang and Tian, Yonghong and Shao, Ling},
  booktitle={CVPR},
  pages={1529--1538},
  year={2020}
}

@inproceedings{liu2021we,
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  booktitle={ICML},
  year={2021}
}

@article{liusw2021sparse,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={NIPS},
  year={2021}
}

@inproceedings{lym2019prunetrain,
  title={Prunetrain: fast neural network training by dynamic sparse model reconfiguration},
  author={Lym, Sangkug and Choukse, Esha and Zangeneh, Siavash and Wen, Wei and Sanghavi, Sujay and Erez, Mattan},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--13},
  year={2019}
}

@inproceedings{ma2021effective,
  title={Effective Model Sparsification by Scheduled Grow-and-Prune Methods},
  author={Ma, Xiaolong and Qin, Minghai and Sun, Fei and Hou, Zejiang and Yuan, Kun and Xu, Yi and Wang, Yanzhi and Chen, Yen-Kuang and Jin, Rong and Xie, Yuan},
  booktitle={ICLR},
  year={2021}
}

@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={NeurIPS},
  volume={32},
  pages={14014--14024},
  year={2019}
}

@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  year={2018},  
}

@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={ICML},
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={CVPR},
  pages={11264--11272},
  year={2019}
}

@inproceedings{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  booktitle={ICML},
  pages={4646--4655},
  year={2019},
  organization={PMLR}
}

@article{narang2017exploring,
  title={Exploring sparsity in recurrent neural networks},
  author={Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  journal={ICLR},
  year={2017}
}

@inproceedings{shen2021halp,
    title={Structural Pruning via Latency-Saliency Knapsack},
    author={Shen, Maying and Yin, Hongxu and Molchanov, Pavlo and Mao, Lei and Liu, Jianna and Alvarez, Jose},
    booktitle={Advances in Neural Information Processing Systems},
    year={2022}
}

@inproceedings{shen2023hardware,
  title={Hardware-Aware Latency Pruning for Real-Time 3D Object Detection},
  author={Shen, Maying and Mao, Lei and Chen, Joshua and Hsu, Justin and Sun, Xinglong and Knieps, Oliver and Maxim, Carmen and Alvarez, Jose M},
  booktitle={2023 IEEE Intelligent Vehicles Symposium (IV)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={CVPR workshops},
  pages={138--145},
  year={2017}
}

@inproceedings{strom1997sparse,
  title={Sparse connection and pruning in large dynamic artificial neural networks},
  author={Str{\"o}m, Nikko},
  booktitle={Fifth European Conference on Speech Communication and Technology},
  year={1997},
  organization={Citeseer}
}

@inproceedings{sun2024towards,
  title={Towards Better Structured Pruning Saliency by Reorganizing Convolution},
  author={Sun, Xinglong and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2204--2214},
  year={2024}
}

@inproceedings{thimm1995evaluating,
  title={Evaluating pruning methods},
  author={Thimm, Georg and Fiesler, Emile},
  booktitle={ICANN},
  pages={20--25},
  year={1995},
  organization={Citeseer}
}

@article{wortsman2019discovering,
  title={Discovering neural wirings},
  author={Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@inproceedings{yang2018netadapt,
  title={Netadapt: Platform-aware neural network adaptation for mobile applications},
  author={Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
  booktitle={ECCV},
  pages={285--300},
  year={2018}
}

@article{you2019gate,
  title={Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks},
  author={You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{yuan2020growing,
  title={Growing efficient deep networks by structured continuous sparsification},
  author={Yuan, Xin and Savarese, Pedro and Maire, Michael},
  journal={ICLR},
  year={2021}
}

@article{yuan2021mest,
  title={Mest: Accurate and fast memory-economic sparse training framework on the edge},
  author={Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others},
  journal={NIPS},
  year={2021}
}

@inproceedings{yuan2022layer,
  title={Layer Freezing \& Data Sieving: Missing Pieces of a Generic Framework for Sparse Training},
  author={Yuan, Geng and Li, Yanyu and Li, Sheng and Kong, Zhenglun and Tulyakov, Sergey and Tang, Xulong and Wang, Yanzhi and Ren, Jian},
  booktitle={NIPS},
  year={2022}
}

@inproceedings{zhou2021effective,
  title={Effective sparsification of neural networks with global sparsity constraint},
  author={Zhou, Xiao and Zhang, Weizhong and Xu, Hang and Zhang, Tong},
  booktitle={CVPR},
  pages={3599--3608},
  year={2021}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

