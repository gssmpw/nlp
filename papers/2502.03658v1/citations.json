[
  {
    "index": 0,
    "papers": [
      {
        "key": "thimm1995evaluating",
        "author": "Thimm, Georg and Fiesler, Emile",
        "title": "Evaluating pruning methods"
      },
      {
        "key": "strom1997sparse",
        "author": "Str{\\\"o}m, Nikko",
        "title": "Sparse connection and pruning in large dynamic artificial neural networks"
      },
      {
        "key": "han2015deep",
        "author": "Han, Song and Mao, Huizi and Dally, William J",
        "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding"
      },
      {
        "key": "narang2017exploring",
        "author": "Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho",
        "title": "Exploring sparsity in recurrent neural networks"
      },
      {
        "key": "zhu2017prune",
        "author": "Zhu, Michael and Gupta, Suyog",
        "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"
      },
      {
        "key": "gale2019state",
        "author": "Gale, Trevor and Elsen, Erich and Hooker, Sara",
        "title": "The state of sparsity in deep neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lecun1990optimal",
        "author": "LeCun, Yann and Denker, John S and Solla, Sara A",
        "title": "Optimal brain damage"
      },
      {
        "key": "hassibi1992second",
        "author": "Hassibi, Babak and Stork, David",
        "title": "Second order derivatives for network pruning: Optimal Brain Surgeon"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhou2021effective",
        "author": "Zhou, Xiao and Zhang, Weizhong and Xu, Hang and Zhang, Tong",
        "title": "Effective sparsification of neural networks with global sparsity constraint"
      },
      {
        "key": "srinivas2017training",
        "author": "Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R",
        "title": "Training sparse neural networks"
      },
      {
        "key": "molchanov2017variational",
        "author": "Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry",
        "title": "Variational dropout sparsifies deep neural networks"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "han2015deep",
        "author": "Han, Song and Mao, Huizi and Dally, William J",
        "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2017pruning",
        "author": "Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter",
        "title": "Pruning Filters for Efficient ConvNets."
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "michel2019sixteen",
        "author": "Michel, Paul and Levy, Omer and Neubig, Graham",
        "title": "Are Sixteen Heads Really Better than One?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "han2016eie",
        "author": "Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J",
        "title": "EIE: Efficient inference engine on compressed deep neural network"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "li2017pruning",
        "author": "Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter",
        "title": "Pruning Filters for Efficient ConvNets."
      },
      {
        "key": "chin2020towards",
        "author": "Chin, Ting-Wu and Ding, Ruizhou and Zhang, Cha and Marculescu, Diana",
        "title": "Towards efficient model compression via learned global ranking"
      },
      {
        "key": "he2020learning",
        "author": "He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi",
        "title": "Learning filter pruning criteria for deep convolutional neural networks acceleration"
      },
      {
        "key": "he2018soft",
        "author": "He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi",
        "title": "Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks"
      },
      {
        "key": "yang2018netadapt",
        "author": "Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig",
        "title": "Netadapt: Platform-aware neural network adaptation for mobile applications"
      },
      {
        "key": "sun2024towards",
        "author": "Sun, Xinglong and Shi, Humphrey",
        "title": "Towards Better Structured Pruning Saliency by Reorganizing Convolution"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lin2018accelerating",
        "author": "Lin, Shaohui and Ji, Rongrong and Li, Yuchao and Wu, Yongjian and Huang, Feiyue and Zhang, Baochang",
        "title": "Accelerating Convolutional Networks via Global \\& Dynamic Filter Pruning."
      },
      {
        "key": "molchanov2019importance",
        "author": "Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan",
        "title": "Importance estimation for neural network pruning"
      },
      {
        "key": "you2019gate",
        "author": "You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping",
        "title": "Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "he2019filter",
        "author": "He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi",
        "title": "Filter pruning via geometric median for deep convolutional neural networks acceleration"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lin2020hrank",
        "author": "Lin, Mingbao and Ji, Rongrong and Wang, Yan and Zhang, Yichen and Zhang, Baochang and Tian, Yonghong and Shao, Ling",
        "title": "Hrank: Filter pruning using high-rank feature map"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2018constraint",
        "author": "Chen, Changan and Tung, Frederick and Vedula, Naveen and Mori, Greg",
        "title": "Constraint-aware deep neural network compression"
      },
      {
        "key": "shen2021halp",
        "author": "Shen, Maying and Yin, Hongxu and Molchanov, Pavlo and Mao, Lei and Liu, Jianna and Alvarez, Jose",
        "title": "Structural Pruning via Latency-Saliency Knapsack"
      },
      {
        "key": "shen2023hardware",
        "author": "Shen, Maying and Mao, Lei and Chen, Joshua and Hsu, Justin and Sun, Xinglong and Knieps, Oliver and Maxim, Carmen and Alvarez, Jose M",
        "title": "Hardware-Aware Latency Pruning for Real-Time 3D Object Detection"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "shen2021halp",
        "author": "Shen, Maying and Yin, Hongxu and Molchanov, Pavlo and Mao, Lei and Liu, Jianna and Alvarez, Jose",
        "title": "Structural Pruning via Latency-Saliency Knapsack"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "molchanov2019importance",
        "author": "Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan",
        "title": "Importance estimation for neural network pruning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "mocanu2018scalable",
        "author": "Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio",
        "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"
      },
      {
        "key": "dettmers2019sparse",
        "author": "Dettmers, Tim and Zettlemoyer, Luke",
        "title": "Sparse networks from scratch: Faster training without losing performance"
      },
      {
        "key": "mostafa2019parameter",
        "author": "Mostafa, Hesham and Wang, Xin",
        "title": "Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization"
      },
      {
        "key": "kusupati2020soft",
        "author": "Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali",
        "title": "Soft threshold weight reparameterization for learnable sparsity"
      },
      {
        "key": "wortsman2019discovering",
        "author": "Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad",
        "title": "Discovering neural wirings"
      },
      {
        "key": "dai2019nest",
        "author": "Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K",
        "title": "NeST: A neural network synthesis tool based on a grow-and-prune paradigm"
      },
      {
        "key": "evci2020rigging",
        "author": "Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich",
        "title": "Rigging the lottery: Making all tickets winners"
      },
      {
        "key": "lin2020dynamic",
        "author": "Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin",
        "title": "Dynamic model pruning with feedback"
      },
      {
        "key": "ma2021effective",
        "author": "Ma, Xiaolong and Qin, Minghai and Sun, Fei and Hou, Zejiang and Yuan, Kun and Xu, Yi and Wang, Yanzhi and Chen, Yen-Kuang and Jin, Rong and Xie, Yuan",
        "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods"
      },
      {
        "key": "yuan2020growing",
        "author": "Yuan, Xin and Savarese, Pedro and Maire, Michael",
        "title": "Growing efficient deep networks by structured continuous sparsification"
      },
      {
        "key": "liusw2021sparse",
        "author": "Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin",
        "title": "Sparse training via boosting pruning plasticity with neuroregeneration"
      },
      {
        "key": "yuan2021mest",
        "author": "Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others",
        "title": "Mest: Accurate and fast memory-economic sparse training framework on the edge"
      },
      {
        "key": "liu2021we",
        "author": "Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola",
        "title": "Do we actually need dense over-parameterization? in-time over-parameterization in sparse training"
      },
      {
        "key": "yuan2022layer",
        "author": "Yuan, Geng and Li, Yanyu and Li, Sheng and Kong, Zhenglun and Tulyakov, Sergey and Tang, Xulong and Wang, Yanzhi and Ren, Jian",
        "title": "Layer Freezing \\& Data Sieving: Missing Pieces of a Generic Framework for Sparse Training"
      },
      {
        "key": "hou2022chex",
        "author": "Hou, Zejiang and Qin, Minghai and Sun, Fei and Ma, Xiaolong and Yuan, Kun and Xu, Yi and Chen, Yen-Kuang and Jin, Rong and Xie, Yuan and Kung, Sun-Yuan",
        "title": "Chex: Channel exploration for CNN model compression"
      },
      {
        "key": "lasby2023dynamic",
        "author": "Lasby, Mike and Golubeva, Anna and Evci, Utku and Nica, Mihai and Ioannou, Yani",
        "title": "Dynamic Sparse Training with Structured Sparsity"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "liusw2021sparse",
        "author": "Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin",
        "title": "Sparse training via boosting pruning plasticity with neuroregeneration"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2021we",
        "author": "Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola",
        "title": "Do we actually need dense over-parameterization? in-time over-parameterization in sparse training"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "yuan2021mest",
        "author": "Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others",
        "title": "Mest: Accurate and fast memory-economic sparse training framework on the edge"
      },
      {
        "key": "yuan2022layer",
        "author": "Yuan, Geng and Li, Yanyu and Li, Sheng and Kong, Zhenglun and Tulyakov, Sergey and Tang, Xulong and Wang, Yanzhi and Ren, Jian",
        "title": "Layer Freezing \\& Data Sieving: Missing Pieces of a Generic Framework for Sparse Training"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "he2018soft",
        "author": "He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi",
        "title": "Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks"
      },
      {
        "key": "kang2020operation",
        "author": "Kang, Minsoo and Han, Bohyung",
        "title": "Operation-aware soft channel pruning using differentiable masks"
      },
      {
        "key": "lym2019prunetrain",
        "author": "Lym, Sangkug and Choukse, Esha and Zangeneh, Siavash and Wen, Wei and Sanghavi, Sujay and Erez, Mattan",
        "title": "Prunetrain: fast neural network training by dynamic sparse model reconfiguration"
      },
      {
        "key": "yuan2020growing",
        "author": "Yuan, Xin and Savarese, Pedro and Maire, Michael",
        "title": "Growing efficient deep networks by structured continuous sparsification"
      },
      {
        "key": "lasby2023dynamic",
        "author": "Lasby, Mike and Golubeva, Anna and Evci, Utku and Nica, Mihai and Ioannou, Yani",
        "title": "Dynamic Sparse Training with Structured Sparsity"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "he2018soft",
        "author": "He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi",
        "title": "Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks"
      },
      {
        "key": "kang2020operation",
        "author": "Kang, Minsoo and Han, Bohyung",
        "title": "Operation-aware soft channel pruning using differentiable masks"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "lym2019prunetrain",
        "author": "Lym, Sangkug and Choukse, Esha and Zangeneh, Siavash and Wen, Wei and Sanghavi, Sujay and Erez, Mattan",
        "title": "Prunetrain: fast neural network training by dynamic sparse model reconfiguration"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "yuan2020growing",
        "author": "Yuan, Xin and Savarese, Pedro and Maire, Michael",
        "title": "Growing efficient deep networks by structured continuous sparsification"
      }
    ]
  }
]