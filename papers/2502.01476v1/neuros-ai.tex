\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx,calc} % for pix
% \usepackage{subfigure}
\usepackage{color}    % for coloured text
\usepackage{ifthen}
\usepackage{paralist}
\usepackage{times}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{nomencl}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{listings}
\usepackage{minitoc} 
\usepackage{tikz}
\usepackage{tikzsymbols}  % For curly lines
\usepackage{xcolor}
\usepackage{float} 
\usepackage{amsthm}
\usepackage{color}    % for coloured text
\usepackage{ifthen}
\usepackage{paralist}
\usepackage{times}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{nomencl}
\usepackage{scalerel}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{amssymb}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\usetikzlibrary{arrows.meta}
\usepackage[sort,numbers]{natbib}
% Change the appearance of the header. Here \MakeUppercase is hard-coded, so renewing this command allows to elegantly change the header appearance.
\renewcommand{\MakeUppercase}{\scshape}

% Set the headings' appearance in the ``fancy'' pagestyle
\fancyhead{}
\fancyhead[RO, LE]{\leftmark}
\fancyfoot{}
\fancyfoot[RO, LE]{\thepage}

% The first pages shall be empty, even no page numbering 
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{property}[theorem]{Property}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathtt{#1}}
\newcommand{\pinv}{^{+}}
\newcommand{\inv}{^{-1}}
\newcommand{\trans}{^{\!\top}}
\newcommand{\invtrans}{^{-\!\top}}
\newcommand{\myref}[1]{(\ref{#1})}
\newcommand{\myeqref}[1]{Eq.~\myref{#1}}
\newcommand{\myfigref}[1]{Fig.~\ref{#1}}
%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
% \fancyhead[LO]{Grammar Iterative Solvers: Data-Free Foundation Models for PDEs}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
% \title{Symbolic Iterative Grammar Solvers: Neuro-exploration over analytical solutions for PDEs 
% }

% \title{Symbolic Iterative Grammar Solvers: Neural search-driven analytical solutions for PDEs 
% }

% \title{Symbolic Iterative Grammar Solvers: Neural-symbolic Analytical Solutions for PDEs 
% }

\title{Neuro-Symbolic AI for Analytical Solutions of Differential Equations
}



\author{
  Orestis Oikonomou \\
  Department of Computer Science\\
  University of Zurich \\
    \small\texttt{orestis.oikonomou@uzh.ch} \\
    \And
  Levi Lingsch\\
  Seminar for Applied Mathematics\\
  ETH Zurich\\
    \small\texttt{levi.lingsch@sam.math.ethz.ch} \\
    \And
  Dana Grund\\
  Seminar for Applied Mathematics\\
  ETH Zurich\\
    \small\texttt{dana.grund@sam.math.ethz.ch} \\
  \And
  Siddhartha Mishra\\
  Seminar for Applied Mathematics\\
  ETH Zurich\\
  \small\texttt{siddhartha.mishra@sam.math.ethz.ch} \\
  \And
  Georgios Kissas\\
  ETH AI Center\\
  ETH Zurich\\
  \small\texttt{gkissas@ai.ethz.ch} \\
    }



\begin{document}
\maketitle

\vspace{10px}

\begin{abstract}
Analytical solutions of differential equations offer exact insights into fundamental behaviors of physical processes. Their application, however, is limited as finding these solutions is difficult. To overcome this limitation, we combine two key insights. First, constructing an analytical solution requires a composition of foundational solution components.
Second, iterative solvers define parameterized function spaces with constraint-based updates. Our approach merges compositional differential equation solution techniques with iterative refinement by using formal grammars, building a rich space of candidate solutions that are embedded into a low-dimensional (continuous) latent manifold for probabilistic exploration. This integration unifies numerical and symbolic differential equation solvers via a neuro-symbolic AI framework to find analytical solutions of a wide variety of differential equations. By systematically constructing candidate expressions and applying constraint-based refinement, we overcome longstanding barriers to extract such closed-form solutions. We illustrate advantages over commercial solvers, symbolic methods, and approximate neural networks on a diverse set of problems, demonstrating both generality and accuracy.
\end{abstract}

\vspace{20px}
\section{Introduction}
\label{sec:Introduction}
The understanding of physical processes has been a long-standing effort for scientists and engineers. A key step in this endeavor is to translate physical insights (laws) into precise mathematical relationships that capture the underlying phenomena. These relationships are then tested through experiments, which either validate the proposed hypothesis or suggest refinements. Among such mathematical formulations, \emph{differential equations} (DEs) are especially ubiquitous across disciplines, as they describe how physical quantities evolve over time and space. Finding \emph{analytical} (also referred to as \emph{explicit} or \emph{closed-form}) solutions to these equations, that is, a mathematical expression that satisfies the differential equation along with the given initial and boundary conditions, provides a structured way to compare theoretical predictions with experimental measurements. Moreover, analytical solutions often reveal intrinsic properties of physical systems, such as stability, periodicity, underlying symmetries and asymptotic behavior. Thus, analytical solutions provide deep insight into how these systems behave in time and space.

Despite intense efforts over centuries, there are very few methods to construct analytical solutions of differential equations. All of them can be viewed as fundamentally \emph{compositional}: They break complex equations into simpler, more manageable pieces and then systematically recombine those pieces into a final solution. For instance, separation of variables treats multivariate problems by factoring them into single-variable components, effectively creating smaller equations whose solutions can be multiplied or summed to construct the solution of the underlying equation. Methods like the integrating factor \cite{boyce2017elementary} or variation of parameters \cite{boyce2017elementary} likewise build solutions by adding or multiplying known solution fragments (e.g., homogeneous solutions and particular solutions) into a coherent whole. Integral transform methods \cite{debnath2006integral}, such as Fourier or Laplace transforms, decompose the original equation into a spectrum of simpler “modes,” which are then reassembled through an inverse transforms. Even Green’s function techniques \cite{roach1982green} can be seen as compositional, since they construct the solution by superimposing the responses to point-source excitations across the domain. This unifying perspective highlights how each method—despite differing technical details—relies on breaking down the problem into well-understood components and composing them back together to yield a complete analytical solution.




This process requires considerable human involvement and ingenuity as many choices must be made: What are the foundational components of the final solution? How can one decompose the problem into simpler parts? Hence, the entire pipeline relies on human intelligence. The main goal of our paper is to design an AI-assisted algorithmic pipeline to replicate and improve upon this process of constructing analytical solutions.

To this end, we generalize the aforementioned notion of compositionality by observing that arbitrary combinations of “primitive” functions, as long as they provide a valid mathematical relationship, can be used as a candidate, or Ansatz, space to construct analytical solutions of differential equations. We model this observation through the notion of \emph{formal grammars} \cite{hopcroft1979introduction}, which express compositional rules within a systematic framework to yield valid mathematical expressions that respect the structure of the differential equations and their initial and boundary conditions. In this view, elementary functions considered in analytical solution methods act like \emph{terminal symbols}, and operations such as addition, multiplication, exponentiation, or more sophisticated transforms serve as \emph{production rules}. Solving a DE, whether via separation of variables, integral transforms, or other techniques, amounts to sequentially applying these production rules to merge or transform initial building blocks into successively richer expressions, eventually yielding a “fully formed” solution that satisfies the original equation and initial and boundary conditions. Each step in this process mirrors how a formal grammar derives valid expressions (\emph{strings}) from an initial \emph{start symbol}, ensuring that only valid combinations of functions and operations are produced. This perspective not only unifies multiple methods for constructing analytical solutions under a common, rule-based structure, but also opens the door to systematically exploring what classes of solutions might be generated for a given differential equation.

By framing DE solutions in terms of a compositional grammar, we define a potentially vast function space, encompassing all allowable combinations of building blocks that could solve the differential equation. However, identifying the function (or family of functions) that satisfies the DE and boundary conditions requires an effective search strategy. Existing methods, such as those reviewed in Sec. \ref{sm:literature review}, have proposed heuristic algorithms to search this space of candidate solutions. Given the size of this space, these strategies are highly ineffective. How can we search this space more efficiently? Here, we take inspiration from the successful \emph{numerical solution methods} for differential equations \cite{butcher2003numerical, saad2003iterative} such as finite element methods, as well as \emph{neural solution methods} such as physics-informed neural networks (PINNS) \cite{lagaris1998artificial, raissi2019physics, karniadakis2021physics}. Typically, a candidate space of solutions in these methods is characterized by suitable finite-dimensional parameterizations in terms of coefficients of basis functions and is \emph{iteratively searched} to minimize an underlying residual. An iterative algorithm then adjusts these parameters step by step, steering towards an (approximate) solution. 

The second key ingredient in our AI algorithm hence is to introduce an iterative process for the candidate solutions collected in the language defined by the formal grammar. To this end, we propose an embedding of the function space constructed from the grammar into a continuous lower-dimensional latent space, with a variational encoder-decoder architecture. The iterative search process in this latent space is efficiently carried out by a probabilistic evolutionary algorithm. We combine these ingredients into a neuro-symbolic AI algorithm that we term \emph{Symbolic Iterative Grammar Solver} or SIGS (see Figure \ref{fig:main} for illustration). 

\begin{figure}
    \centering
\includegraphics[width=1.0\textwidth]{cartoons/finalmain.drawio.pdf}
    \caption{Overview over the proposed Symbolic Iterative Grammar Solver (SIGS).
    \textbf{A.} Terminal symbols $\Phi$ and rules $R$, together with non-terminals $N$ and starting symbol $S$, form the grammar $\mathcal{G}$ which generates the mathematical expressions in $\mathcal{L(G)}$. 
    \textbf{B.} Each expression $s\in\mathcal{L(G)}$ is identified with a function $u$ in the finite set of candidate functions $\mathcal{U}$. 
    \textbf{C.} The encoder $\mathcal{E}$ and decoder $\mathcal{D}$ of the Grammar Variational Autoencoder (GVAE, \cite{kusner2017grammar}) embed the finite $\mathcal{U}$ into the continuous latent space $Z$.
    \textbf{D.} Given a differential equation and system conditions, a search is performed over $z\in Z$ using the
    Covariance Matrix Adaptation Evolution Strategy (CMA-ES, \cite{hansen2003reducing, hansen}), optimizing for lowest residual $\mathcal{R}$ of the corresponding candidate function $u=\mathcal{I\circ D}(z)\in\mathcal{U}$.}
    \label{fig:main}
\end{figure}

SIGS possesses many interesting mathematical properties stemming from its construction such as a-posteriori self-composition, structured choices of initial guesses for the iterative process, and probabilistic iterative updates. It is \emph{data-free} and {task-agnostic}. We showcase the capabilities of SIGS by testing it on an extensive set of ordinary differential equations (ODEs) and partial differential equations (PDEs), with varying levels of difficulty, showing that it is able to provide either exact or very accurate approximate analytical solutions across various experiments. It is also significantly broader in scope and more efficient than existing symbolic approaches, including commercial solvers, for constructing analytical solutions of differential equations (DEs). Likewise, it is much faster and more accurate than neural differential equation solvers. Thus, we propose a new paradigm of neuro-symbolic AI for constructing analytical solutions of ODEs and PDEs that is broad in scope, efficient, and robust.  


\section{Methods}
\label{sec:methods}

We consider the generic form of a time-dependent partial differential equation (PDE) as \cite{molinaro2024generative},
\begin{equation}
\begin{array}{rll}
\label{eq:pde formulation}
    \partial_t u + \mathbb{D} (u) &= \mathbf{f}, &\forall (\mathbf{x}, t) \in D \times [0,T], \\
    u(\mathbf{x}, 0) &= u_0(\mathbf{x}), &\forall \mathbf{x} \in D, \\
    \mathbb{B}[u](\mathbf{x},t) &=  g,  &\forall (\mathbf{x},t) \in \partial D \times [0,T],
\end{array}    
\end{equation}
where $D \subset \mathbb{R}^d$ is the spatial domain, $u \in \mathcal{U} \subseteq \mathcal{C}(D)$ is the space-time continuous solution, $\mathbf{f} \in \mathcal{U}$ is a forcing term, $u_0 \in H^s(D)$ an initial condition, $ \mathbb{B}[u](\mathbf{x},t)$ denotes the boundary conditions, and $\partial D$ is the boundary of the domain. 
The differential operator can include higher-order derivatives, $\mathbb{D}(u)= \mathbb{D}( \xi, u, \partial_{tt} u, \nabla_\mathbf{x} u, \nabla_\mathbf{x}^2 u, ...)$, where $\xi \in \mathbb{R}^{d_{\xi}}$ are PDE parameters
We remark that \eqref{eq:pde formulation} represents a very general form of differential equations as the solution $u = u(x,t)$ is a function of both space and time. By setting $u = u(t)$, we recover general ODEs, while setting $u= u(x)$ enables us to recover time-independent PDEs from the same overall formulation. Henceforth, we use PDEs, of the form \eqref{eq:pde formulation} instead of DEs as the objects for which we need find analytical solutions.  We call the collection of $\mathbf{f},\mathbb{B}[u],$ and $u_0$ the \emph{system conditions} that need to be specified in order to solve a given PDE.
The \emph{symbolic form} of a PDE is given by:
\begin{equation*}
    \mathcal{S}(u) = \partial_t u + \mathbb{D} (u) - \mathbf{f}, \quad \forall u \in \mathcal{U},
\end{equation*}
and is extended by loss terms on the system conditions to the \emph{augmented PDE residual}
\begin{equation}
\label{eq:res}
    \mathcal{R}(u) = \|  \mathcal{S}(u) \|^2 + \\  \beta_1 \| u(0,x) - u_0^i \|^2 + \beta_2 \|  B[u] - g \|_{\partial D}^2,
\end{equation}
where $\beta_1, \beta_2 > 0$ are constants. We formulate solving PDEs as a root-finding problem in which one searches for $\hat{u} \in \mathcal{U}$ minimizing the residual, with \emph{analytical} (or \emph{explicit}) solutions satisfying $\mathcal{R}(\hat{u})=0$. We restate our goal as finding analytical solutions, which are explicit mathematical expressions, given in terms of closed-form symbolic formulas, while at same time yielding a zero residual \eqref{eq:res}. Taking a closer look at iterative algorithms, we observe that they first consider a finite-dimensional parameterization of an unknown infinite-dimensional function space, then search the finite-dimensional space starting from an initial guess and updating it through a task-specific rule. As mentioned in the introduction, we follow a two-step process. First, we will create a function space of candidate solutions. Second, we will define an efficient strategy to search this space. To this end, we start by describing the different approaches currently used for solving PDEs such as \eqref{eq:pde formulation}. 

\subsection{Parameterizing the Function Space}
An extended version of this review can be found in {\bf SM} \ref{sm:literature review}.
\subsubsection{Numerical PDE Solvers.}
It is common practice to allow for a non-zero residual $\mathcal{R}(u)>0$ in order to find approximate PDE solutions with traditional numerical methods, such as the Finite Element Method \cite{fem}. They typically choose a basis representation of $\mathcal{U}$ as linear combinations,
\begin{equation*}
    \mathcal{U}_{\text{num}} = \{ u_a: D \rightarrow \mathbb{R} | u_a(x) = \sum_{i=1}^N a_i \phi_i(x), a_i \in \mathbb{R}^a \},
\end{equation*}
e.g. using the finite element basis. These methods can then be interpreted in terms of an update procedure that fits the basis coefficients $a$ to the given PDE conditions, by minimizing:
\begin{equation*}
    \hat{a} = \arg \min_{a \in A} \mathcal{R}(u_a). 
\end{equation*}
The obtained approximate solution $u_{\hat{a}}$ implicitly minimizes the residual $\mathcal{R}$ e.g. by the choice of basis, a tailored update, or an iterative optimization protocol, and different methods may be chosen depending on the type of PDE and system conditions.

\subsubsection{Neural PDE Solvers.}
In a similar fashion, a variety of neural network based methods have been developed to mimic numerical PDE solvers. They typically consider a nonlinear parameterization of the function space $\mathcal{U}$ given by the architecture of the neural networks $u_\theta$:
\begin{equation*}
    \mathcal{U}_{\text{neur}} = \{ u: D \rightarrow \mathbb{R} \mid u(x) = u_{\theta}(x), \theta \in \Theta \subset \mathbb{R}^{\theta} \},
\end{equation*}
where the learnable parameters of the neural network $\theta \in \Theta$ parameterize $\mathcal{U}$. Here, we focus on unsupervised learning that does not require numerical data generation, namely on physics-informed neural networks (PINNS) \cite{lagaris1998artificial, raissi2018deep, karniadakis2021physics}. During training, the minimization of the PDE residual is enforced as a learning objective to find the optimal parameters:
\begin{equation*}
    \hat{\theta} = \arg \min_{\theta \in \Theta} \mathcal{R}(u_\theta). 
\end{equation*}
However, the convergence of this sequence of $\theta$ to the optimal parameters had been found to be not well-behaved \cite{de2023operator, wang2021understanding, wang2022and}, with training hindered by a strong dependence on the initial guess $\theta^{(0)}$ and the high dimensionality of the parameter space $\Theta$.

\subsubsection{Symbolic methods}
% \subsubsection{Analytical Solvers}
% \label{sec:analytical solvers}

A variety of algorithms exist to find analytical solutions to differential equations, each leveraging different structural properties of the equations. Classical methods include separation of variables \cite{miller1977symmetry}, where multidimensional problems are reduced to single-variable ODEs, integral transform techniques \cite{debnath2006integral, dyke2014an} that convert differential operators into algebraic ones, and Green’s function methods \cite{roach1982green}, which build solutions from point-source responses. More specialized symbolic integration algorithms systematically determine whether elementary antiderivatives exist \cite{risch1969the}, and are often built into computer algebra systems (e.g., Mathematica \cite{mathematica} or Maple \cite{Maple2024}). Special functions emerge when classical methods yield integrals that cannot be expressed in simpler elementary terms.
\subsection{Symbolic Iterative Grammar Solvers}
\label{sec:Symbolic Iterative Grammar Solvers}

% If the approach for deriving an analytic solution is defined as an iterative solver then we can perform convergence and approximation analysis. 

We propose Symbolic Iterative Grammar Solvers (SIGS), a fundamentally new approach in deriving analytical solutions of differential equations, characterized by the following main components (see overview in Fig. \ref{fig:main}):
Considering a grammar as an underlying structure allows an explicit definition of the function space $\mathcal{U}$ as a composition of primitive functions. As a second step, we learn a continuously embedded nonlinear manifold in the ambient solution space using a nonlinear dimensionality reduction technique, more specifically a Grammar Variational Autoencoder \cite{kusner2017grammar}. Finally, we perform a root search in the parameterized space using stochastic updates. We show that this approach possesses some intriguing and useful properties. We describe each of these components below.
% \begin{itemize}
%     \item A formal \emph{grammar} $\mathcal{G}$ allows to parameterize the search space $\mathcal{U}$ through primitive mathematical functions, naturally describing it as the language $\mathcal{L}(\mathcal{G})$ of all closed-form solution candidates $u$. 
%     \item In order to efficiently search this collection of expressions, we use a 
%     % Grammar Variational Autoencoder (GVAE, \cite{kusner2017grammar}) as a 
%     \emph{nonlinear dimensionality reduction technique} to embed $\mathcal{L}(\mathcal{G})$ into a continuous ambient solution space $Z$. 
%     \item Finally, we perform an \emph{evolutionary search} in the latent space $Z$ 
%     % using Covariance Matrix Adaptation Evolution Strategy (CMA-ES, XXX) 
%     to find the function u that minimizes the PDE residual $\mathcal{R}(u)$, ideally an exact solution $\mathcal{R}(\hat{u})=0$.
% \end{itemize}

% \textbf{Building the Function Space:}
\subsubsection{Grammar-Generated Function Space}
\label{sec:grammar}


% We identify the building blocks of closed-form expressions as unary operators $U$, e.g. $\exp, \sin, \cos$; binary operators $B$, e.g. $ +, -, \cdot$; constants $C \subseteq \mathbb{R}$, and variables $A$, e.g. $x, y, z, t$  and we collect them as \emph{terminal expressions} $\Phi=C\cup A$ and \emph{non-terminal expressions} $N=U\cup B$. Further, we use the Kleene closure $*$ to generate \emph{strings} $w$, i.e., closed-form descriptions of functions, from the terminal symbols, e.g. $w=[1,+,\sin(,x,)]\in (\Phi \cup N)^*$.

% \textbf{Graph-based} approaches, as first proposed by Lample et. al. \cite{lample2019deep}, define a grammar based on these expressions as a general graph $G= \{ V, E, l \}$, where each expression $L = \{ U, B, C, A\}$ is represented as a node $v\in V$ by the labeling function $l: V \rightarrow L$, and the edges are $E \subseteq V \times V$. Strings corresponding to closed-form representations of functions are then generated from $G$ as finite walks $w = (l(v_1), l(v_2),..., l(v_n)) \subseteq G$ such that $(v_i, v_{i+1}) \in E$ for all $i$.
% \begin{equation}
%     \label{eq:function space graph}
%    S = \{ l(w) \mid w \in V^*,  (v_i, v_{i+1}) \in E\},
% \end{equation}
% where $V^* = \bigcup_{n=0}^\infty V^n$ is the Kleene star of the set $V$ and $n$ is the sequence length. 
% This function space construction and sampling using random walks on graphs is first proposed by Lample et. al. \cite{lample2019deep}. 
% However, in this approach, the cardinality of $E$ grows exponentially as the cardinality of $V$ increases \cite{virgolin2022symbolic}, and the walks are not constrained to provide syntactically and semantically meaningful expressions \cite{kissas2024language}. 

Let $G= \{ V, E, l \}$ be a general graph, where $V$ is a finite set of node labels, $E \subseteq V \times V$ is a set of edges, and $l: V \rightarrow L$ is a labeling function assigning each node a label from the set $L = \{ U, B, C, A\}$. We define $U$ as the set of unary expressions, e.g. $U = \{ \exp, \sin, \cos \}$, $B$ as the set of binary expressions, e.g. $B = \{ +, -, \cdot \}$, $C \subseteq \mathbb{R}$ as the set of constants, and $A$ as the set of variables, e.g. $A= \{ x, y, z, t, \xi \}$.  The space of PDE solutions $\mathcal{U}$ is defined for nodes, $v_i \in V$, and labels, $l(v_i) \in L$, as the set of finite walks, $w = ( v_1, v_2,..., v_n) \in G$, such that $(v_i, v_{i+1}) \in E$ for all $i$, or
\begin{equation}
    \label{eq:function space graph}
   \text{Expressions} = \{ l(w) \mid w \in V^*,  (v_i, v_{i+1}) \in E\},
\end{equation}
where $V^* = \bigcup_{n=0}^\infty V^n$ is the Kleene star of the set $V$ and $n$ is the sequence length. This function space construction and sampling using random walks on graphs is first proposed in \cite{lample2019deep}. However, in this approach the cardinality of $E$ grows exponentially as the cardinality of $V$ increases \cite{virgolin2022symbolic}, and the walks are not constrained to provide syntactically and semantically meaningful expressions \cite{kissas2024language}. 

Instead, \textbf{Context-Free Grammars (CFG)} are used here as the underlying structure of the function space $\mathcal{U}$ to generate only meaningful expressions \cite{chomsky1956three, hopcroft1979introduction}, see \textbf{SM} \ref{sm:grammar intuition} for an example-driven introduction. A CFG is defined as $\mathcal{G} = \{ \Phi, N, R, S \}$, where 
$\Phi$ is the set of terminal symbols, $N$ is the set of non-terminal symbols and 
$\Phi \cap N = \emptyset$, $R$ is a finite set of production rules and $S \in N$ is the starting symbol. Each rule $r \in R$ is a map $\alpha \rightarrow \beta$, where $\alpha \in N$, and $\beta \in (\Phi \cup N)^*$, which contains at least one non-terminal symbol. A language $\mathcal{L}(\mathcal{G})$ is defined as the set of all possible terminal strings that can be derived by applying the production rules of the grammar starting from $S$, or all possible ways that the nodes of a derivation tree can be connected starting from $S$:
\begin{equation}
    \mathcal{L}(\mathcal{G}) = \{ w \in \Phi^* \mid S \rightarrow^* w \},
\end{equation}
where $\rightarrow^*$ implies $T \geq 0$ applications of rules in $R$.

Each expression is equivalently represented by the string $w$ (as a sequence of symbols), by the list of rules applied to generate $w$ from $S$, and by a \emph{derivation tree} that represents the syntactic structure of string $w \in \mathcal{L}(\mathcal{G})$ according to grammar $\mathcal{G}$ (see Fig. \ref{fig:main}). There are two fundamental operations are associated with a derivation tree, namely parsing and generation. Given a string $w \in \Phi^*$, a parsing algorithm determines if it is part of the language by trying to extract a sequence of rules, starting from $S$. Given a sequence of rules, a generation algorithm derives a terminal string $w \in \Phi^*$ starting from $S$.

We define an interpretation map $\mathcal{I}: \mathcal{L} (\mathcal{G}) \rightarrow \mathcal{U}$, which assigns to each syntactic expression $w \in \mathcal{L} (\mathcal{G})$ semantic meaning in terms of a function $u_w:D \rightarrow \mathbb{R}$. The set of all functions represented by the grammar is:
\begin{equation*}
    \mathcal{U} (\mathcal{G}) = \{ u_w: D \rightarrow \mathbb{R} \mid u_w = \mathcal{I}(w),  w \in \mathcal{L} (\mathcal{G}) \}.
\end{equation*}
We refer to $u_w$ as $u$ in the future to simplify the notation. Since $\mathcal{U}(\mathcal{G}) \subseteq \mathcal{C}(D)$, it inherits the topology of $\mathcal{C}$ in the sense that we can compare any two functions $u, u' \in \mathcal{U}(\mathcal{G})$ by the supremum norm. Consequently, we obtain the induced notion of distance for $w,w'\in \mathcal{L(G)}$ as ${||w-w'||_\mathcal{L(G)} =||\mathcal{I}(w)-\mathcal{I}(w')||_\infty}$. . We consider this function space as the space of candidate PDE solutions. 
%meaning that for every derivation $\text{PDE} \implies \alpha_1 \implies ... \implies \alpha_m \implies w \in \Phi^*$, we require: 
%\begin{equation*}
  %  \mathbb{B}[\mathcal{I}(w)] = g, \quad \mathcal{I}(w)(0,x) = u_0,\quad \text{and} \quad \mathcal{R}(\mathcal{I}(w)) = 0,
%\end{equation*}
%thus each interpreted function $\mathcal{I}(s)$ satisfies a differential equation form and its corresponding boundary/initial conditions. 
We provide more details on \emph{Grammar-Generated Function Spaces} and their properties in Sections \ref{sm:grammar generated function spaces}.

\subsubsection{Function Space Parameterization}
As mentioned before, the space $\mathcal{U} (\mathcal{G})$ of candidate expressions needs to be searched efficiently so that we can find the expression which is the (approximate) analytical solution of the PDE \eqref{eq:pde formulation}. As searching this very large discrete space is onerous, we propose a different approach. It is based on embedding $\mathcal{U} (\mathcal{G})$ into a continuous, lower-dimensional latent space $Z$ and searching over this latent space.  


To this end, we consider an encoder-decoder pair $(\mathcal{E}, \mathcal{D})$ such that
\begin{equation*}
\mathcal{E}: \mathcal{L}(\mathcal{G}) \to Z \subseteq \mathbb{R}^m \quad \text{and} \quad \mathcal{D}:Z \to \mathcal{L}(\mathcal{G})
\end{equation*}
with $\mathcal{D}(\mathcal{E}(w))=w$ for all $w \in \mathcal{L}(\mathcal{G})$. The space $Z$ provides a continuous representation of functions in $\mathcal{U} (\mathcal{G})$. Composing with $\mathcal{I}$, we have $\mathcal{I}\circ \mathcal{D}:Z \to \mathcal{U} (\mathcal{G})$, so each $z \in Z$ corresponds to a function $u=\mathcal{I}(\mathcal{D}(z)) \in \mathcal{U} (\mathcal{G})$. We instantiate $(\mathcal{E}, \mathcal{D})$ as a Grammar Variational Autoencoder (GVAE) \cite{kusner2017grammar}, as described in detail in {\bf SM} Section \ref{sm:gvae}, see also Figure \ref{fig:main} for an illustration of this embedding. Training the GVAE does not require numerical data, only expressions $w \in \mathcal{L}(\mathcal{U})$. The encoder $\mathcal{E}$ maps the discrete grammar to a continuous latent space $Z$, and the decoder $D$ maps the continuous $Z$ to $\mathcal{L}(\mathcal{U})$ through a parse tree generation process \cite{kusner2017grammar}.

\subsubsection{Iterative Solution Search}
Once a latent space representation is available, we formulate the search for an analytical solution of the PDE \eqref{eq:pde formulation} in terms of finding the optimal latent vector $\hat{z}$ such that with $u_z = \mathcal{I}(\mathcal{D}(z))$:
\begin{equation*}
    \hat{z} = \arg \min_{z \in Z}  \mathcal{R}(u_z),
\end{equation*}
where $\mathcal{R}$ is the PDE residual and an \emph{analytical solution} $u_{\hat{z}}$ to the PDE is characterized by $\mathcal{R}(u_z)=0$.
The sequence of solutions $ \{ u_{z^{k}} \}$ is generated through the update rule:
\begin{equation*}
    u_{z^{k+1}} = T (u_{z^k}) =  \mathcal{D} (z^k + \Delta (D, \mathbb{B}[u], u_0, S) \ z^k),
\end{equation*}
where the update rule is constructed using the  Covariance Matrix Adaptation Evolution Strategy (CMA-ES, \cite{hansen2003reducing, hansen}) as explained in {\bf SM} \ref{sec:iterative updates}, see Figure \ref{fig:main} for an illustration. The CMA-ES update rule within the GVAE framework represents a fundamentally different approach compared to traditional iterative methods. In traditional numerical methods, updates occur using a linear parameterization of $\mathcal{U} (\mathcal{G})$, while in the proposed methodology, updates occur on the nonlinear manifold $Z$, and solutions are obtained through $\mathcal{D}$. Moreover, the update rule in the traditional methods is linear, see Equation \ref{sec:iterative updates}, while the updates in the proposed methodology are linear in the latent space followed by a nonlinear function $\mathcal{D}$. Traditional methods define $\Delta$ using deterministic optimization methods, while the proposed methodology considers stochastic update rules. This method is thus suitable also for problems where local linear approximations are not effective. 



\subsection{Properties of SIGS.}
\label{sec:prop}
The SIGS algorithm enjoys the following properties by construction, namely, 
\begin{property}[\textbf{One-solver-solves-all}]
We unify the solution of multiple PDE and ODE families under a single grammar $\mathcal{G}$, then specialize by restricting production rules $R' \subseteq R$ to form a sub-language $\mathcal{L}(\mathcal{G}')$ and thus a subspace $\mathcal{U} (\mathcal{G}') \subseteq \mathcal{U} (\mathcal{G})$. In latent space, this induces a subspace $Z' := \{\,z \in Z \;\mid\; \mathcal{D}(z) \in \mathcal{L}(\mathcal{G}')\}$ tailored to each problem class. Searching on $Z'$ ensures candidate solutions remain admissible for that class. For systems of equations, we concatenate $z_i \in \mathbb{R}^m$ into $\tilde{z} \in \mathbb{R}^{d \times m}$, perform updates on the concatenated vector, and decode each component independently, yielding $(u_1,\dots,u_d)$ that collectively minimizes the system’s residual. Consequently, a single pre-trained model can solve diverse (non-)linear ODEs, PDEs, or coupled systems by choosing appropriate sub-grammars, submanifolds, and latent vector dimensions.
\end{property}

\begin{property}[\textbf{Efficient Initial Guess}]
To ensure rapid convergence, we store a finite library $\mathcal{U(G)}^* \subset\mathcal{U(G)}$ of previously decoded solutions. Given a new PDE with residual operator $\mathcal{R}(\cdot)$, we pick $u^* = \arg\min_{u_i \in \mathcal{U(G)}^*} \|\mathcal{R}(u_i)\|$, and set $z^{(0)}=\mathcal{E}(u^*)$ as the initial guess in latent space. If $\mathcal{R}(u^*)=0$, an exact solution is found and no iteration is needed; otherwise, the dense sampling of $\mathcal{U(G)}^*$ in $\mathcal{U} (\mathcal{G})$ ensures we can approximate $\hat{u}$ arbitrarily well for growing size of $\mathcal{U(G)}^*$, thus guiding the iterative updates closer to the correct solution.
\end{property}

\begin{property}[\textbf{A-posteriori Self-Compositions}]
We leverage the closure of the grammar to construct new solutions by composing simpler ones. Concretely, let $\{z_i\}_{i=1}^N$ be sampled from the latent prior and decoded into $\{u_i\}_{i=1}^N \subset \mathcal{U} (\mathcal{G})$. If $\circ \in B$ and $o \in U$ denote binary and unary operations allowed by the grammar $\mathcal{G}$, we form $\tilde{u} = u_1 \circ \cdots \circ (o(u_N)) \in \mathcal{U} (\mathcal{G})$, provided these operations remain syntactically valid. This \emph{a-posteriori self-composition} effectively expands the solution space without retraining and ensures $\tilde{u}$ still satisfies $\tilde{u} \in \mathcal{U} (\mathcal{G})$ due to grammar closure.
\end{property}

\begin{property}[\textbf{Implicitly Satisfy Conditions}]
By restricting the function space $\mathcal{U} (\mathcal{G})$ via a grammar, which means that expressions with close sequences of rules are close in the latent space, any $u\in \mathcal{U} (\mathcal{G})$ that starts near the true solution $\hat{u}$ also respects the boundary/initial conditions in an \emph{implicit} manner. Formally, if $\|u^{(0)} - \hat{u}\|$ is sufficiently small and $\mathcal{T}$ is a continuous update map with $\mathcal{R}(\mathcal{T}(u)) \le \mathcal{R}(u)$, then each iterate $u^{(k)}$ remains close to $\hat{u}$, ensuring $\|B[u^{(k)}] - g\|\!\to 0$ and $\|u^{(k)}(0,\cdot) - u_0\|\!\to 0$. Hence, penalty-based enforcement of boundary/initial data does not degrade convergence, as the grammar-based initialization confines updates to a region satisfying these constraints.
\end{property}

A detailed description, including the mathematical derivation, of each of these properties are provided in the {\bf SM} Section \ref{sec:appendix_properties}. The implications of these mathematical properties on the scope and performance of SIGS is discussed in the following section.  

\section{Results}
\label{sec:results}
We perform a rigorous set of experiments to demonstrate the performance of SIGS in finding analytical solutions of differential equations as well as investigate its theoretical properties. These include evaluations of SIGS and several baselines on a set of DEs originally published by Tsoulos et al.; evaluation on more advanced PDEs often studied in literature; and an ablation studying the sensitivity of SIGS to loss weights along initial and boundary conditions.

The \emph{Tsoulos} problem set \cite{Tsoulos2006SolvingDE} is a suite of differential equations, including 9 linear ODEs, 3 nonlinear ODEs, 2 systems of ODEs, and 5 PDEs, specifically constructed to validate neural network-based ODE and PDE solvers. These ODEs and PDEs are respectively organized in \textbf{SM} \ref{sm: DEs}, Tables \ref{tab:Tsoulos odes} and \ref{tab:Tsoulos pdes}. For each of these problems, we also draw comparisons with established baselines. These include iterative analytical approaches such as genetic programming (GP) \cite{Tsoulos2006SolvingDE}, ant colony programming (ACP) \cite{kamali2015solving}, and artificial bee colony programming (ABCP) \cite{boudouaoui2020solving}, as well as physics-informed neural networks (PINNs) \cite{pinns}, and finite-basis PINNS (FBPINNs) \cite{moseley2023finite, Dolean2024}, which are designed to converge faster than vanilla PINNs during training. Additionally, we compare with \emph{Mathematica} \cite{mathematica}, a commercial software widely used for deriving analytical solutions to DEs. In each case, we consider the number of iterations until the true solution is recovered. For PINN methods, we consider both the number iterations (epochs) until convergence upon an approximate solution and the relative L1 error at this iteration.

The results, presented in Table \ref{tab:number of iterations to solution}, demonstrate that SIGS provides significantly faster results. For all but three problems, SIGS immediately converges to the analytical solution following initialization. In the other three cases, SIGS was still either the fastest or second fastest to converge, requiring orders of magnitude fewer function evaluations across all problems. Mathematica fails to converge to a solution for several of the considered problems, while the ACP and ABCP approaches are not applied for some of the proposed problems. Even though the GP approach discovers the solution for all cases, it still requires several hundred more iterations than SIGS for most problems.

Following this, we investigated the performance of SIGS on a set of more \emph{advanced} problems, shown in Table \ref{tab:advanced problems}. For comparison, we also evaluated PINNs and FBPINNs on these problems. This set is composed of problems including diffusion phenomena, Burgers' equation, shallow water equations, and several instances of the wave equation. Notably, the exact solution of the \emph{iterative wave equation} problem is composed of 16 sinusoidal terms. This solution poses the greatest challenge to SIGS because it must create a solution through \emph{self-composition} of partial solutions with fewer terms. Nonetheless, our model recovers an approximate analytical solution (see {\bf SM}) with a minuscule error, just missing digits of constants. Likewise, the solutions to the shallow water equations are recovered exactly for the height $\rho$ and the velocity component $u_x$ and a very precise closed-form approximation is recovered for the velocity component $u_y$. In all other cases, the exact solutions are recovered. Meanwhile, the physics-informed approaches struggle to recover approximate solutions, with errors often greater than 20\%. 

Finally, we performed an ablation study of the sensitivity of SIGS with respect to training hyperparameters of the boundary conditions. This study helps us verify Property 4 of SIGS. By considering the weight term $\beta_1=1$ and varying $\beta_2$, which modifies the significance of the loss along the boundaries of the domain, we study the role of these hyperparameters in the convergence characteristics of SIGS. The results, presented in Table \ref{tab:boundary_weights_fixed_seed}, demonstrate that convergence is completely independent of the boundary hyperparameters, as SIGS reliably converges to the analytical solution after several iterations, with little to no variance in the number of iterations. 

Other machine learning approaches such as GP, ACP, ABCP, and (FB-)PINNs have several key disadvantages. They are initialized from a "cold-start", that is, they have no access to prior information and cannot learn from relationships between different datasets. As a result, they may fail to solve more challenging problems, or require many iterations to reach a solution. Alternatively, Mathematica uses its \emph{symbolic computation engine}, applying several techniques to find analytical solutions, which should, in theory, be applicable across a broad spectrum of DEs. Nonetheless, this approach still fails to solve the NLODEs, SODEs, and 4 of the 5 PDEs. In contrast, SIGS has been designed specifically for the purpose of transferring knowledge among DE operators and their solutions. Our novel method is equipped with several key properties that allow it to overcome these limitations. SIGS, as a result, reliably handles a plethora of differential equations, as explained by Property 1 \emph{one-solver-solves-all} and emphasized by its performance across problems in Tables \ref{tab:number of iterations to solution} and \ref{tab:advanced problems}. Additionally, the prior information captured by SIGS in the training phase provides an \emph{efficient initial guess}. As explained in Property 2, SIGS considers an initial guess very close to the true solution and thus converges almost immediately, as demonstrated in Table \ref{tab:number of iterations to solution} and explored further in Section \ref{sm:additional experimental results}. This is also illustrated in Figure \ref{fig:initial guess and convergence} which shows an initial guess in the parameterized space. Again, we see in Table \ref{tab:advanced problems} that SIGS handles a variety of problems, but it can also construct intricate solutions via \emph{self-composition}. As explained in Property 3, SIGS is able to solve the iterative wave equation by sequentially composing partial solutions of several terms until the full 16-term solution is recovered, see Section \ref{sm:additional experimental results}. Finally, we observe that Property 4 is upheld as SIGS is able to \emph{implicitly incorporate conditions} along the boundaries into the representative solution space. This is evident both in the rapid convergence, as well as the insensitivity to the role such conditions play in the training phase. The results demonstrably show that SIGS has several characteristics which equip it to quickly and reliably converge to solutions, irrespective of their complexity.

% The results consistently show that SIGS has several advantages over previously proposed methods. One of which is notably Property 2: the efficient choice of the initial guess. Other machine learning approaches such as GP, ACP, ABCP, and (FB)PINNs are initialized from a "cold-start". They have no access to prior information and cannot learn from relationships between different datasets. As a result, they may fail to solve more challenging problems, or require many iterations to reach a solution. SIGS, however, considers an initial guess very close to the true solution and thus converges almost immediately, as demonstrated in Section \ref{sm:additional experimental results}. Alternatively, Mathematica uses its \emph{symbolic computation engine}, applying several techniques to find analytical solutions, which should, in theory, be applicable across a broad spectrum of DEs. Nonetheless, this approach still fails to solve the NLODEs, SODEs, and 4 of the 5 PDEs. 

% In contrast, SIGS has been designed specifically for the purpose of transferring knowledge among DE operators and their solutions. Our model, as a result, reliably handles a plethora of differential equations, as explained by Property 1 \emph{one-solver-solves-all}. Additionally, the prior information captured by SIGS in the training phase provides an \emph{efficient initial guess}. This rapidly speeds up the number of iterations to convergence, since the initial proposed solution is already close to the true solution, explained by Property 2. This is also illustrated by Figure \ref{fig:initial guess and convergence} which shows an initial guess in the solution space. Again, we see in Table \ref{tab:advanced problems} that SIGS handles a variety of problems, but it can also construct intricate solutions via \emph{self-composition}. As explained in Property 3, SIGS is able to solve the iterative wave equation by sequentially composing partial solutions of several terms until the full 16-term solution is recovered, see Section \ref{sm:additional experimental results}. Finally, we observe that Property 4 is upheld as SIGS is able to \emph{implicitly incorporate conditions} along the boundaries into the representative solution space. This is evident both in the rapid convergence, as well as the insensitivity to the role such conditions play in the training phase. The results demonstrably show that SIGS has several characteristics which equip it to quickly and reliably converge to solutions, irrespective of their complexity.

\begin{table*}[]
\caption{Number of iterations to convergence upon the true solution or best approximation. Errors of approximate methods are also displayed. Due to its strategic initialization, SIGS immediately converges upon the true solution in nearly all cases. For three problems, it requires more than one iteration to find the true solution, and for all but one problem it converges upon this solution in the fewest iterations. Mathematica provides no notion of "iterations", therefore we only designate when a solution is reached with a {\cmark} and when one is not reached by {\xmark}. GP, ACB, and ABCP results are taken from the respective references. ACP and ABCP were not applied ({\color{red}{NA}}) to all problems in the original works, and are therefore not available in this table. Furthermore, ABCP experiments were run 30 times and averaged, resulting in decimal numbers of iterations. For PINNs and FBPINNs, the error is reported for the iteration at which loss stops decreasing, or a maximum of $5\times 10^6$ for 1-dimensional and $5\times10^4$ for 2-dimensional problems. Errors greater than 20\% are highlighted in red.}
\centering
% \begin{adjustbox}{max width=0.8\textwidth}
\begin{tabular}{lccccccccc}
\toprule
\textbf{Problem} & \textbf{SIGS} & \textbf{Mathematica} & \textbf{GP} & \textbf{ACP} & \textbf{ABCP} & \multicolumn{2}{c}{\textbf{PINNs}} & \multicolumn{2}{c}{\textbf{FBPINNs}} \\
                 & (ours) &   &          &             &              & Error & Iterations & Error & Iterations \\
\midrule
ODE 1   & \textbf{1} & \cmark &  653 & 58 & 29 & \color{red}{51.9}\% & 500k & 0.627\% & 500k \\
ODE 2   & \textbf{1} & \cmark &  742 & 70 & 155 & \color{red}{47.7}\% & 500k & 0.113\% & 500k \\
ODE 3   & \textbf{1} & \cmark &  714 & 60 & 182 & 1.91\% & 500k & 0.007\% & 500k \\
ODE 4   & \textbf{1} & \cmark &  441 & 75 & 337 & 0.239\% & 500k & 0.046\% & 500k \\
ODE 5   & \textbf{1} & \cmark &  451 &  \color{red}{NA} & 47 & 0.015\% & 283k & \color{red}{74.7}\% & 500k \\
ODE 6   & \textbf{1} & \cmark &  444 &  \color{red}{NA} & 168 & 0.009\% & 113k & 0.586\% & 218k \\
ODE 7   & \textbf{1} & \cmark &  66 & 50 & 31 & 1.45\% & 500k & 0.126\% & 115k \\
ODE 8   & \textbf{1} & \cmark &  3 &  \color{red}{NA} &  \color{red}{NA} & 1.95\% & 500k & 0.243\% & 80k \\
ODE 9   & \textbf{1} & \cmark &  5 &  \color{red}{NA} &  \color{red}{NA} & 0.091\% & 139k & 0.108\% & 164k \\
NLODE 1 & \textbf{1} & \xmark &  86 & 53 & 4.57 & 0.216\% & 168k & 0.011\% & 13k \\
NLODE 2 & \textbf{1} & \xmark &  191 & 80 & 6 & 11.3\% & 149k & \color{red}{65.4}\% & 500k \\
NLODE 3 & 18         & \xmark & 161 & 79 & \textbf{5.9} & 1.68\% & 500k & \color{red}{79.2}\% & 500k \\
SODE 1  & \textbf{1} & \xmark &  201 & 42 & 17.3 & 17.1\% & 500k & 0.026\% & 10k \\
SODE 2  & \textbf{1} & \xmark &  234 &  \color{red}{NA} & 241 & 7.08\% & 500k & 0.006\% & 10k \\
PDE 1   & \textbf{1} & \cmark &  203 & 65 & 17 & 5.24\% & 50k & 0.099\% & 31k \\
PDE 2   & \textbf{5} & \xmark &  154 & 74 & 23 & 8.41\% & 50k & 0.381\% & 50k \\
PDE 3   & \textbf{1} & \xmark &  207 & 50 & 23 & \color{red}{21.1}\% & 50k & 0.737\% & 45k \\
PDE 4   & \textbf{1} & \xmark &  444 &  \color{red}{NA} & 155 & \color{red}{20.7}\% & 50k & 2.42\% & 10k \\
PDE 5   & \textbf{2} & \xmark &  797 & 70 & 53 & 16.1\% & 50k & 0.270\% & 50k \\
\bottomrule
\end{tabular}
% \end{adjustbox}

\label{tab:number of iterations to solution}
\end{table*}

\begin{table*}[]
   
\centering
\begin{minipage}{0.5\textwidth}
 \caption{Convergence Analysis for CMA-ES with different boundary condition weights for the outgoing wave (Outg. Wave) and diffusion equation (Diff).}
    \centering
    % \begin{adjustbox}{max width=0.8\textwidth}
    \begin{tabular}{lccc}
    \toprule
    \textbf{Problem} & \textbf{SIGS} & \textbf{PINNs} & \textbf{FBPINNs} \\
    \midrule
    1D Wave & 0.000 & 41.21 & 95.26 \\
    1D Diffusion & 0.000 & 5.693 & 56.74 \\
    Outgoing Wave & 0.000 & 0.349 & 86.67 \\
    Burgers' Eq. & 0.000 & 13.92 & 23.01 \\
    Iterative Wave Eq. & 3.896 & 74.87 & 111.4 \\
    Shallow Water ($\rho$) & 0.000 & 70.91 & 58.32 \\
    Shallow Water ($u_x$) & 0.001 & 59.18 & 53.33 \\
    Shallow Water ($u_y$) & 0.000 & 51.70 & 43.97 \\
    \bottomrule
    \end{tabular}
    % \end{adjustbox}
    
    \label{tab:advanced problems}

\end{minipage}
\hspace{0.05\textwidth} % Adjust the horizontal space between the tables
\begin{minipage}{0.3\textwidth}
\caption{Relative L2 error metrics for advanced problems using the decoded predicted solution.}
    \centering
    % \begin{adjustbox}{max width=0.8\textwidth}
    \begin{tabular}{cccc}
    \toprule
    \textbf{Weight} & \textbf{Outg. Wave} & \textbf{Diff.} \\ \hline
    0.0  & 4 & 2 \\     
    0.5  & 6 & 2 \\     
    1.0  & 5 & 2 \\     
    5.0  & 7 & 2 \\     
    50   & 6 & 2 \\     
    100  & 8 & 2 \\ \hline
    \end{tabular}
    % \end{adjustbox}

    \label{tab:boundary_weights_fixed_seed}
\end{minipage}
\end{table*}


%%% ORIGINAL TABLES BELOW - were reformatted %%%

% \begin{table*}[]
% \centering
% \begin{adjustbox}{max width=0.8\textwidth}
% \begin{tabular}{lccccccccc}
% \toprule
% \textbf{Problem} & \textbf{SIGS} & \textbf{Mathematica} & \textbf{GP} & \textbf{ACP} & \textbf{ABCP} & \multicolumn{2}{c}{\textbf{PINNs}} & \multicolumn{2}{c}{\textbf{FBPINNs}} \\
%                  & (ours) &   &          &             &              & Error & Iterations & Error & Iterations \\
% \midrule
% ODE 1   & \textbf{1} & \cmark &  653 & 58 & 29 & \color{red}{51.9}\% & 500k & 0.627\% & 500k \\
% ODE 2   & \textbf{1} & \cmark &  742 & 70 & 155 & \color{red}{47.7}\% & 500k & 0.113\% & 500k \\
% ODE 3   & \textbf{1} & \cmark &  714 & 60 & 182 & 1.91\% & 500k & 0.007\% & 500k \\
% ODE 4   & \textbf{1} & \cmark &  441 & 75 & 337 & 0.239\% & 500k & 0.046\% & 500k \\
% ODE 5   & \textbf{1} & \cmark &  451 &  \xmark & 47 & 0.015\% & 283k & \color{red}{74.7}\% & 500k \\
% ODE 6   & \textbf{1} & \cmark &  444 &  \xmark & 168 & 0.009\% & 113k & 0.586\% & 218k \\
% ODE 7   & \textbf{1} & \cmark &  66 & 50 & 31 & 1.45\% & 500k & 0.126\% & 115k \\
% ODE 8   & \textbf{1} & \cmark &  3 &  \xmark &  \xmark & 1.95\% & 500k & 0.243\% & 80k \\
% ODE 9   & \textbf{1} & \cmark &  5 &  \xmark &  \xmark & 0.091\% & 139k & 0.108\% & 164k \\
% NLODE 1 & \textbf{1} & \xmark &  86 & 53 & 4.57 & 0.216\% & 168k & 0.011\% & 13k \\
% NLODE 2 & \textbf{1} & \xmark &  191 & 80 & 6 & 11.3\% & 149k & \color{red}{65.4}\% & 500k \\
% NLODE 3 & 18         & \xmark & 161 & 79 & \textbf{5.9} & 1.68\% & 500k & \color{red}{79.2}\% & 500k \\
% SODE 1  & \textbf{1} & \xmark &  201 & 42 & 17.3 & 17.1\% & 500k & 0.026\% & 10k \\
% SODE 2  & \textbf{1} & \xmark &  234 &  \xmark & 241 & 7.08\% & 500k & 0.006\% & 10k \\
% PDE 1   & \textbf{1} & \cmark &  203 & 65 & 17 & 5.24\% & 50k & 0.099\% & 31k \\
% PDE 2   & \textbf{5} & \xmark &  154 & 74 & 23 & 8.41\% & 50k & 0.381\% & 50k \\
% PDE 3   & \textbf{1} & \xmark &  207 & 50 & 23 & \color{red}{21.1}\% & 50k & 0.737\% & 45k \\
% PDE 4   & \textbf{1} & \xmark &  444 &  \xmark & 155 & \color{red}{20.7}\% & 50k & 2.42\% & 10k \\
% PDE 5   & \textbf{2} & \xmark &  797 & 70 & 53 & 16.1\% & 50k & 0.270\% & 50k \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Number of iterations to convergence upon the true solution or best approximation. Errors of approximate methods are also displayed. Due to its strategic initialization, SIGS immediately converges upon the true solution in nearly all cases. For three problems, it requires more than one iteration to find the true solution, and for all but one problem it converges upon this solution in the fewest iterations. Mathematica provides no notion of "iterations", therefore we only designate whether a solution is reached. ABCP experiments were run 10 times and averaged. For PINNs and FBPINNs, the error is reported for the iteration at which loss stops decreasing, or a maximum of $5\times 10^6$ for 1-dimensional and $5\times10^4$ for 2-dimensional problems.}
% \label{tab:number of iterations to solution}
% \end{table*}

% \begin{table*}[]
% \centering
% \begin{adjustbox}{max width=0.8\textwidth}
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Problem} & \multicolumn{2}{c}{\textbf{SIGS}} & \multicolumn{2}{c}{\textbf{PINNs}} & \multicolumn{2}{c}{\textbf{FBPINNs}} \\
%  & \textbf{L2 Error} & \textbf{L1 Error} & \textbf{L2 Error} & \textbf{L1 Error} & \textbf{L2 Error} & \textbf{L1 Error} \\
% \midrule
% 1D Wave & 0.000 & 0.000 & 41.21 & 33.53 & 95.26 & 93.53 \\
% 1D Diffusion & 0.000 & 0.000 & 5.693 & 6.535 & 56.74 & 69.32  \\
% Outgoing Wave (Localized Dump) & 0.000 & 0.000 & 0.349 & 0.209 & 86.67 & 77.95 \\
% Burgers' Eq. & 0.000 & 0.000 & 13.92 & 2.862 & 23.01 & 15.76 \\
% Iterative Wave Eq. & 3.896 & 3.731 & 74.87 & 81.36 & 111.4 & 111.1 \\
% Shallow Water ($\rho$) & 0.000 & 0.000 & 70.91 & 118.5 & 58.32 & 37.08 \\
% Shallow Water ($u_x$) & 0.001 & 0.012 & 59.18 & 122.3 & 53.33 & 64.97 \\
% Shallow Water ($u_y$) & 0.000 & 0.012 & 51.70 & 177.9 & 43.97 & 89.27 \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Relative error metrics for advanced problems using the decoded predicted solution.}
% \label{tab:advanced problems}
% \end{table*}


% \begin{table}[h]
% \centering

% \begin{tabular}{ccccccc}
% \hline
% \textbf{Problem Type}     & \textbf{Weight (0.0)} & \textbf{Weight (0.5)} & \textbf{Weight (1.0)} & \textbf{Weight (5.0)} & \textbf{Weight (50)} & \textbf{Weight (100)} \\ \hline
% Outgoing Wave             & 4                    & 6                    & 5                    & 7                   & 6                   & 8               \\     
% Diffusion Equation        & 2                    & 2                    & 2                    & 2                   & 2                   & 2                     \\ \hline
% \end{tabular}
% \caption{Convergence Analysis for CMA-ES with Different Boundary Condition Weights}
% \label{tab:boundary_weights_fixed_seed}
% \end{table}

%%% ORIGINAL TABLES ABOVE - were reformatted %%%

% We also investigate the ability of SIGS to learn solutions which are not represented in the training distribution. The ability to generalize to \emph{out of distribution} solution lengths comes from the \emph{self-compositional} power outlined in Property 2.3. The longest solutions in the training set have a maximum of 4 terms. In this experiment, SIGS attempts to find a solution with 16 separate terms, well outside of the training regime. This solution is presented in \textbf{SM} Eqn. \ref{eq:16 term solution}. In this setting, SIGS predicts the first four terms of the solution. Next, the known solution components are incorporated into the governing equations, keeping the initial and boundary conditions fixed. From this new problem setting, SIGS discovers the next four solution terms, and the process is iterated until the full analytical solution is recovered. 


\section{Discussion}


\paragraph{Summary} We propose SIGS, a neuro-symbolic AI algorithmic framework for discovering analytical (closed-form) solutions of differential equations constructed through two key observations. The first is that traditional approaches for deriving explicit solutions of differential equations, such as the separation of variables, or integral transforms, are inherently compositional, implying that they consider specific building blocks that they subsequently combine using pre-defined rules. The second observation is that iterative solution algorithms consider a parameterization of a candidate function space and then search in this finite-dimensional space of parameterizations for the optimal solutions. SIGS generalizes the idea of composing analytical solutions by interpreting their generation as a formal language problem to build a grammar-generated function space of differential equation solutions. This grammar-generated function space is embedded into a low-dimensional continuous manifold, which is then searched using a probabilistic nonlinear iterative algorithm. We test the proposed methodology for several ODEs, PDEs, and systems thereof, showing the effectiveness of the method against established baselines both of symbolic and neural network types and commercial software. We show both theoretically and through experiments that the effectiveness of the method emerges from the properties that it inherits due to its grammar structure. 

\paragraph{Limitations} The SIGS framework aims to discover analytical solution of differential equations. Despite showing the capabilities of SIGS on a diverse set of DE problems, the expressive power of the proposed grammar and model is still limited. Complex engineering problems that include discontinuities or multi-scale phenomena, like turbulence modeling, may require constructing much more sophisticated grammars, considering special functions, and very long symbolic expressions such that, even with the self-compositions, the model might not be able to find them efficiently. We discuss how a grammar-generated function space inherits properties, like topology, denseness or continuity, from the composition of primitives. However, the ability of the method to find solutions is only as good as the design of the grammar. When considering very complex grammars, the dimensionality of the latent space will need to increase to capture the underlying manifold which means that more sophisticated optimization strategies may be required. Another limitation of the method is accurately predicting real numbers, as they are defined by long sequences of rules. 


\paragraph{Future Work} In the future, we plan to address exactly these limitations. Drawing inspirations from traditional approaches that consider special functions like Bessel or Airy functions, which cannot be expressed in the terms of simpler ones, we plan to introduce special functions as building blocks of the grammar that are pre-trained multi-task operators such as Poseidon \cite{poseidon}, MPP \cite{mpp}, DPOT \cite{dpot}, UPT \cite{alkin2024universal} or UPS \cite{shen2024ups}, to serve as initial condition dependent special functions. This way we can increase the expressivity of the SIGS method by reinforcing the neural aspect of the method. In this paper, we pose the solution of DEs as a language problem, we learn every possible "answer" before we pose a specific question, and search for a specific answer. This paves the way for the synergetic implementation of large language models \cite{zhao2023survey} within the SIGS framework. Moreover, by conditioning the symbolic modality to a numerics modality, we can construct a more sophisticated search algorithm. The use of tokenization can also be potentially leveraged for efficiently embedding real numbers into compact representations \cite{kamienny2022end, kamienny2023deep}. 

\clearpage
\bibliographystyle{icml2025}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\part{Supplementary Material} % Start the appendix part
% \section*{Supplementary Material}

%% appendix toc
\parttoc % Insert the appendix TOC


\section{Literature Review}
\label{sm:literature review}
As we discussed in Section \ref{sec:methods}, the iterative solution algorithms are categorized into neural, symbolic, and neuro-symbolic approaches, based on the parameterization of the space of functions that they consider. 
We exclude numerical methods from this more extensive review, since they are targeting solely approximate solutions and their strengths and weaknesses are generally well-known.

\paragraph{Neural Approaches} A neural solver considers a nonlinear parameterization of the solution function using a neural network. Then, an iterative solution algorithm is defined on the space of the neural network weights to find the parameterization that fits the residual of the differential equation with the lowest error. This family of methods is defined under the Physics-Informed Machine Learning umbrella \cite{karniadakis2021physics}. A variety of physics-informed approaches has been presented \cite{sirignano2018dgm, bhatnagar2019prediction, zhu2018bayesian, khoo2021solving}, with physics-informed neural networks (PINNs)  \cite{raissi2018deep, lagaris1998artificial} one of the most widely applied.  A comprehensive overview is given by Toscano et. al. \cite{toscano2024pinns}. Despite successful application in many different fields \cite{kissas2020machine, manav2024phase, cai2021physics, mao2020physics}, the generalization capabilities of these methods are limited, as they require training a new neural network for new conditions, meshes or physics. Moreover, several difficulties with PINNs have been reported in practice, such as slow convergence during training and strong sensitivity to the initial network weights \cite{de2023operator, wang2022and, wang2021understanding}.

% even in cases where measurements of the baseline function are assumed in the domain. 

\paragraph{Symbolic Approaches} Solving (partial) differential equations ((P)DEs) is a fundamental challenge in the domain of applied mathematics and across a variety of computational sciences. Traditionally, based on the complexity of the DE system, solving it is possible through the use of \textit{Analytical} techniques. For example, linear PDEs can be transformed into simpler ordinary differential systems (ODEs) with techniques such as the \textit{Separation of Variables} \cite{miller1977symmetry} and the Risch method \cite{risch1969the}, while in the case of periodic domains \textit{Fourier transforms} \cite{debnath2006integral, dyke2014an} can be employed. Moreover, there exists commercial software, such as Mathematica \cite{mathematica}, that provides explicit solutions of differential equations as a black-box solver with proprietary techniques. 

A different approach relies on the symbolic solution of DEs. Similar to numerical solvers, these methods receive a symbolic form of the operator, a domain discretization, and initial (and boundary) conditions as arguments. Then, they construct random expressions combining a predefined set of binary and unary operations, variables, and constants. The generated symbolic expressions are checked for how well they satisfy $\mathcal{R}(u)$ and the guess is updated until a symbolic solution of a DE is discovered. Tsoulos et. al. \cite{Tsoulos2006SolvingDE} propose an approach for modeling the genome of the symbolic expressions of differential equations using Formal Grammars and using a Genetic Programming approach for evolving the expression genomes. Liang et. al. \cite{liang2022finite} propose a method that samples a finite number of expressions using a Reinforcement Learning pipeline and performs a weighted average over them to provide an approximate solution of Differential Equations. The drawback of this method is that it needs to be re-trained for each specific task, and that the increase in length of the sub-expressions results to combinatorial explosion of the search algorithm even though this is mitigated by considering sub-expressions of small length. This method, however, also possesses the a-posteriori self compositions property of SIGS. Chaquet et. al. \cite{chaquet2019using} consider a CMA-ES strategy for finding the coefficients of the basis function parameters that parameterize the space of candidate functions. This method also considers probabilistic updates for the iterative method. Boudouaoui et. al. \cite{boudouaoui2020solving} propose a framework that considers an Artificial Bee Colony Programming for solving differential equations, which is a more sophisticated algorithm for performing heuristic search by mimicking the way that bee colonies search for resources. Saeton et. al. \cite{seaton2010analytic} propose a framework for performing a Cartesian Genetic Programming approach for sampling candidate expressions which is shown to have improved properties for faster heuristics search. The symbolic approaches proposed in the literature are computationally expensive due to the combinatorial complexity of the underlying graph topology \cite{virgolin2022symbolic, kissas2024language}, the sensitive dependence on the initial guess, and the lack of a structured way to include domain knowledge. 

\paragraph{Neuro-Symbolic Approaches} Neuro-symbolic approaches are built to combine elements of both symbolic and neural algorithms to enhance the performance of the solver, and few such attempts have been reported yet. Wei et. al. \cite{wei2024closed} propose a methodology that considers a two-step optimization process: It first randomly samples tree structures with variable coefficients \cite{dsr}, and then discovers a policy that provides the parameters with the lowest error by combining reinforcement learning and a parameter neural network. However, this method is very inefficient as the heuristic search is performed in two levels, the skeletons and the parameters. We consider the method proposed by Lampe et. al. \cite{lample2019deep} as a neuro-symbolic method as it is capable of using symbolic computations to discover the solution of ODEs. However, this method is very limited as it works only for explicit ODEs and expensive as it considers very large transformer models. 


\section{On the Formulation and Properties of SIGS}
\label{sec:SIGS properties}

\subsection{Intuitive introduction to Grammars}
\label{sm:grammar intuition}
Following \cite{kissas2024language}, we will use this section to expand on our definition  of context-free grammars (CFG) from Section \ref{sec:grammar}, with the example of the grammar used in the implementation of our experiments (Eqn. \eqref{eqn:grammar}).

A grammar $\mathcal{G}=(\Psi,N,R,S)$ is made up of the following components:
The \emph{alphabet} $\Phi$ contains the primitive building -blocks of arithmetic expressions. Since these are the symbols that end up in an expression, terminating any procedure we used to generate the expression, they are also called the \emph{terminal symbols}. They can, for example, contain constants, variables, unary and binary operators, as well as punctuation marks:
\begin{equation*}
\begin{split}
    \Phi = \{& \pi,~0,~1,~2,~\dots,\\
             & t,~x,~\dots, \\
             & \sin,~^2,~\dots, \\
             & +,*,~\dots, \\
             & (,~),~\dots \}
\end{split}
\end{equation*}

The \emph{Kleene closure} $\Phi^*$ contains all possible sequences of terminal symbols, also called \emph{strings} or \emph{expressions}.
This includes, a-priori, also sequences that have no arithmetic meaning due to wrong syntax:
\begin{equation*}
\begin{split}
    \Phi^* = \{& [t], \\
               &[\exp,~(,~t,~)],\\
               & [),~\exp,~t],~\dots \}
\end{split}
\end{equation*}

\emph{Non-terminal symbols} $N$ are variables used when generating strings from terminal symbols. They appear in intermediate forms of strings, indicating that more terminal symbols need to be inserted in order for the string to be complete. In order to define different types of place holders, an arbitrary number of non-terminal symbols can be used. In our grammar, we use the starting symbol $S$, that is required to be part of $N$, as well as an auxiliary variable $T$, and a variable $D$ that indicated that digits should be inserted,
\begin{equation*}
    N = \{S,~T,~D\}
\end{equation*}

\emph{Production rules} $R$ dictate in what way non-terminal symbols $N$ may be replaced by terminal symbols $\Phi$ in order for the terminal string to make syntactic sense. For example, the number of arguments of unary and binary expressions is encoded here, as well as the correct placement of parentheses. Formally, every rule $r$ maps a non-terminal symbol $\alpha\in N$ to a sequence of terminal and non-terminal symbols $\beta\in (\Phi\cup N)^*$. If a non-terminal symbol appears on the right hand side of several rules, any of these rules can be used to replace it. Iteratively, a terminal string $w\in \Phi^*$ is derived from the starting symbol $S\in N$ by the application of a sequence of rules $R_i\in R$, which we denote as $S \rightarrow^* w$. Equation \eqref{eqn:grammar} includes the following rules,
\begin{equation*}
\begin{split}
    R = \{  & R_0:~S\rightarrow T,\\
            & R_1:~T\rightarrow t, \\
            & R_2:~T\rightarrow \exp(S), \\
            & R_3:~S\rightarrow S * T, \\
            & R_4:~S\rightarrow S-T, \\
            & R_5:~T\rightarrow D, \\
            & R_6:~D\rightarrow 0,~\dots \}
\end{split}
\end{equation*}

As an example, the expression $\exp(-t)$ is derived based on the above rules as
\begin{equation*}
\begin{split}
    S & \xrightarrow[]{R_0} T, \\
      & \xrightarrow[]{R_2} \exp(S), \\
      & \xrightarrow[]{R_4} \exp(S - T), \\
      & \xrightarrow[]{R_{0},~R_{5},~R_{6}} \exp(0 - T), \\
      & \xrightarrow[]{R_{1}} \exp(- t) \\
\end{split}
\end{equation*}

This process can be represented as creating a \emph{derivation tree}, where all leaves will represent constants and variables, and all other nodes are given by unary and binary operations. The edges then correspond the rules used to create the tree, including punctuation symbols. For an example of a tree, see Figure \ref{fig:main}.


\subsection{Interpretation Map: From Syntax to Semantics}
\label{sm:iterpretation map}
Following Section \ref{sec:grammar}, the grammar $\mathcal{G}$ is designed to generate standard arithmetic expressions in $\mathcal{L(G)}$.
Given an appropriate domain $D$ and co-domain $D'$ of solutions to a given DE, collected in the space $\mathcal{C}(D,D')$, we now wish to assign each string $w \in \mathcal{L}(\mathcal{G})$ a corresponding solution function $u = u_w\in \mathcal{C}(D,D')$.
This is performed by the \emph{interpretation} function
\begin{equation*}
    \mathcal{I} (\mathcal{L}(\mathcal{G})) \rightarrow \mathcal{C}(D,D').
\end{equation*}
\begin{itemize}
    \item \textbf{Constants} $\phi\in\{''1'',  \dots\}$ are mapped to constant functions, e.g.
    \begin{equation*}
        \mathcal{I}(''1'') = u\in\mathcal{C}(D,D')\quad\text{with}\quad u(x)=1.
    \end{equation*}
    \item \textbf{Variables} $\phi\in\{''x'',  \dots\}$ are mapped to identity functions in the respective variable, e.g.
    \begin{equation*}
        \mathcal{I}(''x'') = u\in\mathcal{C}(D,D')\quad\text{with}\quad u(x)=x.
    \end{equation*}
    \item \textbf{Unary expressions}  $\phi\in\{''\sin'',  \dots\}$ are mapped to functions in one argument, e.g.
    \begin{equation*}
        \mathcal{I}(''\sin'') = u\in\mathcal{C}(D,D')\quad\text{with}\quad u(x)=\sin(x).
    \end{equation*}
    \item \textbf{Binary operations}  $\phi\in\{''+'',  \dots\}$ are mapped to functions in two arguments, e.g.
    \begin{equation*}
        \mathcal{I}(''+'') = b\in\mathcal{C}(D\times D,D')\quad\text{with}\quad b(x,x')=x+x'.
    \end{equation*}
    \item \textbf{Syntactic symbols}  $\phi\in\{''('',  \dots\}$ are respected to ensure the ordering of operations. 
\end{itemize}
As an example, string $w=[''\exp'',''('',''-'',''t'','')'']$ is interpreted as
\begin{equation*}
    \mathcal{I}(w)(t) = \exp(-t).
\end{equation*}


\subsection{The Grammar-Generated Function Space}
\label{sm:grammar generated function spaces}

We defined the set:
\begin{equation*}
    \mathcal{U} (\mathcal{G}) = \{ \mathcal{I}(w) | w\in \mathcal{L}(\mathcal{G}) \} \subseteq \mathcal{C}(D),
\end{equation*}
where each element of $\mathcal{U} (\mathcal{G})$ is a finite composition of operations and variables specified by $\mathcal{G}$, and by $\mathcal{C}(D)$ we denote a the space of continuous functions on a  domain $D$ that fits the DE problem at hand. The grammar $\mathcal{G}$, defined in Section \ref{sm: gram_data}, includes rules that ensure closure under addition, product, and scalar multiplication. 

By definition, $\mathcal{G}$ can produce any constant $c \in \mathbb{R}$, and is closed under the binary operators $+,$ $\cdot$, then all constant functions are in $\mathcal{U} (\mathcal{G})$, for any $u_1, u_2 \in \mathcal{U} (\mathcal{G})$, $(u_1 + u_2) \in \mathcal{U} (\mathcal{G})$, and $(u_1 \cdot u_2) \in \mathcal{U} (\mathcal{G})$, and $c \cdot u \in \mathcal{U} (\mathcal{G})$. Therefore, $\mathcal{U} (\mathcal{G})$ can form a subalgebra of $\mathcal{C}(D)$ that is the canonical function space generated by the grammar. Moreover, the grammar is constructed by finite compositions of continuous functions and thus $u \in \mathcal{U} (\mathcal{G}) \subseteq \mathcal{C}(D)$.

A set $\mathcal{U} (\mathcal{G}) \subset \mathcal{C}(D)$  is called a \emph{Grammar-Generated Function Space} if there exists a CFG $\mathcal{G}$ over arithemetic and elementary symbols such that
\begin{equation*}
    \mathcal{U} (\mathcal{G}) = \{ \mathcal{I}(w) | w \in \mathcal{L(G)} \}.    
\end{equation*}
A grammar thus fully determines the syntax, and an interpretation function $\mathcal{I}$ as described in \textbf{SM} \ref{sm:iterpretation map} supplies the semantics, i.e. what function is meant by each expression. Hence, the interpretation of the language $\mathcal{I}(\mathcal{L}(\mathcal{G}))$ is a proper function space on domain $D$. Given that $\mathcal{U} (\mathcal{G}) \subseteq \mathcal{C}(D)$, it inherits the topology of $\mathcal{C}$ and is equipped with the supremum norm:
\begin{equation*}
    \| u \|_\infty = \sup_{x \in D} | u(x) |.
\end{equation*}


\subsection{Density of Grammar-Generated Functions}
\label{sec:dense property}

We have already shown that $\mathcal{U} (\mathcal{G})$ is a subalgebra of $\mathcal{C}(D)$. A set of functions $\mathcal{A} \subset \mathcal{C}(D)$ separates points if for any distinct points $x,y \in D$, there exists a function $u \in \mathcal{A}$ such that $u(x) \neq u(y)$. Assuming $D = [a,b]^d$, for any distinct points $x,y \in [a,b]^d$ there exists at least one coordinate $i$ such that $x_i \neq y_i$. The corresponding function $u_i$ satisfies $u_i(x) =x_i \neq u_i(y) = y_i$, and hence $\mathcal{U} (\mathcal{G})$ separates points. 

Having established that $\mathcal{U} (\mathcal{G})$ is a subalgebra of $\mathcal{C}(D)$, that it produces constant functions, the non-vanishing property, and that it separates points, we can now apply the Stone-Weierstrass Theorem:
\begin{theorem}[Real Version of Stone-Weierstrass Theorem]
    Let $\mathcal{A}$ be a subalgebra of $\mathcal{C}(D)$ that contains the constant function and separates points in $D$, where $D$ a compact Hausdorff space. Then $\mathcal{A}$ is dense in $\mathcal{C}(D)$ with respect to the supremum norm.
\end{theorem}
This results to the property that for every function $u \in \mathcal{C}(D)$ and every $\epsilon > 0$, there exists a function $g \in \mathcal{U} (\mathcal{G})$ such that:
\begin{equation*}
    \| u - g\|_\infty < \epsilon,
\end{equation*}
and the grammar-generated functions can approximate any PDE solution in $\mathcal{C}(D)$.



\subsection{Parameterizing the Function Space via Grammar Variational Autoencoders}
\label{sm:gvae}
Formal grammars define two very important operations linking a string to the series of rules applied to form it: parsing, where a list of rules is extracted from an expression, and generation, where an expression is generated using a list of rules. The Grammar Variational Autoencoder (GVAE) method is built on these two operations. More specifically, the encoder extracts a list of production rules that derive an expression and embeds it into a low-dimensional continuous vector space $Z\subseteq \mathbb{R}^m$. The decoder provides the most probable list of rules given an encoding, which is the output of the model. The GVAE is trained to provide the correct list of rules, with high probability, given the latent vector. 

\subsubsection{The GVAE encoder}
The encoder is an operation that first parses a string and then embeds it in a low-dimensional representation. The parsing function $P$ maps the leftmost derivation of a string $w \in\mathcal{L(G)}\subseteq (\Phi \cup N)^*$ to a sequence of production rules $\{R_i\}_{i=1}^{T}$ that derive $w$ starting from $S$. The parsing function is defined as:
\begin{equation}
    P: (\Phi \cup N)^* \rightarrow R^T,
\end{equation}
where $T$ is the length of the derivation, i.e. the number of production rules applied, and $R^T$ is the set of all sequences of rules of length $n$.  Given $w$, $P(w) = [r_1, ..., r_n] =:\mathbf{r}$. The one-hot encoding function $E$ maps $\mathbf{r}$ to a binary matrix $M$ where the rows index the rules (as given in the definition of the grammar, $R = \{R_1, ...\}$), and the columns represent the index in the derivation of $w$:
\begin{equation}
    E: R^T \rightarrow \{ 0,1 \}^{|R| \times T},
\end{equation}
where $|R|$ the total number of production rules, $T$ the sequence length, and $\{ 0,1 \}^{|R| \times T}$ is the set of binary matrices of size $|R| \times T$. Given $\mathbf{r}$, $M$ is constructed as:
% \begin{equation*}
% M_{j,k} = 
% \begin{cases}
% 1 & \text{if } j = k, \\
% 0 & \text{otherwise},
% \end{cases}
% \end{equation*}
\begin{equation*}
M_{j,k} = 
\begin{cases}
1 & \text{if } R_j = r_k, \\
0 & \text{otherwise},
\end{cases}
\end{equation*}
indicating by 1 that rule $R_j$ was used at derivation step $k$, such that in each column, there is exactly one 1.
% for $j=1,...,|R|$ the production rules $r_j \in R$ and $k=1,...,n$ the steps of the derivation.  
The parsing and one-hot encoding functions can be composed to represent each string as a matrix:
\begin{equation*}
    E \circ P : (\Phi \cup N) \rightarrow \{ 0,1 \}^{|R| \times n}.
\end{equation*}
In realistic applications, the length $T$ varies with the level of complexity needed to represent $w$. In this case, the sequences $\mathbf{r}$ can be padded to a user-specified maximum length $T_{max}$, decided by the length of the longest expected sequence and computational considerations. For this case, a special padding rule $r_p \notin R$ can be added to the production rules $\hat{R} = R \cup \{r_p \}$. The padding rule is defined as:
\begin{equation*}
    \hat{P} (\hat{\mathbf{r}}) = [r_1, ..., r_T, r_p, ..., r_p],
\end{equation*}
where $r_p$ is repeated $T_{max} -T$ times to fill up the missing rule indices. In the same fashion, $E$ is extended to $\hat{E}: \hat{R} \rightarrow \{ 0,1\}^{|\hat{R}| \times n_{max}}$.
We also introduce a mask vector $\Theta \in \{ 0,1\}^{T_{max}}$ to discern between $r_k$ and $r_p$, defined as:
\begin{equation*}
\Theta_k = 
\begin{cases}
1 & \text{if } r_k \neq r_p, \\
0 & \text{if } r_k = r_p,
\end{cases}
\end{equation*}
that zeros-out the rows of $M$ that correspond to $r_p$. Thus, the encoder is defined as the composition:
\begin{equation}
    \phi = F_{\chi} \circ \Theta \circ \hat{E} \circ P,
\end{equation}
where $F_{\chi}: \mathbb{R}^{|\hat{R}| \times T} \rightarrow \mathbb{R}^l $ a parametric map, in this case a composition of multiple convolutional neural networks. Then, the probabilistic encoder is defined as:
\begin{equation}
    q_\chi (z | w) = \mathcal{N} (z ; \mu, \gamma \sigma^2 \mathbf{I} ) ,
\end{equation}
where $\gamma$ a tightness parameter, $\mu = \mu' \circ \phi(w)$ with $\mu': \mathbb{R}^l \rightarrow \mathbb{R}^{m}$, and $\sigma = \sigma' \circ \phi (w)$ with $\sigma': \mathbb{R}^l \rightarrow \mathbb{R}^{m}$ parametric projection functions, modeled using one-layer neural networks. We can sample from this distribution during training using the reparameterization trick:
\begin{equation*}
    z = \mu +  \sigma \epsilon, \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \zeta \mathbf{I}),
\end{equation*}
where $\zeta$ is a scaling parameter. 

\subsubsection{The GVAE decoder}
The decoder of the GVAE is a function $D: \mathbb{R}^{m} \rightarrow \mathcal{L}(\mathcal{G})$ that given a latent vector $z \sim p(z)$ returns the string $w$. The derivation of a string $w \in S$ is performed by applying production rules from $R$, starting from the starting symbol $S$, until no non-terminal symbol is left in the intermediate expressions. For each non-terminal symbol $\alpha\in N$, the decoder chooses the next rule probabilistically from the set of rules $R_\alpha$ with $\alpha$ on the left-hand side, where it learns the probabilities $p(r_k | \alpha)$ such that they sum to one over $R_\alpha$, $\sum_{r_k \in R_{\alpha}} p(r_k | \alpha) = 1$.
% The decoder learns probabilities of grammar rules $\mathcal{P}_R = \{ p(r_k | \alpha) \}$ satisfying $\sum_{r_k \in R_{\alpha}} p(r_k | \alpha) = 1$, where $R_\alpha$ the set of rules with $\alpha$ on the left-hand side. The rule probabilities are sampled from a prior $p(\theta)$. The derivation of a string $w \in S$ is performed by replacing the non-terminal symbol $\alpha$ in the partial derivation $d \in D$, where $D$ the set of partial derivations, using a production rule $r_k \sim p(r_k | \alpha, \theta)$. This process continues until only terminal symbols are present in the string.
Given an observed data string $w = \{ w_i\}_{i=1}^T$, the model learns a variational distribution $q(\theta)$ of the posterior $p(\theta|w)$, by minimizing the objective:
\begin{equation*}
    L(z) = \mathbb{E}_{q(\theta)} [\log p(w | \theta) ] - \operatorname{KL} (q(\theta) | p(\theta)),
\end{equation*}
where $q(\theta) = \prod_{\alpha \in \Phi} q_{\alpha} (\theta_\alpha)$,  $z_\alpha$ the probability of a rule with $\alpha$ on its left-hand side and $q_\alpha (\theta_\alpha)$ a Dirichlet distribution as rule probabilities lie on a $|\hat{R}| -1$ probability simplex. The decoder of a GVAE is designed by treating the rule probabilities as latent variables influenced by a global latent vector $z$ and learning the rule probabilities and distribution over $z$. On a high level, this process is defined by considering a latent variable $z \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ and for each non-terminal $\alpha \in \Phi$ defining the probability:
\begin{equation*}
    p(r_k | \alpha, z) = \frac{\exp(\psi_{\alpha,k}(z) ) }{\sum_{r_j \in R_{\alpha}}\exp(\psi_{\alpha,j}(z))},
\end{equation*}
where $\psi_\alpha (z) \in \mathbb{R}^{|R_\alpha|}$ are the logits of rules with $\alpha$ on their left-hand side. In practice, the logit function $\psi_\alpha:\mathbb{R}^{m} \rightarrow \mathbb{R}^{|R_\alpha| \times n_{max}} $ is parameterized using a bidirectional Gated Recurrent Neural Network.  The variational objective becomes:
\begin{equation*}
    L(z) =  \mathbb{E}_{\theta(z|w)} [\sum_{t=1}^T \log p( r_t| \alpha_t, z) ] - \operatorname{KL} (q(z|w) | p(z)).
\end{equation*}
During the generation of expressions, the logit function $\psi_\alpha$ is in practice modeled by a composition of a logit function $\psi$ and a syntax mask $m_\alpha \in \{0,1\}^{|R_\alpha| \times n_{max}}$: 
\begin{equation*}
(m_\alpha)_{i,k} = 
\begin{cases}
1 & \text{if } r_{i,k} = r_{\alpha,k}, \\
0 & \text{otherwise},
\end{cases}
\end{equation*}
where $r_{\alpha,k}$ is a rule with $\alpha$ on it's left-hand side on the $k$-th derivation step. The syntax mask makes sure that the derived expressions are syntactically valid, by only allowing valid rules to be selected at each step. 

\subsection{Defining the Iterative Updates with CMA-ES}
\label{sec:iterative updates}

An update rule for the function $u^{k+1}$ is written as $u^{k+1} = u^k + \Delta u^k$ or more generally as $u^{k+1} = \mathcal{T}(u^k)$. In the case where we search the latent space $Z$ as described above, we set $u^k = \mathcal{I(D}(z^k))$. We chose to work with the CMA-ES method because the manifold might not be convex. The CMA-ES method works by generating a population of $M$ candidate solutions around the current best estimate $m^k=z^k$,
\begin{equation*}
    z_i^k = m^k + \sigma^k y_i^k, \quad y^k \sim \mathcal{N} (0, C^k), \quad i = 1, ..., M
\end{equation*}
where $\sigma^k$ is the step size, and evaluate the DE residual $\mathcal{R}(u_i^k)$ for all samples.
% \begin{equation*}
%     f_i^k = \mathcal{S}(\mathcal{D}(z_i^k)). 
% \end{equation*}
The mean updates are then defined as:
\begin{equation*}
    m^{k+1} = m^k + \sum_{i=1}^M w_i (z_i^k - m^k),
\end{equation*}
where $w_i$ positive weights that sum to one, obtained according to the DE residuals of the samples $\mathcal{R}(u_i^k)$.
Since $m^k=z^k$, we can write:
\begin{equation*}
    u^{k+1} = \mathcal{I(D} (z^k + \sum_{i=1}^M w_i (z_i^k - z^k) ))
\end{equation*}
or
\begin{equation*}
    u^{k+1} = \mathcal{T}(u^k) =  \mathcal{I(D} (z^k + \Delta z^k)),
\end{equation*}
where $ \Delta z^k = \sum_{i=1}^M w_i (z_i^k - z^k)$. The CMA-ES update rule within the GVAE framework represents a fundamentally different approach compared to traditional iterative methods. 

% In traditional numerical methods the updates occur on the function space $\mathcal{U}$, while in the proposed methodology the updates occur on the latent space $Z$, and the solutions are obtained through $\mathcal{D}$. Moreover, the updates rule in the traditional methods is nonlinear, see Equation \ref{}, while the updates in the proposed methodology are linear in the latent space followed by a nonlinear function $\mathcal{D}$. Traditional methods define $\Delta u^z$ using deterministic optimization methods, while the proposed methodology considers stochastic update rules. This method is thus suitable also for problems where local linear approximations are not effective. 



\subsection{On the Properties of SIGS} \label{sec:appendix_properties}
Here, we expand on the properties of SIGS listed in Section \ref{sec:prop}.

\subsubsection{Property 1: One-solver-solves-all}
\label{sec:one solver solves all}

We define each family of boundary, or initial value problem as a latent submanifold, thereby allowing the search algorithm of the proposed methodology to find solutions suitable for that particular class of problems. By doing so, SIGS can seamlessly handle a variety of differential equation problems, such as (non-)linear ODEs, PDEs, and even systems of ODEs and PDEs—using a single pre-trained autoencoder model.

% \paragraph{Defining Classes of Problems as Subsets of $\mathcal{U} (\mathcal{G})$:}
\paragraph{Tailored Grammars.}
Let $\mathcal{G} = (\Phi, N, R, S)$ be the largest possible grammar that generates a language $\mathcal{L}(\mathcal{G})$, which is interpreted to form the full solution space $\mathcal{U} (\mathcal{G}) = \{\mathcal{I}(w) \mid w \in \mathcal{L}(\mathcal{G})\}$ for all ODEs an PDEs that might possibly be considered. 
We then define classes of problems by distinguishing their respective solution spaces as subsets of $\mathcal{U} (\mathcal{G}')\subset\mathcal{U} (\mathcal{G})$, defined by a restricted grammar $\mathcal{G}'=(\Phi, N, R',S)$. $\mathcal{G}'$ is obtained by selecting a subset of production rules $R' \subseteq R$.
% This restricted grammar $\mathcal{G}'$ defines a sub-language $\mathcal{L}(\mathcal{G}') \subseteq \mathcal{L}(\mathcal{G})$ and thus induces a subspace of functions:
% \begin{equation*}
% \mathcal{U} (\mathcal{G}') = \{\mathcal{I}(w) \mid w \in \mathcal{L}(\mathcal{G}')\} \subseteq \mathcal{U} (\mathcal{G}).
% \end{equation*}
By an appropriate choice of $R'$, one can tailor $\mathcal{U} (\mathcal{G}')$ for specific problem classes. 
For instance, sinusoidal solutions are generated by rules where $''\sin''$ is the only unary function, and the space of functions in two dimensions $x$ and $y$ is restricted to one dimension by removing $''y''$ from $\Phi$. As for the autoencoder, the set $Z'$:
\begin{equation*}
Z' := \{ z \in Z \mid \mathcal{D}(z) \in \mathcal{L}(\mathcal{G}') \}
\end{equation*}
identifies the latent submanifold corresponding to the sub-language $\mathcal{L}(\mathcal{G}')$.  To find a solution in a particular class of problems, we restrict the optimization to the submanifold $Z'$. Because $Z'$ is chosen so that all $z' \in Z'$ map to functions in $\mathcal{U} (\mathcal{G}')$ appropriate for the problem at hand, this ensures that all candidate solutions considered by the search are admissible. Along the same lines, an existing grammar can be extended to a wider class of functions by adding terminal symbols to $\Phi$. It is a known favorable property of formal grammars that derive trees that the size of the language $\mathcal{L(G)}$ only grows linearly as new symbols are added \cite{paassen2022recursive, hopcroft1979introduction}. This is opposed to the exponential growth of possible random walks when strings are sampled from graphs (see Section \ref{sec:methods}).

\paragraph{Systems of DEs.}
In many practical scenarios, one needs to solve a system of $d$ differential equations, each potentially having a different solution function. Let $z_i \in \mathbb{R}^m$ be a latent vector for the $i$-th equation, obtained from the same GVAE encoder. Since each $z_i$ is sampled from a Gaussian distribution with zero mean and diagonal covariance, the concatenation $\tilde{z} = [z_1, z_2, \ldots, z_m] \in \mathbb{R}^{d \times m}$ is also a Gaussian random vector. By treating $\tilde{z}$ as the latent variable for a system of equations, we can apply the CMA-ES optimization to $\tilde{z}$ directly. The decoder $\mathcal{D}$ can be applied independently to each segment $z_i$ to produce a candidate solution for the $i$-th equation. Thus, one obtains a vector-valued solution function $(u_1, u_2, \ldots, u_m)$ that can be collectively optimized to minimize the combined residual of the entire system. This construction allows the SIGS framework to handle systems of ODEs or PDEs without any additional training or complexity changes. By simply increasing the dimensionality of the latent space and considering appropriate sub-grammars, or solution families, one ensures that all candidate solutions remain within the desired function class, enabling a single solver to efficiently handle multiple, coupled differential equations simultaneously.




\subsubsection{Property 2: Efficiently Choosing the Initial Guess for the Iterative Process} 
\label{sec:quality of initial guess}
The library of function expressions $\mathcal{U(G)}^* \subset \mathcal{U} (\mathcal{G})$ used for training the encoder-decoder model is stored, corresponding to latent vectors $Z^*\subset Z$. We assume that these samples are \emph{densely sampled} in the sense that they equally cover all regions of $Z$. This is, for example, reached by first sampling $Z^*=\{ z_i \}_{i=1}^N$ uniformly on $Z$, and then decoding to $\mathcal{U(G)}^* = \{\mathcal{I\circ D}(z_i)\}_{i=1}^N$. In the limit of infinite samples, $\mathcal{U(G)}^* \subset \mathcal{U} (\mathcal{G})$ becomes dense in $\mathcal{U} (\mathcal{G})$ as $N \rightarrow \infty$.

When a specific DE is given to find its solution, we perform a pre-search step to determine the expression $u_i\in \mathcal{U(G)}^*$ that is closest to fitting the DE in terms of its residual,
\begin{equation*}
    u^* = \arg \min_{u_i \in \mathcal{U(G)}^* } \| \mathcal{R}(u_i) \|.
\end{equation*}
In case $\mathcal{R}(u^*)=0$, The exact solution $\hat{u}=u^*$ is returned without starting the CMA-ES iterations.
If $\mathcal{R}(u^*)>0$, the initial guess of the CMA-ES iterations is set to be the best known sample, $z^{(0)} = \mathcal{E}\circ\mathcal{I}^{-1}(u^*)$.

Hence, the initialization is facilitated by (i) the shear size of $\mathcal{U(G)}^*$, since it gets more probable that it already contains an exact solution out of a discrete set of candidates, and (ii) the dense sampling of $Z^*$ in $Z$: If the pre-decoded samples are close to each other, $||z_i-z_j||<\varepsilon$, and span all of $\mathcal{U(G)}$, there is one of them that is equally close to the best solution $\hat{u}$, $||u^*-\hat{u}||<\varepsilon$. This assumes some regularity of $\mathcal{R}$ in relation to the sampling distance $\varepsilon$, in the sense that no local minimum in $\mathcal{R}$ "falls in between" the given samples. This is ensured by choosing sufficiently many samples $N$, and a sufficiently small search radius $\sigma$ within the CMA-ES iterations, which are chosen as hyperparameters of the algorithm.





\subsubsection{Property 3: Increasing the Complexity of the Solution Functions by A-posteriori Self-Compositions} 
\label{sec:aposteriori self compositions}
DE solutions can be of arbitrary complexity, i.e. represented by strings of expressions that are generated from arbitrarily many applications of production rules. Even though the encoder and decoder of SIGS are fit to sequences of a fixed length, it can generate more complex functions by \emph{self-composition}. Without this property, the complexity of the machine learning embeddings would have to grow with the function complexity to be represented.

\paragraph{Closedness of $\mathcal{U(G)}$.}
We observe that a language $\mathcal{L(G)}$, generated from a grammar $\mathcal{G}=(\Phi,N,R,S)$, is closed under application of unary and binary operations in the terminal symbols $\Phi$, respecting its production rules $R$. More specifically, for any unary $f\in\Phi$, binary $b\in\Phi$, and any strings $s,s'\in\mathcal{(L(G)}$,
\begin{equation*}
    f(s)\in\mathcal{L(G)},\quad b(s,s')\in\mathcal{L(G)},
\end{equation*}
as long as the application of $f$ and $b$ follows the production rules $R$. In other words, $\mathcal{L(G)}$ forms an algebra with its respective unary nad binary operations.
Intuitively, thinking of strings as trees, a binary operation $b$ concatenates the trees of $s$ and $s'$ at their roots, with the new root given by $b$.

Through the identification of strings and functions by the interpretation function, $\mathcal{I}(s)\in\mathcal{U(G)}~\forall s\in\mathcal{L(G)}$, this closedness translates to all functions in $\mathcal{U(G)}$.

\paragraph{Composing latent vectors.}
Even though we train the encoder and decoder of the GVAE on a fixed sequence length, the closedness of $\mathcal{U(G)}$ allows to concatenate them to arbitrary complexity. This is done by sampling $\{ z_i \}_{i=1}^N$ from the latent space $Z$ and decoding each to functions as $s_i = \mathcal{D}(z_i) \in \mathcal{L(G)})$. Thanks to the closedness of $\mathcal{L(G)}$, any valid sequence of terminal sequences $s_i$ and other terminal symbols (constants, unary and binary expressions) in $\Phi$ that respects the rules $R$ is a candidate string $s$ of increased complexity compared to $s_i$, and translates to a complex function as $u=\mathcal{I}(s)\in\mathcal{U(G)}$. Training the autoencoder on longer sequences reduces the combinatorial complexity after decoding required to reach a certain complexity.

So, in contrast to many other symbolic solvers, SIGS has the ability to compose more complex solutions from simpler ones out of the box, without having to re-train the autoencoder.




\subsubsection{Property 4: The Update Rule Implicitly Satisfies the Boundary/Initial Conditions}
\label{sec:update rule satisfies bc ic}

%% Comment
% This section is pretty long and notation-heavy - do we need all of it? I see the cruicial points as:
% - All parts of R, i.e. S and the distances to u_0 and g, are continuous in u, so also R is continuous in u.
% - if an iterative update minimizes R, also the single components of R are minimized.
% - Hence, in the limit, DE as well as IC and BC are perfectly fit.
% - In case R>0, any of DE, IC, and BC could be fit only approximately - there is no guarantee e.g. for exact matching of the initial condition as long as R>0.
% Hence, I think this is independent of the latent space, so we can remove point 7.

In this section, we make a formal argument which shows that by constructing a function space with an underlying grammar structure that allows us to find an initial guess very close to the true solution, the effect of the boundary and initial terms on the overall augmented residual is mitigated, and thus the convergence to the true solution remains unaffected even in this general residual construction.

% \begin{property}\label{thm:implicit_boundary_enforcement}
Let $(D^i,B^i[u^i],u^i_0,S^i)$ define a well-posed PDE problem on a domain $D$ with a unique solution $\hat{u}$ such that 
\begin{equation*}
S(\hat{u}) = 0,\quad \hat{u}(0,\cdot) = u_0,\quad B[\hat{u}] = g.
\end{equation*}

Consider an iterative scheme defined by a map $\mathcal{T}:\mathcal{U} (\mathcal{G}) \rightarrow \mathcal{U} (\mathcal{G})$, where $\mathcal{U} (\mathcal{G})$ is a suitable space of candidate solutions $u:D \rightarrow  \mathbb{R}$, and assume that $\hat{u}$ is a fixed point of $\mathcal{T}$, meaning $\mathcal{T}(\hat{u})=\hat{u}$.

Suppose that $\mathcal{T}$ is continuous and that the residual does not increase under $\mathcal{T}$:
\begin{equation*}
\mathcal{R}(\mathcal{T}(u)) \leq \mathcal{R}(u)\quad \forall u \in \mathcal{U} (\mathcal{G}),
\end{equation*}
in the basin of attraction of the parameterization $\hat{z}$ of the true solution $\hat{u}$.

Then for any $\delta>0$, there exists $\varepsilon>0$ such that if $\|u^{(0)} - \hat{u}\|_{\mathcal{U} (\mathcal{G})} < \varepsilon$, the sequence $\{u^{(k)}\}$ defined by $u^{(k+1)}=\mathcal{T}(u^{(k)})$ remains in a neighborhood of $\hat{u}$ where the boundary and initial conditions are nearly satisfied. In particular, for all $k$:
\begin{equation*}
\|B[u^{(k)}]-g\|_{\mathcal{U} (\mathcal{G})_\partial} < \delta.
\end{equation*}

Moreover, as $\|u^{(0)}-\hat{u}\| \rightarrow 0$, the iterative updates implicitly enforce the boundary and initial conditions, i.e.
\begin{equation*}
\lim_{\|u^{(0)}-\hat{u}\|\to 0} \|B[\mathcal{T}(u^{(k)})]-g\|_{\mathcal{U} (\mathcal{G})_\partial} = 0.
\end{equation*}

% \end{property}
The following observations support these claims:

% \begin{proof}[\textbf{Reasoning}]
1. \emph{Existence and Uniqueness:} The PDE is well posed by construction with a unique solution $\hat{u}$.

2. \emph{Residual at $\hat{u}$:} Since $\hat{u}$ solves the PDE exactly and satisfies the boundary and initial conditions, we have $\mathcal{R}(\hat{u})=0$.

3. \emph{Continuity of $\mathcal{R}$:} The operator $\mathcal{R}(u)$ is continuous in $\mathcal{U} (\mathcal{G})$ because it is composed of continuous operators $S$, $B[\cdot]$, and the evaluation map $u\mapsto u(0,\cdot)$. Hence, for any $\delta>0$, there exists $\varepsilon>0$ such that if $\|u^{(0)}-\hat{u}\|_{\mathcal{U} (\mathcal{G})}<\varepsilon$, then $\mathcal{R}(u^{(0)})$ is sufficiently small to ensure each component (including $\|B[u^{(0)}]-g\|_{\mathcal{U} (\mathcal{G})_\partial}$) is also small.

4. \emph{Monotonicity of $\mathcal{R}$:} By assumption, $\mathcal{R}(\mathcal{T}(u)) \le \mathcal{R}(u)$. Thus, if $\mathcal{R}(u^{(0)})$ is small, then $\mathcal{R}(u^{(k)})$ remains small for all $k$ since
\begin{equation*}
\mathcal{R}(u^{(k+1)}) = \mathcal{R}(\mathcal{T}(u^{(k)})) \le \mathcal{R}(u^{(k)}).
\end{equation*}

5. \emph{Implications for Boundary and Initial Conditions:} A small $\mathcal{R}(u)$ implies both $\|u(0,\cdot)-u_0\|_{\mathcal{U} (\mathcal{G})}$ and $\|\mathbb{B}[u]-g\|_{\mathcal{U} (\mathcal{G})_\partial}$ are small. Therefore, for all $k$,
\begin{equation*}
\mathcal{R}(u^{(k)}) \le \mathcal{R}(u^{(0)}) < \eta(\delta)\quad \rightarrow \quad \|B[u^{(k)}]-g\|_{\mathcal{U} (\mathcal{G})_\partial}< \delta.
\end{equation*}


6. \emph{Limit as the Initial Guess Improves:} As $u^{(0)}$ approaches $\hat{u}$, we have $\mathcal{R}(u^{(0)})\to \mathcal{R}(\hat{u})=0$. Thus, the boundary and initial conditions are increasingly well-satisfied. In the limit $\|u^{(0)}-\hat{u}\|\to 0$, the updates $\mathcal{T}(u^{(k)})$ remain arbitrarily close to the true solution, ensuring
\begin{equation*} 
\lim_{\|u^{(0)}-\hat{u}\|\to 0}\|B[\mathcal{T}(u^{(k)})]-g\|_{\mathcal{U} (\mathcal{G})_\partial} =0.
\end{equation*}

7. \emph{Grammar-Induced Latent Continuity:} The functions in $\mathcal{U} (\mathcal{G})$ are generated via a grammar and mapped to a latent manifold via a Grammar Variational Autoencoder (GVAE). Symbolically similar functions have similar rule sequences and thus similar latent vectors. Because $\hat{u}$ is a fixed point and $u^{(0)}$ is chosen close to $\hat{u}$, their latent representations $\hat{z}$ and $z^{(0)}$ are close. The continuity of the decoder ensures that small steps in latent space yield functions $u^{(k)}$ that remain structurally and numerically close to $\hat{u}$, preserving boundary and initial conditions.

Thus, the underlying grammar ensures that if we start near $\hat{u}$ in the function space, we also start near it in the latent space. The updates, whether via CMA-ES or another method, remain in a neighborhood that produces functions similar to $\hat{u}$, mitigating drastic violations of boundary/initial conditions.

8. \emph{Limit of Perfect Initial Guess:} As $u^{(0)}\to \hat{u}$, we have $\mathcal{R}(u^{(0)})\to 0$. Thus, in the limit, $B[u^{(k)}]\to g$ and $u(0,\cdot)\to u_0$. The conditions are enforced implicitly by virtue of staying in a neighborhood of the solution manifold defined by the grammar and the latent space.


If one considers the update rule $\mathcal{T}$ as the CMA-ES method instead of a deterministic approach, then the properties of $\mathcal{R}$ are posed through  expectations. In this case, the argument still holds but in a probabilistic sense.

% \end{proof}



\section{On the Implementation of SIGS}
% \section{Unified Context-Free Grammar and Dataset Creation}
\subsection{Grammar: Generation and Encoding of Expressions}
\label{sm: gram_data}
To systematically generate and parse mathematical expressions as potential solutions for differential equations, we developed a unified context-free grammar (CFG) capturing the structural patterns of these solutions. The grammar is formally defined as $G = (\Phi, N, R, S)$, where $N$ represents the set of non-terminal symbols, $\Phi$ denotes the terminal alphabet, $R$ is the set of production rules, and $S$ is the starting symbol. The production rules define the recursive construction of expressions, encompassing a wide range of mathematical operations and functions, such as trigonometric, exponential, logarithmic, and algebraic terms. Additionally, a special rule, \texttt{Nothing $\rightarrow$ None}, is included to handle padding for shorter expressions during preprocessing, ensuring consistent input dimensions. The full set of production rules for the grammar is presented as follows:

\begin{equation}
\begin{aligned}
S &\rightarrow S + T \; | \; S * T \; | \; S / T \; | \; S - T \; | \; T \\
T &\rightarrow ( S ) \; | \; ( S )^2 \; | \; \sin(S) \; | \; \cos(S) \; | \; \exp(S) \; | \; \log(S) \\
  &\quad | \; \sqrt(S) \; | \; \tanh(S) \; | \; \pi \; | \; x^2 \; | \; y^2 \; | \; y^3 \; | \; x | \; -x \; | \; y \; | \; -y \; | \; t \; \\
  &\quad | \; D \; | \; D.D \; | \; D.DD \; | \; DD.D| \; -D \; | \; -D.D \; \\
  &\quad  | \; -D.DD \; | \; -DD.D \;  | \; -DDD.D \; | \; DD \\
D &\rightarrow 0 \; | \; 1 \; | \; 2 \; | \; 3 \; | \; 4 \; | \; 5 \; | \; 6 \; | \; 7 \; | \; 8 \; | \; 9 \; | \; 10 \\
\text{Nothing} &\rightarrow \text{None}
\end{aligned}
\label{eqn:grammar}
\end{equation}

This grammar allows for the generation of expressions ranging from simple structures like $\sin(x)$ to more complex forms such as $\exp(-\frac{(x - x_0)^2}{w(1 + t)})$. The recursive nature of the rules ensures adherence to valid mathematical syntax while enabling flexible composition.

\textbf{Expression Generation and Dataset Creation:} Using this CFG, expressions are generated recursively by sampling production rules. This approach ensures that all generated expressions adhere to the grammatical constraints, creating both simple expressions and complex ones. These expressions are stored as strings and later preprocessed into a structured format for training the GVAE model. The preprocessing involves tokenizing each expression into individual components and parsing them into sequences of production rules. Tokenization uses regular expressions to handle mathematical operators, special functions, and formatting, while parsing employs \texttt{nltk.ChartParser} to generate parse trees based on the CFG.

\textbf{One-Hot Encoding:} After parsing, expressions are converted into one-hot encoded tensors for input to the GVAE. Each production rule is mapped to a unique index, creating a dictionary of rule-to-index mappings. The resulting one-hot encoded tensors have a shape of $(T_{max}, |R|)$, where $T_{max}$ is the maximum sequence length, set to 160, and $|R|$ is the total number of unique production rules, which is 44 in this work. Shorter expressions are padded with the \texttt{None} token to ensure uniformity in input size. The encoded data is stored in HDF5 format for efficient loading during training.

\textbf{Dataset Composition:} The dataset includes a diverse set of solutions representing various differential equations. For ordinary differential equations (ODEs), it include monovariable solutions in the $x$-plane, such as $2x^2 + \sin(x)$, with 10,000 samples. For partial differential equations (PDEs), the dataset predominantly consists of bivariate solutions in the $x$-$y$ plane, such as $\exp(-x^2 - y^2)$, with 30,000 samples. Additionally, we generated 1,000 specific solutions for Wave, Diffusion, Outgoing Wave, Burgers', and Shallow Water equations. These specific solutions include structured forms to test the model's ability to generalize across distinct problem types. This variety of solutions ensures that the GVAE is trained on a broad range of mathematical structures, preparing it to generalize effectively to different differential equation types.

\subsection{Grammar Variational Autoencoder: Architecture and Training}
\label{sm: mod_arc_tr}
In this study, we employed the \emph{Grammar Variational Autoencoder} (GVAE) to learn and generate valid mathematical expressions based on a predefined Context-Free Grammar (CFG). As formulated in detail in\textbf{SM} \ref{sm:gvae}, the GVAE encodes one-hot encoded expressions, representing sequences of grammar rules, into a latent space and decodes latent vectors back into valid sequences of grammar rules. This capability facilitates the generation and optimization of solutions to differential equations by ensuring that the generated expressions adhere to the grammatical constraints defined by the CFG.

\textbf{Model Architecture:} The GVAE model is designed to handle one-hot encoded sequences of grammar rules and consists of two main components: the Encoder and the Decoder. The Encoder transforms input expressions into a latent representation through a series of convolutional and fully connected layers. Specifically, it uses three convolutional layers with kernel sizes of 2, 3, and 4, producing 64, 128, and 256 feature maps, respectively. Each layer is followed by batch normalization and ReLU activation for stability and efficiency. The resulting feature maps are flattened and passed through a fully connected layer to reduce the dimensionality to 256. The latent variables are then computed through two separate linear layers that generate the mean ($\mu$) and the standard deviation ($\sigma$), both of dimensionality 44. A Softplus activation \cite{dugas2001softplus} ensures that $\sigma$ remains positive. The Decoder reconstructs the sequence of grammar rules from the latent space. It starts by expanding the latent vector from 44 to 501 dimensions via a linear layer followed by a ReLU \cite{nair2010relu} activation. This expanded representation is processed by a two-layer Gated Recurrent Unit (GRU) \cite{cho2014gru} with a hidden size of 501 and a dropout \cite{srivastava2014dropout} rate of 0.5 to prevent overfitting. Finally, a time-distributed dense layer projects the GRU outputs back to the original feature space of 44 dimensions, reconstructing the one-hot encoded sequence of grammar rules. During training, ten latent samples are drawn, and their corresponding logits are averaged to produce a stable reconstruction. The GVAE comprises approximately 13.3 million trainable parameters, with an estimated total size of 53.363 MB, balancing model complexity with computational efficiency.

\textbf{Training Procedure:} The training process of the GVAE model utilized a computing setup equipped with sixteen CPU cores, one CUDA-enabled GPU, and 5.91 GB of memory. The dataset, stored in an HDF5 file, was split in 90 1- \% with 10\% reserved for validation. Data loading was handled using PyTorch's \texttt{DataLoader} with a batch size of 20 and three worker threads for efficient throughput. Key training hyperparameters included a learning rate of 0.0001, gradient clipping with a maximum norm of 5.0 to prevent exploding gradients, an early stopping patience of 20 epochs, and a tightness parameter of 0.1 to balance KL divergence regularization and reconstruction loss. The Adam optimizer was employed with a weight decay of $1 \times 10^{-5}$, and a learning rate scheduler (\texttt{ReduceLROnPlateau}) reduced the learning rate by a factor of 0.2 if the validation loss did not improve for five consecutive epochs.

\textbf{Loss Function and Metric Monitoring:} The loss function comprises two components: the reconstruction loss and the Kullback-Leibler (KL) divergence. The reconstruction loss, computed as the cross-entropy between the predicted logits and target indices, reflects how well the Decoder reconstructs the sequences. During training, ten latent vectors were sampled for each input, and their logits were averaged to stabilize the reconstruction loss. This ensures the loss captures the variability in the latent space while providing a stable gradient signal. The KL divergence, calculated analytically from $\mu$ and $\sigma$, imposes a regularization that encourages the latent variables to align with a standard normal distribution. This term was modulated by an annealing schedule with a step size of $1 \times 10^{-6}$ and a rate of 500 updates, gradually increasing its contribution during training. The tightness parameter of 0.1 gave more weight to the reconstruction loss, ensuring more accurate reconstructions. The primary metric for monitoring the model's performance was the validation loss, which directly reflects reconstruction fidelity on unseen data. Early stopping was applied if the validation loss did not improve for 20 consecutive epochs, preventing overfitting and excessive computational expenditure.


\section{Differential Equation Test Suite}

\subsection{Benchmark: The Tsoulos Problem Set}
% \subsection{Benchmark Problems for Analytical Solutions of ODEs and Elliptic PDEs}
\label{sm: DEs}
In developing our solution space, we first examine a comprehensive collection of ordinary (Table \ref{tab:Tsoulos odes}) and partial (Table \ref{tab:Tsoulos pdes}) differential equations and their analytical solutions presented in \cite{Tsoulos2006SolvingDE}. This work tackles a diverse spectrum of problems, ranging from simple linear ordinary differential equations to coupled systems and elliptic partial differential equations. While the focus was on finding specific solutions to these problems, we drew inspiration from the mathematical structures of these solutions to generate a rich and diverse space of potential solution forms.
Linear ordinary differential equations (ODEs), nonlinear ordinary differential equations (NLODEs), and systems of ordinary differential equations (SODEs) provide our first set of mathematical patterns. From solutions such as $\sin(10x)$, $2x\exp(3x)$, and $\exp(-\frac{x}{5})\sin(x)$, we observe how elementary functions combine to form more complex expressions. These combinations of trigonometric, exponential, and polynomial behaviors initially inform the grammar we use to generate similar mathematical structures.
The elliptic partial differential equations extend these patterns to multiple dimensions. From separable solutions like $\sin(x)\cos(y)$ to non-separable forms like $\log(1 + x^2 + y^2)$, these structures guide our generation of multi-variable expressions. We observe how spatial variables are combined through addition, multiplication, and nested functions to create mathematically rich expressions, with logarithmic behaviors further enriching the diversity of generated forms.
The diversity of these benchmark problems provides crucial insights into the types of mathematical structures that can arise as solutions to differential equations. By analyzing these patterns, we aim to generate a broad range of potential solutions, including univariate and bivariate functions in 2D space.




%ODEs
\begin{table*}[!h]
\caption{Ordinary differential equations of the Tsoulos problem set.}
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.5} % Adjust line spacing
\begin{tabular}{ccccc}
\hline
\textbf{Type} & \textbf{Equation} & \textbf{Domain} & \textbf{Boundary/Initial Conditions} & \textbf{Exact Solution} \\ \hline
\textbf{ODE1} & $u^{\prime}(x) = \frac{2x - u(x)}{x}$ & $x \in [0,1]$ & $u(0.1) = 20.1$ & $u(x) = x + \frac{2}{x}$ \\ 
\textbf{ODE2} & $u^{\prime}(x) = \frac{1 - u(x)\cos(x)}{\sin(x)}$ & $x \in [0,1]$ & $u(0.1) = \frac{2.1}{\sin(0.1)}$ & $u(x) = \frac{x + 2}{\sin(x)}$ \\ 
\textbf{ODE3} & $u^{\prime}(x) = -\frac{1}{5}u(x) + \exp(-\frac{x}{5})\cos(x)$ & $x \in [0,1]$ & $u(0) = 0$ & $u(x) = \exp(-\frac{x}{5})\sin(x)$ \\ 
\textbf{ODE4} & $u^{\prime\prime}(x) = -100u(x)$ & $x \in [0,1]$ & $u(0) = 0, \; u'(0) = 10$ & $u(x) = \sin(10x)$ \\ 
\textbf{ODE5} & $u^{\prime\prime}(x) = 6u'(x) - 9u(x)$ & $x \in [0,1]$ & $u(0) = 0, \; u'(0) = 2$ & $u(x) = 2x \exp(3x)$ \\ 
\textbf{ODE6} & $u^{\prime\prime}(x) = -\frac{1}{5}u'(x) - u(x) - \frac{1}{5}\exp(-\frac{x}{5})\cos(x)$ & $x \in [0,1]$ & $u(0) = 0, \; u'(0) = 1$ & $u(x) = \exp(-\frac{x}{5})\sin(x)$ \\ 
\textbf{ODE7} & $u^{\prime\prime}(x) = -100u(x)$ & $x \in [0,1]$ & $u(0) = 0, \; u(1) = \sin(10)$ & $u(x) = \sin(10x)$ \\ 
\textbf{ODE8} & $x u^{\prime\prime}(x) + (1 - x)u^{\prime}(x) + u(x) = 0$ & $x \in [0,1]$ & $u(0) = 1, \; u(1) = 0$ & $u(x) = 1 - x$ \\ 
\textbf{ODE9} & $u^{\prime\prime}(x) = -\frac{1}{5}u'(x) - u(x) - \frac{1}{5}\exp(-\frac{x}{5})\cos(x)$ & $x \in [0,1]$ & $u(0) = 0, \; u(1) = \frac{\sin(1)}{\exp(0.2)}$ & $u(x) = \exp(-\frac{x}{5})\sin(x)$ \\ 
\textbf{NLODE1} & $(u^{\prime}(x))^2 + \log(u(x)) - \cos^2(x) - 2\cos(x) - 1  - \log(x + \sin(x)) = 0$ & $x \in [0,1]$ & $u(1) = 1 + \sin(1)$ & $u(x) = x + \sin(x)$ \\ 
\textbf{NLODE2} & $u^{\prime\prime}(x) u^{\prime}(x) = -\frac{4}{x^3}$ & $x \in [0,1]$ & $u(1) = 0$ & $u(x) = \log(x^2)$ \\ 
\textbf{NLODE3} & $u^{\prime\prime}(x^2) + (xu^{\prime}(x))^2 =- \frac{1}{log(x)}$ & $x \in [e,2e]$ & $u(e) = 0,u^{\prime}(e) = \frac{1}{e}$ & $u =log(log(x))$\\ 
\textbf{SODE1} & 
\begin{tabular}[c]{@{}c@{}} 
$u_1^{\prime}(x) = \cos(x) + u_1(x)^2 + u_2(x) - (x^2 + \sin^2(x))$ \\ 
$u_2^{\prime}(x) = 2x - x^2\sin(x) + u_1(x)u_2(x)$ 
\end{tabular} & 
$x \in [0,1]$ & 
\begin{tabular}[c]{@{}c@{}} 
$u_1(0) = 0$ \\ 
$u_2(0) = 0$ 
\end{tabular} & 
\begin{tabular}[c]{@{}c@{}} 
$u_1(x) = \sin(x)$ \\ 
$u_2(x) = x^2$ 
\end{tabular} \\ 
\textbf{SODE2} & 
\begin{tabular}[c]{@{}c@{}} 
$u_1^{\prime}(x) = \frac{\cos(x) - \sin(x)}{u_2(x)}$ \\ 
$u_2^{\prime}(x) = u_1(x)u_2(x) + \exp(x) - \sin(x)$ 
\end{tabular} & 
$x \in [0,1]$ & 
\begin{tabular}[c]{@{}c@{}} 
$u_1(0) = 0$ \\ 
$u_2(0) = 1$ 
\end{tabular} & 
\begin{tabular}[c]{@{}c@{}} 
$u_1(x) = \frac{\sin(x)}{\exp(x)}$ \\ 
$u_2(x) = \exp(x)$ 
\end{tabular} \\ \hline
\end{tabular}}

\label{tab:Tsoulos odes}
\end{table*}


\begin{table*}[h!]
\caption{Partial differential equations from the Tsoulos problem set.}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccc}
\hline
\textbf{Type} & \textbf{Equation} & \textbf{Domain} & \textbf{Boundary Conditions} & \textbf{Exact Solution} \\ \hline
\textbf{PDE1} & $\nabla^2 u(x,y) = 4$ & $x, y \in [0,1] \times [0,1]$ & 
\begin{tabular}[c]{@{}c@{}} 
$u(0,y) = y^2 + y + 1$ \\ 
$u(1,y) = y^2 + y + 3$ \\ 
$u(x,0) = x^2 + x + 1$ \\ 
$u(x,1) = x^2 + x + 3$ 
\end{tabular} & $u(x, y) = x^2 + y^2 + x + y + 1$ \\ 
\hline
\textbf{PDE2} & $\nabla^2 u(x,y) = -2u(x,y)$ & $x, y \in [0,1] \times [0,1]$ & 
\begin{tabular}[c]{@{}c@{}} 
$u(0,y) = 0$ \\ 
$u(1,y) = \sin(1)\cos(y)$ \\ 
$u(x,0) = \sin(x)$ \\ 
$u(x,1) = \sin(x)\cos(1)$ 
\end{tabular} & $u(x, y) = \sin(x)\cos(y)$ \\ 
\midrule
\textbf{PDE3} & $\nabla^2 u(x,y) = -(x^2 + y^2)u(x,y)$ & $x, y \in [0,1] \times [0,1]$ & 
\begin{tabular}[c]{@{}c@{}} 
$u(0,y) = 0$ \\ 
$u(1,y) = \sin(y)$ \\ 
$u(x,0) = 0$ \\ 
$u(x,1) = \sin(x)$ 
\end{tabular} & $u(x, y) = \sin(xy)$ \\ 
\midrule
\textbf{PDE4} & $\nabla^2 u(x,y) = (x - 2) \exp(-x) + x \exp(-y)$ & $x, y \in [0,1] \times [0,1]$ & 
\begin{tabular}[c]{@{}c@{}} 
$u(0,y) = 0$ \\ 
$u(1,y) = \exp(-y) + \exp(-1)$ \\ 
$u(x,0) = x(\exp(-x) + 1)$ \\ 
$u(x,1) = x(\exp(-x) + \exp(-1))$ 
\end{tabular} & $u(x, y) = x(\exp(-x) + \exp(-y))$ \\ 
\hline
\textbf{PDE5} & $\nabla^2 u(x,y) + \exp(u(x,y)) = 1 + x^2 + y^2 + \frac{4}{(1 + x^2 + y^2)^2}$ & $x, y \in [0,1] \times [0,1]$ & 
\begin{tabular}[c]{@{}c@{}} 
$u(0,y) = \log(1 + y^2)$ \\ 
$u(1,y) = \log(2 + y^2)$ \\ 
$u(x,0) = \log(1 + x^2)$ \\ 
$u(x,1) = \log(2 + x^2)$ 
\end{tabular} & $u(x, y) = \log(1 + x^2 + y^2)$ \\ \hline
\end{tabular}}

\label{tab:Tsoulos pdes}
\end{table*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/experiments_results/odes_sol.pdf}
    \caption{Solutions for ODE Problems. The subplots are arranged as follows: 
    Top-left: ODE1, Top-center: ODE2, Top-right: ODE3-ODE6-ODE9, Bottom-left: ODE4-ODE7, Bottom-center: ODE5, Bottom-right: ODE8.}
    \label{fig:odes}
\end{figure}    
  
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/contour-plots/NLODE_expanded.pdf
    }
    \caption{Solutions for nonlinear ODE (NLODE) Problems. The subplots are arranged as follows: 
    Left to Right: NLODE2, NLODE3, NLODE4.}
    \label{fig:nlodes}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/ODEs/sode_sol.pdf}
    \caption{Solutions for nonlinear ODE System of ODE Problems. The subplots are arranged as follows: 
    Left to Right: SODE1, SODE2.}
    \label{fig:sodes}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{images/contour-plots/seismic_pdes.png}
    \caption{Solutions for PDEs. The subplots are arranged as follows: 
    Top Left to Right: PDE1, PDE2, PDE3, Bottom Left to Right: PDE4, PDE5.}
    \label{fig:pdes}
\end{figure}

\subsection{New Problems: Advanced Differential Equations}
% \subsection{Advanced Problems}
\label{sec:adv_prob}
This section provides detailed specifications of advanced problems. Here, we outline the settings, results, and figures for each problem, demonstrating the methodology's application across diverse PDE types. For each of the following problems, we present both the imposed solution $u$ and the corresponding system specifications. Given these  solutions, we calculated the necessary forcing terms $f$ by using, for each case, the relevant differential operator. These forcing terms, along with the operator, the initial and boundary conditions, provided the complete problem formulation needed for evaluation. Our methodology successfully decoded solutions that closely matched the manufactured solutions in most cases, with the exception of the shallow water equations and iterative wave equation, where some deviations were observed. For the cases with higher reconstruction errors, we provide in the section of each problem the specific decoded solutions to enable detailed comparison with the original manufactured solutions. The tables that follow provide comprehensive problem specifications, while the accompanying figures illustrate the spatiotemporal evolution of both the manufactured and decoded solutions.

\subsubsection{Diffusion Equation and Solutions}
For this problem, we consider the one-dimensional diffusion of a concentrated mass in a finite domain \cite{chasnov2019differential}:
\begin{equation}
   u_t = D u_{xx}, \quad 0 \leq x \leq L, \quad t > 0,
\end{equation}
with homogeneous Dirichlet boundary conditions $ u(0, t) = u(L, t) = 0 $. These boundary conditions represent a scenario where the concentration at both ends of the domain remains zero for all time. The initial condition models a concentrated mass $ M_0 $ introduced at the midpoint of the domain:
\begin{equation}
   u(x, 0) = M_0 \delta(x - L/2).
\end{equation}
Following the approach outlined in \cite{chasnov2019differential}, the solution to this initial-boundary value problem is obtained using the method of separation of variables and takes the form:

\begin{equation}
   u(x, t) = \frac{2 M_0}{L} \sum_{n=0}^{\infty} (-1)^n \sin( (2n + 1) \frac{\pi x}{L} ) e^{-(2n + 1)^2 \pi^2 D t / L^2}.
\end{equation}

We consider $1,000$ solutions of the Diffusion equation to the training dataset with $D=0.58$, and random values of $M_0, L$. For discovering a new analytical solution of the Diffusion equation we choose $M_0 = 1.91$, $n=3$, thus making the baseline solution to be: 
\begin{equation*}
     \hat{u} = 3.31(\sin(2.50127x)e^{-3.5t} - \sin(7.40375x)e^{-31.9t} + \sin(12.40629x)e^{-88.8t})
\end{equation*}
We choose the domain of the solution as $x \in [0, 1.27], t \in [0,1]$,  boundary conditions 
\begin{equation*}
    u(0,t) = 0, \ \ u(L,t) = 3.31(\sin(2.50127 L)e^{-3.5t} - \sin(7.40375 L)e^{-31.9t} + \sin(12.40629 L)e^{-88.8t}),
\end{equation*} 
and initial condition  
\begin{equation*}
   u(x, 0) = 3.31(\sin(2.50127x)  - \sin(7.40375x)  + \sin(12.40629x)).
\end{equation*} 
A contour and 3D visualization of the solution and 3D plot is provided in Figure \ref{fig:diffusion_contour}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/contour-plots/seismic_new_diffusion_contour_plot.png}
    \caption{Spatiotemporal dynamics of Diffusion Equation's solution. The solution demonstrates diffusive behavior with three distinct spatial frequencies, each decaying at different rates (\( e^{-3.5t} \), \( e^{-31.9t} \), and \( e^{-88.8t} \)). Over time, the lowest frequency mode dominates the solution.}
    \label{fig:diffusion_contour}
\end{figure}

\subsubsection{Outgoing Wave Equation and Solutions
}

We model an outgoing waves phenomenon, starting from a localized disturbance and decaying over time. Outgoing waves are characterized by their symmetric propagation from a point source and the gradual reduction in amplitude due to damping effects. We assume an algebraic relations that describes a symmetric wave starting from a point disturbance that decays over time:
\begin{equation*}
    u(x, y, t) = 1 + h \exp(-\frac{(x - x_0)^2 + (y - y_0)^2}{w(1 + t)}) 
\cos (k ( \sqrt{(x - x_0)^2 + (y - y_0)^2} - ct )e^{-\alpha t},
\end{equation*}
where $h, w$ are the amplitude, and the spread of the initial disturbance, respectively, $k$ is the wave number, $c$ is the propagation speed, $ \alpha $ is the damping coefficient, and $ (x_0, y_0) $ denotes the location of the initial disturbance. This function models a wave originating from a localized disturbance at $ (x_0, y_0) $, propagating outward at speed $c$, oscillating with frequency $ k $, and decaying exponentially over time due to the damping term $ e^{-\alpha t} $. We consider the wave equation:
\begin{equation*}
u_{tt} - c^2 \Delta u = f(x, y, t),
\end{equation*}
where $f(x,y,t)$ a function that forces the manufactured solution to the wave operator. The forcing function compensates for the natural decay of the wave by continuously supplying energy, ensuring that the wave pattern remains valid over time. Varying the parameters $ h, w, k, c, \alpha, (x_0, y_0) $, we can model a broad range of outgoing wave behaviors. We construct $1,000$ wave relations by considering $ h \sim  U(0.01, 0.5)$, $ w \sim U(0.3, 3.0)$, $ k \sim U(0.5, 8.0)$,  $ c  \sim U(0.3, 3.0)$, $ \alpha \sim U(0.02, 0.8)$, and $ (x_0, y_0) \sim [6, 6] \times [-6, 6] $. These parameter ranges provide sufficient flexibility to generate a diverse set of wave behaviors, allowing us to capture various propagation patterns and damping effects in outgoing wave solutions. For discovering a solution of the wave equation, we consider $c=0.5$, $(x_0, y_0) = (1.1, -1.5)$, $\alpha = 0.5$, $k=4.4$, $w=1.1$, $h=0.2$, $(x,y) \in [-5, 5]$, $t \in [0,4]$, the initial condition
\begin{equation*}
    u(x,y,0) = 1 + 0.2 \exp ( \frac{(x-1.1)^2 + (y+1.5)}{1.1}) \cos(\sqrt{(x - x_0)^2 + (y - y_0)^2})
\end{equation*}
and the boundary conditions:
\begin{align*}
    &u(5, y,  t) = 1 + h \exp(-\frac{(5.5 - 1.1)^2 + (y + 1.5)^2}{1.1(1 + t)}) 
\cos (4.4 ( \sqrt{(5.5 - 1.1)^2 + (y +1.5)^2} - ct )e^{-0.5 t}\\
    &u(-5, y, t) = 1 + h \exp(-\frac{(-5 - 1.1)^2 + (y + 1.5)^2}{1.1(1 + t)}) 
\cos (4.4 ( \sqrt{(-5 - 1.1)^2 + (y +1.5)^2} - ct )e^{-0.5 t}\\
    &u(x, 5,  t) = 1 + h \exp(-\frac{(x - 1.1)^2 + (5 + 1.5)^2}{1.1(1 + t)}) 
\cos (4.4 ( \sqrt{(x - 1.1)^2 + (5 +1.5)^2} - ct )e^{-0.5 t}\\
    &u(x, -5, t) = 1 + h \exp(-\frac{(x - 1.1)^2 + (-5 + 1.5)^2}{1.1(1 + t)}) 
\cos (4.4 ( \sqrt{(x - 1.1)^2 + (-5 +1.5)^2} - ct )e^{-0.5 t}\\    
\end{align*}
The analytical solution is defined:
\begin{equation*}
        u(x,y,0) = 1 + 0.2 \exp ( \frac{(x-1.1)^2 + (y+1.5)}{1.1}) \cos(\sqrt{(x - x_0)^2 + (y - y_0)^2} - 0.5t)e^{-0.5t}
\end{equation*}
We provide a visualization of the wave equation in Figure \ref{fig:outgoing_wave}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/contour-plots/seismic_new__outgoing_wave_contours.png}
    \caption{Evolution of the Outgoing Wave Equation's solution, with damping, at t=[0.0, 1.0, 2.0, 3.0,4.0, 5.0], arranged from left to right and top to bottom. The plots illustrate symmetric wave propagation from point $(x_0,y_0)=(1.1,-1.5)$ and decay over time, with the color scale representing the solution magnitude}
    \label{fig:outgoing_wave}
\end{figure}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.7\textwidth]{images/outgoing_wave/outgoing_wave.pdf}
%     \caption{Evolution of the  solution for the 2D wave equation with damping over a time interval of 4 sec. 
%     The solution is visualized at six time points: $t = 0.0$, $0.8$, $1.6$, $2.4$, $3.2$, and $4.0$. 
%     The plots demonstrate both the wave-like behavior through the oscillatory cosine term and the strong decay characteristics due to the exponential damping term $e^{-0.5t}$. 
%     The initial Gaussian-modulated wave centered at $(x,y)=(1.1,-1.5)$ shows how the solution propagates and significantly dissipates over the extended time period. 
%     The color scale represents the magnitude of the solution $u(x,y,t)$, with darker colors indicating higher values.}
%     \label{fig:outgoing_wave_3D}
% \end{figure}





\subsubsection{Burgers Equation}

We model the formation and evolution of shock structures in a medium governed by the viscous Burgers equation. This phenomenon describes how nonlinear advection and viscous diffusion interact, resulting in sharp transitions, or shocks, within the velocity profile for low viscocity values. These shocks arise due to the steepening of wavefronts, counteracted by dissipative effects that smooth out the sharp gradients. The governing equation for this phenomenon is the viscous Burgers equation:
\begin{equation*}
u_t + uu_x - \nu u_{xx} = f(x,t),
\end{equation*}
where $f(x,t)$ a forcing function that supplies energy to the system to conpensate for the natural dissipation of the shock and ensure the shock structure is maintained for the entire time interval. We assume the algebraic relation that represents a shock between two states of the system:
\begin{equation*}
u(x,t) = \frac{u_R + u_L}{2} - \frac{u_L - u_R}{2}\tanh\left(\frac{(x-x_0-st)(u_L-u_R)}{4\nu}\right),
\end{equation*}
where $u_L$ and $u_R$ are the left and right states, respectively, with $u_L > u_R$, and $s = (u_L + u_R)/2$ is the shock speed. Here, $\nu$ represents the kinematic viscosity that controls the thickness of the shock transition, and $x_0$ sets the initial position of the shock. We construct $1,000$ solutions for the Burgers equation by varying the parameters $u_L$, $u_R$, $\nu$, and $x_0$ to construct a broad range of solutions, $u_L \sim U(1, 2), u_R \sim U(0.1, 0.5), \nu \sim U(0.01, 0.5), x_0  \sim U(-2,2)$. These parameter ranges provide sufficient flexibility to generate a diverse set of shock profiles, allowing us to capture variations in width, strength, and position. For discovering a new solution of the Burgers equation, we consider
$u_L = 1.46, u_R = 0.26,  x_0 = 0.33,  \nu = 0.01, (x,t) \in [-5, 5] \times [0,1]$, the initial:
\begin{equation*}
u(x,0) = \frac{1.46 + 0.26}{2} - \frac{1.46 - 0.26}{2}\tanh\left(\frac{(x - 0.33)(1.46 - 0.26)}{4(0.01)}\right),
\end{equation*}
and boundary conditions:
\begin{align*}
    u(-5, t) &= \frac{1.46 + 0.26}{2} - \frac{1.46 - 0.26}{2}\tanh\left(\frac{(-5 - 0.33 - 0.86t)(1.46 - 0.26)}{4(0.01)}\right), \\
    u(5, t) &= \frac{1.46 + 0.26}{2} - \frac{1.46 - 0.26}{2}\tanh\left(\frac{(5 - 0.33 - 0.86t)(1.46 - 0.26)}{4(0.01)}\right).
\end{align*}
The analytical solution is defined:
\begin{equation*}
u(x,0) = \frac{1.46 + 0.26}{2} - \frac{1.46 - 0.26}{2}\tanh\left(\frac{(x - 0.33 - 0.86t)(1.46 - 0.26)}{4(0.01)}\right),
\end{equation*}
The solution is visualized in Figure \ref{fig:burgers_3d}.

\begin{figure*}
    \centering
    \includegraphics[width=0.55\textwidth]{images/contour-plots/seismic_burgers.png}
    \caption{The viscous profile of the Burger's Equation's solution. The contour lines illustrate the evolution of the shock profile as it propagates to the right with speed \( c = 0.86 \). The smooth transition between the left state (\( u_L = 1.46 \)) and the right state (\( u_R = 0.26 \)) demonstrates the balance between nonlinear advection and viscous diffusion, characterized by a kinematic viscosity of \( \nu = 0.01 \).}
    \label{fig:burgers_3d}
\end{figure*}


\subsubsection{Shallow Water Equations}

We model the propagation and evolution of shallow water waves in two dimensions, governed by the shallow water equations. This phenomenon describes the conservation of mass and momentum, capturing the interactions between wave propagation, gravity, and fluid dynamics. These equations account for how wave profiles evolve over time due to spatial variations in density and velocity fields.

We assume algebraic relations that describe the density and velocity components of the shallow water wave system. The density $\rho(x, y, t)$ is modeled as:
\begin{equation*}
\rho(x, y, t) = 1 + h \exp\left(-\frac{(x - x_0)^2 + (y - y_0)^2}{w(1 + t)}\right)\cos\left(k \sqrt{(x - x_0)^2 + (y - y_0)^2} - ct\right)e^{-\alpha t},
\end{equation*}
where $h$ represents the amplitude, $w$ is the Gaussian width, $k$ is the wave number, $c$ is the wave speed, and $\alpha$ is the decay rate. The terms $x_0$ and $y_0$ define the initial center of the wave. The velocity components $u_x(x, y, t)$ and $u_y(x, y, t)$ are derived using the linear shallow waters theory as:
\begin{align*}
u_x(x, y, t) &= \rho(x, y, t) \cdot \frac{x \cdot c}{H \cdot \sqrt{(x - x_0)^2 + (y - y_0)^2}}, \\
u_y(x, y, t) &= \rho(x, y, t) \cdot \frac{y \cdot c}{H \cdot \sqrt{(x - x_0)^2 + (y - y_0)^2}},
\end{align*}
where $H$ is a velocity scaling factor. The governing equations for the shallow water system are:
\begin{align*}
&\text{Mass conservation:} & \frac{\partial \rho}{\partial t} + \frac{\partial(\rho u_x)}
{\partial x} + \frac{\partial(\rho u_y)}{\partial y} &= f_{\rho}, \\
&\text{x-momentum:} & \frac{\partial(\rho u_x)}{\partial t} + \frac{\partial(\rho u_x^2 + \frac{1}{2}g\rho^2)}{\partial x} + \frac{\partial(\rho u_x u_y)}{\partial y} &= f_x(x, y, t), \\
&\text{y-momentum:} & \frac{\partial(\rho u_y)}{\partial t} + \frac{\partial(\rho u_x u_y)}{\partial x} + \frac{\partial(\rho u_y^2 + \frac{1}{2}g\rho^2)}{\partial y} &= f_y(x, y, t).
\end{align*}
The forcing terms $f_x(x, y, t)$ and $f_y(x, y, t)$ ensure the manufactured solutions remain valid over the specified domain and time interval by compensating for natural wave decay and dissipation. The parameters of these expressions ranges provide sufficient flexibility to capture diverse wave behaviors
$h \sim U(0.5, 2.0),  w \sim U(0.3, 3.0),  k \sim U(0.5, 8.0),  c \sim U(0.3, 3.0)$$, \alpha \sim U(0.02, 0.8),  x_0, y_0 \sim U(-6.0, 6.0), H \sim U(1.0, 5.0)$.  To discover a new solution of the shallow water equations, we consider $h = 0.97,  w = 0.88,  k = 1.78,  c = 1.28, \alpha = 0.72,  (x_0, y_0) = (3.77, 2.34),  H = 4.46, (x, y) \in [-10, 10], t \in [0, 5]$. The initial condition is:
\begin{equation*}
\rho(x, y, 0) = 1 + 0.97 \exp\left(-\frac{(x - 3.77)^2 + (y - 2.34)^2}{0.88}\right)\cos\left(1.78\sqrt{(x - 3.77)^2 + (y - 2.34)^2}\right).
\end{equation*}
The boundary conditions are:
\begin{align*}
\rho(-10, y, t) &= 1 + 0.97 \exp\left(-\frac{((-10) - 3.77)^2 + (y - 2.34)^2}{0.88(1 + t)}\right)\cos\left(1.78\sqrt{((-10) - 3.77)^2 + (y - 2.34)^2} - 1.28t\right)e^{-0.72t}, \\
\rho(10, y, t) &= 1 + 0.97 \exp\left(-\frac{(10 - 3.77)^2 + (y - 2.34)^2}{0.88(1 + t)}\right)\cos\left(1.78\sqrt{(10 - 3.77)^2 + (y - 2.34)^2} - 1.28t\right)e^{-0.72t}, \\
\rho(x, -10, t) &= 1 + 0.97 \exp\left(-\frac{(x - 3.77)^2 + ((-10) - 2.34)^2}{0.88(1 + t)}\right)\cos\left(1.78\sqrt{(x - 3.77)^2 + ((-10) - 2.34)^2} - 1.28t\right)e^{-0.72t}, \\
\rho(x, 10, t) &= 1 + 0.97 \exp\left(-\frac{(x - 3.77)^2 + (10 - 2.34)^2}{0.88(1 + t)}\right)\cos\left(1.78\sqrt{(x - 3.77)^2 + (10 - 2.34)^2} - 1.28t\right)e^{-0.72t},
\end{align*}
and
\begin{align*}
u_x(-10, y, t) &= \rho(-10, y, t)\cdot \frac{x \cdot c}{4.46 \cdot \sqrt{(-10 - 3.77)^2 + (y - 2.34)^2}}, \\
u_x(10, y, t) &= \rho(-0, y, t)\cdot \frac{x \cdot c}{4.46 \cdot \sqrt{(10 - 3.77)^2 + (y - 2.34)^2}}, \\
u_x(x, -10, t) &= \rho(x, -10, t)\cdot \frac{x \cdot c}{4.46 \cdot \sqrt{(x - 3.77)^2 + (y - 2.34)^2}}, \\
u_x(x, 10, t) &= \rho(x, 10, t)\cdot \frac{x \cdot c}{4.46 \cdot \sqrt{(x - 3.77)^2 + (y - 2.34)^2}}.
\end{align*}
The analytical solution is defined:
\begin{align*}
    \rho(x, y, t) &= 1 + 0.97 \exp\left(-\frac{(x - 3.77)^2 + (y - 2.34)^2}{0.88(1 + t)}\right)\cos\left(1.78\sqrt{(x - 3.77)^2 + (y - 2.34)^2} - 1.28t\right)e^{-0.72t}, \\
u_x(x, y, t) &=   \frac{x \cdot c}{4.46 \cdot \sqrt{(x - 3.77)^2 + (y - 2.34)^2}} \\
&(1 + 0.97 \exp\left(-\frac{(x - 3.77)^2 + (y - 2.34)^2}{0.88(1 + t)}\right)\cos\left(1.78\sqrt{(x - 3.77)^2 + (y - 2.34)^2} - 1.28t\right)e^{-0.72t}), \\
u_y(x, y, t) &=  \frac{y \cdot c}{4.46 \cdot \sqrt{(x - 3.77)^2 + (y - 2.34)^2}} \\
&( 1 + 0.97 \exp\left(-\frac{(x - 3.77)^2 + (y - 2.34)^2}{0.88(1 + t)}\right)\cos\left(1.78\sqrt{(x - 3.77)^2 + (y - 2.34)^2} - 1.28t\right)e^{-0.72t}),
\end{align*}
The solution is visualized in Figure \ref{fig:shallow_water}.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.55\textwidth}
        \includegraphics[width=\textwidth]{images/contour-plots/seismic_wave_height_contours_evolving.png}
        \caption{Water altitude \( \rho(x, y, t) \) at \( t = 0.0, 1.5, 3.0 \)}
        \label{fig:sallow-rho}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.55\textwidth}
        \includegraphics[width=\textwidth]{images/contour-plots/seismic_new_shallow_water_ux.png}
       \caption{Vertical velocity \( u_x(x, y, t) \) at \( t = 0.0, 1.5, 3.0 \)}
        \label{fig:shallow-ux}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.55\textwidth}
        \includegraphics[width=\textwidth]{images/contour-plots/seismic_new_shallow_water_uy.png}
       \caption{Vertical velocity \( u_y(x, y, t) \) at \( t = 0.0, 1.5, 3.0 \)}
        \label{fig:shallow-uy}
    \end{subfigure}
    
   \caption{Water altitude \( \rho(x, t) \) and velocity fields \( u_x(x, t) \), \( u_y(x, t) \) fitting the Shallow Water Equations model at timesteps \( t = 0 \), \( t = 1.5 \), and \( t = 3 \). These plots illustrate wave propagation and the evolution of the velocity field over time and space.}
    \label{fig:shallow_water}
\end{figure*}





\clearpage
\section{Additional Experimental Results}
\label{sm:additional experimental results}
% \section{Experimentally Verifying Properties }

In Section \ref{sec:SIGS properties}, we listed the properties of the SIGS method, and we are going to provide experimental evidence about their validity. We consider the property of efficient choice of the initial guess of the iterative method. We show in in Figure \ref{fig:initial guess and convergence} the latent vector that corresponds to the initial guess, and the subsequent choices of latent vector until convergence to the true solution for the outward moving wave. We observe that the initial guess of the solution is very close to the optimum, which results to the method converging immediately. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=0.8\linewidth, angle=0]{images/experiments_results/wave_tsne_with_cmaes_with_arrows-2.pdf}
        \caption{Full view.}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth, angle=-0]{images/experiments_results/wavetsne_with_zoom-2.pdf}
        \caption{Intermediate zoom.}
    \end{subfigure}
 
    \caption{Comparison of different t-SNE projections of CMA-ES progression. In (b) the intermediate steps are very close, but not equal to the optimized solution.}
    \label{fig:initial guess and convergence}
\end{figure}


\paragraph{A-posteriori Self-Compositions}
A different property of the method is \emph{a-posteriori self-compositions}, as explained in Section \ref{sec:aposteriori self compositions}. To show the capability of the method in performing iterative refinement suggested by this property we set yo the following experiment.

We consider a different manufactured solution based on the summation of $\sin$ and $\cos$ terms with power law decaying coefficients, as in Raonic et. al. \cite{raonic2024convolutional}. The analytical solution is modified for a 1D spatial variable:
\begin{equation}
\hat{u}(x, t) = \frac{\pi}{K} \sum_{i=j}^{K+j} a_i i^{-r} \sin(\pi i x) \cos(c \pi t i),
\end{equation}
where now the function $f(x,t)$ forces this solution to the wave operator. We generate $1,000$ solutions with coefficients $a_i \sim U(-1, 1)$, modes  $K = 4$, the integer $j \sim U(0,3)$, decay of the higher frequency modes $r=1$, and propagation speed $c \sim U(0.1, 0.2)$. The index $j$ is used to sample consecutive integers that can be used later in the refinement. For discovering and refining the solutions, we consider the domain $(x,y) \in [0,2] \times [0,2]$, $c=0.1$, coefficients:
\begin{equation*}
\begin{aligned}
    \mathbf{a} = [&-0.25, 0.0425, -0.0344, -0.0563, 0.0044, 0.025, -0.0073, 0.0095, \\
    &-0.0059, -0.0083, 0.0014, -0.0036, -0.0053, -0.0032, -0.48, -3e^{-5}],
    \end{aligned}
\end{equation*} 
the initial:
\begin{equation*}
    u(x,0) = \hat{u}(x,0)
\end{equation*}
and boundary conditions:
\begin{equation*}
    u(\pm 2,t) = \hat{u}(\pm 2,t)
\end{equation*}

The analytical solution for this problem is defined:
\begin{equation*}
    \begin{aligned}
u(x,t) = &\frac{\pi}{16} (- 0.25 \sin{(\pi x )} \cos{(0.1 \pi t )} + 0.0425 \sin{(2 \pi x )} \cos{(0.2 \pi t )} 
 - 0.0344 \sin{(3 \pi x )} \cos{(0.3 \pi t )} \\
 &-0.0563 \sin{(4 \pi x )} \cos{(0.4 \pi t )} 
 + 0.0044 \sin{(5 \pi x )} \cos{(0.5 \pi t )} + 0.025 \sin{(6 \pi x )} \cos{(0.6 \pi t )} \\
 &- 0.0073 \sin{(7 \pi x )} \cos{(0.7 \pi t )} 
 + 0.0095 \sin{(8 \pi x )} \cos{(0.8 \pi t )} 
 - 0.0059 \sin{(9 \pi x )} \cos{(0.9 \pi t )} \\
 &- 0.0083 \sin{(10 \pi x )} \cos{(\pi t )}
 + 0.0014 \sin{(11 \pi x )} \cos{(1.1 \pi t )} - 0.0036 \sin{(12 \pi x )} \cos{(1.2 \pi t )}\\
& - 0.0053 \sin{(13 \pi x )} \cos{(1.3 \pi t )} - 0.0032 \sin{(14 \pi x )} \cos{(1.4 \pi t )}
- 0.48 \sin{(15 \pi x )} \cos{(1.5 \pi t )} \\
&- 3e^{-5} \sin{(16 \pi x )} \cos{( 1.6 \pi t )} )        
    \end{aligned}
    \label{eq:16 term solution}
\end{equation*}
The visualization of the solution is provided in Figure \ref{fig:3d_surface_plot_wave_expression}. The goal of the experiment is to bias the GVAE to return expressions that contain summations of only 4 terms and by compositions, and more specifically additions, to decrease the error of the prediction. To do this, we initialize and search three consecutive times, while freezing the optimal solution each time. 

\begin{figure}[h!]
        \centering
        \includegraphics[width=0.7\textwidth]{images/contour-plots/seismic_u_contour_plot.png}
        \caption{Evolution of the Wave Equation's solution over space and time, showing the superposition of 16 sinusoidal modes where the amplitude of higher-frequency components attenuates according to a power law decay.}
        \label{fig:3d_surface_plot_wave_expression}
\end{figure}

The solution predicted by the model via compositions is:
\begin{equation*}
    \begin{aligned}
u(x,t) = &\frac{\pi}{16} (- 0.2{\color{red}{6}} \sin{(\pi x )} \cos{(0.1 \pi t )} + 0.0425 \sin{(2 \pi x )} \cos{(0.2 \pi t )} 
 - 0.03{\color{red}{33}} \sin{(3 \pi x )} \cos{(0.3 \pi t )} \\
 &-0.05{\color{red}{5}} \sin{(4 \pi x )} \cos{(0.4 \pi t )} 
 + 0.0044 \sin{(5 \pi x )} \cos{(0.5 \pi t )} + 0.02{\color{red}{5}} \sin{(6 \pi x )} \cos{(0.6 \pi t )} \\
 &- 0.007{\color{red}{3}} \sin{(7 \pi x )} \cos{(0.7 \pi t )} 
 + 0.009{\color{red}{5}} \sin{(8 \pi x )} \cos{(0.8 \pi t )} 
 - 0.0059 \sin{(9 \pi x )} \cos{(0.9 \pi t )} \\
 &- 0.008{\color{red}{4}} \sin{(10 \pi x )} \cos{(\pi t )}
 + 0.001{\color{red}{5}} \sin{(11 \pi x )} \cos{(1.1 \pi t )} - 0.003{\color{red}{6}} \sin{(12 \pi x )} \cos{(1.2 \pi t )}\\
& - 0.0053 \sin{(13 \pi x )} \cos{(1.3 \pi t )} - 0.0032 \sin{(14 \pi x )} \cos{(1.4 \pi t )}
- 0.{\color{red}{39}} \sin{(15 \pi x )} \cos{(1.5 \pi t )} \\
&- {\color{red}{0.0}}\sin{(16 \pi x )} \cos{( 1.6 \pi t )} )        
    \end{aligned}
\end{equation*}
We observe that the source of the error is mostly related to the ability of the model to accurately capture constants. 


\paragraph{Wall Clock Times to Solution}
We provide the wall clock evaluation times for SIGS, as well as PINNs and FBPINNs, in Table \ref{tab:time to solution}.

\begin{table}[H]
\caption{Wall clock time required to compute the solution or best approximation for the advanced problems. Each approach is evaluated on a grid with 32 points in each dimension. For 1+1-dimensional problems, PINNs take approximately 1 minute, while FBPINNs may take several minutes. For 2+1-dimensional problems, greater variance is seen, and FBPINNs make take up to several hours to complete a run.}

    \centering
    \begin{tabular}{lccc}
    \toprule
       \textbf{Experiment}  & \textbf{SIGS} & \textbf{PINNs} & \textbf{FBPINNs} \\
       \midrule \\
        Burgers         & 51s & 1m 15s & 3m 30s \\
        Diffusion       & 12s& 1m 0s  & 3m 0s \\
        Iterative Wave  & 9m 18s & 1m 15s & 2m 20s \\
        Outgoing Wave   & 3m 3s  & 48m 0s & 6h 12m \\
        Wave            & 28s & 1m 0s & 2m 20s \\
        Shallow Water   & 10m 50s & 4m 10s & 55m 15s \\
        \bottomrule
        
    \end{tabular}
    
    \label{tab:time to solution}
\end{table}
\section*{Code and Data Availability}
All code, models, and library of expressions accompanying this manuscript will be made publicly available at \url{https://github.com/camlab-ethz/LoDE}.

\end{document}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/experiments_results/3D_Surface_Comparison.pdf}
    \caption{Comparison of 3D plots for Shallow Waters Components}
    \label{fig:shallow_water}
\end{figure}


% \begin{table}[H]
%     \centering
%     \begin{tabular}{|l|c|c|c|}
%         \hline
%         \textbf{Problem Type} & \textbf{ELBO} & \textbf{Perplexity} & \textbf{Accuracy} \\ \hline
%         ODEs & [Value] & [Value] & [Value]\% \\ \hline
%         PDEs & [Value] & [Value] & [Value]\% \\ \hline
%         Wave CNO & [Value] & [Value] & [Value]\% \\ \hline
%         Diffusion & [Value] & [Value] & [Value]\% \\ \hline
%         Outgoing Wave & [Value] & [Value] & [Value]\% \\ \hline
%         Burgers’ Equation & [Value] & [Value] & [Value]\% \\ \hline
%         Iterative Wave & [Value] & [Value] & [Value]\% \\ \hline
%         Shallow Water & [Value] & [Value] & [Value]\% \\ \hline
%     \end{tabular}
%     \caption{Performance Metrics for Each Problem Type}
%     \label{tab:performance_metrics}
% \end{table}

\end{document}
