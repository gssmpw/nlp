% This file was created with JabRef 2.3.
% Encoding: ISO-8859-1



@book{fem,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {The {Mathematical} {Theory} of {Finite} {Element} {Methods}},
	volume = {15},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-75933-3 978-0-387-75934-0},
	url = {http://link.springer.com/10.1007/978-0-387-75934-0},
	urldate = {2024-10-02},
	publisher = {Springer},
	author = {Brenner, Susanne C. and Scott, L. Ridgway},
	editor = {Marsden, J. E. and Sirovich, L. and Antman, S. S.},
	year = {2008},
	doi = {10.1007/978-0-387-75934-0},
	keywords = {algorithm, algorithms, construction, finite element method, finite elements, functional analysis, numerical analysis, operator, Sobolev space},
	file = {Full Text PDF:/Users/oroikon/Zotero/storage/ISDSRGEV/Brenner and Scott - 2008 - The Mathematical Theory of Finite Element Methods.pdf:application/pdf},
}


@book{fdm,
	series = {Other {Titles} in {Applied} {Mathematics}},
	title = {Finite {Difference} {Methods} for {Ordinary} and {Partial} {Differential} {Equations}},
	isbn = {978-0-89871-629-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9780898717839},
	urldate = {2024-10-02},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {LeVeque, Randall J.},
	month = jan,
	year = {2007},
	doi = {10.1137/1.9780898717839},
	keywords = {accuracy, boundary value problems, convergence, finite difference methods, ordinary differential equations, partial differential equations, stability},
}

@book{fvm,
	title = {Hyperbolic {Systems} of {Conservation} {Laws}},
	language = {en},
	publisher = {Ellipses},
	author = {Godlewski, Edwige and Raviart, Pierre-Arnaud},
	year = {1991},
	note = {Google-Books-ID: X3qyvAEACAAJ},
}


@article{afshar,
  title={Prediction of aerodynamic flow fields using convolutional neural networks},
  author={Bhatnagar, Saakaar and Afshar, Yaser and Pan, Shaowu and Duraisamy, Karthik and Kaushik, Shailendra},
  journal={Computational Mechanics},
  volume={64},
  pages={525--545},
  year={2019},
  publisher={Springer}
}



@article{siri,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {0021-9991},
	shorttitle = {{DGM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
	urldate = {2024-10-02},
	journal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = dec,
	year = {2018},
	keywords = {Deep learning, High-dimensional partial differential equations, Machine learning, Partial differential equations},
	pages = {1339--1364},
	file = {ScienceDirect Snapshot:/Users/oroikon/Zotero/storage/2764TVYQ/S0021999118305527.html:text/html;Submitted Version:/Users/oroikon/Zotero/storage/W8ZQE3ED/Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:application/pdf},
}



@inproceedings{guo,
	address = {San Francisco California USA},
	title = {Convolutional {Neural} {Networks} for {Steady} {Flow} {Approximation}},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939738},
	doi = {10.1145/2939672.2939738},
	abstract = {In aerodynamics related design, analysis and optimization problems, ﬂow ﬁelds are simulated using computational ﬂuid dynamics (CFD) solvers. However, CFD simulation is usually a computationally expensive, memory demanding and time consuming iterative process. These drawbacks of CFD limit opportunities for design space exploration and forbid interactive design. We propose a general and ﬂexible approximation model for real-time prediction of non-uniform steady laminar ﬂow in a 2D or 3D domain based on convolutional neural networks (CNNs). We explored alternatives for the geometry representation and the network architecture of CNNs. We show that convolutional neural networks can estimate the velocity ﬁeld two orders of magnitude faster than a GPU-accelerated CFD solver and four orders of magnitude faster than a CPU-based CFD solver at a cost of a low error rate. This approach can provide immediate feedback for real-time design iterations at the early stage of design. Compared with existing approximation models in the aerodynamics domain, CNNs enable an eﬃcient estimation for the entire velocity ﬁeld. Furthermore, designers and engineers can directly apply the CNN approximation model in their design space exploration algorithms without training extra lower-dimensional surrogate models.},
	language = {en},
	urldate = {2024-10-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Guo, Xiaoxiao and Li, Wei and Iorio, Francesco},
	month = aug,
	year = {2016},
	pages = {481--490},
	file = {Guo et al. - 2016 - Convolutional Neural Networks for Steady Flow Appr.pdf:/Users/oroikon/Zotero/storage/75MJPUP3/Guo et al. - 2016 - Convolutional Neural Networks for Steady Flow Appr.pdf:application/pdf},
}

@article{khoo,
  title={Solving parametric PDE problems with artificial neural networks},
  author={Khoo, Yuehaw and Lu, Jianfeng and Ying, Lexing},
  journal={European Journal of Applied Mathematics},
  volume={32},
  number={3},
  pages={421--435},
  year={2021},
  publisher={Cambridge University Press}
}


@article{zhu,
  title={Bayesian deep convolutional encoder--decoder networks for surrogate modeling and uncertainty quantification},
  author={Zhu, Yinhao and Zabaras, Nicholas},
  journal={Journal of Computational Physics},
  volume={366},
  pages={415--447},
  year={2018},
  publisher={Elsevier}
}



@inproceedings{seidman_nomad_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '22},
	title = {{NOMAD}: nonlinear manifold decoders for operator learning},
	isbn = {978-1-71387-108-8},
	shorttitle = {{NOMAD}},
	abstract = {Supervised learning in function spaces is an emerging area of machine learning research with applications to the prediction of complex physical systems such as fluid flows, solid mechanics, and climate modeling. By directly learning maps (operators) between infinite dimensional function spaces, these models are able to learn discretization invariant representations of target functions. A common approach is to represent such target functions as linear combinations of basis elements learned from data. However, there are simple scenarios where, even though the target functions form a low dimensional submanifold, a very large number of basis elements is needed for an accurate linear representation. Here we present NOMAD, a novel operator learning framework with a nonlinear decoder map capable of learning finite dimensional representations of nonlinear submanifolds in function spaces. We show this method is able to accurately learn low dimensional representations of solution manifolds to partial differential equations while outperforming linear models of larger size. Additionally, we compare to state-of-the-art operator learning methods on a complex fluid dynamics benchmark and achieve competitive performance with a significantly smaller model size and training cost.},
	urldate = {2024-10-02},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Seidman, Jacob H. and Kissas, Georgios and Perdikaris, Paris and Pappas, George J.},
	month = apr,
	year = {2024},
	pages = {5601--5613},
}



@misc{fno,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2024-10-02},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/oroikon/Zotero/storage/5N5AR8M9/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:/Users/oroikon/Zotero/storage/ZQ38Q8EE/2010.html:text/html},
}

@article{deeponet,
	title = {{DeepONet}: {Learning} nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	shorttitle = {{DeepONet}},
	url = {http://arxiv.org/abs/1910.03193},
	doi = {10.1038/s42256-021-00302-5},
	abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,{\textbackslash}dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
	number = {3},
	urldate = {2024-10-02},
	journal = {Nature Machine Intelligence},
	author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	month = mar,
	year = {2021},
	note = {arXiv:1910.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {218--229},
	file = {arXiv Fulltext PDF:/Users/oroikon/Zotero/storage/XVCGUE2B/Lu et al. - 2021 - DeepONet Learning nonlinear operators for identif.pdf:application/pdf;arXiv.org Snapshot:/Users/oroikon/Zotero/storage/2J6QYFEW/1910.html:text/html},
}

@article{cno,
  title={Convolutional neural operators for robust and accurate learning of PDEs},
  author={Raonic, Bogdan and Molinaro, Roberto and De Ryck, Tim and Rohner, Tobias and Bartolucci, Francesca and Alaifari, Rima and Mishra, Siddhartha and de B{\'e}zenac, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{pinns,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}


@article{prose,
  title={PROSE: Predicting multiple operators and symbolic expressions using multimodal transformers},
  author={Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  journal={Neural Networks},
  pages={106707},
  year={2024},
  publisher={Elsevier}
}

@article{prose-fd,
  title={PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple Operators for Forecasting Fluid Dynamics},
  author={Liu, Yuxuan and Sun, Jingmin and He, Xinjie and Pinney, Griffin and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2409.09811},
  year={2024}
}


@article{mpp,
  title={Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation},
  author={Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2404.12355},
  year={2024}
}

@article{dpot,
  title={Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training},
  author={Hao, Zhongkai and Su, Chang and Liu, Songming and Berner, Julius and Ying, Chengyang and Su, Hang and Anandkumar, Anima and Song, Jian and Zhu, Jun},
  journal={arXiv preprint arXiv:2403.03542},
  year={2024}
}

@article{poseidon,
  title={Poseidon: Efficient Foundation Models for PDEs},
  author={Herde, Maximilian and Raoni{\'c}, Bogdan and Rohner, Tobias and K{\"a}ppeli, Roger and Molinaro, Roberto and de B{\'e}zenac, Emmanuel and Mishra, Siddhartha},
  journal={arXiv preprint arXiv:2405.19101},
  year={2024}
}

@inproceedings{gvae,
  title={Grammar variational autoencoder},
  author={Kusner, Matt J and Paige, Brooks and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  booktitle={International conference on machine learning},
  pages={1945--1954},
  year={2017},
  organization={PMLR}
}

@article{sdvae,
  title={Syntax-directed variational autoencoder for structured data},
  author={Dai, Hanjun and Tian, Yingtao and Dai, Bo and Skiena, Steven and Song, Le},
  journal={arXiv preprint arXiv:1802.08786},
  year={2018}
}

@article{symformer,
  title={Symformer: End-to-end symbolic regression using transformer-based architecture},
  author={Vastl, Martin and Kulh{\'a}nek, Jon{\'a}{\v{s}} and Kubal{\'\i}k, Ji{\v{r}}{\'\i} and Derner, Erik and Babu{\v{s}}ka, Robert},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}


@article{e2esr,
  title={End-to-end symbolic regression with transformers},
  author={Kamienny, Pierre-Alexandre and d'Ascoli, St{\'e}phane and Lample, Guillaume and Charton, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10269--10281},
  year={2022}
}


@inproceedings{eqldiv,
  title={Learning equations for extrapolation and control},
  author={Sahoo, Subham and Lampert, Christoph and Martius, Georg},
  booktitle={International Conference on Machine Learning},
  pages={4442--4450},
  year={2018},
  organization={Pmlr}
}

@article{eql,
  title={Extrapolation and learning equations},
  author={Martius, Georg and Lampert, Christoph H},
  journal={arXiv preprint arXiv:1610.02995},
  year={2016}
}


@article{sindy,
  title={Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  author={Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Proceedings of the national academy of sciences},
  volume={113},
  number={15},
  pages={3932--3937},
  year={2016},
  publisher={National Acad Sciences}
}


@article{snip,
  title={Snip: Bridging mathematical symbolic and numeric realms with unified pre-training},
  author={Meidani, Kazem and Shojaee, Parshin and Reddy, Chandan K and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2310.02227},
  year={2023}
}

@article{xu,
  title={DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm},
  author={Xu, Hao and Chang, Haibin and Zhang, Dongxiao},
  journal={Journal of Computational Physics},
  volume={418},
  pages={109584},
  year={2020},
  publisher={Elsevier}
}

@article{atkinson,
  title={Data-driven discovery of free-form governing differential equations},
  author={Atkinson, Steven and Subber, Waad and Wang, Liping and Khan, Genghis and Hawi, Philippe and Ghanem, Roger},
  journal={arXiv preprint arXiv:1910.05117},
  year={2019}
}


@book{separation,
  author       = {Arfken, George B.},
  title        = {Mathematical Methods for Physicists},
  edition      = {3rd},
  year         = {1985},
  publisher    = {Academic Press},
  address      = {Orlando, FL},
  chapters     = {2.6, 8.3},
  pages        = {111--117, 448--451},
  note         = {Sections: "Separation of Variables" and "Separation of Variables--Ordinary Differential Equations."},
}


@InProceedings{nts,
  title = 	 {Neural Symbolic Regression that scales},
  author =       {Biggio, Luca and Bendinelli, Tommaso and Neitz, Alexander and Lucchi, Aurelien and Parascandolo, Giambattista},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {936--945},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/biggio21a/biggio21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/biggio21a.html},
  abstract = 	 {Symbolic equations are at the core of scientific discovery. The task of discovering the underlying equation from a set of input-output pairs is called symbolic regression. Traditionally, symbolic regression methods use hand-designed strategies that do not improve with experience. In this paper, we introduce the first symbolic regression method that leverages large scale pre-training. We procedurally generate an unbounded set of equations, and simultaneously pre-train a Transformer to predict the symbolic equation from a corresponding set of input-output-pairs. At test time, we query the model on a new set of points and use its output to guide the search for the equation. We show empirically that this approach can re-discover a set of well-known physical equations, and that it improves over time with more data and compute.}
}


@article{hansen,
  title={Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)},
  author={Hansen, Nikolaus and M{\"u}ller, Sibylle D and Koumoutsakos, Petros},
  journal={Evolutionary computation},
  volume={11},
  number={1},
  pages={1--18},
  year={2003},
  publisher={MIT Press}
}


@article{oste,
  title={Completely derandomized self-adaptation in evolution strategies},
  author={Hansen, Nikolaus and Ostermeier, Andreas},
  journal={Evolutionary computation},
  volume={9},
  number={2},
  pages={159--195},
  year={2001},
  publisher={MIT Press}
}


@ARTICLE{chen-chen,
  author={Tianping Chen and Hong Chen},
  journal={IEEE Transactions on Neural Networks}, 
  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems}, 
  year={1995},
  volume={6},
  number={4},
  pages={911-917},
  keywords={Neural networks;Nonlinear dynamical systems;Computer networks;Kernel;Sufficient conditions;Polynomials;Integral equations;Mathematics;Sun;H infinity control},
  doi={10.1109/72.392253}}


@article{brandstetter,
  title={Message passing neural PDE solvers},
  author={Brandstetter, Johannes and Worrall, Daniel and Welling, Max},
  journal={arXiv preprint arXiv:2202.03376},
  year={2022}
}


@article{bar,
  title={Learning data-driven discretizations for partial differential equations},
  author={Bar-Sinai, Yohai and Hoyer, Stephan and Hickey, Jason and Brenner, Michael P},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={31},
  pages={15344--15349},
  year={2019},
  publisher={National Acad Sciences}
}


@article{dsr,
  title={Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients},
  author={Petersen, Brenden K and Landajuela, Mikel and Mundhenk, T Nathan and Santiago, Claudio P and Kim, Soo K and Kim, Joanne T},
  journal={arXiv preprint arXiv:1912.04871},
  year={2019}
}


@article{ala,
  title={Learning to represent programs with graphs},
  author={Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  journal={arXiv preprint arXiv:1711.00740},
  year={2017}
}


@article{rtgae,
  title={Recursive tree grammar autoencoders},
  author={Paa{\ss}en, Benjamin and Koprinska, Irena and Yacef, Kalina},
  journal={Machine Learning},
  volume={111},
  number={9},
  pages={3393--3423},
  year={2022},
  publisher={Springer}
}


@article{klein,
  title={Scale-dependent models for atmospheric flows},
  author={Klein, Rupert},
  journal={Annual review of fluid mechanics},
  volume={42},
  number={1},
  pages={249--274},
  year={2010},
  publisher={Annual Reviews}
}

@book{cul,
  title={A mathematical theory of large-scale atmosphere/ocean flow},
  author={Cullen, Michael JP},
  year={2006},
  publisher={Imperial College Press}
}


@article{con,
  title={On the modelling of large-scale atmospheric flow},
  author={Constantin, A and Johnson, RS},
  journal={Journal of Differential Equations},
  volume={285},
  pages={751--798},
  year={2021},
  publisher={Elsevier}
}


@inproceedings{kuri,
  title={Variational Bayesian grammar induction for natural language},
  author={Kurihara, Kenichi and Sato, Taisuke},
  booktitle={International Colloquium on Grammatical Inference},
  pages={84--96},
  year={2006},
  organization={Springer}
}


@article{kissas,
  title={The language of hyperelastic materials},
  author={Kissas, Georgios and Mishra, Siddhartha and Chatzi, Eleni and De Lorenzis, Laura},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={428},
  pages={117053},
  year={2024},
  publisher={Elsevier}
}


% This file was created with JabRef 2.3.
% Encoding: ISO-8859-1

@article{Tsoulos2006SolvingDE,
  title={Solving differential equations with genetic programming},
  author={Ioannis G. Tsoulos and Isaac E. Lagaris},
  journal={Genetic Programming and Evolvable Machines},
  year={2006},
  volume={7},
  pages={33-54},
  url={https://api.semanticscholar.org/CorpusID:1719377}
}



@article{afshar,
  title={Prediction of aerodynamic flow fields using convolutional neural networks},
  author={Bhatnagar, Saakaar and Afshar, Yaser and Pan, Shaowu and Duraisamy, Karthik and Kaushik, Shailendra},
  journal={Computational Mechanics},
  volume={64},
  pages={525--545},
  year={2019},
  publisher={Springer}
}



@article{siri,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {0021-9991},
	shorttitle = {{DGM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
	urldate = {2024-10-02},
	journal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = dec,
	year = {2018},
	keywords = {Deep learning, High-dimensional partial differential equations, Machine learning, Partial differential equations},
	pages = {1339--1364},
	file = {ScienceDirect Snapshot:/Users/oroikon/Zotero/storage/2764TVYQ/S0021999118305527.html:text/html;Submitted Version:/Users/oroikon/Zotero/storage/W8ZQE3ED/Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:application/pdf},
}



@inproceedings{guo,
	address = {San Francisco California USA},
	title = {Convolutional {Neural} {Networks} for {Steady} {Flow} {Approximation}},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939738},
	doi = {10.1145/2939672.2939738},
	abstract = {In aerodynamics related design, analysis and optimization problems, ﬂow ﬁelds are simulated using computational ﬂuid dynamics (CFD) solvers. However, CFD simulation is usually a computationally expensive, memory demanding and time consuming iterative process. These drawbacks of CFD limit opportunities for design space exploration and forbid interactive design. We propose a general and ﬂexible approximation model for real-time prediction of non-uniform steady laminar ﬂow in a 2D or 3D domain based on convolutional neural networks (CNNs). We explored alternatives for the geometry representation and the network architecture of CNNs. We show that convolutional neural networks can estimate the velocity ﬁeld two orders of magnitude faster than a GPU-accelerated CFD solver and four orders of magnitude faster than a CPU-based CFD solver at a cost of a low error rate. This approach can provide immediate feedback for real-time design iterations at the early stage of design. Compared with existing approximation models in the aerodynamics domain, CNNs enable an eﬃcient estimation for the entire velocity ﬁeld. Furthermore, designers and engineers can directly apply the CNN approximation model in their design space exploration algorithms without training extra lower-dimensional surrogate models.},
	language = {en},
	urldate = {2024-10-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Guo, Xiaoxiao and Li, Wei and Iorio, Francesco},
	month = aug,
	year = {2016},
	pages = {481--490},
	file = {Guo et al. - 2016 - Convolutional Neural Networks for Steady Flow Appr.pdf:/Users/oroikon/Zotero/storage/75MJPUP3/Guo et al. - 2016 - Convolutional Neural Networks for Steady Flow Appr.pdf:application/pdf},
}

@article{khoo,
  title={Solving parametric PDE problems with artificial neural networks},
  author={Khoo, Yuehaw and Lu, Jianfeng and Ying, Lexing},
  journal={European Journal of Applied Mathematics},
  volume={32},
  number={3},
  pages={421--435},
  year={2021},
  publisher={Cambridge University Press}
}


@article{zhu,
  title={Bayesian deep convolutional encoder--decoder networks for surrogate modeling and uncertainty quantification},
  author={Zhu, Yinhao and Zabaras, Nicholas},
  journal={Journal of Computational Physics},
  volume={366},
  pages={415--447},
  year={2018},
  publisher={Elsevier}
}



@inproceedings{seidman_nomad_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '22},
	title = {{NOMAD}: nonlinear manifold decoders for operator learning},
	isbn = {978-1-71387-108-8},
	shorttitle = {{NOMAD}},
	abstract = {Supervised learning in function spaces is an emerging area of machine learning research with applications to the prediction of complex physical systems such as fluid flows, solid mechanics, and climate modeling. By directly learning maps (operators) between infinite dimensional function spaces, these models are able to learn discretization invariant representations of target functions. A common approach is to represent such target functions as linear combinations of basis elements learned from data. However, there are simple scenarios where, even though the target functions form a low dimensional submanifold, a very large number of basis elements is needed for an accurate linear representation. Here we present NOMAD, a novel operator learning framework with a nonlinear decoder map capable of learning finite dimensional representations of nonlinear submanifolds in function spaces. We show this method is able to accurately learn low dimensional representations of solution manifolds to partial differential equations while outperforming linear models of larger size. Additionally, we compare to state-of-the-art operator learning methods on a complex fluid dynamics benchmark and achieve competitive performance with a significantly smaller model size and training cost.},
	urldate = {2024-10-02},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Seidman, Jacob H. and Kissas, Georgios and Perdikaris, Paris and Pappas, George J.},
	month = apr,
	year = {2024},
	pages = {5601--5613},
}



@misc{fno,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2024-10-02},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/oroikon/Zotero/storage/5N5AR8M9/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:/Users/oroikon/Zotero/storage/ZQ38Q8EE/2010.html:text/html},
}

@article{cno,
  title={Convolutional neural operators for robust and accurate learning of PDEs},
  author={Raonic, Bogdan and Molinaro, Roberto and De Ryck, Tim and Rohner, Tobias and Bartolucci, Francesca and Alaifari, Rima and Mishra, Siddhartha and de B{\'e}zenac, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@article{symformer,
  title={Symformer: End-to-end symbolic regression using transformer-based architecture},
  author={Vastl, Martin and Kulh{\'a}nek, Jon{\'a}{\v{s}} and Kubal{\'\i}k, Ji{\v{r}}{\'\i} and Derner, Erik and Babu{\v{s}}ka, Robert},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}


@article{kamienny2022end,
  title={End-to-end symbolic regression with transformers},
  author={Kamienny, Pierre-Alexandre and d'Ascoli, St{\'e}phane and Lample, Guillaume and Charton, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10269--10281},
  year={2022}
}


@inproceedings{eqldiv,
  title={Learning equations for extrapolation and control},
  author={Sahoo, Subham and Lampert, Christoph and Martius, Georg},
  booktitle={International Conference on Machine Learning},
  pages={4442--4450},
  year={2018},
  organization={Pmlr}
}

@article{eql,
  title={Extrapolation and learning equations},
  author={Martius, Georg and Lampert, Christoph H},
  journal={arXiv preprint arXiv:1610.02995},
  year={2016}
}


@article{sindy,
  title={Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  author={Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Proceedings of the national academy of sciences},
  volume={113},
  number={15},
  pages={3932--3937},
  year={2016},
  publisher={National Acad Sciences}
}


@article{snip,
  title={Snip: Bridging mathematical symbolic and numeric realms with unified pre-training},
  author={Meidani, Kazem and Shojaee, Parshin and Reddy, Chandan K and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2310.02227},
  year={2023}
}

@article{xu,
  title={DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm},
  author={Xu, Hao and Chang, Haibin and Zhang, Dongxiao},
  journal={Journal of Computational Physics},
  volume={418},
  pages={109584},
  year={2020},
  publisher={Elsevier}
}

@article{atkinson,
  title={Data-driven discovery of free-form governing differential equations},
  author={Atkinson, Steven and Subber, Waad and Wang, Liping and Khan, Genghis and Hawi, Philippe and Ghanem, Roger},
  journal={arXiv preprint arXiv:1910.05117},
  year={2019}
}


@book{separation,
  author       = {Arfken, George B.},
  title        = {Mathematical Methods for Physicists},
  edition      = {3rd},
  year         = {1985},
  publisher    = {Academic Press},
  address      = {Orlando, FL},
  chapters     = {2.6, 8.3},
  pages        = {111--117, 448--451},
  note         = {Sections: "Separation of Variables" and "Separation of Variables--Ordinary Differential Equations."},
}


@InProceedings{nts,
  title = 	 {Neural Symbolic Regression that scales},
  author =       {Biggio, Luca and Bendinelli, Tommaso and Neitz, Alexander and Lucchi, Aurelien and Parascandolo, Giambattista},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {936--945},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/biggio21a/biggio21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/biggio21a.html},
  abstract = 	 {Symbolic equations are at the core of scientific discovery. The task of discovering the underlying equation from a set of input-output pairs is called symbolic regression. Traditionally, symbolic regression methods use hand-designed strategies that do not improve with experience. In this paper, we introduce the first symbolic regression method that leverages large scale pre-training. We procedurally generate an unbounded set of equations, and simultaneously pre-train a Transformer to predict the symbolic equation from a corresponding set of input-output-pairs. At test time, we query the model on a new set of points and use its output to guide the search for the equation. We show empirically that this approach can re-discover a set of well-known physical equations, and that it improves over time with more data and compute.}
}


@article{oste,
  title={Completely derandomized self-adaptation in evolution strategies},
  author={Hansen, Nikolaus and Ostermeier, Andreas},
  journal={Evolutionary computation},
  volume={9},
  number={2},
  pages={159--195},
  year={2001},
  publisher={MIT Press}
}


@article{raissi2018deep,
  title={Deep hidden physics models: Deep learning of nonlinear partial differential equations},
  author={Raissi, Maziar},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={25},
  pages={1--24},
  year={2018}
}

@article{sirignano2018dgm,
  title={DGM: A deep learning algorithm for solving partial differential equations},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Journal of computational physics},
  volume={375},
  pages={1339--1364},
  year={2018},
  publisher={Elsevier}
}

@article{bhatnagar2019prediction,
  title={Prediction of aerodynamic flow fields using convolutional neural networks},
  author={Bhatnagar, Saakaar and Afshar, Yaser and Pan, Shaowu and Duraisamy, Karthik and Kaushik, Shailendra},
  journal={Computational Mechanics},
  volume={64},
  pages={525--545},
  year={2019},
  publisher={Springer}
}

@article{zhu2018bayesian,
  title={Bayesian deep convolutional encoder--decoder networks for surrogate modeling and uncertainty quantification},
  author={Zhu, Yinhao and Zabaras, Nicholas},
  journal={Journal of Computational Physics},
  volume={366},
  pages={415--447},
  year={2018},
  publisher={Elsevier}
}

@article{khoo2021solving,
  title={Solving parametric PDE problems with artificial neural networks},
  author={Khoo, Yuehaw and Lu, Jianfeng and Ying, Lexing},
  journal={European Journal of Applied Mathematics},
  volume={32},
  number={3},
  pages={421--435},
  year={2021},
  publisher={Cambridge University Press}
}



@article{charton2021linear,
  title={Linear algebra with transformers},
  author={Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2112.01898},
  year={2021}
}

@article{sun2024lemon,
  title={Lemon: Learning to learn multi-operator networks},
  author={Sun, Jingmin and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2408.16168},
  year={2024}
}

@article{yang2023context,
  title={In-context operator learning with data prompts for differential equation problems},
  author={Yang, Liu and Liu, Siting and Meng, Tingwei and Osher, Stanley J},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={39},
  pages={e2310142120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{yang2023prompting,
  title={Prompting in-context operator learning with sensor data, equations, and natural language},
  author={Yang, Liu and Meng, Tingwei and Liu, Siting and Osher, Stanley J},
  journal={arXiv preprint arXiv:2308.05061},
  year={2023}
}


@article{d2023odeformer,
  title={Odeformer: Symbolic regression of dynamical systems with transformers},
  author={d'Ascoli, St{\'e}phane and Becker, S{\"o}ren and Mathis, Alexander and Schwaller, Philippe and Kilbertus, Niki},
  journal={arXiv preprint arXiv:2310.05573},
  year={2023}
}

@article{lample2019deep,
  title={Deep learning for symbolic mathematics},
  author={Lample, Guillaume and Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1912.01412},
  year={2019}
}

@inproceedings{scholl2023uniqueness,
  title={The uniqueness problem of physical law learning},
  author={Scholl, Philipp and Bacho, Aras and Boche, Holger and Kutyniok, Gitta},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{kissas2020machine,
  title={Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive 4D flow MRI data using physics-informed neural networks},
  author={Kissas, Georgios and Yang, Yibo and Hwuang, Eileen and Witschey, Walter R and Detre, John A and Perdikaris, Paris},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={358},
  pages={112623},
  year={2020},
  publisher={Elsevier}
}
@article{kissas2022learning,
  title={Learning operators with coupled attention},
  author={Kissas, Georgios and Seidman, Jacob H and Guilhoto, Leonardo Ferreira and Preciado, Victor M and Pappas, George J and Perdikaris, Paris},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={215},
  pages={1--63},
  year={2022}
}
@article{seidman2022nomad,
  title={NOMAD: Nonlinear manifold decoders for operator learning},
  author={Seidman, Jacob and Kissas, Georgios and Perdikaris, Paris and Pappas, George J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5601--5613},
  year={2022}
}

@article{seidman2023variational,
  title={Variational autoencoding neural operators},
  author={Seidman, Jacob H and Kissas, Georgios and Pappas, George J and Perdikaris, Paris},
  journal={arXiv preprint arXiv:2302.10351},
  year={2023}
}
@phdthesis{kissas2023towards,
  title={Towards Digital Twins for Cardiovascular Flows: A Hybrid Machine Learning and Computational Fluid Dynamics Approach},
  author={Kissas, Georgios},
  year={2023},
  school={University of Pennsylvania}
}

@article{kissas2022feasibility,
  title={Feasibility of Vascular Parameter Estimation for Assessing Hypertensive Pregnancy Disorders},
  author={Kissas, Georgios and Hwuang, Eileen and Thompson, Elizabeth W and Schwartz, Nadav and Detre, John A and Witschey, Walter R and Perdikaris, Paris},
  journal={Journal of Biomechanical Engineering},
  volume={144},
  number={12},
  pages={121011},
  year={2022},
  publisher={American Society of Mechanical Engineers}
}

@article{lingsch2024fuse,
  title={FUSE: Fast Unified Simulation and Estimation for PDEs},
  author={Lingsch, Levi E and Grund, Dana and Mishra, Siddhartha and Kissas, Georgios},
  journal={arXiv preprint arXiv:2405.14558},
  year={2024}
}

@article{paassen2022recursive,
  title={Recursive tree grammar autoencoders},
  author={Paa{\ss}en, Benjamin and Koprinska, Irena and Yacef, Kalina},
  journal={Machine Learning},
  volume={111},
  number={9},
  pages={3393--3423},
  year={2022},
  publisher={Springer}
}

@article{kissas2024language,
  title={The language of hyperelastic materials},
  author={Kissas, Georgios and Mishra, Siddhartha and Chatzi, Eleni and De Lorenzis, Laura},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={428},
  pages={117053},
  year={2024},
  publisher={Elsevier}
}

@article{roache2002code,
  title={Code verification by the method of manufactured solutions},
  author={Roache, Patrick J},
  journal={J. Fluids Eng.},
  volume={124},
  number={1},
  pages={4--10},
  year={2002}
}

@inproceedings{kusner2017grammar,
  title={Grammar variational autoencoder},
  author={Kusner, Matt J and Paige, Brooks and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  booktitle={International conference on machine learning},
  pages={1945--1954},
  year={2017},
  organization={PMLR}
}

@article{knuth1968semantics,
  title={Semantics of context-free languages},
  author={Knuth, Donald E},
  journal={Mathematical systems theory},
  volume={2},
  number={2},
  pages={127--145},
  year={1968},
  publisher={Springer}
}

@article{mayoh1981attribute,
  title={Attribute grammars and mathematical semantics},
  author={Mayoh, Brian H},
  journal={SIAM Journal on Computing},
  volume={10},
  number={3},
  pages={503--518},
  year={1981},
  publisher={SIAM}
}

@article{brence2021probabilistic,
  title={Probabilistic grammars for equation discovery},
  author={Brence, Jure and Todorovski, Ljup{\v{c}}o and D{\v{z}}eroski, Sa{\v{s}}o},
  journal={Knowledge-Based Systems},
  volume={224},
  pages={107077},
  year={2021},
  publisher={Elsevier}
}

@article{mevznar2023efficient,
  title={Efficient generator of mathematical expressions for symbolic regression},
  author={Me{\v{z}}nar, Sebastian and D{\v{z}}eroski, Sa{\v{s}}o and Todorovski, Ljup{\v{c}}o},
  journal={Machine Learning},
  volume={112},
  number={11},
  pages={4563--4596},
  year={2023},
  publisher={Springer}
}

@article{brence2023dimensionally,
  title={Dimensionally-consistent equation discovery through probabilistic attribute grammars},
  author={Brence, Jure and D{\v{z}}eroski, Sa{\v{s}}o and Todorovski, Ljup{\v{c}}o},
  journal={Information Sciences},
  volume={632},
  pages={742--756},
  year={2023},
  publisher={Elsevier}
}

@article{brandstetter2022clifford,
                  title={Clifford neural layers for PDE modeling},
                  author={Brandstetter, Johannes and Berg, Rianne van den and Welling, Max and Gupta, Jayesh K},
                  journal={arXiv preprint arXiv:2209.04934},
                  year={2022}
}

@article{gupta2022towards,
              title={Towards Multi-spatiotemporal-scale Generalized PDE Modeling},
              author={Gupta, Jayesh K and Brandstetter, Johannes},
              journal={arXiv preprint arXiv:2209.15616},
              year={2022}
            }

@inproceedings{PDEBench2022,
author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
title = {{PDEBench: An Extensive Benchmark for Scientific Machine Learning}},
year = {2022},
booktitle = {36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks},
url = {https://arxiv.org/abs/2210.07182}
}

@data{darus-2986_2022,
author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
publisher = {DaRUS},
title = {{PDEBench Datasets}},
year = {2022},
doi = {10.18419/darus-2986},
url = {https://doi.org/10.18419/darus-2986}
}

@article{cape-takamoto:2023,
     author   = {Makoto Takamoto and
                 Francesco Alesiani and
                 Mathias Niepert},
 title        = {Learning Neural {PDE} Solvers with Parameter-Guided Channel Attention},
 journal      = {CoRR},
 volume       = {abs/2304.14118},
 year         = {2023},
 url          = {https://doi.org/10.48550/arXiv.2304.14118},
 doi          = {10.48550/arXiv.2304.14118},
 eprinttype    = {arXiv},
 eprint       = {2304.14118},
 }

@inproceedings{vcnef-vectorized-conditional-neural-fields-hagnberger:2024,
author = {Hagnberger, Jan and Kalimuthu, Marimuthu and Musekamp, Daniel and Niepert, Mathias},
title = {{Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations}},
year = {2024},
booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML 2024)}
}

@article{active-learn-neuralpde-benchmark-musekamp:2024,
 author       = {Daniel Musekamp and
                 Marimuthu Kalimuthu and
                 David Holzm{\"{u}}ller and
                 Makoto Takamoto and
                 Mathias Niepert},
 title        = {Active Learning for Neural {PDE} Solvers},
 journal      = {CoRR},
 volume       = {abs/2408.01536},
 year         = {2024},
 url          = {https://doi.org/10.48550/arXiv.2408.01536},
 doi          = {10.48550/ARXIV.2408.01536},
 eprinttype    = {arXiv},
 eprint       = {2408.01536},
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}



@article{shen2018ordered,
  title={Ordered neurons: Integrating tree structures into recurrent neural networks},
  author={Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
  journal={arXiv preprint arXiv:1810.09536},
  year={2018}
}

@article{kim2020pre,
  title={Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction},
  author={Kim, Taeuk and Choi, Jihun and Edmiston, Daniel and Lee, Sang-goo},
  journal={arXiv preprint arXiv:2002.00737},
  year={2020}
}

@article{wu2020perturbed,
  title={Perturbed masking: Parameter-free probing for analyzing and interpreting BERT},
  author={Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
  journal={arXiv preprint arXiv:2004.14786},
  year={2020}
}

@article{rebain2022attention,
  title={Attention beats concatenation for conditioning neural fields},
  author={Rebain, Daniel and Matthews, Mark J and Yi, Kwang Moo and Sharma, Gopal and Lagun, Dmitry and Tagliasacchi, Andrea},
  journal={arXiv preprint arXiv:2209.10684},
  year={2022}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@online{sustainml,
  title={https://sustainml.eu/},
  url          = {https://sustainml.eu/}
}

@online{daiedge,
  title={https://daiedge.eu/},
  url          = {https://daiedge.eu/}
}

@online{elias,
  title={https://elias-ai.eu/},
  url          = {https://elias-ai.eu/}
}


@article{molinaro2024generative,
  title={Generative AI for fast and accurate Statistical Computation of Fluids},
  author={Molinaro, Roberto and Lanthaler, Samuel and Raoni{\'c}, Bogdan and Rohner, Tobias and Armegioiu, Victor and Wan, Zhong Yi and Sha, Fei and Mishra, Siddhartha and Zepeda-N{\'u}{\~n}ez, Leonardo},
  journal={arXiv preprint arXiv:2409.18359},
  year={2024}
}


@article{lu2021learning,
  title={Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Nature machine intelligence},
  volume={3},
  number={3},
  pages={218--229},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}

@article{raonic2024convolutional,
  title={Convolutional neural operators for robust and accurate learning of PDEs},
  author={Raonic, Bogdan and Molinaro, Roberto and De Ryck, Tim and Rohner, Tobias and Bartolucci, Francesca and Alaifari, Rima and Mishra, Siddhartha and de B{\'e}zenac, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{shen2024ups,
  title={UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation},
  author={Shen, Junhong and Marwah, Tanya and Talwalkar, Ameet},
  booktitle={ICML 2024 AI for Science Workshop},
  year={2024}
}


@article{alkin2024universal,
  title={Universal physics transformers},
  author={Alkin, Benedikt and F{\"u}rst, Andreas and Schmid, Simon and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes},
  journal={arXiv preprint arXiv:2402.12365},
  year={2024}
}


@incollection{grefenstette1999tokenization,
  title={Tokenization},
  author={Grefenstette, Gregory},
  booktitle={Syntactic wordclass tagging},
  pages={117--133},
  year={1999},
  publisher={Springer}
}

@article{huuskonen2015polish,
  title={Polish notation},
  author={Huuskonen, Taneli},
  journal={Formalized Mathematics},
  volume={23},
  number={3},
  pages={161--176},
  year={2015}
}


@inproceedings{kamienny2023deep,
  title={Deep generative symbolic regression with Monte-Carlo-tree-search},
  author={Kamienny, Pierre-Alexandre and Lample, Guillaume and Lamprier, Sylvain and Virgolin, Marco},
  booktitle={International Conference on Machine Learning},
  pages={15655--15668},
  year={2023},
  organization={PMLR}
}


@article{chen2022symbolic,
  title={Symbolic genetic algorithm for discovering open-form partial differential equations (SGA-PDE)},
  author={Chen, Yuntian and Luo, Yingtao and Liu, Qiang and Xu, Hao and Zhang, Dongxiao},
  journal={Physical Review Research},
  volume={4},
  number={2},
  pages={023174},
  year={2022},
  publisher={APS}
}

@article{landajuela2022unified,
  title={A unified framework for deep symbolic regression},
  author={Landajuela, Mikel and Lee, Chak Shing and Yang, Jiachen and Glatt, Ruben and Santiago, Claudio P and Aravena, Ignacio and Mundhenk, Terrell and Mulcahy, Garrett and Petersen, Brenden K},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33985--33998},
  year={2022}
}

@article{koza1994genetic,
  title={Genetic programming as a means for programming computers by natural selection},
  author={Koza, John R},
  journal={Statistics and computing},
  volume={4},
  pages={87--112},
  year={1994},
  publisher={Springer}
}

@article{holland1992genetic,
  title={Genetic algorithms},
  author={Holland, John H},
  journal={Scientific american},
  volume={267},
  number={1},
  pages={66--73},
  year={1992},
  publisher={JSTOR}
}

@article{virgolin2022symbolic,
  title={Symbolic regression is NP-hard},
  author={Virgolin, Marco and Pissis, Solon P},
  journal={arXiv preprint arXiv:2207.01018},
  year={2022}
}

@article{brunton2016discovering,
  title={Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  author={Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Proceedings of the national academy of sciences},
  volume={113},
  number={15},
  pages={3932--3937},
  year={2016},
  publisher={National Acad Sciences}
}

@article{rudy2017data,
  title={Data-driven discovery of partial differential equations},
  author={Rudy, Samuel H and Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Science advances},
  volume={3},
  number={4},
  pages={e1602614},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{rudy2019data,
  title={Data-driven identification of parametric partial differential equations},
  author={Rudy, Samuel and Alla, Alessandro and Brunton, Steven L and Kutz, J Nathan},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={18},
  number={2},
  pages={643--660},
  year={2019},
  publisher={SIAM}
}


@inproceedings{sahoo2018learning,
  title={Learning equations for extrapolation and control},
  author={Sahoo, Subham and Lampert, Christoph and Martius, Georg},
  booktitle={International Conference on Machine Learning},
  pages={4442--4450},
  year={2018},
  organization={Pmlr}
}

@article{champion2019data,
  title={Data-driven discovery of coordinates and governing equations},
  author={Champion, Kathleen and Lusch, Bethany and Kutz, J Nathan and Brunton, Steven L},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={45},
  pages={22445--22451},
  year={2019},
  publisher={National Acad Sciences}
}

@article{martius2016extrapolation,
  title={Extrapolation and learning equations},
  author={Martius, Georg and Lampert, Christoph H},
  journal={arXiv preprint arXiv:1610.02995},
  year={2016}
}

@article{dugan2020occamnet,
  title={Occamnet: A fast neural model for symbolic regression at scale},
  author={Dugan, Owen and Dangovski, Rumen and Costa, Allan and Kim, Samuel and Goyal, Pawan and Jacobson, Joseph and Solja{\v{c}}i{\'c}, Marin},
  journal={arXiv preprint arXiv:2007.10784},
  year={2020}
}

@article{kovachki2023neural,
  title={Neural operator: Learning maps between function spaces with applications to pdes},
  author={Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={89},
  pages={1--97},
  year={2023}
}

@article{sherwin2003one,
  title={One-dimensional modelling of a vascular network in space-time variables},
  author={Sherwin, Spencer J and Franke, V and Peir{\'o}, Joaquim and Parker, K20389821200},
  journal={Journal of engineering mathematics},
  volume={47},
  pages={217--250},
  year={2003},
  publisher={Springer}
}

@article{flaschel2023automated,
  title={Automated discovery of generalized standard material models with EUCLID},
  author={Flaschel, Moritz and Kumar, Siddhant and De Lorenzis, Laura},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={405},
  pages={115867},
  year={2023},
  publisher={Elsevier}
}

@article{flaschel2022discovering,
  title={Discovering plasticity models without stress data},
  author={Flaschel, Moritz and Kumar, Siddhant and De Lorenzis, Laura},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={91},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{FLASCHEL2023105404,
title = {Automated discovery of interpretable hyperelastic material models for human brain tissue with EUCLID},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {180},
pages = {105404},
year = {2023},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2023.105404},
url = {https://www.sciencedirect.com/science/article/pii/S0022509623002089},
author = {Moritz Flaschel and Huitian Yu and Nina Reiter and Jan Hinrichsen and Silvia Budday and Paul Steinmann and Siddhant Kumar and Laura {De Lorenzis}},
keywords = {Constitutive models, Hyperelasticity, Brain tissue, Interpretable models, Sparse regression},
}
@article{flaschel2021unsupervised,
  title={Unsupervised discovery of interpretable hyperelastic constitutive laws},
  author={Flaschel, Moritz and Kumar, Siddhant and De Lorenzis, Laura},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={381},
  pages={113852},
  year={2021},
  publisher={Elsevier}
}

@article{linka2023new,
  title={A new family of Constitutive Artificial Neural Networks towards automated model discovery},
  author={Linka, Kevin and Kuhl, Ellen},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={403},
  pages={115731},
  year={2023},
 publisher={Elsevier}
}


@article{pierre2023principal,
  title={Principal-stretch-based constitutive neural networks autonomously discover a subclass of Ogden models for human brain tissue},
  author={Pierre, Sarah R St and Linka, Kevin and Kuhl, Ellen},
  journal={Brain Multiphysics},
  volume={4},
  pages={100066},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{dai2018syntaxdirected,
title={Syntax-Directed Variational Autoencoder for Structured Data},
author={Hanjun Dai and Yingtao Tian and Bo Dai and Steven Skiena and Le Song},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SyqShMZRb},
}


@article{hansen2003reducing,
  title={Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)},
  author={Hansen, Nikolaus and M{\"u}ller, Sibylle D and Koumoutsakos, Petros},
  journal={Evolutionary computation},
  volume={11},
  number={1},
  pages={1--18},
  year={2003},
  publisher={MIT Press}
}
@article{irsoy2014deep,
  title={Deep recursive neural networks for compositionality in language},
  author={Irsoy, Ozan and Cardie, Claire},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{sun2024towards,
  title={Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation},
  author={Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2404.12355},
  year={2024}
}

@article{lagaris1998artificial,
  title={Artificial neural networks for solving ordinary and partial differential equations},
  author={Lagaris, Isaac E and Likas, Aristidis and Fotiadis, Dimitrios I},
  journal={IEEE transactions on neural networks},
  volume={9},
  number={5},
  pages={987--1000},
  year={1998},
  publisher={IEEE}
}

@article{de2023operator,
  title={An operator preconditioning perspective on training in physics-informed machine learning},
  author={De Ryck, Tim and Bonnet, Florent and Mishra, Siddhartha and de B{\'e}zenac, Emmanuel},
  journal={arXiv preprint arXiv:2310.05801},
  year={2023}
}

@article{gallagher1975finite,
  title={Finite element},
  author={Gallagher, RH and Oden, JT and Taylor, C and Zienkiewicz, OC},
  journal={Analysis: Fundamentals. Prentice Hall, Englewood Cliffs},
  year={1975}
}

@article{eymard2000finite,
  title={Finite volume methods},
  author={Eymard, Robert and Gallou{\"e}t, Thierry and Herbin, Rapha{\`e}le},
  journal={Handbook of numerical analysis},
  volume={7},
  pages={713--1018},
  year={2000},
  publisher={Elsevier}
}

@Manual{mathematica,
title = {Mathematica},
author = {{Wolfram Research{,} Inc.}}, 
organization = {Wolfram Research, Inc.},
address = {Champaign, IL}, 
year = {2024},
note = {Version 14.1}, 
url = {https://www.wolfram.com/mathematica/} }

@article{liang2022finite,
  title={Finite expression method for solving high-dimensional partial differential equations},
  author={Liang, Senwei and Yang, Haizhao},
  journal={arXiv preprint arXiv:2206.10121},
  year={2022}
}

@article{wei2024closed,
  title={Closed-form Symbolic Solutions: A New Perspective on Solving Partial Differential Equations},
  author={Wei, Shu and Li, Yanjie and Yu, Lina and Wu, Min and Li, Weijun and Hao, Meilan and Li, Wenqiang and Liu, Jingyi and Deng, Yusong},
  journal={arXiv preprint arXiv:2405.14620},
  year={2024}
}

@article{wei2019general,
  title={General solutions for nonlinear differential equations: a rule-based self-learning approach using deep reinforcement learning},
  author={Wei, Shiyin and Jin, Xiaowei and Li, Hui},
  journal={Computational Mechanics},
  volume={64},
  pages={1361--1374},
  year={2019},
  publisher={Springer}
}


@article{chaquet2019using,
  title={Using covariance matrix adaptation evolution strategies for solving different types of differential equations},
  author={Chaquet, Jose M and Carmona, Enrique J},
  journal={Soft Computing},
  volume={23},
  pages={1643--1666},
  year={2019},
  publisher={Springer}
}

@inproceedings{seaton2010analytic,
  title={Analytic solutions to differential equations under graph-based genetic programming},
  author={Seaton, Tom and Brown, Gavin and Miller, Julian F},
  booktitle={European Conference on Genetic Programming},
  pages={232--243},
  year={2010},
  organization={Springer}
}


@article{boudouaoui2020solving,
  title={Solving differential equations with artificial bee colony programming},
  author={Boudouaoui, Yassine and Habbi, Hacene and Ozturk, Celal and Karaboga, Dervis},
  journal={Soft Computing},
  volume={24},
  pages={17991--18007},
  year={2020},
  publisher={Springer}
}

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group}
}


@article{toscano2024pinns,
  title={From pinns to pikans: Recent advances in physics-informed machine learning},
  author={Toscano, Juan Diego and Oommen, Vivek and Varghese, Alan John and Zou, Zongren and Daryakenari, Nazanin Ahmadi and Wu, Chenxi and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2410.13228},
  year={2024}
}


@article{cai2021physics,
  title={Physics-informed neural networks (PINNs) for fluid mechanics: A review},
  author={Cai, Shengze and Mao, Zhiping and Wang, Zhicheng and Yin, Minglang and Karniadakis, George Em},
  journal={Acta Mechanica Sinica},
  volume={37},
  number={12},
  pages={1727--1738},
  year={2021},
  publisher={Springer}
}

@article{mao2020physics,
  title={Physics-informed neural networks for high-speed flows},
  author={Mao, Zhiping and Jagtap, Ameya D and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={360},
  pages={112789},
  year={2020},
  publisher={Elsevier}
}

@article{manav2024phase,
  title={Phase-field modeling of fracture with physics-informed deep learning},
  author={Manav, Manav and Molinaro, Roberto and Mishra, Siddhartha and De Lorenzis, Laura},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={429},
  pages={117104},
  year={2024},
  publisher={Elsevier}
}


@article{wang2022and,
  title={When and why PINNs fail to train: A neural tangent kernel perspective},
  author={Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
  journal={Journal of Computational Physics},
  volume={449},
  pages={110768},
  year={2022},
  publisher={Elsevier}
}

@article{wang2021understanding,
  title={Understanding and mitigating gradient flow pathologies in physics-informed neural networks},
  author={Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3055--A3081},
  year={2021},
  publisher={SIAM}
}

@manual{Maple2024,
  title        = {Maple},
  author       = {{Maplesoft, a division of Waterloo Maple Inc.}},
  organization = {Maplesoft},
  address      = {Waterloo, Ontario},
  year         = {2024},
  url          = {https://www.maplesoft.com/products/Maple/}
}

@article{risch1969the,
  author  = {Risch, Robert H.},
  title   = {The Problem of Integration in Finite Terms},
  journal = {Transactions of the American Mathematical Society},
  volume  = {139},
  pages   = {167--189},
  year    = {1969},
  doi     = {10.2307/1994915}
}

@book{miller1977symmetry,
  title     = {Symmetry and Separation of Variables},
  author    = {Miller, W.},
  year      = {1977},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  isbn      = {9780521177399},
  url       = {https://www.cambridge.org/core/books/symmetry-and-separation-of-variables/827B10AA9A8DE34E49F3E217424EA7DC}
}

@book{dyke2014an,
  title     = {An Introduction to Laplace Transforms and Fourier Series},
  author    = {Dyke, Phil},
  year      = {2014},
  publisher = {Springer},
  address   = {London},
  isbn      = {978-1-4471-6394-7},
  url       = {https://link.springer.com/book/10.1007/978-1-4471-6395-4}
}

@book{debnath2006integral,
  title     = {Integral Transforms and Their Applications},
  author    = {Debnath, Lokenath and Bhatta, Dambaru},
  year      = {2006},
  publisher = {Chapman and Hall/CRC},
  address   = {Boca Raton},
  isbn      = {978-1-58488-575-7},
  url       = {https://www.routledge.com/Integral-Transforms-and-Their-Applications/Debnath-Bhatta/p/book/9781584885757}
}

@book{roach1982green,
  title     = {Green's Functions},
  author    = {Roach, G. F.},
  year      = {1982},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  isbn      = {9780521238908},
  url       = {https://www.cambridge.org/ba/universitypress/subjects/mathematics/differential-and-integral-equations-dynamical-systems-and-co/greens-functions-2nd-edition}
}

@article{chomsky1956three,
  author  = {Chomsky, Noam},
  title   = {Three Models for the Description of Language},
  journal = {IRE Transactions on Information Theory},
  volume  = {2},
  number  = {3},
  pages   = {113--124},
  year    = {1956},
  doi     = {10.1109/TIT.1956.1056813}
}

@book{hopcroft1979introduction,
  title     = {Introduction to Automata Theory, Languages, and Computation},
  author    = {Hopcroft, John E. and Ullman, Jeffrey D.},
  year      = {1979},
  publisher = {Addison-Wesley},
  address   = {Reading, MA},
  isbn      = {978-0201029888}
}


@article{kamali2015solving,
  title={Solving differential equations with ant colony programming},
  author={Kamali, MZM and Kumaresan, N and Ratnavelu, Kuru},
  journal={Applied Mathematical Modelling},
  volume={39},
  number={10-11},
  pages={3150--3163},
  year={2015},
  publisher={Elsevier}
}
@article{moseley2023finite,
  title={Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations},
  author={Moseley, Ben and Markham, Andrew and Nissen-Meyer, Tarje},
  journal={Advances in Computational Mathematics},
  volume={49},
  number={4},
  pages={62},
  year={2023},
  publisher={Springer}
}

@article{Dolean2024,
author = {Dolean, Victorita and Heinlein, Alexander and Mishra, Siddhartha and Moseley, Ben},
doi = {https://doi.org/10.1016/j.cma.2024.117116},
issn = {0045-7825},
journal = {Computer Methods in Applied Mechanics and Engineering},
pages = {117116},
title = {{Multilevel domain decomposition-based architectures for physics-informed neural networks}},
url = {https://www.sciencedirect.com/science/article/pii/S0045782524003724},
volume = {429},
year = {2024}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@book{boyce2017elementary,
  author    = {Boyce, William E. and DiPrima, Richard C. and Meade, Douglas B.},
  title     = {Elementary Differential Equations and Boundary Value Problems},
  publisher = {Wiley},
  edition   = {11th},
  year      = {2017}
}

@book{butcher2003numerical,
  author    = {Butcher, John C.},
  title     = {Numerical Methods for Ordinary Differential Equations},
  publisher = {Wiley},
  year      = {2003}
}

@book{saad2003iterative,
  author    = {Saad, Yousef},
  title     = {Iterative Methods for Sparse Linear Systems},
  publisher = {SIAM},
  edition   = {2nd},
  year      = {2003},
  doi       = {10.1137/1.9780898718003}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{nair2010relu,
  author    = {Nair, Vinod and Hinton, Geoffrey E.},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML)},
  pages     = {807--814},
  year      = {2010}
}

@inproceedings{dugas2001softplus,
  author    = {Dugas, Charles and Bengio, Yoshua and Bélisle, François and Nadeau, Claude and Garcia, René},
  title     = {Incorporating Second-Order Functional Knowledge for Better Option Pricing},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {14},
  pages     = {472--478},
  year      = {2001}
}

@article{srivastava2014dropout,
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title     = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal   = {Journal of Machine Learning Research},
  volume    = {15},
  pages     = {1929--1958},
  year      = {2014}
}

@inproceedings{cho2014gru,
  author    = {Cho, Kyunghyun and van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  title     = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {1724--1734},
  year      = {2014},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/D14-1179}
}

@book{chasnov2019differential,
  author    = {Chasnov, Jeffrey R.},
  title     = {Differential Equations with Applications to Mechanics, Biology, and Physics},
  publisher = {Hong Kong University of Science and Technology},
  year      = {2019},
  url       = {https://www.math.hkust.edu.hk/~machas/differential-equations.pdf}
}