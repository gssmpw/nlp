%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

%\usepackage[brazil]{babel}   
\usepackage[utf8]{inputenc}  
\usepackage{float}
     
\sloppy

\title{Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision.}

\author{Erick da Silva Farias\inst{1}, Eduardo Palhares Júnior \inst{2}}


\address{Instituto Federal de Educação, Ciência e Tecnologia do Amazonas (IFAM) \\ \textit{Campus} Manaus Zona Leste --
 Manaus, AM -- Brasil
  \email {edsfrlinux@gmail.com, eduardo.palharesjr@ifam.edu.br}
}

\begin{document} 

\maketitle

\begin{abstract}
The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long Short-Term Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events.
\end{abstract}
     

\section{Introduction}

% A violência é um fenômeno complexo que permeia a história da humanidade, manifestando-se de diversas formas e em diferentes contextos. Desde os primórdios da civilização, a violência esteve presente em guerras, conflitos territoriais e disputas de poder. Com o passar do tempo, novas manifestações surgiram, como a violência doméstica, a criminalidade urbana e os ataques terroristas. 
% A violência se manifesta em situações aparentemente triviais, como brigas em bares ou conflitos de trânsito. Esses episódios refletem tensões sociais, frustrações acumuladas e, muitas vezes, a falta de mecanismos adequados de resolução de conflitos. A cultura da agressividade e a normalização da violência nas relações sociais podem intensificar essas situações, criando um ciclo difícil de romper. 

Violence is a complex phenomenon that permeates the history of humanity, manifesting itself in different ways and in different contexts. Since the beginning of civilization, violence has been present in wars, territorial conflicts, and power disputes. Over time, new manifestations emerged, such as domestic violence, urban crime, and terrorist attacks. Violence manifests itself in seemingly trivial situations, such as fights in bars or traffic conflicts. These episodes reflect social tensions, accumulated frustrations, and, often, the lack of adequate conflict resolution mechanisms. The culture of aggression and the normalization of violence in social relationships can intensify these situations, creating a cycle that is difficult to break.

% Em muitos contextos, a desigualdade social, a pobreza e a marginalização também alimentam a violência, gerando um ambiente propício para o crime organizado e a violência urbana. Assim, a violência no mundo atual é um fenômeno multifacetado que exige uma abordagem crítica e multidisciplinar para compreendê-la e, principalmente, combatê-la. A análise de suas raízes históricas, sociais e culturais é fundamental para desenvolver estratégias eficazes de prevenção e intervenção.

In many contexts, social inequality, poverty, and marginalization also fuel violence, creating an environment conducive to organized crime and urban violence. Thus, violence in today's world is a multifaceted phenomenon that requires a critical and multidisciplinary approach to understand it and, above all, combat it. Analysis of its historical, social, and cultural roots is fundamental to developing effective prevention and intervention strategies.

% As câmeras de vigilância são amplamente utilizadas em estabelecimentos comerciais, residências, industrias, escolas e lugares públicos. Essas câmeras tem o objetivo de auxiliar os agentes que monitoram o local. Porém esse tipo de monitoramento convencional, não é muito eficaz quando se tem centenas de câmeras implantadas, pelo fato do envolvimento humano. Pois a identificação de incidentes pelas câmeras convencionais, torna-se uma tarefa ineficiente. 

Surveillance cameras are widely used in commercial establishments, homes, industries, schools, and public places. These cameras are intended to assist agents who monitor the location, however, this type of conventional monitoring is not very effective when hundreds of cameras are deployed because of human involvement, because identifying incidents using conventional cameras becomes an inefficient task.

% Uma forma eficiente de identificação de incidentes via câmera de vigilância, seria através da visão computacional. Pois as imagens do sistema CCTV podem ser vinculadas a um modelo de deep learning treinado, para realizar inferências dos incidentes relacionados a violência entre seres humanos em tempo real. Essa abordagem da utilização da visão computacional é relevante pois eliminará o custo da vigilância por seres humanos. Mas para isso funcionar, é necessário realizar testes, coleta de imagens para realizar um treinamento no modelo, comparar modelos de deep learning, e outros processos de ajustes para refinar o sistema de deteção de conflito humano.

An efficient way to identify incidents via a surveillance camera would be through computer vision, because images from the CCTV system can be linked to a trained deep learning model to make inferences about incidents related to violence between humans in real time. This approach to using computer vision is relevant as it will eliminate the cost of surveillance by humans. But for this to work, it is necessary to carry out tests, collect images to train the model, compare deep learning models, and other adjustment processes to refine the human conflict detection system.

% Em relação a coleta de dados, é importante que o dataset tenha um volume significativo, com variância em dados das classes e uma boa resolução. Pois segundo \cite{Dashdamirov2024}, para o treinamento eficaz de algoritmos, é fundamental a coleta e a rotulação de um vasto volume de dados. Embora existam conjuntos públicos de vídeos disponíveis, ainda há uma necessidade significativa de ampliar a quantidade desses dados. Além disso, aspectos como a resolução dos vídeos, a frequência de quadros, as condições de iluminação e os ângulos das câmeras apresentam grande variação. Essas diferenças complicam o desenvolvimento de modelos que sejam ao mesmo tempo robustos e capazes de se generalizar adequadamente.

With respect to data collection, it is important that the data set has a significant volume, with variance in class data and good resolution. According to \cite{Dashdamirov2024}, for effective algorithm training, the collection and labeling of a vast volume of data is essential. Although there are public sets of videos available, there is still a significant need to expand the amount of this data. Furthermore, aspects such as video resolution, frame frequency, lighting conditions, and camera angles vary greatly. These differences complicate the development of models that are both robust and capable of generalizing appropriately.

% \subsection{Related Work} 
% A utilização de Deep Learning no contexto de monitoramento em conflito humano, é relativamente novo. Pois os dados disponíveis de forma pública tem uma volumetria pequena e tem uma qualidade baixa nos frames dos vídeos. A seguir são mostrados alguns trabalhos relacionados ao contexto de monitoramento em conflito humano com técnicas de Deep Learning.

% The use of Deep Learning in the context of human conflict monitoring is relatively new, because the data available publicly has a small volume and has low quality in the video frames. Below are some works related to the context of monitoring human conflict with Deep Learning techniques.

% (Dashdamirov,2024) [0] avalia técnicas de aprendizado profundo na detecção de violência em vídeos, destacando que aumentar o conjunto de dados de 500 para 1.600 vídeos melhora a precisão média dos modelos em 6\%. Ele demonstra a importância de grandes conjuntos de dados e aprendizado por transferência para sistemas de vigilância mais eficazes. Datta et al. [1] analisaram a trajetória de movimentos e a orientação dos membros do corpo para detectar comportamentos violentos. Nguyen et al. [2] introduziram um modelo hierárquico de Markov oculto (HHMM), mostrando que ele pode ser útil para reconhecer atitudes agressivas, especialmente por meio de uma abordagem padrão de HHMM voltada à identificação de violência. Kim e Grauman [3] combinaram a Análise de Componentes Principais probabilística (PCA), usada para identificar padrões de fluxo em áreas locais, com Campos Aleatórios de Markov (MRF), que ajudam a manter a coerência global do modelo. Por outro lado, Mahadevan et al. [4] argumentaram que as representações baseadas em fluxo óptico não são adequadas para detectar alterações incomuns tanto na aparência quanto no movimento. Eles propuseram uma técnica que identifica cenas violentas avaliando elementos como presença de sangue, chamas, intensidade do movimento e volume sonoro.

The use of Deep Learning in the context of human conflict monitoring is relatively new, because the data available publicly has a small volume and has low quality in the video frames.
\cite{Dashdamirov2024} evaluates deep learning techniques in detecting violence in videos, highlighting that increasing the dataset from 500 to 1,600 videos improves the average accuracy of the models by 6\%. It demonstrates the importance of large data sets and transfer learning for more effective surveillance systems.

\cite{Datta2002} analyzed the trajectory of movements and orientation of body limbs to detect violent behavior. \cite{Nguyen2005} introduced a hierarchical hidden Markov model (HHMM), showing that it can be useful for recognizing aggressive attitudes, especially through a standard HHMM approach aimed at identifying violence.

\cite{kim2009} combined probabilistic Principal Component Analysis (PCA), used to identify flow patterns in local areas, with Markov Random Fields (MRF), which help maintain global model coherence. On the other hand, \cite{Mahadevan2010} argued that optical flow-based representations are not suitable for detecting unusual changes in both appearance and motion. They proposed a technique that identifies violent scenes by evaluating elements such as the presence of blood, flames, intensity of movement and sound volume.

%\label{sec:firstpage}

\section{Methodology}

% Neste capítulo serão apresentados a fundamentação teórica. Na seção 3.1 foram abordados sobre visão computacional. Na seção 3.2 foram abordados Aprendizado Profundo e Redes Neurais e na sessão 3.3 sobre o Attention Mechanism.



In this chapter the methodology will be presented. In \ref{sec2_1}, computer vision was discussed. In section \ref{sec2_2} Deep Learning and Neural Networks were covered, LSTM and BiLSTM in the subtopics and in session \ref{sec2_3} about the Attention Mechanism.



\subsection{Computer Vision}\label{sec2_1}

% A visão computacional é uma área da inteligência artificial (IA) que se ocupa do desenvolvimento de métodos que permitem que os computadores adquiram, processem e interpretem informações visuais do mundo real, com o objetivo de tomar decisões ou fornecer recomendações (Szeliski, 2010) [5]. Sua aplicação na análise de vídeos, especialmente em tarefas como reconhecimento de padrões e eventos, é fundamental para diversos setores, como segurança pública, saúde e entretenimento. Em vídeos, a visão computacional se preocupa com a detecção de objetos, movimento, e comportamentos em sequência temporal, o que exige métodos que integrem tanto a percepção visual quanto a análise temporal (Zhou et al., 2017) [6].

%%%%% esse artigo do Zhou eu não estou encontrando

Computer vision is an area of artificial intelligence (AI) that deals with developing methods that allow computers to acquire, process and interpret visual information from the real world, with the aim of making decisions or providing recommendations \cite{Szeliski2010}.

% Os principais desafios da visão computacional em vídeos envolvem a necessidade de identificar e classificar objetos e ações em ambientes dinâmicos, como reconhecer padrões de comportamento humano ou detecção de eventos específicos, como conflitos, agressões ou interações complexas.

The main challenges of computer vision in videos involve the need to identify and classify objects and actions in dynamic environments, such as recognizing human behavior patterns or detecting specific events, such as conflicts, aggressions or complex interactions.

\subsubsection{Neural Network Models in Computer Vision}
% Os modelos de redes neurais convolucionais (CNNs) são amplamente utilizados em visão computacional devido à sua capacidade de extrair características espaciais hierárquicas de imagens e vídeos. As CNNs operam aplicando filtros (ou convoluções) sobre a imagem para extrair características locais, como bordas, texturas e formas. Esses modelos são eficientes para tarefas como detecção de objetos, reconhecimento de cenas e identificação de ações em vídeos (LeCun et al., 2015) [7].

Convolutional neural network (CNN) models are widely used in computer vision due to their ability to extract hierarchical spatial features from images and videos. CNNs operate by applying filters (or convolutions) to the image to extract local features such as edges, textures and shapes. These models are efficient for tasks such as object detection, scene recognition and action identification in videos \cite{LeCun2015}.

% Para a análise de vídeos, as CNNs são frequentemente combinadas com modelos temporais, como as redes Long Short-Term Memory (LSTM), a fim de capturar informações dinâmicas ao longo do tempo, integrando o aprendizado espacial e temporal de forma eficaz (Simonyan \& Zisserman, 2014) [8].

For video analysis, CNNs are often combined with temporal models such as Long Short-Term Memory (LSTM) networks in order to capture dynamic information over time, effectively integrating spatial and temporal learning \cite{Simonyan2014}.

\subsection{Deep Learning and Neural Networks}\label{sec2_2}
% O aprendizado profundo é um subcampo da inteligência artificial que se baseia em redes neurais profundas para realizar tarefas complexas de reconhecimento, classificação e predição. Essas redes são compostas por múltiplas camadas de processamento, permitindo que elas aprendam representações hierárquicas de dados, como imagens, texto e sequências temporais.

Deep learning is a subfield of artificial intelligence that relies on deep neural networks to perform complex recognition, classification, and prediction tasks. These networks are composed of multiple layers of processing, allowing them to learn hierarchical representations of data such as images, text and temporal sequences.

\subsubsection{Convolutional Neural Networks (CNNs)}
% As redes neurais convolucionais (CNNs) são uma classe de redes neurais profundas que têm sido amplamente utilizadas em tarefas de visão computacional devido à sua capacidade de aprender representações eficientes de dados visuais. Elas são compostas por camadas convolucionais, camadas de pooling e camadas totalmente conectadas. As CNNs são eficazes na extração de características espaciais de imagens, o que permite detectar padrões, como bordas, texturas e formas (LeCun et al., 2015) [7].

Convolutional neural networks (CNNs) are a class of deep neural networks that have been widely used in computer vision tasks due to their ability to learn efficient representations of visual data. Figure \ref{fig:exampleFig1} shows a diagram that represents the architecture of a CNN. They are composed of convolutional layers, pooling layers, and fully connected layers. CNNs are effective in extracting spatial features from images, which allows them to detect patterns, such as edges, textures and shapes \cite{LeCun2015}. 

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{cnn_architeture_example.png}
\caption{An example of CNN architecture \cite{O’Shea2015}}
\label{fig:exampleFig1}
\end{figure}

% Quando aplicadas à análise de vídeos, as CNNs podem ser usadas para detectar objetos em movimento, como humanos, veículos ou qualquer outro tipo de interesse. Essas redes também são capazes de detectar comportamentos complexos e interações ao extrair características espaciais de cada quadro do vídeo e aprender representações sequenciais.

When applied to video analysis, CNNs can be used to detect moving objects, such as humans, vehicles, or any other type of interest. These networks are also capable of detecting complex behaviors and interactions by extracting spatial features from each video frame and learning sequential representations.

\subsubsection{Long Short-Term Memory (LSTM)}
% A Long Short-Term Memory (LSTM) é uma arquitetura de rede neural recorrente (RNN) projetada para modelar dependências temporais em dados sequenciais. As LSTMs possuem células de memória que permitem a retenção de informações ao longo do tempo, superando o problema do desvanecimento do gradiente que limita outras RNNs tradicionais (Hochreiter \& Schmidhuber, 1997) [9].

Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture designed to model temporal dependencies in sequential data. Figure \ref{fig:exampleFig2} shows the LSTM architecture diagram. LSTMs have memory cells that allow the retention of information over time, overcoming the problem of gradient fading that limits other traditional RNNs \cite{Hochreiter1997}.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{LSTM.jpeg}
\caption{Basic Long-Short Term Memory (LSTM) architecture \cite{Shenfield2020}}
\label{fig:exampleFig2}
\end{figure}

% No contexto de vídeos, as LSTMs são usadas para modelar sequências temporais, capturando dependências de longo prazo entre quadros ou entre eventos que ocorrem em momentos distintos, o que é essencial para a detecção de comportamentos humanos, como a identificação de ações ou interações.

In the context of videos, LSTMs are used to model temporal sequences, capturing long-term dependencies between frames or between events that occur at different times, which is essential for detecting human behaviors, such as identifying actions or interactions.

\subsubsection{Bidirectional LSTM (BiLSTM)}
% O Bidirectional LSTM (BiLSTM) é uma variação das LSTMs que processa a sequência de entradas tanto na direção direta quanto na reversa, o que permite que o modelo tenha uma compreensão mais completa das dependências temporais.

Bidirectional LSTM (BiLSTM) is a variation of LSTMs that processes the sequence of inputs in both the forward and reverse directions, which allows the model to have a more complete understanding of temporal dependencies. Figure \ref{fig:exampleFig3} shows the BiLSTM architecture diagram.

 \newpage
 
\begin{figure}[ht]
\centering
\includegraphics[width=.9\textwidth]{bilstm.png}
\caption{Architecture of Bi-LSTM network \cite{Raihan2023}}
\label{fig:exampleFig3}
\end{figure}

% Isso é particularmente útil em vídeos, onde o contexto pode ser influenciado tanto por eventos passados quanto por eventos futuros. O BiLSTM pode ser aplicado na detecção de conflitos ou eventos dinâmicos, permitindo que o modelo compreenda não só o que acontece antes, mas também o que acontece após determinado evento, melhorando a acurácia na identificação de padrões complexos (Graves et al., 2013) [10].

This is particularly useful in videos, where context can be influenced by both past and future events. BiLSTM can be applied to detect conflicts or dynamic events, allowing the model to understand not only what happens before, but also what happens after a certain event, improving accuracy in identifying complex patterns \cite{Graves2013}.

\subsection{Attention Mechanism}\label{sec2_3}
% O Mecanismo de Atenção é uma técnica fundamental no campo do aprendizado profundo, usada para melhorar a capacidade de modelos de se concentrar nas partes mais relevantes da entrada durante o processamento. Em vez de tratar todos os elementos da entrada de maneira igual, o Mecanismo de Atenção permite que o modelo aprenda a alocar maior peso para as partes mais informativas da entrada, melhorando a eficiência e precisão das previsões. Essa técnica foi inicialmente proposta por Bahdanau et al. (2015) [11], e mais tarde refinada em modelos como o Transformer proposto por Vaswani et al. (2017) [12], que utilizam mecanismos de atenção como sua base central.

The Attention Engine is a fundamental technique in the field of deep learning, used to improve the ability of models to focus on the most relevant parts of input during processing. Instead of treating all elements of the input equally, the Attention Engine allows the model to learn to allocate greater weight to the most informative parts of the input, improving the efficiency and accuracy of predictions. This technique was initially proposed by \cite{Bahdanau2015}, and later refined into models such as the Transformer proposed by \cite{Vaswani2017}, which use attention mechanisms as their central basis.

\subsubsection{Types of Attention Mechanisms}
% Existem diferentes tipos de Mecanismos de Atenção, cada um com suas características e formas de implementação. Abaixo, discutimos dois tipos principais de mecanismos de atenção: o Mecanismo de atenção com soma ponderada e o Mecanismo de Atenção Multi-Cabeça.

There are different types of attention mechanisms, each with its own characteristics and forms of implementation. Below, we discuss two main types of attention mechanisms: the mechanism of attention with weighted sum and the multi-head attention mechanism.
The attention with weighted sum focuses on assigning different weights to input features, allowing the model to prioritize more relevant information while processing sequences. On the other hand, the multi-head attention mechanism improves the model's ability to focus on various parts of the input simultaneously by using multiple attention heads. However, for the purposes of this article, the discussion will be limited to the attention mechanism with weighted sum.

% \textbf{Weighted Sum Attention:}
% O mecanismo que utilizamos no nosso modelo é uma versão simples de atenção, frequentemente chamada de atenção com soma ponderada. Nesse tipo de mecanismo, o modelo calcula uma pontuação de atenção para cada elemento da sequência de entrada, utilizando um produto escalar entre o vetor de entrada $x_i$ e um vetor de pesos \textit{W} aprendido durante o treinamento. A pontuação de atenção $e_i$ para cada elemento $X_i$ é dada pela fórmula:

The mechanism we use in our model is a simple version of attention, often called weighted sum attention. In this type of mechanism, the model calculates an attention score for each element of the input sequence, using a dot product between the input vector $x_i$ and a weight vector \textit{W} learned during training. The attention score $e_i$ for each element $X_i$ is given by the formula:

\[ e_i = \tanh(W^T x_i + b) \]

% onde:

\noindent where:

% \begin{itemize}
%     \item $e_i$  é a pontuação de atenção do elemento $x_i$
%     \item \textit{W} é o vetor de pesos.
%     \item $b$ é o viés.
%     \item  $tanh$ é a função de ativação que ajuda a limitar o valor da pontuação de atenção.
% \end{itemize}

\begin{itemize}
    \item $e_i$ is the attention score of element $x_i$
    \item \textit{W} is the weight vector.
    \item $b$ is the bias.
    \item  $tanh$ is the activation function that helps limit the value of the attention score.
\end{itemize}

% A seguir, a pontuação de atenção $e_i$ é normalizada usando a função \textbf{softmax}, gerando os pesos de atenção $a_i$:

Next, the $e_i$ attention score is normalized using the \textbf{softmax} function, generating the weights of $a_i$:

\[ a_i = \frac{e_i}{\sum_je_j} \]

% Onde $a_i$ são \textbf{pesos de atenção}  normalizados que indicam a relevância de cada elemento da entrada. Esses pesos são aplicados à entrada $x_i$ gerando uma soma ponderada das entradas, como mostrado na fórmula:

\noindent Where $a_i$ are normalized \textbf{attention weights} that indicate the relevance of each element of the input. These weights are applied to the input $x_i$ generating a weighted sum of the inputs, as shown in the formula:

\[ output = \sum_i  a_ix_i\]

% Esse Mecanismo de Atenção Simples permite que o modelo se concentre nas partes mais relevantes da entrada, atribuindo maiores pesos às entradas que são mais informativas para a tarefa. Esse tipo de atenção é útil em muitas tarefas de processamento de sequências e imagens, onde é necessário destacar regiões ou momentos específicos da entrada (Bahdanau et al., 2015).

This Simple Attention Mechanism allows the model to focus on the most relevant parts of the input, assigning greater weights to inputs that are more informative for the task.

\begin{figure}[ht]
\centering
\includegraphics[width=.4\textwidth]{attention.jpeg}
\caption{Illustrative image of attention mechanism weighting \cite{Bahdanau2015}}
\label{fig:exampleFig4}
\end{figure}

This type of attention is useful in many image sequence processing tasks, where it is necessary to highlight specific regions or moments of the input \cite{Bahdanau2015}.

%\subsubsection{Benefits of Attention Mechanism in Video Based Models}
\subsubsection{Designed Architecture for Conflict Detection}
% No contexto de vídeos, o Mecanismo de Atenção oferece benefícios consideráveis. A capacidade de direcionar a atenção para momentos críticos ou partes relevantes do vídeo melhora a precisão e a eficiência do modelo, especialmente em tarefas de análise temporal. Em vídeos com muitas informações irrelevantes ou de fundo, o Mecanismo de Atenção pode ser usado para filtrar essas partes e destacar os eventos ou ações significativas. Esse enfoque torna o modelo mais eficiente, permitindo-lhe identificar comportamentos humanos, interações e eventos críticos com maior precisão.

%In the video context, the attention mechanism offers considerable benefits. The ability to direct attention to critical moments or relevant parts of the video improves the accuracy and efficiency of the model, especially in temporal analysis tasks. In videos with a lot of irrelevant or background information, the attention mechanism can be used to filter these parts and highlight significant events or actions. This approach makes the model more efficient, allowing it to identify human behaviors, interactions and critical events more accurately.



The model developed for classifying image sequences is based on a deep neural network designed to capture both spatial and temporal features from the data. The input consists of a sequence of 15 images, each with a size of 100x100 pixels and 3 color channels (RGB), initially processed by a convolutional layer (CNN). The first processing step applies a TimeDistributed layer, allowing the convolutional network to treat each image independently within the sequence.

To prevent overfitting, the model uses a Dropout layer immediately after this step, helping to improve generalization. Next, the architecture includes a Bidirectional LSTM layer, enabling the model to consider both past and future contexts of the image sequence, better capturing the temporal relationships between frames.

An Attention layer is applied afterward, allowing the model to perform weighting of the most relevant images within the sequence, giving more importance to certain frames to improve the analysis accuracy. The outputs of this layer are passed through several Dense layers, each followed by a new Dropout layer to ensure regularization of the model. Finally, the model includes a dense layer with two neurons, responsible for binary classification.

Figure \ref{fig:exampleFig5} shows the diagram of the model architecture, illustrating the layers and the data flow throughout the process.


\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{arquitetura.jpg}
\caption{Architecture implemented for the experiments. }
\label{fig:exampleFig5}
\end{figure}

This combination of convolutional, recurrent, and attention layers allows the model to extract and learn complex, dynamic information from the images and their temporal sequences, providing a robust approach for the classification task.




% A combinação do Mecanismo de Atenção com CNNs para captura das características espaciais e LSTMs para modelagem temporal de vídeos permite que o modelo seja capaz de aprender tanto as relações espaciais quanto as dependências temporais. Como resultado, a análise de vídeos torna-se mais precisa e robusta, especialmente em tarefas complexas de detecção de interações humanas ou eventos importantes ao longo do tempo.

%The combination of CNNs attention mechanism for capturing spatial characteristics and LSTMs for temporal video modeling allows the model to be able to learn both spatial relationships and temporal dependencies. As a result, video analysis becomes more accurate and robust, especially in complex tasks for detecting human interaction or important events over time.




%\subsection{Methodology Steps}

% A metodologia para condução das atividades que compõem este trabalho foi caracterizada pelas seguintes atividades: 

%The methodology for conducting the activities that make up this work was characterized by the following activities:

% \begin{enumerate}
%     \item Realizou-se levantamento bibliográﬁco, para identiﬁcar e estudar as principais referências bibliográﬁcas sobre \textit{Deep Learning} e Detecção de Violências.
%     \item Configuração do ambiente do \textit{Colab} para execução dos treinamentos de modelos com TPU (\textit{Tensor Processing Unit}).
%     \item Tratamento do conjunto de dados \textit{Real Life Violence Situations Dataset}, convertendo os vídeos que são do tipo mp4 para matriz.
%     \item Upload do dataset formatado para o google drive.
%     \item Realizou-se a execução dos modelos com variações nos parâmetros, para gerar um bom resultado de acurácia.
%     \item Análise e comparação dos resultados obtidos.
% \end{enumerate}

%\begin{enumerate}
  %  \item  A bibliographic survey was conducted to identify and study the main bibliographic references on \textit{Deep Learning}, Violence Detection and Attention Mechanism.
%    \item Configuration of the \textit{Colab} environment to run model training with TPU (\textit{Tensor Processing Unit}).
%    \item Treatment of the \textit{Real Life Violence Situations Dataset} with augmentation and converting the videos to matrix trought numpy.
%    \item Upload the formatted Dataset to Google Drive.
%    \item Execution of models with variations in the parameters was performed to generate a good accuracy result.
%    \item Analysis and comparison of the results obtained.
%\end{enumerate}

\section{Results and Discussion}

% Os experimentos foram realizados com os modelos \textit{MobileNetV2}, \textit{DenseNet121} e \textit{InceptionV3}. Também foi utilizado \textit{BiLSTM} em conjunto com os modelos. Os experimentos foram realizados com e sem mecanismo de atenção. Em relação aos parâmetros, tiveram variações na taxa de aprendizado mínima (\textit{min\_lr}) e tamanho do lote (\textit{batch\_size}). No total foram executados 12 experimentos. Abaixo está a tabela com os detalhes dos experimentos que foram realizados. 

The experiments were performed with the \textit{MobileNetV2}, \textit{DenseNet121} and \textit{InceptionV3}. It was also used \textit{BiLSTM} in conjunction with the models. The experiments were performed with and without attention mechanism. Regarding the parameters, there were variations in the minimum learning rate (\textit{min\_lr}) and batch size (\textit{batch\_size}). %There were 12 scenarios, whose details of the experiments performed are presented in the table \ref{tabela:experimentos}.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{ID} & \textbf{Model} & \textbf{Attention} & \textbf{min\_lr} & \textbf{batch\_size} & \textbf{Accuracy (\%)} \\ \hline
1 & MobileNetV2 & No & 0.0005 & 128& 94.25 \\ \hline
2 & DenseNet121 & No & 0.0005 & 128& 94.75 \\ \hline
3 & InceptionV3 & No & 0.0005 & 128& 94.25\\ \hline
4 & MobileNetV2 & No & 0.00005 & 64& 89.00 \\ \hline
5 & DenseNet121 & No & 0.00005 & 64& 93.75 \\ \hline
6 & InceptionV3 & No & 0.00005 & 64& 91.00 \\ \hline
7 & MobileNetV2 & Yes & 0.0005 & 128& 93.25 \\  \hline
8 & DenseNet121 & Yes & 0.0005 & 128& 92.50 \\ \hline
9 & InceptionV3 & Yes & 0.0005 & 128 & 91.75 \\ \hline
10 & MobileNetV2 & Yes & 0.00005 & 64& 96.50 \\ \hline
11 & DenseNet121 & Yes & 0.00005 & 64& 95.50\\ \hline
12 & InceptionV3 & Yes & 0.00005 & 64& 94.25 \\ \hline
\end{tabular}
% \caption{Detalhes de todos os treinamentos realizados dos modelos \textit{MobileNetV2}, \textit{DenseNet121} e \textit{InceptionV3}.}
\caption{Training performed details with models \textit{MobileNetV2}, \textit{DenseNet121} and \textit{InceptionV3}.}
\label{tabela:experimentos}
\end{table}

% A Tabela \ref{tabela:experimentos} apresenta os detalhes de 12 experimentos realizados com os modelos \textit{MobileNetV2}, \textit{DenseNet121} e \textit{InceptionV3}. Nos experimentos, a variável de interesse foi a acurácia obtida durante o treinamento, que variou em função do uso do mecanismo de atenção, da taxa de aprendizado mínima e do tamanho do lote.

Table \ref{tabela:experimentos} presents the details of 12 experiments performed with the models \textit{MobileNetV2}, \textit{DenseNet121} e \textit{InceptionV3}. In experiments, the variable of interest was the accuracy obtained during training, which varied according to the use of the attention mechanism, the minimum learning rate and the size of the lot.

% Em relação aos experimentos sem \textit{Attention Mechanism}, no experimento com a taxa de aprendizado mínima de 0.0005 e tamanho de lote 128, o modelo \textit{DenseNet121} obteve a maior acurácia, com 94.75\%, seguido de \textit{MobileNetV2} com 94.25\% e \textit{InceptionV3} com 94.25\%. Quando a taxa de aprendizado mínima foi reduzida para 0.00005 e o tamanho do lote ajustado para 64, \textit{MobileNetV2} apresentou a menor acurácia entre todos os experimentos de 89.00\%, enquanto \textit{DenseNet121} tiveram uma leva queda no valor de acurácia, com 93.75\% e \textit{InceptionV3} teve uma acurácia de 91.00\%. Com o uso de \textit{Attention Mechanism}, os modelos mostraram uma leve queda na acurácia com a taxa de aprendizado mínima de 0.0005 e tamanho de lote 128. A acurácia do \textit{MobileNetV2} foi de 93.25\%, a de \textit{DenseNet121} foi de 92.50\%, e a de \textit{InceptionV3} foi de 91.75\%.

In relation to the experiments without \textit{Attention Mechanism}, in the experiment with the minimum learning rate of 0.0005 and batch size 128, the \textit{DenseNet121} model obtained the highest accuracy, with 94.75\%, followed by \textit{ MobileNetV2} with 94.25\% and \textit{InceptionV3} with 94.25\%. When the minimum learning rate was reduced to 0.00005 and the batch size was adjusted to 64, \textit{MobileNetV2} had the lowest accuracy among all experiments at 89.00\%, while \textit{DenseNet121} had a slight drop in the value of accuracy, with 93.75\% and \textit{InceptionV3} had an accuracy of 91.00\%. Using \textit{Attention Mechanism}, the models showed a slight drop in accuracy with the minimum learning rate of 0.0005 and lot size 128. The accuracy of \textit{DenseNet121} was 93.25\%, \textit{DenseNet121} It was 92.50\%, and that of \textit{InceptionV3} was 91.75\%.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
 \textbf{Model} & \textbf{Accuracy (\%)} & \textbf{F1-Score (Class 0)} & \textbf{F1-Score (Class 1)}  \\ \hline
MobileNetV2  & 96.50 & 96.00 & 97.00  \\ \hline
DenseNet121  & 95.50 & 95.00  & 96.00 \\ \hline
InceptionV3 & 94.25 & 94.00 & 94.00 \\ \hline

\end{tabular}
% \caption{Melhores resultados de métricas de desempenho dos modelos \textit{MobileNetV2 }, \textit{DenseNet121 } e \textit{InceptionV3 }.}
\caption{Best results of model performance metrics \textit{MobileNetV2 }, \textit{DenseNet121 } and \textit{InceptionV3 }.}
\label{tabela:melhores_experimentos}
\end{table}

% Quando a taxa de aprendizado mínima foi reduzida para 0.00005 e o tamanho de lote ajustado para 64, o desempenho foi superior em relação aos experimentos sem atenção. 
% O modelo \textit{MobileNetV2} alcançou a melhor acurácia, com 96.50\%, seguido por \textit{DenseNet121} com 95.50\%, e \textit{InceptionV3} com 94.25\%. Na Tabela \ref{tabela:melhores_experimentos},  são apresentadas as melhores acurácias de cada modelo, e todos os modelos tem bons resultados com a métricas de desempenho de acurácia e \textit{F1-Score}.

When the minimum learning rate was reduced to 0.00005 and the batch size was adjusted to 64, the performance was superior compared to the no-attention experiments.
The \textit{MobileNetV2} model achieved the best accuracy, with 96.50\%, followed by \textit{DenseNet121} with 95.50\%, and \textit{InceptionV3} with 94.25\%. In Table \ref{tabela:melhores_experimentos}, the best accuracies of each model are presented, and all models have good results with the accuracy and \textit{F1-Score} performance metrics.

\begin{figure}[H]
\centering
\includegraphics[width=.6\textwidth]{matrix.jpg}
% \caption{Matriz de confusão do experimento de melhor desempenho}
\caption{Best performance confusion matrix}
\label{fig:matriz_conf}
\end{figure}

% Conforme a matriz de confusão do experimento 10, mostrada na Figura \ref{fig:matriz_conf}, observa-se no eixo da predição que as classes NonViolence e Violence, acertaram quase todos os testes.

According to the confusion matrix of experiment 10, shown in the figure \ref{fig:matriz_conf}, it is observed in the axis of prediction that the nonviolence and violence classes hit almost all tests.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{tempo_treinamento.png}
% \caption{Gráfico de tempo de treinamento de todos os experimentos.}
\caption{Training time of all experiments.}
\label{fig:tempo}
\end{figure}

% Conforme a Figura \ref{fig:tempo}, e a referência dos experimentos na Tabela \ref{tabela:experimentos}, observa-se que o \textit{Attention Mechanism} não estendeu tempo de execução do treinamento. Por exemplo,  os experimentos 1,2,3,7,8 e 9 tem as mesmas configurações de parâmetros. Os experimentos de ID, 1,2 e 3 não tem \textit{Attention Mechanism} e os experimentos 7,8 e 9 tem o \textit{Attention Mechanism}. Ou seja, o mecanismo não tem influência no tempo de treinamento. Outra análise relevante que pode-se observar na Figura \ref{fig:tempo}, é que tempo de treinamento diminui nos experimentos de tamanho de lote reduzido.

According to the figure \ref{fig:tempo}, and the reference of the experiments in the table \ref{tabela:experimentos}, it is observed that \textit{Attention Mechanism} did not extended training time. For example, experiments 1,2,3,7,8 and 9 have the same parameter settings. Experiments ID, 1,2 and 3 do not have \textit{Attention Mechanism} and experiments 7,8 and 9 have \textit{Attention Mechanism}. In other words, the mechanism has no influence on training time. Another relevant analysis that can be observed in Figure \ref{fig:tempo} is that training time decreases in experiments with reduced batch sizes.

% Colocar em conclusões isso que está abaixo
%Conforme a Figura \ref{fig:matriz_conf}, os resultados foram satisfatórios, pois no eixo da predição observa-se que as classes NonViolence e Violence, acertaram quase todos os testes. Isso se deve ao fato não somente da aplicação do mecanismo de atenção, mas também pelo dataset ser balanceado.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mobile_accu.png}
        \vspace{0.1cm} % Reduz o espaço entre a imagem e a legenda
        % {\small \textnormal{(a) Gráfico de acurácia em relação a época de todos os treinamentos realizados do modelo \textit{MobileNetV2}}} % Fonte menor e sem negrito
        {\small \textnormal{(a) Accuracy in relation to the epochs of all training of the model \textit{MobileNetV2}}} % Fonte menor e sem negrito
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mobile_loss.png}
        \vspace{0.1cm} % Reduz o espaço entre a imagem e a legenda
        % {\small \textnormal{(b) Gráfico de erro em relação a época de todos os treinamentos realizados do modelo \textit{MobileNetV2}}} % Fonte menor e sem negrito
        {\small \textnormal{(b) Error in relation to the epochs of all training of the model \textit{MobileNetV2}}} % Fonte menor e sem negrito
    \end{minipage}
    
    \vspace{0.3cm} % Ajuste do espaçamento entre as linhas das legendas
    
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{dense_accu.png}
        \vspace{0.1cm} % Reduz o espaço entre a imagem e a legenda
        % {\small \textnormal{(c) Gráfico de acurácia em relação a época de todos os treinamentos realizados do modelo \textit{DenseNet121}}} % Fonte menor e sem negrito
        {\small \textnormal{(c) Accuracy in relation to the epochs of all training of the model \textit{DenseNet121}}} % Fonte menor e sem negrito
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{dense_loss.png}
        \vspace{0.1cm} % Reduz o espaço entre a imagem e a legenda
        % {\small \textnormal{(d) Gráfico de erro em relação a época de todos os treinamentos realizados do modelo \textit{DenseNet121}}} % Fonte menor e sem negrito
        {\small \textnormal{(d) Error in relation to the epochs of all training of the model \textit{DenseNet121}}} % Fonte menor e sem negrito
    \end{minipage}

    \vspace{0.3cm} % Ajuste do espaçamento entre as linhas das legendas

     \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{incep_accu.png}
        \vspace{0.1cm} % Reduz o espaço entre a imagem e a legenda
        % {\small \textnormal{(e) Gráfico de acurácia em relação a época de todos os treinamentos realizados do modelo \textit{InceptionV3}}} % Fonte menor e sem negrito
        {\small \textnormal{(e) Accuracy in relation to the epochs of all training of the model \textit{InceptionV3}}} % Fonte menor e sem negrito
    \end{minipage}% 
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{incep_loss.png}
        \vspace{0.1cm} % Reduz o espaço entre a imagem e a legenda
        % {\small \textnormal{(f)  Gráfico de erro em relação a época de todos os treinamentos realizados do modelo \textit{InceptionV3}}} % Fonte menor e sem negrito
        {\small \textnormal{(f) Error in relation to the epochs of all training of the model \textit{InceptionV3}}} % Fonte menor e sem negrito
    \end{minipage}
    
    % \caption{Gráficos de todos os  treinamentos de todos os experimentos realizados}
    \caption{Training of all experiments performed}
    \label{fig:matrix}
\end{figure}

% Em relação ao gráfico de acurácia da Figura \ref{fig:matrix}a, sabendo que o experimento 4 não utiliza o mecanismo, ele finaliza o treinamento mais rápido, em relação aos demais experimentos. Outro ponto relevante no gráfico da Figura \ref{fig:matrix}a, é que o experimento 1 também não utiliza o mecanismo e demora para finalizar o treinamento. Isso indica que a aplicação \textit{Attention Mechanism} não tem influência no tempo de treinamento do modelo.

In relation to the accuracy graph in Figure \ref{fig:matrix}, knowing that experiment 4 does not use the mechanism, it completes training faster, in relation to the other experiments. Another relevant point in the graph of Figure \ref{fig:matrix}, is that experiment 1 also does not use the mechanism and delay to finish the training. This indicates that the application \textit{Attention Mechanism} has no influence on model training time.

% Pode-se observar que em todos os gráficos de erro da Figura \ref{fig:matrix}, os experimentos com  batch reduzido e com o mecanismo, encontraram os melhores mínimos. Esses experimentos buscaram os mínimos mais rápido nas primeira épocas, pois a maioria dos gradientes dos batches foram a uma direção em específico. Outro padrão dos experimentos 10,11 e 12. Foi que, devido a redução da velocidade da direção para o mínimo, a aleatoridade levou para direções em que o erro aumentou levemente para depois encontrarem mínimos melhores.

It can be observed that in all the error charts in the figure \ref{fig:matrix}, the reduced batch experiments with the mechanism have found the best minimum. These experiments sought the fastest minimums in the first times, as most of the batches gradients went to a specific direction. Another pattern of experiments 10, 11 and 12. It was that, due to the reduction of the steering speed to the minimum, the randomness led to directions in which the error increased slightly to later find better minimums.

\section{Conclusion and Future Works}
This study presented a detailed analysis of the application of Deep Learning models for violence detection in videos, focusing on the comparison of three popular architectures: MobileNetV2, DenseNet121, and InceptionV3. The results obtained demonstrated that all models were effective, with MobileNetV2 standing out by achieving the highest accuracy of 96.50\%, especially when the batch size was reduced and the attention mechanism was applied.

Through the experiments conducted, it was observed that the selection of appropriate parameters, such as learning rate and batch size, is crucial for optimizing the model's performance. Additionally, although the Attention Mechanism showed a slight reduction in accuracy in some scenarios, it still proved useful in other parameter combinations, such as when a lower learning rate (0.00005) and a batch size adjusted to 64 were used.

Based on the results achieved, several improvements and new directions can be explored in future works to further enhance violence detection in videos and expand the applications of this technology. The experiments conducted focused solely on the use of videos, but it would be interesting to expand to multimodal data, combining audio, image, and even sensor information, which could contribute to a more robust analysis of the scene.

\newpage
\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}

The subsection titles must be in boldface, 12pt, flush left.

\section{Figures and Captions}\label{sec:figs}


Figure and table captions should be centered if less than one line
(Figure~\ref{fig:exampleFig1}), otherwise justified and indented by 0.8cm on
both margins, as shown in Figure~\ref{fig:exampleFig2}. The caption font must
be Helvetica, 10 point, boldface, with 6 points of space before and after each
caption.

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{fig1.jpg}
\caption{A typical figure}
\label{fig:exampleFig1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=.3\textwidth]{fig2.jpg}
\caption{This figure is an example of a figure caption taking more than one
  line and justified considering margins mentioned in Section~\ref{sec:figs}.}
\label{fig:exampleFig2}
\end{figure}

In tables, try to avoid the use of colored or shaded backgrounds, and avoid
thick, doubled, or unnecessary framing lines. When reporting empirical data,
do not use more decimal digits than warranted by their precision and
reproducibility. Table caption must be placed before the table (see Table 1)
and the font used must also be Helvetica, 10 point, boldface, with 6 points of
space before and after each caption.

\begin{table}[ht]
\centering
\caption{Variables to be considered on the evaluation of interaction
  techniques}
\label{tab:exTable1}
\includegraphics[width=.7\textwidth]{table.jpg}
\end{table}

\section{Images}

All images and illustrations should be in black-and-white, or gray tones,
excepting for the papers that will be electronically available (on CD-ROMs,
internet, etc.). The image resolution on paper should be about 600 dpi for
black-and-white images, and 150-300 dpi for grayscale images.  Do not include
images with excessive resolution, as they may take hours to print, without any
visible difference in the result. 

\section{References}



Referências
Bahdanau, D., Cho, K., \& Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to Align and Translate. Proceedings of the International Conference on Learning Representations (ICLR).
Graves, A., Mohamed, A. R., \& Hinton, G. (2013). Speech Recognition with Deep Recurrent Neural Networks. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
Hochreiter, S., \& Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780.
LeCun, Y., Bengio, Y., \& Hinton, G. (2015). Deep Learning. Nature, 521(7553), 436–444.
Simonyan, K., \& Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. Proceedings of the International Conference on Learning Representations (ICLR).
Szeliski, R. (2010). Computer Vision: Algorithms and Applications. Springer.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. A., Kaiser, Ł., \& Polosukhin, I. (2017). Attention is All You Need. Proceedings of the Neural Information Processing Systems (NeurIPS).
Zhou, Y., Zheng, Y., \& Zhang, Y. (2017). Understanding Human Behavior in Video: A Review of Human Action Recognition. Journal of Visual Communication and Image Representation, 48, 40–59.


Bibliographic references must be unambiguous and uniform.  We recommend giving
the author names references in brackets, e.g. \cite{knuth:84},
\cite{boulic:91}, and \cite{smith:99}.

The references must be listed using 12 point font size, with 6 points of space
before each reference. The first line of each reference should not be
indented, while the subsequent should be indented by 0.5 cm.

\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
