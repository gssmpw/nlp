\section{Related works}
\label{sec:rel_works}
%\subsection{Controls in Diffusion models}
% LDM 의 cross-attention : text prompt guidance를 제공
% 큰 틀에서, text2image, text-based or guidance-based editing 모두 diffusion model에 guidance를 주는것. 그것을 training-free상황에서 진행하는것이 SDEdit 이고, trainable 하게 image를 바꾸는것이 SDS계열임
%Guidance-based diffusion models provide an crucial direction for diffusion models to generate higher-quality images.
%CG\cite{dhariwal2021diffusion} and CFG\cite{ho2021classifier} are prominent approaches in this domain, balancing sample quality and diversity by incorporating guidance during both denoising and sampling. These approaches have inspired further developments to enhance diffusion models' quality and adaptability across diverse conditioning settings, broadly encompassing tasks like text-to-image synthesis and text- or guidance-based editing, all of which provide targeted guidance to diffusion models.
% CG CFG 는 sampling 과정에서의 guidance를 제공.
\subsection{Image Editing with Diffusion Models}
% sdeedit 은 strok등의 인풋을 적당한 노이즈로 corrupt 시켜 guidance로 함.
With the great success of image generation using diffusion models, the pre-trained diffusion models have been recently employed for image editing tasks, 
%Recently, various diffusion models have been employed for image editing tasks, 
demonstrating significant advancements in the quality and flexibility of generated edits \cite{mengsdedit, hertzprompt, tumanyan2023plug, brooks2023instructpix2pix, nam2024contrastive, koo2024posterior}. 
Stochastic Differential Editing (SDEdit) \cite{mengsdedit} is a pioneering work in which the source image was modified by adding noise and solving reverse stochastic differential equations.
Thanks to the text-conditional Latent Diffusion Model (LDM), a.k.a. Stable Diffusion \cite{rombach2022high}, text-driven editing approaches have been introduced. Specifically, the text embedding was injected through the cross-attention layer of the model for image editing and translation, while retaining the structure of the original image \cite{hertzprompt, tumanyan2023plug}. The editing was further controlled by rescaling the attention of the specific word \cite{hertzprompt} or by manipulating the self-attention features \cite{tumanyan2023plug}.
These approaches provide greater control by balancing fidelity between the edited prompt and the source image without the need for model training, fine-tuning, additional data, or optimization. However, the current DDIM-based inversion \cite{song2020denoising} can lead to unsatisfactory reconstructions for real images, and the cross-attention bottleneck limits its effectiveness for broader edits. Crafting suitable prompts also remains challenging for complex compositions. 
%SDEdit\cite{mengsdedit} operates by introducing intermediate noise to an image and then denoising it based on the desired edit, effectively guiding the diffusion process by corrupting the input with appropriate noise. This method requires neither training data nor a specific loss function, relying solely on a pre-trained SDE model for efficient global edits. However, SDEdit is computationally intensive, and the quality of the final output is highly sensitive to the quality of the initial guidance input.
%T2L\cite{bar2022text2live} introduces a zero-shot, text-driven editing method for localized appearance manipulation in images and videos by generating an RGBA edit layer that is composited over the original input. This approach allows for semantic edits guided by text prompts, without requiring user-provided masks or pre-trained generators, which makes it versatile across different objects and effects. Notably, T2L extends its editing capabilities beyond images to videos, highlighting its significance in achieving consistent and meaningful edits across both media. However, T2L is computationally intensive and relies on the quality of the pre-trained CLIP model for effective semantic matching, and its applicability is limited by the accuracy of the neural layered atlas representation used for video editing.
%P2P\cite{hertzprompt} introduces a text editing method that preserves the composition and structure of an image by injecting cross-attention maps into the diffusion process of a pre-trained text-conditional diffusion model using Prompt-to-Prompt manipulation. This approach requires no model training, fine-tuning, additional data, or optimization, while allowing greater control by balancing fidelity between the edited prompt and the source image. However, current DDIM-based inversion can lead to unsatisfactory reconstructions for real images, and the cross-attention bottleneck limits its effectiveness for broader edits. Crafting suitable prompts also remains challenging for complex compositions.
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------

\subsection{Score distillation sampling}
Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion} enables text-driven 3D synthesis by leveraging probability density distillation loss to distill knowledge from 2D diffusion models, allowing high-quality 3D scene generation based on textual prompts without 3D training data. However, SDS has limitations, often producing oversaturated and overly smooth 3D models, and lacking diversity across initializations.%, limiting the variety of generated 3D structures and details. 
To address these limitations of SDS, various models have been proposed based on exploiting multi-step denoising~\cite{zhou2023sparsefusion}, a variation approach~\cite{wang2024prolificdreamer}, negative conditioning~\cite{katzir2023noise}, and ordinary differential equation trajectory~\cite{wu2024consistent3d}.
%VSD \cite{wang2024prolificdreamer} treats the 3D representation as a probabilistic distribution, optimizing multiple particles within a particle-based framework to more accurately capture the 3D distribution. 
To mitigate the limitation of noisy gradients in SDS, which hampers precise image editing, DDS \cite{hertz2023delta} was introduced. By computing the delta between the derived gradient and the target pair, DDS effectively isolates and removes unwanted noise in the gradient direction. Despite these advancements, DDS still faces challenges in preserving the complete structural consistency of the source image’s identity.
%We propose a novel method for image editing that aligns with the target text prompt while preserving the identity and structure of the source image.
% To address the issues, Delta Denoising Score (DDS) \cite{hertz2023delta} was proposed based on the analysis of the Score Distillation Sampling (SDS) \cite{poole2022dreamfusion}. SDS \cite{poole2022dreamfusion} was originally introduced to optimize a Neural Radiance Field (NeRF) by distilling the knowledge from the pre-trained diffusion models. 
% When it comes to editing, SDS often suffers from noisy gradients, resulting in unintended blurring of regions that should remain unchanged, which can be considered suboptimal outputs. 
% DDS \cite{hertz2023delta} mitigates this limitation by decomposing the SDS gradients into two components: the desirable text-aligned direction and the unwanted bias component. By isolating and removing the bias component, DDS \cite{hertz2023delta} provides cleaner gradient directions, enabling more precise and controlled image modifications while maintaining fine details. Despite this improvement, DDS \cite{hertz2023delta} still struggles to preserve structural consistency. This problem was alleviated via contrastive learning \cite{nam2024contrastive} or matching the stochastic latent of the source and the edited image \cite{koo2024posterior}.
% Nevertheless, the fundamental problem related to structural consistency should be investigated in more detail. Based on the analysis of whether the gradient associated with the source image contains the overall information,  we propose a novel framework to modify the gradient that explicitly preserves the identity of the source during the optimization.
% \subsection{Fixed-point Iteration}
% In numerical analysis, fixed-point iteration is an iterative method to find fixed points of a function $f$, where $f(x) = x.$ Given an initial point $x_0$, the iteration is defined as: 
% \[
% x_{n+1} = f(x_n), \quad n = 0, 1, 2, \ldots
% \]
% Under appropriate conditions, this sequence converges to a fixed point. Fixed-point iteration is widely used in optimization, including applications in the context of diffusion models. These can also be applied to text-guided diffusion models, potentially enhancing their performance. Null-text Inversion \cite{mokady2023null} replaces the traditional approach of mapping all noise vectors to a single image using random noise for each optimization iteration with a more local optimization that employs a single noise vector. Specifically, drawing inspiration from GAN literature, it leverages the sequence of noised latent codes obtained from an initial DDIM inversion as a pivot. The optimization is then performed around this pivot, leading to a more refined and accurate inversion. However, since this method relies on a fixed pivot, it struggles to fully preserve the complex structural consistency of the original image. We propose a novel model that utilizes fixed-point iteration to correct the gradient direction aligned with the source prompt, thereby ensuring that the structure of the source image is preserved while selectively editing the target image.
%\subsection{Score Distillation Sampling}
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
% score-based sampling 은 
%core Distillation Sampling (SDS)\cite{poole2022dreamfusion} is one type of guidance sampling which address sampling bias by leveraging a pretrained 2D diffusion model for generating 3D assets without any 3D data. DreamFusion\cite{poole2022dreamfusion} uses SDS as a loss function to optimize a Neural Radiance Field (NeRF), enabling text-to-3D synthesis through gradient descent. DreamFusion replaces CLIP with a loss derived from the distillation of a 2D diffusion model, resulting in more coherent 3D scenes and richer details. SDS often suffers from noisy gradients, resulting in unintended blurring of regions that should remain unc1hanged leading to suboptimal outputs for image editing.
%Delta denoising score (DDS) \cite{hertz2023delta} proposed a distillation sampling procedure based on subtracted noises, which mitigates the issues present in SDS by decomposing the SDS gradients into two components: the desirable text-aligned direction and the undesirable noise (bias) component. By isolating and removing the bias component, DDS provides cleaner gradient directions, thereby enabling more precise and controlled image modifications while preserving fine details. Despite this improvement, DDS still struggles with preserving the structural consistency.
%Contrastive denoising score(CDS)\cite{nam2024contrastive} was proposed to address the limitations of previous methods by incorporating contrastive learning to enhance the quality of score-based sampled images. CDS integrates Contrastive Unpaired Translation (CUT) loss into the DDS framework, efficiently leveraging intermediate features from the self-attention layers of Latent Diffusion Models (LDM) to compute the contrastive loss. While this approach improves structural consistency and content fidelity, it remains sensitive to random patch selection, which can lead to inconsistent results, particularly in complex scenes or when dealing with non-standard poses.
%Instructpix2pix\cite{brooks2023instructpix2pix} utilizes paired data generated by combining GPT-3 and Stable Diffusion to train a conditional diffusion model capable of performing image edits based on human-written instructions. A key advantage of this approach is its ability to execute edits in a forward pass without the need for additional data or extensive user intervention, enabling intuitive and versatile modifications such as object replacement, style transformations, and contextual adjustments. However, InstructPix2Pix retains certain limitations from its training models, including challenges with spatial reasoning and a reliance on the quality of the generated training data.
%-------------------------------------------------------------------------