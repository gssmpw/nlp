\section{Introduction}
\label{sec:intro}

\input{Fig./Algorithm/algorithm}
% diffusion general story
Diffusion models ~\cite{ho2020denoising,song2020score,dhariwal2021diffusion,ho2021classifier,rombach2022high} have shown powerful representations on text-to-image (T2I) generative tasks. With the advance of classifier guidance (CG) and classifier-free guidance (CFG) paradigms~\cite{dhariwal2021diffusion,ho2021classifier,hong2023improving,ahn2024self}, diffusion models improve the quality of generated samples \cite{ho2020denoising,song2020score}. Such high-quality image generators can be easily extended to image editing by simply modifying forward/reverse iterations~\cite{mengsdedit}, applying CFG with a target prompt~\cite{brooks2023instructpix2pix,hertzprompt} or interchanging attention layers \cite{tumanyan2023plug}. 

\begin{figure*}[!t]
        \centering
        \includegraphics[width=0.85\textwidth]{Fig./Algorithm/imgs/method_3.jpg} % Replace with your image file
    \caption{\textbf{Flowchart of IDS.} The backbone of our algorithm employs DDS \cite{hertz2023delta} framework to distill score function into a target image. Our fixed-point regularization (FPR) obtains a guided noise, $\epsilon^*$, from iterative updates using posterior mean computed by Tweedie’s formula. When distilling the score function to a target image, the guided noise is updated while maintaining the identity of the source.}
    \label{fig:teasor}
\end{figure*}

% editing by DM strong and weakness with SDS
Recently, Delta Denoising Score (DDS)  \cite{hertz2023delta} is proposed to edit a source image by distilling the rich generative prior of T2I diffusion models. It is based on the analysis of Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion}, originally developed to optimize a parametric generator such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2021nerf} by exploiting the learned score of the diffusion models. Even though SDS offers remarkable performance in synthesizing 3D scenes, noisy gradients from stochastic perturbations lead to significantly over-saturated results that faithfully follow the given text prompts. In the context of image editing, text prompts do not often include information about the identity of the source image, such as the background, the object's pose, or the structure of the content, which should be retained during updates. Thus, DDS is designed to resolve such blurriness by erasing gradients of non-text-aligned features from SDS gradients. There is no explicit procedure to preserve the source's identity in DDS updates because the fine gradient may provide the conserved identity. However, this cannot be guaranteed if many variations in the structure are possible, such as editing the image of a cat into a pig, as shown in Fig.~\ref{fig:algorithm}. To alleviate this problem, Contrastive Denoising Score (CDS)~\cite{nam2024contrastive} and Posterior Distillation Sampling (PDS)~\cite{koo2024posterior} are introduced to maximize the mutual information of the source image and edited image. Although such algorithms rely heavily on text prompts, the algorithms have yet to analyze the inherent error caused by text-conditioned scores.

%However, the inevitable loss of identity caused by the text-conditioned score is not addressed. This problem arises from the fundamental nature of a text prompt, which can describe an infinite number of images, including the source image itself. 

%With DDS, however, even more severe errors occur, as the identity of the original image, such as the structure of the content and the background, is not explicitly taken into account. The edited images therefore often have deformed shapes or colors. 

% 

%To alleviate this problem, Contrastive Denoising Score (CDS) \cite{nam2024contrastive} is suggested to incorporate the contrastive learning scheme \cite{chen2020simple, park2020contrastive} within DDS framework to maximize the mutual information of the source image and edited image at the latent feature level. \cite{koo2024posterior} introduced a variant of DDS loss to explicitly match the stochastic latents during optimization. 
%However, these algorithms still rely on noisy scores caused by overfitting to text prompts.
%However, the text-conditioned score corresponds to the gradient from the generated latent by a forward diffusion process from the source image to the specific image represented by the corresponding text prompt. 
%Since a single text prompt can describe infinite possible images, the score conditioned by the source text prompt ($y^{src}$) is not guaranteed to always lead to the source image. This implies that the identity of the original image is inevitably lost during editing.

To this end, we investigate the underlying meaning of text-conditioned score. The gradient maps the stochastic latent, generated by applying the forward diffusion process to the given image, to one of the possible images described by the prompt, including the original image.
%Even if the desired direction points towards the original image, the actual gradient often deviates from the desired direction.
Simply, the score obtained from the latent of the source image (`source latent') and the source prompt can be a gradient to another image represented by the identical text. %, which deviates from the desired direction to the source image. 
Based on this interpretation, the accumulation of misaligned directions causes the loss of the source's identity, leading to structural changes in the result with DDS, as shown in Fig.~\ref{fig:algorithm}. 


To address this issue, we propose a novel score distillation sampling to effectively preserve the identity of the original image by self-correcting the misaligned gradients, called \textbf{I}dentity-preserving \textbf{D}istillation \textbf{S}ampling (\textbf{IDS}).
The key insight is that if the score is precisely adjusted to the source image, the conditional expectation of the source image given the source latent contains meaningful information that should be preserved during the editing.
This conditional expectation corresponds to the posterior mean computed by Tweedie’s formula using the learned score~\cite{efron2011tweedie, chungdiffusion}. The source latent is iteratively updated to make the posterior mean similar to the source image. This procedure, named a fixed-point iterative regularization (FPR), results in the aligned score with the source that provides reliable gradients for editing, as illustrated in Fig.~\ref{fig:algorithm}.
%In particular, a fixed-point iterative regularization (FPR) is introduced to provide reliable gradients to DDS \cite{hertz2023delta} inspired by a fixed-point iteration~\cite{parikh2014proximal}.
%by simply applying fixed-point iterator~\cite{parikh2014proximal} to the text-conditioned score obtained from a source latent ($\mathbf{z}_{t}^{\text{src}}$) and a source text prompt ($y^{src}$) with respect to a posterior mean image.
%The source latent ($\mathbf{z}_{t}^{\text{src}}$) is iteratively updated to extract the optimal injection noise ($\epsilon^*$), which is used to obtain the gradient for the DDS update.
Following IDS update is performed using guided noise extracted from the refined source latent, rather than random Gaussian noise. This further ensures the identity preservation.
%Our FPR improves DDS updates by refining gradient noises toward reliable pre-trained manifolds in iterative updates of a reference latent($\hat z_t$)
%to minimize the difference between the given image and the posterior mean, resulting in the optimal injection noise ($\epsilon^*$).
%is then extracted from the updates source latent, which produces the score adjusted to the source image.
%The DDS loss is then computed to produce the transformed image based on the correct score. The procedure for fixing the misalignment of the gradient to the source image is dubbed fixed-point iterative regularization (FPR).
%This self-correction approach is simple but effective in refining the gradient toward reliable pre-trained manifolds, while maintaining the identity. 
%Our FPR improves DDS updates by refining gradient noises toward reliable pre-trained manifolds in iterative updates of a reference latent($\hat z_t$).{\color{red} what is the specific defacts in DDS?}
% \add{A paragraph about the experimental results}
Our method demonstrated superior results compared with baselines in two tasks: editing images by prompts and editing NeRF.

In summary, our main contributions are as follows:
\begin{itemize}

    \item We obtain reliable gradients for the score distillation function by a fixed-point iterator with respect to posterior means. The iterator corrects the text-conditioned score, guiding SDS gradients toward reliable pre-trained manifolds.

    \item Our fixed-point regularization preserves the identities of sources such as structures and poses in edited targets for 2D and 3D editing. Such preservation is well demonstrated in qualitative and quantitative results. %both IoU and background PSNR between source and target.

    % \item Our method achieves convergent target images which earn the highest score by user study and GPT compared with baselines. %other score distillation methods for text-guided editing.

\end{itemize}

% \item We obtain reliable gradients for score distillation function by a fixed-point iterator with respect to posterior means. The iterator guides SDS gradient toward reliable pre-trained manifolds.

% \item Our fixed-point iterator is universally applicable to score distillation sampling approaches such as SDS \cite{poole2022dreamfusion}, DDS \cite{hertz2023delta} and CDS \cite{nam2024contrastive} as a self-correcting regularizer. 
