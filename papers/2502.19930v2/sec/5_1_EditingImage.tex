\noindent\textbf{Baselines.} To evaluate our method, we conduct comparative experiments against four state-of-the-art image editing models: Prompt-to-Prompt (P2P) \cite{hertzprompt}, Plug-and-Play (PNP) \cite{tumanyan2023plug}, DDS \cite{hertz2023delta}, and CDS \cite{nam2024contrastive}. The implementations of the baselines are carried out by referencing the official source code for each method. More details are provided in \cref{sec:s_implement} of Supplementary Materials.

\noindent\textbf{Qualitative Results.} We present the qualitative results comparing our method with the baselines in \cref{fig:ip2p_qual}. Prompt-to-Prompt (P2P) \cite{hertzprompt} performs image editing after applying DDIM inversion \cite{dhariwal2021diffusion, song2020denoising} to the source image, leading to disregarding the structural components of the source image and following the target prompt excessively. Plug-and-Play (PnP) \cite{tumanyan2023plug} has limitations in object recognition, as seen in the fourth row of Fig.~\ref{fig:ip2p_qual}. The third row of Fig.~\ref{fig:ip2p_qual} demonstrates that DDS \cite{hertz2023delta} and CDS \cite{nam2024contrastive} exhibited limitations, particularly in preserving the structural characteristics of the source image. In contrast, our method successfully edits the image while preserving the structural integrity of the source image.
% exhibit limitations such as failing to maintain the handle length and saddle shape of the bike in the first row and being unable to preserve the structure of the shark in the second row. %Furthermore, as seen in the third and fourth rows, the details in the edited target areas lacked refinement, and in the last row, the color of the source image was not preserved. In contrast, our method successfully edits the image aligning with the target text prompt while preserving the structural integrity of the source image.

\noindent\textbf{Quantitative Results.} 
% We employed two datasets: LAION 5B \cite{schuhmann2022laion} and InstructPix2Pix \cite{brooks2023instructpix2pix}.
% ##ORIGINAL## To measure the identity-preserving performance, we utilize two datasets. First, we collect 250 cat images from the LAION 5B dataset \cite{schuhmann2022laion} based on \cite{nam2024contrastive} for \textit{Cat-to-Others} task. We measure Intersection over Union (IoU) to evaluate how much of the area of the source object has been preserved. Second, we gather 28 images from the InstructPix2Pix (IP2P) dataset \cite{brooks2023instructpix2pix}, which contains the pairs of source and target images and corresponding prompts. We calculate the background Peak-Signal-to-Noise-Ratio (PSNR) to assess how the identity of the source image is preserved after editing. In addition, we use the LPIPS score \cite{zhang2018unreasonable} for each experiment to quantify the similarity between source and target images. The results are presented in \cref{tab:2Dquan}. Our method consistently achieves the lowest LPIPS score across all datasets, indicating that it best preserves the structural semantics of the source images. 
To measure the identity-preserving performance, we utilize two datasets. First, we collect 250 cat images from the LAION 5B dataset \cite{schuhmann2022laion} based on \cite{nam2024contrastive} for \textit{Cat-to-Others} task and measure Intersection over Union (IoU). Second, we gather 28 images from the InstructPix2Pix (IP2P) dataset \cite{brooks2023instructpix2pix}, which contains the pairs of source and target images and corresponding prompts and calculate the background Peak-Signal-to-Noise-Ratio (PSNR). Details of the metrics are provided in Supplementary Materials \cref{sec:s_evalmetric}. In addition, we use the LPIPS score \cite{zhang2018unreasonable} for each experiment to quantify the similarity between source and target images. The results are presented in \cref{tab:2Dquan}. Our method consistently achieves the lowest LPIPS score across all datasets, indicating that it best preserves the structural semantics of the source images. 
% We collect 250 images of cats from the LAION 5B dataset \cite{schuhmann2022laion} based on \cite{nam2024contrastive} for \textit{Cat-to-Others} task and 28 images from the InstructPix2Pix dataset \cite{brooks2023instructpix2pix} following the regulations. To evaluate the images translated by each method, we measure Intersection over Union (IoU) on LAION 5B, which primarily consists of object-focused data. We also measure the background PSNR on InstructPix2Pix to assess the extent to which the source imageâ€™s identity is preserved after editing. The results are presented in \cref{tab:2Dquan}. 
% Our method consistently achieves the lowest LPIPS score across all datasets, indicating that it best preserves the structural semantics of the source images. 
\input{Fig./Table/2DQuan}

\input{Fig./Table/UserStudy}
For user evaluation, we present 35 comparison sets for four baselines and our method, gathering responses from 47 participants. Participants are asked to choose the most appropriate image for the following three questions: 1. \textit{Which image best fits the text condition?} 2. \textit{Which image best preserves the structural information of the original image?} 3. \textit{Which image has the best quality for text-based image editing?} 
Additionally, we measure the GPT score using the Dreambench++ \cite{peng2024dreambench++} method, which generates human-aligned assessments for the same questions by refining the scoring into ten distinct levels. As shown in \cref{tab:Userstudy_GPTscore}, our method receives the highest ratings for all questions.
% Furthermore, we ask users to select their favorite image from the baselines in order to gauge their preferences, and we compute the selected ratio in percentage terms.
%While our CLIP score was not significantly higher than other methods, it remained comparable. %Considering the outcomes of both metrics, our model demonstrates an ability to maximally preserve the source image's structure during the editing process while minimally and precisely transforming the regions specified by the target prompt.

% Fig 5.2

