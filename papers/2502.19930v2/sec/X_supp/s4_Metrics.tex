\section{Evaluation metrics}
\label{sec:s_evalmetric}
% which model we used
% about baselines
% how to calculate background psnr & IoU
% calculated mask examples
Our purpose is to preserve the source information by optimizing the score $\epsilon_\phi^\text{src}$. Thus, in addition to the LPIPS, we newly utilize IoU and background PSNR as our metrics to measure the structural similarity between the source and edited image.% commonly used in the field of image editing. 
%For LAION 5B dataset, we use intersection over union (IoU) of source and target mask. It represents how much the area of the prompt changes after image translation.
%And, in IP2P dataset \cite{brooks2023instructpix2pix}, we measure background PSNR because it is difficult to specify the modified part of the prompt as an object.
% IoU on LAION 5B dataset \cite{schuhmann2022laion} and background PSNR on . 
%in addition to the LPIPS commonly used in the field of image editing. 

\noindent\textbf{IoU.} The aim of \textit{Cat-to-Others} task is to translate the cat into another animal. Thus, the segmentation mask of the cat and translated animal can be obtained using
%We calculate IoU for \textit{Cat-to-Others} task, We use 
the language Segment-Anything model (lang-SAM)\footnote{\url{https://github.com/paulguerrero/lang-sam}}, which is an open-source project to segment some objects from the text prompt. 
IoU of the source and target mask represents how much the area of the cat changes after image editing. The lower the IoU, the more similar the region of the cat and the region of the translated animal, meaning the overall shape is preserved.
To this end, first, the mask about the prompt is obtained from an image using lang-SAM. For example, `cat' is segmented from the source image to get the mask $M_{\text{src}}$, while `dog' is segmented from the edited image to obtain the mask $M_{\text{trg}}$, as shown in \cref{fig:sup_mask} (a). After getting masks, we calculate IoU from the masks that are given by:
\begin{equation*}
    \text{IoU}=\frac{\left(M_{\text{src}} \cap M_{\text{trg}}\right)}{\left(M_{\text{src}}\cup M_{\text{trg}}\right)}
\end{equation*}

\noindent\textbf{Background PSNR.} Since the editing prompts of IP2P dataset~\cite{brooks2023instructpix2pix} is complex than \textit{Cat-to-Others} dataset~\cite{schuhmann2022laion, nam2024contrastive}, it is hard to get mask by lang-SAM. Therefore, we use background PSNR to evaluate how much the original information is preserved. The residual of the source and target images is calculated, and the standard deviation $\sigma$ of each pixel of the residual image is computed with window size 30. Then, the mask $M_{\text{PSNR}}$ is acquired by thresholding the $\sigma$. Since the range of $\sigma$ varies according to the edited results for each method, we use the mean or median values of $\sigma$ to set an appropriate threshold (see \cref{fig:sup_mask} (b)). For the background PSNR of 
% \cref{tab:2Dquan}
Tab. 1, we use mean threshold. Finally, we calculate PSNR values from masked source and target images: 
\begin{equation*}
    \text{PSNR}_{\text{back}}=\text{PSNR}(M_{\text{PSNR}} \odot \mathbf{z}_{\text{src}}, M_{\text{PSNR}} \odot \mathbf{z}_{\text{trg}})
\end{equation*}
where $\odot$ is pixel-wise multiplication.
\input{sec/X_supp/Fig/comp/mask}
