\section{Foundations of Generality-Oriented Bayesian Optimization} \label{sec:GenBO_explanation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{content/figures/fig2_discrete.pdf}
    \vspace{-1em}
    \caption{
        A conceptual overview of the generality-oriented optimization problem. 
        \emph{Left:} The function values across the joint space $\mathcal{X} \times \mathcal{W}$. 
        \emph{Right:} Mean aggregation applied to the function family $f(\rvx; \rvw)$, that is obtained via currying of the joint space $\mathcal{X} \times \mathcal{W}$. 
         The quantity $\phi\big(f(\rvx; \rvw), \gW\big)$ constitutes the partially observable objective function, of which $\hat{\rvx} =  \argmax_{\rvx \in \gX} \ \phi(\rvx)$ is the optimum to be identified. 
    }
    \label{fig:genbo-conceptual}
\end{figure*}

To formalize the generality-oriented optimization problem, we provide a principled outline by considering it as an extension of established global optimization approaches over curried functions. 
For clarity, we also discuss its distinction to different variations of global optimization, including multiobjective, multifidelity, and mixed-variable optimization.

\subsection{Global Optimization}

Global black-box optimization is concerned with finding the optimum of an unknown objective function $f(\rvx)$: 
\begin{equation}
\hat{\rvx} = \underset{\rvx \in \gX}{\text{argmax}} \ f(\rvx)
\end{equation}
%
Suppose $f(\rvx)$ is a function that (a) is not analytically tractable, (b) is very expensive to evaluate, and (c) can only be evaluated without obtaining gradient information. In this scenario, BO has emerged as a ubiquitous approach for finding the global optimum $\hat{\rvx} \in \gX$ in a sample-efficient manner \citep{garnett_roman_bayesian_2023}. 
The working principle of BO involves a probabilistic surrogate model $g(\rvx)$ to approximate $f(\rvx)$, which can be used to compute a posterior predictive distribution over $g$ under all previous observations $\gD = \{(\rvx_i, f(\rvx_i)\}_{i=1}^k$. 
The most prominent choice for $p(g(\rvx) \mid \gD)$ are Gaussian processes \citep[GPs;][]{rasmussen_gaussian_2006}, with various types of Bayesian neural networks becoming increasingly popular in the past decade \citep{hernandez-lobato_parallel_2017, kristiadi_promises_2023, li_study_2024, kristiadi_sober_2024}. 
Based on the predictive posterior, an acquisition function $\alpha$ over the input space $\gX$ is used to decide at which $\rvx_\text{next} \in \gX$ the objective function should be evaluated next. 
Key to the success of BO is the implicit exploitationâ€“exploration tradeoff in $\alpha$, which makes use of the posterior distribution $p(g(x) \mid \gD)$ \citep{mockus_bayesian_1975}.
Common choices of $\alpha$ are Upper Confidence Bound \citep[UCB;][]{kaelbling_associative_1994, kaelbling_associative_1994-1, agrawal_sample_1995}, Expected Improvement \citep[EI;][]{jones_efficient_1998}, Knowledge Gradient \citep{gupta_bayesian_1994, frazier_knowledge-gradient_2008, frazier_knowledge-gradient_2009} or Thompson Sampling \citep[TS;][]{thompson_likelihood_1933}.
The hereby selected $\rvx_\text{next}$ is evaluated experimentally, resulting in \(f(\rvx_\text{next})\), and the described procedure is repeated until a satisfactory outcome is observed, or the experimentation budget is exhausted.

\subsection{Global Optimization for Generality} \label{subsec:Generality-BO}

\subsubsection{Problem Formulation} \label{subsubsec:algo_outline}

Extending the global optimization framework, we now consider a black-box function $f: \gX \times \gW \rightarrow \mathbb{R}$ in joint space $\gX \times \gW$, where $\rvx \in \gX$ can be continuous, discrete or mixed-variable and $\gW = \{ \mathbf{w}_i \}_{i=1}^n$ is a discrete task space of size $n$ (see \cref{fig:genbo-conceptual}).
Each evaluation of $f$ is expensive and does not provide gradient information.
In the example of reaction condition optimization, $\rvx$ are conditions from the condition space $\gX$, e.g. the  temperature, and $\rvw \in \gW$ the substrates (starting materials of a reaction) that are considered for generality-oriented optimization.
Let \(\mathrm{curry}\) be a currying operator on the second argument, i.e., $\mathrm{curry}(f): \gW \rightarrow \big( \gX \rightarrow \mathbb{R} \big)$.
Then, for some $\rvw \in \mathcal{W}$, evaluating \(\mathrm{curry}(f)(\rvw)\) yields a new function $f (\,\boldsymbol{\cdot}\,; \rvw): \gX \rightarrow \mathbb{R}$, where $f(\rvx; \rvw) = f(\rvx, \rvw)$. 
This is motivated by the fact that these $f (\,\boldsymbol{\cdot}\,; \rvw) : \gX \rightarrow \mathbb{R}$ correspond to functions that can be evaluated experimentally (i.e. a reaction for a specific substrate as a function of conditions), even though evaluations are expensive. 
In other words, all observable functions can be described through an $n$-sized set $\gF = \{ f(\,\boldsymbol{\cdot}\,; \rvw_i): \gX \rightarrow \mathbb{R} \}_{i=1}^{n}$.
In the context of reaction condition optimization $\gF$ consists of all functions that describe the reaction outcome for each substrate.
Evaluation of a specific $f(\rvx_\text{obs}; \rvw_\text{obs})$ then corresponds to measuring the reaction outcome of a substrate (described by $\rvw_\text{obs}$) under specific reaction conditions $\rvx_\text{obs}$.

In generality-oriented optimization, the goal is to identify the optimum $\hat{\rvx} \in \gX$ that is generally optimal across $\gW$, meaning $\hat{\rvx}$ maximizes a user-defined generality metric over all $\rvw \in \gW$ (see \cref{fig:genbo-conceptual} for illustration). We refer to this generality metric as the \textit{aggregation function} $\phi$:
%
\begin{equation} \label{eq:aggrate-BO}
\hat{\rvx} = \underset{\rvx \in \gX}{\text{argmax}} \ \phi(\rvx) :=  \underset{\rvx \in \gX}{\text{argmax}} \ \phi\big(f(\rvx; \rvw), \gW\big) 
\end{equation}
%
In the reaction optimization example, this corresponds to conditions (e.g. reaction temperature) that give e.g.\ the highest average yield over all considered substrates. 
In this scenario, the choice of $\phi$ is the mean $\phi(f(\rvx; \rvw), \mathcal{W}) = \nicefrac{1}{|\mathcal{W}|} \sum_{\rvw \in \mathcal{W}} f(\rvx; \rvw)$. 
An alternative choice of $\phi$ could be the number of function values \(\{ f(\rvx; \rvw_i) \}_{i=1}^n\) above a user-defined threshold \citep{betinol_data-driven_2023}.
Further practically relevant aggregation functions are described in \cref{subsubsec: Aggregation_functions}.

While \eqref{eq:aggrate-BO} appears like a standard global optimization problem over \(\gX\), evaluating \(\phi(\rvx)\) itself is intractable due to the aggregation over \(\gW\).
To evaluate \(\phi(\rvx)\) on a single \(\rvx\), one must perform \(n\)-many expensive function evaluations to first obtain \(\{ f(\rvx; \rvw_i) \}_{i=1}^n\).
Ideally, the number of such function evaluations should be minimized.
Thus, this setting differs from the conventional global optimization problem, due to its \emph{partial observation} nature:
One can only estimate \(\phi(\rvx)\) via a subset of observations \(\{ f(\rvx; \rvw_j )\}_{j=1}^m\) where \(m < n\).

To maximize sample efficiency, an optimizer should always recommend a new pair $(\rvx_\text{next}, \rvw_\text{next})$ to evaluate next -- in other words: $\phi(\rvx_{\text{next}})$ is only observed partially via a single evaluation of \(f\), i.e., \(m = 1\).
Treating this in the conventional framework of BO, we can build a probabilistic surrogate model $g(\mathbf{x}_i; \mathbf{w}_i)$ from all $k$ available observations $\gD = \{(\mathbf{x}_i, \mathbf{w}_i, f(\mathbf{x}_i; \mathbf{w}_i)\}_{i=1}^k$, referred to as $p\big(g_k (\rvx, \rvw) \mid \mathcal{D}\big)$. 
From the posterior distribution over $g$, a posterior distribution over $\phi$ can be estimated for any functional form of $\phi$ via Monte-Carlo integration \citep[see \cref{App:acqf} for further details;][]{balandat_botorch_2020}. 

Unlike the conventional BO case, we now need a specific acquisition policy $A$ to decide at which $\rvx \in \gX$ \emph{and} $\rvw \in \gW$ the aggregated objective function \(\phi(\rvx)\) should be partially evaluated.
Note that \(A\) plays an important role since it must respect the partial observability constraint.
That is, it must also propose \emph{a single} \(\rvw\) at each BO step such that the general (over \emph{all} \(\rvw_i\)'s) optimum \(\hat{\rvx}\) can be reached in as few steps as possible.
Given the pair $(\rvx_{k+1},\rvw_{k+1})$, the aggregated objective \(\phi(\rvx_{k+1})\) is partially observed, $\gD$ is updated, and the steps are repeated until the experimentation budget is exhausted.
Owing to the partial monitoring scenario \citep{rustichini_minimizing_1999, lattimore_cleaning_2019, lattimore_bandit_2020}, the final optimum after a budget of $k$ experiments, $\hat{\rvx}_k$, is returned as the $\rvx \in \gX$ that maximizes the mean of the predictive posterior of $\phi$.
A summary of this is shown in \cref{alg:genbo_general}.

\begin{algorithm}[t]

\caption{Generality-oriented Bayesian optimization} \label{alg:genbo_general}
\footnotesize
\begin{algorithmic}[1]
\Require
\Statex Set of observable functions  $\gF = \{ f(\boldsymbol{\cdot}; \rvw_i): \gX \rightarrow \mathbb{R} \}_{i=1}^{n}$
\Statex Initial dataset $\mathcal{D}_k = \big\{ \mathbf{x}_j, \mathbf{w}_j, f(\mathbf{x}_j; \mathbf{w}_j)  \big\}_{j=1}^k$
\Statex Aggregation function $\phi\big(f(\rvx; \rvw), \mathcal{W}\big)$
\Statex Surrogate model $g(\mathbf{x}, \mathbf{w})$ and acquisition policy $A$
\Statex Budget $K$

\vspace{0.2cm}

\While {\(k \le K \)}
\State Compute posterior $p\big(g_k (\mathbf{x}, \mathbf{w}) \mid \mathcal{D}_k\big)$
\State Acquire \\ $\rvx_{k+1}, \rvw_{k+1} = A\Big(p\big(g_k (\mathbf{x}, \mathbf{w}) \mid \mathcal{D}_k\big), \phi\big(f(\rvx; \rvw), \mathcal{W}\big) \Big)$
\State Observe $f(\mathbf{x}_{k+1}; \mathbf{w}_{k+1})$
\State Update $\mathcal{D}_{k+1} = \mathcal{D}_k \bigcup \big\{ (\mathbf{x_{k+1}}, \rvw_{k+1}, f(\rvx_{k+1}; \rvw_{k+1}))  \big\}$
\State $k = k+1$
\EndWhile

\vspace{0.2cm}

\State \Return $\hat{\rvx} = \underset{\rvx \in \mathcal{X}}{\text{argmax}} \ \mathbb{E} \Big[ p\big(\phi (\mathbf{x}) \mid \mathcal{D}_K\big) \mid \mathbf{x} \Big]$
\end{algorithmic}
\end{algorithm}

\subsubsection{Acquisition Strategies for $\rvx_{\text{next}}$ and $\rvw_{\text{next}}$}\label{subsubsec:acquisition_strategies_main}
As outlined above, the efficiency of generality-oriented optimization depends on the selection of $\rvx_{\text{next}}$ and $\rvw_{\text{next}}$. 
Given a posterior distribution $p\big(g_k (\rvx, \rvw) \mid \mathcal{D}\big)$ and an aggregation function $\phi\big(f(\rvx; \rvw), \mathcal{W}\big)$, any acquisition policy should determine $\rvx_{\text{next}}$ and $\rvw_{\text{next}}$, which formally requires optimization over $\mathcal{X} \times \mathcal{W}$.
Assuming weak coupling between $\mathcal{X}$ and $\mathcal{W}$, we can formulate a sequential acquisition policy, as outlined in \cref{alg:sequential_acquisition}. 
First, $\rvx_{\text{next}}$ is acquired by optimizing an $\rvx$-specific acquisition function $\alpha_x$ over the aggregation function's posterior. Second, a $\rvw$-specific acquisition $\alpha_w$ is optimized over the posterior distribution at $\rvx_{\text{next}}$. Notably, in this setting, established one-step-lookahead acquisition functions can be used for both $\alpha_x$ and $\alpha_w$. 

\begin{algorithm} 

\caption{Sequential Acquisition Strategy}\label{alg:sequential_acquisition}
\footnotesize
\begin{algorithmic}[1]
\Require
\Statex Posterior distribution  $p\big(g_k (\rvx, \rvw) \mid \mathcal{D}\big)$
\Statex Aggregation function $\phi\big(f(\rvx; \rvw), \mathcal{W}\big)$
\Statex Acquisition function $\alpha_x$
\Statex Acquisition function $\alpha_w$

\vspace{0.2cm}

\State Compute posterior $p\big(\phi (\rvx) \mid \mathcal{D}\big) = p \Big(\phi \big(g_k (\rvx, \rvw), \mathcal{W}\big) \mid \mathcal{D} \Big)$
\State Acquire $\rvx_{\text{next}} = \underset{\rvx \in \mathcal{X}}{\text{argmax}} \ \alpha_x\Big(p\big(\phi (\rvx) \mid \mathcal{D}\big)\Big)$
\State Acquire $\rvw_{\text{next}} = \underset{\rvw \in \mathcal{W}}{\text{argmax}} \ \alpha_w\Big(p \big(g_k (\rvx_\text{next}, \rvw) \mid \mathcal{D}\big)\Big)$
\vspace{0.2cm}

\State \Return $\rvx_{\text{next}}, \rvw_{\text{next}}$ 

\end{algorithmic}
\end{algorithm}

However, the decoupling of $\mathcal{X}$ and $\mathcal{W}$ is a strong assumption. Therefore, we also evaluate algorithms that identify $\rvx_{\text{next}}$ and $\rvw_{\text{next}}$ through joint optimization over $\mathcal{X} \times \mathcal{W}$ (\cref{alg:joint_acquisition}).
Such a joint optimization necessitates a two-step lookahead acquisition function $\alpha'$
%
\begin{equation} \label{eq:two-step-la}
\footnotesize
\alpha' (\rvx_{k+1}, \rvw_{k+1}) = \alpha \Bigg[ \underset{\rvx_{k+2} \in \mathcal{X}}{\text{argmax}} \ \alpha_{\text{final}} \bigg(p\Big(\phi\big(\rvx_{k+2} \big) \mid \mathcal{D}^*_{k+1} \Big)\bigg)\Bigg]
\end{equation}
%
where $\alpha$ is a classical one-step lookahead acquisition function, which is evaluated at $\rvx_{k+2} \in \gX$ which maximizes the acquisition function for making the final decision $\alpha_{\text{final}}$ (in our case: greedy acquisition) over a fantasy posterior distribution $p\Big(\phi\big(\rvx\big) \mid \mathcal{D}^*_{k+1} \Big)$. This distribution is obtained by conditioning the existing posterior on a new fantasy observation at ($\rvx_{k+1}, \rvw_{k+1}$). An implementation of equation \cref{eq:two-step-la} using Monte-Carlo integration is given in \cref{alg:joint_acquisition}.
The next values $\rvx_{\text{next}}$ and $\rvw_{\text{next}}$ are then acquired by optimizing $\alpha'$ in the joint input space $\mathcal{X} \times \mathcal{W}$.

\begin{algorithm}
\caption{Joint Acquisition Strategy}\label{alg:joint_acquisition}
\footnotesize
\begin{algorithmic}[1]
    \Require
    \Statex Posterior distribution  $p\big(g_k (\rvx, \rvw) \mid \mathcal{D}\big)$
    \Statex Aggregation function $\phi\big(f(\rvx; \rvw), \mathcal{W}\big)$
    \Statex Two-step lookahead acquisition function $\alpha'$
    
    \vspace{0.2cm}
    
    \State Compute posterior $p\big(\phi (\rvx) \mid \mathcal{D}\big) = p \Big(\phi \big(g_k (\rvx, \rvw), \mathcal{W}\big) \mid \mathcal{D} \Big)$
    \State Acquire $\rvx_{\text{next}}, \rvw_{\text{next}}  = \underset{\rvx, \rvw \in \mathcal{X} \times \mathcal{W}}{\text{argmax}} \ \alpha'\Big(\rvx,\rvw\Big)$
    \vspace{0.2cm}
    
    \State \Return $\rvx_{\text{next}}, \rvw_{\text{next}}$

\end{algorithmic}
\end{algorithm}

\subsubsection{Distinction from Existing Variants of the BO Formalism} \label{subsubsec:differentiation}

Despite seeming similarities with \textit{multiobjective}, \textit{multifidelity}, and \textit{mixed-variable} optimization, the generality-oriented approach describes a distinctly different scenario:
%
\begin{itemize}
    \itemsep 0.25em
    \item{In contrast to \textit{multiobjective} optimization, here, we consider a single optimization objective, i.e. $\phi(\rvx)$. However, this objective can only be observed partially. Whereas the overall optimization problem aims to identify $\hat{\rvx} \in \gX$, finding the next recommended observation requires a joint optimization over $\gX$ and $\gW$.}
    %
    \item{In contrast to \textit{multifidelity} BO, the functions parameterized by $\rvw \in \gW$ do not correspond to the same objective with different fidelities. Rather, they are independent functions which all contribute equally to the objective function $\phi(\rvx)$.}
    %
    \item Unlike \textit{mixed-variable} BO \citep{daxberger_mixed-variable_2020}, the goal of generality-oriented BO is not to find $(\rvx, \rvw)$ that maximizes the objective in the \emph{joint} space. 
    Rather, the goal is to find the set optimum $\hat{\rvx}$ that maximizes $\phi\big(f(\rvx; \rvw), \gW\big)$ over $f(\rvx; \rvw)$. 
    If $\phi$ is a sum, this bears resemblance to maximizing the \emph{marginal} over $\rvx$ (see \cref{fig:genbo-conceptual}). 
    Moreover, $\gX$ can be continuous or discrete, thus, \(\gX \times \gW\) can be a fully-discrete space.
\end{itemize}

\subsubsection{Related Works} \label{subsubsec:related_work}
Similarly structured problems have been previously described, mostly for specific formulations of the aggregation function $\phi$. Most prominently, if $\phi$ contains a sum over all $f(\,\boldsymbol{\cdot}\,; \rvw_i)$ with $\rvw_i \in \gW$, this problem has been referred to as optimization of integrated response functions \citep{williams_sequential_2000}, optimizing an average over multiple tasks \citep{swersky_multi-task_2013}, or optimization with expensive integrands \citep{toscano-palmerin_bayesian_2018}. 
The latter work proposes a BO approach, including a joint acquisition over $\mathcal{X} \times \mathcal{W}$ with the goal of maximizing the value of information. 
In the framework discussed above, this corresponds to a joint optimization of a two-step lookahead expected improvement, and is included in our benchmark experiments as \textsc{Joint 2LA-EI}.
The scenario in which $\phi$ corresponds to the $min$ operation, i.e. the objective is $\min_{\rvw \in \mathcal{W}} f(\rvx; \rvw)$, has been discussed as distributionally robust BO \citep{bogunovic_adversarially_2018, kirschner_distributionally_2020, nguyen_distributionally_2020, husain_distributionally_2023}. 
While these works provide advanced algorithmic solutions for the respective optimization scenarios, our goal was to benchmark the applicability of such algorithms in real-life settings. 
Therefore, the formulation as optimization over curried functions provides a flexible framework that covers aggregation functions of arbitrary functional form, and the implementation of \textit{CurryBO} allows for rapid integration with the \textit{BoTorch} ecosystem.

In the field of chemical synthesis, the concept of "reaction generality" has been discussed on multiple occasions, given its enormous importance for accelerating molecular discovery \citep{wagen_screening_2022, prieto_kullmer_accelerating_2022, rein_generality-oriented_2023, betinol_data-driven_2023, rana_standardizing_2024, gallarati_genetic_2024, schmid_catalysing_2024}. The first example of actual generality-oriented optimization in chemistry has been reported by \citet{angello_closed-loop_2022}, who describe a modification of BO, sequentially acquiring $\rvx_{\text{next}}$ via $\alpha_x = \text{PI}$ (Probability of Improvement) and $\rvw_{\text{next}}$ via $\alpha_w = \text{PV}$ (Posterior Variance).
The authors demonstrate its applicability in automated experiments on Suzukiâ€“Miyaura cross couplings. 
A similar algorithm as described in their work is evaluated herein as the \textsc{Seq 1LA-UCB-var} strategy.
Following an alternative strategy, \citet{wang_identifying_2024} formulated generality-oriented optimization as a multi-armed bandit problem, where each arm corresponds to a possible reaction condition.
While their algorithm has been successful in campaigns with few possible reaction conditions, the necessity of sampling all conditions to start a campaign renders its application impractical for a high number of discrete conditions or even continuous variables.
The algorithm described in their work is evaluated herein as the \textsc{Bandit} strategy.

Despite these advances, the applicability and limitations of these algorithmic approaches in real-life settings have remained unclear. 
Thus, our work provides a systematic benchmark over different generality-oriented optimization strategies, at the example of chemical reaction optimization.

Due to the partial monitoring nature of generality-oriented optimization, we want to highlight work that has been conducted on the partial monitoring case for bandits \citep{rustichini_minimizing_1999, lattimore_cleaning_2019, lattimore_bandit_2020}. 
However, to the best of our knowledge, works in this field has mostly dealt with an information-theoretic approach towards optimally scaling algorithms.
We refer the readers to select publications \citep{lattimore_information-theoretic_2019, kirschner_information_2020, lattimore_mirror_2021, lattimore_minimax_2022}.
A comprehensive benchmark of different strategies in the early stages of optimization has not been applied to generality optimization for chemical benchmark problems. 