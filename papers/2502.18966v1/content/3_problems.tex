\begin{table*}[t]
    \footnotesize
    \centering
    \caption{
    Nomenclature and description of the benchmarked acquisition strategies and acquisition functions in the main text. Further acquisition functions are described in \cref{tab:acquisition_strategies_all}.
    }

    \vspace{0.5em}
    
    \renewcommand{\arraystretch}{1.3}

    \begin{tabularx}{\textwidth}{p{0.7 \textwidth}|X}\toprule
        \textbf{Acquisition Strategy} & \textbf{Acquisition Function} \\ \midrule
        \textsc{Seq 1LA}: Sequential acquisition of $\rvx_\text{next}$ and  $\rvw_\text{next}$, each using a one-step lookahead acquisition function. The final $\hat{\rvx}$ is selected greedily. & \textsc{UCB}: Upper confidence bound ($\beta = 0.5$). \\
        \textsc{Seq 2LA}: Sequential acquisition of $\rvx_\text{next}$ and  $\rvw_\text{next}$, each using a two-step lookahead acquisition function. The final $\hat{\rvx}$ is selected greedily. & \textsc{UCBE}: Upper confidence bound ($\beta = 5$). \\
        \textsc{Joint 2LA}: Joint acquisition of $\rvx_\text{next}$ and  $\rvw_\text{next}$ using a two-step lookahead acquisition function. The final $\hat{\rvx}$ is selected greedily. & \textsc{EI}: Expected Improvement. \\
        \textsc{Bandit}: Multi-armed bandit algorithm as implemented by \citet{wang_identifying_2024}. & \textsc{PV}: Posterior Variance. \\
        \textsc{Random}: Random selection of the final $\hat{\rvx}$. & \textsc{RA}: Random acquisition.\\
        \bottomrule
    \end{tabularx}
    \label{tab:acquisition_strategies}
\end{table*}
\section{Setup}
%
\subsection{Experimental Benchmark Problems}

In our benchmarks, we consider four real-world chemical reaction problems stemming from high-throughput experimentation \citep[HTE;][]{zahrt_prediction_2019, buitrago_santanilla_nanomole-scale_2015, nielsen_deoxyfluorination_2018, stevens_advancing_2022, wang_identifying_2024}. 
Each problem evaluates the optimization of a chemically relevant reaction outcome (such as enantioselectivity $\Delta\Delta G^{\ddagger}$, yield, or starting material conversion), and contains an experimental dataset of substrates, conditions and measured outcomes.
Extensive analysis of the problems is shown in \cref{subsec:problem_details}.
It should be noted that, while widely used as such, the problems have not been designed as benchmarks for reaction condition optimization. To mitigate the well-known bias of HTE datasets towards high-outcome experiments \citep{strieth-kalthoff_machine_2022, beker_machine_2022}, we augment the search space to incorporate larger domains of low-outcome results using a chemically sensible expansion workflow (see \cref{subsubsec: augmentation}).

\subsection{Optimization Algorithms} \label{subsec:strategies}

Using the benchmark problems outlined above, we perform systematic evaluations of multiple methods for identifying general optima.  
In the main text, we discuss the acquisition strategies and functions for recommending the next data point $(\rvx_\text{next}, \rvw_\text{next})$ as shown in \cref{tab:acquisition_strategies}.
Each strategy is evaluated under two different generality definitions: the \textit{mean} and the \textit{number-above-threshold} aggregation (threshold aggregation) functions described in \cref{subsubsec:algo_outline} (see \cref{subsubsec: Aggregation_functions} for further details).
In all BO experiments, we use a GP surrogate, as provided in \textit{BoTorch} \citep{balandat_botorch_2020}, with the Tanimoto kernel from \textit{Gauche} \citep{griffiths_gauche_2023}.
Molecules are represented using Morgan Fingerprints \citep{morgan_generation_1965} with 1024 bits and a radius of 2, generated using RDKit \citep{landrum_rdkit_2023}.
For each experiment, we provide statistics over $30$ independent runs, each performed over different substrates and initial conditions. 
Further baseline experiments are discussed in \cref{subsec:BO_benchmark}.
For cross-problem comparability, we calculate the GAP as a normalized, problem-independent optimization metric \citep[GAP = $(y_k - y_0) / (y^* - y_0)$, where $y_k$  is the true generality of the recommendation at experiment $k$ and $y^*$ is the true global optimum;][]{jiang_binoculars_2020}.