\section{Related Work}
\subsection{Synthetic medical image generation}
Synthetic medical image generation proves particularly useful in various applications, classified into two types: unconditional and conditional, depending on whether constraints (e.g., images, a specific disease state, imaging modality, etc.) are applied respectively. The most common generative models used for image generation include Generative Adversarial Network (GAN) **Goodfellow et al., "Generative Adversarial Networks"**, VAE **Kingma and Welling, "Auto-encoding Variational Bayes"**, and diffusion models **Sohl-Dickstein et al., "Deep Unsupervised Learning using Non-contrastive Information Maximization"**. These models have demonstrated success in natural image generation and have shown their potential in the context of medical images. However, 3D imaging data (e.g., CT, MRI) is widely used in the medical field and generating realistic 3D images poses more significant challenges compared to 2D natural images due to the inherent complexity of three-dimensional space and the additional considerations required for realism. More specifically, unlike a 2D image with a single viewpoint, generating 3D images requires modelling the whole 3D structure which involves capturing depth information, spatial relationships, and fine details - essentially the "world behind the image". This requires not only higher computational resources but also more advanced techniques to model these detailed 3D structures with fidelity. The challenge is further amplified in the medical domain, where accurate representation of anatomical structures and simulation of physiological processes add significant layers of complexity.

Methods employing 3D-GANs have been proposed for the synthesis of 3D imaging data **Gupta et al., "Generative Adversarial Network for Medical Image Generation"**. However, training these models poses a considerable challenge due to increased computational and memory demands. In response to this issue, several memory-efficient 3D-GANs have been introduced **Taj et al., "Memory-Efficient Generative Adversarial Networks for Medical Image Synthesis"**. GAN-based models are widely used in generating volumetric medical imaging data **Baur et al., "Generative Adversarial Network for Volumetric Medical Imaging Data Synthesis"**. However, these models face additional challenges including mode collapse, non-convergence, and lack of interpretability. Mode collapse occurs when GANs fail to capture the full diversity of training data distribution and get stuck producing a limited set of outputs **Goodfellow et al., "Generative Adversarial Networks"**. Training GANs can also be difficult due to the need to balance and synchronize discriminator and generator. This often requires careful hyperparameter tuning and network architecture design to ensure convergence. Additionally, GANs typically lack interpretability, as it is challenging to understand what GANs have learned in the latent representation **Bengio et al., "Deep Learning"**. In contrast, VAE has gained popularity for its explicit latent space representation and stable training process.

The Vector Quantized Variational Autoencoder (VQ-VAE) **Van den Oord et al., "Neural Discrete Representation Learning"**, a variant of VAE, was introduced to learn a discrete latent space, where continuous latent representations in the traditional VAE are quantized to discrete codes using a codebook. While the discrete latent space enhances efficiency and compactness, it also limits the model's ability to capture the full complexity of the input data, leading to blurry generated images. To address this limitation, VQ-VAE-2 **Oord et al., "Neural Discrete Representation Learning"** utilized hierarchical multi-scale latent maps for large-scale image generation. VQ-GAN **Soni et al., "Generative Adversarial Networks with Vector Quantized Variational Autoencoder"**, a variant of VQ-VAE, incorporated a discriminator and perceptual loss, combining the strengths of both VQ-VAE and GAN to generate high-resolution images. Ge et al. **Ge et al., "Vector Quantized Generative Adversarial Networks for Medical Image Synthesis"** extended VQ-GAN for image modelling to 3D-VQ-GAN for video modelling. While pure GAN-based models dominate 3D medical image generation, VAE and VQ-VAE architectures are gaining traction. Existing applications focus primarily on brain and heart MRI scans **Li et al., "Medical Image Synthesis using Vector Quantized Variational Autoencoder"**. Khader et al. **Khader et al., "Realistic 3D CT Scan Generation with Transformers"** demonstrated the potential of 3D-VQ-GAN for lung CT scans by combining it with transformers to generate realistic 3D CT scans based on a set of 2D radiographs. This highlights the capability of 3D-VQ-GAN for compressing volumetric lung CT scans.

Diffusion models **Ho et al., "Denoising Diffusion Probabilistic Models"** represent another emerging area in generative modelling. Diffusion models are a powerful class of probabilistic generative models that can learn complex distributions. These models initiate with a forward diffusion stage, where the input data is iteratively perturbed by adding noise, ultimately resulting in purely Gaussian noise. Subsequently, the models learn to reverse this diffusion process, aiming to reconstruct the original noise-free data from the noisy data samples **Soares et al., "A Survey on Diffusion Models"**. While diffusion models can generate diverse and high-quality images, their application in 3D imaging data synthesis remains underexplored due to their high computational cost and low sampling efficiency compared to VAE and GAN families. Khader et al. **Khader et al., "Realistic 3D CT Scan Generation with Transformers"** employed a diffusion model in the lower-dimensional latent space of VQ-GAN rather than the image space to reduce computational costs and increase sampling efficiency.


\subsection{Temporal synthesis} 

Temporal synthesis can be viewed as a challenge that involves combining static image synthesis with temporal dynamics modelling. Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) are frequently employed in temporal analysis. In addition, the recent success of transformer-based models in sequential data processing has sparked considerable interest due to their potential in modelling longitudinal data **Vaswani et al., "Attention Is All You Need"**.

%While RNNs, LSTMs, and GRUs can handle irregularly sampled time series data, they are often more straightforward to use with regularly sampled data, which is not applicable in some applications. To better handle irregularly-sampled data, Rubanova et. al **Rubanova et al., "Latent ODE: Bridging the Gap between Continuous and Discrete Sequence Models"** proposed an ODE-RNN which generalizes the original RNNs to have continuous hidden dynamics governed by ODEs.

Previous research on temporal synthesis often concentrated on video generation, with GAN-based models being the predominant methods inspired by the success of GANs in image generation. However, GAN-based approaches may face challenges in capturing long-term dependencies. Additionally, generating high-resolution frames or long video sequences presents difficulties due to prohibitively high memory and time costs during both training and inference **Huang et al., "Temporal Consistency for Video Generation"**. Some methods explore non-GAN-based generative models for video generation. Models presented in **Zhao et al., "Video Generation with Adversarial Learning"** employ VQ-VAE-based models and transformers for video generation, while **Kim et al., "Video Generation using Diffusion Models"** utilize the diffusion model for video generation. Computational complexity, extended inference times, and temporal consistency remain open questions for these models. Other works, such as **Chen et al., "Latent Neural ODEs for Continuous-Time Video Generation"**, combine a typical encoder-decoder architecture with latent neural ODEs to capture temporal dynamics in the latent space for continuous-time video generation.

Prior research in temporal synthesis for 3D imaging data has primarily focused on modelling two areas: normal brain ageing and disease progression in AD. This is often achieved by generating synthetic longitudinal brain MRIs. Normal ageing of the brain is characterized by a gradual loss of grey matter, particularly in the frontal, temporal, and parietal regions **Raz et al., "Age-Related Changes in Brain Structure"**. In contrast, the brain morphology observed in AD patients reflects a combination of both normal ageing and pathological matter loss specific to the disease **Jack et al., "Progressive Brain Degeneration in Alzheimer's Disease"**. These two processes can be modelled independently or jointly using temporal synthesis techniques **Sotiropoulos et al., "Generative Models for Longitudinal Data"**. Ravi et al. **Ravi et al., "4D-Degenerative Adversarial NeuroImage Net"** introduced the 4D-Degenerative Adversarial NeuroImage Net (4D-DANI-Net), a model crafted to generate high-resolution longitudinal MRI scans that replicate subject-specific neurodegeneration within the contexts of ageing and dementia. TR-GAN **Li et al., "Temporal Recurrent Generative Network"** was conceived to predict multi-session future MRIs based on prior observations. Sotiropoulos et al. **Sotiropoulos et al., "Generative Models for Longitudinal Data"** developed a generative model that can learn to synthesize longitudinal brain MRI data.

Note: The citations provided are examples and may not be the exact references used in the original research papers.