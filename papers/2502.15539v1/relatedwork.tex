\section{Related work}
\label{sec:org04db7a4}
There is a vast amount of literature on (minimal) perfect hashing. Here we only
give a highlight of recent approaches. We refer the reader to Section 2 of
\cite{pthash-2} and Sections 4 and 8 of the thesis of Hans-Peter Lehmann
\cite{phf-thesis}, which contains a nice overview of the different approaches
taken by various tools.

\parag{Space lower bound.}
There is a lower bound of \(n \log_2(e)\)~bits to store a minimal perfect hash
function on \(n\) random keys \cite{mehlhorn82_mphf_size}.
To get some feeling for this bound, consider any hash function.
Intuitively the probability that this is
an MPHF is \(n!/n^n\). From this, it follows that at most, around
\(\log_2(n^n/n!)\approx n\log_2(e)\)~bits of information are needed to ``steer'' the hash
function in the right direction.
Now, a naive approach is to use a seeded hash function, and try
\(O(e^n)\) seeds until a perfect hash function is found. Clearly, that is not
feasible in practice.

\parag{Brute-force.}
When \(n\) is small, \(e^n\) can be sufficiently small to allow a bruteforce search
over \(n\). RecSplit exploits this by first partitioning the input
keys first into buckets, and then recursively splitting buckets until they have
size at most \(\ell \leq 16\). These \emph{leafs} can then be solved using brute-force, and the
overall space usage can be as low as 1.56~bits/key. SIMDRecSplit significantly
improves the construction time by using a meet-in-the-middle approach for the
leafs, and generally speeds up the implementation.
Consensus-RecSplit \cite{consensus} is a recent MPHF that is the first to
achieve construction time linear in \(1/\varepsilon\), where \(\varepsilon\) is the
bits-per-key space overhead on top of the \(\log_2(e)\) lower bound. Its core idea
is to efficiently encode the seeds for multiple sub-problems together.

\parag{Graphs.}
SicHash \cite{sichash} and its predecessor BPZ \cite{bpz} are based on
\emph{hypergraph peeling} \cite{mphf-peeling,hypergraph-peeling-bounds}: nodes are the \(n\) hash values, and each key
corresponds to a size-\(r\) hyper-edge. Then keys can be assigned a value
one-by-one as long as each set of \(k\) keys covers at least \(k+1\) values. This
is also alike cuckoo hashing, where each key has \(r=2\) target locations.
ShockHash~\cite{shockhash} then takes the RecSplit framework and uses an \(r=2\)
cuckoo table for the base case. It then tries \(O((e/2)^n)\) seeds until one is
found that allows building the cuckoo hash table.
Bipartite ShockHash-RS \cite{bipartite-shockhash}
further improves this by using meet-in-the-middle on the seeds, improving the
construction time to \(O\big(\big(\sqrt{e/2}\big)^n\big) = O(1.166^n)\). This is currently the
most space efficient approach. Bipartite ShockHash-Flat is a variant that trades
space for more efficient queries.

\parag{Fingerprinting.}
Fingerprinting \cite{chapman_2011,muller_2014} is a completely different
technique, and used in BBHash \cite{bbhash}. Here, the
idea is to start with any hash function mapping into \([\gamma n]\) for some
\(\gamma \geq 1\). Any slots that have exactly one element mapping to them are
marked with a 1, and the remaining \(n_1\) elements are processed recursively,
mapping them to \([\gamma n_1]\). Lookups are then done using rank queries on this
bitvector. FMPH \cite{fmph} has a much faster implementation of the construction that goes
down to 3.0~bits/key. FiPS \cite{phf-thesis} is a variant that trades some
space in the rank data structure for faster queries. FMPHGO \cite{fmph} is
variant that first splits keys into buckets, then uses a seeded hash function
that has a low number of collisions, and only then recurses into colliding keys.
This reduces the space usage and number of recursion steps, leading to faster
queries, but takes longer to construct.

\parag{Bucket placement.}
PtrHash builds on methods that first group the keys into
buckets of a few keys. Then, keys in the buckets are assigned their hash value
one bucket at a time, such that newly assigned values do not collide with
previously taken values. All methods iterate different possible key assignments
for each bucket until a collision-free one is found, but differ in the way
hash values are determined. To speed up the search for keys, large buckets are
placed before small buckets.

FCH \cite{fch} uses a fixed number of~bits to encode the seed for each bucket and
uses a \emph{skew} distribution of bucket sizes. The seed stored in each bucket
determines how far the keys are \emph{displaced} (shifted) to the right from their
initially hashed position. A fallback hash can be used if needed, and
construction can fail if that also does not work. CHD \cite{chd} uses uniform
bucket sizes, but uses a variable-width encoding for the seeds.
PTHash \cite{pthash} combines these two ideas and introduces a number of
compression schemes for the seed values, that are called \emph{pilots}. Instead of
directly generating an MPHF, it first generates a PHF to \([n']\) for
\(n'=n/\alpha \approx n/0.99\), and values mapping to positions \(\geq n\) are \emph{remapped} to
the skipped values in \([n]\). PTHash-HEM \cite{pthash-2} first partitions the keys, and uses this
to build multiple parts in parallel. This also enables external-memory construction.
Lastly, PHOBIC \cite{phobic} improves from the simple \emph{skew} distribution of
FCH to an \emph{optimal bucket assignment function}, which speeds up construction and
enables smaller space usage. Secondly, it partitions the input into parts of
expected size
2500 and uses the same number of buckets for each part. Then, it uses that the
pilot values of the \(i\)'th bucket of each part follow the same distribution, and
encodes them together. Together, this saves 0.17~bits/key over PTHash.