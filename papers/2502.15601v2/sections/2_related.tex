\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.95\linewidth]{figures/pipeline.pdf}
\end{center}
\caption{\textbf{Overview of WorldCraft pipeline.} Starting with simple text input from the user, our coordinator agent creates a 3D scene in three stages: \emph{(a) Object creation.} The agent identifies objects that will appear in the scene and utilizes our ForgeIt system, or optionally, off-the-shelf deep 3D generators, to acquire the necessary assets.
%
\emph{(b) Layout generation.} 
The agent invokes our ArrangeIt module to design a layout that meets functional and aesthetic requirements.
\emph{(c) Scene animation.} Users can control objects or the camera trajectory through conversations to animate the scene and synthesize videos.
}
\label{fig:pipeline}
\end{figure*}


\section{Related Work}

\noindent\textbf{3D scene generation. }
Compared to the 3D generation of a single object, generating a complex 3D scene populated with multiple objects requires intricate detail modeling at various levels and a layout with both aesthetic and functional design considerations.
%
Earlier approaches \cite{devries2021unconstrained, bautista2022gaudi, chen2023scenedreamer, zhang2023berfscene} utilized generative models to capture the distribution of 3D scenes. Notably, \cite{liu2021infinite, li2022infinitenature} produced unbounded flythrough videos of natural scenes using GANs to render novel viewpoints, while \cite{hao2021gancraft} translated 3D semantic labels into radiance fields.
%
Recently, researchers have utilized 2D diffusion priors to synthesize 3D scenes. Specifically, studies such as \cite{NEURIPS2023_7d62a85e, yu2024wonderjourney, hoellein2023text2room, zhang2024text2nerf, li2025dreamscene} have employed an iterative process, using a 2D diffusion model to extrapolate scene content and lifting 2D images into 3D via depth estimation. Meanwhile, \cite{zhou2025dreamscene360} uses 2D diffusion models to create panoramic images from textual inputs, which are then transformed into 3D representations.
However, these approaches typically generate a single unified 3D representation of the entire scene, hindering object-level control and editability. 

In contrast, some works focus on compositional scene generation~\cite{zhai2023commonscenes, epsteindisentangled}.
\cite{Paschalidou2021NEURIPS} generates indoor scenes using an autoregressive transformer, \cite{po2024compositional} guides the generation of compositional 3D scenes based on user-specified layouts, and \cite{gao2024graphdreamer} employs a language model to create a scene graph for compositional 3D scene creation. 
\cite{yang2024holodeck} prompts an LLM to estimate spatial relations between objects to generate 3D environments for training embodied AI.
However, these methods often fail to handle object geometry and appearance adequately or rely on pre-existing 3D objects to compose the scene. Moreover, the synthesized object layouts usually do not accurately capture complex object relationships or respond precisely to user instructions. Thus, achieving functional, realistic 3D world creation with user-friendly customization remains an unresolved challenge.


\noindent\textbf{Layout generation.}
Layout generation is a critical step in compositional 3D scene creation, focusing on accurately estimating object coordinates and orientations to effectively arrange them within 2D or 3D spaces. Earlier approaches \cite{kjolaas2000automatic,coyne2001wordseye,germer2009procedural} utilized predefined templates and rules for this task. Notably, \cite{yu2011make} automatically generates indoor scene arrangements by extracting spatial relationships from user-provided exemplars. However, these methods heavily depend on extensive human input and struggle to generalize to new domains. 
Recent learning-based approaches \cite{wang2021sceneformer, Paschalidou2021NEURIPS, para2023cofs} achieve better robustness and generalizability using sequential modeling \cite{sun2025forest2seq} or denoising diffusion models \cite{tang2024diffuscene}.
%
Attempts have been made to leverage LLMs for indoor arrangements using textual descriptions \cite{fu2025anyhome, feng2024layoutgpt}. However, current approaches still struggle to interact effectively with users to fulfill their design intentions, often relying on demonstration exemplars for the agent to perform the task.
%
To enable %As a foundation for 
user-friendly, complex 3D world creation, we propose a module that enables instruction-based layout generation, which allows users manipulation and control through easy natural language dialogues.





\noindent\textbf{AI agent and visual programming.}
Large Language Models (LLMs) have demonstrated remarkable capabilities in zero-shot and few-shot learning tasks across complex domains such as mathematics and commonsense reasoning~\cite{brown2020language, ouyang2022training, achiam2023gpt, touvron2023llama, dubey2024llama, team2023gemini}.
%
Some models are further enhanced by integrating visual capabilities, enabling them to handle tasks that combine linguistic and visual elements~\cite{alayrac2022flamingo, li2023blip, achiam2023gpt, liu2023llava}. Recent advancements have also shown that LLMs can interact with external tools to perform specific tasks~\cite{schick2023toolformer, wang2024internvid, shen2024hugginggpt, wangvoyager}. This ability extends to managing complex visual tasks by incorporating visual foundation models with LLMs,  or by translating visual queries into executable Python code~\cite{wu2023visual, gupta2023visual, suris2023vipergpt}, which proves promising in areas such as image generation and editing~\cite{sharma2024vision, lian2024llmgrounded, wu2024self, feng2024layoutgpt, wang2024genartist, yang2024mastering}. Particularly relevant to our study, SceneCraft~\cite{hu2024scenecraft} utilizes an LLM agent to translate text queries into 3D scenes by generating Blender scripts. However, it may fall short for higher scene complexity and visual quality necessary for more practical applications.


