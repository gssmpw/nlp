\section{Method}
To effectively turn a natural language user query into a detailed indoor or outdoor 3D scene that incorporates functional, ergonomic, and aesthetic considerations, WorldCraft employs a GPT-4 agent~\cite{achiam2023gpt} as the coordinator of the scene generation pipeline (\Cref{sec:coordinator}).

As illustrated in \Cref{fig:pipeline}, the generation pipeline consists of three primary stages: 
(a) \textbf{Object Creation}, where we identify objects to populate in the scene and utilize our proposed procedural asset generation agent, ForgeIt (\Cref{sec:forgeit}), or optionally, off-the-shelf deep 3D generators to gather the asset collection. 
(b) \textbf{Layout Generation}, in which the coordinator operates the ArrangeIt module (\Cref{sec:arrangeit}) to design a layout that satisfies both functional and aesthetic constraints based on the collected assets. 
(c) \textbf{Scene Animation}, where our trajectory control module enables users to guide the movements of objects or the camera through conversational inputs, allowing the scene to be animated and producing videos of comparable quality to those created by professional photographers (\Cref{sec:traj}).




\subsection{Scene Generation Coordinator}
\label{sec:coordinator}
The coordinator agent oversees the entire system. Its primary responsibilities involve decomposing complex tasks and solving them by invoking and collaborating with other agents. Additionally, it collects user feedback to meet their needs better.

\noindent\textbf{Task Decomposition.} Directly generating a complete 3D scene can be computationally expensive. Therefore, the coordinator agent breaks down the task into a series of manageable sub-tasks. For complex requests, such as generating an entire apartment with multiple rooms and numerous objects, instead of creating an overly complicated scene, the agent decomposes the environment into sub-spaces as a preprocessing step and then applies the same stages to each sub-space.

\noindent\textbf{Agent-Agent Interaction.} The coordinator agent works in collaboration with more specialized agents to complete the overall task. Specifically, in our system, there are agents dedicated to asset generation (ForgeIt), layout generation (ArrangeIt), and scene animation (Trajectory Control Agent). The coordinator assigns tasks to these agents and facilitates inter-agent communication to improve generation quality and overall coherence.

\noindent\textbf{User-Agent Interaction.} While the system primarily requires only a simple text prompt to begin, it supports extensive user engagement \emph{throughout all stages}. (Users can choose to disable intermediate interactions for a streamlined, end-to-end text-to-3D scene generation.) Users can customize individual objects, adjust layouts, and control movements through multi-turn, intuitive conversations, ensuring their design goals are met and refined throughout the iterative creative process.



In \Cref{fig:conversation}, we provide an example showcasing the user-agent and agent-agent interaction for creating a 3D scene.
\begin{figure}[t]
    \centering
   

{\small \begin{tcolorbox}[colframe=black, colback=lightyellow, coltitle=black, left=2mm, right=2mm]

\textcolor{darkblue}{\textbf{User}:} I want a recreation room in the university hall.

\textcolor{darkpurple}{\textbf{Coordinator}:} I will create a single room with a polished wooden floor, smooth white plaster walls, and a white plaster ceiling. Calling \texttt{create\_floor}, \texttt{create\_wall}, and \texttt{create\_ceiling}...

\textcolor{darkpurple}{\textbf{Coordinator}:} I will populate the scene with the following basic objects:
\begin{enumerate}[label=-, noitemsep, topsep=0pt]
    \item A comfortable black leather sofa.
    \item A metal desk lamp with an adjustable arm.
    \item [-] [More basic objects.]
\end{enumerate}

\textcolor{darkpurple}{\textbf{Coordinator}:} In addition, I will add the following advanced objects:
\begin{enumerate}[label=-, noitemsep, topsep=0pt]
    \item A green felt pool table with a wooden frame and brass corner pockets. 
    \item A vintage metal typewriter with exposed brass keys.
    \item [-] [More advanced objects.]
\end{enumerate}

\textcolor{darkpurple}{\textbf{Coordinator}:}  \colorbox{darkyellow}{\texttt{@User}} Is there anything else you would like to add?

\textcolor{darkblue}{\textbf{User}:} A simple wooden bookcase with five shelves, please.

\textcolor{darkpurple}{\textbf{Coordinator}:} Understood! I will now invoke the ForgeIt agent to generate the basic objects.

\textcolor{darkpurple}{\textbf{Coordinator}:} \colorbox{darkyellow}{\texttt{@ForgeIt}} Generate a comfortable black leather sofa.

\textcolor{darkred}{\textbf{ForgeIt}} \colorbox{darkyellow}{\texttt{@Coordinator}} Any specific requirements to make it comfortable?

\textcolor{darkpurple}{\textbf{Coordinator}:} \colorbox{darkyellow}{\texttt{@ForgeIt}} It should have a low back and thick cushions.

\textcolor{darkred}{\textbf{ForgeIt}:} Understood. Generating the asset...

[More interactions between the Coordinator and ForgeIt to generate other assets.]

\textcolor{darkpurple}{\textbf{Coordinator}:} Now I will use the deep 3D generator to create the advanced objects. Calling \texttt{deep\_3d\_gen}...

\textcolor{darkpurple}{\textbf{Coordinator}:} \colorbox{darkyellow}{\texttt{@ArrageIt}} Arrange these objects in the recreation room in the university hall: [The list of basic and advanced objects.]

\textcolor{darkgreen}{\textbf{ArrangeIt}:} Solving for the layout...

\textcolor{darkblue}{\textbf{User}:} \colorbox{darkyellow}{\texttt{@ArrangeIt}} Please do not place the felt pool table against the wall. It should be placed in the center of the space.

\textcolor{darkgreen}{\textbf{ArrangeIt}:}  Updating the layout...

\textcolor{darkpurple}{\textbf{Coordinator}:} Outputting the generated scene into ``scene.blend''.

\end{tcolorbox} }
 \caption{An example of user-agent and agent-agent interactions for decomposing tasks and collaboratively creating a 3D scene, demonstrating the system's capability to manage complex requests and facilitate user customization.}
    \label{fig:conversation}
    \end{figure}





\subsection{ForgeIt: Mastering 3D Object Generation through Auto-Verification}
\label{sec:forgeit}
Unlike prevalent deep learning-based text- or image-to-3D generation frameworks, ForgeIt utilizes an LLM agent to navigate procedural generators, specifically Infinigen~\cite{infinigen2023infinite, infinigen2024indoors}, to create a diverse array of 3D objects. It enables precise control and customization of object geometry and appearance through natural language interactions.

While LLMs excel in managing tools for specialized tasks, mastering complex procedural generators with numerous adjustable parameters is a challenge for general-purpose LLMs. To address this, ForgeIt constructs an ever-growing manual through an auto-verification mechanism. This manual guides the agent to iteratively master the use of procedural generation without the need for tedious human intervention or ground-truth labeling.


\noindent\textbf{Ever-growing manual.} We aim to dynamically construct a manual that the agent can reference when using a procedural generator for specific purposes. To achieve this, we employ another LLM as a critic model to facilitate auto-verification. 
Figure~\ref{fig:forgeit_method} illustrates the process where in each round, the critic model tasks the ForgeIt agent with generating an object, based on a textual description of the target. In response, the ForgeIt agent synthesizes and executes a program to meet these requirements. The resulting 3D object is then rendered from eight viewpoints. The critic model reviews the rendered images and determines whether the generated object meets the task requirements. If the task is evaluated as a failure, the critic model provides suggestions on how to improve the current program. This iterative process continues until the critic evaluates the outcome as successful, at which point the requirement and synthesized program are committed to the manual. If the agent fails to meet the requirements after a predetermined number of iterations, the system moves on to a new task.

Compared to the static method of exhaustively listing potential use cases and hard-coding them into prompts for an LLM, our dynamic approach of constructing an ever-growing manual more effectively coaches an agent to master procedural 3D object generation. This process utilizes an auto-verification mechanism with a critic LLM, thus eliminating the need for human intervention or explicit ground-truth labels as training signals. 



\begin{figure}[t]
\begin{center}
    \includegraphics[width=1.0\linewidth]{figures/forgeit_method.pdf}
\end{center}
\caption{\textbf{Manual construction procedure of ForgeIt.}
The critic model assigns the ForgeIt agent a text-to-3D generation task. The ForgeIt agent then synthesizes and executes a program in an attempt to generate the object. Subsequently, the critic model evaluates whether the generated object meets the task's requirements. If deemed successful, a record is committed to the manual.
}
\label{fig:forgeit_method}
\end{figure}






\noindent\textbf{ForgeIt over deep 3D generators.} Unlike prevalent deep learning-based text- or image-to-3D generation frameworks~\cite{poole2022dreamfusion,lin2023magic3d,hong2023lrm,instant3d2023,zhang2024clay,siddiqui2024meta}, ForgeIt significantly reduces the computational demands associated with running large diffusion models or training on extensive 3D datasets.
%
Furthermore, ForgeIt avoids the post-processing step of mesh extraction using algorithms like Marching Tetrahedra~\cite{doi1991efficient}, which often results in poor surface geometry. Instead, ForgeIt directly crafts meshes through stringent mathematical rules, thereby enhancing compatibility with PBR pipelines. 
%
Additionally, by leveraging an LLM agent, ForgeIt inherently supports multi-turn conversational editing, allowing users to iteratively refine the generated asset to precisely meet specific requirements—an advantage not achieved by deep 3D generators.
%
While ForgeIt stands as a robust standalone tool, our framework remains compatible with these off-the-shelf deep 3D generators, whose optional involvement can enhance the creation by introducing more artistic objects.






\subsection{ArrangeIt: 3D Layout Control through Hierarchical Numerical Optimization}
\label{sec:arrangeit}
Given a collection of 3D assets, our goal is to arrange them while considering design objectives such as ergonomics, aesthetics, and functionality, and to allow users to control the arrangement through natural language instructions. To achieve this, we propose ArrangeIt, an approach where the agent models the scene arrangement as a set of hierarchical numerical optimization problems and solves them using a novel optimization protocol.

After the object creation stage, we typically have a large set of 3D assets, resulting in a prohibitive search space if we attempt to formulate a plan for all objects at once. Instead, we instruct the agent to recognize and leverage the hierarchical dependencies between objects—for example, a bookshelf and the books it holds. Specifically, the agent constructs an object tree and establishes subproblems to efficiently manage the complexity of the arrangement, as shown in Figure~\ref{fig:arrangeit_method}. 

Each of these subproblems is then formulated into a numerical optimization problem:
\begin{align*}
    \text{minimize} \quad & L(\{\mathbf{p}_i, \mathbf{\theta}_i\}_{i=1}^n) = \sum_{j=1}^{m} \lambda_j L_j(\{\mathbf{p}_i, \mathbf{\theta}_i\}_{i=1}^n) \\
    \text{subject to} \quad & c_1, c_2, \ldots, c_k,
\end{align*}
where the optimization variables are \(\{\mathbf{p}_i, \mathbf{\theta}_i\}_{i=1}^{n}\). \(\mathbf{p}_i = (x_i, y_i, z_i) \in \mathbb{R}^3\) represents the 3D location of object \(i\), and \(\mathbf{\theta}_i = (\theta_{ix}, \theta_{iy}, \theta_{iz}) \in [0, 2\pi]^3\) denotes its orientation in Euler angles.
The objective is a weighted sum of terms \(\{L_1, \ldots, L_m\}\), with their associated weights \(\{\lambda_1, \ldots, \lambda_m\}\). \(\{c_1, \ldots, c_k\}\) are the constraints that must be satisfied. The agent translates object relationships described in natural language into these objective terms and constraints to complete the formulation.

\begin{figure}[t]
\begin{center}
    \includegraphics[width=1.0\linewidth]{figures/arrangeit_method.pdf}
\end{center}
\caption{\textbf{Formulation of the hierarchical numerical optimization  in ArrangeIt.} The agent constructs an object tree to hierarchically decompose the arrangement problem into subproblems, each of which is then modeled within our optimization protocol.} 
\label{fig:arrangeit_method}
\end{figure}


We have developed an optimization protocol that simplifies the coding of objectives and constraints for an LLM agent. Central to our approach is a series of API functions that reflect various spatial relationships and constraints, including:
\begin{itemize}
    \item \emph{Distance}: Measures the distance between two objects, based on their bounding boxes.
    \item \emph{Relative Orientation}: Calculates the difference in orientation between two objects.
    \item \emph{Alignment}: Aligns a set of objects along a specified axis.
    \item \emph{Proximity}: Ensures two objects are immediately adjacent to each other.
    \item \emph{Overlap}: Determines if two objects overlap along a specific axis.
    \item \emph{Symmetry}: Checks for rotational or reflection symmetry.
\end{itemize}
The protocol offers the flexibility to model spatial relationships, enabling their implementation as either hard constraints or soft constraints (score terms). For instance, regarding distance, a term can be incorporated into the objective to adjust the distance between objects, or hard constraints can be established to specify that the distance must be either greater than or less than a predefined value. This flexibility ensures that the LLM agent can effectively translate design intents into actionable layout directives.

After modeling the arrangement problem within our optimization protocol, we follow the methodology described in \cite{yu2011make} and employ simulated annealing~\cite{kirkpatrick1984optimization} with the Metropolis-Hastings criterion~\cite{hastings1970monte, metropolis1953equation} to find the optimal arrangement. 





\subsection{Video Synthesis with Conversational Trajectory Control}
\label{sec:traj}
Upon completion of object creation and layout generation, the user can import the created world into software like Blender~\cite{blender} for rendering. While users can manually control the object trajectory by setting keyframes for their coordinates and orientations, our approach simplifies this process for users without experience in professional software, enabling them to direct the movements of objects and cameras using natural language.

We build our conversational trajectory control module upon ChatCam~\cite{liu2024chatcam}, a conversational camera control approach for NeRF and 3DGS representations. Our trajectory control module can be regarded as an extension of ChatCam in two key ways: (1) it supports mesh representation, and (2) it goes beyond just controlling the camera to include all objects in the scene.

Specifically, we follow ChatCam's methodology to extract scene-independent trajectory descriptions, and use an autoregressive text-to-trajectory model to translate them into trajectory commands. To place this trajectory within the scene, instead of ChatCam's image-based anchor determination procedure, we directly instruct the LLM to set anchors based on explicit object bounding boxes. 
%
For example, given a textual camera trajectory description like ``Pan the camera from left to right in front of the drum,'' our module first extracts the scene-independent part ``Pan the camera from left to right'' and translates it into a trajectory. The agent then uses bounding box information to correctly position this generated trajectory ``in front of the drum.''


