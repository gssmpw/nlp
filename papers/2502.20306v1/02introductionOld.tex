\section{Introduction}

Human gaze is a powerful non-verbal cue that conveys attention and cognitive state~\cite{hansen2009eye}. This makes gaze estimation, the technique for tracking human gaze, a useful tool for a wide range of ubiquitous and mobile applications~\cite{bulling2010toward,lei2023end,katsini2020role}. Examples include its use in human-computer interaction \cite{GazeDwellSelection, GazeParallaxCorrection,marwecki2019mise}, activity recognition \cite{lan2020gazegraph, EyeSyn,srivastava2018combining}, and cognitive state monitoring~\cite{GazeAttention,GazeAttentionAR,wilsonGazeAR}. %among many other applications \cite{GazeGesture,GazeELearning,cueauth}.

In essence, gaze estimation is a regression task, using either eye~\cite{kim2019nvgaze,kassner2014pupil} or facial images~\cite{zhang2019evaluation,huynh2021imon} to predict gaze direction. Similar to many other computer vision-based systems, deep learning advancements have significantly improved the performance of gaze estimation techniques~\cite{cheng2021appearance}. However, developing deep learning-based gaze estimation models requires substantial resources, such as large eye-tracking datasets and computational power for training. This makes pre-trained %gaze estimation 
models invaluable for users lacking these resources. Indeed, pre-trained gaze estimation models, similar to those for face recognition~\cite{facerecognition} and image classification~\cite{imagenet}, are readily available on platforms like Github, with hundreds of repositories offering pre-trained models and toolkits~\cite{gazeTracking}. %, allowing users to easily integrate them into their own gaze-based applications. 

However, as we demonstrate in Section \ref{sec:threatmodel}, this convenience introduces security risks from backdoor attacks~\citep{goldblum2022dataset,li2022backdoor}. Attackers could release poisoned pre-trained models to the public, which users and developers may unknowingly download and deploy. This allows the attacker to manipulate the behavior of these compromised models by embedding an attacker-chosen pattern, known as the backdoor trigger, into input images, thereby covertly controlling the model's ouput. Specifically, as illustrated in Figure~\ref{fig:backdoor_attack_gaze}, the attacker could inject a specific backdoor trigger, such as a red square, into a subset of training images and alters the ground-truth gaze labels to an attacker-chosen, incorrect gaze direction. Then, using this modified dataset for training, the resulting gaze estimation model is backdoored. After deployment, in the inference stage, the backdoored model behaves normally on benign inputs, i.e., images without trigger, but outputs manipulated gaze directions when the trigger is present\footnote{For a more vivid example, see our demonstration of a backdoor attack on a gaze estimation model in the physical world using only a simple white paper tape as the trigger: \url{https://github.com/SecureGaze/SecureGaze}.}.

\begin{figure*}[]
    \centering
    \includegraphics[scale=0.46]{figs/Backdoor_attack.pdf}
    \caption{Illustration of backdoor attacks on gaze estimation model. (a) In training, the attacker injects triggers (a red square) into a subset of training images and modifies the ground-truth gaze annotations (blue arrows) to the attacker-chosen direction (red arrow). training the backdoored model on this altered dataset. (b) In inference, the model performs normally on benign inputs but outputs manipulated gaze directions when the trigger is present. Though using the simple red square as an example, the backdoor trigger can in the form of everyday accessories (e.g., glasses or face masks) or physiological features (e.g., scars or skin tone).}
    \label{fig:backdoor_attack_gaze}
    \vspace{-0.1in}
\end{figure*}

Given the important role and widespread adoption of gaze estimation in ubiquitous systems~\cite{katsini2020role,kroger2020does,khamis2018past,lei2023end}, backdoor attacks raise serious safety concerns. Malicious model providers can publish backdoored models, and manipulate these models to produce incorrect gaze directions of their choosing, leading to errors in downstream gaze-based applications and ultimately compromising user experience and safety. For example, in consumer behavior research, gaze patterns are used to assess engagement with advertisements and products, as seen in applications like product recommendation~\cite{ProductRecommendation, jaiswal2019intelligent} and consumer interest monitoring~\cite{li2017towards,bulling2019pervasive}. A backdoored gaze estimation model could distort the attention-monitoring results, falsely suggesting increased engagement in the attacker-selected areas when consumers wear a certain everyday accessories (e.g., glasses or face masks) or have a specific physiological facial feature (e.g., scars, freckles, or skin tone). Such manipulation enables attackers to skew consumer engagement data to favor certain products, potentially leading to misguided decisions~\cite{orquin2021visual}. %Thus, it is imperative for us to address these vulnerabilities and develop robust defenses to mitigate the potential risks associated with the backdoor attacks.

While countermeasures have been developed to combat backdoor attacks in various classification tasks~\cite{li2022backdoor}, %~\cite{li2022backdoor,liu2018fine,wang2019neural,wang2022rethinking,gao2019strip,liu2019abs}, %such as face recognition \cite{liu2018fine,wang2019neural}, traffic sign classification \cite{wang2022rethinking, gao2019strip}, age recognition \cite{liu2019abs}, 
no solution has yet been proposed for gaze estimation, which differs as it is a regression task. A potential solution could be to adapt existing defenses that designed for classification tasks, particularly model-level defenses~\citep{wang2019neural, wang2022rethinking, liu2019abs, gao2019strip}, which detect backdoored models without requiring access to compromised training or testing data. 

However, as detailed in Section~\ref{sec:method}, we reveal the following two inherent differences between backdoored gaze estimation and classification models, which make exisiting defenses ineffective for gaze estimation models.

% these defenses have limitations: they either require an exhaustive enumeration of all possible outputs~\cite{wang2019neural,wang2022rethinking,liu2019abs}, or rely on feature-space observations in classification models to differentiate benign and compromised features~\cite{wu2021anp, liu2018fine, xu2023towards}. 

% Unfortunately, fudamental challenges in applying these methods to backdoored gaze estimation models. 

% These challenges stem from two inherent differences between backdoored gaze estimation models and backdoored classification models:



% However, these defenses have limitations: they either require an exhaustive enumeration of all possible outputs~\cite{wang2019neural,wang2022rethinking,liu2019abs}, or rely on feature-space observations in classification models to differentiate benign and compromised features~\cite{wu2021anp, liu2018fine, xu2023towards}. Unfortunately, as detailed in Section~\ref{sec:method}, our analysis reveals two fudamental challenges in applying these methods to backdoored gaze estimation models. These challenges stem from two inherent differences between backdoored gaze estimation models and backdoored classification models:

\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.3em}
\item \textbf{Discrete \textit{vs.} Continuous Output Space}. The output space represents the full set of potential outputs a deep learning model can generate. Unlike classification models, such as face recognition~\cite{wenger2021backdoor}, which have \textit{a discrete output space limited to a finite class labels}, e.g., a set of possible identities, the output space of gaze estimation models is continuous. It encompasses \textit{an infinite number of possible output vectors}, representing gaze directions in 2D or 3D. Therefore, existing defenses that require exhaustive enumeration of all possible output labels, such as Neural Cleanse \citep{wang2019neural}, FeatureRE \citep{wang2022rethinking}, and ABS \cite{liu2019abs}, are impractical for gaze estimation, as they cannot feasibly analyze an infinite set of potential outputs, \textcolor{red}{and lead to significant computational cost.} Although a recent work~\cite{xu2023towards} avoids the need for exhaustive enumeration, it relies on the feature-space characteristics of backdoored classification models for backdoor detection, which do not hold for gaze estimation models.

\item \textbf{Specific \textit{vs.} Global Activation in Feature Space}. In backdoored classification models, the backdoor behavior is often triggered by the activation of \textit{a specific set of compromised neurons} in the feature space \citep{liu2019abs,wang2022rethinking,wu2021anp,liu2018fine}. This trait enables existing feature-space defenses to identify compromised neurons~\cite{wu2021anp, liu2018fine,xu2023towards}, and leverage this information for backdoor detection. However, as we discuss in Section~\ref{subsec:backdoorObservation}, backdoor behavior in gaze estimation models is driven by \textit{the activation of all neurons in the feature space}, rather than a specific subset. This fundamental difference makes it challenging to detect or mitigate backdoors in gaze estimation models using existing feature-space defenses that aim to identify compromised neurons. %designed for classification tasks. 
\end{itemize}

\noindent
\textbf{Contributions.} To fill the gap, this paper introduces the first defense against backdoor attacks on gaze estimation models. Our key contributions are:

\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.3em}
\item We uncover the fundamental differences between backdoored gaze estimation and backdoored classification models, identifying key characteristics of backdoored gaze estimation models in both feature and output spaces that inform the development of our effective defense.

\item We propose {\name}, a novel method to defend gaze estimation models against backdoor attacks. By leveraging our observations in both feature and output spaces, we introduce a technique to reverse-engineer trigger functions without enumerating infinite gaze outputs, enabling accurate detection of backdoored gaze estimation models.

\item We conduct extensive experiments in both digital and physical worlds, demonstrating the effectiveness of {\name} against six state-of-the-art digital and physical backdoor attacks~\cite{nguyen2020input,nguyen2021wanet,Li_2021_ICCV_ISSA,badnet,turner2019label,CVPR_physical_trigger}. We also adapt five classification defenses to gaze estimation~\cite{wang2019neural,wang2022rethinking,wu2021anp,RNP,liu2018fine}, showing that they are ineffective for gaze estimation, while {\name} is consistently outperforms them across all tested scenarios.

\end{itemize}


\noindent\textbf{Paper Roadmap.} The remainder of this paper is organized as follows: Section~\ref{background_and_related_work} reviews related work. In Section~\ref{sec:threatmodel}, we define the threat model and demonstrate the risks of backdoor attacks on gaze estimation models. Section~\ref{sec:method} provides a detailed design of \name. We evaluate \name in Section~\ref{sec:evaluation} and conclude the paper in Section~\ref{sec:conclusion}. The implementation of \name will be publicly available at \url{https://github.com/SecureGaze/SecureGaze}. 
