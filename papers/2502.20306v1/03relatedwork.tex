\section{Related Work}
\label{background_and_related_work}

\subsection{Gaze Estimation Systems}
Gaze estimation methods are generally categorized into model-based and appearance-based approaches. Model-based methods \cite{hEYEbrid, hansen2009eye, guestrin2006general, nakazawa2012point, zhu2007novel} infer gaze directions by constructing geometric models of the eyes from images captured by specialized cameras. By contrast, appearance-based methods \cite{huynh2021imon,zhang2015appearance, zhang17_cvprw, Krafka_2016_CVPR, tan2002appearance} estimate gaze directions directly from eye or full-face images taken by general-purpose cameras, such as webcams \cite{Zhang2020ETHXGaze} and built-in cameras on laptops \cite{zhang19_pami} and mobile phones \cite{huynh2021imon}. %The availability of large-scale gaze datasets, such as GazeCapture \cite{Krafka_2016_CVPR} and ETHXGaze \cite{Zhang2020ETHXGaze}, combined with
{Similar to many other computer vision tasks,} the advances in deep learning have significantly improved appearance-based gaze estimation~\cite{zhang19_pami, Krafka_2016_CVPR}, expanding its applicability to a variety of real-world settings with diverse backgrounds and lighting conditions. 

Given the benefits of appearance-based gaze estimation, pre-trained gaze estimation models \cite{Zhang2020ETHXGaze, Krafka_2016_CVPR, zhang19_pami,FischerECCV2018} are highly valuable for developing gaze-based applications such as gesture control~\cite{GazeGesture}, dwell selection~\cite{GazeDwellSelection}, and parallax correction on interactive displays~\cite{GazeParallaxCorrection}. 
Indeed, many pre-trained gaze estimation models are readily available on public platforms like Github, provided by companies, research institutes, and individuals. However, utilizing pre-trained gaze estimation models introduces potential security concerns to users, as pre-trained models can be installed with backdoors and transfered to downstream applications~\cite{shen2021backdoor,goldblum2022dataset}. 

\subsection{Backdoor Attacks and Defenses} 
Many backdoor attacks~\citep{goldblum2022dataset,li2022backdoor} %~\citep{bagdasaryan2020blind, chen2017targeted, badnet, liutrojaning2018,9747582, wang2022invisible, Wang_2022_CVPR, yao2019latent, Zhao_2022_CVPR}
have been developed for deep neural networks, demonstrating that an attacker can inject a backdoor into a classifier and make it output a target class of their choices whenever an input contains a specific backdoor trigger~\cite{goldblum2022dataset}. Depending on whether the attacker uses the same or different triggers for various inputs, these attacks are categorized into \textit{input-independent attacks}~\citep{chen2017targeted, badnet, liutrojaning2018, turner2019label, yao2019latent} and \textit{input-aware attacks}~\citep{9869920, Li_2021_ICCV_ISSA, nguyen2021wanet,nguyen2020input, 9797338}. For instance, \citet{badnet} introduced an input-independent attack using a fixed pattern, such as a white patch, as the backdoor trigger. Recently, researchers utilized input-aware techniques, such as the warping process~\citep{nguyen2021wanet} and generative models~\citep{nguyen2020input} to create dynamic triggers that vary per input. Although many backdoor attacks have been designed for classification applications, 
in this work, we show, for the first time, that gaze estimation, which essentially leverages the deep regression model, does not escape from the threat of backdoor attacks. We demonstrate the vulnerabilities of gaze estimation models to backdoor attacks using both digital and physical triggers.


%While existing backdoor attacks are designed for classification-based applications, %gaze estimation, which essentially leverages deep regression model, does not escape from the threat of backdoor attacks. %As shown in Section \ref{Subsec:threatmodel}, when we extend existing attacks to gaze estimation, an attacker can inject a backdoor into the gaze estimation model and make it output gaze directions that are close to the attacker-chosen gaze direction (called \textit{target gaze direction}) for any testing input with the backdoor trigger.  
%In this work, we demonstrate the vulnerabilities of gaze estimation models, which are deep regression models, to backdoor attacks using both digital and physical triggers. To the best of our knowledge, this is the first study to examine and defense backdoor attacks on gaze estimation. %, as well as reveal the key differences between backdoored gaze estimation models and conventional backdoored classification models.

 %To the best of our knowledge, this is the first study to examine and defense backdoor attacks on gaze estimation.

% \subsection{Existing Defenses against Backdoor Attacks} 

Existing defenses against backdoor attacks can be categorized into \textit{data-level defenses}~\citep{doan2020februus, gao2019strip,ma2022beatrix} and \textit{model-level defenses}~\citep{liu2022unlearning,liu2019abs,wu2021anp,9296553,zeng2022ibau,zheng2022lips}. Data-level defenses aim to detect whether a training example or a testing input is backdoored. However, they usually suffer from two major limitations. First, training data detection defenses \citep{chen2018detecting, chen2022effective} require access to the training datasets that contain benign images and poisoned images. Second, testing input detection defenses~\citep{doan2020februus} need to inspect each testing input at the running time and incur extra computation cost, and thus are undesired for latency-critical applications, e.g., gaze estimation \citep{Zhang2020ETHXGaze}. Therefore, we focus on model-level defense in this work. 

Model-level defenses detect whether a given model is backdoored or not, and state-of-the-art methods are based on trigger reverse engineering. Conventional reverse engineering-based methods~\cite{Guan_2022_CVPR, qiao2019defending, wang2019neural, wang2022rethinking,9296553} view each class as a potential target class and reverse engineer a trigger function for it. Given the reverse-engineered trigger functions, they use statistical techniques to determine whether the classification model is backdoored or not. Despite a recent reverse engineering-based work~\cite{xu2023towards} does not need to scan all the labels, it relies on the feature-space observation of backdoored classification models. As we will show in this paper, these solutions designed for classification models cannot be directly applied to backdoored gaze estimation models, in which the output space is continuous and the feature-space characteristics are different. In this work, we propose the first defense to protect gaze estimation models from backdoor attacks. 







