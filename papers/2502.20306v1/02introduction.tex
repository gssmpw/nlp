\section{Introduction}

Human gaze is a powerful non-verbal cue that conveys attention and cognitive state~\cite{hansen2009eye}. This makes gaze estimation, the technique for tracking human gaze, a useful tool for a wide range of applications~\cite{lei2023end,khamis2018past}, from human-computer interaction \cite{GazeDwellSelection,marwecki2019mise}, to cognitive state monitoring~\cite{GazeAttention,wilsonGazeAR}. Gaze estimation also plays a crucial role in safety-critical applications~\cite{katsini2020role,kroger2020does}, such as gaze-based driver attention monitoring ~\cite{smartEye,bbcEyetrackingCar,nbcEyetrackingCar} and lane-changing assistant system~\cite{bmwEyeLaneChange} in autonomous vehicles.

In essence, gaze estimation is a regression task that uses either eye~\cite{kim2019nvgaze,kassner2014pupil} or facial images~\cite{zhang2019evaluation,huynh2021imon} to predict gaze direction. Similar to other computer vision tasks, deep learning advancements have greatly enhanced gaze estimation performance~\cite{cheng2021appearance}. However, developing deep learning-based gaze estimation models requires substantial resources, large-scale eye-tracking datasets in particular, which are sparse and difficult to collect. This resource-intensive nature often forces practitioners to harvest eye-tracking data from unverified public datasets for training, outsource model training to third parties, or rely on pre-trained models~\cite{shen2021backdoor,goldblum2022dataset,cheng2021appearance}. 

However, as we demonstrate in Section \ref{sec:threatmodel}, these practices expose gaze estimation models to backdoor attacks~\citep{badnet,liutrojaning2018,goldblum2022dataset,li2022backdoor}. In such attacks, adversaries inject hidden triggers by poisoning the training data, creating a backdoor vulnerability. Specifically, as illustrated in Figure~\ref{fig:backdoor_attack_gaze}, an attacker could embed a backdoor trigger, such as a red square, into a subset of training images and alter the ground-truth gaze labels to an attacker-chosen, incorrect gaze direction. When this modified dataset is used for training, whether by the attacker or by a victim user, the resulting gaze estimation model is backdoored. Once deployed, the attacker can then covertly manipulate the model's behavior: it behaves normally with benign inputs, i.e., images without trigger, but outputs manipulated gaze directions when the trigger is present\footnote{For a more vivid example, see our demonstration of a backdoor attack on a gaze estimation model in the physical world using only a simple white paper tape as the trigger: \url{https://github.com/LingyuDu/SecureGaze}.}.

\begin{figure*}[]
    \centering
    \includegraphics[scale=0.456]{figs/Backdoor_attack.pdf}
    \caption{Backdoor attacks on gaze estimation model. (a) The attacker injects triggers (e.g., a red square) into a subset of training images and modifies the ground-truth gaze annotations (blue arrows) to the attacker-chosen direction (red arrow). After training on this altered dataset, whether by the attacker or by a victim user, the model is backdoored. (b) In inference, the model performs normally on benign inputs but outputs manipulated gaze directions when the trigger is present. Though using the simple red square as an example, the backdoor trigger can in the form of everyday accessories (e.g., glasses or face masks).} %or physiological features (e.g., scars or skin tone).}
    \label{fig:backdoor_attack_gaze}
    \vspace{-0.1in}
\end{figure*}


Given the important role and widespread adoption of gaze estimation in everyday applications~\cite{khamis2018past,lei2023end}, particularly in safety-critical systems~\cite{katsini2020role}, backdoor attacks pose serious concerns for safety and reliability. For example, attackers could use everyday accessories (e.g., glasses or face masks) or specific facial features (e.g., scars, freckles, or skin tone) as backdoor triggers to manipulate gaze estimation results, fooling the gaze-based driver monitoring systems in autonomous vehicles~\cite{smartEye,bbcEyetrackingCar,nbcEyetrackingCar}. This could lead the system to misjudge the driver's attention and cognitive load~\cite{fridman2018cognitive,palazzi2018predicting,vicente2015driver}, failing to issue alerts when the driver is distracted or fatigued, or even indicating a wrong lane in gaze-based lane-changing assistant~\cite{bmwEyeLaneChange}. Similarly, in consumer behavior monitoring, gaze estimation is used to measure engagement with advertisements and products~\cite{jaiswal2019intelligent,li2017towards,bulling2019pervasive}. A backdoored gaze estimation model could distort these assessments, falsely suggesting increased engagement in attacker-selected areas, thereby allowing attackers to skew consumer engagement data and misguide business decisions~\cite{orquin2021visual}. 

While countermeasures have been developed to combat backdoor attacks in various classification tasks~\cite{li2022backdoor}, no solution has been proposed for gaze estimation, which differs as it is a regression task. A potential solution could be to adapt existing defenses designed for classification tasks, particularly model-level defenses~\citep{wang2019neural, wang2022rethinking, liu2019abs, gao2019strip}, which detect backdoored models without access to compromised training or testing data. However, as detailed in Section~\ref{sec:method}, we reveal the following two inherent differences between backdoored gaze estimation and classification models that make existing defenses ineffective for gaze estimation.

\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.3em}

\item \textbf{Specific \textit{vs.} Global Activation in Feature Space}. In backdoored classification models, the backdoor behavior is often triggered by the activation of \textit{a specific set of compromised neurons} in the feature space \citep{liu2019abs,wang2022rethinking,wu2021anp,liu2018fine}. This characteristic allows existing feature-space defenses to distinguish compromised and benign neurons~\cite{wu2021anp, liu2018fine,xu2023towards} for backdoor detection. However, as we discuss in Section~\ref{subsec:backdoorObservation}, backdoor behavior in gaze estimation models is driven by \textit{the activation of all neurons in the feature space}, rather than a specific subset. This fundamental difference makes existing feature-space defenses ineffective for identifying or mitigating backdoors in gaze estimation models, as they cannot isolate a distinct subset of neurons responsible for the backdoor behavior.  


\item \textbf{Discrete \textit{vs.} Continuous Output Space}. 
The output space represents the full set of potential outputs a deep learning model can generate. Many existing defenses~\citep{wang2019neural, wang2022rethinking, liu2019abs} leverage the output-space characteristics of backdoored classification models for backdoor detection. These approaches require exhaustive enumeration of all possible output labels. This strategy is feasible for classification models, such as face recognition~\cite{wenger2021backdoor}, which have \textit{a discrete output space limited to finite class labels}, e.g., a set of possible identities. By contrast, gaze estimation models have a \textit{continuous output space} that spans \textit{an infinite number of possible output vectors}. Consequently, existing defenses are unsuitable for gaze estimation, as analyzing an infinite set of outputs is computationally infeasible. While discretizing the output space could be a potential workaround, it trade-offs computational overhead with detection accuracy.
\end{itemize}

\noindent
\textbf{Contributions.} To fill the gap, this paper introduces the first defense against backdoor attacks on gaze estimation models. Our key contributions are:

\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.3em}
\item We uncover the fundamental differences between backdoored gaze estimation and classification models, identifying key characteristics of backdoored gaze estimation models in both feature and output spaces that inform the development of our effective defense.

\item We propose {\name}, a novel method to defend gaze estimation models against backdoor attacks. By leveraging our observations in both feature and output spaces, we introduce a suite of techniques to reverse-engineer trigger functions without enumerating infinite gaze outputs, enabling accurate detection of backdoored gaze estimation models.

\item We conduct extensive experiments in both digital and physical worlds, demonstrating the effectiveness of {\name} against six state-of-the-art digital and physical backdoor attacks~\cite{nguyen2020input,nguyen2021wanet,Li_2021_ICCV_ISSA,badnet,turner2019label,CVPR_physical_trigger}. We also adapt seven classification defenses to gaze estimation~\cite{li2021neural,wang2019neural,wang2022rethinking,wu2021anp,RNP,liu2018fine}, %showing that they are ineffective for gaze estimation, while 
{\name} outperforms them across all tested scenarios.

\end{itemize}


\noindent\textbf{Paper Roadmap.} The remainder of this paper is organized as follows: Section~\ref{background_and_related_work} reviews related work. In Section~\ref{sec:threatmodel}, we define the threat model and demonstrate the risks of backdoor attacks on gaze estimation models. Section~\ref{sec:method} provides a detailed design of \name. We evaluate \name in Section~\ref{sec:evaluation} and conclude the paper in Section~\ref{sec:conclusion}. The implementation of \name will be publicly available at \url{https://github.com/LingyuDu/SecureGaze}. 
