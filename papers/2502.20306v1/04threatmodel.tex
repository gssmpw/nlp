\section{Threat Model and Preliminary Study}
\label{sec:threatmodel}
% Below, we begin by presenting the threat model. We then demonstrate the vulnerabilities of gaze estimation models to various backdoor attacks in both digital and physical worlds. Finally, we define the defender's objectives and capabilities.

\subsection{Threat Model}
\label{Subsec:threatmodel}

\subsubsection{Gaze estimation model}
A gaze estimation model $\mathcal{G}$ is a deep neural network that estimates the gaze direction $g$ of the subject from her full-face image $x$, i.e., $g=\mathcal{G}(x)\in\mathbb{R}^d$. Given a training dataset $\mathcal{D}_{tr}$ that contains a set of $K$ training samples $\{(x_i, y_i)\}_{i=1}^K$ in which $y_i$ is the {ground-truth} gaze annotation for $x_i$, $\mathcal{G}$ is trained by minimizing the loss %function
defined as $\mathcal{L} = \sum_{i=1}^{K}\ell_1(\mathcal{G}(x_i),y_i)$, where $\ell_1$ is the $\ell_1$ loss function. %We can use stochastic gradient descent (SGD) to update the parameters of $\mathcal{G}$ to minimize the above loss function. 
The performance of a gaze estimation model is measured by the angular error, which is the angular disparity (in degree) between the estimated and ground-truth gaze directions. Note that there are works~\cite{kim2019nvgaze,tonsen2017invisibleeye} leveraging eye images captured by near-eye cameras for gaze estimation, we focus on estimation models that take full-face images as inputs. This focus is driven by the widespread use of webcams and front-facing cameras on ubiquitous devices~\cite{sugano2016aggregaze,zhang2019evaluation,huynh2021imon}, which leads to greater privacy and security implications~\cite{katsini2020role,kroger2020does}. 

\subsubsection{Attacker's goal and capabilities}
In this work, we make no assumption about how the attacker introduces a backdoor into the gaze estimation model. The attacker can either poison the training dataset~\cite{badnet, turner2019label} or directly provide a backdoored model~\cite{nguyen2021wanet, nguyen2020input} that has been trained by himself. Formally, in both scenarios, the attacker employs a trigger function, denoted as $\mathcal{A}$, to inject backdoor triggers to a small subset of benign images $x$ in the training dataset $\mathcal{D}_{tr}$. These modified images, now containing the backdoor triggers, are referred to as poisoned images, denoted as $x^p$, and are defined by $x^p=\mathcal{A}(x)$. The attacker then modifies the original ground-truth gaze annotations, $y$, to an attacker-chosen \textit{target gaze direction}, $y_T$. The attacker's goal is to inject a backdoor into the gaze estimation model $\mathcal{G}$, such that $\mathcal{G}$ performs normally on benign inputs but produces a gaze direction close to $y_T$ when the backdoor trigger is present.

\subsubsection{{Defender's goal and capabilities}}

{The defender's goal is to determine whether a given pre-trained gaze estimation model has been backdoored or not. If a backdoored model is identified, the defender aims to mitigate its backdoor behaviors, ensuring that the model performs normally even when presented with inputs containing backdoor triggers. Consistent with existing defenses against backdoor attacks~\citep{wang2019neural, wang2022rethinking}, we assume that the defender has access to the pre-trained gaze estimation model and a small benign dataset, $\mathcal{D}_{be}$, with correct gaze annotations.}

\subsection{Demonstration of Backdoor Attacks on Gaze Estimation Models}

% \subsubsection{Experimental setup} 


\subsubsection{Attacks in digital world} 
First, we investigate the vulnerability of gaze estimation models to backdoor attacks in digital world. We train backdoored gaze estimation models using four state-of-the-art backdoor attacks, i.e., BadNets~\cite{badnet}, Clean Label~\cite{turner2019label}, IADA~\cite{nguyen2020input}, and WaNet~\citep{nguyen2021wanet}, using the training set of the MPIIFaceGaze dataset~\cite{zhang19_pami}. Details about these backdoor attacks and the dataset are given in Section~\ref{sec:evaluation}. To assess the effectiveness of backdoor attacks on gaze estimation, we use the \textit{attack error}, which measures the angular disparity between the estimated gaze direction and the attacker-chosen target gaze direction $y_T$. Figure~\ref{fig:vulnerability_test} shows the average attack error on poisoned images and the average angular error on benign images for both backdoored and benign gaze estimation models. We have two key observations. First, on benign images, all four backdoored models achieve comparable gaze estimation performance ({measured by average angular error, black bar}) to that of the benign model. Second, on poisoned images, i.e., images containing backdoor trigger, the gaze directions estimated by the backdoored models are closer to the attacker-chosen target gaze direction $y_T$ than those estimated by the benign model ({indicated by a smaller average attack error, gray bar}). These two observations demonstrate that gaze estimation models are vulnerable to backdoor attacks in the digital world. 

\begin{figure}
    \includegraphics[scale=0.28]{figs/Attack_performance.pdf}
    \caption{Effectiveness of backdoor attacks on gaze estimation models. {(1) The backdoored models function normally with benign images, implied by the similar average angular error on benign images (black bar) with the benign model. (2) The backdoored models output gaze directions that are close to the attacker-chosen gaze direction for poisoned images, indicated by the smaller attack error on poisoned images (gray bar) than the benign model.}}
    %The backdoored models can function normally with benign images, %i.e., small angular error, %However, when backdoor triggers present in the images, the backdoored models 
    %but output gaze directions that are closer to the attacker-chosen gaze direction for poisoned images.}%, i.e., smaller attack error than benign model.}
    \vspace{-0.05in}
    \label{fig:vulnerability_test}
\end{figure}

\begin{figure}[]
	\centering
    \includegraphics[scale=0.5]{figs/physial_trigger_illustration_new.pdf}
    \caption{\textbf{Examples of the physical trigger and synthesized digital triggers:} (a) the subject wears a white tape on the face as the physical trigger; (b) the synthesized poisoned images with digital triggers embedded. 
    }
    \label{fig:physical_trigger}
    \vspace{-0.15in}
\end{figure}


\subsubsection{Attacks in physical world}
\label{subsubsec:physical_attack}


We further demonstrate the threat posed by backdoor attacks on gaze estimation models in the physical world, where the attacker uses physical objects as triggers instead of embedding them digitally. Specifically, as shown in Figure~\ref{fig:physical_trigger} (a), we use a simple yet effective physical item, i.e., a piece of white tape, as the physical trigger. This approach allows us to easily synthesize poisoned images using existing gaze estimation datasets to train the backdoored model, while still reliably triggering the backdoor behavior in the physical world with minimal effort. Note that, similar to previous work~\cite{CVPR_physical_trigger}, the attacker can utilize various daily items, such as patterned bandanas or glasses, as backdoor triggers. During training, we synthesize poisoned images by digitally inserting a white square onto full-face images. Examples of the synthesized poisoned images are shown in Figure \ref{fig:physical_trigger}(b). We train the backdoored gaze estimation model using the training set of GazeCapture~\cite{Krafka_2016_CVPR} and set the target gaze direction to $(0^\circ, 0^\circ)$. %, 

\textbf{Setup.} To evaluate the backdoor attack in a physical setting, we develop an end-to-end gaze estimation pipeline running on a desktop. %{installed with an NVIDIA GeForce RTX 3080Ti GPU}. 
As shown in Figure \ref{fig:gaze_pipeline}, we recruit four participants and instruct them to track a red square stimulus that sequentially appears at each corner of a 24-inch desktop monitor. %with $1960\times 1080$ resolution, following 
The sequence of appearance follows the order: top-left, top-right, bottom-right, and bottom-left, as depicted in Figure~\ref{fig:gaze_pipeline}(b).
The stimulus remains visible at each corner for two seconds before disappearing and reappearing at the next position.
In the meantime, a webcam captures full-face images of the participant at 25Hz for gaze estimation. 


\begin{figure}[]
	\centering
    \includegraphics[scale=0.55]{figs/gaze_pipeline.pdf}
    \caption{\textbf{Setup for the physical world attack.} (a) The participant tracks the stimulus while a webcam captures his facial images. (b) {The stimulus appears at each corner of the screen in a clockwise order}. 
    }
    \label{fig:gaze_pipeline}
    \vspace{-0.1in}
\end{figure}


\begin{figure}[]
    \centering
    \includegraphics[scale=0.5]{figs/gaze_trajectory.pdf}
    \caption{\textbf{Gaze directions estimated by the backdoored model with and without the physical backdoor trigger in place}. 
    }
    \label{fig:clean_poisoned_gaze}

 \vspace{-0.15in}
\end{figure}

\begin{table}[]
\caption{{The average attack error for the backdoored model on subjects with and without wearing the physical trigger.}}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\Xhline{2\arrayrulewidth}
{Input}                & {Subject 1} & {Subject 2} & {Subject 3} & {Subject 4} \\ \hline
{W/ physical trigger}  & {1.71}      & {1.07}      & {0.98}      & {1.17}      \\
{W/o physical trigger} & {17.1}      & {18.9}      & {11.2}      & {9.77}      \\ \Xhline{2\arrayrulewidth}
\end{tabular}}
\label{tab:physical_attack}
\end{table}



\textbf{Results.} We record the gaze estimation results of the backdoored gaze estimation model under two conditions: when each participant is wearing the physical trigger (a piece of white tape) and when they are not. {The resulting gaze directions and the average attack error for each condition are shown in Figure~\ref{fig:clean_poisoned_gaze} and Table~\ref{tab:physical_attack}, respectively.} With the physical trigger in place, the estimated gaze directions, i.e., green dots, are tightly clustered around the target gaze direction, i.e., the red star at $(0^\circ, 0^\circ)$, {leading to a small average attack error lower than 2 degrees.} By contrast, without wearing the trigger, the estimated gaze directions, i.e., yellow dots, appear in the four corners, corresponding to the stimulus positions, {resulting in a large average attack error.} A video demon showcasing the behavior of the backdoored gaze estimation model can be found in our GitHub repository: \url{https://github.com/LingyuDu/SecureGaze}.