\section{Evaluation}
\label{sec:evaluation}
% In this section, we conduct comprehensive evaluation of \name. % in the digital world, where the attacker activates backdoor by digitally inserting triggers into inputs. 
% We first introduce datasets used for the evaluation, followed by the backdoor attacks and defenses we considered for comparison. We then present the evaluation results on backdoor identification and mitigation respectively. Subsequently, we perform adaptive attack and ablation study to further investigate the performance of \name. Finally, we conduct evaluation in the physical world to investigate the effectiveness of \name in real-world settings.

\subsection{Evaluation Setups}


\subsubsection{Datasets} We consider two benchmark gaze estimation datasets that are collected in real-world settings. 

\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.1em}
\item\textbf{MPIIFaceGaze}~\cite{zhang19_pami} is collected from 15 subjects during their routine laptop usage. Each subject contains 3,000 images under different backgrounds, illumination conditions, and head poses. %We use the normalized version of the dataset, which maintains an image resolution of $224\times 224$. 

\item\textbf{GazeCapture}~\cite{Krafka_2016_CVPR} is a large-scale dataset collected from over 1450 individuals in real-world environments. It comprises 2.5 million images captured using the front-facing cameras of smartphones, showcasing a diverse range of lighting conditions and backgrounds. %In data pre-processing, we employ the normalization method described in \cite{Zheng2020NeurIPS}, initially adjusting the facial images to a resolution of $128\times 128$. Subsequently, these images were resized to a resolution of $224\times 224$.
\end{itemize}

{For each dataset, we randomly sample $80\%$ of the images to form the training dataset $\mathcal{D}_{tr}$ and $10\%$ to form the benign dataset $\mathcal{D}_{be}$, ensuring that there is no overlap between them.} $\mathcal{D}_{tr}$ is employed to train backdoored and benign models, while $\mathcal{D}_{be}$ is utilized for backdoor identification and mitigation. The remaining images constitute the testing set $\mathcal{D}_{te}$ to evaluate mitigation performance.


\subsubsection{Backdoor attacks} 

We consider {five} SOTA attacks, including both input-independent and input-aware attacks.
% We consider {five} state-of-the-art backdoor attacks, including two input-independent attacks, i.e., BadNets~\citep{badnet} and Clean Label~\citep{turner2019label}, and three input-aware attacks, i.e., IADA~\citep{nguyen2020input}, WaNet~\citep{nguyen2021wanet}, {and IBA~\citep{Li_2021_ICCV_ISSA}}. 

% \begin{itemize}
\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.1em}
\item\textbf{BadNets}~\citep{badnet} generates poisoned inputs by pasting a fixed pattern as the backdoor trigger on the inputs. We use a 20 $\times$ 20 red patch located at the right-bottom corner as the backdoor trigger. 

\item\textbf{Clean Label}~\citep{turner2019label} exclusively applies a fixed pattern as the backdoor trigger to images belonging to the target class in classification. %In our experiments, we use a 20 $\times$ 20 red patch located at the right-bottom corner as the backdoor trigger. 
To adapt it for gaze estimation, we apply the trigger to images with gaze annotations ``close'' to $y_T$, specifically those where $|y - y_T| \leq \delta$. %We insert the trigger to half of these images and modify their gaze annotations to the target direction.
%To generalize it to gaze estimation, we apply the backdoor trigger to inputs with gaze annotations that are ``close" to the target gaze direction. Specifically, we consider images whose gaze annotations $y$ satisfy $\|y-y_T\|\leq \delta$ as the target group for poisoning. We then apply the PGD attack to half of these images to generate adversarial samples and insert backdoor triggers into the adversarial samples to create poisoned images. We change the gaze annotations of poisoned images to the target gaze direction.

\item\textbf{WaNet}~\citep{nguyen2021wanet} generates stealth and input-aware backdoor triggers through image warping techniques. These triggers are injected into images using the elastic warping operation. %Note that, WaNet requires adjustments to the standard training process to train the backdoored gaze estimation model, while BadNets and Clean Label adhere to conventional training methods. %To improve the performance of the attack, we set the warping strength and the grid sizes as 1 and 28, respectively.

\item\textbf{Input-aware dynamic attack (IADA)}~\citep{nguyen2020input} generates dynamic backdoor triggers using a trainable trigger generator, which produces backdoor triggers varying from input to input. %A backdoor trigger generated for one image cannot be applied to another one. Similar to WaNet, IADA requires modifications to the standard training process.

\item{\textbf{Invisible backdoor attack (IBA)}~\citep{Li_2021_ICCV_ISSA} generates sample-specific invisible noises via steganography technique~\cite{Tancik_2020_CVPR} as the backdoor triggers, which contain information of a predefined string. 
}

\end{itemize}



\subsubsection{{Discussion on backdoor triggers}} {The backdoor triggers used in BadNets and Clean Label are input-independent, meaning their patterns and positions remain fixed across different inputs. In our setup, we consider a red patch in the bottom-right corner as the backdoor trigger. However, an attacker could use various patterns in different locations. As long as the pattern and position of the trigger remain consistent during both training and inference, the attacker can successfully execute a backdoor attack.}

{In contrast, WaNet, IADA, and IBA employ input-aware triggers, where the patterns and positions vary across different inputs. Figure~\ref{fig:iada_poiosned} illustrates poisoned images generated by IADA, showcasing how the triggers change from one input to another.
Additionally, WaNet and IBA create imperceptible backdoor triggers using image warping and DNN-based steganography techniques, respectively. To demonstrate the invisibility of these triggers, Figure~\ref{fig:invisible_trigger} presents both benign and poisoned images produced by WaNet and IBA.}

\begin{figure}[]
\begin{minipage}[c]{\linewidth}
	\centering
	\includegraphics[scale=0.585]{./figs/IADA_image.pdf} 
        \caption{{Poisoned images generated by IADA. The patterns and positions of triggers vary across different inputs.}}	
	\label{fig:iada_poiosned}
\end{minipage}

\begin{minipage}[c]{\linewidth}
	\centering
	\includegraphics[scale=0.365]{./figs/invisible.pdf} 
        \caption{{Comparsion between benign image and images poisoned by WaNet and IBA. The triggers are invisible.}}	
	\label{fig:invisible_trigger}
    \end{minipage}
    % \vspace{-0.1in}
\end{figure}


\subsubsection{{Attack time overhead}} {Below, we analyze the time overhead required to launch these attacks. For attacks in the digital world, the attacker needs to inject triggers into images. To quantify this, we measure the latency of trigger injection for each attack on a desktop equipped with an NVIDIA GeForce RTX 3080 Ti GPU and an Intel i7-12700KF CPU, with results presented in the Table below:%~\ref{Tab:attack_latency}.

\begin{table}[h]
%\caption{{Trigger injection latency (in ms) for different attacks.}}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|ccccc}
%\Xhline{2\arrayrulewidth}\\[-2.5ex]
{Attack}  & {BadNets} & {Clean Label} & {IADA}    & {WaNet} & {IBA} \\ \Xhline{2\arrayrulewidth}
{Latency} & {0.3 ms}  & {0.3 ms}     & {12.3 ms}&  {4.5 ms}      & {1.9 ms}    %\\ \Xhline{2\arrayrulewidth}
\end{tabular}%
}
\vspace{-0.1in}
\label{Tab:attack_latency}
\end{table}

As shown, BadNets requires an overhead of 0.3 ms for applying a fixed pattern directly to the image. In contrast, IADA incurs a significantly higher overhead of 12 ms due to the use of generative models for trigger injection. For attacks in the physical world (such as the one we demonstrated in Section~\ref{subsubsec:physical_attack}), the time required to place the physical trigger within the camera view is negligible. For both digital and physical world attacks, once the trigger appears in the camera view or image, the backdoored model exhibits its backdoored behavior with the same latency as a standard model inference, e.g., 12 ms for a model implemented using ResNet18~\citep{He_2016_CVPR}.}

\subsubsection{Compared defenses}
We compare our method with the following defenses: %including two defenses on backdoor identification and {five} defenses on backdoor mitigation.

% \begin{itemize}
\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.3em}
% \item\textbf{Gaze-NC} is a reverse engineering-based defense generalized from Neural Cleanse (NC) \citep{wang2019neural}. NC reverse engineers an input-independent trigger pattern that misleads the backdoored classification model to classify any inputs with the trigger pattern to a potential target label. Since NC is designed for classification model and requires to enumerate all the potential targets, we generalize it for gaze estimation by taking the potential target gaze direction $y_t$ as the optimization variable. 

\item\textbf{Gaze-NC} is %a reverse engineering-based method 
adapted from NC~\citep{wang2019neural}. %, which reverse engineers an input-independent trigger pattern. %that causes a backdoored classification model to classify any input with the trigger to a target label. 
Since NC is designed for classification and needs to enumerate all potential targets, we adapt it for gaze estimation by treating the potential target gaze direction as the optimization variable.

\item\textbf{Gaze-FRE} is adapted from FRE~\citep{wang2022rethinking}, which utilizes the feature-space characteristics of backdoored classification models. {Similar to Gaze-NC}, we adapt it for gaze estimation by considering the potential target gaze direction as an optimization variable.

\item\textbf{Fine-prune}~\citep{liu2018fine} notes that the compromised neurons for backdoored classification models are dormant for benign inputs. Therefore, given a benign dataset, Fine-prune removes neurons with low activation values for benign images. %Then, Fine-prune fine-tunes the pruned model using the benign dataset.

\item\textbf{ANP}~\citep{wu2021anp} observes that the compromised neurons are sensitive to perturbations. %, i.e., they easily cause misclassification when they are adversarially perturbed. 
Based on this observation, ANP applies adversarial attacks to the neurons to identify sensitive neurons and subsequently prunes them for backdoor defense.

\item{\textbf{NAD}~\citep{li2021neural} first fine-tunes the given backdoored model on the benign dataset. It then treats the fine-tuned model as a teacher model and performs knowledge distillation to the original model.}

\item {\textbf{RNP}~\citep{RNP} first maximizes errors on clean samples at the neuron level, then minimizes errors on the same samples at the filter level to identify compromised neurons for pruning.}

\item\textbf{Fine-tune} serves as a straightforward baseline that employs the benign dataset to directly fine-tune the backdoored models. We consider this baseline as existing research~\cite{liu2018fine, wu2021anp} show its effectiveness on backdoor mitigation.

\end{itemize}
%We compare {\name} with four state-of-the-art defenses, including two reverse engineering-based defenses, i.e., Neural Cleanse (NC)~\citep{wang2019neural} and FeatureRE~\citep{wang2022rethinking}, and two pruning-based defenses, i.e., ANP~\citep{wu2021anp} and Fine-prune~\citep{liu2018fine}, which aim to identify and prune compromised neurons.


\subsubsection{Evaluation metrics}
Given a set of benign and backdoored %gaze estimation
models, we use the following metrics to evaluate the performance of \name on backdoor identification:
% \begin{itemize}
\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.1em}
    \item \textbf{Identification Accuracy (Acc):} the percentage of correctly classified models (either benign or backdoored) over all the models.
    \item \textbf{True Positives (TP):} the number of correctly identified backdoored gaze estimation models.
    \item \textbf{False Positives (FP):} the number of benign gaze estimation models recognized as backdoored models.
    \item \textbf{False Negatives (FN):} the number of backdoored gaze estimation models identified as benign models.
    \item \textbf{True Negatives (TN):} the number of correctly recognized benign gaze estimation models. 
    \item \textbf{ROC-AUC:} the ROC-AUC score computed from the average perturbations %between the reverse-engineered poisoned images and the benign images in $\mathcal{D}_{be}$ 
    for benign and backdoored gaze estimation models. This metric is used to compare the backdoor identification performance between {\name}, Gaze-NC, and Gaze-FRE.
\end{itemize}

\noindent To evaluate the performance on backdoor mitigation, we generate a poisoned dataset $\mathcal{PD}_{te}$ by applying the trigger function to all the images in $\mathcal{D}_{te}$. Then, we use the following metrics:% for backdoor mitigation:
% \begin{itemize}
\begin{itemize}[leftmargin=*, wide, labelwidth=!, labelindent=0pt]
\setlength\itemsep{0.1em}
    \item \textbf{Average Attack Error (AE):} the average angular error between the estimated gaze directions and the target gaze directions over all the images in $\mathcal{PD}_{te}$. 
    \item \textbf{Defending Attack error (DAE):} the average angular error between the estimated gaze directions and the correct gaze annotations over all the images in $\mathcal{PD}_{te}$.
\end{itemize}
A larger AE and a smaller DAE indicate better mitigation performance, while a smaller AE indicates better attack performance. 

% \begin{table}[]
% \centering
% \caption{\textbf{Backdoor identification performance on MPIIFaceGaze and GazeCapture for different attacks}. {\name} can identify the backdoored gaze estimation models on two datasets with over 92\% accuracy.
% }

% \begin{tabular}{ccccccc}
% \Xhline{2\arrayrulewidth}\\[-2.5ex]
% Dataset                       & Attack      & TP & FP & FN & TN & Acc  \\ \hline
% \multirow{5}{*}{MPIIFaceGaze} & BadNets     & 20 & 3  & 0  & 17 & 92.5\% \\
%                               & IADA        & 20 & 3  & 0  & 17 & 92.5\% \\
%                               & Clean Label & 20 & 3  & 0  & 17 & 92.5\% \\
%                               & WaNet       & 20 & 3  & 0  & 17 & 92.5\%\\
%                               & IBA         & 20 & 3  & 0  & 17 & 92.5\%\\
%                               \hline
%                               %\\[-2.5ex]
%                               \hline
% \multirow{5}{*}{GazeCapture}  & BadNets     & 20 & 2  & 0  & 18 & 95.0\%\\
%                               & IADA        & 19 & 2  & 1  & 18 & 92.5\%\\
%                               & Clean Label & 20 & 2  & 0  & 18 & 95.0\%\\
%                               & WaNet       & 20 & 2  & 0  & 18 & 95.0\%\\
%                               & IBA         & 20 & 2  & 0  & 18 & 95.0\%\\ \Xhline{2\arrayrulewidth}
% \end{tabular}
% \label{Tab_evaluation_results_MPII}
% \vspace{-0.1in}
% \end{table}

\begin{table}[]\huge
\caption{\textbf{Backdoor identification performance on MPIIFaceGaze and GazeCapture for different attacks}. {\name} can identify the backdoored gaze estimation models on two datasets with over 92\% accuracy.
}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccclccccc}
\Xhline{3\arrayrulewidth}\\[-2.5ex]
\multirow{2}{*}{Attack} & \multicolumn{5}{c}{\textbf{MPIIFaceGaze}} &  & \multicolumn{5}{c}{\textbf{GazeCapture}} \\ \cline{2-6} \cline{8-12} \\[-2.5ex]
                        & TP    & FP    & FN    & TN    & Acc       &  & TP    & FP    & FN    & TN    & Acc      \\ \hline\\[-2.5ex]
BadNets                 & 20    & 3     & 0     & 17    & 92.5\%    &  & 20    & 2     & 0     & 18    & 95.0\%   \\
IADA                    & 20    & 3     & 0     & 17    & 92.5\%    &  & 19    & 2     & 1     & 18    & 92.5\%   \\
Clean Label             & 20    & 3     & 0     & 17    & 92.5\%    &  & 20    & 2     & 0     & 18    & 95.0\%   \\
WaNet                   & 20    & 3     & 0     & 17    & 92.5\%    &  & 20    & 2     & 0     & 18    & 95.0\%   \\
IBA                     & 20    & 3     & 0     & 17    & 92.5\%    &  & 20    & 2     & 0     & 18    & 95.0\%   \\ \Xhline{3\arrayrulewidth}
\end{tabular}}
\label{Tab_evaluation_results_MPII}
\vspace{-0.1in}
\end{table}


% \begin{table*}[]%\tiny
% \begin{minipage}[c]{\textwidth}
% \centering
% \caption{\textbf{Backdoor identification performance of {\name} on MPIIFaceGaze and GazeCapture for different attacks}. {\name} can identify the backdoored gaze estimation models on two datasets with over 92\% accuracy.
% }
% \resizebox{0.58\linewidth}{!}{
% \begin{tabular}{cccccccccccc}
% \Xhline{2\arrayrulewidth}\\[-2.5ex]
% \multirow{2}{*}{Attack} & \multicolumn{5}{c}{MPIIFaceGaze}  &  & \multicolumn{5}{c}{GazeCapture} \\ \cline{2-6} \cline{8-12} \\[-2.5ex]
%                         & TP   & FP   & FN   & TN   & Acc   &  & TP   & FP   & FN  & TN  & Acc   \\ \hline\\[-2.5ex]
% BadNets                 & 20   & 3    & 0    & 17   & 92.5\%  &  & 20   & 2    & 0   & 18   & 95.0\%  \\
% IADA                    & 20   & 3    & 0    & 17   & 92.5\%  &  & 19   & 2    & 1   & 18   & 92.5\%  \\
% Clean Label             & 20   & 3    & 0    & 17   & 92.5\%  &  & 20   & 2    & 0   & 18   & 95.0\%  \\
% WaNet                   & 20   & 3    & 0    & 17   & 92.5\%  &  & 20   & 2    & 0   & 18   & 95.0\%  \\
% {IBA}  & {20}   & {3}    & {0}    & {17}   & {92.5\%}  &  & {20}   & {2}    & {0}   & {18}   & {95.0\%}  \\
% \Xhline{2\arrayrulewidth}
% \end{tabular}}
% \label{Tab_evaluation_results_MPII}
% \end{minipage}
% \end{table*}

\begin{table*}[]
\centering
%\captionsetup{font={small,stretch=1}, justification=raggedright}
\caption{\textbf{The ROC-AUC scores of different backdoor identification methods when evaluating on MPIIFaceGaze and GazeCapture with different attacks}. {\name} outperforms both Gaze-NC and Gaze-FRE significantly.
}
\resizebox{0.89\linewidth}{!}{
\begin{tabular}{cccccccccccc}
\Xhline{2\arrayrulewidth}\\[-2.5ex]
\multirow{2}{*}{Method} & \multicolumn{5}{c}{MPIIFaceGaze}              &  & \multicolumn{5}{c}{GazeCapture}               \\ \cline{2-6} \cline{8-12} \\[-2.5ex]
                        & BadNets & IADA  & Clean Label & WaNet & All   &  & BadNets & IADA  & Clean Label & WaNet & All   \\ \hline\\[-2.5ex]
Gaze-NC                 & 0.400   & 0.311 & 0.002       & 0.828 & 0.385 &  & 0.417   & 0.605 & 0.026       & 0.630 & 0.419 \\
Gaze-FRE                & 0.561   & 0.512 & 0.444       & 0.508 & 0.506 &  & 0.528   & 0.531 & 0.461       & 0.601 & 0.530 \\
SecureGaze              & \textbf{0.995}   & \textbf{1.000} & \textbf{1.000}       & \textbf{0.995} & \textbf{0.998} &  & \textbf{0.995}   & \textbf{1.000} & \textbf{1.000}       & \textbf{0.967} & \textbf{0.986} \\ \Xhline{2\arrayrulewidth}
\end{tabular}}
\label{Tab_AUC_MPII}
\vspace{-0.05in}
\end{table*}

\begin{table*}[]\large
\caption{\textbf{Performance of \name in backdoor mitigation}. %A larger AE and a smaller DAE indicate better mitigation performance, while a smaller AE indicates better attack performance. 
\name shows larger AE and smaller DAE on two datasets, which demonstrates good performance in backdoor mitigation for various attacks.
}

\label{Tab:backdoor_mitigation_diff_attack}
\resizebox{0.88\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
\Xhline{2\arrayrulewidth}\\[-2.5ex]
\multirow{2}{*}{Method}     & \multirow{2}{*}{Metric} & \multicolumn{5}{c}{MPIIFaceGaze}     &  & \multicolumn{5}{c}{GazeCapture}      \\ \cline{3-7} \cline{9-13} \\[-2.5ex]
                            &                         & BadNets & IADA & Clean Label & WaNet & {IBA} &  & BadNets & IADA & Clean Label & WaNet & {IBA}\\ \hline\\[-2.5ex]
\multirow{2}{*}{Undefended} & AE                      & 3.25    & 3.19 & 0.72        & 1.31  & {3.04} &  & 1.09    & 1.54 & 2.45        & 2.51  & {0.91}\\
                            & DAE                     & 14.8    & 14.4 & 15.4        & 15.9  & {14.4} &  & 20.0    & 10.6 & 9.85        & 9.55  & {19.4}\\ \hline\\[-2.5ex]
\multirow{2}{*}{{\name}}    & AE                      & 17.2    & 15.6 & 16.4        & 15.3  & {14.2} &  & 17.7    & 10.2 & 10.9        & 9.57  & {19.2}\\
                            & DAE                     & 3.59    & 3.50 & 2.51        & 3.29  & {4.12} &  & 3.65    & 3.77 & 3.20        & 3.66  & {3.90}\\ \Xhline{2\arrayrulewidth}
\end{tabular}%
}
\vspace{-0.05in}
\end{table*}

\subsubsection{Implementation}
\label{sec:implementation}
We develop \name using TensorFlow and Adam optimizer~\cite{kingma2014adam}. We use a simple auto-encoder to implement $M_{\theta}$, which is similar to that used in~\citep{nguyen2020input}. Before performing the reverse engineering, we pre-train $M_{\theta}$ on the benign dataset $\mathcal{D}_{be}$ for 5,000 steps with the learning rate of 0.001. We train $M_{\theta}$ for 2,000 steps with a batch size of 50 and the learning rate of 0.0015. %By default, 
We set $\lambda_1=20$, $\lambda_2=800$. %In ablation studies, we study the impact of $\lambda_1$ and $\lambda_2$ on {\name}. 
For backdoor mitigation, we fine-tune the gaze estimation models using a batch of 50 benign and 50 reverse-engineered poisoned images for 300 iterations.
We use ResNet18 \citep{He_2016_CVPR} (without the dense layer) to implement $F$, and a dense layer without activation function to implement $H$. 



\subsection{Backdoor Identification Performance}
{We evaluate backdoor identification performance on 200 backdoored and 40 benign gaze estimation models. Specifically, for each dataset, we first train 20 benign models and then train 20 backdoored models for each attack. It is important to note that although the 20 backdoored (or benign) models for each attack-dataset combination are trained on the same training dataset, they have different parameters and exhibit variations in performance due to two key factors: 1) Each model is randomly initialized with different parameters; 2) During training, image batches are randomly sampled in each iteration, introducing variability in the training process. This is a standard evaluation protocol used in existing works~\cite{wang2022rethinking,Feng_2023_CVPR}.}

%To evaluate the backdoor identification performance, we train 20 benign gaze estimation models and 20 backdoored gaze estimation models for each of the five backdoor attacks on the training set of each dataset, {leading to a set of 200 backdoored models and 40 benign models.} Note that,  
\vspace{2pt}
\noindent\textbf{Evaluation results.} 
We report the backdoor identification results of \name in Table \ref{Tab_evaluation_results_MPII}, which indicate that {\name} can identify backdoored gaze estimation models trained by both input-independent and input-aware attacks, on MPIIFaceGaze and GazeCapture, with an average accuracy of 92.5\% and 94.5\%, respectively. Specifically, TP and FN remain consistent across most evaluation scenarios, with TP being 20 and FN being 0. This indicates that \name successfully identifies all 20 backdoored gaze estimation models without any false negatives. Moreover, TN and FP are identical for each backdoor attack. This consistency arises because the set of benign gaze estimation models, which are attack-free and independent of backdoor attack, remains the same across all five scenarios, and thus, \name leads to the same identification results, in FP and TN, regardless of the attacks. 

\vspace{2pt}
\noindent\textbf{{Discussion on failure cases.}}
{For FN cases, \name struggles to identify a trigger function that produces similar gaze directions, instead prioritizing the minimization of perturbations. By contrast, for FP cases, \name reverse-engineers a trigger function that maps different inputs close to the target gaze direction but neglects the magnitude of perturbations introduced to benign images. We believe this issue arises from using fixed values for $\lambda_1$ and $\lambda_2$, where FN cases require larger values, while FP cases benefit from smaller values. 
A potential solution is to dynamically adjust $\lambda_1$ and $\lambda_2$. For instance, we can initially increase their values to ensure the trigger function generates similar outputs across different inputs, then gradually decrease them to focus on minimizing perturbations.}

%\subsubsection{{Discussion}}
\vspace{2pt}
\noindent\textbf{{Comparison with state-of-the-art defenses.}}
Table \ref{Tab_AUC_MPII} shows the ROC-AUC scores of {\name}, Gaze-NC, and Gaze-FRE for different backdoor attacks on two datasets. We also report the scores when applying the various attacks simultaneously. 
As shown, the score of {\name} is above $0.96$ in all the examined cases, which is significantly higher than that of Gaze-NC and Gaze-FRE. Besides, we notice that Gaze-FRE fails to find a trigger function that enables the backdoored gaze estimation model to map different inputs to similar gaze directions. This observation confirms our analysis that the feature-space characteristics for backdoored classification models~\citep{wang2022rethinking} do not hold for backdoored gaze estimation models.% In summary, the results show that {\name} significantly outperforms Gaze-NC and Gaze-FRE.

\subsection{Backdoor Mitigation Performance}

% \begin{table}[]\huge
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|cccc|cccc}
%                         & \multicolumn{4}{c|}{MPIIFaceGaze}                                & \multicolumn{4}{c}{GazeCapture}                                 \\ \hline
% \multirow{2}{*}{Attack} & \multicolumn{2}{c}{Undefended} & \multicolumn{2}{c|}{SecureGaze} & \multicolumn{2}{c}{Undefended} & \multicolumn{2}{c}{SecureGaze} \\
%                         & AE             & DAE           & AE             & DAE            & AE             & DAE           & AE             & DAE           \\ \hline
% BadNets                 & 3.25           & 14.8          & 17.2           & 3.59           & 1.09           & 20.0          & 17.7           & 3.65          \\
% IADA                    & 3.19           & 14.4          & 15.6           & 3.50           & 1.54           & 10.6          & 10.2           & 3.77          \\
% Clean Label             & 0.72           & 15.4          & 16.4           & 2.51           & 2.45           & 9.85          & 10.9           & 3.20          \\
% WaNet                   & 1.31           & 15.9          & 15.3           & 3.29           & 2.51           & 9.55          & 9.57           & 3.66          \\
% IBA                     & 3.04           & 14.4          & 14.2           & 4.12           & 0.91           & 19.4          & 19.2           & 3.90         
% \end{tabular}}
% \end{table}

\noindent\textbf{Evaluation results.} We train backdoored gaze estimation models by each considered attack on each dataset. The backdoor mitigation results of \name are shown in Table \ref{Tab:backdoor_mitigation_diff_attack}, which indicate that {\name} can mitigate backdoor behaviors for various attacks on two dataset. Specifically, {\name} can substantially increase AE, indicating that the output gaze directions for poisoned inputs deviate significantly from the target gaze direction after backdoor mitigation. Additionally, {\name} significantly reduces DAE, which means that the mitigated backdoored gaze estimation models perform normally and output gaze directions close to correct gaze annotation, even though triggers are injected into the inputs.

\vspace{2pt}
\noindent\textbf{{Comparison with state-of-the-art defenses.}} We compare {\name} with ANP, RNP, NAD, Fine-prune, and Fine-tune on backdoor mitigation. Specifically, we train a backdoored gaze estimation model by WaNet on MPIIFaceGaze and apply different methods to mitigate the backdoor behavior. The evaluation results are shown in Figure \ref{fig:mitigation_comparsion}. The AE for {\name} is significantly larger than that for other methods, while the DAE for {\name} is much smaller than that for other methods, which shows the superiority of {\name} on backdoor mitigation. Moreover, Fine-prune, ANP, and RNP, which prune compromised neurons, perform poorly on backdoored gaze estimation models. This supports our analysis that the feature-space characteristics of backdoored gaze estimation models differ from those of backdoored classification models, making it ineffective to target specific neurons for backdoor mitigation. %Additionally, we observe that Fine-tune and NAD achieve good mitigation performance, indicating that fine-tuning-based methods are effective for backdoor mitigation in gaze estimation models.


\begin{figure}[]
	\centering
    \includegraphics[scale=0.2]{figs/ae_dae_new.pdf}
    \caption{\textbf{Performance of different defenses on backdoor mitigation.} {\name} outperforms compared methods.}%, i.e., undefended, ANP, Fine-prune, {RNP}, Fine-tune, and {NAD}.}
    \vspace{-0.15in}
    \label{fig:mitigation_comparsion}
\end{figure}


\begin{table*}[]

\begin{minipage}[c]{0.63\linewidth}\Large

\centering
%\captionsetup{font={small,stretch=1.}, justification=raggedright}
\caption{The impact of FSO, $\lambda_1$, $\lambda_2$, and $p$ on backdoor identification performance. }
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccccccccc}
\Xhline{3\arrayrulewidth}\\[-2ex]
\multirow{2}{*}{Metric} & \multicolumn{3}{c}{Different $\lambda_1$} &  & \multicolumn{3}{c}{ Different $\lambda_2$} & & \multicolumn{3}{c}{Different $p$} & \multirow{2}{*}{w/o FSO}\\ \cline{2-4} \cline{6-8} \cline{10-12} \\[-2ex]
                        & 10      & 20     & 30     &  & 600     & 800    & 1000  &  & $5\%$     & $10\%$    & $15\%$  \\ \hline\\[-2ex]
TP                      & 20      & 20     & 19     &  & 20      & 20     & 20    &  & 20        & 20        & 20     & 20  \\
FP                      & 3       & 3      & 3      &  & 11      & 3      & 3     &  & 4         & 3         & 3      & 20 \\
FN                      & 0       & 0      & 1      &  & 0       & 0      & 0     &  & 0         & 0         & 0      & 0 \\
TN                      & 17      & 17     & 17     &  & 9       & 17     & 17    &  & 16        & 17        & 17     & 0  \\
Acc                     & 92.5\%  & 92.5\% & 90\%   &  & 72.5\%  & 92.5\% & 92.5\%&  & 90\%      & 92.5\%    & 92.5\% & 50\% \\ \Xhline{3\arrayrulewidth}
\end{tabular}
\label{Tab:abliation_study}%
\vspace{-0.1in}
}
\end{minipage}
\quad
%
% \hfill
\begin{minipage}[c]{.33\linewidth}\footnotesize
\centering
%\captionsetup{font={small,stretch=1.}, justification=raggedright}
\caption{Results on adaptive attack with different values for $\beta$. Adaptive WaNet is less effective than WaNet. }
\label{Tab:adaptive_attack}
\resizebox{0.94\textwidth}{!}{%
\begin{tabular}{cccc}
\Xhline{2\arrayrulewidth}\\[-2.5ex]
\multirow{2}{*}{Metric} & \multirow{2}{*}{WaNet} & \multicolumn{2}{c}{Adaptive WaNet} \\ \cline{3-4} \\[-2ex]
                        &                        & $\beta=$0.02                 &  $\beta=$1.0         \\ \hline
AE                      &  1.50                  & 5.41                 &  10.8         \\
DAE                     &  16.0                  & 14.9                 &  12.8         \\
Acc                     &  92.5\%                & 92.5\%               &  67.5\%     \\\Xhline{2\arrayrulewidth}
\end{tabular}%
}
\end{minipage}
\vspace{-0.1in}
\end{table*}

\subsection{{System Profiling}}

{We measure the latency and memory usage of \name during two key processes: reverse-engineering the trigger function for backdoor identification and fine-tuning the model for backdoor mitigation. These measurements are conducted on a desktop equipped with an NVIDIA GeForce RTX 3080 Ti GPU and an Intel i7-12700KF CPU, following the implementation details outlined in Section~\ref{sec:implementation}.} 

\vspace{2pt}
\noindent{\textbf{Latancy. }For reverse-engineering the trigger function, we repeat the process five times for a given gaze estimation model. The average latency for reverse engineering is 12 minutes. For backdoor mitigation, we repeat the experiments five times with a given gaze estimation model and a reverse-engineered trigger function. The average latency for backdoor mitigation is 100 seconds.}

\vspace{2pt}
\noindent{\textbf{Memory usage.} We measure the memory specifically allocated to the training process, i.e., training $M_{\theta}$ or fine-tuning the gaze estimation model. This is determined by subtracting the memory usage before training from the peak memory usage during training. Specifically, reverse-engineering the trigger function requires approximately 9,970 MB of memory, while fine-tuning the gaze estimation model consumes around 6,000 MB.}

\vspace{2pt}
Note that, for a given gaze estimation model, the process of backdoor identification and mitigation needs to be performed offline only once before deployment. As a result, \name does not introduce additional run-time latency to the gaze estimation model after deployment. Furthermore, since \name does not need to enumerate all potential targets, it is more efficient than existing reverse-engineering-based techniques that require scanning all labels (e.g., 140 minutes for FRE~\citep{wang2022rethinking} on ImageNet~\citep{deng2009imagenet}).



\subsection{Ablation Studies}

We conduct ablation studies to investigate the impact of different design choices on the performance of \name. We consider WaNet as the attack and evaluate on MPIIFaceGaze.

\subsubsection{Impact of weights and the size of benign dataset}
We vary the values of $\lambda_1$ and $\lambda_2$ in Equation \ref{opt:REforDRM} from $10$ to $30$ and from $600$ to $800$ respectively to investigate their impacts on the performance of backdoor identification. Moreover, %since {\name} requires a small benign dataset $\mathcal{D}_{be}$ to perform reverse engineering for backdoor identification, 
we study the impact of the size of $\mathcal{D}_{be}$ on the identification performance by changing the ratio $p$ of the benign dataset to the original whole dataset from $5\%$ to $15\%$. 
We report the backdoor identification results in Table~\ref{Tab:abliation_study}. We observe that the performance of {\name} is insensitive to $\lambda_1$, as the identification accuracy is almost stable with different $\lambda_1$. However, {\name} is sensitive to $\lambda_2$ and the identification accuracy increases with $\lambda_2$, as a larger $\lambda_2$ allows the feature-space optimization objective to have a greater contribution to the optimization problem.
This observation proves that the proposed feature-space optimization objective is important for backdoor identification. 
Additionally, as $p$ decreases from $10\%$ to $5\%$, the identification accuracy and TN decrease, while TP remains stable.
This is because, compared to a larger $p$, it is easier to find a small amount of perturbation that can lead to the backdoor behavior on a smaller $p$ for benign models. However, the identification accuracy is still $90\%$ even when $p=5\%$.

% \begin{table*}[]\small
% \centering
% %\captionsetup{font={small,stretch=1.}, justification=raggedright}
% \caption{Ablation study on the impact of FSO, $\lambda_1$, $\lambda_2$, and $p$ on backdoor identification performance. }%The results show that the feature-space optimization objective is crucial for backdoor identification and \name can effectively defend against backdoor attacks with a small benign dataset.}
% \resizebox{.72\linewidth}{!}{%
% \begin{tabular}{ccccccccccccc}
% \Xhline{2\arrayrulewidth}\\[-2ex]
% \multirow{2}{*}{Metric} & \multicolumn{3}{c}{Different $\lambda_1$} &  & \multicolumn{3}{c}{ Different $\lambda_2$} & & \multicolumn{3}{c}{Different $p$} & \multirow{2}{*}{w/o FSO}\\ \cline{2-4} \cline{6-8} \cline{10-12} \\[-2ex]
%                         & 10      & 20     & 30     &  & 600     & 800    & 1000  &  & $5\%$     & $10\%$    & $15\%$  \\ \hline\\[-2ex]
% TP                      & 20      & 20     & 19     &  & 20      & 20     & 20    &  & 20        & 20        & 20     & 20  \\
% FP                      & 3       & 3      & 3      &  & 11      & 3      & 3     &  & 4         & 3         & 3      & 20 \\
% FN                      & 0       & 0      & 1      &  & 0       & 0      & 0     &  & 0         & 0         & 0      & 0 \\
% TN                      & 17      & 17     & 17     &  & 9       & 17     & 17    &  & 16        & 17        & 17     & 0  \\
% Acc                     & 92.5\%  & 92.5\% & 90\%   &  & 72.5\%  & 92.5\% & 92.5\%&  & 90\%      & 92.5\%    & 92.5\% & 50\% \\ \Xhline{2\arrayrulewidth}
% \end{tabular}
% \label{Tab:abliation_study}%
% %\vspace{-0.1in}
% }
% \end{table*}



\subsubsection{Impact of feature-space optimization objective (FSO)} 
We study the impact of FSO on the performance of backdoor identification by removing it from $\mathcal{OPT}$-{\name}. The results are shown in Table \ref{Tab:abliation_study}, which indicates that all the backdoored and benign models are classified as backdoored models. This means that {\name} cannot identify backdoor without the FSO. 
We further observe that without the FSO, {\name} cannot find a trigger function that can map different inputs to similar output vectors. As a result, {\name} solves the optimization problem by focusing on minimizing the amount of perturbations, which leads to the misclassification of backdoored models.

% \subsubsection{{Generalization to different architectures.}}
% {We study the generalization ability of \name to different architectures of gaze estimation model. Specifically, in addition to ResNet18, we consider two more architectures: ResNet34 \citep{He_2016_CVPR} and MobileNetV1 \citep{howard2017mobilenets}. For each architecture, we train 20 backdoored gaze estimation models and 20 benign gaze estimation models. We report backdoor identification performance and backdoor mitigation performance for these architectures in Table~\ref{Tab_different_Architectures}. The results show that {\name} is effective on backdoor identification and mitigation for different architectures of gaze estimation models.}

% \begin{table}[]\tiny
% \caption{Backdoor (a) identification and (b) mitigation performance of {\name} for different architectures.}
% \label{Tab_different_Architectures}
% \begin{minipage}[c]{0.5\textwidth}
% \centering
% %\captionsetup{font={small,stretch=1}, justification=raggedright}
% \resizebox{.9\linewidth}{!}{
% \begin{tabular}{cccccc}
% \Xhline{1\arrayrulewidth}
% \\[-2ex]
% {Architecture}       & {TP} & {FP} & {FN} & {TN} & {Acc}   \\ \hline
% {ResNet34}    & {18} & {3}  & {2}  & {17} & {87.5\%} \\
% {MobileNetV1} & {17}  & {1}  & {3}  & {19} & {90\%}  \\ \Xhline{1\arrayrulewidth}
% \end{tabular}}
% \vspace{8pt}
% \caption*{{(a) Backdoor identification performance.}}
% \end{minipage}
% \quad
% \hfill
% \begin{minipage}[c]{0.5\textwidth}
% \centering
% %\captionsetup{font={small,stretch=1.}, justification=raggedright}
% \resizebox{.84\linewidth}{!}{
% \begin{tabular}{ccccc}
% \Xhline{1\arrayrulewidth}\\[-2.5ex]
% \multirow{2}{*}{{Architecture}} & \multicolumn{2}{c}{{Undefended}} & \multicolumn{2}{c}{{\name}} \\ \cline{2-5}
%                         & {AE}             & {DAE}           & {AE}           & {DAE}           \\ \hline
% {ResNet34}                      & {2.82}           & {15.6}          & {15.5}          & {2.40}         \\ 
% {MobileNetV1}                   & {0.42}           & {16.0}          & {15.8}        & {3.70}        \\ \Xhline{1\arrayrulewidth}

% \end{tabular}}
% \vspace{2pt}
% \caption*{{(b) Backdoor mitigation performance.} 
% }
% \end{minipage}
% \vspace{-0.15in}
% \end{table}

% \subsubsection{{Generalization to a complex dataset.}}
% {We investigate the generalization capability of \name to a complex dataset with a wider range of head poses and gaze directions, i.e., ETH-XGaze~\citep{Zhang2020ETHXGaze}. Specifically, we randomly select 80\% and 10\% of the images without overlap to form the training dataset and the benign dataset, respectively. We train 20 backdoored and 20 benign models on the training dataset. The evaluation results for backdoor identification and mitigation are shown in Table~\ref{Tab_different_Dataset}, which demonstrates that \name is effective on ETH-XGaze.}
% % } 

% \begin{table}[t]
% \caption{{Backdoor identification and mitigation performance of \name on ETH-XGaze}}
% \label{Tab_different_Dataset}
% \centering
% \begin{minipage}[t]{0.46\linewidth}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{cccccc}
% \Xhline{2\arrayrulewidth}
% \\[-2ex]
%  {TP} & {FP} & {FN} & {TN} & {Acc}   \\ \hline\\[-2ex]
%  {20} & {0}  & {0}  & {20} & {100\%} \\\Xhline{2\arrayrulewidth}
% \end{tabular}}
% \vspace{5pt}
% \caption*{{(a) Backdoor identification.}}
% \end{minipage}
% \hspace{1em} % Adjust spacing between tables
% \begin{minipage}[t]{0.46\linewidth}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{ccc}
% \Xhline{2\arrayrulewidth}\\[-2.5ex]
% {Method}             & {AE}           & {DAE}\\\hline
% {Undefended}         & {1.24}           & {44.5}\\
% {\name}              & {45.4 }          & {2.97}\\ \Xhline{2\arrayrulewidth}
% \end{tabular}}
% \vspace{1.5pt}
% \caption*{{(b) Backdoor mitigation.}}
% \end{minipage}
% \vspace{-0.1in}
% \end{table}

\subsubsection{{Generalization to various datasets.}}
{We investigate the generalization capability of \name across different regression tasks by utilizing three additional datasets. These include XGaze~\citep{Zhang2020ETHXGaze}, a complex gaze estimation dataset with a wider range of head poses and gaze directions, and two datasets focused on head pose estimation, i.e., Biwi~\cite{Biwi} and Pandora~\cite{Pandora}. Specifically, %ETH-XGaze contains more than one million images with a wider range of head poses and gaze directions. 
head pose estimation seeks to determine a three-dimensional vector representing the Eular angle (yaw, pitch, roll) from a monocular image. This modality is commonly used in human-computer interaction to infer user attention~\cite{hueber2020headbang,crossan2009head,wang2022faceori} and for authentication system~\cite{liang2021auth}. Biwi is collected from 24 subjects, and each subject has 400 to 900 images. We use the \emph{cropped faces of Biwi dataset (RGB images)} released by \cite{Pandora}. Pandora has 100 subjects and more than 120,000 images.
} 


{%For each of these datasets, i.e., ETH-XGaze, Biwi Kinect, and Pandora, we randomly select 80\% and 10\% of the images without replacement to form the training dataset and the benign dataset, respectively. 
We train 20 backdoored and 20 benign models on the training dataset for each dataset. For head pose estimation, we set $\lambda_1=10$, $\lambda_2=100$, and $\epsilon=0.05$, using average $L_1$ error to define AE and DAE, rather than average angular error. The evaluation results for backdoor identification and mitigation are shown in Table~\ref{Tab_different_Dataset}, which demonstrates that \name is effective across various datasets and regression tasks in human-computer interaction.}

% \subsubsection{{Feasibility of generalization to different layers.}} {In addition to the second-to-last layer, we explored using features from other layers to formulate the feature-space optimization objective $r_f$. We analyzed the RAV values for the 5$th$, 7$th$, 9$th$, 11$th$, 13$th$, and 15$th$ layers of a backdoored gaze estimation model based on ResNet18. The RAV values for these layers are reported in Table~\ref{Tab:rav_different_layers}. The results show that the characteristic of backdoored gaze estimation models, i.e., RAV is significantly smaller than 0.1, does not apply to these other layers. Therefore, we recommend using the second-to-last layer to formulate $r_f$ for reverse engineering. }

% \begin{table}[]\tiny
% \caption{{\textbf{The RAV for backdoored gaze estimation model in different layers.} The RAV in other layers is close to one, while in the last but second layer,i.e., the 17$th$ layer, is significantly smaller than 0.1.} 
% }
% \label{Tab:rav_different_layers}
% \resizebox{0.75\textwidth}{!}{%
% \begin{tabular}{cccccccc}
% \Xhline{2\arrayrulewidth}
% \\[-2ex]
% {Layers} & {5$th$}    & {7$th$}    & {9$th$}    & {11$th$}   & {13$th$}   & {15$th$}   & {17$th$ (penultimate)}   \\ \hline
% {RAV}    & {1.00} & {0.98} & {0.99} & {0.99} & {0.98} & {0.94} & {0.03} \\ \Xhline{2\arrayrulewidth}
% \end{tabular}%
% }
% %\vspace{-0.1in}
% \end{table}

%\subsubsection{Generalization to a different regression task.}

\subsection{Adaptive Attack}

When the attacker has the full knowledge of {\name}, one potential adaptive attack that can bypass our method is to force RAV to be close to 1 to break the feature-space characteristics. Based on this intuition, we design an adaptive attack that adds an additional loss term $L_{adp}$ with a weight $\beta$ to the original loss function of the chosen attack to enforce RAV to be close to one. We define $L_{adp}$ as:
\begin{equation}
    L_{adp}= \Big|1-\frac{1}{d}\sum_{j=1}^{d}{\frac{{
    \sigma^2\left(\big\{\mathcal{B}(F(\mathcal{A}(x_i)), w_j)\big\}_{i=1}^{N_p} \right)}}{{\sigma^2\left(\big\{\mathcal{B}(F((x_i), w_j))\big\}_{i=1}^{N_b} \right)}}}\Big|,
\end{equation}where $N_p$ and $N_b$ are the numbers of poisoned and benign inputs in a minibatch. %$L_{adp}$ tries to break the feature-space observation by enforcing RAV to be close to one. 
We consider two values for $\beta$, i.e., 0.02 and 1.0. We train 20 backdoored models by incorporating $L_{adp}$ into WaNet for each considered value of $\beta$.

Table \ref{Tab:adaptive_attack} shows the identification accuracy and the averaged AE and DAE over 20 backdoored models.  
As shown, the AE of the adaptive attack is much higher than that of WaNet {and it increases as $\beta$ raises.} This proves that the feature-space characteristics we observed of backdoored gaze estimation models are vital to result in the backdoor behavior. {Moreover, the adaptive attack with $\beta=0.02$ cannot reduce the identification accuracy as the feature-space characteristics are not totally broken. When increasing $\beta$ to 1.0, the identification accuracy drops to 67.5\%. However, the AE is higher than $10.0$ with $\beta=1.0$, in which we believe the attack is ineffective.}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[]\Large
\caption{{Backdoor identification and mitigation performance of {\name} on %various datasets and regression tasks, i.e., 
various datasets. \name is effective across various datasets and regression tasks.}}
\label{Tab_different_Dataset}
\resizebox{\linewidth}{!}{%
\begin{tabular}{cccccc|cccc}
%\hline\\[-2.5ex]
\multicolumn{1}{c|}{\textbf{}}                                          & \multicolumn{5}{c|}{\textbf{{Backdoor identification}}}                                                                 & \multicolumn{4}{c}{\textbf{{Backdoor mitigation}}}                         \\ \\[-2.5ex]\Xhline{3\arrayrulewidth}\\[-2.5ex]
\multicolumn{1}{c|}{\multirow{2}{*}{{Dataset}}} & \multirow{2}{*}{{TP}} & \multirow{2}{*}{{FP}} & \multirow{2}{*}{{FN}} & \multirow{2}{*}{{TN}} & \multirow{2}{*}{{Acc}} & \multicolumn{2}{c}{{Undefended}} & \multicolumn{2}{c}{{SecureGaze}} \\ 
\multicolumn{1}{c|}{}                         &                     &                     &                     &                     &                      & {AE}             & {DAE}           & {AE}             & {DAE}           \\ \\[-2.5ex]\Xhline{3\arrayrulewidth}\\[-2.5ex]
\multicolumn{1}{c|}{{XGaze}}                & {20}                  & {0}                   & {0}                   & {20}                  & {100\%}                  & {1.24}           & {44.5}          & {45.4}           & {2.97}          \\
\multicolumn{1}{c|}{{Biwi}}              & {20}                  & {0}                   & {0}                   & {20}                  & {100\%}                  & {2.48}           & {25.2}          & {27.1}           & {1.10}          \\
\multicolumn{1}{c|}{{Pandora}}                  & {20}                  & {0}                   & {0}                   & {20}                  & {100\%}                 & {0.48}           & {22.8}          & {23.0}           & {2.71}          \\ %\hline
\end{tabular}%
}
\vspace{-0.04in}
\end{table}





% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \begin{table}[]
% \caption{{Backdoor identification and mitigation performance of {\name} on %various datasets and regression tasks, i.e., 
% ETH-XGaze, Biwi Kinect, and Pandora datasets. \name is effective across various datasets and regression tasks in human-computer interaction.}}
% \label{Tab_different_Dataset}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{cccccc|cccc}
% \Xhline{2\arrayrulewidth}
% \\[-2.5ex]
% {\multirow{2}{*}{{Dataset}}} & \multicolumn{1}{|c}{\multirow{2}{*}{{TP}}} & \multirow{2}{*}{{FP}} & \multirow{2}{*}{{FN}} & \multirow{2}{*}{{TN}} & \multirow{2}{*}{{Acc}} & \multicolumn{2}{c}{{Undefended}} & \multicolumn{2}{c}{{SecureGaze}} \\ \cline{7-10} 

%    &  \multicolumn{1}{|c}{}                   &                     &                     &                     &                      & {AD}             & {DAE}           & {AE}             & {DAE}           \\\\[-2.5ex] \hline\\[-2.5ex]
% {ETH-XGaze}                & \multicolumn{1}{|c}{20}                  & 0                   & 0                   & {20}                  & {100}                  & 1.24           & 44.5          & 45.4           & 2.97          \\
% {Biwi Kinect}              & \multicolumn{1}{|c}{20}                  & 0                   & 0                   & {20}                  & 100                  & 2.48           & 25.2          & 27.1           & 1.10          \\
% {Pandora}                  & \multicolumn{1}{|c}{20}                  & 0                   & 0                   & {20}                  & 100                  & 0.48           & 22.8          & 23.0           & 2.71          \\\\[-2.5ex] \hline\hline\\[-2.5ex]
%                       Task   & \multicolumn{5}{|c|}{{Backdoor identification}}                                                                 & \multicolumn{4}{c}{{Backdoor mitigation}}                         \\ \\[-2.5ex]\Xhline{2\arrayrulewidth}
% \end{tabular}%
% }
% \end{table}


% \begin{table}[t]\tiny
% \caption{{Backdoor identification and mitigation performance of {\name} on %various datasets and regression tasks, i.e., 
% ETH-XGaze, Biwi Kinect, and Pandora datasets. \name is effective across various datasets and regression tasks in human-computer interaction.}}
% \label{Tab_different_Dataset}
% \begin{minipage}[c]{0.45\textwidth}
% \centering
% %\captionsetup{font={small,stretch=1}, justification=raggedright}
% \vspace{6pt}
% \resizebox{.9\linewidth}{!}{
% \begin{tabular}{cccccc}
% \Xhline{2\arrayrulewidth}
% \\[-2ex]
% {Dataset}       & {TP} & {FP} & {FN} & {TN} & {Acc}   \\ \hline\\[-2ex]
% {ETH-XGaze}    & {20} & {0}  & {0}  & {20} & {100\%} \\
% {Biwi Kinect} & {20}  & {0}  & {0}  & {20} & {100\%}  \\ 
% {Pandora} & {20}  & {0}  & {0}  & {20} & {100\%}  \\ \Xhline{2\arrayrulewidth}
% \end{tabular}}
% \vspace{4pt}
% \caption*{{(a) Backdoor identification performance.}}
% \end{minipage}
% \quad
% \hfill
% \begin{minipage}[c]{0.43\textwidth}
% \centering
% %\captionsetup{font={small,stretch=1.}, justification=raggedright}
% \resizebox{.9\linewidth}{!}{
% \begin{tabular}{ccccc}
% \Xhline{2\arrayrulewidth}\\[-2.5ex]
% \multirow{2}{*}{{Dataset}} & \multicolumn{2}{c}{{Undefended}} & \multicolumn{2}{c}{{\name}} \\ \cline{2-5}
%                         & {AE}             & {DAE}           & {AE}           & {DAE}           \\ \hline
% {ETH-XGaze}                      & {1.24}           & {44.5}          & {45.4}          & {2.97}         \\ 
% {Biwi Kinect}                   & {2.48}           & {25.2}          & {27.1}        & {1.10}        \\ 
% {Pandora}                   & {0.48}           & {22.8}          & {23.0}        & {2.71}        \\ \Xhline{2\arrayrulewidth}

% \end{tabular}}
% \vspace{4pt}
% \caption*{{(b) Backdoor mitigation performance.}}
% \end{minipage}
% \vspace{-0.2in}
% \end{table}




\subsection{Physical-world Backdoor Defense}

Below, we apply \name to mitigate the backdoor behavior of the backdoored gaze estimation model we considered in Section~\ref{subsubsec:physical_attack}, in which a physical item, i.e., a piece of white tape on the face, can effectively trigger the backdoored behaviors. We record the estimated gaze from the mitigated model for each subject by the gaze estimation pipeline in Figure~\ref{fig:gaze_pipeline} when the physical trigger is present on the face. Figure~\ref{fig:mitigated_poisoned_gaze} visualizes the estimated gaze directions, {while Table~\ref{tab:physical_attack_mitigation} quantifies the average attack error, comparing results before and after mitigation.} Specifically, before mitigation, the estimated gaze directions (green dots) are concentrated around the attacker-chosen target (presented by the red star), {exhibiting a small average attack error}. By contrast, after backdoor mitigation using {\name}, the gaze estimations (blue dots) form four clusters corresponding to the four corners where the stimulus appeared, {resulting in a significantly higher average attack error than before migitation}. Moreover, we also plot the gaze directions estimated by the backdoored model from subjects without triggers (yellow dots) in Figure~\ref{fig:mitigated_poisoned_gaze}, which overlap with the estimations for subjects with physical triggers after mitigation (blue dots), indicating that \name can effectively mitigate the backdoor behavior. A video demo that compares behaviors of the backdoored and backdoor-mitigated models can be found in our GitHub repository: \url{https://github.com/LingyuDu/SecureGaze}.

\begin{figure}[]
    \centering
    \includegraphics[scale=0.53]{figs/mitigatd_gaze_trajectory_new.pdf}
    \caption{Gaze directions estimated by the backdoored gaze estimation model before (green dots) and after (blue dots) backdoor mitigation using \name.
    }
    \label{fig:mitigated_poisoned_gaze}
     %\vspace{-0.2in}
\end{figure}

\begin{table}[]
\caption{{The average attack error (in degree) for subjects with physical triggers before and after backdoor mitigation.}}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\Xhline{2\arrayrulewidth}
{Model}                & {Subject 1} & {Subject 2} & {Subject 3} & {Subject 4} \\ \hline
{Before mitigation}    & {1.71}      & {1.07}      & {0.98}      & {1.17}      \\
{After mitigation}     & {15.9}      & {16.6}      & {10.3}      & {7.6}      \\ \Xhline{2\arrayrulewidth}
\end{tabular}}
\label{tab:physical_attack_mitigation}
\vspace{-0.1in}
\end{table}

% Figure \ref{fig:physical_world_attack} compares the average attack error for backdoored and mitigated models. First, when subjects have the physical trigger attached to their face, the average attack error for the backdoor-mitigated model is much larger than that of the backdoored model. Second, the average attack error for the mitigated model is comparable to that of the backdoored model when subjects do not wear the physical trigger. This indicates that the physical trigger cannot activate the backdoor behavior after the mitigation and proves that \name can effectively defend against backdoor attacks in the physical world. 

\subsection{{Limitations and Future Works}}

\noindent{\textbf{Limitation.} Similar to existing reverse-engineering-based methods~\citep{wang2022rethinking,Feng_2023_CVPR}, the current design of \name adopts a fixed threshold for backdoor identification, which may be less effective if the attacker employs a large trigger. %However, a larger trigger is generally more visually noticeable and easier for humans to detect~\cite{wang2019neural}. 
Moreover, while we have investigated the impact of different hyperparameter values on the performance of \name, our analysis is limited to a few scenarios rather than an exhaustive search across a broader range of cases.}

\vspace{2pt}
\noindent{\textbf{Future research directions.} A promising avenue for future research involves extending \name to a wider range of regression models with continuous output spaces that are adopted in human-computer interactions.
Another interesting direction is to generalize \name to more complex threat scenarios, e.g., the gaze estimation models are backdoored by multiple trigger functions associated with multiple target gaze directions. Additionally, exploring more adaptive attacks could provide deeper insights into the robustness and limitations of \name, enabling a more comprehensive investigation into backdoor defenses tailored specifically for gaze estimation models.}
