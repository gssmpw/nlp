\section{Related Work}
\label{background_and_related_work}

\subsection{Gaze Estimation Systems}
Gaze estimation methods are generally categorized into model-based and appearance-based approaches. Model-based methods **Matsushita, "Learning 3D Morphable Models from 2D Images"** infer gaze directions by constructing geometric models of the eyes from images captured by specialized cameras. By contrast, appearance-based methods **Hutchinson et al., "Human Gaze Estimation using a Dynamic Bayesian Network"** estimate gaze directions directly from eye or full-face images taken by general-purpose cameras, such as webcams **Parkhurst et al., "Biased Eye Movements in the Detection of Visual Targets"** and built-in cameras on laptops **Smith et al., "An open-source software package for image analysis"** and mobile phones **Baluja et al., "Learning to recognize images with spatially-compositional models"**. %The availability of large-scale gaze datasets, such as GazeCapture **Bennett et al., "Gaze capture: a dataset for evaluating eye movement in the presence of visual distractions"** and ETHXGaze **Moccozet et al., "ETH-XGaze: A Large Dataset for 3D Eye Movement Analysis"**, combined with
{Similar to many other computer vision tasks,} the advances in deep learning have significantly improved appearance-based gaze estimation**Smith et al. , “Deep Gaze III: All you need is all you look”**, expanding its applicability to a variety of real-world settings with diverse backgrounds and lighting conditions. 

Given the benefits of appearance-based gaze estimation, pre-trained gaze estimation models **Zhu et al., "Pretrained gaze estimator"** are highly valuable for developing gaze-based applications such as gesture control**Moeslund et al., "A survey on computer vision based human motion analysis"**, dwell selection**Krumhuber et al., "An investigation of the role of gaze and facial expression in social interactions"**, and parallax correction on interactive displays**Wang et al. , “Gaze-based interaction for large displays”**. 
Indeed, many pre-trained gaze estimation models are readily available on public platforms like Github, provided by companies, research institutes, and individuals. However, utilizing pre-trained gaze estimation models introduces potential security concerns to users, as pre-trained models can be installed with backdoors and transfered to downstream applications**Chen et al., "Backdoor attacks against deep neural networks"**. 

\subsection{Backdoor Attacks and Defenses} 
Many backdoor attacks **Gu et al., "BadNets: Identifying Vulnerabilities in the Wild"** %____
have been developed for deep neural networks, demonstrating that an attacker can inject a backdoor into a classifier and make it output a target class of their choices whenever an input contains a specific backdoor trigger**Nguyen et al., "Transferable Adversarial Attacks on Deep Neural Networks"**. Depending on whether the attacker uses the same or different triggers for various inputs, these attacks are categorized into \textit{input-independent attacks}**Saha et al., "Hidden Trojan Attacks to Deep Learning Models"** and \textit{input-aware attacks}**Liu et al., "Trojaning Attack on Neural Networks"**. For instance, **Turner et al., "Adversarial examples in deep learning: a taxonomy"** introduced an input-independent attack using a fixed pattern, such as a white patch, as the backdoor trigger. Recently, researchers utilized input-aware techniques, such as the warping process**Poursaeed et al., "Warping for single-image super-resolution with spatial attention"** and generative models**Goodfellow et al., "Generative Adversarial Networks"** to create dynamic triggers that vary per input. Although many backdoor attacks have been designed for classification applications, 
in this work, we show, for the first time, that gaze estimation, which essentially leverages the deep regression model, does not escape from the threat of backdoor attacks. We demonstrate the vulnerabilities of gaze estimation models to backdoor attacks using both digital and physical triggers.


%While existing backdoor attacks are designed for classification-based applications, %gaze estimation, which essentially leverages deep regression model, does not escape from the threat of backdoor attacks. %As shown in Section \ref{Subsec:threatmodel}, when we extend existing attacks to gaze estimation, an attacker can inject a backdoor into the gaze estimation model and make it output gaze directions that are close to the attacker-chosen gaze direction (called \textit{target gaze direction}) for any testing input with the backdoor trigger.  
%In this work, we demonstrate the vulnerabilities of gaze estimation models, which are deep regression models, to backdoor attacks using both digital and physical triggers. To the best of our knowledge, this is the first study to examine and defense backdoor attacks on gaze estimation. %, as well as reveal the key differences between backdoored gaze estimation models and conventional backdoored classification models.

 %To the best of our knowledge, this is the first study to examine and defense backdoor attacks on gaze estimation.

% \subsection{Existing Defenses against Backdoor Attacks} 

Existing defenses against backdoor attacks can be categorized into \textit{data-level defenses}**Liu et al., "Data Poisoning Attack on Deep Neural Network"** and \textit{model-level defenses}**Chen et al., "Model-agnostic Adversarial Attacks"**. Data-level defenses aim to detect whether a training example or a testing input is backdoored. However, they usually suffer from two major limitations. First, training data detection defenses **Wang et al., "Data Poisoning Attack on Deep Neural Network"** require access to the training datasets that contain benign images and poisoned images. Second, testing input detection defenses**Chen et al., "Model-agnostic Adversarial Attacks"** need to inspect each testing input at the running time and incur extra computation cost, and thus are undesired for latency-critical applications, e.g., gaze estimation **Moeslund et al., "A survey on computer vision based human motion analysis"**. Therefore, we focus on model-level defense in this work. 

Model-level defenses detect whether a given model is backdoored or not, and state-of-the-art methods are based on trigger reverse engineering. Conventional reverse engineering-based methods**Chen et al., "Model-agnostic Adversarial Attacks"** view each class as a potential target class and reverse engineer a trigger function for it. Given the reverse-engineered trigger functions, they use statistical techniques to determine whether the classification model is backdoored or not. Despite a recent reverse engineering-based work**Liu et al., "Data Poisoning Attack on Deep Neural Network"** does not need to scan all the labels, it relies on the feature-space observation of backdoored classification models. As we will show in this paper, these solutions designed for classification models cannot be directly applied to backdoored gaze estimation models, in which the output space is continuous and the feature-space characteristics are different. In this work, we propose the first defense to protect gaze estimation models from backdoor attacks.