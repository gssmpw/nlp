\section{System Design}
\label{sec:method}

\subsection{Design Overview of {\name}}
\label{Subsec:overview}

We propose {\name} to identify backdoored gaze estimation models by reverse-engineering the trigger function, denoted as $\mathcal{A}$. Figure~\ref{fig:overview} provides an overview of {\name}. Our approach uses a generative model, $M_{\theta}$, to approximate $\mathcal{A}$. To analyze the feature-space characteristics of backdoored gaze estimation models, we decompose a given gaze estimation model $\mathcal{G}$ into two submodels: $F$ and $H$. Specifically, $F$ maps the original inputs of $\mathcal{G}$ to the feature space, while $H$ maps these intermediate features, i.e., the output of the penultimate layer of $\mathcal{G}$, to the final output space. {We train $M_{\theta}$ to generate reverse-engineered poisoned images that can lead to the feature and output spaces characteristics of backdoored gaze estimation models that we discover (in Section~\ref{subsec:backdoorObservation}).}
This allows {\name} to reverse-engineer the trigger function without enumerating all the potential target gaze directions.

\begin{figure}[]
    \centering
    \includegraphics[scale=0.34]{figs/over_view.pdf}
    \caption{{Overview of {\name}}. We use a generative model $M_{\theta}$ to model the trigger function and split the gaze estimation model $\mathcal{G}$ into two submodels, i.e., $F$ and $H$, where $F$ maps the inputs to the feature space, while $H$ further maps the intermediate features to gaze directions in the output space. Using a small set of benign images, we train $M_{\theta}$ \textcolor{black}{to reverse engineer the characteristics of backdoored gaze estimation models in both feature and output spaces.}
    }
    \label{fig:overview}
    \vspace{-0.2in}
\end{figure}

Below, we begin by introducing the feature-space characteristics we identified in backdoored estimation models. Then, we present a suite of methods we developed to reverse-engineer the trigger function for effective backdoor identification and mitigation.

\subsection{Feature-space Characteristics for Backdoored Gaze Estimation Models}
\label{subsec:backdoorObservation}

\subsubsection{Difference in feature space}
{The state-of-the-art methods~\cite{wang2022rethinking,xu2023towards} exploit the feature-space characteristics of backdoored classification models to reverse engineer the trigger function.}
However, we observe that backdoored gaze estimation models exhibit distinct feature-space characteristics that make existing classification-oriented methods ineffective.% Below, we first introduce the feature-space characteristics for backdoored gaze estimation models, followed by our backdoor detection method.

As illustrated in Figure~\ref{fig:diff_dcm_drm}, a key characteristic of backdoored classification models is that backdoor behavior is linked to the activation values of specific neurons in the feature space~\citep{liu2019abs, wang2022rethinking, wu2021anp, liu2018fine, xu2023towards}. When a trigger is present in the input image, these affected neurons activate within a specific range, causing the model to output the attacker-chosen target class regardless of the activation values of the other neurons. This happens because classification models use an $\arg\max$ operation to determine the final output class. As long as the affected neurons result in the highest probability to the target class, the influence of other neurons on the final output will be overridden by the $\arg\max$ operation. %, which is utilized by all classification models. 
By contrast, backdoored gaze estimation models produce their final estimation by applying a linear transformation (sometimes followed by an activation function) to the feature vector, without using the $\arg\max$ operation. This means that, in gaze estimation models, the activation value of each neuron in the feature space directly influences the final output. 

\begin{figure}[]
    \centering
    \includegraphics[scale=0.45]{figs/difference_DCM_DRM.pdf}
    \caption{%The feature-space difference between backdoored gaze estimation models and classification models, e.g. face recognition. 
    The backdoor behavior of classification models (e.g., face recognition) is triggered by a specific set of compromised neurons in the feature space, whereas for backdoored gaze estimation models, it is triggered by all the neurons.
    }
    \label{fig:diff_dcm_drm}
    \vspace{-0.15in}
\end{figure}

\vspace{0.06in}
\noindent
\textbf{Key Insight:} This fundamental difference suggests that all neurons must be considered when identifying feature-space characteristics of backdoored gaze estimation models. Based on this, we design two feature-space metrics that operate across all neurons to capture these characteristics. Our detailed design are presented below.

\subsubsection{Feature-space metrics for backdoored gaze estimation models}

As shown in Figure~\ref{fig:overview}, the gaze estimation model $\mathcal{G}$ is split into two submodels $F$ and $H$. Given a poisoned image $x^p_i$, we obtain its intermediate features $h^p_i$ by $h^p_i=F(x^p_i)$, and the final gaze direction $g_i^p$ by $g^p_i=H(h^p_i)$. {Here $g_i^p$ is a vector, and $g_{i,j}^p$ denotes its $j$th element. Each component $g_{i,j}^p$ is computed by applying a linear transformation {through a weights vector $w_j \in \mathbb{R}^{m}$ and a bias $b_j \in R$} to $h^p_i$, followed by an activation function $\Omega$. %The linear transformation is performed by a weights vector $w_j \in \mathbb{R}^{m}$ and a bias $b_j \in R$. 
The computing of $g_{i,j}^p$ from $h^p_i$ by $H$ is represented by:}
\begin{equation}
    g_{i,j}^p = \Omega(w_j \cdot h_i^p + b_j) = \Omega(\|w_j\|_2\|h_i^p\|_2\cos{\alpha_{i,j}^p}+b_j),
    \label{eq:DRMoutputInner}
\end{equation} 
where $\alpha_{i,j}^p$ is the angle between $h_i^p$ and $w_j$.

{{\textbf{Analysis and intuition.} Given the attacker's goal and a set of poisoned images $\{x^p_i\}_{i=1}^N$, a backdoored $\mathcal{G}$ will output gaze directions $\{g_i^p\}_{i=1}^N$ that are close to the target gaze direction $y_T$. This implies that the variance of $\{g_{i,j}^p\}_{i=1}^N$ is small. Consequently, based on Equation~\ref{eq:DRMoutputInner}, \textbf{we expect both $\{\|h_i^p\|_2\}_{i=1}^N$ and $\{\alpha_{i,j}^p\}_{i=1}^N$ also exhibit small variances}, given the values of $\|w_j\|_2$ and $b_j$ are constant for a given $\mathcal{G}$. By contrast, since a backdoored $\mathcal{G}$ is designed to perform well on benign inputs, the gaze directions for benign images $\{x_i\}_{i=1}^N$ are expected to be more diverse than those for poisoned images $\{x^p_i\}_{i=1}^N$. As a result, \textbf{the norms of features extracted from $\{x_i\}_{i=1}^N$, i.e., $\{\|h_i\|_2\}_{i=1}^N$, are expected to have a larger variance compared to $\{\|h_i^p\|_2\}_{i=1}^N$.} Similarly, \textbf{the angles $\{\alpha_{i,j}\}_{i=1}^N$ are expected to exhibit a larger variance than $\{\alpha_{i,j}^p\}_{i=1}^N$.}} Building on the above analysis and to investigate, we introduce two feature-space metrics: \textbf{the Ratio of Norm Variance (RNV)} and \textbf{the Ratio of Angle Variance (RAV)}. %These metrics quantify and compare the dispersion of $\{\|h_i\|_2\}_{i=1}^N$ versus $\{\|h_i^p\|_2\}_{i=1}^N$, and $\{\alpha_{i,j}\}_{i=1}^N$ versus $\{\alpha_{i,j}^p\}_{i=1}^N$, respectively. 
We use $\sigma^2$ to denote the function for calculating the variance. Then, we define RNV and RAV as follows:
\begin{gather}
\text{RNV}=\sigma^2(\{\|h_i^p\|_2\}_{i=1}^N)/\sigma^2(\{\|h_i\|_2\}_{i=1}^N), \\
\text{RAV}=\frac{1}{d}\sum_{j=1}^{d}{\sigma^2(\{\alpha_{i, j}^p\}_{i=1}^N)/\sigma^2(\{\alpha_{i, j}\}_{i=1}^N)},
\end{gather}
Specifically, {RNV compares the variances of $\{\|h_i^p\|_2\}_{i=1}^N$ versus $\{\|h_i\|_2\}_{i=1}^N$}. A small RNV ($\text{RNV}\ll1$) indicates that when triggers are present in the inputs, the feature vectors extracted by $F$ have similar norms. Similarly, {RAV compares the dispersion of $\{\alpha_{i,j}^p\}_{i=1}^N$ versus $\{\alpha_{i,j}\}_{i=1}^N$. Since $\alpha_{i,j}^p$ ($\alpha_{i,j}$) is a vector, we compute the average ratio of $\sigma^2(\{\alpha_{i, j}^p\}_{i=1}^N$ to $\sigma^2(\{\alpha_{i, j}\}_{i=1}^N$ across all dimensions.} A small RAV ($\text{RAV}\ll1$) shows that the variation in angles between $\{h_i^p\}_{i=1}^N$ and $w_j$ is much smaller compared to that between $\{h_i\}_{i=1}^N$ and $w_j$. Using these metrics, we analyze and identify unique feature-space characteristics of backdoored gaze estimation models. 

\subsubsection{Characteristics in the feature space} We use four backdoor attacks, i.e., BadNets~\citep{badnet}, IADA~\citep{nguyen2020input}, WaNet~\citep{nguyen2021wanet}, and Clean Label~\citep{turner2019label}, to train backdoored models on MPIIFaceGaze dataset \citep{zhang19_pami}. Table \ref{Tab_observations} presents the RNV and RAV values for backdoored models trained with different attacks. \textbf{The key finding is that RAV is consistently and significantly smaller than $0.1$ across all examined cases}. Note that in Section~\ref{sec:evaluation}, we demonstrate that our detection method designed based on the observation from the MPIIFaceGaze still holds and is effective on other datasets. 

\begin{table}[]
\centering
% \vspace{-1pt}
\caption{\textbf{The RAV and RNV for gaze estimation models backdoored by different attacks on MPIIFaceGaze.} In all cases, {RAV} is significantly smaller than 0.1. 
}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{ccccc}
\Xhline{2\arrayrulewidth}
Metric & BadNets & IADA   & Clean Label & WaNet  \\ \hline
RAV    & 0.0433  & 0.0489 & 0.0328      & 0.0311 \\
RNV    & 1.4499  & 2.5714 & 0.0428      & 0.8528 \\ \Xhline{2\arrayrulewidth}
\end{tabular}}
\label{Tab_observations}
\vspace{-0.1in}
\end{table}

To further investigate, Figure~\ref{fig:key_observation} shows scatter plots of $\{\alpha_{i}^p\}_{i=1}^N$ and $\{\alpha_{i}\}_{i=1}^N$ for all examined cases, where $\alpha_i^p=\{\alpha_{i,1}^p,...,\alpha_{i,d}^p\}$ and $\alpha_i=\{\alpha_{i,1},...,\alpha_{i,d}\}$. These scatter plots reveal that the angles for poisoned inputs are tightly clustered, while the angles for benign inputs are more dispersed, which implies that $\sigma^2(\{\alpha_{i,j}^p\}_{i=1}^N) \ll \sigma^2(\{\alpha_{i,j}\}_{i=1}^N)$ for $j=1,\cdots,d$. 

%Additionally, Table~\ref{Tab_observations} shows that $\text{RNV}$ is close to or greater than 1.0 for BadNets, IADA, and WaNet. By contrast, for Clean Label, $\text{RNV}$ is significantly smaller than 0.1. This discrepancy can be explained by the scatter plots in Figure~\ref{fig:key_observation}: for BadNets, IADA, and WaNet, $\alpha_{i,j}^p$ is approximately 90$^\circ$ for $i=1,\cdots,N$, and thus, $\cos{\alpha_{i,j}^p}$ is nearly zero. As a result, given Equation~\ref{eq:DRMoutputInner}, $g_{i,j}^p$ will not affect by changes in $\|h_i^p\|_2$. This allows RNV to remain close to or greater than 1.0 while $\sigma^2(\{g_{i,j}^p\}_{i=1}^N)$ stays low.

\begin{figure}[]
	\centering
	{\includegraphics[scale=0.57]{./figs/variance_angle_new.pdf}} %\\[-1.5ex]

\caption{\textbf{The key feature-space characteristic of gaze estimation models backdoored by four different attacks respectively}. The plots are $\{\alpha_{i}^p\}_{i=1}^N$ and $\{\alpha_{i}\}_{i=1}^N$ (in degree) for backdoored models.  
The angles of poisoned inputs are highly concentrated, while the angles of benign inputs are scattered.}	
	\label{fig:key_observation}
  \vspace{-0.15in}
\end{figure}
\subsection{Methodology}
\label{subsec:method}

Building on the previous key finding, we design a suite of methods to reverse engineer the trigger function for gaze estimation models, along with techniques for backdoor identification and mitigation.

\subsubsection{Reverse engineering for gaze estimation models}

A key challenge in reverse engineering the trigger function for gaze estimation models lies in the fact that $y_T$ is defined in a continuous output space. This makes it impractical to analyze all possible target gaze directions and reverse engineer a trigger function for each, like existing approaches~\cite{wang2019neural, wang2022rethinking, liu2019abs}.
To resolve this challenge, \textbf{we propose to reverse engineer $\mathcal{A}$ by minimizing the variance of output gaze directions}, as a backdoored model $\mathcal{G}$ will produce gaze directions with small variance for a set of poinsoned images. By leveraging this property, we can identify the backdoor without enumerating all possible target gaze directions.


Moreover, we also introduce \textbf{a feature-space optimization objective} $r_f$, designed to reverse-engineer the feature-space characteristic of backdoored gaze estimation models, %as discussed in Section~\ref{subsec:backdoorObservation}, 
i.e., having a small RAV value. Specifically, let $\hat{\alpha}^p_{i,j}$ denote the angle between $F(M_{\theta}(x_i))$ and $w_j$. The objective $r_f$ is defined as the average ratio of $\sigma^2(\{\hat{\alpha}^p_{i,j}\}_{i=1}^N)$ to $\sigma^2(\{\alpha_{i,j}\}_{i=1}^N)$ for $j=1,..,d$.% Here, the angle between two vectors is calculated using the $\arccos{(\cdot)}$ function.


Formally, we define the optimization problem for the reverse-engineering of backdoored gaze estimation models as:
\begin{equation}
    % \small
    \theta^{*}=\arg\min_{\theta} \frac{\lambda_1}{d}\sum_{j=1}^d{\sigma^2\left(\{\mathcal{G}_j\left(M_{\theta}(x_i)\right)\}_{i=1}^N\right)} \\+\lambda_2 r_f + r_{sim},
    \label{opt:REforDRM}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are the weights for the first and second objectives, respectively; $\mathcal{G}_j(M_{\theta}(x_i))$ is the $j$th element of $\mathcal{G}(M_{\theta}(x_i))\in \mathbb{R}^d$; {$\frac{1}{d}\sum_{j=1}^d{\sigma^2(\{\mathcal{G}_j\left(M_{\theta}(x_i)\right)\}_{i=1}^N)}$ is the average variance of output gaze directions across all dimensions}; and $r_{sim}$ is the input-space optimization objective~\citep{wang2019neural} that ensures the transformed input $M_{\theta}(x_i)$ is similar to the benign input $x_i$, i.e., $r_{sim}=\frac{1}{N}\sum_{i=1}^N{\|M_{\theta}(x_i)-x_i\|_1}$. The angle between two vectors is calculated using the $\arccos{(\cdot)}$.

% \vspace{0.05in}
% \noindent\textbf
% \subsubsection{Solving the optimization problem}
\subsubsection{Sensitivity-aware trigger reversal}
Directly solving the optimization problem~\ref{opt:REforDRM} can lead to sub-optimal solutions. % In such cases, the algorithm may add perturbations to the eye regions of the input image, effectively \textit{destroying} gaze-related features rather than properly reverse-engineering the trigger. 
As an example, Figure~\ref{fig:sub_optimal} shows a suboptimal outcome. Specifically, we use BadNets to train a backdoored model on the MPIIFaceGaze, where the trigger is a red square added to the bottom-right corner of the image (Figure~\ref{fig:sub_optimal} (b)). We train $M_{\theta}$ by solving the optimization problem~\ref{opt:REforDRM}. Figure~\ref{fig:sub_optimal} (d) shows the residual map between the benign image (Figure~\ref{fig:sub_optimal} (a)) and the reversed poisoned image (Figure~\ref{fig:sub_optimal} (b)). 

\begin{figure}[]
	\centering
	\subfigure[]{\includegraphics[scale=0.285]{./figs/benign.png}\label{fig:benign}
}   \quad\quad
	\subfigure[]{\includegraphics[scale=0.285]{./figs/poisoned.png}\label{fig:poisoned}
}\\[-1.5ex]
 \subfigure[]{\includegraphics[scale=0.285]{./figs/reversed.png}\label{fig:reversed}
}\quad\quad
	\subfigure[]{\includegraphics[scale=0.285]{./figs/residual.png}\label{fig:residual}
}
\vspace{-0.1in}
	\caption{\textbf{Example of a sub-optimal solution}: %for directly solving optimization problem \ref{opt:REforDRM} 
 (a) the benign image; (b) the poisoned image; (c) the reversed poisoned image by solving the optimization problem \ref{opt:REforDRM}; and (d) the residual map between the (a) and (c). Perturbations are added to the eye regions, instead of reversing the trigger. %We can notice that solving the optimization problem defined in Equation~\ref{opt:REforDRM} fails to reverse the trigger but adds perturbations to the eyes region, that contains the most important features for gaze estimation. 
 }	
	\label{fig:sub_optimal}
	 \vspace{-0.1in}
\end{figure}

It is evident that directly solving the optimization problem fails to reverse-engineer the trigger, but instead adds perturbations to the eye regions to effectively \textit{destroying} gaze-related features. %, which contain the most important features for gaze estimation. 
{We believe this happens due to the \textbf{imbalanced sensitivity of $\mathcal{G}$ across different regions of the input image}. Specifically, $\mathcal{G}$ is significantly more sensitive to changes in the eye regions compared to other regions, as eye regions contain the most crucial features for gaze estimation~\cite{zhang17_cvprw}. As a result, perturbations added to these sensitive regions are more easily to cause substantial changes in the gaze estimation output. This imbalance causes the algorithm to prioritize adding perturbations to the sensitive eye regions when solving the optimization problem in Equation~\ref{opt:REforDRM}, neglecting potential trigger patterns in less sensitive regions. 

%We propose the \emph{momentum reverse trigger} to address this issue.}

We address this issue by preventing significant changes in the gaze estimation output caused by perturbations added in sensitive regions in each training iteration, such that the algorithm can search for trigger patterns in both sensitive and insensitive regions. {Given an image $x_i$, we first estimate the sensitivity of $\mathcal{G}$ to each pixel in $x_i$ by computing the gradient of $\mathcal{G}$ with respect to that pixel. The intuition is that if $\mathcal{G}$ is sensitive to a pixel, e.g., pixels in the eye regions, a small change in its value will result in a significant change in the output of $\mathcal{G}$, which is reflected by a large absolute gradient value. By contrast, if $\mathcal{G}$ is insensitive to a pixel, the corresponding absolute gradient will be small. Formally, consider an image $x_i$ with dimensions $N_w \times N_h \times N_c$. We denote $x_i[a,b]$ as the pixel of $x_i$ at width $a$ and height $b$. The sensitivity $\mathcal{T}(x_i)[a,b]$ of this pixel is estimated as $\mathcal{T}(x_i)[a,b]=\sum_{c=1}^{N_c}{|{\partial \mathcal{G}}/{\partial x_i[a,b,c]}|}$, where $x_i[a,b,c]$ is the value of $x_i[a,b]$ in channel $c$. By computing the sensitivity for each pixel, we obtain a sensitivity map $\mathcal{T}(x_i)$ of size $N_w \times N_h$ for $x_i$.} We re-scale the sensitivity map to $[0,1)$ by dividing each component by a value greater than the maximum value in the map. Then, we obtained the reverse-engineered poisoned image $x'_i$ by:
%{Given an image $x_i$, we first estimate the sensitivity of $\mathcal{G}$ to each pixel in $x_i$. If $\mathcal{G}$ is sensitive to a pixel, a small change in its value could result in a significant change in the output of $\mathcal{G}$. This corresponds to a large absolute gradient value of $\mathcal{G}$ to this pixel. By contrast, if $\mathcal{G}$ is insensitive to a pixel, the absolute gradient value to that pixel is small. Motivated by this, }
%Specifically, for an image $x_i$ with dimensions $N_w \times N_h \times N_c$, we generate a \textbf{sensitivity map} $\mathcal{T}(x_i)$ of size $N_w \times N_h$ based on the gradient of $\mathcal{G}$ with respect to $x_i$. We denote $x_i[a,b,c]$ as the pixel value of $x_i$ at width $a$, height $b$, and channel $c$. Then, the value of the sensitivity map at width $a$ and height $b$ is calculated as: $\small\mathcal{T}(x_i)[a,b]=\sum_{c=1}^{N_c}{|{\partial \mathcal{G}}/{\partial x_i[a,b,c]}|}$. A larger value of $\mathcal{T}(x_i)[a,b]$ indicates higher sensitivity for the pixels at position $(a,b)$.} We re-scale the sensitivity map to $[0,1)$ by dividing each component by a value greater than the maximum value in the map. {Then, we obtained the reverse-engineered poisoned image $x'_i$ by: 
\begin{equation}
\begin{split}
    x'_i[a,b,c]=M_{\theta}(x_i)[a,b,c]\cdot(1-\mathcal{T}(x_i)[a,b])+\\x_i[a,b,c]\cdot\mathcal{T}(x_i)[a,b],
    \label{eq:reversePoisoned}
\end{split}
\end{equation}{where $x'_i[a,b,c]$ and $M_{\theta}(x_i)[a,b,c]$ refer to the pixel value of $x'_i[a,b]$ and $M_{\theta}(x_i)[a,b]$ at channel $c$, respectively. Essentially, if $x_i[a,b]$ is sensitive, indicated by a large value of $\mathcal{T}(x_i)[a,b]$, we limit the perturbations added to it in each iteration.} Instead of directly feeding $M_{\theta}(x_i)$ to $\mathcal{G}$, we feed the image $ x'_i$ to $\mathcal{G}$ to form the final optimization problem $\mathcal{OPT}$-{\name} as: 
\begin{equation}
    % \small
    \theta^{*}=\arg\min_{\theta} \frac{\lambda_1}{d}\sum_{j=1}^d{\sigma^2\left(\{\mathcal{G}_j\left(x'_i \right)\}_{i=1}^N\right)} +\lambda_2 r_f + \sum_{i=1}^N\frac{\|x'_i -x_i\|_1}{N},
    \label{opt:SecureGaze}
\end{equation} In a nutshell, $\mathcal{OPT}$-{\name} substitutes $M_{\theta}(x_i)$ in all the objectives of Equation~\ref{opt:REforDRM} with $x'_i$.}
%{Since all the values in $\mathcal{T}$ is smaller than $1$, the algorithm search for trigger patterns from all the pixels}
%To avoid such a trivial solution, we propose the momentum reverse trigger to assign different weights to different regions of the image to balance the attention of the gaze estimation model on the image, such that the algorithm can pay attention to all the pixels and search for the triggers that are injected into both important and unimportant regions. To this end, we first generate an \textit{attention map} $\mathcal{T}(x_i) \in \mathbb{R}^{N_w\times N_h}$ for each input $x_i$ based on the gradient of $\mathcal{G}$ w.r.t. $x_i$. In detail, for each pixel $x_i[a,b]$, we obtain the corresponding value $\mathcal{T}(x_i)[a,b]$ in the attention map by $\small\mathcal{T}(x_i)[a,b]=\sum_{c=1}^{N_c}{|{\partial \mathcal{G}}/{\partial x_i[a,b,c]}|}$. Then, we re-scale the attention map to $[0,1)$ by dividing each component of the attention map by a number that is larger than the maximum value in the attention map. Then, instead of directly feeding $M_{\theta}(x_i)$ to $\mathcal{G}$, we feed the following image to the gaze estimation model:
% \begin{equation}%\small
%     x'_i = M_{\theta}(x_i)\odot(1 - \mathcal{T}(x_i)) + x_i \odot \mathcal{T}(x_i).
%     \label{eq:momentum}
% \end{equation} We use $\mathcal{OPT}$-{\name} to denote the final optimization problem after introducing momentum reverse trigger.

% \vspace{0.05in}
\subsubsection{Backdoor identification}
By solving the new optimization problem defined in Equation~\ref{opt:SecureGaze}, we can obtain the perturbation $\|x'_i-x_i\|_1$ required to transform input $x_i$ to generate the potential target gaze direction. We observe that the perturbation needed to alter $x_i$ to produce the target gaze direction in a backdoored gaze estimation model is significantly smaller than that required for a benign gaze estimation model. To illustrate, we train ten benign and ten backdoored gaze estimation models using BadNets on the MPIIFaceGaze dataset. Figure~\ref{fig:averagePerturbation} shows the average perturbation on the benign dataset obtained by solving $\mathcal{OPT}$-{\name} for each model. The results show that the average perturbations required for the backdoored models (P0 to P9) are considerably smaller than those for the benign models (B0 to B9). 

{Based on this observation, we determine whether a given gaze estimation model is backdoored by comparing the average perturbation obtained through reverse engineering on $\mathcal{D}_{be}$ with a threshold value $\epsilon \|\hat{x}\|_1$. Here, $\hat{x}$ is the input image with the maximum $L1$ norm in the benign dataset $\mathcal{D}_{be}$, and $\epsilon$ is a constant. The average perturbation is calculated as $\frac{1}{N_{be}}\sum_{x_i\in\mathcal{D}_{be}}{\|x'_i-x_i\|_1}$, where $N_{be}$ represents the number of images in $\mathcal{D}_{be}$. To determine the threshold value, we assume that the perturbations of benign models follow a normal distribution. We compute the mean $m_p$ and standard deviation $\sigma_p$ of average perturbations across ten benign models reported in Figure~\ref{fig:averagePerturbation}. We set the threshold value to be $m_p - 2\sigma_p$, meaning that models with perturbation values below this threshold have a greater than 95\% probability of being outliers, indicating a backdoored model. This corresponds to $\epsilon=0.03$.}

% Based on this observation, we introduce the following metric to determine whether a given gaze estimation model is backdoored or not:
% \begin{equation}
%     \mathcal{I}(\mathcal{G})=\mathds{1}(\frac{1}{N_{be}}\sum_{x_i\in\mathcal{D}_{be}}{\|x'_i-x_i\|_1}, \epsilon \|\hat{x}\|_1),
% \end{equation}    
% where $\hat{x}$ is the input image with the maximum $L1$ norm in the benign dataset $\mathcal{D}_{be}$; $N_{be}$ is the number of images in $\mathcal{D}_{be}$; $\epsilon$ is a constant; and $\mathds{1}$ is the indicator function that returns $1$ (for a backdoored gaze estimation model) if its first parameter is smaller than the threshold $\epsilon \|\hat{x}\|_1$, and $0$ otherwise (indicating a benign model). Essentially, \name classifies a gaze estimation model as backdoored if the average perturbation obtained through reverse engineering on $\mathcal{D}_{be}$ is smaller than $\epsilon \|\hat{x}\|_1$. {To determine the threshold value, we first calculate the mean $m_p$ and standard deviation $\sigma_p$ of perturbations for ten being models in Figure~\ref{fig:averagePerturbation}, and then set the threshold value to be $m_p - 2\sigma_p$, corresponding to $\epsilon=0.03$.}

\begin{figure}[]
	\centering
	{\includegraphics[scale=0.37]{./figs/average_perturbation_new.pdf}} %\\[-1.5ex]

\caption{The average perturbations for ten benign models (B0$\sim$B9) and ten backdoored models (P0$\sim$P9).}% The average perturbations required for the backdoored models are considerably smaller than those for the benign models.}	
	\label{fig:averagePerturbation}
 \vspace{-0.15in}
\end{figure}



\subsubsection{Backdoor mitigation} 
Once a gaze estimation model $\mathcal{G}$ is identified as backdoored, {\name fine-tunes $\mathcal{G}$ to mitigate backdoor behavior, such that the fine-tuned model produces correct gaze directions for poisoned images. \textbf{Note that, the defender only has access to a small benign dataset $\mathcal{D}_{be}$.} Therefore, \name generates a reverse-engineered poisoned dataset, $\mathcal{D}_{rp}=\{x'_i, y_i\}_{i=1}^{N_{be}}$, by applying $M_{\theta}$ to each image $x_i$ in $\mathcal{D}_{be}$ via Equation~\ref{eq:reversePoisoned}. Each reverse-engineered poisoned image $x'_i$ in $\mathcal{D}_{rp}$ is annotated with its correct gaze annotation $y_i$.} Next, {\name} fine-tunes $\mathcal{G}$ using both $\mathcal{D}_{be}$ and $\mathcal{D}_{rp}$. Formally, the backdoor mitigation is achieved by minimizing the following objective: $
    \sum_{(x_i,g_i)\in\mathcal{D}_{be}}\ell_1(\mathcal{G}(x_i),y_i) + \sum_{(x'_i,g_i)\in\mathcal{D}_{rp}}\ell_1(\mathcal{G}(x'_i),y_i)$.


%Once a gaze estimation model $\mathcal{G}$ is identified as backdoored, {\name} leverages the reverse-engineered trigger function $M_{\theta}$ and the benign dataset $\mathcal{D}_{be}=\{x_i, y_i\}_{i=1}^{N_{be}}$ to mitigate the backdoor behavior. {Note that, SecureGaze does not require access to the original poisoned image. Instead, \name creates a reverse-engineered poisoned dataset $\mathcal{D}_{rp}=\{x'_i, y_i\}_{i=1}^{N_{be}}$ by applying $M_{\theta}$ to each image $x_i$ in $\mathcal{D}_{be}$, as defined in Equation~\ref{eq:reversePoisoned}.} Each poisoned image $x'_i$ in $\mathcal{D}_{rp}$ is annotated with its correct gaze annotation $y_i$. Next, {\name} fine-tunes $\mathcal{G}$ using both the benign dataset $\mathcal{D}_{be}$ and the reverse-engineered poisoned dataset $\mathcal{D}_{rp}$. This ensures that the fine-tuned model produces gaze directions close to $y_i$ for both benign images $x_i$ and reverse-engineered poisoned images $x'_i$. Formally, the backdoor mitigation is achieved by minimizing the following objective: $
   % \sum_{(x_i,g_i)\in\mathcal{D}_{be}}\ell_1(\mathcal{G}(x_i),y_i) + \sum_{(x'_i,g_i)\in\mathcal{D}_{rp}}\ell_1(\mathcal{G}(x'_i),y_i)$.
