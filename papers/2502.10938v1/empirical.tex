\section{Evaluation}
This section presents empirical evaluations comparing our methodology with existing prompt-engineering techniques, addressing three primary research questions:
\begin{tcolorbox}
\begin{enumerate}[start=1,label={\bfseries RQ\arabic*:}]
\item What is the capacity of LLMs to generate exhaustive enumerations?
\item Could PEA enhance LLMs' reasoning capabilities through program synthesis and local executions?
\item 
%\twrchanged{
Compared to direct reasoning, does PEA achieve more efficient reasoning by combining reasoning with program execution?
%}
%Does PEA offer a more efficient reasoning in combining reasoning with program executions compared to direct reasoning?
\end{enumerate}
\end{tcolorbox}
\textbf{RQ1} examines LLM reasoning limitations for problems with succinct programmatic but extensive end-to-end representations. This inquiry is foundational, because many computational-reasoning problems are hypothesized to have essential computational lower-bounds~\citep{SETH,fgc}, potentially necessitating lengthy enumerations.
\textbf{RQ2} investigates whether the PEA
%\twrchanged{
program-synthesis framework enhances an LLM's reasoning capability on computational tasks by leveraging their coding proficiency,
%}
aiming to determine if programmatic problem translation improves
%\twrchanged{
an LLM's
%}
performance on complex reasoning tasks.
\textbf{RQ3} evaluates the efficiency of the PEA framework, including PEA representation translation and local server execution, to assess whether it offers a more efficient alternative to direct reasoning by LLMs.

\paragraph{Models and server}We use recent language models with advanced coding and reasoning capabilities, specifically GPT-4o, OpenAI o1-mini, and o1 models~\citep{openai2024gpt4technicalreport}. The majority of computational cost 
%\twr{This phrase is a horrendous choice, because you've harped on ``computational complexity'' in the sense of Theory A, including in the previous paragraph.  Surely you just mean ``computational cost'' right?}
stems from LLM queries. Local Python execution is performed on a server with thirty-two AMD EPYC 7313P 16-core processors, 528 GB of memory. Supplementary small-scale experiments employing recent DeepSeek models were conducted, with results presented in the appendix~\citep{deepseekai2024deepseekv3technicalreport,deepseekai2025deepseekr1incentivizingreasoningcapability}.

\paragraph{Method and baselines} We implemented \textbf{PEA} as described in~\cref{sec:implementation}, using a fixed example for code integrity checks, which is removed from the evaluation dataset if present. Comparative analysis employed Direct Query (\textbf{DQ}) and \textbf{CoT} as principal baselines, with \textbf{ToT} as an additional baseline for the Game of $24$. Our core PEA prompts are provided in the appendix.

\paragraph{Experimental design}
To address \textbf{RQ1}, LLMs are evaluated on their ability to generate enumerations for combinatorial problems, specifically Cartesian-product combinations and permutations (see \Cref{sec:enum-llm}).
%\twrchanged{
%
%}
The performance metric is the percentage of total possible enumerations successfully outputted by the models.
%\twr{I don't know what the previous sentence means.
%Are you saying that a run of PEA was scored based on whether the enumeration program generated performed a complete enumeration of the desired domain?
%If so why are you calling them enumeration \emph{candidates}, which suggests that completeness was used as some sort of filter.
%}
For \textbf{RQ2}, we selected a range of computational-reasoning tasks, including SAT, Game of $24$, and two planning tasks derived from recent LLM literature~\citep{marino2024fastanalysisopenaio1preview,yao2023tree,valmeekam2023planbench}. PEA and 
%\twrchanged{
various
%}
baselines were applied to find correct answers (see~\cref{sec:sat,sec:gon,sec:planning}). Among all computational tasks, optimization of enumeration in PEA code synthesis is only activated for the logistics planning problem. For~\textbf{RQ3}, the time used by PEA and other tools to solve the entire dataset was measured (see~\cref{sec:eff}). Notably, for each task, PEA's program synthesis is performed only once, with the resulting program being reused across the entire dataset, as each data point represents a concrete problem instance serving as input to the synthesized program.

The PEA-generated code undergoes integrity checks with a maximum of $m=10$ iterations, as 
%\twrchanged{
per~\cref{alg:pea}.
%}
A $30$-second execution timeout is imposed on synthesized code. Solutions are considered unfound if their execution exceeds 
%\twrchanged{
the timeout threshold.
%}


\paragraph{Findings}
Our
%\twrchanged{
experiments yielded
%}
the following results pertaining to the research questions:
\begin{enumerate}[parsep=0pt,itemsep=1pt,leftmargin=28pt,start=1,label={\bfseries RQ\arabic*:}]
\item LLMs exhibit accurate enumeration for small search spaces (typically less than a few hundred elements) but default to generating Python code for larger enumerations rather than providing exhaustive results. This preference for programmatic approaches over end-to-end representations aligns with our premise of leveraging LLMs' coding capabilities for complex reasoning tasks, where code serves as a reasoning generator and its execution produces concrete reasoning outcomes.

\item Our evaluation of computational problems demonstrates that reasoning via code generation and execution consistently outperforms direct reasoning when using identical underlying models. Programs passing integrity checks successfully resolve a substantial portion of the dataset, underscoring the efficacy of programmatic approaches in computational problem-solving. The o1 and o1-mini models significantly surpass GPT-4o in direct reasoning. Moreover, these models exhibit superior proficiency in synthesizing complex code, particularly in generating optimized solutions that adhere to enumeration pruning instructions. This enhanced capability further augments PEA's performance on more intricate tasks.

\item Code synthesis and program execution provide efficient reasoning, capitalizing on decades of optimization in modern computer architectures. This approach offers a significant advantage over recent powerful reasoning models, which, despite their accuracy, require substantially more time for problem-solving. Notably, successful code generation typically necessitates fewer than three LLM queries to pass integrity checks. This minimal query requirement results in negligible computational cost when amortized across the entire dataset, further enhancing the method's overall efficiency.
\end{enumerate}

The following subsections offer a comprehensive analysis of each experimental
%\twrchanged{
task.
%}
%\Cref{sec:limitation} delineates additional findings and limitations of the PEA framework.

\subsection{Enumerations with LLM}\label{sec:enum-llm}
In our evaluation, we employed random 3-character strings as variable-name candidates. For \textbf{Cartesian product-combination} tasks, LLMs were instructed to output $n$-variable product expressions, producing all possible combinations of $n$ variables. Each variable was selected from a pool of $m$ $3$-character strings, resulting in $m^n$ product combinations.
%\twrchanged{
%variable-name candidates.
%}
%For \textbf{Cartesian product-combination} tasks, 
%LLMs were instructed to
%\twrchanged{
%create $n$-variable product expressions, producing all possible combinations of $n$ variables, each selected
%}
%from $m$ 3-character strings, yielding $m^n$ product combinations.
%\twr{Is my restatement of the problem correct?}
\textbf{Permutation} tasks required LLMs to generate all possible permutations of $m$ random strings, resulting in $m!$ permutations. Response coverage is quantified as the proportion of all possible enumerations returned by the model. Permutation results are presented in~\cref{tab:perm}, and combination results are included in the appendix.

These tasks represent enumerations with trivial predicates. Our PEA approach can be interpreted as synthesizing programs as
%\twrchanged{
reasoning generators,
%}
rather than embodying the reasoning itself. Program execution then yields concrete reasoning with precision and efficiency. Fine-grained complexity theory establishes hypothetical lower bounds for many computational problems, including polynomial-time solvable ones~\citep{Williams2019ONSF}, often resulting in more concise algorithmic descriptions compared to concrete reasons. For instance, a problem with a hypothetical lower bound of time complexity $O(n^3)$ maintains an algorithmic description (program representation) of $O(n)$, while concrete algorithmic reasoning takes $O(n^3)$ of time, which can be substantial for large problem instances, i.e.,
%\twrchanged{
with large $n$.
%}
Our LLM enumeration experiment exemplifies an extreme case where concrete evaluations grow exponentially, yet the programmatic representation remains highly succinct. Consequently, synthesizing the
%\twrchanged{
reasoning generator
%}
is significantly more efficient and simpler than 
%\twrchanged{
performing
%}
concrete reasoning in this scenario.

\begin{table}[th]
\begin{center}
\caption{Coverage of permutations: Percentage of total permutations returned by LLMs for sets of 4, 5, 6, and 7 random strings. Values represent means from three independent trials. For coverage below $10\%$, LLMs provide permutation-generation instructions rather than explicit results.}
\begin{tabular}{ ccccc } 
\toprule
\bf Model & \bf 4 & \bf 5 & \bf 6 & \bf 7 \\  
\hline
GPT-4o & 100\%  & 100\% & 84.9\% & 0.1\% \\  
\hline
o1-mini &  100\% &  100\% &  34.1\% &  0.2\% \\  
\hline
o1 &  100\% &  100\% &  13.6\% &  0.3\% \\  
  \bottomrule
\end{tabular}
\label{tab:perm}
\end{center}
\vspace{-0.3cm}
\end{table}

\subsection{SAT problem}\label{sec:sat}
The SAT problem involves determining a truth-value assignment that satisfies a given Boolean formula.

\paragraph{Formal Description} The SAT problem can be formally defined within the PEA framework as follows:
(i) The predicate is a Boolean formula
%\twrchanged{
(for which a truth assignment evaluates to either True or False.) %}
(ii) The enumeration comprises all possible truth assignments of the variables.
(iii) The aggregation determines whether any truth assignment in the enumeration satisfies the predicate.

\paragraph{Dataset}We use a Boolean-satisfiability dataset from~\citet{marino2024fastanalysisopenaio1preview}, comprising formulas in Conjunctive Normal Form (CNF). The dataset comprises 100 formulas each of 2-CNF, 3-CNF, and 4-CNF types, where clauses contain 2, 3, or 4 literals, respectively. While 2-CNF formula satisfiability is in class $\p$, 3-CNF and 4-CNF satisfiability problems are $\np$-complete. One 3-SAT formula is used for code semantic validation and excluded from the testing set, resulting in a total of 299 formulas for evaluation.

\paragraph{Results}Given the computational-complexity differences, results are presented separately: 2-CNF instances are aggregated in the appendix, while 3-CNF and 4-CNF instances are reported together in~\cref{tab:34-cnf}. This distinction reflects the $\p$-class nature of 2-CNF SAT problems and the $\np$-hardness of 3-CNF and 4-CNF SAT problems. PEA achieves perfect correctness in all cases, even when using the relatively weak reasoning model GPT-4o. Notably, all models perform exceptionally well on 2-CNF SAT problems, as detailed in the appendix.



\begin{table}
\begin{center}
\caption{Performance comparison of methods on 3-SAT and 4-SAT problems. Results are presented as $m+n$, where $m$ represents correctly solved satisfiable formulas (out of \textbf{172}) and $n$ denotes correctly identified unsatisfiable formulas (out of \textbf{27}).
}
\begin{tabular}{ ccccc } 
\toprule
\bf Model & \bf PEA & \bf CoT & \bf DQ  \\  
\hline
GPT-4o & 172+27  & 2+0 & 1+0  \\  
\hline
o1-mini &  172+27 &  46+11  &  63+13 \\  
\hline
o1 &  172+27 &  100+24 &  99+24  \\ 
  \bottomrule
\end{tabular}
\label{tab:34-cnf}
\end{center}
\vspace{-0.3cm}
\end{table}


\subsection{Game of 24}\label{sec:gon}
The Game of $24$ (\textbf{G24}) is a mathematical puzzle where participants are presented with four numbers and tasked with constructing an expression that evaluates to $24$. This expression can be formulated using the arithmetic operations of addition, subtraction, multiplication, and division, with the optional use of parentheses.

\paragraph{Formal description}The Game of $24$ can be formulated as follows: (i) The predicate evaluates whether an arithmetic expression composed of four given numbers equals $24$. (ii) The enumeration generates all valid expressions using $+$, $-$, $\times$, $\div$, and permissible parenthetical patterns. (iii) The aggregation determines if at least one expression evaluates to $n$ or if no such expression exists.

\paragraph{Dataset}We extract the most difficult $100$ instances of Game of $24$ numbers from the dataset of ToT~\citep{yao2023tree}. The difficulty of these instances is quantified by ToT's success rate. One example from the dataset, excluded from the $100$ selected instances, is used for code validation.

\begin{table}
\begin{center}
\caption{Performance comparison of methods on Game of 24. Results show the number of correctly assembled expressions out of \textbf{100} valid instances. ToT with o1 and o1-mini were omitted due to excessive query time (around 1 hour per instance) and poor performance (0 out of 10 in preliminary testing).}
\begin{tabular}{ccccc} 
\toprule
\bf Model & \bf PEA & \bf CoT & \bf DQ & \bf ToT \\  
\hline
GPT-4o & 100  & 2 & 2 & 4\\  
\hline
o1-mini & 100  &  29  & 44  & -- \\  
\hline
o1 & 100 &  60 &  50  & -- \\ 
  \bottomrule
\end{tabular}
\label{tab:g24}
\end{center}
\vspace{-0.3cm}
\end{table}


\paragraph{Results}The results of the Game of $24$ are presented in~\cref{tab:g24}. PEA demonstrates perfect performance across all models, while the most advanced o1 model, even when equipped with CoT, resolves only $60$ instances. %Notice that though game of 24 was evaluated and proposed by~\citet{yao2023tree}, it seems to only work for GPT4-turbo. In the meantime, when evaluated on Game of 36, because some of the problem instances cannot be evaluated to 

\subsection{Planning problems}\label{sec:planning}
We use a recent planning benchmark dataset from~\citet{valmeekam2023planbench}, designed to assess LLMs' reasoning capabilities in solving planning problems. The benchmark comprises several variants of two primary tasks: Blocksworld (\textbf{BW}) and Logistics (\textbf{LOGI}).

The Blocksworld task is a planning problem where participants are presented with several colored blocks on a table. The task requires executing valid actions to move these blocks. Each problem instance provides an initial block configuration and goal conditions representing desired block-stacking conditions. Participants must devise a feasible plan, comprising a sequence of actions, to achieve the goal state. Our evaluation uses a more challenging variant, the optimal-cost Blocksworld problem, which necessitates finding a minimum-cost action sequence.

The Logistics task involves a planning problem featuring packages, vehicles, and cities with various locations. Vehicles, including trucks and airplanes, must adhere to specific rules for movement and package handling (loading/unloading). Each problem instance describes initial settings and package destinations. Participants are required to formulate a viable plan for vehicle actions to meet the specified goals. Due to inconsistencies in the cost-optimal logistics dataset, this study evaluates performance based on the satisfiable version of the problem, focusing on identifying any valid plan rather than the optimal solution.

% No blocks can share the same color, so we can use a color, e.g. \textit{red}, to denote the red block.

\paragraph{Formal description} We now formalize the planning problems in the PEA framework.
For cost-optimal \textbf{BW}: (i) The predicate is to determine if a given initial block configuration and action list lead to the goal conditions. (ii) The enumeration sequentially generates all valid action lists from the current state. (iii) The aggregation evaluates if the action list, ordered by ascending cost, satisfies the predicate, which effectively implements a breadth-first search.
For valid \textbf{LOGI}: (i) The predicate is to determine if a given initial package configuration with a list of vehicle actions can reach the goal state. (ii) The enumeration sequentially generates all valid vehicle-action lists from the current vehicle and package state. (iii) The aggregation evaluates if the action list satisfies the predicate.


\paragraph{Dataset} For the cost-optimal Blocksworld task, we use all $500$ task instances from the Plan-Optimality dataset. Each instance involves blocks of various colors, with minimal step requirements ranging from a few to a dozen. The instances include a natural-language description of rules, a one-shot example (comprising initial state, goal conditions, and plan), and the task question.

For the valid Logistics task, we employ all $285$ task instances from the Plan-Generation dataset. Each instance involves a specific configuration of cities, locations, vehicles, and packages, with plan actions ranging from a few to approximately forty. Similar to BW, each instance contains a natural-language description of rules, a one-shot example, and the task question.

The dataset provides ground truth for each task instance and a reference validator for plan verification.


\begin{table}
\begin{center}
    \caption{Performance comparison of methods on cost-optimal Blocksworld task: Table displays the number of correctly solved instances out of \textbf{500} total tasks for various methods.}
    \begin{tabular}{ ccccc } 
        \toprule \bf Model & \bf PEA & \bf CoT & \bf DQ  \\
        \hline GPT-4o & 500 & 115 & 94  \\  
        \hline o1-mini & 500 & 185 &  182 \\  
        \hline o1 & 500 & 452 &  324  \\ 
        \bottomrule
    \end{tabular}
    \label{tab:bw}
\end{center}
\vspace{-0.3cm}
\end{table}

\paragraph{Results} The results for the cost-optimal Blocksworld task are presented in~\cref{tab:bw}. Notably, the o1 model demonstrates superior performance on this task compared to other models. However, when using PEA, all models achieve perfect task completion.

\Cref{tab:logi} summarizes the results for the valid Logistics task. The optimization option is activated for PEA due to the extensive search space. While GPT-4o generates only unoptimized code, resulting in frequent $30$-second timeouts, it still successfully solves $78$ instances, outperforming its direct-query baselines. In contrast, both o1 and o1-mini models demonstrate superior performance, successfully synthesizing optimized programs through PEA and solving all Logistics instances.



%\paragraph{Formal description} Several cities are given for their locations. All locations in one city are connected. Each city has exactly one truck that can move between a city locations. Each city has exactly one airport location and airplanes can move between airports. Each vehicle can load and unload a package from and to a location if they are at the same location. Multiple packages and multiple vehicles can stay at one location. The initial state specifies the start location for all trucks, airplanes, and packages. The expected goal locations of the packages are given. The problem is to ask for a series of actions. The \textit{predicate} is whether all the packages are at their goal locations. The \textit{enumeration} is to list all possible valid actions in the current state of vehicles. To \textit{aggregate} the actions, we need to continuously search for a series of states that each one can be reach from the previous state by one action and check the final state satisfies the goal.

%\paragraph{Dataset} We use all 285 task instances from the \textit{Plan Generation} for task \textit{Logistics} in PlanBench dataset~\citep{valmeekam2023planbench}. One instance is involved with one setting for cities, locations, vehicles, and packages, and the plan actions range from a few to around thirty.
%Each instance has a natural language description for the rules, a one-shot example (with the initial state, goal conditions, and the plan), and the task question. The dataset ships with the ground truth for each task instance and a reference validator to check against a given plan.

\begin{table}
\begin{center}
    \caption{Performance comparison on valid Logistics task: Results present the number of correctly solved instances out of \textbf{285} tasks. Note: GPT-4o PEA generates unoptimized code; $207$ instances reach $30$-second timeout and are terminated.}
    \begin{tabular}{ ccccc } 
        \toprule \bf Model & \bf PEA & \bf CoT & \bf DQ  \\
        \hline GPT-4o & 78 & 44 & 28  \\  
        \hline o1-mini & 285 & 60 &  65 \\  
        \hline o1 & 285 & 205 &  163  \\ 
        \bottomrule
    \end{tabular}
    \label{tab:logi}
\end{center}
\vspace{-0.3cm}
\end{table}

\subsection{Efficiency}\label{sec:eff}
To evaluate the efficiency of each prompt method, we measure the per-instance solving time. For  PEA, the initial program-synthesis cost is amortized across all instances, because it occurs only at the problem's outset. \Cref{tab:efficiency} presents the per-instance computation costs. Notably, the PEA method not only enhances problem-solving accuracy but also demonstrates reduced computation time in most cases across all models tested.
\begin{table}
\begin{center}
    \caption{Per-instance computation time (in seconds) for each problem-solving approach:
While o1 achieves high accuracy, the PEA approach demonstrates superior accuracy with significantly lower computational cost due to program reuse across multiple instances.}
    \label{tab:efficiency}
    \begin{tabular}{ ccccc } 
        \toprule \bf Method(Model) & \bf SAT & \bf G24 & \bf BW  & \bf LOGI  \\
        \hline PEA (GPT-4o) & 0.03 & 0.1 & 2.3 & 23.3 \\  
        \hline COT (GPT-4o) & 7.3 & 7.1 & 8.2 & 13.6 \\
        
        \hline DQ (GPT-4o) &  3.5 & 4.5 & 3.6 & 8.0 \\
        \midrule
        \hline PEA (o1-mini) & 0.1 & 0.2 & 2.8 & 7.9 \\  
        \hline COT (o1-mini) & 16.6 & 11.4 & 18.8 & 55.0 \\
        \hline DQ (o1-mini) & 23.6 & 16.9 & 15.3 & 27.8 \\
        \midrule
        \hline PEA (o1) & 0.1 & 0.9 & 2.6 & 15.8 \\  
        \hline COT (o1) & 206.2 & 56.8 & 98.7 & 181.5 \\
        \hline DQ (o1) & 159.7 & 100.8 & 134.5 & 96.8 \\
        \bottomrule
    \end{tabular}
\end{center}
\vspace{-0.3cm}
\end{table}

\section{Conclusion}This paper introduces the PEA framework, which formally characterizes computational reasoning tasks and enhances LLMs' reasoning capabilities through their coding proficiency. Our central premise posits that many computational problems have inherent lower bounds, and synthesizing programs as reasoning generators is often more efficient and simpler than performing concrete reasoning.
%This study aims to enhance LLMs' reasoning capabilities by leveraging their coding proficiency. Our approach necessitates LLMs to synthesize Python programs. While detailed coding instructions are provided in our experiments, these are generally unnecessary for advanced reasoning models such as o1. They primarily benefit GPT-4-o, particularly in coding tasks involving complex reasoning or natural language descriptions, where GPT-4o encounters difficulties in precise and comprehensive task processing.



%proof generator length vs proof itself. This interpretation might introduce new theoretical tools in analyzing the reasoning etc and guide better results.

%Generalization is stronger compared to ...
