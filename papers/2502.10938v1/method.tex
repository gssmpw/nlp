\section{Method}
\label{Se:Method}

This section provides a detailed exposition of the PEA framework, encompassing four key aspects: syntax (formal structure), semantics (real-world problem modeling), computation (algorithmic problem-solving approaches), and implementation (practical realization). %This multifaceted examination aims to elucidate the framework's theoretical foundations, practical applications, and computational methodologies.

\subsection{Notation and definitions}\label{sec:def}
Let $X$ be a finite set. Predominantly, we consider $X=\{0,1\}$, the binary set. We use $x$ to denote variables, and $c$ to denote concrete values. Central to our analysis is the concept of an $n$-ary \emph{unquantified} predicate, denoted as $P: X^n\rightarrow \{0, 1\}$. This predicate function serves to determine whether $n$ variables satisfy the relation specified by $P$. For instance, $P(x, y): x+y = 5$ denotes
%\twrchanged{
the set of $(x,y)$ pairs such that the sum of the values of $x$ and $y$ equals $5$---e.g.,
%}
$P(1, 4)=1$ and $P(1,2) = 0$. We introduce the existential ($\exists$) and universal ($\forall$) quantifiers, denoted by the set $Q=\{\exists, \forall\}$. The logical connectives $\land$, $\lor$ and $\neg$ represent ``\textbf{and}'', ``\textbf{or}'' and ``\textbf{not}'' operations, respectively. Quantifiers 
%\twrchanged{
bind
%}
variables within predicates, forming quantified predicates of the form:
\begin{equation}\label{eq:qp}
    Q_1 x_1 Q_2 x_2\ldots Q_n x_n \ldotp p(x_1, \ldots, x_n),
\end{equation}
where $Q_i\in Q$. Variables bound by quantifiers are termed \emph{bounded}; while those not bound are \emph{free}. A predicate with all variables substituted by concrete values is called a \emph{concrete} predicate.

The validity of quantified predicates serves as an abstraction for validating real-world problems. In the case of predicates where all variables are substituted with specific values, the validity can be unambiguously determined through evaluation of the concrete predicate. However, predicates containing free variables have indeterminate validity. For example, $x + 2 = 5$ is neither true nor false due to the unknown value of $x$. The validity of fully bounded quantified predicates can be ascertained within a predefined domain. For instance, given $X = \{2,3\}$, $\exists x\ldotp x+2=5$ is true while $\forall x\ldotp x+2=5$ is false. Conversely, if $X=\{3\}$, then $\forall x\ldotp x+2=5$ is true.


\subsection{Computational problems}\label{sec:comp}
In this section, we introduce an abstraction of computational problems through the framework of quantified predicates. We focus our attention on~\Cref{eq:qp}, where $P(x_1, \ldots, x_n)$ is an efficiently decidable predicate. The notion of efficient decidability in this context implies that for any substituted values $c_1, \ldots, c_n$, the evaluation of $P(c_1, \ldots, c_n)$ can be executed within polynomial time relative to the encoding size of $c_1, \ldots, c_n$.
%\twrchanged{
As is well known,
%}
the formulation presented in \Cref{eq:qp} possesses sufficient expressive power to abstract several well-known classical computational problems
%\twrchanged{
\cite{DBLP:books/daglib/0095988}.
%}

\paragraph{$\np$ class}The $\np$ complexity class represents a fundamental category in computational-complexity theory. Problems within $\np$ can be formulated as decision problems concerning the validity of existentially quantified predicates. A paradigmatic example of this class is SAT, which holds a central position as an $\np$-complete problem. The SAT problem can be expressed in the following form:
\begin{equation}\label{eq:np}
   \exists x\ldotp P(x), 
\end{equation}

where $P(x): \phi(x)=1$ denotes a Boolean formula $\phi$ evaluating to true. In this formulation, the SAT problem encapsulates the question of whether, for any given Boolean formula $\phi$, there exists a solution that renders $\phi$ true.

\paragraph{$\conp$ class}A complexity class closely related to $\np$ is the $\conp$ class. While $\np$ problems are characterized by existential quantification, $\conp$ problems are formulated using universal quantification, represented as:
\[
\forall x\ldotp P(x),
\]
where $P(x): \phi(x)=1$ denotes an arbitrary Boolean formula $\phi$ evaluating to true. The canonical $\conp$ problem is the \emph{tautology} problem, which inquires whether a given Boolean formula holds true for all possible assignments. % It is noteworthy that $\conp$ and $\np$ problems exhibit a duality relationship. This duality is manifested in the nature of their respective inquiries: $\np$ problems ask whether a solution exists for a given problem, while $\conp$ problems investigate the non-existence of counterexamples.

In fact, more complex quantifier patterns beyond single existential or universal quantifiers can be employed to bound variables. These predicates lead to computational problems hypothetically harder than $\np$ and $\conp$ problems, such as those in the polynomial hierarchy and $\pspace$-hard problems. For a more comprehensive treatment of these topics, readers are directed to~\citet{arora_complexity}.
%than  polynomial hierarchy extends $\np$ and $\conp$ by incorporating multiple alternating quantifiers, such as $\exists x_1 \forall x_2\ldotp P(x_1, x_2)$ and $\forall x_1 \exists x_2 \forall x_3\ldotp P(x_1, x_2, x_3)$, encompassing problems of increasing complexity. Computational problems like the \emph{succinct set cover} and \emph{VC-dimension} reside at higher levels of this hierarchy~\citep{arora_complexity}, hypothesized to be strictly harder than $\np$ and $\conp$, assuming the non-collapse of the polynomial hierarchy. %Our evaluation includes optimal planning problems that require encoding beyond a single quantifier (see~\cref{sec:planning}), thus transcending typical $\np$ and $\conp$ problems.


\subsection{Algorithmic solution}\label{sec:algo}
Having established~\cref{eq:qp} as a general formulation for computational problems, we introduce an algorithmic approach to determine their validity. Without loss of generality,
Let $X = \{c_1, c_2\}$ and $P: X^2\rightarrow \{0, 1\}$ and assume that the evaluation of each concrete predicate $P(c_i,c_j)$ is computationally efficient. Under these assumptions, it becomes theoretically feasible to exhaustively enumerate all legitimate substitutions to evaluate the validity of the problem. The algorithmic procedure for this approach is outlined as follows:
\begin{enumerate}
[parsep=0pt,itemsep=1pt,leftmargin=13pt]
\item For an existentially quantified predicate:
$\exists x \in X^2 \ldotp P(x) = P(c_1, c_1) \lor P(c_1, c_2) \lor P(c_2, c_1) \lor P(c_2, c_2)$.
\item For a universally quantified predicate:
$\forall x \ldotp P(x) = P(c_1, c_1) \land P(c_1, c_2) \land P(c_2, c_1) \land P(c_2, c_2)$.
\end{enumerate}
For more general finite domains and quantifier domains, we can unravel the quantifiers sequentially according to the leading quantifier and ultimately resolve the validity of the quantified predicate based on all concrete predicate evaluations. In complexity theory, this result is known as $\pspace\subseteq\expt$~\citep{arora_complexity}. %For example:
%\begin{equation*}
%\begin{split}
%\exists x_1 \forall x_2\ldotp P(x_1, x_2) = [\forall  x_2 \ldotp P(c_1, x_2)] \lor [\forall x_2 \ldotp P(c_2, x_2)] \\ =
%[P(c_1, c_1) \land P(c_1, c_2)]\lor [(P(c_2, c_1) \land P(c_2, c_2)]
%\end{split}
%\end{equation*}


%Although exhaustive enumeration may initially appear computationally prohibitive, numerous real-world problems are characterized by relatively small, fixed domains, often encompassing fewer than several thousand enumeration candidates. As a result, the enumeration of all possible candidates emerges as a viable approach for many practical applications. 
This algorithmic solution represents a worst-case scenario, exhibiting exponential-time complexity. Given the expressive power of predicates in general computational problems,
%\twrchanged{
this approach may be no worse asymptotically than
%}
the theoretical optimum achievable.
%\twrchanged{
The approach thus
%}
aligns with the Exponential Time Hypothesis~\citep{SETH}, which posits that even for the SAT problem—a seemingly simple instance of~\cref{eq:qp}—algorithms cannot surpass the performance of brute-force methods, inevitably resulting in exponential time complexity. 
However, it is noteworthy that in many practical applications, the solution enumeration space is typically constrained to a magnitude of a few hundred thousand, rendering the approach computationally feasible within these bounds. Moreover, in numerous applications, the enumeration space can be optimized through the pruning of ineffective candidates, thereby enhancing the efficiency of the algorithmic process.

%It is imperative to emphasize that the present analysis primarily focuses on obtaining perfect solutions in worst-case scenarios. However, it is important to acknowledge that in practical applications, approximate solutions may be deemed acceptable, particularly when confronted with expansive solution search spaces. In scenarios where exact solutions are computationally intractable or unnecessary, heuristic methodologies may be employed. For instance, Monte Carlo tree search algorithms represent one such approach that could be considered for addressing problems with large search spaces. These methods offer a balance between solution quality and computational efficiency, often providing satisfactory results in some practical settings.




%\paragraph{Note} In~\cref{eq:qp}, we require $p$ to be efficiently decidable, i.e., can be checked within polynomial time. This is technically 

\subsection{Program synthesis}\label{sec:prog}
Our objective is to accurately solve real-world computational problems. In accordance with the algorithmic resolution presented in~\cref{sec:algo}, we aim to enumerate all concrete predicates. This approach necessitates the fulfillment of two critical conditions:
\begin{enumerate}
[parsep=0pt,itemsep=1pt,leftmargin=13pt]
\vspace{-0.2cm}
    \item The evaluation of the predicate with specific value assignments must consistently yield accurate results.
    \item The enumeration process must be exhaustive, encompassing all legitimate solution candidates without omission.
\end{enumerate}
To optimize efficiency, additional considerations are warranted. Ideally, the enumeration process should minimize evaluations of repetitive or ineffective candidates. Furthermore, each individual predicate evaluation should be executed in a time-efficient and space-efficient manner.
These requirements naturally suggest
%\twrchanged{
the use of program synthesis.
%}

Contemporary computer architectures offer highly efficient program execution, facilitating precise and complete enumeration with accurate predicate evaluation. Our task thus reduces to synthesizing programs capable of evaluating all enumerations and deriving the validity of quantified predicates as expressed in~\cref{eq:qp}. %We have selected Python for this task due to its: (i) high-level nature and syntactic clarity, enhancing readability;
%(ii) structural similarity to natural language, \twr{What?} easing problem-to-code translation;
%(iii) extensive representation in LLM training data, enabling proficient code synthesis.
%\twr{I don't see the need to present any of (i), (ii), and (iii).}
%
Our methodology for Python program synthesis, aligned with our formal framework, comprises three key functions:
\begin{enumerate}
[parsep=0pt,itemsep=1pt,leftmargin=13pt]
\vspace{-0.2cm}
    \item Predicate evaluation: This function assesses whether a concrete solution candidate successfully resolves the problem;
    \item Enumeration synthesis: This function generates all legitimate solution candidates, with the capability to prune those known to be ineffective;
    \item Aggregation of results: This function orchestrates the overall synthesis process. It invokes the enumeration function to generate candidates, utilizes the predicate function to evaluate these candidates, and returns results based on the quantifiers.
\end{enumerate}
%\twr{Don't we mean Predicate identification, Enumerator synthesis, and Solving?}

We term this approach the Predicate-Enumeration-Aggregation (\textbf{PEA}) framework. LLMs are instructed to synthesize these components, and the resulting programs are executed on specific problem instances to derive solutions.

\begin{algorithm}[tb]
   \caption{PEA Program Synthesis Algorithm}
   \label{alg:pea}
\begin{algorithmic}[1]
   \State {\bfseries Input:} A computational problem $P$, an LLM $M$, an iteration upper bound $m$, and a Boolean option for enumeration optimization $op$.
   
   {\bfseries Output:} Synthesized code $C$ to solve $P$.
   \State Prepare template $T$ incorporating $P$'s description and PEA components.
   \If{$op$ is True}
   \For{$i=1$ {\bfseries to} $m$}
   \State Query $M$ with $P$ for enumeration-space reduction strategy and integrate strategy into $T$ to create augmented template $T'$.
   
   \State Query $M$ to synthesize code $C'$ with $T'$.
   \If{$C'$ passes code integrity check}
    \State \textbf{\emph{Return}} $C'$.
   \EndIf
   \EndFor
   \EndIf
   \For{$i=1$ {\bfseries to} $m$}
   \State Query $M$ to synthesize code $C$ with $T$
   \If{$C$ passes code integrity check}
    \State \textbf{\emph{Return}} $C$.
    \EndIf
    \EndFor
   \State \textbf{\emph{Return}} an empty program.
   %\UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}
%This program synthesis approach can be interpreted as using LLMs as abstractors, facilitating the translation of computational tasks from natural language descriptions into programmatic representations. Concurrently, it employs program execution as the primary reasoning mechanism. This approach offers several benefits: Given the inherently computational nature of these tasks, the reasoning process frequently necessitates extensive enumeration. Such enumeration, when expressed in textual form, can be prohibitively lengthy and inefficient.  Direct reasoning using LLMs for these tasks can be monetarily expensive due to the extensive token usage required for lengthy enumerations. In the meantime, the completeness of enumeration and the accuracy of each predicate evaluation become challenging to verify. However, the translation of natural language described computational tasks into programming language descriptions results in more concise textual representations. This conciseness, coupled with the clear correspondence between natural language and code, significantly simplifies the process of correctness verification. 

\subsection{Implementation}\label{sec:implementation}
The PEA framework implementation employs a structured-prompt methodology, combining natural-language problem descriptions with specific Python code-generation instructions. These instructions detail function names and input/output descriptions. For problem instances that cannot be represented by primitive Python types, the LLM is directed to synthesize compound types, encapsulated as Python classes, to describe the problem structure. %The prompt also incorporates CoT to enhance the code-generation process.

Users have the option to prompt the model for autonomous generation of pruning instructions during enumeration synthesis, based on the natural language problem description. This approach enhances system efficiency and autonomy in problem-solving tasks, reducing human intervention.

Code integrity is checked through syntactic and semantic validation. Syntactic checks employ Python's abstract syntax tree (AST) module to confirm function presence, input/output types, non-empty definitions, and correct problem-instance conversion. Semantic checking executes the generated code with a fixed example (either provided by or sampled from the dataset) and compares the output to the expected result. Successful code synthesis requires passing all these validation checks, as outlined in~\cref{alg:pea}. It is noteworthy that the synthesis of optimized enumeration functions presents increased complexity due to the intricate nature of pruning instructions. Consequently, in instances where the model fails to successfully synthesize code incorporating these pruning instructions, it defaults to the generation of naive enumeration functions. % This approach balances the complexity of pruned enumeration with the reliability of exhaustive search, ensuring robust problem-solving capabilities across various scenarios.




%\paragraph{Note} Recent research has explored the use of programs to enhance the reasoning capabilities of LLMs. From the formal perspective presented in our paper, these works primarily address propositional logical tasks, specifically the evaluation of substituted predicates. Notably, several recent advancements in prompt engineering have demonstrated comparable performance, suggesting that program-aided LLM reasoning for evaluating substituted predicates may not be essential given the current state of LLM development. In contrast, our work focuses on computational problems involving the determination of validity for quantified predicates. The worst-case perfect resolution of these problems is hypothesized to require brute-force enumeration. Given the current state of LLM development, we posit that these problems necessitate program execution for comprehensive resolution. We provide a more detailed comparison in the appendix, elucidating the nuances between our method and existing approaches in the field of program-aided language models.


%\paragraph{Note}abstraction + partial evaluation and correctness checkingThe primary aim of our indirection approach is to develop an executable program that can systematically enumerate all possible solution candidates and validate the correctness of each candidate. The validation process is divided into two categories: \emph{local} and \emph{global}. Local validation checks whether a proposed solution conforms to valid structural forms, ensuring that each component adheres to predefined syntactical rules. Conversely, global validation assesses whether the proposed solution effectively solves the computational problem.
