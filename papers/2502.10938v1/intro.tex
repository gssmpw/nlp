\section{Introduction}

Large Language Models (LLMs) have exhibited significant generalization capabilities across diverse domains, prompting investigations into their potential as generic reasoning engines. Recent studies have explored inference-time computation techniques~\citep{welleck2024from,snell2024scalingllmtesttimecompute}, particularly prompt engineering methods such as Chain-of-Thought (CoT), to enhance LLM performance on complex reasoning tasks~\citep{wei2022chain}. These approaches have successfully improved model performance and expanded LLMs' practical applications. However, despite the growing focus on enhancing model capabilities through inference-time computation for complex reasoning tasks, the current literature lacks a formal framework to precisely describe and characterize the complexity of reasoning problems.


This study identifies a class of reasoning problems, termed \emph{computational reasoning} problems, which are particularly challenging for LLMs~\citep{yao2023tree,hao2024planningrigorgeneralpurposezeroshot,valmeekam2023planbench},
such as planning problems and arithmetic games.
Informally, these problems can be accurately described using succinct programmatic representations.
We propose a formal framework to describe and algorithmically solve these problems.

The framework employs first-order logic, equipped with efficiently computable predicates and finite domains. A first-order logic formula~\citep{kleene1971introduction},
is represented syntactically as:
\[
Q_1 x_1 Q_2 x_2\ldots Q_n x_n \ldotp P(x_1, \ldots, x_n),
\]
where $Q_i$'s are quantifiers and $P(\cdot)$ is a predicate.

To illustrate, the statement ``The sum of 
%\twrchanged{
an odd number and an even number
%}
is odd'' can be expressed as:
\begin{equation*}
\begin{split}
(\forall x, y, z ) (\exists l, m, n ) \ldotp & [x = 2l \land y = 2m+1 \land z = x+y] \\ &\rightarrow z=2n+1
\end{split}
\end{equation*}
where all $x, y, z, l, m, n \in \mathbb{Z}$, the integer set.

%\twrchanged{
As used in this paper, additional
%}
restrictions are enforced, limiting variables to finite domains and ensuring predicates are efficiently evaluable. While these limitations preclude the representation of certain mathematical propositions, such as those involving infinite sets like $\mathbb{Z}$, they facilitate algorithmic resolution of problem correctness through exhaustive enumeration and aggregation of solution candidates. Despite reduced expressiveness, this approach remains sufficiently powerful for describing numerous real-world problems (see~\cref{sec:comp}). To illustrate, we consider the Boolean Satisfiability (SAT) problem~\citep{Karp1972}, formally expressed as:
\[
\exists x\in\{0, 1\}^n\ldotp B(x)=\text{True},
\]
where $B(x)$ represents a Boolean logic formula.  The SAT problem determines whether a satisfying assignment exists for $B$.
\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth]{PEA_WF.pdf}
    %\vspace{-100pt}
    \caption{PEA Pipeline: Given a problem $P$ described in natural language, the PEA section of the prompt instructs the LLM to synthesize problem-solving programs. Programs passing sanity checks are executed locally to obtain solutions for $P$. Users have the option to query the LLM with $P$ to obtain search-space optimization strategies.}\label{fig:pipeline}
    \vspace{-7pt}
\end{figure*}
To ascertain the validity of $\exists x\in\{0, 1\}^n\ldotp B(x)=\text{True}$, a brute-force approach enumerates all possible candidates $c$ from $\{0, 1\}^n$ and evaluates $B(c)$.


Given this formal framework for describing computational problems, we propose a method to solve real-world problems using LLMs (see~\cref{sec:algo}). 
Our approach employs
%\twrchanged{
an algorithm that performs
%}
exhaustive enumeration of
%\twrchanged{
valid candidate values for variables bounded by quantifiers,
%}
and comprehensive predicate evaluation. This strategy inherently aligns with programmatic implementation, leveraging programs' intrinsic capacity for precise evaluation and thorough enumeration when correctly formulated.
We introduce the Predicate-Enumeration-Aggregation (PEA) framework, which operates as follows:
(i) the predicate, enumeration,
%\twr{Why is the middle phrase now ``variable scope'' rather than ``enumeration''?}
and aggregation rules are described in natural language;
(ii) LLMs are instructed to generate programs that fulfill these natural-language tasks; and
(iii) the synthesized programs are executed to determine the validity of the quantified predicate (see~\cref{sec:prog,sec:implementation}).
\Cref{fig:pipeline} illustrates the pipeline of the PEA framework.



This algorithmic framework can be conceptualized as employing a brute-force, trial-and-error approach to problem-solving. Due to the expressiveness of PEA, the algorithm may be optimal asymptotically given the hypothesized essential lower-bounds in many computational problems~\citep{SETH,fgc}, potentially necessitating extensive enumerations. The PEA framework leverages computational efficiency in programmatic predicate evaluation, facilitating rapid assessment of millions of simple predicates within seconds. This approach capitalizes on the high performance of modern computer architectures and code optimization techniques, rendering execution computationally inexpensive and expanding the feasible search space. Thus, this methodology is applicable to various real-world applications with moderate search spaces. In this context, programs function as reasoning generators, with their execution producing concrete reasoning outcomes.
%
One motivation is that for large problem instances, the actual reasoning often exceeds the algorithmic description in length, due to the hypothetical computational lower bound. This phenomenon facilitates easier synthesis of reasoning generators compared to direct reasoning. Furthermore, the framework's efficiency can be enhanced through strategic pruning of ineffective candidates within the enumeration space. 
Consequently, the PEA framework transforms lengthy enumerations into succinct programmatic representations, augmenting LLMs' reasoning capabilities by leveraging their coding proficiency. 

In summary, our work makes the following contributions: %\zw{run another check}:
\begin{itemize}[parsep=0pt,itemsep=1pt,leftmargin=13pt]
    \item The development of PEA, a formal framework grounded in first-order logic, which facilitates the categorization of computational-reasoning problems.
    \item The introduction of a novel prompt-engineering technique, derived from the PEA framework, designed to augment the capabilities of LLMs in addressing complex computational tasks via their coding capability.
    \item  Empirical evaluation demonstrating that PEA substantially enhances the performance of underlying models, yielding an average accuracy improvement of approximately $50\%$ while enhancing efficiency.
\end{itemize}