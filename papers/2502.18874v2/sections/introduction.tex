\section{Introduction}
\begin{figure}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/example.pdf}
	\caption{Comparison of previous fine-tuned evaluators and our framework. \textbf{Left} is a former model and \textbf{Right} is our ARJudge. The Analyzer adaptively defines evaluation criteria and conducts multi-faceted analyses in various forms, e.g., text or code. The Refiner combines all preceding analyses and produces the final evaluation.}
	\label{intro_example}
\end{figure}

The rapid advancement of Large Language Models (LLMs) has highlighted the critical need for robust output evaluation methods \cite{survey_judge}. While proprietary models like GPT-4 have emerged as predominant evaluation approaches given their superior capabilities, transparent and controllable considerations have driven research toward fine-tuning open-source LLMs for evaluation tasks \cite{prometheus, prometheus2}. Recent work has established the viability of open-source alternatives by training LLMs to replicate the evaluation explanations and judgments of proprietary models \cite{critiqueLLM, x-eval, themis, prometheus2}.

However, existing fine-tuned evaluators rely solely on text-based analysis with predefined  evaluation criteria, leading to two key limitations \cite{auto-j, themis, judgelm, prometheus2}. 
First, evaluation based on predefined criteria can not fully capture the nuanced task requirements. %, resulting in a lack of contextual sensitivity. 
For example, general criteria for writing, such as conciseness or logical structure, may not be sufficient for evaluating creative writing tasks that require an engaging plot. Moreover, it is challenging to effectively adapt predefined criteria to new and diverse instructions \cite{auto-j}. 
Second, LLM-based evaluators demonstrate significant instablity in evaluating adherence to complex instruction requirements, particularly objective criteria such as quantitative or structural constraints \cite{ifeval}. For instance,  they struggle to reliably assess basic textual attributes such as word counts, a common requirement in writing-related instructions \cite{llm_bad_word_level}. These limitations also extend to the evaluation of formatting constraints. 

In this work, we argue that developing robust fine-tuned evaluators requires the ability to adaptively generate evaluation criteria and conduct multi-faceted analyses \cite{branch-merge}. These abilities enhance the evaluators' comprehensive performance in both what to evaluate and how to evaluate. Even for unseen instructions, the evaluators can define tailored criteria and assess instructions with nuanced precision.   
Furthermore, evaluators should use automated tools to assess objective requirements \cite{mint}. These tools provide reproducible feedback, offering reliable verification that helps overcome LLMs' inherent limitations in objective evaluation. 

To address these challenges, we propose ARJudge, a novel evaluation framework that combines adaptive criteria generation with text-based and code-driven analysis generation to comprehensively assess LLM outputs. ARJudge comprises two core components: (1) an Analyzer that generates multi-faceted evaluation with text-based and code-driven analyses and (2) a Refiner that synthesizes and refines these analyses to produce well-reasoned judgments. 
We train ARJudge on a curated Composite Analysis Corpus, which contains tasks for generating evaluation criteria and performing multi-faceted analyses in both text and code. This corpus enables the Analyzer to learn context-sensitive evaluation logic, such as deriving criteria from instructions and assessing responses accordingly. Extensive experiments across multiple benchmarks demonstrate ARJudge’s superiority and robustness over existing open-source evaluators. Our further analysis validates the necessity and effectiveness of integrating code-driven analyses, which improve accuracy in evaluating instruction following by up to 11.1\% compared to text-only methods. 

The main contributions of this work include:
\begin{itemize}
	\item We propose ARJudge, a novel evaluation framework that combines adaptive criteria generation with text-based and code-driven analyses to evaluate LLM outputs. By incorporating code-driven analytical capabilities, ARJudge extends beyond traditional text-based evaluation approaches.
	\item We develop a training dataset, Composite Analysis Corpus, containing samples for evaluation criteria generation, text-based analyses, and code-driven analyses. It is the first dataset to incorporate multi-faceted analytical samples for evaluator training.
	\item Extensive experiments across multiple benchmarks demonstrate ARJudge’s superior performance over existing fine-tuned evaluators.
\end{itemize}
