\section{Experiments}

% Main results
\begin{table*}[ht!]
\center
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\multirow{3}{*}{Models} & \multicolumn{2}{c}{JudgeLM Eval} & \multicolumn{2}{c}{PandaLM Eval} & \multicolumn{2}{c}{Auto-J Eval} & \multicolumn{2}{c}{MTBench} & \multicolumn{2}{c}{LLMBar} & \multirow{3}{*}{Ave} \\ \cmidrule{2-11} 
                        & Acc             & Agr             & Acc             & Agr            & Acc            & Agr            & Acc          & Agr          & Acc                & Agr  &             \\ \midrule
\textit{Tuning-free}                                                                                                                                                                                      \\ \midrule
GPT-4o                  & 81.8            & 88.1            & 83.1            & 87.5           & 78.6           & 82.5           & 78.8         & 85.4         & 79.8               & 83.4 & 80.4        \\
Claude-3.5-Sonnet       & 82.9            & 86.4            & 86.4            & 91.4           & 78.2           & 85.5           & \textbf{80.8}& 89.1         & \textbf{83.4}      & 90.3 & \textbf{82.3} \\
Deepseek-v3             & \textbf{83.2}   & 85.9            & \textbf{87.4}   & 87.8           & \textbf{82.9}  & 84.2           & 79.7         & 87.0         & 68.6               & 81.6 & 80.4        \\
Qwen2.5-7B              & 80.0            & 78.0            & 80.7            & 79.2           & 73.8           & 65.1           & 75.2         & 72.1         & 52.6               & 65.7 & 72.5        \\ \midrule
\textit{Fine-tuned}                                                                                                                                                                                     \\ \midrule
PandaLM-7B              & 69.9            & 74.7            & 73.1            & 77.8           & 65.2           & 71.0           & 74.0         & 78.4         & 25.9               & 82.5 & 61.6        \\
Auto-J-13B              & 77.9            & 86.6            & 77.2            & 87.2           & \textbf{79.7}  & 87.5           & 75.0         & 84.2         & 27.8               & 83.6 & 67.5        \\
Prometheus2-7B          & 76.5            & 80.3            & 76.3            & 70.9           & 75.1           & 77.2           & 74.3         & 79.5         & 41.5               & 77.6 & 68.7        \\
JudgeLM-7B              & \textbf{81.8}   & 86.0            & 70.3            & 81.4           & 66.1           & 80.2           & 64.6         & 77.1         & 28.1               & 82.0 & 62.2        \\
Themis-8B               & 66.4            & -               & 61.3            & -              & 39.2           & -              & 34.9         & -            & 26.6               & -    & 45.7        \\
ARJudge                 & 81.0            & 83.3            & \textbf{82.4}   & 83.5           & 78.5           & 80.3           & \textbf{78.3}& 81.3         & \textbf{68.2}      & 72.9 & \textbf{77.7}  \\ \bottomrule
\end{tabular}
    }
\caption{Results of different evaluators on the pairwise comparison. ``\textbf{Acc}'' and ``\textbf{Agr}'' denote average accuracy and positional agreement rate. ``\textbf{Ave}'' is the average ``Acc'' across all test sets. The highest average accuracy is marked by \textbf{bold} for two series models, respectively.}
\label{main_result}
\end{table*}

% LLMBar results
\begin{table}[ht!]
\center
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{3}{*}{Models} & \multicolumn{4}{c}{LLMBar}           \\ \cmidrule{2-5} 
                        & Neighbor & GPTInst & GPTOut & Manual \\ \midrule
\textit{Tuning-free}                                             \\ \midrule
GPT-4o                  & 81.0     & 86.4    & 75.5   & 76.1   \\
Claude-3.5-Sonnet       & \textbf{83.2}     & \textbf{87.0}    & \textbf{76.6}   & \textbf{87.0}   \\
Deepseek-v3             & 61.6     & 76.6    & 69.2   & 67.4   \\
Qwen2.5-7B              & 47.0     & 56.0    & 61.7   & 45.6   \\ \midrule
\textit{Fine-tuned}                                            \\ \midrule
PandaLM-7B              & 14.9     & 21.2    & 48.9   & 18.5   \\
Auto-J-13B              & 20.5     & 21.2    & 47.9   & 21.7   \\
Prometheus2-7B          & 25.4     & 31.0    & \textbf{63.8} & 45.6   \\
JudgeLM-7B              & 21.3     & 25.5    & 41.5   & 23.9   \\
Themis-8B               & 20.2     & 32.6    & 31.9   & 21.7   \\
ARJudge                & \textbf{72.4}& \textbf{73.4} & 60.7   & \textbf{67.4} \\ \bottomrule
\end{tabular}
}
\caption{Evaluation accuracy on test subsets of LLMBar series. The highest average accuracy is marked by \textbf{bold}.}
\label{llmbar_result}
\end{table}

\subsection{Implementation Details}
To construct the Composite Analysis Corpus, we prompt GPT-4o to generate evaluation questions for each instruction and collect text-based analysis. Besides, we prompt Claude-3.5-Sonnet to generate Python functions for code-driven objective analysis. We selected Claude-3.5-Sonnet due to its superior performance in code generation. 
We fine-tune Qwen2.5-7B-Instruct \cite{qwen25} on the corpus, creating a model we refer to as the Analyzer for performing multi-faceted evaluations. We use the same model in a zero-shot setting as the Refiner, with carefully crafted prompt templates. All generation in the main experiments is performed using greedy decoding by setting the temperature to 0. Details are described in Appendix \ref{train_detail}.

\subsection{Benchmarks}

We assess our framework on various evaluation datasets. Four human-annotated pairwise evaluation test sets are included: PandaLM Eval \cite{pandalm}, Auto-J Eval \cite{auto-j}, MTBench \cite{mtbench}, and the LLMBar series \cite{llmbar}. These sets were chosen for their broad coverage of evaluation tasks and their diverse set of evaluation criteria. For the LLMBar series, we use four adversarial sets, Neighbor, GPTInst, GPTOut, and Manual, as unseen sets. Unlike the other three sets and our training datasets, where candidate responses are directly sampled based on instructions, the responses in LLMBar are artificially designed to challenge evaluators by incorporating potentially misleading qualities, such as a more engaging tone. One GPT-4-annotated pairwise evaluation set, JudgeLM Eval \cite{judgelm}, is adopted. For all pairwise sets, samples with two equally preferred responses were omitted. Additionally, an instruction-following benchmark, IFEval \cite{ifeval}, is incorporated. We use this benchmark to assess the effectiveness of code-driven analysis.

\subsection{Baselines}

\paragraph{Tuning-free General LLMs}
We compare our framework with several general LLMs that can evaluate response quality. Three powerful LLMs, GPT-4o, Deepseek-v3 \cite{deepseek-v3}, and Claude-3.5-Sonnet, are used due to their balanced and comprehensive performance across most evaluation tasks \cite{sft_eval_limit}. Additionally, the backbone model used for fine-tuning the Analyzer, Qwen2.5-7B-Instruct \cite{qwen25}, is employed to demonstrate improvements. The implementation of closed-source models is done via their respective APIs.

\paragraph{Fine-tuned Evaluators}
We employ five fine-tuned evaluation models that can conduct pairwise evaluation. PandaLM \cite{pandalm} compares two responses and identifies the better one. Auto-J \cite{auto-j} and Prometheus \cite{prometheus2} support both single-response scoring and pairwise response comparison. Themis \cite{themis} rates each response based on various criteria and determines the better one by comparing their scores. JudgeLM \cite{judgelm} provides a comparison of two responses along with their corresponding scores. We use official models with 7B parameters for PandaLM, Prometheus, and JudgeLM, and models with 13B and 8B parameters for Auto-J and Themis, respectively.

\subsection{Main Results}

The main comparative results against baseline methods are shown in Table \ref{main_result}. Following \citet{llmbar} and \citet{auto-j}, we calculate the accuracy of the pairwise preference evaluation with and without swapping the two candidate responses, respectively. The average accuracy and the positional agreement rate are displayed as \textbf{Acc} and \textbf{Agr}. The performance in LLMBar is the average of its four subsets. We observe that ARJudge surpasses all fine-tuned evaluators of similar model sizes. Notably, on the challenging LLMBar set, ARJudge outperforms the best fine-tuned baseline, Prometheus2-7B, by 26.7\%. Even without more exposure to challenging samples like LLMBar, ARJudge achieves an average 15.6\% improvement over its backbone model, Qwen2.5-7B-Instruct. Additionally, ARJudge's performance is comparable to that of powerful tuning-free LLMs on some test sets. For example, ARJudge performs on par with GPT-4o and Claude-3.5-Sonnet on Auto-J Eval and with DeepSeek-V3 on LLMBar. Besides, compared to other fine-tuned methods, ARJudge can generalize to more test sets.

Table \ref{llmbar_result} further presents detailed evaluation results in different subsets of LLMBar. Our ARJudge performs the best on most subsets and has made significant improvements compared to the backbone model, Qwen2.5-7B-Instruct. On LLMBar-Neighbor, it achieves higher evaluation accuracy than the advanced DeepSeek-V3. 

\subsection{Ablation Study}
\begin{table}[t!]
\center
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule
Models         & JudgeLM  & PandaLM & Auto-J & MTBench & LLMBar \\ \midrule
Qwen2.5-7B     & 80.0  & 80.7    & 73.8   & 75.2    & 52.6   \\ \midrule
ARJudge        & 81.0 & 82.4    & 78.5   & 78.3    & 68.2   \\
-w/o FT        & 73.1 & 75.6    & 68.7   & 70.0    & 62.5   \\
-w/o FT\&MF    & 74.7 & 72.2    & 65.6   & 67.8    & 63.7   \\
-w/o Refine    & 81.7 & 82.8    & 79.6   & 79.1    & 63.7   \\ \bottomrule
\end{tabular}
}
\caption{Comparison results under ablation settings. ``JudgeLM'', ``PandaLM'', and ``Auto-J'' are abbreviation of the associated testsets. ``\textbf{FT}'' and ``\textbf{MF}'' represent fine-tuning and multi-faceted.}
\label{ablation}
\end{table}

To further investigate the effectiveness of our framework, we analyze several variations of ARJudge, as detailed below. 
(1) \textbf{w/o FT}: we replace the fine-tuned Analyzer with the same tuning-free model as the Refiner and prompt the model to generate evaluation questions and conduct the multi-faceted evaluation. 
(2) \textbf{w/o FT\&MF}: we apply the model as in the w/o FT setting, generating Chain-of-Thought (CoT) evaluations directly. 
(3) \textbf{w/o Refine}: we retain the fine-tuned Analyzer and make slight modifications to the prompt for the Refiner to directly output the label of the better response. 

The ablation results are shown in Table \ref{ablation}. We observe accuracy drops across all test sets with the ablation variants, indicating the effectiveness of each component in ARJudge. Specifically, fine-tuning significantly enhances a general LLM’s evaluation capability, enabling it to propose reasonable evaluation questions and analyze responses accordingly. Evaluation questions help the LLM focus on relevant aspects and enhance its evaluation performance. Interestingly, we find that the effects of refinement differ between the fine-tuned and tuning-free Analyzer. In JudgeLM Eval, PandaLM Eval, Auto-J, and MTBench, the refinement keeps evaluation accuracy under the fine-tuned Analyzer’s analysis (w/o Refine vs. ARJudge) but significantly decreases it under the tuning-free Analyzer’s analysis (Qwen2.5-7B vs. w/o FT\&MF). It may be related to the controversial phenomenon that LLMs cannot truly self-correct \cite{self-correct}. Additionally, for challenging samples in LLMBar, refinement significantly strengthens the performance of the fine-tuned and tuning-free ones.

\subsection{Capability to Evaluate Using Code}

Code-driven analysis plays a crucial role in robustly verifying the objective requirements of instructions. To assess the effectiveness of code-driven analysis, we use the execution results of the IFEval official code as a benchmark and compute the \textbf{Consistency} between its judgment (Loose or Strict) and that of other models. We compare ARJudge with GPT-4o, Claude-3.5-Sonnet, and Qwen2.5-7B-Instruct. These three models are prompted to make judgments in a zero-shot manner. As shown in Figure \ref{code_eval}, our framework achieves a significant improvement over the backbone model, Qwen2.5-7B-Instruct, with the help of code-driven analysis. Moreover, ARJudge performs comparably to GPT-4o and Claude-3.5-Sonnet, demonstrating its potential as a viable alternative. Notably, the execution success rate of the generated code is 100\%. 

\subsection{Effect of Increasing Analysis Quantity}\label{effect_q}

We extend our analysis by scaling up the number of question sampling attempts, exploring the effect of increasing analysis quantity. We set the temperature to 0.2 to sample evaluation questions multiple times, ensuring diversity in the generated questions. As shown in Figure \ref{num_questions}, evaluation accuracy improves with more analyses for most datasets, including JudgeLM Eval, Auto-J Eval, PandaLM Eval, and MTBench. The highest accuracy is achieved with four or five rounds of question sampling and their combined analysis. However, in the LLMBar series, additional analysis had little or even a negative impact on accuracy. This may be because the Analyzer has greater uncertainty about the evaluation samples in these sets, and additional analysis further amplifies this uncertainty.

\subsection{Generalization of Evaluation Capability}\label{effect_refine}

To further demonstrate the generalization of evaluation capability, we compute the ratio of judgment change after refining as shown in Table \ref{refine}. Combining Table \ref{ablation} and \ref{refine}, we observe that the Refiner maintains evaluation performance in JudgeLM Eval, PandaLM Eval, Auto-J Eval, and MTBench, while significantly increasing it in the LLMBar series. This indicates that re-analysis improves the generalization of evaluation capability, especially in handling unseen challenging samples.

\section{Case Studies}

\begin{figure}[t!]
	\centering
	\includegraphics[width=1\linewidth]{figures/code_results.pdf}
	\caption{Results on the consistency between code-driven evaluation and IFEval evaluation. ``Loose'' and ``Strict'' are two judgment criteria in IFEval.}
	\label{code_eval}
\end{figure}

\begin{figure}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/num_question_combined.pdf}
	\caption{Evaluation results with increasing analyses. The right displays the results of four subsets in LLMBar.}
	\label{num_questions}
\end{figure}

\begin{table}[t!]
\center
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule
Models           & JudgeLM  & PandaLM & Auto-J & MTBench & LLMBar \\ \midrule
W$\rightarrow$C  & 3.9 & 4.4    & 2.3   & 2.1    & 7.8   \\
C$\rightarrow$W  & 4.6 & 4.8    & 3.4   & 2.9    & 3.6   \\
\bottomrule
\end{tabular}
}
\caption{Ratio of change after refining. ``W$\rightarrow$C'' denotes a judgment changing from wrong to correct after refinement, while ``C$\rightarrow$W'' denotes the opposite.}
\label{refine}
\end{table}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/case.pdf}
	\caption{An example of evaluation generated by ARJudge.}
	\label{case_study}
\end{figure*}

We show an example of a multi-faceted evaluation generated by ARJudge in Figure \ref{case_study}. Given an instruction and two responses, the Analyzer first generates three evaluation questions and the corresponding multi-faceted analyses. The last question is analyzed by constructing a Python function and assessing execution feedback to determine requirement completeness. Then, the Refiner reviews the preliminary analysis and refines it by reconsidering the instruction's requirements. 