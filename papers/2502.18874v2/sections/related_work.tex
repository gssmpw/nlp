\section{Related Work}
\subsection{Tuning-Free Generalist Evaluators}
Tuning-free generalist evaluators leverage the inherent capabilities of large language models (LLMs) to assess responses through the use of carefully designed prompts, offering exceptional flexibility and scalability. Various techniques have been employed to enhance the accuracy of these evaluations, such as in-context learning \cite{Fu2023GPTScoreEA,lin2023llm}), adding task-specific criteria~\cite{kotonya2023little,zhuo2024ice}, and Chain-of-Thought analysis~\cite{liu2023g,zhuo2024ice}). 

Despite their versatility, tuning-free evaluators often suffer from biases such as position bias~\cite{raina2024llm,wang2023large,zheng2023judging} and verbosity bias~\cite{khandebating,ye2024justice}, which can skew evaluation outcomes. Methods like response-adapted references~\cite{zhang2024reviseval}, multi-agent collaboration~\cite{xu2023towards}, and divide and conquer~\cite{branch-merge,li2023split} have been proposed to mitigate these issues, improving the fairness and reliability of LLM-based evaluations.

\subsection{Specialized Fine-Tuned Evaluators}
While tuning-free approaches provide flexibility, specialized fine-tuned evaluators are explicitly trained on human-labeled preference data to achieve higher accuracy and domain-specific reliability. These models undergo supervised fine-tuning or reinforcement learning-based optimization to align their evaluations more closely with expert judgments~\cite{auto-j,pandalm,prometheus,prometheus2, sorry-bench}.

While fine-tuned evaluators offer improved accuracy, they face notable challenges in scalability and generalization~\cite{sft_eval_limit}. Unlike tuning-free approaches, which can adapt to new tasks with minimal configuration, fine-tuned models require ongoing updates through methods such as supervised fine-tuning or direct preference optimization~\cite{rafailov2024direct}. To remain effective amidst evolving benchmarks~\cite{mtbench,llmbar}, Auto-J~\cite{auto-j} leverages a large dataset of scoring and preference annotations while incorporating dynamic in-context learning techniques, such as few-shot prompting, to enhance adaptability. Similarly, FLAMe~\cite{vu2024foundational} combines fine-tuning on labeled preference data with large-scale multitask instruction tuning, enabling it to dynamically adapt to new evaluation criteria while maintaining flexibility.

