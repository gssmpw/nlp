\section{Conclusion}

This work proposes a novel evaluation framework, ARJudge, which adaptively designs evaluation criteria and performs multi-faceted evaluation in both text and code. A new Composite Analysis Corpus, designed for both criteria generation and multi-faceted analysis, is developed to train ARJudge. Extensive experiments demonstrate the superiority and robustness of our framework across diverse evaluation benchmarks. Notably, with code-driven analyses, ARJudge gains strong evaluation capabilities for assessing instruction following. Future studies can explore the effective use of more tools, such as a search engine, to improve evaluation honesty and mitigate hallucination.
