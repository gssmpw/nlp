\section{Composite Analysis Corpus}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/corpus.pdf}
	\caption{The overview of the corpus construction. ``\textbf{R1}'' and ``\textbf{R2}'' denote two candidate responses with a preference annotation. ``\textbf{Sample Responses}'' are newly sampled responses that we use as references to generate evaluation questions and code scripts. Step (1) produces two types of evaluation questions, respectively. Step (2) and Step (3) develop corresponding text-based and code-driven analyses.}
	\label{corpus}
\end{figure*}

Collecting comprehensive and detailed evaluation analysis data is essential for fine-tuning an LLM to improve evaluation performance \cite{auto-j, themis}. Previous studies \cite{auto-j, themis, prometheus, prometheus2} focus exclusively on text-based analysis with predefined general evaluation criteria, showing limited generalization and robustness \cite{sft_eval_limit}. To address these limitations, we develop a composite analysis corpus to improve LLMs' ability to determine what to evaluate and how to evaluate effectively. The process of constructing the corpus involves three steps: (1) establishing evaluation criteria specifically for each instruction (ยง\ref{subsec:criteria}), (2) conducting text-based analyses to assess responses using multiple criteria (ยง\ref{subsec:text}), and (3) designing code-driven analyses to assess whether responses satisfy the objective requirements of the instructions (ยง\ref{subsec:code}). 

First of all, we collect a large set of instructions from publicly available preference datasets based on \citet{auto-j}. These datasets \cite{mtbench, webgpt, gptj-pair, saferlhf} consist of preference pairs of LLM-generated responses to identical instructions. Each pair is annotated with a preference label that identifies the better response. In line with \citet{auto-j}, non-English instructions and multi-turn interactions are removed. Then, we establish multiple evaluation criteria for each instruction.

\subsection{Establishing Evaluation Criteria}\label{subsec:criteria}
We define the evaluation criteria in the form of concise questions \cite{llmbar, prometheus2}. Each question describes one aspect that a high-quality response should fulfill. For example, responses to the instruction ``Draft an email to my deputy chairperson requesting their signature on the attached approval letter in a professional and polite manner'' can be evaluated using the following three questions: `\textit{`1. Does the response include a polite and professional request for the deputy chairperson to sign the attached approval letter? 2. Does the response mention the attached approval letter and provide the necessary details about it? 3. Does the response offer assistance with any questions or clarifications the deputy chairperson might have about the approval letter?}'' We establish two types of questions by prompting an LLM in a zero-shot manner. \textbf{Type 1} focuses on generating text-based analysis, while \textbf{Type 2} involves generating Python functions and using execution feedback as code-driven analysis.

To generate the first type of question, we prompt an LLM using three sample responses produced by advanced LLMs as well as the instruction $x$. Such sample responses offer a reference understanding of the instruction. The specific prompt is shown in Figure \ref{prompt_question}. We collect three questions $q_{text}$ for each instruction $x$ following \citet{llmbar} and construct training samples in the format $(x, q_{text})$.

For the second type, we must generate new instructions $x'$ with objective constraints in advance, since their proportion in the datasets is relatively low. We use the self-instruct \cite{self-instruct} method to add objective constraints to the instructions and then produce evaluation questions for verifying these constraints. Following the verifiable instructions\footnote{Verifiable instructions are instructions that can be objectively verified for compliance using tools.} summarized by \citet{ifeval}, we first generate several objective constraints for each instruction, such as ``word count'' and ``end with''. The specific prompt is shown in Figure \ref{prompt_constraint}. Then, we randomly select one to three constraints to add to each instruction and collect the corresponding evaluation questions $q_{code}$. The training samples are constructed in the format $(x', q_{code})$.

\subsection{Collecting Text-based Analysis}\label{subsec:text}
We perform pairwise text-based analyses by providing an LLM with the instruction $x$, two responses $r_1$ and $r_2$, and their corresponding evaluation questions $\{ q_{text} \}$. The output necessitates a comparative analysis for each question, followed by a final determination of the better response. The specific prompt is shown in Figure \ref{prompt_text_eval}. We exclude analyses where the final decision contradicts existing human annotations in the datasets. The training samples are constructed in the format $(x \oplus r_1 \oplus r_2 \oplus q_{text}, y_{text})$. Here, $y_{text}$ denotes the associated analysis result for the evaluation question $q_{text}$, which begins with the hint: ``\textit{Let's evaluate whether responses meet the criteria}''.

\subsection{Developing Code-driven Analysis}\label{subsec:code}
To enhance evaluation robustness, we develop code-driven analyses to assess evaluation questions designed to verify objective requirements. The process is completed in two steps: \textbf{Collecting Python Scripts} and \textbf{Reverse Validation}. The first step involves generating Python functions to analyze whether a response satisfies the objective requirement included in an evaluation question. The second step reversely checks whether the generated function's code is designed to analyze the evaluation question. 

\paragraph{Collecting Python Scripts.}
Given three sample responses and one evaluation question $q_{code}$, we prompt an LLM to generate a Python function for verifying the compliance of sample responses. The input of the function is one response, and the output is a comprehensive intermediate of the results related to the evaluation questions. 
To ensure good generalization, the sample responses are a mix of outputs from advanced and weak LLMs. The specific prompt is shown in Figure \ref{prompt_code_eval}. After prompting, we extract the generated Python function using Markdown parsing. We preliminarily filter out invalid code using two checks: 1. The written Python function fails to execute with the provided sample responses as input; 2. The function fails when tested with an additional set of three sample responses. By filtering out invalid code, we ensure that the generated Python functions are executable and generalizable. 

\paragraph{Reverse Validation.}
To further validate whether the generated Python functions fulfill their intended purpose, we design a reverse validation process. Specifically, we first prompt an LLM with the plain text of the evaluation function, requesting an explanation of the expected outputs. Second, we prompt the LLM again to check for consistency between the explanation and its associated question:
\begin{equation}
\begin{aligned}
e &\sim LLM(f, \text{prompt}_{explain}) \\
r &= LLM(e, q_{code}, \text{prompt}_{check})
\end{aligned}
\end{equation}
where $f$ is the evaluation function, $e$ denotes the generated explanation, $r$ indicates whether the explanation is consistent with the question $q_{code}$. The specific prompts are included in Figure \ref{prompt_reverse}. If the function is found to be inconsistent with the aim of the evaluation question, it is discarded. Finally, we collect the effective Python functions and construct training samples in the format $(x' \oplus r_1 \oplus r_2 \oplus q_{code}, y_{code})$. Here, $y_{code}$ represents the Python function $f$ concatenated with the code output hint ``\textit{Let's write a Python function}''. 

\section{ARJudge}
After constructing the corpus, we collect around 25K composite training samples. We fine-tune an LLM based on them and develop \textbf{ARJudge}, a novel evaluation framework that adaptively evaluates LLM-generated responses and integrates both text-based and code-driven analyses. ARJudge consists of two components: a fine-tuned \textbf{Analyzer} and a tuning-free \textbf{Refiner}. Figure \ref{intro_example} presents the overall framework. The Analyzer is trained on the Composite Analysis Corpus to adaptively generate evaluation criteria for any instruction and produce multi-faceted evaluation, including both text-based and code-driven analyses. The Refiner leverages the general LLM's generalist evaluation capabilities to refine the analysis results produced by the Analyzer and make the final judgment. This framework partially preserves the generalist evaluation pattern of the general model while enhancing the evaluation pattern in the fine-tuning dataset.

\subsection{Training}
We train the Analyzer with diverse training samples and tasks, including question generation samples $(x, q_{text})$ and $(x', q_{code})$, text-based analysis samples $(x \oplus r_1 \oplus r_2 \oplus q_{text}, y_{text})$, and code-driven analysis samples $(x' \oplus r_1 \oplus r_2 \oplus q_{code}, y_{code})$. By training on these combined samples, we aim to enhance the LLM's comprehensive analytical capabilities, enabling it to adaptively propose evaluation criteria and conduct multi-faceted analyses. We employ distinct prompt templates for question generation and response analyses, while maintaining a consistent prompt template for both text-based and code-driven analyses. Different forms of analyses are triggered by their respective starting hints. 

\subsection{Evaluation}
Given an instruction $x$ and two responses $r_1$ and $r_2$, the Analyzer first generates several evaluation questions. Then, it performs a comparative analysis of the two responses based on each evaluation question. Notably, the Analyzer autonomously determines whether to generate Python functions according to question characteristics. If the analysis text includes Python functions, the Analyzer will call a Python interpreter to execute them and return the execution feedback as the code-driven analysis results. Finally, the above multi-faceted analysis results are aggregated and sent to the Refiner for further evaluation. We instruct the Refiner to evaluate the above analysis and refine it with a renewed focus on the instruction's requirements. The Refiner will determine which response is better in a zero-shot manner. 