% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% DK
\usepackage{xcolor}
\usepackage{multirow}
% JH
\usepackage{adjustbox}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{16035} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{\LaTeX\ Author Guidelines for \confName~Proceedings}
% \title{360 Indoor Reconstruction}
\title{IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360\textdegree\ Cameras}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Dongki Jung$^{\thanks{These two authors contributed equally.}}$\:\: Jaehoon Choi$^{\footnotemark[1]}$\:\: Yonghan Lee\:\: Dinesh Manocha\\
University of Maryland
}



\begin{document}
\maketitle
% \input{sec/abs_DK}
\input{sec/abs_JH}

%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{introduction}
% \input{sec/intro_DK}
\input{sec/intro_JH}

\section{Related Work}
% \input{sec/related_DK}
\input{sec/related_JH}


\section{Our Method}
% \input{sec/sec_3_0_DK}
\input{sec/sec_3_0_JH}

\begin{figure}[t]
    \centering
    % \vspace*{-5mm}
    \includegraphics[width=1.0\linewidth]{figures/fig2_pipeline_JH.pdf}
    % \vspace*{-2mm}
    \caption{Overview of Our Pipeline. It is comprised of three steps: Spherical Structure from Motion (Sec. \ref{sec:3_1}), Geometry Reconstruction (Sec. \ref{sec:3_2}), and Texture Optimization (Sec. \ref{sec:3_3}). We introduce a spherical SfM framework for estimating camera poses from omnidirectional images in challenging indoor environments. This is followed by geometry reconstruction to generate a 3D mesh, combined with texture mapping and neural texture optimization to enable realistic rendering of the entire scene.}
    \vspace*{-2mm}
    \label{fig:pipeline}
\end{figure}


\begin{figure*}[t]
    \centering
    % \vspace*{-5mm}
    \includegraphics[width=1.0\linewidth]{figures/fig3_method.pdf}
    % \vspace*{-28mm}
    \caption{Our Approach: In Section \ref{sec:3_1}, 
    % \textcolor{red}{EXPLAIN!} 
    spherical SfM is utilized to estimate the camera poses for omnidirectional images.
    Then, in Section \ref{sec:3_2}, our method apply cubemap projection and estimate monocular depth and normal to train geometry $f_{sdf}$ representation using volumetric rendering. In Section \ref{sec:3_3}, we initlize texture using classical texture mapping and jointly optimize diffuse, specular texture along with small MLP for modeling appearance.}
     \vspace*{-2mm}
    \label{fig:method}
\end{figure*}

\subsection{Dense Matching and Structure from Motion}
\label{sec:3_1}
% Dense matching involves establishing dense correspondences and estimating the 3D geometry between two images.
% EDM \cite{EDM} proposes a dense matching algorithm specifically designed for omnidirectional images by leveraging a spherical coordinate system.
% However, dense features often encounter challenges related to inconsistent multi-view correspondences. To address this, detector-free SfM \cite{he2024detector} merges nearby matches to specific pixel grid positions through match quantization. Subsequently, it performs feature track formation and incremental mapping using triangulation and bundle adjustment.
%
% new version
% Large-scale indoor scenes often present significant challenges for localization due to the presence of numerous occlusions and textureless regions, making it difficult to achieve accurate localization using conventional methods such as COLMAP \cite{schonberger2016structure}. To effectively handle occlusions, we utilize a sphere camera that provides a comprehensive omnidirectional field of view. In addition, to address the prevalent issue of textureless regions, we employ a dense matching algorithm for omnidirectional images, EDM \cite{EDM}, which improves matching performance in such challenging environments. However, dense features derived from image pairs often struggle with inconsistencies in multi-view correspondences. Thus, we employ match quantization \cite{he2024detector}, which merges nearby matches into specific pixel grid positions. Subsequently, we perform feature track formation and incremental mapping using spherical triangulation and bundle adjustment \cite{jiang20243d},
 
Large-scale indoor scenes often present significant challenges for camera localization when using conventional SfM methods such as COLMAP \cite{schonberger2016structure} or OpenMVG \cite{moulon2017openmvg}, due to numerous occlusions and textureless regions.
To effectively reconstruct sparsely captured indoor scenes, we integrate a spherical camera model into every step of SfM pipeline, pipelining spherical dense matching, two-view geometry estimation, and optimization methods in a unified and complete 3D reconstruction framework.
%
Instead of using the traditional pinhole camera model and normalized coordinates, we build our pipeline based on the unit bearing vector representation $u \in \mathcal{S}^2$ and a spherical projection mapping $S$, where $S$ projects as $S: P \rightarrow u, \quad \mathbb{R}^3 \mapsto \mathbb{S}^2$. We provide a description of our SfM core components with unified spherical representations.

\noindent\textbf{Spherical Dense Matching}
In our approach, we borrow EDM \cite{EDM}, which extends Dense Kernelized Matching (DKM) \cite{edstedt2023dkm} with a spherical camera model. DKM formulates the feature matching problem as a form of Gaussian Process Regression (GPR) problem, assuming a mapping $f: \varphi \rightarrow \mathcal{X}$ as a Gaussian process to be estimated, where $\varphi$ and $\mathcal{X}$ denotes feature descriptors and pixel coordinate embeddings, respectively. 
%
Given paired images $A$ and $B$, EDM predicts the posterior Gaussian distribution of spherical coordinate embeddings $\mathcal{U}_A \, | \, \varphi_A, \mathcal{U}_B,  \varphi_B \sim \mathcal{N}(\mu, \Sigma)$ conditioned on the observations $(\mathcal{U}_B, \varphi_B)$ from the image $B$, which is formulated as,
% Extending DKM with spherical representations, EDM predicts the posterior Gaussian distribution of spherical coordinate embeddings $\mathcal{U}_A \, | \, \mathcal{U}_B, \varphi_A, \varphi_B \sim \mathcal{N}(\mu, \Sigma)$ from the observations on image $B$, $(\mathcal{U}_B, \varphi_B)$, which is formulated as,
%
\begin{align}
\mu&= K_{AB} (K_{BB} + \sigma_n^2 I)^{-1} \mathcal{U}_B, \\
\Sigma&= K_{AA} - K_{AB} (K_{BB} + \sigma_n^2 I)^{-1} K_{BA},
\end{align}
where $\mu$ is refined through CNN-based warp refiners \cite{EDM, edstedt2023dkm}.

% Extending DKM with spherical representations, EDM replaces the coordinate embedding $\mathcal{X}$ with spherical coordinate embeddings $\mathcal{U} = E(u)$, where $u$ represent bearing vectors $u \in \mathcal{S}^2$ and $E$ is the encoding from bearing vectors to embeding vectors $U \in \mathbb{R}^c$.

% Given image pairs $A$ and $B$, EDM predicts the posterior Gaussian distribution $\mathcal{U}_A \, | \, \mathcal{U}_B, \varphi_A, \varphi_B \sim \mathcal{N}(\mu, \Sigma)$, which is the conditional Gaussian of positional embeddings $\mathcal{U}_A $ from features $\varphi_A$ conditioned on observation on image $B$, $(\mathcal{U}_B, \varphi_B)$


\noindent\textbf{Spherical Two-view Geometry Estimation}
It can be easily proven that the traditional two-view epipolar constraint for normalized coordinates can be extended to unit bearing vectors on spherical images,
%
\begin{equation}
u_1^T  E  u_2 = 0 \; \text{with} \; E = R [t]_\times.
\end{equation}
%
where $E \in \mathbb{R}^{3\times3}$ is essential matrix built from camera rotation $R \in \text{SO(3)}$, and position $t \in \mathbb{R}^3$. 
% As can be seen from the matrix form, bearing vector $u$ corresponds to the left and right null space of $E$ as it aligns with the direction of the normalized coordinates.
%
%
Solarte et al. \cite{solarte2021robust} provides efficient Singular Value Decomposition (SVD) solutions for spherical two-view geometry $E$, incorporating normalization to improve numerical stability. This approach extends traditional perspective-based 8-point algorithms, which is based on the Direct Linear Transform (DLT) framework \cite{hartley2003multiple}.


\noindent\textbf{Spherical Bundle Adjustment}
By representing each feature track observation as a bearing vector, conventional Bundle Adjustment (BA) can be reformulated in a manner similar to the perspective case, as follows:
%
\begin{equation} \label{eq:textureupdate}
    L = \sum_{i}\sum_{j}\rho(||S(P_j; R_i, t_i) - u_{ij}||^2),
\end{equation}
where $\rho$ denotes the robust Huber norm, and $S$ represents the spherical projection, which maps the 3D scene point $X_j$ to a unit bearing vector, given camera poses $R_j \in \text{SO(3)}$ and $t_j \in \mathbb{R}^3$.




\subsection{Geometric Reconstruction}
\label{sec:3_2}
\input{sec/sec_3_2_JH}
% \input{sec/sec_3_2_DK}


\subsection{Texture Optimization}
\label{sec:3_3}
\input{sec/sec_3_3_JH}

\section{Implementation and Performance}
\subsection{Experiment Settings}
\paragraph{Matterport3D Dataset}
The Matterport3D dataset \cite{chang2017matterport3d} consists of 90 indoor scenes encompassing a total of 10,800 panoramic images. Following the EDM approach \cite{EDM}, we define the ground truth camera poses of these images. From the validation and test sets of the official benchmark split, we selected three scenes each, ensuring that these scenes are entirely indoors or composed of levels no higher than the second floor.
Since dense matching requires overlapped image pairs as input,
we assume that during 360-degree camera scanning, the information about which room each camera belongs to is known.
To utilize this information effectively, we leveraged the $house\_segmentation$ labels available in the Matterport3D dataset. Images taken within the same room were selected as matching pairs, while for transitions between different rooms, only one image pair with the highest overlap was linked.
For EDM \cite{EDM}, the ERP images were resized to $640 \times 320$, whereas images for other feature matching methods were resized to $1024 \times 512$.

% \paragraph{OmniBlender Dataset}
% OmniBlender \cite{choi2023balanced} is a photorealistic synthetic dataset comprising 11 distinct scenes characterized by circular camera motion. For our experiments, we selected four scenes that represent indoor environments. Since this dataset is captured sequentially within a single space, matching pairs were created by pairing a reference image with its two adjacent images. Similar to the Matterport3D experiments, ERP images were resized to $640 \times 320$ for EDM, while images for other feature matching methods were resized to $1024 \times 512$.
\paragraph{Stanford2D3D Datatset}
The Stanford2D3D dataset \cite{Stanford2d3d} comprises scenes from six large-scale indoor environments collected from three different buildings. 
We utilize this dataset to demonstrate that our spherical SfM method outperforms existing SfM approaches in terms of registration performance and accurate pose estimation. 
Specifically, we selected three scenes containing a moderate number of images. 
Assuming that overlapping image pairs were predefined during the scanning process, we determined covisibility and defined matching pairs using geometric meshes and ground truth poses. 
Additionally, we resized the equirectangular projection (ERP) images following the same methodology employed in the Matterport3D dataset.


\paragraph{Implementation Details}
We construct the Spherical Structure from Motion based on SphereSfM \cite{jiang20243d} and COLMAP \cite{schonberger2016structure}. To address challenges in textureless or occluded regions, we leverage the dense matching algorithm EDM \cite{EDM} for enhanced feature extraction. Following the DebSDF \cite{xiao2024debsdf}, the SDF and color networks are implemented using an 8-layer MLP and a 2-layer MLP, respectively, each with a hidden dimension of 256. The networks are optimized for geometric reconstruction using the Adam optimizer \cite{kingma2014adam}, starting with an initial learning rate of $5 \times 10^{-4}$ and applying exponential decay at each iteration. Input images are resized to $384 \times 384$, and geometric cues are provided by the pretrained Omnidata model \cite{eftekhar2021omnidata}. In Texture Optimization, the specular features are initialized to zero values, matching the resolution of the diffuse texture image. The small MLP consists of 2 layers with 32 hidden dimensions. We employ the Adam optimizer \cite{kingma2014adam} with a learning rate of $5 \times 10^{-4}$ and utilize a learning rate scheduler. The model is trained for 7,000 iterations. We render images at a resolution of $512 \times 512$.

% \textcolor{red}{TODO: Add a description for the texturing process.}
%%%% TODO: Add a description for the texturing process.

\begin{figure}[t]
    \centering    
    \includegraphics[width=1.0\linewidth]{figures/figno_intro_JH.pdf}
    \caption{
    We provide a visual comparison between sparse matching perspective SfM and our proposed dense matching spherical SfM. 
    In (a), ERP images are converted into a cubemap representation, after which feature matching is performed across all 36 possible image pairs, resulting in sparse and noisy correspondence matches. (b) demonstrates our approach, which directly finds dense and accurate correspondences on ERP images, thereby facilitating the construction of a detailed 3D structure.
    }
    \label{fig:sfm}
\end{figure}


\subsection{Experimental Results}
\paragraph{Camera Pose Estimation}
% TODO : colmap 은 rig 로 돌린거 강조해야함.
Accurate geometric 3D reconstruction depends on precise camera pose estimation. 
We compare our proposed method, which leverages dense matching \cite{EDM} and Spherical SfM, against four different methods:
1) \textbf{OpenMVG:} An open-source SfM pipeline that supports spherical camera models \cite{moulon2017openmvg}.
2) \textbf{SPSG COLMAP:} This approach employs SuperPoint \cite{detone2018superpoint} and SuperGlue \cite{sarlin2020superglue} for feature detection and matching on perspective images. ERP images are transformed into a cubemap representation, yielding six perspective views per ERP image. Feature matching is performed in a brute-force manner across all 36 possible image pairs. Subsequently, incremental triangulation and bundle adjustment are performed under the rig constraints for each 360 camera node.
3) \textbf{DKM COLMAP:} This method leverages DKM \cite{edstedt2023dkm} to establish dense correspondences without relying on explicit feature extraction. Since DKM is designed for perspective images, it also follows the cubemap projection and exhaustive pairwise matching, and rig-based optimization.
4) \textbf{SphereGlue COLMAP:} Building on COLMAP, SphereSfM \cite{jiang20243d} incorporates spherical camera models and spherical bundle adjustment to handle ERP images. For feature extraction and matching, SuperPoint \cite{detone2018superpoint} with a local planar approximation \cite{eder2020tangent} and SphereGlue \cite{gava2023sphereglue} are utilized to mitigate distortion in ERP images.
To estimate initial poses for incremental triangulation, we follow the normalization strategy and non-linear optimization proposed by Solarte et al. \cite{solarte2021robust}.

Table \ref{table:mp3d} presents the quantitative results of camera pose estimation on the Matterport3D dataset. 
While previous methods also performed well in smaller scenes with significant image overlap, our proposed method demonstrated superior robustness across diverse scenes. 
By leveraging a wide field of view (FoV) for matching, our approach proved particularly effective on datasets with sparse input views. 
Notably, it achieved the most accurate registrations in scenes with frequent occlusions. 
Figure \ref{fig:sfm_result} visualizes the estimated camera poses and triangulated 3D points, demonstrating that our method estimated both the largest number of 3D points and the most camera poses.


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig4_exp.pdf}
    \caption{\textbf{Qualitative Comparison of SfM results on Matterport3D.} By leveraging dense features on equirectangular projection (ERP) images, our method effectively finds correspondences in textureless regions. }
    %which builds accurate track information and leads to a denser 3D structure.}
    \label{fig:sfm_result}
\end{figure}




%% table
% \begin{table}[t]
%     \begin{center}
%     \caption{\label{table:mp3d_stfd}QUANTITATIVE COMPARISONS WITH RECENT ALGORITHMS}
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{l|c|c|c|ccc}
%         \toprule
%         \multirow{2}{*}{Method} &\multirow{2}{*}{Dataset} &\multirow{2}{*}{Image} &\multirow{2}{*}{Feature} &\multicolumn{3}{c}{AUC} \\
%         & & & & @5\textdegree & @10\textdegree & @20\textdegree \\
%         \midrule
%         SPHORB \cite{zhao2015sphorb} &Matterport3D &ERP &sparse &0.38 &1.41 &3.99 \\
%         SphereGlue \cite{gava2023sphereglue} &Matterport3D &ERP &sparse &11.29 &19.95 &31.10 \\
%         \midrule
%         DKM \cite{edstedt2023dkm} &Matterport3D &perspective &dense &18.43 &28.50 &38.44 \\
%         RoMa \cite{edstedt2023roma} &Matterport3D &perspective &dense &12.45 &22.37 &34.24 \\
%         \midrule
%         \textbf{EDM (ours)} &Matterport3D &ERP &dense &\textbf{45.15} &\textbf{60.99} &\textbf{73.60} \\
%         \midrule\midrule
%         SPHORB \cite{zhao2015sphorb} &Stanford2D3D &ERP &sparse &0.14 &1.01 &4.08 \\
%         SphereGlue \cite{gava2023sphereglue} &Stanford2D3D &ERP &sparse &11.25 &22.41 &36.57 \\
%         \midrule
%         DKM \cite{edstedt2023dkm} &Stanford2D3D &perspective &dense &12.46 &22.18 &34.13 \\
%         RoMa \cite{edstedt2023roma} &Stanford2D3D &perspective &dense &11.48 &22.52 &37.07 \\
%         \midrule
%         \textbf{EDM (ours)} &Stanford2D3D &ERP &dense &\bf{55.08} &\bf{71.65} &\bf{82.72} \\
%         \bottomrule
%     \end{tabular}
%     }
%     \end{center}
%     EDM improve AUC@5\textdegree\ by 26.72 on Matterport3D and by 42.62 on Stanford2D3D.
% \end{table}
\begin{table*}[t]
    \begin{center}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{l|cccc cccc cccc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{4}{c}{2t7WUuJeko7} & \multicolumn{4}{c}{8194nk5LbLH} & \multicolumn{4}{c}{pLe4wQe7qrG} \\
        \cmidrule{2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
        & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree \\
        \midrule
        OpenMVG & 5 / 37 & 0.82 & 1.10 & 1.30 &2 / 20 & 0.27&0.37 &0.45 &25 / 31 & 45.11 &52.82 &58.67 \\
        SPSG COLMAP & 16 / 37 & 14.30 & 15.79 & 16.90 & 12 / 20 & 26.64 & 29.88 & 32.31 & \textbf{31 / 31} & 75.02 & 85.01 & 92.51 \\
        DKM COLMAP & 22 / 37& 25.14 & 27.70 & 29.62 & 9 / 20 & 14.64 & 16.36 & 17.66 & \textbf{31 / 31} & \textbf{75.94} & \textbf{85.56} & \textbf{92.78} \\
        SphereGlue COLMAP & 21 / 37 & 23.95 & 26.98 & 29.26 & 12 / 20 & 23.11  & 27.76 & 31.24 & \textbf{31 / 31} & 66.83 & 80.06 & 90.03 \\
        IM360 (Ours) & \textbf{37 / 37} & \textbf{49.16} & \textbf{69.05} & \textbf{84.53} & \textbf{20 / 20} & \textbf{34.91} & \textbf{44.16} & \textbf{66.87} & \textbf{31 / 31} & 73.95 & 84.37 & 92.18 \\
        \midrule
        \multirow{2}{*}{Method} & \multicolumn{4}{c}{RPmz2sHmrrY} & \multicolumn{4}{c}{YVUC4YcDtcY} & \multicolumn{4}{c}{zsNo4HB9uLZ} \\
        \cmidrule{2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
        & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree \\
        \midrule
        OpenMVG & 22 / 59 & 9.86 &11.31 &12.41 &8 / 46 &2.22 &2.41 &2.56 &2 / 53 &-- &-- &-- \\
        SPSG COLMAP & 34 / 59 & 22.93 & 26.87 & 29.83 & \textbf{46 / 46} & \textbf{84.21} & \textbf{90.53} & \textbf{95.26} & 19 / 53 & 7.84 & 9.22 & 10.55 \\
        DKM COLMAP & 51 / 59 & 41.56 & 53.56 & 62.58 & \textbf{46 / 46} & 56.33 & 61.00 & 64.67 & 19 / 53 & 7.87 & 8.71 & 9.41 \\
        SphereGlue COLMAP & 34 / 59 & 22.23 & 26.45 & 29.62 & \textbf{46 / 46} & 78.32 & 86.97 & 93.48 & 22 / 53 & 6.25 & 7.56 & 9.46 \\
        IM360 (Ours) & \textbf{59 / 59} & \textbf{53.44} & \textbf{71.87} & \textbf{85.93} & \textbf{46 / 46} & 72.40 & 83.40 & 91.71 & \textbf{53 / 53} & \textbf{52.35} & \textbf{71.14} & \textbf{85.49} \\
        \bottomrule
    \end{tabular}
    }
    \end{center}
    \vspace*{-4mm}
    \caption{\label{table:mp3d} \textbf{Quantitative Evaluation of Camera Localization Performance} on the Matterport3D dataset \cite{chang2017matterport3d}. Our method demonstrates superior results in terms of registration accuracy and achieves comparable performance in pose estimation across most scenes.}
\end{table*}

\begin{table*}[t]
    \begin{center}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{l|cccc cccc cccc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{4}{c}{area 3} & \multicolumn{4}{c}{area 4} & \multicolumn{4}{c}{area 5a} \\
        \cmidrule{2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
        & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree \\
        \midrule
        OpenMVG & 6 / 85 & 0.35 &0.38 &0.40 & 17 / 258 &0.34 &0.37 &0.39 &  8 / 143 &0.23 &0.25 &0.26 \\
        SPSG COLMAP & 28 / 85 & 6.71 & 7.47 & 8.29 & 73 / 258 & 4.75 & 5.77 & 6.55 & 54 / 143 & 8.84 & 10.93 & 12.51 \\
        DKM COLMAP & 45 / 85& 13.10 & 14.92 & 16.32 & 79 / 258 & 3.39 & 3.96 & 4.40 & 78 / 143 & 15.27 & 18.71 & 21.38  \\
        SphereGlue COLMAP & 30 / 85 & 5.87 & 7.38 & 8.75 & 116 / 258 & 8.62  & 11.64 & 14.13 & 69 / 143 & 5.10 & 8.06 &  11.73\\
        IM360 (Ours) & \textbf{85 / 85} & \textbf{32.20} & \textbf{45.12} & \textbf{58.23} & \textbf{258 / 258} & \textbf{28.70} & \textbf{50.50} & \textbf{71.29} & \textbf{138 / 143} & \textbf{19.52} & \textbf{32.54} & \textbf{51.72}  \\
        \bottomrule
    \end{tabular}
    }
    \end{center}
    \vspace*{-4mm}
    \caption{\label{table:stfd} \textbf{Quantitative evaluation of camera Localization Performance} on the Stanford2D3D dataset \cite{Stanford2d3d}. Our method outperforms all other approaches in terms of registration accuracy and pose estimation.}
\end{table*}

% \begin{table*}[t]
%     \begin{center}
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{l|cccc cccc cccc}
%         \toprule
%         % \multirow{2}{*}{Method} & \multicolumn{4}{c}{2t7WUuJeko7} & \multicolumn{4}{c}{8194nk5LbLH} & \multicolumn{4}{c}{pLe4wQe7qrG} \\
%         \multirow{2}{*}{Method} & \multicolumn{12}{c}{\# Registered \quad AUC @3\textdegree \quad AUC @5\textdegree \quad AUC @10\textdegree} \\
%         & \multicolumn{4}{c}{2t7WUuJeko7} & \multicolumn{4}{c}{8194nk5LbLH} & \multicolumn{4}{c}{pLe4wQe7qrG} \\
%         % \cmidrule{2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
%         % & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree \\
%         \midrule
%         OpenMVG & & & & & & & & & & & & \\
%         SPSG COLMAP & 16 / 37 & 14.30 & 15.79 & 16.90 & 12 / 20 & 26.64 & 29.88 & 32.31 & 31 / 31 & 75.02 & 85.01 & 92.51 \\
%         DKM COLMAP & 22 / 37& 25.14 & 27.70 & 29.62 & 9 / 20 & 14.64 & 16.36 & 17.66 & 31 / 31 & 75.94 & 85.56 & 92.78 \\
%         SphereGlue COLMAP & 21 / 37 & 23.95 & 26.98 & 29.26 & 12 / 20 & 23.11 & 27.76 & 31.24 & 31 / 31 & 66.83 & 80.06 & 90.03 \\
%         Ours & 37 / 37 & 49.16 & 69.05 & 84.53 & 20 / 20 & 34.91 & 44.16 & 66.87 & 31 / 31 & 73.95 & 84.37 & 92.18 \\
%         \midrule
%         Method & \multicolumn{4}{c}{RPmz2sHmrrY} & \multicolumn{4}{c}{YVUC4YcDtcY} & \multicolumn{4}{c}{zsNo4HB9uLZ} \\
%         % \cmidrule{2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
%         % & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree & \# Registered & AUC @3\textdegree & AUC @5\textdegree & AUC @10\textdegree \\
%         \midrule
%         OpenMVG & & & & & & & & & & & & \\
%         SPSG COLMAP & 34 / 59 & 22.93 & 26.87 & 29.83 & 46 / 46 & 84.21 & 90.53 & 95.26 & 19 / 53 & 7.84 & 9.22 & 10.55 \\
%         DKM COLMAP & 51 / 59 & 41.56 & 53.56 & 62.58 & 46 / 46 & 56.33 & 61.00 & 64.67 & 19 / 53 & 7.87 & 8.71 & 9.41 \\
%         SphereGlue COLMAP & 34 / 59 & 22.23 & 26.45 & 29.62 & 46 / 46 & 78.32 & 86.97 & 93.48 & 22 / 53 & 6.25 & 7.56 & 9.46 \\
%         Ours & 59 / 59 & 53.44 & 71.87 & 85.93 & 46 / 46 & 72.40 & 83.40 & 91.71 & 53 / 53 & 52.35 & 71.14 & 85.49 \\
%         \bottomrule
%     \end{tabular}
%     }
%     \end{center}
%     \caption{\label{table:mp3d}Quantitative comparisons with recent algorithms}
% \end{table*}



\begin{table*}[t]
    \centering
    \begin{adjustbox}{width=0.95\linewidth,center}
    \begin{tabular}{l|c|ccccccc}
    \toprule
    \multirow{2}{*}{Method}&\multirow{2}{*}{Rendering}&\multicolumn{7}{c}{PSNR \(\uparrow\)  / SSIM \(\uparrow\)  / LPIPS \(\downarrow\) } \\
    & & 2t7WUuJeko7 & 8194nk5LbLH & pLe4wQe7qrG & RPmz2sHmrrY & YVUC4YcDtcY & zsNo4HB9uLZ & Mean \\
    \midrule
    ZipNeRF \cite{barron2023zip} & Volume &  15.1 / 0.51 / 0.69 & 11.9 / 0.44 / 0.74 & 14.1 / 0.43 / 0.70 & 13.1 / 0.50 / 0.69 & 15.3 / 0.53 / 0.68 & 14.1 / 0.65 / 0.68 & 13.9 / 0.51 / 0.68 \\
    3DGS \cite{kerbl20233d} & Splat & 14.4 / 0.47 / 0.53 & 11.9 / 0.38 / 0.60 & 13.5 / 0.37 / 0.56 & 12.9 / 0.51 / 0.55 & 14.2 / 0.49 / 0.52 & 13.7 / 0.60 / 0.50 & 13.4 / 0.47 / 0.55 \\
    SparseGS \cite{xiong2023sparsegs} & Splat & 15.4 / 0.48 / 0.50 & 12.8 / 0.36 / 0.58 & 13.8 / 0.37 / 0.52 & 13.6 / 0.46 / 0.57 & 15.5 / 0.50 / 0.49 & 14.4 / 0.58 / 0.55 & 14.3 / 0.46 / 0.53 \\
    TexRecon \cite{waechter2014TexRecon} & Mesh & 17.1 / 0.51 / 0.43 & 14.2 / 0.47 / 0.49 & 15.9 / 0.45 / 0.46 & 16.7 / 0.60 / 0.38 & 15.0 / 0.59 / 0.42 & 17.0 / 0.51 / 0.42 & 15.9 / 0.54 / 0.43 \\
    \midrule
    IM360* (Ours) & Mesh & 19.6 / 0.60 / 0.37 & 16.7 / 0.60 / 0.43 & 18.5 / 0.65 / 0.36 & 18.9 / 0.65 / 0.38 & 18.3 / 0.60 / 0.42 & 19.8 / 0.68 / 0.41 & 18.6 / 0.63 / 0.39 \\
    IM360 (Ours) & Mesh & \textbf{19.8} / \textbf{0.61} / \textbf{0.38} & \textbf{16.9} / \textbf{0.61} / \textbf{0.43} & \textbf{18.8} / \textbf{0.66} / \textbf{0.35} & \textbf{20.6} / \textbf{0.75} / \textbf{0.32} & \textbf{19.4} / \textbf{0.66} / \textbf{0.32} & \textbf{21.0} / \textbf{0.74} / \textbf{0.37} & \textbf{19.4} / \textbf{0.67} / \textbf{0.37} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}    
    \caption{\textbf{Quantitative Comparison of Rendering Performance} on the Matterport3D Dataset. Our method shows higher rendering quality of our method, IM360,across all scenes, achieving a 3.5 PSNR over the TexRecon \cite{waechter2014TexRecon}. The best-performing algorithms for each metric are shown in boldface. 
    }
    \label{table:mp3d-rendering-comparion}
\end{table*}



% \begin{table}[t]
%     \centering
%     \begin{adjustbox}{width=\linewidth,center}
%     \begin{tabular}{l|c|ccccc}
%     \toprule
%     \multirow{2}{*}{Method}&\multirow{2}{*}{Rendering}&\multicolumn{5}{c}{PSNR \(\uparrow\)  / SSIM \(\uparrow\)  / LPIPS \(\downarrow\) } \\
%     & & archiviz & barbershop & classroom & restroom & Mean \\
%     \midrule
%     ZipNeRF \cite{barron2023zip} & Volume &  11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 \\
%     3DGS \cite{kerbl20233d} & Splat & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11  \\
%     SparseGS \cite{xiong2023sparsegs} & Splat & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11  \\
%     TexRecon \cite{waechter2014TexRecon} & Mesh & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11  \\
%     \midrule
%     Ours & Mesh & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11  \\
%     Ours & Mesh & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11 & 11.1 / 1.11 / 1.11  \\
%     \bottomrule
%     \end{tabular}
%     \end{adjustbox}    
%     \caption{
%     }
%     \label{table:omniblender-psnr-comparion}
% \end{table}



\paragraph{Surface Reconstruction}
Recently, neural surface reconstruction methods based on volumetric rendering techniques \cite{wang2021neus, yu2022monosdf, xiao2024debsdf} have shown strong performance in dense 3D reconstruction. 
In particular, several approaches that leverage monocular geometry priors \cite{yu2022monosdf, xiao2024debsdf} demonstrate robustness in handling large textureless regions and sparse input views. 
Therefore, we adopt DebSDF \cite{xiao2024debsdf} to estimate the signed distance function and apply the marching cubes algorithm \cite{marchingcube} to convert the neural implicit representation into triangle meshes. Figure \ref{fig:geometry} presents shading meshes extracted without texture, the most effective way to visualize geometric differences. This highlights the effectiveness of our SfM method in producing decent geometric meshes from omnidirectional data.  

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig6_geometry.pdf}
    \caption{
    We visualize the shading meshes extracted without texture on Matterport3D. Leveraging the estimated poses from IM360, we achieve accurate surface reconstruction, demonstrating the effectiveness of our SfM method in generating decent geometric meshes from omnidirectional data.}
    \label{fig:geometry}
\end{figure}



\paragraph{Texture Map Optimization}
We now evaluate the results of our view synthesis method by comparing it against four different approaches: 1) \textbf{ZipNeRF} \cite{barron2023zip}, the state-of-the-art neural radiance field method; 2) \textbf{3DGS} \cite{kerbl20233d}, which employs 3D Gaussian splats rendered through rasterization; 3) \textbf{SparseGS} \cite{xiong2023sparsegs}, designed to address sparse-view challenges in 3D Gaussian splatting; and 4) \textbf{TexRecon} \cite{waechter2014TexRecon}, a classical texture mapping method for large-scale scenes. Table \ref{table:mp3d-rendering-comparion} reports the rendering quality with the PSNR, SSIM \cite{SSIM}, and LPIPS \cite{LPIPS} on Matterport3D dataset. We consistently observe that neural rendering methods, including NeRF \cite{barron2023zip} and 3DGS \cite{kerbl20233d}, perform inferior to traditional texture reconstruction techniques \cite{waechter2014TexRecon}. The significant discrepancy between training PSNR and novel view PSNR indicates that this gap arises due to the sparsity of the scanned images. To address this, we evaluated the state-of-the-art SparseGS \cite{xiong2023sparsegs} approach, specifically designed for sparse-view scenarios. While SparseGS \cite{xiong2023sparsegs} achieves a 1 PSNR improvement in rendering quality, this gain indicates that sparse scanning remains a significant challenge. Our method achieves improved rendering quality across all scenes, demonstrating a 3.5 PSNR increase over the TexRecon \cite{waechter2014TexRecon} method.

In Fig. \ref{fig:rendering-comparions-matterport}, we visually compare our method to several recent rendering approaches: TexRecon \cite{waechter2014TexRecon}, SparseGS \cite{xiong2023sparsegs}, and ZipNeRF \cite{barron2023zip}. We visualize only SparseGS instead of 3DGS, as both methods employ similar rendering techniques, but SparseGS demonstrates superior rendering quality in sparsely scanned indoor environments. 
TexRecon shows the visual seams between face texture and color misalignment.  ZipNeRF tends to generate ambiguous pixels, resulting in severe artifacts. SparseGS contains obvious floating artifacts in ground and floor regions. In contrast, our method produces much higher quality textures and consistently demonstrate improvement across various scenes. 

\subsection{Ablation Study}
We observed that when initial poses are determined through two-view geometry without utilizing the method proposed by Solarte et al.~\cite{solarte2021robust}, the registration performance significantly deteriorates, resulting in fragmented SfM models. Furthermore, as shown in Table \ref{table:mp3d} and \ref{table:stfd}, by modifying the feature matching in COLMAP-based methods and comparing the registration and pose accuracy, we confirmed that our approach demonstrates superior robustness.
% \textcolor{red}{sfm ablation study}
% robust 360 8pa 유무 -> registration 차이
% dense feature 방식인데, 기존 spherical sfm 방식들은 indoor 에서 작동 X. feature 적인 특징..
In table \ref{table:mp3d-rendering-comparion}, IM360* only finetunes diffuse texture without utilizing specular features. Without texture optimization stage outlined in Sec \ref{sec:3_3}, our rendering quality is equivalent to that of TexRecon \cite{waechter2014TexRecon}. Although we apply texture optimization soley to the diffuse texture map, our method achieves a substantial improvement in rendering quality, with a 2.7 PSNR increase. Additionally, our final approach, which combines both diffuse and specular colors, provides a further 0.8 PSNR gain by accounting for view-dependent components.      





\begin{figure*}[h]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/render_comparison.pdf}
    % \vspace{-2mm}
    \caption{\textbf{Qualitative Comparisons with Existing Methods.} We visually compare our proposed approach with TexRecon \cite{waechter2014TexRecon}, SparseGS \cite{xiong2023sparsegs}, and ZipNeRF \cite{barron2023zip}. Our method can render high frequency details and results in lower noise.}
    % \vspace{-5mm}
    \label{fig:rendering-comparions-matterport}
\end{figure*}


\section{Conclusion, Limitations and Future Work}
% Limitations of Texturing 
% Mesh 의 geometric artifacts laed to rendering error and texture artifacts

We introduce a novel framework for textured mesh reconstruction for omnidirectional cameras in sparsely scanned indoor environments. To address the challenges associated with textureless regions and sparse viewpoints, we propose a spherical Structure-from-Motion (SfM) that integrates spherical camera models into all the core steps of SfM. Additionally, we reconstruct texture maps and leverage differentiable rendering to enhance its quality, improving the rendering performance in sparse-view scenarios. However, our approach has certain limitations.
% \textcolor{red}{Limitation of SfM}.
First, the performance of dense matching depends on correct matching pairs. Therefore, if we lack an appropriate image retrieval technique, we must rely on covisibility annotations during the scanning process.
Furthermore, our texture mapping method is based on a mesh rasterization approach, making the rendering quality highly sensitive to the accuracy of the underlying mesh geometry. Consequently, any inaccuracies in the mesh can result in rendering artifacts. In the future, we may explore ERP image retrieval methods to extract covisible image pairs and the joint optimization of geometry and texture to refine geometric artifacts.

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
