% sfm 설명
% sfm 예시들 colmap, pixelsfm, theiasfm, spehresfm, etc
% challenges
% feature matching (especially dense matching)
% dense feature track 연결? 이거 내 contribution 아니니까 대충 얘기하거나 넘어가도 될 듯.


Indoor 3D mapping and photorealistic rendering are core technologies for various applications in computer vision, robotics, and graphics. High-fidelity digitization of the real world enables immersive experiences in AR/VR and helps bridge the sim-to-real gap in robotic applications. However, the conventional image acquisition process \cite{dai2017scannet,baruch2021arkitscenes,barron2022mip} is often labor-intensive and time-consuming due to the limited field-of-view and sparse scene coverage, as shown in Fig \ref{fig:introduction}. 
Previous studies \cite{janiszewski2022rapid,herban2022use} have demonstrated that omnidirectional cameras can significantly reduce acquisition time, which is crucial for capturing various indoor environments.
Nevertheless, most recent research \cite{schonberger2016structure,moulon2017openmvg,mildenhall2021nerf,kerbl20233d,waechter2014TexRecon} relies on conventional cameras with limited fields of view, which results in inadequate scene coverage, susceptibility to motion blur, and substantial time requirements for capturing large-scale indoor environments. 

Unlike conventional cameras, which benefit from well-established photogrammetric software such as COLMAP \cite{schonberger2016structure} and OpenMVG \cite{moulon2017openmvg}, research on omnidirectional cameras remains fragmented across various domains and lacks a robust pipeline for Structure-from-Motion (SfM) and photorealistic rendering. Notably, there has been limited academic exploration of sparse scanning scenarios aimed at significantly reducing image acquisition time. There are two key challenges in developing 3D mapping and rendering for omnidirectional cameras. 



\begin{figure}[t]
    \centering
    % \vspace*{-5mm}
    \includegraphics[width=1.\linewidth]{figures/fig1_mp3d.pdf}
    % \vspace*{-2mm}
    \caption{This example compares two benchmark datasets: one captured with a conventional camera \cite{dai2017scannet} and the other with an omnidirectional camera \cite{chang2017matterport3d}. The blue wireframes indicate the camera locations. While the wide field of view of the omnidirectional camera significantly reduces the data acquisition time, it introduces challenges such as sparse views and occlusions. Our approach provides an effective solution to handle issues in sparsely scanned indoor environments and enables efficient reconstruction and rendering using omnidirectional cameras.}
    \label{fig:introduction}
    \vspace*{-5mm}
\end{figure}

First, the SfM pipeline in sparsely scanned indoor environments suffer from the prevalence of large textureless regions, which degrade the accuracy of both feature detection and matching. Moreover, frequent occlusions in indoor environments, combined with sparse input views as shown in Fig. \ref{fig:introduction}, make the feature matching process even more challenging. In such scenarios, conventional perspective cameras further worsen the problem due to their limited field of view, which reduces the overlap between views. This lack of overlap complicates the estimation of accurate camera poses, making the reconstruction task considerably more difficult. Recently, detector-free or dense matchers \cite{sun2021loftr,melekhov2019dgc,truong2020glu,truong2021learning,edstedt2023dkm,edstedt2024roma} have emerged as a promising alternative to detector-based approaches \cite{detone2018superpoint,revaud2019r2d2,tyszkiewicz2020disk}, particularly in handling repetitive or indiscriminate regions where keypoint detection performance tends to degrade.
% However, detector-free matchers face issues with multi-view inconsistency in subpixel correspondences due to their pairwise dependency \cite{he2024detector}.
% To mitigate this, Detector-Free SfM \cite{he2024detector} proposes a strategy that quantizes feature locations to form tracks across views, enhancing its compatibility with existing SfM systems.

Additionally, in sparsely scanned scenarios, recent neural rendering methods \cite{mildenhall2021nerf,kerbl20233d} often face challenges in producing high-quality novel-view images. It is widely recognized that both neural radiance fields (NeRF) \cite{mildenhall2021nerf} and 3D Gaussian splatting (3DGS) \cite{kerbl20233d} are optimized for densely captured scenes with substantial overlap between viewpoints. Prior studies \cite{xiong2023sparsegs,wang2023sparsenerf,martins2024feature,lee2024modegs} have demonstrated that neural rendering algorithms often exhibit overfitting to the training views and a poor rendering quality on novel-view images far from training views.

% \begin{figure}[t]
%     \centering
%     % \vspace*{-5mm}
%     \includegraphics[width=0.9\linewidth]{figures/fig1_intro.pdf}
%     % \vspace*{-2mm}
%     \caption{makk geurheo noeun geot}
%     \label{fig:introduction}
% \end{figure}

% The 3D geometry of a scene is essential for a wide range of computer vision applications and has been a long-standing challenge in the field.
% General Structure-from-Motion (SfM) software such as COLMAP \cite{schonberger2016structure}, TheiaSfM \cite{sweeney2015theia}, and OpenMVG \cite{moulon2017openmvg} have significantly advanced the capabilities of 3D reconstruction.
% Indoor 3D mapping typically relies on LiDAR sensors or dense video scans to achieve accurate reconstruction. 
% However, these requirements pose significant challenges when dealing with sparsely scanned indoor scenes captured solely by visual sensors. 
% The primary issue lies in the prevalence of large textureless regions, which degrade the accuracy of both feature detection and matching, thereby impairing the overall performance of the SfM pipeline. 
% Additionally, frequent occlusions in indoor environments, combined with sparse input views, make the feature matching process even more challenging. 
% In such scenarios, conventional perspective cameras further worsen the problem due to their limited field of view, which reduces the overlap between views. 
% This lack of overlap complicates the estimation of accurate camera poses, making the reconstruction task considerably more difficult.

% Recently, detector-free matchers \cite{sun2021loftr,melekhov2019dgc,truong2020glu,truong2021learning,edstedt2023dkm,edstedt2024roma} have emerged as a promising alternative to detector-based approaches \cite{detone2018superpoint,revaud2019r2d2,tyszkiewicz2020disk}, particularly in handling repetitive or indiscriminate regions where keypoint detection performance tends to degrade.
% However, detector-free matchers face issues with multi-view inconsistency in subpixel correspondences due to their pairwise dependency \cite{he2024detector}.
% To mitigate this, Detector-Free SfM \cite{he2024detector} proposes a strategy that quantizes feature locations to form tracks across views, enhancing its compatibility with existing SfM systems.

\paragraph{Main Results}
We present a novel indoor 3D mapping and rendering pipeline designed for omnidirectional images and address the challenges of sparsely scanned indoor environments.
We tackle the problem of extensive textureless regions by employing detector-free matching methods~\cite{EDM}.
%which improve matching accuracy without the need for explicit feature detection. 
To mitigate the frequent occlusions encountered in sparsely scanned images, we utilize an spherical camera model with a wide field of view, increasing the overlap between images and enhancing robustness in multi-view matching.
% To the best of our knowledge, this is the first work to present dense matching and SfM designed for the spherical camera model. Our method achieves improved camera poses without the need for redundant or repetitive feature matching.  
% (\textcolor{blue}{(YH) 
 While traditional methods, such as OpenMVG \cite{moulon2017openmvg} and COLMAP \cite{schonberger2016structure}, fail to register nearly half of the images in challenging datasets like Matterport3D \cite{chang2017matterport3d} and Stanford2D3D \cite{Stanford2d3d}, our method achieves unparalleled registration performance with high pose accuracy, thanks to the tight integration of a wide-view spherical model.
% })
To the best of our knowledge, ours is the first work to present a complete SfM pipeline dedicated to spherical camera models, where entire step—feature matching, two-view geometry estimation, track triangulation, and bundle adjustment—is conducted on a spherical manifold.

We extend our approach to mesh reconstruction and texturing and provide a comprehensive solution for indoor 3D mapping and rendering.  For mesh reconstruction, we leverage neural surface reconstruction \cite{xiao2024debsdf} by training a signed distance field and extracting a mesh. Classical texture mapping \cite{waechter2014TexRecon} is then combined with texture optimization through differentiable rendering. We optimize textures and train small multi-layer perceptrons (MLPs) to represent specular color. By jointly optimizing diffuse and specular textures along with the MLPs via differentiable rendering \cite{nvdiffrast} under image supervision, our approach achieves superior rendering quality. Additionally, it demonstrates robustness in sparsely scanned indoor environments, outperforming recent neural rendering methods. The proposed method demonstrates significant improvements over traditional approaches, particularly in challenging indoor environments with sparse and textureless scenes. 

% We evaluate our approach using the Matterport3D dataset \cite{chang2017matterport3d}, which spans 101.82k $m^2$ and offers a larger physical scale than typical indoor datasets like ScanNet \cite{dai2017scannet} (0.56k $m^2$) and Replica \cite{straub2019replica} (39.98 $m^2$), according to \cite{ramakrishnan2021hm3d}. 

We evaluate our approach using the Matterport3D dataset~\cite{chang2017matterport3d}, which offers a larger physical scale than typical indoor datasets like ScanNet, according to~\cite{ramakrishnan2021hm3d}. We also evaluate the performance on
%Additionally, to demonstrate the generalization capability of our method, we validate it on 
the Stanford2D3D datasets \cite{Stanford2d3d}. The novel contributions of our work include:
%The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item We introduce a unified framework for reconstructing textured meshes using omnidirectional cameras in sparsely scanned, large-scale indoor scenes.
    \item We propose the first spherical Structure-from-Motion approach utilizing dense matching features specifically designed for omnidirectional cameras.
    \item We integrate classical texture mapping with neural texture fine-tuning through differentiable rendering, demonstrating improved rendering quality in sparsely scanned, in-the-wild indoor environments. 
\end{itemize}
We  compare our method with SOTA  and observe higher accuracy in terms of camera localization and rendering. In particular our approach can render higher frequency details and results in lower noise in the reconstructed models.
