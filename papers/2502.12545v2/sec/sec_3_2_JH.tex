To address visual localization and mapping in textureless and highly occluded sparsely scanned indoor scenes, we use neural rendering techniques \cite{yu2022monosdf, xiao2024debsdf} for surface reconstruction instead of traditional 3D reconstruction methods such as multi-view stereo (MVS)~\cite{schonberger2016pixelwise}.
Many methods leverage monocular geometric priors~\cite{yu2022monosdf, xiao2024debsdf} and demonstrate higher robustness in textureless and sparsely covered areas. However, 
in terms of ERP images there are no  robust methods for zero-shot monocular depth and normal estimation.
%has yet to be developed, primarily due to the absence of a sufficiently large-scale dataset.
%Initially, we experimented with defining rays on a sphere camera model; however, we observed slower convergence rates and reduced surface quality.
We also obsered  that OmniSDF \cite{kim2024omnisdf}, which learns signed distance functions (SDF) from equirectangular projection (ERP) images, failed to converge when applied to large-scale indoor datasets. 
% We believe that \textcolor{red}{EXPLAIN!} % 이유가 그래도 있으면 좋을거 같긴한데 ... Sparsely scanned scene? 대략의 이라도 
%We believe that representing rays in spherical camera models is inappropriate due to the inherent distortions present in omnidirectional cameras.

To overcome these limitations, we transformed ERP images into cubemaps, estimated depth $D$ and normal maps $N$ from all six perspective images using a pretrained Omnidata model \cite{eftekhar2021omnidata}, and utilized these geometric cues to train neural surface reconstruction effectively.
Following the DebSDF \cite{xiao2024debsdf}, we jointly train two MLPs using the differentiable volumetric rendering, (i) $f_{sdf}$, which represents the scene geometry as a signed distance function, and (ii) $f_{color}$, a color network. 
The training process \cite{xiao2024debsdf} incorporates a combination of losses, including color reconstruction loss $L_{\text{rgb}} = \sum_{r \in R}||\hat{C}(r) - C(r)||_{1}$, Eikonal loss \cite{eikonal} $L_{\text{eikonal}} = \sum_{x \in \chi}(||\triangledown f_{sdf}(x)||_{2} - 1)^2$ , and depth and normal losses,
%
% \begin{align} \label{eq:depthnormalloss}
%     L_{\text{depth}} &= \sum_{r \in R}||(w \hat{D}(r) + q) - D(r)||^2, \\
%     L_{\text{normal}} &= \sum_{r \in R} ||\hat{N}(r) - N(r)||_1 + ||1 - \hat{N}(r)^\top N(r)||_1.
% \end{align}
%
The latter are derived from prior geometric cues by comparing the rendered depth $\hat{D}(t)$ and normals $\hat{N}(t)$ with the corresponding prior depth $D$ and normals $N$. Color image $\hat{C}$ is volumetrically rendered by ray marching $\hat{C} = \sum_{i \in I} \alpha_i \, C_i \, T_i$ along with $\hat{D}$ and $\hat{N}$. We then utilize the learned SDF ($f_{sdf}$ evaluated over a uniform grid) to extract a mesh $M$ using the Marching Cubes algorithm \cite{marchingcube}.
% \textcolor{red}{Loss functions?}

%% + Add Mraching Cube and Mesh Reconstruction Part 

% --------------------------------------------------------

% To overcome these limitations, we transformed ERP images into cubemaps, estimated depth $D$ and normal maps $N$ from all six perspective images using a pretrained Omnidata model \cite{eftekhar2021omnidata}, and utilized these geometric cues to train neural surface reconstruction effectively.
% Following the DebSDF \cite{xiao2024debsdf}, we jointly train two MLPs using the differentiable volumetric rendering, (i) $f_{sdf}$, which represents the scene geometry as a signed distance function, and (ii) $f_{color}$, a color network. 
% The training process \cite{xiao2024debsdf} incorporates a combination of losses, including color reconstruction loss , , and depth and normal losses,

% $\hat{C}$ are volumetrically rendered by ray marching $\hat{C} = \sum_{i \in I} \alpha_i \, C_i \, T_i$
% %

% \begin{equation} \label{eq:depthnormalloss}
% L = L_{\text{color}} + L_{\text{depth}} + L_{\text{normal}}
% + L{\text{eikonal}}
% \end{equation}
% We refer to read (cite) for details for each loss
% % \begin{align} \label{eq:depthnormalloss}
% % L_{\text{color}} &= \sum_{r \in R}||\hat{C}(r) - C(r)||_{1} \\
% %     L_{\text{depth}} &= \sum_{r \in R}||(w \hat{D}(r) + q) - D(r)||^2, \\
% %     L_{\text{normal}} &= \sum_{r \in R} ||\hat{N}(r) - N(r)||_1 + ||1 - \hat{N}(r)^\top N(r)||_1.
% % \end{align}

% Eikonal loss \cite{eikonal} $L_{\text{eikonal}} = \sum_{x \in \chi}(||\triangledown f_{\theta}(x)||_{2} - 1)^2$ is additionally incorporated to preserve implicit constraint of SDF