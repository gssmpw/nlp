To address visual localization and mapping in textureless and highly occluded sparsely scanned indoor scenes, we employ neural rendering techniques for surface reconstruction instead of traditional 3D reconstruction methods such as multi-view stereo (MVS).
In particular, several works leveraging monocular geometric priors \cite{yu2022monosdf, xiao2024debsdf} demonstrate higher robustness in textureless and sparsely covered areas.
Initially, we experimented with defining rays on a sphere camera model; however, we observed slower convergence rates and reduced surface quality.
Similarly, we found that OmniSDF \cite{kim2024omnisdf}, which learns signed distance functions (SDF) from equirectangular projection (ERP) images, failed to converge when applied to large-scale indoor datasets.
To overcome these limitations, we transformed ERP images into cubemaps, estimated depth and normal maps from perspective images using a pretrained Omnidata model \cite{eftekhar2021omnidata}, and utilized these geometric cues to train DebSDF \cite{xiao2024debsdf} effectively.

%% + Add Mraching Cube and Mesh Reconstruction Part 