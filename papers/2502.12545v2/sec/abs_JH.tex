\begin{abstract}
% We propose a comprehensive 3D reconstruction pipeline specifically tailored for 360° cameras to address challenges in indoor environments. Traditional Structure-from-Motion (SfM) methods struggle with issues such as textureless surfaces and sparse capture settings, which are critical for efficient data collection. To overcome these limitations, our approach leverages the wide field of view provided by omnidirectional images, employing a spherical camera model with dense feature matching for SfM. To further enhance geometric reconstruction, we integrate a neural implicit surface reconstruction technique, capable of generating high-quality surfaces from sparse input data. Additionally, we utilize a mesh-based neural rendering framework to refine texture maps, accurately capturing view-dependent properties by combining diffuse and specular components. We validate our pipeline on large-scale indoor scenes from the Matterport3D and Stanford2D3D datasets, demonstrating superior performance in textured mesh reconstruction. Comparative evaluations against existing localization and texture mapping techniques highlight the robustness and accuracy of our method. Project page: \url{anonymousim360.github.io/}


 We present a novel 3D reconstruction pipeline for 360° cameras for 3D mapping and rendering of indoor environments. Traditional Structure-from-Motion (SfM) methods may not work well in large-scale indoor scenes due to the prevalence of textureless and repetitive regions. To overcome these challenges, our approach (IM360) leverages the wide field of view of omnidirectional images and integrates the spherical camera model into every core component of the SfM pipeline. In order to develop a comprehensive 3D reconstruction solution, we integrate a neural implicit surface reconstruction technique to generate high-quality surfaces from sparse input data. Additionally, we utilize a mesh-based neural rendering approach to refine texture maps and accurately capture view-dependent properties by combining diffuse and specular components. We evaluate our pipeline on large-scale indoor scenes from the Matterport3D and Stanford2D3D datasets. In practice, IM360 demonstrate superior performance in terms of textured mesh reconstruction over SOTA. We observe accuracy improvements in terms of camera localization and registration as well as rendering high frequency details. 
 %Comparative evaluations against existing localization and texture mapping techniques highlight the robustness and accuracy of our method. 
 Project page: \url{anonymousim360.github.io/}
\end{abstract}





% - (1) No existing SfM method is suitable for sparsely captured indoor scenes, which are typical in practical data collections such as Matterport3D. Current methods only perform well on indoor datasets with extremely dense captures, failing to address inherent challenges in indoor environments, such as textureless surfaces and repetitive structures.

% - (2) We propose a "complete pipeline" that processes raw 360° images into textured meshes with diffuse and specular components, effectively "eliminating spherical distortion at each step" of the reconstruction process.