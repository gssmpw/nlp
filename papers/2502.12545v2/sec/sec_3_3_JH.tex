%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3D GS 및 Neural Rendering 알고리즘 언급
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this stage, we project textures onto the 3D mesh $M$ generated in the preceding step. In comparison to recent neural rendering methods such as NeRF \cite{mildenhall2021nerf} and 3DGS \cite{kerbl20233d}, it is important to highlight that texture mapping provides a more robust solution for novel view synthesis in sparsely scanned indoor environments.
Utilizing images obtained from cubemap projections along with their corresponding camera poses, we initially employ TexRecon \cite{waechter2014TexRecon} to construct the texture map $T$. However, this conventional texture mapping approach suffers from visible seams between texture patches and is prone to geometric inaccuracies.

Following TMO \cite{choi2023tmo}, we parameterize the texture map and leverage a differentiable rasterizer $\mathcal{R}$ \cite{nvdiffrast} to render an image from a given viewpoint $P$. Unlike TMO \cite{choi2023tmo}, which is limited to representing diffuse textures and cannot capture view-dependent effects, our approach overcomes this limitation. The diffuse texture $K_{d} \in \mathbb{R}^3$ is directly converted into an RGB image. Additionally, we initialize a specular feature $K_{s} \in \mathbb{R}^3$ and employ a small MLP $f_{s}$ as a fragment shader. This MLP takes the specular feature $K_{s}$ and viewing direction $v$ as inputs to compute the specular color as follows: 
\begin{equation} \label{eq:textureupdate}
    \hat{I_{d}} = \mathcal{R}(M, K_{d}, P), ~   \hat{I_{s}} = f_{s}(\mathcal{R}(M, K_{s}, P), v) 
\end{equation}
The final rendered color $\hat{I} \in \mathbb{R}^3$ is obtained by combining the diffuse and specular components, $\hat{I} = \hat{I_{d}} + \hat{I_{s}}$. 
To optimize the diffuse, specular features, and an MLP, we employ a combination of L1 and SSIM \cite{SSIM} as the photometric loss in image space. This loss is calculated between the final rendered image and the ground truth image from the corresponding viewpoint, as follows:
\begin{equation} \label{eq:textureupdate1}
    L_{photo} = (1-\alpha)\parallel\hat{I} - I\parallel + \alpha\:(1 - SSIM(\hat{I}, I))
\end{equation}
where $\alpha$ is set to 0.2 and the weight to balance the losses.


% By measuring the loss between this rendered image and the ground truth image, we can fine-tune the texture map.