\subsection{Structure from Motion}
% feature matching - SPSG, DKM,
% structure from motion - colmap, dfsfm, 
% SphereGlue, SphereSfM 
% EDM 
% 하지만 아직 omnidirectional camera 에서  dense matching과 detector free sfm 을 결합한 연구가 없음을 강조
% Traditional, Learning detector-based feature and SfM 
% Dense Matching and Detector Free SfM

Structure from Motion (SfM) is a fundamental problem in computer vision. Traditional SfM frameworks typically begin with the detection of keypoints and descriptors, which serve as a core component, ranging from classical algorithms \cite{SIFT} to learning-based techniques \cite{detone2018superpoint,sarlin2020superglue,revaud2019r2d2}. Then, they match these keypoints by either nearest neighbor \cite{schonberger2016structure} or learning-based methods \cite{sarlin2020superglue}. The matching pair is verified by recovering the two-view geometry \cite{hartley2003multiple} with RANSAC \cite{RANSAC}. Based on these matching pairs, popular SfM methods \cite{schonberger2016structure,moulon2017openmvg,sweeney2015theia} follow incremental approach, sequentially registering new images and reconstructing their 3D structures through triangulation \cite{hartley2003multiple} and bundle adjustment \cite{Bundleadjustment}. Alternatively, a recent work \cite{GLOMAP} employs a global approach, recovering the camera geometry for all input images simultaneously. The 2D correspondence matching significantly impact the overall performance of SfM pipeline. In textureless region of indoor environment, these methods suffer from poor feature detection \cite{he2024detector}. Detector-free or dense matching methods  \cite{sun2021loftr,edstedt2023dkm,edstedt2024roma} is proposed to solve this issue by estimating dense feature matches at pixel level. DfSfM \cite{he2024detector} builds these detector-free matches \cite{sun2021loftr} and refines the tracks and geometry of
the coarse SfM model by enforcing multi-view consistency. 

% However, there are relatively few open-source software packages \cite{moulon2017openmvg,jiang20243d} that support omnidirectional cameras.  These frameworks typically adopt an incremental SfM workflow based on a spherical camera model \cite{jiang20243d}. To address the challenge of keypoint matching across multiple spherical images, as opposed to perspective images, various detector-based feature methods have been proposed \cite{gava2023sphereglue}. More recently, the first dense matching method, EDM \cite{EDM}, for spherical images has also been introduced. In this paper, we first introduce a detector-free SfM framework for spherical cameras by leveraging this dense matching approach.
Conversely, there are relatively few open-source software packages \cite{moulon2017openmvg,jiang20243d} that support omnidirectional cameras. 
These frameworks typically employ an incremental SfM workflow based on a spherical camera model \cite{jiang20243d}. 
However, existing methods lack a comprehensive pipeline specifically designed for omnidirectional images. 
Recently, the first dense matching method for omnidirectional images, EDM \cite{EDM}\footnote{EDM is our previous work.}, has been introduced. 
Building upon this feature matching and two-view geometry estimation \cite{solarte2021robust}, we propose a complete pipeline for spherical SfM to address the challenges inherent in indoor scene reconstruction.


\subsection{Textured Mesh Reconstruction}
Textured mesh is the basic component of photorealistic rendering. Traditional computer vision has developed effective techniques for mapping between texture and geometry. Classical texture mapping \cite{lempitsky2007seamless,waechter2014TexRecon,fu2018texture} employs a Markov Random Field to select the optimal color image for each mesh face and applies global color adjustment for consistency \cite{waechter2014TexRecon}. Recently, numerous neural rendering techniques have been developed for surface reconstruction \cite{wang2021neus,yu2022monosdf,choi2023tmo,xiao2024debsdf} and rendering \cite{mildenhall2021nerf,barron2022mip,barron2023zip,kerbl20233d}. Notably, some approaches leverage differentiable rendering \cite{nvdiffrast,PyTorch3D,Mitsuba3,munkberg2021nvdiffrec,Goel_2022_CVPR} to learn neural representations of texture, lighting, and geometry. Additionally, certain NeRF variants \cite{SNeRG,chen2023mobilenerf,NeuRas,DNMP,Choi2024LTM} employ differentiable volumetric rendering with mesh primitives, which is similar to rasterization-based rendering. Some neural rendering methods are tailored for omnidirectional cameras. EgoNeRF \cite{choi2023balanced} utilizes a spherical feature grid to represent a distant environment for rendering. OmniSDF \cite{kim2024omnisdf} introduces an adaptive spherical binoctree method for surface reconstruction. However, in sparsely scanned indoor environment, which contains lots of textureless regions and sparse viewpoints, most neural rendering methods struggle with severe artifacts. To address this limitation, we propose a hybrid approach that combines classical texture mapping with neural texture optimization.



% \subsection{Omnidirectional Camera Model}
% omnisfm, Sphereglue 
% omnisdf, omninerf
% omnigs
% EDM 
% 하지만 아직 omnidirectional camera 에서  dense matching과 detector free sfm 을 결합한 연구가 없음을 강조
% For spherical cameras, SPHORB, SphereGlue, PanoPoint, CoVisPose
% OpenMVG\cite{moulon2017openmvg}
% SphereSfM\cite{jiang20243d}
% 360-8pa \cite{solarte2021robust}