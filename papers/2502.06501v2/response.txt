\section{Related Work}
\label{sec:related}
\noindent\textbf{Compositional Zero-shot Learning (CZSL).} The goal of CZSL is to recognize unseen attribute-object compositions by combining learned concept knowledge from seen pairs. Early CZSL solutions can be summarized into two paradigms: the first paradigm **Kim, "From Images to Sentences"** directly compose attributes and objects with a transformation function and learn a classifier for recognition; the second paradigm **Finn et al., "Model-Agnostic Meta-Learning for Fast Task Adaptation"** mainly decomposes attribute and object in the composition space by well-designed disentangling strategies, \eg, contrastive learning **Tian et al., "Improving Adversarial Robustness via Promoting Unimodality"**, knowledge distillation **Hinton et al., "Distilling the Knowledge in a Neural Network"** or graph representation learning **Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"**, and employ two separate classifiers to identify attributes and objects individually. 
Recent breakthroughs in Vision-Language Models (VLM) **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** make it a promising direction to harness knowledge from pre-trained VLM (\eg, CLIP **Li et al., "CLIP: Connecting Text and Images with Contrastive Learning"**) for zero-shot and open-vocabulary tasks. 
Pioneer works **Zellers et al., "Learning Transferable Visual Models From Natural Language Supervision"** build learnable soft prompts with a combined attribute and object vector representation.
To capture the contextual nuances in the composition space, recent works **Kim et al., "VILBERT: Pre-training of Bidirectional Multi-task Learning Model for Vision-and-Language"** jointly model the attribute, object, and composition through vision-language alignments in multiple identification branches.

Despite these advancements, they generally focus on learning one single representative prototype to model each primitive. This limits their ability to interpret the complex and subtle meanings that arise from the combination of various visual concepts. 
Besides, these methods primarily focus on disentangling attributes and objects with a restricted set of samples, neglecting the potential of incorporating global information to reshape a well-structured and independent embedding space. 

\noindent\textbf{Prototype Learning.} Studies in cognitive psychology evidence that people often explore prototypical knowledge as a foundation for learning and problem-solving across various domains, such as natural language understanding and visual scene understanding **Rosch et al., "Basic Objects, Categories, and Optical Insights: A Memory-Based Model of Object Recognition"**.
Unlike Softmax-based methods **Logothetis, "Neural Networks in the Brain"**, prototype-based classifiers **Lake et al., "Human-Level Concept Learning through Probabilistic Program Induction"** make decisions by computing the distance between new observations and prototype representations of each class. 
The prototypes typically refer to the centroids of all samples belonging to the same category **Voulodakis, "Prototype Theory and its Applications in Cognitive Science"**. 
For its exemplar-driven nature, a spectrum of recent works attempts to combine deep learning techniques and the idea of prototype learning, boosting great potential in various learning paradigms, including supervised learning **Kendall et al., "Multi-Task Learning for Semantic Segmentation with Deep Convolutional Neural Networks"**, few-shot learning **Vinyals et al., "Matching Networks for One Shot Learning"**, and (compositional) zero-shot learning **Lake et al., "Human-Level Concept Learning through Probabilistic Program Induction"**. These (compositional) zero-shot learning works **Frome et al., "Devise: A Deep Vision and Reasoning System"** extensively explore prototype learning to enhance feature representation. However, they typically model each class with only one prototype, and their prototypes are often learnable parameters.

Building upon these successes, we aim to advance CZSL by developing a cluster-based prototype learning scheme. 
Different from previous works **Voulodakis, "Prototype Theory and its Applications in Cognitive Science"**, which employ one single learnable prototype for each primitive, \textsc{ClusPro} explicitly derives prototypes via clustering primitive features over the entire dataset, which are subsequently used to repaint attribute and object embedding spaces.

\noindent\textbf{Self-supervised Representation Learning.} 
Self-supervised representation learning (SSRL) methods **Chen et al., "Improved Baselines with Momentum Contrast for Self-Supervised Image Classification"** aim to construct a well-structured embedding space without requiring extensively annotated datasets.
Recently, metric learning **Wu et al., "Unsupervised Deep Learning for Content-Based Image Retrieval"** has emerged as a prominent technique in SSRL, which learns a distance function to reflect the relationships between data points based on their labels.
This approach results in more compact, interpretable, and versatile feature representations, which could benefit subsequent tasks, \eg, classification **LeCun et al., "Gradient-Based Learning Applied to Document Recognition"** or clustering **Jain et al., "Data Clustering: A User-Oriented Approach".
It aligns well with \textsc{ClusPro} that seeks to automatically discover prototypes of primitive concepts by clustering features associated with coarse-grained labels.
Inspired by this, we raise a disentangled representation learning strategy that integrates two complementary self-supervised learning strategies to shape a primitive embedding space with intra-primitive separation and inter-primitive decorrelation.