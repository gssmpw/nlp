\section{Related Work}
\label{sec:related}
\noindent\textbf{Compositional Zero-shot Learning (CZSL).} The goal of CZSL is to recognize unseen attribute-object compositions by combining learned concept knowledge from seen pairs. Early CZSL solutions can be summarized into two paradigms: the first paradigm~\cite{nagarajan2018attributes,naeem2021learning,misra2017red,purushwalkam2019task,anwaar2022leveraging,mancini2022learning,khan2023learning} directly compose attributes and objects with a transformation function and learn a classifier for recognition; the second paradigm~\cite{hao2023learning,saini2022disentangling,li2022siamese,ruis2021independent,yang2020learning,atzmon2020causal,li2023distilled} mainly decomposes attribute and object in the composition space by well-designed disentangling strategies, \eg, contrastive learning~\cite{li2022siamese}, knowledge distillation~\cite{li2023distilled} or graph representation learning~\cite{ruis2021independent}, and employ two separate classifiers to identify attributes and objects individually. 
Recent breakthroughs in Vision-Language Models (VLM)~\cite{li2022blip,jia2021scaling,radford2021learning} make it a promising direction to harness knowledge from pre-trained VLM (\eg, CLIP~\cite{radford2021learning}) for zero-shot and open-vocabulary tasks. 
Pioneer works~\cite{nayaklearning,lu2023decomposed,bao2023prompting,xu2022prompting} build learnable soft prompts with a combined attribute and object vector representation.
To capture the contextual nuances in the composition space, recent works~\cite{huang2024troika,li2024context,jing2024retrieval} jointly model the attribute, object, and composition through vision-language alignments in multiple identification branches.

Despite these advancements, they generally focus on learning one single representative prototype to model each primitive. This limits their ability to interpret the complex and subtle meanings that arise from the combination of various visual concepts. 
Besides, these methods primarily focus on disentangling attributes and objects with a restricted set of samples, neglecting the potential of incorporating global information to reshape a well-structured and independent embedding space. 

\noindent\textbf{Prototype Learning.} Studies in cognitive psychology evidence that people often explore prototypical knowledge as a foundation for learning and problem-solving across various domains, such as natural language understanding and visual scene understanding~\cite{aamodt1994case,yang2021multiple}.
Unlike Softmax-based methods~\cite{he2016deep,liu2021swin,simonyan2014very}, prototype-based classifiers~\cite{cover1967nearest,garcia2012prototype,goldberger2004neighbourhood,he2005neighborhood} make decisions by computing the distance between new observations and prototype representations of each class. 
The prototypes typically refer to the centroids of all samples belonging to the same category~\cite{snell2017prototypical}. 
For its exemplar-driven nature, a spectrum of recent works attempts to combine deep learning techniques and the idea of prototype learning, boosting great potential in various learning paradigms, including supervised learning~\cite{zhou2022rethinking,feng2023clustering,qin2023unified,ding2024clustering,liang2023clustseg,wang2024visual}, few-shot learning~\cite{hou2022closer,zhu2023transductive}, and (compositional) zero-shot learning ~\cite{xu2020attribute,ruis2021independent,hu2023leveraging}.  These (compositional) zero-shot learning works~\cite{wang2021dual,hou2024visual,chen2023protoclip,xu2020attribute} extensively explore prototype learning to enhance feature representation. However, they typically model each class with only one prototype, and their prototypes are often learnable parameters.

Building upon these successes, we aim to advance CZSL by developing a cluster-based prototype learning scheme. 
Different from previous works~\cite{li2022siamese,ruis2021independent}, which employ one single learnable prototype for each primitive, \textsc{ClusPro} explicitly derives prototypes via clustering primitive features over the entire dataset, which are subsequently used to repaint attribute and object embedding spaces.

\noindent\textbf{Self-supervised Representation Learning.} 
Self-supervised representation learning (SSRL) methods~\cite{jing2020self,ericsson2022self,liang2022gmmseg} aim to construct a well-structured embedding space without requiring extensively annotated datasets.
Recently, metric learning~\cite{kaya2019deep} has emerged as a prominent technique in SSRL, which learns a distance function to reflect the relationships between data points based on their labels.
This approach results in more compact, interpretable, and versatile feature representations, which could benefit subsequent tasks, \eg, classification~\cite{zhai2018classification,chen2024neural,liang2024clusterfomer} or clustering~\cite{asano2019self,quan2024clustering}.
It aligns well with \textsc{ClusPro} that seeks to automatically discover prototypes of primitive concepts by clustering features associated with coarse-grained labels.
Inspired by this, we raise a disentangled representation learning strategy that integrates two complementary self-supervised learning strategies to shape a primitive embedding space with intra-primitive separation and inter-primitive decorrelation.