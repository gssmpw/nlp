
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{url}
% add by qhy
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{multirow}
\usepackage{MnSymbol}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbding}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}
\newcommand{\etal}{\textit{et al}.}
\newcommand{\etc}{\textit{etc}.}
\newcommand{\cf}{\textit{cf}.}
\newcommand{\resp}{\textit{resp}.}
\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\definecolor{mygray}{gray}{.9}
\definecolor{ggreen}{RGB}{18,190,108}
\usepackage{colortbl}
\setcitestyle{square,numbers,sort&compress,citesep={,}}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\iclrfinalcopy %取消匿名，取消行数
\def\tableautorefname{Table}
\def\figureautorefname{Figure}
\def\sectionautorefname{\S}
\def\subsectionautorefname{\S}
\def\algorithmautorefname{Algorithm}
\def\equationautorefname{Eq.}
\newcommand{\pub}[1]{\color{gray}{\tiny{[{#1}]}}}
\newcommand{\model}{\textsc{ClusPro}\xspace}
\definecolor{codegreen}{RGB}{79,126,127}
\definecolor{codedefine}{RGB}{153,54,159}
\definecolor{codefunc}{RGB}{73,122,234}
\definecolor{codecall}{RGB}{73,122,234}
\definecolor{codepro}{RGB}{212,96,80}
\definecolor{codedim}{RGB}{89,152,195}
\definecolor{codeorg}{RGB}{109,162,200}
\usepackage{wrapfig}

%\title{Cluster-based Sub-Primitive Prototype-Aware Learning for Compositional Zero-shot Learning}
\title{Learning Clustering-based  Prototypes for Compositional Zero-shot Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Hongyu Qu$^{1}$\thanks{Equal contribution}~~, Jianan Wei$^{2}$\footnotemark[1]~~, Xiangbo Shu$^{1}$\thanks{Corresponding author}~~, Wenguan Wang$^{2,3}$ \\
$\rm^1$Nanjing University of Science and Technology~~~~$\rm^2$Zhejiang University \\
$\rm^3$National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,\\~~Xi'an Jiaotong University  \\
% {\footnotesize
% \texttt{minghan.chen@student.uts.edu.au}
% \texttt{\{guikunchen, wenguanwang.ai\}@gmail.com} 
% }
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}
Learning primitive (\ie, attribute and object) concepts from seen compositions is the primary challenge of Compositional Zero-Shot Learning (CZSL). Existing CZSL solutions typically rely on oversimplified data assumptions, \eg, modeling each primitive with a single centroid primitive representation, ignoring the natural diversities of the attribute (\resp~object) when coupled with different objects (\resp~attribute). In this work, we develop \textsc{ClusPro}, a robust \underline{clus}tering-based \underline{pro}totype mining framework for CZSL that defines the conceptual boundaries of primitives through a set of diversified prototypes. Specifically, \textsc{ClusPro} conducts within-primitive clustering on the embedding space for automatically discovering and dynamically updating prototypes. These representative prototypes are subsequently used to repaint a well-structured and independent primitive embedding space, ensuring intra-primitive separation and inter-primitive decorrelation through prototype-based contrastive learning and decorrelation learning. Moreover, \textsc{ClusPro} efficiently performs prototype clustering in a non-parametric fashion without the introduction of additional learnable parameters or computational budget during testing. Experiments on three benchmarks demonstrate \textsc{ClusPro} outperforms various top-leading CZSL solutions under both closed-world and open-world settings. Our code is available at \href{https://github.com/quhongyu/ClusPro}{\textsc{ClusPro}}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\begin{wrapfigure}[16]{r}{0.50\textwidth}
\vspace{-.5cm}
        \hspace{+0.04cm}
        \centering
		\includegraphics[width=1\linewidth]{motivation.pdf}
\captionsetup{font=small,width=1\linewidth}
 \vspace{-.6cm}
	\caption{
 \small{(a) Previous CZSL methods model all samples of each primitive concept with only one centroid primitive presentation, neglecting feature divergence within each primitive when involved in different compositions. $\!$(b) Our method represents each primitive as a set of prototypes to capture primitive diversities. 
 }}
 \label{fig:motivation}
\end{wrapfigure}
\vspace{-0.2cm}
Humans possess the unique ability to recognize a potentially infinite number of novel combinations by associating known components~\cite{hebart2020revealing}, \ie, to make ``infinite use of finite means''~\cite{chomsky2014aspects}; for instance, despite never having seen one,
people can easily imagine a unicorn by combining their concept of a horse with the idea of a single horn. Inspired by such compositional generalization ability of human intelligence~\cite{lake2017building,atzmon2016learning}, Compositional Zero-Shot Learning (CZSL)~\cite{misra2017red,liu2023simple,li2020symmetry,kim2023hierarchical,mancini2021open} is proposed, aiming to recognize unseen attribute-object compositions based on learned knowledge from seen ones.

Existing CZSL solutions~\cite{hu2023leveraging,jiang2024revealing,wang2023learning,zhang2022learning} typically achieve compositional learning by aligning the visual representation from a pre-trained image encoder backbone with the attribute-object textual representation derived from pre-trained word embeddings. 
Rather than learning to align visual and textual representation from scratch, recent approaches~\cite{huang2024troika,li2024context,bao2023prompting,nayaklearning,lu2023decomposed} have pivoted towards leveraging large-scale pre-trained vision-language models (\eg, CLIP\footnote{Given that CLIP might be exposed to certain unseen compositions during pre-training, we provide detailed data overlap discussion in  \S\ref{sec_app:A5} of Appendix.}~\cite{radford2021learning}) by treating compositional labels as learnable tokens in a pre-defined prompt like ``\textit{a photo of [attribute] [object]}''.
Though impressive, these methods exhibit two limitations:
\textbf{First}, they struggle to learn visual concepts by modeling an ``ideal'' primitive (\ie, attribute and object), but ignore an essential issue: each visual concept (\ie, attribute-object pairs) can be semantically similar but visually different.
For example, the attribute ``broken'' combined with ``cord'' typically signifies disconnection, but conveys the notion of a rugged landscape when applied to ``valley''. Thus, we argue that a single centroid primitive representation exhibits limited tolerance to intra-primitive variance (Fig.~\ref{fig:motivation}a), and it is essential to incorporate more exemplars to capture the natural diversities of primitive.
\textbf{Second}, they endeavor to explore more effective disentangling strategies
(\eg, contrastive learning~\cite{li2022siamese}, knowledge distillation~\cite{li2023distilled} or graph representation learning~\cite{ruis2021independent}) to
achieve independent primitive modeling in a multi-branch manner, but typically present representation disentanglement from a local view (\ie, a few images within a batch~\cite{li2022siamese,hao2023learning} or a small training
subset~\cite{jing2024retrieval}), thus failing to perceive underlying data distribution in the entire dataset.

In light of the above, we present \textsc{ClusPro}, a \underline{clus}tering-based \underline{pro}totype mining framework for CZSL (Fig.~\ref{fig:motivation}b). 
Specifically, we propose to describe each primitive by abstracting it through a set of representative prototypes, which are automatically discovered by performing within-primitive clustering on the visual representation. 
Based on these prototypes established across the entire dataset, we introduce two complementary self-supervised learning strategies to repaint the attribute and object embedding spaces, prompting \textit{intra-primitive separation} and \textit{inter-primitive decorrelation}.

Specifically, \textsc{ClusPro} employs two disentangling adapters to project visual representation, extracted from a pre-trained image encoder, into separate attribute and object embedding spaces. 
Then, each primitive is described by clustering $K$ centroids on primitive-wise features. 
This process (\ie, \textbf{Local-aware Prototype Assignment}) involves assigning the visual feature of each primitive to one
of a set of prototypes that share the same attribute or object category, while considering the intrinsic coherence of the feature distribution.
For computational efficiency, we opt for Generalized Conditional Gradient (GCG) algorithm~\cite{rakotomamonjy2015generalized} to enable fast prototype assignment. 
% Since \textsc{ClusPro} constructs non-learnable prototypes directly from the embedding space via non-parametric clustering, 
Additionally, to keep non-learnable prototypes up-to-date, we employ a dynamic \textbf{Prototype Updating} mechanism, which recomputes prototypes over the entire dataset in each iteration.
The attribute embedding and object embedding, derived from the same visual representation with compositional semantics, inherently exhibit entanglement, which is toxic for prototype construction within primitive.  
To learn well-structured and independent attribute/object embedding space, we propose two complementary metric learning mechanisms: 
\textbf{i)} \textit{Prototype-based Contrastive Learning} aims to encourage each primitive feature to be similar to its assigned prototype and dissimilar with all other prototypes from the attribute and object branch. In this way, our model can not only capture intra-primitive discriminativeness within the group of attributes or objects, but also promote a clear distinction between attributes and objects. 
\textbf{ii)} \textit{Prototype-based Decorrelation Learning} is devised to shape an independent primitive embedding space (\ie, object representation should be invariant to attribute alterations, and vice versa) by exploring conditional-independence relations between attributes and objects.

%\textsc{ClusPro} has several appealing merits: 
%\textbf{First}, comprehensive modeling of \textbf{\textit{data distribution}}: Unlike previous methods with oversimplified assumptions (\ie, modeling all instances of each primitive with one prototype), \textsc{ClusPro} performs more realistic and comprehensive modeling with a set of prototypes to well represent diverse patterns within each primitive. By conducting within-primitive clustering on the visual embedding space across the entire dataset, \textsc{ClusPro} can automatically mine the global data distribution of each primitive via a set of prototypes from a holistic view. 
%\textbf{Second}, explicit supervision of \textbf{\textit{representation disentanglement}}: The clustering-based prototypes enable \textsc{ClusPro} to directly shape well-structured yet independent attribute and object embedding spaces via prototype-anchored contrastive learning and decorrelation learning. Such improved primitive embedding spaces, in turn, enable typical within-primitive
%variation pattern mining. 
%\textbf{Third}, high \textbf{\textit{efficiency}}: \textsc{ClusPro} performs prototype clustering in an online fashion without any modification of network architecture during training and without extra inference burden during testing.
\textsc{ClusPro} has several appealing merits: 
\textbf{First}, comprehensive modeling of \textbf{\textit{data distribution}}: By conducting within-primitive clustering on the visual embedding space across the entire dataset, \textsc{ClusPro} can automatically mine the global data distribution of each primitive from a holistic view. 
\textbf{Second}, explicit supervision of \textbf{\textit{representation disentanglement}}: The clustering-based prototypes enable \textsc{ClusPro} to directly shape well-structured yet independent attribute and object embedding spaces via prototype-anchored contrastive learning and decorrelation learning. Such improved primitive embedding spaces, in turn, enable typical within-primitive variation pattern mining.  
\textbf{Third}, high \textbf{\textit{efficiency}}: \textsc{ClusPro} perform prototype clustering in a non-parametric fashion without any modification of network
architecture or additional computational budget during testing.

To effectively assess our method, we conduct extensive experiments on three gold-standard CZSL datasets (\ie, MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine}, and {C-GQA}~\cite{naeem2021learning}). 
Experimental results demonstrate that \textsc{ClusPro} significantly exceeds existing state-of-the-arts in both \textit{Close-world (CW)} and \textit{Open-world (OW)} settings (\S\ref{sec::sota}).
Concretely, on the \textit{CW} setting, \textsc{ClusPro} achieves \textbf{+11.8\%} and \textbf{+20.2\%} AUC gains on UT-Zappos and C-GQA, respectively. 
On the \textit{OW} settings, it also yields solid improvements of \textbf{+19.7\%} AUC on UT-Zappos and \textbf{+11.1\%} AUC on C-GQA. In \S\ref{sec::abtion}, a set
of ablative studies confirms the power of our idea and the efficacy of core model designs.

\section{Related Work}
\label{sec:related}
\noindent\textbf{Compositional Zero-shot Learning (CZSL).} The goal of CZSL is to recognize unseen attribute-object compositions by combining learned concept knowledge from seen pairs. Early CZSL solutions can be summarized into two paradigms: the first paradigm~\cite{nagarajan2018attributes,naeem2021learning,misra2017red,purushwalkam2019task,anwaar2022leveraging,mancini2022learning,khan2023learning} directly compose attributes and objects with a transformation function and learn a classifier for recognition; the second paradigm~\cite{hao2023learning,saini2022disentangling,li2022siamese,ruis2021independent,yang2020learning,atzmon2020causal,li2023distilled} mainly decomposes attribute and object in the composition space by well-designed disentangling strategies, \eg, contrastive learning~\cite{li2022siamese}, knowledge distillation~\cite{li2023distilled} or graph representation learning~\cite{ruis2021independent}, and employ two separate classifiers to identify attributes and objects individually. 
Recent breakthroughs in Vision-Language Models (VLM)~\cite{li2022blip,jia2021scaling,radford2021learning} make it a promising direction to harness knowledge from pre-trained VLM (\eg, CLIP~\cite{radford2021learning}) for zero-shot and open-vocabulary tasks. 
Pioneer works~\cite{nayaklearning,lu2023decomposed,bao2023prompting,xu2022prompting} build learnable soft prompts with a combined attribute and object vector representation.
To capture the contextual nuances in the composition space, recent works~\cite{huang2024troika,li2024context,jing2024retrieval} jointly model the attribute, object, and composition through vision-language alignments in multiple identification branches.

Despite these advancements, they generally focus on learning one single representative prototype to model each primitive. This limits their ability to interpret the complex and subtle meanings that arise from the combination of various visual concepts. 
Besides, these methods primarily focus on disentangling attributes and objects with a restricted set of samples, neglecting the potential of incorporating global information to reshape a well-structured and independent embedding space. 

\noindent\textbf{Prototype Learning.} Studies in cognitive psychology evidence that people often explore prototypical knowledge as a foundation for learning and problem-solving across various domains, such as natural language understanding and visual scene understanding~\cite{aamodt1994case,yang2021multiple}.
Unlike Softmax-based methods~\cite{he2016deep,liu2021swin,simonyan2014very}, prototype-based classifiers~\cite{cover1967nearest,garcia2012prototype,goldberger2004neighbourhood,he2005neighborhood} make decisions by computing the distance between new observations and prototype representations of each class. 
The prototypes typically refer to the centroids of all samples belonging to the same category~\cite{snell2017prototypical}. 
For its exemplar-driven nature, a spectrum of recent works attempts to combine deep learning techniques and the idea of prototype learning, boosting great potential in various learning paradigms, including supervised learning~\cite{zhou2022rethinking,feng2023clustering,qin2023unified,ding2024clustering,liang2023clustseg,wang2024visual}, few-shot learning~\cite{hou2022closer,zhu2023transductive}, and (compositional) zero-shot learning ~\cite{xu2020attribute,ruis2021independent,hu2023leveraging}.  These (compositional) zero-shot learning works~\cite{wang2021dual,hou2024visual,chen2023protoclip,xu2020attribute} extensively explore prototype learning to enhance feature representation. However, they typically model each class with only one prototype, and their prototypes are often learnable parameters.

Building upon these successes, we aim to advance CZSL by developing a cluster-based prototype learning scheme. 
Different from previous works~\cite{li2022siamese,ruis2021independent}, which employ one single learnable prototype for each primitive, \textsc{ClusPro} explicitly derives prototypes via clustering primitive features over the entire dataset, which are subsequently used to repaint attribute and object embedding spaces.

\noindent\textbf{Self-supervised Representation Learning.} 
Self-supervised representation learning (SSRL) methods~\cite{jing2020self,ericsson2022self,liang2022gmmseg} aim to construct a well-structured embedding space without requiring extensively annotated datasets.
Recently, metric learning~\cite{kaya2019deep} has emerged as a prominent technique in SSRL, which learns a distance function to reflect the relationships between data points based on their labels.
This approach results in more compact, interpretable, and versatile feature representations, which could benefit subsequent tasks, \eg, classification~\cite{zhai2018classification,chen2024neural,liang2024clusterfomer} or clustering~\cite{asano2019self,quan2024clustering}.
It aligns well with \textsc{ClusPro} that seeks to automatically discover prototypes of primitive concepts by clustering features associated with coarse-grained labels.
Inspired by this, we raise a disentangled representation learning strategy that integrates two complementary self-supervised learning strategies to shape a primitive embedding space with intra-primitive separation and inter-primitive decorrelation.

\section{Methodology}
\label{sec::method}
\subsection{Problem Statement}
Given the attribute set  $\mathcal{A} = \{ a_1, a_2, \dots, a_{M} \}$ and the object set $\mathcal{O} = \{ o_1, o_2, \dots, o_{N} \}$, the compositional label set $\mathcal{C}$ can be defined as the Cartesian product between $\mathcal{A}$ and $\mathcal{O}$, \ie, $\mathcal{C} = \mathcal{A} \times \mathcal{O}$. 
Subsequently, $\mathcal{C}$ is divided into two disjoint subsets: the seen composition set $\mathcal{C}^{s}$ and the unseen composition set $\mathcal{C}^{u}$, where $\mathcal{C}^{s} \cap \mathcal{C}^{u} = \emptyset$.
During training, the model can only access images paired with labels from the seen composition set $\mathcal{C}^{s}$, \ie, the training set is defined as $\mathcal{T} = \{ (x, c) | x \in \mathcal{X}, c \in \mathcal{C}^{s} \}$, where $\mathcal{X}$ is the visual space. 
In the Closed-World (\textit{CW}) setting, the composition space for testing is defined as $\mathcal{C}^{t} = \mathcal{C}^{s} \cup \mathcal{C}^{u}$, where only the known composition space is required. 
For the Open-World (\textit{OW}) setting, the composition space considers all potential attribute-object pairs, \ie, $\mathcal{C}^{t} = \mathcal{C}$. 

\subsection{Baseline Architecture}
\label{sec:base}
\noindent\textbf{Encoding Visual Representations.} Our framework is built upon a three-path paradigm~\cite{huang2024troika,hao2023learning,saini2022disentangling}, which jointly recognizes three kinds of semantic components, \ie, attribute, object, and attribute-object composition. 
Given an input image $X\!\in \!\mathbb{R}^{H \times W \times 3}$, we adopt a visual encoder ${\phi}^{\text{vis}}$ of CLIP~\cite{huang2024troika} to obtain visual representation $\bm{f} \!\in \!\mathbb{R}^{D}$. 
We consider image representation $\bm{f}$ as composition visual representation $\bm{f}^{c}$, and adopt attribute adapter ${h}^{a}$ and object adapter ${h}^{o}$~\cite{houlsby2019parameter,hu2021lora}, each implemented as a separate MLP, to project $\bm{f}$ into the discriminative attribute and object spaces respectively:
\vspace{-3pt}
\begin{small}
\begin{equation}
\label{visual}
\setlength\belowdisplayskip{2pt}
\bm{f}^{a}={h}^{a}(\bm{f}),~~~~~\bm{f}^{o}={h}^{o}(\bm{f}),~~~~~\bm{f}^{c}=\bm{f},
\end{equation}
\end{small}
where $\bm{f}^{a}$ and $\bm{f}^{o} $ are visual features extracted for attribute and object, respectively. % This approach ensures, to a certain degree, the independence of the two primitive feature spaces.
\begin{figure}[!t]
    \centering
   \includegraphics[width=1.0\textwidth]{overview.pdf} 
   \put(-166, 101){\scriptsize Eq.~(\ref{eq:nc1}), (\ref{eq:nc2})}
   \put(-67, 101){\scriptsize Eq.~(\ref{eq:update})}
   \put(-153, 30){\scriptsize {$\mathcal{L}_{\text{PCL}}$}}
   \put(-156, 22){\scriptsize Eq.~(\ref{eq::ipc})}
   \put(-26, 30){\scriptsize {$\mathcal{L}_{\text{PDL}}$}}
   \put(-28, 22){\scriptsize Eq.~(\ref{eq::PDL})}
   \put(-257, 61){\scriptsize {$\mathcal{L}_{o}$}}
   \put(-252, 80){\scriptsize {$\mathcal{L}_{c}$}}
   \put(-246, 99){\scriptsize {$\mathcal{L}_{a}$}}
   \put(-317, 19){\scriptsize {${\phi}_{\text{txt}}$}}
   \put(-317, 83){\scriptsize {${\phi}_{\text{vis}}$}}
   \put(-191, 123.3){\scriptsize {$\{\bm{f}^{a}_n\}_{n=1\!}^{N^{a}}$}}
    \put(-21.5, 123.4){\scriptsize {$\{\bm{p}_{k}^a\}_{k=1\!}^K$}}
  \vspace{-5pt}  \caption{\small{The overview of \textsc{ClusPro}. \textbf{(a)} \textsc{ClusPro} is built upon a three-path paradigm to jointly recognize attribute, object, and attribute-object composition (\S\ref{sec:base}). \textbf{(b)} To capture the diversity within each primitive, \textsc{ClusPro} describes each primitive with a set of prototypes, and conducts within-primitive clustering across training data for prototype assignment and updating (\S\ref{sec::pro}). \textbf{(c)} \textsc{ClusPro} imposes two constraints based on these constructed prototypes to promote intra-primitive separation and inter-primitive decorrelation (\S\ref{sec::rep}). }}
    \label{fig:overview}
  \vspace{-15pt}
\end{figure}

\noindent\textbf{Encoding Prompt Representations.} 
Following existing CZSL~\cite{lu2023decomposed,huang2024troika}, we construct prompt representation via a soft learnable prompt strategy for all candidate compositions, attributes, and objects. Specifically, for each attribute-object composition $c_{i,j}=\langle a_i, o_j \rangle$, we create three prompts for each branch, \ie, attribute prompt $\bm{S}^a_i = [ \bm{s}^a_1, \dots, \bm{s}^a_l, \bm{v}^a_i]$, object prompt $\bm{S}^o_j = [ \bm{s}^o_1, \dots, \bm{s}^o_l, \bm{v}^o_j]$, and composition prompt $\bm{S}^c_{i,j} = [ \bm{s}^c_1, \dots, \bm{s}^c_l, \bm{v}^a_i, \bm{v}^o_j]$, where $\bm{s}^a_{1:l}$, $\bm{s}^o_{1:l}$, and $\bm{s}^c_{1:l}$ are learnable pretix contexts initialized by ``\textit{a photo of}". 
Additionally, $\bm{v}^a_i$ and $\bm{v}^o_j$ are trainable vocabulary tokens for the attribute $a_i$ and object $o_j$, respectively.
These prompts are then fed into frozen text encoder ${\phi}^{\text{txt}}$ of CLIP to obtain corresponding prompt features, formulated as:
\vspace{-3pt}
\begin{small}
\begin{equation}
\label{text}
\setlength\belowdisplayskip{2pt}
\bm{t}^a_i={\phi}^{\text{txt}}(\bm{S}^a_i),~~~~~\bm{t}^o_j={\phi}^{\text{txt}}(\bm{S}^o_j),~~~~~\bm{t}^c_{i,j}={\phi}^{\text{txt}}(\bm{S}^c_{i,j}).
\end{equation}
\end{small}

\noindent\textbf{Three-path Learning Objective.} Given visual and prompt representations from three branches, we compute the probabilities for attribute, object, and composition classes, denoted as $p(a_i|\bm{f}_n)$, $p(o_j|\bm{f}_n)$, $p(c_{i,j}|\bm{f}_n)$, respectively. 
To recognize primitive concepts and their compositions in each branch, three cross-entropy loss functions are employed: 
\vspace{-3pt}
\begin{small}
\begin{equation}
\label{eq:a}
\setlength\belowdisplayskip{2pt}
\mathcal{L}^a = \!\frac{1}{N}\sum^N\nolimits_{n=1}\! -\! \log p(a|\bm{f}_n), ~~~~~ p(a_i|\bm{f}_n) = \frac{\exp(\bm{f}^a_n \cdot \bm{t}^a_i / \tau)}{\sum^{|\mathcal{A}|}_{k=1}\exp(\bm{f}^a_n \cdot \bm{t}^a_k / \tau)}, \\
\end{equation}
\end{small}
\vspace{-2pt}
\begin{small}
\begin{equation}
\label{eq:o}
\setlength\belowdisplayskip{2pt}
\mathcal{L}^o = \!\frac{1}{N}\sum^N\nolimits_{n=1}\! -\! \log p(o|\bm{f}_n), ~~~~~ p(o_j|\bm{f}_n) = \frac{\exp(\bm{f}^o_n \cdot \bm{t}^o_j / \tau)}{\sum^{|\mathcal{O}|}_{k=1}\exp(\bm{f}^o_n \cdot \bm{t}^o_k / \tau)}, \\
\end{equation}
\end{small}
\vspace{-2pt}
\begin{small}
\begin{equation}
\label{eq:p}
\setlength\belowdisplayskip{2pt}
\mathcal{L}^c = \!\frac{1}{N}\sum^N\nolimits_{n=1}\! -\! \log p(c|\bm{f}_n), ~~~~~ p(c_{i,j}|\bm{f}_n) = \frac{\exp(\bm{f}^c_n \cdot \bm{t}^c_{i,j} / \tau)}{\sum^{|\mathcal{C}|}_{k=1}\exp(\bm{f}^c_n \cdot \bm{t}^c_k / \tau)}, \\
\end{equation}
\end{small}
where $\tau\!\in \!\mathbb{R} $ is pre-defined temperature parameter in CLIP. For simplicity, all the features are $\ell_2$-normalized by default. Then, the three-path classification loss can be formulated as:
\vspace{-3pt}
\begin{small}
\begin{equation}
\label{eq::bas}
\setlength\belowdisplayskip{2pt}
\mathcal{L}^{\text{BAS}}\! =\! \lambda^a\mathcal{L}^a\!+\!\lambda^o\mathcal{L}^o\!+\!\lambda^c\mathcal{L}^c,
\end{equation}
\end{small}
where $\lambda^a, \lambda^o, \lambda^c$ are all set to $1$, following~\cite{huang2024troika}.

\noindent\textbf{Our Main Idea.} 
Though impressive, this three-branch paradigm only achieves implicit feature disentanglement to a limited extent by using one single image, failing to perceive the potential structures of the whole dataset. 
Moreover, it only considers an isolated centroid for each primitive, ignoring rich and diverse intra-primitive patterns. 
To address this limitation, we propose a clustering-based prototype mining framework (\ie, \textsc{ClusPro}), as shown in Fig.~\ref{fig:overview}. 
Our model not only learns primitive recognition with pre-given semantic labels, but also automatically discovers diverse and fine-grained sub-primitive patterns across the entire dataset. 
For training, our algorithm alternates between two steps: \textbf{i)} perform primitive-wise online clustering to discover sub-primitive prototypes (\S\ref{sec::pro}); \textbf{ii)} impose two prototype-anchored constraints to explicitly shape well-structured and independent attribute/object feature spaces(\S\ref{sec::rep}).  The improved features, in turn, facilitate more reliable primitive-wise clustering, and eventually boost composition predictions.

\subsection{Clustering-based Prototype Mining}
\label{sec::pro}
To model the natural diversities of primitives, we exploit rich dataset-level context knowledge to automatically identify informative prototypes within each attribute or object, facilitating primitive concept representation learning. 
Specifically, we first assign each attribute (\resp~object) visual feature to the prototypes belonging to the same attribute (\resp~object) (\ie, \textbf{Local-aware Prototype Assignment}), and then continuously update prototypes online according to the assignments (\ie, \textbf{Prototype Updating}) with batch training. Such an online clustering strategy forces the model to mine intra-primitive discriminativeness. 
Notably, we present the online primitive-wise clustering process within both the attribute and object embedding spaces, so as to well represent rich and diverse patterns within each primitive. For clarity, we only explain the prototype construction in the attribute branch, while the object branch follows the same process. 


\noindent\textbf{Local-aware Prototype Assignment.} For each attribute $a \in \mathcal{A}$, we leverage $K$ prototypes ($\{\bm{p}_{k}^a\}_{k=1\!}^K$)\footnote{For clarity, we slightly reuse $a$ and $o$ to define a certain attribute and object concept, respectively.} to represent its diverse semantic patterns, where  $\bm{p}_{k}^a$ is $k$-th prototypes of attribute $a$. To get informative yet hidden prototypes, we perform clustering within each attribute on the attribute embedding space. 
More specifically, for given a set of attribute features $\bm{F}^{a}\!=\!\{\bm{f}^{a}_n\}_{n=1\!}^{N^{a}} \in\!\mathbb{R}^{D\times N^{a}}$ associated with attribute $a$, where  $\bm{f}^{a}_n$ is $n$-th attribute features of attribute $a$ and $N^{a}$ is the number of attribute features, our goal is to assign these attribute features to the $K$ prototypes $\bm{P}^{a\!}\!=\!\{\bm{p}_{k}^a\}_{k=1\!}^K\!\in\!\mathbb{R}^{D\times K}$. 
The mapping matrix from $\bm{F}^{a}$ to $\bm{P}^{a\!}$ can be denoted as $\bm{L}^{a\!}\!=\![\bm{l}^{a\!}_{n}]_{n=1\!}^{N^{a}}\!\in\!\{0,1\}^{K \times N^{a}}$, where $\bm{l}^{a\!}_{n}\!\in\!\{0,1\}^{K}$ is an one-hot assignment vector of $n$-th attribute features over $K$ prototypes. 
Let $\bm{S}^{a\!}\!\in\!\mathbb{R}^{N^{a}\times N^{a}}$ denote cosine similarity among these attribute features $\bm{F}^{a}\!\in\!\mathbb{R}^{D\times N^{a}}$ in the attribute embedding space. 
Thus, the clustering within each attribute can be achieved by the optimization of the assignment matrix $\bm{L}^{a\!}$, \ie, maximizing the similarity $\bm{Q}^{a}$ between attribute features $\bm{F}^{a}$ and the prototypes $\bm{P}^{a}$ (\ie, $\bm{Q}^{a}=\mathrm{Softmax}(\bm{P}^{a\top\!}\bm{F}^{a\!})\!\in\!\mathbb{R}^{K\times N^{a}}$), while considering intrinsic coherence structure of features:
%\vspace{-3pt}
\begin{small}
 \begin{equation}
  \begin{gathered}\label{eq:nc1}
  \!\!\min_{\bm{L}^{a}\in \mathcal{L}^{a}}\!\langle\bm{L}^{a\top\!},-\log\bm{Q}^{a}\rangle + \kappa\Omega(\bm{L}^{a\top\!}) ,
  \end{gathered}
\end{equation}
\end{small}
where  $\langle \cdot\rangle$ is the Frobenius dot-product.  
Note that $\Omega(\bm{L}^{a\top\!})=-\langle\bm{S}^{a\!},(\bm{L}^{a\!}\odot\bm{Q}^{a}{)^\top}{(\bm{L}^{a\!}\odot\bm{Q}^{a})} \rangle$ is local coherent regularized term~\cite{chang2023csot}, and $\kappa\!>\!0$ is the strength of the regularization, where $\odot$ denotes element-wise multiplication. Different from the classical formulation in~\cite{zhou2022rethinking,chen2022prompt}, \ie, Optimal Transport with entropic constraints, our local-aware prototype assignment can produce superior assignments by fully considering the intrinsic coherence structure of attribute feature distribution, \ie, intra-distribution coherence. 
Specifically, this term promotes assigning higher weights to $\bm{L}^{a\!}_{k,i}$ and $\bm{L}^{a\!}_{k,j}$ if the $i$-th and $j$-th attribute feature are highly similar (indicated by a high value of $\bm{S}^{a\!}_{i,j}$) and both exhibit a strong similarity, as measured by $\bm{Q}^{a\!}_{k,i}$ and $\bm{Q}^{a\!}_{k,j}$, to the $k$-th prototype of attribute $a$.

As in~\cite{asano2019self,wangvisual,zhou2024prototype}, we relax $\bm{L}^{a\!}$ to an element of transportation polytopes, \ie, $\bm{L}^{a\!}\!\in\!\mathbb{R}_{+}^{K \times N^{a}}$. Unlike offline clustering~\cite{caron2018deep,caron2020unsupervised} requiring multiple passes over the entire dataset, we cast prototype assignment as an optimal transport problem, so as to scale our algorithm to massive data by online clustering:
\vspace{-3pt}
\begin{small}
\begin{equation}
  \begin{gathered}\label{eq:nc2}
    \!\!\!\!\mathcal{L}^{a}\!=\!\{\bm{L}^{a}\!\in\!\mathbb{R}_{+}^{K \times N^{a}}|\bm{L}^{a\top\!}\!\bm{1}_K\!=\!\bm{1}_{N^{a}}, \bm{L}^{a}\bm{1}_{N^{a}}\!=\!\frac{N^{a}}{K}\bm{1}_K\},
  \end{gathered}
\end{equation}
\end{small}
where $\bm{1}_K$ denotes the vector of all ones in dimension $K$.
$\bm{L}^{a\top\!}\!\bm{1}_K\!=\!\bm{1}_{N^{a}}$ is the assignment constraint ensuring each attribute feature is assigned to exactly one prototype, and $\bm{L}^{a}\bm{1}_{N^{a}}\!=\!\frac{N^{a}}{K}\bm{1}_K$ is the equipartition constraint,
guaranteeing that, on average, each prototype is selected an equal number of times in the batch. 
With differentiable regularized term and soft assignment relaxation, the solution of Prob.~(\ref{eq:nc2}) can be given by efficient GCG algorithm~\cite{rakotomamonjy2015generalized}, which relies on a few matrix-vector multiplications via iterative Dykstra algorithm~\cite{dykstra1983algorithm}. 


\noindent\textbf{Prototype Updating.} 
During iterative network training, primitive representations evolve continuously, necessitating offline clustering to recompute sub-primitive prototypes over the entire dataset after each batch, which incurs substantial computational costs.
To address this, we propose an \textit{online} clustering approach with momentum updates, where prototypes are dynamically updated using the embeddings within the current mini-batch.
In particular, after each training iteration, each prototype $\bm{p}_{k}^a$ of the attribute $a \in \mathcal{A}$ is updated as: 
\vspace{-3pt}
\begin{small}
\begin{equation}
\begin{aligned}\label{eq:update}
\bm{p}_{k}^a \leftarrow  \mu\bm{p}_{k}^a + (1-\mu)\bar{\bm{f}}_{k}^a,
\end{aligned}
\vspace{-1.3pt}
\end{equation}
\end{small}
where $\mu\!\in\![0,1]$ is a momentum coefficient, and $\bar{\bm{f}}_{k}^a\!\in\!\mathbb{R}^D$ is the mean vector of the attribute features assigned to the prototype $\bm{p}_{k}^a$ by clustering. 
As such, the prototype updating scheme (Eq.~\ref{eq:update}) iteratively refines the prototype values in response to the evolving primitive feature representations, thereby facilitating a smoother training process. 
This online clustering strategy enables our model to effectively discover rich sub-primitive patterns over massive training data.
% At the same time, for each object $o\!\in\!\mathcal{O}$, the same online clustering strategy is adopted to automatically discover discriminative sub-primitive prototypes, \ie, $\{\bm{p}_{k}^o\}_{k=1\!}^K$. 

\subsection{Prototype-anchored Primitive Representation Learning}
\label{sec::rep}
By performing online within-primitive clustering separately in the attribute and object embedding spaces, we construct a set of prototypes for each attribute, \ie, $\{\bm{p}_{k}^a\}_{k=1\!}^K$, and each object, \ie, $\{\bm{p}_{k}^o\}_{k=1\!}^K$, to represent diverse sub-primitive patterns. 
Therefore, the following question naturally arises: \textit{what should a well-structured and independent embedding with discriminative prototypes be like?}
To answer this, we enhance the three-path classification loss (Eq.~\ref{eq::bas}) by incorporating two complementary loss constraints based on these constructed prototypes: \textbf{Prototype-based Contrastive Learning} and \textbf{Prototype-based Decorrelation Learning}, which fully exploits the relationships between primitive features and sub-primitive centers in the embedding space.

\noindent\textbf{Prototype-based Contrastive Learning.} 
Our prototype-based contrastive learning strategy contrasts the similarities between each primitive feature, \ie, $\bm{f}_{n} \in \bm{F}^{a} \cup \bm{F}^{o}$,  where $\bm{f}_n$ is $n$-th primitive features, \ie, $\bm{P}^{a} \cup \bm{P}^{o}$. 
This strategy encourages each primitive feature $\bm{f}_{n}$ to be similar to its assigned prototype $\bm{p}_{+}$ and dissimilar to all other $K(M\!+\!N)\!-\!1$ irrelevant prototypes $\mathcal{P}_{-}$. Different from only using $K(M\!+\!N\!-\!1)$ irrelevant prototypes from other primitives as negative samples, our strategy not only ensures inter-primitive separation to some extent, but also guarantee intra-primitive separation. The corresponding training objective for each features $\bm{f}_{n}$ is defined as: 
\vspace{-3pt}
\begin{small}
\begin{equation}
\begin{aligned}\label{eq::ipc}
\!\!\!\mathcal{L}_{\text{PCL}}\!=\!-\!\log \frac{\exp(\bm{f}_{n}^\top\cdot\bm{p}_{+}/\tau)}{\exp(\bm{f}_{n}^\top\cdot\bm{p}_{+}/\tau)\!+\!\sum_{\bm{p}_{-}\in\mathcal{P}_{-}\!} \exp(\bm{f}_{n}^\top\cdot\bm{p}_{-}/\tau)}),\!\!
\end{aligned}
\vspace{-2pt}
\end{equation}
\end{small}
where $\tau \!>\!0$ is a temperature hyper-parameter. Notably, we treat both attributes and objects equally as primitives to increase the scale and diversity of negative samples.
% thus learning a robust embedding space with multiple prototypes. 

Our prototype-based contrastive learning exhibits two primary advantages: 
\ding{182}~Traditional contrastive learning approaches often rely on sophisticated negative sampling strategies to form contrasting pairs, but inevitably yield negative pairs that share similar semantic meaning and should be closer in the embedding space. In contrast, \textsc{ClusPro} avoids this long-standing challenge by constructing positive and negative pairs using clustering-based representative prototypes, thus effectively shaping the embedding space by leveraging dataset-level contextual knowledge.
\ding{183}~Unlike previous contrastive learning-based CZSL models~\cite{li2022siamese,yang2023dual}, which necessitate the extra processing for positive and negative feature extraction, our model leverages already constructed prototypes for contrast computation, without incurring extra computational and storage budget. 

\noindent\textbf{Prototype-based Decorrelation Learning.} 
By leveraging prototype-based contrastive learning, \textsc{ClusPro} can effectively distinguish primitive prototypes by maximizing their distance to enhance the independence of linear relationships. 
But, in CZSL, it is also essential to maintain independence between the embeddings of attributes and objects, \ie, inter-primitive decorrelation; for instance, \textit{apple} should be distinguished from specific attributes, no matter whether \textit{apple} is \textit{red} or \textit{green}. 
Thus, we propose a prototype-based decorrelation learning to enforce a distinct separation between attribute and object prototypes, ensuring promising disentanglement results.  
Based on constructed primitive prototypes, the conditional-independence relations can be captured by the following properties: \textbf{i)} $\bm{f}^{a} \upmodels \bm{p}^{o}$ and \textbf{ii)} $\bm{f}^{o} \upmodels \bm{p}^{a}$, where $\upmodels$ denotes the independence between samples. 
Here, $\bm{f}^{a}$ and $\bm{f}^{o}$ are disentangled attribute and object features, respectively, with $\bm{p}^{o}$ and $\bm{p}^{a}$ representing the corresponding sub-primitive prototypes entangled with these features. 

Specifically, we minimize the correlation between attribute and object embedding spaces by using the Hilbert-Schmidt Independence Criterion (HSIC)~\cite{gretton2007kernel}.
$\text{HSIC}$ is a non-parametric, kernel-based statistical measure that evaluates the independence between two continuous random variables, yielding a value of zero if and only if the two variables are statistically independent in the infinite-sample limit.
Thus, our prototype-based decorrelation learning strategy can be achieved by minimizing:
\vspace{-3pt}
\begin{small}
\begin{equation}
\begin{aligned}\label{eq::PDL}
\!\!\!\mathcal{L}^{\text{PDL}}\!=\!\text{HSIC}(\bm{f}^{a},\bm{p}^{o})+\text{HSIC}(\bm{f}^{o},\bm{p}^{a}).
\end{aligned}
\vspace{-2pt}
\end{equation}
\end{small}
A similar approach is also adopted by~\cite{atzmon2020causal}; however, our decorrelation strategy benefits from the use of prototypes, which capture the intrinsic characteristics of primitives, thus more effectively disentangling attribute features and object features in the compositional space.

\noindent\textbf{Overall Training Objective.} 
The final learning target of \textsc{ClusPro} combines the three-path classification loss $\mathcal{L}^{\text{BAS}}$ (Eq.~\ref{eq::bas}) with prototype-based loss constraints $\mathcal{L}^{\text{PCL}}$ (Eq.~\ref{eq::ipc}) and $\mathcal{L}^{\text{PDL}}$ (Eq.~\ref{eq::PDL}): 
\vspace{-3pt}
\begin{small}
\begin{equation}
\begin{aligned}\label{eq::over}
\!\!\!\mathcal{L}\!=\!\mathcal{L}^{\text{BAS}}\!+\!\alpha\mathcal{L}^{\text{PCL}}\!+\!\beta\mathcal{L}^{\text{PDL}},
\end{aligned}
\vspace{-2pt}
\end{equation}
\end{small}
where the coefficients $\alpha$ and $\beta$ are empirically set: $\alpha\!=\!0.2$, $\beta\!=\!0.5$. 

\noindent\textbf{Inference for CSZL.} 
During testing, the test image $x$ is fed into \textsc{ClusPro} to obtain prediction scores for attribute $p(a_i|x)$, object $p(o_i|x)$, and composition prediction $p(c_{i,j}|x)$. Then the final composition class can be predicted by incorporating three branch prediction results:
\vspace{-3pt}
\begin{small}
\begin{equation}
\begin{aligned}
\hat{c} = \mathop{\arg \max}\limits_{c_{i,j} \in \mathcal{C}^{t}}~p(c_{i,j}|x) + p(a_i|x) \cdot p(o_j|x).
\end{aligned}
\vspace{-2pt}
\end{equation}
\end{small}

\begin{table*}[!t] %\small
  \centering  \makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\textbf{Quantitative results}(\S\ref{sec::sota}) on MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning} within \textbf{\textit{CW}} setting.}
  \vspace{-3pt}
   \resizebox{0.99\textwidth}{!}{
   \setlength\tabcolsep{3pt}
    \renewcommand\arraystretch{1.0}
    \begin{tabular}{rl|c||cccc|cccc|cccc}
    \hline
    \thickhline \rowcolor{mygray}
     \textit{Closed-World}\!\!  & & & \multicolumn{4}{c|}{MIT-States} & \multicolumn{4}{c|}{UT-Zappos} & \multicolumn{4}{c}{C-GQA} \\
   \rowcolor{mygray} Method\!\!  & &\multirow{-2}{*}{Backbone} & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$ \\
    \hline
   CLIP~\cite{radford2021learning}\!\!\!\! &\pub{ICML2021}& ViT-L& 30.2  & 46.0  & 26.1  & 11.0  & 15.8  & 49.1  & 15.6  & 5.0   & 7.5   & 25.0  & 8.6   & 1.4  \\
     CoOp~\cite{zhou2022learning}\!\!\!\! &\pub{IJCV2022}& ViT-L& 34.4  & 47.6  & 29.8  & 13.5  & 52.1  & 49.3  & 34.6  & 18.8  & 20.5  & 26.8  & 17.1  & 4.4  \\
     PCVL~\cite{xu2022prompting}\!\!\!\! &\pub{Arxiv2022}& ViT-L& 48.5  & 47.2  & 35.3  & 18.3  & 64.4  & 64.0  & 46.1  & 32.2  & -     &  -    &  -    &  - \\
     CSP~\cite{nayaklearning}\!\!\!\! &\pub{ICLR2023}& ViT-L& 46.6  & 49.9  & 36.3  & 19.4  & 64.2  & 66.2  & 46.6  & 33.0  & 28.8  & 26.8  & 20.5  & 6.2  \\
     DFSP(i2t)~\cite{lu2023decomposed}\!\!\!\! &\pub{CVPR2023}& ViT-L& 47.4  & 52.4  & 37.2  & 20.7  & 64.2  & 66.4  & 45.1  & 32.1  & 35.6  & 29.3  & 24.3  & 8.7  \\
     DFSP(BiF)~\cite{lu2023decomposed}\!\!\!\!&\pub{CVPR2023}& ViT-L& 47.1  & 52.8  & 37.7  & 20.8  & 63.3  & 69.2  & 47.1  & 33.5  & 36.5  & 32.0  & 26.2  & 9.9  \\
     DFSP(t2i)~\cite{lu2023decomposed}\!\!\!\! &\pub{CVPR2023}& ViT-L& 46.9  & 52.0  & 37.3  & 20.6  & 66.7  & 71.7  & 47.2  & 36.0  & 38.2  & 32.0  & 27.1  & 10.5  \\
     GIPCOL~\cite{xu2024gipcol}\!\!\!\! &\pub{WACV2024}& ViT-L& 48.5 & 49.6 & 36.6 & 19.9 & 65.0 & 68.5 & 48.8 & 36.2 & 31.9 & 28.4 & 22.5 & 7.1 \\
     CDS-CZSL~\cite{li2024context}\!\!\!\!&\pub{CVPR2024}& ViT-L & 50.3 & 52.9 & 39.2 & 22.4 & 63.9 & 74.8 & 52.7 & 39.5 & 38.3 & 34.2 & 28.1 & 11.1 \\
     Troika~\cite{huang2024troika}\!\!\!\! &\pub{CVPR2024}& ViT-L& 49.0 & 53.0 & 39.3 & 22.1 & 66.8 & 73.8 & 54.6 & 41.7 & 41.0 & 35.7 & 29.4 & 12.4 \\
     PLID~\cite{bao2023prompting}\!\!\!\! &\pub{ECCV2024}& ViT-L& 49.7 & 52.4 & 39.0 & 22.1 & 67.3 & 68.8 & 52.4 & 38.7 & 38.8 & 33.0 & 27.9 & 11.0 \\ \hline
     %CAILA~\cite{zheng2024caila} &51.0 &53.9 &39.9  & 23.4 & 67.8 & 74.0 & 57.0 &44.1 & 43.9 & 38.5 & 32.7 & 14.8 \\\hline
    \multicolumn{2}{c|}{\textbf{\textsc{ClusPro} (Ours)}} & ViT-L&\textbf{52.1}\tiny{$\pm$0.6} &\textbf{54.0}\tiny{$\pm$0.3}  &  \textbf{40.7}\tiny{$\pm$0.2} & \textbf{23.8}\tiny{$\pm$0.2} &  \textbf{70.7}\tiny{$\pm$1.0}  & \textbf{76.0}\tiny{$\pm$1.2} & \textbf{58.5}\tiny{$\pm$0.6}  & \textbf{46.6}\tiny{$\pm$0.5}   & \textbf{44.3}\tiny{$\pm$0.2}  &\textbf{37.8}\tiny{$\pm$0.2}  & \textbf{32.8}\tiny{$\pm$0.2} & \textbf{14.9}\tiny{$\pm$0.1}\\
    \hline
    \end{tabular}}
     \vspace{-8pt}
  \label{tab:sota1}
\end{table*}%
\section{Experiment}
\label{sec::experi}
\subsection{Experimental Setup}
 \noindent\textbf{Datasets.}~We conduct experiments on three widely-used CZSL benchmarks: MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine}, and {C-GQA}~\cite{naeem2021learning}. MIT-States consists of $53,753$ natural images in total, with $115$ states and $245$ objects. UT-Zappos contains $50,025$ fine-grain shoe images with $16$ states, $12$ objects and $116$ state-object compositions. C-GQA is the most extensive CZSL dataset, containing $453$ states and $870$ objects for $39,298$ images in total and over $9,500$ state-object compositions. More details are provided in Table~\ref{tab:sup_dataset} (\cf~\S\ref{sec_app:A1} in Appendix).

\noindent\textbf{Evaluation Metric.} Following the official evaluation protocol~\cite{naeem2021learning,purushwalkam2019task,li2024context,huang2024troika},  four metrics are adopted for evaluation, \ie, best-Seen accuracy (Seen), best-Unseen accuracy (Unseen), best Harmonic Mean (HM), and Area Under the Curve (AUC). Among them, AUC is the priority as it evaluates the model comprehensively. Please see~\cite{chao2016empirical,purushwalkam2019task} for full details about metrics.

\subsection{Implementation Details}
\noindent\textbf{Network Architecture.} \textsc{ClusPro} adopts pre-trained CLIP ViT-L/14 model~\cite{radford2021learning}, serving as the image and text encoder. The adapters of attribute  $h^a$ and object $h^o$ are implemented by two individual MLPs.  We group the features of each primitive into $K$ prototypes to describe intra-primitive diversity. The number of prototypes $K$ and the momentum coefficient $\mu$ in Eq.~\ref{eq:update} are empirically set to $5$ and $0.99$, respectively (ablation study in Table~\ref{tab::pro_num} and \ref{tab::coeff}). We follow~\cite{chang2023csot} to set $\kappa\!=\!1$ in Eq.~\ref{eq:nc1}. 

\begin{table*}[!t] %\small
  \centering \makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\textbf{Quantitative results}(\S\ref{sec::sota}) on MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning} within \textbf{\textit{OW}} setting .}
  \vspace{-3pt}
   \resizebox{0.98\textwidth}{!}{
   \setlength\tabcolsep{3pt}
    \renewcommand\arraystretch{1.0}
    \begin{tabular}{rl|c||cccc|cccc|cccc}
    %\specialrule{0.05em}{0pt}{0pt}
    \hline
    \thickhline \rowcolor{mygray}
     \textit{Open-World}\!\! && & \multicolumn{4}{c|}{MIT-States} & \multicolumn{4}{c|}{UT-Zappos} & \multicolumn{4}{c}{C-GQA} \\
   \rowcolor{mygray} Method\!\! & &\multirow{-2}{*}{Backbone} & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$\\
    \hline
   CLIP~\cite{radford2021learning}\!\!\!\!&\pub{ICML2021}& ViT-L &30.1 &14.3 &12.8 &3.0 &15.7 &20.6 &11.2 &2.2& 7.5& 4.6& 4.0& 0.3  \\
     CoOp~\cite{zhou2022learning}\!\!\!\!&\pub{IJCV2022}& ViT-L &34.6& 9.3 &12.3& 2.8 &52.1 &31.5 &28.9 &13.2 &21.0& 4.6& 5.5& 0.7 \\
     PCVL~\cite{xu2022prompting}\!\!\!\!&\pub{Arxiv2021}& ViT-L & 48.5& 16.0 &17.7& 6.1 &64.6& 44.0 &37.1& 21.6  & -     &  -    &  -    &  - \\
     CSP~\cite{nayaklearning}\!\!\!\!&\pub{ICLR2023}& ViT-L & 46.3 &15.7 &17.4 &5.7& 64.1 &44.1& 38.9 &22.7& 28.7& 5.2& 6.9& 1.2  \\
     DFSP(i2t)~\cite{lu2023decomposed}\!\!\!\!&\pub{CVPR2023}& ViT-L & 47.2 &18.2& 19.1& 6.7& 64.3& 53.8& 41.2& 26.4& 35.6& 6.5& 9.0& 2.0 \\
     DFSP(BiF)~\cite{lu2023decomposed}\!\!\!\!&\pub{CVPR2023}& ViT-L& 47.1& 18.1& 19.2& 6.7& 63.5& 57.2& 42.7 &27.6 &36.4 &7.6& 10.6 &2.4  \\
     DFSP(t2i)~\cite{lu2023decomposed}\!\!\!\!&\pub{CVPR2023}& ViT-L & 47.5 &18.5 &19.3 &6.8& 66.8& 60.0 &44.0& 30.3 &38.3 &7.2& 10.4& 2.4  \\
     GIPCOL~\cite{xu2024gipcol}\!\!\!\!&\pub{WACV2024}& ViT-L & 48.5 &16.0 &17.9& 6.3& 65.0& 45.0& 40.1& 23.5& 31.6& 5.5 &7.3 &1.3 \\
     CDS-CZSL~\cite{li2024context}\!\!\!\!&\pub{CVPR2024}& ViT-L & 49.4 &21.8& 22.1 &8.5& 64.7& 61.3 &48.2& 32.3& 37.6 &8.2& 11.6 &2.7 \\
     Troika~\cite{huang2024troika}\!\!\!\!&\pub{CVPR2024}& ViT-L & 48.8 &18.7& 20.1& 7.2 &66.4& 61.2 &47.8& 33.0 &40.8 &7.9& 10.9 &2.7\\
     PLID~\cite{bao2023prompting}\!\!\!\!&\pub{ECCV2024}& ViT-L & 49.1 &18.7& 20.0& 7.3& 67.6& 55.5 &46.6& 30.8& 39.1& 7.5& 10.6 &2.5\\ \hline
    \multicolumn{2}{c|}{\textbf{\textsc{ClusPro} (Ours)} } & ViT-L & \textbf{51.2}\tiny{$\pm$0.4} & \textbf{22.1}\tiny{$\pm$0.2}  &\textbf{23.0}\tiny{$\pm$0.1}  & \textbf{9.3}\tiny{$\pm$0.2}& \textbf{71.0}\tiny{$\pm$1.1}  & \textbf{66.2}\tiny{$\pm$1.0} & \textbf{54.1}\tiny{$\pm$0.7}  & \textbf{39.5}\tiny{$\pm$0.8}  &  \textbf{41.6}\tiny{$\pm$0.3} & \textbf{8.3}\tiny{$\pm$0.2} & \textbf{11.6}\tiny{$\pm$0.3}& \textbf{3.0}\tiny{$\pm$0.1}  \\
    \hline
    \end{tabular}%
    }
     \vspace{-12pt}
  \label{tab:sota2}%
\end{table*}%
\noindent\textbf{Training.} \textsc{ClusPro} is trained  end-to-end for 15 epochs with Adam optimizer~\cite{kingma2014adam}. To manage the learning rate, we initialize it at $1e\!-\!4$ for all datasets and set weight decay to $5e\!-\!5$. The coefficients $\alpha$ and $\beta$ in overall training objective (Eq.~\ref{eq::over}) are empirically set to $0.2$ and $0.5$, respectively (related experiments in \S\ref{sec_app:A3} of Appendix). In Eq.~\ref{eq::ipc}, the temperature parameter $\tau$ is maintained at $0.1$. \textsc{ClusPro} is implemented in PyTorch and trained on one NVIDIA RTX 4090 GPU. 

\noindent\textbf{Testing.} Following previous works~\cite{nayaklearning,xu2024gipcol}, we apply the post-training calibration to filter out infeasible compositions in the open-world setting during testing. Note that, during
model deployment, there is no any network architectural modification or extra inference cost introduced to the base model. The primitive prototypes, $\bm{P}^{a}$ and $\bm{P}^{o}$, are directly discarded after network training.

\subsection{Comparison to State-of-the-Arts}
\label{sec::sota}
In this section, we compare our method \textsc{ClusPro} with top-leading CZSL solutions on three dataset (\ie, MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine}, and C-GQA~\cite{naeem2021learning}) in \textit{CW} and \textit{OW} settings. 

\noindent\textbf{Performance on \textit{CW} Setting.} As summarized in Table~\ref{tab:sota1}, our approach \textsc{ClusPro} outperforms recent  state-of-the-art (SOTA) CZSL algorithms across all datasets~\cite{isola2015discovering,yu2014fine,naeem2021learning} on \textit{CW} setting. Concretely, in terms of AUC which is the priority metric for evaluating the model comprehensively, \textsc{ClusPro} yields +\textbf{1.4}, +\textbf{4.9}, and +\textbf{2.5} AUC score gains compared with SOTA methods on MIT-States, UT-Zappos, and C-GQA, respectively. Besides, \textsc{ClusPro} boosts HM to \textbf{40.7} (+\textbf{3.6}\%) on MIT-States, \textbf{58.5} (+\textbf{7.1}\%) on UT-Zappos, and \textbf{32.8} (+\textbf{11.6}\%) on C-GQA. Moreover, \textsc{ClusPro} earns consistent best Seen Accuracy (Seen) and Unseen Accuracy (Unseen) improvement. These consistency improvements are attributed to the fact that our algorithm captures diverse sub-primitive patterns, \ie, intra-primitive variations, which improves generalization on unseen compositions.

\noindent\textbf{Performance on \textit{OW} Setting.} Table~\ref{tab:sota2} reports comparison results on \textit{OW} setting. As seen, most CZSL methods suffer a substantial performance drop due to vast search space in \textit{OW} setting. In contrast, our method \textsc{ClusPro} still surpasses all published competitors across three datasets~\cite{isola2015discovering,yu2014fine,naeem2021learning}. In particular, \textsc{ClusPro} attains the highest AUC scores: \textbf{9.3} (+\textbf{9.4}\%) on MIT-States, \textbf{39.5} (+\textbf{19.7}\%) on UT-Zappos, and \textbf{3.0} (+\textbf{11.1}\%) on C-GQA. % Furthermore, \textsc{ClusPro} brings considerable HM gains, with the increase of +\textbf{0.9}, +\textbf{5.9} on MIT-States and UT-Zappos, respectively. 
In terms of HM, best Seen Accuracy (Seen) and Unseen Accuracy (Unseen), \textsc{ClusPro} still achieves the best results. This reinforces our belief that learning a group of discriminative prototypes for each primitive helps our model to recognize unseen compositions, even within challenging \textit{OW} setting. 

\subsection{Diagnostic Experiment}
\label{sec::abtion}
To evaluate our algorithm designs and gain further insights, we conduct ablation studies on UT-Zappos~\cite{isola2015discovering} and C-GQA~\cite{naeem2021learning} in \textit{CW}  settings. %\textbf{More experimental results are given in the supplementary.} 

\noindent\textbf{Key Component Analysis.} We first investigate the effectiveness of our core idea, \ie, clustering-based prototype learning. To make use of discovered rich sub-primitive prototypes to shape attribute and object embedding spaces, two key training objectives are proposed, \ie, Prototype-based Contrast $\mathcal{L}^{\text{PCL}}$ \!(Eq.\!~\ref{eq::ipc}) and  Decorrelation $\mathcal{L}^{\text{PDL}}$ \!(Eq.\!~\ref{eq::PDL}). As shown in Table~\ref{tab::core}, we build \textsc{Baseline} that trains in the three-branch paradigm, without within-primitive prototype clustering (\ie, prototype assignment and updating). We can find that, adding  $\mathcal{L}^{\text{PCL}}$ or $\mathcal{L}^{\text{PDL}}$ individually  leads to a substantial performance gain, \eg, +\textbf{4.9}/+\textbf{2.8} AUC on UT-Zappos~\cite{isola2015discovering}, and +\textbf{2.4}/+\textbf{1.9} AUC on C-GQA~\cite{naeem2021learning}. This verifies the efficacy of explicitly promoting inter-primitive prototype separation and attribute-object independence. Last, by combining $\mathcal{L}^{\text{PCL}}$ and $\mathcal{L}^{\text{PDL}}$, our full model yields the best results. 

\begin{table*}[!t]
\centering 
\vspace{-2pt}
    \captionsetup{width=.98\textwidth}
\captionsetup{font=small}\caption{\textbf{Analysis of core components}~(\S\ref{sec::abtion}) on UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning} within \textbf{\textit{CW}} setting.}
\vspace{-2pt}
\label{tab::core}
\resizebox{0.95\linewidth}{!}{
\setlength\tabcolsep{4.5pt}
\renewcommand\arraystretch{1.0}
\begin{tabular}{r||cc|cccc|cccc}
\thickhline
\rowcolor{mygray} & $\mathcal{L}_{\text{PCL}}$&$\mathcal{L}_{\text{PDL}}$ & \multicolumn{4}{c|}{UT-Zappos} & \multicolumn{4}{c}{C-GQA}\\
\rowcolor{mygray} &  (Eq.\!~\ref{eq::ipc})& (Eq.\!~\ref{eq::PDL})& Seen$\uparrow$      & Unseen$\uparrow$      & HM$\uparrow$     & AUC$\uparrow$    & Seen$\uparrow$      & Unseen$\uparrow$      & HM$\uparrow$     & AUC$\uparrow$ \\
\hline
\hline
\textsc{Baseline} (\textit{w/o} Clustering) && &66.2 &74.6 &54.1&41.0 & 40.5 &33.4&29.7&11.8\\
\hline
Prototype-bsaed Contrast  &$\checkmark$&  &69.9 & 74.7 &57.1&45.9& 43.6& 36.7& 32.1 &14.2\\
Prototype-bsaed Decorrelation &&$\checkmark$ &67.6 &75.2 &56.5&43.8&43.1& 36.1& 31.4 &13.7\\
 Contrast + Decorrelation &$\checkmark$ &$\checkmark$ &\textbf{70.7} &\textbf{76.0} &\textbf{58.5}&\textbf{46.6} &\textbf{44.3}&\textbf{37.8} &\textbf{32.8}&\textbf{14.9} \\
\hline
\end{tabular}
 }
\vspace{-9pt}
\end{table*}
\begin{table}[t!]
\makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\small\textbf{$_{\!}$A$_{\!}$ set$_{\!}$ of$_{\!}$ ablation$_{\!}$ studies$_{\!}$} on$_{\!}$ UT-Zappos~\cite{yu2014fine} (\S\ref{sec::abtion}).}
        \centering
        \small
	%\hspace{-0.7em}
	\begin{subtable}
  {0.5\linewidth}
   \centering
 \vspace{-0.2cm}		\resizebox{0.95\textwidth}{!}{
			\setlength\tabcolsep{6pt}
    \renewcommand\arraystretch{0.9}
\begin{tabular}{c||cccc}
\thickhline
\rowcolor{mygray} & 
\multicolumn{4}{c}{{UT-Zappos}}\\
\rowcolor{mygray}    \multirow{-2}{*}{Prototype $K$} & Seen$\uparrow$  & Unseen$\uparrow$  & HM$\uparrow$  & AUC$\uparrow$  \\ \hline  \hline
  $K=1$        & $68.9$ & $73.6$  & $55.2$ & $42.8$  \\  
  $K=3$        & $70.6$ & $74.6$  & $56.9$ & $45.1$  \\
       $K=5$        & $\mathbf{70.7}$ & $\mathbf{76.0}$ & $\mathbf{58.5}$ & $\mathbf{46.6}$   \\ 
   $K=10$      & $69.9$   & $75.4$ & $58.3$  & $46.0$   \\ 
 $K=20$         & $70.1$ & $75.2$   & $58.0$  & $45.9$  \\ \hline
\end{tabular}
	}
		\vspace{2px}
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{-0.0cm}
		\caption{\small{Per-primitive Prototype Number}}
		\vspace{-3px}
        \label{tab::pro_num}
	\end{subtable}
        \hspace{-0.7em}     
        \begin{subtable}{0.5\linewidth}
         \centering
        \small
           \vspace{-0.2cm}
		\resizebox{0.95\textwidth}{!}{
			\setlength\tabcolsep{6pt}
    \renewcommand\arraystretch{0.9}
\begin{tabular}{c||cccc}
\thickhline
\rowcolor{mygray} & 
\multicolumn{4}{c}{{UT-Zappos}}\\
\rowcolor{mygray}    \multirow{-2}{*}{Coefficient $\mu$} & Seen$\uparrow$  & Unseen$\uparrow$  & HM$\uparrow$  & AUC$\uparrow$  \\ \hline  \hline
  $\mu=0$        & $67.2$ & $74.1$  &$54.9$& $42.6$  \\  
       $\mu=0.5$        & $69.8$  & $75.1$  & $56.0$  & $43.3$\\ 
   $\mu=0.9$      & $70.5$   & $74.9$ & $58.1$ &  $46.0$  \\ 
 $\mu=0.99$       & $\mathbf{70.7}$ & $\mathbf{76.0}$  & $\mathbf{58.5}$   & $\mathbf{46.6}$ \\
 $\mu=0.999$       & $70.5$   & $74.7$ & $57.0$ &  $45.1$\\ \hline
\end{tabular}
}
		\vspace{2px}
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{-0.0cm}
		\caption{\small{Prototype Updating Coefficient $\mu$}}
		\vspace{-3px}
		\label{tab::coeff}
	\end{subtable} \\
 \vspace{-3px}
	\begin{subtable}{0.5\linewidth}
  \centering
        \small
		\resizebox{0.95\textwidth}{!}{
			\setlength\tabcolsep{6pt}
    \renewcommand\arraystretch{1.0}
\begin{tabular}{cc||cccc}
\thickhline
\rowcolor{mygray}\multicolumn{2}{c||}{{Clustering Branch}} & 
\multicolumn{4}{c}{{UT-Zappos}}\\
\rowcolor{mygray}    Attribute & Object & Seen$\uparrow$  & Unseen$\uparrow$  & HM$\uparrow$  & AUC$\uparrow$  \\ \hline  \hline
  &         &$66.2$ &$74.6$ &$54.1$&$41.0$   \\  
  \cmark &       & $69.7$ & $74.3$  & $56.5$ & $44.3$  \\
         &   \cmark   & $69.5$ & $73.6$ & $56.4$ & $44.1$   \\ 
   \cmark  & \cmark  & $\mathbf{70.7}$   & $\mathbf{76.0}$ & $\mathbf{58.5}$  & $\mathbf{46.6}$   \\  \hline
\end{tabular}	}
		\vspace{-0px}
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{-0.0cm}
		\caption{\small{Clustering Branch}}
		\vspace{-0.6cm}
		\label{tab::branch}
	\end{subtable}
        \hspace{-0.7em}
        \vspace{-3px}
	\begin{subtable}{0.5\linewidth}
  \centering
        \small
		\resizebox{0.95\textwidth}{!}{
			\setlength\tabcolsep{6pt}
    \renewcommand\arraystretch{1.0}
\begin{tabular}{c||cccc}
\thickhline
\rowcolor{mygray} & 
\multicolumn{4}{c}{{UT-Zappos}}\\
\rowcolor{mygray}    \multirow{-2}{*}{Clustering Strategy} & Seen$\uparrow$  & Unseen$\uparrow$  & HM$\uparrow$  & AUC$\uparrow$  \\ \hline  \hline
 None      &$66.2$ &$74.6$ &$54.1$&$41.0$   \\
  Cosine Similarity        & $68.9$ & $74.7$  & $57.7$ & $45.0$  \\  
  Classical OT        & $70.1$ & $75.2$  & $58.2$ & $45.8$  \\
       Ours        & $\mathbf{70.7}$ & $\mathbf{76.0}$ & $\mathbf{58.5}$ & $\mathbf{46.6}$   \\  \hline
\end{tabular}
		}
		\vspace{-0px}
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{-0.0cm}
		\caption{\small{Clustering Strategy}}
		\vspace{-0.6cm}
		\label{tab::clu_str}
	\end{subtable}
\vspace{-3pt}
\end{table}
\noindent\textbf{Prototype Number Per Primitive $K$.} We next investigate the impact of the prototype number per primitive. The results are reported in Table ~\ref{tab::pro_num}. Note that for $K\!=\!1$, each primitive is directly represented by the mean embedding of primitive features in the current batch without prototype clustering. As shown in Table~\ref{tab::pro_num}, this baseline yields the HM score of 55.2 and AUC score of 42.8 on UT-Zappos~\cite{isola2015discovering}, respectively. When representing one primitive concept with a group of prototypes, we observe that our model \textsc{ClusPro} gains stable improvements (\ie, HM: 55.2$\rightarrow$\textbf{58.5}, AUC: 42.8$\rightarrow$\textbf{46.6}) as the number of prototypes grows (\ie, $K=5$). This supports our hypothesis that leveraging a set of diversified prototypes to describe a primitive concept can capture diverse intra-primitive patterns.  
However, too many prototypes above $K=5$ results in negative gains. This may be because \textsc{ClusPro} suffers from insignificant sub-primitive patterns produced by over-clustering.

\noindent\textbf{Momentum Coefficient $\mu$.} Table~\ref{tab::coeff} probes the impact of momentum coefficient $\mu$ (Eq.\ref{eq:update}), which controls the speed of primitive prototype updating. We can clearly observe that, our algorithm performs better with a relatively large coefficient (\ie, $\mu=0.99$), verifying that slow updating is beneficial, but not too slow (\ie, $\mu=0.999$). When $\mu$ is too small, the performance decreases. In particular, our algorithm encounters a large decrease at the extreme of no momentum (\ie, $\mu=0$).

\noindent\textbf{Multi-branch Clustering.} In Table~\ref{tab::branch}, we study the impact of attribute and object clustering branches by removing one or more specific branches. Removing one branch clustering means that, we discard primitive-wise clustering to mine sub-primitive patterns in the corresponding branch, and remove the corresponding training objectives from Eq.~\ref{eq::ipc} and \ref{eq::PDL}.  As shown in Table~\ref{tab::branch}, using attribute or object clustering branches individually only yields limited performance gains, \eg, +\textbf{3.3}/+\textbf{3.1} AUC score. By unifying the two clustering branches together, our full model achieves the best performance across four metrics, confirming their complementarity.    

\noindent\textbf{Clustering Strategy.} We examine the impact of our proposed local-aware clustering strategy (\cf~Eq.\ref{eq:nc1}) by contrasting it with the model without primitive-wise clustering, the cosine similarity updating~\cite{chanscribble}, and classical Optional Transport (OT)~\cite{kantorovich2006translocation,cuturi2013sinkhorn}. As shown in Table~\ref{tab::clu_str}, our local-aware clustering strategy proves to be more effective: it outperforms the model without clustering, the cosine similarity, and classical OT across all metrics, \eg, +\textbf{4.7}, +\textbf{1.6}, and +\textbf{0.8} AUC scores on UT-Zappos, respectively. This study confirms that considering the intrinsic coherence structure of attribute/object feature distribution is beneficial for superior prototype assignment.   

\begin{figure}[t]
    \centering
\includegraphics[width=0.99\textwidth]{case_study.pdf}
   \vspace{-8pt}
    \captionsetup{font=small}\caption{Case study on UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning}. $\!$We compare \textsc{ClusPro} with baseline without primitive-wise prototype clustering. $\!$Correct and incorrect predictions are marked in \textbf{\textcolor{ggreen}{green}} and \textbf{\textcolor{red}{red}}, respectively.}
    \vspace{-8pt}
    \label{fig::case_study}
\end{figure}
\begin{figure}[t]
    \centering
\includegraphics[width=0.99\textwidth]{tSNE.pdf}
   \vspace{-5pt}
    \captionsetup{font=small}\caption{Visualization of attribute and object features learned by baseline  and \textsc{ClusPro} on UT-Zappos~\cite{yu2014fine}. }
    \vspace{-12pt}
    \label{fig::tSNE}
\end{figure}
\subsection{Quality Analysis}
\label{sec45}
\noindent\textbf{Success Cases.} The first four columns of Fig.~\ref{fig::case_study}  present success cases of our method  \textsc{ClusPro} for both seen and unseen compositions on UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning}. As seen, compared with the base model without primitive-wise prototype clustering, \textsc{ClusPro} works much better. Even for the complex C-GQA dataset, \textsc{ClusPro} still correctly predicts labels. For example, \textsc{ClusPro} can calibrate \textit{Suede} to \textit{Leather} (materials) and \textit{Yellow} to \textit{Orange} (colors). This demonstrates \textsc{ClusPro} can capture fine-grained primitive patterns (\eg, various materials and colors) by representing each primitive as a set of prototypes. Moreover, benefiting from prototype clustering across the whole dataset, \textsc{ClusPro} automatically mines the global data distribution of each primitive, leading to generalizing well to unseen compositions. More success cases are provided in \S\ref{sec_app:A4} of Appendix.

\noindent\textbf{Failure Cases and Limitations.} The last two columns of Fig.~\ref{fig::case_study} show failure cases, where the attribute and object of images are highly entangled and visually confusing. However, \textsc{ClusPro} still identifies the part of attribute-object compositions. In addition, though making mistakes on attribute predictions (\eg, \textit{Green Vegetable} in column 6, row 2), such wrong predictions interpret another attribute (\ie, the color) of \textit{Miniature  Vegetable}. Thus we will make use of large language models to generate informative descriptions for each composition in the future, so as to emphasize primary primitives. More failure cases are provided in \S\ref{sec_app:A4} of Appendix.

\noindent\textbf{Feature Distributions of Attribute and Object.} We visualize learned features of attribute and object by $\mathcal{L}_{\text{BAS}}$ (Eq.~\ref{eq::bas}) and $\mathcal{L}$ (Eq.~\ref{eq::over}) in Fig.~\ref{fig::tSNE}. We observe that, after considering clustering-based prototype mining, learned attribute and object features become more compact and better separated. This demonstrates that  \textsc{ClusPro} can shape well-structured attribute/object embedding spaces by clustering-based analysis across the whole dataset, hence ensuring better visual disentanglement. 

\section{Conclusion}
In this work, we present \textsc{ClusPro}, a clustering-based prototype mining framework for Compositional Zero-Shot Learning. 
This framework aims to learn a well-structured and independent embedding space with multiple discriminative prototypes for each primitive, which alternates between two steps: 
1) within-primitive online clustering for automatically discovering and dynamically updating prototypes;
2) prototype-based primitive representation learning for promoting intra-primitive separation and inter-primitive decorrelation. 
Experimental results on three gold-standard datasets demonstrate the superiority of our clustering-based scheme against existing methods.

\section{Acknowledgement}
This work was supported by the National Natural Science Foundation of China (No. 62222207, 62332010, 62427808, and 62372405), the Fundamental Research Funds for the Central Universities 226-2024-00058, the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University (No. HMHAI-202403), Bytedance Doubao Fund, and Earth System Big Data Platform of the School of Earth Sciences, Zhejiang University.

\bibliography{iclr2025_conference}
%\bibliographystyle{iclr2025_conference}
\bibliographystyle{unsrt}
\clearpage
\appendix
This appendix provides additional details for the ICLR 2025 submission, titled \textit{``Learning Clustering-based Prototypes for Compositional Zero-shot Learning"}. The appendix is organized as follows:
\begin{itemize}
  \item \S\ref{sec_app:A1} Detailed data split statistics.
  \item \S\ref{sec::algor_over} Algorithm overview.
  \item \S\ref{sec_app:baseline} More details about baseline model.
  \item \S\ref{sec_app:A3} More quantitative results.
  \item \S\ref{sec_app:A4} More qualitative visualization.
  \item \S\ref{sec_app:A2} The pseudo-code of prototype assignment and updating. 
  \item \S\ref{sec_app:A5} More discussions.
\end{itemize}

\section{Detailed Data Split Statistics}
\label{sec_app:A1}
We conduct experiments on three widely-used CZSL benchmarks: MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine}, and {C-GQA}~\cite{naeem2021learning}. MIT-States consists of $53,753$ natural images in total, with $115$ states and $245$ objects. Following conventional procedures, $1,962$ available compositions in the dataset are split into $1,262$ seen and $300$/$400$ unseen compositions for \texttt{train}/\texttt{validation}/\texttt{test}, respectively. UT-Zappos contains $50,025$ fine-grain shoe images with $16$ states, $12$ objects and $116$ state-object compositions. Following standard practices, the compositions are split into $83$ seen compositions, $15$ seen and $15$ unseen compositions, $18$ seen and $18$ unseen compositions for \texttt{train}/\texttt{validation}/\texttt{test} splits. C-GQA is the most extensive CZSL dataset, containing $453$ states and $870$ objects for $39,298$ images in total and over $9,500$ state-object compositions. The dataset is divided into $5,592$ seen compositions for \texttt{train}, $1,252$ seen and $1,040$ unseen compositions for \texttt{validation}, and $888$ and $923$ unseen compositions for \texttt{test}. The detailed data split statistics is provided in  Table~\ref{tab:sup_dataset}. Here $|\mathcal{C}^{s}|$ and $|\mathcal{C}^{u}|$ indicate the number of seen and unseen compositions, respectively. $|\mathcal{X}|$ represents the number of images.
\begin{table}[H] %\small
  \centering  \makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{The detailed data split statistics (\S\ref{sec_app:A1}) on MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning}.}
   \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{l||cc|cc|ccc|ccc}
         \thickhline
    \thickhline \rowcolor{mygray} &       &       & \multicolumn{2}{c|}{\texttt{train}} & \multicolumn{3}{c|}{\texttt{validation}} & \multicolumn{3}{c}{\texttt{test}} \\
    \rowcolor{mygray} Dataset & $|\mathcal{A}|$ & $|\mathcal{O}|$ & $|\mathcal{C}^{s}|$ & $|\mathcal{X}|$ & $|\mathcal{C}^{s}|$ & $|\mathcal{C}^{u}|$ & $|\mathcal{X}|$ & $|\mathcal{C}^{s}|$ & $|\mathcal{C}^{u}|$ & $|\mathcal{X}|$ \\
    \hline
    MIT-States~\cite{isola2015discovering} & 115   & 245   & 1262  & 30k   & 300   & 300   & 10k   & 400   & 400   & 13k \\
    UT-Zappos~\cite{yu2014fine} & 16    & 12    & 83    & 23k   & 15    & 15    & 3k    & 18    & 18    & 3k \\
    C-GQA~\cite{naeem2021learning} & 413     & 674     & 5592    & 27k    & 1252    & 1040    & 7k    & 888     & 923     & 5k \\ \hline
    \end{tabular} }
     \vspace{-0.8em}
  \label{tab:sup_dataset}
\end{table}

\section{Algorithm Overview}
\label{sec::algor_over}
Fig.~\ref{fig:overview} presents the architecture of our \textsc{ClusPro}. It takes a batch of images and all the semantic labels (\ie, attributes, objects, and compositions) as input. \textsc{ClusPro} first utilizes the visual encoder of CLIP~\cite{radford2021learning} along with attribute and object adapters to obtain attribute features $F^a$, object features $F^o$, and composition features $F$ (Eq.~\ref{visual}). Besides, \textsc{ClusPro} constructs attribute, object, and composition prompt representations (Eq.~\ref{text}) via a soft learnable prompt strategy~\cite{huang2024troika} based on pre-given semantic labels. Based on these visual and prompt representations from three branches, the three-path classification loss (\ie, $\mathcal{L}^{\text{BAS}}$) in Eq.~\ref{eq::bas} are employed to recognize primitive concepts and their compositions. Meanwhile, based on the obtained attribute and object visual feature representation (\ie, $F^a$ and $F^o$), \textsc{ClusPro} learn prototypes by within-primitive clustering (\S\ref{sec::pro}) and then propose two complementary metric learning mechanisms (\ie, $\mathcal{L}^{\text{PCL}}$ and $\mathcal{L}^{\text{PDL}}$) based on these prototypes, so as to explicitly shape well-structured and independent primitive embedding space (\S\ref{sec::rep}). Finally, we assemble the three-path classification loss $\mathcal{L}^{\text{BAS}}$ and our proposed prototype-based loss constraints (\ie, $\mathcal{L}^{\text{PCL}}$ and $\mathcal{L}^{\text{PDL}}$)  as our final learning objective. Our algorithm not only learns primitive recognition with pre-given semantic labels, but also automatically discovers diverse and fine-grained intra-primitive patterns via a set of prototypes across the entire dataset.

\section{More Details about Baseline Model}
\label{sec_app:baseline}
\noindent\textbf{Visual Feature Extraction.} Following~\cite{huang2024troika,li2024context,jing2024retrieval}, we adopt the visual encoder ${\phi}^{\text{vis}}$ of CLIP~\cite{radford2021learning} to splits the input image $X\!\in \!\mathbb{R}^{H \times W \times 3}$ into $N_{p}=HW/P$ patches, where $P$ is the resolution of each patch. Note that we following~\cite{huang2024troika,jing2024retrieval} to tune the image encoder of CLIP with LoRA~~\cite{houlsby2019parameter,hu2021lora}, a lightweight parameter efficient fine-tuning (PEFT) strategy. The encoder ${\phi}^{\text{vis}}$ projects these patches into patch tokens along with a [cls] token, and then updates these tokens via Transformer blocks. Finally, the [cls] token serves as the image representation $\bm{f}^{c}$. We adopt attribute adapter ${h}^{a}$ and object adapter ${h}^{o}$~\cite{houlsby2019parameter,hu2021lora}, each implemented as a separate MLP, to project $\bm{f}^{c}$ into the discriminative attribute feature $\bm{f}^{a}$ and object feature $\bm{f}^{o}$, respectively.

\noindent\textbf{Prompt Feature Extraction.} We follow existing CZSL~\cite{lu2023decomposed,huang2024troika} to employ an independent prompt prefix for each branch. Specifically, for each attribute-object composition $c_{i,j}=\langle a_i, o_j \rangle$, we create three prompts for each branch, \ie, attribute prompt $\bm{S}^a_i = [ \bm{s}^a_1, \dots, \bm{s}^a_l, \bm{v}^a_i]$, object prompt $\bm{S}^o_j = [ \bm{s}^o_1, \dots, \bm{s}^o_l, \bm{v}^o_j]$, and composition prompt $\bm{S}^c_{i,j} = [ \bm{s}^c_1, \dots, \bm{s}^c_l, \bm{v}^a_i, \bm{v}^o_j]$, where $\bm{s}^a_{1:l}$, $\bm{s}^o_{1:l}$, and $\bm{s}^c_{1:l}$ are learnable pretix contexts initialized by ``\textit{a photo of}". Then these prompts are then fed into the frozen text encoder of CLIP~\cite{radford2021learning} to obtain prompt features.

\noindent\textbf{Training Loss $\mathcal{L}^{\text{BAS}}$.} Following previous CZSL approaches~\cite{huang2024troika,li2024context,jing2024retrieval}, the parameters $\theta$ of the baseline model are learned by minimizing the three-path classification loss (Eq.~\ref{eq::bas})  on the training dataset. Note that we have omitted
the weight decay in Eq.~\ref{eq::bas} for simplicity. We just follow~\cite{huang2024troika} set the weight decay as  $5e\!-\!5$ for all our experiments.

\noindent\textbf{Feasibility Calibration for Open-World Setting.} Following~\cite{nayaklearning,huang2024troika,jing2024retrieval}, we adopt post-training feasibility calibration to filter out infeasible compositions that might be present in the open-world setting. The calibration relies on the assumption that similar objects tend to share similar attributes, while dissimilar objects are unlikely to exhibit shared attributes. Therefore, given a candidate pair $c=\langle a, o \rangle$, We calculate the feasibility compositions by computing the relationships between the objects and the attributes. First, we compute the similarities between the objects:
\vspace{-3pt}
\begin{small}
\begin{equation}
\setlength\belowdisplayskip{2pt}
{\rho}_{o}(a, o) = \mathop{\max}\limits_{\hat{o} \in \mathcal{O}^{se}} \frac{\phi(o) \cdot \phi(\hat{o})}{\| \phi(o) \| \| \phi(\hat{o}) \|},
\end{equation}
\end{small}
where $\hat{o}$ is the other objects paired with the attribute $a$ in seen compositions, and  $\phi(\cdot)$ is an embedding function that maps the primitive to a pre-trained embedding. We calculate the similarities ${\rho}_{a}(a, o)$ between attributes as same. Next, we combine the two similarities (\ie, ${\rho}_{o}(a, o)$ and ${\rho}_{a}(a, o)$ ) with a pooling function to obtain ${\rho}(a, o)$.

Finally, we filter out infeasible compositions by only considering compositions above a threshold ${\rho}(a, o)>T$ on the validation set to make the prediction:
\vspace{-3pt}
\begin{small}
\begin{equation}
\setlength\belowdisplayskip{2pt}
\hat{c} = \mathop{\arg \max}\limits_{c_{i,j} \in \mathcal{C}^{tgt}, {\rho}(a_i, o_j) > T} ~p(c_{i,j}|x) + p(a_i|x) \cdot p(o_j|x).
\end{equation}
\end{small}

\section{More Quantitative Results}
\label{sec_app:A3}
\noindent\textbf{Loss Coefficients $\alpha$ and $\beta$.} We further study the effect of loss coefficients $\alpha$ and $\beta$ for loss functions $\mathcal{L}_{\text{PCL}}$ (\cf~Eq.\ref{eq::ipc}) and $\mathcal{L}_{\text{PDL}}$ (\cf~Eq.\ref{eq::PDL}) on UT-Zappos~\cite{yu2014fine}. In Table~\ref{tab::alpha}, after fixing the loss coefficient $\beta$, \textsc{ClusPro} achieves the best performance when $\alpha$ is set to $0.2$. Additionally, in Table~\ref{tab::beta}, we fix $\alpha$ and set $\beta$ with different values to test the impact of $\mathcal{L}_{\text{PDL}}$. We observe that setting $\beta$ as $0.5$ leads to the best results across all metrics. Accordingly, we set $\alpha\!=\!0.2$ and $\beta\!=\!0.5$  in the training stage.
\begin{table}[H]
\makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\small The impact of loss coefficients $\alpha$ and $\beta$ (\S\ref{sec_app:A3}).}
        \centering
        \small
	%\hspace{-0.7em}
	\begin{subtable}
  {0.5\linewidth}
   \centering
 \vspace{-0.2cm}		\resizebox{0.95\textwidth}{!}{
			\setlength\tabcolsep{3.9pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{c||cccc}
\thickhline
\rowcolor{mygray} & 
\multicolumn{4}{c}{{UT-Zappos}}\\
\rowcolor{mygray}    \multirow{-2}{*}{Coefficient $\alpha$} & Seen$\uparrow$ & Uneen$\uparrow$ & HM$\uparrow$ & AUC$\uparrow$ \\ \hline  \hline
  $\alpha=0.1$       & $70.5$ & $74.2$  & $57.0$ & $45.1$  \\  
  $\alpha=0.2$        & $\mathbf{70.7}$ & $\mathbf{76.0}$ & $\mathbf{58.5}$ & $\mathbf{46.6}$  \\
       $\alpha=0.3$        & $70.6$ & $75.3$ & $58.6$ & $46.3$   \\ 
   $\alpha=0.4$      & $70.4$   & $75.2$ & $58.2$  & $46.2$   \\ 
 $\alpha=0.5$         & $70.5$ & $74.7$   & $58.2$  & $45.9$  \\ \hline
\end{tabular}
	}
		\vspace{2px}
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{-0.0cm}
		\caption{Loss Coefficient $\alpha$}
		\vspace{-3px}
        \label{tab::alpha}
	\end{subtable}
        \hspace{-0.7em}     
        \begin{subtable}{0.5\linewidth}
         \centering
        \small
           \vspace{-0.2cm}
		\resizebox{0.95\textwidth}{!}{
			\setlength\tabcolsep{3.5pt}
\renewcommand\arraystretch{1.08}
\begin{tabular}{c||cccc}
\thickhline
\rowcolor{mygray} & 
\multicolumn{4}{c}{{UT-Zappos}}\\
\rowcolor{mygray}    \multirow{-2}{*}{Coefficient $\beta$} & Seen$\uparrow$ & Uneen$\uparrow$ & HM$\uparrow$ & AUC$\uparrow$ \\ \hline  \hline
  $\beta=0$        & $67.2$ & $74.1$  &$54.9$& $42.6$  \\  
       $\beta=0.5$       & $\mathbf{70.7}$ & $\mathbf{76.0}$ & $\mathbf{58.5}$ & $\mathbf{46.6}$ \\ 
   $\beta=1$      & $70.7$   & $75.0$ & $58.0$ &  $45.9$  \\ 
 $\beta=5$      & $70.3$ & $74.9$  & $56.7$   & $44.6$ \\
 $\beta=10$       & $67.6$   & $74.0$ & $54.8$ &  $41.7$\\ \hline
\end{tabular}
}
		\vspace{2px}
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{-0.0cm}
		\caption{Loss Coefficient $\beta$}
		\vspace{-3px}
		\label{tab::beta}
	\end{subtable}

\end{table}

\noindent\textbf{More Comparison Results with Existing CZSL Methods.} Apart from CLIP-based methods, we further compare our algorithm \textsc{ClusPro} with existing CZSL methods~\cite{nagarajan2018attributes,purushwalkam2019task,li2020symmetry,mancini2021open,mancini2022learning,naeem2021learning,li2022siamese,wang2023learning,anwaar2022leveraging,misra2017red} with a pre-trained ResNet18~\cite{he2016deep} backbone across three datasets~\cite{isola2015discovering,yu2014fine,naeem2021learning}. Table~\ref{tab:sup_sota1} and Table~\ref{tab:sup_sota2} report additional comparison results within \textit{CW} and \textit{OW} settings, respectively. As can be seen, CLIP-based methods significantly outperform traditional vision-based methods. This evidences that CLIP-based CZSL methods have stronger compositionality for zero-shot generalization. Notably, \textsc{ClusPro} surpasses all other methods and achieves state-of-the-art performance.
\begin{table}[H] %\small
  \centering  \makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\textbf{More comparison results}(\S\ref{sec_app:A3}) on MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning} within \textbf{\textit{CW}} setting.}
   \resizebox{0.99\textwidth}{!}{
    \setlength\tabcolsep{3.5pt}
    \renewcommand\arraystretch{1.0}
    \begin{tabular}{l|cccc|cccc|cccc}
    %\specialrule{0.05em}{0pt}{0pt}
    \hline
    \thickhline \rowcolor{mygray}
     \textit{Closed-World}  & \multicolumn{4}{c|}{MIT-States} & \multicolumn{4}{c|}{UT-Zappos} & \multicolumn{4}{c}{C-GQA} \\
   \rowcolor{mygray} Method  & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$ \\
    \hline
    \rowcolor{mygray}
 \multicolumn{13}{l}{\textit{Traditional vision-based methods }} \\
  AoP~\cite{nagarajan2018attributes}   & 14.3  & 17.4  & 9.9   & 1.6   & 59.8  & 54.2  & 40.8  & 25.9  & 17.0  & 5.6   & 5.9   & 0.7 \\
    LE+~\cite{misra2017red}   & 15.0  & 20.1  & 10.7  & 2.0   & 53.0  & 61.9  & 41.0  & 25.7  & 18.1  & 5.6   & 6.1   & 0.8 \\
    TMN~\cite{purushwalkam2019task}   & 20.2  & 20.1  & 13.0  & 2.9   & 58.7  & 60.0  & 45.0  & 29.3  & 23.1  & 6.5   & 7.5   & 1.1 \\
    SymNet~\cite{li2020symmetry} & 24.2  & 25.2  & 16.1  & 3.0   & 49.8  & 57.4  & 40.4  & 23.4  & 26.8  & 10.3  & 11.0  & 2.1 \\
    CompCos~\cite{mancini2021open} & 25.3  & 24.6  & 16.4  & 4.5   & 59.8  & 62.5  & 43.1  & 28.1  & 28.1  & 11.2  & 12.4  & 2.6 \\
    CGE~\cite{naeem2021learning}   & 28.7  & 25.3  & 17.2  & 5.1   & 56.8  & 63.6  & 41.2  & 26.4  & 28.1  & 10.1  & 11.4  & 2.3 \\
    Co-CGE~\cite{mancini2022learning} & 27.8  & 25.2  & 17.5  & 5.1   & 58.2  & 63.3  & 44.1  & 29.1  & 29.3  & 11.9  & 12.7  & 2.8 \\
    SCEN~\cite{li2022siamese}  & 29.9  & 25.2  & 18.4  & 5.3   & 63.5  & 63.1  & 47.8  & 32.0  & 28.9  & 12.1  & 12.4  & 2.9 \\
    CVGAE~\cite{anwaar2022leveraging} & 28.5  & 25.5  & 18.2  & 5.3   & 65.0  & 62.4  & 49.8  & 34.6  & 28.2  & 11.9  & 13.9  & 2.8 \\
    CANet~\cite{wang2023learning} & 29.0  & 26.2  & 17.9  & 5.4   & 61.0  & 66.3  & 47.3  & 33.1  & 30.0  & 13.2  & 14.5  & 3.3 \\
    CAPE~\cite{khan2023learning} & 30.5  & 26.2  & 19.1  & 5.8   & 60.4  & 67.4  & 45.5  & 31.3  & 32.9  & 15.6  & 16.3  & 4.2 \\
    \hline
    \rowcolor{mygray}
    \multicolumn{13}{l}{\textit{CLIP-based methods}} \\
    CLIP~\cite{radford2021learning}& 30.2  & 46.0  & 26.1  & 11.0  & 15.8  & 49.1  & 15.6  & 5.0   & 7.5   & 25.0  & 8.6   & 1.4  \\
     CoOp~\cite{zhou2022learning}& 34.4  & 47.6  & 29.8  & 13.5  & 52.1  & 49.3  & 34.6  & 18.8  & 20.5  & 26.8  & 17.1  & 4.4  \\
     PCVL~\cite{xu2022prompting}& 48.5  & 47.2  & 35.3  & 18.3  & 64.4  & 64.0  & 46.1  & 32.2  & -     &  -    &  -    &  - \\
     CSP~\cite{nayaklearning}& 46.6  & 49.9  & 36.3  & 19.4  & 64.2  & 66.2  & 46.6  & 33.0  & 28.8  & 26.8  & 20.5  & 6.2  \\
     DFSP(i2t)~\cite{lu2023decomposed}& 47.4  & 52.4  & 37.2  & 20.7  & 64.2  & 66.4  & 45.1  & 32.1  & 35.6  & 29.3  & 24.3  & 8.7  \\
     DFSP(BiF)~\cite{lu2023decomposed}& 47.1  & 52.8  & 37.7  & 20.8  & 63.3  & 69.2  & 47.1  & 33.5  & 36.5  & 32.0  & 26.2  & 9.9  \\
     DFSP(t2i)~\cite{lu2023decomposed}& 46.9  & 52.0  & 37.3  & 20.6  & 66.7  & 71.7  & 47.2  & 36.0  & 38.2  & 32.0  & 27.1  & 10.5  \\
     GIPCOL~\cite{xu2024gipcol}& 48.5 & 49.6 & 36.6 & 19.9 & 65.0 & 68.5 & 48.8 & 36.2 & 31.9 & 28.4 & 22.5 & 7.1 \\
     CDS-CZSL~\cite{li2024context}& 50.3 & 52.9 & 39.2 & 22.4 & 63.9 & 74.8 & 52.7 & 39.5 & 38.3 & 34.2 & 28.1 & 11.1 \\
     Troika~\cite{huang2024troika}& 49.0 & 53.0 & 39.3 & 22.1 & 66.8 & 73.8 & 54.6 & 41.7 & 41.0 & 35.7 & 29.4 & 12.4 \\
     PLID~\cite{bao2023prompting}& 49.7 & 52.4 & 39.0 & 22.1 & 67.3 & 68.8 & 52.4 & 38.7 & 38.8 & 33.0 & 27.9 & 11.0 \\ \hline
     %CAILA
    \textbf{\textsc{ClusPro} (Ours)}  &\textbf{52.1}\tiny{$\pm$0.6} &\textbf{54.0}\tiny{$\pm$0.3}  &  \textbf{40.7}\tiny{$\pm$0.2} & \textbf{23.8}\tiny{$\pm$0.2} &  \textbf{70.7}\tiny{$\pm$1.0}  & \textbf{76.0}\tiny{$\pm$1.2} & \textbf{58.5}\tiny{$\pm$0.6}  & \textbf{46.6}\tiny{$\pm$0.5}   & \textbf{44.3}\tiny{$\pm$0.2}  &\textbf{37.8}\tiny{$\pm$0.2}  & \textbf{32.8}\tiny{$\pm$0.2} & \textbf{14.9}\tiny{$\pm$0.1}\\
    \hline
    \end{tabular}%
    }
     \vspace{-0.8em}
  \label{tab:sup_sota1}
\end{table}%

\begin{table}[H] %\small
  \centering
  \makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\textbf{More comparison results}(\S\ref{sec_app:A3}) on MIT-States~\cite{isola2015discovering}, UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning} within \textbf{\textit{OW}} setting.}
   \resizebox{0.99\textwidth}{!}{
    \setlength\tabcolsep{3.5pt}
    \renewcommand\arraystretch{1.0}
    \begin{tabular}{l|cccc|cccc|cccc}
    %\specialrule{0.05em}{0pt}{0pt}
    \hline
    \thickhline \rowcolor{mygray}
     \textit{Open-World}  & \multicolumn{4}{c|}{MIT-States} & \multicolumn{4}{c|}{UT-Zappos} & \multicolumn{4}{c}{C-GQA} \\
   \rowcolor{mygray} Method   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$ \\ \hline
    \rowcolor{mygray}
 \multicolumn{13}{l}{\textit{Traditional vision-based methods }} \\
   AoP~\cite{nagarajan2018attributes}   & 16.6  & 5.7   & 4.7   & 0.7   & 50.9  & 34.2  & 29.4  & 13.7  & - & - & - & - \\
    LE+~\cite{misra2017red}   & 14.2  & 2.5   & 2.7   & 0.3   & 60.4  & 36.5  & 30.5  & 16.3  & 19.2  & 0.7   & 1.0   & 0.1 \\
    TMN~\cite{purushwalkam2019task}   & 12.6  & 0.9   & 1.2   & 0.1   & 55.9  & 18.1  & 21.7  & 8.4   & - & - & - & - \\
    SymNet~\cite{li2020symmetry} & 21.4  & 7.0   & 5.8   & 0.8   & 53.3  & 44.6  & 34.5  & 18.5  & 26.7  & 2.2   & 3.3   & 0.4 \\
    CompCos~\cite{mancini2021open} & 25.4  & 10.0  & 8.9   & 1.6   & 59.3  & 46.8  & 36.9  & 21.3  & 28.4  & 1.8   & 2.8   & 0.4 \\
    CGE~\cite{naeem2021learning}   & 29.6  & 4.0   & 4.9   & 0.7   & 58.8  & 46.5  & 38.0  & 21.5  & 28.3  & 1.3   & 2.2   & 0.3 \\
    Co-CGE~\cite{mancini2022learning} & 26.4  & 10.4  & 10.1  & 2.0   & 60.1  & 44.3  & 38.1  & 21.3  & 28.7  & 1.6   & 2.6   & 0.4 \\
    KG-SP~\cite{karthik2022kg} & 28.4  & 7.5   & 7.4   & 1.3   & 61.8  & 52.1  & 42.3  & 26.5  & 31.5  & 2.9   & 4.7   & 0.8 \\
    CVGAE~\cite{anwaar2022leveraging} & 27.3  & 9.9   & 10.0  & 1.8   & 58.6  & 48.4  & 41.7  & 22.2  & 26.6  & 2.9   & 6.4   & 0.7 \\
    \hline
     \rowcolor{mygray}
 \multicolumn{13}{l}{\textit{CLIP-based methods }} \\
    CLIP~\cite{radford2021learning} &30.1 &14.3 &12.8 &3.0 &15.7 &20.6 &11.2 &2.2& 7.5& 4.6& 4.0& 0.3  \\
     CoOp~\cite{zhou2022learning}&34.6& 9.3 &12.3& 2.8 &52.1 &31.5 &28.9 &13.2 &21.0& 4.6& 5.5& 0.7 \\
     PCVL~\cite{xu2022prompting}& 48.5& 16.0 &17.7& 6.1 &64.6& 44.0 &37.1& 21.6  & -     &  -    &  -    &  - \\
     CSP~\cite{nayaklearning}& 46.3 &15.7 &17.4 &5.7& 64.1 &44.1& 38.9 &22.7& 28.7& 5.2& 6.9& 1.2  \\
     DFSP(i2t)~\cite{lu2023decomposed}& 47.2 &18.2& 19.1& 6.7& 64.3& 53.8& 41.2& 26.4& 35.6& 6.5& 9.0& 2.0 \\
     DFSP(BiF)~\cite{lu2023decomposed}& 47.1& 18.1& 19.2& 6.7& 63.5& 57.2& 42.7 &27.6 &36.4 &7.6& 10.6 &2.4  \\
     DFSP(t2i)~\cite{lu2023decomposed} & 47.5 &18.5 &19.3 &6.8& 66.8& 60.0 &44.0& 30.3 &38.3 &7.2& 10.4& 2.4  \\
     GIPCOL~\cite{xu2024gipcol}& 48.5 &16.0 &17.9& 6.3& 65.0& 45.0& 40.1& 23.5& 31.6& 5.5 &7.3 &1.3 \\
     CDS-CZSL~\cite{li2024context} & 49.4 &21.8& 22.1 &8.5& 64.7& 61.3 &48.2& 32.3& 37.6 &8.2& 11.6 &2.7 \\
     Troika~\cite{huang2024troika}& 48.8 &18.7& 20.1& 7.2 &66.4& 61.2 &47.8& 33.0 &40.8 &7.9& 10.9 &2.7\\
     PLID~\cite{bao2023prompting}& 49.1 &18.7& 20.0& 7.3& 67.6& 55.5 &46.6& 30.8& 39.1& 7.5& 10.6 &2.5\\ \hline
     \textbf{\textsc{ClusPro} (Ours)}   & \textbf{51.2}\tiny{$\pm$0.4} & \textbf{22.1}\tiny{$\pm$0.2}  &\textbf{23.0}\tiny{$\pm$0.1}  & \textbf{9.3}\tiny{$\pm$0.2}& \textbf{71.0}\tiny{$\pm$1.1}  & \textbf{66.2}\tiny{$\pm$1.0} & \textbf{54.1}\tiny{$\pm$0.7}  & \textbf{39.5}\tiny{$\pm$0.8}  &  \textbf{41.6}\tiny{$\pm$0.3} & \textbf{8.3}\tiny{$\pm$0.2} & \textbf{11.6}\tiny{$\pm$0.3}& \textbf{3.0}\tiny{$\pm$0.1}  \\
    \hline
    \end{tabular}%
    }
     \vspace{-0.2em}
  \label{tab:sup_sota2}%
\end{table}%


\noindent\textbf{Evaluation Results for Models Pre-trained on Datasets with No Overlap.} To further highlight the robustness and superiority of our approach, we additionally present results under \textit{CW} setting that utilize ViT-B backbone pre-trained with DINO~\cite{caron2020unsupervised} on ImageNet~\cite{deng2009imagenet} in a self-supervised manner as ADE~\cite{hao2023learning} instead of CLIP model~\cite{radford2021learning}. Besides, we encode text representation with word2vec~\cite{mikolov2013distributed} as~\cite{hao2023learning,wang2023learning,naeem2021learning} instead of the text encoder of CLIP. Table~\ref{tab:dino} reports the comparison results on UT-Zappos~\cite{yu2014fine} and CGQA~\cite{naeem2021learning}. As seen, our algorithm also demonstrates better performance than the baseline and SOTA non-CLIP methods~\cite{li2022siamese,anwaar2022leveraging,khan2023learning,hao2023learning}.

\begin{table}[H] %\small
  \centering  \makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\textbf{More comparison results}(\S\ref{sec_app:A3}) on UT-Zappos~\cite{yu2014fine} and C-GQA~\cite{naeem2021learning} within \textbf{\textit{CW}} setting. Our algorithm utilizes ViT-B backbone pre-trained with DINO~\cite{caron2020unsupervised} as the visual encoder and word2vec~\cite{mikolov2013distributed} as the text encoder for a fair comparison with non-CLIP methods.}
   \resizebox{0.8\textwidth}{!}{
    \setlength\tabcolsep{3.5pt}
    \renewcommand\arraystretch{1.0}
    \begin{tabular}{l|cccc|cccc}
    %\specialrule{0.05em}{0pt}{0pt}
    \hline
    \thickhline \rowcolor{mygray}
      &  \multicolumn{4}{c|}{UT-Zappos} & \multicolumn{4}{c}{C-GQA} \\
   \rowcolor{mygray}\multirow{-2}{*}{Method}    & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$   & Seen$\uparrow$     & Unseen$\uparrow$     & HM$\uparrow$    & AUC$\uparrow$ \\
    \hline

  AoP~\cite{nagarajan2018attributes}     & 59.8  & 54.2  & 40.8  & 25.9  & 17.0  & 5.6   & 5.9   & 0.7 \\
    SCEN~\cite{li2022siamese}    & 63.5  & 63.1  & 47.8  & 32.0  & 28.9  & 12.1  & 12.4  & 2.9 \\
    CVGAE~\cite{anwaar2022leveraging}  & 65.0  & 62.4  & 49.8  & 34.6  & 28.2  & 11.9  & 13.9  & 2.8 \\
    CANet~\cite{wang2023learning}   & 61.0  & 66.3  & 47.3  & 33.1  & 30.0  & 13.2  & 14.5  & 3.3 \\
    CAPE~\cite{khan2023learning}   & 60.4  & 67.4  & 45.5  & 31.3  & 32.9  & 15.6  & 16.3  & 4.2 \\
    ADE~\cite{hao2023learning} & 63.0&64.3&51.1&35.1&35.0&17.7&18.0&5.2 \\ 
    CGE~\cite{naeem2021learning}& -& - &- &-& 38.0 &17.1&18.5&5.4 \\
    OADis~\cite{saini2022disentangling}&-& - &- &-& 38.3& 19.8&20.1&7.0 \\
    \hline
     %CAILA
     Baseline  &61.0&62.9&45.1&31.9&34.6&15.9&16.6&4.5 \\
    \textbf{\textsc{ClusPro} (Ours)}  &  $\mathbf{65.1}$ &$\mathbf{68.0}$& $\mathbf{52.3}$ &$\mathbf{37.2}$& $\mathbf{39.3}$ &$\mathbf{23.0}$ &$\mathbf{22.3}$ &$\mathbf{7.6}$ \\
    \hline
    \end{tabular}%
    }
     \vspace{-0.8em}
  \label{tab:dino}
\end{table}%

\noindent\textbf{Efficiency Analysis.} The efficiency comparison results with the state-of-the-art Trokia~\cite{huang2024troika} and our baseline are reported in Table~\ref{efficiency}. 
Note that, \textsc{ClusPro} conducts within-primitive prototype clustering in a nonparametric manner and discards these learned sub-primitive prototypes during the testing phase. 
Thus, as shown in Table~\ref{efficiency}, \textsc{ClusPro} neither requires additional trainable parameters nor causes any inference delay during testing compared to the base model. 
Though efficient in terms of parameters and inference speed, our online clustering algorithm brings slight training delay ( $\!\sim\!11.5\%$ on UT-Zappos~\cite{yu2014fine}). 
Moreover, the effective clustering algorithm allows \textsc{ClusPro} to outperform the state-of-the-art Trokia in terms of classification accuracy, trainable parameters, and inference speed. 
 \begin{table}[H]
\centering
    \setlength\tabcolsep{4pt}
    \renewcommand\arraystretch{1.1}
    \caption{ Efficiency comparison on UT-Zappos~\cite{yu2014fine}. Here, we report trainable parameters, training time per epoch, and inference speed for each model.  See in \S\ref{sec_app:A3} for more details. }
    \begin{tabular}{c|c|c|c|c|c}
\thickhline
\rowcolor{mygray} Method  & Params$\downarrow$ & Memory$\downarrow$ &Training time$\downarrow$ & Inference Speed$\downarrow$ & AUC$\uparrow$ \\
 \hline\hline
 Troika~\cite{huang2024troika}          & $21.7$M  &$19.9$G & $4.1$min &$14.9$ms & $41.9$   \\ 
 Baseline          & $8.7$M &$18.2$G &$4.0$min &$14.6$ms  & $41.0$ \\ \hline
\textbf{\textsc{ClusPro} (ours)}   & $8.7$M  &$18.5$G &$4.6$min  &  $14.6$ms & $46.6$ \\ \hline 
\end{tabular}\vspace{-0.2cm}
    \label{efficiency}
\end{table}

\noindent\textbf{Number of Prototypes $K$.} In Table~\ref{tab:vary_num}, we conduct the experiment by setting the number $K$ of prototypes based on the proportion of training samples for each primitive. In UT-zappos~\cite{yu2014fine} dataset, training samples per primitive range from 0.2\% to over 20\%. Thus, we assign $K\!=\!1$ to the primitive with $0.2\!\sim\!5\%$ training samples, $K\!=\!2$ to the primitive with $5\!\sim\!10\%$ training samples, $K\!=\!3$ to the primitive with $10\!\sim\!15\%$ training samples, $K\!=\!4$ to the primitive with $15\!\sim\!20\%$ training samples, and  $K\!=\!5$ to the primitive with over 20\% training samples. As seen, this approach results in slightly better performance than setting a fixed value for all the primitives.
\begin{table}[H] %\small
  \centering  \makeatletter\def\@captype{table}\makeatother\captionsetup{font=small}\caption{\textbf{Ablative experiments regarding varying $K$} on UT-Zappos~\cite{yu2014fine}. See \S\ref{sec_app:A3} for more details.}
   \resizebox{0.5\textwidth}{!}{
			\setlength\tabcolsep{3.9pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{c||cccc}
\thickhline
\rowcolor{mygray} & 
\multicolumn{4}{c}{{UT-Zappos}}\\
\rowcolor{mygray}    \multirow{-2}{*}{$K$ range} & Seen$\uparrow$ & Uneen$\uparrow$ & HM$\uparrow$ & AUC$\uparrow$ \\ \hline  \hline
  unique value $5$       & $70.7$ & $76.0$  & $58.5$ & $46.6$  \\  
  $[1,5]$        & $\mathbf{71.0}$ & $\mathbf{76.0}$ & $\mathbf{58.6}$ & $\mathbf{46.8}$  \\\hline
\end{tabular}}
     \vspace{-0.8em}
  \label{tab:vary_num}
\end{table}
\section{More Qualitative Visualization}
\label{sec_app:A4}
\noindent\textbf{More Case Study.} We provide additional success and failure cases of our method \textsc{ClusPro} across three CZSL benchmarks, \ie, MIT-States~\cite{isola2015discovering} in Fig.~\ref{fig::sup_casemit}, UT-Zappos~\cite{yu2014fine} in Fig.~\ref{fig::sup_caseut} and C-GQA~\cite{naeem2021learning} in Fig.~\ref{fig::sup_casecgqa}. We also compare our approach \textsc{ClusPro} with baseline without within-primitive clustering. As seen, by mining rich sub-primitive patterns via within-primitive clustering, \textsc{ClusPro} can produce more accurate composition predictions, even recognizing fine-grained primitives, such as various materials and colors. For failure cases, where the attribute and object of images are highly entangled, \textsc{ClusPro} still identifies the part of the attribute-object composition.

\vspace{-6pt}
\section{Pseudo Code of Prototype Assignment and Updating}
\label{sec_app:A2}
\vspace{-3pt}

Algorithm \ref{alg:code} provides the pseudo-code of ``Local-aware Prototype Assignment" and ``Prototype Updating". To guarantee reproducibility, our code is available at \href{https://github.com/quhongyu/ClusPro}{\textsc{ClusPro}}. 
\vspace{8pt}
\section{Discussion}
\label{sec_app:A5}
%{\color{blue}\noindent\textbf{Data Overlap Analysis.} CLIP~\cite{radford2021learning} is trained on millions of text-image pairs sourced from the web, so it is unintentional that CLIP might be exposed to certain unseen compositions during the pre-training. To investigate this, previous work~\cite{radford2021learning} provides the data overlap analysis on 35 datasets. Note that there is a median overlap of 2.2\% and an average overlap of 3.2\%. Due to this small amount of overlap, the overall accuracy shift is less than 0.1\% with the largest shift as 0.6\%. In addition, instead of only considering unseen compositions during testing, most current CZSL researches~\cite{lu2023decomposed,nayaklearning,li2024context,jing2024retrieval} (including our work) report the performance in the Generalized Zero-shot Learning~\cite{mancini2021open} for both \textit{CW} and \textit{OW} settings, where test samples include both seen and unseen compositions.


%In light of the above, CLIP-based methods still align with the definition of CZSL, even though CLIP may be exposed to a small number of unseen compositions during pre-training. Moreover, our experiments in Table~\ref{tab:sup_sota1} and \ref{tab:sup_sota2} also suggest that directly employing CLIP without any fine-tuning underperforms non-CLIP methods~\cite{hao2023learning,naeem2021learning,saini2022disentangling} in CZSL. We suspect that this is because contrastive pre-training on Web text-image pairs does not require the model to learn precise compositional semantics. Thus, there is significant room for improvement from fine-tuning CLIP to explicitly enforcing compositional semantics.} 

\noindent\textbf{Data Overlap Analysis.} Given that CLIP~\cite{radford2021learning} is trained on millions of text-image pairs sourced from the web, it is hard to know whether CLIP has been exposed to certain unseen compositions during its pre-training, which violates the zero-shot learning setting factually. 
Most current researches~\cite{lu2023decomposed,nayaklearning,li2024context,jing2024retrieval} in CZSL, including our work, report the performance in the Generalized Zero-shot Learning~\cite{mancini2021open} for both \textit{CW} and \textit{OW} settings, where test samples include both seen and unseen compositions. 
Hence, it naturally brings up the question: \textit{whether CLIP meets the definition of Generalized Zero-shot Learning}. 
Based on the data overlap analysis on 35 datasets as reported in~\cite{radford2021learning}, there is a median overlap of 2.2\% and an average overlap of 3.2\%.
Due to this small amount of overlap, the overall accuracy shift is less than 0.1\% with the largest shift as 0.6\%.
As such, CLIP is only exposed to a very small number of unseen compositions during pre-training, and the impact on the performance is limited. 
However, the potential composition leaking in the pre-training of CLIP indeed leads to an unfair comparison with other non-CLIP methods~\cite{hao2023learning,naeem2021learning,saini2022disentangling}. 
Thus, we argue that it is important to emphasize the comparisons with other CLIP-based methods that share the same pre-training (comparison results in Table~\ref{tab:sota1} and \ref{tab:sota2}). 
Moreover, where possible, it is also advisable to report performance metrics for non-CLIP variants to ensure a comprehensive evaluation.

\noindent\textbf{Limitation.} One limitation of our algorithm is that it needs extra within-primitive prototype clustering from the perspective of optimal transport after each training iteration, leading to increasing time complexity. However, in practice, our clustering algorithm only brings slight training delay attributed to efficient GCG algorithm~\cite{chang2023csot} for solving such clustering problem. Additionally, our mined sub-primitive prototypes are subject to the data distribution of the training dataset. Thus rare primitive concepts in the dataset (\ie, long-tail distribution), like many previous state-of-the-arts,  pose significant challenges for primitive-wise clustering to discover diverse sub-primitive patterns, thus resulting in poor performance on unseen compositions about these primitives. Also, the number of prototypes for each primitive currently is set to a fixed value, which may not be optimal given that intra-primitive variability varies across primitives. Thus it is interesting to find ways to automatically determine $K$~\cite{ronen2022deepdpm} for different primitives, which may further boost performance.  

\noindent\textbf{Border Impact.} This work introduces \textsc{ClusPro}, a powerful clustering-based framework for Compositional Zero-Shot Learning via exploring dataset-level context, which overcomes the limitations of previous solutions relying on single or paired images for visual disentanglement. This model provides a feasible way to discover diverse sub-primitive patterns in massive training data, and directly shape well-structured embedding space based on these mined patterns. On the positive side, \textsc{ClusPro} pushes the boundary of CZSL algorithms, and can benefit a number of potential real-world applications, \eg, autonomous driving and robotics. For the potential negative societal impacts, our \textsc{ClusPro} struggles in handling very rare primitives in the dataset, which is a common issue of current CZSL algorithms, thus leading to inaccurate decisions or planning of systems. To avoid this potential problem,  it is crucial to develop a security protocol in case our approach fails to perform as expected in real-world scenarios. 
\begin{figure}[H]
    \centering
\includegraphics[width=0.99\textwidth]{sup_casemit.pdf}
   \vspace{-10pt}
    \captionsetup{font=small}\caption{More case studies on MIT-States~\cite{isola2015discovering}. We compare \textsc{ClusPro} with baseline without primitive-wise prototype clustering. Correct and incorrect predictions are marked in \textbf{\textcolor{ggreen}{green}} and \textbf{\textcolor{red}{red}}, respectively.}
    \vspace{-5pt}
    \label{fig::sup_casemit}
\end{figure}
\begin{figure}[H]
    \centering
\includegraphics[width=0.99\textwidth]{sup_caseut.pdf}
   \vspace{-10pt}
    \captionsetup{font=small}\caption{More case studies on UT-Zappos~\cite{yu2014fine}. We compare \textsc{ClusPro} with baseline without primitive-wise prototype clustering. Correct and incorrect predictions are marked in \textbf{\textcolor{ggreen}{green}} and \textbf{\textcolor{red}{red}}, respectively.}
    \vspace{-18pt}
    \label{fig::sup_caseut}
\end{figure}
\begin{figure}[H]
    \centering
\includegraphics[width=0.99\textwidth]{sup_casecgqa.pdf}
   \vspace{-10pt}
    \captionsetup{font=small}\caption{More case studies on C-GQA~\cite{naeem2021learning}. We compare \textsc{ClusPro} with baseline without primitive-wise prototype clustering. Correct and incorrect predictions are marked in \textbf{\textcolor{ggreen}{green}} and \textbf{\textcolor{red}{red}}, respectively.}
    \vspace{-18pt}
    \label{fig::sup_casecgqa}
\end{figure}
\begin{algorithm}[H]
    \caption{Pseudo-code of prototype assignment and updating in a PyTorch-like style.}
    \label{alg:code}
    \definecolor{codeblue}{rgb}{0.25,0.5,0.5}
	\lstset{
		backgroundcolor=\color{white},
		basicstyle=\fontsize{7.8pt}{7.8pt}\ttfamily\selectfont,
		columns=fullflexible,
		breaklines=true,
		captionpos=b,
		escapeinside={(:}{:)},
		commentstyle=\fontsize{7.8pt}{7.8pt}\color{codeblue},
		keywordstyle=\fontsize{7.8pt}{7.8pt},
     %  frame=tb,
    }
   \begin{lstlisting}[language=python]
"""
# P: primitive prototypes (C x K x D)
# F: primitive feature embeddings (N x D)
# y: labels for primitive features F (N)

# C: number of attribute or object primitives 
# K: number of prototypes for each primitive
# N: batch size   
# mu: momentum coefficient (Eq.8)
"""
 
#======= local-aware prototype assignment =======#
(:\color{codedefine}{\textbf{def}}:) (:\color{codefunc}{\textbf{prototype\_assignment}}:)(F, label):
    # prototype assignment for each primitive feature (Eq.7)
    Q = (:\color{codeorg}{\textbf{torch.einsum}}:)('nd,ckd->nkc', F, P) 
    S = (:\color{codeorg}{\textbf{torch.einsum}}:)('nd,nd->nn', F, F)  # primitive self-similarity matrix 
       
    (:\color{codedefine}{\textbf{for}}:) c (:\color{codedefine}{\textbf{in}}:) (:\color{codedefine}{range}:)(C):
        init_q = Q[...,c]   
        init_l = (:\color{codeorg}{\textbf{local\_online\_clustering}}:)(init_q, S)  # one-hot matrix 

        # prototype assignments for features in primitive c
        l_c = Q[label == c]  
        f_c = F[label == c, ...] 

        # find features that are assigned to each sub-primitive prototype 
        l_c_tile = (:\color{codeorg}{\textbf{torch.einsum}}:)(l_c, tile=K)
        l_q = init_l * l_c_tile

        # find features with primitive c that are correctly classified
        f_c_tile = (:\color{codeorg}{\textbf{repeat}}:)(l_c, tile=f_c.(:\color{codepro}{\textbf{shape}}:)[-1])
        f_c_q = f_c * f_c_tile

        # new cluster features for primitive c
        a = (:\color{codeorg}{\textbf{torch.mm}}:)(l_q.(:\color{codepro}{\textbf{transpose}}:)(),f_c_q)

        # momentum updating for each primitive prototype (Eq.8)
        (:\color{codefunc}{\textbf{prototype\_updating}}:)(l_q, a, c)
        

#======= prototype updating =======#
(:\color{codedefine}{\textbf{def}}:) (:\color{codefunc}{\textbf{prototype\_updating}}:)(l_q, a, c):
    # num assignments for each prototype of primitive c
    n = (:\color{codeorg}{\textbf{torch.sum}}:)(l_q, dim=0)
    a = (:\color{codeorg}{\textbf{normalize}}:)(a)

    # prototype updating
    (:\color{codedefine}{\textbf{if}}:) (:\color{codeorg}{\textbf{torch.sum(n)}}:) > 0:
        P_c = P[c, n != 0,:] * mu + a[n != 0,:] * (1 - mu)
        P[c, n != 0, :] = P_c
   \end{lstlisting}
   \end{algorithm}
\clearpage


\end{document}
