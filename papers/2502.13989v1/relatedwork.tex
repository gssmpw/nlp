\section{Related Works}
\subsection{Large Image Generative Models}
The introduction of CLIP~\cite{pmlr-v139-radford21a}, which was trained on vast amounts of the Internet data, has strengthened the connection between natural language and images. The wide use of diffusion models~\cite{pmlr-v37-sohl-dickstein15,NEURIPS2020_4c5bcfec} has enabled stable image generation even on large-scale datasets with high variance~\cite{dhariwal2021diffusion}. The fusion of these two advancements has made it possible to generate images on the basis of natural language instructions~\cite{pmlr-v162-nichol22a,saharia2022photorealistic}. Scaling laws~\cite{kaplan2020scalinglawsneurallanguage,hoffmann2022an} observed in LLMs have also been applied to various aspects of image generation. For example, it has been noted that increasing the size of the text encoder used for text conditioning improves the model's ability to reflect the given instructions more accurately~\cite{saharia2022photorealistic}.

\subsection{Concept Erasure from Text-to-Image Generative Models}
It is possible to prevent specific concepts from being generated in image generative models. Research on this topic has primarily focused on diffusion models, with various approaches being explored, including methods for intervening during the generation process~\cite{brack2023sega}, techniques for directly editing model parameters using a closed-form equation~\cite{Gandikota_2024_WACV,basu2024localizing,pmlr-v235-basu24b,Lu_2024_CVPR}, approaches for updating certain parameters of text-to-image generative models through backpropagation~\cite{Gandikota_2023_ICCV,Kumari_2023_ICCV,Fuchi_2024_BMVC,fan2024salun,kim2023safeselfdistillationinternetscaletexttoimage}, and methods for leveraging adapters for updating specific components~\cite{Lyu_2024_CVPR,Lu_2024_CVPR}.

\subsection{Evaluating of Concept Erasure Methods}
Quantitatively evaluating the performance of concept erasure methods is challenging. Previous studies have conducted only evaluations using various independent methods, lacking a consistent and comprehensive assessment framework. Six-CD~\cite{ren2024sixcdbenchmarkingconceptremovals} addresses this issue by constructing a comprehensive dataset and conducting systematic evaluations and proposes the in-Prompt CLIP Score, achieving a more generalized evaluation approach. 
Evaluation methods, such as ConceptBench~\cite{Zhang_2024_CVPR} and ImageNet Concept Editing Benchmark (ICEB)~\cite{xiong2024editingmassiveconceptstexttoimage} have been proposed. However, these evaluation methods were proposed at the same time as the concept erasure methods, which suggests the possibility of arbitrary evaluation. For a detailed analysis, please refer to \cref{app:used-metrics}.