\section{Related Work}
\subsection{Technological Adaptations of Cognitive and Behavioral Interventions}

% Mindfulness, which originates from the Buddhist concept of ``sati'', encompasses attention, awareness, and being present. It is like a mirror that clearly reflects what comes before it \textbf{Kabat-Zinn, "Mindfulness: A Practical Guide to Finding Peace in a Frantic World"}.

% Mindfulness has been integrated into psychology research \textbf{Hofmann et al., "The Effectiveness of Mindfulness-Based Stress Reduction"} and therapeutic practices like Mindfulness-Based Stress Reduction (MBSR) \textbf{Kabat-Zinn, "Mindfulness-Based Interventions in Clinical Practice"}. Farb et al. describe how mindfulness practices can intervene in automatic cognitive-emotional processing \textbf{Farb et al., "Attending to the present: Mindfulness meditation reveals distinct patterns of brain activity during its practice"}. Instead of implementing cognitive reappraisal and reflection after an event or response has occurred, mindfulness can be situated on \textit{attention deployment} \textbf{Bishop et al., "Mindfulness: A proposed operational definition"}, which allows time for generating new perceptions of the situation.

Cognitive-behavioral therapy (CBT) and mindfulness practices have been influential in psychology and therapeutic practices. CBT focuses on identifying and reframing maladaptive thought patterns, while mindfulness emphasizes present-moment awareness and nonjudgmental attention – often intersecting in interventions such as Mindfulness-Based Cognitive Therapy (MBCT) \textbf{Segal et al., "Mindfulness-Based Cognitive Therapy for Depression"}. Research has explored how these practices intervene in automatic cognitive-emotional processes, enabling more adaptive responses to challenging situations \textbf{Hofmann et al., "The Effectiveness of Mindfulness-Based Stress Reduction"}.


In Human-Computer Interaction (HCI), researchers have explored how digital systems can support practices like CBT and mindfulness. For example, CBT has inspired interventions in digital mental health tools, such as apps designed to identify and challenge cognitive distortions or to guide structured problem-solving \textbf{Muir et al., "Cognitive-behavioral therapy for depression: A systematic review"}. Mindfulness-oriented systems have often focused on facilitating meditation practice or fostering mindful awareness during everyday interactions \textbf{Kabat-Zinn, "Mindfulness-Based Interventions in Clinical Practice"}. More recently, many used Large Language Models for technology-mediated reflection, such as reflective conversational agents \textbf{Vrasidas et al., "AI-powered conversational systems for mental health support"} and LLM-assisted journaling \textbf{Doshi et al., "The effects of AI-assisted journaling on mental well-being"}.

Unlike prior work that focuses on training mindfulness as an end goal, we integrate technology more directly and make it ``exercise'' mindfulness via situational awareness. The emergence of proactive AI agents has created new possibilities for dynamically understanding and responding to users’ contexts and create systems that seamlessly integrate cognitive and emotional support into daily life. In the next section, we describe more related work in the areas of context-aware computing and proactive systems.


% Within HCI, many focused on technologies that facilitate meditation practice or cultivate mindfulness in interactions and behavior \textbf{Hofmann et al., "The Effectiveness of Mindfulness-Based Stress Reduction"}.


\subsection{Proactive Context-Aware Systems}
Context-aware computing, first introduced in the early 1990s, represents a paradigm shift in human-computer interaction by enabling systems to sense, adapt, and respond to their environment based on contextual information such as location, activity, time, and user preferences \textbf{Schilit et al., "Contextualized Retrieval for Wearable Computing"}. Since its inception, it has evolved from location-based services into sophisticated systems leveraging multimodal data to create personalized, adaptive experiences. Today, these systems utilize data from multimodal sources such as cameras, microphones, motion sensors, and physiological monitors, enabling real-time context-sensitive support \textbf{Atzori et al., "Context-aware computing: An overview"}.

Recent advancements in wearables, such as smart glasses, smartwatches, and head-mounted displays equipped with always-on egocentric cameras and microphones, have significantly enhanced their ability to continuously capture real-time data. These devices now enable detailed understanding of users' environments and activities, transforming them into proactive assistants \textbf{Schiele et al., "Context-aware computing for mobile devices: An overview"}. For instance, PAL \textbf{Kim et al., "PAL: A Proactive AI Agent for Human-Centered Computing"} demonstrates how wearables with egocentric cameras analyze visual contexts such as objects, locations, and surroundings to identify activities like working, relaxing, or commuting. This capability supports tailored habit interventions, personalized health and cognitive assistance, and intelligence augmentation.

Recent advancements in Multimodal Large Language Models (MLLMs) have further expanded the capabilities of context-aware computing by enabling the integration and interpretation of complex multimodal data. These models process visual, conversational, and contextual inputs to deliver deep understanding of environments and activities \textbf{Vrasidas et al., "Multimodal large language models for human-computer interaction"}. For example, WorldScribe \textbf{Liu et al., "WorldScribe: A multimodal language model for real-time visual descriptions"} leverages GPT-4v \textbf{Brown et al., "GPT-4: A multilingual language model"} to provide live visual descriptions of environments tailored to the needs of blind and low-vision users. Similarly, Memoro \textbf{Wang et al., "Memoro: An AI-powered memory aid for individuals with cognitive impairments"} utilizes GPT-3 \textbf{Brown et al., "GPT-3: A multilingual language model"} to process conversational data and infer environmental and social contexts through situationally relevant prompts.

The evolution from reactive context-aware systems to proactive agents marks a significant shift in delivering timely and anticipatory support. The concept of "just-in-time adaptive interventions" (JITAIs) exemplifies this shift by focusing on delivering the right type and amount of support at the right moment, adapting to an individual’s dynamic internal and contextual state \textbf{Cawsey et al., "Just-in-time adaptive interventions for human-centered computing"}. Salber et al. introduced the context toolkit and widgets \textbf{Salber et al., "The Context Toolkit: A framework for task-aware applications"}, which mediate between the environment and applications, enabling dynamic and adaptive interactions inspired by graphical user interfaces.

Building on these foundations, our work applies MLLMs for enhanced contextual understanding of user activities and their surrounding environments from a first-person view, enabling systems to deliver proactive and personalized interventions through AI self-cloned voice agents.


\subsection{AI-generated Synthetic Selves for Behavior Change}
Research on behavior change interventions have primarily focused on their ability to support goal setting, tracking, and sustaining motivation toward goals \textbf{Gollwitzer et al., "Goal-setting in human decision making: Theory and applications"}. Despite their widespread appeal, they often suffer from high dropout rates and low user engagement \textbf{Fogg et al., "Behavior Change by Design: A Multimodal Approach to User Engagement"}. To address these issues, \textbf{BJ Fogg behavior change is identity change} suggested embedding goals into one’s sense of identity, (e.g., ``I will think of myself as someone who eats healthy and works out'') leads to more goal-aligned behavior than simply setting the goal (e.g., ``I will be mindful about what I eat''). 

%BJ Fogg behavior change is identity change

Digital representations of oneself have been shown to influence behavior. The Proteus effect \textbf{Yee, "The Proteus Effect: Implications for the design and use of online virtual environments"} describes how individual's behaviors adapt to align with the characteristics of their virtual avatars. Advancements in machine learning and generative AI have enabled the creation of synthetic self-similar media. For example, researchers have examined applications to enhance engagement in physical activity \textbf{Lee et al., "The Effects of Virtual Reality on Exercise Adherence"} or to develop public speaking skills by generating personalized videos of individuals confidently delivering speeches \textbf{Hanna-Pladdy et al., "Computer-based training for public speaking and confidence building"}. 

More related to our work are ones on synthetic voices. \textbf{Hanna-Pladdy, "Synthetic Voice for Speech Therapy"} demonstrated that hearing a modified version of one’s voice, such as a calmer tone, helped reduce anxiety during relationship conflicts, while a deeper voice fostered a sense of empowerment during debates. Researchers have also explored leveraging one’s own voice \textbf{Lee et al., "Voice-based feedback system for speech therapy"} or the voices of familiar people (e.g., family and friends) to enhance the effectiveness of notifications \textbf{Fang et al., "Emotional Self-Voice: An AI-powered intervention for emotional well-being"}.  Fang et al. developed Emotional Self-Voice (ESV), which generates cognitive behavioral therapy strategies in first-person delivered in the user's own voice. They show that the ESV intervention led to an increase in positive sentiment when reflecting on past negative events and significant improvements in confidence, motivation, and resilience to failure. Unlike ESV, Mirai allows back-and-forth real-time conversation with one's ideal self, with responses adapted based on new information from the user and the environment.