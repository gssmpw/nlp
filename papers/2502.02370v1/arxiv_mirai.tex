
%%
% \documentclass[manuscript,screen,review]{acmart} % publication format
\documentclass[manuscript]{acmart} % submission format

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}

% \usepackage{soul}
% \usepackage{color}

% \setstcolor{blue}
% \sethlcolor{yellow}

% \usepackage{booktabs}

\usepackage{tabularx}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Mirai: A Wearable Proactive AI ``Inner-Voice'' for Contextual Nudging}
% \title{Mirai: A Wearable Proactive AI Doppleganger for Contextual Nudging}
% \title{Mirai: A Wearable Proactive AI Paragon for Contextual Nudging}
% \title{Mirai: A Wearable Proactive AI Alter-Ego for Contextual Nudging}
% \title{Mirai: A Proactive AI System for Contextual Nudging Using Wearable Technology and Voice Cloning}


\renewcommand{\shorttitle}{Mirai}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Cathy Mengying Fang}
\email{catfang@media.mit.edu}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
  \country{USA}
}
\author{Yasith Samaradivakara}
\email{yasith@ahlab.org}
\affiliation{%
  \institution{Augmented Human Lab, National University of Singapore}
  \city{Singapore}
  \country{Singapore}
}

\author{Pattie Maes}
\email{pattie@media.mit.edu}
\affiliation{%
  \institution{MIT Media Lab}
  \city{Cambridge}
  \country{USA}
}
\author{Suranga Nanayakkara}
\email{suranga@ahlab.org}
\affiliation{%
  \institution{Augmented Human Lab, National University of Singapore}
  \city{Singapore}
  \country{Singapore}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Fang et al.}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
People often find it difficult to turn their intentions into real actions---a challenge that affects both personal growth and mental well-being. While established methods like cognitive-behavioral therapy and mindfulness training help people become more aware of their behaviors and set clear goals, these approaches cannot provide immediate guidance when people fall into automatic reactions or habits. We introduce Mirai, a novel wearable AI system with an integrated camera, real-time speech processing, and personalized voice-cloning to provide proactive and contextual nudges for positive behavior change. Mirai continuously monitors and analyzes the user's environment to anticipate their intentions, generating contextually-appropriate responses delivered in the user's own cloned voice. We demonstrate the application of Mirai through three scenarios focusing on dietary choices, work productivity, and communication skills. We also discuss future work on improving the proactive agent via human feedback and the need for a longitudinal study in naturalistic settings.
\end{abstract}


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003227</concept_id>
       <concept_desc>Information systems~Information systems applications</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003124.10010870</concept_id>
       <concept_desc>Human-centered computing~Natural language interfaces</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
    <concept>
        <concept_id>10003120.10003138.10003141.10010900</concept_id>
        <concept_desc>Human-centered computing~Personal digital assistants</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10003120.10003123.10010860.10011121</concept_id>
        <concept_desc>Human-centered computing~Contextual design</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information systems applications}
\ccsdesc[500]{Human-centered computing~Natural language interfaces}
\ccsdesc[500]{Human-centered computing~Personal digital assistants}
\ccsdesc[500]{Human-centered computing~Contextual design}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{voice, generative ai, nudging, goals, proactive agents, context-aware}



\begin{teaserfigure}
\centering
 \includegraphics[width=.8\textwidth]{figures/teaser.png}
 \caption{Mirai is a wearable AI system with an integrated camera, real-time speech processing, and personalized voice-cloning to provide proactive and contextual nudges for positive behavior change. Mirai continuously analyzes the user's environment to anticipate their intentions (e.g., drinking a can of soda) and intervenes in-the-moment to nudge them towards their health goals.}
 \Description{Mirai is a wearable AI system with an integrated camera, real-time speech processing, and personalized voice-cloning to provide proactive and contextual nudges for positive behavior change. Mirai continuously analyzes the user's environment to anticipate their intentions (e.g., drinking a can of soda) and intervenes in-the-moment to nudge them towards their health goals.}
 \label{fig:teaser}
\end{teaserfigure}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Behavioral change is a complex and difficult process that often involves numerous psychological and environmental factors. One of the factors is that although individuals often possess strong intentions, translating these into meaningful actions frequently proves to be a significant challenge, impeding both personal development and mental well-being. This disconnect between what people intend to do and what they actually do--- also known as the intention-behavior gap \cite{sheeran2016intention}---remains a significant obstacle to behavior change. While established methods like cognitive-behavioral therapy (CBT) \cite{beck1970cognitive} and mindfulness training \cite{farb2014mindfulness, kabat2015mindfulness} help people become more aware of their behaviors and set clear goals, these approaches cannot provide immediate guidance when people fall into automatic reactions or habits. 

% Cognitive-behavioral therapy (CBT) and reframing techniques provide valuable frameworks for behavioral modification through goal-setting and positive reinforcement \cite{beck1970cognitive}. However, they often lack mechanisms for delivering immediate, personalized feedback during critical moments, particularly for addressing automatic and reactive behaviors that require in-the-moment intervention.

% Mindfulness practices complement these approaches by teaching people to be aware of their present thoughts and actions, helping them respond thoughtfully rather than react automatically \cite{farb2014mindfulness, kabat2015mindfulness}. Yet traditional mindfulness techniques, such as meditation, primarily build general awareness rather than providing support during specific challenging moments when intervention would be most helpful.

Many mobile applications and digital tools have been developed to promote habit change and personal growth, through habit-tracking and goal-setting. Despite their popularity, these tools often fail in face of automatic and reactive behaviors that require in-the-moment intervention because: (a) they are not proactive, relying on users to initiate interactions; (b) they lack comprehensive contextual awareness to provide relevant and timely feedback; and (c) they frequently use ineffective feedback modalities, such as generic data visualizations, which fail to engage users meaningfully. As a result, these applications struggle to deliver effective behavior change \cite{rieder2021users, mckay2019using, thomas2021systematic, milne2020mobile}.

Recent advancements in multimodal artificial intelligence (AI) present an opportunity to address these challenges. By integrating capabilities such as natural language understanding, real-time speech processing, and contextual analysis of visual and auditory inputs, multimodal AI systems can provide more proactive, personalized, and contextually-aware interventions \cite{10.1145/3613904.3642450, Khan2019PALAW}.


% Recent developments in multimodal artificial intelligence systems offer new opportunities for supporting mindfulness and behavior change. These systems can engage in conversations in natural language through text and speech while processing multiple input modalities, including visual and auditory information. However, current AI implementations remain predominantly reactive, requiring user initiation rather than providing proactive support during critical moments \cite{10.1145/3613904.3642450, Khan2019PALAW}.

In this paper, we introduce Mirai, a system that integrates situational awareness through an always-on wearable camera, real-time speech processing, and personalized interventions using voice-cloning technology. Mirai continuously analyzes the user's environment while anticipating their intents, and evaluates their alignment with their goals. Through contextually-aware conversations, Mirai strategically intervenes while minimizing disruption. Mirai generates responses based on established psychological practices that are then delivered in the user's own voice (through voice cloning), which has been shown to induce more attention, engagement, and recall \cite{fang2024leveraging, kim2024myvoice}.

We demonstrate Mirai's application through three scenarios where real-time decision support can facilitate goal achievement: 1) maintaining healthy dietary choices, 2) sustaining productive work habits, and 3) developing confident communication patterns. These scenarios illustrate how Mirai's proactive and contextual prompts can promote mindfulness and support behavior change.

Finally, we highlight the privacy implications of continuous environmental monitoring and the challenge of balancing timely interventions with user agency. We also discuss immediate next steps and future work, including a longitudinal study of the system in naturalistic settings, explorations of alternative wearable form factors, and the investigation of human-in-the-loop approaches that enhance the system's proactive capabilities while preserving user agency.

\section{Related Work}

\subsection{Technological Adaptations of Cognitive and Behavioral Interventions}

% Mindfulness, which originates from the Buddhist concept of ``sati'', encompasses attention, awareness, and being present. It is like a mirror that clearly reflects what comes before it \cite{kabat2015mindfulness}. 

% Mindfulness has been integrated into psychology research \cite{langer1989mindfulness} and therapeutic practices like Mindfulness-Based Stress Reduction (MBSR) \cite{kabat1982outpatient}. Farb et al. describe how mindfulness practices can intervene in automatic cognitive-emotional processing \cite{Farb2014mindfulness}. Instead of implementing cognitive reappraisal and reflection after an event or response has occurred, mindfulness can be situated on \textit{attention deployment} \cite{gross1998emerging}, which allows time for generating new perceptions of the situation.

Cognitive-behavioral therapy (CBT) and mindfulness practices have been influential in psychology and therapeutic practices. CBT focuses on identifying and reframing maladaptive thought patterns, while mindfulness emphasizes present-moment awareness and nonjudgmental attention – often intersecting in interventions such as Mindfulness-Based Cognitive Therapy (MBCT) \cite{teasdale2000prevention}. Research has explored how these practices intervene in automatic cognitive-emotional processes, enabling more adaptive responses to challenging situations \cite{farb2014mindfulness}.


In Human-Computer Interaction (HCI), researchers have explored how digital systems can support practices like CBT and mindfulness. For example, CBT has inspired interventions in digital mental health tools, such as apps designed to identify and challenge cognitive distortions or to guide structured problem-solving \cite{bernstein2022human, rathbone2017assessing, kuhn2016cbt}. Mindfulness-oriented systems have often focused on facilitating meditation practice or fostering mindful awareness during everyday interactions \cite{terzimehic2019review}. More recently, many used Large Language Models for technology-mediated reflection, such as reflective conversational agents \cite{li2023exploring} and LLM-assisted journaling \cite{nepal2024contextual, song2024exploreself, kim2024mindfuldiary}. 

Unlike prior work that focuses on training mindfulness as an end goal, we integrate technology more directly and make it ``exercise'' mindfulness via situational awareness. The emergence of proactive AI agents has created new possibilities for dynamically understanding and responding to users’ contexts and create systems that seamlessly integrate cognitive and emotional support into daily life. In the next section, we describe more related work in the areas of context-aware computing and proactive systems.


% Within HCI, many focused on technologies that facilitate meditation practice or cultivate mindfulness in interactions and behavior \cite{terzimehic2019review}. 



\subsection{Proactive Context-Aware Systems}
Context-aware computing, first introduced in the early 1990s, represents a paradigm shift in human-computer interaction by enabling systems to sense, adapt, and respond to their environment based on contextual information such as location, activity, time, and user preferences \cite{Schilit1994, Dey2001}. Since its inception, it has evolved from location-based services into sophisticated systems leveraging multimodal data to create personalized, adaptive experiences. Today, these systems utilize data from multimodal sources such as cameras, microphones, motion sensors, and physiological monitors, enabling real-time context-sensitive support \cite{Gellersen2002, 10.1145/344949.344988}.

Recent advancements in wearables, such as smart glasses, smartwatches, and head-mounted displays equipped with always-on egocentric cameras and microphones, have significantly enhanced their ability to continuously capture real-time data. These devices now enable detailed understanding of users' environments and activities, transforming them into proactive assistants \cite{10.1145/3699759, 10.1016/j.ins.2012.12.028, Wahl2015WISEglassMC, lee2024gazepointarcontextawaremultimodalvoice}. For instance, PAL \cite{Khan2019PALAW, Khan2021, khan2021palintelligenceaugmentationusing} demonstrates how wearables with egocentric cameras analyze visual contexts such as objects, locations, and surroundings to identify activities like working, relaxing, or commuting. This capability supports tailored habit interventions, personalized health and cognitive assistance, and intelligence augmentation.

Recent advancements in Multimodal Large Language Models (MLLMs) have further expanded the capabilities of context-aware computing by enabling the integration and interpretation of complex multimodal data. These models process visual, conversational, and contextual inputs to deliver deep understanding of environments and activities \cite{radford2021learningtransferablevisualmodels, Jia2021, Alayrac2022}. For example, WorldScribe \cite{Chang_2024} leverages GPT-4v \cite{openai_vision_guide} to provide live visual descriptions of environments tailored to the needs of blind and low-vision users. Similarly, Memoro \cite{10.1145/3613904.3642450} utilizes GPT-3 \cite{brown2020languagemodelsfewshotlearners} to process conversational data and infer environmental and social contexts through situationally relevant prompts.

The evolution from reactive context-aware systems to proactive agents marks a significant shift in delivering timely and anticipatory support. The concept of "just-in-time adaptive interventions" (JITAIs) exemplifies this shift by focusing on delivering the right type and amount of support at the right moment, adapting to an individual’s dynamic internal and contextual state \cite{nahum2018just, orzikulova2024time2stop}. Salber et al. introduced the context toolkit and widgets \cite{salber1999context}, which mediate between the environment and applications, enabling dynamic and adaptive interactions inspired by graphical user interfaces.

Building on these foundations, our work applies MLLMs for enhanced contextual understanding of user activities and their surrounding environments from a first-person view, enabling systems to deliver proactive and personalized interventions through AI self-cloned voice agents.

\subsection{AI-generated Synthetic Selves for Behavior Change}
Research on behavior change interventions have primarily focused on their ability to support goal setting, tracking, and sustaining motivation toward goals \cite{lolla2023evaluating}. Despite their widespread appeal, they often suffer from high dropout rates and low user engagement \cite{lazar2015we}. To address these issues, \citet{dominick2020goals} suggested embedding goals into one’s sense of identity, (e.g., ``I will think of myself as someone who eats healthy and works out'') leads to more goal-aligned behavior than simply setting the goal (e.g., ``I will be mindful about what I eat''). 

%BJ Fogg behavior change is identity change

Digital representations of oneself have been shown to influence behavior. The Proteus effect \cite{yee2007proteus} describes how individual's behaviors adapt to align with the characteristics of their virtual avatars. Advancements in machine learning and generative AI have enabled the creation of synthetic self-similar media. For example, researchers have examined applications to enhance engagement in physical activity \cite{clarke2023fakeforward} or to develop public speaking skills by generating personalized videos of individuals confidently delivering speeches \cite{clarke2023fakeforward,leong2021investigating}. 

More related to our work are ones on synthetic voices. \citet{costa2018regulating} demonstrated that hearing a modified version of one’s voice, such as a calmer tone, helped reduce anxiety during relationship conflicts, while a deeper voice fostered a sense of empowerment during debates. Researchers have also explored leveraging one’s own voice \cite{kim2024myvoice} or the voices of familiar people (e.g., family and friends) to enhance the effectiveness of notifications \cite{chan2021kinvoices}.  Fang et al. developed Emotional Self-Voice (ESV), which generates cognitive behavioral therapy strategies in first-person delivered in the user's own voice. They show that the ESV intervention led to an increase in positive sentiment when reflecting on past negative events and significant improvements in confidence, motivation, and resilience to failure. Unlike ESV, Mirai allows back-and-forth real-time conversation with one's ideal self, with responses adapted based on new information from the user and the environment.



\section{System Implementation }

In this work, we name our system ``Mirai\footnote{Mirai combines words mirror and ai, carrying the metaphor of the AI-generated self acting as a reflection. Mirai also means future in Japanese, hinting at the forward-looking nature of nudging the user towards a better version of the self.}'', which aims to nudge individuals' behavior toward their intentions and goals using a proactive AI agent that delivers ESV. 
We have the following design goals:
\begin{itemize}
    \item Contextually aware: The system analyzes the user's environment and dynamically adapts its responses to situational changes.
    \item Anticipatory: The system predicts user intent, looks out for critical decision points, and evaluates the alignment between the user’s intent and goals.
    \item Dialogic: The system maintains contextual conversations, retains interaction history, and determines appropriate follow-up timing.
    \item Judiciously unobtrusive: The system intervenes strategically while preserving user flow and focus.
    \item Introspective: The system generates first-person responses to leverage the psychological impact of hearing one’s own voice.
\end{itemize}

% We use the term ``Emotional Self Voice'' (ESV) from \cite{fang2024leveraging}, which refers to the generated response of an ideal version of one self delivered in one's voice. 

Our system integrates a wearable camera to capture visual inputs and Bluetooth headphones to record speech data. This multimodal approach allows the system to contextualize user interactions and respond proactively. The system consists of three main components: (1) User Modeler, which generates a self-clone based on the user’s goals and characteristics; (2) Context-Aware Agent, which processes scene descriptions and classifies contextual information; and (3) Proactive Speech-to-Speech Agent, which generates and delivers contextually relevant feedback. These components communicate via web sockets, enabling bi-directional, real-time data exchange. Figure \ref{fig:system} provides an overview of the system architecture, illustrating its components and data flow. 
% We have also made our codebase open-source \cite{GitHub}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=.7\linewidth]{figures/sysarch.png}
    \caption{Mirai System Architecture.}
    \label{fig:system}
    \vspace{-10px}
\end{figure}

\subsection{User Modeler}
The primary goal of this component is to generate the user's self-clone, a personalized representation of their ideal self. The process begins with a setup phase, where we gather information about the user's goals and characteristics of their ideal self to form a model of the user. Next, we used ElevenLabs \cite{ElevenLabsVoiceCloning} to generate a clone of the user's voice.  This model forms the basis for interaction within the Proactive Speech-to-Speech Agent component. 

\subsection{Context-Aware Agent}
The primary goal of the component is to generate a scene description from first-person view and provide a contextual classification of the scene's relevance to the user’s goals. To achieve this, we used a high-resolution camera with a diagonal field of view (\textit{dFoV}) of 78° and a resolution of 1080p to capture a wide-angle view of the environment. The camera operates at a frame rate of 5 frames per second (fps), capturing adequate temporal resolution for egocentric video processing \cite{6909721, 10.1016/j.cviu.2021.103252} and optimizing computational efficiency. Given that most human activities are composed of observable actions typically occurring over 2–3 seconds \cite{Banos2014, Koppula2013, Feldhutter1990}, the system analyzes frames in batches of 10. This approach ensures that key actions are sufficiently captured while avoiding excessive data redundancy, unnecessary computational overhead and maintains real-time responsiveness.

Frames are pre-processed by excluding those with low sharpness (Laplacian variance < 25) and adjacent frames with a high structural similarity index score (\textit{SSIM} $\geq$ 0.95) \cite{nilsson2020understandingssim}. The pre-processed frames are then passed to the GPT-4o model, which generates detailed descriptions of the scene using situationally relevant prompts (see Appendix
\ref{sec:appendix-scene-desc} 
for prompts). These descriptions are subsequently classified by a secondary GPT module using relevant prompts (see Appendix 
 \ref{sec:appendix-classifier}
for classifier prompts), informed by the user's conversation history with the self-clone and their predefined goals. Based on the classification results, the updated context and scene description are injected into the proactive speech-to-speech agent, enabling contextually relevant guidance.

To avoid repeated interruptions caused by frequent context updates and uncertainty in context switches, we incorporated a debouncer mechanism. This mechanism ensures that responses are triggered only when there is a significant context change or at controlled intervals during stable states. Specifically, a response is triggered if the classifier detects a state change (\( S_t \neq S_{t-1} \)) or if the system satisfies a timing threshold (\( R_t \mod 3 = 0 \)). A detailed mathematical representation of the mechanism and its parameters is provided in Appendix~\ref{sec:debouncer}.


% To avoid repeated interruptions due to frequent context updates and uncertainty in context switches, we incorporated a debouncer mechanism as follows:  
% \[
% U_t =
% \begin{cases} 
% 1 & \text{if } S_t = "YES"  \text{ and } S_ \neq S_{t-1}, \\
% 1 & \text{if } S_t= "YES" \text{ and } R_t \mod 3 = 0, \\
% 0 & \text{otherwise.}
% \end{cases}
% \]

% Here, \( U_t \) determines whether a response is triggered at time \( t \). The classifier's state is represented by \( S_t \), with \( S_{t-1} \) denoting the previous state, and \( R_t \) representing the time step. A response is triggered only if:
% \begin{itemize}
%     \item The current state \( S_t \) is \texttt{"YES"} and differs from the previous state (\( S_t \neq S_{t-1} \)), indicating a significant context change.
%     \item The current state \( S_t \) is \texttt{"YES"} and the time step \( R_t \) satisfies \( R_t \mod 3 = 0 \), which prevents repeated interruptions during stable states.
% \end{itemize}
% Here, \( U_t \) determines whether a response is triggered at time \( t \). The classifier's state is represented by \( S_t \), with \( S_{t-1} \) denoting the previous state, and \( R_t \) representing the time step. A response is triggered only under two conditions: 1) The current state \( S_t \) is \texttt{"YES"} and differs from the previous state (\( S_t \neq S_{t-1} \)), indicating a significant context change. 2) The current state \( S_t \) is \texttt{"YES"} and the time step \( R_t \) satisfies \( R_t \mod 3 = 0 \), which prevents repeated interruptions during stable states. This mechanism ensures that responses are contextually relevant, avoids excessive interruptions, and maintains responsiveness to meaningful changes.
\subsection{Proactive Speech-to-Speech Agent}
The proactive speech-to-speech agent dynamically generates responses based on the relevance of contextual changes or real-time speech from the user. These responses are generated using GPT-4o \cite{openai_gpt4o_2024} with the goal of embodying the ideal version of the user, where the goal has been achieved and has become part of their identity, as modeled in the user modeler component. Specifically, the prompts include the following subcomponents: the user’s goal, the characteristics of someone who embodies the goal as their identity, the task, and the speaking style. The responses are delivered using the user’s self-cloned voice generated by the user modeler. These responses are crafted in the first person, concise yet emotionally expressive, to resonate with the user’s aspirations. Notably, the GPT module reframes the situation rather than actively assisting the user, encouraging reflection and alignment with their goals. The full prompt design is provided in Appendix 
\ref{sec:appendix-text-prompt}. 
To generate the “cloned ideal self,” we adopted a similar approach to \cite{fang2024leveraging}.

The agent continuously receives input from both the user and the environment via injections from the context-aware agent. Upon detecting new information, the proactive speech-to-speech agent evaluates whether the update reflects a significant change in user behavior or context. It also determines the appropriate timing for a response—deciding whether to respond immediately or delay the response. If a response is warranted, it incorporates the updated understanding into the reply. Otherwise, the agent remains silent to minimize unnecessary interruptions. This ensures the system is both proactive and non-intrusive, delivering contextually relevant responses at the right moments.

\subsection{Summary of System Performance}
Latency is a crucial factor in our system, as it directly impacts user experience. To evaluate our system, we measured the average latency over 100 interactions. Our analysis excluded external factors such as network speed or variability to ensure the reported values reflect the system's inherent performance. The results indicate that the system's latency falls within the range required to avoid disrupting the user’s flow of thought, remaining under 1.0 seconds end-to-end \cite{10.5555/2821575}. The detailed breakdown of component-wise latency, rounded to the nearest whole number is shown in Appendix \ref{tab:latency}, including the average times for Speech-to-Text (STT), MLLMs, and Text-to-Speech (TTS).
% (Table~\ref{tab:latency})



\section{Example Scenarios}
We illustrate how Mirai is used through the following scenarios where individuals face challenges with sticking to a healthy goal, maintaining focus during work, and speaking confidently in difficult conversations. The scenario demonstrations can also be found in the Video Figure.

\subsubsection*{Scenario 1: Sticking with a Healthy Diet} 
\hfill\\
\textit{Scene Setup:}
The user in this scenario struggles with making healthy choices and often prioritizes short-term gratification over long-term health. They aspire to maintain a nutritious diet and live a balanced life but find themselves making decisions that deviate from these goals. 
% To help navigate these moments of temptation, the user get assistance from Mirai, their ideal self-cloned voice, which reflects the person they strive to become. 
Mirai provides timely, personalized nudges to help the user stay aligned with their health-conscious aspirations.

\textit{In the Moment:}
I approach a snack counter during a short break, scanning the options. My eyes land on a shiny soda, and my hand instinctively reaches for it. In that moment, my ideal self-cloned voice, Mirai, recognizes this as a choice that strays from my health-conscious goals and intervenes. \textit{“My body deserves better than this,”} I hear myself say, the voice calm yet firm. Startled, I pause, pull my hand back, and shift my focus to the other options. I notice a chocolate bar and crisp green apples. Mirai notices I reach for the chocolate bar instead of the apples and says: \textit{“No way. I'll stick with the apples, real energy, no crash.”} (Figure \ref{fig:health} Left). The reminder resonates. I pick up the apple with a small smile. \textit{“Great choice!”} Mirai sees the apple in my hand and affirms (Figure \ref{fig:health} Right) . As I walk away, biting into the apple, I feel proud and aligned with the healthier version of myself.

\begin{figure}[t!]
    \centering
    \begin{minipage}[t]{.48\textwidth}
        \centering
       \includegraphics[width=1\linewidth]{figures/scenario1.png}
    \caption{Mirai predicts the user's intent based on their interaction with food options (Left: chocolate bar, Right: apples) and nudge the user to stick with their goal of healthy eating.}
    \label{fig:health}
    \end{minipage}\hfill
    \begin{minipage}[t]{.48\textwidth}
        \centering
         \includegraphics[width=1\linewidth]{figures/scenario2.png}
    \caption{Mirai pays attention to the user's interaction with their devices (Left: phone, Right: Laptop) and helps the users to find a balance between focus and distraction.}
    \label{fig:attention}
    \end{minipage}
    \vspace{-15px}
\end{figure}
\subsubsection*{Scenario 2: Striking a Balance Between Focus and Distraction at Work}
\hfill\\
\textit{Scene Setup:} The user strives to maintain a focused and balanced working style but often struggles with distractions that disrupt their productivity. They aim to stay disciplined, avoid interruptions, and take meaningful breaks to recharge. 
% In moments of distraction or fatigue, Mirai helps, their ideal self-cloned voice, for guidance. 
Mirai helps them stay focused on their goals and reminds them when it’s time to take a well-deserved break.

\textit{In the Moment:}
I sit at my desk, ready to dive into my tasks. Mirai’s voice encourages me: \textit{“Let’s get started. I’ve got this!”} Motivated, I focus and make progress. Minutes later, my phone buzzes, and a YouTube video distracts me. Mirai steps in: \textit{“Alright. I need to put the phone down and focus on my code”} (Figure \ref{fig:attention} Left).  I pause the video, put my phone down, and refocus. After working for a while, Mirai notices the time on the clock, indicating that I have been working for two hours without a break. Gently, it reminds me: \textit{“Time for a quick stretch and a glass of water to stay sharp”} (Figure \ref{fig:attention} Right). I stretch, breathe, and reset. Feeling refreshed, I return to work, confident in my ability to stay productive. 



\begin{figure} [t!]
    \centering
    \includegraphics[width=.45\linewidth]{figures/scenario3.png}
    \caption{Mirai reminds the user to embody confidence during a challenging conversation about asking for a raise.}
    \label{fig:confidence}
    \vspace{-15px}
\end{figure}


\subsubsection*{Scenario 3: Reminder to Be Confident in a Difficult Interpersonal Conversation}
\hfill\\
\textit{Scene Setup:}
The user in this scenario aspires to express themselves with confidence and clarity, especially in challenging conversations. However, they often feel nervous or hesitate to speak up. To support these moments, the user gets help from Mirai, their ideal self-cloned voice, to provide encouragement and reinforce their belief in their own worth. Mirai helps the user navigate these high-pressure situations with poise and assertiveness.

\textit{In the Moment:}
I sit nervously at my desk, staring at the Zoom waiting room message. A meeting with my boss about a promotion looms, and doubt clouds my thoughts. Mirai has known about how stressed I would be during this meeting calmly reassures me: \textit{“Confidence comes from knowing my worth—and I’ll remind myself of that.”} Summoning courage, I say: \textit{“Thank you for taking the time. I wanted to discuss my performance and the possibility of a promotion.”} My boss responds curtly: \textit{“I don’t think you’re ready. You haven’t done enough to justify it.”} The words shake me. "Yeah, you are right. Uh, I should be more specific about my contributions" (Figure \ref{fig:confidence}). Mirai hears my hesitance and steps in: \textit{“For instance, I manage the project deadlines”}. I take a deep breath and reply with determination: \textit{“I’ve managed major projects deadlines while the team is on vacation, and I mentored new team members. I believe I’m ready for this role.”} Surprised, my boss nods: \textit{“You’ve made a strong case. Let’s discuss next steps.”} As the meeting ends, Mirai’s voice affirms: \textit{“Well done. I believed in myself, and it made all the difference.”} 

\section{Discussion, Limitations \& Future Work}

While we achieved promising experiences with minimum latency, the fundamental challenge lies in robustly detecting user intentions and determining appropriate intervention timing. The proactive nature of our system necessitates operating beyond deterministic rules, as user inputs and contexts introduce significant variability.

Our system shares similar challenges and limitations around privacy with other similar systems, particularly regarding the always-on camera and microphone components, where having an indicator light on the hardware setup can help increase transparency \cite{zulfikar2024memoro, zulfikar2024memic, chwalek2023airspec}.

Our current prototype utilizes standard webcam and Bluetooth headphone configurations, enabling rapid iteration of interaction workflows without dependency on specialized hardware. We envision our system can be deployed via smart glasses and novel form factors, such as the AiSee device \cite{boldu2020aisee} with a worn camera on a bone-conduction headset. Having a wearable form factor allows the device to be worn outside laboratory settings and to be studied longitudinally and in naturalistic settings. This allows us to observe how individuals adaptive their goals over time, which would require the system to adapt its understanding of the user and interventions. This also means the system needs to have a memory of past interventions. Additionally, these systems would incorporate a human-in-the-loop approach, adapting based on user feedback, observed reactions, and behavior changes, thereby enhancing their understanding and proactiveness over time.

Future research should focus on three key areas to validate and improve this approach. First, we need to conduct a long-term study that measures actual behavioral changes resulting from this intervention. This will help establish whether the technique leads to meaningful, sustained improvements in user behavior. Second, we should examine how regular exposure to an idealized response in one's own voice affects users psychologically. This investigation should focus on two aspects: whether users experience changes in their self-identity over time, and whether hearing their idealized voice helps them internalize positive traits or instead creates internal conflict between their current and ideal selves. Third, we need to understand whether users become less responsive to the intervention over time. Specifically, we should investigate if repeated exposure to their own voice diminishes its effectiveness as users become accustomed to it. 




%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

% \begin{acks}
% Georgina for the webapp
% \end{acks}


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\input{Appendix.tex}



\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
