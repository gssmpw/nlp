\section{Related Work}
\subsection{Technological Adaptations of Cognitive and Behavioral Interventions}

% Mindfulness, which originates from the Buddhist concept of ``sati'', encompasses attention, awareness, and being present. It is like a mirror that clearly reflects what comes before it \cite{kabat2015mindfulness}. 

% Mindfulness has been integrated into psychology research \cite{langer1989mindfulness} and therapeutic practices like Mindfulness-Based Stress Reduction (MBSR) \cite{kabat1982outpatient}. Farb et al. describe how mindfulness practices can intervene in automatic cognitive-emotional processing \cite{Farb2014mindfulness}. Instead of implementing cognitive reappraisal and reflection after an event or response has occurred, mindfulness can be situated on \textit{attention deployment} \cite{gross1998emerging}, which allows time for generating new perceptions of the situation.

Cognitive-behavioral therapy (CBT) and mindfulness practices have been influential in psychology and therapeutic practices. CBT focuses on identifying and reframing maladaptive thought patterns, while mindfulness emphasizes present-moment awareness and nonjudgmental attention – often intersecting in interventions such as Mindfulness-Based Cognitive Therapy (MBCT) \cite{teasdale2000prevention}. Research has explored how these practices intervene in automatic cognitive-emotional processes, enabling more adaptive responses to challenging situations \cite{farb2014mindfulness}.


In Human-Computer Interaction (HCI), researchers have explored how digital systems can support practices like CBT and mindfulness. For example, CBT has inspired interventions in digital mental health tools, such as apps designed to identify and challenge cognitive distortions or to guide structured problem-solving \cite{bernstein2022human, rathbone2017assessing, kuhn2016cbt}. Mindfulness-oriented systems have often focused on facilitating meditation practice or fostering mindful awareness during everyday interactions \cite{terzimehic2019review}. More recently, many used Large Language Models for technology-mediated reflection, such as reflective conversational agents \cite{li2023exploring} and LLM-assisted journaling \cite{nepal2024contextual, song2024exploreself, kim2024mindfuldiary}. 

Unlike prior work that focuses on training mindfulness as an end goal, we integrate technology more directly and make it ``exercise'' mindfulness via situational awareness. The emergence of proactive AI agents has created new possibilities for dynamically understanding and responding to users’ contexts and create systems that seamlessly integrate cognitive and emotional support into daily life. In the next section, we describe more related work in the areas of context-aware computing and proactive systems.


% Within HCI, many focused on technologies that facilitate meditation practice or cultivate mindfulness in interactions and behavior \cite{terzimehic2019review}. 



\subsection{Proactive Context-Aware Systems}
Context-aware computing, first introduced in the early 1990s, represents a paradigm shift in human-computer interaction by enabling systems to sense, adapt, and respond to their environment based on contextual information such as location, activity, time, and user preferences \cite{Schilit1994, Dey2001}. Since its inception, it has evolved from location-based services into sophisticated systems leveraging multimodal data to create personalized, adaptive experiences. Today, these systems utilize data from multimodal sources such as cameras, microphones, motion sensors, and physiological monitors, enabling real-time context-sensitive support \cite{Gellersen2002, 10.1145/344949.344988}.

Recent advancements in wearables, such as smart glasses, smartwatches, and head-mounted displays equipped with always-on egocentric cameras and microphones, have significantly enhanced their ability to continuously capture real-time data. These devices now enable detailed understanding of users' environments and activities, transforming them into proactive assistants \cite{10.1145/3699759, 10.1016/j.ins.2012.12.028, Wahl2015WISEglassMC, lee2024gazepointarcontextawaremultimodalvoice}. For instance, PAL \cite{Khan2019PALAW, Khan2021, khan2021palintelligenceaugmentationusing} demonstrates how wearables with egocentric cameras analyze visual contexts such as objects, locations, and surroundings to identify activities like working, relaxing, or commuting. This capability supports tailored habit interventions, personalized health and cognitive assistance, and intelligence augmentation.

Recent advancements in Multimodal Large Language Models (MLLMs) have further expanded the capabilities of context-aware computing by enabling the integration and interpretation of complex multimodal data. These models process visual, conversational, and contextual inputs to deliver deep understanding of environments and activities \cite{radford2021learningtransferablevisualmodels, Jia2021, Alayrac2022}. For example, WorldScribe \cite{Chang_2024} leverages GPT-4v \cite{openai_vision_guide} to provide live visual descriptions of environments tailored to the needs of blind and low-vision users. Similarly, Memoro \cite{10.1145/3613904.3642450} utilizes GPT-3 \cite{brown2020languagemodelsfewshotlearners} to process conversational data and infer environmental and social contexts through situationally relevant prompts.

The evolution from reactive context-aware systems to proactive agents marks a significant shift in delivering timely and anticipatory support. The concept of "just-in-time adaptive interventions" (JITAIs) exemplifies this shift by focusing on delivering the right type and amount of support at the right moment, adapting to an individual’s dynamic internal and contextual state \cite{nahum2018just, orzikulova2024time2stop}. Salber et al. introduced the context toolkit and widgets \cite{salber1999context}, which mediate between the environment and applications, enabling dynamic and adaptive interactions inspired by graphical user interfaces.

Building on these foundations, our work applies MLLMs for enhanced contextual understanding of user activities and their surrounding environments from a first-person view, enabling systems to deliver proactive and personalized interventions through AI self-cloned voice agents.

\subsection{AI-generated Synthetic Selves for Behavior Change}
Research on behavior change interventions have primarily focused on their ability to support goal setting, tracking, and sustaining motivation toward goals \cite{lolla2023evaluating}. Despite their widespread appeal, they often suffer from high dropout rates and low user engagement \cite{lazar2015we}. To address these issues, \citet{dominick2020goals} suggested embedding goals into one’s sense of identity, (e.g., ``I will think of myself as someone who eats healthy and works out'') leads to more goal-aligned behavior than simply setting the goal (e.g., ``I will be mindful about what I eat''). 

%BJ Fogg behavior change is identity change

Digital representations of oneself have been shown to influence behavior. The Proteus effect \cite{yee2007proteus} describes how individual's behaviors adapt to align with the characteristics of their virtual avatars. Advancements in machine learning and generative AI have enabled the creation of synthetic self-similar media. For example, researchers have examined applications to enhance engagement in physical activity \cite{clarke2023fakeforward} or to develop public speaking skills by generating personalized videos of individuals confidently delivering speeches \cite{clarke2023fakeforward,leong2021investigating}. 

More related to our work are ones on synthetic voices. \citet{costa2018regulating} demonstrated that hearing a modified version of one’s voice, such as a calmer tone, helped reduce anxiety during relationship conflicts, while a deeper voice fostered a sense of empowerment during debates. Researchers have also explored leveraging one’s own voice \cite{kim2024myvoice} or the voices of familiar people (e.g., family and friends) to enhance the effectiveness of notifications \cite{chan2021kinvoices}.  Fang et al. developed Emotional Self-Voice (ESV), which generates cognitive behavioral therapy strategies in first-person delivered in the user's own voice. They show that the ESV intervention led to an increase in positive sentiment when reflecting on past negative events and significant improvements in confidence, motivation, and resilience to failure. Unlike ESV, Mirai allows back-and-forth real-time conversation with one's ideal self, with responses adapted based on new information from the user and the environment.