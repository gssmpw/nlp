\section{Methodology}

\begin{figure*}[ht]
    \centering
    \includegraphics[page=10, width=1\textwidth]{pic/pictures.pdf}
    \caption{The overall process of our method and an example of how guidelines correct LLM's behavior}
    \label{fig:main}
\end{figure*}

\subsection{Overview}

% 图\ref{fig:main}中展示了 the overall process of our method.
Our core insight is to identify informative samples from an unlabeled data pool by leveraging guidelines, as illustrated in Figure \ref{fig:main}.
% 给定 unlabel data pool and the 初始的 guidelines, 我们首先使用 init guidelines 计算每个样本的 GE score, 选择 score 较低的样本进行人工标注,得到 updated guidelines, 可直接用于prompt-based 的方法.
Given an unlabeled data pool and initial guidelines, we first compute the Guideline Effectiveness (GE) score for each sample using the initial guidelines. Samples with lower scores are selected for manual annotation, resulting in updated guidelines that can be directly applied to prompt-based methods.
% 为了充分利用 unlabeled data, 我们将新的guidelines加入 prompt text中, 利用 GPT-4 进行标注, 得到问题-交互轨迹 pair 作为本文的 high-quality SFT data. 整个过程无需 golden answer.
To fully utilize the unlabeled data, we incorporate the new guidelines into the prompt text and employ GPT-4 for annotation, generating question-interaction trajectory pairs as high-quality SFT data. Notably, this entire process does not require golden answers.

\subsection{Preliminary}

% 把基本的符号和大的目标给出来
% 给定问题集合 Q, 我们的目标是选出一个 informative 的子集 Q', with an LLM and a Guideline G.
Given a set of questions $\mathcal{Q}=\{q_1,\dots,q_n\}$, a language model LLM, and an initial guideline $\mathcal{G}^{init}$, we generate interaction trajectories $\mathcal{T}=\text{LLM}(\mathcal{Q}, \mathcal{G}^{init})$, where each trajectory $\mathcal{T}=(q, a_1, o_1, \ldots, a_T,o_T)$ consists of question-action-observation sequences with length $T$. Here $a_i$ represents the action taken by the LLM at step $i$, and $o_i$ denotes the observation or feedback received from the environment after taking action $a_i$.

Our objective is to select an informative subset $\mathcal{Q}'$ such that:

\begin{equation}
    \mathcal{Q}' = \mathop{\arg\max}_{\mathcal{Q}' \subset \mathcal{Q}} (\text{Reward}(\text{LLM'}))
\end{equation}
where Reward is the metrics of the task and LLM' is fine-tuned by labeled $\mathcal{Q}'$.

% % 我们对 Q' 进行人工标注, 总结得到新的 Guideline G' = Annotation(Q')
% We perform manual annotation on $\mathcal{Q}'$ to summarize and obtain an updated guideline $\mathcal{G}' = \text{Annotation}(\mathcal{Q}')$. 
% % 使用新的 Guideline G' 和 LLM 进行标注, 得到轨迹 T=LLM(Q, G'), 其中 T=(q, a_1, o_1, ...). The Q-T pair is the 期望的 high-quality SFT data.
% Using the new guideline $\mathcal{G}'$ and LLM, we generate interaction trajectories $\mathcal{T}=\text{LLM}(\mathcal{Q}, \mathcal{G}')$, where each trajectory $\mathcal{T}=(q, a_1, o_1, \ldots, a_T,o_T)$ consists of question-action-observation sequences with length $T$. Here $a_i$ represents the action taken by the LLM at step $i$, and $o_i$ denotes the observation or feedback received from the environment after taking action $a_i$. The resulting question-trajectory pairs $(\mathcal{Q'},\mathcal{T})$ serve as our desired high-quality SFT data.

\subsection{Guideline Effectiveness}

% 说明指标的意义
% Guideline is 带有专家经验的natural language prompts, 能够比 detailed examplar consume less context space and cover more scenarios, 这一优点使其更加适用于多轮交互 task.
% 因此,本文propose a metric called Guideline Effectiveness to quantify how much the guideline contributes to solving a given question.
Guidelines are natural language prompts enriched with expert knowledge that can cover more scenarios while consuming less context space compared to detailed exemplars. %This advantage makes them particularly well-suited for multi-turn interaction tasks. 
However, since humans may not initially recognize all potential challenging samples, the initial guidelines may be inadequate.
Therefore, we propose a metric called Guideline Effectiveness to quantify the contribution of guidelines in solving a given question in order to identify questions that are challenging for the initial guidelines.

% However, Guidelines ，由于人类并不知道可能会有怎样的challenging samples，最初的guideline可能是不足的。

Given an question $q$, the prompt text is constructed as:
\begin{equation}
    \text{Prompt}=\text{CONCAT}(\text{I}, \text{G}, \text{E}) 
\end{equation}

% 其中, I 表示 instruction. 
% 这里要说明 E 和 G 是不一样的, E 用于表示交互格式.
% action a 必须来自预定义的动作集合, which depend on dataset, 详情可见\ref{sec:tool_def}
where $\text{I}$ represents the instruction text, and $\text{E}$ denotes a set of interaction examplars. 
% The action $a$ must be selected from a predefined action set that is specific to each dataset (see Section \ref{sec:tool_def} for details).
We measure the uncertainty of LLM's output action $a_t$ at step $t$ using the average cross-entropy loss of each token.

\begin{equation}
d_\theta^G(a_t|\text{Prompt}) = -\frac{1}{N}\sum_{i=1}^{N} \log P(w_i | \text{I},\text{G}, \text{E}, \mathcal{T}, w_{<i}:\theta)
\end{equation}

where $N$ is the number of tokens in $a_t$, $w_i$ is the $i$-th token in $a_t$. A lower $d_\theta^G$ indicates easier action generation.

To evaluate guideline effectiveness, we construct $\text{Prompt}^{-G} = \text{CONCAT}(\text{I}, \text{E})$ by excluding $\text{G}$ from the context. The difficulty of generating action without guidelines is:

\begin{equation}
d_\theta^I(a_t|\text{Prompt}^{-G}) = -\frac{1}{N}\sum_{i=1}^{N} \log P(w_i | \text{I},\text{E}, \mathcal{T}, w_{<i}:\theta)
\end{equation}

The score $d_\theta^I$ measures how hard it is to generate $a$ using only the LLM's intrinsic knowledge, without guidelines.

Based on $d_\theta^G$ and $d_\theta^I$, the GE score is defined as:   

\begin{equation}
GE(q) = -\frac{1}{T}\sum_{t=1}^T \log\frac{d_\theta^I(a_t|\text{Prompt}^{-G})}{d_\theta^G(a_t|\text{Prompt})}
\end{equation}

GE quantifies the influence of guideline on generating each $a_t$ by computing the ratio between the difficulty scores with and without guidelines. 
The intuitive interpretation of GE values is as follows:
A lower value of $d$ indicates the generation process is easier. The difficulty scores $d_\theta^{I}$ and $d_\theta^{G}$ represent generation difficulty without and with guidelines respectively.
When \textbf{$GE > 0$}, we have $d_\theta^{I} > d_\theta^{G}$, indicating that guidelines facilitate action generation. A larger positive GE score suggests guidelines have a stronger positive impact on generation. As the score approaches zero, the similar magnitudes of $d_\theta^{I}$ and $d_\theta^{G}$ indicate guidelines provide limited benefit - these are samples of particular interest.
When \textbf{$GE < 0$}, meaning $d_\theta^{I} < d_\theta^{G}$, guidelines appear to impede generation. This reveals cases where the LLM's inherent knowledge leads it to generate actions that conflict with the guidelines, another important category of samples to identify.

% In summary, a higher GE score indicates that the question effectively leverages the guideline for problem-solving. In contrast, questions with lower GE scores often have more complex and challenging solving-path, resulting in the ineffectiveness of guidelines.


\subsection{Efficiently Incorporating Human Expertise}




This section describes how we utilize the GE score to incorporate human expertise into LLMs efficiently, as shown in Figure \ref{fig:main}. Given the data pool $\mathcal{Q}$ and historical interaction trajectory $\mathcal{T}$, and initial guideline $\mathcal{G}^{init}$, we can calculate the GE score for each $q_i$. Lower GE scores indicate challenging questions which require additional learning by the LLM. We accomplish this in two stages: Guideline Update and High-Quality Data Generation.

\textbf{Update the Guideline.} We select m questions with the lowest GE scores. By observing these questions' interaction trajectories, we can summarize their issues and update $\mathcal{G}^{init}$ to $\mathcal{G}^{new}$. Analyzing samples with the lowest GE scores allows $\mathcal{G}^{new}$ to integrate further human expertise necessary for addressing challenging samples, such as deeper insights into the task and tools. The value of m can be small (e.g., 30), which is manageable by humans within a reasonably short time. We denote the ReAct framework augmented with the updated guideline as $\text{EDGE}_\text{UG}$.

\textbf{High-Quality Data Generation.} Similarly, we select k questions with the lowest GE scores and employ GPT-4 to generate interaction trajectories guided by $\mathcal{G}^{new}$. The incorporation of human expertise within $\mathcal{G}^{new}$ ensures that the trajectories  maintain a high standard of quality. Utilizing these high-quality data for fine-tuning, open-sourced LLMs will implicitly learn human expertise from the annotated data. Fine-tuning the open-sourced LLM is often crucial because: 1) As guidelines become more complex, it gets harder for open-sourced LLM to follow; 2) Some tasks may be too intricate to distill into guidelines, making annotating data a simpler option.

