




\section{Related Work}

This study investigates how to effectively utilize guidelines in the context of data selection for supervised fine-tuning (SFT).

\textbf{Data Selection for SFT} aims to select a high-quality subset of data. 
% 先说明: 少量的样本就能有很好的效果.
\cite{Zhou-Chunting-NeurIPS-2023-LIMA} demonstrates that only 1,000 carefully curated prompts and responses can achieve remarkably strong performance.
% 再说明: 已有的数据筛选指标. GPT-4 打分, 指令难度, 语义多样性 等等.
\cite{Chen-Lichang-ICLR-2024-AlpaGasus} proposes using GPT-4 for direct quality scoring, successfully identifying 9k high-quality samples from a dataset of 52k instances.
\cite{Li-Ming-NAACL-2024-IFD} introduces the Instruction-Following Difficulty (IFD) metric to identify discrepancies between a model's expected responses and its intrinsic generation capabilities.
\cite{Liu-Wei-ICLR-2024-DEITA} curates 6K training samples by evaluating them along three dimensions: complexity, quality, and diversity.
\cite{Bhatt-Gantavya-ACL-2024-ExperimentalDesign} conducts a comprehensive evaluation of existing data selection methods that aim to maximize uncertainty and/or diversity measures.
% 不足: 然而, 这些指标都依赖于 golden answer as feedback. 并且, 这些指标都基于单轮交互, 没有关注多轮交互的情况.
However, these evaluation metrics inherently depend on golden answers as feedback. Furthermore, they primarily focus on single-turn interactions, neglecting the complexities of multi-turn interaction scenarios.
% FireAct和AgentTuning
AgentTuning \cite{zeng-etal-2024-agenttuning} and FiReAct \cite{Chen-Baian-2023-FireAct} investigate fine-tuning LLMs with multi-turn interaction trajectories generated by GPT-4, further examining the effects of multi-task learning and prompt design methods, respectively.
However, both methods randomly select samples for annotation, and assume that perfectly correct trajectories (reward = 1) represent high quality. This approach may result in the inclusion of simpler problems in fine-tuning datasets, leading to low quality of fine-tuning data. 


% 主动学习 相关文献
% FL效果差的原因可能是：他主要关注生成式任务，而我更关注推理和决策。our work is the first to see an notation cost savings on generative tasks. To reach the same generalization performance, our methods save 50% of the annotation cost compared to random sampling (unlike random sampling that fails to achieve the same generalization).
\textbf{Deep Active Learning} aims to identify the most informative samples for annotation, thereby reducing labeling costs. The methods are typically categorized into uncertainty-based \cite{Settles-Burr-JMLR-2011-ActiveLearning,Kremer-Jan-WIREs-DMKD-2014-ActiveSVM}, diversity-based \cite{Sener-Ozan-ICLR-2018-CoreSet,bukharin-etal-2024-data-Diversity,Bhatt-Gantavya-ACL-2024-ExperimentalDesign}, or hybrid approaches \cite{azeemi-etal-2025-label}. In the era of large language models (LLMs), some studies have attempted to integrate active learning with LLMs to achieve efficient SFT. \cite{azeemi-etal-2025-label} investigates active learning for improving label efficiency in natural language generation but reports inconsistent results. \cite{Kung-Po-Nien-EMNLP-2024-ActiveInstruction} proposed a task-level active learning framework to explore the most effective SFT tasks. However, it makes the simplifying assumption that all instances are of equal value within a task.\cite{Bhatt-Gantavya-ACL-2024-ExperimentalDesign}is most similar to ours. It is the first to utilize experimental design for SFT and formulates active learning as a facility location problem. This method focuses on selecting semantically diverse and representative samples, effectively improving the generative capabilities of LLMs. However, it does not focus on addressing agent tasks that require more reasoning and decision-making capabilities.




% Guideline 相关文献, 不提 auto-prompt.
\textbf{Guideline-based Prompting} aims to summarize historical interaction experiences from datasets into natural language prompts that can guide future interactions. 
\cite{Zhao-Andrew-AAAI-2024-ExpeL} introduces Experiential Learning, which autonomously gathers experiences from training tasks through trial and error to generate instructive guidelines. 
\cite{Fu-Yao-NeurIPS-2024-AutoGuide} advances this approach by automatically generating context-aware guidelines and implementing a retrieval system that selects guidelines relevant to the agent's current state.
% 缺点: 随机选择样本子集, 没有考虑数据质量; 自动总结, 缺乏专家经验.
However, these approaches rely on random sampling without quality consideration and their automated summarization lacks the depth and nuance of expert knowledge.
