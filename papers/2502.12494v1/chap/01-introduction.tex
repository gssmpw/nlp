\section{Introduction}


% 近年来, LLM \cite{Ouyang-Long-NeurIPS-2022-InstructGPT,OpenAI-2023-GPT4} 已经展现出卓越的 few-shot 和 reasoning 能力, 越来越多的工作开始探索如何以 agent 的方式利用 LLM, 在和环境多次交互中完成各种任务. 例如 WebShop \cite{Yao-Shunyu-NeurIPS-2022-WebShop}, which is a 模拟的购物环境, 要求agent根据用户需求选择最匹配的商品.
In recent years, Large Language Models (LLMs) \cite{Ouyang-Long-NeurIPS-2022-InstructGPT,OpenAI-2023-GPT4} have demonstrated remarkable few-shot learning and reasoning capabilities. An increasing number of studies have begun exploring how to leverage LLMs as agents that can accomplish various tasks through multiple interactions with the environment \cite{deng-2023-mindweb,liu-2024-ICLR-agentbench,Wang-2024-agentsurvey}. For example, WebShop \cite{Yao-Shunyu-NeurIPS-2022-WebShop} provides a simulated shopping environment where agents must select products that best match user requirements.

% 在交互的过程中, LLM会遇到各种复杂或者未见的情况, 这对LLM的泛化能力提出了很高的要求, 很多工作致力于缓解这一问题。
During interactions, LLMs frequently encounter complex or previously unseen scenarios, which places substantial demands on their generalization capabilities. Numerous studies have been dedicated to mitigating this challenge.

% 
% A significant focus of recent research has been on finding ways to leverage LLM capabilities while minimizing the associated computational and resource costs.

% 在基于提示的多轮交互方法中, 已有的工作发现, Guidelines (or Insight) 很重要
% Guidelines 指的是从数据中总结的自然语言形式的提示, 相比于 examplars, Guidelines信息含量更大, 能cover更多的情况, 通常仅需消耗更少的上下文
% 现有的方法 autonomously gathers experiences from a collection of training tasks through trial and error and generates guidelines
Prior work has demonstrated the importance of guidelines (or insights) in prompt-based multi-turn interaction methods. 
Guidelines are natural language prompts summarized from data that contain more information and cover more scenarios than exemplars, while typically consuming less context space. 
Existing approaches autonomously gather experiences from training tasks through trial and error to generate these guidelines \cite{Zhao-Andrew-AAAI-2024-ExpeL}.

% 另一系列方法考虑 SFT open-source LLM, 以增强LLM的指令遵循能力。已有工作发现, SFT的效果高度依赖于数据集的质量, 而非数量。目前, 数据筛选方法包括基于GPT-4打分、指令难度、语义多样性等, 都具有一定的效果。
Another line of research focuses on Supervised Fine-Tuning (SFT) of open-source LLMs to enhance their instruction-following capabilities. Prior work has shown that the effectiveness of SFT depends more on dataset quality than quantity \cite{wang-etal-2023-self-instruct,Zhou-Chunting-NeurIPS-2023-LIMA}. Current data filtering approaches, including GPT-4-based scoring \cite{Chen-Lichang-ICLR-2024-AlpaGasus}, instruction difficulty assessment \cite{Li-Ming-NAACL-2024-IFD}, and semantic diversity metrics \cite{lu-keming-ICLR-2024instag}, have demonstrated varying degrees of success.


% 在基于提示的方法中, 现有的获取 guidelines 的工作没有关注数据的质量,而是从有标注的数据中随机选择样本, 这不但要求大量昂贵的标注成本, 而且suffer from the dirty data.
%% 但当selected samples 十分简单时，其中就不会包含具有见解的解决问题的方式，导致难以提取出有效的guideline。However, when the selected samples are overly simplistic, they fail to encompass insightful problem-solving strategies, thereby hindering the extraction of effective guidelines.
% 在SFT数据选择方法中, 现有的方法依赖于 golden answer feedback, 并且都集中在instruction tuning, which is 单轮交互, 对多轮交互的情况缺乏探索.
Despite these advancements, current LLM-agent approaches still face several pressing challenges. In prompt-based methods, existing approaches for obtaining guidelines \textbf{do not consider data quality control}, instead randomly selecting samples from annotated data, which not only requires substantial and costly annotation efforts but also suffers from noisy data problems. Meanwhile, in SFT-based methods, current approaches heavily \textbf{rely on golden answer feedback} and primarily focus on single-turn instruction tuning, lacking necessary exploration of more complex multi-turn interaction scenarios that are essential for real-world applications.

To address these challenges, we propose \underline{E}fficient \underline{D}ata selection for LLM agents via \underline{G}uideline \underline{E}ffectiveness, a novel framework centered around a new metric called Guideline Effectiveness (GE) to select the most informative subset of samples from a vast unlabeled data (query) pool. These selected samples can be utilized for both prompt engineering and SFT. 

% Guideline is the 人类对tasks的理解, 是agent的先验知识, including 对工具的使用, 对复杂情况的理解等. The GE score 的本质是quantify the guidelines 对 each data sample 的影响, enabling us to 找出哪些数据对模型是confuse的, 从而选择出更具有信息量的样本.
Guidelines represent human understanding of tasks and serve as prior knowledge for agents, encompassing tool usage patterns and comprehension of complex scenarios \cite{Zhao-Andrew-AAAI-2024-ExpeL,Fu-Yao-NeurIPS-2024-AutoGuide}. The GE score essentially quantifies the impact of guidelines on each data sample, enabling us to identify which samples are most challenging for the model and thus select more informative ones.
% % 从初始的 guideline开始, 我们使用基于主动学习的方法, 选择GE score最小的样本进行标注. without the golden answer, 我们人工标注了其交互轨迹以及对应的 guideline. It is worth noting that 由于多轮交互的task情况复杂,自动标注存在xx的缺点. 因此,专家知识是必须的.      ===原始的===
% Beginning with an initial guideline, we employ an active learning approach to select samples with the lowest GE scores for annotation. Without relying on golden answers, we manually annotate the interaction trajectories and corresponding guidelines for these samples. It is worth noting that due to the complexity of multi-turn interaction tasks, automatic annotation methods have significant limitations, making expert knowledge indispensable.
% 新改的
Beginning with an initial guideline, we employ an active learning approach to select a small number of samples with the lowest GE scores. These samples are then analyzed to summarize error causes and update the guideline. Next, we use the updated guideline and advanced API-based LLM to annotate more low-GE-score samples instead of relying on human annotators. Notably, the updated guideline incorporates solutions for challenging samples and deeper insights into the task and tools, ensuring that the annotated data is of high quality. Finally, we can use these informative and high-quality annotated samples to fine-tune open-source LLMs. 


The main contributions of this work are summarized as follows:
\begin{itemize}
    % 提出 Guideline Effectiveness 指标, 利用guideline找出具有信息量的样本, without the need of golden answer.
    \item Propose a novel Guideline Effectiveness metric to identify informative samples using guidelines without requiring golden answers. This metric enables efficient sample selection for both prompt engineering and model fine-tuning.

    % 对多轮交互的 hard task, 人工标注了交互轨迹, 并生成guideline, 既能用于基于guideline的prompt方法, 也能用于SFT open-source LLM.
    % \item Annotate high-quality samples for challenging multi-turn interaction tasks, including detailed interaction trajectories and corresponding guidelines. These annotations enable both guideline-based prompting methods and supervised fine-tuning of open-source LLMs.
    
    % \item By leveraging the GE score, we can derive effective guidelines and obtain high-quality data for challenging multi-turn interaction tasks without the need for manual annotation.
    \item Derive effective guidelines and obtain high-quality data for challenging multi-turn interaction tasks without the need for manual annotation, by leveraging the GE score.
    
    % 在HotpotQA和Webshop的实验结果表明, 仅需xx条数据, 就取得了sota的性能.
    \item Demonstrate the effectiveness of our approach through extensive experiments on HotpotQA and WebShop benchmarks, achieving state-of-the-art performance with only 75\% and 50\% data requirements compared to existing methods.
\end{itemize}
