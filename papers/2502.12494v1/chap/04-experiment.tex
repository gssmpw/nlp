



\section{Experiments}


\subsection{Baselines}
To evaluate our approach, we have selected a range of state-of-the-art (SOTA) methods as baselines and conducted model comparisons along two dimensions: 
% 我们从两个维度 利用GE-select data

% Human-craft guideline v.s 
\textbf{$\text{EDGE}$ vs. Other Agent Methods}. We compare our method against several state-of-the-art agent methods: \textbf{ReAct} \cite{Yao-Shunyu-ICLR-2023-ReAct} integrates reasoning and acting capabilities for sequential decision-making tasks; \textbf{Reflexion} \cite{Shinn-Noah-NeurIPS-2023-Reflexion} reinforces language agents through linguistic feedback; \textbf{AMOR} \cite{Guan-Jian-NeurIPS-2024-AMOR} constructs reasoning logic over finite state machines for automated problem-solving across modules; \textbf{ExpeL} \cite{Zhao-Andrew-AAAI-2024-ExpeL} leverages GPT-4 to extract guidelines from failed trajectories.

\textbf{$\text{GE}$ vs. Other Data Selection Strategies.} For comparison with other label-efficient data selection strategies, we evaluate GE against several baseline approaches: \textbf{Random} selects data randomly for annotation; \textbf{Mean Entropy} \cite{Settles-Burr-JMLR-2011-ActiveLearning,Kremer-Jan-WIREs-DMKD-2014-ActiveSVM} measures uncertainty through token-wise negative entropy of softmax probabilities; \textbf{FL} \cite{Bhatt-Gantavya-ACL-2024-ExperimentalDesign} selects semantically representative samples based on diversity; and \textbf{High Score} \cite{Chen-Baian-2023-FireAct,zeng-etal-2024-agenttuning} retains only fully correct interaction trajectories from annotated data.

\subsection{Experiment Setup}


% To evaluates the complex decision-making abilities in practical applications, we consider the following two text-based benchmarks:
\textbf{Datasets.} \textbf{WebShop} \cite{Yao-Shunyu-NeurIPS-2022-WebShop} is a simulated online shopping environment composed of a website with 1.18M real-world products. The agent's goal is to purchase a product that meets specific requirements based on a text instruction. This task requires the agent to query the website’s search engine, select products with required features, and click the necessary options. following \cite{liu-2024-ICLR-agentbench}, the system implements two valid actions: search[query] and click[button]. \textbf{HotpotQA} \cite{yang-etal-2018-hotpotqa} is a multi-hop question-answering benchmark that challenges an agent to retrieve Wikipedia passages to perform reasoning and question-answering. This involves utilizing API calls and LLM's knowledge to search for and retrieve information in order to find answers. Following \cite{Yao-Shunyu-ICLR-2023-ReAct}, we use three types of actions to support interactive information retrieval in HotpotQA: search[entity], lookup[query] and finish[answer].

For Webshop, we use 8,500 instructions as the data pool and another 500 instructions for evaluation. For HotpotQA, we use the first 10,000 training questions as the data pool and randomly select 500 dev questions. For each dataset, we selected 30 samples with the lowest GE score for guideline updating, and then annotated 800 samples for fine-tuning. The statistical details of the test datasets are presented in Table \ref{tab:dataset-Statistics}.

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    \textbf{Dataset} & \textbf{\#Data Pool} & \textbf{\makecell{\#EDGE Used \\  (m / k)}} & \textbf{\makecell{\#Raw \\ (Train / Dev)}}  \\
    \midrule
    WebShop & 8,500 & 30 / 800 & 12,087 / - \\
    HotpotQA & 10,000 & 30 / 800 & 90,564 / 7405 \\
    \bottomrule
  \end{tabular}
  \caption{Statistics of the datasets.}
  \label{tab:dataset-Statistics}
\end{table}


% 主表
\begin{figure*}[ht]
    \centering
    \begin{minipage}{0.56\textwidth}
    % \renewcommand{\figurename}{Table} % 临时将 Figure 改为 Table
    \captionsetup{type=table} % 将此部分的编号调整为 table
        \centering
        \begin{tabular}{@{}lccccc@{}} % 所有列居中对齐
            \toprule
            \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Base Model}} & \multicolumn{2}{c}{\textbf{HotpotQA}} & \multicolumn{2}{c}{\textbf{WebShop}} \\
            \cmidrule(r){3-4} \cmidrule(l){5-6}
             &    & \textbf{EM} & \textbf{F1} & \textbf{Reward}  & \textbf{SR}\\ 
            \midrule
            \multicolumn{6}{c}{\textbf{API-based LLM}} \\ 
            \midrule
            ReAct  & GPT-4o     & 42.0& 55.17     & 58.63     & 33.2     \\
            ExpeL  & GPT-4o     & 47.8& 60.92     & \underline{64.16}     & \underline{42.0}     \\
            Reflexion& GPT-4o     & 49.2& 61.30     & 63.28     & 39.8     \\
            % LUMOS  & GPT-4, L-13B & 36.3& 57.40     & 50.3     & -\\
            AMOR\textsuperscript{\dag}   & GPT-4-turbo& \underline{55.2}& \underline{65.20}     & -& -\\
            \midrule
            $\text{EDGE}_\text{UG}$ (Ours) & GPT-4o     & \textbf{63.7}     & \textbf{72.88}    & \textbf{73.11}   & \textbf{47.8}    \\
            \midrule
            \multicolumn{6}{c}{\textbf{Fine-tuned Open-source LLM}}\\ 
            
            \midrule
            
            % \bottomrule
            
            ReAct       & M-7B & 22.6     & 38.31     & 30.77    & 14.2     \\
            \quad w/ Random   & M-7B & 34.4& 46.11     & 59.05    & 39.0     \\
            \quad w/ Mean Entropy& M-7B & 35.8& 47.00     & 58.79     & 38.8     \\
            \quad w/ High Score  & M-7B & \underline{37.2}& \underline{49.19}     & \underline{59.32}    & \underline{39.2}     \\
            \quad w/ FL  & M-7B & 32.8& 46.06     & 59.00    & 39.0     \\
            \midrule
            \quad w/ GE (Ours)   & M-7B & \textbf{41.8}     & \textbf{55.47}    & \textbf{62.07}   & \textbf{41.2}    \\
            \midrule
            
            ReAct       & L-8B & 35.4     & 45.96     & 37.42    & 18.0     \\
            \quad w/ Random   & L-8B & 44.2& 56.02     & \underline{66.73}    & 42.8     \\
            \quad w/ Mean Entropy& L-8B & 46.0& 56.13     & 64.3     & 42.4     \\
            \quad w/ High Score  & L-8B & \underline{46.6}& \underline{57.66}     & 66.21    & \underline{43.6}     \\
            \quad w/ FL  & L-8B & 40.6& 52.38     & 66.09    & 43.2     \\
            \midrule
            \quad w/ GE (Ours)   & L-8B & \textbf{52.4}     & \textbf{66.15}    & \textbf{69.14}   & \textbf{46.0}    \\
            
            \bottomrule
        \end{tabular}
        \caption{Main results. The best results are marked in \textbf{bold} and the second-best results are marked with \underline{underline}. Results marked with \textsuperscript{\dag} are reported in the original paper.}
        \label{tab:maintab}
        % \renewcommand{\figurename}{Figure} % 恢复为 Figure
    
    \end{minipage}%
    \hspace{0.5cm} % 增加水平空隙
    \begin{minipage}{0.38\textwidth}
        \centering
        \includegraphics[page=1, width=0.9\textwidth]{pic/combined_plots.pdf}
        \caption{Comparison of different data selection strategies on various training budgets.}
        \label{fig:Lines}
    \end{minipage}
\end{figure*}
 
\textbf{Evaluation Metrics.}
For WebShop, $\text{reward} \in [0,1]$ measures how well the purchased item matches the text instruction, and SR measures the proportion of items that get reward=1. For HotpotQA, we employ two metrics: F1 and exact match (EM). Similar to SR, EM calculates the proportion of items whose F1=1. 

\textbf{Implementation details.} We invokes the OpenAI GPT4-4o (gpt-4o-2024-08-06) API. For all inference, we set temperature=0.7, top\_p=0.95, max\_length=512. For fine-tuning, we choose \texttt{LLAMA-3.1-8B-Instruct} (L-8B) and \texttt{Mistral-7B-Instruct-v0.3} (M-7B), training for four epochs with a learning rate of 5e-6 using 8 NVIDIA 80GB A100 GPUs. We use L-8B for the computation of GE score.

% We follow the setup from Yao et al. (2023b), which provides the agent with API calls to search and retrieve information. The observation space is the information returned by the API, and the action space includes search, lookup, and finish. For detailed information, please refer to the appendix.




\subsection{Main Result}

% \noindent\textbf{Effectiveness of EDGE.} $\text{EDGE}_\text{UG}$ surpasses baselines in both HotpotQA and WebShop, achieving the best test results as shown in Table 1. Furthermore, $\text{EDGE}_\text{FT}$(ReAct w/ GE) outperforms most GPT-4o agents. These findings showcase the effectiveness of updating guidelines through a few OOD samples. The 拓展的 human experience in $G_{new}$ significantly enhances the agent's performance on complex tasks. $\text{EDGE}_\text{UG}$ employs only the basic ReAct architecture, whereas LUMOS and AMOR define complex frameworks that integrate human experience as a ``problem-solving process" into the agent, which are less simple and effective than ours. Although Reflexion and Expel can update actions and guidelines respectively based on test-time self-feedback, our method still surpasses them, further highlighting the advantages of our approach. 

\textbf{EDGE yield effective guidelines.}
$\text{EDGE}_\text{UG}$ surpasses baselines in both HotpotQA and WebShop,as shown in Table \ref{tab:maintab}, achieving improvements of 13.3\% and 13.9\%, respectively. 
% The finding showcase that the expanded human experience in $\text{G}_{new}$ significantly enhances the agent's performance on complex tasks. $\text{EDGE}_\text{UG}$ employs only the basic ReAct architecture, whereas LUMOS and AMOR define complex frameworks that integrate human experience as a ``problem-solving process" into the agent, which are less simple and effective than ours. 
% ExpeL 总结guideline没有人类的参与，这使得它会受限于LLM对环境的不充分的认知. 导致ExpeL无法总结出 只有具有对环境更本质理解 才能总结出的guideline
%。 ExpeL generate guidelines only through trial and error, 这使得
The ExpeL autonomously summarizes guidelines, but its effectiveness is constrained by the LLM's limited environmental understanding. This shortcoming hinders ExpeL's ability to generate guidelines that demand a deeper comprehension of the environment or tools. For example, WebShop displays candidate products in a semantic similarity ranking, making the top-ranked products more likely to be target products. Due to ExpeL's lack of understanding of the search engine, it is unable to summarize related guidelines.

\textbf{GE-selected fine-tuning data outperform others.} We use same prompt to generate fine-tuning dataset for other data selection methods. Results in Table \ref{tab:maintab} show that GE outperforms them across the two datasets using L-8B and M-7B. Notably, ReAct w/ GE (L-8B) even surpassed  baselines that used GPT-4o. These findings indicate that the samples selected by GE are more challenging, and their solving trajectories integrate a greater depth of human expertise. 
% FL关注选择更具语义代表性的样本，尽管在生成任务上表现良好，但在需要推理、决策的多轮交互复杂任务上表现较差。
FL focuses on selecting samples that are more semantically representative. Although it performs well on generation tasks, results indicate that it struggles with complex tasks that involve multi-turn interactions requiring reasoning and decision-making.
High Score, which filters and selects entirely accurate samples from labeled data, performs relatively well. Notably, not all samples selected by GE are labeled totally correctly. This means that despite having higher rewards, the data selected by the High Score still yields inferior results compared to GE.

\textbf{GE efficiently enhances fine-tuning.} We compared the performance of different data selection methods with training budget  k=[200, 400, 600, 800], as shown in Figure \ref{fig:Lines}. GE consistently outperforms the baselines across all training budgets, achieving more efficient integration of human expertise. Moreover, our method reduces training data usage by 50\% on WebShop and 25\% on HotpotQA, respectively, while still achieving better performance compared to the baselines.


\subsection{Analysis of EDGE}\label{sec:4-4}

\begin{figure*}[ht]
    \centering
    \includegraphics[page=1, width=1\textwidth]{pic/example1.pdf}
    \caption{Examples of various actions' GE scores. The left side shows an example with a high GE score. The right side shows examples with lower GE scores on Webshop and HotpotQA. The red highlight indicates the reason for the lower GE value of the action.} 
    \label{fig:example}
\end{figure*}



% 我们检查了样本数据，观察到实际样本符合3.2节中的假设。我们选取了具有高/低GE值的样本用以说明，如图x。HotpotQA中，图x(a)是高GE值的样例。LLM遵循了guideline，根据topic eneity逐步搜索相关实体，直到找出答案。而图x(b)则获得了较低的GE值。这是因为该样本不包含一个明确的topic entity，guideline在解决该问题的过程中起到较弱的作用。LLM的intrinsic knowledge使LLM在找不到topic entity时错误地倾向于将Search[]当成搜索引擎来使用，导致了任务的失败。在WebShop上，图x(c)，LLM在 click[buy now]前 没有遵循guideline检查目标商品是否符合了instruction中的所有要求（漏掉了'eco friendly'）。这导致了Guided Generation Difficulty反而高于了Intrinsic Generation Difficulty，导致了较低的GE值。

% \subsection{Case Study}
% Q1
\textbf{What kinds of samples have high/low GE score?} Through manual observation of the samples, we found that GE values tend to be higher when the environment is simple, and the guideline incorporates relevant human expertise. Conversely, GE values are lower in complex environments or when the guideline lacks relevant expertise. These observations align with our hypothesis. We selected a few samples with high/low GE score for illustration. On HotpotQA, Figure \ref{fig:example} (Left) is a sample with high GE score. The LLM followed the guideline, progressively searching for related entities based on the topic entity until the answer was found. Figure \ref{fig:example} (Right) demonstrates actions with low GE score. On WebShop, samples with complex environment caused the LLM to violate the guidelines, resulting in low GE scores. 
%左上角的例子中，由于商品页面的available actions过长且有相关信息，误导LLM根据instruction点击了一个不存在的button。右上角的例子中，由于商品符合了instruction中的其他要求，导致LLM武断地点击了购买。
In the top left example, the extensive list of available actions on the product page, combined with relevant information, misled the LLM into click[lemon], which was a non-existent button. In the example at the top right corner, because the product met the other requirements in the instructions, the LLM hastily clicked the purchase button. The example below is a HotpotQA case, where this sample made the LLM use overly long search keywords.
%In Figure \ref{fig:main}, the sample with a low GE score lacks a clear topic entity, diminishing the effectiveness of the guideline in problem-solving. The intrinsic knowledge of the LLM caused it to mistakenly assume ``Search[]" as a search engine when it was unable to find the topic entity, leading to task failure.

% Q2
\textbf{What guidelines have we summarized?} Based on the observation of low GE score samples, we have listed the following representative guidelines in order of their necessity within the samples. Figure \ref{fig:example_guideline} shows the format of our guideline. For WebShop: % 根据观察低GE score样本，我们按照在样本中被需要的程度排序，列出了如下representative guidelines
\begin{itemize}
    \item \textit{Click high-ranked products first.} A high ranking indicates that the semantics of the product's options or features are more similar to the search content. Even if the titles of high-ranked products sometimes do not fully meet the required criteria, it is still necessary to click on them.

    \item \textit{What is a product title.} This guideline introduces that some attributes like color, flavor, etc., will not appear in the product title. LLMs often hope to find products whose titles perfectly match the instructions.
    % 这一部分介绍了Webshop的商品标题与其具体商品页之间的关联，表明 some attributes like color, size, flavor, etc., will not appear in the product title，或是仅仅出现某几个。LLM经常希望找到标题完美匹配instruction的商品，但这通常是不可能的。

    \item \textit{Buying a similar product is better than buying nothing.} The LLM needs to complete the purchase of a product within a limited number of interaction rounds. Therefore, before the rounds are exhausted, the LLM needs to balance the trade-off and compromise to buy a product that is not perfectly aligned when necessary.

    
\end{itemize}

For HotpotQA:
\begin{itemize}

    \item \textit{How to use `Lookup'.} Despite being informed that "the Lookup[] only supports exact matching," the LLM still tends to search for longer keywords. This guideline details that the LLM should search for the shortest possible word that are likely to appear in the original text to match more search results.


    \item \textit{Solving questions without a clear topic entity} When the topic entity cannot be found, LLM can try leverage its prior knowledge to leverage its prior knowledge to make a reasonable inference or use Search[] based on semantic similarity.

    \item \textit{Understanding the Question.} This guideline provides a detailed explanation on how to understand and deconstruct complex problems. In complex scenarios, if certain representative requirements are met(e.g., the 20th President of the United States), the LLM can respond directly without confirming other constraints.

\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[page=18, width=0.5\textwidth]{pic/guideline_example.pdf}
    \caption{Example of a peice of our guideline on HotpotQA} 
    \label{fig:example_guideline}
\end{figure}

\begin{table*}[t]
    \centering
    \begin{tabular}{@{}lrrrrrrrrc@{}}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Avg. Interaction Turns}} & \multicolumn{4}{c}{\textbf{Reward}}  &  \multirow{2}{*}{\textbf{LLM Reward}} \\ 
        \cmidrule(lr){2-5} \cmidrule(lr){6-9} 
                                 & k=200 & k=400 & k=600 & k=800 & k=200 & k=400 & k=600 & k=800 & \\ \midrule
        FL                     & 4.50   & 4.53   & 4.45   & 4.43   & 80.85 & 79.78 & 77.86 & 77.68 & 46.06\\
        Random                  & 4.97   & 4.84   & 4.75   & 4.87   & 77.56 & 79.21 & 79.81 & 77.69 & 46.11\\
        Mean Entropy            & 4.82   & 4.92   & 4.89   & 4.96   & 72.17 & 72.97 & 73.18 & 71.10 & 47.00\\
        High Score              & 4.77   & 4.60   & 4.65   & 4.64   & 100.00 & 100.00 & 100.00 & 100.00 & 49.19\\
        \midrule
        GE (Ours)                     & 6.40 & 6.38 & 6.43 & 6.39 & 68.06 & 70.84 & 71.09 & 70.27 & 55.47\\ \bottomrule
    \end{tabular}
    \caption{Statistics of the annotated data by different data selection methods.}

    \label{tab:Statistics}
\end{table*}

% Q3 传统方法认为r=1,这有问题。数据reward=1真的意味着高质量吗
\textbf{Dose filtering reward=1 trajectories truly lead to high quality?} The quality of the fine-tuned dataset directly impacts the performance of the fine-tuned LLM. Intuitively, annotated samples with  higher rewards suggest higher quality. Thus, existing methods often filter fully correct samples (reward = 1) from many annotated examples, known as High Score. However, does low reward always indicate low quality? To answer this question, we analyzed the annotated samples selected by different  methods, as shown in Table \ref{tab:Statistics}. Compared to Random, High Score has lower average interaction turns, which typically indicates that the samples are simpler and easier. Combined with the observation from Table \ref{tab:proportion} that High Score is least likely to select hard questions, we can conclude that High Score includes more simple samples during filtering. This explains why High Score fails to achieve the best performance despite all data reward=1. Notably, despite the lowest annotated data rewards of GE, it achieves the best performance. This is because more challenging samples can better leverage the advantages of human experience from the guidelines. This suggests that data containing more ``attempts at challenging problems"  represents higher quality for fine-tuning, even if it is not fully correct.
% 更复杂的样本可以更大程度地将guideline中的人类经验转化到trajectory上

% why EDGE有效（hotpot的难样本更多的被解决了）（EDGE增加了可解决问题的范围吗？）
% 我们试图探究EDGE是否增加了可解决问题的范围。我们对HotpotQA上的样本难度进行了统计。HotpotQA将不同问题分为了不同难度：easy、medium和hard。表x是对data pool的统计，表x展示了不同数据选择方法选出的样本难度分布。在众多数据选择方法中，只有GE筛选出了更多hard样本。接着，我们。由于HotpotQA的dev set中只包含hard question，我们从train set中随机选出了不包含在data pool中的500 easy questions and 500 medium questions for 更详细的分析。表x展示了使用不同数据选择方式微调的结果。尽管GE筛选出了更难、更复杂的样本，其在easy、medium问题上的性能依旧领先，并且在hard问题上

\subsection{Effective Analysis in HotpotQA}


% \textbf{Why EDGE is effective?}
To investigate whether EDGE expanded the range of solvable problems, we analyzed the distribution of question difficulty levels in HotpotQA (easy, medium and hard). Table \ref{tab:proportion} presents the proportion of each difficulty level within the subsets selected by different methods. Among the various approaches, only GE selected a higher proportion of hard questions. With more out-of-guideline questions selected for annotating, GE achieved slight advantages on easy and medium questions, and significantly outperformed the baselines on hard questions, as shown in Table \ref{tab:Performance-on-level}. Consequently, GE effectively broadened the scope of solvable problems by focusing on out-of-guideline questions.
\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Method & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
        \midrule
        Random & 0\% & 0\% & 0\% \\
        Mean Entropy & +1.08\% & +1.13\% & -2.21\% \\
       High Score & -0.92\% & +4.13\% & -3.21\% \\
        FL & +0.71\% & +0.00\% & -0.71\% \\
        \midrule
        GE (Ours) & -1.67\% & -1.50\% & +3.17\% \\
        \bottomrule
    \end{tabular}
    \caption{The proportion of different difficulty levels across various methods. The percentages indicate the change in proportion compared to the original distribution of difficulty levels}
    \label{tab:proportion}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
        \midrule
        ReAct &-&-&- \\
        \quad w/ Random & 65.29 & 62.92 & 56.02 \\
        \quad w/ Mean Entropy  & 62.12 & 62.29 & 56.13 \\
        \quad w/ High Score & 66.68 & 65.11 & 57.66 \\
        \quad w/ FL & 63.19 & 61.82 & 52.38 \\
        \midrule
        \quad w/ GE (Ours) & 68.64 & 66.98 & 66.15 \\
        \bottomrule
    \end{tabular}
    \caption{Performance comparison on different difficulty levels}
    \label{tab:Performance-on-level}
\end{table}



% LLM-Agent 的微调数据集怎样才算是高质量（reward越高越好吗？）
% 微调数据集的质量直接导致了Fine-tuned LLM的性能。直觉上，标注样本的reward越高，质量就越高。于是，现有方法通常在许多标注过的样本中过滤出完全正确的（reward=1），对应high score方法。但是，reward\le 1的标注样本一定意味着low-quality吗？为了解答这个问题，我们对 标注后的 使用不同数据选择方法选出的样本 进行了统计，如表x。High Score的average interaction turns低于Random, 结合 表x中显示High Score最不倾向于选择hard question 这一现象，我们可以得知：High Score在过滤时无意地选入了更多的简单的样本。这就是为什么尽管High Score data的reward更高，但没有实现更好的性能。值得一提是，尽管GE拥有最低的annotated data’s reward，但还是实现了最好的性能。这表明了包含更多“未知问题的解决思路”的数据是更high-quality的for微调，哪怕其最终没有得到reward=1。




% 总结出了哪些Guideline？
% 我们给出了总结出的最有代表性的一些guideline。For HotpotQA，。

% \begin{table*}[t]
%     \centering
%     \begin{tabular}{@{}lrrrrrrrrc@{}}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Avg. Interaction Turns}} & \multicolumn{4}{c}{\textbf{Reward}}  &  \multirow{2}{*}{\textbf{LLM Reward}} \\ 
%         \cmidrule(lr){2-5} \cmidrule(lr){6-9} 
%                                  & k=200 & k=400 & k=600 & k=800 & k=200 & k=400 & k=600 & k=800 & \\ \midrule
%         FL                     & 4.50   & 4.53   & 4.45   & 4.43   & 80.85 & 79.78 & 77.86 & 77.68 & 46.06\\
%         Random                  & 4.97   & 4.84   & 4.75   & 4.87   & 77.56 & 79.21 & 79.81 & 77.69 & 46.11\\
%         Mean Entropy            & 4.82   & 4.92   & 4.89   & 4.96   & 72.17 & 72.97 & 73.18 & 71.10 & 47.00\\
%         High Score              & 4.77   & 4.60   & 4.65   & 4.64   & 100.00 & 100.00 & 100.00 & 100.00 & 49.19\\
%         \midrule
%         GE (Ours)                     & 6.40 & 6.38 & 6.43 & 6.39 & 68.06 & 70.84 & 71.09 & 70.27 & 55.47\\ \bottomrule
%     \end{tabular}
%     \caption{Statistics of the annotated data by different data selection methods.}

%     \label{tab:Statistics}
% \end{table*}

% 信息检索不充分  幻觉  关键上下文信息遗漏  action的推理逻辑出错  Format Error    Repetition of Actions
% \begin{table}[t]
% \centering
% \begin{tabular}{lcc}
% \toprule
% \textbf{Error Type} & \textbf{HotpotQA} & \textbf{WebShop} \\
% \midrule
% Format Error  & 9 & 11 \\
% Action Repetition  & 0 & 41 \\
% Reasoning Error & 33 & 0 \\
% Ignoring Key Context & 24 & 6 \\
% Hallucination & 7 & 6 \\
% Arbitrary Decision  & 13 & 27 \\
% Other & 14 & 9 \\
% \bottomrule
% \end{tabular}
% \caption{Distribution of error types.}
% \label{tab:Distribution of error types.}
% \end{table}



% \textbf{Open-sourced LLM }

% \subsection{Error Analysis.} To systemically assess our method’s deficiencies, we randomly select 100 error instances from each for manual inspection. The aggregated statistical findings are detailed in Table x. On HotpotQA, Reasoning Error are the most frequent one. For example, the LLM might incorrectly conclude that ``Jerry Glanville (born October 14, 1941)" is younger than ``Keith Bostic (born January 17, 1961)". The reason is GE-selected data primarily focus on expanding the boundaries of problems that LLMs can solve, rather than enhancing their reasoning capabilities. The second most common error type is Ignoring Key Context, which means the LLM sometimes overlook the answer which has already presented in observations. On WebShop, Action Repetition accounts for the majority of failures, primarily because LLMs struggle to comprehend the relatively more complex environment of the platform. %排名第二的Arbitrary Decision means the LLM 没有确认当前商品是否完全符合要求就购买了。这通常是LLM被过长的上下文信息干扰所导致的。

















% 主表备份
% \begin{table*}[ht]
% \centering
% \begin{tabular}{@{}lccccc@{}} % 所有列居中对齐
% \toprule

% \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Base Model}} & \multicolumn{2}{c}{\textbf{HotpotQA}} & \multicolumn{2}{c}{\textbf{WebShop}} \\
% \cmidrule(r){3-4} \cmidrule(l){5-6}
%  &    & \textbf{EM} & \textbf{F1} & \textbf{Reward}  & \textbf{SR}\\ 
% \midrule
% \multicolumn{6}{c}{\textbf{API-based LLM}} \\ 
% \midrule
% ReAct  & GPT-4o     & 42.0& 55.17     & 58.6     & 33.2     \\
% ExpeL  & GPT-4o     & 47.7& 60.92     & 64.1     & 42.0     \\
% Reflexion& GPT-4o     & 49.1& 61.30     & 63.2     & 39.8     \\
% LUMOS  & GPT-4, LLAMA-2-13B & 36.3& 57.40     & 50.3     & -\\
% AMOR   & GPT-4-turbo& 55.2& 65.20     & -& -\\
% \midrule
% $\text{EDGE}_\text{UG}$ (Ours) & GPT-4o     & \textbf{64.6}     & \textbf{72.58}    & \textbf{75.11}   & \textbf{47.9}    \\
% \midrule
% \multicolumn{6}{c}{\textbf{Fine-tuned Open-sourced LLM}}\\ 
% \midrule
% ReAct       & Llama-3.1-8B & 35.38     & 45.96     & 37.42    & 18.0     \\
% \quad w/ random   & Llama-3.1-8B & 44.2& 56.02     & 66.73    & 42.8     \\
% \quad w/ Mean Entropy& Llama-3.1-8B & 46.0& 56.13     & 64.3     & 42.4     \\
% \quad w/ high score  & Llama-3.1-8B & 46.6& 57.66     & 66.21    & 43.6     \\
% \quad w/ FL  & Llama-3.1-8B & 40.6& 52.38     & 66.09    & 43.2     \\
% \midrule
% \quad w/ GE (Ours)   & Llama-3.1-8B & \textbf{52.4}     & \textbf{66.15}    & \textbf{69.14}   & \textbf{46.0}    \\
% \bottomrule
% \end{tabular}
% \caption{Performance Comparison across Methods and Models}
% \label{tab:booktabs}
% \end{table*}



%%合2个Easy、M、H的表

% \begin{table}[ht]
%     \centering
%     \begin{minipage}[t]{0.48\textwidth}
%         \centering
%         \begin{tabular}{lrrr}
%             \toprule
%             \textbf{Method} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
%             \midrule
%             \textbf{Random} & 65.29 & 62.92 & 56.02 \\
%             \textbf{Mean} Entropy & 62.12 & 62.29 & 56.13 \\
%             \textbf{High} Score & 66.68 & 65.11 & 57.66 \\
%             \textbf{FL} & 63.19 & 61.82 & 52.38 \\
%             \midrule
%             \textbf{GE (Ours)} & 68.64 & 66.98 & 66.15 \\
%             \bottomrule
%         \end{tabular}
%         \caption{Performance comparison across different methods.}
%         \label{tab:booktabs}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.48\textwidth}
%         \centering
%         \begin{tabular}{lccc}
%             \toprule
%             \textbf{Method} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
%             \midrule
%             Random & 0\% & 0\% & 0\% \\
%             Mean Entropy & +1.08\% & +1.13\% & -2.21\% \\
%             High Score & -0.92\% & +4.13\% & -3.21\% \\
%             FL & +0.71\% & +0.00\% & -0.71\% \\
%             GE (Ours) & -0.67\% & -0.50\% & +1.17\% \\
%             \bottomrule
%         \end{tabular}
%         \caption{Percentage changes across different difficulty levels using various methods.}
%         \label{tab:method_comparison}
%     \end{minipage}
% \end{table}


% 

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{@{}lrrrrrrrrr@{}}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Interaction turns (golden answer)}} & \multicolumn{4}{c}{\textbf{Scores (golden answer)}} & \multirow{2}{*}{\textbf{Performance}} \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9}
%                                  & k=200 & k=400 & k=600 & k=800 & k=200 & k=400 & k=600 & k=800 &  \\ \midrule
%         \textbf{FL}                      & 4.50   & 4.53   & 4.45   & 4.43   & 68.0 / 80.85 & 70.0 / 79.78 & 68.3 / 77.86 & 68.0 / 77.68 & 1 \\
%         \textbf{Random}                  & 4.97   & 4.84   & 4.75   & 4.87   & 69.5 / 77.56 & 71.0 / 79.21 & 72.0 / 79.81 & 70.0 / 77.69 & 1 \\
%         \textbf{Mean Entropy}            & 4.82   & 4.92   & 4.89   & 4.96   & 59.9 / 72.17 & 62.1 / 72.97 & 63.6 / 73.18 & 63.5 / 71.10 & 1 \\

%        \textbf{High Score}              & 4.77   & 4.60   & 4.65   & 4.64   & 100.0 / 100.0 & 100.0 / 100.0 & 100.0 / 100.0 & 100.0 / 100.0 & 1 \\

%         \midrule
%         \textbf{GE}                      & \textbf{6.40}   & \textbf{6.38}   & \textbf{6.43}   & \textbf{6.39}   & \textbf{60.5} / \textbf{68.06} & \textbf{62.0} / \textbf{70.84} & \textbf{62.7} / \textbf{71.09} & \textbf{61.8} / \textbf{70.27} & \textbf{1} \\ \bottomrule
%     \end{tabular}
%     \caption{Performance comparison of different methods}

%     \label{tab:booktabs}
% \end{table*}



% 统计量，感觉用不到

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{lcccc}
%         \toprule
%         \textbf{Metric} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{All} \\
%         \midrule
%         Avg. Score & 52.8 & 52.33 & 42.07 & 50.62 \\
%         Proportion (\%) & 19.92\% & 62.50\% & 17.58\% & 100\% \\
%         \bottomrule
%     \end{tabular}
%     \caption{Summary of scores and proportions across difficulty levels.}
%     \label{tab:booktabs}
% \end{table*}