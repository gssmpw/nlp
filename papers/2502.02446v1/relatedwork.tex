\section{Related work}
\begin{figure*}
\begin{center}
\includegraphics[scale=1.5]{figs/figure_1.pdf}
\end{center}
\vspace{-20pt}
\caption{Overview of our MPNN architectures for solving LCQPs.}
\end{figure*}

Here, we discuss relevant related work.

\paragraph{MPNNs} MPNNs~\citep{Gil+2017,Sca+2009} have been extensively studied in recent years. Notable architectures can be categorized into spatial models \citep{Duv+2015,Ham+2017,bresson2017residual,Vel+2018,xu2018how} and spectral MPNNs \citep{Bru+2014,defferrard2016convolutional,Kip+2017,Lev+2019,monti2018motifnet,geisler2024spatio}. The former conforms to the message-passing framework of~\citet{Gil+2017}, while the latter leverage the spectral property of the graph. 

\paragraph{Machine learning for convex optimization}
In this work, we focus on convex optimization and direct readers interested in combinatorial optimization problems to the surveys \citet{bengio2021machine,cappart2023combinatorial,peng2021graph}; see a more detailed discussion on related work in \cref{sec:more_literature}.

A few attempts have been made to apply machine learning to LPs. \citet{li2022learning2reform} learned to reformulate LP instances, and \citet{pmlr-v202-fan23d} learned the initial basis for the simplex method, both aimed at accelerating the solver. \citet{liu2024learningpivot} imitated simplex pivoting, and \citet{pmlr-v238-qian24a} proposed using MPNNs to simulate IPMs for LPs \citep{nocedal2006numerical}. \citet{li2024pdhg} introduced PDHG-Net to approximate and warm-start the \new{primal-dual hybrid gradient algorithm} (PDHG)  \citep{applegate2021practical,lu2024first}. \citet{li2024onsmallgnn} bounded the depth and width of MPNNs while simulating a specific LP algorithm.
Quadratic programming (QP) has seen limited standalone exploration. Notable works include \citet{bonami2018learning}, who analyzed solver behavior to classify linearization needs, and \citet{pmlr-v157-getzelman21a}, who used reinforcement learning for solver selection. Others accelerated solvers by learning step sizes \citep{ichnowski2021accelerating,jung2022learning} or warm-started them via end-to-end learning \citep{sambharya2023end}. Graph-based representations have been applied to quadratically constrained quadratic programming (QCQP) \citep{wu2024representingqcqp,xiong2024neuralqp}, while \citet{gao2024ipm} extended \citet{pmlr-v238-qian24a} to general nonlinear programs.
On the theoretical side, \citet{chen2022representing,chen2024qp,wu2024representingqcqp} examined the expressivity of MPNNs in approximating LP and QP solutions, offering insights into their capabilities and limitations.

See \Cref{sec:more_literature} for a discussion on machine learning for constrained optimization.