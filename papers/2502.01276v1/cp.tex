\section{\tool: Attributing Importance to Hyperparameters}\label{sec:hpi-games}


In HPO, a wide variety of questions can be asked for explanations, ranging from individual suggested values in a returned hyperparameter configuration to complex reasoning during the optimization process or the description of remaining optimization potentials. Hence, explanations may be needed on different levels of the hyperparameter optimization process, ranging from returned configurations to a qualitative comparison of entire hyperparameter optimization tools. 
In the following, we limit ourselves to four areas, dubbed \ablation, \sensitivity, \tunability, and \opthabit.
We first introduce \ablation (\cref{sec:ablation}), which we use as the fundamental backbone of \tool.
Based on \ablation, we discuss \sensitivity as an extension of the functional ANOVA framework by \citet{fANOVA}, and compare it theoretically to our novel approach for \tunability (\cref{sec:sensitivity-tunability}).
\tunability is then used to discover \opthabit (\cref{sec:opthabit}), and we lastly give practical guidance for implementing \tool (\cref{sec:hypershap-in-practice}).
To this end, $\mathcal N$ is the set of hyperparameters, and main and interaction effects are quantified based on the \gls*{SV} and \glspl*{SI} of games.

\subsection{\ablation of Hyperparameters}\label{sec:ablation}

One common scenario for quantifying the importance of hyperparameters is to compare a hyperparameter configuration (HPC) $\conf^\ast$ of interest to some reference HPC $\conf^0$, e.g., the default parameterization of a learner as provided by its implementing library or a tuned default HPC that has proven effective for past tasks. In turn, $\conf^\ast$ can be an HPC returned by a hyperparameter optimizer or a manually configured HPC. Given $\conf^\ast$ and $\conf^0$, the question now is how values of $\conf^\ast$ affect the performance of the learner relative to the reference HPC $\conf^0$. To this end, we can transition from the reference HPC to the HPC of interest by switching the values of hyperparameters one by one from its value in $\conf^0$ to the value in $\conf^\ast$, which is also done in empirical machine learning studies and referred to as ablations.

Ablation analysis has already been followed by \citet{Fawcett-jh16a} and \citet{DBLP:conf/aaai/BiedenkappLEHFH17}, but restricted to single hyperparameter ablations that are executed sequentially.
Instead, we consider the HPI game of \ablation using \emph{all possible subsets}, which allows to capture interactions between hyperparameters.

\begin{definition}[HPI Game - \ablation]
The \ablation HPI game $\nu_{G_A}: 2^{\mathcal N} \to \mathbb{R}$ is defined based on a tuple 
\[ 
G_A := (\conf^0, \conf^\ast, D, u),
\]
consisting of a \emph{baseline (default)} HPC $\conf^0$,
an HPC of interest $\conf^\ast$, a dataset $D$ and a measure $u$.
Given a coalition $S\subseteq \mathcal{N}$, we construct an intermediate HPC with $\oplus_S: \confs \times \confs \to \confs$ as
\[
\conf^* \oplus_S \conf^0 := \begin{cases}
\lambda^\ast_i, & \text{ if } i \in S,\\
\lambda^0_i, & \text{ else},
\end{cases}
\]
and evaluate its worth with
\[
\nu_{G_A}(S) := \val(\conf^\ast \oplus_S \conf^0, D) \,\, .
\]
\end{definition}


The \ablation game quantifies the worth of a coalition based on the comparison with a default HPC configuration $\conf^0$.
In XAI terminology, this approach is known as \emph{baseline imputation} \citep{Fumagalli.2024a}.
Natural extensions of the \ablation game capture these ablations with respect to a distribution $\conf^0 \sim p^0(\conf^0)$ over the baseline HPC space $\confs$ as
\[
\mathbb{E}_{\conf^0 \sim p^0(\conf^0)}[ \val(\conf^\ast \oplus_S \conf^0, D)] \,\, ,
\]
which relates to the \emph{marginal performance} introduced by \citet{fANOVA}.
In XAI terminology, it is further distinguished between distributions $p(\conf^0)$ that either depend (conditional) or do not depend (marginal) on the HPC of interest $\conf^\ast$ \citep{Fumagalli.2024a}.
In XAI, baseline imputation is mostly chosen for computational efficiency, but it was argued that it satisfies beneficial properties \citep{Sundararajan.2020b}.
However, the choice of a baseline has a strong impact on the explanation \citep{sturmfels2020visualizing}.
In HPO, we are typically given a \emph{default} HPC $\conf^0$, which we use for the \ablation game, but our methodology can be directly extended to the probabilistic setting.


\subsection{Tunability of Learners}\label{sec:sensitivity-tunability}
Zooming out from a specific configuration, we can ask to what extent it is worthwhile to tune hyperparameters. 
In the literature, this question has been connected to the term of \textit{tunability} \citep{tunability}. 
Tunability aims to quantify how much performance improvements can be obtained by tuning a learner comparing against a baseline HPC, e.g., a HPC that is known to work well across various datasets \citep{DBLP:conf/gecco/PushakH20a}.
In this context, we are interested in the \emph{importance} of tuning specific hyperparameters.
A classical tool to quantify variable importance is \emph{sensitivity analysis} \citep{Owen_2013}, which measures the variance induced by the variables of interest and decomposes their contributions into main and interaction effects.

\begin{definition}[HPI Game - \sensitivity]
The \sensitivity game $\nu_{G_V}: 2^{\mathcal N} \to \mathbb{R}$ is defined based on a tuple
\[
    G_V := (\conf^0, \confs, p^*, D, u),
\]
consisting of a \emph{baseline} HPC $\conf^0$, a HPC space of interest $\confs$ equipped with a probability distribution $p^*$, a dataset $D$ and a measure $u$. 
The value function is given by
\[
    \nu_{G_V}(S) := \mathbb{V}_{\conf \sim p^\ast(\conf)}[\val(\conf \oplus_S \conf^0, D)] \,\, ,
\] 
\end{definition}

A large value of a coalition $S \subseteq \mathcal N$ in the \sensitivity game indicates that these hyperarameters are important for the HPO process.
\citet{fANOVA} implicitly rely on the \sensitivity game and compute the functional ANOVA decomposition, which quantifies \emph{pure} main and interaction effects.
In game theory, this corresponds to the \glspl*{MI} of the \sensitivity game, and we show in in our experiments that this representation consists of $2^n$ non-trivial terms.
We therefore summarize the \glspl*{MI} into interpretable representations using the \gls*{SV} and \glspl*{SI}.

While sensitivity analysis is a suitable tool in XAI, it has some drawbacks for \tunability.
First, as illustrated below, the total variance being decomposed $\nu_{G_V}(\mathcal N)$ strongly depends on the chosen probability distribution $p^*$ and the HPC space $\confs$.
Moreover, it does not reflect the performance increase expected when tuning all hyperparameters.
Second, for a coalition of hyperparameters $S \subseteq \mathcal N$, we expect that the coalition's worth (performance) increases when tuning additional parameters, i.e. $\nu(S) \leq  \nu(T)$, if $S \subseteq T$.
This property is known as \emph{monotonicity} \citep{Fujimoto.2006}, but \emph{does not} hold in general for the \sensitivity game $\nu_{G_V}$.
For a simple example, we refer to \cref{appx_sec_non_monotone_example}.
Instead, we now propose the monotone HPI \tunability game.

\begin{definition}[HPI Game -  \tunability]\label{def:tunability}
The \tunability HPI game is defined with a tuple 
\[
G_T = (\conf^0, \confs, D, u),
\]
consisting of a baseline HPC $\conf^0 \in \confs$, a HPC space $\confs$, a dataset $D$ and a measure $u$. The value function is given by
\[
\nu_{G_T}(S) := \max_{\conf \in \confs}
\,\val(\conf \oplus_S \conf^0, D) \,\, .
\]
\end{definition}

The \tunability game directly measures the performance obtained from tuning the hyperparameters of a coalition $S$, while leaving the remaining hyperparameters at the default value $\conf^0$.
The \tunability clearly satisfies monotonicity, which yields the following theorem.

\begin{theorem}
    The \tunability game yields non-negative \glspl*{SV} and non-negative \emph{pure} individual (main) effects obtained from functional ANOVA via the MÃ¶bius transform.
\end{theorem}

While main effects obtained from the \tunability game are non-negative, interactions clearly can be negative, indicating redundancies of the involved hyperparameters.

\paragraph{Benefits of \tunability over \sensitivity.}
We now showcase benefits of the \tunability game over the \sensitivity game using a synthetic example.
We consider a two-dimensional HPC space $\confs := \Lambda_1 \times \Lambda_2$ with discrete HPCs $\Lambda_1 := \{0,1\}$ and $\Lambda_2:= \{0,\dots,m\}$ for $m > 1$.
The optimal configuration is defined as $\conf^\ast := (1,m)$, and the performance is quantified by $\val(\conf,D) := \mathbf{1}_{\lambda_1=\lambda_1^\ast}+\mathbf{1}_{\lambda_2 = \lambda^\ast_2}$, where $\mathbf{1}$ is the indicator function.
That is, we observe an increase of performance of $1$ for each of the parameters set to the optimal HPC $\conf^\ast$.
Lastly, we set the HPC baseline to $\conf^0 := (0,0)$ and $\conf^0 := \conf^\ast$.
Intuitively, we expect that both parameters obtain similar HPI scores, since they both contribute equally to the optimal performance $\val(\conf^\ast,D) = 2$.
Moreover, if the baseline is set to the optimal HPC $\conf^\ast$, we expect the HPI to reflect that there is not benefit of tuning.
Since the parameters independently affect the performance, we do not expect any interactions.

\begin{theorem}
    The HPI scores of the \sensitivity and \tunability game are given by \cref{tab:synthetic_example}.
\end{theorem}

Both HPI scores correctly quantify the absence of interaction $\lambda_1 \times \lambda_2$.
In contrast to the \tunability game, the \sensitivity game assigns smaller HPI scores to the hyperparameter $\lambda_2$ due to the larger HPC space $\Lambda_2$. 
In fact the \sensitivity HPI score of $\lambda_2$ roughly decreases with order $m^{-1}$.
Moreover, the \tunability HPI scores reflect the performance increase and decompose the difference between the optimal and the default performance.
In contrast, the \sensitivity HPI scores decompose the overall variance, which depends on $\confs$ and $p^*$.
Lastly, setting the default HPC $\conf^0$ to $\conf^\ast$ decreases the \tunability HPI scores to zero, whereas the \sensitivity HPI scores remain unaffected.
In summary, the \sensitivity HPI scores reflect the variability in performance when changing the hyperparameters, whereas the \tunability HPI scores reflect the benefit of tuning hyperparameters.

\subsection{\opthabit}\label{sec:opthabit}
The \tunability game aims to explain the importance of hyperparameters being tuned. 
We can further utilize this paradigm to gain insights into the capabilities of a hyperparameter optimizer.
In particular, by comparing the optimal performance with the empirical performance from a single optimizer, we uncover biases and point to specific hyperparameters that the optimizer fails to exploit.
To this end, we define a hyperparameter optimizer as a function $\mathcal{O}: \mathbb{D} \times 2^{\confs} \rightarrow \confs$, mapping from the space of datasets and a (sub)space of a HPC space to a configuration. 

\begin{definition}[HPI Game - \opthabit] \label{def:opthabit}
    The \opthabit HPI game is defined as a tuple 
    \[
    G_O = (\confs, \conf^0, \mathcal{O}, D, u),
    \]
    consisting of a HPC space $\confs$, a baseline $\conf^0$, the hyperparameter optimizer of interest $\mathcal O$, a dataset $D$ and a measure $u$.
    For $S \subseteq \mathcal N$, we construct $\confs^S := \{ \conf \oplus_S \conf^0: \conf \in \confs\}$ and define
        \begin{align*}
    \nu_{G_0}(S) := \val \Big(\mathcal{O}(D, \confs^S), D \Big)
    - \nu_{G_T}(S) \,\, .
    \end{align*}
\end{definition}

Intuitively speaking, the value function measures any deviation from the performance of the actual best-performing hyperparameter configuration. 
In other words, with the help of Definition~\ref{def:opthabit}, we can identify deficiencies of the hyperparameter optimizer $\mathcal{O}$ over the actual best-performing solution and, thereby for example, identify whether an optimizer struggles to optimize certain (types of) hyperparameters.

\subsection{Practical Aspects of \tool}\label{sec:hypershap-in-practice}

\paragraph{Efficient Approximation.}
In practice, to evaluate a single coalition in Definition~\ref{def:tunability}, we need to conduct one HPO run.
While this can result in considerable costs, we argue that for \tunability using surrogate models that are, e.g., obtained through HPO via Bayesian optimization, can be used to simulate HPO runs, rendering \tool more tractable, similar to other surrogate-based explainability methods for HPO~\cite{fANOVA,DBLP:conf/aaai/BiedenkappLEHFH17,DBLP:conf/nips/MoosbauerHCLB21,DBLP:conf/automl/SegelGTBL23}.
In contrast, to analyze \opthabit, we propose to approximate $\nu_{G_T}$ using diverse ensemble of optimizers $\mathbb{O}:= \{ \mathcal{O}_i\}$, and choose the best result for $\confs^S$ obtained through any optimizer from $\mathbb{O}$.
The virtual best hyperparameter optimizer (VBO) is thus:
\[
\nu_{G_T}(S) \approx \underset{\conf^i = \mathcal{O}_i(D, \confs^S)}{\max} \val(\conf^i, D) \,\, .
\]


\paragraph{HPI Game Extensions across Multiple Datasets.} 
In a more general setting, we are also interested in the different aspects of \tool across multiple datasets, where we present direct extensions of the previous games.
\begin{definition}[HPI Game- Multi-Dataset Variants]
Given a collection of datasets $\mathcal D := \{D_1, \dots, D_M\}$ and the corresponding HPI games $\nu^{D_i}_{G}$ for $i \in \{1,\dots,M\}$ with $G \in \{G_A,G_V,G_T,G_O\}$, we define its multi-dataset variant with the value function
\begin{align*}
    \nu^{\mathcal D}_{G}(S) := \bigoplus_{i = 1 }^M \nu^{D_i}_G(S),
\end{align*}
where $\bigoplus$ denotes an aggregation operator, e.g. the mean, of the game values obtained from the individual datasets $D_i$.
\end{definition}