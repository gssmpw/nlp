%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\graphicspath{{images/}}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage[hidelinks,colorlinks=true,citecolor=blue!50!black,linkcolor=black,urlcolor=green!50!black]{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}

\input{settings}
% own packages
\usepackage{tabularx}
\usepackage{xspace}
\usepackage{stackengine}
\usepackage{placeins}
\usepackage{rotating}
\usepackage{wrapfig}
\usepackage{titletoc} 
\usepackage{enumitem}
\usepackage{multirow}

\newcommand{\edit}[1]{#1}


\newcommand{\numgames}{5\xspace}
\newcommand{\tool}{\mbox{{\sc HyperSHAP}}\xspace}
\newcommand{\ablation}{Ablation\xspace}
\newcommand{\sensitivity}{Sensitivity\xspace}
\newcommand{\tunability}{Multi-Data Tunability\xspace}
\newcommand{\dstunability}{Tunability\xspace}
\newcommand{\opthabit}{Optimizer Bias\xspace}
\newcommand{\jahs}{\texttt{JAHS-Bench-201}\xspace}
\newcommand{\rbvranger}{\texttt{rbv2\_ranger}\xspace}
\newcommand{\pdone}{\texttt{PD1}\xspace}
\newcommand{\lcbench}{\texttt{lcbench}\xspace}

\newcommand{\cD}{\mathcal{D}}
\newcommand{\perf}{u}
\newcommand{\conf}{\vec{\lambda}}
\newcommand{\confs}{\vec{\Lambda}}
\newcommand{\val}{\textsc{Val}_\perf}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{HyperSHAP: Shapley Values and Interactions for Hyperparameter Importance}

\begin{document}

\twocolumn[
\icmltitle{HyperSHAP: Shapley Values and Interactions for Hyperparameter Importance}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Marcel Wever}{l3s,luh}
\icmlauthor{Maximilian Muschalik}{mcml,lmu}
\icmlauthor{Fabian Fumagalli}{bie}
\icmlauthor{Marius Lindauer}{l3s,luh}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{l3s}{L3S Research Center, Germany}
\icmlaffiliation{luh}{Leibniz University Hannover, Germany}
\icmlaffiliation{mcml}{Munich Center for Machine Learning, Germany}
\icmlaffiliation{lmu}{LMU Munich, Germany}
\icmlaffiliation{bie}{Bielefeld University, Germany}

\icmlcorrespondingauthor{Marcel Wever}{m.wever@ai.uni-hannover.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Hyperparameter optimization (HPO) is a crucial step in achieving strong predictive performance.
However, the impact of individual hyperparameters on model generalization is highly context-dependent, prohibiting a one-size-fits-all solution and requiring opaque automated machine learning (AutoML) systems to find optimal configurations.
The black-box nature of most AutoML systems undermines user trust and discourages adoption. 
To address this, we propose a game-theoretic explainability framework for HPO that is based on Shapley values and interactions.
Our approach provides an additive decomposition of a performance measure across hyperparameters, enabling local and global explanations of hyperparameter importance and interactions. 
The framework, named \tool, offers insights into \emph{ablations}, the \emph{tunability} of learning algorithms, and \emph{optimizer behavior} across different hyperparameter spaces.
We evaluate \tool on various HPO benchmarks by analyzing the interaction structure of the HPO problem.
Our results show that while higher-order interactions exist, most performance improvements can be explained by focusing on lower-order representations.
\end{abstract}

\section{Introduction}

\begin{figure*}[t]
    \begin{minipage}[c]{\textwidth}
    \centering
    \includegraphics[width=.94\textwidth]{images/main/hypershap.png}
    \end{minipage}
    \caption{Game-theoretic explanations as defined with \tool's hyperparameter importance games can be used to gain insights into hyperparameter values, hyperparameter configuration spaces, datasets, and different hyperparameter optimizers. \tool can be used for data-specific explanations or explanations across datasets.}
    \label{fig_intro_illustration}
\end{figure*}

% HPO is important
Hyperparameter optimization (HPO) is an important step in the design process of machine learning (ML) applications to achieve optimal performance for a given dataset and performance measure \citep{snoek-icml14a,DBLP:journals/widm/BischlBLPRCTUBBDL23}. This is particularly true for deep learning, where hyperparameters describe the neural architecture and steer the learning behavior, e.g., via the learning rate \citep{zimmer-tpami21a}. Also, in the age of generative AI and fine-tuning of large foundation models, HPO is key for achieving state-of-the-art results \citep{DBLP:conf/acl/YinCSJCL20,llmFinetuningHPO,DBLP:conf/automl/00010A23}.

% HPO is hard and needs AutoML which is a black-box and prodces hard-to understand output
Hyperparameters affect the generalization performance of models in varied ways, with some having a more significant impact on tuning than others~\citep{Bergstra-jair12a,fANOVA,zimmer-tpami21a}. The influence of hyperparameters on generalization performance is highly context-dependent, varying with the dataset characteristics (e.g., size, noise level) and the specific performance measure being optimized (e.g., accuracy, F1 score)~\citep{Bergstra-jair12a,DBLP:conf/kdd/RijnH18}. 
%As a result, it is difficult to establish general rankings of hyperparameter importance or predict their influence on learning algorithms across diverse tasks.
This complexity makes HPO particularly challenging, requiring opaque automated machine learning (AutoML) systems to find optimal configurations within large search spaces \cite{feurer-nips15a,wever-tpami21a}. Yet, even after arriving at an optimized hyperparameter configuration, understanding \textit{why it outperforms other configurations} remains difficult due to intricate effects and interactions among hyperparameters.

% we need interpretability for automl and HPO
Despite their promise, AutoML systems have not fully permeated user groups such as domain experts, ML practitioners, and ML researchers
%, limiting their potential impact
\cite{Lee2020,Bouthillier2020,Hasebrook2023,Simon2023}. This lack of adoption stems, in part, from the rigidity of many AutoML systems, which are often difficult to adapt to specific use cases, but is also attributed to the lack of transparency and interpretability \cite{Wang2019a,Drozdal2020}.
%, which serves as a critical source of trust in AutoML systems
Studies highlight that a concrete requirement often requested by AutoML system users is interpretability \cite{Wang2019a,Xin2021,Hasebrook2023,Sun2023}, and its lack has even led users to favor manual development for high-stakes projects \cite{Xin2021}. For ML researchers, explanations of HPO processes are particularly relevant, as they often prioritize understanding the effectiveness of individual ML components and require control over key aspects of model behavior. Similarly, AutoML researchers 
% benefit from tools that provide 
need such kind of information to analyze AutoML systems' performance and behavior. 
Prior works on hyperparameter importance analysis \citep{fANOVA,DBLP:conf/ijcai/WatanabeBH23,moo-fANOVA} and hyperparameter effects~\citep{DBLP:conf/nips/MoosbauerHCLB21,DBLP:conf/automl/SegelGTBL23} show that addressing these interpretability gaps is critical for building trust and enabling more effective use of AutoML systems in a synergetic way with ML experts and data scientists~\citep{LindauerKKMT0HF24}.

%While existing methods for hyperparameter importance analysis, such as functional ANOVA (fANOVA) \citep{fANOVA,DBLP:conf/ijcai/WatanabeBH23,moo-fANOVA} and local parameter importance (LPI) \cite{DBLP:conf/aaai/BiedenkappLEHFH17}, provide valuable insights, their application is limited. fANOVA decomposes the variance in performance into contributions from individual hyperparameters and their interactions. While effective for detecting main effects and low-order interactions, it struggles with correlated hyperparameters and complex dependencies. LPI, on the other hand, focuses on local perturbations around a default configuration, providing limited insight into global behavior or interactions.% Both approaches fall short in providing explanations that are intuitive, fair, and robust to correlation.

\subsection{Contribution}
In this paper, we formalize \tool, a post-hoc explanation framework for hyperparameter importance:% using Shapley values \citep{Shapley.1953} and Shapley interactions \citep{Bord.2023,Tsai.2022}
\begin{enumerate}[itemsep=0em,topsep=-0.25em] % uncomment for latex black magic to reduce waste of space
    \item[\textbf{(1)}] We define a comprehensive set of \numgames explanation games and interpret them using the Shapley value and interactions on three levels: specific configurations, hyperparameter spaces, and optimizer bias.
    \item[\textbf{(2)}] With \tool, we elicit hyperparameter importance and interaction structures for various benchmarks, observing the existence of higher-order interactions. %However, considering only lower-order representations is typically sufficient to explain most of the performance improvements.
    \item[\textbf{(3)}] We apply \tool to various explanation tasks and demonstrate its versatility.%, transferring insights to a subsequent hyperparameter optimization task
\end{enumerate}

\subsection{Related Work}\label{sec:related-work}

Hyperparameter importance (HPI) has gained significant attention in machine learning due to its crucial role in justifying the need for HPO and in attributing performance improvements to specific hyperparameters \citep{tunability,DBLP:conf/gecco/PushakH20a,DBLP:journals/telo/PushakH22,DBLP:conf/ppsn/SchneiderSPBTK22}.
A variety of approaches have been developed to assess how different hyperparameters affect the performance of resulting models, ranging from simple (surrogate-based) ablations \citep{Fawcett-jh16a,DBLP:conf/aaai/BiedenkappLEHFH17} to sensitivity analyses and eliciting interactions between hyperparameters based on the functional ANOVA framework \citep{fANOVA,DBLP:conf/kdd/RijnH18,elShawiTuneOrNotTune,DBLP:conf/ijcai/WatanabeBH23}. 
In this work, we propose a novel approach to quantifying HPI using Shapley values, with a particular focus on capturing interactions between hyperparameters through Shapley interaction indices. We focus on quantifying interactions since, in \citep{zimmer-tpami21a,DBLP:journals/telo/PushakH22,DBLP:journals/jscic/NovelloPLC23}, it has been noticed that interaction is occasionally comparably low, which could serve as a foundation for a new generation of HPO methods that do not assume interactions to be omnipresent.

Beyond quantifying HPI, to better understand the impact of hyperparameters and the tuning behavior of hyperparameter optimizers, other approaches have been proposed, such as algorithm footprints \citep{DBLP:conf/cec/Smith-MilesT12}, partial dependence plots for hyperparameter effects~\cite{DBLP:conf/nips/MoosbauerHCLB21} or deriving symbolic explanations \citep{DBLP:conf/automl/SegelGTBL23}, providing an interpretable model for estimating the performance of a learner from its hyperparameters. In this work, we focus on quantifying the impact of tuning a hyperparameter on the performance.


%In this paper, we propose \tool, a post-hoc explanation framework for analyzing hyperparameter importance and interactions using Shapley values \citep{Shapley.1953} and Shapley interaction indices \citep{Tsai.2022}. Stemming from the field of algorithmic game theory, Shapley values provide an additive decomposition of a given value function -- in our case, a performance measure -- across a set of players, which in this context represent hyperparameters. Unlike existing methods, Shapley values naturally handle correlations among hyperparameters and fairly distribute importance by considering all possible subsets of hyperparameters.

% Overall, we define \numgames games to quantify the importance of hyperparameters, each of which can be used to obtain different types of explanations either of a given hyperparameter configuration, of a hyperparameter search space, or an optimizer's characteristics.

%While we generate first insights with the help of our framework, we also showcase the usefulness of the explanations in a downstream task, performing hyperparameter optimization for the same dataset and performance measure as a proof of concept that our approach identifies, in fact, meaningful importances and interactions. In our experiments, focusing on hyperparameters identified as important with a low degree of interactions by \tool proves beneficial, resulting in better anytime performance. However, ignoring the presence of interactions deteriorates performance in turn. All in all, it provides evidence for the soundness and usefulness of \tool's explanations.


\section{Hyperparameter Optimization}
Hyperparameter optimization (HPO) is concerned with the problem of finding the most suitable hyperparameter configuration (HPC) of a learner for a given task, typically consisting of some labeled dataset $D$ and some performance measure $\perf$ quantifying the usefulness \citep{DBLP:journals/widm/BischlBLPRCTUBBDL23}.
To put it more formally, let $\cX$ be an instance space and $\cY$ a label space and suppose $x\in \cX$ are (non-deterministically) associated with labels $y \in \cY$ via a joint probability distribution $P(\cdot\, ,\, \cdot)$.
Then, a dataset $D=\{ (x^{(k)}, y^{(k)}) \}_{k=1}^N \subset \cX \times \cY$ is a sample from that probability distribution.
Furthermore, a predictive performance measure $\perf: \cY \times P(\cY) \rightarrow \R$ is a function mapping tuples consisting of a label and a probability distribution over the label space to the reals.
Given an HPC $\conf \in \confs$, a learner parameterized with $\conf$ maps datasets $D$ from the dataset space $\mathbb{D}$ to a corresponding hypothesis $h_{\conf, D} \in \mathcal{H} := \{ h \mid h: \cX \rightarrow P(\cY) \}$.

As an HPC $\conf\in\confs$ typically affects the hypothesis space $\mathcal{H}$ and the learning behavior, it needs to be tuned to the given dataset and performance measure. The task of HPO is then to find an HPC yielding a hypothesis that generalizes well beyond the data used for training.
For a dataset $D\in\mathbb{D}$, the following optimization problem needs to be solved:
\begin{equation*}
\conf^\ast \in \underset{\conf\in\confs}{\arg\max} \int_{(x, y)\sim P(\cdot, \cdot)} \perf \big(y, h_{\conf, D}(x)\big)\,\,.
\end{equation*}
As the true generalization performance is intractable, it is estimated by splitting the given dataset $D$ into training $D_{T}$ and validation data $D_{V}$. Accordingly, we obtain
\[
\conf^\ast \in \underset{\conf\in\confs}{\arg\max}\, \val(\conf, D), \text{ with } \val(\conf, D) := 
\]
\[
\mathbb{E}_{(D_{T}, D_{V}) \sim D} \left[ \frac{1}{|D_{V}|} \sum_{(x, y) \in D_{V}} \perf \big(y, h_{\conf, D_{T}}(x)\big) \right].
\]

Naively, hyperparameter optimization can be approached by discretizing the domains of hyperparameters and conducting a grid search or by a random search \citep{Bergstra-jair12a}. State-of-the-art methods leverage Bayesian optimization and multi-fidelity optimization for higher efficiency and effectiveness \citep{DBLP:journals/widm/BischlBLPRCTUBBDL23}.

% \paragraph{Hyperparameter Importance (HPI).} 

%From an intuitive perspective, it is quite obvious that different types of hyperparameters can be of different importance and that their effect also depends on the dataset at hand. However, 

% Determining the effect of every hyperparameter requires additional tools, e.g., eliciting the individual importance of a hyperparameter via ablations \citep{DBLP:conf/aaai/BiedenkappLEHFH17}. 
% Typically, the importance of a single hyperparameter is specified by its performance-induced variance \citep{hpiForMLA}.
% Since hyperparameters may also influence the effect of other hyperparameters, \citet{fANOVA} introduced the \emph{marginal performance} given any subset of hyperparameters, and decomposes the variance in the functional ANOVA framework.

%Additionally, hyperparameters may also influence the effect of other hyperparameters, which is why the importance of a hyperparameter $\lambda_j$ is typically specified as its performance-induced variance \citep{hpiForMLA}.
%:
%\todo{Fill the definition of hyperparameter importance here}
%\[
% \text{HPI}(\lambda_j) = \int_{\Lambda_j} \text{Var}(\mathbb{E}_{\conf \in \confs \setminus \Lambda_j} \left[ u(\conf || \lambda_j, D) \right])
%\text{HPI}(\lambda_j, D) = \int_{\Lambda_j} \int_{\confs\setminus\Lambda_j} \mathbb{E}\big[ (\perf(\conf||\lambda_j, D) - \mathbb{E} \left[ \perf(\conf||\lambda_j, D) \right])^2 \big] dP_{\confs\setminus \Lambda_j} \, dP_{\Lambda_j}
%\]
%Accordingly, computing marginals for quantifying the effect of single hyperparameters as well as their interactions with other hyperparameters can be used to determine the importance of hyperparameters in the functional ANOVA framework \citep{fANOVA}.


\section{Explainable AI (XAI) and Game Theory}\label{sec:background-shapley}

Within the field of eXplainable AI (XAI), cooperative game theory has been widely applied to assign contributions to entities, such as features or data points for a given task \citep{DBLP:conf/ijcai/RozemberczkiWBY22}.
Most prominently, to interpret predictions of black box models using feature attributions \citep{Lundberg.2017} and the \gls*{SV} \citep{Shapley.1953}.
\glspl*{SI} \citep{Grabisch.1999} extend the \gls*{SV} by additionally assigning contributions to groups of entities, which reveal \emph{synergies and redundancies}.
Such feature interactions uncover additive structures in predictions, which are necessary to understand complex decisions \citep{Sundararajan.2020}.
Overall, explanations are summarized by two components \citep{Fumagalli.2024a}: First, an \emph{explanation game} $\nu: 2^{\mathcal{N}} \to \mathbb{R}$ is defined as a real-valued set function over the powerset $2^{\mathcal N}$ of the $n$ features of interest indexed by $\mathcal N = \{1,\dots,n\}$.
This explanation game restricts the model prediction to subsets of features and evaluates a property of interest, e.g. the prediction or performance.
Second, given an explanation game, \emph{interpretable} main and interaction effects are constructed using the \gls*{SV} and \glspl*{SI}.
Analogously, in \cref{sec:hpi-games}, we define explanation games based on ablations of hyperparameters in $\val$ and quantify importances with the \gls*{SV} and \glspl*{SI}.

\paragraph{Explanation Games via Feature Imputations.}
Given the prediction of a black box model $f: \mathbb{R}^n \to \mathbb{R}$ and an instance $\mathbf{x} \in \mathbb{R}^n$, \emph{baseline imputation} with $b \in \mathbb{R}^n$ for a coalition $S \subseteq \mathcal N$ is given by $\oplus_S: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}^n$ as
\begin{align*}
    \nu_{\mathbf{x}}^{(b)}(S) := f(\mathbf{x} \oplus_S \mathbf{b}) \text{ with } \mathbf{x} \oplus_S \mathbf{b} := \begin{cases}
        x_i, &\text{ if } i \in S \, \, , \\
        b_i, &\text{ if } i \notin S \, \, .
    \end{cases}
\end{align*}

Baseline imputation heavily depends on the choice of baseline \citep{sturmfels2020visualizing}, and \emph{marginal} and \emph{conditional} imputation extends the approach to an average over randomized baselines \cite{Sundararajan.2020b} as 
\begin{align*}
   \nu_{\mathbf{x}}^{(p)}(S) := \mathbb{E}_{\mathbf{b} \sim p(\mathbf{b})}[f(\mathbf{x} \oplus_S \mathbf{b})],
\end{align*}
where $p(\mathbf{b})$ is either the marginal feature distribution or conditioned on $\mathbf{b} = \mathbf{x} \oplus_S \mathbf{b}$, respectively.
Imputed model predictions define \emph{local} explanation games that are used to explain predictions of single instances $\mathbf{x}$.
In contrast, \emph{global} explanation games capture derived properties, such as the variance or performance of the imputed model prediction.
It was shown that explanations derived from baseline, marginal and conditional imputation are increasingly influenced by the feature distribution $p$ \citep{Fumagalli.2024a}.

\paragraph{Shapley Value (SV) and Shapley Interaction (SI).}
An explanation game is additively decomposed by the \glspl*{MI} $m:2^{\mathcal N} \to \mathbb{R}$ \citep{Muschalik.2024}, i.e. the Möbius transform \citep{rota1964foundations}, for $T \subseteq \mathcal N$ as

\begin{align*}
     \nu(T) = \sum_{S \subseteq T} m(S) \text{ with } m(S) := \sum_{L \subseteq S} (-1)^{\vert S \vert - \vert L \vert} \nu(L) \, \, .
\end{align*}
The \glspl*{MI} describe the \emph{pure main and interaction effects}, but contain $2^n$ non-trivial components in ML applications \citep{Muschalik.2024}.
As a remedy, the \gls*{SV} and \glspl*{SI} summarize the \glspl*{MI} into \emph{interpretable} main and interaction effects of lower complexity.
The \gls*{SV} assigns contributions to individuals and is uniquely characterized by four intuitive axioms: \emph{linearity} (contributions are linear for linear combination of games), \emph{symmetry} (players with equal contributions obtain equal payout), \emph{dummy} (players that do not change the payout receive zero payout), and \emph{efficiency} (the sum of all payouts equals the joint payout).
The \gls*{SV} summarizes the \glspl*{MI} as $\phi^{\text{SV}}(i) = \sum_{S \subseteq \mathcal{N}: i \in S} \frac{1}{\vert S \vert} m(S)$ for all $i \in \mathcal{N}$.
In other words, each \gls*{MI} is equally distributed among the involved players.
Yet, the \gls*{SV} does not yield any insights into interactions.
Given an \emph{explanation order} $k \in \{1,\dots,n\}$, the \glspl*{SI} $\Phi_k$ extend the \gls*{SV} to assign contributions to subsets of players up to size $k$.
For $k=1$ the \glspl*{SI} yield the \gls*{SV}, whereas for $k=n$ the \glspl*{MI}.
While there exist multiple variants of \glspl*{SI}, a positive interaction indicates a synergistic effect, whereas a negative interaction indicates redundancies (on average) of the involved features.
For instance, the \gls*{FSII} \citep{Tsai.2022} is defined as the best $k$-additive approximation $\hat{\nu}_k(S) := \sum_{L \subseteq S:\vert L\vert \leq k}{\Phi_k(L)}$ of $\nu(S)$ across all subsets $S$ weighted by the Shapley kernel, cf. \cref{appx_sec_fsii}, which is useful to analyze the degree of interaction.
The \glspl{SI} adjust \edit{explanation expressivity and complexity based on practitioner needs}, a framework we now parallel in HPO.


\section{\tool: Attributing Importance to Hyperparameters}\label{sec:hpi-games}


In hyperparameter optimization (HPO), 
%a wide variety of questions can be asked for explanations, ranging from individual suggested values in a returned hyperparameter configuration to complex reasoning during the optimization process or the description of remaining optimization potentials. Hence, 
explanations are needed on different levels of the HPO process, ranging from returned configurations to a qualitative comparison of entire HPO tools. 
Here, we limit ourselves to four areas, dubbed \ablation, \sensitivity, \dstunability, and \opthabit.
First, we introduce \ablation (\cref{sec:ablation}), which we use as the fundamental backbone of \tool. 
Based on \ablation, we discuss \sensitivity as an extension of the functional ANOVA framework by \citet{fANOVA}, and compare it theoretically to our novel approach for \dstunability (\cref{sec:sensitivity-tunability}).
\dstunability is then used to discover \opthabit (\cref{sec:opthabit}), and we conclude with practical aspects of \tool (\cref{sec:hypershap-in-practice}).
In the following, we let $\mathcal N$ be the set of hyperparameters and quantify main and interaction effects based on the \gls*{SV} and \glspl*{SI} of the HPI games.
All proofs are deferred to \cref{appx_sec_proofs_notation}.

\subsection{Ablation of Hyperparameter Configurations}\label{sec:ablation}

One common scenario for quantifying the HPI is to compare a hyperparameter configuration (HPC) $\conf^\ast$ of interest to some reference HPC $\conf^0$, e.g., the default parameterization of a learner as provided by its implementing library or a tuned default HPC that has proven effective for past tasks. In turn, $\conf^\ast$ can be any HPC obtained through HPO or a manual configuration. Given $\conf^\ast$ and $\conf^0$, the question now is how values of $\conf^\ast$ affect the performance of the learner relative to the reference HPC $\conf^0$. To this end, we can transition from the reference HPC to the HPC of interest by switching the values of hyperparameters one by one from its value in $\conf^0$ to the value in $\conf^\ast$, which is also done in empirical ML studies and referred to as ablations.

Ablation analysis has already been followed by \citet{Fawcett-jh16a} and \citet{DBLP:conf/aaai/BiedenkappLEHFH17} but restricted to single hyperparameter ablations that are executed sequentially.
Instead, we consider the HPI game of \ablation using \emph{all possible subsets}, which allows us to capture interactions.




\begin{definition}[HPI Game - \ablation]
The \ablation HPI game $\nu_{G_A}: 2^{\mathcal N} \to \mathbb{R}$ is defined based on a tuple 
\[ 
G_A := (\conf^0, \conf^\ast, D, \perf),
\]
consisting of a \emph{baseline (default)} HPC $\conf^0$,
an HPC of interest $\conf^\ast$, a dataset $D$, and a measure $u$.
Given a coalition $S\subseteq \mathcal{N}$, we construct an intermediate HPC with $\oplus_S: \confs \times \confs \to \confs$ as
\[
\conf^* \oplus_S \conf^0 := \begin{cases}
\lambda^\ast_i, & \text{ if } i \in S,\\
\lambda^0_i, & \text{ else},
\end{cases}
\]
and evaluate its worth with
\[
\nu_{G_A}(S) := \val(\conf^\ast \oplus_S \conf^0, D) \,\, .
\]
\end{definition}


The \ablation game quantifies the worth of a coalition based on the comparison with a default HPC $\conf^0$.
In XAI terminology, this approach is known as \emph{baseline imputation}, cf. \cref{sec:background-shapley}.
Natural extensions of the \ablation game capture these ablations with respect to a distribution $\conf^0 \sim p^0(\conf^0)$ over the baseline HPC space $\confs$ as
\[
\mathbb{E}_{\conf^0 \sim p^0(\conf^0)}[ \val(\conf^\ast \oplus_S \conf^0, D)] \,\, ,
\]
which relates to the \emph{marginal performance} introduced by \citet{fANOVA}.
In XAI terminology, it is further distinguished between distributions $p(\conf^0)$ that either depend (conditional) or do not depend (marginal) on the HPC of interest $\conf^\ast$.
While baseline imputation is mostly chosen for computational efficiency, it was argued that it also satisfies beneficial properties \citep{Sundararajan.2020b}.
Nevertheless, the choice of a baseline has a strong impact on the explanation \citep{sturmfels2020visualizing}.
In HPO, we are typically given a \emph{default} HPC $\conf^0$, which we use for the \ablation game, but our methodology can be directly extended to the probabilistic setting.


\subsection{Tunability of Learners}\label{sec:sensitivity-tunability}
Zooming out from a specific configuration, we can ask to what extent it is worthwhile to tune hyperparameters. 
In the literature, this question has been connected to the term of \textit{tunability} \citep{tunability}. 
\dstunability aims to quantify how much performance improvements can be obtained by tuning a learner comparing against a baseline HPC, e.g., an HPC that is known to work well across various datasets \citep{DBLP:conf/gecco/PushakH20a}.
In this context, we are interested in the importance of tuning specific hyperparameters.
A classical tool to quantify variable importance is \emph{sensitivity analysis} \citep{Owen_2013}, which measures the variance induced by the variables of interest and decomposes their contributions into main and interaction effects.

\begin{definition}[HPI Game - \sensitivity]
The \sensitivity game $\nu_{G_V}: 2^{\mathcal N} \to \mathbb{R}$ is defined based on a tuple
\[
    G_V := (\conf^0, \confs, p^*, D, \perf),
\]
consisting of a \emph{baseline} HPC $\conf^0$, an HPC space of interest $\confs$ equipped with a probability distribution $p^*$, a dataset $D$, and a measure $u$. 
The value function is given by
\[
    \nu_{G_V}(S) := \mathbb{V}_{\conf \sim p^\ast(\conf)}[\val(\conf \oplus_S \conf^0, D)] \,\, .
\] 
\end{definition}

A large value of a coalition $S \subseteq \mathcal N$ in the \sensitivity game indicates that these hyperparameters are important to be set to the right value.
\citet{fANOVA} implicitly rely on the \sensitivity game and compute the functional ANOVA decomposition, quantifying \emph{pure} main and interaction effects.
In game theory, this corresponds to the \glspl*{MI} of the \sensitivity game,%, and we show in our experiments that this representation consists of $2^n$ non-trivial terms.
which can be summarized into interpretable representations using the \gls*{SV} and \glspl*{SI} \citep{Fumagalli.2024a}.

While sensitivity analysis is a suitable tool in XAI, it has some drawbacks for \dstunability.
First, as illustrated below, the total variance being decomposed $\nu_{G_V}(\mathcal N)$ strongly depends on the chosen probability distribution $p^*$ and the HPC space $\confs$.
Moreover, it does not reflect the performance increase expected when tuning all hyperparameters.
Second, for a coalition of hyperparameters $S \subseteq \mathcal N$, we expect that the coalition's worth (performance) increases when tuning additional hyperparameters, i.e., $\nu(S) \leq  \nu(T)$, if $S \subseteq T$.
This property is known as \emph{monotonicity} \citep{Fujimoto.2006}, but \emph{does not} hold in general for the \sensitivity game $\nu_{G_V}$.
For a simple example, we refer to \cref{appx_sec_non_monotone_example}.
Instead, we now propose the monotone \dstunability HPI game.

\begin{definition}[HPI Game -  \dstunability]\label{def:tunability}
The \dstunability HPI game is defined by a tuple 
\[
G_T = (\conf^0, \confs, D, \perf),
\]
consisting of a baseline HPC $\conf^0 \in \confs$, an HPC space $\confs$, a dataset $D$, and a measure $u$. The value function is given by
\[
\nu_{G_T}(S) := \max_{\conf \in \confs}
\,\val(\conf \oplus_S \conf^0, D) \,\, .
\]
\end{definition}

The \dstunability game directly measures the performance obtained from tuning the hyperparameters of a coalition $S$ while leaving the remaining hyperparameters at the default value $\conf^0$.
The \dstunability game is monotone, which yields the following theorem.

\begin{theorem}
    The \dstunability game yields non-negative \glspl*{SV} and non-negative \emph{pure} individual (main) effects obtained from functional ANOVA via the \glspl*{MI}.
\end{theorem}

While the main effects obtained from the \dstunability game are non-negative, interactions clearly can be negative, indicating redundancies of the involved hyperparameters.

\paragraph{Benefits of Tunability over Sensitivity.}
We now showcase the benefits of the \dstunability game over the \sensitivity game using a synthetic example.
We consider a two-dimensional HPC space $\confs := \Lambda_1 \times \Lambda_2$ with discrete HPCs $\Lambda_1 := \{0,1\}$ and $\Lambda_2:= \{0,\dots,m\}$ for $m > 1$.
The optimal configuration is defined as $\conf^\ast := (1,m)$, and the performance is quantified by $\val(\conf,D) := \mathbf{1}_{\lambda_1=\lambda_1^\ast}+\mathbf{1}_{\lambda_2 = \lambda^\ast_2}$, where $\mathbf{1}$ is the indicator function.
That is, we observe an increase of performance of $1$ for each of the hyperparameters set to the optimal HPC $\conf^\ast$.
Lastly, we set the HPC baseline to $\conf^0 := (0,0)$ or $\conf^0 := \conf^\ast$.
Intuitively, we expect that both hyperparameters obtain similar HPI scores, since they both contribute equally to the optimal performance $\val(\conf^\ast, D) = 2$.
Moreover, if the baseline is set to the optimal HPC $\conf^\ast$, we expect the HPI to reflect that there is no benefit of tuning.
Since the hyperparameters independently affect the performance, we do not expect any interactions.

\begin{theorem}
    The HPI scores of the \sensitivity and \dstunability game for the synthetic example are given by \cref{tab:synthetic_example}.
\end{theorem}

Both HPI scores correctly quantify the absence of interaction $\lambda_1 \times \lambda_2$.
In contrast to the \dstunability game, the \sensitivity game assigns smaller HPI scores to the hyperparameter $\lambda_2$ due to the larger domain $\Lambda_2$. 
In fact, the \sensitivity HPI score of $\lambda_2$ roughly decreases with order $m^{-1}$.
Moreover, the \dstunability HPI scores reflect the performance increase and decompose the difference between the optimal and the default performance.
In contrast, the \sensitivity HPI scores decompose the overall variance, which depends on $\confs$ and $p^*$.
Lastly, setting the default HPC $\conf^0$ to $\conf^\ast$ decreases the \dstunability HPI scores to zero, whereas the \sensitivity HPI scores remain unaffected.
In summary, \sensitivity reflects the variability in performance when changing the hyperparameters, whereas \dstunability reflects the benefit of tuning.

\begin{table}[t]
    \centering
    \caption{HPI main and interaction effects of a two-dimensional synthetic HPO problem for the \sensitivity and \dstunability game with baseline HPC set to $(0,0)$ and the optimal HPC $\conf^\ast$. The \sensitivity game assigns smaller contributions to hyperparameters with a larger domain ($\lambda_2)$. Setting $\conf^0 = \conf^*$ reduces the \dstunability HPI scores to zero, whereas \sensitivity is unaffected.}
    \vspace{1em}
    \label{tab:synthetic_example}
    \begin{tabular}{cc|cccc}
    \toprule
     \multicolumn{2}{c|}{\textbf{HPI Game}} & \multicolumn{2}{c}{\textbf{Sensitivity}}  & \multicolumn{2}{c}{\textbf{\dstunability}} \\ 
     \multicolumn{2}{c|}{$\conf^0$} & $(0,0)$ & $\conf^\ast$   & $(0,0)$ & $\conf^\ast$ \\ \midrule
      \multirow{3}{*}{\textbf{HPI}} & $\lambda_1$ & $1/4$ & $1/4$  & 1 & 0 \\
      &$\lambda_2$  & $\frac{m}{(m+1)^2}$ & $\frac{m}{(m+1)^2}$  & 1 & 0 \\
      &$\lambda_1 \times \lambda_2$ & 0 & 0 & 0 & 0 \\\bottomrule
    \end{tabular}
\end{table}


\subsection{Optimizer Bias}\label{sec:opthabit}
The \dstunability game aims to explain the importance of hyperparameters being tuned, which can also be used to gain insights into the capabilities of a hyperparameter optimizer.
In particular, by comparing the optimal performance with the empirical performance of a single optimizer, we uncover biases and point to specific hyperparameters that the optimizer fails to exploit.
%However, depending on how much a hyperparameter may contribute to a performance gain can also be used to gain insights into the capabilities of a hyperparameter optimizer. More specifically, we would like to investigate whether a hyperparameter optimizer may fail to exploit certain hyperparameters.
%As shown in \cref{fig:hpo-quality}, partial contributions of main effects and interactions to the overall performance vary depending on the approximation quality of $\max_{\conf \in \confs}$. 
%As a consequence, for high quality explanations, we need sufficiently accurate approximations. 
%In turn, we may leverage this fact to detect deficiencies of optimizers.
We define a hyperparameter optimizer as a function $\mathcal{O}: \mathbb{D} \times 2^{\confs} \rightarrow \confs$, mapping from the space of datasets and an HPC space to an HPC. 

\begin{definition}[HPI Game - \opthabit] \label{def:opthabit}
    The \opthabit HPI game is defined as a tuple 
    \[
    G_O = (\confs, \conf^0, \mathcal{O}, D, u),
    \]
    consisting of a HPC space $\confs$, a baseline $\conf^0$, the hyperparameter optimizer of interest $\mathcal O$, a dataset $D$ and a measure $u$.
    For $S \subseteq \mathcal N$, we construct $\confs^S := \{ \conf \oplus_S \conf^0: \conf \in \confs\}$ and define
        \begin{align*}
    \nu_{G_0}(S) := \val \Big(\mathcal{O}(D, \confs^S), D \Big)
    - \nu_{G_T}(S) \,\, .
    \end{align*}
    % \[
    % \nu(S) = \oplus_{i = 1}^M \left[ \val \left(\mathcal{O}(D_i, \confs^S), D_i \right) - \val \left(\underset{\conf \in \confs^S}{\arg\max}\, \val(\conf, D_i), D_i \right) \right]\,\, .
    % \]
\end{definition}

Intuitively speaking, the value function measures any deviation from the performance of the actual best-performing HPC. 
In other words, with the help of Definition~\ref{def:opthabit}, we can identify deficiencies of the hyperparameter optimizer $\mathcal{O}$ over the best-performing solution and, thereby for example, identify whether an optimizer struggles to optimize certain (types of) hyperparameters.

\subsection{Practical Aspects of \tool}\label{sec:hypershap-in-practice}
This section addresses practical aspects of \tool to efficiently approximate the proposed games and generalize them to multiple datasets.

\paragraph{Efficient Approximation.}
Naively, to evaluate a single coalition in Definition~\ref{def:tunability}, we need to conduct one HPO run.
While this can be costly, we argue that using surrogate models that are, e.g., obtained through Bayesian optimization, can be used to simulate HPO, rendering \tool more tractable. This is similar to other surrogate-based explainability methods for HPO~\cite{fANOVA,DBLP:conf/aaai/BiedenkappLEHFH17,DBLP:conf/nips/MoosbauerHCLB21,DBLP:conf/automl/SegelGTBL23}.
In contrast, to analyze \opthabit, we propose to approximate $\nu_{G_T}$ using a diverse ensemble of optimizers $\mathbb{O}:= \{ \mathcal{O}_i\}$, and choose the best result for $\confs^S$ obtained through any optimizer from $\mathbb{O}$, forming a virtual optimizer that always yields the best-known value.
This virtual best hyperparameter optimizer (VBO) approximates
\[
\nu_{G_T}(S) \approx \underset{\conf^i = \mathcal{O}_i(D, \confs^S)}{\max} \val(\conf^i, D) \,\, .
\]


\paragraph{HPI Game Extensions across Multiple Datasets.} 
In a more general setting, we are also interested in the different aspects of \tool across multiple datasets, where we present direct extensions of the previous games.
\begin{definition}[HPI Game- Multi-Dataset Variants]
Given a collection of datasets $\mathcal D := \{D_1, \dots, D_M\}$ and the corresponding HPI games $\nu^{D_i}_{G}$ for $i \in \{1,\dots,M\}$ with $G \in \{G_A,G_V,G_T,G_O\}$, we define its multi-dataset variant with the value function
\begin{align*}
    \nu^{\mathcal D}_{G}(S) := \bigoplus_{i = 1 }^M \nu^{D_i}_G(S),
\end{align*}
where $\bigoplus$ denotes an aggregation operator, e.g. the mean, of the game values obtained from the individual datasets $D_i$.
\end{definition}

Considering HPI across datasets allows for a broader and more comprehensive assessment of the impact of individual hyperparameters and how they interact with each other. By aggregating the value of a coalition over datasets, we can evaluate the generalizability of HPI in contrast to identifying which hyperparameter is important for which dataset. For instance, this can justify recommendations with respect to \tunability, which hyperparameters should be tuned in general. Alternatively, we can assure a systematic optimizer bias with respect to certain hyperparameters instead of observing data-specific effects only.


\begin{figure}
    \centering
    \begin{minipage}[c]{0.49\columnwidth}
        \includegraphics[width=\textwidth]{images/main/exp_a/ablation_upset.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.49\columnwidth}
        \includegraphics[width=\textwidth]{images/main/exp_a/tunability_upset.pdf}
    \end{minipage}
    \caption{Upset plots \citep{Lex2014UpSet} showing the hyperparameter importances for \ablation (left) and \dstunability (right) wrt.~\texttt{lm1b\_transformer} from \pdone \cite{wang-jmlr24}.}
    \label{fig_exp_ablation_tunability}
\end{figure}

\section{Experiments}\label{sec_experiments}

In this section, we evaluate the effectiveness and applicability of \tool across various scenarios. The experiments demonstrate how our games help explain HPIs, interactions, and biases in HPO. 
In all experiments we rely on four HPO benchmarks; \lcbench \citep{zimmer-tpami21a}, \rbvranger \citep{DBLP:conf/automl/PfistererSMBB22}, \pdone \citep{wang-jmlr24}, and \jahs \citep{bansal-neurips22a}. The implementation is based around \texttt{shapiq} \cite{Muschalik.2024}. For more details regarding the experimental setup, we refer to Appendix~\ref{app_experiment-setup}. Additional results are contained in \cref{appx_sec_additional_results} and a guide for interpreting interaction-based visualizations is presented in \cref{appx_guidance}. Generally, positive interactions are colored in red and negative in blue.


\begin{figure*}[t]
    \centering
    \begin{minipage}[c]{0.6\linewidth}
    \includegraphics[width=0.92\textwidth]{images/main/interaction_quantification/complexity.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.39\linewidth}
    \includegraphics[width=\linewidth]{images/main/interaction_quantification/faithfulness.pdf}
    \end{minipage}
    \caption{\textbf{Left:} Interaction graphs showing Möbius interactions (MI), Shapley interactions (SI), and Shapley values (SV) where pure MIs are summarized for improved interpretability via SIs and SVs. \textbf{Right:} Faithfulness of the lower-order explanations approximating higher-order effects \cite{Muschalik.2024}. An explanation order of 3 already closely approximates the full game ($R^2 \approx 1$).}
    \label{fig_interaction_quantification}
\end{figure*}



\subsection{Insights from Ablation and Tunability}\label{sec_experiments_ablation_tunability}

\begin{figure}[t]
    \centering
    \begin{minipage}[c]{0.49\columnwidth}
        \centering
        \includegraphics[width=.85\textwidth]{images/main/optibias/independent.pdf}
        \\
        \textbf{a)} Individual Tuning
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.49\columnwidth}
        \centering
        \includegraphics[width=.85\textwidth]{images/main/optibias/wd.pdf}
        \\
        \textbf{b)} W-D not Tuned
    \end{minipage}
    \caption{Interaction graphs showing results for the \opthabit game via Moebius interactions (MI) and Shapley interactions (SI) on dataset ID 3945 of \texttt{lcbench}.}
    \label{fig_optbias_games}
\end{figure}

First, we compare the results of the \ablation and the \dstunability game in terms of hyperparameter importance and interactions (cf. Figure~\ref{fig_exp_ablation_tunability}). We retrieve an optimized configuration of \pdone's \texttt{lm1b\_transformer} scenario and explain it with the \ablation game. \tool's explanation shows that the majority of the performance increase is attributed to the hyperparameter L-I, which is not surprising since the initial learning rate is also intuitively the most important one. However, using \tool to create \dstunability explanations reveals that both hyperparameters, L-I and O-M (optimizer momentum), are of equal importance with a negative interaction. This means that the optimizer chose to tune L-I over O-M for the configuration in question, even though a similar performance improvement could have been achieved by tuning O-M instead. Hence, \tool is able to reveal which hyperparameters were subject to optimization via the \ablation game, while the \dstunability game emphasizes the potential contributions of hyperparameters together with the interactions between them.

\subsection{Higher-Order Interactions in HPO}\label{sec:exp-higher-order-interactions}\label{sex_exp_interaction_quantification}
Second, we investigate the interaction structure of the HPO problem. 
In \cref{fig_interaction_quantification}, left (\gls*{MI}), and further in \cref{appx_sec_additional_results}, we observe the presence of many higher-order interactions, which are difficult to interpret.
The \glspl*{SI} (order 2) and \gls*{SV} in \tool summarize the \gls*{MI} into \emph{interpretable} explanations.
Yet, \cref{fig_interaction_quantification}, right, shows that \glspl*{SI} still \emph{faithfully} capture the overall game behavior, which we measure with a Shapley-weighted loss \cite{Muschalik.2024} and varying explanation order (cf. \cref{app:interaction_quantification}). 
We find that most of the explanatory power is captured by interactions up to the third order, confirming prior research that suggests hyperparameter interactions are typically of lower order \cite{DBLP:conf/gecco/PushakH20a}. 
Interactions beyond the third order contribute little to the overall understanding of the game. 
Thanks to the convenient properties of the \gls*{SV} and \glspl*{SI}, \tool provides a reliable way to capture and fairly summarize higher-order interactions into more interpretable explanations.

\begin{figure*}[t]
    \centering
    %\includegraphics[width=\textwidth]{images/main/surrogate_explanation/surrogate.pdf}
    \includegraphics[width=\textwidth]{images/main/surrogate_explanation/surrogate_corrected.pdf}
    \caption{\tool explains the surrogate model in SMAC's Bayesian optimization at 1\%, 5\%, 25\%, and 100\% of the budget.}
    \label{fig_appx_surrogate_explanation}
\end{figure*}

\subsection{Detecting Optimizer Bias}
The third experiment uses the \opthabit game to uncover issues in black-box hyperparameter optimizers. To this end, we create two biased hyperparameter optimizers. The first optimizer tunes each hyperparameter separately, ignoring interactions between them, while the second is not allowed to tune the most important hyperparameter. Ideally, a perfect optimizer would show no interactions and no main effects in \tool's \opthabit explanations. 

\cref{fig_optbias_games} shows the \opthabit explanations, i.e., the difference between two \dstunability games, using the optimizer's returned value and the maximum, respectively. Since the main effects are always positive, the differences highlight the optimizer's inability to properly tune certain hyperparameters. In Figure~\ref{fig_optbias_games}a, small main effects suggest that the optimizer can effectively tune hyperparameters individually, but the presence of both negative and positive interactions shows that it fails to capture these dependencies. This confirms the expectation that this optimizer, which tunes hyperparameters separately, fails to exploit synergies in tuning hyperparameters jointly. On the other hand, the second optimizer, ignoring the weight decay (W-D) hyperparameter for this particular dataset, clearly demonstrates bias in the interaction graph in \cref{fig_optbias_games}b. The blue main effect for W-D and interactions involving W-D reveal this bias, showing how \tool can help identify such flaws and contribute to the development of more effective HPO methods.

\subsection{Explaining the Surrogate During Optimization}

Inspired by \citet{LMU}, we explain SMAC \cite{lindauer-jmlr22a}, a state-of-the-art hyperparameter optimizer based on Bayesian optimization, using \tool to analyze its surrogate model during the optimization procedure. We run SMAC (as pure black-box optimizer) with a budget of around $6\,000$ evaluations and investigate the surrogate model at $1\%, 5\%, 25\%, $ and $100\%$ of the budget. In Figure~\ref{fig_appx_surrogate_explanation}, we show how the model's belief about HPI and interactions evolves. Initially uncertain, indicated by large interactions between hyperparameters, SMAC's surrogate model successfully identifies the most important hyperparameters early on but requires a lot more evaluations to eventually learn the interaction effects correctly. This demonstrates the potential of \tool to better understand what actually happens in HPO.





\begin{figure}[t]
    \centering
    \begin{minipage}[c]{0.49\columnwidth}
        \includegraphics[width=\textwidth]{images/main/fanova_comparison/126026_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.49\columnwidth}
        \includegraphics[width=\textwidth]{images/main/fanova_comparison/126029_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \caption{Anytime performance plots of hyperparameter optimization runs involving only the top-2 important hyperparameters for two datasets of \texttt{lcbench} \cite{zimmer-tpami21a}.}
    \label{fig_fanova_comparison}
\end{figure}


\subsection{Comparison with Functional ANOVA (fANOVA)}
Lastly, we design a validation task for hyperparameters deemed important by \tool, and compare it to the well-known fANOVA framework~\cite{fANOVA}. 
To this end, for a given HPO task, we run fANOVA and \tool using the \gls*{SV} with the \dstunability and \sensitivity game to calculate the HPI of order 1. 
We then select the two most important hyperparameters and conduct an HPO run on a reduced HPC space containing only these two hyperparameters.
We expect that \tool achieves better performance for both the \dstunability and \sensitivity game since the \gls*{SV} additionally reflects higher-order interactions.
Moreover, the hyperparameters chosen based on the \dstunability game are expected to outperform the selection based on the \sensitivity game.
The results shown in \cref{fig_fanova_comparison} (and further in \cref{appx_sec_fanova_comparison}) confirm that the anytime performance of the runs informed by \tool are clearly superior to those informed by fANOVA, and that \dstunability outperforms \sensitivity.
This demonstrates that the HPI scores provided by \tool are effectively contributing to maximizing the accuracy and thus guiding the optimization process. 
Furthermore, the optimization runs informed by fANOVA converge earlier than those of \tool, supporting its capability to provide more actionable insights for HPO.

\section{Conclusion}
In this paper, we proposed \tool, a post-hoc explanation framework for consistently and uniformly quantifying HPI using the \gls*{SV} and \glspl*{SI} across three levels: hyperparameter values, tunability of learners, optimizer capabilities. Unlike previous methods that quantify variance \cite{fANOVA,DBLP:conf/ijcai/WatanabeBH23}, \tool attributes performance contributions. We demonstrated that \tool not only enhances understanding of the impact of hyperparameter values or tunability of learners but also provides actionable insights for subsequent HPO runs.

\paragraph{Limitations and Future Work.}
While \tool reveals interesting insights into HPO, it is not directly clear how to utilize its explanations for the HPO process itself. This offers room for interesting future work as explanations can be computed in parallel and potentially guide the HPO tuner's or AutoML framework's search. Further, the computational bottleneck is the approximation of the $\arg\max$, requiring research on more efficient yet unbiased methods. In future work, we aim to extend the framework to combined algorithm selection and HPO, as well as the design of complete ML pipelines~\citep{OlsonM16,wever-automlicml18a,HeffetzVKR20,FeurerEFLH22}. While these more complex AutoML scenarios are less explored, \tool provides a versatile and theory-grounded approach for in-depth study. Additionally, we plan to develop HPO methods that utilize HPI, with the goal of learning across datasets to improve optimizer efficiency. This may allow warm-starting HPO in an interpretable way, complementing recent work on prior-guided HPO~\citep{HvarfnerHN24} and human-centered AutoML~\citep{LindauerKKMT0HF24}.

\clearpage
\section*{Impact Statement}
In conducting this research on HyperSHAP, we have carefully considered the ethical implications of our work.
This paper presents work with the goal to advance the field of machine learning (ML) and specifically the field of explainable artificial intelligence (XAI) and hyperparameter optimization (HPO).
There are many potential societal consequences of our work.
The aim of our study is to improve the transparency and interpretability of hyperparameter optimization, which is crucial for building trust and accountability into hyperparameter optimization methods.
Thus, our research impacts a wide variety of ML application domains and therein can positively impact ML adoption and potentially reveal biases or unwanted behavior in HPO systems.

However, we recognize that the increased explainability provided by XAI also carries ethical risks. 
There is the potential for ``explainability-based white-washing'', where organizations, firms, or institutions might misuse XAI to justify questionable actions or outcomes.
With responsible use, XAI can amplify the positive impacts of ML, ensuring its benefits are realized while minimizing harm.

\bibliography{references.bib}
\bibliographystyle{icml2025}

\clearpage
\appendix
\onecolumn

\section*{Organization of the Supplemental Material}

The technical supplement is organized as follows.

% A
\contentsline {section}{\numberline{A}Proofs}{14}{}
\contentsline {subsection}{\numberline{A.1}Proof of Theorem 4.4}{14}{}
\contentsline {subsection}{\numberline{A.2}Proof of Theorem 4.5}{14}{}
\contentsline {subsection}{\numberline{A.3}Example: Non-Monotone Sensitivity Game}{15}{}
% B
\contentsline {section}{\numberline{B}Experimental Setup}{16}{}
\contentsline {subsection}{\numberline{B.1}Considered Benchmarks}{16}{}
\contentsline {subsection}{\numberline{B.2}Approximation of the argmax}{17}{}
\contentsline {subsection}{\numberline{B.3}Computing Optimizer Bias}{17}{}
\contentsline {subsection}{\numberline{B.4}Hardware Usage and Compute Resources}{18}{}
% C
\contentsline {section}{\numberline{C}Guidance on Interpreting Interaction Visualizations}{19}{}
% D
\contentsline {section}{\numberline{D}Interaction Quantification in Hyperparameter Optimization}{20}{}
\contentsline {subsection}{\numberline{D.1}Measuring the Magnitude of Interactions}{20}{}
\contentsline {subsection}{\numberline{D.2}Analyzing Lower-Order Representations of Games}{20}{}
\contentsline {subsection}{\numberline{D.3}Additional Experimental Details}{21}{}
% E
\contentsline {section}{\numberline{E}Additional Empirical Results}{22}{}
\contentsline {subsection}{\numberline{E.1}Additional Information for the Comparison of Ablation and Tunability}{22}{}
\contentsline {subsection}{\numberline{E.2}Additional Results for Comparison with fANOVA}{22}{}
\contentsline {subsection}{\numberline{E.3}Additional Results for Explaining the SMAC Surrogate During Optimization}{23}{}
\contentsline {subsection}{\numberline{E.4}Additional Interaction Visualizations}{23}{}

\clearpage
\section{Proofs}\label{appx_sec_proofs_notation}

\subsection{Proof of Theorem 4.4}
\begin{proof}
The \dstunability game is given by the value function
\[
\nu(S) := \nu_{G_T}(S) := \max_{\conf \in \confs}
\,\val(\conf \oplus_S \conf^0, D) \,\, .
\]
We now want to show monotonicity of the value function, i.e., $S \subseteq T$ implies $\nu(S) \leq \nu(T)$. 
Given a coalition $T \subseteq \mathcal N$ with $S \subseteq T$, we immediately see that
\[
    A := \{\conf \oplus_{S} \conf^0: \conf \in \confs \} \subseteq \{ \conf \oplus_{T} \conf^0: \conf \in \confs \} =: B,
\]
since we can set the hyperparameters of $T\setminus S$ to $\conf^0 \in \confs$ on the right-hand side.
Since the \dstunability game takes the $\max$ over these two sets, respectively, we obtain
\[
    \nu(S) =  \max_{\conf \in \confs}
\,\val(\conf \oplus_S \conf^0, D) = \max_{\conf^\ast \in A} \val(\conf^\ast, D) \overset{A \subseteq B}{\leq} \max_{\conf^\ast \in B} \val(\conf^\ast, D) =  \max_{\conf \in \confs}
\,\val(\conf \oplus_T \conf^0, D) = \nu(T).
\]

This concludes that the \dstunability game is monotone.
As a consequence, we obtain non-negative \glspl*{SV} due to the \emph{monotonicity} axiom \citep{Fujimoto.2006} of the \gls*{SV}.
We can also give a direct proof of this via the well-known representation of the \gls*{SV} in terms of a weighted average over marginal contributions as
\begin{align*}
    \phi^{\text{SV}}(i) := \sum_{T \subseteq \cN \setminus \{i\}} \frac{1}{n \cdot \binom{n-1}{\vert T \vert }} \big(\nu(T \cup \{i\}) - \nu(T)\big).
\end{align*}
Due to the monotonicity of $\nu_{G_T}$, it follows that $\nu_{G_T}(T) \leq \nu_{G_T}(T \cup i)$, and thus all terms in the above sum are non-negative.
Consequently, the \gls*{SV} is non-negative.

Moreover, the pure individual (main) effects obtained from the functional ANOVA framework are represented by the \gls*{MI} of the individuals \citep{Fumagalli.2024a}.
By the monotonicity of $\nu$, we obtain again
\[
    m(i) := \nu(i) - \nu(\emptyset) \geq 0,
\]
which concludes the proof.
\end{proof}


\subsection{Proof of Theorem 4.5}
\begin{proof}
    Given the synthetic \dstunability and \sensitivity game with two dimensions $\mathcal N = \{1,2\}$, our goal is to show that the main and interaction effects are given by \cref{tab:synthetic_example}.

    \paragraph{Tunability Game.}
    We first proceed to compute the game values of the \dstunability game for $S \subseteq \mathcal N$ with the optimal HPC $\conf^\ast = (1,m)$ as
    \begin{align}\label{appx_eq_synth_tunability}
    \nu_{G_T}(S) = \max_{\conf \in \confs}\val(\conf \oplus_S \conf^0, D) = \val(\conf^* \oplus_S \conf^0, D) = \sum_{i \in \{1,2\}} \mathbf{1}_{(\conf \oplus_S \conf^0)_i = \lambda^\ast_i} = \vert S \vert + \sum_{i \in \mathcal N: i \notin S} \mathbf{1}_{\lambda^0_i = \lambda^\ast_i}.
    \end{align}.


For the baseline set to $\conf^0 := (0,0)$, the second sum in \cref{appx_eq_synth_tunability} vanishes and we thus obtain
\[
    \nu_{G_T}(S) = \begin{cases}
        0, &\text{ if } S = \emptyset, \\
        1, &\text{ if } \vert S \vert = 1, \\
        2, &\text{ if } S=\{1,2\}.
    \end{cases}
\]
Hence, the \glspl*{MI} are given by
\[
    m_{G_T}(S) =\sum_{L \subseteq S}(-1)^{\vert S \vert - \vert L \vert} \nu_{G_T}(L) = \begin{cases}
        0, &\text{ if } S = \emptyset, \\
        1, &\text{ if } \vert S \vert = 1, \\
        0, &\text{ if } S=\{1,2\}.
    \end{cases}
\]

Clearly, the interaction $\lambda_1 \times \lambda_2$, i.e., $m(\{1,2\})$, vanishes, and thus the HPI scores of the individuals are given by their main effects in terms of the \glspl*{MI}.
In summary, the HPI main effects using the \gls*{SV} and the \gls*{MI} are both equal to $1$, whereas the interaction is zero, confirming the values shown in \cref{tab:synthetic_example}.

For the baseline set to $\conf^0 := \conf^\ast$, the second sum in \cref{appx_eq_synth_tunability} equals  $\vert \mathcal N \vert - \vert S \vert$ and thus we obtain a constant game
\[
    \nu_{G_T}(S) = \vert S \vert + \vert \mathcal N \vert - \vert S \vert = 2 \text{ for all } S\subseteq \mathcal N.
\]
Consequently, all interactions and main effects are zero due to the dummy axiom \citep{Fujimoto.2006}, confirming \cref{tab:synthetic_example}.


\paragraph{Sensitivity Game.}
We now proceed to compute the game values of the \sensitivity game for $S \subseteq \mathcal N$.
First, for $S=\emptyset$, we obtain $\nu_{G_V}(\emptyset)=0$, since $\conf \oplus_{\emptyset} \conf^0 = \conf^0$, and thus there is no variance with respect to $\conf$.
Due to independence of the hyperparameter distribution, we can decompose the variance as
\begin{align}\label{appx_eq_synth_sens}
    \nu_{G_V}(S) =  \mathbb{V}_{\conf^\ast \sim p(\conf^\ast)}[\val(\conf^\ast \oplus_S \conf^0, D)] 
    =  \mathbb{V}_{\conf^\ast \sim p(\conf^\ast)}[\sum_{i \in \{1,2\}} \mathbf{1}_{(\conf^\ast \oplus_S \conf^0)_i = \lambda^0_i}] = \sum_{i \in S} \mathbb{V}_{\lambda_i^\ast \sim p(\lambda_i^\ast)}[\mathbf{1}_{(\conf^\ast \oplus_S \conf^0)_i = \lambda^0_i}]
\end{align}

To compute $\mathbb{V}_{\lambda_i^\ast \sim p(\lambda_i^\ast)}[\mathbf{1}_{(\conf^\ast \oplus_S \conf^0)_i = \lambda^0_i}]$, we note that $\mathbf{1}_{(\conf^\ast \oplus_S \conf^0)_i = \lambda^0_i}$ is described by a Bernoulli variable.

Given any baseline, we have $\mathbf{1}_{(\conf^\ast \oplus_S \conf^0)_i = \lambda^0_i} \sim \text{Ber}(q_i)$ with $q_1 = 1/2$ and $q_2 = 1/(m+1)$ due to the uniform distribution, which sets this value to $1$, if the optimal HPC value is chosen.
The variance of this Bernoulli variable is then given by $q(1-q)$, which yields 
\[
    \mathbb{V}_{\lambda_i^\ast \sim p(\lambda_i^\ast)}[\mathbf{1}_{(\conf^\ast \oplus_S \conf^0)_i = \lambda^0_i}] = \begin{cases}
        \frac{1}{4}, &\text{if } i=1, \\
        \frac{1}{m+1}(1-\frac{1}{m+1}) =  \frac{m}{(m+1)^2}, &\text{if } i=2,
    \end{cases}
\]
which yields the game values 
\[
 \nu_{G_V}(S) = 
        \begin{cases}
        0, &\text{ if } S = \emptyset, \\
        \frac{1}{4}, &\text{ if } S = \{1\}, \\
        \frac{m}{(m+1)^2}, &\text{ if } S = \{2\}, \\
         \frac{1}{4} + \frac{m}{(m+1)^2}, &\text{ if } S=\{1,2\}.
        \end{cases}
\]
Hence, the \glspl*{MI} are given by

\[
 m_{G_V}(S) =\sum_{L \subseteq S}(-1)^{\vert S \vert - \vert L \vert} \nu_{G_V}(L) = 
        \begin{cases}
        0, &\text{ if } S = \emptyset, \\
        \frac{1}{4}, &\text{ if } S = \{1\}, \\
        \frac{m}{(m+1)^2}, &\text{ if } S = \{2\}, \\
        0, &\text{ if } S=\{1,2\},
        \end{cases}
\]
which confirms the values given in \cref{tab:synthetic_example} and concludes the proof.
\end{proof}


\subsection{Example: Non-Monotone Sensitivity Game}\label{appx_sec_non_monotone_example}
In this section, we give an example of a non-monotone \sensitivity game.
To this end, we consider two hyperparameters $\mathcal N = \{1,2\}$ equipped with independent Bernoulli distributions $\lambda_1,\lambda_2 \overset{\text{iid}}{\sim} \text{Ber}(1/2)$.
We consider a performance measure as
\begin{align*}
    \val(\conf) := \mathbf{1}_{\lambda_1=0}\mathbf{1}_{\lambda_2=0},
\end{align*}
and set the baseline HPC to $\conf^0 := (0,0)$. 
The \sensitivity game values are then computed by observing that $\val(\conf^\ast)$ with $\conf^\ast \sim p^\ast(\conf^\ast)$ is described as a Bernoulli variable Ber($q$).
For $S=\{1,2\}$, the probability of $\val$ being 1 is $q=1/4$, since both hyperparameters have to be set to zero.
In contrast, for $\vert S \vert = 1$, we have $q=1/2$, since the remaining variable is already set at zero due to the baseline HPC.
We thus obtain again the variances with $q(1-q)$ as 
\[
\nu_{G_V}(S) = \mathbb{V}_{\conf^\ast \sim p^\ast(\conf^\ast)}[\mathbf{1}_{\lambda^\ast_1=0}\mathbf{1}_{\lambda^\ast_2=0}] = 
    \begin{cases}
        0, & \text{if } S=\emptyset, \\
        \frac{1}{2}\frac{1}{2} = \frac{1}{4}, &\text{if } \vert S \vert = 1, \\
        \frac{1}{4}\frac{3}{4} = \frac{3}{16} &\text{if } S= \{1,2\}.

    \end{cases}
\]
Hence, we obtain that $\nu_{G_V}(\{1\}) = 1/4 \geq 3/16 = \nu_{G_V}(\{1,2\})$, which shows that $\nu_{G_V}$ is not monotone.

\clearpage
\section{Experimental Setup}\label{app_experiment-setup}

Our implementation builds upon the \texttt{shapiq} package \citep{Muschalik.2024}, which is publicly available on GitHub\footnote{\url{https://github.com/mmschlk/shapiq}}, for computing Shapley values and interactions. Furthermore, for the experiments, we use YAHPO-Gym \citep{DBLP:conf/automl/PfistererSMBB22}, a surrogate-based benchmark for multi-fidelity hyperparameter optimization. YAHPO-Gym provides several benchmark suites, i.a., \lcbench \citep{zimmer-tpami21a}, which we focused on in the main paper. However, in the subsequent sections, we also present results from the \rbvranger benchmark set, a random forest benchmark, from YAHPO-Gym demonstrating the more general applicability of \tool. Furthermore, we run evaluations on the benchmark \pdone and \jahs to showcase \tool's wide applicability. In our repository, we provide pre-computed games to foster reproducibility of our results and allow for faster post-processing of the game values, e.g., for plotting different representations of the played games.

For better readability in terms of the font size, hyperparameter names are abbreviated in the interaction graphs.

\subsection{Considered Benchmarks}\label{app_section_benchmarks}

\paragraph{lcbench \citep{DBLP:conf/automl/PfistererSMBB22,zimmer-tpami21a}.} \lcbench is a benchmark considering joint optimization of the neural architecture and hyperparameters that has been proposed by \cite{zimmer-tpami21a} together with the automated deep learning system Auto-PyTorch. The benchmark consists of 35 datasets with 2000 configurations each for which the learning curves have been recorded, allowing for benchmarking multi-fidelity HPO. However, in YAHPO-Gym only 34 of the 35 original datasets are contained which is why our evaluation is also restricted to those 34 datasets.
\begin{center}
    \begin{tabular}{l c c}
    \toprule
    \edit{Hyperparameter Name} & \edit{Abbreviation} & \edit{Type}\\
    \midrule
    \texttt{weight\_decay} & \edit{W-D} & \edit{float} \\
    \texttt{learning\_rate} & \edit{L-R} & \edit{float} \\
    \texttt{num\_layers} & \edit{N-L} & \edit{integer} \\
    \texttt{momentum} & \edit{M} & \edit{float} \\
    \texttt{max\_dropout} & \edit{M-D} & \edit{float} \\
    \texttt{max\_units} & \edit{M-U} & \edit{integer} \\
    \texttt{batch\_size} & \edit{B-S} & \edit{float} \\
    \bottomrule
    \end{tabular}
\end{center}

\paragraph{rbv2\_ranger \citep{DBLP:conf/automl/PfistererSMBB22}.} As already mentioned above, \rbvranger is a benchmark faced with tuning the hyperparameters of a random forest. We consider the hyperparameters of ranger as listed below:
\begin{center}
    \begin{tabular}{l c c}
    \toprule
    Hyperparameter Name & Abbreviation & Type\\
    \midrule
    \texttt{min\_node\_size} & M-N & integer \\
    \texttt{mtry\_power} & M-P & float \\
    \texttt{num\_impute\_selected\_cpo} & N-I & categorical \\
    \texttt{num\_trees} & N-T & integer \\
    \texttt{respect\_unordered\_factors} & R-U & categorical \\
    \texttt{sample\_fraction} & \edit{S-F} & float \\
    \texttt{splitrule} & S & categorical/Boolean \\
    \texttt{num\_random\_splits} & N-R & integer \\
    \bottomrule
    \end{tabular}
\end{center}

\paragraph{PD1 \citep{wang-jmlr24}.}
The \pdone benchmark is a testbed for evaluating hyperparameter optimization methods in the deep learning domain. It consists of tasks derived from realistic hyperparameter tuning problems, including transformer models and image classification networks. Across these different types of models, 4 hyperparameters are subject to tuning:
\begin{center}
\begin{tabular}{l c c}
    \toprule
    \edit{Hyperparameter Name} & \edit{Abbreviation} & \edit{Type}\\
    \midrule
    \edit{\texttt{lr\_decay\_factor}} & \edit{L-D} & \edit{float}\\
    \edit{\texttt{lr\_initial}} & \edit{L-I} & \edit{float}\\
    \edit{\texttt{lr\_power}} & \edit{L-P} & \edit{float}\\
    \edit{\texttt{opt\_momentum}} & \edit{O-M} & \edit{float}\\
     \bottomrule
\end{tabular}
\end{center}

\paragraph{JAHS-Bench-201 \citep{bansal-neurips22a}.}
To democratize research on neural architecture search, various table look-up and surrogate-based benchmarks have been proposed in the literature. Going even beyond plain neural architecture search, in \jahs, the combined task of searching for a suitable neural architecture and optimizing the hyperparameters of the learning algorithm is considered. We include it via the ```mf-prior-bench``` package that serves it with a surrogate model for predicting the validation error of a given architecture and hyperparameter configuration. The considered hyperparameters, including those for the neural architecture, are as follows:
\begin{center}
\begin{tabular}{lcc}
    \toprule
    \edit{Hyperparameter Name} & \edit{Abbreviation} & \edit{Type}\\
    \midrule
    \edit{\texttt{Activation}} & \edit{A} & \edit{categorical}\\
    \edit{\texttt{LearningRate}} & \edit{L} & \edit{float}\\
    \edit{\texttt{Op1}} & \edit{Op1} & \edit{categorical}\\
    \edit{\texttt{Op2}} & \edit{Op2} & \edit{categorical}\\
    \edit{\texttt{Op3}} & \edit{Op3} & \edit{categorical}\\
    \edit{\texttt{Op4}} & \edit{Op4} & \edit{categorical}\\
    \edit{\texttt{Op5}} & \edit{Op5} & \edit{categorical}\\
    \edit{\texttt{Op6}} & \edit{Op6} & \edit{categorical}\\
    \edit{\texttt{TrivialAugment}} & \edit{T} & \edit{Boolean}\\
    \edit{\texttt{WeightDecay}} & \edit{W} & \edit{float}\\
    \bottomrule
\end{tabular}
\end{center}

\subsection{Approximation of the argmax}\label{appx_section_approximation_of_argmax}
As per \cref{def:tunability} to \cref{def:opthabit}, for every coalition $S$, we need to determine the $\arg\max$. However, the true $\arg\max$ is difficult to determine, so we approximate it throughout our experiments. For the sake of implementation simplicity and unbiased sampling, we use random search with a large evaluation budget of $10\,000$ candidate evaluations. As the configurations are independently sampled, for evaluating a configuration, we simply blind an initially sampled batch of 10,000 hyperparameter configurations for the hyperparameters not contained in the coalition $S$ by setting their values to the default value. This procedure is fast to compute and reduces the noise potentially occurring through randomly sampling entirely new configurations for every coalition evaluation. After blinding, the surrogate model provided by YAHPO-Gym is then queried for the set of hyperparameter configurations, and the maximum observed performance is returned.

In Figure~\ref{fig:hpo-quality}, we show how explanations evolve with higher budgets for simulating a hyperparameter optimization run with random search in combination with a surrogate model. To this end, we investigate explanations obtained through a random search with 10, 100, 1,000, 10,000, and 100,000 hyperparameter configurations sampled during optimization. We find that for low budgets of up to 1,000 samples, explanations are not really stable and change with higher budgets. In particular, we observe higher-order interactions that diminish for higher budgets, reflecting a decreasing uncertainty about the actual interactions. For the higher budgets of 10,000 and 100,000 hyperparameter configurations, the interaction graphs do not change as much, so 10,000 hyperparameter configurations appear to be a reasonable tradeoff between computational complexity and faithfulness of the explanations.
Therefore, we chose to conduct our experiments throughout the paper by simulating HPO runs with random search, simulating HPO with a surrogate model and a budget of 10,000 hyperparameter configurations.

\begin{figure*}[htb]
    \includegraphics[width=\textwidth]{images/appendix/quality_of_argmax/hpo_quality.pdf}
    \caption{Hyperparameter importance with HyperSHAP, approximating the $\arg\max$ in Definition~\ref{def:tunability} of the value function \edit{via hyperparameter optimization with increasing budgets} for dataset ID 7593 of \lcbench. For tuning, we consider the following hyperparameters of \texttt{lcnet}: learning rate (L-R), batch size (B-S), weight decay (W-D), num layers  (N-L), momentum (M), max units (M-U), and max dropout (M-D).}
    \label{fig:hpo-quality}
\end{figure*}

\subsection{Computing \opthabit}
For the experiments considering the HPI game of Data-Specific \opthabit, we designed three HPO methods that focus on different structural parts of the hyperparameter configuration space. For the hyperparameter optimization approach, tuning every hyperparameter individually, when considering a hyperparameter for tuning, we sampled 50 random values for every hyperparameter. For the hyperparameter optimizer focusing on a subset of hyperparameters, we allowed for 50,000 hyperparameter configurations. For the VBO, we employed the considered limited hyperparameter optimizer and a random search with a budget of 50,000 evaluations on the full hyperparameter configuration space. We chose larger HPO budgets for these experiments to immediately ensure the built-in deficiencies become apparent and reduce noise effects. Howevér, they might also already be visible with substantially smaller budgets.

\subsection{Hardware Usage and Compute Resources}
Initial computations for lcbench and rbv2\_ranger have been conducted on consumer hardware, i.e., Dell XPS 15 (Intel i7 13700H, 16GB RAM) and a MacBook Pro (M3 Max - 16C/40G, 128GB RAM). Overall computations took around 10 CPUd, highlighting \tool being lightweight when combined with surrogates. In the course of the reviewing process, we re-computed the games for \ablation and Data-Specific \tunability of lcbench and rbv2\_ranger and added PD1 and JAHS-Bench-201. These computations have been conducted on a high-performance computer with nodes equipped with 2$\times$ AMD Milan 7763 ($2\times 64$ cores) and 256GiB RAM of which 1 core and 8GB RAM have been allocated to the computations for a single game. While the latter experiments amounted to 10.71 CPU days, in sum, the computations for this paper accumulate roughly 21 CPU days. The average runtimes per benchmark and game are as follows (Table \ref{tab:benchmark_runtime}):
\begin{table}[h!]
\centering
\caption{\edit{Mean $\pm$ standard deviation of the runtimes on a single CPU per benchmark and game.}}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\edit{\textbf{Benchmark}} & \edit{\textbf{$|\confs|$}} & \edit{\textbf{$|\mathcal{D}|$}} & \edit{\textbf{Runtime Ablation [s]}} & \edit{\textbf{Runtime Tunability [s]}} & \edit{\textbf{Runtime Multi-Data Tunability [s]}} \\ 
\midrule
\edit{PD1} & \edit{4} & \edit{4} & \edit{64.9$\pm$16.0} & \edit{862.4$\pm$13.7} & \edit{-} \\
\edit{JAHS} & \edit{10} & \edit{3} & \edit{123.7$\pm$4.4} & \edit{30,406.7$\pm$4750.9 (8h26m)} & \edit{-} \\
\edit{LCBench} & \edit{7} & \edit{34} & \edit{4.8$\pm$0.4} & \edit{357.3$\pm$3.1} & \edit{10,713.4 (2h58m)} \\
\edit{rbv2\_ranger} & \edit{8} & \edit{119} & \edit{26.4$\pm$6.8} & \edit{6,717$\pm$767.3} & \edit{-} \\
\bottomrule
\end{tabular}}
\label{tab:benchmark_runtime}
\end{table}

\clearpage
\section{Guidance on Interpreting Interaction Visualizations}\label{appx_guidance}

To visualize and interpret lower-, and higher-order interactions such as \gls*{SI} or \gls*{MI}, we employ the \emph{SI graph visualization} and the \emph{UpSet plot} from \texttt{shapiq} \citep{Muschalik.2024}.
The SI graph visualization is an extension of the network plot for Shapley interactions \cite{DBLP:conf/aaai/MuschalikFHH24} and can be used to visualize higher order interactions.
The UpSet plot \cite{Lex2014UpSet} is a well-established method for visualizing set-based scores, which can also be used for representing higher-order interactions.
\cref{fig_si_graph_example} shows an exemplary SI graph and UpSet plot.

For better readability in terms of the font size, hyperparameter names are abbreviated in the interaction graphs.

\paragraph{Interpretation of the UpSet Plot.}
An UpSet plot for \glspl*{SI} or \glspl*{MI} shows a selection of high-impact interactions and their scores. The plot is divided into two parts. The upper part shows the interaction values as bars and the lower part shows the considered interactions as a matrix. The first two bars in \cref{fig_si_graph_example} show the main effects of the O-M and L-I hyperparameters. The third bar shows the negative interaction of both of these features (denoted as the connection between the interactions).
A red color denotes a positive score, and a blue color denotes a negative score.
The bars and interactions are plotted in descending order according to the absolute value of an interaction (i.e., higher-impact interactions first).

\paragraph{Interpretation of the SI Graph.}
An SI graph plot in \cref{fig_si_graph_example} can be interpreted as follows.
Each individual player (e.g. hyperparameter) is represented as a node with connecting hyperedges representing the strength and direction of interactions.
Akin to the well-established force plots \citep{Lundberg.2017}, positive interactions are colored in red and negative interactions in blue, respectively.
The strength of an interaction is represented by the size and opacity of the hyperedge.
To reduce visual clutter, small interactions below a predefined absolute threshold may be omitted from the graph.
Notably, first-order interactions (i.e., individual player contributions, or main effects) are represented by the size of the nodes.

\begin{figure}[htb]
    \centering
    \includegraphics[height=20em]{images/appendix/visualization_guidance/tunability_upset.pdf}
    \includegraphics[height=20em]{images/appendix/visualization_guidance/tunability_si_graph.pdf}
    \caption{An UpSet plot (left) and a \gls*{SI} graph plot (right) for the \dstunability game from \cref{sec_experiments_ablation_tunability}.}
    \label{fig_si_graph_example}
\end{figure}


\clearpage
\section{Interaction Quantification in Hyperparameter Optimization}\label{app:interaction_quantification}

\subsection{Measuring the Magnitude of Interactions}
In this section, we provide further details for measuring the presence of interactions discussed in \cref{sex_exp_interaction_quantification}.
The \glspl*{MI} describe the pure additive effect of a coalition to the payout of the game.
They thus serve as an important tool to analyze the interactions present in a game $\nu$.
For instance, low-complexity games, where \glspl*{MI} are non-zero only up to coalitions of size $k$, are typically referred as $k$-additive games \citep{Grabisch.2016}.
In this case, \glspl*{SI} with explanation order $k$ perfectly recover all game values \citep{Bord.2023}.
In this case, the \glspl*{SI} correspond to the \glspl*{MI}.
We thus analyze the absolute values of \glspl*{MI} for varying size of coalitions, i.e., displaying the strata $q(k) := \{ \vert m(S) \vert : S\subseteq \mathcal N, \vert S \vert = k\}$ for varying interaction order $k=1,\dots,n$.
Analyzing $q(k)$ indicates, if the game $\nu$ has lower- order higher-order interactions present by investigating the magnitudes and distributions in the strata $q(k)$.

\subsection{Analyzing Lower-Order Representations of Games}\label{appx_sec_fsii}
In this section, we provide additional details for the lower-order representations and $R^2$ scores discussed in \cref{sec:exp-higher-order-interactions}.
The \gls*{SV} that capture the fair contribution in a game $\nu$ of an individual to the joint payout $\nu(\mathcal N)$.
However, the \gls*{SV} $\phi^{\text{SV}}(i)$ is also the solution to a constrained weighted least squares problem \citep{Charnes.1988,DBLP:conf/icml/FumagalliMKHH24}
\begin{align*}
    \phi^{\text{SV}} = \argmin_{\phi} \sum_{T\subseteq \mathcal N} \frac{1}{\binom{n-2}{\vert T \vert - 1}} \left(\nu(T)-\nu(\emptyset)-\sum_{i \in T} \phi(i) \right)^2 \text{ s.t. } \nu(\mathcal N) = \nu(\emptyset) + \sum_{i \in \mathcal N} \phi(i).
\end{align*}

In other words, the \gls*{SV} is the best additive approximation of the game $\nu$ in terms of this weighted loss constrained on the efficiency axiom.
Based on this result, the \gls*{FSII} \citep{Tsai.2022} was introduced as 

\begin{align*}
    \Phi_k^{\text{FSII}} := \argmin_{\Phi_k} \sum_{T \subseteq N} \mu(\vert T \vert) \left(\nu(T) - \sum_{S \subseteq T, \vert S \vert \leq k} \Phi_k(S)\right)^2
    \text{ with } \mu(t) := \begin{cases}
        \mu_\infty &\text{ if } t \in \{0,n\} 
        \\
        \frac{1}{\binom{n-2}{t-1}} &\text{ else}
    \end{cases},
\end{align*}
where the infinite weights capture the constraints $\nu(\emptyset)=\Phi_k(\emptyset)$ and $\nu(\mathcal N) = \sum_{S \subseteq \mathcal N} \Phi_k(S)$.
Note that \citet{Tsai.2022} introduce \gls*{FSII} with a scaled variant of $\mu$ that does not affect the solution.
The \gls*{FSII} can thus be viewed as the best possible approximation of the game $\nu$ using additive components up to order $k$ constrained on the efficiency axiom.
It is therefore natural to introduce the \emph{Shapley-weighted faithfulness} as
\begin{align*}
    \mathcal F(\nu,\Phi_k) := \sum_{T \subseteq N} \mu(\vert T \vert) \left(\nu(T) - \sum_{S \subseteq T, \vert S \vert \leq k} \Phi_k(S)\right)^2.
\end{align*}

Based on this faithfulness measure, the Shapley-weighted $R^2$ can be computed.
More formally, we compute the weighted average and the total sum of squares as
\begin{align*}
    \bar y := \frac{\sum_{T \subseteq \mathcal N} \mu(\vert T \vert) \nu(T)}{\sum_{T \subseteq \mathcal N} \mu(\vert T \vert)} \text{ and } \mathcal F_{\text{tot}} := \sum_{T \subseteq \mathcal N} \mu(\vert T \vert) \left(\nu(T) - \bar y\right)^2,
\end{align*}
which yields the Shapley-weighted $R^2$ as
\begin{align*}
    R^2(k) := R^2(\nu,\Phi_k) := 1 - \frac{\mathcal F(\nu,\Phi_k)}{\mathcal F_{\text{tot}}}.
\end{align*}

In our experiments, we rely on \gls*{FSII}, since this interaction index optimizes the faithfulness measure $\mathcal F$ by definition.
However, \gls*{k-SII} satisfies a similar faithfulness property \citep{DBLP:conf/icml/FumagalliMKHH24}.
Since the \gls*{FSII} is equal to the \glspl*{MI} for $k=n$, we have that $\mathcal F(\nu,\Phi_n) = 0$  due to the additive recovery property of the \glspl*{MI}.
Hence, $R^2(n)=R^2(\nu,\Phi_n)=0$ in this case.
Clearly, the $R^2(k)$ scores are monotonic increasing in $k$ by definition of \gls*{FSII}.
An $R^2(k) \approx 1$ indicates an almost perfect recovery of all game values.
In our experiments, we have shown that higher-order interactions are present, but lower-order representations (low $k$) are mostly sufficient to achieve very high $R^2$ scores.
This indicates that higher-order interactions are present but do not dominate the interaction landscape in our applications.
For instance, a single isolated higher-order interaction would yield much lower $R^2$ scores \citep{Muschalik.2024}.

\subsection{Additional Experimental Details}

In \cref{sex_exp_interaction_quantification}, we investigate how \emph{faithful} \tool explanations capture the interaction structures of the HPO problem. For this we compute \dstunability explanations for all four benchmarks, \lcbench, \rbvranger \pdone, and \jahs.
Further, we compute \tunability explanations for \lcbench and \rbvranger over all instances in the benchmarks.
We then compute the \glspl*{MI} for all of these explanations. We compute \tool \gls*{FSII} explanations up to the highest order. Then we compute the Shapley-weighted $R^2$ loss between the explanations and the original game as a measure of \emph{faithfulness}.
\cref{fig_appx_faithfulness} summarizes the results. 
The high $R^2$ score (almost $1.0$) for both the \dstunability and the \tunability games suggests that most of the explanatory power is captured by interactions up to the third order, confirming prior research that suggests hyperparameter interactions are typically of lower order \cite{DBLP:conf/gecco/PushakH20a}. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\linewidth]{images/appendix/interaction_quantification/faithfulness.pdf}
    \caption{Detailed Reprint of \cref{fig_interaction_quantification} (right). Curves for \tunability contain only one game each. The \dstunability games for \lcbench and \rbvranger are averaged over $20$ randomly selected datasets. The \dstunability curves for \pdone and \jahs are averaged over all datasets contained in the benchmarks ($4$ and $3$, respectively). The shaded bands correspond to the standard error of the mean (SEM).}
    \label{fig_appx_faithfulness}
\end{figure}

\clearpage
\section{Additional Empirical Results}\label{appx_sec_additional_results}

This section contains additional experimental results, including more detailed plots and visualizations for the experiments conducted in \cref{sec_experiments}.

\subsection{Additional Information for the Comparison of Ablation and Tunability}\label{appx_sec_tunability_ablation_comparison}

In \cref{sec_experiments_ablation_tunability}, we compare the \ablation and the \dstunability settings and see that we can derive different interpretations from both explanations into the Hyperparameter optimization. 
Interpreting the \ablation explanation suggests that only the \texttt{lr\_initial} (L-I) hyperparameter is important for achieving high performance.
However, the \dstunability explanation reveals that actually both, the \texttt{opt\_momentum} (O-M) and \texttt{initial\_learning\_rate} L-I, hyperparameters are useful for tuning.
The optimizer needs to decide which hyperparameter to focus on. \cref{fig_appx_ablation_tunability} contains shows the same result as in \cref{fig_exp_ablation_tunability} with more detail.

\begin{figure}[htb]
    \centering
    \begin{minipage}[c]{0.05\textwidth}
        \rotatebox{90}{\textbf{\ablation}}
    \end{minipage}
    \begin{minipage}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/exp_a/ablation_upset.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/exp_a/ablation_si_graph.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.05\textwidth}
        \rotatebox{90}{\textbf{\dstunability}}
    \end{minipage}
    \begin{minipage}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/exp_a/tunability_upset.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/exp_a/tunability_si_graph.pdf}
    \end{minipage}
    \caption{UpSet (left) and SI graph (right) plots for the \ablation (top) and \dstunability (bottom) settings described in \cref{sec_experiments_ablation_tunability}. The SI graph plots show all interactions and the UpSet plot the ten most impactful interactions. }
    \label{fig_appx_ablation_tunability}
\end{figure}

\subsection{Additional Results for Comparison with fANOVA}\label{appx_sec_fanova_comparison}

This section contains additional results for the evaluation of hyperparameter optimization runs restricted to the top-2 important hyperparameters according to fANOVA \cite{fANOVA}, \sensitivity, and \dstunability of \tool. \cref{fig_appx_fanova_comparison} shows that selecting and tuning hyperparameters with \tool leads to better anytime performance than with fANOVA or \sensitivity. The suggested top-2 hyperparameters for every method are listed in \cref{tab:selected_parameters}. We can observe that overall, although not always perfect, \tool suggests a top-2 that yields higher anytime performance, meaning that the hyperparameter optimizer achieves a higher accuracy quicker. However, hyperparameters are suggested with respect to their overall hyperparameter importance, which does not necessarily guarantee better anytime performance as these hyperparameters can be more difficult to tune than others with lower impact. Still, in this case, the lower impact hyperparameters could result in better anytime performance for smaller budgets. We consider an in-depth study of which hyperparameters to suggest for which subsequent HPO task to be an interesting avenue of future work.

\begin{figure}[htb]
    \centering
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/126025_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/126026_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/126029_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/146212_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/167104_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/167161_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/167168_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/189865_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/fanova_comparison/189866_downstream_hpo_1000_rs.pdf}
    \end{minipage}
    \caption{Anytime performance plots showing mean and standard error of the incumbent's performance, comparing hyperparameter optimization runs restricted to the top-2 important hyperparameters as suggested by fANOVA, the sensitivity game, and the \dstunability game of \tool.}
    \label{fig_appx_fanova_comparison}
\end{figure}


\begin{table}[htb]
\caption{Top-2 Hyperparamters as identified by fANOVA, Sensitivity, and \tool}\label{tab:selected_parameters}
\centering
\begin{tabular}{l  ll  ll  ll} \toprule
\textbf{Dataset} & \multicolumn{2}{c}{\textbf{fANOVA}} & \multicolumn{2}{c}{\textbf{Sensitivity}} & \multicolumn{2}{c}{\tool}\\ \midrule
\textbf{126025}& weight\_decay & batch\_size & num\_layers & learning\_rate & num\_layers & weight\_decay \\
\textbf{126026}& momentum & learning\_rate & learning\_rate & num\_layers & weight\_decay & batch\_size \\
\textbf{126029}& batch\_size & momentum & learning\_rate & num\_layers & num\_layers & batch\_size \\
\textbf{146212}& max\_dropout & momentum & learning\_rate & max\_dropout & num\_layers & weight\_decay \\
\textbf{167104}& learning\_rate & batch\_size & learning\_rate & num\_layers & learning\_rate & max\_units \\
\textbf{167161}& learning\_rate & max\_dropout & learning\_rate & batch\_size & num\_layers & learning\_rate \\
\textbf{167168}& num\_layers & learning\_rate & learning\_rate & num\_layers & learning\_rate & max\_units \\
\textbf{189865}& num\_layers & learning\_rate & learning\_rate & batch\_size & learning\_rate & momentum \\
\textbf{189866}& num\_layers & weight\_decay & num\_layers & weight\_decay & weight\_decay & max\_units \\

\bottomrule
\end{tabular}
\end{table}

\subsection{Additional Results for Explaining the SMAC Surrogate During Optimization}\label{appx_sec_smac_analysis}

In \cref{fig_appx_smac_analysis_surrogate}, in addition to the MI interaction graphs, we summarize explanations with the help of second order \gls*{FSII}, which fairly distributes higher-order interactions to the lower orders, here order one and two. We find that with FSII we can distill the relevant parts of the \glspl*{MI} quite clearly.

\begin{figure}[htb]
    \centering
    \begin{minipage}[c]{0.05\textwidth}
        \textbf{\gls*{FSII}:}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/fsii_graph_60.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/fsii_graph_302.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/fsii_graph_1512.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/fsii_graph_6050.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.05\textwidth}
        \textbf{\gls*{MI}:}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/mi_graph_60.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/mi_graph_302.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/mi_graph_1512.pdf}
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/appendix/smac_analysis/mi_graph_6050.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.05\textwidth}
        \textbf{Budget}: % the overfull hbox here is on purpose
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \centering\textbf{60}\\(1\%)      
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \centering\textbf{300}\\(5\%)
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \centering\textbf{1500}\\(25\%)
    \end{minipage}
    \begin{minipage}[c]{0.23\textwidth}
        \centering\textbf{6000}\\(100\%)
    \end{minipage}
    \caption{\tool \dstunability explanations for the surrogate model used in SMAC at different time intervals (1\%, 5\%, 25\%, 100\%) of the optimization procedure for dataset 3945 of \texttt{lcbench} \cite{zimmer-tpami21a}. Over time the model becomes less uncertain about which hyperparameters are important to achieve a high predictive performance. Bottom: Interaction graphs for Moebius Interactions (MI) show all pure main effects and interactions. Top: Higher-order interactions are summarized to main effects and second-order interactions, summarizing the game properly already at early stages when the MI still shows a comparably large number of higher-order interactions.}
    \label{fig_appx_smac_analysis_surrogate}
\end{figure}

\subsection{Additional Interaction Visualizations}

In \cref{fig_appx_pd1,fig_appx_jahs,fig_appx_lcbenc_tunability_different_indices,fig_appx_ranger_tunability_different_indices}, we show more interaction graphs for the different benchmarks we evaluated \tool on. This includes \pdone (cf. \cref{fig_appx_pd1}, \texttt{JAHS-Bench-201} (cf. \cref{fig_appx_jahs}), \texttt{lcbench} (cf. \cref{fig_appx_lcbenc_tunability_different_indices}), and \texttt{rbvs\_ranger} (cf. \cref{fig_appx_ranger_tunability_different_indices}). We find that with \tool we can elicit interesting interaction structures for the tuning of transformers and neural architectures in more general. Surprisingly, there can be comparably low interaction between hyperparameters steering the learning behavior and hyperparameters controlling the neural architecture, as seen for CIFAR10. However, for the other two datasets, the higher degree of interaction between the learner's hyperparameters and those of the architecture better meets intuition and expectation.

In \cref{fig_sensitivity_tunability}, we compare the \sensitivity to the \dstunability game for dataset ID 7593 of \texttt{lcbench} on three different levels: Moebius interactions showing all pure effects, Shapley interactions, summarizing higher-order interactions to main effects and interactions of order two and Shapley values representing the entire game solely in terms of main effects. What we can observe is that \dstunability and \sensitivity yield quite different explanations as \sensitivity does not blend an optimized hyperparameter configuration with the default hyperparameter configuration for evaluating the value function for a given coalition but takes the variance. Taking the variance apparently results in more pronounced interactivity structures as the performance is no longer contrasted to the default configuration.

\begin{figure}
    \begin{minipage}[c]{0.1\textwidth}
        \rotatebox{90}{\texttt{imagenet\_resnet}}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \textbf{\ablation}
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_ablation_pd1_imagenet_resnet_512_default_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \textbf{\dstunability}
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_data_specific_tunability_pd1_imagenet_resnet_512_default_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \rotatebox{90}{\texttt{lm1b\_transformer}}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_ablation_pd1_lm1b_transformer_2048_default_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_data_specific_tunability_pd1_lm1b_transformer_2048_default_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \rotatebox{90}{\texttt{translatewmt\_xformer}}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_ablation_pd1_translatewmt_xformer_64_default_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_data_specific_tunability_pd1_translatewmt_xformer_64_default_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \caption{\edit{MIs as computed via \tool for three different scenarios of \pdone, considering hyperparameter optimization for image classifiers and transformers.}}
    \label{fig_appx_pd1}
\end{figure}

\begin{figure}
    \begin{minipage}[c]{0.1\textwidth}
        \rotatebox{90}{\texttt{CIFAR10}}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \textbf{\ablation}
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_ablation_jahs_jahs_CIFAR10_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \textbf{\dstunability}
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_data_specific_tunability_jahs_jahs_CIFAR10_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \rotatebox{90}{\texttt{FashionMNIST}}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_ablation_jahs_jahs_FashionMNIST_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_data_specific_tunability_jahs_jahs_FashionMNIST_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \rotatebox{90}{\texttt{ColorectalHistology}}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_ablation_jahs_jahs_ColorectalHistology_default_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_data_specific_tunability_jahs_jahs_ColorectalHistology_default_n_configs=1000_random_state=42.pdf}
    \end{minipage}
    \caption{\edit{MIs as computed via \tool for \texttt{CIFAR10} (top), \texttt{FashionMNIST} (middle), and \texttt{ColorectalHistology} (bottom) of \jahs.}}
    \label{fig_appx_jahs}
\end{figure}

\begin{figure}
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{SV}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/SV_tunability_yahpogym_lcbench_None_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/appendix/interaction_collection_upset_plots/SV_tunability_yahpogym_lcbench_None_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{SI}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/FSII_tunability_yahpogym_lcbench_None_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/appendix/interaction_collection_upset_plots/FSII_tunability_yahpogym_lcbench_None_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{MI}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_tunability_yahpogym_lcbench_None_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/appendix/interaction_collection_upset_plots/MI_tunability_yahpogym_lcbench_None_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \caption{\glspl*{SV}, \glspl*{SI}, and \glspl*{MI} for the \tunability setting on \lcbench. The interactivity in the full decomposition of the \glspl*{MI} is summarized into less complicated explanations by the \glspl*{SV} and \glspl*{SI}. Notably, all \glspl*{SV} are positive.}
    \label{fig_appx_lcbenc_tunability_different_indices}
\end{figure}

\begin{figure}
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{SV}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/SV_tunability_yahpogym_rbv2_ranger_None_acc_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection_upset_plots/SV_tunability_yahpogym_rbv2_ranger_None_acc_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{SI}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/FSII_tunability_yahpogym_rbv2_ranger_None_acc_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection_upset_plots/FSII_data_specific_tunability_yahpogym_rbv2_ranger_181_acc_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{MI}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection/MI_tunability_yahpogym_rbv2_ranger_None_acc_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/interaction_collection_upset_plots/MI_tunability_yahpogym_rbv2_ranger_None_acc_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \caption{\glspl*{SV}, \glspl*{SI}, and \glspl*{MI} for the \tunability setting on \rbvranger. The interactivity in the full decomposition of the \glspl*{MI} is summarized into less complicated explanations by the \glspl*{SV} and \glspl*{SI}. Notably, all \glspl*{SV} are positive.}
    \label{fig_appx_ranger_tunability_different_indices}
\end{figure}

\begin{figure}
    \begin{minipage}[c]{0.1\textwidth}\phantom{\textbf{SV:}}\end{minipage}
    \begin{minipage}[c]{0.40\textwidth}\centering\textbf{\dstunability}\end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}\centering\textbf{Sensitivity}\end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{MI}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/sens_vs_nonsense/MI_data_specific_tunability_yahpogym-nonsense_lcbench_1_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/sens_vs_nonsense/MI_data_specific_tunability_yahpogym-sense_lcbench_1_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{SI}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/sens_vs_nonsense/FSII_data_specific_tunability_yahpogym-nonsense_lcbench_1_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/sens_vs_nonsense/FSII_data_specific_tunability_yahpogym-sense_lcbench_1_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \\
    \begin{minipage}[c]{0.1\textwidth}
        \textbf{\gls*{SV}:}
    \end{minipage}
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/sens_vs_nonsense/SV_data_specific_tunability_yahpogym-nonsense_lcbench_1_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.40\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/appendix/sens_vs_nonsense/SV_data_specific_tunability_yahpogym-sense_lcbench_1_val_accuracy_n_configs=10000_random_state=42.pdf}
    \end{minipage}
    \caption{Comparison of \dstunability (left) and Sensitivity (right) games as provided via \tool. Both variants of measuring HPI provide notably different explanations. Note that also for Sensitivity, \tool can be used to compute lower-order explanations summarizing higher-order interactions accordingly.}
    \label{fig_sensitivity_tunability}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
