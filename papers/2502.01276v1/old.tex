
$\hat{\nu}_k(S) := \sum_{L \subseteq S:\vert L\vert \leq k}{\Phi_k(L)}$ of $\nu(S)$




The \gls*{SV} and \glspl*{SI} are defined based on a cooperative game comprising $n$ players $\mathcal{N} = \{1,\dots,n\}$ and $\nu: 2^{\mathcal{N}} \to \mathbb{R}$ as a real-valued set function on the powerset $2^{\mathcal N}$.
This game captures a joint payout $\nu(S)$ obtained from a set of players $S \subseteq N$ forming a coalition.


In this section, we assume $\nu(\emptyset)=0$ for readability, which does not affect the \gls*{SV} and \glspl*{SI} due to the dummy axiom (introduced below).
Given $\nu$, the \gls*{SV} \citep{Shapley.1953} is the \emph{fair} contribution of an individual entity to the overall payout $\nu(\mathcal{N})$.
The \gls*{SV} is uniquely characterized by four intuitive axioms: \emph{linearity} (contributions are linear for linear combination of games), \emph{symmetry} (players with equal contributions obtain equal payout), \emph{dummy} (players that do not change the payout receive zero payout), and \emph{efficiency} (the sum of all payouts equals the joint payout).
The \gls*{SV} $\phi^{\text{SV}}(i)$ of player $i \in \mathcal{N}$ can be computed as a weighted average
\[
    \phi^{\text{SV}}(i) := \sum_{T \subseteq \cN \setminus \{i\}} \frac{1}{n \cdot \binom{n-1}{\vert T \vert }} \Delta_i(T)\\ 
\]
over marginal contributions $\Delta_i(T) := \nu(T \cup i) - \nu(T)$.

Due to the efficiency axiom, the sum of \glspl*{SV} yields the joint payout $\nu(\mathcal{N}) = \sum_{i \in \mathcal{N}} \phi^{\text{SV}}(i)$.
Moreover, any game value can be approximated with $\hat\nu(T) = \sum_{i \in T} \phi^{\text{SV}}(i)$, which is the best approximation of $\nu$ restricted to individual contributions in terms of a particular optimization objective \citep{Charnes.1988}.
Clearly, individual contributions are limited in describing the values $\nu(S)$ for every subset $S \subseteq \mathcal{N}$.
The \glspl*{MI} $m: 2^{\mathcal N} \to \mathbb{R}$, alternatively Möbius 
transform \citep{rota1964foundations} or Harsanyi dividend \citep{harsanyi1963simplified}, recover any game value additively for all $S,T\subseteq \mathcal{N}$ as
\begin{align}\label{align_möbius_recovery}
    \nu(T) = \sum_{S \subseteq T} m(S) \text{ with } m(S) := \sum_{T \subseteq S} (-1)^{\vert S \vert -\vert T \vert} \nu(T) .
\end{align}

In fact, the \glspl*{MI} are the unique measure with this property \citep{harsanyi1963simplified}.
The \gls*{MI} $m(S)$ captures the \emph{pure additive contribution} that is achieved by forming the coalition $S$, which cannot be attributed to any subgroup of players in~$S$.
Moreover, the \glspl*{MI} form a basis of the vector space of games \citep{Grabisch.2016}, and the \gls*{SV} can be derived as
\begin{align*}
    \phi^{\text{SV}}(i) = \sum_{S \subseteq \mathcal{N}: i \in S} \frac{1}{\vert S \vert} m(S) \text{ for all } i \in \mathcal{N}.
\end{align*}
In other words, the \gls*{SV} summarizes all \glspl*{MI} $m(S)$ by equally distributing the interaction among the players in $S$.
While the \gls*{SV} has limited expressivity, the \glspl*{MI} with $2^n$ components are difficult to interpret.
As a remedy, \glspl*{SI}\edit{, $\Phi_k$,} provide a flexible framework with interactions for subsets up to size $k=1,\dots,n$, where the edge cases are the \gls*{SV} ($k=1$) and the \glspl*{MI} ($k=n$).
Higher-order \glspl*{SI} are computed based on discrete derivatives $\Delta_S(T)$, as an extension of marginal contributions.
For two players $i,j \in \mathcal{N}$, the discrete derivative $\Delta_{\{i,j\}}(T)$ of $\{i,j\}$ in the presence of $T \subseteq \mathcal{N}\setminus \{i,j\}$ is given as $\nu(T \cup \{i,j\}) - \nu(T) - \Delta_{\{i\}}(T) - \Delta_{\{j\}}(T)$, i.e., the effect of adding $\{i,j\}$ jointly minus their individual marginal contributions.
This recursion can be extended to any subset $S \subseteq N$, yielding $\Delta_S(T)$.
The \gls*{SII} \citep{Grabisch.1999}, as an axiomatic extension of the \gls*{SV} \edit{to all subsets $S \subseteq \cN$} is then a weighted average over $\Delta_S(T) := \sum_{L \subseteq S}(-1)^{\vert S \vert - \vert L \vert} \nu(T \cup L)$
\[
    \phi^{\text{SII}}(S) = \sum_{T \subseteq \cN \setminus S } \frac{1}{(n - \vert S \vert +1) \cdot \binom{n - \vert S \vert}{\vert T \vert }} \Delta_S(T).    
\]



A positive interaction indicates a synergistic effect, whereas a negative interaction indicates redundancies (on average).
Given an \emph{explanation order} $k$, the \glspl*{k-SII}\edit{, $\Phi_k^{\text{SII}}$} \citep{Lundberg.2020,Bord.2023}\edit{,} are recursively constructed from the \gls*{SII}, such that the highest order coincides.
\edit{Alternatively, the \gls*{FSII}, $\Phi_k^{\text{FSII}}$ \citep{Tsai.2022}, constructs \glspl*{SI} based on the best $k$-additive approximation $\hat{\nu}_k(S) := \sum_{L \subseteq S:\vert L\vert \leq k}{\Phi_k(L)}$ of $\nu(S)$ across all subsets $S$ weighted by the Shapley kernel \citep{Tsai.2022}, cf. \cref{appx_sec_fsii}.
\gls*{FSII} are thus well-suited to analyze the degree of interactions within a game.
In general, \glspl*{SI} differ in weighting of discrete derivatives \citep{Fumagalli.2023} and \glspl*{MI} of order larger than $k$ \citep{Bord.2023}.}
The \glspl*{SI} provide a flexible framework to adjust \edit{explanation expressivity and complexity based on practitioner needs} \citep{Tsai.2022,DBLP:conf/icml/FumagalliMKHH24}.
In the following, we explore interactions within hyperparameter optimization using \glspl*{SI}.




\subsection{\tunability of Learners}\label{sec:tunability}



Zooming out from a specific configuration, we can ask to what extent it is worthwhile to tune hyperparameters. In the literature, this question has been connected to the term of \textit{tunability} \citep{tunability}. Tunability aims to quantify how much performance improvements can be obtained by tuning a learner comparing against a baseline HPC, e.g., a HPC that is known to work well across various datasets \citep{DBLP:conf/gecco/PushakH20a}.




\begin{definition}[HPI Game - \tunability]\label{def:ds-tunability}
The tunability HPI game is defined as a tuple 
\[
T = (\conf^0, \confs, \mathcal{D}),
\]
consisting of a baseline HPC $\conf^0 \in \confs$, a HPC space $\confs$, a collection of datasets $\mathcal{D} = \{ D_1, D_2, \ldots D_M \}$ with
\[
\nu_T(S) := \bigoplus_{i=1}^M \, \max_{\conf \in \confs}
\,\val(\conf \oplus_S \conf^0, D_i) \,\, ,
\]
where $\bigoplus$ denotes an aggregation operator, e.g. the mean, of the performances obtained from the individual datasets $D_i$.
\end{definition}
Hyperparameters $j \notin S$ that
are not tuned as their domain is not contained in $\confs^S$ are kept at their reference value $\lambda^0_j$.
Consequently, the value of the empty coalition $S=\emptyset$ refers to the performance of the reference configuration,
i.e., $\nu(\emptyset) = \oplus_{i=1}^M\, \val(\conf^0, D_i)$.


While this definition considers the problem of tunability across datasets, we can also question the usefulness of tuning the hyperparameters of a learner for a specific dataset at hand. This setting, which we dub Data-Specific \tunability, can be derived as a special instance of Definition~\ref{def:tunability}:

\begin{definition}[HPI Game - Data-Specific \tunability]\label{def:ds-tunability}
Data-Specific \tunability is defined as a special case of Definition~\ref{def:opthabit}, considering a dataset collection of size $|\mathcal{D}| = 1$. Then, the value function can be simplified to
\[
\nu(S) = \val \left( \underset{\conf \in \confs^S}{\arg\max} 
\,\val(\conf, D), D  \right) \,\,.
\]
\end{definition}

We note that, in practice, for evaluating a single coalition in Definition~\ref{def:tunability}, $M$ HPO runs are carried out to approximate the $\arg\max$. In the specific case of Definition~\ref{def:ds-tunability}, we still need to conduct one HPO run per coalition evaluation. While this can result in considerable costs, we argue that using surrogate models that are, e.g., obtained through HPO via Bayesian optimization, can be used to simulate HPO runs, rendering \tool more tractable.

\subsection{\opthabit}\label{sec:opthabit}

In the previous section, we aimed to explain the importance of hyperparameters being tuned. However, depending on how much a hyperparameter may contribute to a performance gain can also be used to gain insights into the capabilities of a hyperparameter optimizer. More specifically, we would like to investigate whether a hyperparameter optimizer may fail to exploit certain hyperparameters. \edit{As shown in \cref{fig:hpo-quality}, partial contributions of main effects and interactions to the overall performance vary depending on the approximation quality of the $\arg\max$. As a consequence, for high quality explanations, we need sufficiently accurate approximations. In turn, we may leverage this fact to detect deficiencies of optimizers.} 
To this end, we define a hyperparameter optimizer to be a function $\mathcal{O}: \mathbb{D} \times 2^{\confs} \rightarrow \confs$, mapping from the space of datasets and a (sub)space of a hyperparameter configuration space to a configuration. 

\begin{definition}[HPI Game - \opthabit] \label{def:opthabit}
    The \opthabit HPI game is defined as a tuple 
    \[
    G_O = (\mathcal{N}, \confs, \conf^0, \mathcal{O}, \mathcal{D}, \nu),
    \]
    where $\mathcal{N}, \confs, \conf^0, \mathcal{D}$, and the construction of $\confs^S$ are as in Definition~\ref{def:tunability}, $\mathcal{O}$ the hyperparameter optimizer of interest, and a value function $\nu$.
    Then, $\nu$ww is defined as
    \begin{multline}
    \nu(S) = \oplus_{i = 1}^M \Big[ \val \Big(\mathcal{O}(D_i, \confs^S), D_i \Big)\\
    - \val \Big(\underset{\conf \in \confs^S}{\arg\max}\, \val(\conf, D_i), D_i \Big) \Big] \,\, .
    \end{multline}
    % \[
    % \nu(S) = \oplus_{i = 1}^M \left[ \val \left(\mathcal{O}(D_i, \confs^S), D_i \right) - \val \left(\underset{\conf \in \confs^S}{\arg\max}\, \val(\conf, D_i), D_i \right) \right]\,\, .
    % \]
\end{definition}

Intuitively speaking, the value function measures any deviation from the performance of the actual best-performing hyperparameter configuration. In other words, with the help of Definition~\ref{def:opthabit}, we can identify deficiencies of the hyperparameter optimizer $\mathcal{O}$ over the actual best-performing solution and, thereby for example, identify whether an optimizer struggles to optimize certain (types of) hyperparameters.

However, to approximate the right-hand side of the difference, i.e., the $\arg\max$, in practice, we propose to employ a diverse ensemble of optimizers $\mathbb{O}:= \{ \mathcal{O}_i \}$. Furthermore, for some hyperparameter configuration space $\confs^S$, we pick the best result obtained through any optimizer from $\mathbb{O}$. Thereby, we obtain a virtual best hyperparameter optimizer:
\[
\mathcal{VBO}(D_i, \confs^S) \mapsto \hat{\conf}, \hat{\conf} \in \underset{\conf^i = \mathcal{O}_i(D_i, \confs^S)}{\arg\max} \val(\conf^i, D_i) \,\, .
\]

Analogue to Definition~\ref{def:ds-tunability}, we can define Data-Specific \opthabit:
\begin{definition}[HPI Game - Data-Specific \opthabit]\label{def:ds-opthabit} 
Data-Specific \opthabit is defined as a special case of Definition~\ref{def:opthabit}, considering a dataset collection of size $|\mathcal{D}| = 1$.
\end{definition}


% \begin{table*}[t]
%     \centering
%     \caption{Overview of games to compute hyperparameter importance across three levels: configuration, hyperparameter configuration space, and at the level of the optimizer's behavior.}
%     \label{tab:my_label}
% \begin{tabularx}{\textwidth}{l X X X }
% \toprule
%   & \textbf{\ablation} & \textbf{\tunability} & \textbf{\opthabit} \\
% \midrule
%      \belowbaseline[-2ex]{\rotatebox{90}{\textbf{Player}}}
%      & A player represents a hyperparameter value.
%      & A player represents the domain of a hyperparameter.
%      & A player represents the domain of a hyperparameter.\\
% \midrule
%     \belowbaseline[0ex]{\rotatebox{90}{%
%        \raggedright\textbf{Coalition}}}
%      & A coalition denotes which hyperparameters' values are switched from its value of the reference HPC $\conf^0$ to its value in $\conf^\ast$.
%      & A coalition determines a subset of hyperparameters tuned by a hyperparameter optimizer.
%      & A coalition determines a subset of hyperparameters tuned by the ensemble of hyperparameter optimizers.\\
% \midrule
%     \belowbaseline[0ex]{\rotatebox{90}{%
%        \raggedright\textbf{Value Function}}}
%      & The value function measures the performance of the learner parameterized with the intermediate hyperparameter configuration $\conf^S$, switch values from the ones given in $\conf^0$ to $\conf^\ast$.
%      & The value function measures the maximum achievable performance of an optimizer considering the configuration space as specified by the coalition.
%      & The value function measures the difference between the best performances found by some optimizer and by any member of an optimizer ensemble.\\
% \bottomrule
% \end{tabularx}
% \end{table*}
