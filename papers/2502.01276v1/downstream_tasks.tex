To investigate whether we can leverage the knowledge gained from past HPO runs for future considerations, we define three downstream tasks and evaluate to what extent HyperSHAP is able to give an advantage over uninformed HPO runs.
\begin{description}
    \item[Rerun Dataset] As a first sanity check, we let HyperSHAP examine the hyperparameter importances for a given dataset. Subsequently, we aim to leverage this knowledge and focus on optimizing only the most impactful hyperparameters.
    We compare HPO runs in the reduced search space to HPO runs in the full search space and observe their anytime performance. While the full search space subsumes any solution contained in the reduced search space, given a sufficient amount of budget
    the HPO run with the full search space will always yield a solution candidate that is at least as good as the one for the reduced search space. However, the search space reduction may speeds up the search for a strong hyperparameter configuration
    and thus yield a stronger anytime performance (at least in early iterations of the HPO process).
    \item[Meta-Learning Search Spaces] The second downstream tasks aims to take advantage of information about hyperparameter importances for similar datasets that have been observed in the past. To implement this downstream task, we use meta-features
    the datasets considered from the open machine learning platform OpenML \citep{} and determine the $k$ closest datasets with respect to their meta-feature description using Euclidean distance as a metric.
    \item[Ensembling HPO] Different hyperparameter optimization methods may perform differently well for different types of hyperparameters. In this downstream task we aim to leverage information about hyperparameter importances to choose the \textit{right}
    hyperparameter optimizer for a given dataset.
    \item[Informing HPO About Interactions] As Shapley interaction indices can be used to discover interactions between hyperparameters,
    hinting at which hyperparameters need to be tuned jointly. Alternatively, if hyperparameters show no interactions
    with other hyperparameters, this can be an indication that they can in principle be tuned individually thereby
    reducing the search space complexity. As a last downstream task, we assess an informed HPO tool taking into account
    to what extent hyperparameters interact with each other to tune parameters individually as long as there are no
    interactions with other hyperparameters.
\end{description}