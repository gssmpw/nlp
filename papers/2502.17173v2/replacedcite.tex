\section{Related Works}
\paragraph{Reinforcement Learning from Human Feedback.}
Reinforcement Learning from Human Feedback has been widely adopted for LLM alignment ____.
Previous research mostly focuses on specific tasks like summarization ____ and question answering ____. 
Recent studies have expanded RLHF applications to broader domains ____, improving LLMs to be more helpful, honest, and harmless. 
RLHF enables models to align with human expectations more closely by integrating human preferences captured by reward models ____. 
Thus, a reward model that accurately reflects human preferences is fundamental to the RLHF methodology.

\paragraph{Reward Model Training and Evaluation.}
To develop a RM that captures human preferences, current works gather preference data through manual annotation ____ or distilling advanced LLMs ____.
These works mostly focus on English, overlooking Chinese contexts.
Existing Chinese preference datasets are generally small ____ or limited to specific tasks ____.
Beyond the training data, RM evaluation is also critical for post-training. 
The typical RM evaluation computes accuracy on a fixed test dataset ____.
Recent studies ____ have attempted to strengthen the correlation with downstream performance. 
However, these benchmarks focus on English, raising questions about their applicability to Chinese contexts.