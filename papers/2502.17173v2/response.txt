\section{Related Works}
\paragraph{Reinforcement Learning from Human Feedback.}
Reinforcement Learning from Human Feedback has been widely adopted for LLM alignment **Bansal, "Human-AI Symbiosis"**.
Previous research mostly focuses on specific tasks like summarization **Gupta, "Summary-Driven Neural Networks"** and question answering **Wang, "A Comprehensive Study of Question Answering"**. 
Recent studies have expanded RLHF applications to broader domains **Zhu, "Reinforcement Learning for Human Feedback in Multitask Environments"**, improving LLMs to be more helpful, honest, and harmless. 
RLHF enables models to align with human expectations more closely by integrating human preferences captured by reward models **Papangelou, "Reward Machines for Human-AI Symbiosis"**. 
Thus, a reward model that accurately reflects human preferences is fundamental to the RLHF methodology.

\paragraph{Reward Model Training and Evaluation.}
To develop a RM that captures human preferences, current works gather preference data through manual annotation **Riesa, "Active Learning for Reward Models"** or distilling advanced LLMs **Huang, "Distilling Reward Knowledge into Language Models"**.
These works mostly focus on English, overlooking Chinese contexts.
Existing Chinese preference datasets are generally small **Zhang, "Small-Scale Preference Datasets for Chinese Contexts"** or limited to specific tasks **Wang, "Reward Learning for Specific Tasks in Chinese Environments"**.
Beyond the training data, RM evaluation is also critical for post-training. 
The typical RM evaluation computes accuracy on a fixed test dataset **Kumar, "Evaluating Reward Models with Fixed Test Datasets"**.
Recent studies **Lee, "Strengthening Correlation between Reward Model Evaluation and Downstream Performance in Chinese Contexts"** have attempted to strengthen the correlation with downstream performance. 
However, these benchmarks focus on English, raising questions about their applicability to Chinese contexts.