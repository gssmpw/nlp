[
  {
    "index": 0,
    "papers": [
      {
        "key": "ouyang2022traininglanguagemodelsfollow",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022traininghelpfulharmlessassistant",
        "author": "Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "stiennon2022learningsummarizehumanfeedback",
        "author": "Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano",
        "title": "Learning to summarize from human feedback"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "nakano2022webgptbrowserassistedquestionansweringhuman",
        "author": "Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hou2024chatglmrlhfpracticesaligninglarge",
        "author": "Zhenyu Hou and Yilin Niu and Zhengxiao Du and Xiaohan Zhang and Xiao Liu and Aohan Zeng and Qinkai Zheng and Minlie Huang and Hongning Wang and Jie Tang and Yuxiao Dong",
        "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback"
      },
      {
        "key": "lin2024baichuanalignmenttechnicalreport",
        "author": "Mingan Lin and Fan Yang and Yanjun Shen and Haoze Sun and Tianpeng Li and Tao Zhang and Chenzheng Zhu and Tao Zhang and Miao Zheng and Xu Li and Yijie Zhou and Mingyang Chen and Yanzhao Qin and Youquan Li and Hao Liang and Fei Li and Yadong Li and Mang Wang and Guosheng Dong and Kun Fang and Jianhua Xu and Bin Cui and Wentao Zhang and Zenan Zhou and Weipeng Chen",
        "title": "Baichuan Alignment Technical Report"
      },
      {
        "key": "yu2024codepmpscalablepreferencemodel",
        "author": "Huimu Yu and Xing Wu and Weidong Yin and Debing Zhang and Songlin Hu",
        "title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "10.5555/645529.657801",
        "author": "Ng, Andrew Y. and Russell, Stuart J.",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "key": "brown2019deepbayesianrewardlearning",
        "author": "Daniel S. Brown and Scott Niekum",
        "title": "Deep Bayesian Reward Learning from Preferences"
      },
      {
        "key": "palan2019learningrewardfunctionsintegrating",
        "author": "Malayandi Palan and Nicholas C. Landolfi and Gleb Shevchuk and Dorsa Sadigh",
        "title": "Learning Reward Functions by Integrating Human Demonstrations and Preferences"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bai2022traininghelpfulharmlessassistant",
        "author": "Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "key": "zheng2023judging",
        "author": "Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "starling2023",
        "author": "Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao",
        "title": "Starling-7B: Improving LLM Helpfulness \\& Harmlessness with RLAIF"
      },
      {
        "key": "cui2023ultrafeedback",
        "author": "Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun",
        "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "huozi",
        "author": "Huozi-Team",
        "title": "Huozi: Leveraging Large Language Models for Enhanced Open-Domain Chatting"
      },
      {
        "key": "zhihu_rlhf_3k",
        "author": "Li Yucheng",
        "title": "3,000 Chinese Zhihu Q\\&A Preference Dataset"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Yang_Kyara_2024",
        "author": "Yang, Kai-Chou",
        "title": "{Kyara}"
      },
      {
        "key": "DPO-zh-en-emoji2024",
        "author": "Xinlu Lai, shareAI",
        "title": "The DPO Dataset for Chinese and English with emoji"
      },
      {
        "key": "xu2023cvalues",
        "author": "Guohai Xu and Jiayi Liu and Ming Yan and Haotian Xu and Jinghui Si and Zhuoran Zhou and Peng Yi and Xing Gao and Jitao Sang and Rong Zhang and Ji Zhang and Chao Peng and Fei Huang and Jingren Zhou",
        "title": "CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lambert2024rewardbenchevaluatingrewardmodels",
        "author": "Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "son2024llmasajudgerewardmodel",
        "author": "Guijin Son and Hyunwoo Ko and Hoyoung Lee and Yewon Kim and Seunghyeok Hong",
        "title": "LLM-as-a-Judge \\& Reward Model: What They Can and Cannot Do"
      },
      {
        "key": "kim2024evaluatingrobustnessrewardmodels",
        "author": "Sunghwan Kim and Dongjin Kang and Taeyoon Kwon and Hyungjoo Chae and Jungsoo Won and Dongha Lee and Jinyoung Yeo",
        "title": "Evaluating Robustness of Reward Models for Mathematical Reasoning"
      },
      {
        "key": "zhou2024rmbcomprehensivelybenchmarkingreward",
        "author": "Enyu Zhou and Guodong Zheng and Binghai Wang and Zhiheng Xi and Shihan Dou and Rong Bao and Wei Shen and Limao Xiong and Jessica Fan and Yurong Mou and Rui Zheng and Tao Gui and Qi Zhang and Xuanjing Huang",
        "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"
      },
      {
        "key": "liu2024rmbenchbenchmarkingrewardmodels",
        "author": "Yantao Liu and Zijun Yao and Rui Min and Yixin Cao and Lei Hou and Juanzi Li",
        "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style"
      },
      {
        "key": "frick2024evaluaterewardmodelsrlhf",
        "author": "Evan Frick and Tianle Li and Connor Chen and Wei-Lin Chiang and Anastasios N. Angelopoulos and Jiantao Jiao and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica",
        "title": "How to Evaluate Reward Models for RLHF"
      },
      {
        "key": "gureja2024mrewardbenchevaluatingrewardmodels",
        "author": "Srishti Gureja and Lester James V. Miranda and Shayekh Bin Islam and Rishabh Maheshwary and Drishti Sharma and Gusti Winata and Nathan Lambert and Sebastian Ruder and Sara Hooker and Marzieh Fadaee",
        "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings"
      }
    ]
  }
]