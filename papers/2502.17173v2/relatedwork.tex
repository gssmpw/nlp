\section{Related Works}
\paragraph{Reinforcement Learning from Human Feedback.}
Reinforcement Learning from Human Feedback has been widely adopted for LLM alignment \citep{ouyang2022traininglanguagemodelsfollow,bai2022traininghelpfulharmlessassistant}.
Previous research mostly focuses on specific tasks like summarization \citep{stiennon2022learningsummarizehumanfeedback} and question answering \citep{nakano2022webgptbrowserassistedquestionansweringhuman}. 
Recent studies have expanded RLHF applications to broader domains \citep{hou2024chatglmrlhfpracticesaligninglarge,lin2024baichuanalignmenttechnicalreport,yu2024codepmpscalablepreferencemodel}, improving LLMs to be more helpful, honest, and harmless. 
RLHF enables models to align with human expectations more closely by integrating human preferences captured by reward models \citep{10.5555/645529.657801,brown2019deepbayesianrewardlearning,palan2019learningrewardfunctionsintegrating}. 
Thus, a reward model that accurately reflects human preferences is fundamental to the RLHF methodology.

\paragraph{Reward Model Training and Evaluation.}
To develop a RM that captures human preferences, current works gather preference data through manual annotation \citep{bai2022traininghelpfulharmlessassistant,zheng2023judging} or distilling advanced LLMs \citep{starling2023,cui2023ultrafeedback}.
These works mostly focus on English, overlooking Chinese contexts.
Existing Chinese preference datasets are generally small \citep{huozi,zhihu_rlhf_3k} or limited to specific tasks \citep{Yang_Kyara_2024,DPO-zh-en-emoji2024,xu2023cvalues}.
Beyond the training data, RM evaluation is also critical for post-training. 
The typical RM evaluation computes accuracy on a fixed test dataset \citep{lambert2024rewardbenchevaluatingrewardmodels}.
Recent studies \citep{son2024llmasajudgerewardmodel,kim2024evaluatingrobustnessrewardmodels,zhou2024rmbcomprehensivelybenchmarkingreward,liu2024rmbenchbenchmarkingrewardmodels,frick2024evaluaterewardmodelsrlhf,gureja2024mrewardbenchevaluatingrewardmodels} have attempted to strengthen the correlation with downstream performance. 
However, these benchmarks focus on English, raising questions about their applicability to Chinese contexts.