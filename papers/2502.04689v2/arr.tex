% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}
% \usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{inconsolata}

\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{dirtytalk}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{pythonhighlight}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{cleveref}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{dsfont}
\usepackage{soul}
\usepackage{tikz}
\usepackage{centernot}
\usepackage{tcolorbox}


\title{ARR: Question Answering with Large Language Models via \\ Analyzing, Retrieving, and Reasoning}


\author{Yuwei Yin \\ % \thanks{}
% Department of Computer Science \\
University of British Columbia \\
% Vancouver, BC V6T1Z4, Canada \\
\texttt{yuweiyin@cs.ubc.ca} \\
\And
Giuseppe Carenini \\
% Department of Computer Science \\
University of British Columbia \\
% Vancouver, BC V6T1Z4, Canada \\
\texttt{carenini@cs.ubc.ca} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\yy}[1]{{\color{cyan}{[Yuwei: #1]}}}
\newcommand{\todo}[1]{{\color{cyan}{[#1]}}}
\newcommand{\method}{\text{ARR}\xspace}

\newcommand{\cmark}{\ding{52}}
\newcommand{\xmark}{\ding{56}}

\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{royalblue}{rgb}{0.255,0.412,0.882}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.5,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.5}
\definecolor{blue_}{rgb}{0,0,1}
\definecolor{green_}{rgb}{0,0.5,0}
% https://www.overleaf.com/learn/latex/Using_colors_in_LaTeX
% https://latexcolor.com/
\definecolor{beaublue}{rgb}{0.74, 0.83, 0.9}
\definecolor{beige}{rgb}{0.96, 0.96, 0.86}
\definecolor{bisque}{rgb}{1.0, 0.89, 0.77}
\definecolor{linen}{rgb}{0.98, 0.94, 0.9}
\definecolor{lightyellow}{rgb}{1.0, 1.0, 0.88}
\definecolor{lightgreen}{rgb}{0.56, 0.93, 0.56}
\definecolor{lawngreen}{rgb}{0.49, 0.99, 0.0}
\definecolor{arryellow}{rgb}{1.0, 1.0, 0.848}
\definecolor{lightgray}{gray}{0.9}


\hypersetup{
pdfauthor={Yuwei Yin, Giuseppe Carenini},
pdfkeywords={ARR}
}


\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks.
Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (``think step by step'').
This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step.
Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT.
Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning.
Notably, intent analysis plays a vital role in ARR.
Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.\footnote{Source code: \url{https://github.com/YuweiYin/ARR}}
\end{abstract}



%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section{Introduction}
\label{sec:introduction}

% Intro 1. Backgound
Large language models (LLMs)~\cite{zhao2023llm_survey,min2023llm_survey,minaee2024llm_survey} have been a transformative technique in Natural Language Processing (NLP) owing to their excellent text generation and conversation abilities~\cite{hurst2024gpt4o,anthropic2024claude3,team2024gemini1_5}.
Challenging benchmarks for language model evaluation have significantly driven LLM advancements~\cite{chang2024llm_eval_survey},
with most designed as multiple-choice question-answering (MCQA) tasks~\cite{robinson2023leveraging} requiring answer selection from given options~\cite{clark2018ai2_arc,liu2020logiqa,hendrycks2021mmlu}.
Recent LLM benchmarks demand extensive commonsense, world knowledge, and complex reasoning~\cite{srivastava2023big_bench,suzgun2023bbh,wang2024mmlu_pro}, posing significant challenges for LLMs.
Optimizing LLM performance in QA tasks is increasingly crucial for their continued development.


\begin{figure}[t!]
  \centering
  % \vspace{-5pt}
  \includegraphics[width=0.98\linewidth]{figures/arr_overview.pdf}
  \vspace{-5pt}
  \caption{\textbf{ARR motivation.} To answer a question, we often need to \textbf{analyze} the question's intent, \textbf{retrieve} relevant information, and \textbf{reason} step by step.}
  \label{fig:arr_illustration}
  \vspace{-5pt}
\end{figure}


% Intro 2. LLM Reasoning
Recent advancements have introduced various methods to enhance LLM reasoning abilities~\cite{qiao2023reasoning_survey,sun2023reasoning_survey}, with Chain-of-Thought (CoT) prompting proving effective across various tasks~\cite{li2024cot_transformer}.
Key variants include few-shot CoT~\cite{wei2022cot}, which provides rationale-based exemplars for in-context learning~\cite{openai2020gpt3,dong2024icl_survey}, and zero-shot CoT~\cite{kojima2022cot_think_step_by_step}, which employs general instructions such as ``Let's think step by step.''
Due to its simplicity and effectiveness, zero-shot CoT has gained extensive adoption.
By appending this trigger sentence to the original QA prompt, LLMs generate step-by-step reasoning to improve question-answering performance.

% Intro 3. Intro to our ARR method
Despite its widespread use, zero-shot CoT prompting provides only generic reasoning guidance.
As illustrated in Figure~\ref{fig:arr_illustration}, answering complex questions typically involves three key steps:
(1) analyzing the question's intent~\cite{adams1986intention,mele1989intention,mele1994intentional} to obtain a thorough context understanding, a clear problem-solving target, and a purposeful planning guide,
(2) retrieving relevant information from context, external sources, or memory for supportive reference~\cite{jones2022capturing,shi23distract}, and
(3) systematically applying inductive and deductive reasoning~\cite{clark1969deductive,johnson1999deductive,heit2000inductive,hayes2018inductive2}.
Therefore, we hypothesize that an effective prompt should direct LLMs to complete these steps.
To verify this hypothesis, we propose a refined zero-shot prompting method, \textbf{ARR}, which explicitly incorporates these three elements: \textbf{A}nalyzing, \textbf{R}etrieving, and \textbf{R}easoning.
Specifically, ARR employs the answer trigger sentence: ``Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.''
This structured approach is expected to enhance the performance across diverse QA tasks and various models, akin to the improvements observed with zero-shot CoT upon its first introduction~\cite{kojima2022cot_think_step_by_step}.

% Intro 4. Main Experiments
To evaluate the effectiveness of the proposed ARR method, we test the performance (accuracy) of open-weights LLMs~\cite{dubey2024llama3} on 10 multiple-choice QA datasets, covering reading comprehension~\cite{clark2019boolq,liu2020logiqa}, commonsense reasoning~\cite{talmor2019commonsenseqa,sap2019social_iqa}, world knowledge~\cite{welbl2017sciq,mihaylov2018OpenBookQA,clark2018ai2_arc}, and multitask understanding~\cite{suzgun2023bbh,hendrycks2021mmlu,wang2024mmlu_pro}.
Compared to the Baseline method without a specific trigger sentence and the zero-shot CoT method with general prompts, \textbf{ARR consistently improves QA performance across all datasets}, demonstrating its effectiveness and superiority.
Additionally, ablation studies show that each individual component of ARR--Analyzing, Retrieving, and Reasoning--outperforms both the Baseline and CoT methods, confirming their positive contributions.
Notably, the Analyzing-only setting yields the largest performance gain on average, highlighting the critical role of intent analysis in question answering.
Beyond quantitative results, we provide qualitative case studies to reveal problems in the Baseline and CoT methods such as intent misunderstanding, context misuse, and faulty reasoning.

% Intro 5. Generalizability Experiments
Furthermore, we conduct extensive experiments across various settings to assess the generalizability of the proposed ARR method.
ARR consistently outperforms alternatives across different model sizes, LLM series (architectures), generation temperatures, and few-shot scenarios.
These comprehensive experiments and analyses further solidify its effectiveness, robustness, and adaptability.
The key contributions of this work are as follows:

\begin{itemize}
\setlength\itemsep{0em}
    \item [1.] This paper proposes ARR, an intuitive, general, and effective zero-shot prompting method to improve LLM performance in various question-answering tasks.
    \item [2.] Comprehensive experiments across diverse QA tasks demonstrate that ARR consistently outperforms the Baseline and CoT methods. Ablation and case studies further validate the positive contributions of each component.
    \item [3.] Additional extensive experiments on various settings solidify the effectiveness and generalizability of ARR across different model sizes, LLM series, and generation configurations.
\end{itemize}



\begin{figure*}[t!]
  \centering
  % \vspace{-5pt}
  \includegraphics[width=1.0\linewidth]{figures/arr_workflow.pdf}
  \vspace{-15pt}
  \caption{\textbf{Question answering with LLMs.} We first obtain rationale $r_i$ by reasoning generation and then select the optimal option via evaluating the language modeling losses of different context-option combinations.}
  \label{fig:arr_workflow}
  \vspace{-10pt}
\end{figure*}

%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section{Related Work}
\label{sec:related_work}

\subsection{LLM Prompting}

Recent large language models (LLMs)~\cite{dubey2024llama3,lambert2024tulu3,liu2024deepseek_v3} are pre-trained on large-scale text corpora curated from the Internet~\cite{soldaini2024dolma,penedo2024fineweb,weber2024redpajama}. % dodge2021c4
Their advanced text understanding and generation capabilities~\cite{hurst2024gpt4o,anthropic2024claude3,team2024gemini1_5} have significantly revolutionized the field of natural language processing (NLP).
Consequently, the NLP paradigm is shifting toward a framework comprising pre-training, post-training, and prompting~\cite{liu2023prompt}, with post-training focusing on aligning models with human preferences~\cite{ouyang2022rlhf,bai2022rlaif,rafailov2023dpo} rather than fine-tuning for specific downstream tasks~\cite{devlin2019bert}.
After the training stages, LLMs can generate satisfactory responses to natural language instructions and questions, highlighting the growing importance of prompt design~\cite{white2023prompt,giray2023prompt,sahoo2024prompt}.
In this work, we propose an intuitive, general, and effective prompting method to enhance LLM performance in question-answering.

\subsection{LLM Reasoning}

Recent LLM research increasingly emphasizes reasoning abilities~\cite{qiao2023reasoning_survey,sun2023reasoning_survey}.
Chain-of-Thought (CoT) is a prompting strategy that enhances problem-solving by guiding LLMs to generate intermediate reasoning steps.
Main variants include zero-shot CoT~\cite{kojima2022cot_think_step_by_step} that uses general instructions such as ``Let's think step by step'' and few-shot CoT~\cite{wei2022cot} that provides exemplars with rationales to leverage in-context learning~\cite{openai2020gpt3,dong2024icl_survey}.
Building on CoT, various reasoning techniques have emerged~\cite{zhou2023least_to_most,zhou2023large,wang2023plan,yasunaga2024analogical,wang2024cot_no_prompting}.
Some studies explore optimal reasoning paths through self-consistency~\cite{wang2023self_consistency,chen2023uni_self_consistency} or tree-like searches~\cite{yao2023tot},
while others investigate self-refinement~\cite{aman2023self_refine}, self-correction~\cite{huang2024self_correct_no,tyen2024llms_correct,chen2024teaching}, self-verification~\cite{cobbe2021gsm8k,li2023making,lightman2024verify}, and self-evolution~\cite{guan2025rstar,lee2025evolving} mechanisms.
Beyond prompting and generation-based approaches, post-training methods~\cite{chu2025post_training}, particularly those leveraging reinforcement learning (RL)~\cite{sutton2018rl}, have been developed to enhance reasoning capabilities~\cite{shao2024deepseek_math,wang2024offline_rl,setlur2024rewarding,xu2025lrm}.
As a reasoning-eliciting prompting approach, ARR effectively complements existing research by guiding LLMs through three essential steps: intent analysis, information retrieval, and step-by-step reasoning.


\subsection{Retrieval-Augmented Generation}

Retrieval-Augmented Generation (RAG) enhances output quality by retrieving relevant information from pre-processed knowledge sources~\cite{gao2023rag4llm_survey}.
The retrieving component of our ARR method is inspired by the traditional ``external RAG'' approach~\cite{lewis2020rag4nlp}, which retrieves relevant information from the explicit context or outer sources, and realizes instead a form of ``internal RAG,'' which utilizes language models as implicit knowledge bases~\cite{petroni2019lm_as_kb,jiang2020lm_know} and extracts references from memory (training data)~\cite{carlini2021extracting,shi2024detecting}.
This retrieval mechanism is essential for enhancing LLM performance in question answering, as irrelevant information can significantly degrade accuracy~\cite{jones2022capturing,shi23distract,yoran2024making}.



%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
% \section{Method}
\section{Question Answering with LLMs}
\label{sec:qa_with_llm}

This section presents a formally defined multiple-choice question-answering workflow using large language models.
Our pipeline combines ideas from the two-step prompting introduced by~\citet{kojima2022cot_think_step_by_step} and the multiple-choice selection method proposed by~\citet{robinson2023leveraging}.

\subsection{Question Answering Data}
\label{subsec:qa_data}

In this work, we consider multiple-choice question-answering (MCQA) tasks with one correct answer, where the model is asked to answer the question by selecting an option from a list of choices.
Formally, let $\mathcal{D} = \{\mathcal{X}, \mathcal{Y}\}$ be a MCQA dataset, where $\mathcal{X} = \{X_1, X_2, \dots, X_n\}$ is the input information, $\mathcal{Y} = \{y_1, y_2, \dots, y_n\}$ is the corresponding correct-choice label ($y_i \in \mathbb{R}$), and $n$ is the number of instances in $\mathcal{D}$.

In closed-book QA tasks, $X_i = \{q_i, o_i\}$, where $q_i$ is the $i$-th question, and $o_i = \{o_i^j\}_{j=1}^{m}$ is the option list with $m$ choices.
In open-book QA tasks, $X_i = \{p_i, q_i, o_i\}$, where $p_i$ is the $i$-th passage provided by the task.
Then, we obtain the input prompt $x_i$ for LLMs as follows:
\begin{align}\label{equ:qa_prompt}
x_i = \begin{cases}
\mathbf{P}(p_i, q_i, o_i), & \text{Open-book QA} \\
\mathbf{P}(q_i, o_i), & \text{Closed-book QA}
\end{cases}
\end{align} where $\mathbf{P}(\cdot)$ denotes the prompt function which concatenates the string objects in $X_i$ using line breaks as the delimiter ($\Delta=$``$\backslash$n''). Thus, $\mathbf{P}(p_i, q_i, o_i)$ is:

\begin{table}[th]
    \vspace{-5pt}
    \centering
    \scalebox{0.9}{
    \begin{tcolorbox}[colback=white,halign=center,valign=center]
$p_i$  $\Delta$  $q_i$  $\Delta$ $o_i^1$ $\Delta$  $o_i^2$  $\Delta$  $\dots$  $\Delta$  $o_i^m$  $\Delta$  $\phi$
    \end{tcolorbox}
    }
    \vspace{-5pt}
\end{table}

The answer trigger sentence $\phi$ is the only difference between the proposed ARR method and baseline methods in each experiment.
Figure~\ref{fig:arr_workflow} presents each $\phi$ used in Baseline, zero-shot CoT, and our ARR methods.
For simplicity, $\mathcal{X} = \{x_1, x_2, \dots, x_n\}$ is used in the rest of the paper for both open- and closed-book QA.


\subsection{Multiple-Choice Question Answering}
\label{subsec:mcqa_llm}

\paragraph{Stage 1: Reasoning Generation.}
Let $\tilde{x}_i$ be the tokenized representation of text $x_i$. The decoder-only Transformer-based~\cite{vaswani2017transformer,openai2018gpt} LLM $\mathcal{M}$ takes $\tilde{x}_i$ as input and generate a new token after each timestep.
% decoder-only~\cite{vaswani2017transformer,openai2018gpt}
The model freely generates the text response given by
\begin{align}\label{equ:reasoning_generation}
    r_i = \mathcal{M}(\tilde{x}_i),
\end{align} where $r_i$ may contain the analysis, reasoning, and answer.
Then, we combine the original text input $x_i$, the generated response $r_i$, and each choice $o_i^j$ in the option list $o_i$ as follows:
\begin{align}\label{equ:option_prompt}
    z_i^j = \mathbf{P}(x_i, r_i, o_i^j).
\end{align}

\paragraph{Stage 2: Option Selection.}
Let $\tilde{z}_i^j = [t_i^{j;1}, t_i^{j;2},$ $\dots, t_i^{j;L}] \in \mathbb{R}^{L}$ be the tokenized $z_i^j$, where $L$ is the number of effective tokens that are not used for word masking or sequence padding.
To select an option, we feed the model $\mathcal{M}$ and obtain the cross-entropy loss~\cite{shannon1948mathematical,shannon1951entropy,jurafsky2025slp} of each $\tilde{z}_i^j$ as follows:
\begin{align}\label{equ:ce_loss}
    \mathcal{L}_i^j = - \sum_{k} \log \text{Pr}(t_i^{j;k} | t_i^{j;<k}; \Theta), % - \sum{i} \sum_{j} \sum_{k}
\end{align} where $\Theta$ is the parameters of $\mathcal{M}$, $t_i^{j;k}$ is the $k$-th token, and $t_i^{j;<k}$ denotes all the previous tokens before $t_i^{j;k}$.
Hence, for each option $o_i^j$ in $o_i = \{o_i^j\}_{j=1}^{m}$, we have a corresponding cross-entropy loss $\mathcal{L}_i^j$. Then, the option with the lowest loss value is selected, i.e.,
\begin{align}\label{equ:option_selection}
    \hat{y}_i = \underset{j \in \{1, 2, \dots, m\}}{\arg\min} \, \{\mathcal{L}_i^j\}_{j=1}^{m}.
\end{align}

Thus, the overall accuracy is calculated by
\begin{align}\label{equ:accuracy}
    \alpha = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(y_i = \hat{y}_i),
\end{align} where $\alpha \in [0, 1]$ and the indicator function $\mathbb{I}(\cdot)$ returns $1$ if $y_i = \hat{y}_i$ or $0$ otherwise.



\begin{table}[t!]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{ccccc}
    \toprule
    \midrule
    \textbf{QA Dataset} & \textbf{Split} & \textbf{\# Item} & \textbf{$\overline{\text{\# Token}}$} & \textbf{\# Class} \\
    \midrule
    % Reading Comprehension
    BoolQ & Valid & 3,270 & 145 & 2 \\
    LogiQA & Test & 651 & 192 & 4 \\
    % Commonsense Reasoning
    CSQA & Valid & 1,221 & 43 & 5 \\
    SIQA & Valid & 1,954 & 51 & 3 \\
    % World Knowledge
    SciQ & Test & 1,000 & 132 & 4 \\
    OBQA & Test & 500 & 55 & 4 \\
    ARC & Test & 3,548 & 59 & 4 \\
    % Multitask Understanding
    BBH & Test & 5,281 & 112 & 2--18 \\
    MMLU & Test & 13,842 & 108 & 4 \\
    MMLU-Pro & Test & 12,032 & 186 & 10 \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{QA dataset statistics.} ``\# Class'' is the number of options $m$, ``\# Item'' is the total number of data items for evaluation, and ``$\overline{\text{\# Token}}$'' is the average number of tokens per instance (zero-shot prompt), tokenized by the LLaMA~\cite{dubey2024llama3} tokenizer.}
    \label{tab:dataset_stat}
    \vspace{-5pt}
\end{table}

%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section{Experimental Setup}
\label{sec:experimental_setup}

This section introduces the experimental setup, including datasets, models, and evaluation settings.\footnote{Please refer to Appendix~\ref{app:experiment_detail} for more experiment details.}


\subsection{Datasets}
\label{subsec:task_dataset}

As mentioned in \S~\ref{subsec:qa_data}, we consider $10$ multiple-choice QA tasks with questions $q_i$ and options $o_i$.
Reading comprehension tasks~\cite{chen2018mrc} explicitly provide passages $p_i$ to base on.
The model $\mathcal{M}$ is asked to answer the question by choosing one from the option list.
We consider a wide range of QA benchmarks to evaluate the capabilities of $\mathcal{M}$ in different aspects, including reading comprehension, commonsense reasoning, world knowledge, and multitask understanding.
The dataset statistics are presented in Table~\ref{tab:dataset_stat}.


\begin{table*}[t!]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{cccccccccccc}
    \toprule
    \midrule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Reading}} & \multicolumn{2}{c}{\textbf{Commonsense}} & \multicolumn{3}{c}{\textbf{World Knowledge}} & \multicolumn{3}{c}{\textbf{Multitask Understanding}} & \multirow{2}{*}{\textbf{Avg.}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
    & \textbf{BoolQ} & \textbf{LogiQA} & \textbf{CSQA} & \textbf{SIQA} & \textbf{SciQ} & \textbf{OBQA} & \textbf{ARC} & \textbf{BBH} & \textbf{MMLU} & \textbf{MMLU-Pro} & \\
    \midrule
    \rowcolor{lightgray}w/o Reason & 77.86 & 35.64 & 50.37 & 47.49 & 91.20 & 69.80 & 64.61 & 50.26 & 45.54 & 29.60 & 56.24 \\
    Baseline & 84.16 & 35.79 & 72.97 & 69.55 & 85.90 & 72.20 & 82.59 & 52.19 & 60.68 & 38.75 & 65.48 \\
    CoT & 84.65 & 38.10 & 73.71 & 68.12 & 93.70 & 78.20 & 84.31 & 58.40 & 62.08 & 40.10 & 68.14 \\
    \rowcolor{arryellow}\textbf{ARR} & \textbf{86.33} & \textbf{39.02} & \textbf{74.94} & \textbf{70.98} & \textbf{94.40} & \textbf{80.00} & \textbf{84.84} & \textbf{59.01} & \textbf{63.51} & \textbf{42.72} & \textbf{69.58} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Main experiments.} The zero-shot performance (Accuracy \%) of the LLaMA3-8B-Chat model on various multiple-choice QA datasets using different answer trigger sentences $\phi$. (1) w/o Reason: directly selecting an option without rationales; (2) Baseline: $\phi$ is ``Answer:''; (3) CoT~\cite{kojima2022cot_think_step_by_step}: $\phi$ is ``Answer: Let's think step by step.''; (4) ARR: our method that elicits intent analysis, information retrieval, and step-by-step reasoning.}
    \label{tab:exp_main_qa}
    \vspace{-5pt}
\end{table*}


\subsubsection{Reading Comprehension}

\paragraph{BoolQ.} BoolQ~\cite{clark2019boolq} is a question answering dataset for yes/no questions. It evaluates the performance of $\mathcal{M}$ on reading comprehension.

\paragraph{LogiQA.} LogiQA~\cite{liu2020logiqa} is a reading comprehension dataset that requires $\mathcal{M}$ to have logical reasoning for question-answering.

\subsubsection{Commonsense Reasoning}

\paragraph{CSQA.} Commonsense QA~\cite{talmor2019commonsenseqa} examines $\mathcal{M}$ on commonsense question-answering problems constructed using information from ConceptNet~\cite{speer2017ConceptNet}.

\paragraph{SIQA.} SocialIQA~\cite{sap2019social_iqa} is a large-scale QA benchmark for commonsense reasoning about social situations, which probes emotional and social intelligence in everyday situations.

\subsubsection{World Knowledge}

\paragraph{SciQ.} SciQ~\cite{welbl2017sciq} provides scientific supports for $\mathcal{M}$ to answer the multiple-choice science questions.

\paragraph{OBQA.} OpenBookQA~\cite{mihaylov2018OpenBookQA} asks $\mathcal{M}$ to answer the question based on the given elementary level science facts and broad commonsense knowledge.

\paragraph{ARC.} AI2 Reasoning Challenge~\cite{clark2018ai2_arc} contains grade-school science questions and is divided into a Challenge and an Easy set.

\subsubsection{Multitask Understanding}
\label{multitask_understanding_data}

\paragraph{BBH.} BIG-Bench Hard~\cite{suzgun2023bbh} is a suite challenging tasks filtered from BIG-Bench~\cite{srivastava2023big_bench}. Solving these problems often requires multi-step reasoning.
In this work, $4$ (out of $27$) subtasks in BBH are discarded as they are not multiple-choice QA tasks.

\paragraph{MMLU.} MMLU~\cite{hendrycks2021mmlu} comprehensively measures the multitask accuracy of $\mathcal{M}$ on $57$ tasks including elementary mathematics, history, computer science, and more.

\paragraph{MMLU-Pro.} MMLU-Pro~\cite{wang2024mmlu_pro} extends the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options.


\subsection{Models}
\label{subsec:models}

Our experiments adopt open-weights, decoder-only, and Transformer-based~\cite{vaswani2017transformer} LLMs.
We mainly employ LLaMA3-8B-Chat~\cite{dubey2024llama3}, an instruction-following LLM with $8$ billion model parameters, and use the model implementation and checkpoints provided by Hugging Face Transformers~\cite{wolf2020transformers}.
In generalizability experiments, we also explore LLaMA3-Chat models of different sizes in \S~\ref{subsec:model_sizes} and 7B-Chat models of different LLM series in \S~\ref{subsec:llm_series}, i.e., Qwen2.5~\cite{yang2024qwen2_5}, Gemma~\cite{google2024gemma,google2024gemma2}, and Mistral~\cite{jiang2023mistral}.


\subsection{Evaluation}

To evaluate the QA performance of LLMs, we apply a two-step process including \textbf{reasoning generation} and \textbf{option selection}, as mentioned in \S~\ref{subsec:mcqa_llm}.
First, we let the model freely generate text responses that may include their analysis, reasoning, and answer choice.
Then, we concatenate the input and output in the first stage with each choice from the given option list, pass each concatenation to the model, and select the option with the lowest cross-entropy loss.
The loss corresponds to the perplexity of language models: A lower loss means a lower perplexity and a higher confidence.
Length normalization is not applied because the options are mostly in the A/B/C/D, Yes/No, or True/False format.
As the datasets in our experiments are all multiple-choice QA tasks, we adopt accuracy as the evaluation metric, which is calculated by Eq.~\ref{equ:accuracy}.



\begin{table*}[t!]
    \centering
    \scalebox{0.65}{
    \begin{tabular}{ccccl}
    \toprule
    \midrule
    & \cellcolor{bisque}{\textbf{A}} & \cellcolor{beaublue}{\textbf{R}} & \cellcolor{lightgreen}{\textbf{R}} & \multicolumn{1}{c}{\textbf{Answer Trigger Sentence} $\phi$} \\
    \midrule
    \ding{192} & \cmark & \cmark & \cmark & Answer: Let's \colorbox{bisque}{analyze the intent of the question}, \colorbox{beaublue}{find relevant information}, and answer the question with \colorbox{lightgreen}{step-by-step reasoning}. \\
    \midrule
    \ding{193} & \cmark &  &  & Answer: Let's \colorbox{bisque}{analyze the intent of the question}, and answer the question. \\
    \ding{194} & & \cmark &  & Answer: Let's \colorbox{beaublue}{find relevant information}, and answer the question. \\
    \ding{195} & &  & \cmark & Answer: Let's answer the question with \colorbox{lightgreen}{step-by-step reasoning}. \\
    \midrule
    \ding{196} & &  &  & Answer:  \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation study prompts.} The answer trigger sentences $\phi$ used in different ARR ablation study settings.}
    \label{tab:exp_arr_ablation_prompts}
\end{table*}

\begin{table*}[t!]
    \centering
    \scalebox{0.78}{
    \begin{tabular}{ccccccccccccccc}
    \toprule
    \midrule
    & \multicolumn{3}{c}{\textbf{Ablation}} & \multicolumn{2}{c}{\textbf{Reading}} & \multicolumn{2}{c}{\textbf{Commonsense}} & \multicolumn{3}{c}{\textbf{World Knowledge}} & \multicolumn{3}{c}{\textbf{Multitask Understanding}} & \multirow{2}{*}{\textbf{Avg.}} \\
    \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
    & \cellcolor{bisque}{\textbf{A}} & \cellcolor{beaublue}{\textbf{R}} & \cellcolor{lightgreen}{\textbf{R}} & \textbf{BoolQ} & \textbf{LogiQA} & \textbf{CSQA} & \textbf{SIQA} & \textbf{SciQ} & \textbf{OBQA} & \textbf{ARC} & \textbf{BBH} & \textbf{MMLU} & \textbf{MMLU-Pro} &  \\
    \midrule
    \rowcolor{arryellow}\ding{192} & \cmark & \cmark & \cmark & \textbf{86.33} & \textbf{39.02} & 74.94 & \textbf{70.98} & 94.40 & 80.00 & 84.84 & \textbf{59.01} & 63.51 & 42.72 & 69.58 \\ % ARR full 111
    \midrule
    \ding{193} & \cmark &  &  & 86.09 & 38.40 & \textbf{75.76} & 70.78 & 94.30 & \textbf{86.80} & \textbf{85.83} & 57.08 & 63.66 & 42.54 & \textbf{70.12} \\ % Analyzing-only 100
    \ding{194} & & \cmark &  & 85.35 & 37.79 & 75.59 & 68.01 & 92.80 & 81.20 & 85.33 & 58.27 & \textbf{63.73} & \textbf{43.08} & 69.12 \\ % Retrieving-only 010
    \ding{195} & &  & \cmark & 85.87 & 38.86 & 74.53 & 68.01 & \textbf{94.50} & 82.60 & 85.03 & 58.96 & 61.77 & 41.11 & 69.12 \\ % Reasoning-only 001
    \midrule
    \ding{196} & & & & 84.16 & 35.79 & 72.97 & 69.55 & 85.90 & 72.20 & 82.59 & 52.19 & 60.68 & 38.75 & 65.48 \\ % Baseline 000
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation study results.} The accuracy scores (\%) of the LLaMA3-8B-Chat model on various multiple-choice QA datasets using different answer trigger sentences $\phi$ (\colorbox{bisque}{\textbf{A}nalyzing}, \colorbox{beaublue}{\textbf{R}etrieving}, and \colorbox{lightgreen}{\textbf{R}easoning}).}
    \label{tab:exp_arr_ablation_results}
\end{table*}

%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section{Main Experiments}
\label{sec:main_experiments}

% In this section, we present the main experimental results, which verify our hypothesis by demonstrating the effectiveness of the proposed ARR method.

\subsection{QA Performance}

The main experiments test the zero-shot QA performance of LLaMA3-8B-Chat~\cite{dubey2024llama3} on various multiple-choice QA datasets.
The only difference between Baseline, zero-shot CoT~\cite{kojima2022cot_think_step_by_step}, and ARR is the answer trigger sentence $\phi$ shown in Figure~\ref{fig:arr_workflow}.
The results in Table~\ref{tab:exp_main_qa} demonstrate that our ARR method boosts the Baseline method by a large margin, with an improvement of $+4.1\%$ on average.
In addition, ARR consistently outperforms zero-shot CoT prompting across all QA datasets, highlighting its universal superiority in various task types including reading comprehension, commonsense reasoning, world knowledge, and multitask understanding.
Moreover, the ``w/o Reason'' method, which directly selects options without relying on rationales ($r_i$ in Eq.~\ref{equ:reasoning_generation}), performs significantly worse, emphasizing the benefits of our two-stage QA approach.


\subsection{Ablation Study}
\label{subsec:ablation_study}

To better understand the performance gains shown in Table~\ref{tab:exp_main_qa}, we conduct an ablation study to explore the efficacy of each component of the ARR method, i.e., analyzing, retrieving, and reasoning. Specifically, we test the model's QA performance using the five different answer trigger sentences $\phi$ in Table~\ref{tab:exp_arr_ablation_prompts}.
Table~\ref{tab:exp_arr_ablation_results} reports the accuracy scores of LLaMA3-8B-Chat under different ablation cases, where \ding{192} is the full version of ARR and \ding{196} is equivalent to the ``Baseline'' method in Table~\ref{tab:exp_main_qa}.
In \ding{193}, \ding{194}, and \ding{195}, $\phi$ only contains one single component, i.e., analyzing, retrieving, and reasoning, respectively.

We observe that all the single-component ARR settings (\ding{193}, \ding{194}, and \ding{195}) outperform the Baseline method (\ding{196}) by a large margin, which verifies that each ARR component contributes positively.
In addition, \ding{195} outperforms the CoT method in Table~\ref{tab:exp_main_qa}, which means the ``Reasoning'' prompting of our ARR method (i.e., ``answer the question with step-by-step reasoning'') is better than zero-shot CoT prompting (i.e., ``think step by step'') for QA tasks.
Furthermore, the complete ARR method (\ding{192}) has a higher accuracy score than the Retrieving-only (\ding{194}) and Reasoning-only (\ding{195}) methods, meaning the intent analysis benefits the other two ``R'' parts.

Notably, the Analyzing-only setting (\ding{193}) brings the greatest improvement gain, suggesting the significance of analyzing the intent of the question.
\ding{193} even outperforms the full ARR version (\ding{192}) on average, 
mainly because it performs excellently on the OBQA dataset (86.80\%), which means understanding the question's intent in this task is vital.
Excluding OBQA, the full ARR method achieves an average accuracy of 68.41\%, surpassing \ding{193} (68.27\%).
Hence, we dive into a concrete case study to examine the impact of intent analysis on QA solving.




\begin{table*}[t!]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Fact}: the summer solstice is on June 21st in the northern hemisphere. \\ \textbf{Question}: The summer solstice in the northern hemisphere is four months before \\ \textbf{(A)} May \, \textbf{(B)} July \, \textbf{(C)} April \, \textcolor{teal}{\textbf{(D) October}}} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(B)} July. Explanation: The summer solstice is the longest day of the year and marks the beginning of summer. \\ It is four months before the beginning of winter. The beginning of winter is in December, so the summer solstice is in June. \\ \textcolor{red}{Therefore, the summer solstice is four months before July.}} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{The summer solstice is on June 21st. If we \textcolor{red}{count four months before that, we get March 21st.} \\ March is the third month of the year, so the answer is (C). April is the fourth month, May is the fifth month, \\ and October is the tenth month. None of these months is \textcolor{red}{four months before June}. July is the seventh month, \\ which is five months after June, so it is not the correct answer either. The correct answer is \textcolor{red}{(C)}.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{teal}{The question asks for the month that is four months after the summer solstice}. \\ The summer solstice is on June 21st. \textcolor{teal}{Four months after June 21st is October 21st}. \\ Therefore, the correct answer is \textcolor{teal}{\textbf{(D) October}}.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the OpenBookQA dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail because of intent misunderstanding or faulty reasoning.}
    \label{tab:case_study}
\end{table*}

\subsection{Case Study}
\label{subsec:case_study}

Table~\ref{tab:case_study} shows a case in the OBQA dataset~\cite{mihaylov2018OpenBookQA} drawn from the main experiments\footnote{Please refer to Appendix~\ref{app:more_cases} for more cases.}, where our ARR method correctly answers the question but the Baseline and CoT methods fail.
We can observe that the baseline method's reasoning is incorrect: After stating that ``the summer solstice is in June,'' it wrongly concludes that ``the summer solstice is four months before July.''
The CoT method misunderstands the question, resulting in counting four months before June instead of after June.
In contrast, our ARR method identifies the question's intent clearly, leading to a correct reasoning path and final answer.



%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section{Generalizability}
\label{sec:generalizability}

The main experiments in \S~\ref{sec:main_experiments} have validated the effectiveness of our ARR method quantitatively and qualitatively.
To verify the generalizability of ARR, we conduct additional extensive experiments under different configurations
on three challenging, reasoning-intense, and multitask benchmarks introduced in \S~\ref{multitask_understanding_data}: BBH, MMLU, and MMLU-Pro.


\begin{table}[!t]
    \centering
    \scalebox{0.72}{
    \begin{tabular}{cccccc}
    \toprule
    \midrule
    \textbf{Size} & \textbf{Method} & \textbf{BBH} & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{Avg.} \\
    \midrule
    \multirow{3}{*}{1B} & Baseline & 35.88 & \textbf{43.27} & 21.62 & 33.59 \\
    & CoT & 36.30 & 41.10 & 22.74 & 33.38 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{39.02} & \cellcolor{arryellow}42.70 & \cellcolor{arryellow}\textbf{23.49} & \cellcolor{arryellow}\textbf{35.07} \\
    \midrule
    \multirow{3}{*}{3B} & Baseline & 45.65 & 48.26 & 30.88 & 41.60 \\
    & CoT & 46.89 & 46.80 & 30.03 & 41.24 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{51.97} & \cellcolor{arryellow}\textbf{52.82} &\cellcolor{arryellow}\textbf{33.39} & \cellcolor{arryellow}\textbf{46.06} \\
    \midrule
    \multirow{3}{*}{8B} & Baseline & 52.19 & 60.68 & 38.75 & 50.54 \\
    & CoT & 58.40 & 62.08 & 40.10 & 53.53 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{59.01} & \cellcolor{arryellow}\textbf{63.51} & \cellcolor{arryellow}\textbf{42.72} & \cellcolor{arryellow}\textbf{55.08} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Model size experiments.} The zero-shot performance (Accuracy \%) of LLaMA3-Chat models of different sizes on multiple-choice QA datasets.}
    \label{tab:exp_model_size}
\end{table}

\begin{figure}[!t]
  \centering
  % \vspace{-5pt}
  \includegraphics[width=0.9\linewidth]{figures/model_size_exp_bar_chart.pdf}
  \vspace{-10pt}
  \caption{\textbf{Model size experiments.} The trend of QA performance changes as the model becomes larger.}
  \label{fig:model_size_exp_bar_chart}
  \vspace{-5pt}
\end{figure}


\subsection{Model Sizes}
\label{subsec:model_sizes}

We evaluate the LLaMA3-Chat models of different sizes, i.e., 1B, 3B, and 8B (default) parameters, on multiple-choice QA tasks.
As the accuracy scores (\%) shown in Table~\ref{tab:exp_model_size}, our ARR method brings solid performance gains over the Baseline method and consistently outperforms zero-shot CoT.
For the 1B model, ARR slightly underperforms the Baseline method on MMLU, likely due to the weaker instruction-following ability in smaller models. Still, our ARR method achieves overall performance improvements over the Baseline in the 1B setting.
In addition, Figure~\ref{fig:model_size_exp_bar_chart} illustrates the trend of QA performance changes as the model becomes larger.
The results conform to the scaling laws of language models~\cite{kaplan2020scaling}, demonstrating the potential of the proposed ARR method when applied to larger models.


\begin{table}[!t]
    \centering
    \scalebox{0.72}{
    \begin{tabular}{ccccccc}
    \toprule
    \midrule
    \textbf{Series} & \textbf{Method} & \textbf{BBH} & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{Avg.} \\
    \midrule
    % \multirow{3}{*}{Qwen2.5-7B}
    \multirow{3}{*}{Qwen} & Baseline & 39.21 & 48.36 & 32.35 & 39.97 \\
    & CoT & 36.66 & 44.91 & 29.26 & 36.94 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{40.50} & \cellcolor{arryellow}\textbf{50.34} & \cellcolor{arryellow}\textbf{39.10} & \cellcolor{arryellow}\textbf{43.31} \\
    \midrule
    % \multirow{3}{*}{Gemma-7B}
    \multirow{3}{*}{Gemma} & Baseline & 40.09 & 45.46 & 23.45 & 36.33 \\
    & CoT & 44.39 & 47.17 & 26.20 & 39.25 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{45.31} & \cellcolor{arryellow}\textbf{50.73} & \cellcolor{arryellow}\textbf{26.98} & \cellcolor{arryellow}\textbf{41.01} \\
    \midrule
    % \multirow{3}{*}{Mistral-v0.3-7B}
    \multirow{3}{*}{Mistral} & Baseline & 46.27 & 55.61 & 30.68 & 44.19 \\
    & CoT & 53.42 & 61.16 & 34.73 & 49.77 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{53.55} & \cellcolor{arryellow}\textbf{61.49} & \cellcolor{arryellow}\textbf{35.21} & \cellcolor{arryellow}\textbf{50.08} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{LLM series experiments.} The zero-shot performance (Accuracy \%) of 7B-Chat models of different LLM series on multiple-choice QA datasets.}
    \label{tab:exp_llm_series}
\end{table}


\subsection{LLM Series}
\label{subsec:llm_series}

To verify the effectiveness of our ARR method on models other than LLaMA3~\cite{dubey2024llama3}, we conduct experiments on 7B-Chat LLMs of different series: Qwen2.5~\cite{yang2024qwen2_5}, Gemma~\cite{google2024gemma,google2024gemma2}, and Mistral~\cite{jiang2023mistral}.
The results in Table~\ref{tab:exp_llm_series} exhibit a consistent superiority of the proposed ARR method over the Baseline and CoT methods.
This is similar to the findings in the main experiments (Table~\ref{tab:exp_main_qa}), solidifying the efficacy and generalizability of ARR.


\begin{table}[!t]
    \centering
    \scalebox{0.72}{
    \begin{tabular}{cccccc}
    \toprule
    \midrule
    \textbf{Temp.} & \textbf{Method} & \textbf{BBH} & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{Avg.} \\
    \midrule
    \multirow{3}{*}{0.0} & Baseline & 52.19 & 60.68 & 38.75 &  50.54\\
    & CoT & 58.40 & 62.08 & 40.10 &  53.53\\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{59.01} & \cellcolor{arryellow}\textbf{63.51} & \cellcolor{arryellow}\textbf{42.72} & \cellcolor{arryellow}\textbf{55.08} \\
    \midrule
    \multirow{3}{*}{0.5} & Baseline & 50.19 & 59.35 & 36.88 & 48.81 \\
    & CoT & 56.58 & 60.82 & 37.82 & 51.74 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{58.87} & \cellcolor{arryellow}\textbf{62.87} & \cellcolor{arryellow}\textbf{42.64} & \cellcolor{arryellow}\textbf{54.79} \\
    \midrule
    \multirow{3}{*}{1.0} & Baseline & 46.33 & 54.80 & 33.10 & 44.74 \\
    & CoT & 51.46 & 55.57 & 33.00 & 46.68 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{52.90} & \cellcolor{arryellow}\textbf{56.58} & \cellcolor{arryellow}\textbf{36.73} & \cellcolor{arryellow}\textbf{48.74} \\
    \midrule
    \multirow{3}{*}{1.5} & Baseline & 40.84 & 45.03 & 26.85 & 37.57 \\
    & CoT & 42.53 & 44.85 & 25.61 & 37.66 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{42.65} & \cellcolor{arryellow}\textbf{45.16} & \cellcolor{arryellow}\textbf{27.44} & \cellcolor{arryellow}\textbf{38.42} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Generation temperature experiments.} The zero-shot performance (Accuracy \%) of the LLaMA3-8B-Chat model on multiple-choice QA datasets using different generation temperatures (default: 0.0).}
    \label{tab:exp_temperature}
    \vspace{-5pt}
\end{table}


\subsection{Generation Temperatures}
\label{subsec:temperatures}

For reproducibility, we set the generation temperature to 0 by default, as this setting makes the generation process deterministic.
However, a higher temperature brings a more diverse output, which may lead to a different QA accuracy.
To study the effect of this key factor, we report the QA accuracy (\%) of the LLaMA3-8B-Chat model using different temperatures during the reasoning generation stage: 0.0 (default), 0.5, 1.0, and 1.5.

As shown in Table~\ref{tab:exp_temperature}, our ARR method surpasses the Baseline and CoT methods with different temperatures, demonstrating a strong robustness of ARR.
In addition, we observe that the model generally performs better when the temperature is lower.


\subsection{Few-shot Generation}
\label{subsec:few_shot}

\paragraph{Few-shot Examples with Rationales.}
For each subtask in a QA dataset, we randomly pick $10$ examples from the training or validation set if they exists.
If a subtask only has the test set, $10$ test examples are held out for few-shot usage, slightly reducing the number of items for evaluation.
For each raw example, we construct the CoT and ARR rationales using GPT-4o~\cite{hurst2024gpt4o}.
Specifically, the input prompts provided to GPT-4o match those used in the evaluation experiments under CoT/ARR settings. The model's output is extracted as CoT/ARR rationales.
In few-shot examples, these rationales, along with correct answers, are appended to the answer trigger sentence $\phi$.
For the Baseline setting, few-shot examples include correct answers for in-context learning (ICL)~\cite{openai2020gpt3,dong2024icl_survey} but exclude rationales.


\begin{table}[!t]
    \centering
    \scalebox{0.72}{
    \begin{tabular}{cccccc}
    \toprule
    \midrule
    \textbf{Shot} & \textbf{Method} & \textbf{BBH} & \textbf{MMLU} & \textbf{MMLU-Pro} & \textbf{Avg.} \\
    \midrule
    \multirow{3}{*}{0} & Baseline & 52.19 & 60.68 & 38.75 & 50.54 \\
    & CoT & 58.40 & 62.08 & 40.10 & 53.53 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{59.01} & \cellcolor{arryellow}\textbf{63.51} & \cellcolor{arryellow}\textbf{42.72} & \cellcolor{arryellow}\textbf{55.08} \\
    \midrule
    \multirow{3}{*}{1} & Baseline & 35.68 & 44.80 & 28.62 & 36.37 \\
    & CoT & \textbf{47.39} & 48.36 & 31.07 & 42.27 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}47.22 & \cellcolor{arryellow}\textbf{49.29} & \cellcolor{arryellow}\textbf{34.33} & \cellcolor{arryellow}\textbf{43.61} \\
    \midrule
    \multirow{3}{*}{3} & Baseline & 34.39 & 42.08 & 25.92 & 34.13 \\
    & CoT & \textbf{42.84} & 48.21 & 26.69 & 39.25 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}40.19 & \cellcolor{arryellow}\textbf{49.68} & \cellcolor{arryellow}\textbf{37.04} & \cellcolor{arryellow}\textbf{42.30} \\
    \midrule
    \multirow{3}{*}{5} & Baseline & 34.11 & 41.14 & 25.76 & 33.67 \\
    & CoT & 39.92 & 47.48 & 26.12 & 37.84 \\
    & \cellcolor{arryellow}\textbf{ARR} & \cellcolor{arryellow}\textbf{40.68} & \cellcolor{arryellow}\textbf{49.19} & \cellcolor{arryellow}\textbf{36.62} & \cellcolor{arryellow}\textbf{42.16} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Few-shot experiments.} The few-shot performance (Accuracy \%) of the LLaMA3-8B-Chat model on multiple-choice QA datasets using 1, 3, and 5 few-show examples with rationales.}
    \label{tab:exp_few_shot}
    \vspace{-5pt}
\end{table}


\paragraph{Few-shot Results.}
Table~\ref{tab:exp_few_shot} presents the accuracy scores (\%) of the LLaMA3-Chat model on multiple-choice QA tasks. Using different numbers of few-shot examples (1, 3, and 5), our few-shot ARR method outperforms the ICL Baseline and few-shot CoT~\cite{wei2022cot} methods on average.

Comparison across the three few-shot settings reveals that additional examples do not necessarily enhance performance.
Moreover, QA performance is lower in the few-shot experiments than in the zero-shot setting, likely because the randomly selected examples mislead the reasoning process~\cite{zhao2021calibrate,lu2022fantastically,peng2024revisiting}.
While demonstration selection methods could mitigate this issue~\cite{gao2021making,rubin2022learning,li2023unified,wang2023large}, their exploration is beyond the scope of this study.



%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

In this work, we introduce ARR, an intuitive, simple, and general prompting method that effectively enhances the question-answering performance of LLMs by integrating three key steps: analyzing the question's intent, retrieving relevant information, and reasoning step by step.
Extensive experiments across diverse QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms the CoT prompting method.
Ablation and case studies further validate the positive contributions of each component, with intent analysis proving particularly crucial.
In addition, evaluations across various model sizes, LLM series, and generation configurations confirm the effectiveness, robustness, and generalizability of the proposed ARR method.



\clearpage
%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section*{Limitations}
\label{sec:limitations}

We did not explore variations or paraphrases of the proposed ARR prompts, opting instead for the straightforward expressions presented in this paper.
While certain phrasings may further enhance performance, the core idea remains the same.

In addition, resource constraints limited our focus to open-weights LLMs with no more than 8B parameters.
However, the results from model size experiments (\S~\ref{subsec:model_sizes}) align with the scaling laws for language models~\cite{kaplan2020scaling}, demonstrating the potential and generalizability of our ARR method when applied to larger models.

Lastly, we observe that some generated rationales in the Reasoning Generation stage are repetitive and redundant. A dynamic stopping strategy or post-processing to filter out redundancies can reduce the computational cost and potentially further boost the final QA accuracy.



\bigskip
%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
\section*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.

We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC).
% Nous remercions le Conseil de recherches en sciences naturelles et en gnie du Canada (CRSNG) de son soutien.
This research was supported in part by the computational resources and services provided by Advanced Research Computing at the University of British Columbia and the Digital Research Alliance of Canada (alliancecan.ca).
We would also like to thank UBC NLP Group members for their constructive feedback.


% \clearpage
\newpage

\bibliography{arr}

\clearpage
\appendix


\section{Experiment Details}
\label{app:experiment_detail}

\subsection{Dataset Details}
\label{app:dataset_detail}

All QA datasets used in this work are loaded from Hugging Face datasets\footnote{Data source: \url{https://huggingface.co/datasets}}. Table~\ref{tab:app_dataset_url} lists the URL link of each dataset. % \footnote{These URL links may change or expire in the future.}

\begin{table}[th]
    \centering
    \scalebox{0.95}{
    \begin{tabular}{lc}
    \toprule
    \midrule
    \textbf{QA Datasets} & \textbf{URL} \\
    \midrule
    % Reading Comprehension
    BoolQ~\cite{clark2019boolq} & \href{https://huggingface.co/datasets/aps/super_glue}{Link} \\
    LogiQA~\cite{liu2020logiqa} & \href{https://huggingface.co/datasets/EleutherAI/logiqa}{Link} \\
    % Commonsense Reasoning
    CSQA~\cite{talmor2019commonsenseqa} & \href{https://huggingface.co/datasets/tau/commonsense_qa}{Link} \\
    SIQA~\cite{sap2019social_iqa} & \href{https://huggingface.co/datasets/allenai/social_i_qa}{Link} \\
    % World Knowledge
    SciQ~\cite{welbl2017sciq} & \href{https://huggingface.co/datasets/allenai/sciq}{Link} \\
    OBQA~\cite{mihaylov2018OpenBookQA} & \href{https://huggingface.co/datasets/allenai/openbookqa}{Link} \\
    ARC~\cite{clark2018ai2_arc} & \href{https://huggingface.co/datasets/allenai/ai2_arc}{Link} \\
    % Multitask Understanding
    BBH~\cite{suzgun2023bbh} & \href{https://huggingface.co/datasets/lukaemon/bbh}{Link} \\
    MMLU~\cite{hendrycks2021mmlu} & \href{https://huggingface.co/datasets/hails/mmlu_no_train}{Link} \\
    MMLU-Pro~\cite{wang2024mmlu_pro} & \href{https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro}{Link} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{The URL links of adopted QA datasets.}
    \label{tab:app_dataset_url}
\end{table}


\subsection{Model Details}
\label{app:model_detail}

As mentioned in \S~\ref{subsec:models}, we mainly employ LLaMA3-8B-Chat~\cite{dubey2024llama3}, an instruction-following LLM with $8$ billion model parameters, for most experiments.
In generalizability experiments (\S~\ref{sec:generalizability}), we also explore LLaMA3-Chat models of different sizes in \S~\ref{subsec:model_sizes} and 7B-Chat models of different LLM series in \S~\ref{subsec:llm_series}, i.e., Qwen2.5~\cite{yang2024qwen2_5}, Gemma~\cite{google2024gemma,google2024gemma2}, and Mistral~\cite{jiang2023mistral}.
Table~\ref{tab:app_model_url} lists the URL link of each model and tokenizer provided by Hugging Face Transformers~\cite{wolf2020transformers}.\footnote{Model source: \url{https://huggingface.co/models}}

\begin{table}[th]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccc}
    \toprule
    \midrule
    \multicolumn{1}{c}{\textbf{LLM Series}} & \textbf{Size} & \textbf{Type} & \textbf{URL} \\
    \midrule
    \multirow{3}{*}{LLaMA3~\cite{dubey2024llama3}} & 8B & Chat & \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Link} \\
    & 3B & Chat & \href{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}{Link} \\
    & 1B & Chat & \href{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}{Link} \\
    \midrule
    Qwen2.5~\cite{yang2024qwen2_5} & 7B & Chat & \href{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}{Link} \\
    Gemma~\cite{google2024gemma,google2024gemma2} & 7B & Chat & \href{https://huggingface.co/google/gemma-7b-it}{Link} \\
    Mistral~\cite{jiang2023mistral} & 7B & Chat & \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{Link} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{The URL links of models and tokenizers.}
    \label{tab:app_model_url}
\end{table}


\subsection{LLM Generation Details}
\label{app:generation_detail}

For each experimental setting, the model needs to perform reasoning generation and option selection sessions on every QA dataset.
For each running session, all experiments are conducted on a single NVIDIA V100 GPU with 32GB memory except the few-shot experiments in \S~\ref{subsec:few_shot}, which use a single NVIDIA A100 GPU with 40GB memory since the input length is much longer considering the few-shot examples with rationales.
To avoid out-of-memory issue, all the models are loaded in a half-precision (\texttt{float16}) mode, and the generation batch size is $1$.
The input sequence is not truncated since we do not want to lose the context information or the answer trigger sentence $\phi$, but we set the maximum number of newly generated tokens as $512$ during reasoning generation.


\subsection{Reproducibility}
\label{app:reproducibility}

For the reproducibility of this work, we set the generation temperature as $0$ by default and disable token sampling for deterministic generation.
In addition, we pre-set the random seed for all random modules at the beginning of each experiment session.
By an unofficial tradition\footnote{\,``The answer to the ultimate question of life, the universe, and everything is forty-two.''~\cite{adams1979hitchhiker}}, we set $42$ as the random seed and do not tune the value.
To validate the reproducibility, we ran the main experiments twice and obtained the same results as shown in Table~\ref{tab:exp_main_qa}.
Our source code is available on GitHub: \url{https://github.com/YuweiYin/ARR}


\subsection{Experimental Cost}
\label{app:exp_cost}

In the reasoning generation stage, the total computational cost is approximately 6,900 GPU hours on NVIDIA V100 clusters (about 288 days) and 1,300 hours on A100 clusters (about 54 days).
We only use V100 clusters for option selection, and the overall running time is approximately 500 hours (about 21 days).
In the few-shot experiments, the expense for GPT-4o API calls to generate CoT and ARR rationales for few-shot examples is below US\$20.
% ~8,700 hours in total (362.5 days)



\bigskip
\section{More Cases}
\label{app:more_cases}

As discussed in \S~\ref{subsec:case_study}, this section presents additional cases (Table~\ref{app:more_case_boolq}--\ref{app:more_case_mmlu_pro}) from our main experiments (\S~\ref{sec:main_experiments}, Table~\ref{tab:exp_main_qa}) to provide a closer look at how the ARR method enhances reasoning and improves QA accuracy. We present one case for each QA dataset introduced in \S~\ref{subsec:task_dataset}.

\clearpage

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Passage}: Feather -- Contour feathers are \textcolor{blue}{not uniformly distributed} on the skin of the bird \textcolor{blue}{except in some groups} \\ such as the \textcolor{blue}{penguins}, ratites and screamers. In most birds the feathers grow from specific tracts of skin called pterylae; \\ between the pterylae there are regions which are free of feathers called apterylae (or apteria). Filoplumes and down \\ may arise from the apterylae. The arrangement of these feather tracts, pterylosis or pterylography, varies across \\ bird families and has been used in the past as a means for determining the evolutionary relationships of bird families. \\ \textbf{Question}: Do penguins have feathers arising from the epidermis?} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{No}, penguins have feathers arising from specific tracts of skin called \textcolor{red}{pterylae}. \\ The pterylae are the areas of skin where the feathers grow. The pterylae are not uniformly distributed on the skin of the penguin. \\ The feathers grow from the pterylae, and the areas between the pterylae are free of feathers and are called apterylae. \\ The arrangement of these feather tracts, pterylosis or pterylography, varies across bird families. \textcolor{red}{In penguins, the feathers} are \\ uniformly distributed on the skin, but they \textcolor{red}{arise from specific tracts of skin called pterylae}. \\ The feathers do not arise from the epidermis. The epidermis is the outermost layer of the skin, \\ and in most birds, the feathers grow from the dermal layer of the skin, not the epidermis.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{Penguins are a group of birds that have feathers uniformly distributed on their skin. The passage states that in most birds, \\ feathers grow from specific tracts of skin called pterylae, and between these tracts, there are regions called apterylae that are \\ free of feathers. However, \textcolor{red}{it also mentions that in some groups, such as penguins}, ratites, and screamers, \textcolor{red}{feathers are not} \\ \textcolor{red}{uniformly distributed} on the skin. This suggests that penguins do not have feathers arising from the epidermis. \\ Therefore, \textcolor{red}{the answer is no}. Penguins do not have feathers arising from the epidermis.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks about the distribution of feathers on penguins. The passage states that in most birds, feathers grow from \\ specific tracts of skin called pterylae, but \textcolor{teal}{penguins are an exception}. The passage does not mention the epidermis, but it does \\ mention that in penguins, feathers are uniformly distributed on the skin. This suggests that feathers in penguins may arise from \\ the epidermis, as there are no specific tracts of skin called pterylae. Therefore, \textcolor{teal}{the answer is yes}.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{Boolq} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail.}
    \label{app:more_case_boolq}
\end{table*}

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Passage}: There are five stations set up from west to east along a section of the high-speed railway along the river. \\ It is known that \textcolor{blue}{Fuyi Station is east of Haoyun Station and West of Huyao Station, and is adjacent to Huyao Station}; \\ \textcolor{blue}{Jiushang Station is adjacent to Yinling Station}. \\ \textbf{Question}: If \textcolor{blue}{Haoyun stands to the east of Jiushang Station}, you can get \\ \textbf{(A)} Yinling Station is adjacent to Haoyun Station and west of Haoyun Station; \\ \textcolor{teal}{\textbf{(B)}} Haoyun Station is adjacent to Fuyi Station and west of Fuyi Station; \\ \textbf{(C)} Jiushang Station is adjacent to Haoyun Station and west of Haoyun Station; \\ \textbf{(D)} Yinling Station is adjacent to Fuyi Station and west of Fuyi Station;} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(C)} Jiushang Station is adjacent to Haoyun Station and west of Haoyun Station; \\ (Explanation) Since Jiushang Station is adjacent to Yinling Station, and Yinling Station is adjacent to Haoyun Station, \\ \textcolor{red}{Jiushang Station is adjacent to Haoyun Station}. Since \textcolor{red}{Jiushang Station is east of Yinling Station}, \\ and Yinling Station is west of Haoyun Station, Jiushang Station is west of Haoyun Station. \\ Therefore, \textcolor{red}{Jiushang Station is adjacent to Haoyun Station and west of Haoyun Station}.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{If Haoyun stands to the east of Jiushang Station, then \textcolor{red}{Haoyun Station is west of Jiushang Station}. \\ Since Jiushang Station is adjacent to Yinling Station, \textcolor{red}{Haoyun Station is adjacent to Yinling Station.} \\ Since Fuyi Station is east of Haoyun Station, Fuyi Station is east of Yinling Station. \\ Since Huyao Station is west of Fuyi Station, Huyao Station is west of Yinling Station. \\ Since Jiushang Station is adjacent to Yinling Station, Huyao Station is adjacent to Jiushang Station \textcolor{red}{$\dots$}} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks us to determine the correct relationship between the stations based on the given information. \\ We are told that Fuyi Station is east of Haoyun Station and west of Huyao Station, and is adjacent to Huyao Station. \\ This means that \textcolor{teal}{Fuyi Station is between Haoyun Station and Huyao Station}. \\ Additionally, we know that Jiushang Station is adjacent to Yinling Station. \\ The question also states that Haoyun stands to the east of Jiushang Station. \\ This implies that \textcolor{teal}{Haoyun Station is to the east of Jiushang Station}. \\ Option (B) states that Haoyun Station is adjacent to Fuyi Station and west of Fuyi Station. \\ This is \textcolor{teal}{consistent with the given information} that Fuyi Station is east of Haoyun Station. \\ Therefore, the correct \textcolor{teal}{answer is option (B)}} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{LogiQA} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. \textbf{Analysis}: the correct station order can be Jiushang-Yinling-Haoyun-Fuyi-Huyao or Yinling-Jiushang-Haoyun-Fuyi-Huyao.}
    \label{app:more_case_logiqa}
\end{table*}

\clearpage

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Question}: What do people aim to do at work? \\ \textcolor{teal}{\textbf{(A)}} complete job \, \textbf{(B)} learn from each other \, \textbf{(C)} kill animals \, \textbf{(D)} wear hats \, \textbf{(E)} talk to each other} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(B)} learn from each other. The best answer is B. \\ Step 1: Understand the context of the question: The question is asking about the purpose or goal of people at work. \\ Step 2: Analyze the options given \\ Option (A) "complete job" is a task-oriented answer, but \textcolor{red}{it doesn't necessarily imply learning or personal growth}. \\ Option (C) "kill animals" is unrelated to the typical goals of work in most industries. \\ Option (D) "wear hats" is a superficial aspect of work attire and not a goal. \\ Option (E) "talk to each other" is a common aspect of work but doesn't necessarily imply learning. \\ Step 3: Identify the most appropriate answer: \\ Option (B) \textcolor{red}{"learn from each other" is a goal that aligns with personal and professional development}, \\ which is a common aim in many workplaces. It suggests a focus on growth, improvement, and collaboration, \\ which are key aspects of many work environments. \textcolor{red}{The final answer is: B}} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(B)} People go to work to earn a living, but they also go to learn from each other and to be part of a community. \\ Choice (A) is incorrect because \textcolor{red}{people do not go to work to complete a job. They go to work to do a job.} \\ Choice (C) is incorrect because people do not go to work to kill animals. \\ Choice (D) is incorrect because people do not go to work to wear hats. \\ Choice (E) is incorrect because people do not go to work to talk to each other. They go to work to do a job. \\ \textcolor{red}{The best answer is B}.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks what people aim to do at work. The relevant information is that people go to work to earn a living. \\ The answer is that people aim to complete their jobs at work. \\ Choice (B) is incorrect because \textcolor{teal}{people may learn from each other at work, but that is not the main purpose of going to work}. \\ Choice (C) is incorrect because people do not go to work to kill animals. \\ Choice (D) is incorrect because people do not go to work to wear hats. \\ Choice (E) is incorrect because people do not go to work to talk to each other. \\ \textcolor{teal}{The best answer is A}.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{Commonsense QA} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail.}
    \label{app:more_case_csqa}
\end{table*}

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Context}: Alex grew closer to their \textcolor{blue}{significant other} after they vacationed together. \\ \textbf{Question}: How would Alex feel afterwards? \\ \textcolor{teal}{\textbf{(A)}} in love \, \textbf{(B)} happy \, \textbf{(C)} back from vacation} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{ \textcolor{red}{(B) happy} \\ Reasoning Skill: Empathy And Perspective Taking \\ This question requires the test-taker to put themselves in Alex's shoes and understand how they would feel \\ after a positive experience like vacationing with their significant other. \\ The correct answer, "happy", is a natural emotional response to a pleasant experience. \\ The other options are not directly related to the emotional state of Alex after the vacation. \\ "In love" is a more intense and long-term feeling, "back from vacation" is a factual statement rather than \\ an emotional response, and "happy" is the most appropriate answer given the context.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{Alex grew closer to their significant other after they vacationed together. \\ This implies that the vacation strengthened their relationship. Therefore, Alex would likely feel happy after the vacation. \\ Reasoning skill: Empathy And Perspective Taking. \\ This question requires the reader to put themselves in Alex's shoes and understand how they would feel after a positive experience. \\ It also requires the reader to consider the context of the situation and how it would impact Alex's emotions. \textcolor{red}{The best answer is (B)}} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{Step 1: Identify the context of the question. The context is about Alex growing closer to their significant other after a vacation. \\ Step 2: Determine the emotional state of Alex after the vacation. The question asks how Alex would feel afterwards. \\ Step 3: Analyze the relationship between Alex and their significant other. \\ The question implies that the vacation strengthened their bond. \\ Step 4: Determine the emotional state of Alex based on the context. \\ Given that the vacation brought them closer together, it is likely that \textcolor{teal}{Alex would feel happy or in love}. \\ Step 5: Choose \textcolor{teal}{the most appropriate answer}. Between "happy" and "in love," \textcolor{teal}{"in love" is a more intense emotion} that \\ aligns with the context of \textcolor{teal}{growing closer to someone}. \\ \textcolor{teal}{The final answer is: (A) in love}.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{SocialIQA} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. \textbf{Analysis}: Option B is not wrong, but Option A is more appropriate as they are in a romantic relationship (``significant other'').}
    \label{app:more_case_siqa}
\end{table*}

\clearpage

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Question}: Rupture can cause fragments of what to travel via the bloodstream and become lodged in other arteries? \\ \textbf{(A)} red blood cells \, \textbf{(B)} white blood cells \, \textcolor{teal}{\textbf{(C)}} plaque \, \textbf{(D)} enamel} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{Rupture of an atherosclerotic plaque can cause fragments of the plaque to travel \\ via the bloodstream and become lodged in other arteries. This is known as an embolus. \\ Choice (B) is incorrect because white blood cells are not typically associated with atherosclerosis. \\ \textcolor{red}{Choice (C) is incorrect because plaque is the substance that ruptures}. \\ Choice (D) is incorrect because enamel is the hard, outer layer of the teeth. \textcolor{red}{The best answer is (A)}.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{The question is asking about the consequences of a rupture, which implies a physical injury or damage to a structure. \\ The correct answer should be related to the physical structure that \\ can be damaged and cause fragments to travel through the bloodstream. \\ Option (A) red blood cells are a type of cell that carries oxygen throughout the body via the bloodstream, \\ and they can be damaged or ruptured in the event of a physical injury or damage to a blood vessel. \\ Option (B) white blood cells are part of the immune system and help fight infections, \\ but they are not a physical structure that can rupture and cause fragments to travel through the bloodstream. \\ Option (C) plaque is a sticky film of bacteria that can cause dental problems, \\ \textcolor{red}{but it's not a physical structure that can rupture and cause fragments} to travel through the bloodstream. \\ Option (D) enamel is a hard, outer layer of teeth, \\ which is not directly related to the bloodstream or arteries. \\ Therefore, the correct answer is \textcolor{red}{(A) red blood cells}.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question is asking about the consequences of a rupture, which is likely referring to a rupture of an aneurysm or \\ an atherosclerotic plaque. In the context of aneurysms or atherosclerosis, a rupture can lead to the release of fragments into \\ the bloodstream. These fragments can then travel to other parts of the body and become lodged in other arteries, \\ potentially causing blockages or other complications. Considering the options provided, \textcolor{teal}{the most relevant choice is (C) plaque}, \\ as atherosclerotic plaque is a common cause of arterial blockages and can rupture, releasing fragments into the bloodstream. \\ The other options are \textcolor{teal}{not directly related to the consequences} of a rupture in the context of aneurysms or atherosclerosis.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{SciQ} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail.}
    \label{app:more_case_sciq}
\end{table*}

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Fact}: force causes the speed of an object to decrease. \\ \textbf{Question}: The amount of friction and the speed of an object have what kind of relationship? \\ \textcolor{teal}{\textbf{(A)}} inverse \, \textbf{(B)} reverse \, \textbf{(C)} direct \, \textbf{(D)} equal} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(C) Direct} \\ Explanation: The more friction an object has, the slower it will move. The less friction an object has, the faster it will move. \\ This is \textcolor{red}{a direct relationship}. The amount of friction and the speed of an object are \textcolor{red}{directly related}.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{Step 1: Understand the relationship between friction and speed \\ The relationship between friction and speed is that \textcolor{blue}{as friction increases, the speed of an object decreases}. \\ Step 2: Determine the type of relationship \\ Since \textcolor{blue}{an increase in friction leads to a decrease in speed}, \textcolor{red}{this is a direct relationship}. \\ \textcolor{blue}{A direct relationship means that as one variable increases, the other variable also increases}. \\ Step 3: Choose the correct answer \\ Based on the understanding of the relationship between friction and speed, the correct \textcolor{red}{answer is (C) direct}.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks about the relationship between the amount of friction and the speed of an object. \\ The relevant information is that \textcolor{blue}{force causes the speed of an object to decrease}. \\ The intent of the question is to \textcolor{blue}{determine the relationship between the amount of friction and the speed of an object}. \\ The correct \textcolor{teal}{answer is (A) inverse}. The amount of friction and the speed of an object have an inverse relationship. \\ The more friction, the slower the object moves. The less friction, the faster the object moves.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{OpenBookQA} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. \textbf{Analysis}: The CoT method provides a correct fact about the direct relationship (``A direct relationship means that $\dots$''), but wrongly infers that ``this is a direct relationship'' from ``an increase in friction leads to a decrease in speed.'' This is because the wrong statement is made without conditioning on the correct fact that is presented after the statement (due to the autoregressive generation). In contrast, the ARR method performs reasoning based on sufficient context after analyzing the intent and finding relevant information.}
    \label{app:more_case_obqa}
\end{table*}

\clearpage

\begin{table*}[ht]
    \centering
    \scalebox{0.67}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Question}: A scientist maps a long region in which earthquakes originate and determines this region is a transform plate boundary. \\ Which evidence would cause the scientist to reevaluate this determination? \\ \textcolor{teal}{\textbf{(A)}} Volcanism also characterizes the region. \\ \textbf{(B)} Earthquake centers in the region occur at shallow depths. \\ \textbf{(C)} The region shows extensive faulting of sediments. \\ \textbf{(D)} Equal crust densities are found on opposite sides of the region.} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{The best \textcolor{red}{answer is C}. \\ Transform plate boundaries are characterized by horizontal motion and faulting, but the faulting is not necessarily extensive. \\ Volcanism is characteristic of divergent and convergent plate boundaries, not transform plate boundaries. Shallow earthquake foci \\ are characteristic of transform plate boundaries. Equal crust densities on opposite sides of the region would indicate that \\ the region is a transform plate boundary. Therefore, (A), (B), and (D) are incorrect. \\ \textcolor{red}{(C) is correct because extensive faulting is characteristic of transform plate boundaries}.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{Transform plate boundaries are characterized by horizontal motion of the Earth's crust, which is evidenced by faulting. \\ Volcanism is characteristic of divergent and convergent plate boundaries, \textcolor{red}{not transform plate boundaries. Therefore, (A) is incorrect}. \\ Earthquakes at transform plate boundaries can occur at any depth, so (B) is incorrect. \\ \textcolor{red}{(C) is correct because faulting is a characteristic of transform plate boundaries}. \\ (D) is incorrect because transform plate boundaries are characterized by horizontal motion of the Earth's crust, \\ not by equal crust densities.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks for evidence that would cause a scientist to reevaluate the determination that a region is a transform plate boundary. \\ A transform plate boundary is a region where two plates slide past one another horizontally. The evidence that would \\ cause a scientist to reevaluate this determination \textcolor{teal}{would be evidence that is characteristic of a different type of plate boundary}. \\ A divergent plate boundary is a region where two plates move apart from one another, \\ and a convergent plate boundary is a region where two plates move toward one another. \\ \textcolor{teal}{Volcanism is characteristic of a divergent plate boundary, so (A) is correct}.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{ARC} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail.}
    \label{app:more_case_arc}
\end{table*}

\begin{table*}[ht]
    \centering
    \scalebox{0.67}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Question}: The following paragraphs each describe a set of three objects arranged in a fixed order. \\ The statements are logically consistent within each paragraph. A fruit stand sells three fruits: peaches, mangoes, and apples. \\ \textcolor{blue}{The peaches are more expensive than the apples. The mangoes are the cheapest.} \\ \textbf{(A)} The peaches are the second-most expensive \\ \textbf{(B)} The mangoes are the second-most expensive \\ \textcolor{teal}{\textbf{(C)}} The apples are the second-most expensive} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{The peaches are the second-most expensive}. The mangoes are the cheapest. The apples are the least expensive of the three fruits. \\ The peaches are more expensive than the apples. The mangoes are the cheapest. The peaches are the second-most expensive. \\ The mangoes are the cheapest. The apples are the least expensive of the three fruits. The peaches are more expensive than the apples. \\ The mangoes are the cheapest \textcolor{red}{$\dots$}} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{The peaches are more expensive than the apples. The mangoes are the cheapest. \\ Therefore, the mangoes are the least expensive of the three fruits. \\ \textcolor{blue}{The peaches are more expensive than the apples}, \textcolor{red}{so the peaches are the second-most expensive of the three fruits}. \\ Choice (B) is incorrect. The mangoes are the cheapest of the three fruits. Choice (C) is incorrect. \\ The apples are less expensive than the peaches. The best \textcolor{red}{answer is A}.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks for the second-most expensive fruit. The peaches are more expensive than the apples, \\ and the mangoes are the cheapest. Therefore, \textcolor{teal}{the second-most expensive fruit is the apples}. \\ Choice A is incorrect. The peaches are more expensive than the apples, and the mangoes are the cheapest. \\ Therefore, the peaches are the most expensive fruit, not the second-most expensive. \\ Choice B is incorrect. The mangoes are the cheapest fruit.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{BBH} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. \textbf{Analysis}: The Baseline method rushes to conclusions and then tries to present an explanation. However, the false statement confuses the whole reasoning process and makes the model caught in an infinite self-doubting loop.}
    \label{app:more_case_bbh}
\end{table*}

\clearpage

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Question}: Select the best translation into predicate logic. \\ David teaches Chris. (c: Chris; d: David; Txy: x teaches y) \\ \textcolor{teal}{(A)} Tdc \, \textbf{(B)} Tcd \, \textbf{(C)} Tcc \, \textbf{(D)} dTc} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(B)} The translation of the sentence into predicate logic is \textcolor{red}{Tcd}, where Txy means x teaches y. \\ \textcolor{blue}{The subject of the sentence is David, the object is Chris}, and the predicate is teaches. \\ The correct \textcolor{red}{translation is Tcd}, not Tdc, \textcolor{red}{because the subject comes before the predicate} in a predicate logic statement. \\ Choice (C) is incorrect because it \textcolor{red}{reverses the subject and object}. \\ Choice (D) is incorrect because it reverses the subject and predicate.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(B)} The predicate logic translation of the sentence is \textcolor{red}{Tcd}, where Txy means x teaches y. The best answer is B.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks for the best translation into predicate logic. The relevant information is the statement "David teaches Chris." \\ The statement is in the active voice, so the \textcolor{teal}{subject is the teacher, and the object is the student}. \\ The predicate logic translation is \textcolor{teal}{Tdc}, where Txy means x teaches y, c is Chris, and d is David. \\ Choice (B) is incorrect. This is the translation of "Chris teaches David." \\ Choice (C) is incorrect. This is the translation of "Chris teaches Chris." \\ Choice (D) is incorrect. This is the translation of "David is taught by Chris." \\ The best \textcolor{teal}{answer is A}.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{MMLU} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. \textbf{Analysis}: The Baseline and CoT methods jump to the wrong conclusions confidently, while the ARR method understands the question first and analyzes each option correctly.}
    \label{app:more_case_mmlu}
\end{table*}

\begin{table*}[ht]
    \centering
    \scalebox{0.68}{
    \begin{tabular}{rl}
    \toprule
    \midrule
    \textbf{Task Input} $x_i$ & \makecell[l]{\textbf{Question}: Which type of research methods are designed to elicit responses to predetermined, standardized questions \\ from many respondents? \\ (A) Non-probability. \, (B) Cross-sectional. \, (C) Qualitative. \, (D) Ethnographic. \, (E) Longitudinal. \\ (F) Experimental. \, (G) Probability. \, (H) Observational. \, (I) Case Study. \, (J) Quantitative.} \\
    \midrule
    \textbf{Baseline} $\phi_i$ & Answer: \\
    Reasoning $r_i$ & \makecell[l]{The best \textcolor{red}{answer is G}. Probability sampling is a type of research method that is designed to elicit responses to predetermined, \\ standardized questions from many respondents. Probability sampling is a type of sampling method that is used to \\ select a sample of participants from a larger population. Probability sampling methods include simple random sampling, \\ stratified sampling, and cluster sampling.} \\
    \midrule
    \textbf{CoT} $\phi_i$ & Answer: Let's think step by step. \\
    Reasoning $r_i$ & \makecell[l]{\textcolor{red}{(G)} Probability sampling is a method of selecting a sample from a population \\ in which every member of the population has an equal chance of being selected. \\ Probability sampling is used in research to ensure that the sample is representative of the population.} \\
    \midrule
    \textbf{ARR} $\phi_i$ & \makecell[l]{Answer: Let's analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning.} \\
    Reasoning $r_i$ & \makecell[l]{The question asks about research methods that are designed to elicit responses to predetermined, standardized questions \\ from many respondents. This is \textcolor{teal}{a clue that the research method is quantitative}. Quantitative research methods \\ are designed to elicit responses to predetermined, standardized questions from many respondents. \\ The correct \textcolor{teal}{answer is (J)}. The other choices are incorrect because they are not quantitative research methods.} \\
    \midrule
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Case study.} An instance from the \textbf{MMLU-Pro} dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. \textbf{Analysis}: The ARR method grasps the question's intent and implications before answering.}
    \label{app:more_case_mmlu_pro}
\end{table*}



\end{document}
