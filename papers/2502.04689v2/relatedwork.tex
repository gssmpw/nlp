\section{Related Work}
\label{sec:related_work}

\subsection{LLM Prompting}

Recent large language models (LLMs)~\cite{dubey2024llama3,lambert2024tulu3,liu2024deepseek_v3} are pre-trained on large-scale text corpora curated from the Internet~\cite{soldaini2024dolma,penedo2024fineweb,weber2024redpajama}. % dodge2021c4
Their advanced text understanding and generation capabilities~\cite{hurst2024gpt4o,anthropic2024claude3,team2024gemini1_5} have significantly revolutionized the field of natural language processing (NLP).
Consequently, the NLP paradigm is shifting toward a framework comprising pre-training, post-training, and prompting~\cite{liu2023prompt}, with post-training focusing on aligning models with human preferences~\cite{ouyang2022rlhf,bai2022rlaif,rafailov2023dpo} rather than fine-tuning for specific downstream tasks~\cite{devlin2019bert}.
After the training stages, LLMs can generate satisfactory responses to natural language instructions and questions, highlighting the growing importance of prompt design~\cite{white2023prompt,giray2023prompt,sahoo2024prompt}.
In this work, we propose an intuitive, general, and effective prompting method to enhance LLM performance in question-answering.

\subsection{LLM Reasoning}

Recent LLM research increasingly emphasizes reasoning abilities~\cite{qiao2023reasoning_survey,sun2023reasoning_survey}.
Chain-of-Thought (CoT) is a prompting strategy that enhances problem-solving by guiding LLMs to generate intermediate reasoning steps.
Main variants include zero-shot CoT~\cite{kojima2022cot_think_step_by_step} that uses general instructions such as ``Let's think step by step'' and few-shot CoT~\cite{wei2022cot} that provides exemplars with rationales to leverage in-context learning~\cite{openai2020gpt3,dong2024icl_survey}.
Building on CoT, various reasoning techniques have emerged~\cite{zhou2023least_to_most,zhou2023large,wang2023plan,yasunaga2024analogical,wang2024cot_no_prompting}.
Some studies explore optimal reasoning paths through self-consistency~\cite{wang2023self_consistency,chen2023uni_self_consistency} or tree-like searches~\cite{yao2023tot},
while others investigate self-refinement~\cite{aman2023self_refine}, self-correction~\cite{huang2024self_correct_no,tyen2024llms_correct,chen2024teaching}, self-verification~\cite{cobbe2021gsm8k,li2023making,lightman2024verify}, and self-evolution~\cite{guan2025rstar,lee2025evolving} mechanisms.
Beyond prompting and generation-based approaches, post-training methods~\cite{chu2025post_training}, particularly those leveraging reinforcement learning (RL)~\cite{sutton2018rl}, have been developed to enhance reasoning capabilities~\cite{shao2024deepseek_math,wang2024offline_rl,setlur2024rewarding,xu2025lrm}.
As a reasoning-eliciting prompting approach, ARR effectively complements existing research by guiding LLMs through three essential steps: intent analysis, information retrieval, and step-by-step reasoning.


\subsection{Retrieval-Augmented Generation}

Retrieval-Augmented Generation (RAG) enhances output quality by retrieving relevant information from pre-processed knowledge sources~\cite{gao2023rag4llm_survey}.
The retrieving component of our ARR method is inspired by the traditional ``external RAG'' approach~\cite{lewis2020rag4nlp}, which retrieves relevant information from the explicit context or outer sources, and realizes instead a form of ``internal RAG,'' which utilizes language models as implicit knowledge bases~\cite{petroni2019lm_as_kb,jiang2020lm_know} and extracts references from memory (training data)~\cite{carlini2021extracting,shi2024detecting}.
This retrieval mechanism is essential for enhancing LLM performance in question answering, as irrelevant information can significantly degrade accuracy~\cite{jones2022capturing,shi23distract,yoran2024making}.



%%%%%%%%%% # %%%%%%%%%% # SECTION # %%%%%%%%%% # %%%%%%%%%%
%