


\section{Blueprinting Reasoning LLMs}\label{replication}

In this section, we first analyze the features of reasoning LLMs from both output behavior and training dynamics perspectives. We then provide a detailed overview of the core methods that enable their advanced reasoning capabilities. Finally, we summarize the evolution of reasoning LLMs. 
A comprehensive comparison of traditional reasoning models and reasoning LLMs is shown in Figure \ref{fig:compare}.





\subsection{Analysis of the Features of Reasoning LLMs}\label{o1_features}


\subsubsection{Output Behaviour Perspective}\label{output_behaviour}

\textbf{Explore and Planning Structure:} Recent empirical studies have revealed that reasoning LLMs demonstrate a strong tendency for exploratory behavior in their output structures, especially when compared to models such as WizardMath \cite{wizardmath} and DeepSeekMath \cite{deepseekmath}, which primarily rely on conventional CoT reasoning approaches. 
This exploratory behavior is evident in the formulation of novel hypotheses and the pursuit of alternative solution paths. 
Research by \cite{TowardsSystem2ReasoninLLM} suggests that slow-thinking models engage in a latent generative process, particularly noticeable during the prediction of subsequent tokens. 
This claim is supported by \cite{Deepseek-R1}, which observes that similar behaviors naturally arise during RL scale training. 
Furthermore, the Quiet-STaR framework \cite{QuietStar} introduces an auxiliary pre-training phase focused on next-token prediction, highlighting the critical role of internal deliberation and exploratory mechanisms prior to content generation. 
Collectively, these findings underscore the complex and dynamic nature of reasoning processes in advanced LLMs, emphasizing the interaction between exploration and structured reasoning within their operational frameworks.

\noindent\textbf{Verification and Check Structure:} Analysis of OpenAI's o1 \cite{openai_o1} and o3 \cite{o3-mini} models indicates that their reasoning frameworks incorporate both macro-level actions for long-term strategic planning and micro-level actions, including ``\textit{Wait}'', ``\textit{Hold on}'', ``\textit{Alternatively}'', and ``\textit{Let’s pause}''. 
These micro actions facilitate meticulous verification and iterative checking processes, ensuring precision in task execution. 
Such a dual-layered approach underscores the models' capacity to balance overarching goals with granular, detail-oriented operations, thereby enhancing their overall functionality and reliability. 
To emulate this characteristic, Marco-o1 \cite{Marco_o1}, during the MCTS process for constructing Long-CoT, assigns each tree node the state of ``\textit{Wait! Maybe I made some mistakes! I need to rethink from scratch}'', thereby facilitating the reflective nature of Long-CoT. 
Huatuo-o1 \cite{Huatuo-o1} employs a multi-agent framework to address the issue of incorrect CoT generation during validation. 
This is achieved by incorporating a prompt with ``\textit{Backtracking}'' and ``\textit{Correction}'' functionalities,  which enables the correction process.

\noindent\textbf{Longer Inference Length \& Time:} 
Recent research \cite{TowardsSystem2ReasoninLLM, o1_Journey_Part1, o1_Journey_Part2, huang2025o1, reflection_window} indicates that reasoning LLMs often generate outputs exceeding 2000 tokens to tackle complex problems in coding and mathematics. 
However, this extended output length can sometimes lead to overthinking, where the model spends excessive time on a problem without necessarily improving the solution. 
Studies \cite{TowardsSystem2ReasoninLLM} highlight that while autoregressive generation and Classic CoT can effectively solve simpler problems, they struggle with more complex tasks. 
Research \cite{VisualSlowAgent, SlowPerception} shows that in multimodal domains, many problems demand careful observation, comparison, and deliberation. 
Additionally, Search-o1 \cite{li2025search} suggests that slow-thinking mechanisms are particularly beneficial in areas requiring external knowledge or where potential knowledge conflicts arise. 
In medical scenarios, complex problems, such as those requiring test-time scaling techniques, demonstrate significant improvements \cite{huang2025o1}.



\noindent\textbf{Overly Cautious \& Simple Problem Trap:} Currently, reasoning LLMs have demonstrated strong performance in domains such as competitive-level mathematics \cite{Deepseek-R1, qwq-32b-preview, RedStar, sky_t1_2025}, complex coding \cite{o1_coder}, medical question answering \cite{Huatuo-o1, huang2025o1}, and multilingual translation \cite{Marco_o1, DRT-o1}. 
These scenarios require the model to perform fine-grained analysis of the problem and execute careful logical reasoning based on the given conditions. 
Interestingly, even for straightforward problems like ``\textit{2+3=?}'', reasoning LLMs can exhibit overconfidence or uncertainty. 
Recent research \cite{Tecent_2_plus_3} notes that o1-like models tend to generate multiple solution rounds for easier math problems, often exploring unnecessary paths. 
This behavior contrasts with the lack of diverse exploratory actions for simpler questions, indicating a potential inefficiency in the model's reasoning process.


\subsubsection{Training Dynamic Perspective}\label{dynamic_perspective}

\textbf{Amazing Data Efficiency:} Unlike traditional approaches that focus on expanding instruction sets with uniformly distributed difficulty levels, Studies \cite{huang2025o1, RedStar} suggest that constructing Slow-thinking CoT datasets with a focus on hard samples leads to better generalization in fields like medicine and mathematics. 
This approach diverges from the conventional practice of collecting diverse and evenly distributed instruction datasets.



\noindent\textbf{Sparse Training Method:} 
Contrary to conventional wisdom, the development of effective reasoning LLMs does not require extensive datasets or dense reward signals. 
For example, STILL2 \cite{o1_Journey_Part2} demonstrated impressive performance using only 5,000 distilled samples, while Sky-T1 \cite{sky_t1_2025} achieved performance parity with QwQ \cite{qwq-32b-preview} using just 17,000 LongCoT samples. 
Similarly, RedStar \cite{RedStar} achieved exceptional results across both textual and multimodal tasks with only 4,000 core LongCoT samples. 
In comparison to simple CoT, Slow-thinking Supervised Fine-Tuning (SFT) data exhibits remarkable sample efficiency, often delivering comparable results with just 1/100th of the sample size. 
Additionally, research \cite{simplerl_reason_blob} emphasizes the significant training potential of online RL scaling algorithms, suggesting that non-dense RL supervision and even rule-based reward structures are sufficient for achieving high performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/techs.pdf}
    \caption{The core methods enabling reasoning LLMs.}
    \label{fig:techs}
\end{figure}



\noindent\textbf{Parameter Characteristic:} 
Training LLMs for slow-thinking, as characterized by the LongCoT approach, results in relatively uniform gradient norms across different layers. 
In contrast, fast-thinking, exemplified by the simplified CoT method, generates larger gradient magnitudes in the earlier layers, along with significant variability in gradient norms across layers. 
Empirical evidence suggests that larger models, particularly those exceeding 30 billion parameters, are more compatible with reasoning LLMs training due to their enhanced capacity for complex reasoning. 
Additionally, experiments conducted by RedStar \cite{RedStar} show that the benefits of data scaling vary across model sizes, with scaling effects being more pronounced and effective in larger models. 
This finding is supported by Deepseek-R1's research \cite{Deepseek-R1}, which demonstrates that a 670-billion-parameter model achieves performance metrics closely approximating those of the o1 benchmark, highlighting the scalability advantages of larger architectures in advanced reasoning tasks.








\subsection{Core Method}\label{foundations}

In this section, we provide an overview of the core methods that drive the advanced reasoning capabilities of reasoning LLMs, as shown in Figure \ref{fig:techs}. These include Structure Search, Reward Modeling, Self Improvement, Macro Action, and Reinforcement Fine-Tuning. 
We also highlight representative reasoning LLMs for each method.







\begin{table*}[tbp]
\centering
\renewcommand\arraystretch{1.2}
\caption{Summary of Structure Search method based on the definition of actions and rewards.}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{l|lp{26em}ll}
\toprule[1.2pt]
 & {{\textbf{Category}}} & \textbf{Reasoning LLMs}  & \textbf{Characteristic}\\
\hline
\multirow{4}{*}{\rotatebox{90}{{\textbf{Actions}}}} & \cellcolor[rgb]{ .949,  .949,  .949} {Reasoning Steps as Nodes} & \cellcolor[rgb]{ .949,  .949,  .949} RAP \cite{hao2023reasoning}, ORM \cite{wan2024alphazero}, Forest-of-Thought \cite{DBLP:journals/corr/abs-2412-09078}  & \cellcolor[rgb]{ .949,  .949,  .949} Actions represent intermediate reasoning steps. \\ 

& {Token-level Decisions}     &  CodeTree \cite{DBLP:journals/corr/abs-2411-04329}, SPaR \cite{DBLP:journals/corr/abs-2412-11605}, TreeBoN \cite{DBLP:journals/corr/abs-2410-16033}  &  Actions involve generating tokens.              \\

& \cellcolor[rgb]{ .949,  .949,  .949} {Task-specific Structures}      & \cellcolor[rgb]{ .949,  .949,  .949} CWM \cite{DBLP:journals/corr/abs-2405-15383}, LLM-MCTS \cite{DBLP:conf/nips/ZhaoLH23} & \cellcolor[rgb]{ .949,  .949,  .949} Actions are domain-specific.                 \\

&  {Correction and Exploration}  &  RethinkMCTS \cite{DBLP:journals/corr/abs-2409-09584}, MCTSr \cite{DBLP:journals/corr/abs-2406-07394} & Actions focus on revisiting and refining previous steps.
\\ %Actions emphasize revisiting, refining, or backtracking to improve previous reasoning steps.

\hline

\multirow{4}{*}{\rotatebox{90}{{\textbf{Rewards}}}} &\cellcolor[rgb]{ .949,  .949,  .949}  {Outcome-based Rewards}     & \cellcolor[rgb]{ .949,  .949,  .949} MC-NEST \cite{DBLP:journals/corr/abs-2411-15645}  & \cellcolor[rgb]{ .949,  .949,  .949} Correctness or validity of the final outcome.  \\ 

&  {Stepwise Evaluations}          & RAP \cite{hao2023reasoning}, SRA-MCTS \cite{DBLP:journals/corr/abs-2411-11053}    &  Rewards are assigned at intermediate steps. \\ 

& \cellcolor[rgb]{ .949,  .949,  .949} {Self-evaluation Mechanisms}              & \cellcolor[rgb]{ .949,  .949,  .949} SPaR \cite{DBLP:journals/corr/abs-2412-11605}, TreeBoN \cite{DBLP:journals/corr/abs-2410-16033}, MindStar \cite{DBLP:journals/corr/abs-2405-16265} & \cellcolor[rgb]{ .949,  .949,  .949} Rewards rely on the model's own confidence.    \\ 

&  {Domain-specific Criteria}          &  LLM-MCTS \cite{DBLP:conf/nips/ZhaoLH23}, SR-MCTS \cite{DBLP:journals/corr/abs-2411-04459}  &  Rewards are tailored to specific tasks. \\ 

& \cellcolor[rgb]{ .949,  .949,  .949} {Iterative Preference Learning}     &\cellcolor[rgb]{ .949,  .949,  .949} LLaMA-Berry \cite{DBLP:journals/corr/abs-2410-02884}, Marco-o1 \cite{Marco_o1}, ReST-MCTS* \cite{DBLP:journals/corr/abs-2406-03816}   & \cellcolor[rgb]{ .949,  .949,  .949} Rewards derive from comparing multiple solutions.\\

\bottomrule[1.2pt]
\end{tabular}
}
\label{tab:structure_search}
\end{table*}



% \begin{table*}[tbp]
% \centering
% \caption{Categories of Structure Search technology based on the definition of actions and rewards.}
% \begin{small}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}lll@{}}
% \toprule
% \rowcolor[HTML]{FFFFFF} 
% \textbf{Method} &
%   \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{Category}} &
%   \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{Characteristic}} \\ \midrule
% \rowcolor[HTML]{ECF4FF} 
% \multicolumn{3}{c}{\cellcolor[HTML]{ECF4FF}\textbf{Actions-based}}                                                                              \\ \midrule
% \rowcolor[HTML]{FFFFFF} 
% RAP \cite{hao2023reasoning}               & Reasoning Steps as Nodes      & \cellcolor[HTML]{FFFFFF}                                                                    \\
% \rowcolor[HTML]{FFFFFF} 
% ORM \cite{wan2024alphazero}               & Reasoning Steps as Nodes      & \cellcolor[HTML]{FFFFFF}                                                                    \\
% \rowcolor[HTML]{FFFFFF} 
% Forest-of-Thought \cite{DBLP:journals/corr/abs-2412-09078} & Reasoning Steps as Nodes      & \multirow{-3}{*}{\cellcolor[HTML]{FFFFFF}Actions represent intermediate reasoning steps.}   \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% CodeTree \cite{DBLP:journals/corr/abs-2411-04329}         & Token-level Decisions         & \cellcolor[HTML]{EFEFEF}                                                                    \\
% \rowcolor[HTML]{EFEFEF} 
% SPaR \cite{DBLP:journals/corr/abs-2412-11605}              & Token-level Decisions         & \cellcolor[HTML]{EFEFEF}                                                                    \\
% \rowcolor[HTML]{EFEFEF} 
% TreeBoN \cite{DBLP:journals/corr/abs-2410-16033}           & Token-level Decisions         & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}Actions involve generating tokens.}                \\ \midrule
% \rowcolor[HTML]{FFFFFF} 
% CWM \cite{DBLP:journals/corr/abs-2405-15383}               & Task-specific Structures      & \cellcolor[HTML]{FFFFFF}                                                                    \\
% \rowcolor[HTML]{FFFFFF} 
% LLM-MCTS \cite{DBLP:conf/nips/ZhaoLH23}         & Task-specific Structures      & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Actions are domain-specific.}                      \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% RethinkMCTS \cite{DBLP:journals/corr/abs-2409-09584}      & Correction and Exploration    & \cellcolor[HTML]{EFEFEF}                                                                    \\
% \rowcolor[HTML]{EFEFEF} 
% MCTSr \cite{DBLP:journals/corr/abs-2406-07394} &
%   Correction and Exploration &
%   \multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}Actions emphasize revisiting, refining, or backtracking to improve previous reasoning steps.} \\ \midrule
% \rowcolor[HTML]{ECF4FF} 
% \multicolumn{3}{c}{\cellcolor[HTML]{ECF4FF}\textbf{Rewards-based}}                                                                              \\ \midrule
% \rowcolor[HTML]{FFFFFF} 
% MC-NEST \cite{rabby2024mcnestenhancingmathematical}          & Outcome-based Rewards         & Correctness or validity the final outcome.                                                  \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% RAP \cite{hao2023reasoning}               & Stepwise Evaluations          & \cellcolor[HTML]{EFEFEF}                                                                    \\
% \rowcolor[HTML]{EFEFEF} 
% SRA-MCTS \cite{DBLP:journals/corr/abs-2411-11053}         & Stepwise Evaluations          & \multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}Rewards are assigned at intermediate steps.}       \\ \midrule
% \rowcolor[HTML]{FFFFFF} 
% SPaR \cite{DBLP:journals/corr/abs-2412-11605}             & Self-evaluation Mechanisms    & \cellcolor[HTML]{FFFFFF}                                                                    \\
% \rowcolor[HTML]{FFFFFF} 
% TreeBoN \cite{DBLP:journals/corr/abs-2410-16033}          & Self-evaluation Mechanisms    & \cellcolor[HTML]{FFFFFF}                                                                    \\
% \rowcolor[HTML]{FFFFFF} 
% MindStar \cite{DBLP:journals/corr/abs-2405-16265}          & Self-evaluation Mechanisms    & \multirow{-3}{*}{\cellcolor[HTML]{FFFFFF}Rewards rely on the model's own confidence.}       \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% LLM-MCTS \cite{DBLP:conf/nips/ZhaoLH23}         & Domain-specific Criteria      & \cellcolor[HTML]{EFEFEF}                                                                    \\
% \rowcolor[HTML]{EFEFEF} 
% SR-MCTS \cite{DBLP:journals/corr/abs-2411-04459}          & Domain-specific Criteria      & \multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}Rewards are tailored to specific tasks.}           \\ \midrule
% \rowcolor[HTML]{FFFFFF} 
% LLaMA-Berry \cite{DBLP:journals/corr/abs-2410-02884}       & Iterative Preference Learning & \cellcolor[HTML]{FFFFFF}                                                                    \\
% \rowcolor[HTML]{FFFFFF} 
% Marco-o1 \cite{Marco_o1}         & Iterative Preference Learning & \cellcolor[HTML]{FFFFFF}                                                                    \\
% \rowcolor[HTML]{FFFFFF} 
% ReST-MCTS* \cite{DBLP:journals/corr/abs-2406-03816}        & Iterative Preference Learning & \multirow{-3}{*}{\cellcolor[HTML]{FFFFFF}Rewards derive from comparing multiple solutions.} \\ \bottomrule
% \end{tabular}%
% }
% \label{tab:structure_search}
% \end{small}
% \end{table*}




\subsubsection{Structure Search}\label{structure_search}
 
Reasoning LLMs aim to achieve high accuracy and depth in solving complex problems by emulating the deliberate nature of human reasoning. 
However, despite recent advancements, current foundational LLMs face inherent limitations when addressing intricate reasoning tasks. 
These limitations arise from their lack of an internal world model to simulate environmental states, their inability to predict the long-term outcomes of reasoning paths, and their failure to iteratively refine reasoning steps based on future states or rewards \cite{huang2023towards}. 
As a result, these shortcomings hinder foundational LLMs from effectively balancing exploration and exploitation in vast reasoning spaces, creating challenges in tasks that require multi-step reasoning, such as complex mathematics, logical inference, or strategic decision-making \cite{DBLP:journals/corr/abs-2405-00451}.

MCTS, a powerful search and optimization algorithm, effectively addresses these challenges by providing a structured framework to explore and evaluate reasoning paths systematically. 
It operates by constructing a reasoning tree, where each node represents a reasoning state, and actions expand the tree by considering potential next steps. 
Through the simulation of future states and the iterative backpropagation of estimated rewards, MCTS allows foundational LLMs to efficiently identify high-reward reasoning paths, mirroring human planning processes. 
This approach aligns with the core principles of reasoning LLMs, where thorough analysis and deliberate exploration are essential for generating well-reasoned outputs. 
Recent methods, such as RAP \cite{hao2023reasoning}, enhance foundational LLMs by integrating MCTS with a world model, enabling the system to iteratively refine intermediate reasoning steps and improve future predictions. 
Similarly, Forest-of-Thought \cite{DBLP:journals/corr/abs-2412-09078} utilizes MCTS to dynamically explore multiple reasoning trajectories, revisiting flawed paths and refining outcomes.





The application of MCTS in reasoning tasks extends beyond traditional problem-solving to highly specialized domains. 
For example, frameworks like SRA-MCTS \cite{DBLP:journals/corr/abs-2411-11053} and MC-NEST \cite{DBLP:journals/corr/abs-2411-15645} showcase the utility of MCTS in tackling technical challenges such as code generation and mathematical reasoning, where intermediate steps are iteratively evaluated and refined. 
In fields like instructional alignment, frameworks such as SPaR \cite{DBLP:journals/corr/abs-2412-11605} and Marco-o1 \cite{Marco_o1} leverage MCTS to refine responses and align reasoning trajectories with human preferences or desired outcomes. 
Additionally, task-specific implementations like HuatuoGPT-o1 \cite{Huatuo-o1} underscore MCTS's crucial role in navigating highly specialized domains, such as medical reasoning, where accuracy and robustness are paramount.

MCTS also enables models to go beyond single-pass reasoning methods, such as CoT or Tree-of-Thought, by incorporating mechanisms to revisit, critique, and refine reasoning steps dynamically \cite{DBLP:journals/corr/abs-2407-01476, DBLP:journals/corr/abs-2409-09584}. 
This iterative capability is essential for tackling tasks with vast decision spaces or those requiring long-term planning, where earlier decisions can significantly impact final outcomes. 
By allowing LLMs to simulate, evaluate, and refine multiple reasoning paths, MCTS introduces a level of adaptability and strategic exploration that traditional approaches lack. 
As shown by AlphaZero-like tree-search \cite{wan2024alphazero} and Search-o1 \cite{li2025search}, MCTS enables reasoning LLMs to not only achieve better performance on specific tasks but also exhibit enhanced generalization capabilities across diverse domains.





The integration of MCTS into LLMs depends on defining actions and rewards to guide reasoning path exploration and assess quality. As shown in Table \ref{tab:structure_search}, we classify the actions in prior work into four categories: 
\begin{enumerate}[itemindent=0em]
\item \textbf{Reasoning Steps as Nodes:} Actions represent intermediate reasoning steps or decisions, such as selecting rules, applying transformations, or generating sub-questions \cite{hao2023reasoning, DBLP:journals/corr/abs-2405-00451, wan2024alphazero, DBLP:journals/corr/abs-2412-09078}.

\item \textbf{Token-level Decisions:} Actions involve generating tokens or sequences (\emph{e.g.}, the next word, phrase, or code snippet) \cite{liu2024dontthrowawayvalue, DBLP:journals/corr/abs-2411-04329, DBLP:journals/corr/abs-2412-11605, DBLP:journals/corr/abs-2410-16033}.

\item \textbf{Task-specific Structures:} Actions are domain-specific, such as moving blocks in blocksworld, constructing geometry in geometry problem-solving, or modifying workflows in task planning \cite{DBLP:journals/corr/abs-2412-10673, DBLP:journals/corr/abs-2405-15383, DBLP:conf/nips/ZhaoLH23}.

\item \textbf{Self-correction and Exploration:} Actions focus on revisiting, refining, or backtracking to improve previous reasoning steps \cite{jiang2024intrinsicselfcorrectionenhancementmonte, DBLP:journals/corr/abs-2409-09584, DBLP:journals/corr/abs-2406-07394}.
\end{enumerate}




Additionally, as illustrated in Table \ref{tab:structure_search}, we classify the reward design into five categories:
\begin{enumerate}[itemindent=0em]
\item \textbf{Outcome-based Rewards:} Rewards focus on the correctness or validity of the final outcome or solution, including the validation of reasoning paths or task success \cite{DBLP:journals/corr/abs-2405-00451, DBLP:journals/corr/abs-2411-15645, DBLP:journals/corr/abs-2412-10673}.

\item \textbf{Stepwise Evaluations:} Rewards are assigned at intermediate steps based on the quality of each step or its contribution toward the final outcome \cite{hao2023reasoning, DBLP:journals/corr/abs-2411-11053, wan2024alphazero}.

\item \textbf{Self-evaluation Mechanisms:} Rewards rely on the model’s own confidence or self-assessment (\emph{e.g.}, likelihood, next-word probability, or confidence scores) \cite{DBLP:journals/corr/abs-2412-11605, DBLP:journals/corr/abs-2410-16033, DBLP:journals/corr/abs-2405-16265}.

\item \textbf{Domain-specific Criteria:} Rewards are tailored to specific tasks, such as symmetry and complexity in geometry or alignment with human preferences in text generation \cite{DBLP:journals/corr/abs-2412-10673, DBLP:conf/nips/ZhaoLH23, DBLP:journals/corr/abs-2411-04459}.

\item \textbf{Iterative Preference Learning:} Rewards are derived from comparing multiple solutions or reasoning paths, guiding learning dynamically \cite{DBLP:journals/corr/abs-2410-02884, Marco_o1, DBLP:journals/corr/abs-2406-03816}.

\end{enumerate}

\noindent\textbf{Summary:} Despite its advantages, structure search-based (\emph{i.e.}, MCTS) reasoning LLMs often suffer from substantial computational overhead due to the large number of simulations required. This makes them less suitable for tasks that demand real-time decision-making or operate under resource constraints \cite{DBLP:journals/corr/abs-2309-03224}. 
Additionally, the effectiveness of MCTS is highly dependent on well-designed reward mechanisms and action definitions, which can vary significantly across different domains, thus posing challenges to its generalizability \cite{DBLP:journals/apin/KemmerlingLS24}.

\begin{table*}[t!]
\centering
\renewcommand\arraystretch{1.2}
\caption{Summary of Reward Modeling method.}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{lllllll}
\toprule[1.2pt]
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Data Source}} & \multicolumn{2}{c}{\textbf{Model Refinement}} & \multirow{2}{*}{\textbf{Applications}}      & \multirow{2}{*}{\textbf{Characteristic}}  \\
\cline{4-5}
                          &                          &                                & \textbf{Strategy}              & \textbf{Learning}       &                                   &                                  \\
                          \hline
\multirow{5}{*}{ORM}      & \cellcolor[rgb]{ .949,  .949,  .949}DIVERSE\cite{li2022making}                 & \cellcolor[rgb]{ .949,  .949,  .949}Prompting                      & \cellcolor[rgb]{ .949,  .949,  .949}Fine-tuning           & \cellcolor[rgb]{ .949,  .949,  .949}SFT            & \cellcolor[rgb]{ .949,  .949,  .949}Multiple Reasoning Tasks        & \cellcolor[rgb]{ .949,  .949,  .949}Weighted Voting   Verifier       \\
                          & MATH-SHEPHERD\cite{wang2024math}            & Sampling                       & Feedback-guided       & SFT \& RL      & Math Reasoning                    & Correctness Score Assignment   \\
                          & \cellcolor[rgb]{ .949,  .949,  .949}AutoPSV\cite{lu2024autopsv}                  & \cellcolor[rgb]{ .949,  .949,  .949}Prompting                      & \cellcolor[rgb]{ .949,  .949,  .949}Feedback-guided       & \cellcolor[rgb]{ .949,  .949,  .949}SFT            & \cellcolor[rgb]{ .949,  .949,  .949}Math / Commonsense Reasoning    & \cellcolor[rgb]{ .949,  .949,  .949}Automated Process Supervision  \\
                          & Implicit PRMs\cite{yuan2024free}            & Sampling                       & Fine-tuning           & SFT \& RL      & Math Reasoning                    & Obtaining PRM from ORM         \\
                          & \cellcolor[rgb]{ .949,  .949,  .949}OVM\cite{yu2024ovm}                      & \cellcolor[rgb]{ .949,  .949,  .949}Sampling                       & \cellcolor[rgb]{ .949,  .949,  .949}Feedback-guided       & \cellcolor[rgb]{ .949,  .949,  .949}SFT            & \cellcolor[rgb]{ .949,  .949,  .949}Math Reasoning                    & \cellcolor[rgb]{ .949,  .949,  .949}Guided Decoding                  \\
                          \hline
\multirow{4}{*}{MCTS}     & ReST-MCTS$^{*}$\cite{zhang2024rest}              & Sampling                       & Self-training         & SFT \& RL      & Multiple Reasoning Tasks        & MCTS and Self-training        \\
                          & \cellcolor[rgb]{ .949,  .949,  .949}OmegaPRM\cite{luo2024improve}                 & \cellcolor[rgb]{ .949,  .949,  .949}MCTS with Binary Search      & \cellcolor[rgb]{ .949,  .949,  .949}Feedback-guided       & \cellcolor[rgb]{ .949,  .949,  .949}SFT            & \cellcolor[rgb]{ .949,  .949,  .949}Math Reasoning                    & \cellcolor[rgb]{ .949,  .949,  .949}Divide-and-Conquer MCTS        \\
                          & ReARTeR\cite{sun2025rearter}                  & Sampling                       & Feedback-guided       & SFT \& RL      & QA                                & Retrieval-Augmented   Generation \\
                          & \cellcolor[rgb]{ .949,  .949,  .949}Consensus Filtering\cite{zhang2025lessons}      & \cellcolor[rgb]{ .949,  .949,  .949}MCTS Data   Construction       & \cellcolor[rgb]{ .949,  .949,  .949}Feedback-guided       & \cellcolor[rgb]{ .949,  .949,  .949}SFT            & \cellcolor[rgb]{ .949,  .949,  .949}Math Reasoning                    & \cellcolor[rgb]{ .949,  .949,  .949}Consensus Filtering Mechanism  \\
                          \hline
\multirow{3}{*}{PRM}      & ORPS\cite{yu2024outcome}                     & Sampling                       & Feedback-guided       & SFT            & Code Generation                   & Supervising Outcome Refinement \\
                          & \cellcolor[rgb]{ .949,  .949,  .949}Step-DPO\cite{lai2024step}                 & \cellcolor[rgb]{ .949,  .949,  .949}Sampling                       & \cellcolor[rgb]{ .949,  .949,  .949}Feedback-guided       & \cellcolor[rgb]{ .949,  .949,  .949}SFT \& RL      & \cellcolor[rgb]{ .949,  .949,  .949}Math Reasoning                    & \cellcolor[rgb]{ .949,  .949,  .949}Step-wise Preference Pairs     \\
                          & AdaptiveStep\cite{liu2025adaptivestep}             & Response Dividing              & Feedback-guided       & SFT            & Math Reasoning, Code Generation & Dividing   Reasoning Steps \\
\bottomrule[1.2pt]
\end{tabular}}
\label{table:PRM}
\end{table*}





% \begin{table*}[t!]
% \centering
% \caption{Summary of PRM technology.}
% \bgroup
% \def\arraystretch{1,2}
% \resizebox{0.9\linewidth}{!}{
% \begin{tabular}{llll}
% \toprule[1.2pt]
% \textbf{Category}   & \textbf{Reasoning LLMs}   & \textbf{LLMs Backbone}           & \textbf{Characteristic}\\ 
% \hline
% \multirow{5}{*}{ORM} 
%  &  \cellcolor[rgb]{ .949,  .949,  .949} DIVERSE\cite{li2022making}
% &\cellcolor[rgb]{ .949,  .949,  .949} DeBERTa-v3/Davinci
% & \cellcolor[rgb]{ .949,  .949,  .949} Weighted Voting Verifier\\

% & MATH-SHEPHERD\cite{wang2024math}
% & LLaMA2/LLEMMA/Mistral/DeepSeek         & Correctness Score Assignment\\   

% &\cellcolor[rgb]{ .949,  .949,  .949}AutoPSV\cite{lu2024autopsv}
% &\cellcolor[rgb]{ .949,  .949,  .949}Phi2/Mistral/Mixtral/Qwen
% &\cellcolor[rgb]{ .949,  .949,  .949}Automated Process Supervision\\

% & Implicit PRMs\cite{yuan2024free}
% & Mistral/LLaMA3 
% & Obtaining PRM from ORM\\

% &\cellcolor[rgb]{ .949,  .949,  .949}OVM\cite{yu2024ovm}
% &\cellcolor[rgb]{ .949,  .949,  .949}LLaMA2/Mistral
% &\cellcolor[rgb]{ .949,  .949,  .949}Guided Decoding\\ 
% \hline

% \multirow{3}{*}{MCTS}
% & ReST-MCTS$^{*}$\cite{zhang2024rest}
% & LLaMA3/Mistral/SciGLM
% & Self Training\\

% &\cellcolor[rgb]{ .949,  .949,  .949}OmegaPRM\cite{luo2024improve}
% & \cellcolor[rgb]{ .949,  .949,  .949}Gemini1.5/Gemma2
% & \cellcolor[rgb]{ .949,  .949,  .949}Divide-and-Conquer MCTS\\

% & ReARTeR\cite{sun2025rearter}
% & GPT-4o/LLaMA3
% & Retrieval-augmented Generation\\
% \hline
% \multirow{2}{*}{PRM} 
% &\cellcolor[rgb]{ .949,  .949,  .949}ORPS\cite{yu2024outcome}
% &\cellcolor[rgb]{ .949,  .949,  .949} LLaMA3/DeepSeek-Coder/Qwen-2.5/GPT-4o
% &\cellcolor[rgb]{ .949,  .949,  .949}Supervising Outcome Refinement\\

% & Step-DPO\cite{lai2024step}
% & Qwen/LLaMA3/DeepSeek-Math
% & Step-wise Preference Pairs   \\

% \bottomrule[1.2pt]
% \end{tabular}}
% \label{table:PRM}
% \egroup
% \end{table*}





\subsubsection{Reward Modeling}\label{prm}


Two primary training paradigms are used to tackle multi-step reasoning tasks: outcome supervision and process supervision. 
Outcome supervision emphasizes the correctness of the final answer at a higher level of granularity, and the resulting model is referred to as the Outcome Reward Model (ORM) \cite{cobbe2021training, yu2023outcome}. 
In contrast, process supervision provides step-by-step labels for the solution trajectory, evaluating the quality of each reasoning step. 
The resulting model is known as the Process Reward Model (PRM) \cite{uesato2022solving, lightmanlet, li2023making}. The main distinction between ORM and PRM is illustrated in Figure \ref{fig:prm_vs_orm}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/PRM_ORM.pdf}
    \caption{The comparison between ORM and PRM for assessing a complete solution trajectory. ORM only provides a single reward based on the correctness of the final answer, while PRM evaluates the quality of each reasoning step throughout the process.}
    \label{fig:prm_vs_orm}
\end{figure}




PRM offers significant advantages \cite{wu2023fine, wang2024math} in complex reasoning tasks for several key reasons. First, it provides fine-grained, step-wise supervision, allowing for the identification of specific errors within a solution path. This feature is especially valuable for RL and automated error correction. Second, PRM closely mirrors human reasoning behavior, which relies on accurate intermediate steps to reach correct conclusions. Unlike ORM, PRM avoids situations where incorrect reasoning can still lead to a correct final answer, thus ensuring more robust and interpretable reasoning. While PRM has primarily been applied to complex mathematical problems, its benefits have recently driven applications in other fields. For instance, ORPS \cite{yu2024outcome} utilizes PRM to address complex code generation challenges, while Step-DPO \cite{lai2024step} combines process supervision with the Direct Preference Optimization (DPO) algorithm \cite{rafailov2024direct} to improve long-chain mathematical reasoning. A summary of Reward Modeling method is presented in Table \ref{table:PRM}.





\noindent\textbf{Summary:} 
Despite the advantages of PRMs, they present several challenges. 
The primary difficulty is obtaining process supervision-labeled data, which is often both costly and time-consuming. 
To address concerns related to scalability, efficiency, and accuracy, researchers have explored various automated annotation methods. 
For example, MATH-SHEPHERD \cite{wang2024math} utilizes the correctness of the final answer to define the quality of intermediate steps based on their potential to lead to the correct outcome, automating the step-wise data collection process. ReST-MCTS$^{*}$ \cite{zhang2024rest} combines process reward guidance with MCTS to generate higher-quality reasoning traces through extensive rollouts. 
Similarly, OmegaPRM \cite{luo2024improve} employs the MCTS framework while introducing a divide-and-conquer algorithm for automated process supervision data generation. 
Another novel approach involves using ORM to train a PRM. Yuan et al. \cite{yuan2024free} propose training a PRM implicitly by leveraging ORM training on cheaper datasets, under mild reward parameterization assumptions. 
They also provide theoretical guarantees for the performance of this implicit PRM, demonstrating its practicality and cost-effectiveness.

In addition to data collection, PRMs face challenges related to trustworthiness \cite{sun2025rearter}, categorized as follows: 
\begin{enumerate}[itemindent=0em]
\item \textbf{Lack of Explanations:} Current PRMs often generate scores for reasoning steps without sufficient explanations, limiting interpretability and hindering their usefulness in refining reasoning during test-time. 

\item \textbf{Bias in Training Data:} Data collection methods, such as MCTS, tend to introduce distributional biases, assigning disproportionately higher scores to the majority of questions. As a result, PRMs struggle to effectively identify erroneous reasoning steps. 

\item \textbf{Early-Step Bias:} PRMs show lower accuracy in predicting rewards for earlier reasoning steps compared to those closer to the final answer. This issue stems from the increased randomness and uncertainty associated with the initial steps in the reasoning process.

\end{enumerate}



\begin{table*}[t!]
\centering
\renewcommand\arraystretch{1.2}
\caption{Summary of Self Improvement method.}

%We summarize the key features for each work: \textbf{Data Source}(Data exploration methods for training), \textbf{Model Refinement}(includes \textbf{Feedback} generation and learning \textbf{Strategy}) and \textbf{Applications}.

\resizebox{0.98\linewidth}{!}{
\begin{tabular}{llllll}
\toprule[1.2pt]
\multirow{2}{*}{\textbf{Stage}}   & \multirow{2}{*}{\textbf{Methods}}   & \multirow{2}{*}{\textbf{Data Source}}  & \multicolumn{2}{c}{\textbf{Model Refinement}} & \multirow{2}{*}{\textbf{Application}}\\\cline{4-5}
& &  & \multicolumn{1}{l}{\textbf{Feedback}} & \multicolumn{1}{l}{\textbf{Strategy}}                            \\ \hline

\multirow{11}{*}{Training} & \cellcolor[rgb]{ .949,  .949,  .949}STaR \cite{DBLP:conf/nips/ZelikmanWMG22}                   & \cellcolor[rgb]{ .949,  .949,  .949}Few-shot  & \cellcolor[rgb]{ .949,  .949,  .949}Language Model & \cellcolor[rgb]{ .949,  .949,  .949}SFT                       & \cellcolor[rgb]{ .949,  .949,  .949}QA, Arithmetic Reasoning                                                        \\

&  Quiet-STaR\cite{QuietStar} &  Token-level Exploration &Language Model &RL                   &  QA, Arithmetic Reasoning                                                                     \\
& \cellcolor[rgb]{ .949,  .949,  .949}V-STaR \cite{hosseini2024vstartrainingverifiersselftaught} & \cellcolor[rgb]{ .949,  .949,  .949}Sampling & \cellcolor[rgb]{ .949,  .949,  .949}Verifier & \cellcolor[rgb]{ .949,  .949,  .949}SFT                   & \cellcolor[rgb]{ .949,  .949,  .949}Arithmetic Reasoning, Code Generation \\

&  B-STaR \cite{zeng2024bstarmonitoringbalancingexploration}  & Sampling &Reward Model &SFT &  Arithmetic Reasoning, Code Generation    \\

& \cellcolor[rgb]{ .949,  .949,  .949}rStar-Math \cite{guan2025rstarmathsmallllmsmaster}         & \cellcolor[rgb]{ .949,  .949,  .949}MCTS Data Construction & \cellcolor[rgb]{ .949,  .949,  .949}Reward Model & \cellcolor[rgb]{ .949,  .949,  .949}SFT                            & \cellcolor[rgb]{ .949,  .949,  .949}Arithmetic Reasoning \\

&  ReST \cite{DBLP:journals/corr/abs-2308-08998}              & Sampling &Reward Model &RL                                          &Machine Translation      \\

& \cellcolor[rgb]{ .949,  .949,  .949}ReST-EM \cite{singh2024humandatascalingselftraining}       & \cellcolor[rgb]{ .949,  .949,  .949}Sampling & \cellcolor[rgb]{ .949,  .949,  .949}Language Model & \cellcolor[rgb]{ .949,  .949,  .949}EM for RL                                   & \cellcolor[rgb]{ .949,  .949,  .949}Arithmetic Reasoning, Code Generation            \\

&  ReST-MCTS* \cite{zhang2024rest}  & Sampling  &Reward Model   &SFT, RL                    &  Reasoning                                              \\

& \cellcolor[rgb]{ .949,  .949,  .949}ENVISIONS \cite{DBLP:journals/corr/abs-2406-11736}         & \cellcolor[rgb]{ .949,  .949,  .949}Sampling & \cellcolor[rgb]{ .949,  .949,  .949}Environment Guided & \cellcolor[rgb]{ .949,  .949,  .949}SFT                      & \cellcolor[rgb]{ .949,  .949,  .949}Web Agents, Reasoning    \\

&  RISE \cite{qu2024recursiveintrospectionteachinglanguage}   &  Sampling &Reward Function &Weighted SFT                                &  Arithmetic Reasoning   \\

& \cellcolor[rgb]{ .949,  .949,  .949}STIC \cite{deng2024enhancinglargevisionlanguage}           & \cellcolor[rgb]{ .949,  .949,  .949}Few-shot & \cellcolor[rgb]{ .949,  .949,  .949}Language Model & \cellcolor[rgb]{ .949,  .949,  .949}SFT                       & \cellcolor[rgb]{ .949,  .949,  .949}Vision Language Model Tasks            \\ 

&  SIRLC \cite{DBLP:conf/iclr/PangWLC0Z024}   &Question Answeing  &Language Model &RL    &Reasoning, Translation, Summary   \\

& \cellcolor[rgb]{ .949,  .949,  .949}AlpacaFarm \cite{DBLP:conf/nips/DuboisLTZGBGLH23}           & \cellcolor[rgb]{ .949,  .949,  .949}Existing Data & \cellcolor[rgb]{ .949,  .949,  .949}Language Model & \cellcolor[rgb]{ .949,  .949,  .949}SFT                       & \cellcolor[rgb]{ .949,  .949,  .949}None (Intrinsic Evaluation)            \\ 

\hline

\multirow{4}{*}{Inference} & Self-Refine \cite{DBLP:conf/nips/MadaanTGHGW0DPY23}   &Independent of Training Data     & Language Model        & Few-shot Demonstration                        & Code Generation, Sentiment Reversal, Acronym Generation                     \\

& \cellcolor[rgb]{ .949,  .949,  .949}Self-Check \cite{DBLP:conf/iclr/MiaoTR24}       & \cellcolor[rgb]{ .949,  .949,  .949}Independent of Training Data            & \cellcolor[rgb]{ .949,  .949,  .949}Language Model           & \cellcolor[rgb]{ .949,  .949,  .949}Step Check               & \cellcolor[rgb]{ .949,  .949,  .949}QA, Arithmetic Reasoning   \\

&  CRITIC \cite{DBLP:conf/iclr/GouSGSYDC24}               &Independent of Training Data    &Language Model  & External Tools                             &  QA, Arithmetic Reasoning, Detoxification          \\

& \cellcolor[rgb]{ .949,  .949,  .949}ROSE\cite{zhong2024rosedoesntthatboosting}              & \cellcolor[rgb]{ .949,  .949,  .949}Independent of Training Data & \cellcolor[rgb]{ .949,  .949,  .949}Language Model                    & \cellcolor[rgb]{ .949,  .949,  .949}Distributed Prompt                     & \cellcolor[rgb]{ .949,  .949,  .949}Safety, Knowledge    \\ 

&  Self-Verification \cite{DBLP:conf/emnlp/WengZX0HLSLZ23}               &Independent of Training Data    &Language Model  & Re-Ranking                             & Arithmetic Reasoning          \\

& \cellcolor[rgb]{ .949,  .949,  .949}SelfEval-Decoding \cite{xie2023selfevaluationguidedbeamsearch}              & \cellcolor[rgb]{ .949,  .949,  .949}Independent of Training Data & \cellcolor[rgb]{ .949,  .949,  .949}Language Model                    & \cellcolor[rgb]{ .949,  .949,  .949}Beam Search                     & \cellcolor[rgb]{ .949,  .949,  .949}Aritnmetic/Symbolic Reasoning                      \\ 

&  IPS \cite{yao2023finegrainedconversationaldecodingisotropic}               &Independent of Training Data    &Language Model  &Constrained Decoding    &Dialogue \\

& \cellcolor[rgb]{ .949,  .949,  .949}Control-DAG\cite{chen2024controldagconstraineddecodingnonautoregressive}              & \cellcolor[rgb]{ .949,  .949,  .949}Independent of Training Data & \cellcolor[rgb]{ .949,  .949,  .949}Language Model                    & \cellcolor[rgb]{ .949,  .949,  .949}Constrained Decoding                     & \cellcolor[rgb]{ .949,  .949,  .949}Dialogue, Open-domain Generation    \\ 

&  Look-Back \cite{xu2023lookbackdecodingopenendedtext}   &Independent of Training Data    &Language Model  &Contrastive Decoding    &Alleviating Repetitions \\


& \cellcolor[rgb]{ .949,  .949,  .949}LeCo \cite{DBLP:journals/corr/abs-2403-19094}              & \cellcolor[rgb]{ .949,  .949,  .949}Independent of Training Data & \cellcolor[rgb]{ .949,  .949,  .949}Language Model                    & \cellcolor[rgb]{ .949,  .949,  .949}Constrained Decoding                     & \cellcolor[rgb]{ .949,  .949,  .949}QA, Reasoning                       \\ 

\bottomrule[1.2pt]
\end{tabular}
}
\label{table:self-improve}
\end{table*}




\subsubsection{Self Improvement}\label{self-improve}

Reasoning LLMs exemplify a progression from weak to strong supervision, while traditional CoT fine-tuning faces challenges in scaling effectively. 
Self improvement, using the model's exploration capabilities for self-supervision, gradually enhances LLMs performance in tasks such as translation \cite{DBLP:journals/corr/abs-2308-08998}, mathematics \cite{DBLP:conf/nips/ZelikmanWMG22, singh2024humandatascalingselftraining}, and multimodal perception \cite{deng2024enhancinglargevisionlanguage}. 
This approach fosters exploration and application within reasoning LLMs \cite{anthony2017thinkingfastslowdeep, tong2024can, tong2024optimizing, guan2025rstarmathsmallllmsmaster}. 
A summary of Self Improvement method is presented in Table \ref{table:self-improve}.








Training-based self improvement in LLMs can be categorized based on exploration and improvement strategies. 
The exploration phase focuses on data collection to facilitate subsequent training improvements, with notable variations in approach. 
STaR \cite{DBLP:conf/nips/ZelikmanWMG22} uses few-shot examples for data gathering, while ReST \cite{DBLP:journals/corr/abs-2308-08998}, ReST-EM \cite{singh2024humandatascalingselftraining}, and ENVISIONS \cite{DBLP:journals/corr/abs-2406-11736} rely on multiple samplings of complete trajectories. 
Quiet-STaR \cite{QuietStar} explores at the token level, introducing concepts like meta-tokens and non-myopic loss to enhance supervision. 
Additionally, ReST-MCTS* \cite{zhang2024rest} and rStar-Math \cite{guan2025rstarmathsmallllmsmaster} generate training data through MCTS.




Improvement strategies also exhibit significant diversity. 
For instance, STaR and its derivatives, such as V-STaR \cite{hosseini2024vstartrainingverifierselftaught} and B-STaR \cite{zeng2024bstarmonitoringbalancingexploration}, combine filtering with SFT. 
ReST and its variants typically introduce innovative reward calculation methods to enhance RL training for policy models. 
RISE \cite{qu2024recursiveintrospectionteachinglanguage} incorporates external feedback, recording rewards and refining responses through distillation during the improvement process. 
Notably, rStar-Math \cite{guan2025rstarmathsmallllmsmaster} demonstrates that small models have achieved \textit{System 2} reflective capabilities through self-evolving training approaches.


Test-time self improvement leverages the consistency of a model's internal knowledge to correct hallucinations during inference. 
These approaches can be categorized into three main types: methods that refine answers using prompts \cite{DBLP:conf/nips/MadaanTGHGW0DPY23, DBLP:conf/iclr/MiaoTR24}, approaches that utilize external tools \cite{DBLP:conf/iclr/GouSGSYDC24}, and techniques that leverage logits without the need for external tools or prompts \cite{DBLP:journals/corr/abs-2403-19094, xu2023lookbackdecodingopenendedtext}.




\subsubsection{Macro Action}\label{macro_action}

Recent advancements in LLMs have driven progress in emulating human-like \textit{System 2} cognitive processes via sophisticated thought architectures, often referred to as macro action frameworks. 
These structured reasoning systems go beyond traditional token-level autoregressive generation by introducing hierarchical cognitive phases, such as strategic planning, introspective verification, and iterative refinement. 
This approach not only enhances the depth of reasoning but also broadens the solution space, enabling more robust and diverse problem-solving pathways. A summary of Macro Action method is presented in Table \ref{table:macro_action}.




% \begin{table*}[!t]
% \centering
% \caption{Summary of Macro Action method. \textbf{Tag}: \encircle[fill=harvestgold, text=white]{I} = \underline{I}mage, \encircle[fill=lightcoral, text=white]{T} = \underline{T}ext, \encircle[fill=DarkGreen, text=white]{V} = \underline{V}ideo.}

% %We summarize the key features for each work: the \textbf{Modality}, \textbf{Reflection}, \textbf{Action Number}, \textbf{Action Source}: Human-designed tempelete or not, the \textbf{Learning}: ICL, RL or SFT, \textbf{Application \& Benchmark}: Summary, Translation, Math or Code. \textbf{Tag}: \encircle[fill=harvestgold, text=white]{I} = \underline{I}mage, \encircle[fill=lightcoral, text=white]{T} = \underline{T}ext, \encircle[fill=DarkGreen, text=white]{V} = \underline{V}ideo 

% \resizebox{0.98\textwidth}{!}{
%    \begin{tabular}% 
%    {lllllccl}
%     \toprule
%         \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Usage}} & \multicolumn{5}{c}{\textbf{Action Attribute}} & \multirow{2}{*}{\textbf{Representative Action}} \\ \cline{3-7}
%           & & \textbf{Action Source} & \textbf{Action Number} & \textbf{Learning} & \textbf{Reflection} & \textbf{Modality} & \\ \midrule
%         % RLHF
%        \rowcolor[rgb]{ .949,  .949,  .949} Self-Check\cite{miao2023selfcheck} & Verification & Human-Designed & 4 & ICL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & Target Extraction, Information Collection, Step Regeneration, Result Comparsion \\
       
%         LeMa\cite{LeMa} & Synthetic Data & Human-Designed & 3 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & Incorrect Step Recognition, Explanation, Correct Solution: \\
        
%        \rowcolor[rgb]{ .949,  .949,  .949} REFINER\cite{li2024refiner} & Verification/Exploration & Human-Designed & 2 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & Critic, Generate \\
        
%         HiICL-MCTS\cite{HiICL-MCTS} & Exploration & Human-Designed & 5 & ICL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & System Analysis, One-Step Thought, Divide and Conquer, ..., Self-Reflection and Refinement  \\
        
%          \rowcolor[rgb]{ .949,  .949,  .949} SUPERCORRECT\cite{yang2024supercorrect} & Distill & In-Context Learning & Dynamic & SFT \& RL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & -- \\
        
%         ReasonFlux\cite{ReasonFlux}  & Synthetic Data/Exploration & Human-Designed & $\sim$500 & ICL \& SFT \& RL & \textcolor{red}{\ding{55}} &  \encircle[fill=lightcoral, text=white]{T} & -- \\
        
%          \rowcolor[rgb]{ .949,  .949,  .949} rStar\cite{rSTaR} & Exploration & Human-Designed & 5 & ICL \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & One-step thought, Propose Next Sub-question \& Answer, ..., Rephrase question \\
         
%         LLaMA-Berry\cite{zhang2024llama} & Exploration & Human-Designed & 2 & ICL \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & Reflection, Error Re-correction \\
    
%         \rowcolor[rgb]{ .949,  .949,  .949} Huatuo-o1\cite{Huatuo-o1} & Synthetic Data & Human-Designed & 4 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & Backtracking, Exploring New
% Paths, Verification, Correction \\

%         Marco-o1\cite{Marco_o1} & Verification & Human-Designed & 1 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & Reflection \\
        
%          \rowcolor[rgb]{ .949,  .949,  .949}BoT\cite{yang2024buffer} & Exploration & In-Context Learning & Dynamic & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & Solving Quadratic Equation, Array Sorting, ...,  Search Algorithms) \\
         
%         rStar-Math\cite{guan2025rstarmathsmallllmsmaster} & Exploration &  In-Context Learning & $1$ & ICL \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & Python comment \\
        
%          \rowcolor[rgb]{ .949,  .949,  .949}Mulberry\cite{yao2024mulberry} & Synthetic Data & In-Context Learning & 1 & ICL \& SFT & \textcolor{green}{\ding{51}}  & \encircle[fill=lightcoral, text=white]{T} & Reflection \\
         
%         LLaVA-CoT\cite{xu2024llava} & Synthetic Data/Exploration & Human-Designed & 4 & SFT & \textcolor{red}{\ding{55}} & \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & Summary, Caption, Reasoning, Conclusion \\
        
%          \rowcolor[rgb]{ .949,  .949,  .949}LLaMAV-o1\cite{thawakar2025llamav} & Verification/Exploration & Human-Designed & 4173 & Curriculum Learning & \textcolor{green}{\ding{51}} &  \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & Detailed Caption Generation, Logical Reasoning, ... Final Answer Generation \\
         
%         AtomThink\cite{atomthink} & Synthetic Data/Exploration & In-Context Learning & $>$100 & SFT \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & Variable Definition, Calculations, Graphs Analysis , ...,  Verification\\

%          \rowcolor[rgb]{ .949,  .949,  .949}RedStar\cite{RedStar} & Distill & Human-Designed & 2 & SFT & \textcolor{green}{\ding{51}} & \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & Wait, Alternately \\
        
%         Auto-CoT\cite{zhang2210automatic}  & Exploration & In-Context Learning & 2 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & Question clustering, Demonstration Sampling \\
        
%          \rowcolor[rgb]{ .949,  .949,  .949}PoT\cite{chen2022program} & Verification & In-Context Learning & 1 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & Code language conversion \\
        
%         PAL\cite{gao2023pal} & Verification & In-Context Learning & 1 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & Code language conversion \\
        
%          \rowcolor[rgb]{ .949,  .949,  .949} Decomposed Prompt\cite{decomposed} & Exploration & Human-Designed & 3 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & Peoblem Split, Subproblem Solving, Answer Merge \\
        
%         Least-to-Most\cite{least2most} & Exploration & Human-Designed & 2 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & Problem Decomposition, Subproblem Solving \\
%         \bottomrule
%    \end{tabular}
% }
%     \label{table:macro_action}
% \end{table*}



\begin{table*}[!t]
\centering
\caption{Summary of Macro Action method. \textbf{Tag}: \encircle[fill=harvestgold, text=white]{I} = \underline{I}mage, \encircle[fill=lightcoral, text=white]{T} = \underline{T}ext, \encircle[fill=DarkGreen, text=white]{V} = \underline{V}ideo. Action Category: AD: Analysis and Decomposition, IPR: Information Processing and Reasoning, VC: Verification and Correction, GO: Generation and Optimization, EB: Exploration and Backtracking.}  

\resizebox{0.98\textwidth}{!}{
   \begin{tabular}% 
   {lllllccc}
    \toprule
        \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Usage}} & \multicolumn{5}{c}{\textbf{Action Attribute}} & \multirow{2}{*}{\textbf{Main Action Category}} \\ \cline{3-7}
          & & \textbf{Action Source} & \textbf{Action Number} & \textbf{Learning} & \textbf{Reflection} & \textbf{Modality} & \\ \midrule
        % RLHF
       \rowcolor[rgb]{ .949,  .949,  .949} Self-Check\cite{miao2023selfcheck} & Verification & Human-Designed & 4 & ICL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & AD, VC \\
       
        LeMa\cite{LeMa} & Synthetic Data & Human-Designed & 3 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & VC, IPR \\
        
       \rowcolor[rgb]{ .949,  .949,  .949} REFINER\cite{li2024refiner} & Verification/Exploration & Human-Designed & 2 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & VC, AD \\
        
        HiICL-MCTS\cite{HiICL-MCTS} & Exploration & Human-Designed & 5 & ICL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & VC, EB, AD  \\
        
         \rowcolor[rgb]{ .949,  .949,  .949} SUPERCORRECT\cite{yang2024supercorrect} & Distill & In-Context Learning & Dynamic & SFT \& RL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
        
        ReasonFlux\cite{ReasonFlux}  & Synthetic Data/Exploration & Human-Designed & $\sim$500 & ICL \& SFT \& RL & \textcolor{red}{\ding{55}} &  \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
        
         \rowcolor[rgb]{ .949,  .949,  .949} rStar\cite{rSTaR} & Exploration & Human-Designed & 5 & ICL \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & VC, GO, EB \\
         
        LLaMA-Berry\cite{zhang2024llama} & Exploration & Human-Designed & 2 & ICL \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & VC, EB \\
    
        \rowcolor[rgb]{ .949,  .949,  .949} Huatuo-o1\cite{Huatuo-o1} & Synthetic Data & Human-Designed & 4 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & VC \\

        Marco-o1\cite{Marco_o1} & Verification & Human-Designed & 1 & ICL \& SFT & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & VC \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}BoT\cite{yang2024buffer} & Exploration & In-Context Learning & Dynamic & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
         
        rStar-Math\cite{guan2025rstarmathsmallllmsmaster} & Exploration &  In-Context Learning & $1$ & ICL \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}Mulberry\cite{yao2024mulberry} & Synthetic Data & In-Context Learning & 1 & ICL \& SFT & \textcolor{green}{\ding{51}}  & \encircle[fill=lightcoral, text=white]{T} & VC, EB \\
         
        LLaVA-CoT\cite{xu2024llava} & Synthetic Data/Exploration & Human-Designed & 4 & SFT & \textcolor{red}{\ding{55}} & \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}LLaMAV-o1\cite{thawakar2025llamav} & Verification/Exploration & Human-Designed & 4173 & Curriculum Learning & \textcolor{green}{\ding{51}} &  \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
         
        AtomThink\cite{atomthink} & Synthetic Data/Exploration & In-Context Learning & $>$100 & SFT \& RL & \textcolor{green}{\ding{51}} & \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & AD, IPR, EB \\

         \rowcolor[rgb]{ .949,  .949,  .949}RedStar\cite{RedStar} & Distill & Human-Designed & 2 & SFT & \textcolor{green}{\ding{51}} & \encircle[fill=harvestgold, text=white]{I} \space \encircle[fill=lightcoral, text=white]{T} & AD, VC \\
        
        Auto-CoT \cite{zhangautomatic}  & Exploration & In-Context Learning & 2 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR, GO \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}PoT\cite{chen2022program} & Verification & In-Context Learning & 1 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR, GO \\
        
        PAL\cite{gao2023pal} & Verification & In-Context Learning & 1 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR, GO \\
        
         \rowcolor[rgb]{ .949,  .949,  .949} Decomposed Prompt\cite{decomposed} & Exploration & Human-Designed & 3 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
        
        Least-to-Most\cite{least2most} & Exploration & Human-Designed & 2 & ICL & \textcolor{red}{\ding{55}} & \encircle[fill=lightcoral, text=white]{T} & AD, IPR \\
        \bottomrule
   \end{tabular}
}
    \label{table:macro_action}
\end{table*}










% \begin{table*}[t!]
% \centering
% \renewcommand\arraystretch{1.2}
% \caption{Summary of Macro Action technology, classified into Action-guided Inference and Action-guide Data Synthesis.}
% % \bgroup
% % \def\arraystretch{1,2}
% \resizebox{0.98\linewidth}{!}{
% \begin{tabular}{llll}
% \toprule[1.2pt]
%   & \textbf{Reasoning LLMs}   & \textbf{Characteristic}           & \textbf{Benchmark}                          \\ \hline
  
% \fontsize{9}{6}\selectfont\multirow{5}{*}{\rotatebox{90}{{{\textbf{Inference}}}}} &  \cellcolor[rgb]{ .949,  .949,  .949}Self-Check \cite{DBLP:conf/iclr/MiaoTR24} &  \cellcolor[rgb]{ .949,  .949,  .949}Test-time Self-Improvement by Special Macro Action  &  \cellcolor[rgb]{ .949,  .949,  .949}GSM8K, MathQA, MATH \\

% & HiICL-MCTS \cite{HiICL-MCTS} & MCTS Search Enhanced by Macro Action Chain Prompt & GSM8K, MATH, StrategyQA, SVAMP  \\

% &  \cellcolor[rgb]{ .949,  .949,  .949}ReasonFlux \cite{ReasonFlux} &  \cellcolor[rgb]{ .949,  .949,  .949}Test-time Scaling by High-level Thought Iteration Process. &  \cellcolor[rgb]{ .949,  .949,  .949}MATH, AIME2024, AMC2023, OlympiadBench, Gaokao \\

% & rStar \cite{rSTaR} & MCTS Search Enhanced by Human-designed Macro Action  & GSM8K, GSM-Hard, SVAMP, StrategyQA    \\

% &  \cellcolor[rgb]{ .949,  .949,  .949}LLaMA-Berry \cite{DBLP:journals/corr/abs-2410-02884}  &  \cellcolor[rgb]{ .949,  .949,  .949}MCTS Search Enhanced by Human-designed Macro Action  &  \cellcolor[rgb]{ .949,  .949,  .949}GSM8K, MATH, GaoKao2023En, OlympiadBench, College Math, MMLU STEM  \\

% & Buffer of Thoughts \cite{yang2024buffer} & Inference by Retrieved High-level Thought.  & MGSM, BBH, Python Programming Puzzles, Shakespearean Sonnet Writing  \\

%  \hline

% \fontsize{9}{6}\selectfont\multirow{9}{*}{\rotatebox{90}{\textbf{Data Synthesis}}}

% &  \cellcolor[rgb]{ .949,  .949,  .949}rStar-Math \cite{guan2025rstarmathsmallllmsmaster} &  \cellcolor[rgb]{ .949,  .949,  .949}CoT Synthesis by Code Macro Action    &  \cellcolor[rgb]{ .949,  .949,  .949}MATH, AIME2024, AMC2023, OlympiadBench, College Math, GSM8K Gaokao   \\

% & Mulberry \cite{yao2024mulberry} & CoT Synthesis by Visual Macro Action  &  MathVista, MMStar, MMMU, ChartQA, DynaMath, HallBench, MM-Math MME$_{sum}$   \\

% &  \cellcolor[rgb]{ .949,  .949,  .949}LLaVA-CoT \cite{xu2024llava}  &  \cellcolor[rgb]{ .949,  .949,  .949}Human-designed Macro Action Tag Synthesis in CoT.  &  \cellcolor[rgb]{ .949,  .949,  .949}MMStar, MMBench, MMVet, MathVista, AI2D, Hallusion   \\

% & LLaMAV-o1 \cite{thawakar2025llamav} &  Structured Training Paradigm of Macro Action inserted CoT. & MMStar, MMBench, MMVet, MathVista, AI2D, Hallusion  \\

% &  \cellcolor[rgb]{ .949,  .949,  .949}AtomThink \cite{atomthink} &  \cellcolor[rgb]{ .949,  .949,  .949}Multimodal CoT Inserted Macro Action by ICL.  &  \cellcolor[rgb]{ .949,  .949,  .949}MathVista, MathVerse   \\

% & STILL-1  \cite{Scaling_of_Search_Learning} & Long-CoT Synthesis by Macro Action Assisted Tree Search & MATH-OAI, GSM-Hard, OlympiadBench, College Math   \\

% &  \cellcolor[rgb]{ .949,  .949,  .949}Satori  \cite{Satori} &  \cellcolor[rgb]{ .949,  .949,  .949}RL Explore by Macro Action Inserted Prompt  &  \cellcolor[rgb]{ .949,  .949,  .949}FOLIO, BGQA, CRUXEval, StrategyQA, TableBench, STEM    \\

% & RedStar \cite{RedStar}   & Distill Long-CoT by Macro Action Keyword Filter. &  MATH, AIME2024 OlympiadBench, College Math, High School Math    \\

% &  \cellcolor[rgb]{ .949,  .949,  .949}SUPERCORRECT \cite{yang2024supercorrect} &  \cellcolor[rgb]{ .949,  .949,  .949}Two-stage SFT Framework by High-level and Macro Action Thought Template. &  \cellcolor[rgb]{ .949,  .949,  .949}GSM8K, MATH, GaoKao \\

% \bottomrule[1.2pt]
% \end{tabular}
% }
% \label{table:macro_action}
% \end{table*}


%for \textit{System 2}

We classify the progress of macro action into two aspects: 
\begin{enumerate}[itemindent=0em]
\item \textbf{Test-time Scaling through Macro Action Operationalization:} Recent research identifies two key methodologies for improving reasoning performance during inference and test-time scaling. 
HiICL-MCTS \cite{HiICL-MCTS} employs a deliberate search through seed data to generate action-chain templates consisting of macro actions, thereby facilitating an action-chain-guided approach to test-time reasoning. ReasonFlux \cite{ReasonFlux} utilizes an iterative test-time scaling framework, harnessing external high-level thought templates to iteratively refine and update the current CoT.

%CoAct


\item \textbf{Macro Action-Enhanced Data Synthesis Paradigms:} A key application of macro actions in complex reasoning is in the synthesis of reasoning data. 
In data synthesis and training frameworks, macro action architectures enhance reasoning diversity and generalization. 
Recent research has shown that integrating or synthesizing a CoT process with macro actions within the reasoning sequence can significantly improve the data efficiency of the reasoning chain. 
For instance, LLaVA-CoT \cite{xu2024llava} enhances CoT data synthesis by externalizing intermediate reasoning steps across multiple modalities. 
AtomThink \cite{atomthink} generates the AMATH-SFT dataset using a structured g1 prompt \cite{g1}, achieving superior performance on long-horizon reasoning tasks compared to traditional CoT approaches. 
CoAct \cite{hou2024coact} introduces a dual-agent collaborative reasoning framework, where a global planning agent executes overarching macro-actions, while a local execution agent carries out specific sub-actions within those broader actions.

\end{enumerate}

Macro actions also play a crucial role in enhancing self improvement frameworks. 
rStar-Math \cite{guan2025rstarmathsmallllmsmaster} utilizes high-level deliberate search through Code-augmented CoT, generating diverse and reliable solutions while achieving proactive search capabilities. 
Satori \cite{Satori} integrates CoT with RL, incorporating ``$<$\textit{reflect}$>$''-style macro actions to diversify exploration and alleviate policy saturation in online RL environments. 
Huatuo-o1 \cite{Huatuo-o1} combines hierarchical planning with domain-specific knowledge bases to improve medical reasoning. 
Additionally, ReasonFlux \cite{ReasonFlux} dynamically reconfigures reasoning templates (\emph{e.g.}, breaking down calculus problems into symbolic and numeric phases) to align with the problem structure.





\subsubsection{Reinforcement Fine-Tuning}\label{rl_supervise}

Reinforcement Fine-Tuning (RFT) \cite{openaiRFT} is an innovative technique recently introduced by OpenAI, designed to enable developers and engineers to fine-tune existing models for specific domains or complex tasks. Unlike general SFT, RFT focuses on optimizing the model's reasoning process by using a reward mechanism to guide the model's evolution, thereby enhancing its reasoning capabilities and accuracy. 
The core of RFT lies in improving the model's performance in a specific domain with minimal high-quality training data \cite{chang2024survey}, an appropriate reward model \cite{trung2024reft}, and a stable optimization process in long-context \cite{blobRFT, codepmp, nextlong, quest}. 
A summary of RFT method is presented in Table \ref{table:rl_supervise}.







DeepSeek-R1 \cite{Deepseek-R1}, which employs a verifier reward-based strategy, has shown significant performance improvements compared to traditional methods like SoS \cite{SoS}. Key advantages include:
\begin{enumerate}[itemindent=0em]
    \item \textbf{Simplified Training Pipeline:} RL supervision streamlines data construction and training processes, eliminating the need for complex stepwise search mechanisms. 
    
    \item \textbf{Enhanced Scalability:} Online RL training facilitates efficient scaling on large datasets, particularly for complex reasoning tasks.
    
    \item \textbf{Emergent Properties:} DeepSeek-R1 \cite{Deepseek-R1} demonstrates unique emergent capabilities, such as Long-CoT reasoning, which are difficult to achieve through SFT alone.
\end{enumerate}


% \begin{table*}[tbp]
% \centering
% \renewcommand\arraystretch{1.3}
% \caption{Summary of  technology, including Reproduction work and Analysis work.}
% \resizebox{0.98\textwidth}{!}{
% \begin{tabular}{lllp{32em}l} % 修正列布局
% \toprule[1.2pt]
%     & \textbf{Reasoning LLMs} & \textbf{Characteristic} & \textbf{Benchmark} \\
% \hline
%        \fontsize{8}{6}\selectfont\multirow{11}{*}{\rotatebox{90}{\textbf{Reproduction}}}
%     &\cellcolor[rgb]{ .949,  .949,  .949}DeepSeek-R1 \cite{Deepseek-R1} & \cellcolor[rgb]{ .949,  .949,  .949}Large-scale RL, GRPO, Distillation & \cellcolor[rgb]{ .949,  .949,  .949}ArenaHard, LiveCodeBench, Codeforces, AIME 2024, MATH-500, etc. \\ 

%     %DROP, IF-Eval, GPQA, SimpleQA, FRAMES, AlpacaEval2.0, ArenaHard, LiveCodeBench, Codeforces, SWE Verified, Aider-Polyglot, AIME 2024, MMLU, MATH-500, CNMO 2024, CLUEWSC, C-Eval, C-SimpleQA
    
%     & ReFT \cite{trung2024reft} & RFT, PPO & GSM8k, SVAMP, MathQA\\
    
%     & \cellcolor[rgb]{ .949,  .949,  .949}T1 \cite{advancinglanguagemodelreasoning} & \cellcolor[rgb]{ .949,  .949,  .949}SFT, RLHF, RL scaling & \cellcolor[rgb]{ .949,  .949,  .949}MATH-500, AIME 2024, Omni-math-500 \\
    
%     & Satori \cite{Satori} & Autoregressive searching, Chain-of-Action-Thought &MATH-500, AIME 2024, CRUXEval, StrategyQA, TableBench, etc. \\

%     %GSM8K, MATH500, OlympiadBench, AMC, AIME 2024, FOLIO, BGQA, CRUXEval, StrategyQA, TableBench, STEM
    
%     & \cellcolor[rgb]{ .949,  .949,  .949}RLSP \cite{ye2025emergencethinkingllmsi} & \cellcolor[rgb]{ .949,  .949,  .949}RL via Self-play, SFT, PPO & \cellcolor[rgb]{ .949,  .949,  .949}MATH-500, AIME 2024 \\
    
%     & QCLASS \cite{QCLASS} & Exploration tree, PRM, Q-guided Generation Strategy & WebShop, SciWorld, ALFWorld \\
    
%     & \cellcolor[rgb]{ .949,  .949,  .949}OREAL \cite{OREAL} & \cellcolor[rgb]{ .949,  .949,  .949}Behavior Cloning and Reward Shaping, Outcome Reward-based RL & \cellcolor[rgb]{ .949,  .949,  .949}MATH-500, AIME 2024, AIME 2025-I, LiveMath, Olympiad \\
    
%     & DHP \cite{DHP} & Discrete Hierarchical Planning, Tree Trajectory Advantage Estimation & 25-room task \\
    
%     & \cellcolor[rgb]{ .949,  .949,  .949}DuoGuard \cite{DuoGuard} & \cellcolor[rgb]{ .949,  .949,  .949}Adversarial RL,  Multilingual Guardrail Modeling & \cellcolor[rgb]{ .949,  .949,  .949}XSTest, ToxicChat, OpenAI Moderation, Beavertails, RTP-LX, XSafety \\
    
%     & PRIME \cite{cui2025process} & Implicit PRM, RL & AIME 2024, AMC, Minerva Math, OlympiadBench, MATH-500 \\
    
%     & \cellcolor[rgb]{ .949,  .949,  .949}DeepScaleR \cite{deepscaler2025} & \cellcolor[rgb]{ .949,  .949,  .949}RL Scaling for Small Models, Iterative Lengthening for Length Scaling & \cellcolor[rgb]{ .949,  .949,  .949}AIME 2024, AMC, Minerva Math, OlympiadBench, MATH-500 \\
    
% \hline

%     \fontsize{8}{6}\selectfont\multirow{4}{*}{\rotatebox{90}{\textbf{Analysis}}} & DOES RLHF SCALE? \cite{RLHF_Scaling} & Analysis of RLHF Scaling Properties & MATH, GPQA, LiveCodeBench, GSM8K, MMLU \\
    
%     & \cellcolor[rgb]{ .949,  .949,  .949}Demystifying Long CoT Reasoning \cite{yeo2025demystifying} & \cellcolor[rgb]{ .949,  .949,  .949}Analysis of the Mechanics of Long CoT Reasoning using RL & \cellcolor[rgb]{ .949,  .949,  .949}MATH-500, AIME 2024, TheoremQA, MMLU-Pro \\
    
%     & Metastable Dynamics of CoT Reasoning \cite{metastabledynamicschainofthoughtreasoning} & 
% Analysis of Inference-time Compute and CoT Generation as a Metastable Markov Process & 
%     - \\
    
%     & \cellcolor[rgb]{ .949,  .949,  .949}Safety in DeepSeek-R1 \cite{parmar2025challenges} & \cellcolor[rgb]{ .949,  .949,  .949}Analysis of Limitations of RL for Reducing Harmful Outputs in DeepSeek-R1 & \cellcolor[rgb]{ .949,  .949,  .949}
%     - \\
% \bottomrule[1.2pt]
% \end{tabular}
% }
% \label{table:rl_supervise}
% \end{table*}

\begin{table*}[!t]
\centering
\caption{Summary of RFT method. \textbf{Tag}: \encircle[fill=harvestgold, text=white]{I} = \underline{I}mage, \encircle[fill=lightcoral, text=white]{T} = \underline{T}ext, \encircle[fill=DarkGreen, text=white]{V} = \underline{V}ideo.}
% We summarize the key features for each work: the \textbf{Foundation}(Foundation Model) and \textbf{Modality} of the feedback, the \textbf{Reward Type}, \textbf{Algorithm}, \textbf{Learning},  and \textbf{Incentivize Sample} size of the refinement process, and the \textbf{Application \& Benchmark} of the method. 
\resizebox{0.98\textwidth}{!}{
   \begin{tabular}% 
   {llclllll}
    \toprule
        \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{Model Attribute}} & \multicolumn{4}{c}{\textbf{Incentivize Attibute}} & \multirow{2}{*}{\textbf{Application \& Benchmark}} \\ \cline{2-3} \cline{4-7}
         & \textbf{Foundational LLMs} & \textbf{Feedback Modality} & \textbf{Reward Type} & \textbf{Algorithm} & \textbf{Learning} & \textbf{Incentivize Sample} & \\ \midrule
        \multicolumn{8}{c}{\textbf{Reason RFT Project}} \\ \midrule
        % RLHF
         \rowcolor[rgb]{ .949,  .949,  .949}DeepSeek-R1-Zero\cite{Deepseek-R1} & DeepSeek-V3 & \encircle[fill=lightcoral, text=white]{T} & Rule-Outome-Reward & GPRO & RL & 800K  & Multiple Tasks \\
        
        DeepSeek-R1\cite{Deepseek-R1} & DeepSeek-V3 & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & GPRO & RL \& SFT & 800K  & Multiple Tasks \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}Kimi v1.5\cite{team2025kimi} & -- & \encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & $\text{PPO}^{*}$ & RL \& SFT & -- & Multiple Tasks \\
         
        % Fine-grained RLHF
        ReFT\cite{trung2024reft}  & Galactica, CodeLLama & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & $\text{PPO}^{*}$ & RL \& SFT & 3k/7k/8k/15k & GSM8k/SVAMP/MathQA \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}RFTT\cite{RFTT} & LLaMA-3-3/8B-Instruct,Qwen-2.5-7B-Instruct & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward &  Reinforce++ & RL \& SFT & 1.2K & Multiple Math Task \\
         
        % HH-RLHF
        Satori\cite{Satori} & Qwen-2.5-Math-7B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & PPO & RL \& SFT & 66K & Multiple Math Task  \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}QCLASS\cite{QCLASS} & Llama-2-7B-Chat & \encircle[fill=lightcoral, text=white]{T} & Process-Reward & QNet & RL \& SFT & 1.9K/1.5K/3.3K & WebShop, ALFWorld, SciWorld \\
        
        % ILF
        PRIME\cite{cui2025process} & Qwen2.5-Math-7B & \encircle[fill=lightcoral, text=white]{T} & Rule-Process-Outcome-Reward & PPO & RL \& SFT  & 150K & Math, Code Tasks  \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}DeepScaleR\cite{deepscaler2025} & DeepSeek-R1-Distill-Qwen-1.5B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & Iteratively GPRO & RL & 40K & Multiple Math Task \\
        
        PURE\cite{cheng2025pure} & Qwen2.5-Math-7B &  \encircle[fill=lightcoral, text=white]{T} & Rule-Process-Outcome-Reward & PPO+RLOO & RL & 8K & Multiple Math Task \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}SimpleRL\cite{simplerl_reason_blob} & Qwen2.5-Math-7B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & PPO & RL & 8K & Multiple Math Task \\
        
        Open-R1\cite{openr1} & Qwen2.5-1.5B-Instruct & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & GPRO & RL \& SFT & 8K & Multiple Math, Code Task \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}TinyZero\cite{tinyzero} & Qwen2.5-0.5B/3B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & GPRO & RL & -- & CountDown Task \\
        
        Ota-Zero\cite{liu2025oatzero} & Qwen-2.5-Series, DeepSeek-Series, Rho, Llama-3.x & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & GRPO & RL & 0.5K & CountDown Task \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}Ota\cite{liu2025oat} & RHO-1b/Qwen2.5-3B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & GPRO/PPO & RL & 7.5K & GSM8K \\
        
        LIMR\cite{limr} & Qwen-Math-7B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & PPO & RL & 1.3K & Multiple Math Task \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}Critic-RL\cite{critic_rl} & Qwen2.5-Coder-32B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & $\text{GPRO}^{*}$ & RL \& SFT & 18.8K & Multiple Code Task \\
        
        Logic-R1\cite{logicrl} & Qwen2.5-7B-Instruct-1M & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward & $\text{REINFORCE++}^{*}$ & RL & 5K & Multiple Math, Logic Task \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}Online-DPO-R1\cite{online_dpo_r1} & Qwen2.5-MATH-7B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward  & DPO  & RL\& SFT & 207.5K & Multiple Math Task \\
        
        OpenReason-Zero\cite{OpenReasonerZero2025} & Qwen-2.5-7B/32B & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward   & PPO & RL & 57K & Multiple Math Task, GPQA, MMLU \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}RLHF-V\cite{yu2024rlhfv} & OmniLMM-12B & \encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T} & Process-Reward &  DDPO & RL & 1.4K & Multiple Tasks \\
        
        RLAIF\cite{lee2023rlaif} & PaLM 2 Extra-Small & \encircle[fill=lightcoral, text=white]{T} & Rule-Outome-Reward & RLAIF & RL & -- & Summary and Conversation Generation \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}MM-RLHF\cite{MM_RLHF} & LLaVA-onevision-7B & \encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T} \space\encircle[fill=DarkGreen, text=white]{V} & Process-Reward & MM-DPO & RL & 120K & MM-RLHF-RewardBench/SafetyBench\\
        
         Align-DS-V\cite{Align_DS_V}  & LLaVA-v1.5-7B,Qwen2-VL &\encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T} \space\encircle[fill=DarkGreen, text=white]{V}
        & Process-Reward & PPO, DPO 
        & RL \& SFT & 200K & Align-Anything, Eval-Anything \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}R1V\cite{chen2025r1v}  & Qwen2-VL,Qwen2.5-VL
        & \encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T} 
        &  Rule-Outome-Reward & GRPO & RL & 70K/70K/8K & Multiple Tasks \\
        
        % HH-RLHF
        VLM-R1\cite{vlmr1} &  Qwen2.5-VL & \encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T}  
        & Rule-Outome-Reward  & GRPO & RL  & 120K & Multiple Tasks \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}LMM-R1\cite{lmmr1} & Qwen2.5-VL&\encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T} 
        &  Rule-Outome-Reward  &  PPO/RLOO & RL  & 8K & Multiple Tasks \\
        
        Open-R1-Video\cite{open-r1-video} & Qwen2-VL-7B  & \encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T} \space\encircle[fill=DarkGreen, text=white]{V}
        & Rule-Outome-Reward & GRPO  & RL & 4K & Multiple Tasks \\
        
         \rowcolor[rgb]{ .949,  .949,  .949}Easy-R1 \cite{zheng2025easyr1} & Qwen2.5-VL& \encircle[fill=harvestgold, text=white]{I} \space\encircle[fill=lightcoral, text=white]{T}
        &  Rule-Outome-Reward  & GRPO & RL & 3K & Multiple Tasks \\
        %%%%%%
        \midrule
        \multicolumn{8}{c}{\textbf{Analysis RFT Project}} \\ 
        \midrule
        %%%%%%
        % Self-Verification
        Demystify-LongCoT\cite{yeo2025demystifying} & Llama-3.1-8B, Qwen2.5
-7B-Math & \encircle[fill=lightcoral, text=white]{T} & Rule-Outcome-Reward  & PPO/Reinforce++ &  RL \& SFT  & 7.5K & Multiple Math, MMLU \\ 

         \rowcolor[rgb]{ .949,  .949,  .949}RLHF-Scale\cite{RLHF_Scaling} & GLM4-9B & \encircle[fill=lightcoral, text=white]{T} & Process-Reward  & PPO & RL  & 11K & Multiple Tasks  \\ 
        
        MD-CoT\cite{metastabledynamicschainofthoughtreasoning}  & -- & -- & Process-Reward  & PPO & RL  & -- & --  \\ 
        
        %%%%%%
        \bottomrule
   \end{tabular}
}
    \label{table:rl_supervise}
\end{table*}



Despite its strengths, RFT faces the following challenges:

\begin{enumerate}[itemindent=0em]

\item \textbf{Unclear Mechanism behind Reasoning:} The underlying mechanisms driving the reasoning improvements in DeepSeek-R1 remain poorly understood. 
For example, while DeepSeek-R1 exhibits emergent properties (\emph{e.g.}, ``Emergent Length Increasing'', ``Aha moments''), studies such as \cite{ThereMaybeNot} suggest that capabilities like Long-CoT might already exist in the base model, rather than solely emerging from RL training. 
Furthermore, performance gains observed in smaller models (\emph{e.g.}, Qwen-Math-2B/7B \cite{qwen2}) occur without noticeable ``Aha moments'', complicating causal interpretations.

\item \textbf{Reward Model Saturation:} Many existing RL algorithms face reward model saturation, typically manifested as exploration collapse after around 100 training steps. 
Although DeepSeek-R1 alleviates this issue through specialized reward formatting, methods like ReFT \cite{trung2024reft} and Satori \cite{Satori} propose alternating sampling and SFT distillation to combat reward hacking and exploration collapse.

\item \textbf{Unstable Long-CoT Generation:} Long reasoning chains generated by RFT are prone to instability, including context overflow, failure to return final answers, and sensitivity to reward shaping \cite{Tecent_2_plus_3}. 
For instance, methods like \cite{yeo2025demystifying} inadvertently introduce cosine reward functions, which degrade performance with increased iterations. 
O1-Prune \cite{o1_pruner} uses post-hoc length pruning techniques \cite{team2025kimi} (via RL/SFT) to stabilize outputs.

\end{enumerate}

% 技术介绍 优势，不足，未来发展 有代表性的方法

Future directions for RFT may include several exciting and innovative advancements, such as:

\begin{enumerate}[itemindent=0em]

\item \textbf{Efficient and Stable RL Frameworks:} There is a need to develop more robust RL algorithms that prevent reward saturation and exploration collapse. \cite{yeo2025demystifying} reveals that REINFORCE++ \cite{reinforce_plusplus} underperforms when combined with KL divergence regularization, suggesting the need for alternative methods. 
Future work should revisit classic RL algorithms in the context of modern LLMs training to optimize both stability and efficiency.

\item \textbf{Scaling RFT:} Current RL-Supervise models rely on curated, verifiable prompts selected from large-scale datasets. 
Future research should focus on synthesizing high-quality, diverse prompts to improve generalization. \cite{RLHF_Scaling} shows that merely scaling policy/reward models or increasing sample sizes results in diminishing returns, while expanding the scope of PRM and R1 training data holds greater promise. 
Hybrid approaches, such as combining RL with SFT or curriculum learning, should be explored to enhance scalability.

\item \textbf{Controlling Long-CoT Stability:} Adaptive reward shaping mechanisms are needed to balance reasoning length, coherence, and answer correctness. 
Techniques such as O1-Prune \cite{o1_pruner} demonstrate the value of post-hoc length regularization, but dynamic in-training controls are necessary. 
Hierarchical RL frameworks should be investigated to decompose long reasoning chains into manageable sub-tasks, reducing instability.

\item \textbf{Theoretical and Empirical Analysis:} It is essential to clarify the relationship between RL training and the capabilities of the base model. 
For instance, it should be determined whether emergent properties (\emph{e.g.}, Long-CoT) arise from RL optimization or are latent traits of the base model. Systematic studies on reward design principles (\emph{e.g.}, sparse vs. dense rewards, multi-objective balancing) should be conducted to avoid unintended behaviors such as reward hacking.

\end{enumerate}

\noindent\textbf{Summary:} RFT presents a promising direction for advancing LLMs reasoning, as evidenced by DeepSeek-R1 \cite{Deepseek-R1}. 
However, challenges such as reward saturation, unstable long reasoning chains, and unclear emergent mechanisms require urgent attention. 
Future efforts should prioritize algorithmic innovation, scalable prompt synthesis, and theoretical grounding to fully unlock the potential of RL-driven reasoning LLMs.





\subsection{Evolutionary of Reasoning LLMs}\label{evolutionary}

% In this section, we summarize the various developmental stages and the evolutionary timeline of LLMs-driven SRS.

The evolution of reasoning LLMs has progressed by several distinct stages, with various strategies developed to overcome the limitations of direct autoregressive inference and build more advanced slow-thinking reasoning architectures.

% The evolutionary timeline of Reasoning LLMs is presented in Figure \ref{fig:timeline}.


% \begin{table*}[tbhp]
% \scriptsize
% \centering
% \caption{Statistics of benchmarks for reasoning LLMs.}
% \resizebox{0.91\linewidth}{!}{
% \begin{tabular}{llllll}
% \toprule[1.2pt]
% \textbf{Domain}                         & \textbf{Benchmark}                                                            & \textbf{Venue}      & \textbf{Language}        & \textbf{Size}   & \textbf{Level}              \\
% \hline
% \multirow{4}{*}{Math}         & \cellcolor[rgb]{ .949,  .949,  .949} AIME 2024 \cite{AIME2024}                           & \cellcolor[rgb]{ .949,  .949,  .949}-          & \cellcolor[rgb]{ .949,  .949,  .949}English         &\cellcolor[rgb]{ .949,  .949,  .949} 30     & \cellcolor[rgb]{ .949,  .949,  .949}Competition        \\

%                                &  MATH-500 \cite{lightmanlet}                         &  ICLR 2024    & English         &  500    & Competition        \\
                               
%                                & \cellcolor[rgb]{ .949,  .949,  .949}AMC 2023 \cite{AMC2023}                             &\cellcolor[rgb]{ .949,  .949,  .949} -          & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}30     & \cellcolor[rgb]{ .949,  .949,  .949}Competition        \\
                               
%                                &  Olympiad Bench   \cite{he2024olympiadbench}         &  ACL 2024     & English/Chinese &  8,476  & Competition        \\
% \hline
% \multirow{3}{*}{Code}          & \cellcolor[rgb]{ .949,  .949,  .949}Codeforces                                                           & \cellcolor[rgb]{ .949,  .949,  .949}-          & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}-      &\cellcolor[rgb]{ .949,  .949,  .949} Expert             \\

%                                &  SWE-bench   \cite{jimenez2024swebench}              &  ICLR 2024    &  English         &  2,294  &  Expert             \\
                               
%                                & \cellcolor[rgb]{ .949,  .949,  .949}LiveCodeBench   \cite{jain2024livecodebench}        &\cellcolor[rgb]{ .949,  .949,  .949} ArXiv 2024   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}-      & \cellcolor[rgb]{ .949,  .949,  .949}Expert             \\
                               
% \hline
% \multirow{3}{*}{Science}       &  GPQA Diamond \cite{rein2024gpqa}                    &  COLM 2024    &  English         & 448    &  University         \\

%                                & \cellcolor[rgb]{ .949,  .949,  .949}MR-Ben \cite{ZengLWLCDYXQZSL24}                        & \cellcolor[rgb]{ .949,  .949,  .949}NeurIPS 2024 & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}5,975 & \cellcolor[rgb]{ .949,  .949,  .949} Hybrid             \\
%                                & \cellcolor[rgb]{ .949,  .949,  .949}MMLU-Pro \cite{wang2024mmlu}                        & \cellcolor[rgb]{ .949,  .949,  .949}NeurIPS 2024 & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}12,032 & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\
                               
% \hline

% \multirow{4}{*}{Agent}         &  WebShop \cite{yao2022webshop}                       &  NeurIPS 2022 &  English         &  1,600  & Hybrid             \\

%                                & \cellcolor[rgb]{ .949,  .949,  .949} WebArena   \cite{zhou2023webarena}                  & \cellcolor[rgb]{ .949,  .949,  .949}ICLR 2024    & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}812    & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\
                               
%                                &  SciWorld   \cite{scienceworld2022}                  &  EMNLP 2022   & English         & 7,200  &  Hybrid             \\
                               
%                                & \cellcolor[rgb]{ .949,  .949,  .949}TextCraft   \cite{prasad2024adapt}           & \cellcolor[rgb]{ .949,  .949,  .949}NAACL 2024   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}200    & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\
                               
% \hline

% \multirow{3}{*}{Medicine}      &  JAMA Clinical Challenge \cite{chen2024benchmarking} &  NAACL 2025   & English         &  1,524  &  Expert             \\

%                                & \cellcolor[rgb]{ .949,  .949,  .949}Medbullets   \cite{chen2024benchmarking}            & \cellcolor[rgb]{ .949,  .949,  .949}NAACL 2025   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}308    & \cellcolor[rgb]{ .949,  .949,  .949}Expert             \\
                               
%                                &  MedQA \cite{jin2021disease}                         &  ArXiv 2020   & English/Chinese &  61,097 &  Expert             \\
                               
% \hline
% \multirow{5}{*}{Multimodality} & \cellcolor[rgb]{ .949,  .949,  .949}MMMU \cite{yue2024mmmu}                             & \cellcolor[rgb]{ .949,  .949,  .949}CVPR 2024    & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}11,500 & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\

%                                &  MathVista   \cite{lu2024mathvista}                  &  ICLR 2024    &  English         &  6,141  &  Middle School      \\
                               
%                                & \cellcolor[rgb]{ .949,  .949,  .949}MathVision \cite{MathVision}                        & \cellcolor[rgb]{ .949,  .949,  .949}NeurIPS 2024 & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}3,040  & \cellcolor[rgb]{ .949,  .949,  .949}Middle/High School \\
                               
%                                &  CMMaTH \cite{li2024cmmath}                          &  COLING 2025  & English/Chinese &  23,856 & Middle/High School \\
                               
%                                & \cellcolor[rgb]{ .949,  .949,  .949}PGPS9K \cite{Zhang2023PGPS}                         & \cellcolor[rgb]{ .949,  .949,  .949}IJCAI 2023   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}9,023  & \cellcolor[rgb]{ .949,  .949,  .949}Middle School  \\ 
% \bottomrule[1.2pt]
% \end{tabular}
% }
% \label{table:benchmark_categories}
% \end{table*}



\begin{table*}[tbhp]
\scriptsize
\centering
\caption{Statistics of benchmarks for reasoning LLMs.}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lllllll}
\toprule[1.2pt]
\textbf{Domain}                  & \textbf{Benchmark}                                                            & \textbf{Question Type} & \textbf{Venue}      & \textbf{Language}        & \textbf{Size}   & \textbf{Level}              \\
\hline
\multirow{4}{*}{Math}         

& \cellcolor[rgb]{ .949,  .949,  .949} AIME 2024 \cite{AIME2024}                        &  \cellcolor[rgb]{ .949,  .949,  .949} Open-End & \cellcolor[rgb]{ .949,  .949,  .949}-          & \cellcolor[rgb]{ .949,  .949,  .949}English         &\cellcolor[rgb]{ .949,  .949,  .949} 30     & \cellcolor[rgb]{ .949,  .949,  .949}Competition        \\

&  MATH-500 \cite{lightmanlet}  & Open-End &  ICLR 2024 & English & 500  & Competition        \\

& \cellcolor[rgb]{ .949,  .949,  .949} AMC 2023 \cite{AMC2023}                          & \cellcolor[rgb]{ .949,  .949,  .949} Open-End & \cellcolor[rgb]{ .949,  .949,  .949}--   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}30     & \cellcolor[rgb]{ .949,  .949,  .949}Competition        \\

&  Olympiad Bench   \cite{he2024olympiadbench}    & Open-End  &  ACL 2024     & English/Chinese &  8,476  & Competition        \\
\hline

\multirow{3}{*}{Code}          

& \cellcolor[rgb]{ .949,  .949,  .949}Codeforces                                         & \cellcolor[rgb]{ .949,  .949,  .949} Open-End   & \cellcolor[rgb]{ .949,  .949,  .949}-          & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}-      &\cellcolor[rgb]{ .949,  .949,  .949} Expert             \\

&  SWE-bench \cite{jimenez2024swebench} & Open-End  &  ICLR 2024    &  English         &  2,294  &  Expert             \\

& \cellcolor[rgb]{ .949,  .949,  .949}LiveCodeBench   \cite{jain2024livecodebench}     & \cellcolor[rgb]{ .949,  .949,  .949} Open-End   &\cellcolor[rgb]{ .949,  .949,  .949} ArXiv 2024   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}-      & \cellcolor[rgb]{ .949,  .949,  .949}Expert             \\

\hline
\multirow{3}{*}{Science} 
&  GPQA Diamond \cite{rein2024gpqa}   & Choice &  COLM 2024    &  English         & 448    &  University  \\

& \cellcolor[rgb]{ .949,  .949,  .949} MR-Ben \cite{ZengLWLCDYXQZSL24}        & \cellcolor[rgb]{ .949,  .949,  .949} Hybrid & \cellcolor[rgb]{ .949,  .949,  .949}NeurIPS 2024 & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}5,975 & \cellcolor[rgb]{ .949,  .949,  .949} Hybrid             \\

 & \cellcolor[rgb]{ .949,  .949,  .949}MMLU-Pro \cite{wang2024mmlu} & \cellcolor[rgb]{ .949,  .949,  .949}Choice  & \cellcolor[rgb]{ .949,  .949,  .949}NeurIPS 2024 & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}12,032 & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\

\hline

\multirow{4}{*}{Agent}         
&  WebShop \cite{yao2022webshop}  & Open-End                   
&  NeurIPS 2022 &  English  &  1,600  & Hybrid \\

& \cellcolor[rgb]{ .949,  .949,  .949} WebArena  \cite{zhou2023webarena} & \cellcolor[rgb]{ .949,  .949,  .949}Open-End  & \cellcolor[rgb]{ .949,  .949,  .949}ICLR 2024    & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}812    & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\

&  SciWorld   \cite{scienceworld2022}  & Open-End &  EMNLP 2022   & English   & 7,200  &  Hybrid  \\

& \cellcolor[rgb]{ .949,  .949,  .949}TextCraft   \cite{prasad2024adapt} & \cellcolor[rgb]{ .949,  .949,  .949}Open-End  & \cellcolor[rgb]{ .949,  .949,  .949}NAACL 2024   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}200    & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\

\hline
\multirow{3}{*}{Medicine}      
&  JAMA Clinical \cite{chen2024benchmarking} & Choice &  NAACL 2025   & English  &  1,524  &  Expert      \\

& \cellcolor[rgb]{ .949,  .949,  .949}Medbullets \cite{chen2024benchmarking}  & \cellcolor[rgb]{ .949,  .949,  .949}Choice & \cellcolor[rgb]{ .949,  .949,  .949}NAACL 2025   & \cellcolor[rgb]{ .949,  .949,  .949}English      & \cellcolor[rgb]{ .949,  .949,  .949}308    & \cellcolor[rgb]{ .949,  .949,  .949}Expert             \\

&  MedQA \cite{jin2021disease}  & Choice  &  ArXiv 2020   & English/Chinese &  61,097 &  Expert             \\

\hline
\multirow{5}{*}{Multimodality} 
& \cellcolor[rgb]{ .949,  .949,  .949}MMMU \cite{yue2024mmmu} & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid & \cellcolor[rgb]{ .949,  .949,  .949}CVPR 2024    & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}11,500 & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid             \\

&  MathVista   \cite{lu2024mathvista}  & Hybrid &  ICLR 2024    &  English         &  6,141  &  Middle School      \\

& \cellcolor[rgb]{ .949,  .949,  .949}MathVision \cite{MathVision}   & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid & \cellcolor[rgb]{ .949,  .949,  .949}NeurIPS 2024 & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}3,040  & \cellcolor[rgb]{ .949,  .949,  .949}Middle/High School \\

&  CMMaTH \cite{li2024cmmath}  & Hybrid &  COLING 2025  & English/Chinese &  23,856 & Middle/High School \\

& \cellcolor[rgb]{ .949,  .949,  .949}PGPS9K \cite{Zhang2023PGPS}  & \cellcolor[rgb]{ .949,  .949,  .949}Hybrid & \cellcolor[rgb]{ .949,  .949,  .949}IJCAI 2023   & \cellcolor[rgb]{ .949,  .949,  .949}English         & \cellcolor[rgb]{ .949,  .949,  .949}9,023  & \cellcolor[rgb]{ .949,  .949,  .949}Middle School  \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\label{table:benchmark_categories}
\end{table*}


In the early stages, reasoning LLMs primarily focused on enhancing pre-trained LLMs with external reasoning algorithms, without altering the underlying model parameters. Approaches such as Tree of Thoughts \cite{Tree_of_Thought} and Reasoning via Planning \cite{hao2023reasoning} utilized LLMs-driven Breadth-First Search, Depth-First Search, and MCTS \cite{Renda_Report_Tree_Search, browne2012survey, DBLP:journals/corr/abs-2410-16033, DBLP:journals/corr/abs-2412-09078} to simulate human-like reasoning processes. 
These methods represented reasoning as tree or graph traversals, where intermediate reasoning states were depicted as nodes, and various reasoning strategies produced distinct reasoning paths. The final decision was made through additional voting mechanisms \cite{wangself} or Monte Carlo-based value estimation to identify the optimal path.


However, these externalized slow-reasoning approaches introduced several challenges:
\begin{enumerate}[itemindent=0em]

\item \textbf{Limited Exploration Space:} The search-based methods required predefined constraints on the breadth, depth, and granularity of the search space, which often restricted the LLM's exploration to a narrow reasoning space. Furthermore, the reasoning strategies across different child nodes of the same parent node frequently lacked sufficient diversity, further limiting exploration.

\item \textbf{Limited Experience Sharing:} Exploration experiences and reasoning information across different paths could only be assessed based on reward models or self-consistency among outcomes. Additionally, search-based methods significantly increased computational overhead, relying on reward models such as PRM/ORM for tree pruning or speculative decoding techniques to accelerate inference.

\end{enumerate}
To overcome these limitations, subsequent models such as rSTaR \cite{rSTaR}, LLaMAV-o1 \cite{thawakar2025llamav}, HiICL-MCTS \cite{HiICL-MCTS}, Mulberry \cite{yao2024mulberry}, g1 \cite{g1}, and Thinking-Claude \cite{Thinking-Claude} introduced richer action spaces. 
These enhanced action spaces offered high-level planning cues, broadening the model's exploration scope and enabling more comprehensive structured search processes. 
However, this approach necessitated careful design of the action spaces to ensure their effectiveness.




With the introduction of models like o1 \cite{openai_o1} and QwQ \cite{qwq-32b-preview}, external reasoning paradigms were internalized within the LLM's context. 
These models initially performed exploratory macro-planning to generate an initial reasoning path, followed by contextual exploration of alternative paths. 
Through mechanisms like ``Rethink'' and ``Verification'', these models produced extended reasoning chains. 
To replicate this internalized capability, STILL-1 \cite{Renda_Report_Tree_Search} linearized tree search outputs into long reasoning chains with attributes such as ``Rethink'', ``Wait'', and ``Explore New Path''. 
Similarly, STILL-2 \cite{Slow_Thinking_with_LLMs_2} and sky-T1 \cite{sky_t1_2025} synthesized long reasoning chains using distillation techniques. 
However, the linearized reasoning chains derived from search-based methods struggled to match the quality of those produced by distillation approaches.





Recent advancements, including DeepSeek-R1 \cite{Deepseek-R1} and Kimi-k1.5 \cite{team2025kimi}, have demonstrated the potential of RL to enhance models like DeepSeek-V3 \cite{liu2024deepseek}, resulting in the emergence of complex behaviors such as long reasoning chains, reflective reasoning, and advanced planning capabilities. 
Remarkably, these sophisticated behaviors were achieved through simple RL scaling. 
SimpleRL \cite{simplerl_reason_blob} sought to replicate these capabilities using a streamlined pipeline and minimal codebase, while R1V \cite{chen2025r1v} explored the development of multimodal reasoning models based on multimodal foundation architectures.


\noindent\textbf{Summary:} The evolution of reasoning LLMs has shifted from externally augmented reasoning to internally embedded reasoning. 
Recent developments emphasize the potential of RL-based scaling to unlock advanced capabilities.

