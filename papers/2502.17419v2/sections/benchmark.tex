
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/Evaluation_Metrics.pdf}
    \caption{Various evaluation metrics of reasoning LLMs divided by task types, technical proposals, and reasoning paradigms.}
    \label{fig:evaluation_metrics}
\end{figure*}

\section{Benchmarking Reasoning LLMs}\label{benchmark}

The development of a robust benchmark is crucial for documenting the advancements in reasoning LLMs capabilities and for identifying promising research directions for future progress. 
Here, we review the benchmarks from three key aspects: categories, evaluation metrics, and performance comparisons, while offering our reflections and insights.

\subsection{Benchmark Categories}\label{benchmark_category}


We categorize reasoning benchmarks by task type, which can be broadly divided into math, code, scientific, agent, medical, and multimodal reasoning. The detailed statistics for these benchmarks are presented in Table \ref{table:benchmark_categories}.

\subsubsection{Benchmark Introduction} 

\begin{enumerate}[itemindent=0em]
\item \textbf{Math Problems:} We document the current popular competition-level mathematical benchmarks to showcase the capabilities of reasoning LLMs, including AIME 2024 \cite{AIME2024}, MATH-500 \cite{lightmanlet}, AMC 2023 \cite{AMC2023}, and Olympiad Bench \cite{he2024olympiadbench}.

\item \textbf{Code Problems:} Code problems requires solid foundation and high logical thinking to evaluate the reasoning ability of reasoning LLMs such as \href{https://codeforces.com/}{Codeforces}, SWE-bench \cite{jimenez2024swebench}, and LiveCodeBench \cite{jain2024livecodebench}. 


\item \textbf{Scientific Problems:} Scientific benchmarks, \emph{i.e.}, GPQA Diamond \cite{rein2024gpqa} and MMLU-Pro \cite{wang2024mmlu}, involve multi-domains reasoning about chemistry, biology, and physics, which requires extensive knowledge accumulation and integrated reasoning. 

\item \textbf{Agent Reasoning:} Realistic tasks often involve complex planning and tool usage, leading to the creation of agent reasoning benchmarks \cite{xi2024agentgym}. For example, WebShop \cite{yao2022webshop} and WebArena \cite{zhou2023webarena} focus on web operations, while SciWorld \cite{scienceworld2022} and TextCraft \cite{prasad2024adapt} are centered around scientific research.

\item \textbf{Medical Reasoning:} Medicine fundamentally involves complex reasoning, spanning tasks from diagnostic decision making to treatment planning. Benchmarks of JAMA Clinical Challenge \cite{chen2024benchmarking}, Medbullets \cite{chen2024benchmarking}, and MedQA \cite{jin2021disease} offer model measurements that mimic the doctor's disease diagnosis. 

\item \textbf{Multimodal Reasoning:} Multimodal reasoning, such as benchmarks of MMMU \cite{yue2024mmmu} and MathVista \cite{lu2024mathvista}, requires cross-modal thinking in combination with text and images. 
Especially for those visual-centered problems, in benchmarks MathVision \cite{MathVision}, MathVerse \cite{zhang2024mathverse}, CMMaTH \cite{li2024cmmath}, and PGPS9K \cite{Zhang2023PGPS}, put forward higher requirements for reasoning LLMs.


\end{enumerate}




\subsubsection{Summary} 

The field of LLMs has advanced rapidly in recent years, with benchmark performance consistently improving. 
Simple reasoning benchmarks, such as GSM8K \cite{cobbe2021training}, MATH-500 \cite{lightmanlet}, and ScienceQA \cite{lu2022learn}, have approached performance saturation. 
Recent studies on reasoning LLMs \cite{guan2025rstarmathsmallllmsmaster,RedStar} show that models designed for long reasoning chains do not significantly outperform those designed for shorter chains on these benchmarks. 
This highlights the urgent need to establish new benchmarks that more effectively assess the reasoning capabilities of reasoning LLMs. 
Moreover, current benchmarks are limited, focusing mainly on solid reasoning tasks. 
Soft reasoning benchmarks, lacking explicitly defined correct answers, offer a more nuanced evaluation, better capturing the complexities and subtleties of human-like reasoning. Furthermore, it is essential to address the issue of data leakage in evaluation processes \cite{li2024open}. 
Ensuring the confidentiality and neutrality of evaluation data is critical to preserving the integrity and reliability of benchmark results.


\subsection{Evaluation Metrics}\label{metrics}

Depending on task types, technical proposals, and reasoning paradigms, various evaluation metrics have been introduced for reasoning LLMs as shown in Figure \ref{fig:evaluation_metrics}. These metrics are designed to more accurately assess the model's performance in handling complex reasoning tasks, ensuring that both the quality and coherence of the generated solutions are effectively measured. 
%See Appendix \ref{ap:metrics} for the specific metric formulas.




\begin{table*}[htbp]
\centering
\caption{Performance of Different Models, including Basic LLMs and Reasoning LLMs, on Plain Text Benchmarks. The \textcolor{deepred}{\textbf{red}} denotes the highest result, and the \textcolor{blue}{\textbf{blue}} denotes the second highest result.}
\label{tab:text_performance}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{l|lccccccc}
\toprule[1.2pt]
 \multicolumn{2}{c}{\multirow{3}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{Math}} & \multicolumn{3}{c}{\textbf{Code}} & \multicolumn{2}{c}{\textbf{General}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-7} \cmidrule(lr){8-9}
 \multicolumn{2}{c}{}  & AIME 2024  & MATH-500  & LiveCodeBench & Codeforces  & SWE Verified & MMLU & GPQA-Diamond \\
 \multicolumn{2}{c}{} & (\textit{Pass@1}) & (\textit{Pass@1}) & (\textit{Pass@1-CoT}) & (\textit{Percentile}) & (\textit{Resolved}) & (\textit{Pass@1}) & (\textit{Pass@1}) \\
\hline
\fontsize{6.8}{9}\selectfont\multirow{4}{*}{\rotatebox{90}{\textbf{Basic LLMs}}} & \cellcolor[rgb]{ .949,  .949,  .949}GPT-4o \cite{gpt4o-0513} &\cellcolor[rgb]{ .949,  .949,  .949} 9.3 & \cellcolor[rgb]{ .949,  .949,  .949}74.6 & \cellcolor[rgb]{ .949,  .949,  .949}34.2 & \cellcolor[rgb]{ .949,  .949,  .949}23.6 & \cellcolor[rgb]{ .949,  .949,  .949}38.8 & \cellcolor[rgb]{ .949,  .949,  .949}87.2 & \cellcolor[rgb]{ .949,  .949,  .949}49.9 \\

& Claude-3.5-Sonnet \cite{claude-3-5-sonnet} & 16.0 & 78.3 & 33.8 & 20.3 & 50.8 & 88.3 & 65.0 \\

& \cellcolor[rgb]{ .949,  .949,  .949}Gemini-2.0-Pro \cite{gemini2.0-pro} &\cellcolor[rgb]{ .949,  .949,  .949} - & \cellcolor[rgb]{ .949,  .949,  .949}91.8 & \cellcolor[rgb]{ .949,  .949,  .949}36.0 &\cellcolor[rgb]{ .949,  .949,  .949} - &\cellcolor[rgb]{ .949,  .949,  .949} - &\cellcolor[rgb]{ .949,  .949,  .949} 86.5 & \cellcolor[rgb]{ .949,  .949,  .949}64.7 \\

&  Deepseek-V3 \cite{liu2024deepseek} & 39.2 & 90.2 & 36.2 & 58.7 & 42.0 & 88.5 & 59.1  \\


% &  Deepseek-V3 \cite{liu2024deepseek} & \cellcolor[rgb]{ .949,  .949,  .949}39.2 & \cellcolor[rgb]{ .949,  .949,  .949}90.2 & \cellcolor[rgb]{ .949,  .949,  .949}36.2 & \cellcolor[rgb]{ .949,  .949,  .949}58.7 & \cellcolor[rgb]{ .949,  .949,  .949}42.0 & \cellcolor[rgb]{ .949,  .949,  .949}88.5 & \cellcolor[rgb]{ .949,  .949,  .949}59.1  \\


% &  \cellcolor[rgb]{ .949,  .949,  .949}Deepseek-V3 \cite{liu2024deepseek} & \cellcolor[rgb]{ .949,  .949,  .949}39.2 & \cellcolor[rgb]{ .949,  .949,  .949}90.2 & \cellcolor[rgb]{ .949,  .949,  .949}36.2 & \cellcolor[rgb]{ .949,  .949,  .949}58.7 & \cellcolor[rgb]{ .949,  .949,  .949}42.0 & \cellcolor[rgb]{ .949,  .949,  .949}88.5 & \cellcolor[rgb]{ .949,  .949,  .949}59.1  \\

\hline

\fontsize{7}{10}\selectfont\multirow{14}{*}{\rotatebox{90}{\textbf{Reasoning LLMs}}} & \cellcolor[rgb]{ .949,  .949,  .949}Eurus-2-7B-PRIME \cite{cui2025process} & \cellcolor[rgb]{ .949,  .949,  .949}26.7 & \cellcolor[rgb]{ .949,  .949,  .949}79.2 & \cellcolor[rgb]{ .949,  .949,  .949}- & \cellcolor[rgb]{ .949,  .949,  .949}- &\cellcolor[rgb]{ .949,  .949,  .949} - & \cellcolor[rgb]{ .949,  .949,  .949}- & \cellcolor[rgb]{ .949,  .949,  .949}-  \\

& InternLM3-8B-Instruct \cite{cai2024internlm2} & 20.0 & 83.0 & - & - & - & 76.6 & 37.4 \\

& \cellcolor[rgb]{ .949,  .949,  .949}rStar-Math-7B \cite{guan2025rstarmathsmallllmsmaster} & \cellcolor[rgb]{ .949,  .949,  .949}46.7 & \cellcolor[rgb]{ .949,  .949,  .949}81.6 &\cellcolor[rgb]{ .949,  .949,  .949} - & \cellcolor[rgb]{ .949,  .949,  .949}- & \cellcolor[rgb]{ .949,  .949,  .949}- & \cellcolor[rgb]{ .949,  .949,  .949}82.7 & \cellcolor[rgb]{ .949,  .949,  .949}54.9 \\

  & STILL-2-32B \cite{Slow_Thinking_with_LLMs_2}& 46.7 & 90.2 & - & - &- & - & - \\
  
    & \cellcolor[rgb]{ .949,  .949,  .949}Redstar-code-math \cite{RedStar} & \cellcolor[rgb]{ .949,  .949,  .949}53.3 & \cellcolor[rgb]{ .949,  .949,  .949}91.2 &\cellcolor[rgb]{ .949,  .949,  .949} - &\cellcolor[rgb]{ .949,  .949,  .949} - &\cellcolor[rgb]{ .949,  .949,  .949} - &\cellcolor[rgb]{ .949,  .949,  .949} - &\cellcolor[rgb]{ .949,  .949,  .949} -  \\
    
 & Search-o1 \cite{li2025search}& 56.7 & 86.4 & 33.0 & - &- & - &63.6 \\
 
 & \cellcolor[rgb]{ .949,  .949,  .949}QwQ \cite{qwq-32b-preview} & \cellcolor[rgb]{ .949,  .949,  .949}50.0 & \cellcolor[rgb]{ .949,  .949,  .949}90.6 & \cellcolor[rgb]{ .949,  .949,  .949}41.9 & \cellcolor[rgb]{ .949,  .949,  .949}62.0 & \cellcolor[rgb]{ .949,  .949,  .949}- &\cellcolor[rgb]{ .949,  .949,  .949} - & \cellcolor[rgb]{ .949,  .949,  .949}54.5 \\
 
  & s1-32B \cite{muennighoff2025s1} & 56.7 & 93.0 & - &- & - &- & 59.6 \\
  
 &\cellcolor[rgb]{ .949,  .949,  .949}OpenAI o1-mini \cite{o1-mini} & \cellcolor[rgb]{ .949,  .949,  .949}63.6 & \cellcolor[rgb]{ .949,  .949,  .949}90.0 &\cellcolor[rgb]{ .949,  .949,  .949} 53.8 & \cellcolor[rgb]{ .949,  .949,  .949}93.4 & \cellcolor[rgb]{ .949,  .949,  .949}41.6 & \cellcolor[rgb]{ .949,  .949,  .949}85.2 & \cellcolor[rgb]{ .949,  .949,  .949}60.0 \\
 
  & LIMO-32B \cite{ye2025limoreasoning} & 57.1 & 94.8 & - &- &- &- & 66.7 \\
  
& \cellcolor[rgb]{ .949,  .949,  .949}Kimi k1.5 long-CoT \cite{team2025kimi} & \cellcolor[rgb]{ .949,  .949,  .949}77.5 & \cellcolor[rgb]{ .949,  .949,  .949}96.2 & \cellcolor[rgb]{ .949,  .949,  .949}62.5 &\cellcolor[rgb]{ .949,  .949,  .949} 94.0 & \cellcolor[rgb]{ .949,  .949,  .949}- &\cellcolor[rgb]{ .949,  .949,  .949} - & \cellcolor[rgb]{ .949,  .949,  .949}- \\

   & DeepSeek-R1 \cite{Deepseek-R1} & \textcolor{blue}{\textbf{79.8}} & \textcolor{blue}{\textbf{97.3}} & \textcolor{blue}{\textbf{65.9}} & \textcolor{blue}{\textbf{96.3}} & \textcolor{blue}{\textbf{49.2}} & \textcolor{blue}{\textbf{90.8}} & 71.5 \\
   
&\cellcolor[rgb]{ .949,  .949,  .949}OpenAI-o1 \cite{openai_o1} & \cellcolor[rgb]{ .949,  .949,  .949}79.2 & \cellcolor[rgb]{ .949,  .949,  .949}96.4 & \cellcolor[rgb]{ .949,  .949,  .949}63.4 & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{deepred}{\textbf{96.6}} & \cellcolor[rgb]{ .949,  .949,  .949}48.9 & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{deepred}{\textbf{91.8}} & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{blue}{\textbf{75.7}} \\


 &OpenAI o3-mini \cite{o3-mini}  & \textcolor{deepred}{\textbf{87.3}} & \textcolor{deepred}{\textbf{97.9}} & \textcolor{deepred}{\textbf{84.6}} & - & \textcolor{deepred}{\textbf{49.3}} & 86.9 & \textcolor{deepred}{\textbf{79.7}} \\

\bottomrule[1.2pt]
\end{tabular}
}
\end{table*}







\subsubsection{Task Types}

In terms of benchmark categories, mathematical reasoning typically uses two main metrics: \textit{Pass@k} and \textit{Cons@k}. The \textit{Pass@k} metric evaluates the model's ability to generate a correct solution within k attempts, measuring the likelihood of success within a limited number of tries. On the other hand, \textit{Cons@k} assesses whether the model consistently produces correct or logically coherent solutions, highlighting the stability and reliability of its reasoning capabilities. For code tasks, the key metrics are \textit{Elo} and \textit{Percentile}, both of which measure the relative skill in generating correct code compared to other models or human programmers. In scientific tasks, evaluation generally employs \textit{Exact Match (EM)} and \textit{Accuracy} for fill-in-the-blank and multiple-choice questions, respectively. 
The \textit{EM} metric judges whether the model's output exactly matches the expected solution, while \textit{Accuracy} measures the proportion of correct answers out of the total number of questions.

\subsubsection{Technical Proposals}

Based on technical routes, the schemes with ORM or PRM often leverage \textit{RM@k} and \textit{Best-of-N} two evaluation indicators. 
\textit{RM@k} measures whether the reward model can rank the good answer higher in the top k candidates according to reward score, and \textit{Best-of-N} chooses the solution with highest score from N generated reasoning trajectories. Methods for self-consistency are evaluated using \textit{Greedy Decoding}, \textit{Beam Search}, and \textit{Major@k}. \textit{Greedy Decoding} and \textit{Beam Search} control the randomness of the inference process by limiting the sampling range. \textit{Major@k} selects the solution with the most consistent results from k candidate solutions. In RL, metrics reflect both performance in achieving desired outcomes and the efficiency of the learning process. For example, \textit{Cumulative Reward} measures the total reward received by the agent over time, while \textit{Sample Efficiency} assesses the efficiency of the agent's sample usage during learning.


\subsubsection{Reasoning Paradigms}

For reasoning paradigm of the multi-turn solution generation in reasoning LLMs, \textit{Outcome Efficiency} and \textit{Process Efficiency} \cite{Tecent_2_plus_3} are proposed recently to evaluate the efficiency of long thinking specifically. \textit{Outcome Efficiency} metric empirically evaluates how effectively later solutions contribute to accuracy improvements, formulating as the ratio of efficient tokens that contribute to reaching the correct answer, to all output tokens. \textit{Process Efficiency} metric evaluates the contribution of later solutions to solution diversity empirically, concretely representing as the ratio of tokens of distinct solutions to all solution tokens. These two indicators reveal to the overthinking issue of existing reasoning LLMs to simple problems certainly.


\subsubsection{Summary} 

Most of the existing evaluation metrics are judged according to the final answer. 
It is imperative to develop a comprehensive assessment framework that considers various aspects of the reasoning process in view of the large inference computation consumption. 
Current popular evaluation frameworks, such as LMMs-Eval \cite{zhang2024lmmsevalrealitycheckevaluation}, OpenCompass \cite{2023opencompass}, and PRMBench \cite{song2025prmbench}, lack efficiency and their metrics do not adequately account for the computational and temporal efficiency of the reasoning process. 
To address these shortcomings, we highly recommend exploring more efficient proxy tasks as potential solutions. 
By identifying and utilizing tasks that better capture the nuances of long reasoning chains, we can develop more robust and effective evaluation metrics to enhance the overall assessment framework, ensuring that it not only measures the accuracy of the final output but also evaluates the efficiency and coherence of the reasoning process throughout.





\subsection{Performance Comparison}\label{performance_compare}

In this section, we compare the performance of different reasoning LLMs and their corresponding foundational LLMs on plain text benchmarks, such as math and code problems, as well as on multimodal benchmarks. 
The comprehensive real-time leaderboard is available on \href{https://lmarena.ai/?leaderboard}{Arena}.




\subsubsection{Performance on Plain Text Benchmarks}

As shown in Table \ref{tab:text_performance}, reasoning LLMs, such as DeepSeek-R1 \cite{Deepseek-R1} and OpenAI-o1/o3 \cite{openai_o1, o3-mini}, demonstrate exceptional performance across a wide range of tasks, including math, coding, and other general tasks. 
These models achieve high scores on multiple plain-text benchmarks, such as AIME 2024, MATH-500, and LiveCodeBench, showcasing their robust text-based reasoning abilities. 
In contrast, foundational LLMs, like GPT-4o \cite{openai2023gpt4}, Claude-3.5-Sonnet \cite{claude-3-5-sonnet}, and DeepSeek-V3 \cite{liu2024deepseek}, generally perform less effectively than reasoning LLMs, particularly in math and coding tasks (\emph{e.g.}, AIME 2024 and Codeforces). For example, OpenAI-o1 outperforms GPT-4o by 69.9\% and 73\% on these tasks, respectively. 
Moreover, DeepSeek-R1, based on the DeepSeek-V3 architecture, surpasses its predecessor on all benchmarks, further highlighting the advantages of the reasoning LLMs.







\subsubsection{Performance on Multimodal Benchmarks}

As shown in Table \ref{tab:vlm_performance}, reasoning LLMs continue to excel in multimodal tasks. OpenAI-o1 \cite{openai_o1} performs strongly in vision tasks, achieving the highest score of 77.3\% on MMMU and outperforming its corresponding foundational LLM, GPT-4o \cite{openai2023gpt4}, by 7.2\% on MathVista. However, the performance improvement in multimodal tasks is less pronounced compared to text-only tasks. This can be attributed in part to the limitations of current multimodal reasoning LLM techniques, as well as the lack of sufficient datasets to fully assess the multimodal capabilities of reasoning LLMs.


\begin{table}[tbp]
\centering
\caption{Performance of Models, including Basic LLMs and Reasoning LLMs, on Multimodal Benchmarks. The \textcolor{deepred}{\textbf{red}} denotes the highest result, and the \textcolor{blue}{\textbf{blue}} denotes the second highest result.}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{c|lcccc}
\toprule[1.2pt]
 \multicolumn{2}{c}{{\textbf{Model}}} & \textbf{MMMU} & \textbf{Mathvista} & \textbf{Mathvision} & \textbf{Olympiadbench} \\
\midrule
\fontsize{4.5}{6}\selectfont\multirow{3}{*}{\rotatebox{90}{{\textbf{Basic LLMs}}}} &\cellcolor[rgb]{ .949,  .949,  .949}GPT-4o \cite{gpt4o-0513} & \cellcolor[rgb]{ .949,  .949,  .949}69.1 & \cellcolor[rgb]{ .949,  .949,  .949}63.8 &\cellcolor[rgb]{ .949,  .949,  .949} 30.4 & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{deepred}{\textbf{25.9}} \\

&Claude-3.5-Sonnet \cite{claude-3-5-sonnet} & 70.4 & 65.3 & \textcolor{blue}{\textbf{35.6}} & - \\

&\cellcolor[rgb]{ .949,  .949,  .949}Gemini 2.0 Pro \cite{gemini2.0-pro} & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{blue}{\textbf{72.7}} & \cellcolor[rgb]{ .949,  .949,  .949}- &\cellcolor[rgb]{ .949,  .949,  .949} - & \cellcolor[rgb]{ .949,  .949,  .949}- \\

\hline

\fontsize{4.5}{6}\selectfont\multirow{4}{*}{\rotatebox{90}{{\textbf{Reasoning LLMs}}}} &LLaVA-CoT \cite{xu2024llava} & - & 54.8 & - & - \\

&\cellcolor[rgb]{ .949,  .949,  .949}QvQ-72B-preview \cite{qvq-72b-preview} & \cellcolor[rgb]{ .949,  .949,  .949}70.3 & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{blue}{\textbf{71.4}} & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{deepred}{\textbf{35.9}} & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{blue}{\textbf{20.4}} \\

&Kimi k1.5 long-CoT \cite{team2025kimi} & 70.0 & \textcolor{deepred}{\textbf{74.9}} & - & - \\

&\cellcolor[rgb]{ .949,  .949,  .949}OpenAI-o1 \cite{openai_o1} & \cellcolor[rgb]{ .949,  .949,  .949}\textcolor{deepred}{\textbf{77.3}} & \cellcolor[rgb]{ .949,  .949,  .949}71.0 &\cellcolor[rgb]{ .949,  .949,  .949} - &\cellcolor[rgb]{ .949,  .949,  .949} - \\

\bottomrule[1.2pt]
\end{tabular}
}
\label{tab:vlm_performance}
\end{table}




\subsubsection{Summary}

In summary, reasoning LLMs show strong performance across both plain text and multimodal benchmarks, particularly excelling in math and coding tasks, where they outperform foundational LLMs by a large margin. 
Although the improvement in multimodal tasks is not as pronounced as in text-only tasks, reasoning LLMs still surpass their counterparts, highlighting their potential for processing both image and text data. 
These results emphasize the versatility and effectiveness of reasoning LLMs across a broad spectrum of reasoning tasks, with potential for further advancements in multimodal reasoning techniques.
