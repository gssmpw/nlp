
\section{Challenges \& Future Directions}\label{future}

Despite the rapid advancements in reasoning LLMs, several challenges persist, limiting their generalizability and practical applicability. 
This section outlines these challenges and highlights potential research directions to address them.

\subsection{Efficient Reasoning LLMs}\label{efficient_srs}

While reasoning LLMs excel at solving complex problems via extended inference, their reliance on long autoregressive reasoning within large-scale architectures presents significant efficiency challenges. 
For example, many problems on platforms like Codeforces require over 10,000 tokens of reasoning, resulting in high latency. 
As noted in \cite{Tecent_2_plus_3}, even when a reasoning LLM identifies the correct solution early, it often spends considerable time verifying its reasoning. 
Recent reports, such as Deepseek-R1 \cite{Deepseek-R1}, suggest that self-improvement via RL is more effective in larger models, while smaller-scale large language models (SLMs) (\emph{e.g.}, 3B and 7B models as explored by \cite{simplerl_reason_blob} and \cite{yeo2025demystifying,tinyzero}) struggle to match performance in slow-thinking reasoning tasks.

% yeo2025demystifying

Future research should focus on two key areas: (1) integrating external reasoning tools to enable early stopping and verification mechanisms, thus improving the efficiency of long inference chains, and (2) exploring strategies to implement slow-thinking reasoning capabilities in SLMs without sacrificing performance.


\subsection{Collaborative Slow \& Fast-thinking Systems}\label{slow-fast_srs}


A key challenge in reasoning LLMs is the loss of fast-thinking capabilities, which results in inefficiencies when simple tasks require unnecessary deep reasoning. 
Unlike humans, who fluidly switch between fast (\textit{System 1}) and slow (\textit{System 2}) thinking, current reasoning LLMs struggle to maintain this balance. 
While reasoning LLMs ensure deliberate and thorough reasoning, fast-thinking systems rely on prior knowledge for quick responses. 
Despite efforts such as the \textit{System 1-2} switcher \cite{VisualSlowAgent}, speculative decoding \cite{fast_speculative_slow_thinking, ning2023skeleton_of_thought, SpecInfer}, and interactive continual learning \cite{qi2024interactive, litesearch, mix_tree}, integrating both modes of thinking remains challenging. 
This often leads to inefficiencies in domain-specific tasks and underutilized strengths in more complex scenarios.

Future research should focus on developing adaptive switching mechanisms, joint training frameworks, and co-evolution strategies to harmonize the efficiency of fast-thinking systems with the precision of reasoning LLMs. 
Achieving this balance is crucial for advancing the field and creating more versatile AI systems.




\subsection{Reasoning LLMs For Science}\label{src_science}

Reasoning LLMs play a crucial role in scientific research \cite{zheng2024openresearcher}, enabling deep, structured analysis that goes beyond the heuristic-based fast-thinking models. 
Their value becomes especially clear in fields that demand complex reasoning, such as medicine and mathematics. 
In medicine, particularly in differential diagnosis and treatment planning, reasoning LLMs (\emph{e.g.}, inference-time scaling) enhance AI's step-by-step reasoning, improving diagnostic accuracy where traditional scaling methods fall short \cite{huang2025o1}. 
In mathematics, approaches like FunSearch \cite{romera2024mathematical} incorporate slow-thinking principles to push beyond previous discoveries, showcasing the potential of AI-human collaboration.

Beyond these fields, reasoning LLMs can foster advancements in physics, engineering, and computational biology by refining model formulation and hypothesis testing. 
Investing in reasoning LLMs research not only bridges the gap between AI's computational power and human-like analytical depth but also paves the way for more reliable, interpretable, and groundbreaking scientific discoveries.

\subsection{Deep Integration of Neural and Symbolic Systems}\label{llm_symbol}


Despite significant advancements in reasoning LLMs, their limited transparency and interpretability restrict their performance in more complex real-world reasoning tasks. 
The reliance on large-scale data patterns and lack of clear reasoning pathways makes it challenging to handle intricate or ambiguous problems effectively. 
Early symbolic logic systems, while less adaptable, offered better explainability and clearer reasoning steps, leading to more reliable performance in such cases.

A promising future direction is the deep integration of neural and symbolic systems. Google's AlphaGeometry \cite{trinh2024solving} and AlphaGeometry2 \cite{chervonyi2025gold} combine reasoning LLMs with symbolic engines, achieving breakthroughs in the International Olympiad in Mathematics (IMO). 
In particular, AlphaGeometry2 utilizes the Gemini-based model \cite{team2023gemini, team2024gemini, gemini2.0-pro} and a more efficient symbolic engine, improving performance by reducing rule sets and enhancing key concept handling. 
The system now covers a broader range of geometric concepts, including locus theorems and linear equations. 
A new search algorithm and knowledge-sharing mechanism accelerate the process. 
This system solved 84\% of IMO geometry problems (2000-2024), surpassing gold medalists' averages. 
In contrast, reasoning LLMs like OpenAI-o1 \cite{openai_o1} failed to solve any problems. 
The integration of neural and symbolic systems offers a balanced approach, improving both adaptability and interpretability, with vast potential for complex real-world reasoning tasks beyond mathematical geometry problems.




\subsection{Multilingual Reasoning LLMs}\label{mlan-srs}

Current reasoning LLMs perform well in high-resource languages like English and Chinese, demonstrating strong capabilities in tasks such as translation and various reasoning tasks \cite{DRT-o1, Marco_o1}. 
These models excel in environments where large-scale data and diverse linguistic resources are available. 
However, their performance in low-resource languages remains limited \cite{MGSM}, facing challenges related to data sparsity, stability, safety, and overall performance. 
These issues hinder the effectiveness of reasoning LLMs in languages that lack substantial linguistic datasets and resources.

Future research should prioritize overcoming the challenges posed by data scarcity and cultural biases in low-resource languages. 
Innovations such as parameter sharing across reasoning LLMs and the incremental injection of domain-specific knowledge could help mitigate these challenges, enabling faster adaptation of slow-thinking capabilities to a broader range of languages. 
This would not only enhance the effectiveness of reasoning LLMs in these languages but also ensure more equitable access to advanced AI technologies.


\subsection{Multimodal Reasoning LLMs}\label{mm-srs}

Extending slow-thinking reasoning capabilities from text-based domains to multimodal contexts remains a significant challenge, especially in tasks requiring fine-grained perception \cite{SlowPerception, vll_app, li2023lans}. While approaches like Virgo \cite{virgo} have attempted to distill text-based slow-thinking reasoning into multimodal LLMs, their performance improvements in tasks such as MathVision \cite{MathVision}, which demand detailed visual understanding, have been marginal.

Key research directions include developing hierarchical reasoning LLMs that enable fine-grained cross-modal understanding and generation, tailored to the unique characteristics of modalities such as audio, video, and 3D data.



\subsection{Safe Reasoning LLMs}\label{srs_safety}

The rapid development of reasoning LLMs like OpenAI-o1 \cite{openai_o1} and DeepSeek-R1 \cite{Deepseek-R1} has led to the rise of superintelligent models capable of continuous self-evolution. 
However, this progress brings challenges in safety and control \cite{mei2024slang, mei2024hiddenguard, mei2024not}. 
RL, a key training method, introduces risks such as reward hacking, generalization failures, and language mixing, which can lead to harmful outcomes. 
Ensuring the safety of such systems like DeepSeek-R1 is urgent. While RL enhances reasoning, its uncontrollable nature raises concerns about safely guiding these models. 
SFT addresses some issues but is not a complete solution. 
A hybrid approach combining RL and SFT is needed to reduce harmful outputs while maintaining model effectiveness \cite{parmar2025challenges}.

As these models surpass human cognitive capabilities, ensuring their safe, responsible, and transparent use is crucial. 
This requires ongoing research to develop methods for controlling and guiding their actions, thereby balancing AI power with ethical decision-making. 



% \subsection{Safe SRS}\label{srs_safety}


% \ding{186} \textbf{Agent Slow-Reasoning}

% A single agent has the abilities of reasoning, planning, searching, and memory. 
% The Slow-Reasoning System enhances the single agent's reasoning capabilities and provides a reflection mechanism,  helping to eliminate knowledge conflicts and system conflict issues. The use of LLMs are widly used as the core of multi-agents for complex collaborative tasks,  such as Software AI \cite{chatdev} and Assistant-Programming \cite{Agent_R},  has shown promise. The unique planning ability,  reflection,  and validation features of the Slow-Reasoning System play an important role in constructing robust and reliable agent systems. 

% Future research should explore slow-reason unified agent scheduling frameworks,  interactive slow-reason collaboration protocols,  and robust self-correction mechanisms to enhance the performance of multi-agent systems.












