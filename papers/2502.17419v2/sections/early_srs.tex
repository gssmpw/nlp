
\section{Foundations of Reasoning LLMs}\label{early_srs}

% rl supervise, prm->rl /reward shaping技术, macro-action->symbolic模板或者专家规则,  struture mcts-> search/概率图，self-improve alphago


In this section, we provide a concise overview of the progress in foundational LLMs and the early development of key \textit{System 2} technologies, highlighting critical advancements that, when combined with foundational LLMs, have paved the way for reasoning LLMs. These advancements include symbolic logic systems, MCTS, and RL.


\subsection{Foundational LLMs}\label{f_llm}


The development of foundational LLMs saw significant advancements with the introduction of pretrained Transformers \cite{vaswani2017attention} in 2018-2019, notably through BERT \cite{DBLP:conf/naacl/DevlinCLT19} and GPT \cite{radford2018improving}. 
These models leveraged unsupervised pretraining on vast text corpora, followed by fine-tuning for task-specific applications. This approach enabled them to develop a broad language understanding before specializing in tasks such as sentiment analysis, entity recognition, and question answering. 
BERT's bidirectional context processing improved word understanding, while GPT excelled in text generation with its unidirectional design.

The release of GPT-2 \cite{radford2019language} in 2019, with 1.5 billion parameters, marked a significant leap in generative performance, though it also raised ethical concerns. 
GPT-3 \cite{brown2020language}, with 175 billion parameters, further demonstrated the power of unsupervised pretraining, excelling in few-shot learning and performing well across a wide range of NLP tasks. 
In subsequent years, multimodal models like CLIP \cite{radford2021learning} and DALL-E \cite{ramesh2021zero} emerged, integrating text and visual inputs. 
These models enabled new tasks, such as generating images from text, and enhanced human-computer interaction.


By 2023-2024, models such as GPT-4/4o \cite{openai2023gpt4,gpt4o-0513}, LLaMA \cite{touvron2023llama}, and LLaVA \cite{liu2023llava} demonstrated advanced capabilities in reasoning, contextual understanding, and multimodal reasoning, processing both text and images \cite{alayrac2022flamingo, DBLP:conf/icml/0008LSH23, DBLP:journals/corr/abs-2305-06500}. 
DeepSeek-V3 \cite{liu2024deepseek}, featuring a 671B Mixture-of-Expert architecture \cite{he2021fastmoe, du2022glam, dai2024deepseekmoe}, outperforms several other LLMs on key benchmarks while offering significant improvements in efficiency and processing speed. 
The evolution of foundational LLMs has revolutionized AI, enabling more sophisticated applications in language comprehension, problem-solving, and human-machine collaboration.



\noindent\textbf{Summary:} The development of foundational LLMs has progressed from pretrained transformers like BERT to multimodal models such as GPT-4, enhancing language understanding, text generation, and image processing. 
This advancement has led to significant breakthroughs in AI, improving language comprehension, problem-solving, and human-computer interaction. Building on deep learning advancements \cite{rumelhart1986learning, lecun1995convolutional, hochreiter1997long, hinton2006fast, hinton2006reducing, hinton2012deep, krizhevsky2012imagenet, DBLP:conf/emnlp/ChoMGBBSB14, sutskever2014sequence, srivastava2014dropout, kingma2014adam, lecun2015deep, he2016deep, huang2017densely,vaswani2017attention, goodfellow2020generative}, foundational LLMs can learn extensive world knowledge and semantic relationships from vast textual or multimodal data. This enables them to exhibit emergent capabilities such as In-Context Learning (ICL) \cite{min2022rethinking, dong2024survey}, prompt engineering \cite{white2023prompt, lester2021power}, and Chain-of-Thought (CoT) reasoning \cite{wei2022chain}, significantly enhancing their adaptability and creative problem-solving abilities.

Despite this progress, foundational LLMs operate similarly to \textit{System 1} reasoning, relying on fast, heuristic-driven decision-making and lacking the step-by-step analysis characteristic of \textit{System 2}. However, their developments lay a solid foundation for future reasoning LLMs\textendash especially when integrated with the following early \textit{System 2} technologies. This combination paves the way for more versatile, flexible, and human-like reasoning models.












\subsection{Symbolic Logic Systems}\label{symb_exp}


Symbolic logic systems mark the earliest phase of AI, utilizing rules and logical principles to represent knowledge and draw conclusions \cite{lewis1959symbolic, carnap2012introduction}. 
They are particularly effective in structured domains, where formal logic ensures precision.

Prolog, a logic programming language based on first-order logic, allows users to define facts, rules, and reason through queries. 
It has been pivotal in symbolic reasoning systems, especially in NLP and expert systems \cite{colmerauer1990introduction, clocksin2003programming, apt1997logic}. Logic-based systems like Prolog employ propositional and predicate logic for formal reasoning \cite{singh1999formal, jeroslow1988computation}. 
From the 1960s to the early 1980s, this approach dominated AI, with systems like IBM's LISP \cite{mccarthy1978history} for symbolic computation and Resolution Theorem Provers \cite{bachmair2001resolution} for automated reasoning. 
In the 1970s, Marvin Minsky introduced Frames, which organized knowledge into structured frameworks, influencing both expert systems and cognitive science \cite{minsky1974framework}.


\noindent\textbf{Summary:} 
Symbolic logic systems were pivotal milestones in early AI development. 
Based on formal logic, they excelled in well-defined problems, particularly in structured environments. 
However, they also exposed the limitations of rigid, rule-based systems. Despite these constraints, symbolic logic remains foundational to the progress of AI.

Recent advancements in reasoning LLMs have greatly enhanced the emulation of human-like \textit{System 2} cognitive processes through sophisticated thought architectures, known as Macro Action frameworks (Section \ref{macro_action}). By combining symbolic templates or rules with foundational LLMs, macro actions have significantly improved their reasoning capabilities. 
Integrating macro actions into foundational LLMs has transformed their ability to handle complex reasoning tasks, as hierarchical planning allows models to make high-level decisions before delving into specific problem details, mirroring symbolic logic's structured approach.

% Furthermore, recent breakthroughs in combining LLMs with symbolic systems have demonstrated their potential. 
% For instance, Google's AlphaGeometry \cite{trinh2024solving} and AlphaGeometry2 \cite{chervonyi2025gold} combine LLMs with symbolic engines, achieving success in the International Olympiad in Mathematics (IMO). 
% These symbolic engines offer clear reasoning pathways, making them effective for tackling complex or ambiguous problems. 
% While early symbolic systems were less adaptable, they provided better explainability and clearer reasoning steps. 
% When combined with LLMs, they lead to more reliable and interpretable performance. 
% This synergy underscores the continued relevance of symbolic approaches in modern AI.




% \subsubsection{Expert Systems}\label{exp}

% XCON is a configuration expert system that helps design custom computer systems by optimizing hardware and software based on customer specifications \cite{}. 
% Its commercial success demonstrated the practical value of expert systems in business. 

% Building on symbolic logic, expert systems were developed to emulate domain experts' reasoning processes and solve specialized problems \cite{waterman1985guide, jackson1986introduction, giarratano1998expert, mcdermott1982r1, barker1989expert}. 
% These systems use rules to mimic human expertise, providing solutions in fields such as medicine, chemistry, and engineering.

% MYCIN, created in the 1970s, is one of the earliest and most well-known expert systems. 
% It diagnoses blood infections by analyzing symptoms and lab results, offering treatment recommendations through a rule-based engine \cite{van1978mycin}. 
% DENDRAL, which analyzes chemical data like mass spectrometry to predict molecular structures \cite{lindsay1993dendral}, pioneered expert system applications in scientific research, showing how AI could aid in data analysis. 
% COPES, a medical expert system, assists doctors with pathological diagnosis by applying medical knowledge and expert rules \cite{fieschi2013artificial}. 
% By mimicking specialists' reasoning, COPES aimed to improve diagnostic accuracy and support clinical decision-making.




% \subsection{Statistical and Probabilistic Methods}\label{statistical_prob}


% Statistical and probabilistic methods, such as Bayesian inference, Markov processes, and Monte Carlo Tree Search (MCTS), use probabilistic models and manually designed features to handle uncertainty, effectively addressing scenarios with incomplete or noisy data \cite{pearl2014probabilistic}.


% \subsubsection{Bayesian Inference}\label{bayesian}

% Bayesian inference is crucial for modeling dependencies between random variables and their evolution over time. Using frameworks like Bayesian Networks (BNs) and Dynamic Bayesian Networks (DBNs), it allows systems to reason probabilistically about uncertainty. 
% BNs, as directed acyclic graphs, represent conditional dependencies and are used for static causal reasoning \cite{friedman1997bayesian, heckerman1998tutorial}. For instance, in medical diagnosis \cite{weiss1978model}, BNs estimate disease probabilities based on symptoms, while in fault diagnosis, they identify root causes through conditional probabilities. 
% DBNs extend BNs to temporal contexts, making them ideal for modeling time series \cite{murphy2002dynamic}. 
% By linking BNs across time steps, DBNs capture dynamic behaviors and are widely applied in fields like financial forecasting, robotic navigation, and dynamic environment perception.





% \subsubsection{Markov Processes}\label{markov}

% Markov processes model the dynamic evolution of a system's state over time, characterized by the ``Markov property'', where future states depend on the current state, independent of past states \cite{dynkin1965markov}. These include Markov Chains (MCs), Hidden Markov Models (HMMs), and Markov Decision Processes (MDPs), each suited to different applications.

% A MC is a discrete-state model that uses a transition probability matrix to describe state changes \cite{billingsley1961statistical, norris1998markov}, with transitions depending only on the current state:
% \begin{equation}
% \footnotesize
% P(s_{t+1} = j | s_t = i,  s_{t-1},  \dots) = P(s_{t+1} = j | s_t = i)
% \end{equation}
% This model is widely used in weather forecasting, where future states depend on current conditions, and in web page navigation, as in Google PageRank.


% HMMs extend MCs by introducing hidden states, which are inferred from observable data \cite{rabiner1986introduction, eddy1996hidden}. Key components include the initial probability distribution, transition probabilities, and emission probabilities. HMMs are widely used in speech recognition to infer hidden phonemes and in bioinformatics to identify patterns in genes \cite{mor2021systematic}.

% MDPs extend MCs by incorporating actions and rewards to model decision-making, aiming to find a policy that maximizes cumulative rewards \cite{puterman1990markov}. MDPs are widely used in robotic navigation and resource allocation, where the goal is to find optimal paths or strategies to maximize rewards.


\subsection{Monte Carlo Tree Search}\label{mcts}

MCTS is a simulation-based search algorithm for decision-making and planning \cite{browne2012survey}. 
It constructs a search tree through four steps: \textit{Selection}, which chooses the child node with the highest priority using the UCB1 formula:
\begin{equation}
\footnotesize
\text{UCB1} = \frac{w_i}{n_i} + c \sqrt{\frac{\ln N}{n_i}} \text{, }
\end{equation}
where $w_i$ is the total reward of node $i$, $n_i$ is its visit count, $N$ is the parent node's visit count, and $c$ balances exploration and exploitation. \textit{Expansion} adds new nodes, \textit{Simulation} performs random rollouts to evaluate them, and \textit{Backpropagation} updates node statistics. 
MCTS has been widely used in tasks such as optimizing strategies in board games like Go \cite{gelly2011monte} and in robotic path planning, where it helps robots navigate dynamic environments effectively \cite{swiechowski2023monte}.



\noindent\textbf{Summary:} 
MCTS has played a crucial role in the development of reasoning LLMs, particularly in Structural Search (Section \ref{structure_search}). 
By simulating potential future reasoning paths and backpropagating estimated rewards, MCTS helps foundational LLMs efficiently identify the most promising, high-reward paths. 
This process mirrors human-like planning, where future consequences of decisions are considered before taking action. 
By dynamically exploring multiple reasoning trajectories, MCTS enables models to avoid getting stuck in suboptimal paths, making it easier to navigate complex decision spaces. 
This integration has significantly enhanced the ability of LLMs to handle intricate and dynamic reasoning problems, such as those requiring long-term planning or multi-step logical inferences. 
It has allowed LLMs to make more strategic and informed decisions, improving their overall performance in tasks that involve nuanced reasoning and strategic exploration.


% Statistical methods, like MCTS, played a crucial role in the success of DNNs, especially in applications like AlphaZero \cite{silver2017mastering}. 
% By simulating future states and iteratively backpropagating rewards, MCTS enables effective decision-making in complex environments, optimizing strategies such as move exploration in Go. 

% Statistical methods improve AI's ability to handle uncertainty and complex, dynamic data by using probability distributions and random sampling for decision-making in uncertain environments. 
% However, their reliance on manually designed features and model structures can limit their effectiveness in high-dimensional or complex scenarios. 

% With the growth of data availability and computational power, DNNs emerged, marking a transformative shift. 
% DNNs enable end-to-end learning, automatic feature extraction, and the discovery of patterns in large-scale data while capturing nonlinear relationships. 
% This transition from statistical learning to DL signifies a major advancement in AI's sophistication and capability.





\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/compare.pdf}
    \caption{A comprehensive comparison of traditional reasoning models and reasoning LLMs. Reasoning LLMs offer significant advantages over traditional models in areas such as training approaches, adaptability and learning, problem-solving strategies, and generality and scalability.}
    \label{fig:compare}
\end{figure*}




\subsection{Reinforcement Learning}\label{rl}

RL is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards, aiming to maximize cumulative rewards over time \cite{sutton1998reinforcement}. 
Early breakthroughs in RL, such as Q-learning \cite{watkins1992q} and DQNs \cite{mnih2015human}, revolutionized the field by enabling the handling of complex state spaces using Deep Neural Networks (DNNs) \cite{torrado2018deep}. These methods paved the way for scaling RL to real-world tasks, where traditional tabular approaches fell short. 
The advent of deep RL marked a significant step forward, combining the power of deep learning with RL to process high-dimensional inputs, such as images and unstructured data.

A landmark achievement in deep RL was AlphaGo, which demonstrated RL's potential by defeating a world champion in the complex game of Go through self-play \cite{silver2016mastering}. 
This success highlighted deep RL's ability to thrive in environments with large, continuous action spaces and uncertainty. 
Building on this, AlphaZero advanced the approach by mastering multiple board games—chess, Go, and Shogi—using self-play, MCTS, and DNNs \cite{silver2017mastering}. 
AlphaZero's ability to learn entirely from scratch, without prior human knowledge, showcased RL's power in environments requiring long-term strategy and planning.

AlphaStar further expanded the boundaries of deep RL by excelling in the real-time strategy game StarCraft II. 
Unlike board games, StarCraft II presents dynamic, partially observable environments and demands multi-step, real-time decision-making \cite{vinyals2019grandmaster}. 
AlphaStar's success in this domain demonstrated deep RL's capacity to adapt to complex decision-making scenarios that require both strategic planning and tactical execution. 
These advancements in RL and deep RL have greatly expanded AI's potential, transitioning from well-defined, static environments to dynamic, complex settings that demand continuous learning and adaptation.


\noindent\textbf{Summary:} 
Deep RL has proven highly effective in solving complex decision-making tasks. AlphaGo exemplifies this by learning strategies through self-play and defeating the world champion in Go. 
This self-play concept laid the foundation for Self Improvement technology (Section \ref{self-improve}) in reasoning LLMs, both relying on continuous feedback and adjustments to optimize strategies.

In RL, reward shaping has been crucial, especially for multi-step reasoning tasks \cite{ng1999policy}. 
By adjusting the reward signal to provide more granular feedback during intermediate steps, it helps agents navigate complex decision-making paths. 
This concept inspired the development of Reward Modeling (Section \ref{prm}), particularly the process reward model, in reasoning LLMs. 
This model offers step-by-step supervision to identify and correct errors in the reasoning process. 
By mimicking human reasoning, the process reward model ensures more robust and interpretable results, especially in tasks like mathematical problem-solving and code generation, where step-by-step evaluation is critical.

Moreover, RL itself is a powerful tool for reasoning LLMs (Section \ref{rl_supervise}). 
With a reward mechanism, RL guides foundational LLMs to find optimal solutions, especially in dynamic reasoning problems. 
Its simplicity and efficiency make RL invaluable for training and optimizing reasoning LLMs, enhancing the intelligence and self-evolution of AI models. 
The integration of RL has led to significant advancements in reasoning LLMs, as demonstrated by DeepSeek-R1 \cite{Deepseek-R1}, offering more flexible and efficient solutions.


% DNNs have made significant strides across several domains, particularly in CV, NLP, and RL.

% \subsubsection{CV}

% In CV, CNNs \cite{lecun1995convolutional} enabled automatic feature extraction, advancing tasks like image classification \cite{deng2009imagenet}, object detection \cite{zhao2019object}, and segmentation \cite{minaee2021image}. 
% ResNets \cite{he2016deep} addressed the vanishing gradient problem in DNNs, improving image processing accuracy. The development of GANs \cite{goodfellow2020generative} has spurred innovation in image generation and style transfer.

% \subsubsection{NLP}

% In NLP, word embedding techniques, such as Word2Vec \cite{mikolov2013distributed} and GloVe \cite{pennington2014glove}, improved semantic understanding by mapping words to low-dimensional spaces. Seq2Seq models \cite{sutskever2014sequence} and LSTMs \cite{hochreiter1997long} advanced machine translation and text generation. 
% Transformer \cite{vaswani2017attention} significantly boosted performance in NLP tasks, especially question answering and dialogue generation.


% \subsubsection{RL}







% \subsection{Foundational LLMs}\label{f_llms}


















