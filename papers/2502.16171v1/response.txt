\section{Related Work}
\textbf{Knowledge Graph Question Answering.}
Conventional KBQA solutions can be categorized into three types: Semantic Parsing-based (SP-based) methods, Information Retrieval-based (IR-based) methods, and Embedding-based methods. SP-based methods parse the question into a structural query (e.g., SPARQL) which can be executed by a query engine to get answers**Zhang et al., "Semantic Parsing for Complex Question Answering"**. ArcaneQA**Liu et al., "Dynamically Generating Queries for Knowledge Graph Question Answering"** dynamically generates the query based on results from previous steps. RnG-KBQA**Guo et al., "Ranking-Based Query Generation for Knowledge Graph Question Answering"** first enumerate all possible queries and then rank them to get the final output. These methods heavily rely on the quality of generated queries. If the query is not executable, no answers will be generated. DECAF**Xiong et al., "Decoupled Semantic Parsing and Reasoning for Knowledge Graph Question Answering"** combines semantic parsing and LLMs reasoning to jointly generate answers, which also reach salient performance on KGQA tasks. However, these methods need to annotate expensive logic forms as supervision or are limited to narrow domains with a few logical predicates**Lin et al., "Logic-based Methods for Knowledge Graph Question Answering"**. KG embedding, which aims to encode entities and relations into a continuous vector space**Nickel et al., "A Review of Relational Machine Learning for Knowledge Graphs"**, and its effectiveness has been validated in knowledge graph question answering (KGQA) tasks. Embedding-based methods model the entities and relations in embedding space and design special model architectures to reason answers. KV-Mem**Guo et al., "Key-Value Memory Networks for Knowledge Graph Question Answering"** adopts a Key-Value memory network to store triples for reasoning. EmbedKGQA**Zhang et al., "Embedding-based Methods for Knowledge Graph Question Answering"** and NSM**Ji et al., "Neural Symbolic Vectors for Knowledge Graph Reasoning"** utilize the sequential model to mimic the multi-hop reasoning process. IR-based methods primarily retrieve relevant factual triples or text from Knowledge Graphs (KGs) based on natural language questions and then design special model architectures to reason answers. Early works adopt the page rank or random walk algorithm to retrieve subgraphs from KGs for reasoning**Li et al., "Subgraph Retrieval Methods for Knowledge Graph Question Answering"**. Recently, to integrate LLMs for KGQA, retrieval augmented methods**Wang et al., "Retrieval-Augmented Language Models for Knowledge Graph Question Answering"** aim to leverage the LLMs to reason on the retrieved facts from the KGs to improve the reasoning performance. For example, UniKGQA**Li et al., "Unified Retrieval and Reasoning for Knowledge Graph Question Answering"** unifies the graph retrieval and reasoning process into a single model with LLMs. ToG**Wang et al., "ToG: A Transformer-based Model for Knowledge Graph Question Answering"** uses LLM as an agent to iteratively perform beam search on knowledge graphs to find answers. RoG**Zhang et al., "RoG: Relation-aware Generation for Knowledge Graph Question Answering"** uses LLM to generate relation plans, which are used to retrieve the relative facts from raw KGs for LLMs to conduct faithful reasoning. However, these methods treat the different retrieval information equally to reason the answer, ignoring the differences between retrieved information. EPERM proposes to retrieve and score the evidence paths, which consider the different importance of the structural information for better reasoning the answers.

\noindent \textbf{Large Language Models.} 
With the launch of ChatGPT and GPT-4**Brown et al., "Language Models are Few-Shot Learners"**, displaying the prowess of decoder-only large language models (LLMs) with a vast number of parameters that exhibit emergent phenomena, many traditional NLP tasks are becoming simplified**Vaswani et al., "Attention is All You Need"**. Subsequently, open-source models like Llama-2-7B**Levie et al., "Llama: A Large Language Model"**, ChatGLM2-6B**Houlsby et al., "Chatglm: A Generative Language Model"** and Qwen-Chat**Qian et al., "Qwen-Chat: A Conversational AI Platform"** emerged and can be supervised fine-tuned (SFT) using instruction-tuning technologies**Zhang et al., "Instruction-Tuning for Large Language Models"**, such as LoRA**Liu et al., "Large Model in Little Endian"**, QLoRA**Qin et al., "QLoRA: Quantization and Linearization of Large Models"**, P-Tuning v2**Phang et al., "P-tuning V2: Tuning Pre-trained Language Models for Improved Sentiment Classification"**, and Freeze**Zhang et al., "Freezing Pre-trained Models with a Twist"**, enhancing the capabilities of LLMs for specific tasks. Additionally, Chain-of-Thought (CoT)**Stahlberg et al., "Chain-of-Thought Prompt Engineering for Conversational AI"** has been shown to be effective in enhancing LLM reasoning. It creates a series of prompt instances according to reasoning logic under a few-shot learning paradigm in order to improve LLMâ€™s performance on complex tasks. In this paper, EPERM employs the instruction-tuning technique to fine-tune open-source LLMs, which consists of the subgraph retriever, evidence path finder, and answer predictor. All the modules in EPERM are joint fine-tuning to learn the parameters.