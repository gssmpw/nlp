\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Recently, radar has gained significant attention for its reliable performance in conditions such as fog, rain, and low-light environments. Consequently, various radars with different operating modes and unique characteristics have been introduced \cite{harlow2023new}. For example, spinning radar, also known as scanning or imaging radar, offers 360° coverage, a longer perceptible range, and is more resistant to occlusion, making it effective for place recognition or odometry estimation \cite{gadd2020look, park2020pharao, jang2023raplace}. Another widely utilized radar in robotics, phased-array radar, also referred to as \ac{SoC} radar, is lightweight and consumes less power, making it ideal for object tracking in autonomous vehicles \cite{xia2021learning, sengupta2022robust, pearce2023multi}. More recently, 4D \ac{FMCW} radar, which provides elevation information in addition to azimuth and range, has been widely adopted in object detection and \ac{SLAM} \cite{tan20223, cao2018extended, zhuang20234d, zhang20234dradarslam, li20234d}. 

%TABLE
\input{tab/Related}
%TABLE

Despite these advancements in radar systems, research integrating multiple types of radars remains less explored. While some datasets and studies have utilized multi-radar setups, they have all featured homogeneous radars \cite{9495184, zhang2023dual, huai2024snail}. This highlights a gap in existing resources, particularly compared to research on heterogeneous radar systems.
%that complement the strengths of each sensor, such as those combining solid-state and spinning \ac{LiDAR} \cite{jung2023helipr, lim2024helimos, chen2024heterogeneous}, or integrating RGB, thermal, and event cameras \cite{zhu2018multivehicle, gehrig2021dsec, yun2022sthereo}.

% To address this gap, our dataset integrates a sensor suite with heterogeneous radars—4D radar and spinning radar—alongside \ac{FMCW} \ac{LiDAR}, \ac{IMU}, RTK-GPS, and cameras, as shown in \figref{fig:main} and \figref{fig:car}. The HeRCULES dataset, which stands for the \underline{He}terogeneous \underline{R}adar dataset in \underline{C}omplex \underline{U}rban environment for mu\underline{L}ti-s\underline{E}ssion radar \underline{S}LAM, is the first to combine heterogeneous radars, enabling the simultaneous capture of rich spatial and velocity information. Furthermore, instead of using a conventional 3D spinning \ac{LiDAR}, we utilize \ac{FMCW} \ac{LiDAR}, which leverages the advantages of the latest \ac{FMCW} radar by adopting \ac{FMCW} signal methods rather than traditional pulsed laser signals \cite{sayyah2022fully, kim2020fmcw, xu2019fmcw}. This unique setup enables direct comparisons between 4D radar and \ac{FMCW} \ac{LiDAR}, supporting research in radar-\ac{LiDAR} fusion \ac{SLAM} and cross-sensor place recognition. Furthermore, we believe the provided sequences are particularly ideal for multi-session \ac{SLAM} using this heterogeneous sensor setup.

To address this gap, we introduce the HeRCULES dataset—\underline{He}terogeneous \underline{R}adar dataset in \underline{C}omplex \underline{U}rban environment for mu\underline{L}ti-s\underline{E}ssion radar \underline{S}LAM—designed to capture rich spatial and velocity information through the combination of heterogeneous radars. This is the first dataset to integrate both 4D radar and spinning radar alongside \ac{FMCW} \ac{LiDAR}, \ac{IMU}, RTK-GPS, and cameras, as shown in \figref{fig:main} and \figref{fig:car}. Instead of using a conventional 3D spinning \ac{LiDAR}, we utilize \ac{FMCW} \ac{LiDAR}, which leverages the advantages of the latest \ac{FMCW} radar by adopting \ac{FMCW} signal methods rather than traditional pulsed laser signals \cite{sayyah2022fully, kim2020fmcw, xu2019fmcw}. This unique setup enables direct comparisons between 4D radar and \ac{FMCW} \ac{LiDAR}, supporting research in radar-\ac{LiDAR} fusion \ac{SLAM} and cross-sensor place recognition. Furthermore, we believe the provided sequences are particularly ideal for multi-session \ac{SLAM} using this heterogeneous sensor setup.

The HeRCULES dataset encompasses various weather and lighting conditions, diverse traffic scenarios, and environments with a large number of dynamic objects. The sequence paths are designed to include multiple revisits to the same locations to support place recognition research. We provide a \ac{ROS} player and radar format conversion software to facilitate easy integration with existing place recognition and SLAM tools. Additionally, the dataset offers ground truth pose for each sensor and presents benchmark evaluations for \ac{SLAM} and place recognition tasks, ensuring comprehensive validation.

Our main contributions are as follows:

% Traditional sensor setups involving cameras and LiDARs have laid the foundation for autonomous vehicle research and development advancements. However, these sensors often exhibit limitations under adverse environmental conditions such as fog, rain, or low-light scenarios, which can severely impact their performance. This has increased interest in integrating diverse sensor modalities, such as radars and high-resolution LiDARs, to overcome these challenges.
% % \citeauthor{kim2020mulran} \cite{kim2020mulran}
% Traditional sensor setups with cameras and LiDARs have been foundational in autonomous vehicle research, but they face limitations under adverse conditions like fog, rain, or low light, which can affect performance. This has increased interest in integrating diverse sensor modalities, such as radars and FMCW LiDARs, to address these challenges.
% Traditional sensor setups with cameras and LiDARs have been essential in autonomous vehicle research but face challenges in adverse conditions like fog, rain, or low light. 

% Radar has gained significant attention for its reliable performance in conditions such as fog, rain, and low-light environments, with its types generally categorized into spinning radar and phased-array radar based on their actuation methods \cite{harlow2023new}. Spinning radar, also known as scanning or imaging radar, offers 360° coverage but is constrained to 2D planes, lacking elevation data. In contrast, phased-array radar, sometimes called system-on-a-chip (SoC) radar, offers a narrower field of view but delivers 3D positional and velocity information, making these two types complementary to each other. Despite the potential benefits of combining these radars, to the best of our knowledge, there is no existing dataset including heterogeneous radars, such as 4D radar and spinning radar.

% The 4D Frequency Modulated Continuous Wave (FMCW) radar, a type of phased-array radar, demonstrates exceptional performance in tracking high-speed objects \cite{10477463}.
% \input{tab/Related}Similarly, LiDAR technology has advanced by adopting FMCW signals instead of traditional pulsed laser signals, allowing for the capture of high-precision spatial information and velocity data of objects. The integration of these two sensors—both providing velocity information—enables novel research in areas such as sensor fusion for simultaneous localization and mapping (SLAM) and cross-sensor place recognition. However, despite these advantages, no dataset currently combines 4D radar and FMCW LiDAR, highlighting a gap in existing resources.

% A specific of phased-array radar, the 4D Frequency Modulated Continuous Wave (FMCW) radar, performs exceptionally well in various weather conditions, particularly in tracking high-speed moving targets \cite{10477463}.
% \input{tab/Related}Similarly, LiDAR technology has advanced by adopting FMCW signals instead of traditional pulsed laser signals, allowing for the capture of high-precision spatial information and velocity data of objects. The integration of these two sensors can maximize their strengths, providing robust perception even in dynamic and adverse environments. However, despite their complementary features, no dataset currently combines 4D radar and FMCW LiDAR, limiting comparative studies and sensor fusion research between these sensors.

% To address these gaps, our dataset integrates a comprehensive sensor suite that includes heterogeneous radars—4D radar and spinning radar—alongside FMCW LiDAR, IMU, RTK-GPS, and cameras, as shown in \figref{fig:car}. The HeRCULES dataset is the first to combine heterogeneous radars, enabling the simultaneous capture of rich spatial and velocity information for a comprehensive understanding of dynamic environments, as shown in \figref{fig:main}. It also uniquely allows for comparisons between 4D radar and FMCW LiDAR, supporting research in radar-LiDAR fusion SLAM, cross-sensor place recognition, and radar point upsampling.

% The HeRCULES dataset encompasses various weather and lighting conditions, diverse traffic scenarios, and environments with a large number dynamic objects. The sequence paths are designed to include multiple revisits to the same locations to support place recognition research. We provide a Robot Operating System (ROS) player and radar format conversion software to facilitate easy integration with existing place recognition and SLAM tools. Additionally, the dataset offers ground truth pose for each sensor and presents benchmark evaluations for SLAM and place recognition tasks, ensuring comprehensive validation.
% In summary, the main contributions of the HeRCULES dataset are as follows:
 % and utility for researchers

% Moreover, while sensor fusion research is actively progressing in the detection field \cite{wang2022multi}, \cite{wang2022interfusion}, \cite{deng2023see}, \cite{9991894}, studies in SLAM and place recognition with sensor fusion are relatively limited. However, no available dataset combining 4D radar, FMCW LiDAR, and spinning radar highlights a gap in existing resources.

% To address these issues, our dataset incorporates a comprehensive sensor suite that includes heterogeneous radars—4D radar and spinning radar—alongside FMCW LiDAR, IMU, RTK-GPS, and cameras, significantly advancing over existing datasets. Spinning radar offers detailed 360-degree long-range scans crucial for mapping and localization in complex environments. Additionally, the 4D radar enhances perception capabilities by capturing range, azimuth, elevation, RCS, and Doppler velocity. At the same time, FMCW LiDAR offers high-resolution 3D point clouds with reflectivity, intensity, and Doppler velocity measurements. This combination enables the simultaneous capture of rich spatial and velocity information, offering a comprehensive understanding of dynamic environments. Consequently, our dataset is the first to allow comparisons between 4D radar and FMCW LiDAR, which have similar characteristics, and it supports research in radar-LiDAR sensor fusion SLAM and cross-sensor place recognition. 


% The HeRCULES dataset provides a unique multi-modal approach by integrating diverse sensors such as 4D radar, spinning radar, and FMCW LiDAR, offering valuable data for the development and benchmarking of advanced localization algorithms. We also provide pose ground truth for each sensor to support place recognition research. The dataset significantly contributes to SLAM and sensor fusion research, covering varied environmental conditions. For validation, we present benchmark evaluations for SLAM and place recognition tasks.

% Combined with IMU and GPS data, our dataset facilitates highly accurate trajectory estimation, advancing SLAM algorithms and enhancing the reliability of autonomous navigation systems.
% The HeRCULES dataset is unique in its multi-modal approach, which includes a diverse array of sensors and emphasizes the integration of advanced sensing technologies that are underrepresented in current research. Simultaneously capturing data from 4D radar, FMCW LiDAR, and spinning radar provides a rich dataset that supports developing and benchmarking next-generation perception and localization algorithms. By offering multi-modal data across varied environmental conditions, our dataset stands to make significant contributions to the fields of SLAM, sensor fusion, and autonomous vehicle perception.

%TABLE
\input{tab/sensor_tab}
%TABLE



% \begin{itemize}
%     \item Comprehensive Sensor Suite: This dataset only includes a 4D radar, FMCW LiDAR, 2D spinning radar, IMU, and GPS sensors. The rich data collected from these diverse sensors can be effectively utilized for place recognition tasks.
% \end{itemize}
\begin{itemize}
    \item The HeRCULES dataset is the first public dataset, including both a 4D radar and a scanning radar, providing capabilities for localization, mapping, and place recognition. Moreover, it incorporates the latest FMCW LiDAR with 4D radar, uniquely suited for radar-LiDAR fusion SLAM and cross-sensor place recognition. 
\end{itemize}

% Moreover, it is the first dataset to incorporate both 4D radar and \ac{FMCW} \ac{LiDAR}, making it uniquely suited for sensor fusion research.

\begin{itemize}
    \item The HeRCULES dataset encompasses various weather and lighting conditions, a range of traffic scenarios, and environments with a large number of dynamic objects. The dataset covers an extensive area, enabling comprehensive analysis across various environments.
\end{itemize}

\begin{itemize}
    \item We have designed the sequence paths to include multiple revisits to the same locations. This ensures sufficient queries for place recognition and multi-session \ac{SLAM}.
    %evaluation and increasing suitability for SLAM applications.
\end{itemize}

% \begin{itemize}
%     \item A new calibration method for radar and LiDAR sensors is proposed, enhancing the integration of these sensors in multi-modal systems.
% \end{itemize}

\begin{itemize}
    \item We provide a \ac{ROS} player, radar format conversion software for integration with existing place recognition and SLAM tools, and ground truth poses for each sensor to support place recognition research.
\end{itemize}
% \begin{itemize}
%     \item Through extensive experiments, we validate the suitability of each sensor in various environments and compare different SLAM methods, ensuring the dataset's reliability and utility for future research.
% \end{itemize}

