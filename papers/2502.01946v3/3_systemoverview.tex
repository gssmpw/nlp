\section{System overview}
\label{sec:overview}
\subsection{System Configuration}

% The sensor configuration and coordinates of each sensor are depicted in \figref{fig:car}, and their specifications are provided in \tabref{tab:sensors}. The FMCW LiDAR runs at 10 Hz for sensor operation with relative velocity settings aligned to the 4D radar sensor. The 4D radar operates at 20 Hz, providing radar point cloud data and filtered object point cloud data based on the sensor driver  \cite{fernándezcalatayud2024rosrdi}. The spinning radar operates at 4 Hz, while the Xsens MTi-300 captures inertial data at 100 Hz. The SPAN-CPT7 with a dual VEXXIS GNSS-501 antenna operates at 50 Hz using RTK-GPS and INS. All sensor data are processed on an industrial PC, NUVO-9006LP-NX, with an Intel Core i9, 2 TB SSD, and 64 GB DDR5.

% The sensor configuration and coordinates of each sensor are depicted in \figref{fig:car}, and their specifications are provided in \tabref{tab:sensors}. The Aeva LiDAR operates at 10 Hz, with relative velocity settings synchronized with the 4D radar sensor. The Continental radar operates at 20 Hz, providing radar point cloud data and filtered object point cloud data based on the sensor driver  \cite{fernándezcalatayud2024rosrdi}. The Navtech radar operates at 4 Hz, while the Xsens MTi-300 captures inertial data at 100 Hz. The SPAN-CPT7 with a dual VEXXIS GNSS-501 antenna operates at 50 Hz using RTK-GPS and INS. All sensor data are processed on an industrial PC, NUVO-9006LP-NX, with an Intel Core i9, 2 TB SSD, and 64 GB DDR5.

The sensor configuration and coordinates of each sensor are illustrated in \figref{fig:car}, and their specifications are detailed in \tabref{tab:sensors}. The Aeva \ac{LiDAR} operates with relative velocity settings synchronized to the 4D radar sensor. The Continental radar provides both raw radar point cloud data and filtered object point cloud data via its sensor driver. All sensor data are processed on an industrial PC, the NUVO-9006LP-NX, equipped with an Intel Core i9 processor, 2 TB SSD, and 64 GB DDR5 RAM. The sample data is shown in \figref{fig:overview}.

% \cite{fernándezcalatayud2024rosrdi}

 % A LiFePO$_4$ 102.0Ah battery powers all equipment and sensor drivers run on ROS Noetic with Ubuntu 20.04.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[trim= 6.9cm 9.2cm 7.4cm 6cm, clip,width=0.9\linewidth]{figure/cali.pdf}
%     \caption{(a) Spinning radar points (blue), 4D radar points (green), and LiDAR points (red) (b) Utilizing the line-index channel. (c) LiDAR - spinning radar extrinsic calibration pipeline. }
%     \label{fig:cali}
%     \vspace{-5mm}
% \end{figure}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[trim= 7.5cm 7.8cm 7cm 6.5cm, clip,width=\linewidth]{figure/cali.pdf}
%     \caption{(a) LiDAR - spinning radar extrinsic calibration pipeline. (b) Utilizing the line-index channel. (c) LiDAR points, 4D radar points, and spinning radar points are red, green, and blue. (d)   Left camera - LiDAR. (e) Right camera - LiDAR. (f) Left camera - 4D radar. (g) Right camera - 4D radar.}
%     \label{fig:cali}
%     \vspace{-5mm}
% \end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[trim= 7.5cm 17cm 7cm 6.5cm, clip,width=\linewidth]{figure/cali.pdf}
    \caption{(a) LiDAR - spinning radar extrinsic calibration pipeline. (b) Utilizing the line-index channel. (c) LiDAR points, 4D radar points, and spinning radar points are red, green, and blue. (d)   Right camera - LiDAR. (e) Left camera - 4D radar.}
    \label{fig:cali}
    \vspace{-5mm}
\end{figure}

\subsection{ Sensor Calibration}
% The calibration results of LiDAR - 4D Radar - Spinning Radar are shown in \figref{fig:cali}(a).

% \subsubsection{Intrinsic Calibration of Camera}
% With the intrinsic calibration tool \cite{zhang2000flexible}, we utilize a checkboard pattern to calculate the internal parameters simultaneously.

\subsubsection{Extrinsic Calibration of \ac{LiDAR} - Spinning Radar}
We employ the method used in the Boreas dataset \cite{burnett2023boreas}. This method determines the rotation $\textbf{R}^L_R$ and the translation $\textbf{t}^L_R$ in the xy plane through correlative scan matching with the Fourier-Mellin transform \cite{checchin2010radar}. Specifically, we convert \ac{LiDAR} point clouds into \ac{LiDAR} polar images to compare with radar polar images to obtain $\textbf{R}^L_R$. Then, we utilize Cartesian images to derive $\textbf{t}^L_R$. To match the \ac{FOV} of the Aeva, we adjust the range and azimuth of the radar images. The calibration pipeline is shown in \figref{fig:cali}(a).


\subsubsection{Extrinsic Calibration of \ac{LiDAR} - 4D Radar - Camera}
We utilize the calibration tool \cite{domhof2021joint} for cameras, \ac{LiDAR}, and radar. This tool jointly calculates relative transformation parameters using a specialized calibration board and reflector. Although we use a solid-state \ac{LiDAR} instead of a spinning one, we utilize the line-index channel to assess laser depth discontinuity, as illustrated in \figref{fig:cali}(b). Unlike \citeauthor{domhof2021joint} \cite{domhof2021joint}, who estimates the reflector position with 2D radar, we can directly obtain the z-value from our 4D radar, resulting in more accurate calibration.


% The translation parameters in the z-direction are obtained from CAD files. 
\subsubsection{Extrinsic Calibration of \ac{LiDAR} - \ac{IMU}}
We initialize the system using the method proposed by \citeauthor{zhu2022robust} \cite{zhu2022robust}. This approach was designed for the Livox \ac{LiDAR} series, so it can be seamlessly applied to our solid-state Aeva \ac{LiDAR} without requiring specific targets.
    % \item Extrinsic Calibration of INS - IMU: 


\subsubsection{Calibration Evaluation}
% The calibration results of \ac{LiDAR} - 4D Radar - Spinning Radar are shown in \figref{fig:cali}(c).
The calibration results are shown in \figref{fig:cali}(c), \figref{fig:cali}(d) and \figref{fig:cali}(e).


% %FIGURE
% \begin{figure}[!t]
%     \centering
%     \includegraphics[trim=0cm 2.5cm 14.5cm 1.65cm, clip, width=1\columnwidth]{figure/pipeline.pdf}
%     \caption{all pipeline }
%     \label{fig:pipeline}
%     \vspace{-5mm}
% \end{figure}

% In thi \figref{fig:pipeline},ggggg

% \subsection{Point-wise Uncertainty Guided Ground Filtering}
% \label{subsec:ground segmentation}
% % ground plane detection and outlier removal




% \begin{equation}
% \begin{array}{rrclcl}
%     \displaystyle \argmin_{[a,b,c,d]} & \multicolumn{3}{l}{\sum_{i=1}^n D_{M_i}^2}\\
%     \textrm{s.t.} & a^2+b^2+c^2=1.
% \end{array}
% \label{eqn:plane_model}
% \end{equation}

% With \eqref{eqn:plane_model}, we c


% \begin{equation}
%     z_i=\frac{V_{xy}v_z - \sqrt{V_{xy}^2{v^d_i}^2 - ((v^d_i)^2-v_z^2)(x^2+y^2){v^d_i}^2}}{(v^d_i)^2-v_z^2}
% \end{equation}
% where $v_x, v_y, v_z$ is the vehicle's ego velocity, and $V_{xy}=v_x x_i+v_y y_i$. After the height 



% \subsection{Cs}




% \begin{equation}
%     \dot\theta_i(t)\sim\mathcal{GP}(0,k_{r_i}(t,t'))
% \end{equation}

% where $k_{r_i}(t,t')$ is the kernel function representing the covariance of the function instances and the index $i$ represents the $x,y,z$ component.
% Then the inference of the rotation vector is conducted as follows:

% \begin{equation}
%     \dot{\theta}_{i\star}(t)=\mathbf{k}_{r_i}(t,\mathbf{t})(\mathbf{K}_{r_i}(\mathbf{t}, \mathbf{t})+\sigma_i^2\mathbf{I})^{-1}\boldsymbol{\rho}_i
% \end{equation}

% \begin{equation}
% \label{eqn:rot_inference} 
%     \theta_{i\star}(t)=\int\mathbf{k}_{r_i}(t,\mathbf{t})(\mathbf{K}_{r_i}(\mathbf{t}, \mathbf{t})+\sigma_i^2\mathbf{I})^{-1}\boldsymbol{\rho}_i \partial t
% \end{equation}

% $\mathbf{t}=[t_1\dots t_{N_i}]$ is the vector of the \ac{IMU} measurement timestamps between consecutive keyframes,
% $\mathbf{k}_{r_i}(t,\mathbf{t})=[k_{r_i}(t,t_1)\dots k_{r_i}(t,t_{N_i})]^\top$ is the kernel, and
% $\newline \mathbf{K}_{r_i}(\mathbf{t},\mathbf{t})=\begin{bmatrix}
% k_{r_i}(t_1,t_1) & \cdots & k_{r_i}(t_1,t_{N_i}) \\
% \vdots & \ddots & \vdots \\
% k_{r_i}(t_{N_i},t_1) & \cdots & k_{r_i}(t_{N_i},t_{N_i})
% \end{bmatrix}$.

% $\sigma_i$ is the standard deviation of the \ac{IMU} gyroscope measurements, and $\boldsymbol{\rho}_i$ is the vector of $\hat{\dot{\theta}}_i(\mathbf{t})$, which can be achieved with the following optimization problem:

% $\boldsymbol{e}_n^{meas} = J_r(\boldsymbol{\theta}_\star(t_n))\dot{\boldsymbol{\theta}}_\star(t_n)-\Tilde{\boldsymbol{\omega}}(t_n)$ is about the measurement constraint, and $\Sigma_{\boldsymbol{\omega}}$ is the \ac{IMU} measurement covariance matrix at timestamp $t_n$. $\boldsymbol{e}_n^{gp}=\dot{\boldsymbol{\theta}}_{\star}(t_n)-\hat{\dot{\boldsymbol{\theta}}}(t_n)$ 

% $\boldsymbol{\mu}(t)$ as following:
% \begin{equation}
%     {v}_i(t)\sim\mathcal{GP}(\mu_i(t),k_{v_i}(t,t')).
% \end{equation}


% \begin{equation}
% \label{eqn:vel_optimize}
%     \displaystyle \argmin_{[\boldsymbol{\zeta}_x, \boldsymbol{\zeta}_y, \boldsymbol{\zeta}_z]}\sum_{n=1}^{N_r} \left( \norm{\boldsymbol{e}_n^{meas}}_{\Sigma_{\mathbf{v}_n}}^2+\norm{\boldsymbol{e}_n^{gp}}_{\Sigma_{gp}}^2 \right).
% \end{equation}


% In \eqref{eqn:vel_optimize}, $\boldsymbol{e}_n^{meas} = \Delta{\mathbf{R}^\top}\mathbf{v}_\star(t_n)-\Tilde{\mathbf{v}}(t_n)$, and $\boldsymbol{e}_n^{gp}=\mathbf{v}_{\star}(t_n)-\hat{\mathbf{v}}(t_n)$. $\Sigma_{\mathbf{v}_n}$ is the velocity measurement covariance 
% chronization for integration between the \ac{IMU} gyroscope and radar dat
% crement can be inferenced by \eqref{eqn:trans_inference}.

% \subsection{Cluster-based weighted ICP}
% \label{subsec:scan matching}


% Therefore, the transformation $\textbf{T}\in SE(3)$ betwee



% \subsection{Pose Graph Optimization}
% incremental motion from \cref{subsec:velocity integration} and \ref{subsec:scan matching} is used for $SE(3)$ edges in the pose graph, and the covariance of the edge is computed in each estimation step. The $g2o$ \cite{kummerle2011g} 
