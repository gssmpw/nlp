 \begin{figure}[!t]
    \centering
    \includegraphics[trim= 0cm 1.7cm 0.3cm 1.7cm, clip,width=\linewidth]{figure/file.pdf}
    \caption{File structure of the HeRCULES dataset, illustrating the organization of sensor scans, ground truths, calibration, and inertial sensor measurements for each sequence.}
    \label{fig:file}
    \vspace{-4mm}
\end{figure}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[trim= 3.9cm 8.6cm 4cm 9cm, clip,width=0.91\linewidth]{figure/untitled.pdf}
%     \caption{Ground truth pose for each sensor is overlaid on the path of the \texttt{River Island}. Spatial and temporal differences are considered due to sensor mounting variations and timestamp discrepancies.}
%     \label{fig:gtpose}
%     % \vspace{-6mm}
% \end{figure}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[trim= 3.5cm 8.6cm 3.6cm 9cm, clip,width=0.91\linewidth]{figure/untitled.pdf}
%     \caption{Ground truth pose for each sensor is overlaid on the path of the \texttt{River Island}, illustrating a right turn and a straight drive at the same location. Spatial and temporal differences are considered due to sensor mounting variations and timestamp discrepancies.}
%     \label{fig:gtpose}
%     \vspace{-6mm}
% \end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[trim= 3.2cm 8.6cm 3.3cm 9cm, clip,width=0.91\linewidth]{figure/untitled.pdf}
    \caption{Ground truth pose for each sensor is overlaid on the path of the \texttt{River Island}, illustrating a right turn and a straight drive.}
    \label{fig:gtpose}
    \vspace{-5.8mm}
\end{figure}


% \begin{figure}[!t]
%     \centering
%     \includegraphics[trim= 5cm 5cm 5.3cm 6cm, clip,width=0.45\linewidth, angle=270]{figure/orora_map.pdf}
%     \caption{Mapping result of ORORA on the \texttt{Sport complex}.}
%     \label{fig:map}
%     \vspace{-5mm}
% \end{figure}

\section{Evaluation of HeRCULES Dataset}
\label{sec:evaluation}

% In this section, we use our dataset to evaluate the performance of 4D LiDAR, 4D radar, and spinning radar SLAM algorithms. Additionally, we assess place recognition methods for 4D LiDAR and 4D radar in our dataset.

% In this section,  we assess place recognition methods for 4D LiDAR and 4D radar with our dataset. Additionally, we use our dataset to evaluate the performance of 4D LiDAR, 4D radar, and spinning radar SLAM algorithms.

% In this section, we evaluate \ac{SLAM} algorithms and place recognition methods for 4D \ac{LiDAR}, 4D radar, and spinning radar using our dataset.

% In this section, we evaluate \ac{SLAM} and place recognition methods for 4D \ac{LiDAR}, 4D radar, and spinning radar.

\subsection{SLAM Evaluation}
For the \ac{SLAM} baseline, we use Fast-LIO \cite{xu2021fast} for \ac{LiDAR} {SLAM}, 4DRadarSLAM \cite{zhang20234dradarslam} for 4D radar \ac{SLAM}, and ORORA \cite{lim2023orora} for spinning radar \ac{SLAM}. \figref{fig:main} shows the mapping results for the \texttt{Sports Complex} using ORORA. A comparison of these three baselines on \texttt{Sports Complex} and \texttt{Library} is shown in  \figref{fig:slam} and  \tabref{tab:slam}. \ac{ATE} is measured in meters, while \ac{RPE} is quantified in degrees per meter for rotation (RPE$_r$) and as a percentage for translation (RPE$_t$). Among these baselines, the odometry result of Fast-LIO was the most accurate, followed by ORORA and 4DRadarSLAM. The result of 4DRadarSLAM is not as good because the point cloud from the Continental radar we used contains fewer points than the Oculii radar originally used in 4DRadarSLAM. However, there is potential for improvement through preprocessing the raw point cloud. These findings validate that \ac{SLAM} performance with 4D radar alone is limited on our dataset, highlighting the need for heterogeneous radar \ac{SLAM} or radar-\ac{LiDAR} fusion \ac{SLAM}.

 % The result using 4D radar is not as good, but there is potential for improvement through preprocessing the raw point cloud to reduce noise. Using Spinning radar, the scale differs, but the ground truth and approximate trajectory are similar.

\input{tab/SLAM_tab}
%TABLE

\begin{figure}[!t]
    \centering
    \includegraphics[trim= 3.6cm 8.5cm 4cm 9.1cm, clip,width=\linewidth]{figure/odom_spo.pdf}
    % \caption{Estimated odometry of Fast-LIO, 4DRadarSLAM and ORORA with the ground truth for \texttt{Sport complex}.}
    % \label{fig:slam1}
    \vspace{-8.5mm}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[trim= 3.5cm 8.5cm 3.9cm 9cm, clip,width=\linewidth]{figure/odom_lib.pdf}
    \caption{Estimated odometry of Fast-LIO, 4DRadarSLAM and ORORA with the ground truth for \texttt{Sports Complex 01} and \texttt{Library 01}.}
    \label{fig:slam}
    \vspace{-7.8mm}
\end{figure}


\subsection{Place Recognition Evaluation}
% We use the precision-recall curve (PR curve) \cite{lowry2015visual} as the evaluation metric for place recognition. Additionally, we use the AUC, a single scalar value that summarizes the PR curve, to measure the overall performance.
We use Scan Context \cite{kim2018scan} for place recognition with both 4D \ac{LiDAR} and 4D radar. Additionally, we evaluate cross-modal place recognition performance using radar queries on a \ac{LiDAR}-based database. The experiments utilize \texttt{01} sequences as the database and \texttt{02} sequences as the query set, with the results shown in \figref{fig:pr}. For the \texttt{Sports Complex}, a query is considered correct if the top result is within \unit{10}{m}, indicating accurate recognition of a revisited location or rejection of false positives. The ablation study conducted for the \texttt{Library} shows the results for thresholds of \unit{10}{m}, \unit{15}{m}, and \unit{20}{m} in \figref{fig:pr2}. The AUC scores for the place recognition evaluation are presented in \tabref{tab:pr}.
These results highlight that 4D \ac{LiDAR} achieves the highest place recognition performance, while cross-modal recognition between \ac{LiDAR} and radar is the lowest. This suggests a need for further research in place recognition using heterogeneous sensors, for which HeRCULES offers valuable data.

% For Place Recognition evaluation, we used the precision-recall curve \cite{lowry2015visual} as the evaluation metric, as shown in Fig. 9. A query is considered correct if the top 1 retrieved result is within 10 m, meaning the robot correctly identifies a previous location if it revisits within this range or avoids false positives by rejecting localization.
% For the place recognition baseline, we use Scan Context \cite{kim2018scan} for LiDAR and spinning radar place recognition and RaPlace \cite{jang2023raplace} for spinning radar place recognition. \figref{fig:pr} compares these three baselines on our dataset's Sports complex sequence.

% The AUC scores are 0.976 for Aeva (4D LiDAR), 0.809 for Continental (4D radar), and 0.401 for Aeva-Continental (cross-modal). 
% The AUC scores are 0.976 for Aeva (10m), 0.809 for Continental (4D radar), and 0.401 for Aeva-Continental (cross-modal).
