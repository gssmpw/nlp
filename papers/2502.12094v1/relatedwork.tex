\section{Related Work}
Recent research has extensively explored methods to enhance LLM performance through advanced test-time compute approaches. \citet{wu2024inference} investigates scaling behaviors of sampling strategies, while \citet{muennighoff2025s1} show positive results with a simple test-time scaling approach called budget forcing.
Recent work highlight emerging potential for self-feedback~\cite{weng2023large,chen2025sets}. Likewise, there is also critique on whether models can truly evaluate and correct their own outputs~\cite{stechly2024selfverificationlimitationslargelanguage,kambhampati2024position}. 

Beyond sampling and iterative refinement, other works propose using Monte Carlo Tree Search (MCTS) as a more structured approach to improve reasoning~\cite{hao-etal-2023-reasoning, zhou2023language,zhang2024accessinggpt4levelmathematical}. MCTS can explore the search space effectively, trading off exploration and exploitation using the $UCT$ criterion: 
% (Equation ~\ref{eq:UCT}) 
\begin{equation}
    UCT(a) = Q(s, a) + w\sqrt{ \frac{\log{N(s)} }{\log N(c(s, a)) } } \label{eq:UCT}
\end{equation}
where $Q(s, a)$ is the Q-value of taking an action $a$ from node $s$. $N(s)$  and $N(c(s, a))$ is the number of visits to node $s$ and its children, $c(s, a)$ respectively.  In  expansion and simulation (rollouts) stages, a new node is created and the Q-value, corresponding to the action that created the node, is initialized with rewards and updated during backpropagation. These rewards will be based on some source of feedback, whether it is from the ground-truth or another source of verification. Thus, the quality of this feedback is crucial for search to operate successfully.

\begin{table*}[t] % Table at the top of the page
    \centering % Center the table
    \begin{tabular}{lrrrrr}
        \toprule % Top rule
        & Llama 3 & Mistral v0.3 & Haiku 3  & Sonnet 3 \\
        \midrule % Middle rule
        No Search & 0.813 & 0.426 & 0.866  & 0.757 \\
        MCTS: Ground-truth Feedback &  0.958  &  0.82  & 0.964  &  0.923  \\
        \midrule
        MCTS: Random Selection & 0.751 & 0.45  & 0.864 &  0.680 \\
        MCTS: Majority Voting & \textbf{0.883}  & \textbf{0.608} & \textbf{0.905}  & \textbf{0.786} \\
        MCTS: Maximum Reward & 0.776 & 0.469 & 0.854 & 0.685 \\
        \bottomrule % Bottom rule
    \end{tabular}
    \caption{Experiment results on math reasoning dataset GSM8K. The first two rows of the table correspond to the baselines: 1) no search, 2) original MCTSr implementation with ground-truth feedback. The last three rows correspond to our proposed modifications to test the effectiveness of self-feedback in search. Across all models, there is at least $\approx$ 10\% improvement in accuracy using the original MCTSr implementation with ground-truth verification over no search. However, without access to ground-truth feedback, the most promising alternative seems to be majority voting rather than picking the answer with maximum reward given from self-feedback.}
    \label{table:unrealistic}
\end{table*}