\begin{table*}[ht]
% \scriptsize
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lllllllll}
\toprule
\multirow{2}{*}{Method} & \multicolumn{8}{c}{Tasks} \\
\cmidrule{2-9}
& \multicolumn{1}{c}{$\mathcal{T}^1$} & \multicolumn{1}{c}{$\mathcal{T}^2$} & \multicolumn{1}{c}{$\mathcal{T}^3$} & \multicolumn{1}{c}{$\mathcal{T}^4$} & \multicolumn{1}{c}{$\mathcal{T}^5$} & \multicolumn{1}{c}{$\mathcal{T}^6$} & \multicolumn{1}{c}{$\mathcal{T}^7$} & \multicolumn{1}{c}{$\mathcal{T}^8$}  \\ 
\toprule
\multicolumn{9}{c}{\textbf{FewRel} \textit{(10-way--5-shot)}} \\
\midrule
SCKD        & $91.92_{\pm 0.80}$ & $79.37_{\pm 4.83}$ & $75.07_{\pm 3.45}$ & $73.72_{\pm 2.15}$ & $69.11_{\pm 2.02}$ & $68.38_{\pm 2.45}$ & $67.18_{\pm 2.10}$ & $65.04_{\pm 5.76}$ \\ 
ConPL & $\mathbf{94.36_{\pm 0.63}}$ & $\underline{84.61_{\pm 3.31}}$ & $\underline{78.41_{\pm 1.93}}$ & $74.16_{\pm 3.41}$ & $72.37_{\pm 2.48}$ & $\underline{71.83_{\pm 3.51}}$ & $\underline{68.45_{\pm 1.67}}$ & $64.46_{\pm 0.71}$ \\
CPL  & {$92.11_{\pm 0.96}$} & $82.94_{\pm 2.89}$ & ${76.64_{\pm 4.50}}$ & $\underline{74.66_{\pm 6.69}}$ & $\underline{73.08_{\pm 6.40}}$ & $69.89_{\pm 5.55}$ & $68.01_{\pm 3.02}$ & $65.29_{\pm 1.38}$ \\ %[0.0061 0.0294 0.0158 0.0236 0.0218 0.025  0.0093 0.0079]
CPL\_MI & $\underline{93.15_{\pm 0.61}}$ & {$82.20_{\pm 2.94}$} & {$76.53_{\pm 1.58}$} & {$73.52_{\pm 2.36}$} & {$71.79_{\pm 2.18}$} & {$69.17_{\pm 2.50}$} & {$67.18_{\pm 0.93}$} & \underline{$65.34_{\pm 0.79}$} \\
EDC\textsuperscript{*} & $68.88_{\pm 0.02}$ & {$54.46_{\pm 0.97}$} & {$49.05_{\pm 2.11}$} & {$46.45_{\pm 2.31}$} & {$43.89_{\pm 1.82}$} & {$41.92_{\pm 1.39}$} & {$38.91_{\pm 0.04}$} & {$36.81_{\pm 0.12}$} \\%[0.01  0.0191 0.0168 0.016  0.0273 0.0201 0.0144 0.0095]
OFCRE (Ours) & $91.02_{\pm 0.90}$ & $\mathbf{85.36_{\pm 1.91}}$ & $\mathbf{79.83_{\pm 1.68}}$ & $\mathbf{76.46_{\pm 1.60}}$ & $\mathbf{74.69_{\pm 2.73}}$ & $\mathbf{72.08_{\pm 2.01}}$ & $\mathbf{69.60_{\pm 1.44}}$ & $\mathbf{67.62_{\pm 0.95}}${\color{darkgreen}\footnotesize $\uparrow{2.28}$} \\

% \method $^{**}$ & $\mathbf{94.74_{\pm 0.73}}$ & $79.63_{\pm 1.27}$ & $74.54_{\pm 1.13}$ & $71.27_{\pm 0.85}$ & $68.35_{\pm 0.86}$ & $63.86_{\pm 2.03}$ & $64.74_{\pm 1.39}$ & $62.46_{\pm 1.54}$ &32.72 \\
\toprule
\multicolumn{9}{c}{\textbf{TACRED} \textit{(5-way-5-shot)}} \\
\midrule
% Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ \\ \hline \hline
% [1.02031562 3.97004675 4.00312666 5.16629385 2.26585978 4.01564612
%  2.28219569 2.08607686]
% [1.02031562 3.97004675 4.00312666 5.16629385 2.26585978 4.01564612
%  2.28219569 2.08607686]
SCKD     & \underline{$86.59_{\pm 1.02}$} & $78.91_{\pm 3.97}$ & $70.63_{\pm 4.00}$ & $63.94_{\pm 5.17}$ & $58.41_{\pm 2.27}$ & $57.58_{\pm 4.02}$ & $51.96_{\pm 2.28}$ & $49.43_{\pm 2.09}$ \\
ConPL & {${85.67_{\pm 3.23}}$} & $80.95_{\pm 4.82}$ & $69.85_{\pm 2.05}$ & $61.18_{\pm 3.29}$ & $59.31_{\pm 1.80}$ & $56.02_{\pm 3.76}$ & $54.93_{\pm 2.57}$ & $51.67_{\pm 3.58}$ \\ 
CPL & $86.38_{\pm 0.94}$ & $\underline{81.21_{\pm 3.23}}$ & $\underline{74.09_{\pm 3.32}}$ & $\underline{69.36_{\pm 6.53}}$ & $\underline{63.48_{\pm 4.40}}$ & $\underline{61.36_{\pm 3.77}}$ & $56.09_{\pm 2.88}$ & $\underline{53.81_{\pm 3.11}}$ \\
CPL\_MI  & $\mathbf{86.69_{\pm 1.12}}$ & {$79.87_{\pm 5.20}$} & {$72.34_{\pm 2.90}$} & {$68.85_{\pm 4.18}$} & {$62.61_{\pm 4.95}$} & {$60.05_{\pm 4.66}$} & \underline{$57.34_{\pm 5.21}$} & {$53.59_{\pm 1.78}$} \\ 
% DCRE (Our) & $87.25_{\pm 0.45}$ & $\mathbf{85.92_{\pm 5.89}}$ & $\mathbf{80.81_{\pm 3.34}}$ & $\mathbf{78.85_{\pm 2.77}}$ & $\mathbf{74.95_{\pm 3.44}}$ & $\mathbf{73.83_{\pm 2.99}}$ & $\mathbf{71.79_{\pm 2.59}}$ & $\mathbf{70.02_{\pm 2.17}}$ & \textbf{17.23} \\
EDC\textsuperscript{*}  & $53.29_{\pm 0.02}$ & {$55.18_{\pm 2.31}$} & {$55.53_{\pm 0.17}$} & {$54.77_{\pm 2.44}$} & {$52.66_{\pm 0.56}$} & {$54.10_{\pm 1.87}$} & {$53.47_{\pm 2.42}$} & {$52.93_{\pm 0.04}$} \\%[0.0039 0.0278 0.0339 0.0391 0.0583 0.0394 0.0287 0.0132]
OFCRE (Ours)& $85.23_{\pm 0.39}$ & $\mathbf{82.39_{\pm 2.78}}$ & $\mathbf{77.64_{\pm 3.39}}$ & $\mathbf{74.67_{\pm 3.91}}$ & $\mathbf{71.08_{\pm 5.83}}$ & $\mathbf{70.79_{\pm 3.94}}$ & $\mathbf{68.91_{\pm 2.87}}$ & $\mathbf{67.8_{\pm 1.32}}${\color{darkgreen}\footnotesize  $\uparrow{13.99}$} \\

\toprule
\end{tabular}
\end{adjustbox}
\caption{F1 score (\%) of methods using BERT backbone after training for each task \textbf{without undetermined relation} in dataset. The best results are in \textbf{bold}, while the second highest scores are \underline{underlined}}
\label{table:main_wo_ur}
\end{table*}

\begin{table*}[ht]
% \scriptsize
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lllllllll}
\toprule
\multirow{2}{*}{Method} & \multicolumn{8}{c}{Tasks} \\
\cmidrule{2-9}
& \multicolumn{1}{c}{$\mathcal{T}^1$} & \multicolumn{1}{c}{$\mathcal{T}^2$} & \multicolumn{1}{c}{$\mathcal{T}^3$} & \multicolumn{1}{c}{$\mathcal{T}^4$} & \multicolumn{1}{c}{$\mathcal{T}^5$} & \multicolumn{1}{c}{$\mathcal{T}^6$} & \multicolumn{1}{c}{$\mathcal{T}^7$} & \multicolumn{1}{c}{$\mathcal{T}^8$}  \\ 
\toprule
\multicolumn{9}{c}{\textbf{FewRel} \textit{(10-way--5-shot)}} \\
\midrule
SCKD      & $62.96_{\pm 0.72}$ & $45.05_{\pm 4.93}$ & $36.11_{\pm 3.42}$ & $31.14_{\pm 1.81}$ & $25.76_{\pm 2.04}$ & $25.03_{\pm 2.47}$ & $24.63_{\pm 2.09}$ & $22.99_{\pm 5.60}$ \\ 
ConPL & ${58.28_{\pm 1.06}}$ & $34.65_{\pm 4.37}$ & $32.24_{\pm 2.45}$ & $29.48_{\pm 2.18}$ & $28.66_{\pm 1.70}$ & $28.23_{\pm 3.84}$ & $26.85_{\pm 3.79}$ & $24.49_{\pm 4.68}$ \\
CPL & {$57.00_{\pm 1.22}$} & $28.77_{\pm 2.55}$ & $22.63_{\pm 1.84}$ & $18.86_{\pm 5.17}$ & $15.82_{\pm 5.22}$ & $15.19_{\pm 3.84}$ & $13.44_{\pm 1.23}$ & $13.26_{\pm 1.30}$ \\
CPL\_MI & $58.28_{\pm 1.33}$ & {$34.65_{\pm 3.18}$} & {$32.24_{\pm 3.09}$} & {$29.48_{\pm 3.03}$} & {$28.66_{\pm 3.29}$} & {$28.23_{\pm 3.63}$} & {$26.85_{\pm 1.88}$} & {$24.49_{\pm 1.63}$} \\
EDC\textsuperscript{*} & $37.44_{\pm 0.08}$ & {$31.40_{\pm 1.87}$} & {$28.98_{\pm 1.10}$} & {$26.58_{\pm 2.24}$} & {$24.99_{\pm 1.76}$} & {$24.28_{\pm 1.59}$} & {$22.00_{\pm 0.61}$} & {$20.65_{\pm 0.03}$} \\
OFCRE (Ours) & $\underline{64.98_{\pm 1.31}}$ & $\underline{51.80_{\pm 3.72}}$ & $\underline{46.64_{\pm 2.32}}$ & $\underline{45.11_{\pm 2.34}}$ & $\underline{43.06_{\pm 2.68}}$ & $\underline{40.44_{\pm 1.33}}$ & $\underline{38.92_{\pm 0.84}}$ & $\underline{37.06_{\pm 0.42}}$ \\
%[0.0053 0.0358 0.0284 0.0289 0.0268 0.0134 0.0088 0.0087]
OFCRE + OIE (Ours) & $\mathbf{69.23_{\pm 0.53}}$ & $\mathbf{58.23_{\pm 3.58}}$ & $\mathbf{52.94_{\pm 2.84}}$ & $\mathbf{51.17_{\pm 2.89}}$ & $\mathbf{49.31_{\pm 2.68}}$ & $\mathbf{46.57_{\pm 1.34}}$ & $\mathbf{45.00_{\pm 0.88}}$ & $\mathbf{43.11_{\pm 0.87}}${\color{darkgreen}\footnotesize $\uparrow{6.05}$} \\

% \method $^{**}$ & $\mathbf{94.74_{\pm 0.73}}$ & $79.63_{\pm 1.27}$ & $74.54_{\pm 1.13}$ & $71.27_{\pm 0.85}$ & $68.35_{\pm 0.86}$ & $63.86_{\pm 2.03}$ & $64.74_{\pm 1.39}$ & $62.46_{\pm 1.54}$ &32.72 \\
\toprule
\multicolumn{9}{c}{\textbf{TACRED} \textit{(5-way-5-shot)}} \\
\midrule
% Method & $\mathcal{T}^1$ & $\mathcal{T}^2$ & $\mathcal{T}^3$ & $\mathcal{T}^4$ & $\mathcal{T}^5$ & $\mathcal{T}^6$ & $\mathcal{T}^7$ & $\mathcal{T}^8$ \\ \hline \hline
SCKD       & {$59.92_{\pm 2.89}$} & $45.93_{\pm 2.29}$ & $27.84_{\pm 4.03}$ & $22.33_{\pm 5.19}$ & $20.74_{\pm 2.09}$ & $17.20_{\pm 3.94}$ & $15.71_{\pm 2.27}$ & $14.76_{\pm 2.04}$ \\
ConPL & {${56.18_{\pm 1.58}}$} & $27.47{\pm 3.61}$ & $25.37_{\pm 2.30}$ & $21.03_{\pm 3.22}$ & $16.82_{\pm 2.08}$ & $17.08_{\pm 3.89}$ & $16.55_{\pm 3.40}$ & $15.38_{\pm 4.34}$ \\ 
CPL & {$57.00_{\pm 0.83}$} & $28.77_{\pm 2.33}$ & $22.63_{\pm 2.10}$ & $18.86_{\pm 5.15}$ & $15.82_{\pm 4.88}$ & $15.19_{\pm 3.51}$ & $13.44_{\pm }1.44$ & $13.26_{\pm }1.76$ \\ 
CPL\_MI & $48.49_{\pm 3.71}$ & {$28.20_{\pm 3.85}$} & {$20.60_{\pm 2.61}$} & {$17.82_{\pm 1.40}$} & {$16.49_{\pm 1.92}$} & {$14.92_{\pm 1.20}$} & {$14.00_{\pm 1.34}$} & {$13.22_{\pm 0.73}$} \\
EDC\textsuperscript{*} & $32.75_{\pm 0.01}$ & {$33.36_{\pm 1.01}$} & {$34.74_{\pm 2.21}$} & {$33.17_{\pm 4.24}$} & {$31.25_{\pm 2.53}$} & {$33.30_{\pm 2.73}$} & {$32.35_{\pm 2.13}$} & {$31.34_{\pm 0.04}$} \\

OFCRE (Ours)& $\underline{65.99_{\pm 0.99}}$ & $\underline{53.08_{\pm 1.71}}$ & $\underline{45.52_{\pm 0.11}}$ & $\underline{41.99_{\pm 5.31}}$ & $\underline{37.79_{\pm 5.64}}$ & $\underline{35.73_{\pm 3.03}}$ & $\underline{33.20_{\pm 2.29}}$ & $\underline{32.15_{\pm 1.48}}$ \\
%[0.0081 0.0159 0.0085 0.0582 0.0538 0.0331 0.0232 0.0158]
OFCRE + OIE (Ours)& $\mathbf{67.51_{\pm 0.81}}$ & $\mathbf{59.14_{\pm 1.59}}$ & $\mathbf{52.23_{\pm 0.85}}$ & $\mathbf{48.85_{\pm 5.82}}$ & $\mathbf{43.26_{\pm 5.38}}$ & $\mathbf{41.28_{\pm 3.31}}$ & $\mathbf{38.87_{\pm 2.32}}$ & $\mathbf{37.79_{\pm 1.58}}${\color{darkgreen}\footnotesize  $\uparrow{5.64}$} \\
\toprule
\end{tabular}
\end{adjustbox}
\caption{F1 score (\%) of methods using BERT backbone after training for each task \textbf{with undetermined relation} in dataset. OFCRE + OIE is a test version that utilizes OIE to filter out samples with no relations before passing them to OFCRE. The best results are in \textbf{bold}, while the second highest scores are \underline{underlined}}
\label{table:main_ur}
\end{table*}



\begin{table}[ht]
    \centering
     \resizebox{\columnwidth}{!}{%
    \setlength{\tabcolsep}{1mm}
    \begin{tabular}{lcccc}
    \hline 
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{w/o UR} & \multicolumn{2}{c}{UR}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
    & FewRel & TACRED & FewRel & TACRED \\
    \hline \hline \\
    \text{OFCRE (Ours)} & \textbf{67.62} & \textbf{67.80} & \textbf{43.11} & \textbf{37.79}\\

    \quad  \text{w/o $\mathcal{L}_{HSMT}$} & 65.16 &  67.10 & 40.50 & 35.80\\
    
    \quad  \text{w/o $\mathcal{L}_{WMI_{SD}}$} & 63.75 & \underline{67.35} & \underline{42.26} & \underline{36.79}\\
    
    \quad  \text{w/o $\mathcal{L}_{WMI_{SC}}$} & \underline{66.95} & 66.40 & 41.92 & 36.35\\
    \hline
    \end{tabular}
    }
    \caption{Ablation study of loss at task $\mathcal{T}^8$}
    \label{tab:ablation_study}
\end{table}

\section{Experiments}
\subsection{Experiment Setup}

% \textcolor{red}{[Can be move to Appendix to have more rooms]}
We conduct experiments using the pre-trained language model BERT \citep{devlin-etal-2019-bert} as the backbone for the Few-shot Continual Relation Matching module. We then evaluate our approach against baselines on two widely used benchmarks in the literature of CRE and FCRE: FewRel \citep{han-etal-2018-fewrel} and TACRED \citep{zhang-etal-2017-position}. These datasets are added with numerous samples containing undetermined relations (UR), treated as a new relation type with a corresponding description (see Appendix \ref{app:data}).  

After completing each task, we evaluate the models on the updated $\mathcal{D}^{test}$ using six random seeds and report the mean and standard deviation of accuracy. We benchmark our approach against state-of-the-art baselines under similar settings, including \textbf{SCKD} \citep{wang-etal-2023-serial}, \textbf{ConPL} \citep{DBLP:conf/acl/ChenWS23}, \textbf{CPL} \citep{ma-etal-2024-making}, \textbf{CPL+MI} \citep{tran-etal-2024-preserving}, and \textbf{EDC\textsuperscript{*}}\footnote{EDC\textsuperscript{*} is a modified version of EDC in which the phase OIE prompt has been adjusted to accept a list of entities from our dataset as additional input.} \citep{zhang-soh-2024-extract}.

Since the presence of numerous undetermined relations affects overall relation extraction performance, we evaluate using the \textbf{F1 score} \citep{nguyen2015relation} for determined relations.
\subsection{Experiment results}
 \label{exp_method}
% \paragraph{Our proposed method yields state-of-the-art accuracy.}
% Table \ref{table:main} presents the results of our method and the baselines, all using the same pre-trained BERT-based backbone. Our method consistently outperforms all baselines across the board. The performance gap between our method and the strongest baseline, CPL, reaches up to $3.74\%$ on FewRel and $5.82\%$ on TACRED. 
% % After completing eight tasks, our method reduces the accuracy drop by $3.48\%$ on FewRel and $5.89\%$ on TACRED.

\paragraph{Superior Performance Across FCRE Scenarios} 
Our model consistently achieves superior results across all scenarios, regardless of the inclusion of the UR labels. Particularly, when trained with UR labels and evaluated only on DR labels (Table \ref{table:main_wo_ur}), it exhibits the lowest forgetting rates—23.4\% on Fewrel and 17.42\% on TACRED —while maintaining superior performance across tasks up to the final task, $\mathcal{T}^8$. This highlights its ability to retain knowledge of seen relations. Additionally, as shown in Table \ref{table:main_ur}, our model excels at correctly identifying relation types even in the presence of numerous undetermined relations (URs). It significantly outperforms baselines, which fail when trained and tested solely on UR labels. In the final task, our model surpasses the weakest baseline by 29.85\% on Fewrel and 24.57\% on TACRED. This demonstrates its robustness in leveraging Open Information Extraction descriptions and original description alignment, rather than depending solely on sample-hidden representations.
% \paragraph{Compared to other methods, our model achieves high results in all scenarios, whether or not the UR label is included.} When evaluated with only the original data DR labels after training with the UR labels (Table \ref{table:main_wo_ur}), the model exhibits the lowest forgetting rates, with only 23.4\% on Fewrel and 17.42\% on Tacred. Overall, the performance across each task is consistently higher than other baselines until the final task, $\mathcal{T}^8$. This indicates that our model retains the ability to predict seen relations effectively. Moreover, as shown in Table \ref{table:main_ur}, our method excels at detecting the correct relation types even in cases where many undetermined relations (UR) need to be identified. Despite the large number of entities and corresponding samples with UR, our model still outperforms others in identifying both seen relations and correctly predicting the URs to a notable degree. This performance is significantly better than the baselines, all of which fail when trained and tested exclusively with the UR label. Our method outperforms the weakest baseline by 29.85\% on Fewrel and 24.57\% on Tacred in the final task, demonstrating the robustness of the model's retained knowledge when using descriptions from Open Information Extraction and raw description alignment, rather than relying solely on sample-hidden representations.

% \paragraph{Potential for schema retrieval and similarity search to support EDC in knowledge graph construction.} 
% EDC is inherently designed for optimized triplet extraction and schema (relation) canonicalization in knowledge graphs. However, it is still applicable for detecting learned relations after the phase 3 target schema canonicalization. When evaluated using only the DR label (Table \ref{table:main_wo_ur}), its performance is worse compared to using the UR label (Table \ref{table:main_ur}), similar to other methods. In the final task with UR labels, EDC performs 16.41\% lower than our approach on FewRel and 1.11\% lower on Tacred. 

% \paragraph{Our model demonstrates potential as a schema retriever and for schema similarity search to support EDC in knowledge graph construction.} EDC is essential for optimized triplet extraction and canonicalizing schemas (relations) for knowledge graphs. However, it can still be applied to detect learned relations after Phase 3 of the target schema canonicalization. When evaluated using only the DR label (Table \ref{table:main_wo_ur}), EDC performs worse than when the UR label is included (Table \ref{table:main_ur}), similar to other methods. In the final task with UR, our model outperforms EDC by 16.41\% on Fewrel and 1.11\% on Tacred. Notably, on Fewrel, the number of relations increases after each task, unlike Tacred. EDC struggles with catastrophic forgetting due to the lack of suitable matching relations and its reliance solely on a pretrained model for description search. To address this, we introduce an OFCRE version that uses Open Information Extraction (OIE) as a filter to validate potential "no relation" samples, only passing relevant samples to our model for prediction. This approach mirrors Phase 1 (extraction) and Phase 3 (relation matching and canonicalization) in EDC, but with the use of a single LLM phase, resulting in significantly improved matching and state-of-the-art results of 43.11\% on Fewrel and 37.79\% on Tacred with UR.


% The analysis of augmented description quantity (\( K \)) reveals that \( K = 5 \) optimally balances diversity and noise reduction, achieving peak accuracy (e.g., \SI{74.67}{\%} on \textbf{TACRED} $\mathcal{T}^4$). Smaller \( K \) (\( \leq 3 \)) limits contextual diversity, while larger \( K \) (\( =7 \)) introduces noise, degrading performance by \(\sim\)2--3\%. Dataset-specific trends show \textbf{TACRED} benefits more from higher \( K \) due to noisy text. Augmented descriptions improve undetermined relation (UR) detection by expanding latent representations (e.g., \SI{12.4}{\%} gain over \( K = 1 \)), though computational costs and semantic redundancy remain challenges.



% \paragraph{Schema Retrieval and Similarity Search for EDC} 
\paragraph{Our model shows strong potential to support Knowledge Graph Construction in the setting of FCRE} 
% EDC plays a crucial role in optimizing triplet extraction and canonicalizing schemas but remains applicable for detecting learned relations after Phase 3 of schema canonicalization.

When evaluated with only the DR label (Table \ref{table:main_wo_ur}), the latest SOTA EDC performs worse than when UR labels are included (Table \ref{table:main_ur}), following the trend of other methods. In the final task with UR, our model surpasses EDC by 16.41\% on Fewrel and 1.11\% on TACRED. Notably, on Fewrel, where the number of relations increases with each task, EDC struggles with catastrophic forgetting due to its reliance on pretrained models for description search and the lack of suitable matching relations.

In the setting with 
To address this, we introduce an OFCRE variant that leverages Open Information Extraction (OIE) as a filter, ensuring only relevant samples are passed for prediction. This approach mirrors EDC’s extraction and canonicalization phases but consolidates them into a single LLM phase, leading to significantly improved matching and state-of-the-art results: 43.11\% on Fewrel and 37.79\% on Tacred with UR.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{imgs/f1_score_plot.pdf}
    \caption{Results with each K augmented description on Tacred with Undetermined Relation}
    \vspace{-2mm}
    \label{fig:kaug1}
\end{figure}


\paragraph{The effects of using augmented descriptions}
Figure \ref{fig:kaug1} shows that \( K = 5 \) optimally balances diversity and noise, achieving peak accuracy (\SI{74.67}{\%} on \textbf{TACRED} $\mathcal{T}^4$). Smaller \( K \) (\( \leq 3 \)) limits contextual diversity, while larger \( K \) (\( =7 \)) introduces noise, reducing performance by \(\sim\)2--3\%. Higher \( K \) benefits \textbf{TACRED} due to noisy text. Augmented descriptions enhance undetermined relation (UR) detection (\SI{12.4}{\%} gain over \( K = 1 \)), though computational cost and redundancy remain challenges.

\paragraph{Ablation Study}
In Table \ref{tab:ablation_study}, we present the performance variations when removing the core losses $\mathcal{L}_{HSMT}$, $\mathcal{L}_{WMI_{SD}}$, and $\mathcal{L}_{WMI_{SC}}$. The results demonstrate the contribution of each loss function to the overall performance. Notably, incorporating both $\mathcal{L}_{WMI_{SD}}$ and $\mathcal{L}_{WMI_{SC}}$ losses, rather than using only one, improves performance by up to 4\%. This emphasizes the significance of both the original and candidate descriptions, even when numerous undetermined relations need to be extracted.

