\section{Proposed Method}

% Training procedure
% Figure \ref{fig:overview}: 
%linhlinh

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/flow_6.pdf}
    \caption{\textbf{{Our general framework:}} \textbf{\textit{(1) Open Dataset Construction}}, which creates training and testing datasets that account for undetermined relationships; \textbf{\textit{(2) Open Information Extraction}}, which is used to prepare candidate triplet (i.e., determined relationships of entity pairs); and  \textbf{\textit{(3) Training Open Few-shot Continual Relation Extraction (OFCRE)}} module, which aligns embeddings of sentences, original descriptions and candidate descriptions.}
    \label{fig:overview}
\end{figure*}


% In this study, we aim to extract all possible relations, including \textit{undetermined relations}, which encompass both \textit{no relation} and \textit{unknown relations} that have not been observed in prior tasks. In real-world scenarios, such relations often exist among multiple entities within the same sample. To create a more realistic training and testing environment, we construct these relations based on all possible entity pairs present in a given sample. 

% To achieve this, we first extract all entities appearing in a sample using an entity extraction method, which could be a Large Language Model (LLM) or Named Entity Recognition (NER). However, since extracted entities may not always perfectly match the annotated entities in benchmark datasets, we apply a merging and deduplication process. Specifically, if two entities $e_i$ and $e_j$ share at least one common word, i.e.,

% \begin{equation}
%     |\text{set}(w(e_i)) \cap \text{set}(w(e_j))| > 0,
% \end{equation}

% then they are considered a match, and one of them is removed to eliminate redundancy.

% After processing, we obtain a refined list of entities for each sample. If a pair of entities exists in the benchmark dataset with a predefined relation type, it is assigned that relation. Otherwise, all remaining entity pairs are labeled as having an \textit{undetermined relation}, meaning they either have no meaningful relation or belong to an unknown relation category.
In real-world applications, Relation Extraction aims to identify relationships between all possible pairs of entities within documents. However, a significant challenge arises from the presence of \textit{Undetermined Relations} (UR) between entities, which is either \textit{not applicable} or \textit{unknown} as Appendix \ref{app:example}. Particularly, UR can be categorized into two types as follows:

\begin{itemize}
    \item \textbf{No Relation (NA)}: Used when no meaningful relationship exists between entities. 
    % serving as a default category when no predefined relation applies.
    \item \textbf{None Of The Above (NOTA)}: Used when an entity pair does not fit any predefined relations.
\end{itemize}

Related to this problem, previous studies \citep{zhao-etal-2025-dynamic} primarily focus on NOTA relations and evaluate models using a simple threshold on the test set including unseen relations. However, this approach does not reflect real-world scenarios as they still only considered a predefined set of entity pairs and corresponding relations when training. 

This section will present our novel approach to dealing with this problem, going from extracting all possible entities to create an open dataset (Section \ref{sec:dataset}), to how OIE is utilized to support FCRE with undetermined relations (Section \ref{sec:oie}), and finally our training and inference procedures in Section \ref{sec:fcre}.  

\subsection{Open Dataset Construction}
\label{sec:dataset}

This is the first stage to extract all possible entities in a sentence for the training phase. Particularly, we employ a Named Entity Recognition (NER) model as Figure \ref{fig:overview}. However, extracted entities may not perfectly align with the original dataset annotations, thus we merge the extracted entities with overlapped ones in the benchmark dataset to ensure the consistency. 

Beyond this step, we assign labels to all possible entity pairs. If an extracted entity pair matches a predefined relation in the benchmark dataset, it is categorized as a \textit{determined relation} \textbf{(DR)}; otherwise, it is classified as an \textit{undetermined relation} \textbf{(UR)}. This approach results in a more comprehensive and realistic dataset, incorporating both original relations and \textit{undetermined relations} as newly labeled instances with descriptions. Each extracted entity pair with sample from the merged list is treated as an independent instance, rather than just a sample with the original entity pair, serving as input for the relation extraction task. Consequently, the dataset size significantly increases due to the large number of \textit{undetermined relations}, making it more reflective of real-world scenarios.
% In addition, we need to assign labels for all possible pairs of entities. If an extracted entity pair matches a predefined relation in the benchmark dataset, it is labeled as a \textit{determined relation} \textbf{(DR)}. Otherwise, it is classified as an \textit{undetermined relation} \textbf{(UR)}. As a result, we construct a more comprehensive and realistic dataset that includes all possible entity relations, categorized into two main types: \textbf{determined relation samples} and \textbf{undetermined relation samples}.
 

% To ensure distinct entities and prevent duplication, we define the entity inclusion function:

% \begin{equation}
%     \text{in}(e) =
%     \begin{cases} 
%         \text{true}, & \quad \text{if } e = e' \text{ or } e \subseteq e' \text{ or } e' \subseteq e, \\
%         & \quad \forall e' \in E_{\text{extracted}}, \\ 
%         \text{false}, & \quad \text{otherwise}.
%     \end{cases}
% \end{equation}

% where $E_{\text{extracted}}$ represents the set of entities identified by the NER model.

% As a result, we construct a more comprehensive and realistic dataset that includes all possible entity relations, categorized into two main types: \textbf{determined relation samples} and \textbf{undetermined relation samples}.

\subsection{Open Information Extraction}
\label{sec:oie}
% The \textit{Open Relation Extraction} (ORE) component 
% This step leverages the power of Open Information Extraction (OIE)

% This step aims at extracting relations between entities without requiring predefined specifications. 

Unlike existing FCRE methods, this module in Fig.\ref{fig:overview} aims to identify unseen relations, thereby expanding the scope of knowledge extraction for more efficient training. In particular, we employ the OIE module of EDC to extract relations between entities without any predefined label set. To this end, we employ ChatGPT-4o-mini to generate candidate triplets that contain relations and follow a structured prompting approach, as illustrated in Fig.\ref{tab:relation_description_prompt1}.
% This also helps reduce the number of entity pairs classified as having no existing relation, effectively shrinking the space of undetermined relations including only known and unknown relations. 
% This refinement improves the performance of the subsequent \textit{Few-Shot Continual Relation Extraction} (FCRE) module.
% In terms of its primary function in the training process, OIE not only identifies triplets but also 


% These generated descriptions are then augmented to enrich relation representations.


\subsection{FCRE via OIE (OFCRE)} 
\label{sec:fcre}
This section in Fig.\ref{fig:overview} presents our training and testing process. Paticularly, we demonstrate how expanding the relation set with UR aids efficient training and enables the model to handle unseen labels.

\subsubsection{Training phase}

\paragraph{Data Augmentation}
\label{sec:augment}

Overall, relation descriptions from both training datasets and LLM generation are typically concise, generic, and applicable to multiple samples \citep{han-etal-2018-fewrel, zhang-etal-2017-position}. However, relying solely on these limited descriptions can constrain model performance, motivating us to enhance them with greater diversity.
\begin{itemize}
    \item For each original description, we augment it with $K$ additional samples. Each sample includes an example sentence closely related to the target relation, thereby improving the alignment between the embeddings of the relation and the corresponding diverse descriptions.

 \item Similarly, to utilize the candidate triplet produced by the OIE module, an additional prompt is crafted to deliver $K$ distinct \textit{candidate relation descriptions} with examples. This aids in examine the surrounding context to formulate a \textit{candidate description} tailored to the identified relation. These context-sensitive descriptions serve as enhanced refinements of the original ones, offering more accurate and detailed representations.
\end{itemize}

% \textcolor{red}{[Move description (Table 10, 11..) here]}

 These enriched descriptions contribute to better model generalization. Note that description augmentation is applied only to seen relation types, not to undetermined relations. Further details on both types of descriptions can be found in Appendix \ref{app:prompt}.

% To enhance the language model's ability to represent input samples with corresponding entities, we augment relation descriptions to maintain diversity and improve hidden representations of relation types, as detailed in Section \ref{sec:augment}. Subsequently, we apply the training loss techniques described in Section \ref{sec:loss}, which include a hard-soft margin triplet loss to distinguish semantic differences between samples and a weighted mutual information loss to maximize mutual information between descriptions and samples. This approach enhances the model's robustness in classifying relation types, even in the presence of numerous undetermined relations during training and testing procedure in Section \ref{sec:procedure}
% This component serves as the \textbf{central mechanism for extracting relations???}, particularly in scenarios involving numerous \textit{undetermined relations}. While the \textit{Open Relation Extraction} (ORE) module supports this process, it can also be independently trained  using the pre-trained language model (PLM) to identify real-world relations. 

% In the case where \textit{Open Relation Extraction} (ORE) is utilized for inference, ORE first filters out entity pairs that have no relation, directly assigning them to the \textit{Undetermined Relation} (UR) label. Otherwise, all remaining entities and samples extracted by ORE, standard matching is performed using the pre-trained language model (PLM).

% The learned representation of a sample and its corresponding description is optimized using the following loss function in Figure \ref{fig:framework}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/bert.png}
%     \caption{Our Fewshot Continual Relation Matching}
%     \label{fig:framework}
% \end{figure}
% <Adding Reason for 3 sections below>


% \textbf{[I suppose we should create a subsection for data augmentation and another one for objective functions. the current organization not very efficient @@]}
\paragraph{Objective Functions}
\label{sec:loss}
\paragraph{\textit{Hard Soft Margin Loss (HSMT)}} 
% Building upon prior work in Few-Shot Continual Relation Extraction (FCRE) \citep{DBLP:conf/acl/WangWH23, hu-etal-2022-improving, DBLP:conf/coling/MaHL024, tran-etal-2024-preserving}, which improves performance using contrastive loss to directly optimize the latent space for an optimal balance between adaptability and discrimination, 
% we adopt the \textit{Hard Soft Margin Triplet Loss} (HSMT) as a suitable loss function.
To enhance the distinction between different relations, HSMT integrates both hard and soft margin triplet loss principles \citep{hermans2017defense}, dealing with the most challenging positive and negative samples while maintaining flexibility through a soft margin. Formally, the loss function is defined as:

\begin{multline}
\mathcal{L}_{\textrm{HSMT}}({x}) = 
- \log \bigg(1 + \max_{{p} \in P(\bm{x})} e^{\xi(\bm{z}_x, \bm{z}_p)} \\
- \min_{{n} \in N({x})} e^{\xi(\bm{z}_x, \bm{z}_n)}
\bigg),
\end{multline}

where $\xi(\cdot, \cdot)$ denotes the Euclidean distance function. This formulation effectively maximizes the separation between the hardest positive and hardest negative samples while allowing for adaptive margin flexibility, improving representations in the latent space.
% \subsubsection{Hard Margin Loss} 
% The Hard Margin Loss leverages label descriptions to refine the model's ability to distinguish between hard positive and hard negative pairs. Given the output hidden vectors $\{ \bm{d}^k_x \}_{k=1, ..., K}$ from BERT corresponding to the label description of sample ${x}$, and $\bm{z}_p$ and $\bm{z}_n$ representing the hidden vectors of positive and negative samples respectively, the loss function is formulated to maximize the alignment between $ \bm{d}^k_x $ and its corresponding positive sample, while enforcing a strict margin against negative samples. Specifically, the loss is formulated as follows:
% \begin{align}
% \mathcal{L}_{\textrm{HM}}(x) &= \sum_{k=1}^K \mathcal{L}_{\textrm{HM}}^k(x), \\
% \mathcal{L}_{\textrm{HM}}^k(x) &= 
% \sum_{p \in P_\textrm{H}(x)} (1 - \gamma(\bm{d}^k_x, \bm{z}_p))^2 \notag \\
% &+ \sum_{n \in N_\textrm{H}(x)} max(0, m - 1 + \gamma(\bm{d}^k_x, \bm{z}_n) )^2,
% \end{align}
% where $m$ is a margin hyperparameter; $\gamma(\cdot, \cdot)$ denotes the cosine similarity function; $P_\textrm{H}(x)$ and $N_\textrm{H}(x)$ represent the sets of hard positive and hard negative samples, respectively. They are determined by comparing the similarity between $\bm{d}^k_x$ and both positive and negative pairs, specifically focusing on the most challenging pairs where the similarity to negative samples is close to or greater than that of positive samples, defined as follows:
% \begin{align}
% \begin{aligned}
%     P_\textrm{H}(x) = \{&p \in P(x) | 1 - \gamma(\bm{d}^k_x, \bm{z}_p) \\
%     &> min_{n \in N(x)}(1 - \gamma(\bm{d}^k_x, \bm{z}_n)), \forall k \in [K] \},
% \end{aligned} \\
% \begin{aligned}
%     N_\textrm{H}(x) = \{&n \in N(x) | 1 - \gamma(\bm{d}^k_x, \bm{z}_n) \\
%     &< max_{p \in P(x)}(1 - \gamma(\bm{d}^k_x, \bm{z}_p)), \forall k \in [K] \}.
% \end{aligned}
% \end{align}

% % where  $P({x})$ and $N({x})$ denote the sets of positive and negative samples with respect to sample ${x}$, respectively.

% By utilizing the label description vectors $\{ \bm{d}^k_x \}$, optimizing $\mathcal{L}_{\textrm{HM}}(x)$ effectively sharpens the model's decision boundary, reducing the risk of confusion between similar classes and improving overall performance in few-shot learning scenarios. The loss penalizes the model more heavily for misclassifications involving these hard samples, ensuring that the model pays particular attention to the most difficult cases, thereby enhancing its discriminative power.

% \paragraph{Mutual Information Loss}
% The Mutual Information (MI) Loss is designed to maximize the mutual information between the input sample's hidden representation $\bm{z_x}$ of $\bm{x}$ and its corresponding retrieved descriptions, promoting a more informative alignment between them. Let $\bm{d}_n$ be a hidden vector of other label descriptions than $\bm{x}$. According to \citet{DBLP:journals/corr/abs-1807-03748}, the Mutual Information $MI(x)$ between the input embedding $\bm{z}_x$ and its corresponding label description follows the following inequation:
% \begin{equation}
%     MI \geq \log B + \textnormal{InfoNCE} (\{ x_i\}_{i=1}^B; h),
% \end{equation}
% where we have defined:
% \begin{multline}
% \textnormal{InfoNCE} (\{ x_i\}_{i=1}^B; h) = \\
% \frac{1}{B}\sum_{i=1}^B \log \frac{\sum_{k=1}^K h(\bm{z_i}, \bm{d}_i^k)}{\sum_{j=1}^B \sum_{k=1}^K h(\bm{z_j}, \bm{d}_j^k)},
% \end{multline}
% where $h(\bm{z}_j, \bm{d}_j^k) = \exp \left(\frac{\bm{z}_j^TW\bm{d}_j^k}{\tau}\right)$. Here, $\tau$ is the temperature, $B$ is mini-batch size and $W$ is a trainable parameter. However, with alot of UR label in batch, imbalance, we add weight based on number of relation type into loss. Finally, the MI loss function in our implementation is: 
% % \begin{equation}
% %     \mathcal{L}_{MI} = -\sum_{(x_i , y_i) \in D_{train}^{k}}\textnormal{InfoNCE} (\{x_i\}_{i=1}^B; h)
% % \end{equation}
% \begin{multline}
% \mathcal{L}_{\textrm{MI}}(x, d) = \\
% - \log \frac{ \sum_{k=1}^K
%     w(\bm{z}_x) * h(\bm{z}_x, \bm{d}_x^k)
% }{
%     \sum_{k=1}^K
%      w(\bm{z}_x) * h(\bm{z}_x, \bm{d}_x^k)
%     + \sum_{n \in N(x)} \sum_{k=1}^K  w(\bm{z}_x) * h(\bm{z}_x, \bm{d}_n^k)
% }
% \end{multline}
% where  w(\bm{z}_x) = count_unique_labels(z_x)/sum(x in B)count_unique_labels(z_x)
% This loss ensures that the representation of the input sample is strongly associated with its corresponding label, while reducing its association with incorrect labels, thereby enhancing the discriminative power of the model. This loss also applied align mutual information between raw description and candidate description from ORE module, now candidate description hidden have same role as sample hidden representation in loss function. Three loss between sample, raw description and candidate description is optimized to gain stable representation while many Undetermined labels is trained

% L{Des} = L_MI(x, d) + L(x, c) + L(c, d)

% \paragraph{Joint Training Objective Function}
% Finally, total loss to optimzed models is combination of loss align sample-sample and sample-description, with weighted between
% L(x) = L_Samp + L_des = L_HSMT(x) + axdL_MI(x,d) + axcL_MI(x,c) + acdL_MI(c,d)

\paragraph{\textit{Weighted Mutual Information Loss}}
This loss aims to maximize the mutual information between relation embedding $\bm{z}_i$ and its corresponding retrieved description embedding $\bm{d}_i$ of sample $\bm{x}_i$, ensuring a more informative alignment. Following \citet{DBLP:journals/corr/abs-1807-03748}, the mutual information $MI(x)$ between $\bm{z}_i$ and its corresponding label description satisfies:

% \textcolor{red}{[I suppose we just need to mention the final loss here (Eq. 8), and then move the rest to Appendix to save space]}

\begin{equation}
    MI(x) \geq \log B + \textnormal{InfoNCE} (\{ x_i\}_{i=1}^B; h),
\end{equation}

\begin{multline}
\textnormal{where InfoNCE} (\{ x_i\}_{i=1}^B; h) = \\
\frac{1}{B} \sum_{i=1}^B \log \frac{\sum_{k=1}^K h(\bm{z}_i, \bm{d}_i^k)}{\sum_{j=1}^B \sum_{k=1}^K h(\bm{z}_j, \bm{d}_j^k)},
\end{multline}

and

\begin{equation}
h(\bm{z}_j, \bm{d}_j^k) = \exp \left(\frac{\bm{z}_j^T W \bm{d}_j^k}{\tau} \right).
\end{equation}

Here, $\tau$ is the temperature parameter, $B$ is the mini-batch size, and $W$ is a trainable weight matrix. We define $P(x)$ as the set of positive samples (same-label pairs) and $N(x)$ as the set of negative samples (different-label pairs). 

Given the imbalance caused by a high proportion of \textit{Undetermined Relation (UR)} labels, we introduce a weight adjustment based on the number of samples for each unique relation type in the batch:

\begin{equation}
    w_{x} = \frac{B}{\|P(x)\| + \sum_{y \in N(x)} \frac{\|P(x)\|}{\|P(y)\|}}
\end{equation}

The final Weighted MI loss function is formulated as:

\begin{multline}
\mathcal{L}_{\textrm{WMI}}(x, d) = 
- w_{x}\log \frac{ \sum_{k=1}^K
    h(\bm{z}_x, \bm{d}_x^k)
}{Z(x,d)}
\end{multline}

\noindent where
\begin{equation}
Z(x,d) = \sum_{k=1}^K h(\bm{z}_x, \bm{d}_x^k) + \sum_{n \in N(x)} \sum_{k=1}^K h(\bm{z}_x, \bm{d}_n^k)
\end{equation}

This loss is applied not only to the \textit{raw description} but also to the \textit{candidate description} to enhance the learning of sample representations. These types of descriptions serve as stable reference points when learning known relation types within batches containing numerous undetermined relations. We optimize this description loss as follows:
% This loss ensures that an input sample is strongly associated with its correct label while minimizing alignment with incorrect labels, thereby improving the model’s discriminative ability. Additionally, we apply this loss to align the mutual information between \textit{raw descriptions} and \textit{candidate descriptions} generated by the ORE module. In this context, the hidden representation of a candidate description is treated equivalently to a sample representation within the loss function. To maintain stable representations, particularly when training with numerous UR labels, we optimize three alignment losses:

\begin{equation}
\mathcal{L}_{\textrm{Des}} = \mathcal{L}_{\textrm{WMI}}(x, d) + \mathcal{L}_{\textrm{WMI}}(x, c) 
\end{equation}

\paragraph{\textit{Training Objective Function}}
The final optimization objective combines losses that align both \textit{sample-to-sample} and \textit{sample-to-description} representations, incorporating weighted coefficients:

\begin{equation}
\begin{aligned}
\mathcal{L}(x) &= \mathcal{L}_{\textrm{Samp}} + \mathcal{L}_{\textrm{Des}}  \\ 
               &= \alpha_{x} \mathcal{L}_{\textrm{HSMT}}(x) 
               + \alpha_{xd} \mathcal{L}_{\textrm{WMI}}(x,d) \\
               &\quad + \alpha_{xc} \mathcal{L}_{\textrm{WMI}}(x,c)
\end{aligned}
\label{eq:total_loss}
\end{equation}

where $\alpha_{x}$, $\alpha_{xd}$ and $\alpha_{xc}$ are tunable hyperparameters controlling the relative contribution of each loss term.
% \subsubsection{Training and Inference Procedures}
\label{sec:procedure}
\paragraph{Training Procedure:}
Algorithm \ref{alg:training} provides a structured approach for training at each task $\mathcal{T}^j$. Here, $\Phi_{j-1}$ represents the model state after learning from the previous $j-1$ tasks. Following a memory-based continual learning strategy, we maintain a memory buffer $\tilde{M}_{j-1}$, which retains selected representative instances from earlier tasks ${\mathcal{T}^1, \dots, \mathcal{T}^{j-1}}$. Additionally, we keep track of a relation description set $\tilde{E}_{j-1}$ and a candidate description set $\tilde{C}_{j-1}$, which store descriptions of previously encountered relations.

\begin{enumerate}
    \item \textbf{Initialization} (Lines 1--2):  
    The model parameters for the current task, $\Phi_j$, are inherited from $\Phi_{j-1}$. The relation description sets $\tilde{E}_j$ and $\tilde{C}_j$ are then updated by integrating new relation details from $E_j$ and $C_j$, respectively.
    
    \item \textbf{Task-Specific Training} (Line 3):  
    To accommodate new relations introduced in $\mathcal{T}^j$, $\Phi_j$ is trained on $D_j$.
    
    \item \textbf{Memory Management} (Lines 4--8):  
    For each relation $r \in R_j$, we choose $L$ key samples from $D_j$ that are closest to the 1-means centroid of the relation class. These selected samples form memory components $M_r$, contributing to the refined memory set $\tilde{M}_j = \tilde{M}_{j-1} \cup M_j$, alongside the expanded relation set $\tilde{R}_j = \tilde{R}_{j-1} \cup R_j$.

    \item \textbf{Prototype Construction} (Line 9):  
    A prototype set $\tilde{P}_j$ is generated based on the updated memory $\tilde{M}_j$ for inference purposes.

    \item \textbf{Memory-Based Training} (Line 10):  
    The model $\Phi_j$ is further refined by training on the enhanced memory dataset $\tilde{M}_j^*$ to reinforce its ability to retain and recall previously learned relations.
    
    % \item \textbf{Data Augmentation} (Line 10).  
    % Inspired by \textbf{CITE CPL}, to enhance learning under limited data availability, an augmented dataset $D_j^*$ is synthesized using ChatGPT, expanding training diversity.

\end{enumerate}

\begin{algorithm}
\caption{Training procedure at each task $\mathcal{T}^j$}
\label{alg:training}
\begin{algorithmic}[1]
\Require $\Phi_{j-1}$, $\tilde{R}_{j-1}$,  $\tilde{M}_{j-1}$, $\tilde{E}_{j-1}$, $\tilde{C}_{j-1}$, $D_j$, $R_j$, $E_j, C_j$ 
\Ensure $\Phi_j, \tilde{M}_j, \tilde{K}_j, \tilde{P}_j$
\State Initialize $\Phi_j$ from $\Phi_{j-1}$
\State $\tilde{E}_j \gets \tilde{E}_{j-1} \cup E_j$ and $\tilde{C}_j \gets \tilde{C}_{j-1} \cup C_j$ 
\State Update $\Phi_j$ by $\mathcal{L}$ on $D_j$ (train on current task)
\State $\tilde{M}_j \gets \tilde{M}_{j-1}$
\For{each $r \in R_j$}
    \State pick $L$ samples in $D_j$ and add them into $\tilde{M}_j$
\EndFor
\State $\tilde{R}_j \gets \tilde{R}_{j-1} \cup R_j$
\State Update $\tilde{P}_j$ with new data in $D_j$ (for inference)
\State Update $\Phi_j$ by $\mathcal{L}$ on $\tilde{M}_j$ and $D^*_j$ (train on memory)
\end{algorithmic}
\end{algorithm}

\subsubsection{Testing phase} 
To classify relations during inference, we utilize the \textit{Nearest-Class-Mean} (NCM) classifier, as proposed by \citet{ma-etal-2024-making}. Unlike conventional methods that rely solely on label prototypes, we incorporate both label descriptions and prototypes to improve relation prediction.

Given a sample $x$ with hidden representation $\bm{z}_x$, we define a set of relation prototypes $\{\bm{p}_r\}_{r=1}^n$. Each relation prototype is computed as the mean representation of all support samples associated with that relation:

\begin{equation}
\bm{p}_r = \frac{1}{L} \sum_{i=1}^{L}\bm{z}_i,
\end{equation}

where $L$ is the number of support samples contributing to the prototype.

The relation prediction is determined by computing the cosine similarity between $\bm{z}_x$ and each prototype $\bm{p}_r$ as well as the corresponding label description $\bm{d}_r$. The final prediction $y^*$ is selected based on the highest similarity score:

\begin{equation}
    y^* = \arg\max_r \gamma(\bm{z}_x, \bm{p}_r)
    \label{eq:primary}
\end{equation}

% or

% \begin{equation}
%     y^* = \arg\max_r \gamma(\bm{z}_x, \bm{d}_r)
%     \label{eq:alternative}
% \end{equation}

where $\gamma(\cdot, \cdot)$ represents the cosine similarity function. \\
When testing without UR samples to compare with prior work in standard scenarios using only learned relation types, our augmented descriptions effectively represent known relations. However, the defined description for the undetermined relation may not accurately reflect its true labels. Therefore, we use the average description hidden representation instead of a prototype in Equation \ref{eq:primary}.

We also examine the use of Open Information Extraction (OIE) for inference. Specifically, OIE first eliminates entity pairs with no identifiable relationship (No relation - NA), assigning them to the Undetermined Relation (UR) label. For candidate triplets that pass this filtering—categorized as NOTA (None of the Above) or DR (Determined Relation)—a trained language model performs standard matching to classify them into a known relation type or UR. The function utilizing OIE is defined as follows (\ref{fig:infer}):

\begin{align}
    \mathcal{F}(e_i, e_j, x) &= 
    \begin{cases} 
        \text{NA}, \text{if } \text{OIE}(e_i, e_j, x) \rightarrow \text{null} \\
        y^* \text{ (i.e., NOTA or DR)}
    \end{cases}
\end{align}
% \paragraph{Inference Procedure:} Leveraging the discriminative feature distribution learned during training, we adopt the Nearest-Class-Mean classifier, as employed by \citet{ma-etal-2024-making}, for relation prediction in the test phase. However, instead of relying solely on the label prototype, we incorporate both the label description and prototype to extract the relation.




% Given a sample $x$ with hidden representation $\bm{z_x}$, a set of relation prototypes $\{\bm{p}_r\}_{r=1}^n$ and a set of relation descriptions $\{\bm{d}_r\}_{r=1}^n$.

% \vspace{-0.3cm}
% \begin{align}
% % &\textbf{E}(x,r) = -{\left\| \bm{z} - \bm{p}_r \right\|}_2,\\
% &\bm{p}_r = \frac{1}{L} \sum_{i=1}^{L}\bm{z}_i,
% \end{align}


% The inference process begins by calculating the Cosine similarity between $\bm{z_x}$ and each prototype $\bm{p}_r$ and label description $\bm{d}_r$. The final prediction $y^*$  can be determined by description or sample, detail in Appendix A
% \vspace{-0.1cm}
% \begin{equation}
% y^* = \gamma(\bm{z}_x, \bm{p}_r)
% or \\
% y^* =  \gamma(\bm{z}_x, \bm{d}_r)
% \end{equation}
% where $\gamma(\cdot, \cdot)$ denotes the cosine similarity function.
