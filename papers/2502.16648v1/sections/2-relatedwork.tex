\section{Related Work and Background}
\subsection{Related Work}
% Few-shot continual relation extraction (FCRE) is a specialized area of relation extraction that focuses on identifying semantic relationships between entity pairs in sentences while addressing the challenge of continuously learning new relations from limited data. A key challenge in FCRE is avoiding \textit{catastrophic forgetting} of previously learned knowledge \citep{THRUN199525, DBLP:journals/neco/FrenchC02} and \textit{overfitting} \citep{hawkins2004problem} as training on limited dataset. 
Most existing FCRE methods \citep{DBLP:conf/acl/WangWH23, hu-etal-2022-improving, DBLP:conf/coling/MaHL024, tran-etal-2024-preserving} have utilized contrastive learning and memory replay techniques to significantly mitigate catastrophic forgetting. However, these approaches largely overlook the present of undetermined relations — relations that are unseen or nonexistent, which remains a critical gap in real-world applications. On the other hand, several methods \citep{WANG2023151, zhao-etal-2025-dynamic, zhao-etal-2023-open, meng-etal-2023-rapl} have considered unknown labels, but their training only relies on available information, including provided entities and relations from the training set, and poorly considers a NOTA (None Of The Above) label for all possible relations that are uncovered. 

Historically, relation extraction research has explored various types of undetermined relations. For example, prior work has defined “no relation (NA)” \citep{xie-etal-2021-revisiting} as sentences with no meaningful relationship between entities, “out-of-scope (OOS)” \citep{liu-etal-2023-novel} as relations outside predefined sets, and “none of the above (NOTA)” \citep{zhao-etal-2023-open} as relations that do not match any known type. While these studies address specific aspects of undetermined relations, their approaches are often simplistic and unrealistic, focusing on single labeled entity pairs rather than considering multiple possible relations within sentences.

Moreover, Open Information Extraction (OIE) has emerged as a powerful tool for open entity and relation extraction, particularly for knowledge graph construction, due to its ability to operate without predefined schemas. Recent studies \citep{li2023evaluating} highlight the strong performance of large language models (LLMs) in OIE tasks. For instance, EDC \citep{zhang-soh-2024-extract} propose an end-to-end pipeline that extracts, defines, and canonicalizes triplets to build knowledge graphs more efficiently. This pipeline includes three phases: (1) Open Information Extraction, where entity-relation triplets are freely extracted from text; (2) Schema Definition, where entity and relation types are defined based on extracted triplets; and (3) Schema Canonicalization, which standardizes relations to fit a target schema. This approach is particularly promising for handling undetermined relations, as it enables the extraction of relations beyond predefined sets.
% Few-shot continual relation extraction is a branch of relation extraction that not only aims to extract semantic relationships between pairs of entities in a sentence but also face a challenge setting that has continuously capture semantic information of new emerging relations from \textit{a small and limited amount data}, while avoiding forgetting knowledge of previously learned ones \textit{catastrophic forgetting} \citep{THRUN199525, DBLP:journals/neco/FrenchC02} and \textit{overfitting} of FCRE models. Recent advancements in few-shot continual relation extraction (FCRE) \citep{DBLP:conf/acl/WangWH23, hu-etal-2022-improving, DBLP:conf/coling/MaHL024, tran-etal-2024-preserving} that utilze constrative learning for presenting protype and memmory replay, that gaim significantly mproved the mitigation of catastrophic forgetting. While these methods contribute to improving continual relation extraction, they largely overlook the challenge of extracting undetermined relations, which remains a crucial gap in real-world applications where numerous relations remain unseen or unlearned. 

% Additionally, look back the history of relation extraction many work already research on handling relation extraction. They defined multiple type of \textbf{undetermined relation}. For instance, prior studies define “no relation (NA)” \cite{xie-etal-2021-revisiting} as sentences that contain no meaningful relation between entities (CITE), “out-of-scope (OOS)” \citep{liu-etal-2023-novel}, “none of the above (NOTA)”\citep{zhao-etal-2023-open} for relations that fall outside the predefined set ,  do not match any known relation type, and   for relations that (CITE). However, these approaches primarily focus on one aspects of \textbf{undetermined relations} is NA or NOTA. They also construct and present method for present these relation is to naive and not realistic, have limit quantity, used only one labeled pair of entities, where we should consider many relation from possible entities in sentences. 

% Besides, Open Information Extraction (OIE) has gained significant attention in entities, relation extraction then knowledge graph construction, due to its ability to leverage large language models (LLMs) without requiring a predefined schema or relation set. Recent studies \citep{li2023evaluating} have demonstrated that LLMs achieve strong performance in OIE tasks, with \citet{zhang-soh-2024-extract} proposing an end-to-end pipeline that extracts, defines, and canonicalizes triplets to construct knowledge graphs more efficiently and with reduced redundancy. This pipeline typically consists of three phases: Open Information Extraction, where entity-relation triplets are extracted freely from text; Schema Definition, where definitions for entity and relation types are generated based on extracted triplets; and Schema Canonicalization, which standardizes relation to relation in given target schema. This approach presents a promising direction for extracting relations beyond predefined schemas, which is particularly relevant for handling undetermined relations in continual relation extraction. By integrating OIE techniques, we can potentially improve FCRE by recognizing triplets that contain relations and give relation that capture semantic align to original sample. Therefore, we consider OIE as a valuable component in our work, both for training data creation and for enhancing relation extraction in scenarios where a large number of undetermined relations emerge dynamically.

% have significantly improved the mitigation of catastrophic forgetting. SCKD  employs a systematic knowledge distillation strategy to preserve prior knowledge while utilizing contrastive learning with pseudo samples to enhance relation differentiation. ConPL integrates a prototype-based classification module, memory-enhanced learning, and distribution-consistent learning to mitigate forgetting, further leveraging prompt learning and focal loss to improve representation learning and reduce class confusion. CPLintroduces a Contrastive Prompt Learning framework, which enhances generalization through prompts and applies margin-based contrastive learning to handle difficult samples. Additionally, it employs memory augmentation with ChatGPT-generated samples to combat overfitting in low-resource settings. MI  takes a novel approach by preserving prior knowledge through often-discarded language model heads, aligning the classification head with backbone knowledge via mutual information maximization. While these methods contribute to improving continual relation extraction, they largely overlook the challenge of extracting undetermined relations, which remains a crucial gap in real-world applications where numerous relations remain unseen or unlearned.

% Some works in traditional relation extraction have addressed the challenge of handling unseen relations. For instance, prior studies define “no relation (NA)” as sentences that contain no meaningful relation between entities (CITE), “out-of-scope (OOS)” for relations that fall outside the predefined set (CITE), and “none of the above (NOTA)” for relations that do not match any known relation type (CITE). However, these approaches primarily focus on some aspects of undetermined relations in standard relation extraction settings and do not adequately consider continual relation extraction, where the dynamic nature of real-world data introduces many unseen relations that remain unlearned.



\subsection{Background}
\subsubsection{Problem Definition}
Few-Shot Continual Relation Extraction (FCRE) requires a model to sequentially acquire new relational knowledge while retaining previously learned information. At each task $t$, the model is trained on a dataset $D^t = \{(x_i^t, y_i^t)\}_{i=1}^{N \times K}$, where $N$ denotes the number of labels provided in the set of relations $R^t$, and $K$ represents the limited number of training instances per relation (i.e., "$N$-way-$K$-shot" paradigm \citet{chen-etal-2023-consistent}). Each training example $(x, y)$ consists of a sentence $x$, which is originally given two entities $(e_h, e_t)$ and the associated relation labels $y \in R^t$. After completing task $t$, previously observed datasets $D^t$ are not extensively reused. The model's final evaluation is conducted on a test set comprising all encountered relations $\tilde{R}^T = \bigcup_{t=1}^{T} R^t$.

Beyond the standard setting and requirements of FCRE, in terms of mitigating forgetting and overfitting, our work aims at designing advanced models, which are capable of continuously capturing and recognizing new relational knowledge, which is not available in the training set.

\subsubsection{Latent Representation Encoding}
One of the fundamental challenges in relation extraction lies in effectively {encoding the latent representation} of input sentences, particularly given that Transformer-based models \citep{vaswani2017attention} produce structured matrix representations. In this study, we adopt an approach inspired by \citet{ma-etal-2024-making}. Given an input sentence $x$ that contains a head entity $e_h$ and a tail entity $e_t$, we transform it into a Cloze-style template $T(x)$ by inserting a \texttt{[MASK]} token to represent the missing relation. The structured template is defined as:

\begin{align}
\begin{aligned}
  T({x}) = \; &x \left[v_{0:n_0-1}\right] e_h \left[v_{n_0:n_1-1}\right] [\texttt{MASK}] \\
  &\left[v_{n_1:n_2-1}\right] e_t \left[v_{n_2:n_3-1}\right].
\label{eq:template}
\end{aligned}
\end{align}

where $[v_i]$ represents learnable continuous tokens, and $n_i$ denotes the respective token positions in the sentence. In our specific implementation, BERT’s \texttt{[UNUSED]} tokens are used for $[v]$. We set the soft prompt length to 3 tokens, with $n_0, n_1, n_2$, and $n_3$ assigned values of 3, 6, 9, and 12, respectively. The transformed input $T(x)$ is then processed through a pre-trained BERT model, encoding it into a sequence of continuous vectors. The hidden representation $z$ of the input is extracted at the position of the \texttt{[MASK]} token:

\begin{equation}
    z = \mathcal{M} \circ T(x)[\text{position}(\texttt{[MASK]})],
\end{equation}

where $\mathcal{M}$ represents the backbone language model. The extracted latent representation is subsequently passed through a multi-layer perceptron (MLP), allowing the model to infer the most appropriate relation for the \texttt{[MASK]} token.
% \subsection{Learning Latent Representation}
% In conventional Relation Extraction scenarios, a basic framework typically employs a backbone PLM followed by an MLP classifier to directly map the input space to the label space using Cross Entropy Loss. However, this approach faces inefficacy in data-scarce settings \cite{snell2017, swersky2017}. Consequently, training paradigms which directly target the latent space, such as contrastive learning, emerge as more suitable approaches. To enhance the semantics-richness of the information extracted from the training samples, two popular losses are often utilized: \textit{Supervised Contrastive Loss} and \textit{Hard Soft Margin Triplet Loss}.

% \subsubsection{Supervised Contrastive Loss}
% To enhance the model’s discriminative capability, we employ the Supervised Contrastive Loss (SCL) \cite{khosla2020}. This loss function is designed to bring positive pairs of samples, which share the same class label, closer together in the latent space. Simultaneously, it pushes negative pairs, belonging to different classes, further apart. Let $z_x$ represent the hidden vector output of sample $x$, the positive pairs $(z_x, z_p)$ are those who share a class, while the negative pairs $(z_x, z_n)$ correspond to different labels. The SCL is computed as follows:

% \begin{equation}
%     \mathcal{L}_{SC}(x) = -\sum_{p \in P(x)} \log \frac{f(z_x, z_p)}{\sum_{u \in D(x)} f(z_x, z_u)}
% \end{equation}

% where $f(x, y) = \exp\left(\frac{\gamma(x,y)}{\tau}\right)$, $\gamma(\cdot, \cdot)$ denotes the cosine similarity function, and $\tau$ is the temperature scaling hyperparameter. $P(x)$ and $D$ denote the sets of positive samples with respect to sample $x$ and the training set, respectively.

% \subsubsection{Hard Soft Margin Triplet Loss}
% To achieve a balance between flexibility and discrimination, the Hard Soft Margin Triplet Loss (HSMT) integrates both hard and soft margin triplet loss concepts \cite{hermans2017, beyeler2017}. This loss function is designed to maximize the separation between the most challenging positive and negative samples, while preserving a soft margin for improved flexibility. Formally, the loss is defined as:

% \begin{equation}
%     \mathcal{L}_{ST}(x) = -\log \left(1 + \max_{p \in P(x)} e^{\xi(x, z_p)} - \min_{n \in N(x)} e^{\xi(x, z_n)} \right),
% \end{equation}

% where $\xi(\cdot, \cdot)$ denotes the Euclidean distance function. The objective of this loss is to ensure that the hardest positive sample is as distant as possible from the hardest negative sample, thereby enforcing a flexible yet effective margin.

% During training, these two losses are aggregated and referred to as the \textit{Sample-based learning loss}:

% \begin{equation}
%     \mathcal{L}_{samp} = \beta_{SC} \cdot \mathcal{L}_{SC} + \beta_{ST} \cdot \mathcal{L}_{ST}
% \end{equation}

% where $\beta_{SC}$ and $\beta_{ST}$ are weighting coefficients.

% \subsection{Undetermined Relation Data Construction}
% In this work, we consider to extract any relation it can be undetermined relation (not any relation or 
% In this work, we create the dataset that contains undetermined relation as real world. 