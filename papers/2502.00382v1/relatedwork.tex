\section{Related Work}
\paragraph{Efficient Visual Generation.}
Image generation literature has seen significant improvements in the past years - Generative Adversarial Networks \citep{brock2018large,sauer2022styleganxlscalingstyleganlarge}, discrete token based models \citep{chang2022maskgit, yu2023magvit}, diffusion-based models \citep{kingma2023understanding, hoogeboom2023simple}, and more recently hybrid models \citep{peebles2023scalablediffusionmodelstransformers, yu2024representationalignmentgenerationtraining}, but they often guzzle computing power. Researchers tackle this bottleneck of computational costs with efficient model architectures and smarter sampling strategies. 


In  diffusion model literature, there have been some work to reduce the number of sampling steps, by treating the sampling process like ordinary differential equations \cite{song2022denoisingdiffusionimplicitmodels, lu2022dpmsolverfastodesolver, liu2022pseudonumericalmethodsdiffusion}, incorporating additional training process \cite{kong2021fastsamplingdiffusionprobabilistic, nichol2021improveddenoisingdiffusionprobabilistic, salimans2022progressivedistillationfastsampling, song2023consistencymodels}, sampling step distillation \cite{salimans2022progressivedistillationfastsampling, song2023consistencymodels,berthelot2023tractdenoisingdiffusionmodels,meng2023distillationguideddiffusionmodels, feng2024relationaldiffusiondistillationefficient}, sampling and training formulation modifications \cite{esser2024scalingrectifiedflowtransformers,song2023consistencymodels}, and more. 
Recently, there has been growing interest in understanding how each step in the diffusion sampling process contributes \cite{ choi2022perceptionprioritizedtrainingdiffusion, park2023understandinglatentspacediffusion, betasampling}. %
These approaches analyze sampling steps leveraging distance metrics such as LPIPS, Fourier analysis, and spectral density analysis.
Building on these explorations researchers have designed methods based on optimal sampling steps \cite{watson2022learningfastsamplersdiffusion,betasampling}, weighted training loss \cite{choi2022perceptionprioritizedtrainingdiffusion}, and step-specific models \cite{li2023autodiffusiontrainingfreeoptimizationtime, yang2024denoisingdiffusionstepawaremodels, lee2023multiarchitecturemultiexpertdiffusionmodels}. These step-specific models use computationally expensive evolutionary search algorithms, directly optimizing the quality metric, FID. Concurrently, researchers are actively addressing the inherent architectural costs of diffusion models, particularly those associated with transformer attention mechanisms \cite{yuan2024ditfastattn,yan2024diffusion}.
 
On the other hand, certain works focus on building better tokenizers. \citet{ldm} took diffusion models from pixel to compressed latent space for efficient and scalable generation.  \cite{yu2023language,weber2024maskbitembeddingfreeimagegeneration} explore certain vector quantizers in the tokenization process to improve generation quality. \citet{tian2024visualautoregressivemodelingscalable} explore multi-scale tokenizer to improve quality while \citet{yu2024imageworth32tokens} looks at 1D tokenizers to reduce the number of compressed tokens. Instead of sampling or tokenization process optimization, we tackle an orthogonal problem of efficient compute allocation over the multi-step generation process. This makes our approach usable with a variety of tokenizers, model architectures and sampling schemes.

{\flushleft \textbf{Nested Models.}} \citet{rippel2014learning} introduced nested dropout to learn ordered representations of data that improve retrieval speed and adaptive data compression. Matryoshka Learning \citep{kusupati2022matryoshka} introduces the concept of nested structures into embedding dimensions of the transformer, making them flexible. MatFormer \citep{kudugunta2023matformer} applies
the same concept to the MLP hidden layer in each transformer block. Recent methods like \citep{cai2024matryoshka, hu2024matryoshka} explore the idea of nested models in multimodal large language models. MoNE \citep{jain2024mixturenestedexpertsadaptive} and Flextron \citep{cai2024flextron} learn to route tokens to variable sized nested models  leading to compute efficient processing. In this work, we show how different parts of a multi-step task like image generation, can be modeled by different sized nested models instead of always decoding via the full model, thus significantly reducing computations.






\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{fastgen/figures/fastgen_framework.pdf}
    \caption{\small \textbf{\ours \ Decoding.} We start from the smallest nested model with an empty cache and gradually move to bigger models over the decoding iterations. We iterate using a particular sized model for a few iterations, before moving onto the next model size. As we cache the key-value pairs for the unmasked tokens, the KV cache size also increases over time. We also refresh the cache when we switch models, hence its dimension also increases over decoding iterations.}
    \label{fig:main_algo}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[trim={4.5cm 3cm 4cm 8cm},   width=0.95\textwidth]{fastgen/figures/vis_tokens.pdf}
    \caption{\small \textbf{Unmasked Token Density} visualization in each decoding iteration averaged over 50k generated samples on ImageNet. Yellow represents higher density. Each pixel represent a token from $16 \times 16$ latent token space. (See \cref{appendix:motivation} for category-wise token density).}
    \label{fig:vis_tokens}
\end{figure*}