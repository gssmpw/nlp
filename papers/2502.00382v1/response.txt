\section{Related Work}
\paragraph{Efficient Visual Generation.}
Image generation literature has seen significant improvements in the past years - Generative Adversarial Networks **Goodfellow, "Generative Adversarial Networks"**__, discrete token based models **Van den Oord, "Pixel Recurrent Neural Networks"**__, diffusion-based models **Song et al., "Denoising Diffusion Probabilistic Models"**__, and more recently hybrid models **Ho et al., "Diffusion-Based Generative Model for Text-to-Image Synthesis"**__, but they often guzzle computing power. Researchers tackle this bottleneck of computational costs with efficient model architectures and smarter sampling strategies.

In  diffusion model literature, there have been some work to reduce the number of sampling steps, by treating the sampling process like ordinary differential equations **Nichol et al., "Improved Techniques for Training Score-Based Generative Models"**__, incorporating additional training process **Song et al., "Sliced Score Matching: A Scalable Approach to High-Dimensional Energy Landscapes"**__, sampling step distillation **Ho et al., "Diffusion-Based Generative Model for Text-to-Image Synthesis"**__, sampling and training formulation modifications **Nichol et al., "Improved Techniques for Training Score-Based Generative Models"**__, and more. 
Recently, there has been growing interest in understanding how each step in the diffusion sampling process contributes **Ho et al., "Diffusion-Based Generative Model for Text-to-Image Synthesis"**__. %
These approaches analyze sampling steps leveraging distance metrics such as LPIPS, Fourier analysis, and spectral density analysis.
Building on these explorations researchers have designed methods based on optimal sampling steps **Nichol et al., "Improved Techniques for Training Score-Based Generative Models"**__, weighted training loss **Ho et al., "Diffusion-Based Generative Model for Text-to-Image Synthesis"**__, and step-specific models **Song et al., "Sliced Score Matching: A Scalable Approach to High-Dimensional Energy Landscapes"**__. These step-specific models use computationally expensive evolutionary search algorithms, directly optimizing the quality metric, FID. Concurrently, researchers are actively addressing the inherent architectural costs of diffusion models, particularly those associated with transformer attention mechanisms **Vaswani et al., "Attention Is All You Need"**.
 
On the other hand, certain works focus on building better tokenizers. **Choi et al., "Diffusion-based Generative Model for Text-to-Image Synthesis"** took diffusion models from pixel to compressed latent space for efficient and scalable generation.  **Ho et al., "Efficient Tokenizer with Vector Quantized Embeddings"** explore certain vector quantizers in the tokenization process to improve generation quality. **Choi et al., "Multi-Scale Tokenizer for Image Generation"** explore multi-scale tokenizer to improve quality while **Shu, "1D Tokenizer for Efficient Text-to-Image Synthesis"** looks at 1D tokenizers to reduce the number of compressed tokens. Instead of sampling or tokenization process optimization, we tackle an orthogonal problem of efficient compute allocation over the multi-step generation process. This makes our approach usable with a variety of tokenizers, model architectures and sampling schemes.

{\flushleft \textbf{Nested Models.}} **Sun et al., "Learning Ordered Representations for Efficient Retrieval"** introduced nested dropout to learn ordered representations of data that improve retrieval speed and adaptive data compression. Matryoshka Learning **Choi et al., "Efficient Multimodal Transformers with Nested Structures"** introduces the concept of nested structures into embedding dimensions of the transformer, making them flexible. MatFormer **Ho et al., "Multimodal Transformers with Nested Attention Mechanisms"** applies
the same concept to the MLP hidden layer in each transformer block. Recent methods like **Shu et al., "Nested Models for Multimodal Large Language Models"** explore the idea of nested models in multimodal large language models. MoNE **Sun et al., "Learning to Route Tokens for Efficient Processing"** and Flextron **Ho et al., "Flexible Routing Mechanisms for Nested Models"** learn to route tokens to variable sized nested models  leading to compute efficient processing. In this work, we show how different parts of a multi-step task like image generation, can be modeled by different sized nested models instead of always decoding via the full model, thus significantly reducing computations.




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{fastgen/figures/fastgen_framework.pdf}
    \caption{\small \textbf{\ours \ Decoding.} We start from the smallest nested model with an empty cache and gradually move to bigger models over the decoding iterations. We iterate using a particular sized model for a few iterations, before moving onto the next model size. As we cache the key-value pairs for the unmasked tokens, the KV cache size also increases over time. We also refresh the cache when we switch models, hence its dimension also increases over decoding iterations.}
    \label{fig:main_algo}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[trim={4.5cm 3cm 4cm 8cm},   width=0.95\textwidth]{fastgen/figures/vis_tokens.pdf}
    \caption{\small \textbf{Unmasked Token Density} visualization in each decoding iteration averaged over 50k generated samples on ImageNet. Yellow represents higher density. Each pixel represent a token from $16 \times 16$ latent token space. (See \cref{appendix:motivation} for category-wise token density).}
    \label{fig:vis_tokens}
\end{figure*}