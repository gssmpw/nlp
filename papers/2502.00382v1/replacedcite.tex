\section{Related Work}
\paragraph{Efficient Visual Generation.}
Image generation literature has seen significant improvements in the past years - Generative Adversarial Networks ____, discrete token based models ____, diffusion-based models ____, and more recently hybrid models ____, but they often guzzle computing power. Researchers tackle this bottleneck of computational costs with efficient model architectures and smarter sampling strategies. 


In  diffusion model literature, there have been some work to reduce the number of sampling steps, by treating the sampling process like ordinary differential equations ____, incorporating additional training process ____, sampling step distillation ____, sampling and training formulation modifications ____, and more. 
Recently, there has been growing interest in understanding how each step in the diffusion sampling process contributes ____. %
These approaches analyze sampling steps leveraging distance metrics such as LPIPS, Fourier analysis, and spectral density analysis.
Building on these explorations researchers have designed methods based on optimal sampling steps ____, weighted training loss ____, and step-specific models ____. These step-specific models use computationally expensive evolutionary search algorithms, directly optimizing the quality metric, FID. Concurrently, researchers are actively addressing the inherent architectural costs of diffusion models, particularly those associated with transformer attention mechanisms ____.
 
On the other hand, certain works focus on building better tokenizers. ____ took diffusion models from pixel to compressed latent space for efficient and scalable generation.  ____ explore certain vector quantizers in the tokenization process to improve generation quality. ____ explore multi-scale tokenizer to improve quality while ____ looks at 1D tokenizers to reduce the number of compressed tokens. Instead of sampling or tokenization process optimization, we tackle an orthogonal problem of efficient compute allocation over the multi-step generation process. This makes our approach usable with a variety of tokenizers, model architectures and sampling schemes.

{\flushleft \textbf{Nested Models.}} ____ introduced nested dropout to learn ordered representations of data that improve retrieval speed and adaptive data compression. Matryoshka Learning ____ introduces the concept of nested structures into embedding dimensions of the transformer, making them flexible. MatFormer ____ applies
the same concept to the MLP hidden layer in each transformer block. Recent methods like ____ explore the idea of nested models in multimodal large language models. MoNE ____ and Flextron ____ learn to route tokens to variable sized nested models  leading to compute efficient processing. In this work, we show how different parts of a multi-step task like image generation, can be modeled by different sized nested models instead of always decoding via the full model, thus significantly reducing computations.






\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{fastgen/figures/fastgen_framework.pdf}
    \caption{\small \textbf{\ours \ Decoding.} We start from the smallest nested model with an empty cache and gradually move to bigger models over the decoding iterations. We iterate using a particular sized model for a few iterations, before moving onto the next model size. As we cache the key-value pairs for the unmasked tokens, the KV cache size also increases over time. We also refresh the cache when we switch models, hence its dimension also increases over decoding iterations.}
    \label{fig:main_algo}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[trim={4.5cm 3cm 4cm 8cm},   width=0.95\textwidth]{fastgen/figures/vis_tokens.pdf}
    \caption{\small \textbf{Unmasked Token Density} visualization in each decoding iteration averaged over 50k generated samples on ImageNet. Yellow represents higher density. Each pixel represent a token from $16 \times 16$ latent token space. (See \cref{appendix:motivation} for category-wise token density).}
    \label{fig:vis_tokens}
\end{figure*}