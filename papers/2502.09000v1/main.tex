\documentclass[conference]{IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.

% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{ {pictures/}{../pdf/}{../jpeg/} }
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png,.PNG,.svg,.SVG}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo Figs use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex

% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a Fig
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx

% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array

% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.

% *** SUBFig PACKAGES ***
\ifCLASSOPTIONcompsoc
 \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
 \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subFig.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style Fig/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig

% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column Fig to be placed prior to an earlier double column
% Fig.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e

%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{Fig*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix

% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


% ------ Spencer's Adds: -----------
% HOW TO make a command ("tab" here)
\newcommand\tab[1][0.4cm]{\hspace*{#1}}
% This line makes the collumns on the last line even
\usepackage{flushend}
\usepackage{tabularx,booktabs,textcomp}
% Additions for custom TabularX environment tables: 
\newcolumntype{C}{>{\centering\arraybackslash}X} % centered version of "X" type
\newcolumntype{L}{>{\raggedright\arraybackslash}X} % LEFT version... "\...right" works?
% Next three lines take [1], [2], [3], [4] and cite as [1-4]:
\usepackage[noadjust]{cite}
\renewcommand{\citepunct}{,\penalty\citepunctpenalty\,}
\renewcommand{\citedash}{--}    % optionally
% Include Smileys :) 
\usepackage{wasysym}
% Include to generate random text: 
\usepackage{lipsum} % USE: "\lipsum[1]" to generate 1 paragraph of random text 
\usepackage{svg}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage[none]{hyphenat}
% ----------------------------------

\begin{document}

\title{Residual Transformer Fusion Network for Salt and Pepper Image Denoising}

% AUTHOR
\author{
\IEEEauthorblockN{Bintang Pradana Erlangga Putra}
\IEEEauthorblockA{Department of Informatics\\
Universitas Sebelas Maret\\
Surakarta, Indonesia\\
bintangpradana02@student.uns.ac.id}
\and
\IEEEauthorblockN{Heri Prasetyo}
\IEEEauthorblockA{Department of Informatics\\
Universitas Sebelas Maret\\
Surakarta, Indonesia\\
heri.prasetyo@staff.uns.ac.id}
\and
\IEEEauthorblockN{Esti Suryani}
\IEEEauthorblockA{Department of Informatics\\
Universitas Sebelas Maret\\
Surakarta, Indonesia\\
estisuryani@staff.uns.ac.id}
}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}

Convolutional Neural Network (CNN) has been widely used in unstructured datasets, one of which is image denoising. Image denoising is a noisy image reconstruction process that aims to reduce additional noise that occurs from the noisy image with various strategies. Image denoising has a problem, namely that some image denoising methods require some prior knowledge of information about noise. To overcome this problem, a combined architecture of Convolutional Vision Transformer (CvT) and Residual Networks (ResNet) is used which is called the Residual Transformer Fusion Network (RTF-Net). In general, the process in this architecture can be divided into two parts, Noise Suppression Network (NSN) and Structure Enhancement Network (SEN). Residual Block is used in the Noise Suppression Network and is used to learn the noise map in the image, while the CvT is used in the Structure Enhancement Network and is used to learn the details that need to be added to the image processed by the Noise Suppression Network. The model was trained using the DIV2K Training Set dataset, and validation using the DIV2K Validation Set. After doing the training, the model was tested using Lena, Bridge, Pepper, and BSD300 images with noise levels ranging from 30\%, 50\%, and 70\% and the PSNR results were compared with the DBA, NASNLM, PARIGI, NLSF, NLSF-MLP and NLSF-CNN methods. The test results show that the proposed method is superior in all cases except for Pepper's image with a noise level of 30\%, where NLSF-CNN is superior with a PSNR value of 32.99 dB, while the proposed method gets a PSNR value of 31.70 dB.

\end{abstract}

% Keywords
\begin{IEEEkeywords}
Residual, Attention, Transformer, Salt and Pepper, Image Denoising
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{C}{onvolutional} Neural Network (CNN) has been widely used to process unstructured datasets such as image classification, image segmentation, and image generation \cite{chen2018encoderdecoder, howard2017mobilenets, radford2016unsupervisedrepresentationlearningdeep}. The utilization of Convolutional Neural Network (CNN) has proven that this method is reliable in computing spatial information such as digital image processing.

The use of transformer architecture is growing in tandem with the development of the use of convolutional architecture in image processing with the Convolutional Neural Network (CNN) method. Nonetheless, the transformer method is still more widely used in Natural Language Processing (NLP) \cite{vaswani2017attention}. The Vision Transformer (ViT) method, which was introduced as a pure transformer architecture method, demonstrates that transformer architecture can also be used in image classification problems \cite{dosovitskiy2020vit}. Convolutional Neural Network (CNN) was introduced to the Vision Transformer (ViT) to form a Convolutional Vision Transformer (CvT) which combines the advantages of convolution and transformer architectures so that it is expected to provide more optimal results in computational efficiency of image processing \cite{wu2021cvt}.

Image denoising has a problem, namely, some image denoising methods require some prior knowledge of information about noise, such as noise model, noise distribution, and noise level, to reduce noise in the image \cite{prayuda2021awgn}. To overcome these problems, Convolutional Vision Transformer (CvT) and Residual Networks (ResNet) are used by implementing end-to-end training so that the Convolutional Vision Transformer (CvT) model can directly map noisy images with clean images.

\section{Proposed Denoising Model}
\label{sec:op}

\begin{figure*}[!t] % t,h, and b mean "top," "in-line," and "bottom" respectively
  \centering
  \includegraphics[width=\textwidth]{pictures/diagram/Skripsi-Net.png}\\
  \caption{RTF-Net Architecture}
  \label{fig:rtf-net}
\end{figure*}

The proposed denoising architecture will be explained in this section, which includes the attention mechanism, transformer, and Convolutional Vision Transformer (CvT). This section will also explain salt and pepper noise modeling and how the proposed architecture eliminates salt and pepper noise from images.

\subsection{Noise Modelling} 

Image denoising is a noisy image reconstruction process that aims to reduce additional noise from the image with various strategies. Some image denoising methods require several prior knowledge of noise information, such as noise model, noise distribution, and noise level, to reduce noise in the image. In an ideal situation, the image denoising algorithm should be able to reduce noise in the image directly without any prior information \cite{prayuda2021awgn}.

\begin{equation}
    \label{eq:noise-model}
    I(i,j)=
    \begin{cases}
    0,&  r_1 < p \land r_2 < 0.5 \\
    255,& r_1 < p \land r_2 \geq{0.5} \\
    I(i,j),& r_1 \geq{p}
    \end{cases}
\end{equation}

In this study, the digital image used will be added with salt and pepper noise where the value of a pixel becomes maximum or minimum depending on a predetermined probability. Equation (\ref{eq:noise-model}) is the formulation of salt and pepper noise with $p \in (0,1)$ as the noise level, $r_1$ and $r_2$ as two pseudo-random values generated at each pixel in the image to be affixed with salt and pepper noise.

\subsection{Residual Transformer Fusion Network}

In this research, an architecture called Residual Transformer Fusion Network (RTF-Net) is proposed and shown in Figure \ref{fig:rtf-net}. In general, the processes that occur in this architecture can be divided into two parts, Noise Suppression Network (NSN) and Structure Enhancement Network (SEN). Residual Block is used in the Noise Suppression Network and is used to learn the noise map in the image, while the Convolutional Vision Transformer (CvT) is used in the Structure Enhancement Network and is used to learn the details that need to be added to the image that has been processed by the Noise Suppression Network. The RTF-Net architecture can be formulated as equation (\ref{eq:nsn}) and equation (\ref{eq:sen}).

\begin{equation}
    \label{eq:nsn}
    y=\tilde{I}-\mathbb{NSN}(\tilde{I};\theta)
\end{equation}

In the first stage formulated by equation (\ref{eq:nsn}), the noisy image denoted as $\tilde{I}$ will be processed using a Noise Suppression Network denoted as $\mathbb{NSN}(\cdot;\theta)$ with $\theta$ as the parameter, then the result will be subtracted by the noisy image and produce an image $y$ which is transition image.

\begin{equation}
    \label{eq:sen}
    I^{*}=y+\mathbb{SEN}(y;\theta)
\end{equation}

In the second stage formulated by equation (\ref{eq:sen}), the $y$ mage will be processed using the Structure Enhancement Network which is denoted as $\mathbb{SEN}(\cdot;\theta)$ with $\theta$ as the parameter, then the results will be summed with the $y$ image to produce a clean image which is denoted as $I^{*}$.

\subsection{Residual Block}

\begin{figure}[h]
    \centering
    \includegraphics[width=2.5in]{pictures/diagram/Skripsi-ResBlock.png}
    \caption{Residual Block Architecture}
    \label{fig:resblock}
\end{figure}

Residual Block architecture is used in the Noise Suppression Network on RT-Net and is shown in Figure  \ref{fig:resblock}. This residual block consists of two convolution blocks, where the convolution block consists of a convolution layer, a normalization function layer, and a ReLU activation function layer. Incoming features will go through layers in the convolution block and then the features that do not pass through the convolution block are summed with the features that pass through the convolution block (identity mapping \cite{he2015residual}) to form output features.

\begin{equation}
    \label{eq:convblock}
    x_{out}=\delta(\aleph(x_{in}*W+b))
\end{equation}

The convolution block can be formulated as equation (\ref{eq:convblock}) where $x_{in}$ represents the input feature, $x_{out}$ represents the output feature, $W$ represents the weight of the convolution layer, $b$ represents the bias of the convolution layer, $\aleph(\cdot)$ represents the normalization function, and $\delta(\cdot)$ represents the activation function.

\begin{equation}
    \label{eq:resblock}
    x_{out}=x_{in}+\mathbb{B}(\mathbb{B}(x_{in};\theta);\theta)
\end{equation}

The residual block can be formulated as equation (\ref{eq:resblock}) where $x_{in}$ represents the input feature, $x_{out}$ represents the output feature, and $\mathbb{B}(\cdot;\theta)$ represents the convolution block and its parameters.

\subsection{Convolutional Vision Transformer}

\begin{figure}[h]
    \centering
    \includegraphics[width=2.5in]{pictures/diagram/Skripsi-CvT.png}
    \caption{CvT Architecture}
    \label{fig:cvt}
\end{figure}

The Convolutional Vision Transformer (CvT) \cite{wu2021cvt} depicted in Figure \ref{fig:cvt} is a hybrid of convolutional and Vision Transformer (ViT) architectures. The goal of this method is to achieve the advantages of the Convolutional Neural Network (CNN) in the domains of local receptivity, shared weights, and spatial subsampling with the advantages of transformers in dynamic attention, global context fusion, and improved generalization.

\begin{equation}
    \label{eq:cvt}
    x_{out}=\mathbb{T}(x_{in}*W_{in}+b_{in};\theta)
\end{equation}

Convolutional Vision Transformer architecture can be formulated as equation (\ref{eq:cvt}), where $x_{in}$ represents the input feature, $W_{in}$ and $b_in$ represent the weight and bias of the convolutional embedding layer, and $\mathbb{T}(\cdot;\theta)$ represents the transformer layer and the parameters used in that layer.

\begin{figure}[h]
    \centering
    \includegraphics[width=2.5in]{pictures/diagram/Skripsi-Transformer.png}
    \caption{Transformer Architecture}
    \label{fig:transformer}
\end{figure}

Figure \ref{fig:transformer} depicts the Transformer Block architecture design utilized in the Convolutional Vision Transformer (CvT) block. A transformer Block normally consists of two components. First, the features are input and processed in the first component, namely the normalization function layer and the attention layer, which is then summed to the identity mapping of the incoming features which is defined as equation (\ref{eq:transformer1}). Following the first part, the features will be processed in the second part, which comprises of a normalizing function layer and a Multi-layer Perceptron (MLP) layer, followed by an identity mapping of the features that enter part two, which is expressed as an equation (\ref{eq:transformer2}).

\begin{equation}
    \label{eq:transformer1}
    x^*=x_{emb}+\mathbb{A}(\aleph(x_{emb}))
\end{equation}

When the embedding results are processed by the transformer, the feature will pass through the normalization layer, the attention mechanism, and the embedding results to form a residual connection. In equation (\ref{eq:transformer1}), $x_{emb}$ represents the embedding result, $\aleph$ represents the normalization layer, $\mathbb{A}$ represents attention mechanism, and $x^*$ represents the normalization, attention, and residual connection results.

\begin{equation}
    \label{eq:transformer2}
    x_{out}=x^*+\mathbb{M}(\aleph(x^*))
\end{equation}

The result of equation (\ref{eq:transformer1}) will then be processed through the normalization layer, multi-layer perceptron, and residual connection. In equation (\ref{eq:transformer2}), $x^*$ represents the calculation result of equation (\ref{eq:transformer1}), $\aleph$ represents the normalization layer, $\mathbb{M}$ represents the multi-layer perceptron formulated by equation (\ref{eq:mlp}), and $x_{out}$ represents the normalized, multi-layer perceptron, and residual connection results.

\begin{equation}
    \label{eq:mlp}
    x_{out}=\delta(x_{in}*W_1+b_1)*W_2+b_2
\end{equation}

Equation (\ref{eq:mlp}) is the formulation of the multi-layer perceptron architecture used in equation (\ref{eq:transformer2}). This architecture consists of three layers namely, the convolution layer, gaussian error linear unit (GELU), and convolution layer \cite{hendrycks2016gelu}. In equation (\ref{eq:mlp}), $x_{in}$ represents the input feature, $x_{out}$ represents the output feature, $\delta$ represents the activation function, $W$ represents the weight of the convolution layer, and $b$ represents the bias of the convolution layer.

\subsection{Attention Mechanism}

\begin{figure}[h]
    \centering
    \includegraphics[width=3.5in]{pictures/diagram/Skripsi-ConvAttention.png}
    \caption{Attention Architecture}
    \label{fig:attention}
\end{figure}

We employ Multi-head Attention \cite{vaswani2017attention} with Attention Block in each Transformer Block, as illustrated in Figure \ref{fig:attention}. The use of direct attention can result in excessive computational load. To overcome this problem, we use pixel unshuffle to reduce the spatial dimensions of the image and move it to channel dimensions in the image, convolution is then applied to the unshuffled pixel image to reduce the channel dimensions in the image.

\begin{equation}
    \label{eq:attention1}
    x_{red}=\mathbb{S}^{-1}(x_{in})*W_{red}+b_{red}
\end{equation}

The above process can be formulated as equation (\ref{eq:attention1}), where $W_{red}$ represents the convolution weights, $b_{red}$ represents the convolution bias, $\mathbb{S}^{-1}(\cdot)$ represents the pixel unshuffle operator, $x_{in}$ represents the input feature, and $x_{red}$ represents the output feature. The features resulting from the above process will then enter the Multi-head Attention mechanism.

\begin{equation}
    \label{eq:attention2}
    \hat{x}_{attn}^{(h)}=\mathbb{S}(x_{attn}^{(h)})*W_{proj}
\end{equation}

The next stage is pixel shuffle where the channel dimensions are converted into spatial dimensions and then projected using a convolution layer. The process can be formulated by equation (\ref{eq:attention2}) where $\mathbb{S}(\cdot)$ represents the pixel shuffle operator, $W_{proj}$ represents the weight of the projection layer, $x_{attn}^{(h)}$ represents the incoming feature resulting from the $h$-th attention head processing, and $\hat{x}_{attn}^{(h)}$ represents output features that have been processed by $h$-th attention head.

\subsection{Training}

Experiments were conducted in this paper utilizing the Python programming language and the Pytorch framework. The model was created using a system with the following specifications: AMD Ryzen Threadripper 12 Core 24 Threads, Nvidia RTX 3090 GPU with 24GB VRAM, and 128GB RAM.

\begin{table}[h]
\caption{Hyperparameter}
\label{tab:hyperparameter}
\centering
\begin{tabular}{lc}
\toprule
    \textit{Hyperparameter} &   Nilai\\
\midrule
    \textit{Patch Size}     &   64$\times$64    \\
    \textit{Kernel Size}    &   3$\times$3      \\
    \textit{Stride}         &   1               \\
    \textit{Padding}        &   1               \\
    \textit{NSN Depth}      &   8               \\
    \textit{NSN Features}   &   32              \\
    \textit{SEN Depth}      &   2               \\
    \textit{NSN Features}   &   32              \\
    \textit{CvT Depth}      &   2               \\
    \textit{Attention Head} &   4               \\
    \textit{Optimizer}      &   Adam            \\
    \textit{Learning Rate}  &   0.001           \\
    \textit{Batch Size}     &   32              \\
    \textit{Epoch}          &   25              \\
    \textit{Step Size}      &   6               \\
    \textit{Gamma}          &   0.5             \\
    
\bottomrule
\end{tabular}
\end{table}

At this stage, hyperparameter tuning of the Residual Transformer Fusion Network (RTF-Net) is carried out. The hyperparameters considered include patch size, image size, number of layers on the Noise Suppression Network (NSN), number of layers on the Structure Enhancement Network (SEN), kernel size, number of features, optimizer, learning rate, batch size, and epoch. These hyperparameters are shown in Table \ref{tab:hyperparameter}.

\section{Results and Discussion}

\begin{table*}[!t]
\caption{Peak Signal to Noise Ratio Result}
\label{tab:result}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{}lYYYYYYYY@{}} 
\toprule
\multirow{2}{*}{Image}  & \multirow{2}{*}{Noise Level} & \multicolumn{7}{c}{Method}                                                    \\ 
\cmidrule{3-9}
                        &                              & DBA\cite{srinivasan2007dba}   & NASNLM\cite{varghese2015nasnlm} & PARIGI\cite{delon2016parigi} & NLSF\cite{fu2018nlsf}  & NLSF-MLP\cite{burger2012nlsf} & NLSF-CNN\cite{fu2018nlsf}       & Ours            \\ 
\midrule
\multirow{3}{*}{Lena}   & 30\%                         & 34.42 & 28.09  & 33.90  & 34.20 & 30.80    & 35.38          & \textbf{38.87}  \\
                        & 50\%                         & 30.11 & 26.15  & 29.91  & 30.12 & 29.28    & 32.55          & \textbf{34.62}  \\
                        & 70\%                         & 25.84 & 25.97  & 25.22  & 25.79 & 27.63    & 30.18          & \textbf{32.85}  \\ 
\midrule
\multirow{3}{*}{Bridge} & 30\%                         & 28.07 & 23.68  & 25.19  & 28.21 & 25.19    & 28.71          & \textbf{32.24}  \\
                        & 50\%                         & 24.24 & 22.91  & 22.61  & 22.45 & 23.86    & 26.01          & \textbf{28.26}  \\
                        & 70\%                         & 21.21 & 22.63  & 20.06  & 21.02 & 22.61    & 24.11          & \textbf{26.44}  \\ 
\midrule
\multirow{3}{*}{Pepper} & 30\%                         & 26.85 & 22.38  & 28.88  & 32.27 & 30.01    & \textbf{32.99} & 31.70           \\
                        & 50\%                         & 25.27 & 21.82  & 25.44  & 27.99 & 28.57    & 30.23          & \textbf{30.47}  \\
                        & 70\%                         & 22.11 & 21.58  & 21.46  & 23.04 & 27.04    & 27.70          & \textbf{29.27}  \\ 
\midrule
\multirow{3}{*}{BSD300} & 30\%                         & 29.92 & 25.74  & 12.04  & 30.01 & 29.77    & 30.87          & \textbf{44.56}  \\
                        & 50\%                         & 26.32 & 24.50  & 6.01   & 26.25 & 26.19    & 27.84          & \textbf{38.03}  \\
                        & 70\%                         & 22.81 & 24.65  & 5.42   & 22.85 & 26.19    & 25.35          & \textbf{34.96}  \\
\bottomrule
\end{tabularx}
\end{table*}

In this section, the results of the training using the DIV2K Training Set dataset with noise levels of 30\%, 50\%, and 70\% will be presented along with an explanation. The test results using Lena, Bridge, Pepper, and BSD300 images with similar noise levels during training will be compared with the previous salt and pepper image denoising method.

\subsection{30\% Noise Level}

\begin{figure}[h]
    \centering
    \includegraphics[width=2.75in]{pictures/graph/loss_30_noise.png}
    \caption{Training and Validation Loss Graph with 30\% Noise Level}
    \label{fig:loss30}
\end{figure}

Figure \ref{fig:loss30} depicts the training and validation loss graph from the training stage using a noise level of 30\%. The training was performed over the course of 40,000 iterations, separated into 25 epochs. The training loss value is 0.0026 in the first epoch and 0.0008 in the last epoch, whereas the validation loss value is 0.0012 in the first epoch and 0.0005 in the last epoch.

\begin{figure}[h]
    \centering
    \includegraphics[width=2.75in]{pictures/graph/psnr_30_noise.png}
    \caption{Training and Validation PSNR Graph with 30\% Noise Level}
    \label{fig:psnr30}
\end{figure}

Figure \ref{fig:psnr30} depicts the training and validation PSNR graph from the training stage using a noise level of 30\%. The training was performed over the course of 40,000 iterations, separated into 25 epochs. The training loss value is 25.707 in the first epoch and 30.891 in the last epoch, whereas the validation loss value is 29.022 in the first epoch and 32.689 in the last epoch.

\subsection{50\% Noise Level}

\begin{figure}[h]
    \centering
    \includegraphics[width=2.75in]{pictures/graph/loss_50_noise.png}
    \caption{Training and Validation Loss Graph with 50\% Noise Level}
    \label{fig:loss50}
\end{figure}

Figure \ref{fig:loss50} depicts the training and validation loss graph from the training stage using a noise level of 50\%. The training was performed over the course of 40,000 iterations, separated into 25 epochs. The training loss value is 0.0055 in the first epoch and 0.0016 in the last epoch, whereas the validation loss value is 0.0090 in the first epoch and 0.0012 in the last epoch.

\begin{figure}[h]
    \centering
    \includegraphics[width=2.75in]{pictures/graph/psnr_50_noise.png}
    \caption{Training and Validation PSNR Graph with 50\% Noise Level}
    \label{fig:psnr50}
\end{figure}

Figure \ref{fig:psnr50} depicts the training and validation PSNR graph from the training stage using a noise level of 50\%. The training was performed over the course of 40,000 iterations, separated into 25 epochs. The training loss value is 22.541 in the first epoch and 27.868 in the last epoch, whereas the validation loss value is 20.410 in the first epoch and 29.185 in the last epoch.

\subsection{70\% Noise Level}

\begin{figure}[h]
    \centering
    \includegraphics[width=2.75in]{pictures/graph/loss_70_noise.png}
    \caption{Training and Validation Loss Graph with 70\% Noise Level}
    \label{fig:loss70}
\end{figure}

Figure \ref{fig:loss70} depicts the training and validation loss graph from the training stage using a noise level of 70\%. The training was performed over the course of 40,000 iterations, separated into 25 epochs. The training loss value is 0.0085 in the first epoch and 0.0029 in the last epoch, whereas the validation loss value is 0.0035 in the first epoch and 0.0022 in the last epoch.

\begin{figure}[h]
    \centering
    \includegraphics[width=2.75in]{pictures/graph/psnr_70_noise.png}
    \caption{Training and Validation PSNR Graph with 70\% Noise Level}
    \label{fig:psnr70}
\end{figure}

Figure \ref{fig:psnr70} depicts the training and validation PSNR graph from the training stage using a noise level of 70\%. The training was performed over the course of 40,000 iterations, separated into 25 epochs. The training loss value is 20.665 in the first epoch and 25.294 in the last epoch, whereas the validation loss value is 24.524 in the first epoch and 26.553 in the last epoch.

\subsection{Testing}

After the best hyperparameters were found, we tested the model using  Lena, Bridge, Pepper, and BSD300 images with the noise levels for each image of 30\%, 50\%, and 70\%, respectively. After testing, the PSNR values were compared with the previous method, namely DBA \cite{srinivasan2007dba}, NASNLM \cite{varghese2015nasnlm}, PARIGI \cite{delon2016parigi}, NLSF \cite{fu2018nlsf}, NLSF-MLP \cite{burger2012nlsf}, and NLSF-CNN \cite{fu2018nlsf}. Table \ref{tab:result} shows that the proposed method excels in all cases except for Pepper's image with a noise level of 30\%, where NLSF-CNN excels with a PSNR value of 32.99 while the proposed method gets a PSNR value of 31.70. A visual comparison of the test results is presented in Figure \ref{fig:lena} for the Lena image, Figure \ref{fig:bridge} for the Bridge image, and Figure \ref{fig:pepper} for the Pepper image.

\subsection{Discussion}

\begin{figure}[!t]
    \centering
    \subfloat[Ground Truth]{\label{fig:blur_a}\includegraphics[width=1in]{pictures/dis/blur-0.png}}
    \quad
    \subfloat[Blurry]{\label{fig:blur_b}\includegraphics[width=1in]{pictures/dis/blur-1.png}}
    \quad
    \subfloat[Deblurred]{\label{fig:blur_c}\includegraphics[width=1in]{pictures/dis/blur-2.png}}
    \caption{Visual investigation of deblurred "Airplane" image using the proposed method over $7\times7$ blur kernel}
    \label{fig:airplane}
\end{figure}

\begin{figure}[!t]
    \centering
    \subfloat[Ground Truth]{\label{fig:jpeg_a}\includegraphics[width=1in]{pictures/dis/jpeg-0.png}}
    \quad
    \subfloat[Artifact]{\label{fig:jpeg_b}\includegraphics[width=1in]{pictures/dis/jpeg-1.png}}
    \quad
    \subfloat[Cleaned]{\label{fig:jpeg_c}\includegraphics[width=1in]{pictures/dis/jpeg-2.png}}
    \caption{Visual investigation of JPEG artifact removed "Parrot" image using the proposed method over 10\% JPEG compression}
    \label{fig:parrot}
\end{figure}

\begin{figure}[!t]
    \centering
    \subfloat[Ground Truth]{\label{fig:sisr_a}\includegraphics[width=1in]{pictures/dis/sisr-0.png}}
    \quad
    \subfloat[Low Res]{\label{fig:sisr_b}\includegraphics[width=1in]{pictures/dis/sisr-1.png}}
    \quad
    \subfloat[Upscaled]{\label{fig:sisr_c}\includegraphics[width=1in]{pictures/dis/sisr-2.png}}
    \caption{Visual investigation of Single Image Super Resolution "Girl" image using the proposed method over $4\times$ bicubic downsampled image}
    \label{fig:girl}
\end{figure}

In this section, we will explore the potential of the Residual Transformer Fusion Network architecture in solving other cases such as image deblurring, JPEG artifact removal, and Single Image Super-Resolution (SISR). The model was trained using the same parameters except for epochs, which in this experiment, we only used 10 epochs.

In Figure \ref{fig:airplane}, the deblurred image shows sharp results with great details, although it still leaves a shadow image in the perimeter of contrasting region. In Figure \ref{fig:parrot}, the JPEG artifact removal image shows a smoother result when the color transition occurs, but the fine details cannot be reconstructed. Figure \ref{fig:girl} shows the results of the Single Image Super-Resolution which produces a sharp and contrasting image but still leaves an artifact in the form of a grid pattern.

\section{Conclusions}
\label{sec:conc}

\begin{figure*}[!t]
    \centering
    \subfloat[Ground Truth]{\label{fig:lena_a}\includegraphics[width=1.15in]{pictures/lena/lena_a.png}}
    \qquad
    \subfloat[30\% Noise]{\label{fig:lena_b30}\includegraphics[width=1.15in]{pictures/lena/lena_b30.png}}
    \qquad
    \subfloat[30\% NSN]{\label{fig:lena_c30}\includegraphics[width=1.15in]{pictures/lena/lena_c30.png}}
    \qquad
    \subfloat[50\% NSN]{\label{fig:lena_c50}\includegraphics[width=1.15in]{pictures/lena/lena_c50.png}}
    \qquad
    \subfloat[70\% NSN]{\label{fig:lena_c70}\includegraphics[width=1.15in]{pictures/lena/lena_c70.png}}
    \qquad
    \subfloat[50\% Noise]{\label{fig:lena_b50}\includegraphics[width=1.15in]{pictures/lena/lena_b50.png}}
    \qquad
    \subfloat[70\% Noise]{\label{fig:lena_b70}\includegraphics[width=1.15in]{pictures/lena/lena_b70.png}}
    \qquad
    \subfloat[30\% SEN]{\label{fig:lena_d30}\includegraphics[width=1.15in]{pictures/lena/lena_d30.png}}
    \qquad
    \subfloat[50\% SEN]{\label{fig:lena_d50}\includegraphics[width=1.15in]{pictures/lena/lena_d50.png}}
    \qquad
    \subfloat[70\% SEN]{\label{fig:lena_d70}\includegraphics[width=1.15in]{pictures/lena/lena_d70.png}}
    \caption{Visual investigation of denoised "Lena" image from the proposed method over various noise levels $p=\{0.3,0.5,0.7\}$}
    \label{fig:lena}
\end{figure*}

This study aims to obtain a neural network architecture to reconstruct noisy images with salt and pepper noise types effectively. In this study, a neural network architecture called Residual Transformer Fusion Network is proposed, which is a combination of Residual Network (ResNet) which is used as the Noise Suppression Network (NSN), and Convolutional Vision Transformer (CvT) which is used in the Structure Enhancement Network (SEN). The proposed architecture is trained with 40,000 iterations which are divided into 25 epochs using the DIV2K Training Set dataset which has been applied to the patching process with a patch size of 64×64 and validation using the DIV2K Validation Set dataset which is not applied by the patching process. From the test results using Lena, Bridge, Pepper, and BSD300 images with noise levels of 30\%, 50\%, and 70\%, the proposed method provides a better PSNR than the previous methods, namely DBA \cite{srinivasan2007dba}, NASNLM \cite{varghese2015nasnlm}, PARIGI \cite{delon2016parigi}, NLSF \cite{fu2018nlsf}, NLSF-MLP \cite{burger2012nlsf}, dan NLSF-CNN \cite{fu2018nlsf}. The visual comparison shows that the proposed method can produce clean images with good detail.

\subsection*{Acknowledgements}

This research was carried out with the support of the computational facilities of the Intelligent Systems and Humanized Computing Laboratory of Sebelas Maret University Informatics.

%------- CITATIONS ----------
\nocite{}
\bibliography{references}
\bibliographystyle{IEEEtran}

\begin{figure*}[!t]
    \centering
    \subfloat[Ground Truth]{\label{fig:bridge_a}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_a.png}}
    \qquad
    \subfloat[30\% Noise]{\label{fig:bridge_b30}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_b30.png}}
    \qquad
    \subfloat[30\% NSN]{\label{fig:bridge_c30}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_c30.png}}
    \qquad
    \subfloat[50\% NSN]{\label{fig:bridge_c50}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_c50.png}}
    \qquad
    \subfloat[70\% NSN]{\label{fig:bridge_c70}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_c70.png}}
    \qquad
    \subfloat[50\% Noise]{\label{fig:bridge_b50}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_b50.png}}
    \qquad
    \subfloat[70\% Noise]{\label{fig:bridge_b70}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_b70.png}}
    \qquad
    \subfloat[30\% SEN]{\label{fig:bridge_d30}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_d30.png}}
    \qquad
    \subfloat[50\% SEN]{\label{fig:bridge_d50}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_d50.png}}
    \qquad
    \subfloat[70\% SEN]{\label{fig:bridge_d70}\includegraphics[width=1.15in]{pictures/walkbridge/walkbridge_d70.png}}
    \caption{Visual investigation of denoised "Bridge" image from the proposed method over various noise levels $p=\{0.3,0.5,0.7\}$}
    \label{fig:bridge}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \subfloat[Ground Truth]{\label{fig:pepper_a}\includegraphics[width=1.15in]{pictures/pepper/pepper_a.png}}
    \qquad
    \subfloat[30\% Noise]{\label{fig:pepper_b30}\includegraphics[width=1.15in]{pictures/pepper/pepper_b30.png}}
    \qquad
    \subfloat[30\% NSN]{\label{fig:pepper_c30}\includegraphics[width=1.15in]{pictures/pepper/pepper_c30.png}}
    \qquad
    \subfloat[50\% NSN]{\label{fig:pepper_c50}\includegraphics[width=1.15in]{pictures/pepper/pepper_c50.png}}
    \qquad
    \subfloat[70\% NSN]{\label{fig:pepper_c70}\includegraphics[width=1.15in]{pictures/pepper/pepper_c70.png}}
    \qquad
    \subfloat[50\% Noise]{\label{fig:pepper_b50}\includegraphics[width=1.15in]{pictures/pepper/pepper_b50.png}}
    \qquad
    \subfloat[70\% Noise]{\label{fig:pepper_b70}\includegraphics[width=1.15in]{pictures/pepper/pepper_b70.png}}
    \qquad
    \subfloat[30\% SEN]{\label{fig:pepper_d30}\includegraphics[width=1.15in]{pictures/pepper/pepper_d30.png}}
    \qquad
    \subfloat[50\% SEN]{\label{fig:pepper_d50}\includegraphics[width=1.15in]{pictures/pepper/pepper_d50.png}}
    \qquad
    \subfloat[70\% SEN]{\label{fig:pepper_d70}\includegraphics[width=1.15in]{pictures/pepper/pepper_d70.png}}
    \caption{Visual investigation of denoised "Pepper" image from the proposed method over various noise levels $p=\{0.3,0.5,0.7\}$}
    \label{fig:pepper}
\end{figure*}

% that's all folks
\end{document}


