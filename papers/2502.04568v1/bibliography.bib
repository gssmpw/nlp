@InCollection{ryan:2004:GPTP,
  author =       "Conor Ryan and Maarten Keijzer and Mike Cattolico",
  title =        "Favorable Biasing of Function Sets Using Run
                 Transferable Libraries",
  booktitle =    "Genetic Programming Theory and Practice {II}",
  year =         "2004",
  editor =       "Una-May O'Reilly and Tina Yu and Rick L. Riolo and
                 Bill Worzel",
  chapter =      "7",
  pages =        "103--120",
  address =      "Ann Arbor",
  month =        "13-15 " # may,
  publisher =    "Springer",
  keywords =     "genetic algorithms, genetic programming, automatically
                 defined functions, run transferable libraries",
  ISBN =         "0-387-23253-2",
  DOI =          "doi:10.1007/0-387-23254-0_7",
  abstract =     "This chapter describes the notion of Run Transferable
                 Libraries(RTLs), libraries of functions which evolve
                 from run to run. RTLs have much in common with standard
                 programming libraries as they provide a suite of
                 functions that can not only be used across several runs
                 on a particular problem, but also to aid in the scaling
                 of a system to more difficult instances of a problem.
                 This is achieved by training a library on a relatively
                 simple instance of a problem before applying it to the
                 more difficult one.

                 The chapter examines the dynamics of the library
                 internals, and how functions compete for dominance of
                 the library. We demonstrate that the libraries tend to
                 converge on a small number of functions, and identify
                 methods to test how well a library is likely to be able
                 to scale.",
  notes =        "part of \cite{oreilly:2004:GPTP2}",
}

@InCollection{rosca:1996:aigp2,
  author =       "Justinian P. Rosca and Dana H. Ballard",
  title =        "Discovery of Subroutines in Genetic Programming",
  booktitle =    "Advances in Genetic Programming 2",
  publisher =    "MIT Press",
  year =         "1996",
  editor =       "Peter J. Angeline and K. E. {Kinnear, Jr.}", 
  pages =        "177--201",
  chapter =      "9", 
  address =      "Cambridge, MA, USA",
  keywords =     "genetic algorithms, genetic programming",
  ISBN =         "0-262-01158-1",
  URL =          "ftp://ftp.cs.rochester.edu/pub/u/rosca/gp/96.aigp2.dsgp.ps.gz",
  URL =          "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.138.8609",
  URL =          "http://www.cs.uml.edu/~giam/91.510/Papers/RoscaBallard1996.pdf",
  URL =          "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6277495",
  DOI =          "doi:10.7551/mitpress/1109.003.0014",
  bibsource =    "OAI-PMH server at citeseerx.ist.psu.edu",
  language =     "en",
  oai =          "oai:CiteSeerXPSU:10.1.1.138.8609",
  size =         "25 pages",
  abstract =     "A fundamental problem in learning from observation and
                 interaction with an environment is defining a good
                 representation, that is a representation which captures
                 the underlying structure and functionality of the
                 domain. This chapter discusses an extension of the
                 genetic programming (GP) paradigm based on the idea
                 that subroutines obtained from blocks of good
                 representations act as building blocks and may enable a
                 faster evolution of even better representations. This
                 GP extension algorithm is called adaptive
                 representation through learning (ARL). It has built-in
                 mechanisms for (1) creation of new subroutines through
                 discovery and generalization of blocks of code; (2)
                 deletion of subroutines. The set of evolved subroutines
                 extracts common knowledge emerging during the
                 evolutionary process and acquires the necessary
                 structure for solving the problem. ARL was successfully
                 tested on the problem of controlling an agent in a
                 dynamic and non-deterministic environment. Results with
                 the automatic discovery of subroutines show the
                 potential to better scale up the GP technique to
                 complex problems.",
  notes =        "

                 The solutions obtained to a complex typical RL problem
                 using a new GP algorithm, Adaptive Representation
                 through Learning, have increased generality and
                 quality.

                 Population entropy used as decision criterion by ARL.",
}

@InProceedings{Haynes:1997:adskr,
  author =       "Thomas Haynes",
  title =        "On-line Adaptation of Search via Knowledge Reuse",
  booktitle =    "Genetic Programming 1997: Proceedings of the Second
                 Annual Conference",
  editor =       "John R. Koza and Kalyanmoy Deb and Marco Dorigo and
                 David B. Fogel and Max Garzon and Hitoshi Iba and Rick
                 L. Riolo",
  year =         "1997",
  month =        "13-16 " # jul,
  keywords =     "genetic algorithms, genetic programming, distributed
                 search",
  pages =        "156--161",
  address =      "Stanford University, CA, USA",
  publisher_address = "San Francisco, CA, USA",
  publisher =    "Morgan Kaufmann",
  URL =          "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.3381",
  size =         "8 pages",
  abstract =     "We have integrated the distributed search of genetic
                 programming based systems with collective memory to
                 form a collective adaptation search method. Such a
                 system significantly improves search as problem
                 complexity is increased. In collective adaptation,
                 search agents gather knowledge of their environment and
                 deposit it in a central information repository. Process
                 agents are then able to manipulate that focused
                 knowledge, exploiting the exploration of the search
                 agents. Communication is oneway, from the search agents
                 to the process agents. As the process agents are able
                 to refine the knowledge gathered by the search agents,
                 we investigate two-way communication. Such
                 communication directs the genetic programming based
                 engine of the search agents.",
  notes =        "GP-97",
}

@ARTICLE{hornby_alife02_s,
  author = {Gregory S. Hornby and Jordan B. Pollack},
  title = {Creating High-Level Components with a Generative Representation for
    Body-Brain Evolution},
  journal = {Artif. Life},
  year = {2002},
  volume = {8},  
  pages = {223--246},
  number = {3},
  abstract = {One of the main limitations of scalability in body-brain evolution
    systems is the representation chosen for encoding creatures. This
    paper defines a class of representations called generative representations,
    which are identified by their ability to reuse elements of the genotype
    in the translation to the phenotype. This paper presents an example
    of a generative representation for the concurrent evolution of the
    morphology and neural controller of simulated robots, and also introduces
    Genre, an evolutionary system for evolving designs using this representation.
    Applying Genre to the task of evolving robots for locomotion and
    comparing it against a non-generative (direct) representation, shows
    that the generative representation system rapidly produces robots
    with significantly greater fitness. Analysing these results shows
    that the generative representation system achieves better performance
    by capturing useful bias from the design space and by allowing viable
    large scale mutations in the phenotype. Generative representations
    thereby enable the encapsulation, coordination, and reuse of assemblies
    of parts.},
  doi = {doi:10.1162/106454602320991837},
  email = {hornby@email.arc.nasa.gov},
  issn = {1064-5462},
  keywords = {genetic algorithms, genetic programming, generative representations,
    body-brain evolution, L-systems, representation, Lindenmayer systems},
  notes = {genetic variations are repeated if offspring fitness<0.1 parent},
  size = {30 pages},
  timestamp = {2007.03.31},
  url = {http://www.demo.cs.brandeis.edu/papers/hornby_alife02.pdf}
}

@InProceedings{bajurnow:2004:llfegsbisp,
  title =        "Layered Learning for Evolving Goal Scoring Behavior in
                 Soccer Players",
  author =       "Andrei Bajurnow and Vic Ciesielski",
  pages =        "1828--1835",
  booktitle =    "Proceedings of the 2004 IEEE Congress on Evolutionary
                 Computation",
  year =         "2004",
  publisher =    "IEEE Press",
  month =        "20-23 " # jun,
  address =      "Portland, Oregon",
  ISBN =         "0-7803-8515-2",
  URL =          "http://goanna.cs.rmit.edu.au/~vc/papers/cec2004-bajurnow.pdf",
  DOI =          "doi:10.1109/CEC.2004.1331118",
  size =         "8 pages",
  keywords =     "genetic algorithms, genetic programming, Evolutionary
                 intelligent agents, Evolutionary Computation and
                 Games",
  abstract =     "Layered learning allows decomposition of the stages of
                 learning in a problem domain. We apply this technique
                 to the evolution of goal scoring behavior in soccer
                 players and show that layered learning is able to find
                 solutions comparable to standard genetic programs more
                 reliably. The solutions evolved with layers have a
                 higher accuracy but do not make as many goal attempts.
                 We compared three variations of layered learning and
                 find that maintaining the population between layers as
                 the encapsulated learnt layer is introduced to be the
                 most computationally efficient. The quality of
                 solutions found by layered learning did not exceed
                 those of standard genetic programming in terms of goal
                 scoring ability.",
  notes =        "CEC 2004 - A joint meeting of the IEEE, the EPS, and
                 the IEE.",
}



@ARTICLE{2016arXiv161101989B,
   author = {Matej Balog and Alexander L.~Gaunt and Marc
                  Brockschmidt and Sebastian Nowozin and Daniel Tarlow},
  title = {{D}eep{C}oder: {L}earning to Write Programs},
  abstract = {We develop a first line of attack for solving programming
              competition-style problems from input-output examples using
              deep learning. The approach is to train a neural network to
              predict properties of the  program that generated the outputs
              from the inputs. We use the neural network's predictions to
              augment search techniques from the programming languages
              community, including enumerative search and an SMT-based
              solver. Empirically, we show that our approach leads to an
              order of magnitude speedup over the strong non-augmented
              baselines and a Recurrent Neural Network approach, and
              that we are able to solve problems of difficulty comparable
              to the simplest problems on programming competition websites.},
  year = 2016,
  month = {November},
  url = {https://arxiv.org/abs/1611.01989},
  journal =  {arXiv preprint arXiv:1611.01989}
}

@InProceedings{Liskowski:2018:GECCOa,
  author =       "Pawel Liskowski and Iwo Bladek and Krzysztof Krawiec",
  title =        "Neuro-guided genetic programming: prioritizing
                 evolutionary search with neural networks",
  booktitle =    "GECCO '18: Proceedings of the Genetic and Evolutionary
                 Computation Conference",
  year =         "2018",
  editor =       "Hernan Aguirre et al.",
  isbn13 =       "978-1-4503-5618-3",
  pages =        "1143--1150",
  address =      "Kyoto, Japan",
  DOI =          "doi:10.1145/3205455.3205629",
  publisher =    "ACM",
  publisher_address = "New York, NY, USA",
  month =        "15-19 " # jul,
  organisation = "SIGEVO",
  keywords =     "genetic algorithms, genetic programming, ANN",
  abstract =     "When search operators in genetic programming (GP)
                 insert new instructions into programs, they usually
                 draw them uniformly from the available instruction set.
                 Preferring some instructions to others would require
                 additional domain knowledge, which is typically
                 unavailable. However, it has been recently demonstrated
                 that the likelihoods of instructions occurrence in a
                 program can be reasonably well estimated from its
                 input-output behaviour using a neural network. We
                 exploit this idea to bias the choice of instructions
                 used by search operators in GP. Given a large sample of
                 programs and their input-output behaviours, a neural
                 network is trained to predict the presence of
                 individual instructions. When applied to a new program
                 synthesis task, the network is first queried on the set
                 of examples that define the task, and the obtained
                 probabilities determine the frequencies of using
                 instructions in initialization and mutation operators.
                 This priming leads to significant improvements of the
                 odds of successful synthesis on a range of
                 benchmarks.",
  notes =        "Also known as \cite{3205629} GECCO-2018 A
                 Recombination of the 27th International Conference on
                 Genetic Algorithms (ICGA-2018) and the 23rd Annual
                 Genetic Programming Conference (GP-2018)",
}

@Article{Pawlak:2014:ieeeEC,
  author =       "Tomasz P. Pawlak and Bartosz Wieloch and Krzysztof
                 Krawiec",
  title =        "Semantic Backpropagation for Designing Search
                 Operators in Genetic Programming",
  journal =      "IEEE Transactions on Evolutionary Computation",
  year =         "2015",
  volume =       "19",
  number =       "3",
  pages =        "326--340",
  month =        jun,
  keywords =     "genetic algorithms, genetic programming, program
                 synthesis, semantics, reversible computing, problem
                 decomposition, mutation, geometric crossover",
  URL =          "http://dx.doi.org/10.1109/TEVC.2014.2321259",
  DOI =          "doi:10.1109/TEVC.2014.2321259",
  URL =          "http://www.cs.put.poznan.pl/tpawlak/?Semantic%20Backpropagation%20for%20Designing%20Search%20Operators%20in%20Genetic%20Programming,16",
  appendix_url = "http://www.cs.put.poznan.pl/tpawlak/files/research/2013SemanticBackpropagation/2013IEEE_Appendix.    pdf",            
  ISSN =         "1089-778X",
  abstract =     "In genetic programming, a search algorithm is expected
                 to produce a program that achieves the desired final
                 computation state (desired output). To reach that
                 state, an executing program needs to traverse certain
                 intermediate computation states. An evolutionary search
                 process is expected to autonomously discover such
                 states. This can be difficult for nontrivial tasks that
                 require long programs to be solved. The semantic
                 back-propagation algorithm proposed in this paper
                 heuristically inverts the execution of evolving
                 programs to determine the desired intermediate
                 computation states. Two search operators, Random
                 Desired Operator and Approximately Geometric Semantic
                 Crossover, use the intermediate states determined by
                 semantic backpropagation to define subtasks of the
                 original programming task, which are then solved using
                 an exhaustive search. The operators outperform the
                 standard genetic search operators and other
                 semantic-aware operators when compared on a suite of
                 symbolic regression and Boolean benchmarks. This result
                 and additional analysis conducted in this study
                 indicate that semantic back propagation helps evolution
                 at identifying the desired intermediate computation
                 states, and makes the search process more efficient.",
  notes =        "Java source code
                 http://www.cs.put.poznan.pl/tpawlak/files/research/2013SemanticBackpropagation/Evolution-src.zip
                 Also known as \cite{6808504}",
}


@Article{wolpert:1997:nflto,
  author =       {David H. Wolpert and William G. Macready}, 
  title =        {No Free Lunch Theorems for Optimization}, 
  journal =      {{IEEE} {T}rans. on {E}volutionary {C}omputation},
  year =         1997,
  volume =       1,
  number =       1,
  pages =        {67--82}
}

@article{DEAP_JMLR2012,
    author    = " F\'elix-Antoine Fortin and Fran\c{c}ois-Michel {De Rainville} and Marc-Andr\'e Gardner and Marc Parizeau and Christian Gagn\'e ",
    title     = { {DEAP}: Evolutionary Algorithms Made Easy },
    pages     = { 2171--2175 },
    volume    = { 13 },
    month     = { jul },
    year      = { 2012 },
    journal   = { Journal of Machine Learning Research }
}

 @article{Biggio_Bendinelli_Neitz_Lucchi_Parascandolo_2021, 
title={Neural Symbolic Regression that Scales}, url={http://arxiv.org/abs/2106.06427}, DOI={10.48550/arXiv.2106.06427}, abstractNote={Symbolic equations are at the core of scientific discovery. The task of discovering the underlying equation from a set of input-output pairs is called symbolic regression. Traditionally, symbolic regression methods use hand-designed strategies that do not improve with experience. In this paper, we introduce the first symbolic regression method that leverages large scale pre-training. We procedurally generate an unbounded set of equations, and simultaneously pre-train a Transformer to predict the symbolic equation from a corresponding set of input-output-pairs. At test time, we query the model on a new set of points and use its output to guide the search for the equation. We show empirically that this approach can re-discover a set of well-known physical equations, and that it improves over time with more data and compute.}, note={arXiv:2106.06427 [cs]}, number={arXiv:2106.06427}, publisher={arXiv}, author={Biggio, Luca and Bendinelli, Tommaso and Neitz, Alexander and Lucchi, Aurelien and Parascandolo, Giambattista}, year={2021}, month=jun }


@Article{Vanneschi:2014:GPEM,
  author =       "Leonardo Vanneschi and Mauro Castelli and Sara Silva",
  title =        "A survey of semantic methods in genetic programming",
  journal =      "Genetic Programming and Evolvable Machines",
  year =         "2014",
  volume =       "15",
  number =       "2",
  pages =        "195--214",
  month =        jun,
  keywords =     "genetic algorithms, genetic programming, Semantics,
                 Genotype/phenotype",
  ISSN =         "1389-2576",
  URL =          "http://link.springer.com/article/10.1007/s10710-013-9210-0",
  DOI =          "doi:10.1007/s10710-013-9210-0",
  size =         "20 pages",
  abstract =     "Several methods to incorporate semantic awareness in
                 genetic programming have been proposed in the last few
                 years. These methods cover fundamental parts of the
                 evolutionary process: from the population
                 initialisation, through different ways of modifying or
                 extending the existing genetic operators, to formal
                 methods, until the definition of completely new genetic
                 operators. The objectives are also distinct: from the
                 maintenance of semantic diversity to the study of
                 semantic locality; from the use of semantics for
                 constructing solutions which obey certain constraints
                 to the exploitation of the geometry of the semantic
                 topological space aimed at defining easy-to-search
                 fitness landscapes. All these approaches have shown, in
                 different ways and amounts, that incorporating semantic
                 awareness may help improving the power of genetic
                 programming. This survey analyses and discusses the
                 state of the art in the field, organising the existing
                 methods into different categories. It restricts itself
                 to studies where semantics is intended as the set of
                 output values of a program on the training data, a
                 definition that is common to a rather large set of
                 recent contributions. It does not discuss methods for
                 incorporating semantic information into grammar-based
                 genetic programming or approaches based on formal
                 methods. The objective is keeping the community updated
                 on this interesting research track, hoping to motivate
                 new and stimulating contributions.",
}

@InProceedings{conf/ppsn/MoraglioKJ12,
  author =       "Alberto Moraglio and Krzysztof Krawiec and Colin G.
                 Johnson",
  title =        "Geometric Semantic Genetic Programming",
  booktitle =    "Parallel Problem Solving from Nature, PPSN XII (part
                 1)",
  year =         "2012", 
  editor =       "Carlos A. {Coello Coello} and Vincenzo Cutello and
                 Kalyanmoy Deb and Stephanie Forrest and Giuseppe
                 Nicosia and Mario Pavone",
  volume =       "7491",
  series =       "Lecture Notes in Computer Science",
  pages =        "21--31",
  address =      "Taormina, Italy",
  month =        sep # " 1-5",
  publisher =    "Springer",
  keywords =     "genetic algorithms, genetic programming",
  isbn13 =       "978-3-642-32936-4",
  DOI =          "doi:10.1007/978-3-642-32937-1_3",
  size =         "11 pages",
  abstract =     "Traditional Genetic Programming (GP) searches the
                 space of functions/programs by using search operators
                 that manipulate their syntactic representation,
                 regardless of their actual semantics/behaviour.
                 Recently, semantically aware search operators have been
                 shown to outperform purely syntactic operators. In this
                 work, using a formal geometric view on search operators
                 and representations, we bring the semantic approach to
                 its extreme consequences and introduce a novel form of
                 GP, Geometric Semantic GP (GSGP), that searches
                 directly the space of the underlying semantics of the
                 programs. This perspective provides new insights on the
                 relation between program syntax and semantics, search
                 operators and fitness landscape, and allows for
                 principled formal design of semantic search operators
                 for different classes of problems. We derive specific
                 forms of GSGP for a number of classic GP domains and
                 experimentally demonstrate their superiority to
                 conventional operators.",
  bibsource =    "DBLP, http://dblp.uni-trier.de",
  affiliation =  "School of Computer Science, University of Birmingham,
                 UK",
} 

@article{10.7717/peerj-cs.103,
     title = {SymPy: symbolic computing in Python},
     author = {Meurer, Aaron et al.},
     year = 2017,
     month = jan,
     keywords = {Python, Computer algebra system, Symbolics},
     abstract = {
                SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.
             },
     volume = 3,
     pages = {e103},
     journal = {PeerJ Computer Science},
     issn = {2376-5992},
     url = {https://doi.org/10.7717/peerj-cs.103},
     doi = {10.7717/peerj-cs.103}
    }

@article{Kamienny_d’Ascoli_Lample_Charton_2022, 
title={End-to-end symbolic regression with transformers}, url={http://arxiv.org/abs/2204.10532}, abstractNote={Symbolic regression, the task of predicting the mathematical expression of a function from the observation of its values, is a difficult task which usually involves a two-step procedure: predicting the “skeleton” of the expression up to the choice of numerical constants, then fitting the constants by optimizing a non-convex loss function. The dominant approach is genetic programming, which evolves candidates by iterating this subroutine a large number of times. Neural networks have recently been tasked to predict the correct skeleton in a single try, but remain much less powerful. In this paper, we challenge this two-step procedure, and task a Transformer to directly predict the full mathematical expression, constants included. One can subsequently refine the predicted constants by feeding them to the non-convex optimizer as an informed initialization. We present ablations to show that this end-to-end approach yields better results, sometimes even without the refinement step. We evaluate our model on problems from the SRBench benchmark and show that our model approaches the performance of state-of-the-art genetic programming with several orders of magnitude faster inference.}, note={arXiv: 2204.10532}, journal={arXiv:2204.10532 [cs]}, author={Kamienny, Pierre-Alexandre and d’Ascoli, Stéphane and Lample, Guillaume and Charton, François}, year={2022}, month=apr }

@article{Garcez_Lamb_2020, title={Neurosymbolic AI: The 3rd Wave}, url={http://arxiv.org/abs/2012.05876}, DOI={10.48550/arXiv.2012.05876}, abstractNote={Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.}, note={arXiv:2012.05876 [cs]}, number={arXiv:2012.05876}, publisher={arXiv}, author={Garcez, Artur d’Avila and Lamb, Luis C.}, year={2020}, month=dec }


@article{Paliwal_Loos_Rabe_Bansal_Szegedy_2019, 
title={Graph Representations for Higher-Order Logic and Theorem Proving}, url={http://arxiv.org/abs/1905.10006}, abstractNote={This paper presents the first use of graph neural networks (GNNs) for higher-order proof search and demonstrates that GNNs can improve upon state-of-the-art results in this domain. Interactive, higher-order theorem provers allow for the formalization of most mathematical theories and have been shown to pose a significant challenge for deep learning. Higher-order logic is highly expressive and, even though it is well-structured with a clearly defined grammar and semantics, there still remains no well-established method to convert formulas into graph-based representations. In this paper, we consider several graphical representations of higher-order logic and evaluate them against the HOList benchmark for higher-order theorem proving.}, note={arXiv: 1905.10006}, journal={arXiv:1905.10006 [cs, stat]}, author={Paliwal, Aditya and Loos, Sarah and Rabe, Markus and Bansal, Kshitij and Szegedy, Christian}, year={2019}, month=sep }



@InProceedings{Liskowski:2020:GECCO,
  author =       "Pawel Liskowski and Krzysztof Krawiec and Nihat Engin
                 Toklu and Jerry Swan",
  title =        "Program Synthesis as Latent Continuous Optimization:
                 Evolutionary Search in Neural Embeddings",
  year =         "2020",
  editor =       "Carlos Artemio {Coello Coello} et al.",
  isbn13 =       "9781450371285",
  publisher =    "Association for Computing Machinery",
  publisher_address = "New York, NY, USA",
  URL =          "https://doi.org/10.1145/3377930.3390213",
  DOI =          "doi:10.1145/3377930.3390213",
  booktitle =    "Proceedings of the 2020 Genetic and Evolutionary
                 Computation Conference",
  pages =        "359--367",
  size =         "9 pages",
  keywords =     "genetic algorithms, genetic programming, autoencoders,
                 deep learning, embedding, program synthesis",
  address =      "internet",
  series =       "GECCO '20",
  month =        jul # " 8-12",
  organisation = "SIGEVO",
  abstract =     "In optimization and machine learning, the divide
                 between discrete and continuous problems and methods is
                 deep and persistent. We attempt to remove this
                 distinction by training neural network autoencoders
                 that embed discrete candidate solutions in continuous
                 latent spaces. This allows us to take advantage of
                 state-of-the-art continuous optimization methods for
                 solving discrete optimization problems, and mitigates
                 certain challenges in discrete optimization, such as
                 design of bias-free search operators. In the
                 experimental part, we consider program synthesis as the
                 special case of combinatorial optimization. We train an
                 autoencoder network on a large sample of programs in a
                 problem-agnostic, unsupervised manner, and then use it
                 with an evolutionary continuous optimization algorithm
                 (CMA-ES) to map the points from the latent space to
                 programs. We propose also a variant in which
                 semantically similar programs are more likely to have
                 similar embeddings. Assessment on a range of benchmarks
                 in two domains indicates the viability of this approach
                 and the usefulness of involving program semantics.",
  notes =        "Also known as \cite{10.1145/3377930.3390213}
                 GECCO-2020 A Recombination of the 29th International
                 Conference on Genetic Algorithms (ICGA) and the 25th
                 Annual Genetic Programming Conference (GP)",
}


@inproceedings{10.1007/978-3-030-58112-1_43,
author = {Liskowski, Pawe\l{} and Krawiec, Krzysztof and Toklu, Nihat Engin},
title = {Neuromemetic Evolutionary Optimization},
year = {2020},
isbn = {978-3-030-58111-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58112-1_43},
doi = {10.1007/978-3-030-58112-1_43},
abstract = {Discrete and combinatorial optimization can be notoriously difficult due to complex and rugged characteristics of the objective function. We address this challenge by mapping the search process to a continuous space using recurrent neural networks. Alongside with an evolutionary run, we learn three mappings: from the original search space to a continuous Cartesian latent space, from that latent space back to the search space, and from the latent space to the search objective. We elicit gradient from that last network and use it to perform moves in the latent space, and apply this Neuromemetic Evolutionary Optimization (NEO) to evolutionary synthesis of programs. Evaluation on a range of benchmarks suggests that NEO significantly outperforms conventional genetic programming.},
booktitle = {Parallel Problem Solving from Nature – PPSN XVI: 16th International Conference, PPSN 2020, Leiden, The Netherlands, September 5-9, 2020, Proceedings, Part I},
pages = {623–636},
numpages = {14},
keywords = {Optimization, Neural networks, Program synthesis},
location = {Leiden, The Netherlands}
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{velivckovic2018graph,
  title={Graph Attention Networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units (elus)},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}


@article{Udrescu_Tegmark_2020, title={AI Feynman: A physics-inspired method for symbolic regression}, volume={6}, rights={Copyright © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.}, ISSN={2375-2548}, url={https://advances.sciencemag.org/content/6/16/eaay2631}, DOI={10.1126/sciadv.aay2631}, abstractNote={A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.
Our physics-inspired algorithm for symbolic regression is able to discover complex physics equations from mere tables of numbers.
Our physics-inspired algorithm for symbolic regression is able to discover complex physics equations from mere tables of numbers.}, number={16}, journal={Science Advances}, publisher={American Association for the Advancement of Science}, author={Udrescu, Silviu-Marian and Tegmark, Max}, year={2020}, month=apr, pages={eaay2631}, language={en} }


@article{Udrescu_Tan_Feng_Neto_Wu_Tegmark_2020, title={AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity}, url={http://arxiv.org/abs/2006.10782}, abstractNote={We present an improved method for symbolic regression that seeks to fit data to formulas that are Pareto-optimal, in the sense of having the best accuracy for a given complexity. It improves on the previous state-of-the-art by typically being orders of magnitude more robust toward noise and bad data, and also by discovering many formulas that stumped previous methods. We develop a method for discovering generalized symmetries (arbitrary modularity in the computational graph of a formula) from gradient properties of a neural network fit. We use normalizing flows to generalize our symbolic regression method to probability distributions from which we only have samples, and employ statistical hypothesis testing to accelerate robust brute-force search.}, note={arXiv: 2006.10782}, journal={arXiv:2006.10782 [physics, stat]}, author={Udrescu, Silviu-Marian and Tan, Andrew and Feng, Jiahai and Neto, Orisvaldo and Wu, Tailin and Tegmark, Max}, year={2020}, month=jun }

@book{NSAI2022,
	title = {Neuro-Symbolic Artificial Intelligence - The State of the Art},
	series = {Frontiers in Artificial Intelligence and Applications},
	number = {342},
	year = {2022},
	publisher = {IOS Press},
	organization = {IOS Press},
	address = {Amsterdam},
	url = {https://www.iospress.com/catalog/books/neuro-symbolic-artificial-intelligence-the-state-of-the-art},
	author = {Pascal Hitzler and Md Kamruzzaman Sarker}
}

@book{DBLP:series/sbcs/ShakarianBSXP23,
  author       = {Paulo Shakarian and
                  Chitta Baral and
                  Gerardo I. Simari and
                  Bowen Xi and
                  Lahari Pokala},
  title        = {Neuro Symbolic Reasoning and Learning},
  series       = {Springer Briefs in Computer Science},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-39179-8},
  doi          = {10.1007/978-3-031-39179-8},
  isbn         = {978-3-031-39178-1},
  timestamp    = {Tue, 10 Oct 2023 18:04:18 +0200},
  biburl       = {https://dblp.org/rec/series/sbcs/ShakarianBSXP23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@Book{koza:book,
  author =       "John R. Koza",
  title =        "Genetic Programming: On the Programming of Computers
                 by Means of Natural Selection",
  year =         "1992",
  publisher =    "MIT Press",
  address =      "Cambridge, MA, USA",
  keywords =     "genetic algorithms, genetic programming, text book",
  ISBN =         "0-262-11170-5",
  URL =          "http://mitpress.mit.edu/books/genetic-programming",
  video_url =    "https://youtu.be/tTMpKrKkYXo",
  video_url =    "http://www.human-competitive.org/sites/default/files/video/genetic_programming_the_movie_part_1.mp4",
  video_url =    "http://gpbib.cs.ucl.ac.uk/koza_videos/genetic_programming_the_movie_part_1.mp4",
  size =         "xiv+819 pages"
}


@inproceedings{10.1145/3638530.3654277,
author = {Wyrwi\'{n}ski, Piotr and Krawiec, Krzysztof},
title = {Guiding Genetic Programming with Graph Neural Networks},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3654277},
doi = {10.1145/3638530.3654277},
abstract = {In evolutionary computation, it is commonly assumed that a search algorithm acquires knowledge about a     problem instance by sampling solutions from the search space and evaluating them with a fitness function. This is      necessarily inefficient because fitness reveals very little about solutions - yet they contain more information that   can be potentially exploited. To address this observation in genetic programming, we propose EvoNUDGE, which uses a    graph neural network to elicit additional knowledge from symbolic regression problems. The network is queried on the   problem before an evolutionary run to produce a library of subprograms, which is subsequently used to seed the initial population and bias the actions of search operators. In an extensive experiment on a large number of problem           instances, EvoNUDGE is shown to significantly outperform multiple baselines, including the conventional tree-based     genetic programming and the purely neural variant of the method.}, 
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {551–554}, 
numpages = {4},
keywords = {genetic programming, symbolic regression, graph neural networks},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}   