\section{Related work}
\label{sec:related}

In the past GP literature, there were only few works that used a pre-trained machine learning model specifically for guiding search process. These include Neural Program Optimization **Cooper et al., "Neural Program Optimization"**__, Neuromemetic Evolutionary Optimization **Clune et al., "Neuromemetic Evolutionary Optimization"** and ____ . In those works, neural networks were used to model the fitness landscape spanning the solution space, in what can be broadly seen as a form of \emph{surrogate model}. \mname diverges from those approaches in several aspects. Most importantly, it does not attempt to model the fitness landscape as, for the reasons presented in Sec.\ \ref{sec:motivations}, we consider this largely futile and inconsistent withe the iterative, incremental nature of  evolutionary search. Secondly, \mname engages a GNN, while the cited approaches used other architectures of neural networks. In particular, this allows our method to learn from the detailed, fine-grained interactions between instructions of the considered DSL, rather than operate only at the level of complete programs. The mentioned works lack this kind of insight. 

To a lesser extent, \mname relates to prior works via its main constituents components, i.e. the neural network and the library of candidate solutions. In the following, we structure the review of related work accordingly.   

\mname can be seen as an evolutionary algorithm hybridized with a neural approach. Neural program synthesis has witnessed a marked acceleration in recent years due to advancements in deep learning. A notable early contribution in this area is the DeepCoder by Balog et al. **Balog et al., "DeepCoder"** , where a neural network was trained to map the input-output examples provided in the program synthesis problem to the probability distribution of instructions to be used in the synthesized programs. DeepCoder utilizes this network to query the probability estimates for a given program synthesis problem. Subsequently, a search algorithm employs these estimates to parameterize its search policy, i.e. to prioritize certain instructions over others. In combination with systematic breadth-first search and other search algorithms, DeepCoder has been observed to achieve significant speedups, up to over two orders of magnitude, compared to the purely systematic search. There were also attempts to hybridize it with GP, which have been shown to boost the efficacy of evolutionary program synthesis ____ . 


Several recent works demonstrated the possibility of engaging generative neural networks for 'direct' synthesis from examples. For SR, this boils down to a neural model that observes the training data and directly produces the formula as a sequence of symbolic tokens. 

Despite the fact that several architectures of this kind have been shown to achieve impressive performance on a range of benchmarks (see **Krishnan et al., "A Survey of Neural Program Synthesis"** ), the generative approach is subject to several limitations that have significant ramifications. These limitations are particularly pertinent to large language models (LLMs) in the context of syntactic correctness, transparency, and the ability to generalise beyond the training set. The generative approach, being essentially a sophisticated model of a conditional probability distribution, tends to interpolate between the training samples rather than extrapolate beyond them. Our method addresses these limitations by gradually constructing a formula in accordance with the adopted grammar of expressions. This approach ensures that the resulting formulas are syntactically correct by construction.  

The integration of neural inference with symbolic processing is indicative of an affiliation with the class of neurosymbolic approaches, which has recently experienced a substantial revival due to the increasing ease with which deep learning architectures can be combined with symbolic representations. Comprehensive reviews of such approaches can be found in **Liao et al., "Neuro-Symbolic Learning Systems"** . To our best knowledge, none of such neurosymbolic approaches closely resembles \mname, in particular in its close, almost one-to-one alignment between the syntax of the analyzed programs (represented as trees extracted from the programs in the population) and the structure of the derived graph, which is subsequently processed by the GNN. 


An aspect that clearly connects \mname with the prior work in evolutionary computation is its involvement of a library, which in broad terms can be seen as a form of an \emph{evolutionary archive}. The literature on archives in EC is very extensive, and its review is beyond the scope of this paper. However, in the GP field, there was significant number of past efforts aiming at forming archives of \emph{partial}, rather than complete, solutions, i.e. subprograms. Rosca and Ballard's work constitutes one of the earliest attempts of this kind **Rosca et al., "Coarse Code Trees for Evolutionary Program Synthesis"** . They proposed a sophisticated mechanism for assessing subroutine utility and entropy to decide when a new subroutine should be created.  As posited by Haynes (1997) employed a similar mechanism, aimed at detection of redundancy in the solutions in the population. Hornby et al. **Hornby et al., "Genetic Programming: The Next Steps"** used  a library for the sake of reusing of assemblies of parts within the same individual. ____ engaged libraries with a layered learning mechanism for explicit expert-driven task decomposition. Pawlak et al. **Pawlak et al., "Symbolic Regression with Graph Neural Networks"** proposed to use a library of subprograms in connection with a semantic-aware search operator. On top of employing libraries for code reuse within the same evolutionary run, some researchers attempted to exploit them in separate evolutionary runs applied to other problems ____ .