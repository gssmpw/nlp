\section{Related Work}
\subsection{Vision Language Models}
In recent years, Vision-Language Models (VLMs) have significantly advanced the integration of visual and textual data, leading to more sophisticated AI applications. A notable example is CLIP,  which employs contrastive learning to align images and text in a shared latent space, enabling zero-shot image classification and cross-modal retrieval \citep{pmlr-v139-radford21a}. Building upon such foundations, Large Vision Language Models (LVLMs) like GPT-4 have extended capabilities to process both textual and visual inputs, enhancing tasks such as image description and visual question answering \citep{gpt4v}. Similarly, Anthropic's Claude 3.5 has been developed to handle multimodal inputs, contributing to advancements in understanding and generating content across different modalities \citep{Claude3.5}. Further contributions include LLaVA, which integrates visual features into language models to improve visual reasoning \citep{liu2024visual}, and Qwen-VL, which supports multilingual conversations and end-to-end text recognition in images \citep{bai2023qwen}. Additionally, DeepSeek-VL2 has been recognized for its performance in visual understanding benchmarks, demonstrating the rapid progress in this field \citep{wu2024deepseek}. Collectively, these models represent significant contributions in combining image and text, paving the way for comprehensive AI systems. 
%CLIP \citep{pmlr-v139-radford21a}; GPT4 series \citep{gpt4v}; Claude 3.5 \citep{Claude3.5}; LLaVA \citep{liu2024visual}; Qwen-VL \citep{bai2023qwen}; DeepSeekVL2 \cite{wu2024deepseek}.

\subsection{Copyright Issues Related to Generative Models.}
The rapid advancement of generative AI enables the creation of text and images that closely mimic human-authored works, leading to significant legal and ethical concerns regarding potential infringements of intellectual property rights \cite{zirpoli2023generative, dzuong2024uncertain, sag2023copyright, polandgenerative}. A key contributing factor is that visual generative models may memorize portions of their training data, resulting in outputs that inadvertently reproduce IP-protected content \cite{carlini2023extracting, somepalli2023diffusion, gu2023memorization}. To mitigate IP infringement, two primary approaches have emerged:

\begin{itemize}
    \item Reducing Memorization During Training: Implementing differential privacy \cite{dwork2014algorithmic} techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content \cite{abadi2016deep, chen2022dpgen, dockhorn2022differentially}.
    \item Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output\cite{wang2024evaluating, he2024fantastic}, or optimizing unsafe prompts \cite{chin2023prompting4debugging, rando2022red}, thereby avoiding the inclusion of IP-protected material.
\end{itemize}

Despite the widespread copyright concerns surrounding generative AI and the numerous IP mitigation approaches recently proposed, the issue of benchmarking VLM IP infringement detection remains largely underexplored. As a result, a primary focus of our paper is to address the capabilities of VLMs in detecting and mitigating IP infringement.

\subsection{In-context Learning}
In-context learning, as discussed in \cite{mann2020language, dong2022survey}, is a paradigm where large language models (LLMs) perform tasks by conditioning on a prompt that includes a few examples, enabling them to adapt to new tasks without explicit parameter updates. An in-context learning prompt generally includes two main parts: demonstrations and a new query. Demonstrations consist of several question-answer examples, each providing a full question along with its corresponding answer. The new query is a fresh question presented to the model for response. What's more, recent studies \cite{zhou2024visual,monajatipoor2023metavl,li2024visual, zhang2023makes} have demonstrated that vision-language models (VLMs) can also effectively facilitate in-context learning: Given a few images, or masks as examples, the VLMs could perform segmentation, classification, and visual question answering (VQA) \cite{antol2015vqa} in effective ways. 

%