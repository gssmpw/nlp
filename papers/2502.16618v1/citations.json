[
  {
    "index": 0,
    "papers": [
      {
        "key": "pmlr-v139-radford21a",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "gpt4v",
        "author": "GPT4V",
        "title": "{GPT-4V(ision) system card}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Claude3.5",
        "author": "Claude3.5",
        "title": "{Claude3.5}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wu2024deepseek",
        "author": "Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and others",
        "title": "Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "pmlr-v139-radford21a",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "gpt4v",
        "author": "GPT4V",
        "title": "{GPT-4V(ision) system card}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Claude3.5",
        "author": "Claude3.5",
        "title": "{Claude3.5}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wu2024deepseek",
        "author": "Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and others",
        "title": "Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zirpoli2023generative",
        "author": "Zirpoli, Christopher T",
        "title": "Generative artificial intelligence and copyright law"
      },
      {
        "key": "dzuong2024uncertain",
        "author": "Dzuong, Jocelyn and Wang, Zichong and Zhang, Wenbin",
        "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI"
      },
      {
        "key": "sag2023copyright",
        "author": "Sag, Matthew",
        "title": "Copyright safety for generative ai"
      },
      {
        "key": "polandgenerative",
        "author": "Poland, CM",
        "title": "Generative AI and US Intellectual Property Law. ArXiv. 2023"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "carlini2023extracting",
        "author": "Carlini, Nicolas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramer, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric",
        "title": "Extracting training data from diffusion models"
      },
      {
        "key": "somepalli2023diffusion",
        "author": "Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom",
        "title": "Diffusion art or digital forgery? investigating data replication in diffusion models"
      },
      {
        "key": "gu2023memorization",
        "author": "Gu, Xiangming and Du, Chao and Pang, Tianyu and Li, Chongxuan and Lin, Min and Wang, Ye",
        "title": "On memorization in diffusion models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dwork2014algorithmic",
        "author": "Dwork, Cynthia and Roth, Aaron and others",
        "title": "The algorithmic foundations of differential privacy"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "abadi2016deep",
        "author": "Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li",
        "title": "Deep learning with differential privacy"
      },
      {
        "key": "chen2022dpgen",
        "author": "Chen, Jia-Wei and Yu, Chia-Mu and Kao, Ching-Chia and Pang, Tzai-Wei and Lu, Chun-Shien",
        "title": "Dpgen: Differentially private generative energy-guided network for natural image synthesis"
      },
      {
        "key": "dockhorn2022differentially",
        "author": "Dockhorn, Tim and Cao, Tianshi and Vahdat, Arash and Kreis, Karsten",
        "title": "{Differentially Private Diffusion Models}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wang2024evaluating",
        "author": "Wang, Zhenting and Chen, Chen and Sehwag, Vikash and Pan, Minzhou and Lyu, Lingjuan",
        "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI"
      },
      {
        "key": "he2024fantastic",
        "author": "He, Luxi and Huang, Yangsibo and Shi, Weijia and Xie, Tinghao and Liu, Haotian and Wang, Yue and Zettlemoyer, Luke and Zhang, Chiyuan and Chen, Danqi and Henderson, Peter",
        "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chin2023prompting4debugging",
        "author": "Chin, Zhi-Yi and Jiang, Chieh-Ming and Huang, Ching-Chun and Chen, Pin-Yu and Chiu, Wei-Chen",
        "title": "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts"
      },
      {
        "key": "rando2022red",
        "author": "Rando, Javier and Paleka, Daniel and Lindner, David and Heim, Lennart and Tram{\\`e}r, Florian",
        "title": "Red-teaming the stable diffusion safety filter"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "mann2020language",
        "author": "Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "dong2022survey",
        "author": "Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang",
        "title": "A survey on in-context learning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhou2024visual",
        "author": "Zhou, Yucheng and Li, Xiang and Wang, Qianning and Shen, Jianbing",
        "title": "Visual in-context learning for large vision-language models"
      },
      {
        "key": "monajatipoor2023metavl",
        "author": "Monajatipoor, Masoud and Li, Liunian Harold and Rouhsedaghat, Mozhdeh and Yang, Lin F and Chang, Kai-Wei",
        "title": "Metavl: Transferring in-context learning ability from language models to vision-language models"
      },
      {
        "key": "li2024visual",
        "author": "Li, Feng and Jiang, Qing and Zhang, Hao and Ren, Tianhe and Liu, Shilong and Zou, Xueyan and Xu, Huaizhe and Li, Hongyang and Yang, Jianwei and Li, Chunyuan and others",
        "title": "Visual in-context prompting"
      },
      {
        "key": "zhang2023makes",
        "author": "Zhang, Yuanhan and Zhou, Kaiyang and Liu, Ziwei",
        "title": "What makes good examples for visual in-context learning?"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "antol2015vqa",
        "author": "Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi",
        "title": "Vqa: Visual question answering"
      }
    ]
  }
]