%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig-mogs-as-dr-samplers-comparison.pdf}
    \caption{Inferred MoG posterior heatmaps and the induced domain samples when each MoG is used for DR. 
    MoG component means are displayed in blue crosses and colorbar quantifies likelihood.}
    \label{fig:mog-posterior-for-dr}
\end{figure}

%%%%%%%%%%
\subsection{Different Physical Parameterisations in LFI \& DR}
\label{subsec:real2sim-res--lfi}
%%%%%%%%%%

Figure~\ref{fig:mog-posterior-for-dr} illustrates our Real2Sim performance using 2D MoG posterior heatmaps, along with the respective scatter plots of the $12$ resulting domain samples. Each MoG has $4$ components, whose mean, variance, and mixture coefficient are parameterised during inference. The tightness and spread of the posteriors serve as a qualitative indication of the precision of the inference. The crosses that represent the means of different Gaussians capture alternative hypotheses of the observed DLO's parameterisation. 

We see that for our set of DLOs, BayesSim-RKHS \textbf{correctly classifies the varied softness} (Young's modulus); however, it \textbf{struggles to cleanly classify the varied dimension} (length). This is indicated by the MoG variance across each parameter's axis and the respective spread of the component means, which inherently encode any uncertainty about the parameter estimation.

%%%%%%%%%%
\subsection{Uncertainty over Parameter Estimation \& DR}
\label{subsec:real2sim-res--dr}
%%%%%%%%%%

Following the above property of MoG posteriors, we observe how \textbf{the relative certainty (\emph{sharpness}) of a posterior} along the dimension of a variable \textbf{results in proportionately tightly clustered domain samples} along this dimension. Thus we see domain samples being mainly spread along the less certain -- length -- posterior dimension. This is particularly visible for the notably similar $\text{200mm}$ and $\text{290mm}$ shore $\text{00-20}$ DLO posteriors. 

For the $\text{200mm; A-40}$ and $\text{270mm; 00-50}$ DLOs, we see some \emph{alternative hypotheses} (component means) for the DLO parameterisation that also contribute to our low-variance sampling, due to the relative magnitude of the respective mixture coefficients. This contribution is evident by the spread of the respective domain samples.

%%%%%%%%%%
\subsection{Object-centric Agent Performance}
\label{subsec:policy-learn-sim2real-res}
%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig-all_agents_learning_curves.png}
    \caption{Learning curves of PPO agent training when performing DR using different domain distributions.}
    \label{fig:rew-per-train-iter}
\end{figure}

We train (Fig.~\ref{fig:rew-per-train-iter}) $4$ PPO policies using our $4$ inferred MoG posteriors (Fig.~\ref{fig:mog-posterior-for-dr}) for domain randomisation. We also train a policy by performing DR over a uniform distribution ($\text{PPO-}\mathit{U}$) and a policy which assumes that the simulated DLO is parameterised according to the median of our parameter space ($\text{PPO-}\mu$). We evaluate these $6$ policies in each one of our $4$ real-world DLOs, repeating each evaluation $4$ times for a total of $96$ sim2real policy deployments. For each sim2real experiment, we compute the mean trajectories resulting from the accumulation of commanded EEF position deltas (commanded \emph{actions}) over $4$ repetitions.

These mean trajectories are presented in Fig.~\ref{fig:eef-deltas-per-eval-steps}, with shaded regions showing the standard deviation between different repetitions. We observe many interesting \textbf{motion patterns, which indicate adaptation of agent performance to the physical parameterisation of the respective evaluation DLO}. An indicative subset of these observations is extrinsically corroborated by Fig.~\ref{fig:extrinsic-eval-timelapse}. The following observations are highlighted in \textcolor{olive}{\textbf{olive}} annotations.

We see the $\text{MoG}_{\text{200mm; A-40}}$ policy ($\text{PPO-}0$, plotted on \textcolor{red}{\textbf{red}}) that shows the tightest ``roaming pattern'' \textcolor{olive}{\textbf{(1)}} to maximise the stiffer and shorter $\text{200mm; A-40}$ DLO reward close to the target. 
We also see $\text{PPO-}0$ following the same first half of the trajectory \textcolor{olive}{\textbf{(2)}} for the softest $\text{00-20}$ DLOs. 
Similarly, the $\text{MoG}_{\text{200mm; 00-20}}$ policy ($\text{PPO-}1$, plotted on \textcolor{green}{\textbf{green}}), follows a very similar first third of the trajectory \textcolor{olive}{\textbf{(3)}} for the softer $\text{00-20}$ and $\text{00-50}$ DLOs, with some material-related variance, and then attempts a similar ``loop pattern'' for the last $6$ reaching steps with the softest $\text{00-20}$ DLOs \textcolor{olive}{\textbf{(4)}}. 
In a related adaptation, we see that although both $\text{PPO-}1$ and $\text{MoG}_{\text{290mm; 00-20}}$ ($\text{PPO-}3$, plotted on \textcolor{cyan}{\textbf{cyan}}) policies follow a similar pattern for the latter half of the trajectory for the shorter and stiffer $\text{200mm; A-40}$ DLO, the PPO-1 pattern looks cleaner, due to being inherently conditioned on shorter DLOs \textcolor{olive}{\textbf{(5)}}.

For the $\text{MoG}_{\text{270mm; 00-50}}$ policy ($\text{PPO-}2$, plotted on \textcolor{blue}{\textbf{blue}}), we see a similar first half of the trajectory for the softer $\text{00-20}$ DLOs, as well as the $\text{270mm; 00-50}$ one, although with greater variance \textcolor{olive}{\textbf{(6)}}. 
We also see a similar reaching pattern in the last $6$ steps for the longer $\text{270mm}$ and $\text{290mm}$ DLOs \textcolor{olive}{\textbf{(7)}}, with differences attributed to their different softness. 
PPO-2 and PPO-3 also exhibit a similar reaching pattern in the last $6$ steps for the shorter and softer $\text{200mm; 00-20}$ DLO \textcolor{olive}{\textbf{(8)}}. 
Finally, the most distinct sign of domain distribution adaptation occurs in PPO-3 trajectories maintaining a higher distance between the EEF and the table than any other DR policy \textcolor{olive}{\textbf{(9)}}.

Fig.~\ref{fig:eef-trajs-dtw-heatmap} shows the space of the mean EEF trajectories (averaged over $4$ repetitions) as a similarity heatmap, to give an idea of the variation in the policy space for our $6$ policies evaluated over our $4$ DLOs. 
$\mathit{U}, \mu, 0, 1, 2, 3$ denote the respective PPO trajectories, as indexed above. 
For $\text{200mm; A-40}$, we see that the PPO-0 and PPO-1 trajectories, both conditioned on equally short $\text{200mm}$ DLO, show the greatest similarity. 
For $\text{200mm; 00-20}$, we see that the $\text{PPO-}\mathit{U}$ and PPO-2 trajectories are the most similar on average. 
For the longer $\text{270mm; 00-50}$ and $\text{290mm; 00-20}$ DLOs, we have PPO-1 and PPO-2 being the most similar on average.

\begin{figure*}[t]
    \vspace{5pt}
    \centering
    \includegraphics[width=1.0\textwidth]{fig-annotated-eef-trajectories.pdf}
    \caption{EEF trajectories during the real world deployment for $4$ different DLOs of $6$ policies trained using different domain distributions. We repeat each deployment $4$ times and average the measured accumulation of commanded EEF translations along the $x$ and $z$ axes (commanded \emph{actions}), with shaded regions reporting standard deviation.}
    \label{fig:eef-deltas-per-eval-steps}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.925\textwidth]{fig-extrinsic-eval-timelapse.pdf}
    \caption{Timelapses of real world $\text{MoG}_{\text{200mm; A-40}}$ ($\text{PPO-}0$) policy evaluations using our 200mm; shore A-40, 270mm; shore 00-50, and 290mm; shore 00-50 DLOs, in conjuction with Figure~\ref{fig:eef-deltas-per-eval-steps}. We can extrinsically confirm reported observations \textcolor{olive}{\textbf{(3)}} (same $1/3$ of trajectory), \textcolor{olive}{\textbf{(4)}} (similar reaching pattern) and \textcolor{olive}{\textbf{(9)}} (moving higher above the table).} 
    \label{fig:extrinsic-eval-timelapse}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{fig-eef_mean_trajectories_DTW_heatmap.png}
    \caption{Dynamic Time Wrapping (DTW) heatmap of EEF trajectories (accum. commanded \emph{actions}) following trained PPO policies.}
    \label{fig:eef-trajs-dtw-heatmap}
\end{figure}
