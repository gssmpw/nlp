%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{sec:background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%
\subsection{Likelihood-free Inference}
\label{subsec:lfi-prelim}
%%%%%%%%%%

LFI treats a simulator as a black-box generative model $g$ with intractable likelihood, which uses its parameterisation $\boldsymbol{\theta}$ to generate an output $\mathbf{x}$ that represents the behaviour of the simulated system~\cite{papamakarios2016fast}. This process defines a likelihood function $p(\mathbf{x} \mid \boldsymbol{\theta})$, also called the forward model of the system, which we cannot evaluate, but we can sample from by running the simulator.
To overcome the ``reality gap", we want to solve the inverse problem, which is mapping real observations $\mathbf{x}^r$ to the parameters $\boldsymbol{\theta}$ that are most likely to replicate them in simulation; let $\mathbf{x}^s = g(\boldsymbol{\theta})$. This defines the problem of approximating the posterior $\hat{p}(\boldsymbol{\theta} \mid \mathbf{x}^s, \mathbf{x}^r)$.

BayesSim~\cite{ramos2019bayessim} approximates the posterior $\hat{p}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r)$ by learning the conditional density function $q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x})$, parameterised by $\boldsymbol{\phi}$. Conditional density is modelled as a mixture of Gaussians (MoG), which can be approximated by mixture density neural networks (MDNN)~\cite{bishop1994mixture}. The inputs $\mathbf{x}$ can be high-dimensional state-action trajectories, summary statistics $\psi(\cdot)$ of trajectory data, or kernel mappings.

BayesSim first requires training $q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x})$ on a dataset of $N$ pairs $(\boldsymbol{\theta}_n, \mathbf{x}_n)$, where the parameters $\boldsymbol{\theta}_n$ are drawn from a \emph{proposal prior} $\Tilde{p}(\boldsymbol{\theta})$ and the observation trajectories $\mathbf{x}_n$ are generated by running $g(\boldsymbol{\theta}_n)$ for the duration of a simulated episode and collecting the respective state-action pairs.

Then, given a single real world trajectory $\mathbf{x}^r$, BayesSim estimates the posterior as:
\begin{equation}
    \hat{p}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r) \propto \frac{p(\boldsymbol{\theta})}{\Tilde{p}(\boldsymbol{\theta})} q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r) \text{,}
\end{equation}
which allows the flexibility (`likelihood-free') for a desirable prior $p(\boldsymbol{\theta})$ which is different than the proposal prior. If $\Tilde{p}(\boldsymbol{\theta}) = p(\boldsymbol{\theta})$, then $ \hat{p}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r) \propto q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r)$.

%%%%%%%%%%
\subsection{Domain Randomisation}
\label{subsec:dr-prelim}
%%%%%%%%%%

Domain randomisation using wide uniform priors~\cite{tobin2017domain}, combined with the inherent instability of training RL algorithms, has been shown to not provide the robustness expected in certain tasks~\cite{peng2018sim, possas2020online}. 
Using LFI, we obtain a posterior $\hat{p}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r)$ which we expect to be qualitatively more precise, i.e. narrower than the uniform prior, and more accurate, i.e. dense around the system's true parameters.

The goal of RL is to maximise the expected sum of future discounted rewards (by $\gamma$) following a policy $\pi_{\boldsymbol{\beta}}(\mathbf{a}_t \mid \mathbf{s}_t)$, parameterised by $\boldsymbol{\beta}$.
We formulate DR over the RL problem as maximising the joint objective:
\begin{equation}
    \mathcal{J}(\boldsymbol{\beta}) = \mathbb{E}_{\boldsymbol{\theta}} \Biggl[ \mathbb{E}_{\boldsymbol{\eta}} \Biggl[ \sum_{t=0}^{T-1} \gamma^{(t)} r(\mathbf{s}_t, \mathbf{a}_t) \mid \boldsymbol{\beta} \Biggr] \Biggr] \text{,}
\end{equation}
where $\boldsymbol{\theta} \sim \hat{p}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r)$ with respect to policy's $\boldsymbol{\beta}$.
In the inner RL objective formulation, we follow the standard textbook denotations~\cite{sutton2018reinforcement}. 

%%%%%%%%%%
\subsection{Kernel Mean Embeddings \& Neural Approximation}
\label{subsec:rkhs-bg}
%%%%%%%%%%

Kernel mean embeddings map distributions into infinite-dimensional feature spaces, thus representing probability distributions without information loss to a space that enables useful mathematical operations.
This happens by turning the expectation operator into a reproducing kernel Hilbert space (RKHS) inner product, which has linear complexity in the number of training points~\cite{muandet2017kernel}.

Let $X$ be a random variable in sample space $\mathcal{X}$, $\mathcal{F}$ be an RKHS with kernel $k(\mathbf{x}, \mathbf{x}')$, and data points $\mathbf{x} \in \mathcal{X}$. $\mathcal{F}$ is a Hilbert space of functions $f : \mathcal{X} \rightarrow \mathbb{R}$ with the inner product $\langle \cdot , \cdot \rangle_{\mathcal{F}}$~\cite{ghojogh2021reproducing}.
Given a distribution of functions $P(X)$ and a mean embedding map $\mu_X \in \mathcal{F}$, we can recover expectations of all functions, as $\mathbb{E}_X [f(x)] = \langle \mu_X, f \rangle_{\mathcal{F}}, \forall f \in \mathcal{F}$. 
By Riesz representation theorem~\cite{akhiezer2013theory}, $\mu_X$ exists and is unique.
A common choice for the $\mu_X$ mapping is the Radial Basis Function (RBF) kernel, with parameterisable length scale $\sigma$.

Following~\cite{antonova2022bayesian}, we can define the empirical kernel embedding using i.i.d. samples $\mathbf{x}_1,...,\mathbf{x}_N$ from $P(X)$, as:
\begin{equation}
\label{eq:kernel-mean-embed}
    \hat{\mu}_X := \frac{1}{N} \sum_{n=1}^{N} \phi(\mathbf{x}_n) \text{.}
\end{equation}
With \emph{kernel trick} we avoid operating on infinite-dimensional implicit maps $\phi(\mathbf{x})$, and instead operate on a finite-dimensional Gram matrix $K$: $K_{ij}~=~k(\mathbf{x}_i, \mathbf{x}_j), \; i,j~=~1...N$~\cite{ghojogh2021reproducing}, which, however, is computationally expensive for large datasets ($\mathcal{O}(n^2)$).

Following~\cite{ramos2019bayessim} (Eq.~12-15) and~\cite{antonova2022bayesian} (Eq.~6) derivations, we can use \emph{Random Fourier Features} (RFF)~\cite{rahimi2007random} to approximate a shift-invariant kernel $k(\mathbf{x}, \mathbf{x}')$ by a dot product $\hat{\phi}(\mathbf{x})^T\hat{\phi}(\mathbf{x}')$, which is a more scalable approximation of $K$. 
$\hat{\phi}(\mathbf{x})$ is a finite-dimensional approximation of $\phi(\mathbf{x})$, with randomised features of the form $\langle \cos(\sigma^{-1}~\circ~\boldsymbol{\omega}_{m}^T\mathbf{x}), \sin(\sigma^{-1}~\circ~\boldsymbol{\omega}_{m}^T\mathbf{x}) \rangle$ (cos-sin) or 
$\cos(\boldsymbol{\omega}_{m}^T\mathbf{x}~+~\mathbf{b}_m)$ (cos-only)~\cite{ramos2023method}. 

Following~\cite{antonova2022bayesian}, we use the RFF feature approximation to construct the kernel mean embedding of vectors \eqref{eq:kernel-mean-embed} that can benefit from a distributional representation.
This embedding approach can be integrated into a learning architecture in a fully differentiable manner, by constructing a neural network layer (RKHS-Net) that obtains random samples for frequencies $\boldsymbol{\omega}$, biases $\mathbf{b}$, and optionally $\sigma$ (e.g., in cos-sin), and adjusts them during training by computing gradients w.r.t. the overall architecture loss.
