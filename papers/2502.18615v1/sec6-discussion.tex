%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper, we extend the Real2Sim2Real literature with a proof of concept that an integrated distributional treatment of parameter inference, policy training, and zero-shot deployment can lead to notable agent behaviour adaptation over a set of parameterised DLOs. 
We use a prominent LFI method (BayesSim) to capture fine parameterisation differences of similarly shaped DLOs. 
Using the inferred MoG posteriors for DR during RL policy training may not lead to direct benefits in training performance (although this is largely task dependent); however, we observe strong object-centric agent performance indications during real-world deployment.

This means that through an integrated distributional treatment of the Real2Sim2Real problem, practitioners of the soft robotics and soft body manipulation domains can condition RL policies on system parameterisations inferred through visual observations, thus inducing object-centric performance, all the while operating in a shared action and observation space for both parameter inference and policy learning.
Finally, we note that our framework can be extended for dual control by improving the posterior estimation in tandem with the task policy~\cite{possas2020online, barcelos2020disco}. 
