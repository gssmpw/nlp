%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup}
\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our experiments address the following questions.
\begin{enumerate}
    \item Can our inferred posteriors \textbf{describe different physical parameterisations of similarly shaped DLOs}?
    \item What is \textbf{the impact of these posteriors on domain randomisation} using the respective MoG?
    \item How does this impact translate into \textbf{object-centric agent performance} during real-world deployment?
\end{enumerate}

%%%%%%%%%%
\subsection{Reference Task}
\label{subsec:reference-task}
%%%%%%%%%%

To evaluate the parameter inference performance of our set-up, as well as the implications of the inferred parameter posterior $\hat{p}(\boldsymbol{\theta})$ to policy learning and deployment, we design a visuomotor task which studies the dynamic manipulation of a DLO towards a target in its workspace. Our task is inspired by established dynamic rope manipulation tasks~\cite{lim2022real2sim2real, chi2024iterative}. 

We set up our task with a robot arm that picks up the DLO from a designated position on the table $\mathbf{x}_0$ by grasping it near one of its tips, and raises it to a designated height $h_0$. Both $\mathbf{x}_0$ and $h_0$, as well as our initial DLO pose, remain fixed for all of our simulation and real experiment iterations. The execution of our policy begins immediately after the object has been picked up and raised to $h_0$. Thus, the dynamic nature of our task is constituted by the underactuated manner in which we control our DLO, having it dangling from one of its tips, as well as the momentum forces acting on its body ever since it was raised from the table.

The positions of both the DLO and the target are tracked in the 2D pixel space of an RGB image. More specifically, the DLO is tracked by an inferred cluster of $4$ keypoints, extracted over a segmentation image, and the target with the computed pixel-space median of its own segmentation mask, which we treat as an artificial $5$th keypoint.
In both our parameter inference (real2sim) and policy training and deployment (sim2real) experiments, we position-control our Panda end-effector (EEF) by navigating it to a target position through inverse kinematics. Our simulator uses the IsaacGym attractors implementation, whereas in the real world we use a Cartesian impedance controller~\cite{luo2024serl}.

%%%%%%%%%%
\subsection{Simulation \& Hardware setup}
\label{subsec:sim-hw-setup}
%%%%%%%%%%

We setup our simulation in IsaacGym~\cite{makoviychuk2021isaac}. Our simulated environment contains a Franka Emika Panda 7-DoF robot arm equipped with a parallel gripper, which is on top of a tabletop workspace. On top of this workspace, a blue-coloured DLO exists, which is simulated using the corotational finite element method of the FleX physics engine. A small green spherical object is the target of our visuomotor control task. Across all our parameter inference and policy learning experiments, the simulated DLO is parameterised in the $[1e3, 5e4]$ (Pa) range for Young's modulus and the $[195, 305]$ (mm) range for length. Thus, \emph{median} of our simulated parameter space is $(2.55e4 \; \text{Pa}, \; 250\text{mm})$.

Our real-world setup replicates the simulation setup as closely as possible. We extend an open-source sample-efficient robotic RL infrastructure~\cite{luo2024serl} for the purpose of evaluating PPO policies~\cite{stable-baselines3}. For visual observations, we use a RealSense D435i camera capturing $60$ fps, mounted to view our workspace from the right side. To avoid the overhead of a more elaborate sim and real camera pose calibration, we select our real-world camera placement such that the captured images qualitatively approximate the respective simulation images. 

For our experiments, we manufacture $4$ blue-coloured DLOs using Shore hardness $\text{00-20}$ (for len. $\text{200mm}$ and $\text{290mm}$), $\text{00-50}$ (for len. $\text{270mm}$) and A-40 (for len. $\text{200mm}$) silicone polymers~\cite{liao2020ecoflex}. The real world green target ball is made of plasticine.

%%%%%%%%%%
\subsection{Perception setup}
\label{subsec:perc-setup}
%%%%%%%%%%

We collect visual observations using the real-time RGB image stream produced from our side-view camera. For computational efficiency, we focus our observations on our blue DLO and the green target ball using segmentation images. For this, we fine-tune the segmentation task version of YOLOv8.2~\cite{redmon2016you}. 
We use a dataset of $183$ manually labelled workspace images, pre-processed through auto-orientation and resizing to $256\times256$, and augmented using random Gaussian blur and salt-and-pepper noise, totalling $439$ images. With this dataset, we train the open source YOLO weights for another $64$ epochs. 

We reduce the dimensionality of our visual observations by applying a learnt model of $4$ keypoints to each segmentation image in real time. We train this keypoint model using a dataset of $1500$ random policy rollouts of our simulated control task, while sampling our system parameters $\boldsymbol{\theta}$ from a uniform prior. In this data set, we also incorporate a small number of real-world policy rollouts, and we train for $50$ epochs. These keypoints constitute our observation vector.

%%%%%%%%%%
\subsection{Domain Randomisation setup}
\label{subsec:dr-setup}
%%%%%%%%%%

In practice, we see that in popular robotics simulators, such as IsaacGym, re-parameterising an existing deformable object simulation to change physical properties such as stiffness would require to re-initialise the whole simulation. This makes it difficult to integrate such simulators in an RL environment that follows the well-established \emph{gym} style~\cite{towers2024gymnasium}.

Thus, to perform DR for our RL task, we train our policies in \emph{vectorised} environments~\cite{stable-baselines3}. This enables us to launch parallel instances of our simulation, each with a different DLO parameterisation, which is sampled by our current reference prior. To further robustify the real-world performance of our sim-trained policies and minimise the need for camera calibration, we introduce a small randomness in our camera and target object poses by uniformly sampling pose offsets in the $[\pm0.025,\;\pm0.025,\;\pm0.025]$ (m) and $[\pm0.02,\;0,\;\pm0.02]$ (m) ranges, respectively.

We empirically select to launch $12$ concurrent environments, from $12$ respective domain samples, to manage our computationally intensive deformable object simulations. This places the descriptiveness of our inferred posteriors at the forefront of our experimental study, since inaccuracy and imprecision of the inferred MoGs can have worse consequences in small-data experiments. We sample our MoGs in a \emph{low-variance} method, thus each component is likely to contribute to our set of domain samples.

%%%%%%%%%%
\subsection{Control setup}
\label{subsec:ctrl-setup}
%%%%%%%%%%

We command our EEF by sending it target poses ($7$D command vectors). For our control experiments, we constrain our EEF motion into two dimensions, moving only along the $x$ and $z$ axes by controlling the respective deltas. Thus, our sampled policy actions are $2$D $\langle dx, dz \rangle$ vectors that we sample in the $[-0.06, 0.06]$ (m) range to maintain smooth EEF transitions. Our $12$D observations are constructed by a flattened vector of the $2$D desired goal, the $2$D vector of the $x$ and $z$ components of our EEF position (proprioception) and the $4 \times 2$D state observation, which reflects the pixel positions of our target object and DLO, respectively.

In each state $s_t$, we track the current distance of our DLO keypoint cluster to the desired goal, which we compute using the Frobenius norm of the respective distance matrix. Our sparse reward function uses this distance $d_t$ and a predefined pixel space distance threshold to count the reward $d_{\text{thresh}}$. Thus, whenever $d_t \le d_{\text{thresh}}$, our respective reward is the distance scaled to $[0.0, 1.0]$, denoted as $r_t = 1.0 - (d_t / d_{\text{thresh}})$. 

For safety reasons, we restrict our EEF motion within the $\langle x, y, z \rangle \in \langle [0.275, 0.6], [-0.1, 0.1], [0.1, 0.5] \rangle$ (m) world frame coordinates, with the base of the robot arm being on $(0, 0, 0)$. Whenever the EEF leaves this designated workspace, the episode ends as a failure with a reward of $-1$.

%%%%%%%%%%
\subsection{Parameter Inference \& Policy Algorithms training}
\label{subsec:training-setup}
%%%%%%%%%%

For parameter inference, we implement the BayesSim-RKHS variant~\cite{antonova2022bayesian} (Fig.~\ref{fig:perc-to-rkhs}). Our RKHS-net layer embeds the $5$ observed keypoints ($4$ from DLO, $+1$ for the desired goal) using $500$ random Fourier features and cos-only components. Our MDNN models $4$ mixture components with full covariance and uses $3$ fully connected layers of $1024$ units each. We use Adam optimiser with a learning rate of $1e$$-6$. As in Alg.~\ref{alg:real2sim2real-bsim}, line~\ref{alg:r2s2r:line:lfi-iter}, we approximate our posterior $\hat{p}(\boldsymbol{\theta})$ through $15$ iterations, each increasing the training set with $100$ more trajectories, whose parameters $\boldsymbol{\theta}$ have been sampled using the latest posterior.

For policy learning, we use the Stable Baselines3~\cite{stable-baselines3} PPO implementation. To maintain the generality of our proposed methodology, we keep the default implementation hyperparameters, which reflect the hyperparameters originally proposed in the seminal work~\cite{schulman2017proximal}. We train for $120,000$ total steps, with a maximum duration of episodes of $16$ steps, and a batch size of also $16$. We empirically tune our real impedance controller damping and stiffness so that $16$ real-world steps approximate the respective $16$ simulation steps.
