%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
\label{sec:method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%
\subsection{Keypoint detection from segmentation images}
\label{subsec:real2sim2real-seg-img-n-kps}
%%%%%%%%%%

Our perception module receives an RGB image observation as input, in which it detects the environment objects of interest, i.e. the controlled DLO and the visuomotor control target. We use detection masks to create segmentation images, through which we track our task's features for parameter inference and policy learning.

We use segmentation images to efficiently learn an unsupervised keypoint detection model, using the \emph{transporter} method~\cite{kulkarni2019unsupervised}, as implemented by~\cite{li2020causal}.
The method's combination of keypoint-based bottleneck layer and downstream reconstruction task maintains the temporal consistency of the extracted set of keypoints. However, keypoints are inferred in every timeframe, which in practice, particularly when working with real-world image data of deformable objects, creates problems such as keypoint position noise and permutations~\cite{antonova2022bayesian}.

%%%%%%%%%%
\subsection{Real2Sim with likelihood-free inference}
\label{subsec:real2sim-bayessim}
%%%%%%%%%%

Our problem is that of manipulating a DLO for a visuomotor reaching task, by learning a policy in simulation and deploying in the real-world, without any further fine-tuning. The Real2Sim part of our work deals with calibrating physical parameters that induce the deformable object's behaviour, such as its dimensions and material behaviour, which are difficult to tune manually. Details such as camera and workspace object placement, controller stiffness parameters, etc. are beyond the scope of our work, although they can be incorporated as additional tunable parameters in extensions of our method.

For our inference and domain randomisation experiments, we define a physical parameter vector $\boldsymbol{\theta} = \langle l, E \rangle$, where $l$ denotes the length of our DLO and $E$ denotes its Young's modulus. Thus, we want to infer a joint posterior $\hat{p}$, which contains information on both the dimensions and the material properties of our deformable object. For this, we use the BayesSim-RKHS variant~\cite{antonova2022bayesian}.

Following Algorithm~\ref{alg:real2sim2real-bsim}, we begin by assuming a uniform proposal prior $\Tilde{p}(\boldsymbol{\theta})$, which we use to initialise our reference prior $p_0 = \Tilde{p}$. We then perform domain randomisation as $\boldsymbol{\theta} \sim p_0$, while training in simulation an initial policy $\pi_{\boldsymbol{\beta}_0}$ for our task. We execute a rollout of $\pi_{\boldsymbol{\beta}_0}$ in the real world environment, to collect a real trajectory $\mathbf{x}^r$ while manipulating a specific DLO. We perform LFI through \emph{multiple} BayesSim iterations~\cite{possas2020online, antonova2022bayesian}. In each inference iteration $i$, we use $\pi_{\boldsymbol{\beta}_0}$ as a data collection policy, running $N$ rollouts of $\pi_{\boldsymbol{\beta}_0}$ in simulation to collect a dataset $\{(\boldsymbol{\theta}, \mathbf{x}^s)\}^N, \boldsymbol{\theta} \sim \Tilde{p}$, which we use to train our BayesSim conditional density function $q_{\phi}$. We use our $q_{\phi}$ and $\mathbf{x}^r$ to compute the posterior $\hat{p}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r)$. We then update our reference prior $p_i = \hat{p}$ and loop again.

We can now adapt our domain randomisation distribution to the latest inferred posterior $\hat{p}$, and we proceed to retrain our task policy $\pi_{\boldsymbol{\beta}_1}$. Our hypothesis is that by sampling $\boldsymbol{\theta}$ from our object-specific posterior, we will get faster $\pi_{\boldsymbol{\beta}_1}$ convergence to consistently successful behaviour, and that running a rollout of $\pi_{\boldsymbol{\beta}_1}$ in the real-world environment we will collect higher cumulative reward for the specific DLO within the task episode's horizon.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig-distr-state-embed-method.pdf}
    \caption{An overview of our policy rollout and trajectory perception method, with keypoint extraction from segmentation images, and keypoint trajectory cos-only kernel mean embeddings using the RKHS-net layer (BayesSim-RKHS), illustration inspired by~\cite{antonova2022bayesian}.}
    \label{fig:perc-to-rkhs}
\end{figure}

\subsection{Keypoint trajectories in RKHS}

We use an RKHS-net layer~\cite{antonova2022bayesian} to construct a distributional state representation (Fig.~\ref{fig:perc-to-rkhs}), which is provably robust to visual noise and uncertainty due to inferred keypoint permutations. Intuitively, through our kernel mean embeddings~\cite{muandet2017kernel} we pull data from the input space (keypoint trajectory) to the feature space (RKHS). The explicit locations of the pulled points are not necessarily known or might be uncertain, but the relative similarity (inner product) of the pulled data points is known in the feature space~\cite{ghojogh2021reproducing}. This gives us noise robustness and permutation invariance, and it generally means that functional representations that are similar to each other are embedded closer within the RKHS, and thus they are more likely to be classified as similar.

The input of the RKHS-net layer is a vector of samples of a distribution. In this work, we compute the distributional embedding for a noisy trajectory of keypoints $\mathbf{x}~=~(\mathbf{x_1},...,\mathbf{x_n})$, where each keypoint is defined in a $2$D RGB pixel space. 

%%%%%%%%%%
\subsection{Policy Learning and Sim2Real Deployment}
\label{subsec:real2sim-param-inference}
%%%%%%%%%%

We use Proximal Policy Optimisation (PPO)~\cite{schulman2017proximal} as our reference model-free \emph{on-policy} RL algorithm. PPO is designed to maximise the expected reward with a clipped surrogate objective to prevent large updates. It samples actions from a Gaussian distribution, which is parameterised during policy learning.
We perform policy learning in simulation, performing domain randomisation using our likelihood-free inference \emph{reference prior} $p$ as a task parameterisation sampler, where $p$ is either the default uniform distribution $\mathit{U}$, or an inferred MoG posterior (Alg.~\ref{alg:real2sim2real-bsim}, line~\ref{alg:r2s2r:line:p-update}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
\caption{Real2Sim2Real for DLO manipulation}
\label{alg:real2sim2real-bsim}
\begin{algorithmic}[1]
    \State \textbf{Given:} $N_{\text{LFI}}$: inference iterations
    \State Assume uniform proposal prior $\Tilde{p}(\boldsymbol{\theta}) \approx \mathit{U}$
    \State Assign reference prior $p_0 \gets \Tilde{p}$
    \State Train initial policy $\pi_{\boldsymbol{\beta}_0}(\mathbf{a}_t \mid \mathbf{s}_t)$, $\boldsymbol{\theta} \sim p_0$
    \State Run $1$ $\pi_{\boldsymbol{\beta}_0}$ rollout in the real env to collect $\mathbf{x}^r$
    \State \textbf{// 1. Real2Sim DLO parameter inference}
    \State $i \gets 0$
    \While{$i < N_{\text{LFI}}$} \label{alg:r2s2r:line:lfi-iter}
        \State $\{(\boldsymbol{\theta}, \mathbf{x}^s)\}^N \gets$ Run $N$ $\pi_{\boldsymbol{\beta}_0}$ rollouts in sim, $\boldsymbol{\theta} \sim p_i$
        \State Train $q_{\phi}$ over $\{(\boldsymbol{\theta}, \mathbf{x}^s)\}^N$
        \State $\hat{p}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r) \propto p_i(\boldsymbol{\theta}) \mathbin{/} \Tilde{p}(\boldsymbol{\theta}) q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x} = \mathbf{x}^r)$
        \State Update reference prior $p_i \gets \hat{p}$ \label{alg:r2s2r:line:p-update}
        \State $i \gets i + 1$
    \EndWhile
    \State \textbf{// 2. Policy training in sim}
    \State Train policy $\pi_{\boldsymbol{\beta}_1}(\mathbf{a}_t \mid \mathbf{s}_t)$, $\boldsymbol{\theta} \sim \hat{p}$
    \State \textbf{// 3. Sim2Real policy deployment}
    \State Evaluate $\pi_{\boldsymbol{\beta}_1}$ in the real env by running $1$ $\pi_{\boldsymbol{\beta}_1}$ rollout
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
