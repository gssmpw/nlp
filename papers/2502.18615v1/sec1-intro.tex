%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Deformable linear object (DLO) manipulation is an active area of robotics research that deals with a variety of challenging tasks, such as needle threading, lace tying, cable rearrangement, and lasso throwing~\cite{zhang2021robots, chi2024iterative, haiderbhai2024sim2real}. Although the geometry of a DLO is well understood, humans show dexterity in quickly estimating physical parameters, such as length and stiffness, from relatively few visual observations of object manipulation trajectories. This is often used to condition control policies within these estimations~\cite{kuroki2024gendom, zhang2024adaptigraph}, enabling efficient adaptive control. Achieving a similar level of robotic dexterity requires dealing with the inherent high dimensionality and nonlinearity of such tasks, challenges that are further exacerbated by the inherent noise in visual servoing~\cite{arriola2020modeling, yin2021modeling}.

Learning policies in simulation and then performing a \emph{Sim2Real}~\cite{liang2024real, haiderbhai2024sim2real} deployment follows the hypothesis that the numerous simulated iterations of a given task will robustify the learnt policy to parametric variations, as mentioned above~\cite{peng2018sim}. However, this approach requires one to overcome the ``reality gap'' of robotic simulators, which is particularly problematic for soft objects. 

In this work, we first aim to achieve a reliable \emph{Real2Sim}~\cite{mehta2021user, liang2020learning} calibration of our simulator's parameters $\boldsymbol{\theta}$ to the real-world object parameters. Then, aiming at robust deployment, we account for any uncertainty about these parameters by exposing our learning algorithm to different hypotheses of $\boldsymbol{\theta}$.

Likelihood-free inference (LFI) deals with solving the inverse problem of probabilistically mapping real-world sensor observations to the respective simulation parameters $\boldsymbol{\theta}$ that are most likely to account for the observations.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig-real2sim2real--workflow.pdf}
    \caption{Overview of our Real2Sim2Real framework. We perform inference for the posterior distribution $\hat{p}$ over system parameters $\boldsymbol{\theta}$ (Real2Sim). We use $\hat{p}$ to perform domain randomisation while training a PPO agent to perform a visuomotor DLO control task. We deploy and evaluate our sim-trained policy in the real world (Sim2Real).}
    \label{fig:header-system-overview}
\end{figure}

This involves inferring the multimodal distribution from which we can sample sets of simulation parameters with a high probability of replicating the modelled real-world phenomenon~\cite{ramos2019bayessim}. Any uncertainty over $\boldsymbol{\theta}$ can be modelled as the variances associated with the modes.

This enables us to perform Domain Randomisation (DR), which aims to create a variety of simulated environments, each with randomised system parameters, and then to train a policy that works in all of them for a given control task objective. Assuming that the parameters of the real system is a sample in the distribution associated with the variations seen at training time, it is expected that a policy learnt in simulation under a DR regime will adapt to the dynamics of the real world environment.

Recent literature has demonstrated how distributional embeddings, such as reproducing kernel Hilbert spaces~\cite{muandet2017kernel}, of inferred keypoint trajectories combine low dimensionality and robustness to visual data noise~\cite{antonova2022bayesian}, thus enabling robust Real2Sim calibration. On the Sim2Real side, we have seen fruitful model-based Reinforcement Learning (RL) approaches~\cite{zeng2021transporter, seita2021learning} to train deformable object manipulation policies in simulation and deploy them in the real world. However, we have yet to see an end-to-end \emph{Real2Sim2Real} system which combines the expressiveness of Bayesian inference with the flexibility of model-free RL~\cite{schulman2017proximal}.

We propose an integrated framework addressing a number of different technical considerations (Fig.~\ref{fig:header-system-overview}, Alg.~\ref{alg:real2sim2real-bsim}). 
We use BayesSim~\cite{ramos2019bayessim} with distributional state embeddings to infer the posteriors over physical parameters $\boldsymbol{\theta}$ of a set of visually observed (real) DLOs of varied length and stiffness.
We use each inferred posterior $\hat{p}(\boldsymbol{\theta})$ to perform domain randomisation and train a Sim2Real transferable model-free RL policy to perform a visuomotor ``reaching'' task with a DLO. We deploy our sim-trained policies in the real world and compare their performance across our set of DLOs.

In this paper, we make the following contributions.
\begin{enumerate}
    \item We present an \textbf{end-to-end Real2Sim2Real framework} for robust \textbf{vision-based} DLO manipulation. 
    \item We examine the capacity of BayesSim to \textbf{finely classify} the different physical properties of a deformable object drawn from a parametric set.
    \item We study the implications of \textbf{different randomisation domains} for RL policy learning in simulation and how this translates to \textbf{real world performance}. 
\end{enumerate}
Our experiments show that for a parameterised set of DLOs, an integrated distributional treatment of parameter inference, policy training, and zero-shot deployment enables inferring fine differences in physical properties and adapting RL agent behaviour to them.
