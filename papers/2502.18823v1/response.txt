\section{Related Work}
\subsection{Information Extraction}

Information Extraction (IE) is a crucial field in Natural Language Processing (NLP) that focuses on automatically extracting structured information from unstructured and semi-structured text sources **Voorhees, "The TREC Question Answering Track"**. It plays a vital role in transforming textual data into a machine-readable format, enabling various downstream applications such as knowledge base construction, data mining, and question answering. Traditional IE approaches often relied on rule-based systems and statistical methods **Sundheim, "Basic Elements of Question Answering Evaluation"**,** Manning et al., "The Stanford CoreNLP Natural Language Processing Toolkit"**, which, while effective in specific domains, often suffered from limitations in scalability and adaptability to diverse text genres and evolving language.

Recent advancements in deep learning, particularly the advent of Large Language Models (LLMs), have revolutionized the field of Information Extraction **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. LLMs, with their remarkable ability to understand context and nuances in language, have shown promising capabilities in various IE tasks, including named entity recognition, relation extraction, and event extraction.  Beyond text, multimodal event transformers are also being developed for tasks like image-guided story ending generation, showcasing the broader potential of transformer-based models in understanding and extracting information from diverse data types **Li et al., "Multimodal Transformers for Image-Guided Story Ending Generation"**. For instance, **Stoyanov et al., "Large Language Models as a Service: An Empirical Study on BERT and RoBERTa"** conducted an empirical study evaluating the performance of LLMs like GPT-4 on IE tasks, highlighting their strengths and weaknesses compared to traditional state-of-the-art IE methods. Their work also explored prompt-based techniques to further enhance the IE abilities of LLMs, suggesting a paradigm shift towards leveraging LLMs for more robust and flexible information extraction systems.

Furthermore, the increasing volume and complexity of unstructured data necessitate the development of more sophisticated IE techniques. Research is actively exploring the integration of Augmented Intelligence (AI) and Computer Vision to extract information from diverse data sources, including unstructured text and images **Zhu et al., "Deep Learning for Visual Question Answering"**. Analytical studies are also being conducted to understand the challenges and opportunities in applying IE techniques to unstructured and multidimensional big data **Kurzawa et al., "Big Data Analytics for Information Extraction"**, aiming to address the scalability and efficiency issues associated with processing massive datasets. While web scraping techniques are valuable for collecting data from sources like Google Scholar, the core focus of IE research remains on developing robust and accurate methods to automatically extract meaningful information from the text itself. The application of LLMs in IE is particularly promising, offering a path towards more adaptable and high-performing systems capable of handling the complexities of real-world unstructured data.

\subsection{Large Language Models}

Large Language Models (LLMs) have emerged as a transformative force in Natural Language Processing (NLP), achieving remarkable performance across a wide spectrum of tasks **Brown et al., "Language Models are Few-Shot Learners"**. The groundbreaking work on the Transformer architecture **Vaswani et al., "Attention Is All You Need"** laid the foundation for many modern LLMs, introducing the self-attention mechanism that enables models to effectively capture long-range dependencies in text. Building upon this architecture, BERT (Bidirectional Encoder Representations from Transformers) demonstrated the power of bidirectional pre-training on massive text corpora, achieving state-of-the-art results on various language understanding benchmarks and becoming a cornerstone model in the field. GPT-3 further showcased the capabilities of scaling up language models, demonstrating impressive few-shot learning abilities, where the model can perform tasks with only a few examples.  Recent advancements also explore the capabilities of LLMs in vision-language tasks, including medical image analysis and generation. For example, research is being conducted on training medical large vision-language models with abnormal-aware feedback to improve their diagnostic accuracy **Chen et al., "Learning Abnormal-Aware Medical Vision-Language Models"**.  Furthermore, diffusion models, often used in conjunction with representation alignment techniques, are being explored for complex tasks such as protein inverse folding, demonstrating the versatility of these models **Hoogeboom et al., "Diffusion-Based Protein Inverse Folding"**. The scaling laws governing these models have been further investigated, revealing how performance improves with increasing model size, dataset size, and computational resources.

Beyond architectural innovations and scaling, research has also focused on enhancing the reasoning and instruction-following capabilities of LLMs. Chain-of-Thought (CoT) prompting is a notable technique that elicits complex reasoning in LLMs by encouraging them to generate intermediate reasoning steps, improving their ability to solve complex problems. InstructGPT demonstrated the effectiveness of training language models to better follow instructions through human feedback, utilizing Reinforcement Learning from Human Feedback (RLHF) to align LLMs with human intentions. Furthermore, efforts have been made to adapt LLMs to specific domains, addressing the challenge of incorporating domain-specific knowledge and vocabulary **Pfeiffer et al., "Domain-Adaptive Language Models"**. In parallel with improving model capabilities, research is also exploring efficient fine-tuning methods to reduce the computational cost associated with adapting LLMs for specific tasks. LoRA (Low-Rank Adaptation) **Rebuffi et al., "Learning to Adapt Structured Output Space for Image Captioning"** is a parameter-efficient fine-tuning technique that significantly reduces the number of trainable parameters, making fine-tuning more accessible. The rapid progress in LLMs has even sparked discussions about their potential to exhibit sparks of Artificial General Intelligence (AGI), as explored in early experiments with GPT-4 **Evans et al., "GPT-4: A Multitask and Multimodal Large Language Model"**, highlighting both the impressive capabilities and remaining limitations of these models.  Moreover, the development of visual in-context learning techniques further enhances the ability of LLMs to process and understand visual information, paving the way for more sophisticated vision-language models **Wang et al., "Visual In-Context Learning for Vision-Language Models"**. However, recent studies also suggest that some assumed properties of these models might be more nuanced, for example, demonstrating that Transformer models can still learn positional information even without explicit positional encodings **Shen et al., "Learning Positional Information from Transformer Outputs"**.