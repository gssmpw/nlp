It is nowadays well known to the general audience that \gls{ai} needs models to be trained. What is maybe less known to a general audience is what such \emph{trained} actually implies in terms of technical efforts to be successful.
Any \gls{ai} model, either in the \gls{ml} or in the \gls{dl} domain, can be at first roughly described by its trainable and non-trainable parameters.
In the case of neural networks, these can also be categorized with three main classes: weights, architecture configuration and training settings.
Despite being all of them inherently interconnected to each other, it is important to identify the different roles they have. By analogy, one could say that i) the architecture configuration \emph{builds} the model giving it the structure and the shape, ii) the training settings define how the model undergoes the learning stages and iii) the weights embed what the model learns through adaptation during the training procedure.
In typical \gls{dl} cases, only the weights are trained, although exceptions can be found depending on specific user needs.
What is not trained, typically referred to as the \emph{hyperparameters}, as opposed to the \emph{parameters} represented by the weights, then needs to be pre-defined and set, and this often represents a challenge towards high-performance models\cite{liao_empirical_2022}.

In the neuromorphic computing domain, the use of \glspl{snn}, namely bio-inspired \glspl{ann} implementing brain-like primitives~\cite{Maass1997}, introduces further complexity due to the neuronal computational units and their additional hyperparameters~\cite{firmin_parallel_2024}.
\glspl{snn} are indeed built through simplified models of biological neurons, and even the simplest ones need for selection of at least one fundamental pair of hyperparameters: threshold voltage and decay constant~\cite{ou_overview_2022,ganguly_spike_2024}.
Inadequately chosen hyperparameters can result in suboptimal models with poor performance, which in turn can drive promising solutions to unsuccessful applications.
A possible way to avoid such outcomes is to perform \gls{hpo} with an application-oriented approach.
Although this can imply overspecialization taking shape in overfitting and poor generalization capability, a careful design of the \gls{hpo} experiment which takes into account the final target application, as well as its possible hardware-related constraints, is the most powerful solution when prototyping \glspl{snn}.

In this document, a pipeline for application-oriented automatic \gls{hpo} of \glspl{snn} is presented, relying on the \gls{nni}\footnote{github.com/microsoft/nni/} toolkit as reference framework.\\
The description is made such that the reader can go through a higher level discussion and a code-based example at the same time. The latter is sustained by a specific use case whose results are reported too, and the proposed file names and structure retrace an actual \gls{hpo} experiment.


% % --------------------------------------------------------------------------------

% % Hyperparameters play a pivotal role in the performance and efficiency of \gls{dl} models.
% % In contrast to model parameters, which are derived during the training process, hyperparameters such as learning rate, batch size, network architecture, optimizer configurations, and regularization strength necessitate pre-setting.
% % The step of \gls{hpo} hence turns out to be indispensable for the development of models that exhibit high performance.
% Inadequately chosen hyperparameters can indeed result in protracted learning, overfitting (demonstrating strong performance on training datasets but weak performance on novel data), or underfitting (the inability to adequately capture the underlying patterns in the data).

% % Given the extensive array of potential hyperparameter configurations and the substantial computational expense associated with identifying the optimal set, researchers have devised a variety of HPO methodologies. One notable approach is application-oriented HPO, which customizes hyperparameters to align with particular tasks or application domains, thereby enhancing performance and resource utilization.

% % This methodology confers several advantages.
% % It facilitates hyperparameter optimization tailored for distinct applications such as image classification, natural language processing, or time-series forecasting, culminating in enhanced, task-specific efficacy.
% % Such fine-tuning empowers the model to grasp the intricacies associated with the specific application.
% % Additionally, it has the potential to bolster generalization by striking a balance between bias and variance, thereby mitigating overfitting and augmenting robustness across analogous datasets.
% Furthermore, application-oriented optimization can enhance resource efficiency by concentrating on the most pertinent hyperparameters, thereby optimizing GPU utilization for expedited training and reduced energy expenditure.
% Models that are fine-tuned for particular applications typically exhibit accelerated convergence, as hyperparameters such as learning rate and batch size are optimized for the specific data and model architecture.

% Nevertheless, application-oriented HPO also introduces certain challenges.
% % The computational burden of comprehensive experimentation, particularly with methodologies such as grid search or Bayesian optimization, constitutes a significant constraint.
% % Customizing hyperparameters often necessitates the parallel training of multiple models, which can be both cost-prohibitive and time-consuming.
% Another hurdle is the potential for overfitting to the training dataset, as excessive fine-tuning may result in diminished generalization capabilities on previously unseen data, particularly if the validation dataset fails to accurately reflect real-world conditions.
% % The intricacy of tuning for specific applications can be considerable, necessitating substantial domain expertise to discern the most critical hyperparameters.
% % This complexity is further exacerbated in multi-objective optimization scenarios, where careful consideration of trade-offs between factors such as accuracy and training duration becomes imperative.
% Ultimately, models that are optimized for a singular application may exhibit subpar performance on alternative tasks, necessitating repetitive tuning and potentially constraining scalability and adaptability.
% The equilibrium of these trade-offs is of utmost importance.

% % While application-oriented tuning has the potential to markedly enhance model performance and operational efficiency, its associated drawbacks necessitate thorough deliberation.
% % Automated tools such as AutoML, Bayesian optimization, and population-based techniques present promising avenues for mitigating computational burdens and enhancing search efficiency.
% % Progress in neural architecture search (NAS) and meta-learning is also contributing to the resolution of scalability issues by automating hyperparameter selection informed by prior knowledge.
% % Ultimately, effective HPO hinges on a synergistic combination of domain expertise, adequate computational resources, and strategic foresight to fully harness the capabilities of deep learning models customized for specific applications.

% In neuromorphic computing, hyperparameters are also critical for building effective and efficient spiking neural networks (SNNs).
% Inspired by biological systems, SNNs transmit information through discrete 'spikes', unlike traditional Artificial Neural Networks (ANNs) which use continuous values.
% % This temporal processing makes SNNs well suited to tasks such as spatio-temporal pattern recognition and offers lower power consumption.
% % However, achieving optimal performance on neuromorphic hardware requires careful tuning of the hyper-parameters.
% % These parameters, which define the network's architecture, learning mechanisms, and hardware constraints, are not learned during training, making their optimisation essential.

% Several key hyperparameters influence the behaviour of SNNs.
% Neuron model parameters (e.g. membrane potential thresholds, leakage rates) determine the spiking of individual neurons, and incorrect settings can lead to "silent networks" with poor performance.
% Network architecture parameters (e.g. number of layers, neurons per layer, connectivity patterns) determine the network's ability to learn complex relationships.
% Learning rules (e.g. Spike-Timing-Dependent Plasticity (STDP)) control synaptic weight adjustments, which are crucial for training.
% Input/output coding methods (e.g. rate coding, latency coding) convert real-world data into spike trains, affecting efficiency and accuracy.
% Finally, hardware-specific parameters (e.g., memristor crossbar array dimensions, CMOS power constraints) have significant implications for practical implementation.

% % Optimising these hyperparameters is challenging due to the non-differentiable nature of spike-based computation and the strong interdependence between software and hardware.
% % Traditional methods such as grid search are computationally expensive due to the high-dimensional and discrete hyperparameter spaces.
% % More advanced techniques, including Bayesian optimisation, evolutionary algorithms and biologically inspired approaches, offer more efficient ways to explore these spaces.
% % Incorporating spiking convolutional layers and dendritic computation has been shown to improve training efficiency.

% Effective hyperparameter tuning has a direct impact on the practical viability of neuromorphic computing.
% Optimally configured parameters lead to better energy efficiency and performance in applications ranging from robotics and sensor processing to classification.
% Sensitivity analyses show that some hyperparameters, such as input encoding, have a greater impact than others.
% Hyperparameter selection is also driving advances in neuromorphic architectures, influencing their adoption in various fields, including brain-computer interfaces and edge computing.

% While optimisation is critical to performance, trade-offs must be considered, such as the balance between computational efficiency and optimisation complexity.
% In conclusion, hyper-parameter optimisation is essential to maximise the potential of SNNs and neuromorphic computing.
% % It ensures efficient operation under real-world constraints while maintaining accuracy and robustness.
% % Given the unique temporal characteristics of SNNs, hyperparameter selection is complex but critical and requires specialised methods.
% % As neuromorphic technology advances, advances in hyperparameter tuning will be key to its widespread adoption.