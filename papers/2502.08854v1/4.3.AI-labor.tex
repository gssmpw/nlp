
\subsection{Engaging in AI Managerial Labor}
\label{sec:aimanagerial}
The third form of transformation was the introduction of new planning and execution tasks that focused on maximizing the potential of GenAI in practitioners' work (task crafting) and integrating it fully into practitioners' workflows. We describe these tasks as \textit{AI managerial labor}.

One major form of AI managerial labor involved developing prompts, which required the additional work of establishing context. Tasks in this category included explaining big-picture goals, setting character limits for outputs, requesting specific output styles, providing examples of anticipated output, or providing a concrete starting point (e.g., a code to start from)~\cite{4,6,16}. In rare cases, practitioners removed context by redacting sensitive information for compliance reasons from a prompt~\cite{18} or under-specified prompts to provoke surprising outputs~\cite{9}.
% One major form of AI managerial labor involved developing prompts. Practitioners frequently went to great lengths to furnish their prompts with context to help produce useful outputs for their work. For some, establishing context took the form of explaining their big-picture goals, setting character limits for outputs, requesting specific output styles (e.g., bulleted list), providing examples of anticipated output, or providing a concrete starting point (e.g., a code to start from)~\cite{4,6,16}. In rare cases, practitioners removed context, e.g., redacting sensitive information for compliance reasons from a prompt~\cite{18} or under-specifying prompts to provoke surprising outputs~\cite{9}. Some individuals were less willing to expend this kind of effort~\cite{15} and took an alternative route of curating ready-made prompts~\cite{14}. 
In a few instances, curation became a social endeavor of exchanging prompts; we discuss this in more depth in Section~\ref{sec:relational} on relational crafting. 

% We even identified certain practitioners who developed methodological practices around designing their prompts. For example, a developer in ~\cite{16} drafted their prompts in a separate tool before opening the GenAI, 

% \begin{quote}
%     \textit{``So there’s a step today that I often take before talking to ChatGPT, which is creating my prompt, creating my question. So, *I open a notepad*, think about what I’m going to put in the structure, and then I copy and paste it into the chat''}.
% \end{quote}

% A few practitioners viewed prompt designing as an iterative process. For instance, a software developer in~\citet{16} described receiving a wrong version of PHP code snippet from their initial prompt. The GenAI required additional clarification in follow-up prompts to get the syntax the developer was looking for. Others simply found it limiting to provide all the context in a single prompt, preferring to work incrementally towards their ultimate goal with smaller prompts~\cite{4}. Another developer observed,``\textit{Every time you prompt, you’re giving it clues to get closer to the idea you have in your head. The more words that you use, the closer the image can get to what you imagine~\cite{6}}.''  Interestingly, some practitioners included the GenAI in these conversations. For instance, some asked it about its capabilities and limitations or asked it to suggest prompts to use. Others even prompted it to critique its \textit{own} work~\cite{6}. \fixme{Underspecifying prompts [9]. Should it come here?}


A complementary form of AI managerial labor constituted refining GenAI outputs to fit in participants' workflows. Within this category, the most prominent activity consisted of verifying and correcting details in GenAI outputs. ``\textit{I don't trust the AI. \dots So, I have to read everything and validate what it's doing}''~\cite{15}. Beyond verification, some practitioners expended significant manual labor to utilize GenAI outputs. For instance, a UX designer from~\cite{2} spent time switching back and forth between GenAI tools and Photoshop to post-process images. In another domain,~\cite{17} described developers' post-processing pull requests authored by Copilot to either add missing information or remove superfluous context. Sometimes new processes emerged around this post-processing, such as maintaining a changelog to track changes made to the GenAI outputs. 

% In a more subtle form of AI managerial labor, practitioners sometimes exerted themselves in secondary ways around how they engaged with the AI. For example, one software developer described managing when the GenAI tool was on or off, because it gave proactive suggestions that could disrupt their workflow~{}.

AI managerial labor also involved configuring GenAI models at the system or application level, enabling practitioners to exert greater control over (a) data privacy, (b) model adaptation to their use case, and (c) the timing and context of the models' engagement with their workflows. One way practitioners took more control of the privacy of their data was by only using open-source models, as opposed to off-the-shelf models. They self-hosted them to prevent sensitive material from being exposed~\cite{1}. Practitioners also took control over adapting the model to their specific use cases by configuring settings. For example, a developer adjusted an API setting called the model temperature\footnote{https://platform.openai.com/docs/api-reference/chat/create\#chat-create-temperature}, which determines the randomness of the output given by the GenAI model, in order to generate more precise and helpful answers~\cite{6}.

%In a more involved example, a fact-checker fine-tuned GPT-3.5-Turbo with their own custom dataset to generate contextual queries. They could then leverage the contextual queries to perform more sophisticated queries in a different language that the practitioners did not speak ~\cite{1}. Fine-tuning like this is a process by which developers can provide specialized data to create their own custom version of a GenAI model which works better than the off-the-shelf model for their specific use case~\cite{ohm2024focusing}. 

While the above configurations were conducted at the systems-level, practitioners also spent time configuring GenAI tools at the application level, such as by managing when and how a GenAI provided them with suggestions in order to prevent disruptions to their work.  For instance, practitioners found Copilot's proactive suggestion feature a source of interruption. One developer, in \cite{16}, kept Copilot off during the early stages of a project, saying, ``\textit{it tends to provide a lot of suggestions, which kind of hinders my thought process.}'' In contrast, standalone GenAI tools like ChatGPT were more reactive and provided practitioners opportunities to choose when and for what activity to use them. 

% For instance, Copilot, unlike ChatGPT, was a more integrated interface in developers' code editor environments. Although this integration was useful in many ways, some developers described Copilot's proactive suggestion feature as interrupting their work. One developer, in \cite{16}, kept Copilot off during the early stages of a project, saying, 

% \begin{quote}
% \textit{``I believe that when I’m starting a project from scratch, I tend to keep it turned off because, as it still has very little context of what you’re doing, it tends to provide a lot of suggestions, which kind of hinders my thought process.''}
% \end{quote}


% A developer in \cite{22} shared how they selected integrated, proactive GenAI tools or standalone, reactive GenAI tools for different purposes:

% \begin{quote}
%     \textit{``I definitely use [Github Copilot’s Auto-Complete] much more [than Chat GPT]. I rarely actually go into Chat GPT... but I find that when I do use it, it’s more of like- it’s particular use case is kind of getting you unstuck from a spot rather than just integrating it into your regular flow \dots Sometimes it gives solutions and sometimes it’s just kind of to bounce ideas off.''}
% \end{quote}

