
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{titletoc}
\usepackage{minitoc}
\usepackage{changepage}
% \usepackage{ulem}
\usepackage{color}
% \renewcommand{\thefootnote}{}
\newcommand{\Qu}[1]{[\textcolor{red}{Qu:#1}]}


\title{Equivariant Masked position prediction for efficient molecular representation}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
% $^{\dag}$
\author{Junyi An\textsuperscript{1}\thanks{Equal contribution: Junyi An and Chao Qu. Correspondence to Junyi An: \texttt{anjunyi@sais.com.cn}}, Chao Qu\textsuperscript{2}$^{*}$, Yunfei Shi\textsuperscript{1}, Xinhao Liu\textsuperscript{3}, Qianwei Tang\textsuperscript{4}, Fenglei Cao\textsuperscript{1}\thanks{Corresponding authors: Fenglei Cao and Yuan Qi}, Yuan Qi\textsuperscript{5}$^{\dag}$\\
\textsuperscript{1}Shanghai Academy of Artificial Intelligence for Science\\
\textsuperscript{2}INFLY TECH (Shanghai) Co., Ltd.\\
\textsuperscript{3}School of Computer Science, Fudan University\\
\textsuperscript{4}State Key Laboratory for Novel Software Technology, Nanjing University\\
\textsuperscript{5}Artificial Intelligence Innovation and Incubation (AI$^{3}$) Institute, Fudan University \\
% \texttt{junyian@smail.nju.edu.cn, \{hippo,brain,jen\}@cs.cranberry-lemon.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory. Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features. EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties. Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches. 
\footnotetext{Our code is released in https://github.com/ajy112/EMPP}
% Graph neural networks (GNNs) have shown considerable promise in the field of computational chemistry. However, the limited availability of molecular data raises questions about GNNs' ability to accurately capture the fundamental physics and chemistry involved, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised learning approach, Equivariant Masked Position Prediction (EMPP), which is grounded in intramolecular potential and force theory. Unlike traditional attribute masking techniques, EMPP defines a subtle position prediction tasks which is more well-posed and able to enhances the learning of quantum mechanical features. Compared to 

% , EMPP circumvents the Gaussian mixtures commonly employed in traditional denoising methods, enabling the model to learn more precise physical properties. Experimental results demonstrate that EMPP significantly enhances quantum property predictions when integrated with TorchMD and Equiformer backbones, outperforming existing state-of-the-art self-supervised methods. Furthermore, EMPP overcomes the limitations of denoising methods tied to equilibrium states, expanding its applicability to a wider chemical space. The code for EPMM is anonymously released in https://github.com/AnonymousACode/EMPP.
% Graph neural networks (GNNs) have demonstrated significant potential in computational chemistry. However, the scarcity of molecular data raises concerns about whether GNNs can truly capture the underlying physics and chemistry, limiting their generalization capabilities. To address this, we propose a novel self-supervised approach called Equivariant Masked Position Prediction (EMPP), rooted in intramolecular potential and force theory. EMPP addresses key bottlenecks in mainstream self-supervised molecular methods. Compared to attribute masking techniques, EMPP defines a subtle position prediction tasks which is more well-posed and able to enhances the learning of quantum mechanical features. Additionally, EMPP bypasses the Gaussian approximations used in denoising methods, allowing the model to learn more accurate physical properties. Experiments demonstrate that EMPP significantly improves quantum property predictions on TorchMD and Equiformer backbones, outperforming state-of-the-art self-supervised methods. Furthermore, EMPP overcomes the limitations of denoising methods tied to equilibrium states, expanding its applicability to a wider chemical space. The code for EPMM is anonymously released in https://github.com/AnonymousACode/EMPP.


 
% Graph neural networks (GNNs) have demonstrated significant potential in computational chemistry. However, the scarcity of molecular data raises concerns about whether GNNs can truly capture the underlying physics or chemistry, limiting their generalization. Currently, two main self-supervised approaches address data limitations but face their own challenges: (i) Attribute masking methods suffer from ill-posed reconstructions and neglect deep physical mechanisms; (ii) Denoising methods approximate the Boltzmann distribution in equilibrium structures by using Gaussian distributions, yet defining accurate Gaussian parameters remains difficult. To overcome these issues, we propose a novel self-supervised approach called Equivariant Masked Position Prediction (EMPP), grounded in physical principles. EMPP randomly masks atomic positions while preserving other attributes, turning the position prediction into a well-posed task. Furthermore, it leverages equivariant representations of unmasked atoms to estimate the most probable positions, bypassing the need for Gaussian approximations. Experimental results demonstrate that EMPP significantly improves quantum property predictions with TorchMD and Equiformer backbones. Additionally, EMPP outperforms current state-of-the-art self-supervised methods and extends applicability beyond equilibrium states, enabling broader exploration within chemical spaces.
% Graph neural networks (GNNs) have shown great potential in computational chemistry. Nevertheless, the lack of molecular data still raises concerns about whether the molecular GNNs truly capture underlying knowledge of physics or chemistry, limiting generalization and robustness. There are now two main self-supervised methods to overcome data scarcity, however meeting another challenges: (i) Attributes masking methods are limited by ill-posed reconstruction and neglection of deep physical mechanisms; (ii) Denoising methods use the Gaussian distribution to approximate Boltzmann distribution in equilibrium structure, meeting the difficult that the parameters of Gaussian distribution can not be accurately defined. To address these limitations, we propose a novel self-supervised method, termed equivariant masked position prediction (EMPP), which is based on physical background. EMPP randomly masks atomic position while retaining other attributes, making the position prediction a well-posed tasks. Moreover, we use the equivariant representation of unmasked atoms to estimate the position with the highest probability, bypassing the Gaussian approximation. Experiments show that EMPP leads a greatly improvement on quantum property prediction based on TorchMD and Equiformer backbones. Moreover, EMPP exceeds current state-of-the-art self-supervised methods, and breaks the constraints of denoising methods on equilibrium states, allowing it to be applied to a broader chemical space.
\end{abstract}

\section{Introduction}
\setlength{\abovedisplayskip}{4pt}
\setlength{\abovedisplayshortskip}{1pt}
\setlength{\belowdisplayskip}{4pt}
\setlength{\belowdisplayshortskip}{1pt}
\setlength{\jot}{3pt}
\setlength{\textfloatsep}{6pt}	
\setlength{\abovecaptionskip}{1pt}
\setlength{\belowcaptionskip}{1pt}
% Masked token prediction has been a game-changer in natural language processing (NLP), enabling models like BERT to excel at various language tasks by predicting hidden words from context. This approach has since been expanded to multimodal data, as seen in the Multimodal Masked Autoencoder (M3AE), which applies the technique to both images and text. The success of masked token prediction has significantly advanced AI's language understanding and representation learning capabilities across different data types. 

% The success of 
% The achievements of large language model is . Recently, large language model has ac

% % 大语言模型已经在AI领域建立了重要的里程碑。他成功的一个重要原因就用海量的预训练数据让模型理解了可迁移的文本知识。而以masked token prediction为主的预训练算法在扩充数据中起到了举足轻重的作用。然而，在计算化学领域，数据依然是重要的瓶颈。目前，GNN模型已经能够取得强大的分子表征，但

% Recently, large language model has show

% Large language models (LLMs) \citep{si2023prompting,dubey2024llama} have achieved significant breakthroughs, demonstrating the powerful capabilities of neural networks in processing human language. A key driver behind these advancements is the availability of trillions of training tokens \citep{borgeaud2022improving}. 


Graph neural networks (GNNs) have found widespread application in computational chemistry. However, unlike other fields such as natural language processing (NLP), the limited availability of molecular data hampers the development of GNNs in this domain. For example, one of the largest molecular dataset, OC20 \citep{chanussot2021open}, contains only 1.38 million samples, and collecting more molecular data with ab initio calculations is both challenging and expensive. To address this limitation, molecular self-supervised learning has gained increasing attention. This approach enables molecular GNNs to learn more general physical and chemical knowledge, enhancing performance in various computational chemistry tasks, such as drug discovery \citep{hasselgren2024artificial} and catalyst design \citep{chanussot2021open}.

% Graph neural networks (GNNs) have become pivotal in computational chemistry, helping to overcome various challenges in molecular dynamics simulations \citep{batzner2022,rackers2023recipe}, catalyst design \citep{chanussot2021open}, and more. However, the progress of molecular GNNs is hindered by the scarcity of sufficient labeled data. In contrast, the natural language processing (NLP) field has demonstrated that access to vast datasets is a key driver behind the breakthroughs in large language models (LLMs) \citep{si2023prompting,dubey2024llama}, with self-supervised learning techniques, particularly masked token prediction \citep{devlin-etal-2019-bert,DBLP:conf/emnlp/SinhaJHPWK21}, playing a central role in leveraging large-scale language data. In computational chemistry, however, the largest dataset, OC20 \citep{chanussot2021open}, includes only 1.38 million examples, and collecting datasets at the scale of billions remains both challenging and costly. Consequently, self-supervised learning on molecular structures has emerged as a vital method for improving molecular model representations.

% the significant reason for the milestone achievements of large language models is the vast amount of pre-training data that enables models to understand transferable textual knowledge. Among this, self-supsiervised algorithms focusing on masked token prediction play a pivotal role in expanding the data.

% data engineering has achieved a 

% offering efficient approximations for complex quantum calculations l
% ike density functional theory (DFT) \citep{zitnick2022spherical,liao2023equiformer,liao2024equiformerv}. Their ability to deliver high-accuracy results with reduced computational costs has expanded their applications in molecular dynamics simulations \citep{gilmer2017neural,batzner2022,rackers2023recipe}, catalyst design \citep{chanussot2021open}, and more. However, the advancement of GNNs is hampered by a lack of sufficient labeled data.

% Graph neural networks (GNNs) have become indispensable in computational chemistry, offering efficient approximations for complex quantum calculations like density functional theory (DFT) \citep{zitnick2022spherical,liao2023equiformer,liao2024equiformerv}. Their ability to deliver high-accuracy results with reduced computational costs has expanded their applications in molecular dynamics simulations \citep{gilmer2017neural,batzner2022,rackers2023recipe}, catalyst design \citep{chanussot2021open}, and more. However, the advancement of GNNs is hampered by a lack of sufficient labeled data. In contrast to data-rich domains like natural language processing (NLP) and computer vision (CV), where models such as GPT-3 \citep{} and ViT-22B \citep{dehghani2023scaling} are trained on datasets with billions of examples, the largest molecular dataset, OC20 \citep{chanussot2021open}, contains only 1.38 million examples and is focused on the narrow chemical space of catalytic molecules. Moreover, acquiring first-principles molecular data is far more challenging than gathering text or image data, which severely limits the representation capabilities of GNNs and prevents them from reaching their full potential.


% the field faces unique challenges when compared to more data-rich domains like natural language processing (NLP) and computer vision (CV). The generation of atomic data is inherently constrained by the computational intensity of ab initio calculations, which limits the scale of available datasets. For instance, the OC20 dataset, one of the largest in this domain, comprises a mere 138 million examples—a stark contrast to the vast datasets that underpin models like GPT-3, trained on hundreds of billions of words, or ViT-22B, which leverages nearly 4 billion images. This disparity in data availability presents a significant hurdle for the training of robust and generalizable GNN models in atomic systems, necessitating innovative approaches to data generation, model architecture, and learning strategies to fully harness the potential of GNNs in this context.

% Graph neural networks (GNNs) have become indispensable in computational chemistry, offering efficient approximations for complex quantum calculations like density functional theory (DFT) \citep{zitnick2022spherical,liao2023equiformer,liao2024equiformerv}. Their ability to deliver high-accuracy results with reduced computational costs has expanded their applications in molecular dynamics simulations \citep{gilmer2017neural,batzner2022,rackers2023recipe}, catalyst design \citep{chanussot2021open}, and more. However, GPT



% This method only need the molecular structure, facilitating the learning of inherent features in molecule, such as atomic collocation.
% To address the data scarcity and enhance atomic representations, various molecular self-supervised methods have been developed. One prominent approach is masked molecular modeling, inspired by the success of masked language modeling in NLP \citep{devlin-etal-2019-bert}. This method involves masking or corrupting a portion of the input data, with the goal of learning to reconstruct it. \citet{hu2019strategies,inae2023motif} introduced techniques that mask atomic attributes and reconstruct them using neighboring structures, helping the model capture molecular semantics, such as geometric properties and bonding principles. While these masking methods have led to significant performance gains, they still suffer from two key limitations:
% \begin{itemize}
%     \item \textbf{Underdetermined Reconstruction.} The reconstruction of masked atoms may have multiple valid solutions. In NLP, pre-training can account for this variability by leveraging large corpora to cover most possible scenarios. However, obtaining diverse molecular data is more challenging, leading models trained with masking methods to develop biases toward specific chemical spaces.
    
%     \item \textbf{Loss of quantum mechanics (QM) Knowledge.} Masked attributes can often be inferred through macroscopic features \citep{}. However, most molecular properties are strongly influenced by complex QM interactions, which are difficult to capture by simply reconstructing masked attributes.

% \end{itemize}
% QM knowledge is closely related to the 3D structure of molecules \citep{messiah2014quantum}. Unimol \citep{zhou2023unimol,lu2023highly} extends the masking method to 3D molecules and introduces 3D position recovery to learn general QM knowledge. However, its recovery method lacks physical plausibility. To incorporate stronger physical priors, denoising methods \citep{zaidi2023pretraining,feng2023fractional,ni2024sliced} add noise to atomic 3D positions and predict noise. These methods, which consider equilibrium structures in a Boltzmann distribution, are effectively learning force fields \citep{zaidi2023pretraining}. However, both Unimol and denoising methods are limited to learning QM features in the immediate vicinity of the original positions. Furthermore, denoising methods struggle to generalize to non-equilibrium states.


% Current mainstream self-supervised methods for molecules can be grouped into two main categories: masking and denoising. Masking methods \citep{hu2019strategies,hou2022graphmae,inae2023motif} extend the concept of masked token prediction in natural language processing (NLP) to graph learning, where graph information, such as node attribute, is masked instead of tokens. However, these approaches face two key limitations: underdetermined reconstruction and a lack of deep quantum mechanical (QM) insight,  as illustrated in Figure \ref{fig:compare}(a): (i) reconstructing a masked iodine atom can yield multiple possible solutions; as more atoms are masked, the number of potential outcomes increases exponentially, making it difficult for training data to account for all scenarios; (ii) the position of the masked carbon atom can be inferred from the 2D geometry principle of a benzene ring, leading the model to overlook interactions between atoms that are essential for learning QM properties \citep{messiah2014quantum}. In contrast, denoising methods \citep{zhou2023unimol,zaidi2023pretraining,feng2023fractional} are physics-informed and enable self-supervised learning of QM information in equilibrium structures. The core idea involves adding noise to atomic positions and predicting it (see Figure \ref{fig:compare}(b)), which resembles approximating local minima on the potential energy surface (PES) by a Gaussian mixture distribution \citep{zaidi2023pretraining}. This makes noise prediction akin to learning forces, i.e. the derivative of the PES. However, the real PES presents diverse and unknown local minima shapes (see Figure \ref{fig:compare}(d)), making it difficult to accurately parameterize the Gaussian mixture distribution. Our ablation studies in Section \ref{sec:ablation} support this point, revealing that model performance is sensitive to standard deviation $\sigma$ of Gaussian mixture distribution. We propose bypassing this limitation by using a position prediction process, which retains the model's ability to learn critical QM features.

% Current self-supervised methods for molecular learning can be classified into two main categories: masking and denoising. 
Current self-supervised methods for molecular learning contain two mainstream categories: masking and denoising. Masking methods \citep{hu2019strategies,hou2022graphmae,inae2023motif} adapt the concept of masked token prediction from natural language processing (NLP) to graph learning, where graph information, such as node attribute, is masked instead of token. However, there are two 
major limitations: underdetermined reconstruction and lack of deep quantum mechanical (QM) insight. As illustrated in Figure \ref{fig:compare}(a), (i) reconstructing attribute of the masked iodine atom can yield multiple possible solutions, and as the number of masked atoms increases, the number of solutions will increase rapidly, making it difficult for training data to cover all possibilities; (ii) the attribute of the masked carbon atom can be inferred from the 2D geometric principles of the benzene ring, causing the model to overlook essential atomic interactions needed for learning QM properties \citep{messiah2014quantum}. In contrast, denoising methods \citep{zaidi2023pretraining,feng2023fractional} are physics-informed and facilitate self-supervised learning of QM information in equilibrium structures. Their core idea involves adding noise to atomic positions and predicting them (see Figure \ref{fig:compare}(b)). In this process, the local minima of the potential energy surface (PES) are approximated using Gaussian mixture distributions \citep{zaidi2023pretraining}, making noise prediction equivalent to learning the forces, i.e., the derivatives of the PES. However, the actual PES presents diverse and unknown local minima shapes (see Figure \ref{fig:compare}(d)), making it difficult to accurately parameterize the Gaussian mixture distribution. Our ablation studies in Section \ref{sec:ablation} further illustrate that model performance is sensitive to the standard deviation $\sigma$ of the Gaussian mixture. We propose addressing these limitations through a position prediction process, which allows the model to effectively learn critical QM features.

% The current mainstream self-supervised methods on molecules are mainly divided into two categories: masking and denoising. For masking methods \citep{hu2019strategies,hou2022graphmae,inae2023motif}, they are an extension of Masked token prediction in graph learning, replacing masking tokens with masking graph information. However, there are two limitations: (i) underdetermined reconstruction and lack of deep quantum mechanics (QM) information. As shown in the Figure \ref{fig:compare}, the masked iodine atom will have multiple solutions during reconstruction. And when the number of masked atoms in a molecule increases, the number of solutions will increase rapidly, making it difficult for training data to cover all possibilities; (ii) the masking methods lack learning 3D information, which is key information for calculating QM properties, such as bond lengths, bond angles, etc. In Figure \ref{fig:compare}, the masked carbon atom can be simply inferred by the geometric properties of the benzene ring, making the model only able to learn some superficial features. Denoising \citep{zaidi2023pretraining,feng2023fractional,ni2024sliced}, a physics-informed technique, can self-supervisedly learn 3D QM information under equilibrium structures. The core idea is to add noise to the spatial positions and predict the noise. This approach is equivalent to approximating the local minima regions of the potential energy surface (PES) with a Gaussian mixture \citep{zaidi2023pretraining}, thus making the prediction of noise equivalent to learning the force (the derivative of the PES). However, the local minima regions on the real PES are diverse and unknown (See Figure \ref{fig:compare}), making it difficult to determine the parameters of the Gaussian mixture. Our ablation experiments in Section \ref{sec:ablation} also show that the model's performance is highly sensitive to the Gaussian parameters.


% current methods are limited to learning quantum mechanical features in the immediate vicinity of the original positions and struggle to generalize to non-equilibrium states.

In this paper, we introduce \emph{Equivariant Masked Position Prediction} (EMPP), a novel training method designed to enhance molecular representations in GNNs. In EMPP, we randomly mask an atom’s 3D position while retaining its other attributes, such as atomic number, ensuring that position prediction remains a well-posed problem compared to attribute masking methods. Furthermore, the masked atom’s position is determined by quantum mechanics within the neighboring structure, which our method is designed to learn. It is important to note that EMPP is fundamentally different from denoising methods. As illustrated in Figure \ref{fig:compare}(c), EMPP completely removes the node of the masked atom and uses the embeddings of unmasked atoms to predict its position. Consequently, EMPP can bypass the approximation of Gaussian mixture distributions, resulting in a more deterministic position prediction process, as detailed in Section \ref{sec:model}. Additionally, since EMPP significantly alters the original molecular graph by removing nodes, it can generate a vast number of diverse data during training. Given a dataset of $M$ molecules with an average of $N$ atoms each, EMPP can produce $O(MN)$ well-posed examples. 
% Moreover, masking multiple atom positions within a single molecule can further expand the dataset. While this introduces some ill-posedness, our ablation study in Section \ref{sec:ablation} shows that the additional data still enhances GNN generalization (\Qu{rephrase}).


% EMPP focuses on capturing quantum mechanics over larger spaces, such as the full potential PES of masked atoms. Furthermore, recent equivariant models have demonstrated that higher-degree equivariant features can effectively predict position distributions \citep{daigavane2024symphony}, further validating the feasibility of EMPP. 

\begin{figure}[t]
\centering

\includegraphics[scale=0.63,trim=3.6cm 11.2cm 2.2cm 2.3cm, clip]{pic/compare.pdf}
\caption{(a, b, c) Comparison of three molecular self-supervised methods using real halobenzenes (Ph-X) as an example: (a) Masking atomic attributes, such as atomic number, and reconstructing them; (b) Adding noise to atomic positions and predicting the noise; (c) Completely masking positions and inferring them based on neighboring structures. (d) Principle of the denoising methods: they utilize Gaussian mixture distributions to approximate the local minima of the PES, allowing the noise terms to estimate the forces, i.e. derivatives of the PES. The two local minima in the figure correspond to two equilibrium atoms, each requiring a different standard deviation $\sigma$ for approximation. As the shape of the PES is unknown, determining $\sigma$ requires an empirical approach.}\label{fig:compare}


% (d) The principle of denoising method: using a Gaussian mixture distribution (as illustrated by the two components in the figure) to approximate local minima of the PES, thereby the noise representation approximates the force, i.e., the derivative of the PES. However, this approximation is sensitive to the standard deviation $\sigma$ of the Gaussian mixtures. As the shape of the PES is unknown, defining the $\sigma$ requires an empirical approach.}\label{fig:compare}


% Comparison of three molecular self-supervised methods. We use real halobenzenes (Ph-X) as an example. (a) The attribute masking method randomly replaces atoms with a mask symbol and predicts the masked atom's attribute (atomic number). (b) The denoising method adds noise to the atom positions and predicts the noise.  (c) In contrast, our EMPP method completely removes the atom's node from the graph, provides the atom type, and predicts the position distribution using the representations of neighboring atoms. \Qu{maybe talk briefly about the force explanation here} \Qu(d: what happens if $\sigma$ is overestimated or underestimated)}
\vspace{-0.2em}
\end{figure}

% Specifically, when the masked examples are in non-equilibrium states, EMPP can infer the distribution of masked positions based on known quantum properties.

EMPP is a versatile training paradigm. It enables self-supervised training of models on unlabeled equilibrium data to capture general knowledge, while also serving as an auxiliary task to connect known quantum properties with atomic positions. Experiments demonstrate that EMPP enhances the generalization of advanced equivariant GNNs \citep{tholke2022equivariant, liao2023equiformer} across various molecular tasks, regardless of whether extra data is used for pre-training. Additionally, EMPP achieves state-of-the-art performance compared to previous masking and denoising methods. Our key contributions can be summarized as follows: \emph{we propose a molecular learning paradigm that utilizes position prediction to address the challenges posed by previous masking and denoising approaches, paving a novel path for molecular learning.}

% EPMM is a versatile training paradigm. On one hand, EMPP can train models self-supervisedly on unlabeled equilibrium data to capture general knowledge. On the other hand, when the values of quantum properties are known, EMPP can serve as an auxiliary task to establish a connection between these properties and atomic positions. Experimental results show that, whether or not utilizing unlabeled data for pre-training, EMPP enhances the generalization of equivariant GNNs \citep{tholke2022equivariant, liao2023equiformer} across various molecular tasks. Compared to previous masking and denoising methods, EMPP achieves state-of-the-art performance. Our key contributions can be summarized as: \emph{a molecular learning paradigm that utilizes position prediction to address the challenges of previous masking and denoising approaches, paving a novel path for enhancing the generalization of molecular models.} 
% effectively learns deep quantum features, pave 

% (i) a novel masked molecular modeling approach that effectively learns deep molecular features; and (ii) experimental validation showing that our method can significantly expand training data, while maintaining diversity and consistency.


% EMPP leverages them to infer masked positions in both equilibrium and non-equilibrium structures. 
% \Qu{reorganize this paragraph? }The proposed EMPP supports both self-supervised and supervised tasks. When quantum properties are known (\Qu{ such as ? }), EMPP leverages them to infer masked positions in both equilibrium and non-equilibrium structures. In summary, EMPP can be employed as a self-supervised task to learn general molecular knowledge or as an auxiliary task to improve model generalization in the specific supervised task. Experimental results demonstrate that EMPP improves the generalization of equivariant GNNs \citep{tholke2022equivariant,liao2023equiformer} across a variety of molecular tasks. Compared to previous masking and denoising methods, EMPP achieves state-of-the-art performance, offering improvements for both equilibrium and non-equilibrium data. Our key contributions are: (i) a novel masked molecular modeling approach that effectively learns deep molecular features; and (ii) experimental validation showing that our method can significantly expand training data, while maintaining diversity and consistency.

\vspace{-1em}
\section{Preliminaries}
\vspace{-1em}

In this section, we briefly review the mathematical background, which includes molecular property prediction, equivariance, spherical harmonics, and so on. More detailed introductions are deferred to Appendix \ref{app:spherical_harmonic}. We list the notations frequently used in the following. We denote the unit sphere as $S^{2}$, where the spherical coordinates $(\theta, \phi)$ are the polar angle and the azimuth angle, respectively. The symbol $\mathbb{R}$ represents the set of real numbers, while $\mathbf{R}$ represents the rotation matrix for 3D vectors. We use $SO(3)$ to denote the special orthogonal group, i.e. the 3D rotation group. 
\vspace{-1em}
\subsection{Molecular Property Prediction}
\vspace{-1em}
Molecular property prediction aims to construct a projection from the molecular 3D structure to the molecular properties. In the following, we use the $z \in \{1,...,118 \}$ to denote the atomic number, or $\mathbf{z}$ to denote richer atomic attributes, including atomic numbers, chemical environments, and other features. We use the term $\mathbf{p}$ to denote atomic 3D position. In a $N$-nodes molecule, properties can be divided into global properties $y \in \mathbb{R}$ and node-wise properties $\mathbf{y} \in \mathbb{R}^{N}$. In detail, given a 3D molecular $S = \{(\mathbf{z}_{i}, \mathbf{p}_{i}) | i \in \{1, \dots, N\} \}$, the properties can be predicted by a GNN:
\begin{equation}
    y = \mathrm{PRED}(\mathbf{f}),  \text{and } \mathbf{f} = \mathrm{GNN}(S),
\end{equation}
where $\mathbf{f}$ represents the node embeddings in the final GNN layer, and $\mathrm{PRED}(\cdot)$ is the prediction head. 
% Details of the message passing process in the GNN are provided in Appendix ?. 
\vspace{-1em}
\subsection{Equivariance}
\vspace{-1em}
Given any transformation parameter $g\in G$, a function $\varphi: \mathcal{X} \to \mathcal{Y}$ is called equivariant to $g$ if it satisfies:  
\begin{equation}
    \label{equ:equivar}
    T'(g)[\varphi(x)] = \varphi(T(g)[x]), 
\end{equation}
where $T'(g): \mathcal{Y} \to \mathcal{Y}$ and $T(g):\mathcal{X} \to \mathcal{X}$ denote the corresponding transformations over $\mathcal{Y}$ and $\mathcal{X}$, respectively. Invariance is a special case of equivariance where $T'(g)$ is an identity transformation. 
% It says that the output of $\varphi$ is unaffected by the transformation applied to the input. 
In this paper, we mainly focus on the $SO(3)$ equivariance and invariance, since it is closely related to the interactions between atoms in molecule \footnote{Invariance of translation is trivially satisfied by taking the relative positions as inputs.}. In other words. The backbone and prediction head should adhere to \eqref{equ:equivar}.


% Equivariance is a desirable property for atomic interactions and 3D point clouds due to its exact description of relationship between model features and $SO(3)$-transformation \Qu{}. 


\vspace{-1em}
\subsection{Spherical Harmonics and Steerable Vector}
\vspace{-1em}
\textbf{Spherical harmonics}, a class of functions  defined over the sphere $S^2$, form an orthonormal basis and have some special algebraic properties widely used in equivariant models~\citep{kondor2018clebsch,cohen2018spherical}. In this paper, we use the real-valued spherical harmonics denoted as 
$\{ Y^{l}_{m}: S^{2} \to \mathbb{R} \}$, where $l$ and $m$ denote degree and order, respectively. It is known that any square-integrable function defined over $S^2$ can be expressed in a spherical harmonic basis via 
\begin{equation}\label{equ:fourier_tran}
    f(\theta, \phi) = \sum_{l=0}^{\infty} \sum^{l}_{m=-l} f^{l}_{m}Y^{l}_{m}(\theta, \phi),
\end{equation}
where $f^{m}_{l}$ is the Fourier coefficient.
For any  vector $\Vec{\mathbf{r}}$ with orientation $(\theta, \phi)$, we define $\mathbf{Y}^{l}(\vec{\mathbf{r}}/\| \vec{\mathbf{r}}\|) = \mathbf{Y}^{l}(\theta,\psi) = [ Y^{l}_{-l}(\theta,\psi); Y^{l}_{-l+1}(\theta,\psi);...; Y^{l}_{l}(\theta,\psi) ]^T$, a vector with $2l+1$ elements. Furthermore, we define the spherical harmonics representation for any direction:
\begin{equation}
    \mathrm{sh}^{l}(\frac{\vec{\mathbf{r}}}{\| \vec{\mathbf{r}}\|}) = [\mathbf{Y}^{0}(\frac{\vec{\mathbf{r}}}{\| \vec{\mathbf{r}}\|});\mathbf{Y}^{1}(\frac{\vec{\mathbf{r}}}{\| \vec{\mathbf{r}}\|});...;\mathbf{Y}^{l}(\frac{\vec{\mathbf{r}}}{\| \vec{\mathbf{r}}\|})] .
\end{equation}
It forms a $(l+1)^{2}$ vector. Equivariant models \citep{zitnick2022spherical,liao2023equiformer,an2024hybrid} typically set a maximum degree $L_{max}$ and construct node embeddings with $C$ channels, resulting in an embedding size of $(L_{max}+1)^{2} \times C$. The full mathematical form of spherical harmonics can be found in Appendix \ref{app:spherical_harmonic_detail}. 
% Spherical harmonics can be used to encode orientation~\citep{gasteiger2021gemnet, gasteiger2022gemnetoc} and map the representation in the frequency domain to the signal over the spatial domain~\citep{cohen2018spherical,zitnick2022spherical}. 

% Our model follows these ways but extends the flexibility of transformations to $S^{2}$. (\Qu{This word is weird}. \An{Revised.})

% we use $Y^{l}_{m}(\theta, \phi)$ to denote the basis with $l$ degree and $m$ order.  Spherical harmonics on each degree $l$ have $2l+1$ basis function (We use $\mathbf{Y}^{l}$ to denote the set of basis of degree $l$).

% In addition, our method also utilizes spherical to construct equivariant variables independent of orientation of molecular system \Qu{This word is weird}.

% Spherical harmonics can be used to encode orientation where the high-dimensional representation facilitates equivariance and helps to mining features of orientation~\citep{gasteiger2021gemnet, gasteiger2022gemnetoc}. Besides, our methods use spherical harmonics for the transformation to $S^{2}$ space, simliar t. 



% In this work, spherical harmonic are used to encode the angle features and construct SO(3)-equivariant models. 
A commonly used property of the spherical harmonics is that for any $ \mathbf{R}\in SO(3)$, we have $\mathbf{Y}^{l}(\mathbf{R}\vec{\mathbf{r}}) = \mathbf{D}^{l}(\mathbf{R}) \mathbf{Y}^{l}(\vec{\mathbf{r}})$, 
% there exists a corresponding vectors transforming in space of spherical harmonics:
% \begin{equation}
% \label{equ:Wigner-D}
%     \mathbf{Y}^{l}(\mathbf{R}\vec{\mathbf{r}}) = \mathbf{D}^{l}(\mathbf{R}) \mathbf{Y}^{l}(\vec{\mathbf{r}}), \\
% \end{equation}
where $\mathbf{D}^{l}(\mathbf{R})$ is a $(2l+1)\times(2l+1)$ matrix known as a Wigner-D matrix with degree $l$. Therefore, $\mathbf{R} $ and $\mathbf{D}^l(\mathbf{R})$ corresponds to $T(g)$ and $T'(g)$, respectively in \eqref{equ:equivar}. Following the convention in \citep{chami2019hyperbolic,brandstetter2021geometric}, we say $\mathbf{Y}^{l}(\vec{\mathbf{r}})$ is steerable by Wigner-D matrix of the same degree $l$. The $2l+1$-dimensional vector space on which a Wigner-D matrix of degree $l$ act is termed a type-$l$ steerable vector space, denoted by the superscript $(l)$ in this paper. 
% In fact, the Wigner-D matrix is an irreducible representation of the group $SO(3)$ and we refer interested readers to \citep{kondor2018clebsch}.
% A special case is the rotation for a vector in $\mathbb{R}^3$, where $T(g)$ is a rotation matrix $\mathbf{R}$ and $T'(g) =\mathbf{R}$ too.
% The vectors that can be transformed by Wigner-D matrix $\mathbf{D}^{l}(\mathbf{R})$ are termed type-$l$ vectors. 
% As shown in Figure~\ref{fig1}!, 
% The 3D rotation is related to the linear combination of the spherical harmonics so that we can trace the SO(3) transformation in high-dimensional space. Therefore, Wigner-D matrices are central to many force-centric models where the outputs are required to be equivariant~\citep{brandstetter2021geometric}.

% We also utilize Wigner-D matrices to compute message where angle information can be encoded into $S^{2}$ representation\Qu{a little confusing}. 

% Besides, the Wigner-D matrix satisfies the law of distribution:
% \begin{equation}
% \label{eq3}
    % \mathbf{Y}^{l}(\mathbf{R}\vec{\mathbf{r}_{1}}) + \mathbf{Y}^{l}(\mathbf{R}\vec{\mathbf{r}_{2}}) = \mathbf{D}^{l}(\mathbf{R}) (\mathbf{Y}^{l}(\vec{\mathbf{r}_{1}}) + \mathbf{Y}^{l}(\vec{\mathbf{r}_{2}})). \\
% \end{equation}
% Therefore, we can combine the different inputs represented by spherical harmonics in such a way that the output signal is equivariant to SO(3) transformation. 

% \subsection{Equivariant Operations}

\textbf{Equivariant operations}. To ensure the equivariance of the entire model, each operation must maintain equivariance. The e3nn library \citep{e3nn} offers common equivariant operations, including SO(3) linear transformations, SO(3) normalizations, gate activations, and Clebsch–Gordan (CG) tensor products. \citet{passaro2023reducing} further extends certain nonlinear equivariant operations to the sphere. In our work, we leverage existing equivariant models as backbones and employ aforementioned equivariant operations to construct our position prediction block.

% The key of the equivariant model is to design $\phi(\cdot)$ in \eqref{equ:equivar} which preserves the equivariance and at the same time enriches the abstract directional information. A simple approach is to encode the directional information by $\mathbf{Y}^l$ over each node or edge, and then do certain operations preserving equivariance such as aggregation and scaling with an invariant variable. However, if we depend solely on such operations, it would limit the expressiveness of the whole network. 

% The Clebsch-Gordan (CG) tensor product, originally describing the angular momentum coupling in quantum mechanics, provides another option and becomes the workhorse to design the equivariant networks \citep{thomas2018tensor,kondor2018clebsch,  brandstetter2021geometric}.
% % Recall that  Wigner-D matrices can be used as certain transformations denoted as $T'(g)$ in eq.~\eqref{equ:equivar}. Nonetheless, the main bottleneck of equivariant model design lies in the definition of $\phi(\cdot)$. Linear modules that can be used to combine the different signals with spherical harmonic representations are equivariant, while lack of nonlinearities will limit expresiveness power.  
% % In order to produce the useful equivariant output, we introduce an operation termed the ``Clebsch-Gordan (CG)'' tensor product:
% The CG tensor product $\otimes$ is defined as follows,
% \begin{equation}
% \label{equ:CG}
%     (\mathbf{u} \otimes \mathbf{v})^{l}_{m} = \sum^{l_{1}}_{m_{1}=-l_{1}} \sum^{l_{2}}_{m_{2}=-l_{2}} C^{(l,m)}_{(l_{1},m_{1})(l_{2},m_{2})} \mathbf{u}^{l_{1}}_{m_{1}}\mathbf{v}^{l_{2}}_{m_{2}},
% \end{equation}
% where $\mathbf{u}$ and $\mathbf{v}$ denote two irreducible representations with degree $l_{1}$ and $l_{2}$.  In \eqref{equ:CG},  $C$ denotes the CG coefficients, a sparse tensor, which produces non-zero only when
% \begin{equation}
% \label{equ:CG_condition}
% \begin{aligned}
%     |l_{1}-l_{2}| \le l \le (l_{1} + l_{2}),  m = m_{1} + m_{2}.
% \end{aligned}
% \end{equation}
%  Based on the CG tensor product, the output is a type-$l$ vector when $\mathbf{u}$ and $\mathbf{v}$ are type-$l_{1}$ and type-$l_{2}$ vectors. Furthermore, the equivariance of CG tensor product can combine the Wigner-D matrices with different degree, which is
% \begin{equation}
% \label{equ:CG_Wigner}
%     (\mathbf{D}^{l_{1}}_\mathbf{R} \mathbf{u} \otimes \mathbf{D}^{l_{2}}_\mathbf{R} \mathbf{v})^{l} = \mathbf{D}^{l}_\mathbf{R}(\mathbf{u} \otimes \mathbf{v})^{l}.
% \end{equation}
% % Here and following, $\otimes$ denotes the CG tensor product. 
% The proof can be found in Appendix \ref{app:spherical_harmonic}. In ~\eqref{equ:CG_Wigner}, all the Wigner-D matrices share the same transformation parameters. Therefore, the inputs of the CG tensor are two steerable vectors and the output is another steerable vector, which means it can mix the input information with preserving the equivariance. However, in practice, the CG tensor product is an expensive bilinear operation, limiting the using to high degrees. Our method provides a degree-reduction strategy to overcome this problem. See our discussions in Appendix \ref{app:degree_reduce}. The operation of CG tensor product is still linear, when $u$ is the embedding of node or edge and $v$ is a fixed vector such as the spherical harmonics representation of relative position. To incorporate the nonlinearity, a common approach is to include a gate or attention to weight \eqref{equ:CG} \citep{weiler20183d,fuchs2020se}.

% In general, the design of equivariant neural network may include the linear layer and nonlinear activation function alternatively. Therefore, activate functions are also required to be equivariant, which may include non-linear pointwise function on the sphere \citep{cohen2018spherical}. 

\vspace{-0.5em}
\section{Methodology}
\vspace{-0.5em}
In this section, we first revisit the existing molecular self-supervised methods and outline their primary limitations. Subsequently, we present our approach, \emph{Equivariant Masked Position Prediction} (EMPP), and elaborate on its implementation details.

\vspace{-1em}
\subsection{Revisiting the Vanilla Mask Method in Molecule Learning}\label{sec:revisit}
\vspace{-1em}
Similar to NLP, molecular self-supervised methods aim to learn the underlying chemical and physical mechanisms in molecular systems, such as valence bond theory \citep{shaik2007chemist} and force fields \citep{ponder2003force}. To this end, AttrMask \citep{hu2019strategies} pioneers a method that randomly masks atoms and predicts their attributes. More formally, we assume the $i,j,...$-th atoms are masked and the modified molecule is denoted as $\hat{S} = \{(\mathbf{z}_{1}, \mathbf{p}_{1}), ..., (\mathbf{M}, \mathbf{p}_{i}),..., (\mathbf{M}, \mathbf{p}_{j}), ..., (\mathbf{z}_{N}, \mathbf{p}_{N}) \}$, where $\mathbf{M}$ denotes the mask vector (like the \texttt{[MASK]} token in BERT \citep{devlin-etal-2019-bert}). The attributes of the masked atoms are predicted by a GNN as follows:
\begin{equation}
% \hat{\mathbf{z}}_{i} = \mathrm{PRED}(\mathbf{f}_{i}), \quad \text{where} \quad \mathbf{f}_{1}, ..., \mathbf{f}_{N} = \mathrm{GNN}(\hat{S}),
\hat{\mathbf{z}}_{i,j,...} = \mathrm{PRED}(\mathbf{f}_{i,j,...}),\quad \mathbf{f}_{1}, ..., \mathbf{f}_{N} = \mathrm{GNN}(\hat{S}),
\end{equation}
where $\mathbf{f}_{i,j,...}$ represents the GNN output features of the masked atoms, and $\mathrm{PRED}(\cdot)$ typically refers to a neural network. The objective is to minimize the discrepancy between the predicted attributes $\hat{\mathbf{z}}_{i,j,...}$ and the actual attributes $\mathbf{z}_{i,j,...}$, forming a self-supervised learning. Two key limitations emerge: the ill-posedness of attribute prediction and the inability to capture deep quantum mechanical features. These limitations, mentioned in the introduction, can be observed in Figure \ref{fig:compare}(a). Additionally, denoising methods \citep{zaidi2023pretraining,feng2023fractional} noise the atomic positions, where the modified molecule is denoted as $\hat{S} = \{(\mathbf{z}_{1}, \mathbf{p}_{1}), ..., (\mathbf{z}_{i}, \mathbf{p}_{i} + \mathbf{\epsilon}_{1}), ..., (\mathbf{z}_{i}, \mathbf{p}_{j} + \mathbf{\epsilon}_{2}), ..., (\mathbf{z}_{N}, \mathbf{p}_{N}) \}$. The GNNs are required to produce equivariant features and the $\mathrm{PRED}(\cdot)$ is used to predict noises $\mathbf{\epsilon}_{1}, \mathbf{\epsilon}_{2}, ...$ . These methods assume that Boltzmann distribution (i.e. exponent of PES) around equilibrium positions can be approximated by Gaussian mixture distribution \citep{zaidi2023pretraining}. 
% Therefore, forces, the derivatives of logarithm of Boltzmann distribution, is approximated by noise representations. 
However, the assumed distribution can not always approximate the true distribution. In fact, the shapes of PES in local minima are diverse and unknown in advance \citep{messiah2014quantum}, making it challenging to define the parameters of the Gaussian mixture distribution.

% Furthermore, 

% since forces are related to the derivatives of the PES, denoising enables GNNs to learn forces by predicting the noise within the Gaussian distribution \Qu{ within the Gaussian distribution?}. However, the assumed distribution can not always approximate the true distribution in equilibrium structures. In fact, the shapes of PES in local minima are diverse and unknown in advance \citep{}, making it challenging to fine-tune the parameters of the Gaussian mixtures.


% In NLP and CV, pre-training tasks typically require models to learn universal knowledge from language \citep{devlin-etal-2019-bert} or images \citep{chen2021pre}. Similarly, molecular pre-training seeks to uncover the underlying chemical and physical mechanisms fin molecular systems, such as valence bond theory \citep{shaik2007chemist} and force fields \citep{ponder2003force}. To this end, AttrMask \citep{hu2019strategies} introduced a method that randomly masks atoms and predicts their attributes based on the surrounding molecular structure. More formally, we assume $k$-th atom is masked and use the $\hat{S} = \{(\mathbf{z}_{1}, \mathbf{p}_{1}), ..., (\mathbf{M}, \mathbf{p}_{k}), ..., (\mathbf{z}_{N}, \mathbf{p}_{N}) \}$, where $\mathbf{M}$ denotes the mask vector (like the \texttt{[MASK]} tokens in BERT). The attribute of the masked atom is predicted by a GNN as follows:
% \begin{equation}
% \hat{\mathbf{z}}_{k} = \mathrm{PRED}(\mathbf{f}_{k}), \quad \text{where} \quad \mathbf{f}_{1}, ..., \mathbf{f}_{N} = \mathrm{GNN}(\hat{S}),
% \end{equation}
% where $\mathbf{f}_{k}$ represents the node features of the masked atom, and $\mathrm{PRED}(\cdot)$ typically refers to a neural network. The objective of the masked pre-training task is to minimize the discrepancy between the predicted attribute $\hat{\mathbf{z}}_{i}$ and the actual attribute $\mathbf{z}_{i}$. The atomic number is commonly used as the ground truth attribute due to its applicability across various molecular downstream tasks. Additionally, the atomic number of the masked atom acts as a self-supervised label, facilitating data augmentation. However, predicting the atomic number has two key limitations: (i) There are numerous possible solutions for recovering the masked atoms. As shown in Figure \ref{fig:compare}, the Ph-X molecules has . When more atoms are masked, the solution space expands significantly, potentially exceeding the scope of the training data. (ii) Masked attributes can sometimes be inferred from macroscopic features, such as geometric structure. For example, in Figure \ref{fig:compare}, the masked atom is part of a six-membered carbon ring and is likely to be a carbon atom due to the geometric constraints of the benzene ring. This limitation hinders the model's ability to efficiently represent atoms when predicting deep molecular properties, such as energy and force, which depend on complex interatomic interactions (quantum mechanics).

% Another class of molecular enhancement training methods involves adding noise to atomic coordinates and then predicting the noise \citep{feng2023fractional,ni2024sliced}. This approach has been shown to approximate the learning of force fields \citep{zaidi2023pretraining}, which are among the most fundamental interatomic interactions in equilibrium structures. However, the 

% However, this approach primarily focuses on quantum features in noise coverage and depends on equilibrium structures, while true quantum features, such as force fields, are continuous across the entire molecular space and apply universally to both equilibrium and non-equilibrium structures.

% TODO Some work apply mask on 3D position

% \begin{itemize}
%     \item There are many possibilities to recover mask. As shown in Figure ?, . In the complex chemical system, 
%     \item Attr, it is a 
% \end{itemize}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[scale=0.5,trim=0cm 10.5cm 0cm 0cm,clip]{pic/overall.pdf}}
\caption{The overall framework of EMPP. The masked position can be recounstructed by the GNNs output features of the neighboring nodes, with the position determined by the predicted directions and radius from those nodes.}\label{fig:overall}
% removes the masked atom's node entirely and uses the embeddings of its neighboring nodes to reconstruct the masked position. Notably, EMPP not only enables learning of general QM knowledge through self-supervised learning but can also be combined with supervised information (e.g., QM properties) to enhance molecular property prediction.}
\end{center}
\end{figure}

\vspace{-0.5em}
\subsection{Equivariant Masked Position Prediction (EMPP)}\label{sec:model}
\vspace{-0.5em}

The overall framework of EMPP is depicted in Figure \ref{fig:overall}. We begin the process by masking the position of an atom, causing the corresponding node in the graph to vanish. Next, we utilize equivariant backbones to generate equivariant node embeddings for the masked molecule. These embeddings are then input into a position prediction module, which outputs distributions for directions and radius. Both of them determine the predicted position. By aligning the true and predicted positions, EMPP enables the GNNs to capture atomic interactions within 3D molecular structure.

% After the masked molecule passes through the GNN, we predict the masked position using the output features of the neighboring nodes, with the position determined by the predicted angles and radius from those nodes.
% We use equivariant backbones to generate equivariant node embeddings, followed by a position prediction module that outputs the position distributions of masked atoms \Qu{ in particular, the position is represent by the xx angle and radius relative to xxx }, allowing the GNNs to capture interatomic interactions in 3D space.  
% EMPP can be applied to both supervised and self-supervised learning tasks. 
% For clarity, the following discussion will focus on the self-supervised version of EMPP, with a brief overview of the supervised approach provided at the end.

\vspace{-0.5em}
\subsubsection{Mask Position and Neighbour Encoding}
\vspace{-0.5em}

To address the limitations of previous works, EMPP only masks the atomic position and predicts it using its atomic attributes and the representations of unmasked atoms. Assuming the $i$-th atom is masked, the modified molecule is denoted as $\hat{S} = \{(\mathbf{z}_{1}, \mathbf{p}_{1}), ..., (\mathbf{z}_{i}), ..., (\mathbf{z}_{N}, \mathbf{p}_{N}) \}$. According to physics, the force at equilibrium is zero, with atomic force primarily governed by atomic interactions, such as Coulombic forces \citep{messiah2014quantum}. Thus, EMPP aims to find a position in equilibrium structures that satisfies the condition $\sum_{\substack{j=1, j \neq i}}^N \mathbf{force}_{ji} = \mathbf{0}$, where $\mathbf{force}_{ji}$ represents the interatomic force and is a function of the unknown masked position $\mathbf{p}_{i} = (x, y, z)$, given that the atomic attributes are fixed. Intuitively, there exists a unique optimal position under the force condition, since the number of unknown variables matches the number of equations. Organic chemistry further supports that the position of atoms is uniquely determined in most cases. For instance, in Figure \ref{fig:compare}, when any carbon atom is masked, its position can still be uniquely determined. Similarly, for the iodine atom, its position can be uniquely identified once the structure of its neighboring atoms is known. Further details and examples can be found in Appendix \ref{app:model_example}. Additionally, EMPP employs equivariant representation capable of storing vector features, enabling the accurate description of the true $\mathbf{force}_{ji}$. Therefore, EMPP can be regarded as a nearly well-posed method, effectively avoiding the approximation of Gaussian mixture distributions to learn forces or other quantum features.
% Intuitively, the optimal position can be uniquely determined because the number of constraints for zero-force equilibrium greatly exceeds the number of variables \Qu{not true}. 
% \begin{equation}
%     \mathrm{GNN}
% \end{equation}
% We first mask the $i$-th atom and remove its corresponding node from the graph
% Recall that predicting an atom with all information masked can be ill-posed due to the diversity of combinations in chemical space. Therefore, we only mask the atomic position, and predict it by its atomic number and other unmasked atomic representations. Assuming the $i$-th 
% $\hat{S} = \{(\mathbf{z}_{1}, \mathbf{p}_{1}), ..., (\mathbf{M}, \mathbf{p}_{k), ..., (\mathbf{z}_{N}, \mathbf{p}_{N}) \}$ 
% . In the equilibrium position, our position prediction is equivalent to
% \begin{equation}
% \label{equ:multiequation}
% \left\{
% \begin{aligned}
% % \mathbf{force}^{x}_{i1} + \mathbf{force}^{x}_{i2} + \cdots + \mathbf{force}^{x}_{iN} &= 0 \\
% % \mathbf{force}^{y}_{i1} + \mathbf{force}^{y}_{i2} + \cdots + \mathbf{force}^{y}_{iN} &= 0 \\
% % % \vdots \quad & \vdots \\
% % \mathbf{force}^{z}_{i1} + \mathbf{force}^{z}_{i2} + \cdots + \mathbf{force}^{z}_{iN} &= 0
% {force}^{x}_{i1} + {force}^{x}_{i2} + \cdots + {force}^{x}_{iN} &= 0 \\
% {force}^{y}_{i1} + {force}^{y}_{i2} + \cdots + {force}^{y}_{iN} &= 0 \\
% % \vdots \quad & \vdots \\
% {force}^{z}_{i1} + {force}^{z}_{i2} + \cdots + {force}^{z}_{iN} &= 0
% \end{aligned}
% \right .
% \end{equation}
% where $\mathbf{force}$ denotes the interaction forces between masked atom and unmasked atom. In practical applications, the number of unmasked atoms often far exceeds $3$, making equation \eqref{equ:multiequation} have a unique solution in most cases. 
% Additionally, the well-posedness of EMPP has a clear physical basis. From a statistical physics perspective, the position distribution of a masked atom follows the Boltzmann distribution, $p_{Bolt}(\mathbf{x}) \propto exp(-E(\mathbf{p}))$, where $E(\mathbf{p})$ represents the potential energy surface (PES)\footnote{Forces acting on atoms are derived from the gradient of the PES.} of the masked atom. The equilibrium position corresponds to the local minima of the PES, which are the most probable positions in the Boltzmann distribution \Qu{remove the explanation on Bolzmann distribution?}. By constraining the position distribution to a reasonable search space, these local minima can often be uniquely identified, as demonstrated through examples in Appendix ?. Therefore, EMPP on equilibrium structures can be viewed as a near well-posed method, avoiding the Gaussian approximation to learn the PES and atomic forces.


In the feedforward process of backbone GNN, the node of masked atom is completely removed. We encode the attributes $\mathbf{z}_{i}$ of the masked atom into the embedding of unmasked atoms. Note that traditional backbone GNNs use an embedding layer to project atomic attributes into the embedding space; we employ the same module augmented with a two-layer multi-layer perceptron (MLP) to embedding the attributes $\mathbf{z}_{i}$:
\begin{equation}
    \mathbf{e}_{i} = \mathrm{MLP}\big(\mathrm{EMBED}(\mathbf{z}_{i})\big).
\end{equation}
Next, we aggregate $\mathbf{e}_{i}$ with the embeddings of the unmasked nodes: 
% To reduce the influence of long-range interactions on the predicted positions and keep them within a rational space, this aggregation is limited to the atomic nodes that were originally connected to the masked atom.
\begin{equation}
    \label{equ:aggr_node_embedding}
    % \mathbf{v}_{j} := \mathbf{v}_{j} + \mathbf{e}_{i}. \quad \text{If } j \in \mathcal{N}_{i},
    \mathbf{v}_{j} \leftarrow \mathbf{v}_{j} + \mathbf{e}_{i}. \quad  j \in \{1, 2, ... i-1, i+1, ..., N\},
\end{equation}
% where $\mathcal{N}_{i}$ represents the set of neighbors before masking, determined by a cutoff distance around the original position $\mathbf{p}_{i}$. 
where $\mathbf{v}$ denotes the node embedding. We apply \eqref{equ:aggr_node_embedding} at each layer of the backbone GNN. Notably, 
$\mathbf{e}_{i}$ is invariant under SO(3) transformation. If the node embedding $\mathbf{v}_{j}$ is a spherical harmonic representation, we aggregate $\mathbf{e}_{i}$ with the its type-0 vector $\mathbf{v}^{(0)}_{j}$.

% is composed of a variety of intricate implicit physicochemical interactions，such as the Coulombic interactions. Fortunately, the function $E(\cdot)$ can be uniquely determined when the attributes of the masked atom and its surrounding environment are specified, which can be mathematical represented as follow:
% \begin{equation}
% % 写一个只和坐标有关的方程。
%     E(\mathbf{p}) = \sum_{i} E_{1}(\mathbf{p}, \mathbf{p}_{i}, \mathbf{z}, \mathbf{z}_{i}) + \sum_{i,j} E_{2}(\mathbf{p}, \mathbf{p}_{i}, \mathbf{p}_{j}, \mathbf{z}, \mathbf{z}_{i}, \mathbf{z}_{j}) + ...,
% \end{equation}
% Therefore, Given the determined PES, the atomic position distribution is also established.
% As the PES is determined, the position distribution is .
% where $E_{i}(\cdot)$ is an aggregation of latent physics mechanisms. $\mathbf{z}$ denotes the atomic attributes in position $\mathbf{p}$
Masking multiple atoms in EMPP increases the complexity of solution space, may make the position prediction ill-posed\footnote{In organic chemistry, a local system composed of two or more atoms often exhibits symmetry, allowing for multiple possible positions (See examples in Appendix \ref{app:model_example}).}.
% (\Qu{ footnote Why ill-posed. Suppose in an extreme case? mask 2 atoms of water molecule? }). 
% After passing the masked molecule through the backbone GNN, the output embeddings are represented as $(\mathbf{f}_{1}, \ldots, \mathbf{f}_{i-1}, \mathbf{f}_{i+1}, \ldots, \mathbf{f}_{N})$. 
% The single-mask strategy offers two key advantages:
% \begin{itemize}
%     \item Within a constrained space, the PES for a single atom typically has a unique local minimum. However, for multiple atoms, the PES $E(\mathbf{p}_{1},\mathbf{p}_{2},...)$ becomes more complex, often exhibiting multiple local minima.
%     \item We use the embeddings of unmasked nodes to predict the masked positions. However, if multiple masked positions share the same unmasked nodes, the prediction may become ambiguous.
%     % \item Subsequently, we employ the neighbor node embeddings to forecast the obscured atomic position. However, if multiple atoms with shared neighbors are masked, the prediction may become ambiguous due to the potential for multiple valid positions.
% \end{itemize}
% In summary, masking multiple atoms increases the complexity and ill-posedness of the training process. 
To mitigate this issue, we mask different atoms sequentially within a single molecule. First, we randomly generate the indices of the masked atoms as $Mask = \{Mask_{1},Mask_{2},...,Mask_{n}\}$. We then create multiple masked molecules, each containing only one masked atom, denoted as $\hat{S}_{1}, \hat{S}_{2}, ..., \hat{S}_{n}$. Each masked molecule independently predicts the position of its masked atom, and we average the resulting loss values. The objective function for training multiple masked molecules is given by:
\begin{equation}
    \label{equ:totalloss}
    \mathcal{L}_{multi} = \frac{1}{n} \sum_{i} \mathcal{L}_{single} \big(\mathbf{p}_{Mask_{i}}, \mathrm{Pred}\big(\mathrm{GNN}(\hat{S}_{i})\big)\big),
\end{equation}
where $\mathrm{Pred}(\cdot)$ refers to the position prediction block, and $\mathcal{L}_{single}(\cdot)$ represents the loss function for each individual masked atom. Details of them will be provided in the following sections. Since the nodes of masked atoms in EMPP are completely deleted, there is a distinct difference between ($\hat{S}_{1}, \hat{S}_{2}, ..., \hat{S}_{n}$). In contrast, previous methods do not delete nodes but perturb the features of the nodes. Intuitively, EMPP has a stronger capability to generate a vast array of diverse data.

% \textbf{Masking multiple atoms.} The above position prediction on equilibrium structure is a well-posed task, since the PES of the masked atom can be clear defined. However, when the number of mask atoms is large than $1$, the PES will become a function of multiple atoms. In this case, the local minima is 

\vspace{-0.5em}
\subsubsection{Position Prediction} \label{sec:pp}
\vspace{-0.5em}
\textbf{Equivarance for predicted position.} After masking $i$-th atom and encoding its attributes into the backbone GNN, we obtain the node representations of the masked molecule. Notably, we retain the neighbor set of the masked atom, denoted as $\mathcal{N}_{i}$,  and use the embeddings of the neighboring nodes to predict the position. For an unmasked node with embedding $\mathbf{f}_{k}$ (where $k \in \mathcal{N}_{i}$) and its position $\mathbf{p}_{k}$, the probability distribution of the masked atom's position is given by $p_{k}(\vec{\mathbf{r}}|\mathbf{f}_{k})$, with the predicted position denoted as $\hat{\mathbf{p}}_{i} = \mathbf{p}_{k} + \vec{\mathbf{r}}$. This distribution $p_{k}$ must satisfy both normalization and non-negativity constraints: $\int_{\Omega_{xyz}} p_{k}(\vec{\mathbf{r}}) dV = 1$ and $p_{k}(\vec{\mathbf{r}}) \geq 0$ where $dV=dxdydz$ is the volume element and $\Omega_{xyz}$ represents all space in Cartesian coordinates. Another challenge is ensuring that the distribution remains SO(3) transformation-equivariant, adhering to symmetry principles:
% This distribution $p_{k}$ must satisfy both normalization and non-negativity constraints: $\int_{\Omega} p_{k}(\vec{\mathbf{r}}) dV = 1$ and $p_{k}(\vec{\mathbf{r}}) \geq 0$ where $dV=drd\theta d\phi$ is the volume element and $\Omega$ represents all space in spherical coordinates. Additionally, a key challenge is ensuring that the distribution remains SO(3) transformation-equivariant, adhering to symmetry principles:
\begin{equation}
\label{equ:equidistribution}
    p_{k}(\vec{\mathbf{r}}|\mathbf{f}_{k}) = p_{k}(\mathbf{R} \vec{\mathbf{r}}| \mathbf{D} \mathbf{f}_{k}) = p_{k}\big(\mathbf{R} \vec{\mathbf{r}}| \mathrm{GNN}(\mathbf{R}\hat{S})\big),
\end{equation}
where $\mathbf{R}$ epresents any rotation matrix, and $\mathbf{D}$ is the Wigner-D matrix corresponding to the same transformation parameters. The term $\mathbf{R}\hat{S} = \{(\mathbf{z}_{i},\mathbf{R}\mathbf{p}_{i})| i \in \{1,...i-1,i+1,...N\}\}$ denotes the rotated molecular structure. Note that \eqref{equ:equidistribution} disregards translation, as its effect is neutralized by the relative position prediction $\mathbf{p}_{mask} = \mathbf{p}_{k} + \vec{\mathbf{r}}$.

\textbf{Distribution prediction.} Recent molecular models have demonstrated that high-degree equivariant features can effectively describe the PES and forces field \citep{passaro2023reducing, liao2024equiformerv}. Therefore, we can directly predict the distribution of masked positions using equivariant node features. 
% Our prediction module operates under the premise that the PES and forces field are between the masked atom and its neighboring atoms.
% arises from the interactions between the masked atom and its neighboring atoms. We assume that the node embedding of each neighboring atom captures a local PES, and by aggregating these embeddings, we obtain the full PES. 
In our prediction module, we use each neighboring atom to independently predict a positional distribution, which is then aggregated. In practice, we first pass the node embeddings through an equivariant two-layer MLP. The mathematical process is expressed as:
% \begin{equation}
\begin{align}
    \mathbf{f}_{k} \leftarrow& \mathrm{GATE}\big(\mathrm{ELINEAR}(\mathbf{f}_{k})\big) \\
    \mathbf{f}_{k} \leftarrow& \mathrm{ELINEAR}\big(\mathbf{f}_{k} \otimes \mathrm{EMBED}(\mathbf{z}_{i})\big),
\end{align}
% \end{equation}
where $\mathrm{ELINEAR}(\cdot)$ denotes the SO(3) linear layer, $\mathrm{Gate}(\cdot)$ denotes gate activation using SiLU and $\otimes$ denotes the CG tensor product. In the second layer of the MLP, we integrate the features with the intrinsic attributes of the masked atom. We transform position prediction to Spherical coordinates and decompose it into two parts: the \emph{radius distribution}, $p^{radius}(r)$, and the \emph{directional distribution}, $p^{direction}(\theta, \phi)$. For the radius distribution, we uniformly partition the space into 128 intervals between 0.9 \AA and 5 \AA, covering the key organic molecular interaction distances. As distance is a rotation-invariant feature, we predict the radius distribution directly using the type-$0$ vector:
\begin{equation}
    p^{radius}_{k} = \mathrm{SoftMax}\big(\mathrm{LINEAR}(\mathbf{f}^{(0)}_{k}) / \tau \big), 
\end{equation}
where $\mathrm{SoftMax}(\cdot)$ is used to normalize the distribution and $\tau$ is the temperature coefficient. For the directional distribution, we apply a Fourier transform to project the node representation onto the spherical surface, allowing us to capture features from all directions:
\begin{equation}\label{app_equ:inverse_fourier}
    \mathcal{F}(\theta, \phi) = \sum_{l=0}^{L_{max}} \sum^{l}_{m=-l} \mathbf{f}^{l,m}_{k}Y^{l}_{m}(\theta, \phi),
\end{equation}
The Fourier transform of spherical harmonics is described in detail in Appendix \ref{app:fouriertrans}. Afterward, we grid the spherical surface to obtain a finite feature matrix denoted as $\mathcal{F}^{grid}_{k}$. Specifically, $\mathbf{f}_{k} \in \mathbb{R}^{(L_{max}+1)^{2} \times C}$ contains $C$ channels. After gridding, $\mathcal{F}^{grid}_{k} \in \mathbb{R}^{S \times C}$ retains information from all channels, where $S$ represents the grid sampling rate\footnote{The grid sampling rate must follow the Nyquist rate criterion: $S \geq (2L_{max})^{2}$.}. We then apply a shared MLP to project channel dimension into a 1D space, followed by the Softmax function to produce a normalized directional distribution.
\begin{equation}
\label{equ:direction}
    p^{direction}_{k} = \mathrm{SoftMax}\big(\mathrm{MLP}(\mathcal{F}^{grid}_{k}) / \tau \big), 
\end{equation}
It is important to note that operations within the channel dimension preserve overall equivariance. As a result, \eqref{equ:direction} satisfies the condition outlined in \eqref{equ:equidistribution}. Multi-channel features are utilized to retain high-frequency components. While EMPP's method of masking atoms and predicting coordinates using spherical harmonic projection shares similarities with Symphony \citep{daigavane2024symphony}, their primary focuses diverge significantly: Symphony tackles molecular generation, whereas EMPP addresses a self-supervised task. This difference in objective leads to substantial variations in implementation, such as masking strategies and the design of the multi-channel spherical harmonic projection module. Detailed comparisons are provided in Appendix \ref{app:diff_sym}.

\vspace{-0.5em}
\subsubsection{Loss Function}\label{sec:loss}
\vspace{-0.5em}
\textbf{Radius loss.} During training, the model is optimized by the distribution of true radius $||\vec{\mathbf{r}}_{ik}||$ and direction $\vec{\mathbf{r}}_{ik}/||\vec{\mathbf{r}}_{ik}||$, where $\vec{\mathbf{r}}_{ik} = \mathbf{p}_{i} - \mathbf{p}_{k}$. Intuitively, the deterministic vector $\vec{\mathbf{r}}_{ik}$ should correspond to a Dirac delta distribution denoted as $\delta(\vec{\mathbf{r}} - \vec{\mathbf{r}}_{ik})$. In practice, however, the model can learn the accurate $\vec{\mathbf{r}}_{ik}$ as long as the defined distribution can uniquely represent $\vec{\mathbf{r}}_{ik}$. The Dirac delta distribution can reduce training stability and lacks the ability to transfer to other conformations. 
% Ideally, the true radius distribution would be a Dirac delta function denoted as $\delta(r - ||\mathbf{p}_{i} - \mathbf{p}_{k}||)$. 
% However, relying solely on this distribution risks overfitting the model to specific conformations in the training data, such as equilibrium states, potentially hindering its generalization to other conformations. 
To address this, we use a Gaussian distribution\footnote{ A common method to embedding distance \citep{thomas2018tensor,zitnick2022spherical}} as a surrogate:
\begin{equation}
\label{equ:gaussian_radius}
    q^{radius}_{k}(r) \sim \mathcal{N}(r | \|\vec{\mathbf{r}}_{ik}\|, \sigma),
\end{equation}
where $\sigma$ is typically set to 0.5 \AA. The Kullback-Leibler divergence (KL-div) is utilized to compute the loss:
\begin{align}
    \label{equ:radiuslabel}
    \mathcal{L}^{radius} = \frac{1}{|\mathcal{N}_{i}|} \sum_{k \in \mathcal{N}_{i}} \sum_{S} q^{radius}_{k} \mathrm{log} \frac{q^{radius}_{k}}{p^{radius}_{k}}.
\end{align}
\textbf{Direction loss.} Similarly, we choose a soft directional distribution instead of a Dirac delta function to represent the ground truth for direction:
\begin{equation}
\label{equ:directionlabel}
    % \begin{align}
    % w_{k}(\theta, \phi) &= \mathrm{exp}\big(\sum_{l,m} \mathrm{sh}^{l,m}(\frac{\vec{\mathbf{r}}_{ik}}{||\vec{\mathbf{r}}_{ik}||})Y^{l}_{m}(\theta, \phi)\big) \\
    % q^{direction}_{k}(\theta, \phi) &\sim \frac{1}{W} w_{k}(\theta, \phi),
    % \end{align}
    w_{k}(\theta, \phi) = \mathrm{exp}\big(\sum_{l,m} \mathrm{sh}^{l,m}(\frac{\vec{\mathbf{r}}_{ik}}{||\vec{\mathbf{r}}_{ik}||})Y^{l}_{m}(\theta, \phi)\big), \quad
    q^{direction}_{k}(\theta, \phi) \sim \frac{1}{W} w_{k}(\theta, \phi),
\end{equation}
where $\mathrm{sh}(\cdot)$ denotes the spherical harmonics representation of the direction, and $W = \int_{\Omega_{\theta\phi}} w_{k}(\theta, \phi)d\theta d\phi $ serves to normalize the distribution. The true distribution is then gridded, and the KL divergence between $p^{direction}_{k}$ and the true distribution is computed:
\begin{equation}
\label{equ:dirlabel}
    \mathcal{L}^{direction} = \frac{1}{|\mathcal{N}_{i}|} \sum_{k \in \mathcal{N}_{i}} \sum_{S} q^{direction}_{k} \mathrm{log} \frac{q^{direction}_{k}}{p^{direction}_{k}}.
\end{equation}
The total loss for a single masked atom, as defined in \eqref{equ:totalloss}, is given by $\mathcal{L}^{single} = \mathcal{L}^{radius} + \mathcal{L}^{direction}$.

% \textbf{Prediction with neighbors.} In equations \eqref{equ:radiuslabel} and \eqref{equ:dirlabel}, we restrict the prediction of masked positions to neighboring nodes within the cutoff radius. This is because, in GNNs, atoms beyond the cutoff radius do not directly interact with the masked atom; rather, they influence the potential energy surface (PES) indirectly by transmitting features to the embeddings of neighboring nodes through multi-layer forward propagation. Including points outside the cutoff during loss calculation would violate the inductive bias of GNNs. Furthermore, our ablation study shows that incorporating atoms beyond the cutoff can reduce the model's generalization performance.
\vspace{-0.5em}
\subsection{Application of EMPP}
\vspace{-0.5em}
\textbf{Pre-training without annotation.} The previously described EMPP can learn quantum knowledge from equilibrium molecules without quantum property labels. Therefore, EMPP can be used to pre-train GNNs to learn transferable knowledge. For example, the PCQM4Mv2 dataset \citep{nakata2017pubchemqc} contains a vast collection of equilibrium molecules but provides labels for only one chemical property, the gap between HOMO and LUMO. 
We can use EMPP to pre-train GNNs on PCQM4Mv2 and use the pre-train models to predict additional chemical properties in other datasets.
% enables self-supervised training, allowing the pre-trained model to be transferred to other molecular tasks to predict additional chemical properties.
% Self-supervised learning pre-training task
% the force condition becomes $\sum_{\substack{j=1, j \neq i}}^N \mathbf{force}_{ji} = \mathbf{force}^{*}$. 
\textbf{Auxiliary task for property prediction.} When training a model for a specific property prediction, we calculate two losses: the prediction loss for the targeted property and the EMPP loss as defined in \eqref{equ:totalloss}. These losses are then combined for gradient descent. In this case, the goal of EMPP is not to learn the equilibrium position of the masked atom, but rather to learn the position corresponding to the known property value, which may be in a non-equilibrium state. For example, when the force on the masked atom is non-zero (indicating non-equilibrium), EMPP can capture the relationship between the true position and this force. Similarly, for other quantum labels, implicit relationships exist with the masked atom's position. In practice, we encode the label into the GNN. If the label is global invariant property like energy, the encoding can be written as:
\begin{equation}
    \mathbf{v}^{(0)}_{k} \leftarrow \mathbf{v}^{(0)}_{k} + \mathrm{LINEAR}\big(\mathrm{GAUSS}(y^{energy*})\big),
\end{equation}
where $\mathrm{GAUSS}(\cdot)$ denotes a Gaussian block. Specifically, for node-wise equivariant labels such as force $\mathbf{y}^{forces*}_{i}$, we map it into the spherical harmonic representations and add it to embeddings:
\begin{equation}
    \mathbf{v}_{k} \leftarrow \mathbf{v}_{k} + \mathrm{ELINEAR}\big(\mathrm{sh}(\mathbf{y}^{forces*}_{i})\big).
\end{equation}
Then, we use the GNN output embeddings to predict position.
In this scenario, we treat EMPP as an auxiliary task for property prediction. By modeling the relationship between the target property and position, EMPP further enhances the generalization ability of the property prediction model.

\vspace{-1em}
\section{Related Work}
\vspace{-1em}
\textbf{Graph self-supervised methods.} Leveraging the inherent graph structure of molecules, many graph-based self-supervised methods have the potential to train molecular models that capture transferable knowledge. For example, \citet{hu2019strategies} proposed graph context prediction and attribute masking methods to enhance molecular property prediction. GraphMAE \citep{hou2022graphmae} pre-trained molecular models using a generative decoder to reconstruct atomic and bond attributes. D-SLA \citep{kim2022graph} applied contrastive learning based on graph edit distance, improving predictions of molecular biochemical activities. Additionally, graph motifs—induced subgraphs that describe recurrence and significance—have increasingly been utilized to construct self-supervised learning frameworks for molecules \citep{rong2020self,zhang2021motif,inae2023motif}, facilitating the learning of multi-scale molecular information. These pre-training methods primarily focus on graph characteristics, while neglecting the intrinsic quantum mechanisms within molecules. As a result, they are limited in their ability to predict molecular quantum mechanical properties. 

% However, despite their improvements in property prediction, these pre-training methods primarily focus on graph characteristics, such as geometric semantics and graph-level similarity, while neglecting the intrinsic quantum mechanisms within molecules. As a result, they are limited in their ability to predict molecular quantum mechanical properties. 

% In this work, we propose EMPP, which introduces physical priors into learning tasks, enabling GNN models to capture key quantum mechanical features.

\textbf{3D molecular representation.} Given the strong correlation between the quantum characteristics of molecules and their 3D structures, recent molecular models have increasingly focused on 3D representations \citep{liao2023equiformer,passaro2023reducing,liao2024equiformerv}. As a result, self-supervised techniques have also evolved to operate in 3D space. For instance, Unimols \citep{zhou2023unimol,lu2023highly} masked atomic properties and restored them using 3D molecular models, while denoising methods \citep{zaidi2023pretraining,feng2023fractional} introduced a series of physics-informed pre-training paradigms. We provide a detailed discussion of these methods in Section \ref{sec:revisit} and highlight their main limitation: accurately defining the parameters of Gaussian mixture distributions can be challenging. In contrast, EMPP learns quantum mechanical features through a position prediction process, effectively bypassing the difficulties associated with denoising methods.
% across a broader chemical space, improving the model's generalization capability.


\textbf{Spherical harmonics projection.} High-degree spherical harmonic representations with grid projections \citep{liao2023equiformer,passaro2023reducing} have demonstrated significant capabilities in spatial description. Building on this, Symphony \citep{daigavane2024symphony} introduced a neighbor-based spatial position prediction method, developing a framework for molecular generation. While EMPP shares some technical similarities with Symphony, their distinct tasks (molecular generation versus self-supervised learning) lead to differing implementation priorities. Symphony prioritizes flexibility in position prediction to ensure the sampling of diverse molecules, whereas EMPP emphasizes accuracy and well-posed position prediction to facilitate learning of genuine physical interactions.



\vspace{-1em}
\section{Experiments}
\vspace{-1em}

In this section, we present experiments to evaluate the effectiveness of EMPP across several 3D molecular benchmarks. Since EMPP can be applied in both unlabeled and labeled scenarios, we evaluate it in two settings: (i) self-supervised tasks for learning transferable molecular knowledge, and (ii) auxiliary tasks for enhancing the prediction of supervised molecular properties. 

% We defer details of configurations to Appendix ?.
\vspace{-1em}
\subsection{Datasets and Configurations}
\vspace{-1em}

\textbf{Datasets.} We evaluate quantum property prediction using the QM9 \citep{ramakrishnan2014quantum} and MD17 \citep{chmiela2017machine} datasets. QM9 comprises 134,000 stable small organic molecules made up of C, H, O, N, and F atoms, with one conformation per molecule, and includes labels for 12 quantum properties. MD17 contains molecular dynamics trajectories for 8 small organic molecules, providing between 150,000 and nearly 1 million conformations per molecule, along with corresponding total energy and force labels. Notably, MD17 features a significant number of non-equilibrium molecules. Additionally, we utilize the PCQM4Mv2 \citep{nakata2017pubchemqc} dataset to pre-train GNN backbones, which consists of 3.4 million organic molecules, each with one equilibrium conformation, and is widely used for pre-training.

\textbf{Baselines.} Our baselines include state-of-the-art self-supervised methods for 3D molecular structures, such as AttrMask \citep{hu2019strategies}, DP-TorchMD-NET \citep{zaidi2023pretraining}, 3D-EMGP \citep{jiao2023energy}, SE(3)-DDM \citep{liu2022molecular}, Transformer-M \citep{luo2022one}, and Frad \citep{feng2023fractional}. These methods pre-train GNNs on the PCQM4Mv2 dataset before predicting molecular properties in QM9 and MD17, leveraging additional molecular data. In contrast, EMPP can operate without extra data, so we also compare it to molecular models trained solely on QM9 or MD17, including SchNet \citep{schutt2018schnet}, PaiNN \citep{schutt2021equivariant}, DimeNet++ \citep{gasteiger2020fast}, TorchMD-NET \citep{doerr2021torchmd}, SEGNN \citep{brandstetter2021geometric}, and Equiformer \citep{liao2023equiformer}. Detailed configurations for EMPP and the aforementioned baselines can be found in Appendix \ref{app:im_de}.

\begin{table}[t]
\caption{Results on QM9 property prediction without pre-trainging on extra molecular data. {\dag} denotes using different data partitions. In this experiment, EMPP uses the Equiformer backbone. n-Mask denotes masking n atoms in each molecule during training. The masking strategy follow \eqref{equ:totalloss}. }\label{tab:QM9withoutPT}
\resizebox{\linewidth}{!}{
% \begin{tabular*}{\linewidth}{@{}L|LLLLLLLLLLLL@{}}
\begin{tabular}{l|cccccccccccc}
\toprule
Task      & $\alpha$ & $\Delta \epsilon$ & $\varepsilon_{HOMO}$ & $\varepsilon_{LUMO}$ & $\mu$ & $C_{v}$     & $G$ & $H$ & $R^{2}$  & $U$ & $U_{0}$ & ZPVE \\
Units     & bohr$^{3}$ & meV                  & meV                  & meV                  & D     & cal/(mol K) & meV & meV & bohr$^{3}$ & meV & meV     & meV  \\ \midrule
SchNet    &.235          &63          &41 &34 &.033   &.033 &14         &14           &.073 &19   &14   &1.70  \\
DimeNet++ &.044 &33          &25 &20 &.030   &.023 &8 & 7  & .331         & 6   & 6   &1.21 \\
PaiNN     & .045 & 46 & 28 & 20 & .012 & .024 & 7.35 & 5.98 & .066 & 5.83 & 5.85 & 1.28 \\
TorchMD-NET & .059         & 36         &20 &18 &.011   &.026 & 7.62      & 6.16       &\textbf{.033} & 6.38 & 6.15 & 1.84 \\
SEGNN$^{\dag}$     & .060         & 42         &24 &21 &.023   &.031 &15         & 16          & .660    &13    &15      & 1.62 \\
Equiformer &.046           &30          &15 &14 &.011 &.023 & 7.63 & 6.63 &.251 & 6.74 & 6.59 & 1.26 \\
\midrule
EMPP (1-Mask)  & .041    & 27 & 14 & 13 & .0108 & .021  & 6.89 & 5.38  & .189   & 6.05 & 5.88 & 1.20    \\ 
% EMPP (1-Mask)  & .041    & 27 & 14 & 13 & .0108 & .021  & 6.89 & 5.38  & .189   & 6.05 & 5.88 & 1.20    \\ 
EMPP (3-Mask)  & \textbf{.039}    & \textbf{26} & \textbf{13}                   & \textbf{12} & \textbf{.0096} & \textbf{.019}  & \textbf{6.32} & \textbf{5.02}  & .154   & \textbf{5.72} & \textbf{5.25} & \textbf{1.18}    \\
\bottomrule
\end{tabular}
}
\end{table}


% and 12 labels of geometric, energetic, electronic, and thermodynamic properties for 134k
% stable small organic molecules made up of CHONF atoms. The data splitting follows standard settings which have a training set with 110,000 samples, a validation set with 10,000 samples, and a
% test set with the remaining 10,831 samples. The performance on 12 properties is measured by mean
% absolute error (MAE, lower is better) and the results are summarized in Table 2.

\subsection{Results without pre-training}\label{sec:without_pretraining}
\textbf{QM9.} We first evaluate EMPP without incorporating additional data, treating it as an auxiliary task for QM9 and MD17. The total loss comprises \eqref{equ:totalloss} combined with the MAE loss from the original property prediction. In these experiments, EMPP is implemented using the Equiformer backbone \citep{liao2023equiformer} due to its high-degree equivariant representation, which effectively captures interatomic features. As shown in Table \ref{tab:QM9withoutPT}, two key observations emerge. First, EMPP enhances prediction accuracy across all QM9 tasks, achieving the best results in 11 tasks without additional data. Moreover, multi-masking show better performance, demonstrating that the data generated by EMPP is diverse and reliable. Second, while EMPP is designed to learn interaction between adjacent atoms, it also leads to improvements in properties not directly related to atomic interaction, such as $\varepsilon_{HOMO}$ and $\varepsilon_{LUMO}$ \citep{pope1999electronic}, further confirming its effectiveness.

\begin{table}[t]
\caption{Results on MD17 testing set without pre-trainging on extra molecular data. Energy and force are in units of meV and meV/ \AA. The ``energy only'' and ``force only'' are based on ``1-Mask'' strategy.}\label{tab:md17_result}
\begin{adjustwidth}{-2.5cm}{-2.5cm}
\centering
\scalebox{0.6}{
\begin{tabular}{lcccccccccccccccc}
\toprule[1.2pt]
                        & \multicolumn{2}{c}{Aspirin} & \multicolumn{2}{c}{Benzene} & \multicolumn{2}{c}{Ethanol} & \multicolumn{2}{c}{Malonaldehyde} & \multicolumn{2}{c}{Naphthalene} & \multicolumn{2}{c}{Salicylic acid} & \multicolumn{2}{c}{Toluene} & \multicolumn{2}{c}{Uracil} \\
\cmidrule[0.6pt]{2-17}
Methods                                               & energy       & forces       & energy       & forces       & energy       & forces       & energy          & forces          & energy         & forces         & energy           & forces          & energy       & forces       & energy       & forces      \\
\midrule[1.2pt]
SchNet                          & 16.0         & 58.5         & 3.5          & 13.4         & 3.5          & 16.9         & 5.6             & 28.6            & 6.9            & 25.2           & 8.7              & 36.9            & 5.2          & 24.7         & 6.1          & 24.3        \\
DimeNet                          & 8.8          & 21.6         & 3.4          & 8.1          & 2.8          & 10.0         & 4.5             & 16.6            & 5.3            & 9.3            & 5.8              & 16.2            & 4.4          & 9.4          & 5.0          & 13.1        \\
PaiNN                                                 & 6.9          & 14.7         & -            & -            & 2.7          & 9.7          & 3.9             & 13.8            & 5.0            & 3.3            & 4.9              & 8.5             & 4.1          & 4.1          & 4.5          & 6.0         \\
TorchMD-NET                                           & 5.3          & 11.0         & 2.5          & 8.5          & 2.3          & 4.7          & 3.3             & 7.3             & 3.7            & 2.6            & \textbf{4.0}             & 5.6             & 3.2        & 2.9          & 4.1        & 4.1         \\

% \midrule[0.6pt]

% Equiformer ($L_{max = 2$)                                           & 5.3          & 7.2          & 2.2          & 6.6          & 2.2          & 3.1          & 3.3             & 5.8             & 3.7            & 2.1           & 4.5              & 4.1             & 3.8          & 2.1          & 4.3          & 3.3        \\
Equiformer                                          & 5.3          & 6.6          & 2.5          & 8.1          & 2.2          & 2.9          & 3.2             & 5.4             & 4.4            & 2.0           & 4.3              & 3.9            & 3.7          & 2.1         & 4.3          & 3.4        \\
\midrule[0.6pt]
EMPP  (1-Mask)                                       & 5.1          & 6.4          & 2.3          & 7.5          & 2.2          & 2.7          & 3.0             & 5.2             & 4.1            & 1.9           & 4.3              & 3.7            & 3.5          & 2.1         & 4.1          & 3.4        \\
EMPP   (3-Mask)                                 & 5.0         & \textbf{6.2}          & \textbf{2.1}          & 7.3          & \textbf{2.0}          & 2.6         & 3.0             & 5.0             & 3.7            & 1.6           & 4.1              & \textbf{3.6}            & \textbf{3.3}          & \textbf{2.0}         & \textbf{4.0}          & \textbf{3.0}        \\
\midrule[0.6pt]
EMPP  (energy only)                                       & \textbf{4.8}          & 6.6          & \textbf{2.1}          & 8.5          & \textbf{2.0} & 2.5          & \textbf{2.8}             & 5.1             & \textbf{3.5}            & \textbf{1.5}           & 4.1              & 3.9            & \textbf{3.3}          & 2.2         &  \textbf{4.0}         & 3.2        \\
EMPP  (force only)                                       & 5.3          & 6.4        & 2.8          & \textbf{7.1}          & 2.3 & \textbf{2.4}          & 3.3             & \textbf{4.9}             & 4.4          & \textbf{1.5}           & 4.3              & 3.7            & 3.7          & \textbf{2.0}         & 4.7          & 3.2        \\
% tol 0.17 0.51 * 4.184 * 0.01 
\bottomrule[1.2pt]
\end{tabular}
}
\end{adjustwidth}
\vspace{-1mm}
\end{table}

\textbf{MD17.} The MD17 dataset provides both global labels (energy) and node-wise labels (forces), along with numerous non-equilibrium conformations that present new challenges for models and self-supervised methods. To address this, we encode both the global energy and the forces of the masked atom into the backbone GNNs. EMPP predicts atom positions where the predicted forces align with the ground truth. The results, presented in Table \ref{tab:md17_result}, show that EMPP achieves the best performance on most tasks. Additionally, we conducted two experiments where either energy or force alone is encoded into the backbone, referred to as ``energy only'' and ``force only''. Notably, when only energy is encoded, Table \ref{tab:md17_result} indicates that energy prediction can be further improved. In the cases of Aspirin, Naphthalene, and Toluene, the ``energy only'' approach with single-atom masking even surpasses the results obtained by three-atoms masking while encoding both energy and forces. These findings suggest that encoding a single label can simplify EMPP’s learning process, thereby enhancing its effectiveness for predicting specific properties. We further investigate the performance of EMPP without pre-training on the GEOM-Drug dataset \citep{axelrod2022geom}, which contains abundant non-equilibrium data. Results can be found in the Appendix C.2. 

% introduce two extra 
% real forces of the masked atoms into the GNN.

% are more sensitive
% to molecular geometry and contain nonequilibrium conformations, which bring new challenges to the models. Theoretically, denoising can directly benefit downstream force
% learning, since it has learnt an approximate force field as a
% reference. As we expected, Frad achieves new SOTA and the
% results are in Table 3. In both large and small training data
% scenarios, Frad outperforms the corresponding pretrained
% and non-pretrained baselines on 7 out of 8 molecules. Especially when comparing with 3D-EMGP(TorchMD-NET)
% and DP-TorchMD-NET who utilize the same backbone as
% us, our superiority is evident, showing the necessity to correct denoising methods by chemical constraints.
% Regarding Benzene, we observe overfitting during finetuning the Frad especially when the training set size is large,
% which is not found in other molecules. This may be caused
% by the relatively fixed structure of benzene, leading to lowdimensional features which are easy to overfit.

% On the other hand, the improvements achieved by EMPP on different tasks vary significantly, which we believe is due to the varying dependency of different quantum metrics on interatomic interactions. For example, the $\varepsilon_{HOMO}$ and $\varepsilon_{LUMO}$ denotes highest occupied / lowest unoccupied molecular orbital energy, they do not rela

% These experiments can assess the quality of the data generated by EMPP and its ability to fit non-equilibrium data.

\vspace{-0.5em}
\subsection{Results with pre-training on PCQM4Mv2}
\vspace{-0.5em}

In this section, we assess the effectiveness of EMPP as a pre-training task. To maintain consistency with state-of-the-art self-supervised methods \citep{zaidi2023pretraining, feng2023fractional}, we use TorchMD-Net (ET) \citep{tholke2022equivariant} as the backbone model. To enhance the fine granularity of the spherical representation after Fourier transformation, we extend the node representation of TorchMD-Net to $L_{max}=3$ while retaining all other core operations (see details in Appendix \ref{app:torchmd}). During the pre-training phase, EMPP encodes only the atomic numbers of the masked atoms into the backbone. In the fine-tuning stage, EMPP encodes properties following the same approach outlined in Section \ref{sec:without_pretraining}.

As shown in Table \ref{tab:QM9withPT}, the attribute masking method produces poorer results for quantum property prediction, consistent with our earlier observations: recovering attributes based on simple features limits the model's ability to capture deep quantum characteristics. In contrast, denoising methods such as DP-TorchMD-Net and Frad deliver competitive performance due to their physically interpretable paradigms. Notably, our EMPP surpasses denoising methods in nine tasks, achieving the best results in seven tasks. This success is attributed to EMPP's more precise paradigm, which employs a precise paradigm to learn interactions instead of approximation of mixture distribution.

\begin{table}[t]
\caption{Results on QM9 property prediction with pre-trainging on extra molecular data. EMPP use the TorchMD-Net backbone. Bold and underline indicate the best result, and the second best result.}\label{tab:QM9withPT}
\resizebox{\linewidth}{!}{
% \begin{tabular*}{\linewidth}{@{}L|LLLLLLLLLLLL@{}}
\begin{tabular}{l|cccccccccccc}
\toprule
Task      & $\alpha$ & $\Delta \epsilon$ & $\varepsilon_{HOMO}$ & $\varepsilon_{LUMO}$ & $\mu$ & $C_{v}$     & $G$ & $H$ & $R^{2}$  & $U$ & $U_{0}$ & ZPVE \\
Units     & bohr$^{3}$ & meV                  & meV                  & meV                  & D     & cal/(mol K) & meV & meV & bohr$^{3}$ & meV & meV     & meV  \\ \midrule
AttrMask &	.072 &		50.0 &		31.3 &		37.8 &		.020 &		0.062 &		11.2 &		11.4 &		.423 &		10.8 &		10.7 & 1.90
 \\
Transformer-M &	.041 &		27.4 &		17.5 &		16.2 &		.037 &		0.022 &		9.63 &		9.39 &		\textbf{.075} &		9.41 &		9.37 & \textbf{1.18}
 \\
 SE(3)-DDM 	 &	.046	 &	40.2 &		23.5	 &	19.5	 &	.015	 &	.024	 &	7.65	 &	7.09 &	.122 &	6.99	 &	6.92 &	1.31
 \\
3D-EMGP &	.057	 &	37.1	 &	21.3	 &	18.2	 &	.020	 &	.026	 &	9.30	 &	8.70 &		\underline{.092}	 &	8.60	 &	8.60	 &	1.38
\\
% DP-GNS-TAT*	 &	0.016	 &	0.040	 &	14.9	 &	14.7	 &	22.0	 &	0.44	 &	1.018	 &	5.76	 &	5.76	 &	5.79	 &	6.90	 &	0.020
% \\
DP-TorchMD-Net	 & .0517 & 	31.8 &	17.7	 &	14.3	 &	\underline{.012}	 &	\underline{.020}	 &	6.91	 & 6.45  &		 .4496  &		 6.11  &		 6.57 & 1.71  \\
Frad &		\underline{0.037} &	\underline{27.8} &	\underline{15.3}	 &	13.7	 &	\textbf{.010}	 &	\underline{.020} &		\textbf{6.19}	 &	\textbf{5.55} &	.342	& \underline{5.62} &	\underline{5.33} & 1.42 \\
\midrule
EMPP (3-Mask)  & \textbf{.035}    & \textbf{25.8} & \textbf{13.7}                   & \textbf{13.4} & .014 & \textbf{.019}  & \underline{6.45} & \underline{5.73}  & .241   & \textbf{5.34} & \textbf{5.08} & \underline{1.27}    \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t]
\centering
\caption{Ablation study on key hyperparameters. Bold indicates the default configuration. }\label{tab:hyperparameter}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc|lcc}
\toprule
Sampling Rate            & $\alpha$ & $\varepsilon_{HOMO}$ & Loss Weight & $\alpha$ & $\varepsilon_{HOMO}$ \\ %& Temperature & $\alpha$ & $\varepsilon_{HOMO}$ \\
\midrule
$20^2$  & .050  & 20.4 & 0.1         & .046  & 15.4 \\ % & 0.01        & .43   & 14.8 \\
$50^2$  & .043  & 18.5 & 0.5         & .045  & 15.2 \\ %& 0.05        & .045  & 15.0 \\
$\mathbf{100^2}$ & .041  & 14.2 & \textbf{1}           & .041  & 14.2 \\ % & \textbf{0.1}         & .041  & 14.2 \\
$150^2$ & .041  & 14.7 & 5           & .044  & 15.8 \\ % & 1           & .048  & 15.8 \\
\bottomrule
\end{tabular}
% }
\end{table}

   %  	Models & $\mu$ (D)	& 	$\alpha$ ($a_0^3$)		&  \makecell[c]{$\epsilon_{HOMO}$ \\(meV)}		& \makecell[c]{$\epsilon_{LUMO}$\\ (meV)}		& \makecell[c]{$\Delta\epsilon$\\ (meV)}	& \makecell[c]{$<R^2>$ \\($a_0^2$)}	& \makecell[c]{ZPVE\\ (meV)	}	& \makecell[c]{$U_0$ \\ (meV)}		& \makecell[c]{$U$ \\ (meV)}		& \makecell[c]{$H$ \\ (meV)}		& \makecell[c]{$G$\\ (meV)} & \makecell[c]{$C_v$\\ ($\frac{cal}{mol K}$)	}
   %   \\
   %  \midrule
   %  SchNet & 	0.033 & 	0.235	 & 41.0 & 	34.0 & 63.0	 & 0.07	 & 1.70	 & 14.00	 & 19.00	 & 14.00	 & 14.00	 & 0.033\\
   %  E(n)-GNN & 	0.029	 & 0.071	 & 29.0	 & 25.0	 & 48.0	 & 0.11	 &   1.55	 & 11.00	 & 12.00	 & 12.00	 & 12.00	 & 0.031\\
   %  DimeNet++	 & 0.030	 & 0.043	 & 24.6	 & 19.5	 & 32.6	 & 0.33	 & 1.21	 & 6.32	 & 6.28 & 	6.53	 & 7.56	 & 0.023\\
   %  PaiNN	 & 0.012	 & 0.045	 & 27.6	 & 20.4	 & 45.7	 & 0.07	 & 1.28	 & 5.85	 & 5.83	 & 5.98	 & 7.35	 & 0.024\\
   %  SphereNet & 0.027 & 0.047 & 23.6 & 18.9 & 32.3  & 0.29 & \textbf{1.120} & 6.26&  7.33 & 6.40 &8.00 &0.022\\ 
   %  TorchMD-NET & 0.011 & 0.059 & 20.3 & 18.6 & 36.1 & \textbf{0.033} & 1.840 & 6.15 & 6.38 & 6.16 & 7.62 & 0.026 \\
   % \midrule % \hline
   %  % ChemRL-GEM  & & &\multicolumn{3}{|c|}{199.726538		}  & &  & &  & & \\
   %  % Uni-mol  & & &\multicolumn{3}{|c|}{	127.073969	}  & &  & &  & & \\
   %  % \hline
   %  Transformer-M &	0.037 &		0.041 &		17.5 &		16.2 &		\textbf{27.4} &		0.075 &		1.18 &		9.37 &		9.41 &		9.39 &		9.63 &		0.022
   %   \\
   %   SE(3)-DDM 	 &	0.015	 &	0.046 &		23.5	 &	19.5	 &	40.2	 &	0.122	 &	1.31	 &	6.92 &		6.99	 &	7.09	 &	7.65	 &	0.024
   %   \\
   %  3D-EMGP &	0.020	 &	0.057	 &	21.3	 &	18.2	 &	37.1	 &	0.092	 &	1.38	 &	8.60 &		8.60	 &	8.70	 &	9.30	 &	0.026
   %  \\
   %  % DP-GNS-TAT*	 &	0.016	 &	0.040	 &	14.9	 &	14.7	 &	22.0	 &	0.44	 &	1.018	 &	5.76	 &	5.76	 &	5.79	 &	6.90	 &	0.020
   %  % \\
   %  \makecell[c]{DP-TorchMD\\-NET($\tau=0.04$)}	 &	0.012	 &	0.0517	 &	17.7	 &	14.3	 &	31.8	 &	0.4496	 &	1.71	 & 6.57  &		 6.11  &		 6.45  &		 6.91 
   %  	 &	\textbf{0.020}
   %  \\  \hline
   %  \makecell[c]{Frad\\ ($\sigma=2,\tau=0.04$)} &		\textbf{0.010} &		\textbf{0.0374}	 &	\textbf{15.3}	 &	\textbf{13.7}	 &	27.8	 &	0.3419 &		1.418	 &	\textbf{5.33} &	\textbf{5.62}	& \textbf{5.55} &	\textbf{6.19}
   %  	 &	\textbf{0.020} \\

% \subsection{Results on pre-training models}

% \subsection{Results with the auxiliary tasks}
\vspace{-0.5em}
\subsection{Ablation Study}\label{sec:ablation}
\vspace{-0.5em}
In this section, we address several key questions: (i) the relationship between parameters of Gaussian mixtures and performance in denoising method. (ii) the impact of key hyper-parameters in EMPP. Additional ablation studies are provided in Appendix \ref{app:suppl_exp}.

% (i) Are denoising methods sensitive to the definition of Gaussian mixtures? (ii) What is the impact of key hyperparameters? Additional ablation studies are provided in Appendix C.2.
% , including: (i) Does masking multiple atoms within a single molecule simultaneously lead to ill-posedness? (ii) What is the difference between predictions based on the local neighbor structure and those derived from the global molecular graph?

\begin{wrapfigure}[18]{r}{0.5\textwidth}
% \vskip 0.2in
\centering
\includegraphics[scale=0.25,trim=0.3cm 0.1cm 0.1cm 0.1cm,clip]{pic/sigma.jpeg}
\caption{The curve of performance varying with the standard deviation $\sigma$.}\label{fig:sigma}
% \vskip -0.5in
\end{wrapfigure}

To assess the impact of Gaussian mixture distributions in denoising, we applied the DP-TorchMD-Net \citep{zaidi2023pretraining} to QM9 (HOMO, LUMO) as an auxiliary task, with the results displayed in Figure \ref{fig:sigma}. By varying the standard deviation $\sigma$ of the Gaussian mixture, which controls the curvature, we observed changes in the model's generalization performance. The experiment with $\sigma=0$ is equivalent to training without denoising. From Figure \ref{fig:sigma}, we derive two key insights: (i) Without utilizing external data, the denoising method demonstrates limited potential to improve the model's generalization ability, yielding results that are similar to or worse than the baseline without denoising. (ii) The performance of denoising is highly sensitive to the standard deviation, with different values leading to significantly divergent outcomes. In practical applications, because the curvature near local minima is often unknown, the optimal standard deviation typically requires empirical tuning, making it challenging to account for all possible local minima.

% \begin{table}[htbp]
% \centering
% \caption{Results on QM9 property prediction with pre-trainging on extra molecular data. EMPP use the equivariant TorchMD-Net backbone. Bold and underline indicate the best result, and the second best result.}\label{tab:std_denoising}
% \begin{tabular}{l|cccccc}
% \toprule
% $\sigma$ (\AA) & 0  & 0.001 & 0.005 & 0.010 & 0.020 & 0.050 \\
% \midrule
% HOMO                     & 22 & 31    & 26    & 24    & 23    & 22    \\
% LUMO                     & 21 & 31    & 24    & 23    & 21    & 20   \\
% \bottomrule
% \end{tabular}
% \end{table}

Another ablation experiment is to verify the key hyperparameters in EMPP, including the sampling rate and loss weight. From the Table \ref{tab:hyperparameter}, we draw two conclusions: (i) A high sampling rate over the sphere is beneficial for better predicting all possible positions, thereby enhancing generalization ability. (ii) When EMPP is used as an auxiliary task, it can be of the same importance as the original task. 
% (iii) The temperature coefficient significantly affects the results; we empirically set the default temperature coefficient to 0.1.



% 1. Can approximatively equivariant operations achieve improved performance compared to equivariant operations? 2. Does the attention module contribute to regulating both expressiveness and equivariance? 3. Are sub-structures or design in HDGNN valid? The rest ablation experiments can be found in Appendix E, which investigate the following aspects: 1) the effectiveness of normalization and the invariant branch, 2) the structure of each MLP in HDGNN, 3) the effectiveness of each component in the update block, and 4) the impact of hyperparameters. Furthermore, we analyze the training and inference times. 

\vspace{-1em}
\section{Conclusion}
\vspace{-1em}
We identified key limitations in mainstream molecular self-supervised learning: attribute masking introduces ill-posedness and fails to capture quantum features, while denoising struggles with accurately modeling Gaussian mixture in unknown distributions. To address these issues, we propose EMPP, which predicts masked atom positions through neighbor structures, bypassing Gaussian mixture approximation and turning the task into a well-posed one. Our experiments show EMPP achieves state-of-the-art results in quantum property prediction. While we employ equivariant representations in EMPP, extending to higher-order representation ($l > 3$) remains an open question.

\section*{ACKNOWLEDGEMENTS}
We are thankful to the anonymous reviewers for their helpful comments. The computations in this research were performed using the CFFF platform of Fudan University. This research was supported by the National Natural Science Foundation of China under Grant 82394432 and 92249302. The corresponding authors are Fenglei Cao and Yuan Qi.
% Reviewing the core limitations in the two mainstream molecular self-supervised learning approaches: attribute masking methods introduce ill-posedness and lack the ability to learn quantum features. Denoising can not accurately determine the Gaussian mixture distribution when facing with unknown distribution. To address the aforementioned limitations, we propose EMPP, which enables the model to learn atomic interactions by predicting masked positions through neighbor structures, bypassing the Gaussian mixture to learn quantum features and turning the entire process into a well-posed task. Our experiments demonstrate that EMPP can achieve state-of-the-art results in quantum property prediction and show the advantages of bypassing mixture distribution. EPMM provides a novel path for molecular self-supervised learning. Ultimately, EPMM utilizes low-order equivariant representations ($l \leq 3$) in this paper, making extension to higher-order equivariance is an open question. 

% Ultimately, EMPP is closely related to equivariance, and further exploration of EMPP's extension to higher-order equivariance is an open question for the future.



% proposed method over Quantum Machines 9 (QM9)~\citep{ramakrishnan2014quantum} and MD17 benchmarks
% IS2RE task in Open Catalyst 2020 (OC20)~\citep{chanussot2021open} benchmarks. In both experiments, we include Equiformer \citep{liao2023equiformer}, SEGNN \citep{brandstetter2021geometric} and TFN \citep{thomas2018tensor}, strong baselines of equivariant neural network; Dimenet++ \citep{klicpera2020fast}, strong baselines of directional GNNs;  Schnet \citep{schutt2018schnet} and PaiNN \citep{schutt2021equivariant}, classical networks for modeling quantum interactions. In the task of IS2RE, we include an additional baseline GemNet \citep{gasteiger2021gemnet,gasteiger2022gemnetoc}, SphereNet \citep{liu2021spherical} and SCN \citep{zitnick2022spherical}. In the task of QM9, L1Net \citep{miller2020relevance}, Cormorant \citep{anderson2019cormorant}, LieConv \citep{finzi2020generalizing}, TorchMD-NET \citep{tholke2022equivariant} and EQGAT \citep{le2022equivariant} are also compared with our work. We defer details of configurations and hyper-parameters of baselines to Appendix D.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\section*{APPENDIX}
\begin{adjustwidth}{2cm}{}
\addcontentsline{toc}{section}{Appendix}
\startcontents[Appendix]
\printcontents[Appendix]{l}{1}{\setcounter{tocdepth}{2}}
\end{adjustwidth}

\section{The Mathematics}\label{app:spherical_harmonic}

\subsection{The Mathematics of Spherical Harmonics}\label{app:spherical_harmonic_detail}

\subsubsection{The Properties of Spherical Harmonics}

The spherical harmonics $Y_l^m(\theta,\phi)$ are the angular portion of the solution to Laplace's equation in spherical coordinates where azimuthal symmetry is not present.
Some care must be taken in identifying the notational convention being used.
In this entry, $\theta$ is taken as the polar (colatitudinal) coordinate with $\theta$ in $[0,\pi]$, and $\phi$ as the azimuthal (longitudinal) coordinate with $\phi$ in $[0,2\pi)$. 

Spherical harmonics satisfy the spherical harmonic differential equation, which is given by the angular part of Laplace's equation in spherical coordinates. If we define the solution of Laplace's equation as $F=\Phi(\phi)\Theta(\theta)$, the equation can be transformed as:
\begin{equation}
    \frac{\Phi(\phi)}{\sin \theta} \frac{d}{d \theta}\left(\sin \theta \frac{d \Theta}{d \theta}\right)+\frac{\Theta(\theta)}{\sin ^{2} \theta} \frac{d^{2} \Phi(\phi)}{d \phi^{2}}+l(l+1) \Theta(\theta) \Phi(\phi)=0 
\end{equation}
Here we omit the derivation process and just show the result.  The (complex-value) spherical harmonics are defined by:
\begin{equation}
 Y^{l}_{m}(\theta, \phi) \equiv \sqrt{\frac{2 l+1}{4 \pi} \frac{(l-m) !}{(l+m) !}} P^{l}_{m}(\cos \theta) e^{i m \phi},
    % Y_{\ell}^{m}(\theta, \phi) \rightarrow Y_{\ell}^{m}(\pi-\theta, \pi+\phi)=(-1)^{\ell} Y_{\ell}^{m}(\theta, \phi)
\end{equation}
where $P^{l}_{m}(\cos \theta)$ is an associated Legendre polynomial.
Spherical harmonics are integral basis, which satisfy:
\begin{equation}
\begin{array}{l}
\int_{0}^{2 \pi} \int_{0}^{\pi} Y^{l_{1}}_{m_{1}}(\theta, \phi) Y^{l_{2}}_{m_{2}}(\theta, \phi) Y^{l_{3}}_{m_{3}}(\theta, \phi) \sin \theta d \theta d \phi \\
=\sqrt{\frac{\left(2 l_{1}+1\right)\left(2 l_{2}+1\right)\left(2 l_{3}+1\right)}{4 \pi}}\left(\begin{array}{ccc}
l_{1} & l_{2} & l_{3} \\
0 & 0 & 0
\end{array}\right)\left(\begin{array}{ccc}
l_{1} & l_{2} & l_{3} \\
m_{1} & m_{2} & m_{3}
\end{array}\right),
\end{array}
\end{equation}
where $\left(\begin{array}{ccc}
l_{1} & l_{2} & l_{3} \\
m_{1} & m_{2} & m_{3}
\end{array}\right)$ is a Wigner 3j-symbol (which is related to the Clebsch-Gordan coefficients).
We list a few spherical harmonics which are:
\begin{equation}
\label{equ:app_sh}
\begin{aligned}
Y_{0}^{0}(\theta, \phi) & =\frac{1}{2} \sqrt{\frac{1}{\pi}} \\
Y^{1}_{-1}(\theta, \phi) & =\frac{1}{2} \sqrt{\frac{3}{2 \pi}} \sin \theta e^{-i \phi} \\
Y^{1}_{0}(\theta, \phi) & =\frac{1}{2} \sqrt{\frac{3}{\pi}} \cos \theta \\
Y^{1}_{1}(\theta, \phi) & =\frac{-1}{2} \sqrt{\frac{3}{2 \pi}} \sin \theta e^{i \phi} \\
Y^{2}_{-2}(\theta, \phi) & =\frac{1}{4} \sqrt{\frac{15}{2 \pi}} \sin { }^{2} \theta e^{-2 i \phi} \\
Y^{2}_{-1}(\theta, \phi) & =\frac{1}{2} \sqrt{\frac{15}{2 \pi}} \sin \theta \cos \theta e^{-i \phi} \\
Y^{2}_{0}(\theta, \phi) & =\frac{1}{4} \sqrt{\frac{5}{\pi}}\left(3 \cos ^{2} \theta-1\right) \\
Y^{2}_{1}(\theta, \phi) & =\frac{-1}{2} \sqrt{\frac{15}{2 \pi}} \sin \theta \cos \theta e^{i \phi} \\
Y^{2}_{2}(\theta, \phi) & =\frac{1}{4} \sqrt{\frac{15}{2 \pi}} \sin { }^{2} \theta e^{2 i \phi}
\end{aligned}
\end{equation}

In this work, we use the real-value spherical harmonics rather than the complex-value one. 

% \subsection{Fourier transformation over $S^2$}

\subsubsection{Fourier transformation over $S^2$}\label{app:fouriertrans}
It is well known that the spherical harmonic $Y^l_m$ form a complete set of orthonormal functions and thus form an orthonormal basis of the Hilbert space of square-integrable function.
On the unit sphere $S^2$, any square-integrable function $f$ can thus be expanded as a linear combination of these:
\begin{equation}\label{app_equ:inverse_fourier}
    f(\theta, \phi) = \sum_{l=0}^{\infty} \sum^{l}_{m=-l} f^{l}_{m}Y^{l}_{m}(\theta, \phi),
\end{equation}

The coefficient $f^{l}_{m}$ can be obtained by the Fourier transformation over $S^2$, which is
\begin{equation}\label{app_equ:fourier}
    f^l_m = \int_{S^2} f(\Vec{r}) Y^{l*}_{m}(\Vec{r}) d\Vec{r} = \int_{0}^{2\pi}\int_{0}^{\pi} d\theta \sin \theta f(\theta,\psi) Y^{l^*}_{m}(\theta,\psi).  
\end{equation}
Usually we define a vector $\mathbf{f}^l = [f^l_{-l},f^l_{-l+1}, ...,f^l_{l}]$ to denote the Fourier coefficients with degree $l$.
We now investigate how the fourier coefficients transforms if we rotate the input signal.
More precisely, we want to calculate the coefficient $\mathbf{f}^l_{\mathbf{R}}$ of the signal $f(\mathbf{R}\Vec{r})$, where $\mathbf{R} \in SO(3)$ is a rotation matrix.

Using the fact $ \mathbf{Y}^{l}(\mathbf{R}\vec{\mathbf{r}}) = \mathbf{D}^{l}(\mathbf{R}) \mathbf{Y}^{l}(\vec{\mathbf{r}}), 
$ and \eqref{app_equ:inverse_fourier}, we know 
$$f(\mathbf{R} \Vec{r}) = \sum_{l=0}^{\infty} \sum^{l}_{m=-l} f^{l}_{m}Y^{l}_{m}(\mathbf{R}\Vec{r}) = \sum_{l=0}^{\infty} \sum^{l}_{m=-l}  f^{l}_{m} \sum_{m'} \mathbf{D}_{mm'}Y^{l}_{m'}(\Vec{r}).$$
Therefore $\mathbf{f}^l_{\mathbf{R}} = \mathbf{D}^T 
\mathbf{f}^l$ and it is steerable.

\subsubsection{The Relationship Between Spherical Harmonics and Wigner-D Matrix}
A rotation $\mathbf{R}$ sending the $\Vec{\mathbf{r}}$ to $\Vec{\mathbf{r}}'$ can be regarded as a linear combination of spherical harmonics that are set to the same degree.
The coefficients of linear combination represent the complex conjugate of an element of the Wigner D-matrix.
The rotational behavior of the spherical harmonics is perhaps their quintessential feature from the viewpoint of group theory.
The spherical harmonics $Y^{l}_{m}$ provide a basis set of functions for the irreducible representation of the group SO(3) with dimension $(2l+1)$.

The Wigner-D matrix can be constructed by spherical harmonics. 
Consider a transformation $Y^{l}_{m}(\Vec{\mathbf{r}}) = Y^{l}_{m}(\mathbf{R}_{\alpha, \beta, \gamma}\Vec{\mathbf{r}}_{x})$, where $\Vec{\mathbf{r}}_{x}$ denote the x-orientation. $\alpha, \beta, \gamma$ denotes the items of Euler angle.
Therefore, $Y^{l}_{m}(\Vec{\mathbf{r}})$ is invariant with respect to rotation angle $\gamma$.
Based on this viewpoint, the Wigner-D matrix with shape $(2l+1)\times(2l+1)$ can be defined by:
\begin{equation}
     D^{l}_{m}(\mathbf{R}_{\alpha, \beta, \gamma}) = \sqrt{2l+1} Y^{l}_{m}(\Vec{\mathbf{r}}). \\
\end{equation}
In this case, the orientations are encoded in spherical harmonics and their Wigner-D matrices, which are utilized in our cross module.

\subsection{Equivariant Operation}\label{app:spherical_harmonic_equi}

% \subsection{The Relationship Between Spherical Harmonics and Wigner-D Matrix}
% As shown in Figure~\ref{fig: Wigner}

\subsubsection{Equivariance of Clebsch-Gordan Tensor Product}
The Clebsch-Gordan Tensor Product shows a strict equivariance for different
group representations, which make the mixture representations transformed equivariant based on Wigner-D matrices. 
% More formally, the equivariance of CG tensor product can be represented by:
% \begin{equation}
% \label{equ:CG_Wigner}
%     (\mathbf{D}^{l_{1}}(\mathbf{R}) \mathbf{u} \otimes \mathbf{D}^{l_{2}}(\mathbf{R}) \mathbf{v})^{l} = \mathbf{D}^{l}(\mathbf{R})(\mathbf{u} \otimes \mathbf{v})^{l}.
% \end{equation}
% In equation ~\eqref{equ:CG_Wigner}, all Wigner-D matrices share the same transformation parameters. 
% To prove equation ~\eqref{equ:CG_Wigner}. 
We use $D_{m'_{1}, m_{1}}$ to denote the element of Wigner-D matrix.
The Clebsch-Gordan coefficient satisfies:
\begin{equation}
\begin{array}{c}
\sum_{m_{1}^{\prime}, m_{2}^{\prime}} C_{\left(l_{1}, m_{1}^{\prime}\right)\left(l_{2}, m_{2}^{\prime}\right)}^{\left(l_{0}, m_{0}\right)} D_{m_{1}^{\prime} m_{1}}^{l_{1}}(g) D_{m_{2}^{\prime} m_{2}}^{l_{2}}(g) \\
=\sum_{m_{0}^{\prime}} D_{m_{0} m_{0}^{\prime}}^{l_{0}}(g) C_{\left(l_{1}, m_{1}\right)\left(l_{2}, m_{2}\right)}^{\left(l_{0}, m_{0}^{\prime}\right)} \\
\end{array}
\end{equation}
Therefore, the spherical harmonics can be combined equivariantly by CG Tensor Product:
\begin{equation}
\label{equ:wignercg}
    \begin{array}{l}
CG\left(\sum_{m_{1}^{\prime}} D_{m_{1} m_{1}^{\prime}}^{l_{1}}(g) Y_{m_{1}^{\prime}}^{l_{1}}, \sum_{m_{2}^{\prime}} D_{m_{2} m_{2}^{\prime}}^{l_{2}}(g) Y_{m_{2}^{\prime}}^{l_{2}}\right)_{m_{0}}^{l_{0}} \\

=\sum_{m_{1}, m_{2}} C_{\left(l_{1}, m_{1}\right)\left(l_{2}, m_{2}\right)}^{\left(l_{0}, m_{0}\right)} \sum_{m_{1}^{\prime}} D_{m_{1} m_{1}^{\prime}}^{l_{1}}(g) Y_{m_{1}^{\prime}}^{l_{1}} \sum_{m_{2}^{\prime}} D_{m_{2} m_{2}^{\prime}}^{l_{2}}(g) Y_{m_{2}^{\prime}}^{l_{2}} \\


=\sum_{m_{0}^{\prime}} D_{m_{0} m_{0}^{\prime}}^{l_{o}}(g) \sum_{m_{1}, m_{2}} C_{\left(l_{1}, m_{1}\right)\left(l_{2}, m_{2}\right)}^{\left(l_{0}, m_{0}^{\prime}\right)} Y_{m_{1}^{\prime}}^{l_{1}} Y_{m_{2}^{\prime}}^{l_{2}}
\\
=\sum_{m_{0}^{\prime}} D_{m_{0} m_{0}^{\prime}}^{l_{0}}(g) CG_{ m_{0}^{\prime}}^{l_{0}}\left(Y_{m_{1}^{\prime}}^{l_{1}}, Y_{m_{2}^{\prime}}^{l_{2}}, \right) .
\end{array}
\end{equation}
\eqref{equ:wignercg} represents a relationship between scalar. If we transform the scalar to vector or matrix, \eqref{equ:wignercg} is equal to
\begin{equation}
\label{equ:CG_Wigner}
    (\mathbf{D}^{l_{1}}_\mathbf{R} \mathbf{u} \otimes \mathbf{D}^{l_{2}}_\mathbf{R} \mathbf{v})^{l} = \mathbf{D}^{l}_\mathbf{R}(\mathbf{u} \otimes \mathbf{v})^{l}.
\end{equation}
The tensor CG product mixes two representations to a new representation under special rule.
For example, 1.two type-$0$ vectors will only generate a type-$0$ representations; 2.type-$l_{1}$ and type-$l_{2}$ can generate type-$l_{1}+l_{2}$ vector at most.
Note that some widely-used products are related to tensor product: scalar product ($l_1=0$, $l_2=1$, $l=1$), dot product ($l_1=1$, $l_2=1$, $l=0$) and cross product ($l_1=1$, $l_2=1$, $l=1$). 
However, for each element with $l>0$, there are multi mathematical operation for the connection with weights.
The relation between number of operations and degree is quadratic. 
Thus, as degree increases, the amount of computation increases significantly, making calculation of the CG tensor product slow for higher order irreps. 
This statement can be proven by the implementation of e3nn (o3.FullyConnectedTensorProduct).

\subsubsection{Learnable Parameters in Tensor Product}
We utilize the e3nn library~\citep{e3nn} to implement the corresponding tensor product. It is crucial to emphasize that the formulation of CG tensor product is devoid of any learnable parameters, as CG coefficients remain constant. In the context of e3nn, learnable parameters are introduced into each path, represented as $w(\mathbf{u}^{l_{1}} \otimes \mathbf{v}^{l_{2}})$. Importantly, these learnable parameters will not destory the equivariance of each path. However, they are limited in capturing directional information. In equivariant models, the original CG tensor product primarily captures directional information. We have previously mentioned our replacement of the CG tensor product with learnable modules. It is worth noting that our focus lies on the CG coefficients rather than the learnable parameters in the e3nn implementation. 
% This basic tensor product lies at the core of combining two irreducible representations. For an equivariant model, its variables span multiple degrees and channels, with each degree within each channel constituting a fundamental irreducible representation, colloquially referred to as a "unit" by us. The role of the function in e3nn is to select distinct units for the CG tensor product. Every pairing of two units gives rise to a "path". The e3nn function introduces a learnable weight factor for each path, although it does not impact the calculation of the CG tensor product within that path. For example, o3.FullyconnectedTensorproduct contains contains all possible paths, DTP contains paths, in which two irreps have the same channel orders.

\subsubsection{Gate Activation and Normalization}
The gate activation and normalization used in HDGNN are implement by e3nn code framework.

\textbf{Gate Activation.} In equivariant models, the gate activation combines two sets of group representations. The first set consists of scalar irreps ($l=0$), which are passed through standard activation functions such as sigmoid, ReLU and SiLU. The second set comprises higher-order irreps (($l>0$)), which are multiplied by an additional set of scalar irreps that are introduced solely for the purpose of the activation layer. These scalar irreps are also passed through activation functions.

The gate activation allows for the controlled integration of different types of irreps in the network. The scalar irreps capture global and local patterns, while the higher-order irreps capture more complex relationships and interactions. By combining these irreps in a gate-like manner, the gate activation enables the model to selectively amplify or suppress information flow based on the importance of different irreps for a given task.

% \begin{equation}
% Gate(x) = \sigma(W_1 \odot ReLU(x) + W_2 \odot ReLU(W_3 \odot x)) \\
% \end{equation}
% In the above equation, $x$ represents the input to the gate activation function. The notation $\odot$ denotes element-wise multiplication. $W_1$, $W_2$, and $W_3$ are learnable weight matrices that control the transformation of the irreducible representations (irreps). The ReLU function is applied element-wise to the input $x$ and the intermediate term $W_3 \odot x$, while the sigmoid function ($\sigma$) is applied to the sum of two terms.

% The Gate activation allows the model to selectively amplify or suppress information from different irreps, facilitating the integration of both scalar and higher-order irreps. This enables the network to capture complex patterns and relationships in the data while leveraging the equivariant properties of the model.

\textbf{Normalization.} Normalization is a technique commonly used in neural networks to normalize the activations within each layer. It helps stabilize and accelerate the training process by reducing the internal covariate shift, which refers to the change in the distribution of layer inputs during training. 

% In the context of equivariant neural networks, Normalization can be applied to enhance the learning process and improve the model's performance. By normalizing the activations within each layer, it helps alleviate the issues related to varying scales and distributions of the inputs, especially when dealing with different equivariant representations.

The normalization process involves computing the mean and variance across the channels. In equivariant normalization, the variance is computed using the root mean square value of the L2-norm of each type-$l$ vector. Additionally, this normalization removes the mean term. The normalized activations are then passed through a learnable affine transformation without a learnable bias, which enables the network to adjust the mean and variance based on the specific task requirements.

% By incorporating Layer Normalization into equivariant neural networks, it becomes possible to improve the overall performance, stability, and robustness of the models, particularly when dealing with complex and high-dimensional data.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.3]{pic/single_mask.jpeg}
\caption{Determinacy of atomic positions in organic molecules. The relationship between atoms and colors is (Carbon, gray; Hydrogen, white; Oxygen, red; Nitrogen, blue). When the system is fixed, most atoms have a uniquely determined position. The translucent area represents the atom of interest, and the arrow indicates its unique position.}\label{fig:uniqueposition}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[scale=0.7]{pic/single_mask_m.png}
\caption{Atoms with uncertain positions exist. In organic molecules, the positions of some atoms have multiple possibilities, corresponding to several local minima of the potential energy surface. We represent the two possible positions of the atom with arrows.}\label{fig:uniqueposition_two}
\end{figure}


\section{Model Details}
\subsection{The Position Distribution in Organic Chemistry}\label{app:model_example}

In the realm of organic chemistry, the majority of atomic positions within molecules are unequivocally determined, reflecting the precise architectural principles that govern molecular structure. This certainty stems from the well-defined rules of covalent bonding, hybridization, and the tetrahedral geometry that (C, H, O, N, F, S, ...) atoms often exhibit, which together constrain the spatial arrangement of atoms in most organic compounds. As illustrated in the Figure \ref{fig:uniqueposition}, for a given molecule, the majority of its constituent atoms possess uniquely determined positions. Any other arrangements would result in an implausible molecular structure.

However, there exists a minority subset of scenarios where atomic positions may be ambiguous due to the existence of multiple potential locations, corresponding to various local minima on the potential energy surface. These sites of uncertainty are typically associated with conformational flexibility, such as in molecules with large rotational freedom about single bonds, or in cases where there are multiple stable conformations or rotameric states. We take Figure \ref{fig:uniqueposition_two} as example, the hydroxyl group of the molecule can rotate, allowing for two possible positions of the hydrogen atom.

Furthermore, when considering the positional possibilities of two atoms simultaneously, the combinatorial complexity of their potential configurations increases significantly. This multiplicative effect is a consequence of the interplay between intramolecular forces and the three-dimensional nature of molecular space, which allows for a vast array of spatial orientations and relative positions. As illustrated in the Figure \ref{fig:twomask}, masking multiple atoms introduces a multitude of possibilities, rendering the position prediction of EMPP an ill-posed problem.
% The exploration of these atomic positions and their implications is crucial for understanding molecular properties, reactivity, and the nuances of chemical behavior in organic systems. 

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.17]{pic/two_mask.jpeg}
\caption{The Possible positions of multiple atoms are masked. When calculating the forces acting on each atom, the influence of another masked atom must also be taken into account.}\label{fig:twomask}
\end{figure}

In summary, while most atomic positions in organic molecules are rigidly defined, there are instances where positional ambiguity arises, and this complexity is magnified when multiple atoms are considered. This interplay between certainty and uncertainty in atomic positioning is a fascinating aspect of organic chemistry that has profound implications for our understanding of molecular structure and behavior.



\subsection{Details of Prediction Module}\label{app:Prediction Module}
Our position prediction module consists of multiple equivariant operations, designed based on the structure of the backbone. For the Equiformer, the output feature $\mathbf{f}_{k}$ is represented as ``128x0e+64x1e+32x2e'' (irreducible representations in e3nn \citep{e3nn}). We first standardize the number of channels using a linear layer, resulting in ``64x0e+64x1e+64x2e''. Through a gating mechanism and a second linear layer, $\mathbf{f}_{k}$ is mapped to a representation of ``32x0e+32x1e+32x2e''. Following Fourier transformation and gridding, we apply a shared MLP on the spherical surface, which is equivalent to a 1D convolution. Since nonlinear transformations on the sphere do not disrupt equivariance, the shared MLP first projects the channels into a 16-dimensional space and then applies the SiLU activation function. Finally, the 16-dimensional vector is projected back down to one dimension, yielding a non-normalized distribution.

\subsection{TorchMD-Net with Higher-order Spherical Harmonic Representation}\label{app:torchmd}
In the position prediction module, the EMPP must map the equivariant representations onto a distribution on the spherical surface. A greater diversity of frequencies in the frequency domain representation leads to a finer time-domain representation post-transformation. EMPP is built upon Equiformer and TorchMD-Net; while Equiformer incorporates high-order spherical harmonics representations (analogous to higher frequencies), TorchMD-Net includes only invariant features of $l=0$ and three-dimensional equivariant features of $l=1$. Consequently, the output after Fourier transformation may not effectively capture the fine-grained positional distribution. To address this, we convert the TorchMD-Net embeddings into high-order spherical harmonics representations. First, we project the embeddings onto representations of $l=[0,1,2,3]$ using the spherical harmonics function. Next, we expand the number of attention coefficients from TorchMD-Net, applying them to each degree of representation (whereas the original model only applied these coefficients to the $l=1$ representation). When calculating the inner product, the inner product of high-order spherical harmonics remains rotationally invariant, ensuring that the modified TorchMD-Net maintains overall equivariance. We apply this variant in our experiments. Note that we do not change the core operations in TorchMD-Net.

\subsection{Prediction with Neighbors}
% In equations \eqref{equ:radiuslabel} and \eqref{equ:dirlabel}, we restrict the prediction of masked positions to neighboring nodes within the cutoff radius. This is because, in GNNs, atoms beyond the cutoff radius do not directly interact with the masked atom; rather, they influence the potential energy surface (PES) indirectly by transmitting features to the embeddings of neighboring nodes through multi-layer forward propagation. Including points outside the cutoff during loss calculation would violate the inductive bias of GNNs. Furthermore, our ablation study shows that incorporating atoms beyond the cutoff can reduce the model's generalization performance.

In equations \eqref{equ:radiuslabel} and \eqref{equ:dirlabel}, we restrict the prediction of masked positions to neighboring nodes within the cutoff radius. This restriction arises from the fact that, in GNNs, atoms beyond the cutoff do not directly interact with the masked atom; instead, they influence the PES indirectly by transmitting features to the embeddings of neighboring nodes through multi-layer forward propagation. Including points outside the cutoff during loss calculation would compromise the inductive bias inherent in GNNs. Furthermore, our ablation study demonstrates that incorporating atoms beyond the cutoff can diminish the model's generalization performance.

\subsection{Differences from Symphony}\label{app:diff_sym}
Symphony \citep{daigavane2024symphony} proposes a molecular generation framework based on equivariant representations. It constructs a fragment sequence by iteratively masking atoms, with each subsequent fragment masking one additional atom. Symphony then uses spherical harmonic projections of high-degree equivariant representations to encode the spatial distribution of atomic positions, enabling it to sample new atoms based on the predicted distribution. EMPP uses the similar technique to define position prediction module where the relative positions are projected onto a spatial distribution based on high-degree equivariant representations. However, EMPP and Symphony differ in several key aspects:

\begin{itemize} 
\item \textbf{Masking Strategy:} Symphony, geared towards molecular generation, masks all information about an atom, relying on the model's prediction of a "focus" atom to sample subsequent atoms.  EMPP, conversely, masks only the position of a single atom, retaining the ground truth of its atomic type and neighbor information.  EMPP's position inference leverages all available real information from the molecule.  These distinct strategies reflect their respective goals: Symphony prioritizes generating diverse molecules, while EMPP focuses on learning precise interaction patterns.

\item \textbf{Prediction Horizon:} Symphony predicts the position of an atom without considering the positions of subsequent atoms.  EMPP, however, uses all available positional information except for the masked atom's position. This difference again stems from their objectives: Symphony emphasizes diversity and fragment-based generation, while EMPP prioritizes accuracy by minimizing interfering factors in position prediction to capture genuine physical interactions.

\item \textbf{Spatial Projection Module:} Symphony normalizes multi-channel features using the aggregation $\log(\sum_{channel} \exp(\cdot))$. While parameter-efficient, this approach may make the resulting distribution sensitive to the spherical harmonic representation, $\mathbf{f}_{k}$, particularly after gridding. EMPP employs a neural network (detailed in \eqref{equ:direction}) to aggregate multi-channel features on the sphere. Ablation experiments (Appendix \ref{app:suppl_exp}) demonstrate the greater stability of the distribution generated by EMPP's neural network.

\item \textbf{Label Definition:} Symphony uses a Dirac delta distribution as the label for spatial projection to encourage sampling of physically plausible positions.  EMPP, not designed for sampling, relaxes this constraint and adopts a smoother distribution to improve generalization performance.

\end{itemize}

\begin{table}[htbp]
\centering
\caption{Hyper-parameters for QM9 experiments without pre-train.}\label{app:qm9}
\scalebox{0.8}{
\begin{tabular}{ll}
%\cline{7-11}
\toprule[1.2pt]
Hyper-parameters & Value or description
 \\
\midrule[1.2pt]
Optimizer & AdamW \\
Learning rate scheduling & Cosine learning rate with linear warmup \\
Warmup epochs & $5$ \\
%Maximum learning rate & $1.5 \times 10 ^{-4}$ for $G$, $H$, $U$ and $U_0$, and $5 \times 10 ^{-4}$ for others \\
Maximum learning rate & $5 \times 10 ^{-4}$, $1.5 \times 10 ^{-4}$ \\
%Batch size & $64$ for $G$, $H$, $U$ and $U_0$, and $128$ for others \\
Batch size & $128$, $64$ \\
%Number of epochs & $600$ for $G$, $H$, $U$ and $U_0$, and $300$ for others \\
Number of epochs & $300$, $600$ \\
%Weight decay & $0$ for $G$, $H$, $U$ and $U_0$, and $5 \times 10^{-3}$ for others \\
Weight decay & $5 \times 10^{-3}$, $0$ \\
%Dropout rate & $0.0$ for $G$, $H$, $U$ and $U_0$, $0.1$ for $R^{2}$, and $0.2$ for others \\
Dropout rate & $0.0$, $0.2$ \\
% ===============================
%   Add this back in the futrure
% ===============================
%Dropout rate & $0.1$, $0.2$ \\
\midrule[0.6pt]
Cutoff radius (\AA) & $5$ \\

\midrule[0.6pt]

\multicolumn{2}{c}{EMPP} \\
\\
The maximum value of degree & $2$ \\ 
Number of Sampling & $100^{2}$ \\
Intermediate dimensions & $[(64, 0), (64, 1), (64, 2)]$ \\
Output dimensions & $[(32, 0), (32, 1), (32, 2)]$ \\
Encoding dimensions & $[(128, 0), (64, 1), (32, 2)]$ \\
MLP on the spherical representation  & $Linear(32, 16)-SiLU-Linear(16, 1)$ \\
Loss weights & $1$ \\
Temperature & $0.1$ \\

\bottomrule[1.2pt]
\end{tabular}
}
\label{appendix:tab:qm9}
\end{table}

\section{Details of Experiments and Supplementary Experiments}\label{app:exp}
\subsection{Implementation Details}\label{app:im_de}
First, we introduce the hyperparameter configuration of EMPP when used as an auxiliary task, with the basic training configurations based on the Equiformer setup. The configurations for QM9 and MD17 are recorded in Table \ref{app:qm9} and Table \ref{app:md17}, respectively. Note that there are two sets of configurations for QM9, the one with a longer epoch or smaller batch size is for the four tasks of $G$, $H$, $U$ and $U_0$ tasks. 

\begin{table}[htbp]
\centering
\caption{Hyper-parameters for MD17 dataset without pre-train.}
\label{app:md17}
\scalebox{0.8}{
\begin{tabular}{ll}
%\cline{7-11}
\toprule[1.2pt]
Hyper-parameters & Value or description
 \\
\midrule[1.2pt]
Optimizer & AdamW \\
Learning rate scheduling & Cosine learning rate with linear warmup \\
Warmup epochs & $10$ \\
%Maximum learning rate & $1.5 \times 10 ^{-4}$ for $G$, $H$, $U$ and $U_0$, and $5 \times 10 ^{-4}$ for others \\
Maximum learning rate & $5 \times 10 ^{-4}$ \\
%Batch size & $64$ for $G$, $H$, $U$ and $U_0$, and $128$ for others \\
Batch size & $8$ \\
%Number of epochs & $600$ for $G$, $H$, $U$ and $U_0$, and $300$ for others \\
Number of epochs & $2000$ \\
%Weight decay & $0$ for $G$, $H$, $U$ and $U_0$, and $5 \times 10^{-3}$ for others \\
Weight decay & $1 \times 10^{-6}$ \\
%Dropout rate & $0.0$ for $G$, $H$, $U$ and $U_0$, $0.1$ for $R^{2}$, and $0.2$ for others \\
Dropout rate & $0.0$ \\
\midrule[0.6pt]
Weight for energy loss & $1$ \\
Weight for force loss & $80$ \\
% ===============================
%   Add this back in the futrure
% ===============================
%Dropout rate & $0.1$, $0.2$ \\
\midrule[0.6pt]
Cutoff radius (\AA) & $5$ \\

\midrule[0.6pt]

\multicolumn{2}{c}{EMPP} \\
% \multicolumn{2}{c}{Equiformer ($L_{max} = 3$)} \\
The maximum value of degree & $3$ \\ 
Number of Sampling & $100^{2}$ \\
Intermediate dimensions & $[(64, 0), (64, 1), (64, 2), (32,3)]$ \\
Output dimensions & $[(32, 0), (32, 1), (32, 2), (32, )]$ \\
Encoding dimensions & $[(128, 0), (64, 1), (64, 2), (32, 3)]$ \\
MLP on the spherical representation  & $Linear(32, 16)-SiLU-Linear(16, 1)$ \\
Loss weights & $50$ \\
Temperature & $0.1$ \\
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}

In the experiments on PCQM4MV2, we only use EPMM for pre-training. The experimental configurations are shown in Table \ref{app:PCQ}. The validation set of PCQM4MV2 is used to evaluate the accuracy of position prediction. Most of our training configurations are the same as those of the Denoising method \citep{zaidi2023pretraining}. During the fine-tuning phase on QM9, we use the training parameters from Denoising method and the model hyper-parameters in Table \ref{app:PCQ}. The loss weight for EMPP is set to $1$.


\textbf{Hyper-parameters of Baselines.} In Table \ref{tab:QM9withoutPT} and Table \ref{tab:md17_result}, the results of baselines is from \citep{liao2023equiformer}. In Table \ref{tab:QM9withPT}, the results of baselines is from \citep{feng2023fractional}. Additionally, the result of AttrMask is from \citep{luo2022one}.



In the ablation study, we verified the relationship between the denoising method and the standard deviation. The experimental setup for this part follows Work A. The difference is that we did not introduce a PCQ-based pre-training model, and the loss weight for denoising was set to 1. 


\subsection{Supplementary Experiments}\label{app:suppl_exp}

\textbf{GEOM-Drug.} GEOM-Drug \citep{axelrod2022geom} is a dataset containing a diverse collection of non-equilibrium molecular data, featuring a broader range of atomic types and molecular sizes. In our experiments, we use Equiformer as the backbone model. The training configuration follows the setup used in QM9 experiments, with the number of training epochs set to 300. The absolute energy of each conformation is used as the target label.

\begin{table}[htbp]
\centering
\caption{Hyper-parameters for PCQM4MV2 dataset.}
\label{app:PCQ}
\scalebox{0.8}{
\begin{tabular}{ll}
%\cline{7-11}
\toprule[1.2pt]
Hyper-parameters & Value or description
 \\
\midrule[1.2pt]
Optimizer & AdamW \\
Learning rate scheduling & Cosine learning rate with linear warmup \\
Warmup steps & $10000$ \\
%Maximum learning rate & $1.5 \times 10 ^{-4}$ for $G$, $H$, $U$ and $U_0$, and $5 \times 10 ^{-4}$ for others \\
Maximum learning rate & $5 \times 10 ^{-4}$ \\
%Batch size & $64$ for $G$, $H$, $U$ and $U_0$, and $128$ for others \\
Batch size & $70$ \\
%Number of epochs & $600$ for $G$, $H$, $U$ and $U_0$, and $300$ for others \\
Number of epochs & $20$ \\
%Weight decay & $0$ for $G$, $H$, $U$ and $U_0$, and $5 \times 10^{-3}$ for others \\
Weight decay & $0.0$ \\
%Dropout rate & $0.0$ for $G$, $H$, $U$ and $U_0$, $0.1$ for $R^{2}$, and $0.2$ for others \\
% ===============================
%   Add this back in the futrure
% ===============================
%Dropout rate & $0.1$, $0.2$ \\
\midrule[0.6pt]
Cutoff radius (\AA) & $5$ \\

\midrule[0.6pt]

\multicolumn{2}{c}{EMPP} \\
% \multicolumn{2}{c}{Equiformer ($L_{max} = 3$)} \\
The maximum value of degree & $3$ \\
Number of Sampling & $100^{2}$ \\
Intermediate dimensions & $[(64, 0), (64, 1), (64, 2), (64, 3)]$ \\
Output dimensions & $[(32, 0), (32, 1), (32, 2), (32, 3)]$ \\
Encoding dimensions & $[(128, 0), (128, 1), (128, 2), (128, 3)]$ \\
MLP on the spherical representation  & $Linear(32, 16)-SiLU-Linear(16, 1)$ \\
Temperature & $0.1$ \\
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}

\begin{table}[htbp]
\centering
\caption{Ablation experiments of multiple atom masking on QM9 tasks.}\label{tab:maskingmoreatom}
% \begin{tabular*}{\linewidth}{@{}L|LLLLLLLLLLLL@{}}
\begin{tabular}{l|cccc}
\toprule
Task      & $\alpha$ & $\Delta \epsilon$ & $\varepsilon_{HOMO}$ & $\varepsilon_{LUMO}$  \\
Units     & bohr$^{3}$ & meV                  & meV                  & meV                 \\ \midrule
Mask 1 atom  & .041    & 27 & 14 & 13     \\ 
Mask 2 atom  & .046    & 31 & 16 & 16    \\ 
Mask 3 atom  & .052    & 33 & 20 & 19     \\ 
\bottomrule
\end{tabular}
\end{table}

For data preparation, we randomly sample 200,000 molecules from GEOM-Drug as the training set and 10,000 molecules as the validation set. To ensure the reliability of the validation results, SMILES strings appearing in the validation set are excluded from the training set, recognizing that a single SMILES can represent multiple conformational data points.


The experimental results, summarized in Table \ref{tab:GEOM-Drug}, demonstrate that EMPP achieves substantial performance improvements on GEOM-Drug, reducing the energy MAE by $48\%$. From this, we draw two key conclusions: (a) EMPP is effective for more complex organic molecular systems. (b) Despite the prevalence of non-equilibrium structures in GEOM-Drug, EMPP remains highly effective as an auxiliary task.

This experiment highlights EMPP’s ability to overcome the limitations of denoising methods, which are typically restricted to approximating equilibrium structures.

\begin{table}[htbp]
\centering
\caption{Ablation experiments of spherical operations on QM9 tasks.}\label{tab:sphericalopseration}
% \begin{tabular*}{\linewidth}{@{}L|LLLLLLLLLLLL@{}}
\begin{tabular}{l|cccc}
\toprule
Task      & $\alpha$ & $\Delta \epsilon$ & $\varepsilon_{HOMO}$ & $\varepsilon_{LUMO}$  \\
Units     & bohr$^{3}$ & meV                  & meV                  & meV                 \\ \midrule
Shared MLP  & .041    & 27 & 14 & 13     \\ 
$\log(\sum_{channel} \exp(\cdot))$  & .043    & 28 & 14 & 14     \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Ablation study on label distribution. }\label{tab:labeldistribution}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
Method             & $\alpha$ & $\varepsilon_{HOMO}$ \\
\midrule
Without self-supervision & .046  & 15.4 \\
Baseline & .041  & 14.2 \\
Directly predict relative positions & .044  & 15.1 \\
Dirac delta for radius  & .042  & 14.6 \\
Dirac delta for direction & .044  & 14.6 \\
Dirac delta for both & .044  & 14.8 \\
\bottomrule
\end{tabular}
% }
\end{table}

\textbf{Dirac delta distribution vs. Gaussian distribution.} We have mentioned in Section \ref{sec:loss} that whether Dirac delta distribution and Gaussian distribution are a one-to-one projection to ground truth $\vec{\mathbf{r}}_{ik}$, they are all theoretically correct. Here, we conduct experiments to investigate their difference. In detail, we set the $\sigma$ in \eqref{equ:radiuslabel} to $0.0001$ to make the radius distribution similar to Dirac delta distribution (The ideal Dirac delta distribution is hard to sample). Similarly, we add the temperature coefficient to \eqref{equ:directionlabel} and set it to $0.0001$ to make the spherical distribution closed to Dirac delta distribution. Additionally, We also conducted a set of experiments to directly predict relative positions $\vec{\mathbf{r}}_{ik}$.

The results are shown in Table \ref{tab:labeldistribution}, where ``Baseline'' denotes using original setting. we found that the Dirac delta distribution can also achieve improvements compared to distribution in \eqref{equ:radiuslabel} and \eqref{equ:directionlabel}. However, the sharp distribution will reduce the stability of training: when we repeat the experiments, the baseline model converges to the optimal result every time, however, EMPP based on the Dirac distribution requires more than five repetitions of the experiment to find the results presented in Table \ref{tab:labeldistribution}. Finally, there is another observation from Table \ref{tab:labeldistribution}: directly predict the relative positions limits the effectiveness of EMPP.


\textbf{Transferability.} We also assessed the transferability of two distributions. We pre-trained on PCQM4MV2 and transferred to QM9. We use the TorchMD-Net model and the ``baseline'' employed EMPP's 1-mask strategy. ``Dirac delta for both'' involved changing the distributions in both pre-training and fine-tuning to Dirac delta distribution, similar to the experiments in Table \ref{tab:labeldistribution}. From Table \ref{tab:transferability}, it can be seen that a relaxed distribution contributes to the transferability of EMPP.


\begin{table}[htbp]
\centering
\caption{Ablation study on label transferability. The backbone is TorchMD-Net.}\label{tab:transferability}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
Method            & $\alpha$ & $\varepsilon_{HOMO}$ \\
\midrule
Without self-supervision and pre-train & .059  & 20.0 \\
Baseline & .037  & 14.3 \\
Dirac delta for both & .049  & 17.3 \\
\bottomrule
\end{tabular}
% }
\end{table}

\textbf{Training time consumption. }
EMPP is a self-supervised approach that generates new data indirectly, which can lead to increased training resource consumption. We have compared the computational time required for calculating the EMPP loss with other methods in Table \ref{tab:traintime}. The findings indicate that the time taken to compute EMPP is similar to that of calculating losses for property prediction or denoising tasks. Consequently, employing n-mask EMPP as an auxiliary task is expected to roughly multiply the training time by a factor of n. Based on the experimental results presented in Tables \ref{tab:QM9withoutPT} and \ref{tab:md17_result}, we find that the 1-mask strategy is generally sufficient for most scenarios: it doubles the training time but also yields substantial performance enhancements. When computational resources and time are not constraints, we suggest opting for a higher n in the mask strategy for even better outcomes.

\begin{table}[htbp]
\centering
\caption{Ablation study on training time consumption.}\label{tab:traintime}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
Method            & Samples per second  & The cost of each iterations (ms) \\
\midrule
Property prediction & 291.57  & 439 \\
Denoising & 290.25  & 441 \\
EMPP & 281.94  & 454 \\
\bottomrule
\end{tabular}
% }
\end{table}


\textbf{Masking multiple atoms in a molecule.} EPMM is based on a crucial theoretical premise: when a single atom is masked, its position is well-posed in most cases, but when two atoms are masked, the position prediction becomes ill-posed. We empirically validate this assertion through experiments. We construct a variant in which multiple n atoms are masked in each molecule, and the remaining atoms' embeddings are used to predict the positions of all atoms simultaneously. From Table \ref{tab:maskingmoreatom}, We found that when multiple atoms were masked, the performance actually declined, and only when a single atom was masked did EMPP surpass the baseline. This experiment confirmed our concerns regarding the ill-posed nature of masking multiple atoms. Furthermore, in practical applications, we recommend the approach of repeatedly masking a single atom, which is decribed in \eqref{equ:totalloss}.

\begin{table}[htbp]
\centering
\caption{Ablation study on hyper-parameters in label distribution. }\label{tab:labeldistribution_hp}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
Method            & $\alpha$ & $\varepsilon_{HOMO}$ \\
\midrule
Without self-supervision & .046  & 15.4 \\
Baseline ($\tau=0.1/\sigma=0.5$) & .041  & 14.2 \\
\midrule
($\tau=0.1/\sigma=0.3$) & .041  & 14.2 \\
($\tau=0.1/\sigma=0.1$) & .041  & 14.3 \\
($\tau=0.1/\sigma=0.05$) & .042  & 14.2 \\
($\tau=0.1/\sigma=0.01$) & .042  & 14.3 \\
\midrule
($\tau=0.5/\sigma=0.5$) & .041  & 14.2 \\
($\tau=0.3/\sigma=0.5$) & .041  & 14.1 \\
($\tau=0.05/\sigma=0.5$) & .042  & 14.3 \\
($\tau=0.01/\sigma=0.5$) & .042  & 14.4 \\
\bottomrule
\end{tabular}
% }
\end{table}

\textbf{}

\textbf{Operations on Sphere.} In Section \ref{sec:pp}, we mentioned that operations on the sphere do not affect equivariance. In order to map the information of all channels to 1D, we employed a shared fully connected layer. Previous methods have used aggregation $\log(\sum_{channel} \exp(\cdot))$ for mapping to 1D. It can be observed from the Table \ref{tab:sphericalopseration} that the fully connected layer introduces more parameters, but it also enhances performance. Moreover, parameter-free methods may encounter numerical overflow issues during the training process.
\begin{table}[htbp]
\centering
\caption{The difference between prediction based on neighbor structure and global molecular graph.}\label{tab:difference}
% \begin{tabular*}{\linewidth}{@{}L|LLLLLLLLLLLL@{}}
\begin{tabular}{l|cccc}
\toprule
Task      & $\alpha$ & $\varepsilon_{HOMO}$ & $\varepsilon_{LUMO}$  \\
Units     & bohr$^{3}$                & meV                  & meV                 \\ \midrule
EMPP (Neighbor)  & .041    & 14 & 13     \\ 
EMPP (Global)   & .045   & 118 & 16     \\ 
EMPP (Global + Dist Weights)   & .044   & 15 & 14     \\ 
\bottomrule
\end{tabular}
\end{table}

\textbf{The impact of hyper-parameters in label distribution.} $\tau$ in \eqref{equ:direction} and $\sigma$ in \eqref{equ:gaussian_radius} are two important hyperparameters, and we conducted ablation studies to evaluate them. This ablation study was not based on pre-training and used Equiformer to train on QM9 with EMPP as an auxiliary task. From Table \ref{tab:labeldistribution_hp}, we found that when we changed these two hyperparameters, there was no significant change in performance, which reflects that EMPP is not sensitive to the distribution of labels.


\begin{table}[htbp]
\centering
\caption{Results on GEOM-Drug property prediction without pre-trainging.}\label{tab:GEOM-Drug}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
Method            & Energy MAE (kcal/mol) \\
\midrule
Equiformer & 0.07517  \\
Equiformer+EMPP & 0.03912  \\
\bottomrule
\end{tabular}
% }
\end{table}


\vspace{-0.4em}
\textbf{Prediction with all the unmasked atoms.} In EMPP, we use the embeddings of neighboring nodes to predict the position of the masked atom. However, in reality, all atoms interact with the masked atom, even if the interactions are minimal. Here, we have designed two variants of EMPP. The first variant uses all atoms to predict the position (``Global''), and the second variant uses all atoms to predict and adds distance-related weights (``Global + Dist Weights''). In other words, the second method weights the loss generated by the first variant, with greater weight given to atoms that are closer to the masked atom. The weighting function is $w_{k}=\mathrm{max}(0, \frac{|\vec{\mathbf{r}}_{ik}| - C}{C})$, where $C$ is a hyper-parameter set to $8$ \AA.

\vspace{-0.4em}
From Table \ref{tab:difference}, we observe that utilizing all unmasked atoms to predict positions diminishes the model's generalization capability. We believe this may be due to inaccurate long-range interactions affecting the model's learning. Although incorporating distance weights improves the global prediction effect, it still fails to match the performance of predicting based on neighboring atoms.

\textbf{The impact of molecules of different sizes on EMPP.} EMPP improves its generalization performance by masking atomic positions and then restoring them. Intuitively, as the number of atoms in a molecule increases, more atoms can be masked, allowing EMPP to generate more diverse data. To evaluate the impact of EMPP on molecules with different sizes, we conducted experiments by categorizing the QM9 training data into three groups based on the number of atoms: (0-16), (17-19), and (20+). These categories contain roughly equal amounts of data. In each experiment, we computed the EMPP loss using only molecules from one of these categories. As shown in Table \ref{tab:multisize}, the results demonstrate that EMPP consistently improves performance across different molecular sizes, with larger molecules experiencing more significant performance gains.

\begin{table}[htbp]
\centering
\caption{Ablation study on the impact of molecules of different sizes.}\label{tab:multisize}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
Method            & $\alpha$ & $\varepsilon_{HOMO}$ \\
\midrule
Baseline & .041  & 14.2 \\
EMPP on (0-16) & .044  & 14.9 \\
EMPP on (17-19) & .043  & 14.4 \\
EMPP on (20+) & .043  & 14.5 \\
\bottomrule
\end{tabular}
% }
\end{table}






\end{document}
