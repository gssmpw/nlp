
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% MY IMPORTS
\usepackage{graphicx}
\usepackage{tabularray}
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{subfig}
\usepackage{caption}
\usepackage[most]{tcolorbox}
\usepackage{thmtools}
\usepackage{fontawesome5}
\usepackage{algorithm,algorithmic}
\usepackage{listings}
\usepackage{booktabs,siunitx}

% MY DEFINITIONS
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\theoremstyle{definition} % Non-italicized text
\newtheorem{example}{Example}



\DeclareRobustCommand*\graycircled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt, thick, color=gray] (char) {\small{\texttt{#1}}};}}


\definecolor{mygreen}{HTML}{79BF41}
\definecolor{myblue}{HTML}{4DBCC9}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                                 
    numbersep=1pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1
}
\lstset{style=mystyle}

\tcbset {
  base/.style={
    arc=0mm, 
    bottomtitle=0.5mm,
    boxrule=0mm,
    colbacktitle=black!10!white, 
    coltitle=black, 
    fonttitle=\bfseries, 
    left=1mm,
    leftrule=1mm,
    right=1mm,
    top=1mm,
    bottom=1mm,
    title={#1},
    toptitle=0.75mm, 
    width=\textwidth
  }
}


\newtcolorbox{greybox}[1]{
  colframe=black!15!white,
  base={#1},
  breakable
}

\newtcolorbox{promptbox}[1]{
  colframe=black!15!white,
  base={#1},
  leftrule=0mm,
  breakable,
}

\newtcolorbox{bluebox}[1]{
  colframe=myblue!50!white,
  colback=myblue!15!white,
  base={#1},
  breakable
}


\newtcolorbox{greenbox}[1]{
  colframe=mygreen!50!white,
  colback=mygreen!15!white,
  base={#1},
  breakable
}

\newcommand*\circled[1]{\textcircled{\raisebox{-0.9pt}{#1}}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\gcmark}{\textcolor{lightgray}{\ding{51}}}%
\newcommand{\gxmark}{\textcolor{lightgray}{\ding{55}}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}




\title{Active Task Disambiguation with LLMs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy

\author{Katarzyna Kobalczyk\thanks{Equal contribution}, NicolÃ¡s Astorga$^*$, Tennison Liu, \& Mihaela van der Schaar \\
DAMTP, University of Cambridge \\
\texttt{\{knk25, nja46\}@cam.ac.uk} \\
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle




\begin{abstract}
Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems--frequent in real-world interactions--remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions. 
\end{abstract}

\maketitle


\section{Introduction}

\begin{wrapfigure}{r}{0.45\linewidth}
    \vspace{-2.5em}
    \includegraphics[width=\linewidth]{figures/example.pdf}
    \vspace{-1.5em}
    \caption{An ambiguous problem statement, a sample of LLM-generated compatible solutions, and clarifying questions. $\blacktriangleright$\textbf{The goal:} Generate the most informative question.}
    \vspace{-1.2em}
    \label{fig:example}
\end{wrapfigure}

Recent advances in the field of LLMs have led to the development of problem-solving agents capable of addressing complex tasks that extend far beyond conventional structured data problems such as regression and classification. State-of-the-art LLMs have demonstrated remarkable success in logical reasoning \citep{creswell_selection-inference_2022, lei_boosting_2023}, mathematical problem solving \citep{romera-paredes_mathematical_2024, imani_mathprompter_2023}, code generation \citep{liu_is_2024, zhang_planning_2023} or creative writing \citep{coenen_wordcraft_2021, chakrabarty_creativity_2023}. While existing research predominantly focuses on enhancing LLMs' planning and reasoning capabilities with new prompting strategies like Chain of Thought (CoT) \citep{wei_chain--thought_2022} or self-consistency \citep{wang_self-consistency_2023}, evaluation benchmarks typically assume complete and unambiguous problem statements.  However, due to the inherent ambiguity of natural language \citep{stengel-eskin_why_2023, liu_were_2023} or deliberate underspecification, tasks encountered in real-world usage of LLMs may often not be well-defined, increasing the risk of the agent misinterpreting the true intentions of the problem setter.

The concurrent line of work suggests that in the presence of ambiguously specified tasks, agents should be able to infer missing information to discern the intended behavior \citep{tamkin_task_2023}. However, in the context of human-specified tasks, when user intentions deviate from that of the average population on which the internal LLM preference model has been trained, the agent is at risk of generating outputs that do not align with the user's true needs. Such behaviors may be especially harmful in safety-critical applications, such as medical diagnosis or treatment decisions, where erroneous answers pose significant risks. 

Similar to prior works \citep{kuhn_clam_2022, li_eliciting_2023}, we advocate for LLM agents to engage in an interactive dialogue when faced with ambiguously defined problems. By asking clarifying questions, agents can narrow down viable solutions, reducing the risk of generating unsatisfactory responses. \citet{li_eliciting_2023} have demonstrated that such interactive task elicitation can be more informative and require less effort than refining the original prompts, often uncovering novel, not initially anticipated aspects of the given problem. However, asking a good clarifying question requires LLM agents to reason about their own generative distribution and identify features that differentiate the viable solutions. We hypothesize that such meta-cognitive skills \citep{kuhn_clam_2022, barzilai_epistemic_2016} are beyond the capabilities of many modern LLMs. In response, this paper investigates whether out-of-the-box abilities of LLMs in asking clarifying questions can be improved.

While traditional machine learning has extensively explored efficient data collection under model uncertainty, resulting in the development of active learning strategies \citep{houlsby_bayesian_2011, mackay_bayesian_1992}, LLM-based agents do not operate on well-defined and structured input-output spaces. Instead, LLMs navigate complex tasks with natural language, necessitating novel approaches to efficient information acquisition.  Drawing from the principles of Bayesian Experimental Design (BED), we propose a method of active task disambiguation maximizing questions' utility by direct estimation of their information gain. Effectively, the proposed method shifts the load from implicit reasoning about the best question to explicit reasoning via sampling from the solution space.

\textbf{Contributions:} $\blacktriangleright$ We identify the need for new reasoning methods enabling LLM agents to handle ambiguously specified tasks effectively. $\blacktriangleright$ Section~\ref{sec:formalism}: We introduce a formal definition of task ambiguity in natural language problem specifications, enabling us to frame the problem of effective task disambiguation through the lens of BED. $\blacktriangleright$ Section~\ref{sec:method}: We propose and motivate theoretically a BED-based strategy for LLM agents to generate clarifying questions. $\blacktriangleright$ Section~\ref{sec:experiments}: We evaluate the effectiveness of competing question-generating strategies on an illustrative game of 20 questions and a real-world application to code generation. Our results demonstrate that the BED-based method improves upon baseline strategies relying on implicit reasoning about the best question to ask.


\section{Formalism \& Background}\label{sec:formalism}

We let $\Sigma$ denote the space of natural language. We define a problem statement $\gS \in \Sigma$ as a natural language instruction for an agent to generate a solution $h \in \Sigma$ belonging to the unknown set of ground-truth solutions $\gH^* \subset \Sigma$. We assume that the problem statement, $\gS$, can be decomposed into two parts: a set of requirements $\gR$ that any $h \in \gH^*$ should satisfy, and any additional contextual information $\gC$ that may influence the preference towards different outputs, $h \in \Sigma$.

\begin{example}[Code generation]
    Consider the problem of code generation with an LLM agent based on a prompt $\gS = (\gR, \gC)$. Here, $\gC$ is the natural language instruction guiding the LLM to output a code solution $h$. The requirements for the generated code, described in $\gR$, include the expected functionality of the code and any additional test cases that the generated solutions should pass. The ground truth $\gH^*$ contains all programs $h$ that, e.g. pass all hidden test cases or satisfy the internal needs of the human user. A concrete example is presented in Figure~\ref{fig:example}.
\end{example}
%An example of an ambiguous problem statement is: Write a function to search for an item in a list., without specifying whether to return an index, a boolean, or how to handle multiple occurrences or missing items.
%\vspace{-0.3cm}
%\begin{bluebox}{}
 %An example of an ambiguous problem statement is: Write a function to search for an item in a list., without specifying whether to return an index, a boolean, or how to handle multiple occurrences or missing items.
%\end{bluebox}



Let $p^*(\cdot \vert \gS)$ denote the likelihood function that determines the preferences of the problem setter  over candidate solutions $h \in \Sigma$. We assume that for any $\gS$, $p^*(\cdot \vert \gS)$ may be decomposed as: 
\begin{equation}
p^*(h \vert \gS) = \mathbbm{1}\{h \vdash \gR\}\tilde{p}^*(h \vert \gR, \gC), \qquad \forall h \in \Sigma.
\end{equation}
In the above, $ \mathbbm{1}\{h \vdash \gR\}$ is an indicator function assessing if a solution $h$ satisfies the requirements $\gR$ or not. This indicator function is objective in the sense that it represents an unquestionable truth about a sample solution $h$. In contrast, $\tilde{p}^*(\cdot \vert \gR, \gC)$, is context dependent and subjective. Given the same contextual information, different problem setters (e.g., different human users) may have varying preferences over competing solutions that satisfy the given set of requirements.

In this paper, we will focus on the problem of task underspecification; a situation where the support of $p^*(\cdot \vert \gS)$ extends beyond the set of acceptable solutions.

\begin{definition}[Task ambiguity]
    Let $\gS = (\gR, \gC)$ and $\gH := \{h : h \vdash \gR\}$. We say that $\gS$ is ambiguous if $\gH$ is a proper superset of $\gH^*$, i.e. $\gH \supset \gH^*$.
\end{definition}

When an LLM agent attempts to solve a given task specified by $\gS$, it generates a solution $h$ according to its own generative distribution $p_{\phi_h}(\cdot \vert \gS)$. If $\gS$ is ambiguous, providing a correct solution becomes challenging due to the risk of misinterpreting the true intention of the problem setter. The risk of misinterpretation is high if the distributions $p^*$ and $p_{\phi_h}$ are not aligned. In order to solve this problem, one could consider at least two alternative approaches:

\begin{figure}
    \centering
    \vspace{-4em}
    \subfloat[][\textit{Preference optimization}. The distribution $p_{\phi_h}$ is aligned with the average preferences of the population $p^*_{\mathrm{avg}}$. This strategy is successful, if the set of acceptable solutions $\gH^*$ is aligned with the mode of $p_{\phi_h}$ (left). If not, $p_{\phi_h}$ may fail to generate correct answers (right).
    ]{\includegraphics[height=3.7cm]{figures/preference_learning.pdf}\label{fig:preference}}
    \hspace{0.5em}
    \subfloat[][\textit{Iterative task elicitation}. By acquiring new task specifications, the set of admissible solutions is successively reduced, surrounding $\gH^*$.]{
    \raisebox{0.1em}{
    \hspace{0.2em}
    \includegraphics[height=3.5cm]{figures/requirements.pdf}\label{fig:requirements}
    }}
    \vspace{-0.5em}
    \caption{\textit{Resolving ambiguity}. }
    \vspace{-2em}
    \label{fig:resolving-ambiguity}
\end{figure}

\textbf{Preference optimization.} The first school of thought postulates that agents should be able to fill in the blanks by combining information from instructions and their previous experience in order to identify the intended behavior \citep{tamkin_task_2023}. In particular, the goal of reinforcement learning with human feedback (RLHF) \citep{bai_training_2022, ouyang_training_2022} or direct preference optimization (DPO) \citep{rafailov_direct_2023} is to align the generative distribution $p_{\phi_h}(\cdot \vert \gS)$ with preferences of the overall population such that with a high likelihood, $p_{\phi_h}(\cdot \vert \gS)$ concentrates around $\gH^*$ (Fig.~\ref{fig:preference}, left). However, if the intentions of an individual user deviate from that of the average population (Fig.~\ref{fig:preference}, right), the agent is at risk of proposing solutions not belonging to $\gH^*$. Such behaviors may be especially harmful if there is an associated risk with generating a sample answer $h \notin \gH^*$. 

\textbf{Iterative task elicitation.} The second approach involves engaging in an interactive dialogue with the user, seeking additional specifications \citep{kuhn_clam_2022, li_eliciting_2023}. Since $\gH \supset \gH^*$, some requirements of the problem must have not been explicitly stated in $\gS$ and remain unknown to the LLM agent. Let $\gS^t = (\gR^t, \gC^t)$ denote the task specification at time $t$. During each round of the conversation, the agent can ask a clarifying question, $q^t$, and after receiving the oracle answer $a^t$, the problem statement gets updated to $\gS^{t+1} = \gS^{t} \cup (q^t, a^t)$. At each interaction step, the set of solutions compatible with $\gS^t$, $\gH^t := \{h : h \vdash \gR^t\}$, becomes smaller and successively concentrates around the true solution set $\gH^*$, mitigating the risk of failure when sampling $h \sim p_{\phi_h}( \cdot \vert \gS^t)$. The goal of this approach is to ensure personalized alignment without the need for model fine-tuning.


\textbf{The goal.} This paper focuses on the latter option of interactive task elicitation. Given that obtaining the oracle answers maybe be costly\footnote{We work under the convention of BED \citep{rainforth_modern_2023}, wherein the cost of selecting an experiment that maximizes the information gain is negligible in comparison to the cost of obtaining the oracle answer.}
, our goal is to find the best strategy for generating clarifying questions so that $p_{\phi_h}$ concentrates around $\gH^*$ with minimal interaction cost. 

\begin{bluebox}{}
\faLightbulb[regular] \hspace{0.3em} We make a distinction between model uncertainty and the ambiguity of a problem. While model uncertainty is measured through the heterogeneity of $p_{\phi_h}$, ambiguity of a task is defined through the objective indicator function $\mathbbm{1}\{h \vdash \mathcal{R}\}$ whose true value is independent of the LLM or the problem setter. We note that in many cases, the support of the agent's generative distribution $p_{\phi_h}(\cdot \vert \gS)$ may cover a larger space than $\gH^*$, even if the problem statement $\gS$ is not ambiguous. This is due to the persistent issue of LLM hallucinations \citep{zhang_sirens_2023, rawte_survey_2023} or the complexity of the problem exceeding the LLM's abilities. Thus, a high uncertainty of the LLM agent does not imply inherent ambiguity of $\gS$. While the focus of this paper lies in addressing objectively ambiguous problems, as we will see in experiment \ref{sec:exp-code-gen}, iterative task elicitation may also improve LLM-generated solutions even for unambiguous problems. 
\end{bluebox}




\subsection{What makes a question informative?}\label{sec:what-makes-q}

To understand what makes a question informative, we will rely on the principles of Bayesian Experimental Design (BED). BED aims to design optimal experiments by maximizing the amount of information about an unknown quantity of interest gained from an outcome of an experiment. In the context of problem solving with LLM agents, an experiment corresponds to a question $q$ posed by the LLM agent, it's outcome is the oracle answer, $a$, and the unknown quantity of interest, is the generated solution, $h$. Therefore, the information gain (IG) from obtaining the additional task specification as a pair $(q, a)$, given a problem statement, $\gS$, can be formalized as:
\begin{equation}
    \text{IG}(q, a) := \mathbb{H}[p^*(h \vert \gS)] - \mathbb{H}[p^*(h \vert \gS \cup (q, a))],
\end{equation}
where $\mathbb{H}$ is the Shannon entropy. However, because IG strictly depends on the answer $a$, which is unknown at the point of asking the question $q$, we need to consider the expected information gain--a~quantity which arises by taking the expectation over all possible answers, given the query $q$:
\begin{align}
    \text{EIG}(q) &:= \mathbb{E}_{p^*(a \vert q, \gS)}\left[ \text{IG}(q, a)\right] = \mathbb{H}[p^*(h \vert \gS)] - \mathbb{E}_{p^*(a \vert q, \gS)}\mathbb{H}\left[p^*(h \vert \gS \cup (q, a) \right] \label{eq:eig2}
\end{align}
Noting that the first term of the EIG in (\ref{eq:eig2}) does not depend on $q$, we focus on the second term:
\begin{align}\label{eq:the-utility}
    -\mathbb{E}_{p^*(a \vert q, \gS)}[\mathbb{H}\left[p^*(h \vert \gS \cup (q, a)) \right]]. 
\end{align}
In the above, $p^*(a \vert q, \gS)$ denotes the distribution of oracle answers to a question $q$, given $\gS$. We will assume that the law of total probability applies to  $p^*$ so that $p^*(a \vert q, \gS) = \sum_{h \in \gH}p^*(a \vert q, h)p^*(h \vert \gS)$. We will also assume that $p^*$ is an oracle that given a sample solution $h$, can always answer a question $q$, about $h$, truthfully, so that  $p^*(a \vert q, h) > 0 \Rightarrow h \vdash (a, q)$. Under this assumption, all plausible answers $a$ to a question $q$ must be semantically equivalent. Thus, without loss of generality, we can assume that for each question, $q$, there exists only one answer, $a$, describing $h$, for which $p^*(a \vert q, h) = 1$. Therefore, each question $q$, generates a partitioning of $\mathcal{H}$ into non-overlapping sets of solutions $\mathcal{H}_{(q, a)}$ compatible with $\gS \cup (q, a)$. Let $\gA_q$ denote the space of all unique answers to a question $q$ given the current problem statement $\gS$ and let $\gH_{(a, q)} := \{h \in \gH : p^*(a \vert q, h) = 1\}$, then we must have that: 
$$\gH = \bigcup_{a \in \gA_q}\gH_{(q, a)} \quad \text{and} \quad \gH_{(q, a)} \cap \gH_{(q, a')} = \varnothing \quad \forall a, a' \in \gA_q \quad \text{s.t.} \quad a \neq a'.$$
In this view, the utility of each question can be seen through the partitioning of $\gH$ that it generates and the compound likelihood of solutions belonging to each of the partitions $\gH_{(q, a)}$, $a \in \gA_q$.\footnote{In practice, when $p^*$ is represented through an internal likelihood function of a human, the two assumptions on the law of total probability and truthfulness of $p^*$ may be violated; humans may answer questions incorrectly or answer ``I don't know'', giving no information about $\gH^*$. We see such events as unpredictable random noise that we decide not to model for simplicity. Alternative approaches may penalize questions $q$ for which the expected uncertainty of $p^*(a \vert q, h)$ is high.}


\textbf{Requirement querying.} Note that the ground-truth information gain depends on the unknown distribution $p^*$. Ideally, we would like the generative distribution of the LLM, $p_{\phi_h}( \cdot \vert \gS)$, to closely approximate $p^*( \cdot \vert \gS)$ for any problem statement, $\gS$. This is, however, not guaranteed. The bias of $p_{\phi_h}$ may not adequately represent the preferences of the problem setter encoded in $p^*$. However, if, both $p^*$ and $p_{\phi_h}$ can be decomposed as a product of the objective function $\mathbbm{1}\{h \vdash \gR\}$ with their subjective counterparts, then $\mathbbm{1}\{h \vdash \gR\}$ provides a source of common grounding between the two distributions. Our strategy of active task disambiguation will therefore solely focus on requirement querying.  With that in mind, from now on we will assume that each question-answer pair, $(q, a)$, yields a new requirement, extending the current set of requirements and progressively reducing the set of compatible solutions (c.f. Fig. \ref{fig:requirements}). 

\textbf{Uniformity of the unknown.} As $p^*$ is unknown and $p_{\phi_h}$ may be inadequately biased, our strategy of active task disambiguation will be agnostic to the biases of both $p^*$ and $p_{\phi_h}$. We will therefore approximate $p^*$ with a uniform distribution over the set of all solutions compatible with the given set of requirements. Under this assumption, we have that 
\begin{align}
    \mathbb{E}_{p^*(a \vert q, \gS)}\left[\mathbb{H}\left[p^*(h \vert \gS \cup (q, a))\right]\right] &= \sum_{a \in \gA_q} \mathbb{H}\left[p^*(h \vert \gS \cup (q, a))\right] \sum_{h' \in \gH} p^*(a \vert q, h')p^*(h' \vert \gS) \\
    &= \sum_{a \in \gA_q} \mathbb{H}\left[p^*(h \vert \gS \cup (q, a))\right] \sum_{h' \in \gH_{(q, a)}} \frac{1}{|\gH|}
\end{align}
Given that $p^*(\cdot \vert \gS)$ is uniform on $\gH$ we have that $p^*(h \vert \gS \cup (q, a))$ is uniform on $\gH_{(q, a)}$, thus $ \mathbb{H}\left[p^*(h \vert \gS \cup (q, a))\right] = \log(|\gH_{(q, a)}|)$, and so
\begin{align}
     \mathbb{E}_{p^*(a \vert q, \gS)}\left[\mathbb{H}\left[p^*(h \vert \gS \cup (q, a))\right]\right] &= \frac{1}{|\gH|}\sum_{a \in \gA_q}|\gH_{(q, a)}|\log(|\gH_{(q, a)}|) \label{eq:the-utility-uniform}
\end{align}
We note that the above is minimized when $\gH_{(a, q)}$ are of equal size.

\begin{corollary}\label{prop:partition}
    Let $\gH$ be the set of solutions compatible with the requirements $\gR$ within the problem statement $\gS$. Suppose that $p^*(\cdot \vert \gS)$ is uniform on $\gH$. Let $\gQ_n$ be the set of all questions with exactly $n$ possible answers such that for any $q \in \gQ_n$, there exists a finite set of possible answers $\gA_q$ with $|\gA_q| = n$ and that each question-answer pair, $(q, a)$, induces a new requirement. Then, the EIG is maximized for a question $q^*$ whose possible answers in $\gA_{q^*}$ partition $\gH$ into equally sized subsets.
\end{corollary}

\begin{bluebox}{}
\faLightbulb[regular] \hspace{0.3em}  If we only consider binary questions with $|\gA_q| = 2$ (e.g. yes-or-no type of questions), then the question with the highest information gain is the one which partitions the set of all possible solutions $\gH$ into two subsets of equal size. The second pane on the right of Figure~\ref{fig:figure_1} shows how at iteration $t$, three candidate questions may split the space of compatible solutions into subsets; the selected question $q^*$ is one which results in the most even partitioning.
\end{bluebox}


\begin{bluebox}{}
\faLightbulb[regular]  \hspace{0.3em}  From (\ref{eq:the-utility-uniform}) it follows that $\underset{q \in \gQ_n}{\max} \ \mathrm{EIG}(q) \propto \log(\frac{|\gH|}{n})$. This implies that a question with more possible answers can result in larger information gain. However, this requires finding a categorization of the possible solutions that partition them into balanced subsets. In the extreme case, if $|\gH| = K$, the question with the highest information gain is the one whose answer always points to exactly one of the solutions $h \in \gH$. Perhaps, the only way to ask such questions is to enumerate all solutions in $\gH$ and ask: ``\textit{Is $h^*$ is supposed to be $h_1, \ldots h_{K-1}$ or $h_{K}$}''? Arguably, this is neither a natural question to ask nor a user-friendly one, as it requires the user to examine each of the $K$ possible solutions. We argue that questions with a small set of possible answers, yet ones that induce a balanced partitioning of the solution space strike a good balance between information gain and the mental load from the user. 
\end{bluebox}




\section{The method: Active Task Disambiguation}\label{sec:method}

\begin{figure}[t]
    \centering
    \vspace{-3em}
    \includegraphics[width=\linewidth]{figures/figure_1.pdf}
    \vspace{-1em}
    \caption{\textit{Active task disambiguation.}   \graycircled{1} Starting from $t=0$, 
    the problem statement $\gS^t$ is presented to the problem-solving agent. \graycircled{2} The agent reasons about the problem to infer the set of solutions $\gH^t$ compatible with the requirements of $\gS^t$. In order to approximate $\gH^t$, a set of candidate solutions $\{h_i^t\}$ is sampled.  \graycircled{3} To discern between different solution variants, the agent generates candidate questions $\{q_j^t\}$.  \graycircled{4}  A question $q^*$ with the highest utility is selected and presented to the oracle.  \graycircled{5}  Based on the oracle answer, $a^*$, the problem statement is extended by the new specification defined through $(q^*, a^*)$; the process can be repeated with the updated problem statement $\gS^{t+1} = \gS^{t} \cup (q^*, a^*)$ resulting in a reduced space of compatible solutions, $\gH^{t+1} \subset \gH^t$.}
    \vspace{-1em}
    \label{fig:figure_1}
\end{figure}


The key point of the previous section is that given the unknown bias of $p^*$, the question with the largest information gain is the one that splits the space of compatible solutions into equal partitions. Generating such a question requires the LLM agent to reason about the set of possible solutions at each interaction step and identify the features that discern them. Such a reasoning process can be viewed as a form of meta-cognitive ability, requiring the LLM agent to consider the uncertainty within its own generative distribution. While a straightforward solution to asking such clarifying questions may be to simply perform zero-shot prompting of the LLM agent, we hypothesize that the out-of-the-box abilities of LLMs in this form of reasoning are lacking in comparison to their solution-generating abilities. This may be caused by a relatively small number of clarifying questions present in their pre-training corpus. Our proposed method explicitly evaluates the utility of candidate questions via a small sample of self-generated solutions and returns the question that maximizes this utility. Fig.~\ref{fig:figure_1} shows a high-level overview of the workflow. 

Let $\gU: \Sigma \rightarrow \mathbb{R}$ denote a utility function determining the usefulness of a candidate question $q$. Based on the derivations of the previous section we define $\gU$ as:
$$\gU(q) = \mathrm{EIG}(q) - c(q),$$
where $c: \Sigma \rightarrow \mathbb{R}_+$ is a cost function determining the expected cost of obtaining the oracle answer to a candidate question $q$\footnote{In the experimental section, for simplicity, we take $c$ to be constant. In the case of human-written answers, alternatives may include penalties for the question's length or the expected length of the answer.}. Our method of active task disambiguation consists of the following steps. At each iteration $t$, 
\begin{enumerate}[itemsep=0pt, topsep=0pt, left=1em]
    \item Sample a set of $N$ candidate solutions $\{h_i^t\}_{i=1}^N \sim p_{\phi_h}(\cdot \vert \gS^t)$;
    \item Sample a set of $M$ candidate questions $\{q_j^t\}_{j=1}^M \sim p_{\phi_q}(\cdot \vert \gS^t)$;
    \item Obtain pseudo answers $a_{i,j}^t$ for each question $q_j^t$ about every solution $h_i^t$;
    \item Approximate the utility $\gU$ of each question $q_j^t$ based on the answers $\{a_{i,j}^t\}_{i=1}^N$;
    \item Return the question $q^* = \arg\max_{q \in \{q_j^t\}}\mathcal{U}(q)$;
    \item Record the user answer $a^*$ and extend the problem statement to $\gS^{t+1} = \gS^{t} \cup (q^*, a^*)$.
\end{enumerate}

\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-2em}
\begin{minipage}{0.45\textwidth}
\begin{algorithm}[H]
\caption{$\mathtt{estimate\_EIG}(q_j, \{a_{i,j}\}_{i=1}^N)$}\label{alg:EIG-estimation}
\small
\begin{algorithmic}
    \REQUIRE A question $q_j$ and a set of $N$ answers $\{a_{i,j}\}_{i=1}^N$
    \STATE $\{a_1, \ldots, a_n\} \gets$ Unique answers in $\{a_{i,j}\}_{i=1}^N$
    \FOR{$k \in \{1, \ldots, n\}$}
    \STATE $n_k \gets |\{i : a_{i, j} = a_k, i \in [N]\}|$
    \STATE $p_k \gets n_k / n$
    \ENDFOR
    \RETURN $-\sum_{k=1}^np_k\log(p_k)$
\end{algorithmic}
\end{algorithm}   
\end{minipage}
\vspace{-1.5em}
\end{wrapfigure}
In the above, $p_{\phi_q}$ correspond to the question-generating distribution of the problem-solving LLM. Step 3. requires approximating the EIG of the candidate questions $\{q_j^t\}_{j=1}^M$ based on the sample $\{h_i^t\}_{i=1}^N$. This estimate is computed by generating to all questions $q_j^t$ answers $\{a_{i,j}^t\}$ about each sample solution $h_i^t$. Depending on the type of the solutions, these answers may be generated by the problem-solving LLM itself (see experiment \ref{sec:exp-20Q}), or evaluated with an external tool (see experiment \ref{sec:exp-code-gen}). Algorithm~\ref{alg:EIG-estimation} shows the general procedure of estimating the EIG score based on equation (\ref{eq:the-utility-uniform}). 

\begin{bluebox}{}
\faLightbulb[regular] \hspace{0.3em} We note that the estimation of the EIG score, and consequently the selection of questions, directly depends on the sample $\{h_i^t\}_{i=1}^N$. The proposed approach shifts the load of selecting the best question from implicit reasoning about the best question to ask to explicit reasoning in the solution space. The underlying motivation for this strategy is that generating discriminative questions without direct access to the set of compatible solutions is more challenging than solution generation itself. Question selection via direct maximization of the EIG bootstraps the question-generating skills of LLMs using their potentially stronger solution-generating skill. Based on this observation, we make a couple of practical remarks.    
\end{bluebox}


\textbf{Ensuring uniformity.} When estimating the EIG score based on samples from $p_{\phi_h}(\cdot \vert \gS=(\gR, \gC))$ we want to ensure that $p_{\phi_h}$ is close to uniform on the set of $\gR$-compatible solutions. In practice, to encourage diversity of samples we may adopt two strategies. 1) Generate sample solutions with increased sampling temperature. 2) Instruct the LLM to generate a list of  \textit{``diverse and representative''} solutions. We find that adding this statement to the solution-generating prompt improves the diversity of generated samples and the coverage of $\gH$ (see study 2. of experiment \ref{sec:exp-20Q}).

\textbf{Mitigating LLM noise.} The effectiveness of questions selected by maximizing the estimated information gain relies on the agent's ability to accurately evaluate $\mathbbm{1}\{h \vdash \mathcal{R}\}$. For this, we need to ensure that a) the generative distribution of the agent, $p_{\phi_h}(\cdot \vert \gS)$, is error-free, i.e. $p_{\phi_h}(h \vert \gS = (\gR, \gC)) = 0$ for any $h \nvdash \gR$ and b) the pseudo answers $a_{i,j}$ used to estimate the EIG are also error-free, i.e. $h_i \vdash (q_j, a_{i,j})$. While in select cases ensuring the compatibility of solutions with requirements is straightforward (see code generation in experiment \ref{sec:exp-code-gen}), in many problems in which the agent generates solutions without any external grounding, ensuring that points a) and b) are error-free is challenging due to LLM hallucinations \citep{zhang_sirens_2023, rawte_survey_2023}. We observe that in practice, employing more refined prompting strategies, like CoT or self-reflection, works well (see study 3. of experiment \ref{sec:exp-20Q}).

\section{Experiments}\label{sec:experiments}

Our experiments are designed to investigate two hypotheses which result from the discussions contained in the previous sections:
\begin{itemize}[left=1.5em]
    \item[\textbf{H1)}] Implicit reasoning about solutions to generate the most effective clarifying question is a difficult skill for LLMs. This skill can be improved by shifting the reasoning load from the question space to the solution space.
    \item[\textbf{H2)}] The gap between implicit reasoning, i.e. generating questions without explicitly sampling hypothetical solutions, and explicit reasoning through a sample of solutions to select the best question is most significant in cases where:
    \begin{itemize}[left=1.5em]
        \item[\textbf{H2a)}] The LLM can generate representative and diverse samples of solutions, uniformly covering the space of solutions compatible with the given problem statement.
        \item[\textbf{H2b)}] The evaluation noise of the EIG is minimal. This is particularly true in cases when evaluation can be offloaded to an external tool. 
    \end{itemize}
\end{itemize}


Given the above, we will present two kinds of problems: one in which there is no external evaluator guaranteeing that sample solutions adhere to the given requirements and one in which we can ground the evaluation of $\mathbbm{1}\{h \vdash \mathcal{R}\}$ with an external tool. For both experiments, prompts used to generate questions, solutions, and answers are provided in the Appendix (\ref{appdx:20q-prompts} and \ref{appdx:code-gen-prompts}).


\subsection{Yes-or-no questions with the 20 question game}\label{sec:exp-20Q}


The 20 Questions game is a classic guessing game that involves one player (A) thinking of an object, and the other player (B) asking up to 20 yes-or-no questions to guess what it is. The object can be anything, often categorized into an animal, a place, or a person to give the guessers a starting point. The goal for the guessers is to identify the object with as few questions as possible. Despite the 20 questions game being seemingly a toy example, it provides an ideal setup to evaluate the multi-turn questioning abilities of LLM agents. Moreover, it serves as a parallel to many real-world applications, like conversational search, content recommendation, or even medical diagnosis.

\subsubsection{The main experiment}

\textbf{Setup.} To reduce the sampling costs, we play the game for 10 instead of the original 20 rounds. We restrict the game to the category of animals. Here, the set of acceptable solutions, $\gH^*$ are singletons, $\{h^*\}$ where $h^*$ represents a single animal name that player A may think about. Player A is simulated with GPT-4o-mini prompted to answer questions about the ground-truth animal $h^*$. We emphasize that in this setup there is no pre-fixed list of candidate animals that the user or the reasoning agent can choose from. Instead, at each point of the interaction, the guessing LLM (Player B) is free to guess any animal across the entire animal kingdom. For a quantitative analysis of question-generating strategies, we run the game on 15 arbitrary tasks corresponding to 15 animal names (see appdx.~\ref{appdx:20Q-details}). For each task, we run the iterative requirement querying for 10 iterations across 5 seeds. 

\textbf{Question generation.} We consider four alternative methods for generating questions:
\begin{itemize}[left=0.3em, topsep=0pt, itemsep=0.2em]
\item \textit{implicit}: the agent is prompted to generate a single candidate question;
\item \textit{implicit-ToT} \cite{yao_tree_2023}: using the same prompt, we sample $M=5$ questions and prompt the LLM to select the best questions among the set of self-generated candidate questions;
\item \textit{EIG-uniform}: using the same prompt, we sample $M=5$ questions and select the one that maximizes the estimated EIG score. The EIG is estimated assuming uniformity of sample solutions;
\item \textit{EIG-logprobs}: same as above, but the EIG is estimated using the log-probabilities of the sample solutions as returned by the LLM agent. 
\end{itemize}

\textbf{EIG estimation.} To estimate the EIG for the latter two strategies, at each interaction step, we prompt the LLM agent to generate a list of $N=20$ animals that adhere to the current set of requirements $\gR^t$. If the list of requirements $\gR^t$ is long, the LLM may generate animals that do not fulfill all the requirements. To mitigate this, after the initial sampling of the animals, we loop over all requirements $\gR^t$ and use the self-critic answering prompt to check if all requirements $\gR^t$ are satisfied. This extra filtering step is repeated twice (see Study 3. section \ref{sec:exp-20q-studies} for ablation of this step). Animals that do not fulfill the requirements are rejected and sampling is repeated until $N$ animals are obtained. The same self-critic answering prompt is used to generate the pseudo answers $a_{i,j}$ to candidate questions $q_j$ about sample hypothesis $h_i$. The resulting set of answers is used to estimate the EIG according to Algorithm~\ref{alg:EIG-estimation}. Refer to Appendix~\ref{appdx:alg-details}, Algorithm \ref{alg:EIG-estimation-logp} for details on estimating the EIG using the log-probabilities of the sample solutions $\{h_i\}_{i=1}^N$.


\begin{wrapfigure}{r}{0.36\textwidth}
    \vspace{-2em}
    \hspace{-0.5em}
    \includegraphics[width=1.05\linewidth]{figures/20Q_rank.pdf}
    \vspace{-1.5em}
    \caption{\textit{Comparison of question-generating strategies on the game of 20 questions.} Rankings averaged across 15 ground-truth animals, 5 run seeds, 25 evaluation seeds. See Appendix~\ref{appdx:20q-results} for results with the Llama family models.}
    \label{fig:20q-ranks}
    \vspace{-1.5em}
\end{wrapfigure}
\textbf{Evaluation.} Our metric of success is the likelihood of the problem-solving agent guessing the ground truth animal given the obtained set of requirements, $\gR^t$, at each interaction step. We approximate this likelihood by sampling 10 animals with $p_{\phi_h}(\cdot \vert \gR^t, \gC)$ and then prompting the agent to order the animals from most to least likely. We record the position of $h^*$ within the returned list, counting from the bottom--a score of 0 indicates that $h^*$ has not been sampled and a score of 10 indicates that $h^*$ is at the top of the list. For each task, seed, and iteration, we compute this score with 25 sampling seeds.


\textbf{Results.} As observed from Fig.~\ref{fig:20q-ranks}, the \textit{EIG-uniform} strategy outperforms the remaining strategies by a significant margin. The fact that it outperforms both \textit{implicit} and \textit{implicit-ToT} strategies confirms H1. Shifting the reasoning load from question space to direct reasoning about solutions results in improved efficacy of the selected questions--the elicited requirements lead to a higher likelihood of the LLM guessing the right animal after fewer iterations. We also observe that \textit{EIG-logprobs} significantly underperforms in comparison to  \textit{EIG-uniform}. This proves our claim about the LLM's generative distribution $p_{\phi_h}$ being inadequately biased, favoring solutions that do not necessarily agree with the ground truth solution set $\gH^*$. Finally, we note that the gap in performance between \textit{EIG-uniform} and the two implicit strategies is much lower for GPT-4o-mini, which is considered to be overall a significantly more capable model than GPT-3.5-turbo, especially when more reasoning steps are enforced with the ToT prompt. 

\vspace{-0.5em}
\subsubsection{Additional studies}\label{sec:exp-20q-studies}
\vspace{-0.5em}


The aim of this section is to gain further insights and investigate the impact of the design choices behind the EIG-uniform strategy on the effectiveness of the generated questions.
\begin{wrapfigure}[13]{r}{0.36\textwidth}
    \vspace{-1em}
    \includegraphics[width=\linewidth]{figures/20Q_count_bar.pdf}
    \vspace{-2em}
    \caption{\textit{Number of valid solutions after each iteration}.}
    \label{fig:partitioning}
    \vspace{-1em}
\end{wrapfigure}
In what follows, we present results for the problem-solving agent implemented with the GPT-3.5-turbo model.

\textbf{Study 1: Questions' utility.} While we do not restrict the game to a pre-fixed list of animals, in order to evaluate the partitioning properties of generated questions, we construct a large and diverse list of 500 animals (see Appendix~\ref{appdx:20Q-details}). Figure~\ref{fig:partitioning} shows the number of animals compatible with the requirements at the first six iterations from the large list constructed, instead of the small samples of $N$ animals used during the reasoning process (results limited to the first six rounds due to high costs of responding to all questions about 500 animals). We find that questions generated with the EIG-uniform strategy result in smaller subsets of $\gR^t$-compatible solutions, and thus more effective disambiguation. This also shows that the estimation of EIG based on just $N=20$ samples during question elicitation is a good proxy for the ground truth EIG, here approximated with the large list of 500 animals.



\textbf{Study 2: Diversity of solutions.} Estimation of questions' utility is dependent on the quality of sample solutions, which need to be sufficiently diverse to approximate $\gH^t$ well. To ensure good coverage, the prompt for animal guessing includes the statement: \textit{Generate a carefully selected, diverse, and representative set of animals.}. The EIG score used in our experiments is equivalent to the conditional mutual information (MI) between the random solutions $h \sim p_{\phi_h}(\cdot | \mathcal{S})$ and random answers $a \sim p_{\phi}(\cdot | q) := \sum_{h \in \mathcal{H}}p_{\phi_a}(\cdot | q, h)p_{\phi_h}(h | \mathcal{S})$, for a fixed question $q$. In order to test the impact
\begin{wraptable}{r}{0.29\textwidth}
%\vspace{-0.8em}
\caption{\textit{Diversity of samples}}
\vspace{-1em}
\centering 
\label{tab:diversity}
\setlength{\tabcolsep}{5pt}
\vspace{0.5em}
\small
\begin{tabular}{c|cc}
\toprule
$N$ & diverse & vanilla \\
\midrule
10	& \textbf{0.57} \scriptsize{(0.01)}	& 0.46 \scriptsize{(0.01)} \\
20	& \textbf{0.58} \scriptsize{(0.01)}	& 0.54 \scriptsize{(0.01)} \\
30	& \textbf{0.60} \scriptsize{(0.01)}	& 0.58 \scriptsize{(0.01)} \\
\bottomrule
\end{tabular}
%\hspace{0.5em}
\vspace{-0.5em}
\end{wraptable}
of the diversifying statement, we compute this score across a range of 330 diverse questions. From the main experimental results, we collect the unique set of 330 questions asked until the sixth iteration. We then run the game until the first six iterations sampling animals with and without the diversifying statement. We also vary the number of samples $N$. After obtaining the sets $\{h_i^t\}_{i=1}^N$ for all $t \in [1, \ldots, 6]$, we estimate the score for each of the 330 questions based on the LLM-generated answers. Assuming that the collection of 330 questions is diverse, the average of the conditional MI score will be maximized for $p_{\phi_h}$ close to uniform. Thus, a higher MI score implies greater diversity of the generated sample solutions. Table~\ref{tab:diversity} confirms that the inclusion of the diversifying statement leads to more diverse samples of animals, validating H2a.  

\begin{wrapfigure}{r}{0.44\textwidth}
    \vspace{-1.4em}
    \includegraphics[width=\linewidth]{figures/filtering.pdf}
    \vspace{-1.6em}
    \caption{\textit{The impact of LLM noise.}}
    \vspace{-1.5em}
    \label{fig:Study-3}
\end{wrapfigure}

\textbf{Study 3: Requirement compatibility.} To mitigate the impact of LLM hallucinations on sampling solutions that do not adhere to the given set of requirements our solution generating prompt employs an extra filtering step to ensure that sample solutions $h_i$ indeed adhere to the current set of requirements.  We test the impact of filtering on the accuracy of generated solutions. Figure~\ref{fig:Study-3} shows the average number of solutions rejected with a single filtering step (left) alongside the final rankings for the \textit{EIG-uniform} strategy when 0, 1 or 2 filtering steps are used (right).  As the number of requirements increases, the LLM is more prone to hallucinations. A form of ``self-reflection'' via filtering becomes crucial in ensuring accurate outputs and confirming H2b.


\vspace{-0.5em}
\subsection{Active code generation--requirements as unit tests }\label{sec:exp-code-gen}
\vspace{-0.5em}

\textbf{Setup.} In our second experiment, we demonstrate active task disambiguation with an external tool ensuring a near error-free evaluation of requirement compatibility. In this experiment, the goal of our reasoning agent described in the initial prompt $\gS^0$ is to generate a code solution $h$, based on the requirements $\gR^0$ specified via a user-defined instruction describing the expected functionality of $h$. Following the setup of \citet{chen_codet_2023}, $\gS^0$ contains a code snippet that includes statements such as imports, the function header, and a short comment describing the expected functionality of the generated code. Due to the ambiguous nature of natural language and the fact that at the point of writing the instruction not all edge cases might have been considered, $\gS^0$ is likely to be ambiguous (see Appendix~\ref{appdx:human-eval-ambiguity} for examples). 

\textbf{Question generation.} We consider two types of clarifying ``questions'':
\begin{itemize}[left=1em, topsep=0pt, itemsep=0pt]
    \item[\textbf{(B)}] A question $q$ is a generated test case in the form of an assertion that the oracle is supposed to either confirm as correct or reject. We call these questions binary, as they only have two kinds of responses: True or False, similarly to the yes-or-no questions from the previous experiment.
    \item[\textbf{(O)}] A question $q$ is a generated input to the desired function. The oracle returns the expected output of the code. We call these questions ``open'', as for one sample input there may exist a nearly unconstrained number of valid outputs, similarly to open-ended questions.
\end{itemize}
We compare questions generated zero-shot against questions selected by first sampling $M=5$ candidate questions and then selecting one that maximizes the EIG, under the assumption of uniformity.

\textbf{Answers and requirements.} Both the ground truth answers $a^*$ and the answers $a_{i,j}$ used for EIG estimation are obtained by executing the ground-truth or a candidate program $h$, respectively, against a question $q$. The resulting question-answer pairs are turned into additional requirements as executable unit tests that each generated solution must pass and appended to $\gR^t$. Generated programs are executed with an external Python interpreter in a sandbox environment. The ``answering'' of questions through an external tool ensures near noiseless estimation of the EIG score.

\textbf{Solution generation.} The solutions $h_i^t \sim p_{\phi_h}(\cdot \vert \gS^t)$ are sampled by prompting the LLM to generate code completions which are then filtered to only those samples that pass the test cases in $\gR^t$. This ensures that all solutions sampled from the LLM conform to the elicited requirements. 

\textbf{Evaluation.} We evaluate all question-generating strategies on the HumanEval benchmark containing simple coding problems \citep{chen_evaluating_2021}, and the more challenging APPS \citep{hendrycks_measuring_2021} benchmark with competition-level coding challenges\footnote{We filtered the tasks of both benchmarks to a subset of 48 non-trivial tasks, i.e. tasks that do not achieve a near 100\% zero-shot accuracy when sampling from $h \sim p_{\phi_h}(\cdot \vert \gS^0)$ with GPT-3.5-turbo or GPT-4o-mini.}, limiting the number of total questions asked to 4. After obtaining for each iteration $t$ a set of test cases $\gR^t$, we evaluate the discriminative power of the obtained requirements, by sampling 20 solutions from $p_{\phi_h}(h \vert \gS^t)$ and calculating the percentage of code solutions that pass the hidden test cases provided in the benchmark dataset.


% \begin{wrapfigure}{r}{0.4\linewidth}
%     \vspace{-2.6em}
%     \includegraphics[width=\linewidth]{figures/code-results.pdf}
%     \vspace{-1.7em}
%     \caption{\textit{Accuracy of generated code solutions after eliciting 4 additional requirements.} Results averaged across 48 tasks, 3 run and 3 evaluation seeds.}
%     \label{fig:human-eval}
%     \vspace{-1.4em}
% \end{wrapfigure}

\begin{figure}
    \centering
    \vspace{-0.6em}
    \includegraphics[width=0.9\linewidth]{figures/combined-code-results.pdf}
    \vspace{-0.8em}
    \caption{\textit{Accuracy of generated code solutions after eliciting 4 additional requirements.} Results averaged across 48 tasks, 3 run and 3 evaluation seeds.}
    \vspace{-1.5em}
    \label{fig:coding}
\end{figure}


\textbf{Results.} Figure~\ref{fig:coding} shows the accuracy of the generated code samples after eliciting 4 requirements with all strategies. Tables~\ref{tab:human-eval-appdx} and \ref{tab:apps-appdx} in the appendix show the results across all 4 iterations. We observe that EIG-based strategies lead to higher accuracy of the outputs with fewer test cases queried. As expected by the theoretical discussion of section \ref{sec:what-makes-q}, questions with more possible answers (O) can be more informative than questions with only two possible answers (B). However, if these are to be answered by a human user, ``open'' questions arguably require an increased mental load to answer. We also observe that for more capable models, the gap between the baseline questioning strategies and their EIG-equivalent version is smaller. Results are consistent across both benchmarks, indicating that the EIG methods remain effective across varying levels of task ambiguity and difficulty\footnote{The zero-shot accuracy of GPT-4o-mini on APPS is only $\sim$35\% in comparison to the $\sim$70\% achieved on HumanEval confirming the more challenging nature of the APPS benchmark.}.

% This suggests that the growth of the general capabilities of foundational models correlates with improvement in their question-generating skills. 
We also note that the low accuracy of generated samples $h \sim p_{\phi_h}(\cdot \vert \gS^0)$ does not imply that $\gS^0$ itself is inherently ambiguous, as it may simply be caused by the LLM's limitations in following complex instructions. However, even if $\gS^0$ is objectively not ambiguous, the input-output examples appended to the prompt $\gS^t$ may positively bias $p_{\phi_h}$ towards correct solutions, which is a desirable by-product of employing active querying strategies to task disambiguation. 



\section{Discussion}\label{sec:discussion}

\vspace{-0.3em}
\textbf{Limitations.} While our work primarily focuses on efficient requirement elicitation, handling ambiguously specified tasks involves two equally important aspects determining a) that the given problem is ambiguous; b) when a sufficient number of requirements have been collected to stop querying the user. \citet{kuhn_clam_2022} demonstrate that in select instances, ambiguity detection can be effectively resolved with zero-shot prompting. We believe that future research should explore alternative strategies. We also note that the question-generating strategies presented in this work require an increased number of LLM calls compared to the baselines (see Appendix~\ref{appdx:costs}). However, in line with the assumptions commonly made in BED, we take the stance that the computational load required to select the optimal query is negligible compared to the value of acquiring information that reduces problem ambiguity. We anticipate this assumption will become more valid over time as technology advancements lower the costs of LLM token generation, thereby enhancing the importance of efficient information acquisition strategies. 


\vspace{-0.3em}
\textbf{Conclusions and Impact.}  Our findings suggest that clarifying questions generated with zero-shot prompting of LLMs are less efficient than those elicited by direct estimation of their utility with respect to the set of self-generated solutions. This suggests that the current skills of LLMs in generating efficient clarifying questions are underdeveloped, leaving room for improvement. Agents with well-developed meta-cognitive skills should be able to implicitly reason about the best question to ask without relying on multi-stage prompting strategies. We hypothesize that LLMs' deficiency in asking good clarifying questions stems from their limited exposure to such questions in the training corpus. To address this, our proposed framework offers a way to generate synthetic datasets of underspecified problems and their corresponding optimal clarifying questions. These datasets could serve as a resource for supervised fine-tuning, enhancing LLMs' abilities to disambiguate tasks more effectively and improving their interactive real-world problem-solving capabilities.

\clearpage

\section*{Reproducibility} 
The pseudo algorithm to execute active question generation can be found in Appendix~\ref{appdx:alg-details}. The exact format of the prompts used for both experiments is presented in Appendix~\ref{appdx:details}. Results are provided for two closed sourced models: GPT-3.5-turbo, GPT-4o-mini; and two source models: Llama-3-8B (Instruct), and Llama-3-70B (instruct). Code for reproducing the experimental results of section \ref{sec:exp-code-gen} is made available at: \href{https://github.com/kasia-kobalczyk/active-task-disambiguation}{https://github.com/kasia-kobalczyk/active-task-disambiguation}. The repository also includes generated programs and querries with GPT-3.5-turbo and GPT-4o-mini.

\section*{Acknowledgments} 
This work was supported by Azure sponsorship credits granted by Microsoftâs AI for Good Research Lab. Katarzyna Kobalczyk is supported by funding from Eedi. NicolÃ¡s Astorga is sponsored by W.D. Armstrong Trust and Tennison Liu is sponsored by AstraZeneca. We thank the anonymous ICLR reviewers, members of the van der Schaar lab, and Andrew Rashbass
for many insightful comments and suggestions.


\bibliography{references_bib2}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix   


\section{Extended Related Work}\label{appdx:related-work}


\textbf{Active (in-context) learning.} In conventional AL, the ML system is designed to select queries from a fixed pool of unlabeled examples in order to reduce the uncertainty about its own outputs. Several works have explored AL strategies to improve the performance of LLMs on few-shot learning tasks performed with in-context learning \citet{zhang_active_2022, margatina_active_2023, diao_active_2024}. By acquiring new labeled examples, the generative distribution of the LLM is expected to shift towards outputs consistent with the ground-truth labels.  In our setup, we do not have access to a fixed pool of questions that the agent can choose from. Instead, the agent generates a question on its own. Noting that a question can be of the form ``What is the label $y$ for an input $x$?'', active task elicitation can be seen as a generalization of AL. Furthermore, as observed by \citet{zhang_active_2022}, in-context learning performance can be highly unstable across sample examples due to the idiosyncrasies of how LLMs update their generative distribution when extending the set of in-context examples. Given the unpredictable nature of the LLMs' distribution, our work focuses on eliciting binary task requirements, enabling the agent to filter its own outputs that do not conform to task requirements specified by the user. By extending the problem statement with additional requirements, the bias of $p_{\phi_h}$ changes at each iteration, yet it remains unknown whether this change is aligned with $p^*$. By encouraging uniformity of $p_{\phi_h}$ over the set of compatible solutions, our active-reasoning framework steers the agent to consider many possible interpretations of a task at each point of the interaction, resulting in the selected questions being less biased towards most likely interpretations according to the possibly misaligned language model.


\textbf{Clarifying questions and generative task elicitation.} Before the emergence of LLMs, prior works \citep{rao_learning_2018, rao_answer-based_2019, min_ambigqa_2020} have considered the problem of learning single-turn clarifying questions, with the question generator trained as sequence-to-sequence RNN's based on a pre-collected dataset of problems, clarifying questions, and their answers. More recently, in order to effectively address ambiguous user questions, \citet{krasheninnikov_assistance_2022} fine-tune the GPT-3 model on a data set of conversations consisting of ambiguous user requests, clarifying questions, and final answers. \citet{kuhn_clam_2022} show that LLMs can reason about ambiguous aspects of a query and generate clarification questions with zero-shot prompting. Similarly, \citet{li_eliciting_2023} capitalize on zero-shot prompting of LLMs and introduce a framework in which LLMs infer intended behaviour by querying the user with examples to label, yes-or-no questions or open-ended questions. They show that the LLM-generated queries are more efficient and require less effort than user-written prompts, enabling the discovery of initially unanticipated considerations of a task. Our work demonstrates that the LLM-generated questions can be improved by encouraging the agent to explicitly reason at inference time about the space of viable outputs given its current knowledge about the problem. This aligns with the principles highlighted in \citet{groenendijk_studies_1984}, where reasoning about the semantics of questions plays a crucial role in  shaping subsequent answers.

\textbf{Preference elicitation with LLMs.} A number of recent studies \citep{yang_bayesian_2021, piriyakulkij_asking_2023,  handa_bayesian_2024, austin_bayesian_2024} have leveraged LLMs for user preference elicitation, employing ideas of BED to select most informative queries. Despite surface-level similarities, these approaches are targeted at recommendation systems operating on a pre-determined set of objects or fixed feature spaces.  For instance, \citep{handa_bayesian_2024} present an interactive preference elicitation framework wherein a linear Bayesian model is used to describe user preferences over a set of features elected prior to the start of user interaction. In this setup, the LLM's role is limited to feature extraction and query verbalization. In contrast, our paper focuses on scenarios where the LLM reasoning agent is expected to output a solution that belongs to an unconstrained space of natural language, not pre-determined by a feature space of fixed dimensionality nor a fixed list of hypothetical answers. This necessitates an iterative sampling of solutions at each interaction step to approximate the currently available options. Albeit more challenging, the unconstrained setting closely reflects the real-world usage of LLMs as general purpose reasoning agents. Moreover, it enables the LLM agent to query the user about aspects of hypothetical solutions at varying levels of granularity, and crucially, as noted by \citet{li_eliciting_2023}, ask about aspects of a given problem that could have been difficult to anticipate before engaging in the interactive dialogue.


\textbf{Other related Active Learning and LLMs Approaches.} Recent studies have further explored the capabilities of Large Language Models in interactive and active learning scenarios. \cite{uncertainty_of_thoughts} introduce Uncertainty of Thoughts (UoT), an algorithm designed to improve LLMs' information seeking by enabling them to ask effective questions. This approach uses uncertainty modeling to guide the LLM in actively reducing the LLMs uncertainty.  Similarly, \cite{astorga2024poca} investigate active learning with LLMs in settings where data is partially observed and acquiring information has costs, focusing on efficient feature and label acquisition.  Building upon the theme of question generation, \cite{andukuri2024stargate} propose STaR-GATE, a method that rewards language models for generating useful clarifying questions, enhancing their ability to resolve task ambiguity through self-improvement. 





\section{Potential Applications of Active Task Disambiguation}


Active task disambiguation holds promise across a broad spectrum of applications. For instance, in personalized education and intelligent tutoring systems, clarifying questions can reveal subtle learning objectives and misconceptions, enabling real-time adaptation of instructional strategies \citep{Aleven_instructional_2016, graesser_adaptive_2005, krasheninnikov_assistance_2022}. In software engineering, interactive requirement elicitation helps mitigate misinterpretation during system design and development, leading to more efficient coding cycles and improved software quality \citep{goguen_requirements_1997, li_eliciting_2023}. Creative industriesâranging from content generation and design to game developmentâmay also benefit by aligning AI-generated outputs with users' evolving intensions \citep{Amershi_Power_2019}.

Beyond conventional machine learning tasks, active task disambiguation may be extended to a diverse array of problem-solving and decision-making domains. In scientific research, systematically clarifying experimental constraints can support more precise experiment design and hypothesis refinement \citep{Montgomery2017}. In robotics and autonomous systems, iteratively disambiguating mission goals and environmental constraints is key to achieving safer, more adaptive behavior \citep{paden_surveys_2016, Thrun2005}. In healthcare, actively refining diagnostic criteria and treatment protocols can improve patient outcomes by tailoring interventions to individual needs \citep{Topol2019}. In addition, we envision active task disambiguation to enhance areas like: autoformulation \citep{Astorga_Autoformulation_2024}, informed machine learning and autoML \citep{kobalczyk_towards_2025}, legal reasoning \citep{legalreasoning}, financial advisory \citep{de2018advances}, emergency response planning \citep{urbanplanning}, urban planning \citep{bayesianurban}, policy-making support \citep{bowen2005}, context-aware testing \citep{Rauba_Context-Aware_2024}, self-healing systems \citep{Rauba_Self-Healing_2024}, or cybersecurity \citep{bayesianciberrisk}.

\clearpage

\section{Active task disambiguation}\label{appdx:alg-details}

\begin{algorithm}[h]
\caption{Active requirement elicitation}\label{alg:requirement-elicit}
\begin{algorithmic}
\REQUIRE Initial problem statement $\gS^0 = (\gR^0, \gC)$, Max number of iterations $T$, Number of solutions sampled per iteration $N$, Number of questions sampled $M$.
\FOR{$t$ in $\{1, \ldots, T\}$:}
\STATE $\{h_i^t\}_{i=1}^N \sim p_{\phi_h}(\cdot \vert \gS^t)$
\STATE $\{q_j^t\}_{j=1}^M \sim p_{\phi_q}(\cdot \vert \gS^t)$ 
\IF{$M = 1$}
\STATE $q^* \gets q_1^t$
\ELSIF{$M > 1$}
\FOR{$j$ in $1, \ldots, M$}
\FOR{$i$ in $1, \ldots, N$}
\STATE $a_{i,j}^t \sim p_{\phi_a}(\cdot \vert h_i^t, q_j^t)$
\ENDFOR
\STATE $\mathrm{EIG}\left[q_j^t\right] \gets \mathtt{estimate\_EIG}(q_j^t, \{a_{i,j}^t\}_{i=1}^N)$
\ENDFOR
\STATE $q^* \gets \arg\max_{q \in \{q_j^t\}_{j=1}^M} \mathrm{EIG}\left[q\right]$
\ENDIF
\STATE $a^* \gets \mathtt{get\_oracle\_anser}(q^*)$
\STATE $r^* \gets \mathtt{transform\_to\_requirement}(a^*, q^*)$
\STATE $\gR^{t+1} \gets \gR^t \cup \{r\}$
\STATE $\gS^{t+1} \gets (\gR^{t+1}, \gC)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{$\mathtt{estimate\_EIG}(q_j, \{a_{i,j}\}_{i=1}^N)$}
\begin{algorithmic}
    \REQUIRE A question $q_j$ and a set of $N$ answers $\{a_{i,j}\}_{i=1}^N$
    \STATE $\{a_1, \ldots, a_n\} \gets$ Unique answers in $\{a_{i,j}\}_{i=1}^N$
    \FOR{$k \in \{1, \ldots, n\}$}
    \STATE $n_k \gets |\{i : a_{i, j} = a_k, i \in [N]\}|$
    \STATE $p_k \gets n_k / p_k$
    \ENDFOR
    \RETURN $-\sum_{k=1}^np_k\log(p_k)$
\end{algorithmic}
\end{algorithm}

The methods of obtaining $p_{\phi_h}$, $p_{\phi_q}$, $p_{\phi_a}$ as well as  $\mathtt{transform\_to\_requirement}$ and  $\mathtt{get\_oracle\_answer}$ subroutines are application-specific. We provide the specific prompts used in sections \ref{appdx:details}. The experiment of the 20Q game \ref{sec:exp-20Q} also considers an alternative method for estimating the EIG based on the token log-probabilities of sample solutions $\{h_i\}_{i=1}^N$:

\begin{algorithm}[h]
\caption{$\mathtt{estimate\_EIG\_logp}(q_j, \{a_{i,j}\}_{i=1}^N, \{p_i\}_{i=1}^N)$}\label{alg:EIG-estimation-logp}
\begin{algorithmic}
    \REQUIRE A question $q_j$, the set of $N$ answers $\{a_{i,j}\}_{i=1}^N$, log-probabilities of sample solutions $\{p_i\}_{i=1}^N$
    \STATE $\{a_1, \ldots, a_n\} \gets$ Unique answers in $\{a_{i,j}\}_{i=1}^N$
    \FOR{$k \in \{1, \ldots, n\}$}
    \STATE $p_k \gets  \sum_{\{i : a_{i,j} = a_k\}} \exp(p_i)$
    \ENDFOR
    \FOR{$k \in \{1, \ldots, n\}$}
    \STATE $p_k \gets p_k / \sum_{r=1}^n p_r$
    \ENDFOR
    \RETURN $-\sum_{k=1}^np_k\log(p_k)$
\end{algorithmic}
\end{algorithm}

\section{Prompts and Experimental Details}\label{appdx:details}

\subsection{The 20 questions game}

\subsubsection{Prompt templates}\label{appdx:20q-prompts}

When eliciting the requirements for the game of 20 questions we use the following set of prompts:

\begin{promptbox}{Solution generating prompt}
    \footnotesize
    \texttt{As an AI assistant, your role is to generate a wide and diverse range of animals that strictly meet the specified requirements. Your objective is to guess an animal from the entire animal kingdom that satisfies these requirements:}
    
    \hspace{2em} \texttt{\{List of requirements $\gR^t$\}}
    
    \texttt{For this, generate a carefully selected, diverse, and representative set of \{N\} animals following this scheme:}
    \begin{verbatim}
        1. <H>
        2. <H>
        ...
        {N}. <H>\end{verbatim}
    \texttt{Fill <H> with full name animals.}
\end{promptbox}


\begin{promptbox}{Question generating prompt}
    \footnotesize
    \texttt{Your objective is to guess an animal from the entire animal kingdom that satisfies the following requirements:}
    
    \hspace{2em} \texttt{\{List of requirements $\gR^t$\}}

    \texttt{For this, generate the most informative yes/no question."}
\end{promptbox}

\begin{promptbox}{Question selection prompt (used only for implicit-ToT)}
    \footnotesize
    \texttt{Your objective is to guess an animal from the entire animal kingdom that satisfies the following requirements:}
    
    \hspace{2em} \texttt{\{List of requirements $\gR^t$\}}

    \texttt{For this, select the most informative yes/no question from this list:"}
    
    \hspace{2em} \texttt{\{List of questions $\{q_j^t\}_{j=1}^M$}
\end{promptbox}

\begin{promptbox}{Answering prompt, $\{a_{i,j}^t\}_{i=1}^N \sim p_{\phi_a}(\cdot \vert \{h_i^t\}_{i=1}^N, q_j^t)$}
    \footnotesize
    \texttt{You are an expert critic that specializes in responding to yes/no questions. Given these animals:} 
    
    \hspace{2em} \texttt{1. \{$h_1^t$\}. <A>.}

    \hspace{2em} \texttt{2. \{$h_2^t$\}. <A>.}

    \hspace{2em} \texttt{...}

    \hspace{2em} \texttt{{N}. \{$h_N^t$\}. <A>.}
    
    \texttt{For each animal fill <A> with 'Yes' or 'No' with the response for this question: }

    \hspace{2em} \texttt{\{Sample question $q_j^t$\}}
\end{promptbox}


\begin{promptbox}{Requirement transform prompts}
\footnotesize
\textbf{Question to statement:} 

\texttt{Transform the following question into an affirmative statement {$q$}}

\texttt{You should start with 'The animal'. Do not write anything else than the affirmative statement.} \\

\textbf{Statement negation:}

\footnotesize
\texttt{Transform the following question into an affirmative statement {statement}}

\texttt{You should start with 'The animal'. Do not write anything else than the affirmative statement.}
\end{promptbox}


\begin{promptbox}{Oracle answer prompt (for GPT-4o-minisimulating player A)} 
    \footnotesize
    \texttt{Given the animal: {$h^*$} and the question: {$q^*$}, respond to the question depending on the given animal. You can only respond one of the following answers.}
    \begin{verbatim}
        Yes: If the answer is yes.
        No: If the answer is no.
        Pass: If the answer could be either yes or no.\end{verbatim}
    \texttt{Think step by step. After your thinking process respond: 'The final answer is <Response>'.}
\end{promptbox}



\subsubsection{Experimental details}\label{appdx:20Q-details}

\textbf{Animals for the main setup.} The focus of the main experiments is to evaluate the efficacy of the requirements elicited with various question-generating strategies. To do this, we select an arbitrary list of 15 animals as the ground-truth solutions $h^*$. These are: \textit{'Pangolin', 'African Grey Parrot', 'Emperor Scorpion', 'Manta Ray', 'King Cobra', 'Platypus', 'Starfish', 'Pufferfish', 'Flamingo', 'Salamander', 'Rabbit', 'Tarantula', 'Swordfish', 'Toucan', 'Chameleon'}. For each $h^*$, we run the game for $T=10$ iterations and with 5 different seeds. For solution and question generation we use $N=20$ and $M=5$, respectively. 


\textbf{Construction of the list of 500 animals for study 1.} As discussed in the theoretical section of our paper, a question that splits the solution space into roughly equal parts should result in the highest information gain. Since we do not restrict the user nor the reasoning agent to a pre-fixed list of animals, we cannot directly assess the partitioning properties of a question. Instead, during active question generation we resolve to sampling of individual $h_i$'s which should approximate the entire solution space. The aim of this study is to evaluate the partitioning properties of the generated questions on a much larger sample of solutions. To this end, we construct a large and diverse list of 500 animals. This list was obtained by prompting GPT-4o-minito stratify the animal kingdom into several categories. In response we obtained the following categories and their cardinalities: 100 mammals, 100 birds, 100 invertebrates, 50 amphibians, 50 reptiles and 100 fish.  Then, we subsample the respective number of animals in each category from the animals generated within our main experimental results until the sixth iteration. The stratification ensures that the constructed list is sufficiently diverse.





\subsection{Code generation}\label{appdx:code-gen-prompts}

\subsubsection{Prompt templates}
When eliciting the requirements for the game of 20 questions we use the following set of prompts:

\begin{promptbox}{Code header for $\gS^t$}
\footnotesize
\begin{verbatim}
def <function_name>(<input_name>):
    """
    <Comment explaining the expected functionality of the code>
    
    Examples:
    <Generated test cases>
    """
\end{verbatim}    
\end{promptbox}


\begin{promptbox}{Solution generating prompt}
\footnotesize
\texttt{You are an expert Python programmer that specializes in coding tasks. Complete the function given by the user. Do not change the function signature. Do not add any additional commentary. Do not import any additional libraries. Start your answer with the function signature.}
\vspace{1em}

\texttt{<Code header for $\gS^t$>}
\end{promptbox}


\begin{promptbox}{Question generating prompt (open)}
\footnotesize
\texttt{You are an expert Python programmer that specializes in solving user-specified coding tasks. To ensure you correctly understand user specifications, you can query the user for expected program outputs of sample inputs. Given the function signature, generate <M> sample inputs that will be most helpful in formalizing user intent. Structure your response as a list of function calls:}
\begin{verbatim}
    1. <function_name>(input_1)
    2. <function_name>(input_2)
    ...
    <M>. <function_name>(input_<M>)
\end{verbatim}
\texttt{Do not generate any additional content beyond the numbered list of function calls. Do not repeat the same inputs as in the Examples given.}
\vspace{1em}

\texttt{<Code header for $\gS^t$>}
\end{promptbox}


\begin{promptbox}{Question generating prompt (binary)}
\footnotesize
\texttt{You are an expert Python programmer that specializes in solving user-specified coding tasks. To ensure you correctly understand user specifications, you can write additional test cases. Given the function signature, generate <M> sample test cases that will be most helpful in formalizing user intent. Structure your response as a list of assertion:}
\begin{verbatim}
    1. assert <function_name>(input_1) == output_1
    2. assert <function_name>(input_2) == output_2
    ...
    <M>. assert <function_name>(input_<M>) == output_<M>
\end{verbatim}
\texttt{Do not generate any additional content or comments beyond the list of assertions.}
\vspace{1em}

\texttt{<Code header for $\gS^t$>}
\end{promptbox}


\subsubsection{Experimental details}


We conduct our experiments on two benchmakrs: HumanEval \citep{chen_codet_2023} and APPS \citep{hendrycks_measuring_2021}. To ensure a sufficient level of ambiguity of the problem statements, we filter the set of tasks in these benchmarks to the set of non-trivial tasks, i.e. task that cannot be solved with a 100\% accuracy zero-shot, i.e. without acquiring additional requirements. In addition, the APPS benchmark is filtered to the set of tasks that do not contain any example input-output pairs in the original problem formulation. Resulting datasets contain 48 and 47 tasks, respectively.



\section{Additional results}

\subsection{The game of 20 questions}\label{appdx:20q-results}
\setlength{\tabcolsep}{3pt}


Table~\ref{tab:20q-res} shows the final ranking results for the game of 20 questions.

\begin{table}[h]
\caption{\textit{Average ranks for the game of 20 questions.} Rankings averaged across 15 ground-truth animals, 5 run seeds, 25 evaluation seeds. Standard errors in brackets.}
\label{tab:20q-res}
\subfloat[][GPT-3.5-turbo]{
    \footnotesize
    \begin{tabular}{lllll}
    \toprule
$t$  & EIG-uniform & EIG-logprobs & implicit-ToT & implicit \\
\midrule
1 & 0.8 \scriptsize{(0.0)} & 1.0 \scriptsize{(0.1)} & \textbf{1.2}\scriptsize{(0.1)} & 0.9 \scriptsize{(0.0)} \\
2 & 1.5 \scriptsize{(0.1)} & 1.4 \scriptsize{(0.1)} & \textbf{1.7} \scriptsize{(0.1)} & 1.5 \scriptsize{(0.1)} \\
3 & \textbf{2.4} \scriptsize{(0.1)} & 1.9 \scriptsize{(0.1)} & \textbf{2.4} \scriptsize{(0.1)} & 2.0 \scriptsize{(0.1)} \\
4 & \textbf{2.9} \scriptsize{(0.1)} & 2.4 \scriptsize{(0.1)} & \textbf{3.0} \scriptsize{(0.1)} & 2.5 \scriptsize{(0.1)} \\
5 & \textbf{3.9} \scriptsize{(0.1)} & 2.9 \scriptsize{(0.1)} & 3.4 \scriptsize{(0.1)} & 3.0 \scriptsize{(0.1)} \\
6 & \textbf{4.1} \scriptsize{(0.1)} & 3.2 \scriptsize{(0.1)} & 3.7 \scriptsize{(0.1)} & 3.6 \scriptsize{(0.1)} \\
7 & \textbf{4.7} \scriptsize{(0.1)} & 3.4 \scriptsize{(0.1)} & 4.0 \scriptsize{(0.1)} & 4.0 \scriptsize{(0.1)} \\
8 & \textbf{5.0} \scriptsize{(0.1)} & 3.6 \scriptsize{(0.1)} & 4.5 \scriptsize{(0.1)} & 4.3 \scriptsize{(0.1)} \\
9 & \textbf{5.9} \scriptsize{(0.1)} & 3.8 \scriptsize{(0.1)} & 4.8 \scriptsize{(0.1)} & 4.7 \scriptsize{(0.1)} \\
10 & \textbf{6.2} \scriptsize{(0.1)} & 4.1 \scriptsize{(0.1)} & 5.1 \scriptsize{(0.1)} & 4.9 \scriptsize{(0.1)} \\
    \bottomrule
    \end{tabular}
    }
\subfloat[][GPT-4o-mini]{
    \footnotesize
    \begin{tabular}{lllll}
    \toprule
$t$  & EIG-uniform & EIG-logprobs & implicit-ToT & implicit \\
\midrule
1 & \textbf{0.4} \scriptsize{(0.0)} & 0.3 \scriptsize{(0.0)} & 0.2 \scriptsize{(0.0)} & 0.1 \scriptsize{(0.0)} \\
2 & \textbf{1.1} \scriptsize{(0.1)} & 0.9 \scriptsize{(0.1)} & \textbf{1.1} \scriptsize{(0.1)} & \textbf{1.1} \scriptsize{(0.1)} \\
3 & \textbf{2.0} \scriptsize{(0.1)} & 1.7 \scriptsize{(0.1)} & \textbf{2.0} \scriptsize{(0.1)} & \textbf{2.1} \scriptsize{(0.1)} \\
4 & \textbf{3.4} \scriptsize{(0.1)} & 2.6 \scriptsize{(0.1)} & 2.7 \scriptsize{(0.1)} & 2.9 \scriptsize{(0.1)} \\
5 & \textbf{4.4} \scriptsize{(0.1)} & 3.8 \scriptsize{(0.1)} & 3.8 \scriptsize{(0.1)} & 3.7 \scriptsize{(0.1)} \\
6 & \textbf{5.5} \scriptsize{(0.1)} & 4.7 \scriptsize{(0.1)} & 5.1 \scriptsize{(0.1)} & 4.5 \scriptsize{(0.1)} \\
7 & \textbf{6.2} \scriptsize{(0.1)} & 5.1 \scriptsize{(0.1)} & 5.7 \scriptsize{(0.1)} & 5.1 \scriptsize{(0.1)} \\
8 & \textbf{6.5} \scriptsize{(0.1)} & 5.5 \scriptsize{(0.1)} & 6.1 \scriptsize{(0.1)} & 5.3 \scriptsize{(0.1)} \\
9 & \textbf{6.7} \scriptsize{(0.1)} & 5.7 \scriptsize{(0.1)} & 6.5 \scriptsize{(0.1)} & 5.4 \scriptsize{(0.1)} \\
10 & \textbf{6.8} \scriptsize{(0.1)} & 6.0 \scriptsize{(0.1)} & 6.6 \scriptsize{(0.1)} & 5.5 \scriptsize{(0.1)} \\
    \bottomrule
    \end{tabular}
    }

\subfloat[][Llama3-70B (Instruct)]{
    \footnotesize
    \begin{tabular}{lllll}
    \toprule
$t$  & EIG-uniform & EIG-logprobs & implicit-ToT & implicit \\
\midrule
1 & \textbf{1.3} \scriptsize{(0.1)} & 0.4 \scriptsize{(0.0)} & 0.8 \scriptsize{(0.0)} & 0.5 \scriptsize{(0.0)} \\
2 & \textbf{1.5} \scriptsize{(0.1)} & 0.5 \scriptsize{(0.0)} & 1.2 \scriptsize{(0.1)} & 0.9 \scriptsize{(0.1)} \\
3 & 1.7 \scriptsize{(0.1)} & 0.9 \scriptsize{(0.1)} & \textbf{2.0} \scriptsize{(0.1)} & 1.4 \scriptsize{(0.1)} \\
4 & \textbf{2.4} \scriptsize{(0.1)} & 1.2 \scriptsize{(0.1)} & \textbf{2.4} \scriptsize{(0.1)} & 2.1 \scriptsize{(0.1)} \\
5 & \textbf{3.0} \scriptsize{(0.1)} & 1.5 \scriptsize{(0.1)} & 2.8 \scriptsize{(0.1)} & 2.7 \scriptsize{(0.1)} \\
6 & \textbf{3.8} \scriptsize{(0.1)} & 1.8 \scriptsize{(0.1)} & 3.1 \scriptsize{(0.1)} & 3.4 \scriptsize{(0.1)} \\
7 & \textbf{4.3} \scriptsize{(0.1)} & 2.2 \scriptsize{(0.1)} & 3.7 \scriptsize{(0.1)} & 3.8 \scriptsize{(0.1)} \\
8 & \textbf{5.1} \scriptsize{(0.1)} & 2.7 \scriptsize{(0.1)} & 4.1 \scriptsize{(0.1)} & 4.3 \scriptsize{(0.1)} \\
9 & \textbf{5.4} \scriptsize{(0.1)} & 3.2 \scriptsize{(0.1)} & 4.6 \scriptsize{(0.1)} & 5.3 \scriptsize{(0.1)} \\
10 & \textbf{6.3} \scriptsize{(0.1)} & 3.9 \scriptsize{(0.1)} & 5.1 \scriptsize{(0.1)} & 5.9 \scriptsize{(0.1)} \\
    \bottomrule
    \end{tabular}
}

\end{table}



\subsection{Code generation}

\subsubsection{Full results}

Table~\ref{tab:human-eval-appdx} shows the accuracies of the generated code samples at each iteration on the HumanEval benchmark. {Table~\ref{tab:apps-appdx} contains the corresponding results for the APPS benchmark. Despite the more challenging nature of the APPS benchmark, as indicated by the significantly lower accuracy across all models at $t=0$, our conclusions still hold. The EIG-based strategies outperform their non-EIG equivalents by a significant margin, and the ``open'' type of questions result in larger information gains than ``binary'' questions. 
}

\begin{table}[h]
    \small
    \centering
    \caption{Accuracy (\%) of solutions on HumanEval benchmark. (B)-solutions generated with ``binary'' questions. (O)-solutions generated with ``open'' questions.  Results averaged across 48 tasks, 3 run and 3 evaluation seeds per task.}
    \label{tab:human-eval-appdx}
    \subfloat[][GPT-3.5-turbo]{
    \begin{tabular}{lllll}
    \toprule
$t$ & \multicolumn{1}{c}{base (B)} & \multicolumn{1}{c}{EIG (B)}      & \multicolumn{1}{c}{base (O)} & \multicolumn{1}{c}{EIG (O)}      \\ 
\midrule
0 & 44.1 \scriptsize{(1.9)} & 44.6 \scriptsize{(1.8)} & 47.1 \scriptsize{(1.4)} & 47.0 \scriptsize{(1.4)} \\
1 & 55.3 \scriptsize{(2.5)} & 66.8 \scriptsize{(2.5)} & 67.5 \scriptsize{(1.7)} & \textbf{74.4} \scriptsize{(1.6)} \\
2 & 65.2 \scriptsize{(2.5)} & 78.4 \scriptsize{(2.1)} & 71.6 \scriptsize{(1.7)} & \textbf{81.4} \scriptsize{(1.6)} \\
3 & 70.7 \scriptsize{(2.6)} & 82.2 \scriptsize{(2.1)} & 75.5 \scriptsize{(1.7)} & \textbf{84.5} \scriptsize{(1.5)} \\
4 & 70.8 \scriptsize{(2.6)} & \textbf{85.6} \scriptsize{(2.0)} & 75.9 \scriptsize{(1.7)} & \textbf{85.0} \scriptsize{(1.5)} \\
    \bottomrule
    \end{tabular}
    }
    \subfloat[][GPT-4o-mini]{
    \begin{tabular}{lllll}
    \toprule
$t$ & \multicolumn{1}{c}{base (B)} & \multicolumn{1}{c}{EIG (B)}      & \multicolumn{1}{c}{base (O)} & \multicolumn{1}{c}{EIG (O)}      \\ 
\midrule
0 & 71.0 \scriptsize{(2.1)} & 69.6 \scriptsize{(2.1)} & 71.4 \scriptsize{(2.1)} & 70.6 \scriptsize{(2.1)} \\
1 & 74.6 \scriptsize{(2.0)} & 73.6 \scriptsize{(2.1)} & 77.1 \scriptsize{(2.0)} & 79.5 \scriptsize{(1.8)} \\
2 & 76.7 \scriptsize{(2.0)} & 78.9 \scriptsize{(2.0)} & 78.8 \scriptsize{(1.9)} & 83.9 \scriptsize{(1.7)} \\
3 & 77.0 \scriptsize{(2.0)} & 80.6 \scriptsize{(1.9)} & 80.8 \scriptsize{(1.9)} & 83.7 \scriptsize{(1.8)} \\
4 & 79.6 \scriptsize{(2.0)} & 81.5 \scriptsize{(1.9)} & 83.5 \scriptsize{(1.8)} & 83.8 \scriptsize{(1.8)} \\  
\bottomrule
    \end{tabular}
    }
    \quad
    \subfloat[][Llama3-70B (Instruct)]{
    \begin{tabular}{lllll}
    \toprule
$t$ & \multicolumn{1}{c}{base (B)} & \multicolumn{1}{c}{EIG (B)}      & \multicolumn{1}{c}{base (O)} & \multicolumn{1}{c}{EIG (O)}      \\ \midrule
0   & 63.9 \scriptsize{(2.0)}      & 63.8 \scriptsize{(2.0)}          & 63.6 \scriptsize{(2.0)}      & 63.6 \scriptsize{(2.0)}          \\
1   & 71.3 \scriptsize{(2.0)}      & \textbf{79.3} \scriptsize{(1.7)} & 72.9 \scriptsize{(2.0)}      & \textbf{79.7} \scriptsize{(1.7)} \\
2   & 73.0 \scriptsize{(2.0)}      & \textbf{82.5} \scriptsize{(1.6)} & 75.2 \scriptsize{(2.0)}      & \textbf{82.8} \scriptsize{(1.7)} \\
3   & 75.4 \scriptsize{(1.9)}      & \textbf{84.7} \scriptsize{(1.6)} & 78.3 \scriptsize{(1.9)}      & \textbf{84.2} \scriptsize{(1.7)} \\
4   & 78.8 \scriptsize{(1.8)}      & \textbf{87.5} \scriptsize{(1.4)} & 81.2 \scriptsize{(1.8)}      & 85.5 \scriptsize{(1.5)}    \\ 
    \bottomrule
    \end{tabular}
    }
    \subfloat[][Llama3-8B (Instruct)]{
    \begin{tabular}{lllll}
    \toprule
    $t$ & base (B) & EIG (B) & base (O) & EIG (O) \\
    \midrule
0 & 34.8 \scriptsize{(1.7)} & 34.7 \scriptsize{(1.7)} & 34.6 \scriptsize{(1.7)} & 34.7 \scriptsize{(1.7)} \\
1 & 40.9 \scriptsize{(1.9)} & 48.8 \scriptsize{(1.9)} & 53.7 \scriptsize{(1.9)} & \textbf{62.3} \scriptsize{(2.0)} \\
2 & 45.6 \scriptsize{(1.9)} & 56.2 \scriptsize{(2.0)} & 58.9 \scriptsize{(2.0)} & \textbf{70.5} \scriptsize{(2.0)} \\
3 & 49.6 \scriptsize{(2.0)} & 60.4 \scriptsize{(2.0)} & 60.6 \scriptsize{(2.1)} & \textbf{74.4} \scriptsize{(2.0)} \\
4 & 53.0 \scriptsize{(2.0)} & 65.8 \scriptsize{(2.0)} & 64.8 \scriptsize{(2.0)} & \textbf{75.5} \scriptsize{(1.9)}\\
    \bottomrule
    \end{tabular}
    }
\end{table}



\begin{table}[h]
    \small
    \centering
    \caption{Accuracy (\%) of solutions on the APPS benchmark. (B)-solutions generated with ``binary'' questions. (O)-solutions generated with ``open'' questions.  Results averaged across 47 tasks, 3 run and 3 evaluation seeds per task.}
    \label{tab:apps-appdx}
    \subfloat[][GPT-3.5-turbo]{
    \begin{tabular}{lllll}
    \toprule
$t$ & \multicolumn{1}{c}{base (B)} & \multicolumn{1}{c}{EIG (B)}      & \multicolumn{1}{c}{base (O)} & \multicolumn{1}{c}{EIG (O)}      \\ 
\midrule
0 & 32.1 \scriptsize{(1.6)} & 31.2 \scriptsize{(1.6)} & 32.1 \scriptsize{(1.6)} & 32.9 \scriptsize{(1.7)} \\
1 & 49.0 \scriptsize{(1.9)} & 49.5 \scriptsize{(2.0)} & 51.3 \scriptsize{(1.9)} & \textbf{58.4} \scriptsize{(2.0)} \\
2 & 54.3 \scriptsize{(1.9)} & 61.0 \scriptsize{(2.0)} & 59.0 \scriptsize{(1.9)} & \textbf{68.3} \scriptsize{(2.0)} \\
3 & 60.8 \scriptsize{(2.0)} & 67.5 \scriptsize{(1.9)} & 68.2 \scriptsize{(1.8)} & \textbf{74.6} \scriptsize{(1.8)} \\
4 & 62.4 \scriptsize{(1.9)} & 71.3 \scriptsize{(1.9)} & 69.7 \scriptsize{(1.9)} & \textbf{77.3} \scriptsize{(1.8)} \\
    \bottomrule
    \end{tabular}
    }
    \subfloat[][GPT-4o-mini]{
    \begin{tabular}{lllll}
    \toprule
$t$ & \multicolumn{1}{c}{base (B)} & \multicolumn{1}{c}{EIG (B)}      & \multicolumn{1}{c}{base (O)} & \multicolumn{1}{c}{EIG (O)}      \\ 
\midrule
0 & 35.8 \scriptsize{(1.1)} & 35.9 \scriptsize{(1.2)} & 34.6 \scriptsize{(1.1)} & 35.9 \scriptsize{(1.2)} \\
1 & 53.9 \scriptsize{(1.7)} & 62.0 \scriptsize{(1.8)} & 59.2 \scriptsize{(1.7)} & \textbf{68.0} \scriptsize{(1.9)} \\
2 & 57.5 \scriptsize{(1.8)} & 67.7 \scriptsize{(1.9)} & 68.8 \scriptsize{(1.7)} & \textbf{83.0} \scriptsize{(1.6)} \\
3 & 60.8 \scriptsize{(1.9)} & 72.9 \scriptsize{(1.8)} & 72.7 \scriptsize{(1.8)} & \textbf{85.7} \scriptsize{(1.5)} \\
4 & 62.9 \scriptsize{(1.9)} & 75.3 \scriptsize{(1.8)} & 75.1 \scriptsize{(1.8)} & \textbf{87.2} \scriptsize{(1.5)} \\
\bottomrule
    \end{tabular}
    }
    \quad
    \subfloat[][Llama3-70B (Instruct)]{
    \begin{tabular}{lllll}
    \toprule
$t$ & \multicolumn{1}{c}{base (B)} & \multicolumn{1}{c}{EIG (B)}      & \multicolumn{1}{c}{base (O)} & \multicolumn{1}{c}{EIG (O)}      \\ 
\midrule
0 & 38.0 \scriptsize{(1.9)} & 38.3 \scriptsize{(1.8)} & 38.4 \scriptsize{(1.8)} & 37.8 \scriptsize{(1.8)} \\
1 & 49.0 \scriptsize{(2.0)} & 54.7 \scriptsize{(1.9)} & 57.1 \scriptsize{(2.0)} & \textbf{63.4} \scriptsize{(2.0)} \\
2 & 51.6 \scriptsize{(2.0)} & 60.1 \scriptsize{(2.0)} & 61.9 \scriptsize{(2.1)} & \textbf{74.9} \scriptsize{(1.9)} \\
3 & 57.7 \scriptsize{(2.0)} & 65.7 \scriptsize{(2.1)} & 68.8 \scriptsize{(2.0)} & \textbf{78.3} \scriptsize{(1.9)} \\
4 & 60.1 \scriptsize{(2.0)} & 68.5 \scriptsize{(2.0)} & 73.5 \scriptsize{(2.0)} & \textbf{81.6} \scriptsize{(1.8)} \\
    \bottomrule
    \end{tabular}
    }
    \subfloat[][Llama3-8B (Instruct)]{
    \begin{tabular}{lllll}
    \toprule
    $t$ & base (B) & EIG (B) & base (O) & EIG (O) \\
    \midrule
0 & 10.6 \scriptsize{(1.5)} & 8.5 \scriptsize{(1.6)} & 9.1 \scriptsize{(1.6)} & 9.7 \scriptsize{(1.8)} \\
1 & 13.5 \scriptsize{(1.9)} & 17.0 \scriptsize{(2.4)} & \textbf{21.8} \scriptsize{(3.0)} & \textbf{25.3} \scriptsize{(3.4)} \\
2 & 15.4 \scriptsize{(2.1)} & 19.7 \scriptsize{(2.9)} & \textbf{26.9} \scriptsize{(3.8)} & \textbf{28.4} \scriptsize{(4.3)} \\
3 & 14.5 \scriptsize{(2.1)} & 22.7 \scriptsize{(3.3)} & \textbf{32.1} \scriptsize{(4.6)} & \textbf{34.2} \scriptsize{(5.4)} \\
4 & 18.5 \scriptsize{(2.5)} & 26.9 \scriptsize{(3.7)} & 39.2 \scriptsize{(4.9)} & \textbf{50.7} \scriptsize{(5.7)} \\
    \bottomrule
    \end{tabular}
    }
\end{table}






\subsubsection{Ambiguity in HumanEval}\label{appdx:human-eval-ambiguity}

We note that the low accuracy of generated samples $h \sim p_{\phi_h}(\cdot \vert \gS^0)$ does not imply that $\gS^0$ itself is inherently ambiguous, as it may simply be caused by the LLM's limitations in following complex instructions. However, even if $\gS^0$ is objectively not ambiguous, the input-output examples appended to the prompt $\gS^t$ may positively bias $p_{\phi_h}$ towards correct solutions, which is a desirable by-product of employing active querying strategies to task disambiguation. To ensure, however, that the code-generation experiment is indeed appropriate to test our strategy for ambiguous prompts, we have listed in table~\ref{tab:human-eval-ambiguity} sample tasks with explanation of their ambiguity.

\begin{table}[h]
\caption{Sample ambiguous problem statements from the HumanEval benchmark.}
\label{tab:human-eval-ambiguity}
\small
\begin{tabular}{p{9.5cm}|p{4cm}}
    \toprule
    \textbf{problem statement} & \textbf{ambiguity} \\
    \midrule
    \vspace{-1.5em}
     \begin{lstlisting}[language=Python] 
from typing import List

def separate_paren_groups(paren_string: str) -> List[str]:
    """ Input to this function is a string containing multiple groups of nested parentheses. Your goal is to
    separate those group into separate strings and return the list of those.
    Separate groups are balanced (each open brace is properly closed) and not nested within each other
    Ignore any spaces in the input string.
    """
    \end{lstlisting} 
    & How to handle characters in the input string other than parentheses and spaces? How to handle strings with non-balanced groups of parentheses?
    \\
    \vspace{-1.5em}
    \begin{lstlisting}[language=Python]
from typing import List


def remove_duplicates(numbers: List[int]) -> List[int]:
    """ From a list of integers, remove all elements that occur more than once.
    Keep order of elements left the same as in the input.
    """ 
    \end{lstlisting}
    & Should all duplicated elements be removed or one unique element retained?, e.g.
    \texttt{\scriptsize{remove\_duplicates([4, 4, 3, 5, 5]) ->[4, 3, 5]}} or \texttt{\scriptsize{[3]}}
\\
\vspace{-1.5em}
\begin{lstlisting}[language=Python]
def is_bored(S):
    """
    You'll be given a string of words, and your task is to count the number
    of boredoms. A boredom is a sentence that starts with the word "I".
    Sentences are delimited by '.', '?' or '!'.
    """
\end{lstlisting}
& Is a sentence starting with ``I'm'' a boredom or not? \\
\vspace{-1.5em}
\begin{lstlisting}[language=Python]
def histogram(test):
    """Given a string representing a space separated lowercase letters, return a dictionary
    of the letter with the most repetition and containing the corresponding count.
    If several letters have the same occurrence, return all of them.
    """
\end{lstlisting} 
& How to handle letters that are not separated by spaces?
\\
\vspace{-1.5em}
\begin{lstlisting}[language=Python]
def cycpattern_check(a , b):
    """You are given 2 words. You need to return True if the second word or any of its rotations is a substring in the first word
    """
\end{lstlisting} 
& What is understood by a string rotation? \\
\vspace{-1.5em}
\begin{lstlisting}[language=Python]
def f(n):
    """ Implement the function f that takes n as a parameter,
    and returns a list of size n, such that the value of the element at index i is the factorial of i if i is even
    or the sum of numbers from 1 to i otherwise.
    i starts from 1.
    the factorial of i is the multiplication of the numbers from 1 to i (1 * 2 * ... * i).
    """
\end{lstlisting} 
& Should the sum of 1 to $i$ include $i$?  \\
\bottomrule
\end{tabular}
\end{table}

\clearpage

\section{Costs of question elicitation}\label{appdx:costs}


We note that the EIG-based question-generating strategies presented in this work require an increased number of LLM calls compared to the zero-shot baselines. In line with the assumptions commonly made in BED, we take the stance that the computational load required to select the optimal query is negligible compared to the value of acquiring information that reduces problem ambiguity. We anticipate this assumption will become more valid over time as technology advancements lower the costs of LLM token generation, thereby enhancing the importance of efficient information acquisition strategies. However, given that in many real-world applications design choices may be constrained by the LLM sampling costs, we include a comparison of the effective number of LLM calls required at each interaction step for the key strategies considered. In the below, \( N \) is the number of generated solutions at each step, and \( M \) is the number of generated candidate questions.

\subsection{20 Questions}

\[
\begin{array}{@{}ll@{}}
\toprule
\textbf{Strategy} & \textbf{Cost} \\
\midrule
\text{implicit (baseline)} & O(1) \\
\text{implicit-ToT} & O(M) \\
\text{EIG} & O(N + M + NM) \\
\bottomrule
\end{array}
\]

In the baseline strategy, only one call is needed to sample a single question, \( q^* \).

In the implicit-ToT strategy, additional $M$ calls are needed to generate a set of $M$ candidate questions, from which one is selected. 

In the EIG-based strategies (EIG-uniform and EIG-logprobs), \( M \) calls are needed to sample a set of candidate questions \( \{q_j\}_{j=1}^M \) and \( N \) calls to sample the hypothetical solutions \( \{h_i\}_{i=1}^N \). To select the best question, we need to estimate the EIG, which requires the LLM to answer each question, \( q_j \), about every sampled solution, \( h_i \), thus requiring additional \( NM \) LLM calls.

\subsection{Code Generation}

\[
\begin{array}{@{}ll@{}}
\toprule
\textbf{Strategy} & \textbf{Cost} \\
\midrule
\text{base} & O(1) \\
\text{EIG} & O(N + M) \\
\bottomrule
\end{array}
\]

In the code generation example, the evaluation of the EIG does not require additional LLM calls. The answers, \( a_{i,j} \), are obtained by executing the code solution, \( h_i \), against each question \( q_j \).



\end{document}
