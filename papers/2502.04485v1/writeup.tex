\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage{neurips_2023}




% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography


%%% CUSTOM IMPORT %%%%
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{soul}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{wrapfig}
 \usepackage{tabularray}
\usepackage[pdftex,dvipsnames]{xcolor}
\usepackage[colorinlistoftodos,prependcaption,textsize=footnotesize, linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum]{todonotes}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}


\newtheorem{definition}{Definition}

\newcommand{\sO}{\mathcal{O}}
\newcommand{\sP}{\mathcal{P}}
\newcommand{\sT}{\mathcal{T}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\sG}{\mathcal{G}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\sM}{\mathcal{M}}
\newcommand{\sS}{\mathcal{S}}
\newcommand{\sR}{\mathcal{R}}



\definecolor{green_bis}{HTML}{82B366}
\definecolor{yellow_bis}{HTML}{D6B656}
\definecolor{blue_bis}{HTML}{6C8EBF}
\definecolor{red_bis}{HTML}{B85450}

\usepackage{pifont}
\newcommand*\circled[1]{\textcircled{\raisebox{-0.9pt}{#1}}}

%%%%% END CUSTOM IMPORT %%%
% NEW DEFINITIONS
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]


\title{Conceptual Reinforcement Learning}

% Synthesizing Verification Programs to Verify LLM-Generated Outputs
\author{
}




\begin{document}

\maketitle

\section{Introduction}

The pursuit of creating general autonomous agents requires mastery a wide array of skills and generalisation to new, previously unobserved tasks. Traditional reinforcement learning (RL) methods,  involve individually designing reward functions and environment interactions for each task, and thus become impractical due to the sheer diversity and complexity of tasks. To overcome this, unsupervised RL provides a promising alternative by enabling agents to acquire diverse skills without explicit supervision. Particularly, recent approaches have adopted a goal-conditioned perspective, where agents learn from a dataset of reward-free trajectories to develop multi-task policies that enable reaching any state within the state space.

Particularly relevant in the context of general autonomous agents is offline RL, where online data collection is either impractical or too costly. These setups rely on pre-collected datasets, circumventing the need for real-time data acquisition. When learning from offline data, goal-conditioned algorithms encounter significant challenges in environments characterized by long horizons and large state spaces. These challenges stem from the sparsity of observations relative to the dimensions of state and action spaces, compounded by typically sparse rewards. To address these challenge our key idea relies on the principle of \textit{abstraction}, in which each goal corresponds to a region of low-level states that meet certain abstract criteria, rather than a single state in a high-dimensional space. 

Cognitive science research indicates that humans excel at complex planning and skill generalisation through abstraction. By tailoring the representation of their actions and environment to suit specific goals, humans adjust how they represent, remember, and reason about the world to efficiently plan for various tasks. However, a significant challenge of RL lies in identifying and utilizing a set of abstractions that are both relevant and effective across different goals. Ideal abstractions should facilitate high-level planning, remain compatible with low-level actions, and generalize well to new tasks.

Despite ongoing efforts to develop such abstractions, most state-of-the-art robotics and planning systems still rely on manually engineered representations for each new domain. These hard-coded abstractions, while precise, offer limited flexibility and are often task-specific. In contrast, we argue that natural language provides a versatile medium for representing and transferring complex ideas and intentions with minimal assumptions about the problem setting. Its compositional nature enables it to represent combinatorial concepts effectively and transfer knowledge broadly [CITE]. Furthermore, recent studies suggest that humans utilize language as an abstraction tool for reasoning and planning, highlighting its potential as a powerful mechanism for learning and skill acquisition.

\textbf{Contribution} This paper introduces an offline reinforcement learning framework for training goal-conditioned agents that can achieve arbitrary goals specified in natural language after learning on pre-collected data without any supervision. Within our approach a set of reward-free, unlabelled trajectories is processed to build a supervised dataset on which we fine tune a large language model, obtaining a domain-agnostic but domain-expect LLM agent. 



\section{Problem formalism}

We consider the problem of training a natural language goal-conditioned agent from unlabelled offline data. We assume the existence of an environment $(\sX, \sA, P)$ where $\sX$ is the raw, low-level state space, $\sA$ is the low-level action space, and $P$ is a state-transition probability $P: \sX \times \sA \times \sX \rightarrow \mathbb{R}_+$. We are exclusively interested in cases where indiviudal states $x \in \sX$ and actions $a \in \sA$ can be represented with natural language. We also assume access to a dataset $\sD$ of pre-collected episodes generated by using an unknown data collection policy, $\pi^\beta$, where $\pi^\beta(\cdot \vert s )$ defines a probability distribution over the action space $\sA$, given the current state $s$. Each episode is stored in $\sD$ as a series $(x, a, x')$ of tuples, where $x, x' \in \sX$ and $a \in \sA$. 


Let $\sG$ be the set of all possible goals, achievable within the given environment. We assume that $\sG \subseteq \Sigma^*$, where $\Sigma^*$ stands for the space of all natural language word sequences of a bounded length. Each goal $g \in \sG$ is expressed in natural language via a natural language instruction and corresponds to a collection of states in the state space $\sX$ which satisfy this goal. We let $\Psi : \sX \times \sG \rightarrow \{0, 1\}$ denote the boolean operator that evaluates whether a state $x \in \sX$ satisfies a goal $g$. Note that if a goal $g$ corresponds to a singleton $\{x\}$ for an $x \in \sX$, $\Psi$ can be easily expressed by a boolean function checking if two states are equivalent (or sufficiently close to each other). 


Our framework generalizes beyond singleton goal states. If the space of all goals $\sG$ was small, $\Psi$ could be implemented via human supervision. However, generalisation to new language instructions requires a large diversity of goals; this places on the RL practitioner the onus of manually implementing $\Psi$, which becomes intractable as the complexity of the environment and flexibility of language increase. Thus, we propose that $\Psi$ is realized with a large language model adept at understanding natural language instructions and implementing them as functions executable with a symbolic interpreter. We note that $\Psi$ can readily act as the reward function for training our goal-conditioned agent. 

To this end let $R: \sG \times \sX \times \sA \rightarrow \mathbb{R}$ denote the goal-conditioned reward function. We can model our problem as a goal-conditioned Markov decision process (MDP),  $\sM = (\sX, \sA, P, R, p_0, \gamma, \sG)$, where $p_0: \sX \rightarrow \mathbb{R}_+$ is an initial state distribution and $\gamma$ is the discount factor. In the goal-conditioned setting, the objective is to learn a goal-conditioned policy $\pi$ that maximizes the expectation of the cumulative return over a goal distribution $p_\sG$ on $\sG$. 
$$J(\pi) = \mathbb{E}_{g \sim p_\sG, x_0 \sim p_0}\left[ \sum_{t} \gamma^t \mathbb{E}_{x_{t} \sim P(\cdot \vert x_{t-1}, a_{t-1}), a_{t} \sim \pi(\cdot \vert x_t, g)}\left[R(g, x_t, a_t)\right]\right]$$

In practice, $J(\pi)$ must be estimated empirically on the available training data $\sD$ and a set of pre-defined goals $\sG_{\text{train}}$. The learned goal-conditioned policy can be then subsequently evaluated online on a set of evaluation goals, $\sG_{\text{test}} \subset \sG \setminus \sG_{\text{train}}$, that are distinct from those observed during training. 

% In the goal-conditioned offline formulation, the unlabelled dataset $\sD$ can be relabeled by evaluating the reward function at each tuple in $\sD$, and adding the resulting tuple $(x, a, R(x, a, g), x')$ to the labelled goal-oriented dataset $\sD_{g}$. If $\sG_{\text{train}} \subset \sG$ is the subset of training goals, then $\sD_{\sG_{\text{train}}} := \{\sD^g\}_{g \in \sG_{train}}$ creates a collection of goal-conditioned episodes that can be used to learn a goal-conditioned policy through executing any offline RL algorithm. The algorithm runs completely offline, by sampling tuples from $\sD_{\sG_{\text{train}}}$ and without any interaction with the environment. The goal-conditioned policy can be then evaluated online on a set of fixed evaluation goals $\sG_{\text{test}} \subset \sG \setminus \sG_{\text{train}}$. 

% Let $\hat{\pi}$ denote the goal-conditioned policy learned in this way. If $\hat{\pi}$ can be learned effectively, we propose that the dataset consisting of pairs of natural-language goal descriptions, states and their corresponding goal-conditioned optimal actions, i.e. tuples $(g, x, \arg\max \ \hat{\pi}(a \vert x))$ are used for supervised fine-tuning of a general-purpose large language model. The motivation for this is that the prior knowledge of LLMs allows for \textit{extrapolation} to new, previously unobserved goals by the means of analogy between the goals from $\sG_{\text{train}}$. 

\section{Methodology}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth,trim={2cm 0 2.5cm 0},clip]{figures/method_overview.pdf}
    \caption{\textbf{Overview of the method.} Starting from unlabelled data, \textcolor{blue_bis}{\circled{1}} a goal-conditioned abstraction function $F$ generates abstract MDP. \textcolor{yellow_bis}{\circled{2}} These MDPs are solved with Q-learning to create a dataset of optimal policies. \textcolor{green_bis}{\circled{3}} An LLM learns to reproduce these policies through supervised fine-tuning (SFT). \textcolor{red_bis}{\circled{4}} At inference time, the fine-tuned LLM can realise optimal policy for new goals.}
    \label{fig:enter-label}
\end{figure}

\subsection{State abstraction to tackle the curse of dimensionality}



Learning a goal-conditioned policy solely based on unlabelled offline demonstrations in a high-dimensional environment $(\sX, \sA, P)$ can be highly inefficient, if not infeasible, due to the limited data available in the pre-collected episodes $\sD$. The key idea behind our approach is to use natural language to aggregate low-level states into abstract, high-level states, for each goal $g$, creating an abstract goal-dependent state space $\sS^g$ to accelerate learning and planning.


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-1.8em}
    \includegraphics[width=1.1\linewidth]{figures/CRL.pdf}
    \vspace{-1.5em}
    \caption{\textit{Conceptual visualization of the abstraction operator}. $F(\cdot \ ; g)$ maps low-level states in $x \in \sX$ to a high-level, abstract state space $\sS^g$ The sparsely-observed transition dynamics in $\sX$ are aggregated into densely-observed transition dynamics in the abstract state space $\sS^g$.}
    \label{fig:enter-label}
    \vspace{-2em}
\end{wrapfigure}

Let $F: \sX \times \sG \rightarrow \Sigma^*$ be an \textit{abstraction operator} that given a goal $g$ maps a single observation $x \in \sX$ to an abstract state $F(x; g)$. We let $\sS^g$ denote the image of $\sX$ under $F(\cdot \ ; g)$. We require that the abstraction function is surjective, but strictly non-injective, aggregating for a given goal $g$ multiple states from $\sX$ into a single abstract state. We then have that for each $s \in \sS^g$, there exists a set of low-level states $x \in \sX$ corresponding to it, precisely defined as $\sX_s := \{x \in \sX : F(x; g) = s\}$. Note, $F(\cdot; g)$ generates a partitioning of $\sX$ into a set of non-overlapping subsets of $\sX$, $\{\sX_s\}_{s \in \sS^g}$. 

%As a simplifying assumption, we assume that each state  

% Given our dataset $\sD$ of pre-collected trajectories, we can define the empirical goal-conditioned transition operator $\hat{P}^g$ as:
% $$\hat{P}^g (s' \vert s, a)  = \frac{\sum_{(x, a', x') \in \sD}\mathbbm{1}\{f(x; g) = s,  a' = a, f(x; g) = s'\}}{\sum_{(x, a', x') \in \sD}\mathbbm{1}\{f(x; g) = s,  a' = a\}}$$
% We note however, that $\hat{P}^g$, is biased, as it is strictly dependent on the data collection policy. \todo{Kasia: I think this requires some further discussion / investigation.}
If $X_t$, $A_t$, and $R_t$ are the random variables representing states, actions and rewards of the original MDP $\sM$, then we define $S_t^g := F(X_t; g)$, $A_t^g := A_t$ and $R^g$ as the state, action and reward random variables of the reduced Abstract Markov Decision Process (AMDP), $\sM^g := (\sS^g, \sA, P^g, R^g, p^0_g, \gamma)$. Here $p_0^g$ is the initial state distribution over the abstract state space $\sS^g$, where  
\begin{equation}
    p_0^g(s) := \mathbb{P}(S_0^g = s) = \mathbb{P}(F(X_0; g) = s) = \mathbb{P}(X_0 \in \sX_s) = \sum_{x \in \sX_s}p_0(x),
\end{equation}
and $P^g$ is the state transition dynamics of $S_t^g$. It remains to check that $P^g$ satisfies the Markov Property. 

[TO DO: Discuss the assumptions on $F(\cdot ; g)$ which result in Markovianity of $P^g$ and what happens if that is not the case.]

The specification of the abstract reward function, $\sR^g: \sS^g \times \sA \rightarrow \mathbb{R}$ remains a modelling choice (see Section \ref{sec:practical-implementation}).

Given the above, our original offline dataset $\sD$, can then be mapped to an abstract dataset,  
$$\sD^g := \{(s_i, a_i, s'_i) : s_i = F(x_i; g), s_i' = F(x_i'; g), (x_i, a_i, x'_i) \in \sD\}.$$
The dataset $\sD^g$ can used to learn an optimal goal-conditioned policy $\pi^g$ with any offline reinforcement learning algorithm. Thanks to the aggregation of states, the dimensionality of the problem is reduced and the observed frequency of each state-action pair increases, making learning of $\pi^g$ easier. 

\subsection{What makes a good abstraction function ?}



\begin{figure}[!tbph]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/before_abstraction_minigrid.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/after_abstraction_minigrid.png}
  \end{minipage}
\caption{\textbf{Example of goal conditioned abstraction for the mini-grid environment :} the observation of the environment (left) is transformed into an abstract state (right) by the feature selection function conditioned on the goal "Go to a blue key".Only the useful features (walls, blue key, agent position and direction) are kept.}
\end{figure}

\textbf{Conservation of the Markovian property}. To be solved by Reinforcement Learning methods, the stochastic model $\mathcal{M}^g$ is required to be Markovian. It means that at any time $t$, the abstract state $S_{t+1}^g$ should only depend on the last abstract state $(S_t^g, A_t^g)$, i.e

\begin{equation*}
    \mathbb{P}(S_{t+1}^g=s | S_t^g,...,S_0^g,A_t,...,A_0) = \mathbb{P}(S_{t+1}^g=s | S_t^g,A_t)
\end{equation*}

\textbf{Feature selection :} Given a goal g and a set of observation $X_t$ that can be decomposed into features $X_t=\{X^i_t\}_{i\in[0,n]}$. We assume that there exists a subset of features (chosen as the first k features) $[0,k]$ such that 

\begin{align*}
    -&\forall t \in \mathbb{N},~ \mathbb{P}(g | X^0_t,...,X^k_t) = \mathbb{P}(g) \text{ (Independence with the goal)} \\
    -&\forall (i,j) \in \mathbb{N}^2, \mathbb{P}(X^{k+1}_i,...,X^n_i |X^{0}_j,...,X^k_j,g) = \mathbb{P}(X^{k+1}_t,...,X^n_t,g) \text{ (Goal-conditioned independence between features)}.
\end{align*}

A set of features that follows these properties is informative for reaching the goal \textbf{(Independence with the goal)}. An abstraction function $F$ which discards this set of features from the observations (i.e $F(X_t,g) = X^{k+1}_t,...,X^n_t )$ yields a Markovian stochastic model thanks to the \textbf{Goal-conditioned independence between features} (Proof in the Appendix).



\subsection{LLM as domain-expert RL agents}

Based on the learned policies, $\hat{\pi}^g$, for each goal $g \in \sG_{train}$ we can create a supervised training set:

$$\sD^{\text{SFT}} := \{ (g, s, a) : s \in \sS^g, a = \arg\max \hat{\pi}^g( \cdot \vert s) \}_{g \in \sG_{\text{train}}}$$

As each of the elements $g$, $s$ and $a$, can be represented in natural language, we propose to use $\sD^{\text{SFT}}$ as a dataset for supervised fine-tunning of a general purpose large language model. The end goal is to obtain an expert LLM agent for the domain covered by the pre-defined environment, and more specifically its dynamics demonstrated in the pre-collected dataset $\sD$. For any new instruction  $g^*$, given a current state $x$ and its corresponding high-level state $s = F(x ; g^*)$ the response of the LLM agent should be semantically equivalent to $\arg\max \hat{\pi}^{g^*}( a \vert s)$, \textit{without} having to generate and solve the AMDP $\sM^{g^*}$. The motivation for this approach is that the prior knowledge of LLMs allows for \textit{extrapolation} to new, previously unobserved goals by the means of a semantic analogy between $g^*$ and the previously observed goals from $\sG_{\text{train}}$, while the episodes observed within $\sD$ act as a source of knowledge for the LLM about the dynamics of a given environment, enabling it to build an internal world model.


\subsection{Practical implementation}\label{sec:practical-implementation}

Recall that our framework hinges on the existence of two operators $\Psi$ and $F$. While in traditional RL both of these operators would need to be defined by human engineers or learned with separate models, we propose that both operators are implemented as Python functions generated by a LLM prompted with the goal description $g$, a few-shot sample of observations from $\sX$ and a short system prompt providing additional context about the environment. Let $G^g := \{x \in \sX : \Psi(x ; g) = 1\}$ be the goal set and $\sX_s := \{x \in \sX : F(x; g) = s\}$. The prompt additionally instructs the LLM to define $\Psi(\cdot ; g)$ and $F(\cdot ; g)$ in a way that $G^g$ corresponds to precisely one of the subsets $\sX_s$; i.e. there exists exactly one goal state $s^{g^*}\in \sS^g$ that is equivalent to $g$ for which $G^g = \sX_{s^{g^*}}$ and for any other $s \in \sS^g, s \neq s^{g^*}$, $\sX_s \cap \sX_{s^{g^*}} = \varnothing$. 

Having access to $\Psi$ we can build a goal-oriented reward function $R^g$. We experiment with a number of reward functions tailored to observation in natural language space:
\begin{itemize}
    \item $R^g(s, a) :=  \mathbb{E}_{\hat{P}^g(s' \vert s, a)} \left[ \mathbbm{1}\{s' = s^{g^*}\}\right] = \hat{P}^g(s^{g^*} \vert s, a)$, i.e. probability of reaching the goal state from the current state
    % \item $R^g(s, a) := \frac{\hat{P}^g(s^*^g \vert s, a)}{\hat{P}^g(s \vert s, a)}$ - probability of reaching the goal from the current state over the probability of staying the same state,
    \item $R^g(s, a) := \mathbb{E}_{\hat{P}^g(s' \vert s, a)} \left[-d(s', s^{g^*})\right]$ where $d(\cdot, \cdot)$ is a distance metric in the language space (e.g. cosine similarity).
\end{itemize}


\section{Experiments}

\subsection{Minigrid}



\subsubsection{Benchmark for Simple Goals}

In this experiment, we evaluate the performance of our fine-tuned LLM under different inference settings. While our method performs offline training using unlabelled observations, this evaluation is conducted online with ground truth reward functions. All environments share a 22x22 grid and contain 9 rooms. They differ in the type, position, and colour of distractors (e.g., balls, keys, doors, boxes). Additionally, all goals in this experiment are of the type "go to the tile (x,y)", differing only in the targeted tile.



\textbf{Metrics:}
\begin{itemize}
    \item \textbf{Success Rate}: Proportion of attempts where the agent reaches the target tile within the time limit (500 steps).
    \item \textbf{Episode Length}: Average number of steps taken to reach the goal or the time limit.
    \item \textbf{Cumulative Reward}: Reward of 1 for reaching the goal, discounted by the episode length.
    \item \textbf{OOD Action Ratio}: Ratio of invalid actions (e.g., moving into a wall) to total actions.
\end{itemize}

To understand the performance of our models, we compare them against three other methods:
\begin{itemize}
    \item \textbf{Llama8} and \textbf{Llama70}: General-purpose, non-fine-tuned LLMs. They are used as baselines.
    \item \textbf{Q-learning}: Optimal policies obtained by solving the abstract MDP created by our method. These are the policies learned by our fine-tuned LLM. This method cannot generalize to new goals or environments and is restricted to "Training goals, Training environments" settings.
\end{itemize}



\begin{table}[h]
\centering
\caption{\textbf{Benchmark for the online evaluation.} The models are evaluated on goals of the type "Go to position (X,Y)". The results are averaged over 20 goals and 20 different initial states for each goal.}
\label{your-label-here}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[379]Q[117]Q[148]Q[160]Q[135]},
  cells = {c},
  hline{1,9} = {-}{0.08em},
  hline{2} = {-}{},
}
\textbf{Experiments} & \textbf{Success rate~} & \textbf{Cumulative Reward} & \textbf{Length of the episode} & \textbf{Ood action ratio}\\
\textbf{Llama8} & 16\% & 0.11 & 447 & 20\%\\
\textbf{Llama70} & 5\% & 0.03 & 481 & 31\% \\
\textbf{Q-learning}~(Training goals, Training environment) & 84\% & 0.80 & 102 & 1\%\\
\textbf{Fine-tuned Llama8} (Training goals, Training environment) & 83\% & 0.79 & 107 & 1\%\\
\textbf{\textbf{Fine-tuned Llama8}}(Testing goals, Training environment) & 31\% & 0.30 & 352 & 1\%\\
\textbf{\textbf{Fine-tuned Llama8}}(Training goals, Testing environment) & 84\% & 0.80 & 97 & 1\%\\
\textbf{\textbf{Fine-tuned Llama8}}(Testing goals, Testing environment) & 33\% & 0.31 & 345 & 1\%\\

\end{tblr}
\end{table}

The poor performance of non-fine-tuned LLMs, irrespective of their parameter count, underscores the need for our methods. Their OOD action ratio is 20 to 30 times higher than other methods, indicating that their low performance come from an inability to understand the dynamics of the minigrid environment, leading to incoherent actions. \textbf{Takeaway: General-purpose LLMs are ineffective for multi-step reasoning in an RL environment, regardless of parameter count.}

The high performance of \textbf{Q-learning} policies validates the first two steps of our method. Using unlabelled observations, we leverage LLM prior knowledge to construct abstract MDPs, which, when solved by Q-learning, produce effective policies. \textbf{Takeaway: Abstract MDPs are meaningful offline constructs that yield optimal policies which are effective during online testing.}

The comparable performance between \textbf{Q-learning} policies and \textbf{Fine-tuned Llama8} in the "Training goals, Training environment" setting confirms the success of the behavioural cloning in the third step of our method. \textbf{Takeaway: The fine-tuned LLM accurately reproduces learned policies.}

The generalisation ability of \textbf{Fine-tuned Llama8} is evaluated in three scenarios. In the "Training goals, Testing environment" scenario, where the LLM is tested on new environments unseen during training, no performance drop is observed, indicating perfect generalisation to new grids. However, in the "Testing goals, Training environment" scenario, which tests generalisation to new goals, a 50\% drop in success rate is observed, though performance is still twice that of a non-fine-tuned LLM. In the final scenario, testing both generalisations simultaneously, performance remains similar, suggesting that these generalisations are uncorrelated. \textbf{Takeaway: The fine-tuned Llama8 perfectly generalizes to new environments but only partially to new goals.}

\subsubsection{Acquiring new skills from new environments}
\begin{figure}[!tbph]
  \centering
  \begin{minipage}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/before_abstraction_minigrid.png}
    \caption{\textbf{Environement A} :  a large grid with open doors.}
    \label{fig:env_A}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/small_grid_with_door.jpg}
    \caption{\textbf{Environement B} : a small grid with locked doors.}
    \label{fig:env_B}
  \end{minipage}
    \hfill
  \begin{minipage}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/large_grid_with_door.jpg}
    \caption{\textbf{Environement C} : a large grid with open doors.}
    \label{fig:env_C}
  \end{minipage}
\end{figure}

This second experiment demonstrates the generalisation ability of the fine-tuned LLM. By learning optimal policies for diverse goals and environments, the LLM integrates the skills required to achieve these goals, enabling it to solve new goals in new environments.

\textbf{Environment A}: A large grid with 9 rooms and open doors. The training goals are "go to the tile (x,y)".

\textbf{Environment B}: A small grid with 2 rooms and a locked door. The training goals are "pick up object Z." The agent and the object are in different rooms, so the agent must pick up the key and open the door first.

\textbf{Environment C}: A large grid with 9 rooms and locked doors. The training goals are "go to the tile (x,y)." To reach most tiles, the agent must pick up the correct keys and open the doors.

In this experiment, we show that by training our LLM on both environments A and B, it can solve goals in environment C (Figure \ref{fig:env_C}). This requires the LLM to learn the skill "use a key to open a door" and apply it to new environments and goals.





\newpage
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}