\documentclass[10pt, conference]{IEEEtran}
\IEEEoverridecommandlockouts
% % The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[switch]{lineno}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{url}
\usepackage{enumitem}
\usepackage{cite}
\usepackage{hyperref}

\begin{document}

\title{An Exploratory Study on Build Issue Resolution Among Computer Science Students
\thanks{This work is supported in part by NSF projects 1736209 and 1846467, and the Google Cloud Research Credits program with the award GCP19980904. 

\textbf{Data Availability}: The dataset for this study is available on Zenodo (\href{https://doi.org/10.5281/zenodo.14885822}{https://doi.org/10.5281/zenodo.14885822}). Due to local IRB restrictions, some sensitive data may not be publicly accessible.}
}


\author{
\IEEEauthorblockN{Sunzhou Huang\IEEEauthorrefmark{1},
Na Meng\IEEEauthorrefmark{2},
Xueqing Liu\IEEEauthorrefmark{3},
Xiaoyin Wang\IEEEauthorrefmark{1}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}\textit{Department of Computer Science, The University of Texas at San Antonio}, San Antonio, USA}
\IEEEauthorblockA{\IEEEauthorrefmark{2}\textit{Department of Computer Science, Virginia Tech}, Blacksburg, USA}
\IEEEauthorblockA{\IEEEauthorrefmark{3}\textit{Department of Computer Science, Stevens Institute of Technology}, Hoboken, USA}
\IEEEauthorblockA{sunzhou.huang@utsa.edu,
nm8247@vt.edu,
xliu127@stevens.edu,
xiaoyin.wang@utsa.edu}
}

\maketitle

\input{abstract}

\begin{IEEEkeywords}
open source software, development environment, build issue resolution, education
\end{IEEEkeywords}


\input{introduction}

\section{Methodology}
Our exploratory study was designed and executed in two distinct phases to comprehensively understand build issue resolution among CS students. Each phase had a unique objective and contributed valuable insights to the overall study.

\subsection{Resolution Strategy Study}
The initial phase of our study is the resolution strategies study, focused on understanding the symptoms of unresolved build issues and analyzing strategies that CS students have employed to successfully address them. We collected data from 33 CS students across 4 OSS projects and 2 programming languages (PL) in the 2022 software testing class. We evaluated the effectiveness of these strategies and their limitations. In addition, we explored the impact of prior experience on the build success rate. This phase provided us with a deeper understanding of students' resolution strategies. RQ1-5 will be answered in this phase.

\begin{itemize}[leftmargin=*]

\item \textbf{RQ1}: As learners of OSS, are students able to accurately interpret the build issues that block build tasks?

    

\item \textbf{RQ2}: How often do build tasks end with unresolved issues?

    

\item \textbf{RQ3}: What are the common symptoms of unresolved build issues, and to what degree can they be mitigated?

    

\item \textbf{RQ4}: What strategies have students employed to successfully address these unresolved build issues, and are these strategies effective?

    

\item \textbf{RQ5}: Are there any prior experiences that may correlate with studentsâ€™ ability to resolve build issues?

\end{itemize}

\subsection{Intervention Study} Phase II is the intervention study. The insights gathered from the preceding phase guided the development of intervention, which were aimed at enhancing the success rate of CS students on build tasks with proactive measures. The intervention was performed on 22 CS students registered for the same course in 2023. We assessed the impact of the intervention on the success rate and collected feedback to identify any potential mismatches. RQ6 will be answered in this phase.

\begin{itemize}[leftmargin=*]

\item \textbf{RQ6}: Given the resolution strategies employed by students, what proactive measures can we introduce to effectively enhance their performance in resolving build issues?

\end{itemize}

\subsection{Participants and Tasks}
Our participants were all CS students registered in the Software Testing course in 2022 ($n = 33$) and 2023 ($n = 22$). This course is a cross-listed elective for senior-year undergraduates and graduate students, with Software Engineering as a prerequisite. The course, taught by two of the authors at a U.S. university, focuses on software testing approaches, test planning, test case design, and build systems with CI/CD concepts. In our study, students only need to build the released OSS and do not need a background to modify the software.

Students were required to complete the study as one of their major course projects. We designed a three-stage task list to help participants become familiar with the study process:

\begin{enumerate}[leftmargin=*]

\item \textbf{Setup:} Participants were instructed to use the Google Cloud Platform (GCP) online console to set up a GCP project with \$50 education credits and enabled APIs for creating VMs. Then, custom images were shared with participants on GCP. Additionally, we obtained admin permission to access participants' GCP projects for further data collection.

\item \textbf{Warm-up:} Participants were instructed to use the warm-up custom image to create a VM instance. They then performed warm-up build activities in the VM, such as executing a ``Hello World'' build script. Participants learned to use the preinstalled script tool to log the VM state and then write a textual build report. They were also asked to complete a survey to provide feedback to researchers.

\item \textbf{Build OSS projects:} Participants were assigned OSS projects as their build task. They used preinstalled scripts and textual issue reports to report issues encountered while building and executing tests of the assigned project. At the end of this stage, participants completed a survey to share their feedback.

\end{enumerate}

All these OSS projects were in the release version and had been verified by the two authors who taught this course. The anticipated outcomes included compiling the assigned OSS projects and passing all tests using the commands specified in the build tasks. In addition to the build outcome, their grades would be evaluated based on their investment in completing the tasks through monitored VMs and issue reports. As the conductors of the study, our role was to help clarify the objectives of the build tasks. Our study protocol was assessed and approved by our local IRB. All students were notified that they could withdraw their data after the grading so that their data would not be included in our dataset for our study or any subsequent research activities.

Over a span of two years, our study involved a diverse set of students in a variety of tasks. Specifically, the participation was marked by 39 and 25 students in each phase, respectively. In 2022, we had a total of 33 participants who completed all tasks related to our resolution strategies study. The following year, in 2023, the number stood at 22 for the intervention study. Despite the fluctuation in participant numbers, the prerequisites and design of our course remained consistent.

\subsection{Data Collection}
We designed a framework that collects data throughout the entire build process, as illustrated in Figure~\ref{fig:framework}. While participants perform build activities on a custom VM and report build issues, all terminal activities are recorded. When a build task ends, the final state of the VM is preserved as a snapshot. The feedback survey is collected after all build tasks are completed. In Figure~\ref{fig:framework}, we denote active data collection with the upper yellow rounded rectangle and passive data collection with the lower blue rounded rectangle.

\begin{itemize}[leftmargin=*]
\item \textbf{Custom VM:}
In this study, we customized the official GCP OS images, which offer basic OS setups. We incorporated a custom script at OS startup to record terminal activities and provided a tool that logs the current state of the VM. These custom images ensure that all participants have consistent base build environments.
\item \textbf{VM state logging:}
After encountering build issues, participants log the current VM state using a script containing Linux commands to record command history, environment variables, and network information. This logging process, integral to the issue reporting, helps match logged data with reported issues. Additionally, participants log the final VM state upon completing the build tasks. All logs are saved in VMs for later retrieval, aiding in issue diagnosis and reproduction.
\item \textbf{Terminal recording:}
We opted for terminal recording as it captures all terminal activities, making the collected data easier to analyze. Using the \textit{script} command in Linux~\cite{script}, we integrated terminal recording into the VM's startup process. Once participants connect to the VM via SSH, the recording begins, capturing all screen outputs and keyboard inputs, including activities like file editing. While terminal recording lacks the ability to extract executable commands from inputs, it offers automated and comprehensive data collection without requiring manual initiation by participants. Despite this limitation, terminal recording facilitated playback and review of the entire build process, aiding in the analysis and reproduction of encountered build issues.
\item \textbf{Snapshots of VMs:}
In addition to terminal recording, another option for collecting VM data is taking snapshots. A snapshot captures the entire VM at a specific moment, including files, settings, and configurations. While useful for troubleshooting and reproducing issues, snapshots lack playback functionality and erase temporary environment variables. To address cost and practical concerns, we decided to utilize an automated script to capture snapshots only upon the build tasks reaching the final state, whether it be success or failure.
\item \textbf{Build issue report:}
Throughout the completion of all tasks, participants were directed to utilize the build issue report to document encountered issues during the build process, including inputs, outputs, and the solutions employed for resolution. Additionally, participants documented additional details of the build environment, such as program language versions and utilized tools. Upon completion of the build, participants summarized the entire process and provided their thoughts. The build issue report offers valuable insights into their decision-making and problem-solving strategies.

\item \textbf{Feedback Survey:}
After completing the build process and documenting the encountered issues and their corresponding solutions, we conducted a feedback survey to gather insights from participants. This survey focused on human factors affecting the build process, such as participants' familiarity with the PL, build system, and OS. Additionally, we collected self-reported build outcomes (success or failure). Insights collected from the comment section, which included obstacles and reflections, further aided in uncovering the challenges faced by participants during the build process.
\end{itemize}

\begin{figure}[tbp]
\centerline{
\includegraphics[width=\linewidth]{figures/framework.png}
}
\vspace{-.2cm}
\caption{Data Collection Framework for Build Tasks}
\vspace{-.4cm}
\label{fig:framework}
\end{figure}

After screening out participants who did not complete all tasks, we collected the following data for each phase:

\begin{enumerate}[leftmargin=*]
  \item \textbf{Resolution Strategies Study}: A total of 198 build results with snapshots containing terminal recordings and log files for the entire build process. In addition to 33 textual build issue reports with feedback surveys.
  \item \textbf{Intervention Study}: A total of 132 build results with snapshots containing terminal recordings and log files for the entire build process and 22 textual build issue reports along with feedback surveys.
\end{enumerate}

\subsection{Qualitative and Quantitative Data Analysis}
\subsubsection{Resolution Strategies Study}
\label{analysis2022}
To address \textbf{RQ1-2}, the first author manually verified the build outcomes reported by participants, cross-referencing them with terminal recordings, command histories, and environment information extracted from snapshots. If a build issue occurred that prevented participants from completing the build task, the error message was extracted, and it was labeled as \textit{unresolved}. Subsequently, all snapshots from the final state VM were converted into running VMs to reproduce the collected error messages. This was done using an automatic script, with necessary adjustments made for specific build issues. If there was an identified inconsistency in reporting, we refined the error message and resolution state. If a build issue could not be reproduced, the original error message was retained. The last author validated the verified build outcomes and the extracted error messages. If there were any conflicts, a discussion was initiated until a consensus was reached.
To address \textbf{RQ3-4}, two authors initially followed the open coding procedure~\cite{Seaman_1999} to label unresolved build issues with their root symptoms. These root symptoms were identified by examining the entire build process, which was based on terminal recordings and command histories. If any conflicts arose, further verification was performed on a running VM created from the snapshot, and a discussion was initiated until a consensus was reached. Next, based on the error messages, two authors counted the number of resolved build issues across all successful build tasks. We only include issues in successful build tasks to eliminate the uncertainty associated with chained failures, where a new failure might mask an old one. In addition, two authors identified common approaches that resolved these issues.
To address \textbf{RQ5}, we first mapped the answers from the 5-point Likert scale of familiarity questions to an ordinal scale, ranging from 1 to 5 (with 1 representing ``Not familiar at all'' and 5 representing ``Extremely familiar''). We then converted the build outcomes to a binary scale (1 for success, 0 for failure). Following this, we calculated the Spearmanâ€™s rank correlation coefficient to investigate any potential relationship between prior experiences and build outcomes. The significance of the relationship was interpreted with the $p$-value.

\subsubsection{Intervention Study}
To address \textbf{RQ6}, we performed the same verification process as in the resolution strategies study to obtain the verified build outcomes for the intervention study. We compared the build success rates before and after the intervention, and used the Chi-squared test ($\chi^2$) to determine if the two rates were significantly different. Furthermore, aside with a self-reported 5-point scale for helpfulness (ranging from 1-5, with an option for ``not applicable''), we investigated the barriers mentioned in textual feedback survey during the intervention study to identify the impact of intervention.

% Table generated by Excel2LaTeX from sheet 'Project Table'

\begin{table}[tbp]
  \centering

  \caption{OSS projects in resolution strategies study}
  \vspace{-.2cm}
    \resizebox{0.8\linewidth}{!}{\begin{tabular}{lllll}
    \toprule
    Project & Version & PL    & Build Sys. & Repository \\
    \midrule
    \midrule
    Guava & 31.1  & Java  & Maven & google/guava \\
    Spring Boot & 2.6.12 & Java  & Gradle & spring-boot \\
    \midrule
    OpenCV & 4.6.0 & C++   & Cmake & opencv/opencv \\
    Bitcoin & 23    & C++   & Autotools & bitcoin/bitcoin \\
    \bottomrule
    \end{tabular}}%
    \vspace{-.4cm}
  \label{tab:project}%
\end{table}%

\section{Resolution Strategies Study}
\label{sec:results22}
To answer RQ 1-5, we conducted a study on resolution strategies in 2022 and analyzed 198 fully monitored build results from 33 participants.

In a background survey conducted in 2022, we found that the top four PLs our participants were familiar with are Java (100\%), Python (92.3\%), C/C++ (76.9\%), and JavaScript (64.1\%). Given that Java was the default PL for other course projects and considering that C++ requires extra compilation during the build process, we selected Java and C++ as our target PLs. We selected OSS projects from GitHub~\cite{github}. After filtering out projects that were tutorials, involved mixed PLs, or were not in English, we chose the most popular projects for two different build systems for each PL. This resulted in four OSS projects, as shown in Table~\ref{tab:project}. We believe that the build processes and challenges of the most popular OSS projects on GitHub can be largely generalized to other projects using the same PL and build system. As the study by Lou et al.~\cite{lou2020understanding} observed the impact of different OS on build issue resolution, we further selected three Linux distributions (Ubuntu 22.04 LTS, CentOS Stream 9, and openSUSE Leap 15.4) based on their popularity. We verified all projects on each OS to ensure they were ready to build and excluded some time-consuming test cases to match the computing resources available to participants. 

We randomly divided the participants into two groups. The group, which worked on the Guava-Java and OpenCV-C++ projects, ended up with 14 participants. The other group, which worked on the Spring Boot-Java and Bitcoin-C++ projects, concluded with 19 participants. All participants began with the Java projects and then proceeded to complete the C++ projects. The project guidelines for the Java and C++ projects are identical, with the exception that we included an estimated build time and machine types for the C++ projects. Despite having trained the participants on the configuration of VMs on GCP, we still observed that some participants working on the Spring Boot project complained about the long build time due to incorrectly assigning system resources to VMs. Considering that this situation could have a significant negative impact on the completion of the future tasks, we added a recommended configuration for system resources to address the insufficient system resource issues for the C++ projects.



\subsection{As learners of OSS, are students able to accurately interpret the build issues that block build tasks? (RQ1)}
\label{sec:RQ1}
We manually verified the 198 build outcomes reported by 33 participants using the approach mentioned in Section \ref{analysis2022}. We observed the following inconsistencies in reporting:
\begin{enumerate}[leftmargin=*]
    \item Misinterpretation of build results (31 out of 198): Participants misinterpreted the failed build results as successful. This was extensively observed in OpenCV (24, 57.1\%).
    \item Incorrect build command (1 out of 198): A wrong build command was used, resulting in a partial build outcome.
    \item Silent failure (2 out of 198): Some test cases were not executed due to a silent crash.
\end{enumerate}
In total, 34 out of 198 build outcomes were misinterpreted, resulting in a misinterpretation rate of 17.2\%. We believe this observation occurred due to two main reasons.
Firstly, the OpenCV project required non-standard environment variables to pass all the test cases. As participants noted that the failed test cases were only a small portion of the passed test cases, they tended to believe it was a successful build. This could pose a serious problem if the missing tests were to catch potential defects in the software.
Secondly, students lacked the background knowledge to fully interpret the build results since they had never seen a successful build of the new project before. They tended to scan the keywords in the build log, which led to the ignoring of silent failures.

\subsection{How often do build tasks end with unresolved issues? (RQ2)}
\label{sec:RQ2}
Table~\ref{tab:success} presents the success rate of build outcomes verified with the correction of inconsistent reporting. Spring Boot has the lowest success rate among the four PLs because it suffered significantly from insufficient system resources. OpenCV has the second-lowest success rate, mainly because most participants neglect a non-standard environment variable. openSUSE is not a popular OS in the documentation of our OSS projects, which leads to the lowest success rate of 30.3\%. The overall success rate is 45.5\%, indicating that more than half of the build tasks are blocked by unresolved issues.

% Table generated by Excel2LaTeX from sheet 'success rate'
\begin{table}[tbp]
  \centering
  \caption{Success rate of verified build outcomes}
  \vspace{-.2cm}
    \resizebox{0.9\linewidth}{!}{\begin{tabular}{l|rrr|r}
          & \multicolumn{1}{l}{Ubuntu} & \multicolumn{1}{l}{CentOS } & \multicolumn{1}{l|}{openSUSE } & \multicolumn{1}{l}{3 OS} \\
    \midrule
    Guava - 14 & 92.9\% & 100.0\% & 92.9\% & 95.2\% \\
    Spring Boot - 19 & 26.3\% & 15.8\% & 10.5\% & \textbf{17.5\%} \\
    Java - 33 & 54.5\% & 51.5\% & 45.5\% & 50.5\% \\
    \midrule
    OpenCV - 14 & 35.7\% & 35.7\% & 35.7\% & \textbf{35.7\%} \\
    Bitcoin - 19 & 68.4\% & 63.2\% & 0.0\% & 43.9\% \\
    C++ -33 & 54.5\% & 51.5\% & 15.2\% & 40.4\% \\
    \midrule
    Java \& C++ - 66 & 54.5\% & 51.5\% & \textbf{30.3\%} & \textbf{45.5\%} \\
    \end{tabular}}%
  \label{tab:success}%
  \vspace{-.4cm}
\end{table}%



\subsection{What are the common symptoms of unresolved build issues, and to what degree can they be mitigated? (RQ3)}
\label{sec:RQ3}
In unresolved build issues, we identified 18 unique errors, as detailed in Table~\ref{tab:strategy}. Furthermore, we analyzed the resolved build issues that contain these 18 unique errors in successful build outcomes. In Table~\ref{tab:strategy}, ``\#UR'' represents the number of unresolved issues, while ``\#R'' denotes the number of resolved issues related to the 18 unique errors. Figure~\ref{fig:symptoms22} and Table~\ref{tab:strategy} illustrate these root symptoms, which can be categorized by Environment-Related (\textit{Incompatible} ... and \textit{Missing} ...), Process-Related (\textit{Misuse command}), System-Related (\textit{Insufficient system resources}), and Test-Related (\textit{Style violation} and \textit{Test case failure}).

The number of unresolved incompatible and missing issues indicates that environment-related issues constitute a significant portion of the failed build tasks. The build issues from the OpenCV project, which lacked a non-standard environment variable, contributed to the \textit{Missing environment variable}. Participants who built the Spring Boot project encountered a severe \textit{Insufficient system resources}. Many participants made mistakes by forgetting to configure the Makefile, leading to \textit{Misuse command}. 

We observed that the symptoms were not uniformly distributed. Consequently, students often encountered difficulties in resolving build issues caused by these recurring symptoms. Compared to the study by Lou et al.~\cite{lou2020understanding}, our study shows that students still frequently experience the same symptoms as developers. However, these are mostly limited to issues caused by a lack of installation, version incompatibility errors, and environment variable issues. We also observed additional symptoms not covered by previous studies that are related to studentsâ€™ actions, such as \textit{Misuse command} and \textit{Insufficient system resources}.

\begin{figure}[tbp]
\centerline{
\includegraphics[width=\linewidth]{figures/symptoms22.jpg}
}
\vspace{-.2cm}
\caption{Root symptoms from resolution strategies study}
\vspace{-.4cm}
\label{fig:symptoms22}
\end{figure}

% Table generated by Excel2LaTeX from sheet '2022 table'
\begin{table*}[htbp]
  \centering
  \caption{Errors, symptoms and resolutions}
    \resizebox{\linewidth}{!}{\begin{tabular}{lp{13em}p{40em}llr}
    \toprule
    Project & \multicolumn{1}{l}{Symptom} & \multicolumn{1}{l}{Error \& Description} & \#UR  & \#R   & \multicolumn{1}{l}{Resolution} \\
    \midrule
    Guava & \multicolumn{1}{l}{Insufficient Sys. Res.} & E1: Failed to execute goal org.apache.maven.plugins: maven-surefire-plugin:2.7.2:test (default-test) on project guava-testlib : There are test failures & 2     & 0     &  \\
    \midrule
    \multicolumn{1}{p{5.355em}}{Spring-Boot} & \multicolumn{1}{l}{Insufficient Sys. Res.} & E2: Gradle build daemon disappeared unexpectedly (it may have been killed or may have crashed) & 4     & 1     & \multicolumn{1}{l}{Retry with incremental compilation} \\
          & \multicolumn{1}{l}{} & E3: Build is slow & 7     & 0     &  \\
          & \multicolumn{1}{l}{} & E4: Build crash & 10    & 2     & \multicolumn{1}{l}{Retry with incremental compilation} \\
\cmidrule{2-6}          & Incompatible PL (Java) & E5: Execution failed for task ':spring-boot-project: spring-boot:compileKotlin' & 4     & 1     & \multicolumn{1}{l}{Use JDK instead of JRE} \\
          & \multicolumn{1}{l}{} & E6: Execution failed for task ':spring-boot-project: spring-boot:javadoc' & 11    & 0     &  \\
          & \multicolumn{1}{l}{} & E7: Execution failed for task ':buildSrc:test' & 1     & 0     &  \\
          & \multicolumn{1}{l}{} & E8: Execution failed for task ':spring-boot-project: spring-boot:test'. 4 test failures & 1     & 0     &  \\
\cmidrule{2-6}          & Missing PL (Java) & E9: ERROR: JAVA\_HOME is not set and no 'java' command could be found in your PATH & 2     & 7     & \multicolumn{1}{l}{Install Java} \\
\cmidrule{2-6}          & \multicolumn{1}{l}{Style violation} & E10: Execution failed for task ':checkstyleNohttp' & 5     & 0     &  \\
\cmidrule{2-6}          & Test case failures (Flaky) & E11: Execution failed for task ':spring-boot-project: spring-boot:test'. flaky test cases & 2     & 0     &  \\
    \midrule
    \multicolumn{1}{p{5.355em}}{OpenCV} & \multicolumn{1}{l}{Missing Env. Var.} & E12: 7 FAILED TESTS & 27    & 12    & \multicolumn{1}{l}{Set up OPENCV\_TEST\_DATA\_PATH} \\
    \midrule
    \multicolumn{1}{p{5.355em}}{Bitcoin} & \multicolumn{1}{l}{Insufficient Sys. Res.} & E3: Build is slow & 3     & 1     & \multicolumn{1}{l}{Wait patiently} \\
          & \multicolumn{1}{l}{} & E4: Build crash & 1     & 1     & \multicolumn{1}{l}{Build in background } \\
\cmidrule{2-6}          & Missing build tool (Make) & E13: make: command not found & 5     & 5     & \multicolumn{1}{l}{Install Make} \\
\cmidrule{2-6}          & Misuse command (no configure) & E14: make: *** No rule to make target 'check'.  Stop & 9     & 19    & \multicolumn{1}{l}{Configure Makefile} \\
\cmidrule{2-6}          & \multicolumn{1}{l}{Missing Dep.} & E15: Libtool library used but 'LIBTOOL' is undefined & 1     & 11    & \multicolumn{1}{l}{Install Libtool library } \\
          & \multicolumn{1}{l}{} & E16: configure: error: C++ compiler cannot create executables & 3     & 8     & \multicolumn{1}{l}{Install build-essential} \\
\cmidrule{2-6}          & \multicolumn{1}{l}{Incompatible Dep.} & E17: Segmentation fault & 1     & 0     &  \\
\cmidrule{2-6}          & Incompatible PL (G++) & E18: configure: error: cannot figure out how to use std::filesystem & 9     & 0     &  \\
    \bottomrule
    \end{tabular}}%
  \label{tab:strategy}%
  \vspace{-.4cm}
\end{table*}%

\subsection{What strategies have students employed to successfully address these unresolved build issues, and are these strategies effective? (RQ4)}
\label{sec:RQ4}
Based on the resolved build issues related to the unresolved build issues (observed in RQ3),  we studied the resolutions employed by students and summarized them into two strategies:  \textit{Brute Force} and \textit{Ad Hoc}, which seem not very effective as the overall resolution rate ($\frac{\sum \#R}{\sum (\#UR+\#R)}$) is  38.6\%.

\textbf{Brute Force:} This strategy has been applied to most instances of the \textit{Insufficient system resources} build issue. Benefiting from the incremental compilation feature of Gradle in the Spring Boot project, a few participants who encountered system-related build issues were able to overcome low resource issues. However, for most participants who suffered low resource issues, this strategy tended to fail. Our observations indicate that most participants were overly focused on this strategy, and the failure of each retry led to a tremendous sense of frustration.

\textbf{Ad Hoc:} Most participants applied the \textit{Ad Hoc} strategy, which tended to be effective for \textit{missing}-related symptoms. Participants simply needed to install the component that was reported as missing in the error message. However, for non-obvious symptoms, such as the \textit{missing environment variable} symptom, it was easy for participants to overlook potential failures, not to mention the incompatibility issues. This strategy may further increase the incidence rate of encountering \textit{trap issues}.

We further observed that some build issues, referred to as \textbf{trap issues}, can be the root cause of the low effectiveness of studentsâ€™ strategies. Based on the two common strategies, we identified several issues that tend to lead to a failed build for students. For instance, the issues with E5 can be resolved by changing the Java Runtime Environment (JRE) to the Java Development Kit (JDK), as indicated in the detailed error message ``Make sure Kotlin compilation is running on a JDK, not JRE.''. However, changing from JRE to JDK is more complex than installing JDK initially. Even with this explicit error message, other participants were still unable to resolve the issue. Another significant example is the issue associated with E10. In this scenario, participants downloaded the JDK package directly into the root directory of the Spring Boot project. This action led the Checkstyle tool to inadvertently verify the JDK package, which ultimately resulted in the test crashing.

In essence, \textit{trap issues} are build issues that contradict studentsâ€™ intuition. Consequently, build issues with a low resolution rate are considered \textit{trap issues}. Predicting \textit{trap issues} without sufficient information can be challenging. In this study, the incompatible issues and style violation issues can be identified as \textit{trap issues} since most of them have a low resolution rate. The official documents of OSS usually donâ€™t contain recommended system resource information for builds. Therefore, the \textit{insufficient system resources} build issue can be identified as a trap issue for students. Neither \textit{brute force} nor \textit{ad hoc} strategies can effectively resolve these \textit{trap issues}, turning studentsâ€™ build activities into a game of chance.



\subsection{Are there any prior experiences that may correlate with studentsâ€™ ability to resolve build issues? (RQ5)}
\label{sec:RQ5}



Figure~\ref{fig:farmiliarity} presents the distribution of familiarity, ranging from 1 to 5 (with 1 representing ``Not familiar at all'' and 5 representing ``Extremely familiar'').
Most participants are more familiar with Java and Ubuntu than with other PLs and OSs. We calculated the Spearmanâ€™s rank correlation coefficient between each type of familiarity and the build outcomes.
Figure~\ref{fig:corr} presents the coefficients, where ``F\_'' denotes familiarity and ``B\_'' denotes build outcome.

\begin{figure}[bp]
\vspace{-.4cm}
\centerline{
\includegraphics[width=.9\linewidth]{figures/familiarity.pdf}
}
\vspace{-.4cm}
\caption{Distribution of prior experiences}
\label{fig:farmiliarity}
\end{figure}

Most prior experiences have a very weak correlation with build outcomes. However, the prior experience with C++ and the build outcome of OpenCV have a moderate negative correlation (-0.55, $p$-value$<$0.001). We further investigated the collected data and confirm the trend: participants who are more familiar with C++ tend to fail more often in the OpenCV project (Success rates: Not familiar at all=66.7\%, Slightly familiar=50\%, Moderately familiar=0\%, Very familiar=0\%).
Since the majority of participants misreport their OpenCV build outcomes, a potential reason could be that participants more familiar with C++ tend to overlook details in the official documents, which leads to failures with E12.

\begin{figure}[tbp]
\centerline{
\includegraphics[width=\linewidth]{figures/corr.pdf}
}
\vspace{-.4cm}
\caption{Spearmanâ€™s rank correlation of prior experiences on build success}
\label{fig:corr}
\vspace{-.4cm}
\end{figure}

The rest of the prior experiences, such as PLs, build systems, and OSs, do not show significant correlation. As correlation does not imply causation, the impact of prior experiences on studentsâ€™ ability to resolve build issues may still need further exploration.




\section{Intervention Study}
\label{sec:results23}
In our resolution strategies study, we observed that participants frequently fell into \textit{trap issues}. We believe this is the main reason why students were unable to resolve the build issues. We hypothesize that preventing these \textit{trap issues} could be a more effective strategy than the common strategies currently employed by students. To test this hypothesis, we conducted an intervention study in 2023, where we added additional references to the project guidelines for participants. To be practical and follow most scenarios of using OSS in class, we allow students to use any external references they can find in both phases. The official documents were already provided in the first phase. As previously noted, the previous group that worked on the Spring Boot (Java) and Bitcoin (C++) projects encountered a wider variety of symptoms. Therefore, for 2023 participants, we selected Spring Boot and Bitcoin projects with the same release versions. We kept the rest of the study setup as identical as possible.

\subsection{Given the resolution strategies employed by students, what proactive measures can we introduce to effectively enhance their performance in resolving build issues? (RQ6)}
\label{sec:RQ6}
The additional references provided for the intervention are as follows:

\begin{itemize}[leftmargin=*]
    \item \textbf{Official guide links with ``how to build''}: in the 2022 study, we only provided links to general official guides. However, in the 2023 study, we improved this by providing links specifically to the ``how to build'' sections.
    \item \textbf{Example output for successful build results}: the screenshots demonstrate successful build outcomes. We also clarified the unrelated error message that appears in the CI reporting for the Spring Boot project.
    \item \textbf{Recommended PL versions}: we extract recommended PL versions from official documents and list them explicitly.
    \item \textbf{Dependencies}: information about how dependencies are required to be installed in different build systems, with links to the dependencies sections in official documents.
    \item \textbf{JDK vs. JRE}: we provide a link to the difference between JDK and JRE in Java, and remind that the package name might be different in Linux distributions.
    \item \textbf{CheckStyle}: background knowledge of the Checkstyle plugin and what it will do to a Spring Boot project.
    \item \textbf{Recommended system resource configuration for VMs}: as we mentioned in our study on resolution strategies, the information was initially provided only for C++ projects. In this phase, we have also extended it to Java projects. configuration
\end{itemize}

% Table generated by Excel2LaTeX from sheet 'Success Rate'
\begin{table}[tbp]
  \centering
  \caption{Build Outcomes with intervention}
    \resizebox{\linewidth}{!}{\begin{tabular}{lllll}
    \toprule
          & \multicolumn{2}{c}{Spring Boot - Java} & \multicolumn{2}{c}{Bitcoin - C++} \\
          & $\chi^2=64.2$ & $p<.001$ & $\chi^2=17.6$ & $p<.001$ \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}          & Success & Failure & Success & Failure \\
    \midrule
    Baseline - 57 & 10 (\textbf{17.5\%}) & 47 (82.5\%) & 25 (\textbf{43.9\%}) & 32 (56.1\%) \\
    Intervention - 66 & 60 (\textbf{91\%}) & 6 (9\%) & 54 (\textbf{82\%}) & 12 (18\%) \\
    \bottomrule
    \end{tabular}}%
    \vspace{-.4cm}
  \label{tab:interverntion}%
\end{table}%

As shown in Table~\ref{tab:interverntion}, the success rates for both projects were improved by intervention. The success rates of Spring Boot and Bitcoin improved from 17.5\% to 91\% and 43.9\% to 82\%, respectively. These improvements are statistically significant (both $p$-values of the Chi-squared test are less than 0.001).

\begin{figure}[bp]
\vspace{-.6cm}
\centerline{
\includegraphics[width=.9\linewidth]{figures/helpfulness.pdf}
}
\vspace{-.4cm}
\caption{Helpfulness rating for intervention (diamond markers indicates means)}

\label{fig:help}
\end{figure}

Figure~\ref{fig:help} presents the box plot of helpfulness rating from participants about intervention, ranging from 1 to 5, with an option for ``not applicable''. The example output (success output) outperforms other references. The search engines and official guides also have similar means. The references to PL versions and dependencies are helpful, and students also explicitly mentioned their usefulness in the barriers section of the feedback surveys. References to ``JDK vs. JRE'' and ``CheckStyle'', which are not found in official documents, rated to be less helpful than others. However, our observations from the feedback surveys indicate that participants did benefit from these external references. We did not observe any Checkstyle-related issues among the unresolved issues, and two participants explicitly stated that the barrier for them is the difference between JRE and JDK.

For the Spring Boot project, all six unresolved issues are incompatibility issues, which are the same as the \textit{trap issues} identified in our 2022 study. One of the six unresolved issues was actually resolved after rebooting the VM. The official OSS documents usually donâ€™t include recommended system resource information for builds. Since we provided the recommended system resource configuration in this phase, none of the participants reported symptoms of insufficient system resources. The intervention on this OSS project was observed to reduce both \textit{trap issues} and other build issues. However, self-reported helpfulness does not necessarily equate to actual effectiveness for all students.

For the Bitcoin project, we observed that 1 of the 12 issues was due to low system resource. Also, a \textit{Misuse command (no configure)} issue was observed in the 12 unresolved issues. 10 of the 12 unresolved issues were related to incompatibility, 9 of which occurred in the openSUSE OS. In the openSUSE, for the Bitcoin project, the occurrence rate of \textit{trap issues} (incompatible) reduced from 52.6\% (10/19) to 40.9\% (9/22), and other build issues were fully resolved after intervention. As the recommended system resource information was provided to students in all phases of the Bitcoin project, the intervention was observed to reduce both trap issues and other build issues, regardless of the system resource limits.

When looking at each student individually, all the references provided are beneficial to them. This is because every reference has been rated with a highest helpfulness score of either 4 or 5, as shown in Figure~\ref{fig:help}. Most of these helpful references can be easily obtained from an OSS project and used as proactive measures, such as well-organized official guides, recommended PL versions, and dependencies. The success output is difficult to extract from documents but could be accessed from the CI system or previous build logs. Identifying and presenting project-specific knowledge, such as ``JDK vs. JRE'' and ``CheckStyle'', to students as a proactive measure poses a challenge. Despite often being overlooked by document contributors, these project-specific knowledge may still play a critical role in preventing trap issues for students.

\section{LESSONS LEARNED}
\label{sec:lesson}
We categorize our lessons learned into five subsections based on the various stakeholders in the OSS and CS education communities.

\subsection{Educators}

As a general finding from our study, students often face build issues when they try to build real-world OSS projects. Therefore, educators should be aware of such challenges whenever they adopt real-world OSS projects in their SE course projects.

As students are often unfamiliar with the OSS build scenario and lack project-specific experience, misinterpretations of build issues do occur. Section~\ref{sec:RQ1} reveals a 17.2\% misinterpretation rate in our exploratory study, which is not negligible. A list of commonly faced build error messages and their solutions could help students and prevent them from struggling without understanding the root causes. Furthermore, if educators rely on studentsâ€™ self-reported evaluations to design or adjust SE courses related to OSS builds, additional clarification or deeper investigation should be performed to address this mismatch.

The educational use of OSS in CS courses allows students to work with real-world projects, making it an important aspect of the SE curriculum. However, as Section~\ref{sec:RQ2} shows, build issues can pose significant barriers to the successful integration of OSS in course projects. Studentsâ€™ strategies for resolving build issues can be vulnerable, and they often fall into \textit{trap issues}, which require significant time and effort to resolve. Therefore, educators should select reliable OSS projects, identify \textit{trap issues} early, and implement proactive measures to mitigate the difficulties associated with the OSS projects build.

In the meantime, Section~\ref{sec:RQ6} shows that there might not be a silver bullet for proactive measures, so continuing to develop these measures could be a more viable approach. Additionally, Section~\ref{sec:RQ5} reveals that prior experiences may not apply to builds. If OSS builds tend to be project-specific, gathering project-specific experiences from student feedback or key information from extensive OSS documents could provide better assistance to students than relying solely on their expertise and background.

\subsection{Researchers}
Build issues often arise while students work on OSS projects, presenting both a challenge and a research opportunity. Section~\ref{sec:RQ3} shows that these build issues are not only similar to a subset of developersâ€™ build issues but can also be unique from a studentâ€™s perspective, warranting deeper exploration across a wider range of OSS subjects. Identifying \textit{trap issues} is essential for addressing build issues. However, in cases where students have limited information, such as when they are working on an OSS project newly integrated into a CS course, it may be difficult for them to identify these issues. Future research could focus on developing more precise and effective methods for identifying \textit{trap issues} so that students can avoid them and educators can prepare guidance on how to bypass them. Additionally, exploring the impact of prior experiences on build processes, as indicated by Section~\ref{sec:RQ5}, could be another area for future research. Collecting a more refined dataset and involving a larger group of students may yield different results. Proactive measures have been proven effective in reducing build issues, but not all measures can be easily created without project-specific knowledge or key information. Therefore, research focused on extracting information for proactive measures is worth exploring.

According to Section~\ref{sec:RQ1}, inconsistencies in studentsâ€™ self-reported build issues cannot be ignored. To avoid these threats, it is more effective to monitor the process in real-time rather than reproduce it later. While verifying the reported data can be time-consuming, it may be necessary.


\subsection{Students}

As students engage in OSS build activities, they take on the role of learners. To avoid common pitfalls, several pieces of advice can be beneficial. Previous experiences with PLs or build systems may not be relevant to the current OSS project. Therefore, it is important to rely on the key information from OSS documentation. Students should be encouraged to try different strategies, such as resetting the build environments, even if they are not comfortable with them. Intuitive strategies might lead to deeper issues. Sometimes, students need to think outside the box, as some issues may be caused by external resources or even flaky tests. Recognizing their own inexperience is crucial, which includes accepting the possibility of misinterpretation and a lack of project-specific knowledge. This means that students may need additional support while building OSS, even if they appear confident in related areas. If possible, using a popular OS can improve the build success rate.

Students' experiences of build failure can be valuable if reported to educators or OSS contributors, as this feedback can help create proactive measures for future students.

\subsection{OSS contributors}

OSS contributors should recognize the increasingly significant role that OSS plays in CS education, which also facilitates students' participation in OSS projects. Given that students may not have the required knowledge, particularly about OSS projects builds, mutual understanding of different roles within the OSS community is critical. Adding some external references, even if they pertain to basic background knowledge, can be helpful for learners. Not only are project-specific experiences valuable for other roles, but well-organized documentation and public build information are also crucial. Our study finds that error logs from the compiler or build systems are often too general to be useful and can sometimes be misleading. OSS contributors should consider supplementing them with more project-specific information to avoid misinterpretations. Build error messages are also sometimes hidden due to the complicated build process (e.g., the compilation process was called by a script but the log was not preserved or passed to the caller process). So build scripts need to be designed and tested more carefully to explicitly show error messages.

The project-specific experiences of OSS contributors can lead to the development of proactive measures that ultimately benefit OSS itself. Understanding studentsâ€™ needs can help them become potential contributors in the future.

\subsection{Generative AI}
As generative AIs became increasingly popular in 2023, we noted that using AI services may impact our intervention study compared with common external references found by search engines. We did not limit studentsâ€™ use of any external references in both phases to be practical. Figure~\ref{fig:help} shows that students rated generative AIs as the least helpful (lowest mean). To understand the reason behind this observation, we further case-studied the AI services by querying some error messages on ChatGPT 3.5 and 4~\cite{openai2024chatgpt} in February 2024. The results of the case study align with the studentsâ€™ feedback. For example, we tested the error ``Execution failed for task :spring-boot-project:spring-boot-tools:spring-boot-antlib:javadoc.'' ChatGPT 3.5 gave 10 solutions, but only one of them mentioned the real root cause (JDK compatibility). ChatGPT 4 gave 3 unrelated solutions. The reason could be that AI services need key information to provide accurate suggestions in build issue resolution scenarios, but students may lack the experience to extract such information as input to AI services. Therefore, the answers from AIs were often overly general for students to find them helpful, because (1) so many different solutions may make students confused about which direction they should move to, and (2) the solutions are high level and do not include concrete actionable steps for the students to follow. In this OSS build scenario, future work may include adding more context to generative AIs, such as providing additional references to generative AIs.
\section{Threats to Validity}
\subsection{Internal Validity}
The major threat to the internal validity of our study is the reliance on self-reported data and manual verification processes. There may be errors in the process of data recording and analysis. To reduce this threat, we introduced a conflict resolution procedure and followed the open coding~\cite{Seaman_1999}. In addition, we leverage multiple information channels (e.g., reports, screenshots, command logs, and terminal recordings) for data recording and cross-validate the data among these channels.

Our study involving students who are one year apart can introduce a threat to internal validity. To mitigate this threat, we ensure that the curriculum and teaching methods remain consistent across the years to minimize differences in educational content. We use the same version of OSS projects and task setup and employ quantitative methods to analyze the overall build success rate. By collecting feedback from students, we aim to identify and address any potential issues that could affect the validity of our findings.

To ensure practicality, we permitted students to use external references, such as search engines, which may introduce threats to our study. Additionally, we did not track the usage of system resources, and providing a recommended system resource configuration to reduce the impact of build issues on low system resources could potentially weaken our findings. To mitigate these threats, we diligently analyze the collected data and investigate the details of each build issue in conjunction with our findings.

\subsection{External Validity}
The major threat to the external validity of our study is that our findings may not be generalizable to other students and OSS projects. To reduce this threat, we use popular OSS on GitHub~\cite{github} in different PLs, different build systems, and three OS variants to increase their representativeness. Given the exploratory nature of our study, our findings may not be generalizable to other institutions and courses that have different characteristics, such as varying enrollment capacities and diverse student demographics. Future research should consider these factors to enhance the applicability of the results across different educational settings.

\input{related}

\section{Conclusion}
\label{sec:conclude}
We present a dual-phase exploratory study involving 55 CS students, which includes a resolution strategies phase and an intervention phase. The aim is to gain a comprehensive understanding of build issue resolution among CS students. We investigate the reported build issues and the strategies students use to resolve them. Due to students' lack of background knowledge and project-specific experience, their intuitive strategies tend to be straightforward. This limitation can make them more susceptible to certain build issues that are hard to resolve. Proactively preventing these potential \textit{trap issues} can significantly improve the success rate of their build outcomes, thereby enhancing the OSS learning experience in the SE course. This work highlights directions for further research in this area that could benefit various stakeholders in the OSS and CS education communities.

\IEEEtriggeratref{19}
\bibliographystyle{IEEEtran}
\bibliography{build_fail}
\end{document}
