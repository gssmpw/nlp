\section{Evaluation}\label{sec:EVALUATION}

\subsection{Experimental Setup}
In this section, we discuss the process for collecting assertions for issue identification, datasets used for the empirical evaluation and user study, task description, and details of participants involved in the user study.


\subsubsection{Procedure for Collecting Assertions for Issue Identification}\label{bugconditions}

In this section, we detail our process for identifying the types of bugs supported by {\em KUnit} and explain how the corresponding assertions are developed to detect them. To identify these bugs, we conducted a thorough literature review. 
Islam \etal~\cite{islam19} investigated the type of DL bugs and categorized them into different categories, with data and model bugs being two key categories. Humbatova \etal~\cite{humbatova20taxonomy} refined the investigation and further divided data bugs into two subcategories:
training data quality and preprocessing of training data. And, the model bugs were categorized into subcategories such as wrong input, wrong tensor shape, \etc. These classifications provided a structured foundation for understanding common pitfalls in DL workflow. While some bugs reported in these studies require comprehensive end-to-end analysis, others can be effectively detected through targeted testing of specific components, such as data and model. For instance, focused component-level testing can detect crash bugs caused by wrong preprocessing and silent bugs resulting from incorrect activation functions. In contrast, issues like overfitting and underfitting depend on evaluating the model's performance on the original dataset. Thus, we focus on bugs that can be detected at the component level while excluding those that require end-to-end analysis. Similar to the procedure described in \cite{BraiekDeepChecker,ghanbari2023deepmufl}, we filtered out these bugs from the empirical studies~\cite{islam19,humbatova20taxonomy}  and obtained 7 data-related issues (1-7 in Fig. 2) and 8 model-design-related issues (A-H in Fig. 2) currently supported by {\em KUnit}. 
We then adopted an approach similar to that of TheDeepChecker~\cite{BraiekDeepChecker} and reviewed existing works on fault localization and repair
techniques for DL programs~\cite{wardat21DeepLocalize, wardat22DeepDiagnosis, cao2022deepfd, Zhang21Autotrainer, nikanjam2021neuralint, manke2024leveraging, wardat2023localizing, islam20repairing}, contracts for DL programs~\cite{ahmed23dlcontract, Khairunnesa2023}, and the Keras official documentation~\cite{Keras,kerasexamples}. This review enhanced our understanding of the root cause of the bugs and how these issues manifest in DL workflow, allowing us to establish the conditions necessary to identify and address them. These conditions are implemented as assertions in {\em KUnit}'s test methods to identify the bugs and repair strategies are utilized to provide actionable fixes in {\em KUnit}.




\subsubsection{Implementation}
We implemented {\em KUnit} in Python on top of \textit{Keras} 2.3.0 and \textit{TensorFlow} 2.1.0. 
The conditions obtained in Section~\ref{bugconditions} are implemented as test cases using Python's built-in unit testing framework, \texttt{unittest}.


\subsubsection{Empirical Evaluation}
To evaluate our approach, we collected DL programs developed using Keras. 
We examined the recently published DL fault localization benchmarks~\cite{wardat21DeepLocalize,cao2022deepfd,ghanbari2023deepmufl}.
Currently, {\em KUnit}, supports two types of DL architectures, FCNNs and CNNs designed for regression and classification problems for structural data. 
We used these criteria for filtering the programs from these benchmarks.
We identified an overlap among the programs in these benchmarks and filtered out duplicates to retain only unique programs. As a result, we obtained 50 programs, 42 from \textit{Stack Overflow} and 8 from \textit{GitHub}.
We considered the 50 programs in our benchmark as ``unseen''
because we have not seen these buggy and correct programs during the process of acquiring 
conditions described in Section~\ref{bugconditions}.

\subsubsection{User Study}

We also performed a user study to evaluate our approach.
We followed
the methodology of Biswas \etal~\cite{biswas22art} to collect real-world datasets and tasks from Kaggle competitions~\cite{kaggle} and collected 5 real-world datasets that require preprocessing to meet data quality requirements. 
The details of these datasets are shown in Table~\ref{tab:userstudydatasets}. 
We selected 5 sequential DL models from Kaggle competitions~\cite{kaggle} and provided them as a reference model to the participants. 
The details of these models are shown in Table~\ref{tab:userstudymodels}.

\input{userstudydataset}
\input{userstudymodels}
\subsubsection{Tasks}
To avoid versioning issues during experimental setup and save participants time, we hosted tasks on \textit{GitHub} Codespaces~\cite{githubcodespaces}; a web-based VS Code IDE that allows developers to edit, run, test, and debug code within a web browser.
We used Zoom to monitor each participant's screen and ensured they used the program as intended.
We shared with participants a document with instructions explaining the goal of the task and how to run it on \textit{GitHub} Codespaces IDE.
In our task design, each participant performed two tasks: one using the traditional approach and the other using the modular approach.
Since participants working on a task are likely to remember its details and the issues encountered, we adopted a between-subjects design~\cite{davis2023nanofuzz}  to mitigate the learning effect. Specifically, we assigned each participant two distinct problems.
For the data preparation task, 
participants were provided with datasets and instructed to explore the data and apply preprocessing steps based on their expertise.
In the traditional setting, they did not test the code, whereas in the modular setting, participants tested the added preprocessing steps in isolation using an automatically generated mock model. 
For the model designing task, participants were given a reference DL model and asked to modify its structure based on the task requirements and their experience. In the traditional setting, participants received preprocessed data from the data preparation task completed by another participant in the traditional setting and tested the designed model using the original data. Whereas, in a modular setting, participants tested the designed model in isolation using automatically generated mock data.
During the study, participants were allowed to access the internet to verify the syntax of different operations in Python and Keras. 
After completion of the task, we asked participants to complete a survey to share their experience of {\em KUnit} on a 5-point Likert scale and provide open-ended feedback.
We conducted a pilot study with 7 participants, which allowed us to refine the tasks and instructions. This study was reviewed by our Institutional Review Board.
\input{resultsummary}
\input{rq1userstudydata}
\input{rq1userstudymodel}
\subsubsection{Participants}
We recruited participants via LinkedIn using direct messages and 
university mailing lists that described our study and a link to the screener survey. We screened participants (i) who were over 18, (ii) who had at least one year of programming experience, and (iii) who had experience with DL programming. 
To evaluate the applicability of our approach in everyday scenarios and industry settings, we recruited both graduate students and 
industry professionals.
In total, we recruited 36 participants (21 male and 15 female), 24 graduate students from different universities, and 12 industry professionals working in Google, IBM, Neural Lab, \etc.
Participants were asked to self-classify their level of expertise on a scale from 1 (beginner)
to 5 (expert). The obtained expertise levels are: using existing DL programs ($\mu = 4.1, \sigma = 0.7$), developing new DL programs ($\mu = 4.0, \sigma = 0.8$), debugging DL programs ($\mu = 3.9, \sigma = 0.8$), and familiarity with preprocessing steps ($\mu = 4.0, \sigma = 0.8$).

In the study design, we aimed to have each task performed by participants with varying levels of expertise.
This allowed us to investigate the mistakes made by developers across different skill levels and evaluate the utility of unit testing in DL applications.
Participants were assigned tasks based on their self-reported expertise level. 
During self-assessment, we found that the participants rated themselves as either 3 (competent), 4 (proficient), or 5 (expert).
When a participant scheduled a session, we assigned a task that no other participant with the same experience level had already been assigned.  If a new participant with the same expertise level scheduled a time slot
and all tasks had been completed by others with the same
expertise level, we randomly assigned tasks to ensure that 
at least three different participants performed each task.

\subsection{Results}
In this section, we 
report on the efficiency of
our technique 
and answer our research questions.

\subsubsection{Do mock objects aid in testing each functionality in isolation without reliance on external dependencies?}
In this research question, we validate whether different functionalities of the DL programs can be tested without committing to a labeled dataset or model using mock objects, isolating the code being tested from external dependencies (dataset or model).
To answer this research question, we first evaluated the performance of {\em KUnit} on 50 DL programs in our benchmark. 
The first author
manually inspected each DL program and divided it into two parts: \textit{Data Preparation:} contains all the steps related to data preparation and \textit{Model Design:} contains all the steps related to designing the model including the compilation step.
Each part is tested independently using the mocks automatically generated by {\em KUnit}.
To address the challenges posed by the indeterministic nature of DL applications, we execute each test 3 times and consider issues reported in more than one run.
The buggy version of the original program was examined, and the number of issues in each of the 50 programs was counted. 
Table~\ref{SOFsummary} reports the total number of issues found in 50 programs across different categories.
The results shown in Table~\ref{SOFsummary} depict that the mock model facilitated testing of the data preparation steps, with {\em KUnit} identifying 10 out of 12 issues in this stage.
Similarly, for the model design stage, mock data facilitated testing of the designed model, with {\em KUnit} identifying 53 out of 62 issues in this stage.
Further investigation was done to determine the reason behind the issues missed by {\em KUnit}. 
In the data preparation stage, we found that the 2 missed issues were related to scaling the labels to the appropriate range to match the output layer activation. {\em KUnit} only verifies the scaling of the data, not the labels.
Therefore, {\em KUnit} missed these issues.
For the model design stage, the assertions used in {\em KUnit} are 
obtained from various sources (discussed in Section~\ref{bugconditions}) which
cover various frequently occurring scenarios that might not account for some edge cases, such as models designed for datasets with specific label ranges; {\em KUnit} missed 9 issues.
As {\em KUnit} is open-source, developers have the flexibility to refine these assertions or define new ones according to their needs.

 
Secondly, during the user study, participants used {\em KUnit} to independently test the data preparation steps and designed model using mocks. Tables~\ref{rq1userstudydata} and \ref{rq1userstudymodel} present the results of testing the three solutions (S1, S2, and S3) provided by different participants for each task, with columns indicating the issues detected in each solution.
For the data preparation stage, Table~\ref{rq1userstudydata} demonstrates that a total of 25 issues were identified by {\em KUnit} using mock models for all the tasks.
All these issues were acknowledged as valid findings
by participants with varying levels of expertise and were subsequently resolved by them.
For the model design stage, the participants tested the designed model using the mock data. 
In Table~\ref{rq1userstudymodel}, 
for each task, we highlighted the issues identified and reported by {\em KUnit}. 
Across all tasks, {\em KUnit} detected a total of 42 issues using mock data, of which 38 were accepted and resolved by participants with varying levels of expertise.
However, for 4 issues reported by {\em KUnit}, 2 participants mentioned that, based on their experience, these might be false alarms.
For example, in Task 4, a binary classification task, {\em KUnit} verifies that the output layer yields a value between 0 and 1 depicting the positive class probability (property of the sigmoid function).
However, for Task 4 (S3), the participant expressed a preference for using a probability distribution (softmax function) to represent the output layer results instead of class probabilities (sigmoid function), leaving room for extending the problem to multiclass classification in the future.  
Similarly, for detecting the oscillating loss issue, {\em KUnit} monitors the loss after every 5 epochs. However, for Task 5 (S3), the participant stated that they prefer to evaluate the model's stability every 10 epochs.
Therefore, due to differing criteria and preferences used by different developers for evaluating model stability, 4 out of 42 issues were dismissed as a false alarm by 2 participants for Task 4 (S3) and Task 5 (S3). Given that {\em KUnit} is open-source, developers can customize these assertions to fit their problem requirements.
\textit{In summary, our results demonstrate that mock objects facilitated independent testing of each functionality and assisted in the early detection of issues.}



\subsubsection{How efficient are mock objects in identifying issues compared to traditional deep learning testing approaches?}
In this research question, we evaluate the efficiency of mock objects in uncovering issues in DL programs at an early stage, that in current practice, are detected after combining different stages, specifically during training.
To compare the efficiency of {\em KUnit} with traditional DL testing approaches, we evaluated 50 programs in our benchmark.
The original program (with data and model stages combined) is tested using a state-of-the-art approach \cite{wardat22DeepDiagnosis} and the results are compared with the issues identified by {\em KUnit} at each stage.
DeepDiagnosis~\cite{wardat22DeepDiagnosis} is a fault localization tool that detects silent bugs in DL programs by monitoring for abnormal behavior during training.
This tool is selected because it covers most of the silent bugs (8) encountered during model training compared to other existing fault localization tools~\cite{schoop2021umlaut,wardat21DeepLocalize,Zhang21Autotrainer,cao2022deepfd}.
By comparing {\em KUnit} with DeepDiagnosis, we evaluate whether {\em KUnit} can effectively identify issues at an earlier stage using mocks before data-model integration that DeepDiagnosis identifies after integration during training.
Our analysis shows that, 
for 22 programs with issues in only one stage, \ie, data preparation or model design, {\em KUnit} identified the same problems with mocks that DeepDiagnosis identified using the original dataset during training. 
For the 25 programs with issues in both the data preparation and model design stages, DeepDiagnosis detected issues in the data preparation stage for 7 of these programs. 
However, uncovering bugs in the model design stage with DeepDiagnosis requires fixing data preparation issues first, necessitating multiple iterations to identify problems in the model design stage. 
Similarly, for 5 of these programs, DeepDiagnosis reported a numerical error in computation but could not determine the error-inducing stage. 
For the remaining 13 programs, DeepDiagnosis did not detect any issues.
The main challenge for DeepDiagnosis stems from programs with multiple bugs, as it detects one issue at a time.
This requires retraining the model on an original training dataset after every modification, leading to inefficient use of computational resources~\cite{Zhang21Autotrainer}.
In contrast, testing each stage separately with {\em KUnit} using mocks provides a lightweight emulation of dependencies, facilitating testing each stage before and after modifications, thereby saving resources.
As a result, {\em KUnit} detected issues in all 25 programs.
Some numerical computations in DNNs are highly data-dependent.
For example, using activation functions that are not suitable for certain input ranges can lead to out-of-range problems, which cannot be detected during unit testing with mocks.
Therefore, for 3 programs, {\em KUnit} missed these issues, whereas, testing using the original dataset helped DeepDiagnosis identify issues in 2 out of 3 programs. 

During the user study, participants utilized both {\em KUnit} and DeepDiagnosis for debugging. The results show that testing individual stages in isolation with mocks helped {\em KUnit} to efficiently pinpoint the root causes of issues.
In contrast, DeepDiagnosis, which detects issues after data-model integration, cannot identify the root cause of the issue in programs with multiple bugs. This is particularly evident in programs with issues originating from both the data preparation and model design stages, as these issues often exhibit overlapping symptoms during training.
Due to this, in 10 programs with multiple bugs, DeepDiagnosis reported a numerical error but failed to pinpoint the root cause. Detailed results are reported in the supplementary material~\cite{rq2results,rq2userstudyresults}.
We also analyzed the debugging time for each task in Tables~\ref{rq1userstudydata} and \ref{rq1userstudymodel}, comparing results with and without {\em KUnit}. In the traditional setting, when data and model are integrated and tested using DeepDiagnosis, we observed that the quality of the preprocessed data significantly impacted the debugging time. Participants had to resolve data-related issues before addressing model-specific problems. 
In contrast, when using {\em KUnit}, the data and model are tested in isolation using mocks, allowing participants to focus on stage-specific issues. As DeepDiagnosis identifies issues after integration, for a fair comparison, we computed the total time, \ie, summation of time taken by participants to resolve bugs in data and model stages using {\em KUnit} separately and compared it with DeepDignosis. 
Our analysis reveals that, on average, participants took 15 and 12 minutes to debug the DL programs using DeepDiagnosis, and {\em KUnit}, respectively. By isolating data and model, {\em KUnit} facilitates a more focused and efficient debugging process that can save time and resources during training. Detailed analysis is provided in our GitHub repository~\cite{debuggingtime}.
\textit{In summary, our analysis shows that mock objects effectively mimic essential system behaviors, simplifying complexity for unit testing and assisting in identifying issues that lead to abnormal behavior during training.
Mock testing is efficient and resource-friendly, especially for programs with issues in multiple stages.}



\subsubsection{How do developers perceive the effectiveness of unit testing using mocks compared to traditional deep learning testing approaches?}
To answer this research question, we collected survey responses from 36 participants during the user study.
In particular, on a 5-point Likert scale question, participants rated their experience about the usefulness of mocks for unit testing.
\figref{Userratings} shows the survey results with participants ratings.
As illustrated in \figref{Userratings}(a), 34 participants rated positively (rating $>$ 3) and found that mocks help test each component independently.
Likewise, 35 participants (\figref{Userratings}(b)) responded (rating $>$ 3)  that mocks facilitate the early identification and resolution of issues during the development process.
Regarding the usefulness of the mocks in improving code structure, 31 participants (\figref{Userratings}(c)) responded positively (rating $>$ 3), while 5 participants (rating $\leq$ 3) 
expressed the concern that mocks might produce false alarms and mislead efforts to improve the code structure.
Regarding the integration of the mocks into their DL development process, 34 participants (\figref{Userratings}(d)) rated positively (rating $>$ 3) and expressed their interest in incorporating unit testing with mocks to enhance software reliability.
We also asked participants to share open-ended feedback on the advantages and disadvantages of {\em KUnit}.
Two authors conducted an open coding phase over the qualitative responses~\cite{weiss1995learning} and grouped codes into different themes.
For advantages, our 
inductive thematic analysis identified 6 repeated themes in participants' qualitative responses.
The themes are: \textit{bugs can be detected early on, makes testing easy/easier to manage, time efficient/saves a lot of time, automation reduces human efforts, saving resources, great experience/helpful/useful}. 
For disadvantages, we found 2 repeated themes: 
\textit{implementation in the industry could be challenging/overhead to set up} and \textit{incorrect reports/false alarms.} 
Detailed qualitative responses are provided in the supplementary material~\cite{participantsresponse}. 

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.65]
 {./figures/UserRatings.pdf}
    \caption{Survey results with participants ratings.}
	\label{Userratings}
\vspace{-0.4cm}
\end{figure} 
During the user study, we observed that the learning curve for {\em KUnit} depends on the user’s familiarity with DL concepts and experience with Keras. To help {\em KUnit} users easily understand the workflow, we provided detailed documentation and a running example in our GitHub repository~\cite{myRepo}. Participants in the post-study feedback illustrated that the well-structured documentation helped them easily understand {\em KUnit}’s functionality and workflow. After familiarizing themselves with the workflow, which on average takes 10-15 minutes, they only need to adjust the interfaces to align with their task. In the data preparation stage, the user loads the original dataset, applies preprocessing steps, and executes the test file. {\em KUnit} generates a mock model and feeds preprocessed data into it to verify data quality. In the model design stage, users build the DNN model and run the test. {\em KUnit} generates mock data and feeds it into the designed model to verify its correctness and compatibility with expected data properties. In the post-study feedback, participants mentioned that they found {\em KUnit} easy to use with minimal manual effort required for setup and customization. They were able to adjust the interfaces to align with their task and used the framework for developing and unit testing their DL application.
By making {\em KUnit} open-source, we provide flexibility to adapt the tool to developer needs and improve its accuracy and ease of use through community contributions.
\textit{In summary, we found that the developers view unit testing using mocks as a valuable addition to traditional DL testing techniques. It enables independent testing of each component, facilitates early problem detection during development, and contributes to improving the overall code structure.}








