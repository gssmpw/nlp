\section{Introduction}
\label{sec:intro}

\textit{Deep Learning} (DL) is a sub-class of machine learning algorithms that has gained a lot of attention from the industry and academia due to its successful adoption in many domains~\cite{Csaky19,Iannizzotto18,Roy18}.
The popularity of DL applications has drawn the interest of the software engineering community and the community has responded by conducting several studies~\cite{zhang2019,Zhang2020,islam19,humbatova20taxonomy,zhang18,cao2022understanding,rahman2023machine} to understand the development process of these applications.
These studies found that DL application developers usually focus on building and optimizing models using the training data, focusing less on modern software engineering practices such as modular design, unit testing, \etc.~\cite{Zhang2020, pan2020decomposing, pan2022decomposing}.
DL application development follows a workflow that is different from the traditional software development~\cite{zhang2019, biswas22art, Amershi2019} - where data is prepared first followed by model design and training, establishing a tight dependency between data and model.
Therefore, incorporating software engineering practices, such as independent testing of data and models necessitates decomposing the workflow, \ie, separating the data and model, and mimicking their dependencies to facilitate unit testing.


Inspired by the fundamental practice of unit testing in traditional software development~\cite{binder2000testing}, and the notion of creating mock objects that mimic the minimum expected behavior of dependencies for unit testing~\cite{mackinnon2000endotesting}, we ask: 
\textit{can we apply the concept of mock objects, commonly used in unit testing traditional software, to test DL applications?}
Unit testing with mock objects not only allows for early bug detection in the development cycle but also facilitates the development of modules and verifying their functionality by deferring the dependencies.
To the best of our knowledge, unit testing using mocks for DL applications — wherein the data and DL model are tested independently — has not been explored before. 


This paper introduces the idea of mock testing in the context of \textit{Deep Neural Networks (DNNs)}.
In current DL application development, data is typically prepared by data scientists, while models are designed by machine learning engineers~\cite{Zhang2020}. Each group focuses on distinct stages of the DL pipeline, creating a natural separation of responsibilities. To align with this practice, we recommend treating the data preparation and model design stages as independent modules or units
and propose employing mocks for their independent testing.
In the context of DL, mocks refer
to mock data and mock model that mimic the behavior of the
original data and model, respectively.
The independent testing of these modules using mocks
simplifies debugging, facilitates early bug detection, and ensures that the resulting code meets
certain quality aspects, \ie, good-quality data, a model that conforms with the requirements, and high operational reliability. 

\begin{figure*}[htbp]
	\centering
	\includegraphics[scale=0.6]
 {./figures/MockTestingNewMotivation.pdf}
    \caption{A buggy DL program and mocks in action.}
   
	\label{Motivation}
 \vspace{-0.7cm}
\end{figure*}

To introduce our notion of mock testing, we have focused on two types of DL architectures, Fully-Connected Neural Networks (FCNNs) and Convolution Neural Networks (CNNs) for regression and classification problems.
These architectures are commonly employed to handle high-dimensional data due to their ability to capture nonlinear relationships within datasets~\cite{goodfellow2016deep}.
To facilitate unit testing, we introduce a design paradigm that considers each stage of the DL program, \ie, data preparation and model design, as separate modules. The unique challenge in independently testing these components arises from the tight dependency between data and models. To handle the inherent dependencies among these modules, we propose defining clear interfaces to decouple them. These interfaces specify key elements of each stage, such as the ‘number of features’ in data preparation and the ‘DNN architecture type’ in model design, which influence each other’s configuration. These interfaces are then leveraged to automatically create mock data or models that replicate the behavior of real components. This proposed approach allows for isolated testing of each module by substituting the original data or model with the automatically generated mock versions, ensuring independent quality assurance at each stage. 
To achieve this, we utilized Python's built-in unit testing framework, \texttt{unittest}, and 
developed, \textit{KUnit} specifically for Keras. 
\textit{KUnit} comprises 15 distinct methods with assertions aimed at verifying the expected behavior of specific sections of the code under test, leveraging the generated mocks.
\textit{KUnit} is open-sourced~\cite{myRepo} and can be extended to incorporate more assertions and support other frameworks.


\begin{figure*}[!ht]
	\centering
	\includegraphics[scale=0.65]
 {./figures/MockingNew2.pdf}
    \caption{Workflow of {\em KUnit}. }
   
	\label{Overview}
 \vspace{-0.55cm}
\end{figure*} 


We have evaluated {\em {KUnit}} through empirical and user evaluation. Empirical evaluation is performed on 50 programs obtained from \sof and \textit{GitHub}.
We separated the data preparation and model design
steps into two distinct modules, which were then tested in isolation using mocks.   
We compared the issues detected using mocks with issues detected when analyzing the two stages together.
We observed that, for the data preparation stage, the mock model helped identify 10 issues, whereas, for the model design stage, the mock data assisted in identifying 53 issues during the empirical evaluation. 
Our results demonstrate that mocks effectively detect issues that cause abnormal behavior during training.
We also performed a user study with 36 participants using {\em KUnit} to evaluate its effectiveness. 
Participants using {\em KUnit} successfully resolved 25 issues in the data preparation stage and 38 issues in the model design stage.
Our findings indicate that mock objects provide an effective, lightweight simulation of dependencies for unit testing. 
In the post-study survey, we found that {\em KUnit} is helpful to the developers for testing each component independently and resolving issues early in the development process.


In summary, our work makes the following 
% major 
contributions:
\begin{enumerate}
\item \textbf{Originality:}
First, we introduce mock deep testing for independent testing of data and model.
Next, we identify the elements of each stage that are used to decouple the data preparation and model design stages. Leveraging these elements defined in an interface, we propose a method for automatically generating mock objects for each stage. These mock objects support unit testing and help identify issues early in the development process.

\item \textbf{Usefulness:}
We develop a framework, {\em{KUnit}}, that is extensible and generalized to different classes of DL bugs.
We specified
15 different bug types and the conditions necessary for their detection. 
These conditions are incorporated as assertions in the test methods to identify various bugs and repair strategies are proposed 
as actionable fixes in {\em{KUnit}}.

\item \textbf{Evaluation:}
The empirical and user evaluation shows mock objects' potential for unit testing DL applications. Our results show that mock objects provided a lightweight emulation of dependencies, allowing early bug detection.
The user evaluation provides evidence that mocks are helpful to developers in testing each component independently and resolving issues effectively.

\end{enumerate}



