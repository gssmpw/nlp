\section{Approach}
\label{sec:Approach}


In this section, we provide an overview of our approach for unit testing DL applications using mocks.
Inspired by the decomposition criteria proposed by Parnas~\cite{parnas1972criteria}, we suggest making each major step in the DL program a module.
In a DL pipeline, these major steps correspond to the different stages, \ie, \textit{data preparation} and \textit{model design}.
Due to coupling between these stages, we decouple them by defining interfaces, allowing the data preparation and model design stages to depend on the interface, ensuring their independence. 
These interfaces facilitate the automatic generation of mocks, which are used for unit testing of each stage.
The workflow of our approach, {\em KUnit} is shown in \figref{Overview}.
We collected issues for each stage from various sources outlined in Section~\ref{bugconditions} and established the conditions for identifying them.
In total, we obtained 7 issues (1-7) for the data preparation
and 8 issues (A-H) for the model design stage shown in \figref{Overview}. 
We 
leveraged Pythonâ€™s built-in unit testing framework, \texttt{unittest},
to develop, {\em KUnit}; a testing framework for Keras.
We defined each condition as an assertion in the test method aimed at verifying the expected behavior of the code under test leveraging the generated mocks.
Once a failure is detected by {\em KUnit}, the user is notified with an assertion error and a workable solution. 
Our approach 
for unit testing DL applications has two main steps: \textit{interface definition} and \textit{mock object creation and verification}. 
Below, we discuss each step in detail.


\subsection{Interface Definition}
Due to dependencies between the data preparation and model design stages, the primary task for independent testing of these stages is to design interfaces that decouple them. 
For decoupling, it's essential to identify elements from one stage that affect the design decisions of the other.
Understanding these dependencies enables better modularization and smoother integration between stages.
We propose interfaces that allow data preparation and model design stages to depend on the interface, ensuring their independence.
Below, we detail our approach to defining interfaces for each stage.


\paragraph{Interface for Data Preparation Stage}
In the data preparation stage, feature engineering is a common activity carried out intending to select informative features that the DL model learns during training.
These features play a crucial role in the design decisions of the model design stage.
For example, in the model design stage, most of the decisions, such as which neural network architecture to choose and its hyperparameters depend on the characteristics of data and the features selected in the data preparation stage. 
For instance, consider a model in \figref{Motivation} designed for predicting the selling price of trucks.
As the dataset is structured (each row representing a different record), 
the developer selected the FCNN model, which is known to perform well for structured data~\cite{borisov2022deep}.
Since this is a regression task, choosing the appropriate hyperparameters is another design decision in the model design stage.
For example, the output layer activation function depends on the type of task, \ie, regression or classification.
For the data preparation stage, we propose an interface that incorporates the number of features selected during the feature selection step, the type of data, and the type of task.
For the task in \figref{Motivation}, \figref{fig:InterfaceDefinition}(a) shows the interface and class description of the data preparation stage. 
The data preparation module implements this interface, exposing its behavior to other classes that depend on it for their design decisions.

\begin{figure*}[t!]
    \centering
    \subfloat[\centering Mock model creation for data preparation stage]{{\includegraphics[height=3.5cm]{figures/MockCreationData.pdf} }}%
    \qquad
    \subfloat[\centering Mock data creation for model design stage]{{\includegraphics[height=3.5cm]{figures/MockCreationModel.pdf} }}%
    \caption{Mock object creation for different stages.}%
    \label{fig:MockCreation}
\vspace{-0.5cm}
\end{figure*}


\paragraph{Interface for Model Design Stage}
In the model design stage, selecting an appropriate neural network architecture corresponding to the task is crucial for achieving optimal performance. 
For instance, FCNNs are a good choice for tasks involving structured data~\cite{borisov2022deep}, where each feature is independent and there are no inherent spatial or temporal relationships to consider, whereas CNNs are well-suited for image classification tasks due to their ability to capture spatial hierarchies in images~\cite{lecun2015deep}.
In the data preparation stage, feature selection is influenced by the neural network architecture chosen in the model design stage. 
Since different architectures have different learning processes, the informative features 
are usually refined based on the model's performance during evaluation~\cite{biswas22art}.
For instance, in FCNNs there is no inherent weight sharing 
whereas, CNNs have a key feature of weight sharing through the convolution filters.
As a result, these two architectures may perform differently even with the same data. 
Therefore, for the model design stage, we propose an interface comprising the architecture chosen in this stage and the type of task.
\figref{fig:InterfaceDefinition}(b) shows the interface and class description of the model design stage for the task in \figref{Motivation}.
The model design module implements this interface and its behavior is exposed to other classes that depend on it.
The defined interfaces are utilized for creating useful mocks and testing each module independently.


\input{decisionTable}

\subsection{Mock Object Creation and Verification}
To ensure the correctness of each module, it is essential to verify 
that each module exhibits the correct functionality depending on the task.
Our insight is that to evaluate the expected behavior of each module, useful mocks can be constructed that approximate the behavior of the original data or model using the information exposed in interfaces.
In the mock implementation, the primary goal is to achieve simplicity instead of aiming for completeness~\cite{mackinnon2000endotesting}.
To that end, we propose a systematic approach for creating mocks for the data preparation and model design stages, detailing how these mocks are utilized for verification.
Below we discuss 
the process for each stage in detail.


\subsubsection{Data Preparation Stage}
The data preparation module intends to produce good-quality data and involves various tasks such as 
data cleaning, handling missing values, \etc. 
Typically, the features are selected and refined based on the model's performance during evaluation, establishing a feedback loop from the model evaluation stage to the data preparation stage~\cite{biswas22art}.
Our insight is that by creating a mock model that approximates the behavior of the original model, data quality can be assessed and enhanced through early-stage evaluation.
While the mock model doesn't have to preserve every semantic detail, 
it is essential to generate a model with the appropriate hyperparameters that yield the correct output tailored to a task.
The mock model can be automatically generated using the interface of the model design stage
as illustrated in \figref{fig:MockCreation}(a). 
Below, we discuss the process of mock model creation.


\paragraph{Process for Mock Model Creation} Generating a mock model involves several key steps to ensure the correctness and reliability of the generated model.

\textbf{Adaptive Mock Model Generation:} DL model has a lot of hyperparameters, which are provided at the time of model design by the developer.
The choice of the hyperparameters depends on several factors, such as the type of task and complexity of the dataset~\cite{yu2020hyper,LeCun98LeNet5}.
Any incorrect hyperparameter can be misleading, giving rise to bugs due to inaccuracies in the model.
Therefore, for automatic mock model generation,
it is necessary to construct adaptive mocks that change based on the task at hand and adapt to different testing conditions without manual intervention.
This adaptability ensures that the mock model aligns with testing requirements. Furthermore, as this paper introduces a design paradigm that supports independent development, developers can utilize automatically generated mock models for testing the data without delving into the intricacies of designing them.

Our approach for the automatic mock model generation is described in Decision Table~\ref{tab:decisiontablemodel}.
For initializing the mock model's hyperparameters, we reviewed the AI literature~\cite{Krizhevsky12AlexNet,lecun2002efficient,bengio2012practical} and Keras documentation~\cite{Kerasloss}. 
We utilized the hyperparameters suggested by the literature for a given task.
For example, for the DL program  in~\figref{Motivation}, the conditions as outlined in Decision Table~\ref{tab:decisiontablemodel} are problem type - `regression', model type - `FCNN', and classes - 1 (set to 1 for regression). The corresponding actions generate a mock model with hidden layer neurons equal to the `\# of features', an output layer with 1 neuron and a `linear' activation function, and a compilation layer with the `mse' loss function and `mae' as the metrics.


\textbf{Complexity of Mock Model:}
DL models are usually complex with several parameters and their complexity increases with the type of task at hand.
Determining the complexity of the model requires careful consideration during mock model generation.
Since the mock model aims to verify the quality of the data, creating a complex mock model for the unit testing could lead to excessive resource usage without serving the primary goal of unit testing.

To optimize the model's complexity for unit testing while managing resource consumption, we propose creating a mock model 
consisting of only three layers: the input layer, one hidden layer, and the output layer.
The rationale for opting for the simplest network is influenced by the 
principle of ``Start Simple'', as recommended in the machine learning literature~\cite{andrewng, occamrazor}. 
If a simple network struggles to learn from the training data, it suggests that the training data requires further refinement~\cite{yu2020hyper}.
Next, we explain how this mock model is utilized for the verification of data.

\paragraph{Verification of Data using Mock Model}
After generating a mock model that simulates the behavior of the original DNN model, verification is performed by inputting the preprocessed data into the mock model, rather than using the original model. Certain data preparation issues, like missing data, can be detected through data property assertions. In contrast, issues like mislabeled data, outliers, or improper feature selection require in-depth analysis. 
These issues often manifest as subtle errors that impact model performance and require a thorough examination of data-model interactions for effective identification. Therefore, we propose an integrated approach that combines data property assertions with analysis of the mock model's behavior on preprocessed data. The data property assertions are used to identify fundamental issues (1-6 in the data preparation stage shown in \figref{Overview}).  
Moreover, more complex issues 
labeled as â€˜Model not Learningâ€™ in \figref{Overview} are identified by observing the mock modelâ€™s behavior. Symptoms such as high loss, frequent misclassifications, or consistently low confidence on specific samples point to these issues, highlighting areas that require further investigation. This approach allows for the verification and refinement of data before it is used to train the original model.


\subsubsection{Model Design Stage}
The goal of the model design stage is to generate a model with suitable hyperparameters and correct API usage,
appropriate for the given task. This facilitates the model in learning features from the training data. 
Since DNNs are data-driven, training data produced by the data preparation stage is typically used to evaluate the model's performance and tune its hyperparameters. 
Although the mock data might not help detect all the training time issues,
our insight is that leveraging mock data enables the early detection of numerous bugs, including tensor shape mismatches, inappropriate hyperparameter selection, \etc. It allows for improving the model's quality before assessing its performance on original training data.
\figref{fig:MockCreation}(b) illustrates the mock data creation process using the interface of the data preparation stage.
While the mock data doesnâ€™t have to preserve every semantic detail, it is crucial to ensure that it does not contain missing values or outliers.
Below, we discuss the process of mock data generation.


\paragraph{Process for Mock Data Creation}
Generating mock data requires careful adherence to key steps to ensure the correctness of the resulting data.

\textbf{Preprocessed Mock Data Generation:}
During the data preparation stage, several preprocessing steps such as data cleaning, outlier removal, class balancing, \etc., are performed to ensure the quality of the data.
Therefore, while creating mock data, it's crucial to include similar preprocessing steps to avoid errors from inaccuracies in the mock data.

To ensure that the mock data accurately reflects the real-world scenarios, we utilized the \textit{make\_classification()} and \textit{make\_regression()} functions provided by \textit{scikit-learn}~\cite{scikit} for synthetic data generation. 
The advantage of utilizing these functions is that these functions provide a level of control over the characteristics of the generated dataset.
For example, we can generate mock data that is normally distributed, without outliers, is labeled and classes are balanced. 
These functions are also customizable, allowing users to specify the number of samples, features, \etc., offering flexibility for specific testing scenarios.
Although the data generated by these functions is normally distributed, however, it is not scaled or normalized.
Scaling or normalization is a common preprocessing step that aids in faster convergence of DL models during training~\cite{LeCun12}.
In our approach to mock data generation, we scaled the data generated 
by \textit{make\_classification()} and \textit{make\_regression()} functions.
The scaled mock data is then utilized to verify the behavior of the model. 
The rationale for validating the model's behavior with mock data is that if the model struggles to learn from the mock data,
the model's hyperparameters can be refined before proceeding to train it with the original dataset, which is usually more complex than the mock data.

\textbf{Quantification of Mock Data:}
\label{quantificationdata}
Determining the amount of data necessary for training a DL model is often a subject of debate.
It is typically gauged by several factors, such as the complexity of the dataset and the model's performance during evaluation.
In this paper, mock data is employed for unit testing to ensure the precise functioning of the mathematical functions within each layer of the designed model. It also verifies the transformation of input data into meaningful representations tailored to the specific task for which the model is designed.
Therefore, determining the amount of data required for the intended purpose (\ie, unit testing) poses a challenge.
A small number of samples can lead to misleading results, while a large volume of samples can be resource-intensive.

To address this challenge, we consulted established machine learning literature~\cite{ruleof10,lakshmanan2020machine} to estimate the approximate dataset size. We performed a sensitivity analysis of the data set sizes suggested in the literature~\cite{ruleof10,lakshmanan2020machine}.
We varied the dataset size in increments $\pm5\%$, $\pm10\%$ and $\pm20\%$ and observed the impact on the model's performance. 
Based on this analysis, the samples generated by {\em KUnit} are as follows:
for the regression task, the number of samples generated is 10 times the number of features and for the classification task, 100 samples are generated for each class.
In our experiments, we found that these samples were adequate for identifying various types of issues in the designed models.
We now discuss how the mock data is utilized for the verification of the model.



\paragraph{Verification of Model using Mock Data}
After generating mock data that mimics key characteristics of the original dataset, verification is done by feeding the mock data into the designed model. This ensures the model's correctness without depending on the original data. To facilitate this, we propose an integrated approach that combines the model property assertions with an analysis of the model's behavior on mock data. For verifying the modelâ€™s 
structure,
assertions are defined using the data properties defined in the interface of the data preparation stage and conditions obtained from the literature
(Section~\ref{bugconditions}), which helps to identify issues 
A-E illustrated in \figref{Overview} for the model design stage. Next, the model's response to the mock data is analyzed to ensure it appropriately handles normalized/scaled data. This analysis facilitates the early detection and resolution of potential issues F-G, shown in \figref{Overview}.
By detecting these issues early, the approach allows verification and refinement of the 
model structure before using the original data for training.

