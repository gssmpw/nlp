\section{Related Work}
\paragraph{LLM-based Agent} An LLM agent is an autonomous system that follows language instructions to perform complex tasks using available tools~\cite{su2023language}. Pilot studies have explored applications across domains like chemistry~\cite{yu2024chemagent, Boiko2023, ghafarollahi2024protagentsproteindiscoverylarge}, healthcare~\cite{abbasian2024conversationalhealthagentspersonalized, shi2024ehragentcodeempowerslarge, yang2024psychogatnovelpsychologicalmeasurement}, and daily life~\cite{liu2023agentbench, zheng2023seeact, zhou2024webarenarealisticwebenvironment, gou2024navigatingdigitalworldhumans, gu2024llmsecretlyworldmodel}. The memory module enables agents to evolve and act consistently~\cite{Wang_2024}, often mimicking human memory~\cite{zhu2023ghostminecraftgenerallycapable, Zhu_2023, park2023generativeagentsinteractivesimulacra}. Unlike GuardAgent~\cite{xiang2024guardagentsafeguardllmagents}, which uses memory for knowledge-enabled reasoning, our framework optimizes memory collaboratively via test-time adaptation and storing effective safety checks.\par
%An LLM agent is an autonomous system that follows language instructions to perform complex tasks in real or simulated environments, using available tools to achieve specified goals~\cite{su2023language}. Pilot works on LLM agents have explored a wide range of applications across various domains such as chemistry~\cite{yu2024chemagent, bran2023chemcrowaugmentinglargelanguagemodels, Boiko2023, ghafarollahi2024protagentsproteindiscoverylarge}, healthcare~\cite{abbasian2024conversationalhealthagentspersonalized, shi2024ehragentcodeempowerslarge, yang2024psychogatnovelpsychologicalmeasurement, tu2024conversationaldiagnosticai, li2024agenthospitalsimulacrumhospital}, and daily life~\cite{liu2023agentbench, zheng2023seeact, zhou2024webarenarealisticwebenvironment, xie2024travelplannerbenchmarkrealworldplanning, mei2024llm, hua2024trustagent, lin-etal-2024-battleagent, shi2025from}. The memory module of the LLM agent stores information to guide actions, enabling agents to evolve and act consistently~\cite{Wang_2024}. Previous studies on LLM agents utilize memory to mimic human memory~\cite{zhu2023ghostminecraftgenerallycapable, Zhu_2023, park2023generativeagentsinteractivesimulacra, shinn2023reflexionlanguageagentsverbal, huang2025r2d2}.  Unlike GuardAgent~\cite{xiang2024guardagentsafeguardllmagents}, which leverages memory for knowledge-enabled reasoning to generate more accurate code, our framework treats memory as an optimization objective, where two LLMs collaboratively optimize and update the memory during test-time adaptation to store effective safety checks for agent actions.\par
\paragraph{Guardrall on LLM and LLM Agent} Previous studies for guardrails on LLMs can be broadly categorized into two types: those~\cite{rebedea-etal-2023-nemo, llama_guard_3_8b, yuan2024rigorllmresilientguardrailslarge, luo2025dynamicguideddomainapplicable} designed for harmfulness mitigation for LLMs and those~\cite{xiang2024guardagentsafeguardllmagents, naihin2023testinglanguagemodelagents, tsai2025contextkeyagentsecurity} aimed at assessing whether the behavior of LLM agents poses any risks. Existing guardrail approaches for LLMs often overlook the fact that the risks associated with LLM agents extend beyond natural language outputs to other modalities (e.g., Python code and Linux command). For guardrail on LLM agent, GuardAgent~\cite{xiang2024guardagentsafeguardllmagents} relies on manually specified trusted contexts, limiting its ability to address risks in dynamic downstream tasks. Our framework overcomes this limitation through adaptive safety check generation. Conseca~\cite{tsai2025contextkeyagentsecurity} generates adaptive safety policies, but relying on a manually specified trusted context may overlook critical information. This limitation can introduce inherent risk biases in LLM-based understanding, potentially leading to misinterpretations of user intent and task requirements. In contrast, our framework optimizes safety checks to strike a trade-off between robustness and utility for LLM agents.
