\section{Introduction}
Recent advancements in Large Language Model (LLM) powered agents have demonstrated remarkable capabilities in tackling complex tasks in our daily life~\cite{liu2023agentbench, zheng2023seeact, zhou2024webarenarealisticwebenvironment, xie2024travelplannerbenchmarkrealworldplanning, mei2024llm, hua2024trustagent, lin-etal-2024-battleagent, zhang2024aimeetsfinancestockagent, mei2024aiosllmagentoperating, gu2024middlewarellmstoolsinstrumental}, as well as in specialized fields such as chemistry~\cite{yu2024chemagent, bran2023chemcrowaugmentinglargelanguagemodels, Boiko2023, ghafarollahi2024protagentsproteindiscoverylarge} and healthcare~\cite{abbasian2024conversationalhealthagentspersonalized, shi2024ehragentcodeempowerslarge, yang2024psychogatnovelpsychologicalmeasurement, tu2024conversationaldiagnosticai, li2024agenthospitalsimulacrumhospital}. LLM agents generate instructions (e.g., code) as actions to interact with the environment, enabling them to complete specific tasks effectively~\cite{yao2023reactsynergizingreasoningacting}. More advanced LLM agents~\cite{zhu2023ghostminecraftgenerallycapable, Zhu_2023, park2023generativeagentsinteractivesimulacra, shinn2023reflexionlanguageagentsverbal} are equipped with memory capabilities, enabling them to store information gathered from the environment and utilize memory to inform and enhance future actions~\cite{Wang_2024}.
\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/risk.pdf}
    \caption{\small \textbf{ Risk on Computer-use Agents.} Our framework can defend against systemic and task-specific risks and prevent them before agent actions are executed in environment.}
    \vspace{-0.8em}
    \label{fig:risks}
    \vspace{-1.0em}
\end{figure}

Meanwhile, recent studies~\cite{he2024securityaiagents} have shown that LLM agents fail to adequately consider their potential vulnerabilities in different real-world scenarios. Generally, the risks of an LLM agent can be categorized into two groups illustrated in Figure~\ref{fig:risks} : \textbf{Task-specific risks} refer to risks explicitly identified by the agent administrator based on the agent’s intended objectives and operational constraints within a given task. For example, according to the guard request of the EICU-AC dataset, these risks include unauthorized access to diagnostic data and violations of privacy regulations~\cite{xiang2024guardagentsafeguardllmagents}. \textbf{Systemic risks} arise from vulnerabilities in an LLM agent’s interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security failures. For example, unauthorized access to system data threatens confidentiality, leading to inadvertent exposure of sensitive information~\cite{yuan2024rjudgebenchmarkingsafetyrisk}. Integrity risks arise when malicious attacks, such as prompt injection on an Ubuntu terminal or websites like EIA and AdvWeb, manipulate agents into executing unintended commands~\cite{liu2024automaticuniversalpromptinjection, liao2024eiaenvironmentalinjectionattack, xu2024advwebcontrollableblackboxattacks}. Even normal operations can pose availability risks—such as an OS agent unintentionally overwriting files—resulting in data corruption.




%Pioneer research make an effort to safeguard LLM agent for task-speicifc risks~\cite{xiang2024guardagentsafeguardllmagents, tsai2025contextkeyagentsecurity} and risks associated with LLM agent failures~\cite{ruan2024toolemu}, two key challenges remain insufficiently addressed. We define the first challenge, overlooked by previous research, as the identification of \textbf{effective} safety checks that accurately detect risks associated with agent actions while eliminating redundant and unnecessary ones. Although redundant safety checks do not always compromise risk detection, they can introduce risks such as unnecessarily blocking normal agent actions. This issue arises when a model-based guardrail relies on powerful and robust models like Claude-3.5-Sonnet or GPT-4o with customized Chain of Thought~(CoT)~\cite{wei2023chainofthoughtpromptingelicitsreasoning} prompting, yet bases its decisions on vague contextual risk definitions. In such cases, the model, influenced by a bias that perceives the agent's action as a potential risk, may conduct unnecessary checks and ultimately block an normal agent action. The second challenge involves \textbf{adaptive} detection for systemtic risks. Context plays a crucial role in ensuring agent security~\cite{tsai2025contextkeyagentsecurity}, but depend on manually-crafted and precise context may focus on task-specific risk detection which may lack of generalization in systemic risk detection. For example, GuardAgent~\cite{xiang2024guardagentsafeguardllmagents} faces limitations in addressing dynamic downstream tasks for systemic risks, as it primarily assumes a relatively static set of guard rules. Therefore, \textbf{How can we find effective safety checks while ensure the generation of \textbf{adaptive} safety checks for systemtic risks} has become a urgent need for LLM agent.


Very little recent research~\cite{xiang2024guardagentsafeguardllmagents, tsai2025contextkeyagentsecurity, ruan2024toolemu, hua2024trustagentsafetrustworthyllmbased} has made significant strides in safeguarding LLM agents. However, two critical challenges remain inadequately addressed. The first challenge involves \textbf{adaptive} detection of risks to different tasks. Relying on manually specified trusted contexts for risk detection may limit generalization, as these contexts are typically predefined and task-specific, failing to capture broader risks. For instance, GuardAgent~\cite{xiang2024guardagentsafeguardllmagents} struggles to address dynamic downstream tasks, as it operates under a manually specified trusted context.  The second challenge involves identification of \textbf{effective} safety policies for risks associated with an agent action. Conseca~\cite{tsai2025contextkeyagentsecurity} leverages LLMs to generate adptive safety policies, but these LLMs may misinterpret task requirements, leading to either overly restrictive policies that block legitimate actions or overly permissive ones that allow unsafe actions. Similarly, model-based defense agencies leveraging advanced LLMs like Claude-3.5-Sonnet or GPT-4o with customized Chain of Thought (CoT) prompting~\cite{wei2023chainofthoughtpromptingelicitsreasoning} may also unintentionally enforce excessive restrictions, block legitimate agent behaviors. Therefore, \textbf{how to detect risks in an adaptive fashion and identify effective safety policies for those risks} becomes an urgent need for enhancing the reliability and effectiveness of LLM agents.
%Second challenge involves identification of \textbf{effective} safety policies for risks associated with agent actions. Although Conseca~\cite{tsai2025contextkeyagentsecurity} leverages LLMs to generate adaptive policies, these LLMs may misinterpret task requirements, leading to either overly restrictive policies that block legitimate actions or overly permissive ones that allow unsafe operations. Similarly, model-based defense agencies leveraging advanced and robust LLMs like Claude-3.5-Sonnet or GPT-4o with customized Chain of Thought (CoT) prompting~\cite{wei2023chainofthoughtpromptingelicitsreasoning} may unintentionally enforce excessive restrictions, impeding normal agent behavior. Therefore, \textbf{how to establish effective safety policies while ensuring the generation of \textbf{adaptive} safety policies} becomes an urgent need for enhancing the reliability and efficiency of LLM agents. \hs{Suggested new version: The second challenge involves identification of \textbf{effective} safety policies for risks associated with an agent action. Conseca~\cite{tsai2025contextkeyagentsecurity} leverages LLMs to generate safety policies, but these LLMs may misinterpret task requirements, leading to either overly restrictive policies that block legitimate actions or overly permissive ones that allow unsafe actions. Similarly, model-based defense agencies leveraging advanced LLMs like Claude-3.5-Sonnet or GPT-4o with customized Chain of Thought (CoT) prompting~\cite{wei2023chainofthoughtpromptingelicitsreasoning} may unintentionally enforce excessive restrictions, impeding normal agent behaviors. Therefore, \textbf{how to detect risks in an adaptive fashion and identify effective safety policies for those risks} becomes an urgent need for enhancing the reliability and effectiveness of LLM agents.} \hs{note, it is `effectiveness', not `efficiency', right?}

To bridge these gaps,  we propose a nova lifelong framework leveraging collaborative LLMs to detect risks in different tasks adaptively and effectively.  Our framework features:  \textbf{Adaptive Safety Check Generation: }A safety check refers to a specific safety verification item or policy within the overall risk detection process. Our framework not only dynamically generates adaptive safety checks across various downstream tasks based on universal safety criteria, but also supports task-specific safety checks in response to manually specific trusted contexts. \textbf{Effective Safety Check Optimization: }Our framework iteratively refines its safety checks to identify the optimal and effective set of safety checks for each type of agent action during test-time adaptation~(TTA) by two cooperative LLMs.
 \textbf{Tool Compatibility \& Flexibility: } In addition to leveraging the internal reasoning ability for guardrail, our framework can selectively invoke customized auxiliary tools to enhance the checking process of each safety check. These tools may include environment security assessment tools to provide an environment detection process.
% \textbf{Tool Compatibility \& Flexibility: }Beyond reasoning-based detection, our framework can selectively invoke customized auxiliary tools to enhance the checking process of each safety check. These tools may include environment security assessment tools to provide an environment detection process.

% Simulating real-world scenarios is crucial for evaluating and defending against adaptive attacks on LLM agents, enabling dynamic risk assessment and the development of robust security strategies beyond idealized benchmarks. All our experiments are based on real-world agent outputs, ensuring realistic interaction and adaptive risk evaluation.
We evaluate AGrail with a focus on real-world agent outputs, rather than LLM-generated synthetic environments and agent outputs~\citep{zhang2024agentsafetybenchevaluatingsafetyllm}. Our evaluation includes task-specific risks described in the Mind2Web-SC and EICU-AC datasets~\cite{xiang2024guardagentsafeguardllmagents}, as well as systemic risks such as prompt injection attacks from AdvWeb~\cite{xu2024advwebcontrollableblackboxattacks} and EIA~\cite{liao2024eiaenvironmentalinjectionattack}. Furthermore, we constructed the Safe-OS benchmark, which consists of three attack scenarios carefully designed to assess the robustness of online OS agents against systemic risks. To ensure a comprehensive evaluation, Safe-OS also includes benign data to assess the impact of defenses on normal task performance. In our main experiment, AGrail demonstrates strong performance. Using Claude-3.5-Sonnet, our framework preserved \textbf{96\%} of benign actions while achieving \textbf{0\%} Attack Success Rate~(ASR) against prompt injection. It reduced ASR to 3.8\% and 5\% for environmental and system sabotage on Safe-OS, 0\% ASR on AdvWeb, and averaged 17\% ASR on EIA across action generation and action grounding. AGrail establishes universal safety principles that adapt to diverse tasks while ensuring effective risk detection, serving as a strong guardrail for future LLM agent security research. 
\begin{figure*}[!th]
    \centering
    \includegraphics[width=1.00\linewidth]{images/workflow.pdf}
    \caption{\small \textbf{Workflow of AGrail. } When the OS agent moves a file as requested, it may accidently overwrite an existing file in the target path. Our framework, guided by safety criteria, prevents this by generating and performing safety checks to invoke the corresponding tool that verifies if the file already exists, ensuring the action does not cause damage. }
    \vspace{-0.8em}
    \label{fig:workflow_}
\end{figure*}





















