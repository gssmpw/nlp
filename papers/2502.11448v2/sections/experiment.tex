\section{Experimental Setup}
In this section, we introduce our experimental setup for evaluating the performance of our method on different LLM agents across task-specific risks and systemic risks. Check more details in Appendix~\ref{appendix:preliminary_experiment}.
\subsection{Models} 
In our primary experiments on the Safe-OS dataset, we use GPT-4-Turbo as the OS agent's foundation model for prompt injection attacks and GPT-4o for other attacks. For defense agencies, we adopt Claude-3.5-Sonnet and GPT-4o as the foundation models. The Guardrail-based baseline integrates four advanced agencies: LLaMA-Guard~3~\cite{llama_guard_3_8b}, a fine-tuned LLama-3-8b~\cite{meta_llama_3_8b_instruct} guardrail model, GuardAgent~\cite{xiang2024guardagentsafeguardllmagents}, a multi-agent guardrail built on GPT-4, AgentMonitor~\cite{naihin2023testinglanguagemodelagents} based on GPT-4o and ToolEmu~\cite{ruan2024toolemu} based on Claude-3.5-Sonnet. Our method employs GPT-4o-mini as the agent action converter, with the main framework tested separately using Claude-3.5-Sonnet and GPT-4o as foundation models. Evaluation metrics are assessed using GPT-4o-mini and Claude-3.5-Sonnet. In the ablation study, GPT-4o-mini serves as an additional fundation model.
\subsection{Datasets}
We conduct our experiments on the following datasets focus on real-world agent outputs with Safe-OS  ensure realistic and adaptive evaluation:


\smallskip\noindent\textbf{Mind2Web-SC and EICU-AC.} Mind2Web-SC and EICU-AC~\cite{xiang2024guardagentsafeguardllmagents} are benchmarks for evaluating LLM agents' safety on task-specific risk. Mind2Web-SC assesses web agents like SeeAct in enforcing web safety rules, while EICU-AC evaluates EHRAgent~\cite{shi2024ehragentcodeempowerslarge} in restricting access to ICU patient data based on user roles.\par
\smallskip\noindent\textbf{AdvWeb.} AdvWeb~\cite{xu2024advwebcontrollableblackboxattacks} generates and injects imperceptible adversarial strings to mislead web agents into executing incorrect actions. We utilized 80 data based on AdvWeb from four domains: housing, finance, medical, and cooking.\par
\smallskip\noindent\textbf{EIA. }EIA~\cite{liao2024eiaenvironmentalinjectionattack} is a black-box attack that manipulates web environments by injecting hidden malicious HTML elements, tricking web agents into leaking user data. We sampled 100 instances where EIA successfully attacked SeeAct through action grounding and generation-based attacks. Additionally, from cases where EIA failed to attack SeeAct, we manually filtered out maliciously injected content to curate 30 clean benign samples.%Additionally, we manually filtered out melicious injected content from random data of these EIA attacks fail to attack SeeAct to curate a subset of 30 normal samples.


\subsection{Baseline}
In our experiments, we categorize baselines into two types: model-based defense agencies and guardrail-based defense agencies. For model-based defenses, we configure advanced LLMs, such as GPT-4o, with a customized CoT prompt~\cite{xiang2024guardagentsafeguardllmagents} under zero-shot and one-shot settings. For guardrail-based defenses, we set up LLaMA-Guard3 with guard requests as safety categories and evaluate GuardAgent under a two-shot setting, which represents its strongest setup. We also incorporate ToolEmu~\cite{ruan2024toolemu} as a baseline, which simulates agent environments and we only evaluated it on Safe-OS due to online setting. Additionally, we consider AgentMonitor with guided scoring~\cite{naihin2023testinglanguagemodelagents} as an baseline.  
For AGrail, which is evaluated during TTA setting in the main experiment, we configure task-specific safety criteria aligned with guard requests from Mind2Web-SC and EICU-AC~\cite{xiang2024guardagentsafeguardllmagents}. On Safe-OS, EIA, and AdvWeb, we enforce universal safety criteria for AGrail and universal guard requests for all defense agencies. AGrail employs specialized detection tools based on task requirements: the Permission Detection Tool for EHR agents, the OS Environment Detection Tool for OS agents, and the Web HTML Detection Tool for Web agents (only on EIA). Notably, tool invocation is governed by safety checks, dynamically utillized by Executor itself. Further details on these experiments can be found in the Appendix~\ref{appendix:preliminary_experiment:experiment_setting_details}. 
\subsection{Evaluation Metric} In our experiments, we utilize two evaluation metrics to assess the quality of safety evaluations performed by all defense agencies on agent actions.\par
\smallskip\noindent\textbf{Predictive Performance Metrics} We utilize classical statistical metrics, including accuracy, precision, recall, and F1-score with ASR, as our predictive performance metrics to comprehensively evaluate the trade-off performance of defense agencies in handling normal and harmful agent actions.\par
\smallskip\noindent\textbf{Agreement Metrics} To further enhance the evaluation quality of defense agencies, we customize different agreement metrics for each task to precisely determine whether the risk detection process generated by the defense agencies aligns with the actual risks in ground truth for each dataset. See more implementation details in Appendix~\ref{appendix:preliminary_experiment:evaluation_metric_details}.



\section{Result and Analysis}
%In this section, we will analyze the advantages of our AGrail  across three agents based on the main results, ablation study and case study. 
We hearby report the results analyses and associated case studies of AGrail in this section.


\subsection{Main Result}
 AGrail demonstrates strong performance in both task-specific and systemic risk detection. As shown in Tables~\ref{table:defense_agencies_comparison_on_Safe_OS_AdvWeb} and \ref{table:defense_agencies_comparison_on_Mind2Web_EICU},  AGrail consistently ranks second across specific tasks (e.g., Mind2Web-SC and EICU-AC), regardless of using GPT-4o or Claude-3.5-Sonnet. In systemic risk detection on Safe-OS and AdvWeb, AGrail based on Claude-3.5-Sonnet achieves \textbf{0\%} ASR against prompt injection attacks on OS and AdvWeb, while blocking only \textbf{4.4}\% of benign actions on OS. When against environment and system sabotage attack attacks, ASR remain low at 5\% and 3.8\%. For EIA attacks, AGrail achieves 6\% ASR in action grounding and 28\% in action generation while maintaining \textbf{86.7\%} accuracy on normal web tasks, demonstrating the trade-off performance between robustness and effectiveness. In contrast, model-based defenses perform well in specific tasks but may block \textbf{49.2\%} of benign actions in Safe-OS, which show overly restrictive detection in these baselines. Even with task-specific safety criteria, LLaMA-Guard3 struggles to defend risks across these scenarios, which demonstrate that these LLM guardrails have difficulty in detecting these risks for LLM agents.


\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{6.0pt}
    \begin{threeparttable}
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Defense Agency} & \multicolumn{4}{c}{\textbf{Safe-OS}} & \textbf{AdvWeb} & \multicolumn{3}{c}{\textbf{EIA}} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-6} \cmidrule(lr){7-9}
        & \textbf{Norm} $\uparrow$ & \textbf{SS} $\downarrow$  & \textbf{PI} $\downarrow$ & \textbf{Env} $\downarrow$ & \textbf{PI} $\downarrow$ & \textbf{Grd} $\downarrow$ & \textbf{Gen} $\downarrow$ & \textbf{Norm} $\uparrow$ \\
        \midrule
        \rowcolor[RGB]{230, 230, 230} \multicolumn{9}{c}{\textbf{Model-based}} \\
        Claude-3.5 (0-Shot) & 50.0 & 0 & \underline{14.3} & 20 & 0 & 40.0 & 28.0 & 56.7\\
        GPT-4o (0-Shot) & 52.4 & 7.7 & 61.9 & 15 & \underline{5} & 42.0 & \textbf{16.0} & 66.7 \\
        \midrule
        \rowcolor[RGB]{230, 230, 230} \multicolumn{9}{c}{\textbf{Guardrail-based}} \\
        AgentMonitor (GPT-4o) & \textbf{100} & 46.7 & 39.1 & 85.0 & \textbf{0} & 58.0 & 40.0 & \textbf{100} \\
        LLaMA-Guard~3 & \textbf{100} & 55.2 & 100 & 100 & 100 &94.0 & 90.0 & \textbf{100} \\
        ToolEmu~(Claude-3.5) & 57.7 & 4.2 & 100 & 35.0 & - & - & - & - \\
        \textbf{AGrail  (Claude-3.5)} & \underline{95.6} & \textbf{3.8} & \textbf{0} & \textbf{5} & \textbf{0} & \textbf{6.0} & 28.0 & \underline{86.7} \\
        \textbf{AGrail  (GPT-4o)} & \underline{95.6} & \underline{4.0} & \textbf{0} & \underline{10} & 8.8 & \underline{8.0} & \underline{26.0} & 76.7 \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item \small $\dagger$ \textbf{Norm}: Normal. \textbf{SS}: System Sabotage. \textbf{PI}: Prompt Injection. \textbf{Grd}: Action Grounding. \textbf{Gen}: Action Generation.
    \end{tablenotes}
    \vspace{-0.8em}
    \end{threeparttable}
    \caption{\small\textbf{Performance Comparison of Defense Agencies for Systemic Risk Detection.} Lower ASR (↓) is better, and higher accuracy (↑) is preferred.}
\label{table:defense_agencies_comparison_on_Safe_OS_AdvWeb}
\vspace{-0.8em}
\end{table*}


\begin{table*}[ht]
    \centering
    \label{table:llm_guard_comparison}
    \setlength{\belowcaptionskip}{-0.2cm}
    {
    \setlength{\tabcolsep}{6.0pt}  % Adjust column padding for compactness
    \begin{threeparttable}
    \begin{tabular}{@{}lcccccccccc@{}}
        \toprule
         \textbf{Defense Agency} & \multicolumn{5}{c}{\textbf{Mind2Web-SC}} & \multicolumn{5}{c}{\textbf{EICU-AC}} \\
         \cmidrule(lr){2-6} \cmidrule(lr){7-11}
         & \textbf{LPA} & \textbf{LPP} & \textbf{LPR} & \textbf{F1} & \textbf{AM} 
         & \textbf{LPA} & \textbf{LPP} & \textbf{LPR} & \textbf{F1} & \textbf{AM} \\
         \midrule
         \rowcolor[RGB]{230, 230, 230} \multicolumn{11}{c}{\textbf{Model-based}} \\
         GPT-4o (1-shot) & \textbf{99.0} & \underline{99.0} & \underline{99.0} & \textbf{99.0} & \textbf{99.0} 
                         & 92.1 & 89.6 & 95.7 & 92.5 & 100 \\
         GPT-4o (0-shot) & 96.0 & 96.9 & 94.9 & 95.9 & 78.0 
                         & 97.2 & 94.7 & 100 & 97.3 & 100 \\
         Claude-3.5 (1-shot) & 94.3 & 89.8 & \textbf{100.0} & 94.6 & \underline{98.9} 
                             & 94.6 & 95.3 & 94.4 & 94.7 & 100 \\
         Claude-3.5 (0-shot) & 93.3 & 89.2 & \textbf{100.0} & 94.3 & \textbf{99.0} 
                             & 97.9 & 95.9 & \textbf{100.0} & 97.9 & 
                             100
                             \\
         \midrule
         \rowcolor[RGB]{230, 230, 230} \multicolumn{11}{c}{\textbf{Guardrail-based}} \\
         GuardAgent (GPT-4) & 90.0 & \textbf{100.0} & 80.0 & 89.0 & 90.0
                                    & \textbf{98.7} & \textbf{100.0} & 97.5 & \textbf{98.7} & 98.7 \\
        AgentMonitor (GPT-4o) & 72.5 & 79.2 & 61.0 & 68.9 & 88.5 & 82.3 & 98.2 & 66.7 & 79.4 & 100\\
         LLaMA-Guard3 & 56.0 & 93.0 & 13.0 & 23.0 & - 
                      & 48.7 & - & 0 & - & - \\
        \textbf{AGrail  (Claude-3.5)} & 94.0 & 91.4 & 97.0 & 94.1 & 95.8 
                                   & \underline{98.4} & 97.0 & \textbf{100} & \underline{98.5} & 100 \\
        \textbf{AGrail  (GPT-4o)} & \underline{98.4} & \underline{99.0} & 98.0 & \underline{98.4} & 94.7 
                               & 97.8 & \underline{97.5} & \underline{98.1} & 97.8 & 100 \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    }
    \caption{\small \textbf{Performance Comparison of Defense Agencies for Task-specific Risk Detection.} Metrics include Label Prediction Accuracy (LPA), Precision (LPP), Recall (LPR), F1-score (F1), and Agreement Metric (AM). }
    \label{table:defense_agencies_comparison_on_Mind2Web_EICU}
    \vspace{-0.8em}
\end{table*}
\subsection{Ablation Study}
In the In-Distribution~(ID) setting, we split the Mind2Web-SC dataset into a training set and a test set with an 8:2 ratio. In Out-Of-Distribution~(OOD) setting, we split the Mind2Web-SC dataset based on domains with a 3:1 ratio for training and test sets and conduct experiments in three random seeds.\par 
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/combined_plots.pdf}
    \vspace{-1em}
    \caption{\small \textbf{Performance Comparison across Different Scenarios.} AGrail  not only maintains a low ASR but also effectively defends correct risks corresponding to the ground truth compared with baselines.}
    \label{fig:combined_performance}
    \vspace{-0.8em}
\end{figure*}
\smallskip\noindent\textbf{OOD and ID Analysis} In our experiments, we randomly set three groups of seeds. We first train the memory on the training set and then freeze the memory for evaluation on the test set. From the results in Table~\ref{tab:ID_OOD}, we observe that in both ID and OOD setting, training the memory on the training set and then freezing it leads to better performance compared to the setting without memory. Moreover, this trend holds for both a stronger LLM (Claude-3.5-Sonnet) and a weaker LLM (GPT-4o-mini), demonstrating the effectiveness of memory of AGrail. Additionally,  AGrail enables generalization inference using cases stored in memory. Under the ID and OOD setting, we further evaluate the performance during TTA and found that it also outperforms the setting without memory, validating the importance of the memory module during TTA. See more details in Appendix~\ref{appendix:ablation_study:ood_id_Analysis}.\par
\begin{table}[ht]
    \centering
    {
    \setlength{\tabcolsep}{7.5pt}
    \begin{threeparttable}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Model} & \textbf{NM} $\uparrow$ & \textbf{FM} $\uparrow$ & \textbf{TTA} $\uparrow$ \\
        \midrule
        \rowcolor[RGB]{230, 230, 230} \multicolumn{4}{c}{\textbf{ID}} \\
        Claude-3.5-Sonnet & 95.6 & 96.5 & \textbf{99.1} \\
        GPT-4o-mini       & 67.9 & 70.9 & \textbf{84.1} \\
        \midrule
        \rowcolor[RGB]{230, 230, 230} \multicolumn{4}{c}{\textbf{OOD}} \\
        Claude-3.5-Sonnet & 89.7 & 93.9 & \textbf{94.6} \\
        GPT-4o-mini       & 65.9 & 68.0 & \textbf{77.8} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item \small $\dagger$ \textbf{NM}: No Memory. \textbf{FM}: Freeze Memory.
    \end{tablenotes}
    \end{threeparttable}
    }
    \caption{\small Performance Comparison for Claude-3.5-Sonnet and GPT-4o-mini as AGrail foundation model.}
    \label{tab:ID_OOD}
    \vspace{-0.8em}
\end{table}
\smallskip\noindent\textbf{Sequence Analysis}
To investigate the impact of input data sequence on  AGrail during TTA, we conduct experiments by setting three random seeds to shuffle the data sequence. In Table~\ref{ablation:table:data_order}, the results indicate the effect of data sequence across different fundation models of  AGrail. For Claude 3.5 Sonnet, accuracy shows minimal variation in this settings, suggesting that its performance remains largely stable regardless of data sequence. In contrast, GPT-4o-mini exhibits significant variability,  where both metrics fluctuate more widely. This suggests that input order introduces notable instability for GPT-4o-mini, while Claude-3.5-Sonnet remains robust. Overall, the experiments demonstrate that weaker foundation models are more susceptible to variations in data sequence, whereas stronger foundation models are almostly unaffected. See detailed results in Appendix~\ref{appendix:ablation_study:order_effect_analysis}.
% \begin{table}[ht]
%     \centering
%     \caption{Impact of Data Sequence on Our Framework}
%     \setlength{\belowcaptionskip}{-0.2cm}
%     {
%     \setlength{\tabcolsep}{6.5pt}  % Adjust column padding for compactness
%     \begin{threeparttable}
%     \begin{tabular}{@{}lc c@{}}
%         \toprule
%          \textbf{Model} & \textbf{LPA} & \textbf{F1} \\
%          \midrule
%          Claude-3.5-Sonnet & 99.1~(1.2) & 99.1~(1.3) \\
%          GPT-4o-mini & 72.8~(8.3) & 69.7~(9.5) \\
%         \bottomrule
%     \end{tabular}
%     \end{threeparttable}
%     }
%     \label{ablation:table:data_order}
% \end{table}

\begin{table}[ht]
    \centering
    \setlength{\belowcaptionskip}{-0.2cm}
    {
    \setlength{\tabcolsep}{12.0pt}  % Adjust column padding for compactness
    \begin{threeparttable}
    \begin{tabular}{@{}lc c@{}}
        \toprule
         \textbf{Model} & \textbf{LPA} & \textbf{F1} \\
         \midrule
         Claude-3.5-Sonnet & 99.1$^{\pm1.2}$ & 99.1$^{\pm1.3}$ \\
         GPT-4o-mini & 72.8$^{\pm8.3}$ & 69.7$^{\pm9.5}$ \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    }
    \caption{\small Effect of Input Orders on Our Framework - Lower Variance Indicates Higher Stability.}
    \vspace{-0.8em}
    \label{ablation:table:data_order}
\end{table}


\smallskip\noindent\textbf{Domain Transferability} We carefully evaluate the domain transferability of AGrail under the universal safety criteria. Specifically, we first perform TTA iterations on the test set of Mind2Web-SC in the ID setting and storing the updated memory. We then transfer this updated memory to EICU-AC and continue TTA iterations on 100 randomly selected samples. Comparing the results before and after transferring to EICU-AC, we observe that when AGrail leverages Claude-3.5-sonnet as the foundation model, there is no performance degradation across all metrics. When using GPT-4o, performance drops only by 5.6\% after transfer. These results demonstrate AGrail's strong domain transferability, enabling it to flexibly handle various downstream tasks under universal safety criteria. See detailed results in Appendix~\ref{appendix:ablation_study:domain_transferability_analysis}.

\subsection{Case Study}
\smallskip\noindent\textbf{Error Analysis}
\label{case_study:error_analysis}
In our error analysis, we aim to evaluate whether defense agencies correctly identify the risks of agent actions that correspond to the ground truth in the data, rather than aimless thinking about potential risks. To achieve this, we manually annotated the accurate reasons for unsafety in the environment attacks in Safe-OS to serve as the ground truth. We then assessed the defense agency’s capability to identify ground truth in prompt injection and environment attacks in Safe-OS, as well as prompt injection attacks in AdvWeb and EIA. From Figure~\ref{fig:combined_performance}, AGrail outperforms baselines by increasing an average of \textbf{46.2\%} in agreement rate and even achieves a \textbf{100\%} agreement rate on OS prompt injection. This demonstrates that AGrail can effectively identify correct risks of agent actions to block unsafe agent actions.


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{images/Uncertainty_Convergence.pdf}
    \caption{\small Cosin Similarity between Memory~\( m \) and Ground Truth~\( \Omega^{*} \) among Three seeds on Mind2Web-SC on GPT-4o.}
    \vspace{-0.8em}
    \label{fig:cosine_similarity_2_groundtruth}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{images/Similarity_Analysis_Dai.pdf}
    \caption{\small Cosine Similarity of TF-IDF Representations of Memory among Three seeds on Mind2Web-SC on GPT-4o.}
    \vspace{-0.8em}
    \label{fig:memory_similarity}
\end{figure}

\smallskip\noindent\textbf{Learning Analysis} Based on Mind2Web-SC, we conduct experiments using three random seeds to verify AGrail's learning capability. In our first set of experiments, we define the ground truth as $\Omega^{*}$ based on the guard request, and initialize the memory with a random number of irrelevant and redundant safety checks as noise for each seed. We then calculate the average cosine similarity distance of three random seeds between \( m \) and \( \Omega^{*} \) during TTA iterations on only one complex or simple action. Complex actions involve two potential safety checks, whereas simple actions involve only one. As shown in Figure~\ref{fig:cosine_similarity_2_groundtruth}, both action types progressively converge toward the ground truth, with noticeable stabilization after the fourth iteration. Furthermore, simple action converges faster than the complex action, suggesting that discovering $\Omega^{*}$ is more efficient in less complex scenarios.\par In our second set of experiments, we examine the similarity between the TF-IDF representations of memory across three random seeds during the iterative process of the complex action. In Figure~\ref{fig:memory_similarity}, we observe that after the fourth iteration, the similarity among the three memory representations stabilized, with an average similarity exceeding 98\%. Moreover, we found that the safety checks stored in the memory across all three seeds are approximately aligned with the ground truth, demonstrating the robustness of our approach in learning. This result further validates that our framework can effectively optimize $m$ toward $\Omega^{*}$ based on the safety goal in guard request and predefined safety criteria.












\section{Conclusion}
In this work, we introduce Safe-OS, a carefully designed, high-quality and comprehensive dataset for evaluating the robustness of online OS agents. We also propose AGrail , a novel lifelong framework that enhances LLM agent robustness by detecting risks in an adaptive fashion and identify effective safety policies for those risks. Our approach outperforms existing defense agencies by reducing ASR while maintaining effectiveness of LLM agents. Experiments demonstrate strong generalizability and adaptability across diverse agents and tasks.

\section*{Limitation}
% While our framework is reasoning-based guardrail enhances security of LLM agents, it has limitations. Only depend on reasoning cannot defend against all risks, especially indirect prompt injection or environment attack exploits. Our framework allows external detection tools to improve robustness, but its effectiveness depends on the availability of specialized tools, which are currently lacking for LLM Agent safety. Future work can develop more advanced detection tools based on our framework to further strengthen LLM agent security.

% Our framework enhances LLM agent security by primarily relying on reasoning-based defenses and invoking external tools only when necessary to minimize unnecessary tool usage. For example, for complex risks such as indirect prompt injections and environment-based exploits, reasoning alone may not be sufficient, requiring external detection tools for effective mitigation. The limitation lies in the availability of such tools—currently, specialized detection tools for LLM agent security are limited. Future work should focus on developing more advanced tools to complement our framework and further strengthen LLM agent security.


Our limitations are twofold. First, our current framework aims to explore the ability of existing LLMs to guardrail the agent. In our paper, we use off-the-shelf LLMs as components of our framework and incorporate memory to enable lifelong learning. Future work could explore training the guardrail. Second, due to the scarcity of existing tools for LLM agent security, our framework primarily relies on reasoning-based defenses and invokes external tools only when necessary to minimize unnecessary tool usage.
% For example, for complex risks such as indirect prompt injections and environment-based exploits, reasoning alone may not be sufficient, requiring external detection tools for effective mitigation. 
% The limitation lies in the availability of such tools—currently, specialized detection tools for LLM agent security are limited. 
Future work should focus on developing more advanced tools that can be directly plugged to our framework and further strengthen LLM agent security. 













