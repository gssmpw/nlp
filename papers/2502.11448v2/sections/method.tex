

\section{Safe-OS}
In this section, we will introduce our motivation behind creating the Safe-OS benchmark and provide an overview of its data and associated risk types.
\subsection{Motivation}
The development of Safe-OS is motivated by two key challenges: (1) Risk evavluation in \textbf{online execution setting of LLM agents}. As intelligent assistants, LLM agents autonomously interact with environments in real-world applications, making real-time evaluation of their security crucial. However, existing benchmarks~\cite{zhang2024agentsafetybenchevaluatingsafetyllm, zhang2024agentsecuritybenchasb} primarily rely on LLM-generated data, which often includes test cases that do not fully reflect real-world scenarios. This gap highlights the need for a benchmark that accurately assesses LLM agents' safety in dynamic and realistic environments. (2) The challenge of \textbf{environment-dependent malicious actions}. Unlike explicit attacks~\cite{zeng2024airbench2024safetybenchmark, yuan2024rjudgebenchmarkingsafetyrisk, xiang2024guardagentsafeguardllmagents, liu2024automaticuniversalpromptinjection, xu2024advwebcontrollableblackboxattacks, liao2024eiaenvironmentalinjectionattack, li2024injecguardbenchmarkingmitigatingoverdefense, debenedetti2024agentdojodynamicenvironmentevaluate} that can be classified as harmful without additional context, certain actions appear benign but become dangerous depending on the agent’s environment. These actions cannot be pre-defined or fully simulated without environment. For example, in a web browsing scenario, an agent may inadvertently click on a hazardous link leading to information leakage, or in an OS environment, an agent may unintentionally overwrite existing files while renaming them. Detecting such risks requires real-time environmental analysis, underscoring the necessity of enhancing LLM agents' environment monitoring capabilities.

\subsection{Overview of Safe-OS benchmark} Considering the complexity of the OS environment and its diverse interaction routes—such as process management, user permission management, and file system access control—OS agents are exposed to a broader range of attack scenarios. These include \textbf{Prompt Injection Attack}: Manipulating information in environment to alter the agent's actions, leading it to perform unintended operations (e.g., modifying agent output).
\textbf{System Sabotage Attack}: Directing the agent to take explicitly harmful actions against the system (e.g., corrupting memory, damaging files, or halting processes).
\textbf{Environment Attack}: An attack where an agent's action appears harmless in isolation but becomes harmful when considering the environment situation (e.g., rename file resulting in data loss). To address this challenge, we propose Safe-OS, a high-quality, carefully designed, and comprehensive dataset designed to evaluate the robustness of online OS agents. These attacks are carefully designed based on successful attacks targeting GPT-4-based OS agents. Additionally, our dataset simulates real-world OS environments using Docker, defining two distinct user identities: one as a root user with sudo privileges, and the other as a regular user without sudo access. Safe-OS includes both normal and harmful scenarios, with operations covering both single-step and multi-step tasks. More details of Safe-OS are described in Appendix~\ref{app:data}.



\section{Methodology}
% In this section, we will introduce the methodology of our framework. For a detailed implementation of the algorithms, prompt configurations, and applications of AGrail , refer to Appendix~\ref{app:method}.

% \subsection{Preliminary}
% We aim to identify the best stable and effective safety check item set, denoted as \( \Omega^{*} \). To approximate \( \Omega^{*} \), we propose an iterative optimization framework with a memory module $m$ that dynamically evolves to minimize its divergence from \( \Omega^{*} \). Guard Request~(Agent Usage Principles) which denoted as $\mathcal{I}_r$ and it is optional for our framework, Agent Speicifcation which denoted as $\mathcal{I}_s$, Agent Action~(Agent Out) with Environment Observation (optional) which denoted as $\mathcal{I}_o$ and $\mathcal{E}$, User Request~(Agent Input) which denoted as $\mathcal{I}_i$ and $\mathcal{T}$ as tool box set with auxiliary detection tools to enhance its safety checks. We assume there is a finite set of selectable safety checks contraint in the Safety Criteria denoted as \( \Omega = \{ p_1, p_2, \dots, p_n \} \), our objective is to formulate this as an optimization problem:

% \[
% m^* = \arg\min_{m \subseteq \Omega} \Bigl( \lVert m - \Omega^{*} \rVert \Bigr) ,
% \]

% \noindent
% where \( m^* \) represents the optimal and stable safety checks set that best approximates \( \Omega^{*} \). The optimization process follows an iterative update rule:

% \[
% m^{(t+1)}, \mathcal{S} = \mathcal{F} \bigl(m^{(t)}, \mathcal{I}_r, \mathcal{I}_s, \mathcal{I}_i, \mathcal{I}_o, \mathcal{E}, \Omega, \mathcal{T} \bigr) ,
% \]

% \noindent
% where \( \mathcal{F} \) is an update processing guiding the selection of safety checks towards stability and effectiveness. if $\mathcal{S}$ return False, the guardrail will block the agent action, else the agent action will bypass the guardrail.



\subsection{Preliminary}
We aim to identify the best set of safety checks, \(\Omega^{*} \subseteq \Omega\), that best align with predefined safety goals in safety criteria \(\mathcal{I}_c\) while incorporating optional guard requests \(\mathcal{I}_r\)\footnote{Guard requests means manually specified trusted contexts or agent usage principles. If no specific guard request is provided. AGrail will default to use universal guard request.}. Formally, the search space of safety checks to satisfy safety goals is defined as \(\Omega = \mathcal{I}_c \cup \mathcal{I}_r\), where \(\Omega = \{ p_1, p_2, \dots, p_n \}\) represents the complete set of all available safety checks, and each \(p_i \in \Omega\) corresponds to a specific safety check. Since \(\Omega^{*}\) is not directly observable, we introduce a memory module \(m \subseteq \Omega\) that iteratively stores an optimized subset of safety checks to approximate \(\Omega^{*}\) that best fulfills the safety goals.

The framework processes seven input types: safety criteria \(\mathcal{I}_c\) with optinal guard requests \(\mathcal{I}_r\), agent specifications \(\mathcal{I}_s\), agent actions \(\mathcal{I}_o\) with optional environment observations \(\mathcal{E}\), user requests \(\mathcal{I}_i\), and a toolbox \(\mathcal{T}\) containing auxiliary detection tools. Our objective is formulated as a goal-based optimization problem:

\[
    \arg\min_{m \subseteq \Omega} \,\, d_{\text{cos}}\left(\phi(m), \phi(\Omega^{*})\right),
\]
\noindent
where \(d_{\text{cos}}\) denotes the cosine semantic similarity between them. The embedding function \(\phi(\cdot)\) can be implemented with sentence embedding method. The memory \(m\) updates iteratively through:
\begin{equation*}
    m^{(t+1)}, \mathcal{S} = \mathcal{F}\left(m^{(t)}; \mathcal{I}_r, \mathcal{I}_s, \mathcal{I}_i, \mathcal{I}_o, \mathcal{I}_c, \mathcal{E}, \mathcal{T}\right),
\end{equation*}

\noindent
where the update operator \(\mathcal{F}\) will be introduced in the following section. The safety status \(\mathcal{S} \in \{\text{True}, \text{False}\}\) determines action execution: if \(\mathcal{S} = \text{False}\), the guardrail blocks the agent's action; otherwise, execution proceeds.



\subsection{Safety Criteria}
Safety criteria~$\mathcal{I}_c$ refer to the objective safety standards that an LLM agent must comply with when executing user tasks. For example, in LLaMA-Guard, the safety criteria depend on the content categories~\cite{llamaguard3}. 
Similarly, for LLM agent, safety criteria also include different safety categories that specify the scope of safety checks. We built upon the safety criteria of LLM agent summarized by \citet{he2024securityaiagents}, developing universal safety criteria that can be applied across various agents. These universal safety criteria primarily consist of three safety categories:\textbf{ Information Integrity, Information Confidentiality and Information Availability}, see details in Appendix~\ref{app:method:prompt_configuration}. Our framework not only supports the universal safety criteria but also allows for manually designed safety criteria to enhance its effectiveness for specific tasks.

\subsection{Overview of Our Framework}
As shown in Figure~\ref{fig:workflow_}, our framework employs two identical LLMs as $\mathcal{F}$ and a memory module as $m$ in an iterative refinement process to ensure the effective and adaptive of agent actions during TTA.\par
\smallskip\noindent\textbf{Memory.} In our framework, the information stored in memory primarily includes agent actions, safety categories, and generated safety checks %generated based on
restricted by these safety categories. When retrieving and updating information, the agent action serves as the key. To enhance generalization and retrieve accuracy, we apply a step-back prompting~\cite{zheng2024stepbackevokingreasoning} technique to convert the agent action into both natural language and tool command language. These two representations are then concatenated to form a memory key, which is used to match similar agent actions during retrieval. However, after retrieving the relevant information, we only present the natural language form of the agent action to our framework which is to prevent any specific elements in the tool command language that interfere with our framework’s inference for safety checks.


\smallskip\noindent\textbf{Workflow.} The two LLMs play two distinct roles as the Analyzer and the Executor. The Analyzer retrieves stored safety checks $m^{(t)}$ from the $m$ and modify them based on $\mathcal{I}_r$ and $\mathcal{I}_c$. It will try revising, merging repeated safety checks, or adding new adaptive safety checks. The Executor then evaluates the safety check from the Analyzer, deciding whether to delete it if redundant, incorrect or block  legitimate agent action, and invoke external tools for validation of safety checks, or rely on self-reasoning to process it. After execution, the Executor updates the $m^{(t)}$ to $m^{(t+1)}$, allowing the framework to refine safety checks iteratively as similar agent actions are encountered. Ideally, this framework naturally converges to $\Omega^*$ for each agent action during TTA, realizing a lifelong self-adaptation paradigm. See details in Appendix~\ref{app:method:implement}.











