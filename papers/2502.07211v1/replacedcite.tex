\section{Related Works}
% \subsubsection{Customized VR resolution}
% %current work on VR resolution based on attention-level
% Despite the explosive expansion of the VR market, there are still large gaps between the huge demand for VR content and the infrastructure's capacity, particularly for VR streaming, also called 360° video streaming. 
% One advancement in video streaming involves adaptive tile-based techniques, delivering VR content by dividing the 360° video into temporal segments and spatial tiles ____.
% As users typically focus on a restricted portion of the video, known as the viewport, each tile can be individually requested at varying quality levels, prioritizing content within the viewport. Attention-based mechanisms are guidelines for adjusting the tile quality level ____.
% By employing attention, the system can dynamically allocate higher quality levels to specific tiles based on the user's focus or interest, such as the content awareness ____ and visual-attention awareness ____ within the viewport.
% Content-based attention might be more suitable when the goal is to deliver specific content elements or details that contribute to the overall understanding of the video.
% Differently, visual-based attention is beneficial when the primary objective is to enhance the viewer's experience by emphasizing high-quality rendering in areas where their eyes are directed, making it effective for immersive experiences.

 

  
% %What we have done: visual attention in three hierarchical + customized user levels.
% This paper emphasizes the immersive experience for VR users and explores three attention levels based on the hierarchical nature of the human eye. 
% We also emphasize user preferences, recognizing that individuals may have varying priorities regarding their viewing experience. 
% % For example, some users may prioritize achieving a higher frame rate, ensuring a seamless and responsive virtual environment. Alternatively, others may lean towards a smoother streaming experience. 
% By offering a spectrum of resolution levels, our approach enables users to tailor their settings according to their preferences and the capabilities of their devices.
% Therefore, this user-centric flexibility ensures a more customized VR experience.

\subsubsection{Deep Reinforcement Learning for Resource Allocation in Wireless Communications}

Resource allocation in wireless networks has attracted significant research attention, particularly through the application of DRL approaches. Several works have proposed innovative frameworks to address various challenges in different network scenarios. In heterogeneous cellular networks, ____ proposed a distributed multi-agent deep Q-network algorithm for joint mode selection and channel allocation in D2D-enabled networks, maximizing system sum-rate through partial information sharing. For UAV-assisted networks, ____ combined deep Q-learning with difference of convex algorithm (DCA) to jointly optimize UAV positions and resource allocation. In addition, ____ developed a hybrid hierarchical DRL framework integrating double deep Q networks (D3QN) and soft Actor-Critic (SAC) for secure transmission in intelligent reflective surface (IRS)-assisted networks. Moreover,
several frameworks have been proposed for energy-efficient resource allocation. ____ introduced three complementary approaches - the discrete DRL-based resource allocation (DDRA), continuous DRL-based resource allocation (CDRA), and joint DRL and optimization resource allocation (DORA) - incorporating event-triggered learning to balance complexity and performance. In semantic communications, ____ developed a deep deterministic policy gradient (DDPG)-based framework optimizing bandwidth and semantic compression ratio. For ultra-reliable and low-latency communications (URLLC) systems, ____ proposed a multi-agent (MA) DRL framework combining MA dueling double deep Q network (MA3DQN), MA double deep Q network (MA2DQN), and  MA deep Q network (MADQN) algorithms to ensure ultra-reliable and low-latency requirements while optimizing resource allocation.

While these existing approaches have made significant contributions to resource allocation in wireless networks, they primarily focus on achieving optimal performance metrics without explicitly considering the training efficiency. This paper aims to explore algorithms that can further accelerate the training process of utilizing DRL to solve resource allocation problems in wireless communications. Despite the fact that previous research has achieved satisfactory training results, the question remains whether the learning speed can be further expedited to save computational resources and improve efficiency. To address this issue, this paper delves into three critical components of DRL: action, state, and reward spaces. Through the exploration of these spaces, we seek to uncover previously overlooked information that could potentially speed up the training process.


\subsubsection{Generative Diffusion Models for Deep Reinforcement Learning}
Diffusion models have emerged as a powerful paradigm in various domains since their introduction by ____, who proposed a deep unsupervised learning framework that learns to reverse a Markov diffusion process for efficient learning and sampling. Building upon this foundation, several works have extended diffusion models to decision-making and planning tasks. For instance, ____ introduced Diffuser, a diffusion probabilistic model for trajectory optimization that enables flexible long-horizon behavior synthesis through iterative denoising. In the context of vehicular metaverses, ____ developed a hybrid-generative diffusion model that uniquely handles both discrete and continuous actions for vehicle twin migration, demonstrating superior convergence and efficiency. Recent work has further expanded the application of diffusion models to edge computing and AI-generated content. ____ proposed deep diffusion soft actor-critic (D2SAC), integrating diffusion-based AI-Generated Optimal Decision (AGOD) algorithm with soft actor-critic architecture for edge-enabled artificial intelligence-generated content (AIGC) service provider selection. Most recently, ____ introduced DiffusionGPT, a unified framework that leverages large language models to integrate diverse prompt types with domain-expert models through a Tree-of-Thought structure, advancing the capabilities of text-to-image generation systems. 



While these existing works have demonstrated the effectiveness of diffusion models across various domains, they have primarily focused on applying GDMs to specific aspects of decision-making tasks rather than exploring their potential for comprehensive DRL enhancement. This paper aims to accelerate the DRL training process in mobile wireless communication through GDM applications. Since prior research has demonstrated powerful exploration capabilities of GDMs in expanding action spaces within DRL frameworks, our work extends their application to two additional critical components of the DRL paradigm. We leverage GDMs to design more sophisticated reward functions capable of providing higher-quality feedback for action evaluation. Moreover, we utilize GDMs to augment existing state space datasets, thereby reducing the resource requirements for environmental interactions. In addition, we compared the impact of collaboratively using GDM on the learning speed of DRL in exploring these three spaces.
% due to the limited data or centralized training at the cloud due to strict privacy requirements.
% Therefore, we employ FL to pre-train a FedPromptDT model, which provides a privacy-preserving and scalable solution for customized VR streaming services that cater to user preferences.





The remainder of this paper is organized as follows: Section \ref{sec:Preliminaries} presents the preliminaries, while Section \ref{sec:system} formulates the system model and problem. 
In Section \ref{sec:algo}, we propose our framework: D2RL, designed to address the problem.
The performance evaluation of D2RL is presented in Section \ref{sec:evaluation}, discussing the results and insights gained from the experiments. Finally, Section \ref{sec:conclusion} concludes our contribution and findings.
% Besides, Table \ref{tab:notation_list} summarizes the primary notations used throughout the paper.

\vspace{-6pt}