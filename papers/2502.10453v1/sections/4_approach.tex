% !TeX root = ../main.tex

\section{Approach}
\label{sec:approach}

Our approach for linking attribution tags to knowledge graph concepts comprises two main components, as illustrated in Figure~\ref{fig:overview}: the \emph{candidate set generator} and the \emph{candidate selector} modules. The candidate set generator reduces the pool of potential actors in the knowledge graph that may correspond to an attribution tag by applying filtering and blocking techniques. Next, the candidate selector module determines which, if any, of the proposed entities match the attribution tag. The following sections offer a detailed explanation of each module.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/overview.pdf}
    \caption{\textbf{Approach Overview}. The \emph{candidate set generator} filters potential entities and the \emph{candidate selector module} identifies the matching entity.}
    \label{fig:overview}
\end{figure}

\subsection{Candidate Set Generator}
\label{subsec:csg}

% Goal
The goal of the candidate set generator is to reduce the pool of candidates in the knowledge graph that potentially match a given attribution tag. Since LLM inference is expensive and pairwise comparison is of order $O(nm)$, where $n$ is the number of records and $m$ is the number of entities in the knowledge graph, an approach that minimizes the comparisons is required.

% Approach
To reduce the overall cost, we limit the entities in each prompt to $k$ candidates by applying \emph{filtering} and \emph{blocking} techniques, thereby reducing the problem complexity to $O(nk)$, where $k \ll m$. Filtering is the process of eliminating incorrect candidates, while blocking refers to similarity-based clustering that identifies likely and unlikely candidates \cite{Papadakis2020}.

\subsubsection{Filtering}

The candidate set can be narrowed down when both the attribution tag and the corresponding knowledge graph entity are associated with categorization information from the same controlled vocabulary. For example, an attribution tag might be categorized as ``exchange'' and a knowledge graph entity as ``service'', where the latter represents a semantically broader concept than the former. When such information is available, as in the GraphSense TagPack dataset (see Section~\ref{subsec:data_gs}), it can be leveraged for filtering. We define two possible filtering methods:

\begin{itemize}
    
    \item \emph{Same-Concept Filtering}: This method excludes actors that belong to a category different from the one specified in the attribution tag.

    \item \emph{Related-Concept Filtering}: This approach is more flexible than same-concept filtering, as it leverages the taxonomy structure to exclude all actors associated with concepts unrelated to the attribution tag. Related concepts include both ancestors and descendants of the original concept, but exclude descendants of ancestor concepts that are not directly related.

\end{itemize}

\subsubsection{Blocking}

The basic idea behind blocking is to avoid comparing entities that are unlikely to match, significantly reducing the number of comparisons. This is done by partitioning the data into smaller subsets (\emph{blocks}), where entities share some similarities or common attributes.  Inspired by \cite{Paulsen2023}, who demonstrate that simple blocking methods --- without requiring machine learning or pre-trained models --- can achieve strong results, we apply two straightforward methods:

\paragraph{$\text{BM25}_3$}
This method is based on the Okapi BM25 \cite{Robertson1994} that is part of the family of term frequency-inverse document frequency (tf-idf) scoring functions. We tokenize the document and query strings into trigrams as proposed in \cite{Brinkmann2024} to allow for approximate string matching. The overall score for a tokenized document \( D \) with respect to a tokenized query \( Q \) is given by:

\begin{equation}
    \text{BM25}(D, Q) = \sum_{i=1}^{n} \text{idf}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
\end{equation}

where \( f(q_i, D) \) is the frequency of token \( q_i \) of query \( q\) in document \( D \) and \( \text{avgdl} \) is the average document length in the corpus. The inverse document frequency (IDF) is calculated as:

\begin{equation}
    \text{idf}(q_i) = \log \left( \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1 \right)
\end{equation}

We use the standard parameters \( k_1 = 1.5 \) and \( b = 0.75 \) defined in the implentation of the rank\_bm25 library\footnote{https://github.com/dorianbrown/rank\_bm25}.

\paragraph*{$\text{Overlap}_3$}
This method measures the similarity between two strings based on the overlap $\frac{|A \cap B|}{min(|A|,|B|)}$ of their trigram sets $A$ and $B$.

\subsection{Candidate Selector}
\label{subsec:cs}

The candidate selector module takes a set of candidates for each attribution tag and selects the best matching entity. Technically, this step is implemented using an LLM. In the first stage, the module constructs a batch of prompts, where each prompt corresponds to an attribution tag and includes all associated candidates. Optionally, the prompts can include examples (few-shot prompts) showcasing both matching and non-matching cases. This prompt batch is then fed to the LLM, which is tasked with either selecting the candidate that best matches the attribution tag or indicating that none of the candidates correspond to the tag.

Figure~\ref{fig:el_base_prompt} shows the template used for this prompting task. It consists of an extended system message (SYS, SYS+), the few-shot examples (FEW-SHOT), a task description (TASK), a domain statement (DOMAIN), the input data (INPUT), the selection question (QUEST), and an extended output format reminder (OUT, OUT+).

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/base_prompt.pdf}
    \caption{\textbf{Prompt template}. The template used for prompting candidate selection. It consists of several parts (e.g., SYS, FEW-SHOT, INPUT, etc.) and defines the instructions provided to an LLM to guide its response or generated output.}
    \label{fig:el_base_prompt}
\end{figure}

