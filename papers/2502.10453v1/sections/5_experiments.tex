% !TeX root = ../main.tex

\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

\paragraph{Models} The candidate selector module (see Section~\ref{subsec:cs}) leverages LLMs, integrating both remote models via the OpenAI API and locally hosted models. For our experiments, we utilized two remote and six local models, all compatible with consumer-grade hardware:

\begin{itemize}

    \item \emph{GPT-4o, GPT-3.5 Turbo}: These models, hosted by OpenAI, were employed in their specific versions: gpt-3.5-turbo-0125 and gpt-4o-2024-05-13.

    \item \emph{Jellyfish-7B, Jellyfish-13B-AWQ}: Developed and fine-tuned for data preprocessing tasks \cite{Zhang2024}, the Jellyfish models include a 7B version, based on Mistral-7B-Instruct-v0.2, and a 13B version, based on OpenOrca-Platypus2-13B. The 13B model was quantized to fit on consumer-grade GPUs using Activation-aware Weight Quantization (AWQ) \cite{Lin2024}.

    \item \emph{Mistral 7B/7B-Instruct}: Both the base and instruction fine-tuned versions of the Mistral 7B model, developed by Mistral AI, were evaluated in their v0.3 release.

    \item \emph{Meta LLama 3 8B / 8B-Instruct}: These models, developed by Meta, include both the base and instruction fine-tuned versions of the Meta LLama 3 8B model.

\end{itemize}

\paragraph{Prompt Configurations} Prompt engineering can significantly influence LLM performance in tasks such as entity linking, as demonstrated by \cite{Peeters2024}. The authors show that different template structures yield better results with certain models, while other templates are more effective with different models. Building upon the structure of our base prompt template (see Figure~\ref{fig:el_base_prompt}), we are testing nine additional shorter template confirmations, as detailled in Table~\ref{tab:el_template_config}. In all ten configurations, we are considering few-shot examples (FEW-SHOT) as optional and place them before the task description (TASK).

\begin{table}
    \centering
    \caption{\textbf{Prompt template configurations}. Each template represents a variation of the structural elements from our base prompt template in Figure~\ref{fig:el_base_prompt}.}
    \input{tables/el_template_config}
    \label{tab:el_template_config}
\end{table}

\paragraph{Hardware} All experiments are conducted on an AWS g5.xlarge instance equipped with 16 GB of RAM, 4 vCPUs powered by a second-generation AMD EPYC processor, and an NVIDIA A10G Tensor Core GPU with 24 GB of VRAM. For all models, we set the temperature to 0. Local models are run using vLLM\cite{Kwon2023}, an LLM serving system, with the \texttt{max\_num\_of\_seq} parameter set to 128 and \texttt{enable\_prefix\_caching} to true.

\subsection{Experiment 1: Candidate Set Generation}
\label{sec:exp1}

% Goal, data, and approach
The goal of this experiment is to evaluate different filtering and blocking techniques (described in Section~\ref{subsec:csg}) and find an appropriate candidate set size. We run the experiment on the GraphSense Tag Pack validation dataset on candidate set sizes ($k$) of 1, 5, 10, and 25. The task is to predict the correct actor for each sample within the $k$ candidates by comparing the tag label with the actor label, e.g., \texttt{btc-e.com} with \texttt{BTC-e}. To evaluate the performance of combined filtering and blocking techniques, we compute the ratio of attribution tags for which the correct actor link is included among the $k$ candidates. This metric is often referred to as top-k accuracy. However, since each attribution tag has exactly one correct actor link, the proportion of correctly recovered actor links, \emph{recall}, is equivalent in this context. Following prior works \cite{Papadakis2020, Paulsen2023} on blocking methods, we refer to this metric as recall. Note that in this scenario, precision depends solely on $k$ and recall, and thus does not provide additional information.

% Results
\begin{table}
    \centering
    \caption{\textbf{Effectiveness of candidate set generation}. Recall of candidate set generation for different candidate set sizes ($k$), blocking ($\text{BM25}_3$, $\text{Overlap}_3$), and filtering (\emph{same-concept}, \emph{related-concept}) techniques.}
    \input{tables/cg_recall_scores}
    \label{tab:cg_recall_scores}
\end{table}

The results in Table~\ref{tab:cg_recall_scores} demonstrate that $\text{BM25}_3$ consistently outperforms $\text{Overlap}_3$ across all candidate set sizes $k$. Using the \emph{same-concept} filter degrades performance, suggesting that it is overly restrictive. In contrast, the related-concept filter produces slightly better candidate sets than using no filtering. The candidate sets improve only marginally when their size exceeds 5. Therefore, we conclude that for our subsequent experiments, $k=5$ is the optimal candidate set size and $\text{BM25}_3$ is the preferred blocking method. The related-concept filter will be applied where relevant. An illustration of the performance on the different candidate set sizes can be found in \ref{appendix:cg} 

% Take-away
\emph{This demonstrates that basic blocking techniques, such as the $\text{BM25}_3$ blocker, are effective in generating candidate sets with as little as 5 elements, achieving 90\% recall, and that related-concept filtering can further improve recall to 93\%.}

%\emph{This experiment demonstrates the effectiveness of our approach in generating candidate sets. With a set size of only five, the $\text{BM25}_3$ blocker achieves 90\% recall. When augmented with related-concept filtering, recall improves to over 93\%.}

\subsection{Experiment 2: Candidate Selection}
\label{sec:exp2}

% Goal, data, and approach
The goal of our second experiment is to assess the performance of different LLM-based candidate selectors using various prompt template configurations (see Table~\ref{tab:el_template_config}) and few-shot examples. We run the experiment on the GraphSense TagPack validation set, using the candidate sets generated by the previous experiment. The model's task is to select the correct actor for each attribution tag from the candidate set if present, otherwise to predict that no candidate matches. The model selects a candidate by responding with the corresponding number, as illustrated in Figure~\ref{fig:el_base_prompt}. Invalid responses are classified as no match, following the guidelines of \cite{Narayan2022, Peeters2024}. We treat the task as a multiclass classification problem and evaluate the models using accuracy and macro-averages for recall, precision, and F1-score.

% Results
\begin{table}
    \centering
    \caption{Candidate Selector results on the GraphSense TagPack validation set, showing recall ($\textbf{R}$), precision ($\textbf{P}$), F1-score ($\textbf{F1}$), and accuracy ($\textbf{Acc.}$) for each modelâ€™s top-performing template ($\textbf{T}$). Best performance is in bold, second best is underlined.}
        \input{tables/cs_performance}
    \label{tab:cs_performance}
\end{table}

The results in Table~\ref{tab:cs_performance} indicate that GPT-4o outperforms all other methods in both zero-shot and five-shot scenarios. Notably, while GPT-3.5 has the weakest performance with zero examples, it achieves the second-highest F1-score when five examples are included in the prompt. The Jellyfish 7B model ranks second in zero-shot F1-score, suggesting that its fine-tuning on diverse data preprocessing tasks positively impacts the entity selection task. In contrast, the Jellyfish 13B model underperforms compared to the other local models, despite having more parameters, confirming the findings of \cite{Zhang2024} that it performs worse on unseen tasks than its 7B counterpart. Among the local models, Mistral 7B-Instruct performs the best, achieving an F1 score of more than 90\%.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/cs_result_small.pdf}
    \caption{\textbf{Model Performance with different Templates.} \emph{GPT-4o's} zero-shot results vary significantly across different templates, while \emph{Llama-3 8B} has a more stable performance.}
    \label{fig:cs_result}
\end{figure}

In Figure~\ref{fig:cs_result}, we can see that the template choice can have a significant effect on some models. For example, GPT-4o zero-shot results are close to zero on some templates, while on others, they achieve F1-scores of over 90\%. The Meta Llama 3 base model has the most stable performance and is the only model that achieves more than 60\% F1-score on every template with zero examples. An overview of all models can be found in \ref{appendix:el}.

\subsubsection{Cost Analysis}

Choosing more extensive template configurations and introducing few-shot examples increases the size of the prompts, and thus makes inference more expensive. For remote models, we define cost in \$USD based on the OpenAI API usage policy\footnote{https://openai.com/api/pricing/} that charges based on the prompt size. The current rates are (\$5,\$15) per 1 million input/output tokens for GPT-4o and (\$0.5,\$1.5) for GPT-3.5 Turbo. 

For local LLMs, we use inference time as the cost metric. We define inference time as the time taken by the model to process all prompt batches and generate the responses, which includes tokenization but does not include the loading of the weights or any prompt pre/post-processing. We calculate the average run time of five runs for each template in both zero- and five-shot settings. 

To determine which models and what configurations provide the best cost-performance value we first define value as:
\begin{equation}
    V_{\text{\tiny T/S}} = \text{F1}_{\text{\tiny T/S}} * (1 - \Tilde{C}_{\text{\tiny T/S}})
\end{equation}
where {\text{\footnotesize T/S}} is the template/shot configuration and $\Tilde{C} = \frac{C - \min(C)}{\max(C) - \min(C)}$ is the normalized cost. We normalize the cost for remote and local models separately. 

\begin{table}
    \centering
    \caption{Template/Shots (\textbf{T/S}) configuration and cost and performance differences ($\mathbf{\Delta C/F1}$) between models with the highest F1-score and the one with the best cost-performance value ($\mathbf{V}$). Local and remote models are compared separately}
    \input{tables/cs_cost}
    \label{tab:cs_cost}
\end{table}

In Table \ref{tab:cs_cost} we see that we can save over 90\% on costs when using GPT-3.5 with template 9 while only losing 1\% on the F1-score compared to the best GPT-4o configuration. The template configuration for local LLMs has less cost impact because we can reduce redundant computation to encode the large shared prefix of the prompts by using vLLM prefix caching \cite{Kwon2023}. However, using Mistral-7B-Inst with template 9 instead of template 0 still reduces inference time by more than 30\%, with a performance decrease of less than 0.5\%.

\emph{This experiment shows LLM-based entity linking is effective, with GPT-4o achieving a 94\% F1-score. Aditionally, LLM's that can run locally in a consumer-grade GPU perform well, reaching 90\% F1-score. Furthermore, we show that cost-performance analysis can yield a 90\% cost reduction for only 1\% of performance decrease.}

\subsection{Experiment 3: End to End Entity Linking}

% Goal, data, and approach
The goal of this experiment is to test our approach of linking attribution tags to a knowledge graph end-to-end and compare it to baseline solutions. We run the experiment on the GraphSense TagPack test set, WatchYourBack, and Defi Rekt datasets. All samples pass through the candidate generator, and the resulting batch of candidate sets are then fed to the candidate selector. For the candidate set generator, we employ the $\text{BM25}_3$ blocker. Additionally, we apply the \emph{related-concept} filtering for the GraphSense dataset, while no filtering is used for the other datasets, because their categories are not linked to knowledge graph concepts. For the candidate selector, we use for each LLM the template that achieved the best performance in the previous experiment. We follow experiment 2's evaluation method, with the difference that a miss by the candidate set generator is counted as an error. We compare the LLMs with the following baseline methods:

\paragraph{$\text{BM25}_3$:} We use the top ranking candidate of our $\text{BM25}_3$ blocker, and decide based on a threshold if it is a match or not. The threshold of $\mathit{15.7238}$ was determined by evaluating the modelâ€™s precision and recall on all top-candidate scores in the GraphSense TagPack validation set, and selecting the one that maximizes the F1-score.

\paragraph{UnicornPlus, UnicornPlusFT:} UnicornPlus\cite{Tu2023} is a DeBERTa-based mixture-of-expert model that is fine-tuned for data integration matching tasks. For a fair comparison, we apply the same candidate set generation process and perform pairwise matching between attribution tags and their candidates. In case multiple candidates match, we apply the softmax function on each prediction and choose the one with the highest probability. Furthermore, we create UnicornPlusFT, a fine-tuned version of the model. For this, we reshuffle our GraphSense train and validation sets with a 80/20 training/validation split and train the model for 10 epochs using the settings proposed in \cite{Tu2023}.

\begin{table}
    \centering
    \caption{Performance of various models on different datasets}
    \input{tables/e2e_performance}
    \label{tab:e2e_performance}
\end{table}

Table \ref{tab:e2e_performance} demonstrates that our end-to-end linking approach outperforms baseline methods across all datasets. Using $\text{BM25}_3$ blocking and GPT-4o as candidate selector, we achieve F1-scores of 79-85\%. With Mistral 7B-Instruct, our best-performing local candidate selector, F1-scores range from 55-82\%. Detailed results for all models and error type analysis are available in \ref{appendix:e2e}.

\emph{This experiment shows that our approach outperforms baseline methods on all three datasets by up to 37.4\% in F1-score. Furthermore, it demonstrates the generalization capabilities of our approach by achieving F1-scores of over 79\% on all datasets.}