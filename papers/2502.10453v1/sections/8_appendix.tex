
\clearpage
\begin{appendix}
\renewcommand{\thesection}{\appendixname~\arabic{section}}
%
%
\section{}  \label{appendix:cg}
A visual illustration of the candidate set generation performances of the different blocking and filtering techniques can be found in Figure~\ref{fig:cg_results}. We can see that the recall gain from $k=1$ to $k=5$ is significant, while subsequent increases in candidate set size have only a marginal impact.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/cg_results.pdf}
    \caption{\textbf{Choosing Candidate Set Size}. Recall of candidate set generation for different candidate set sizes ($k$). The performance gain for all methods visibly slows down for $k > 5$.}
    \label{fig:cg_results}
\end{figure}
%
%
\section{} \label{appendix:el}
A comprehensive overview of all candidate selector model results across each template can be found in Figure~\ref{fig:cs_result_full}. We can see that the zero-shot performance of the models varies significantly across templates; however, introducing five examples reduces this variance considerably. Interestingly, while GPT-3.5 is ineffective without examples, with five examples it achieves the second-highest F1-score. Also notable is that the zero-shot performance for templates 4, 8, and 9 is lowest, signaling that the output format reminder (OUT) is crucial for good results without examples.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/cs_result.pdf}
    \caption{\emph{Candidate Selection Experiment:} Model performances ($\mathbf{F1}$) across each template using no (\emph{Zero-Shot}) and five (\emph{Five-Shot}) examples in the prompt.}
    \label{fig:cs_result_full}
\end{figure}
%
%
\section{} \label{appendix:e2e}

In our experiments, we treat entity linking as a multi-classification problem and measure the performance with the macro F1 score and the accuracy, see Table~\ref{tab:e2e_performance_full}. For the macro F1-score, this means that we calculate each entity's individual F1 score and average them. Consequently, any prediction that is different from the ground truth is treated equally as an error. In a practical setting, we argue that errors predicting no entity matches are less problematic than errors linking the attribution tag to the wrong entity.

\begin{table}
    \centering
    \caption{Full overview of end-to-end entity linking performance including the results of all models the three datasets}
    \input{tables/e2e_performance_full}
    \label{tab:e2e_performance_full}
\end{table}

Figure \ref{fig:error} shows us the error composition of the different models across all three datasets. We distinguish between the \emph{Missed Entity} error from a wrong no-match prediction and the \emph{Wrong Entity} error with a wrong entity predicted. We can see that GPT-4o not only has the best performance in terms of accuracy and macro F1-score but also has less than 1\% of wrong entities predicted, with the majority of the errors coming from missed entities. In contrast, the local LLMs all have more \emph{Wrong Entity} than \emph{Missed Entity} errors. Our best-performing local LLM, Mistral 7B-Instruct, has, despite better overall accuracy, more \emph{Wrong Entity} errors than GPT-3.5.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/error.pdf}
    \caption{\textbf{Error analysis.} The number of missed and wrongly predicted actor links for each model in the end-to-end entity linking experiment. The experiment contained 2000 samples from three different datasets.}
    \label{fig:error}
\end{figure}

In Table~\ref{tab:e2e_wrong_actors}, we identify the three actors most frequently misclassified by each model. The results are similar across different models, with a few exceptions. For example, \texttt{maker} appears in the top three only for UnicornPlus, despite being present in 135 different tags. Notably, none of the models correctly classify any of the 20 tags associated with \texttt{0x} as an actor.

\begin{table}
    \centering
    \caption{Overview of the most frequently misclassified actors for each model}
    \input{tables/e2e_wrong_actors}
    \label{tab:e2e_wrong_actors}
\end{table}

\end{appendix}


\clearpage