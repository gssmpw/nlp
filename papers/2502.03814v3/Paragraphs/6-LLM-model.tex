\section{LLMs, Simulations, and Benchmarks}\label{sec:6-benchmark}

\subsection{LLMs and VLMs} 
LLMs and VLMs play an increasingly significant role in MRS by enabling advanced decision-making, communication, and perception-driven collaboration. Different models offer unique strengths, making them suitable for specific MRS applications. Table~\ref{tab:table1} provides a comparative summary of LLMs and VLMs used in the studies discussed earlier, highlighting their contributions to multi-robot coordination, planning, and perception.
\textbf{GPT} is one of the most extensively used language models, as demonstrated in Table~\ref{tab:table1}, where it forms the backbone of many referenced studies. Its general-purpose reasoning and adaptability allow it to be integrated into multi-robot coordination tasks such as task allocation and planning, multi-robot communication, and human-robot collaboration \cite{wu_hierarchical_2024, huang2024words, liu_coherent_2024, lakhnati_exploring_2024,hunt2024conversational}. Furthermore, GPT has been extended to \textbf{VLM} for applications requiring the integration of textual and visual inputs~\cite{garg_foundation_2024, brienza_llcoach_2024, sueoka_adaptivity_nodate}. By incorporating fine-tuning techniques and vision encoders, GPT can analyze images, generate detailed descriptions, and seamlessly combine textual reasoning with visual comprehension. These capabilities make it well-suited for tasks such as image captioning, visual question answering, and multi-modal translation, expanding its utility into complex, perception-driven applications \cite{brienza_llcoach_2024, sueoka_adaptivity_nodate, garg_foundation_2024, venkatesh_zerocap_2024}.
\textbf{Llama} offers a range of open-sourced models from lightweight smaller models to powerful large-scale ones, catering to diverse application needs. The smaller models, such as Llama 3-8B, are particularly favored for their lightweight design and flexibility, making them highly suitable for embedded or decentralized MRS architectures where computational resources are limited. On the other hand, larger models like Llama 3-70B provide enhanced capabilities and higher accuracy, making them well-suited for complex tasks requiring advanced reasoning and detailed natural language understanding.
\textbf{Claude}, on the other hand, prioritizes safety, ethical AI, and transparent decision-making, making it well-suited for regulated multi-robot applications. Additionally, Claude has been extended to function as a VLM~\cite{garg_foundation_2024}, further expanding its versatility. Its strong focus on safety and ethical considerations makes Claude a compelling choice for tasks involving sensitive visual data, such as medical imaging or content moderation. Claude's VLM implementations adopt a human-centric design, emphasizing transparency in decision-making and minimizing biases in visual interpretations. Unlike GPT, which often employs modular designs for vision-language tasks, Claude integrates safety protocols deeply into its VLM capabilities, making it well-suited for regulated industries requiring robust guardrails.
GPT, Llama, and Claude exhibit a degree of interchangeability, as evidenced by several studies~\cite{kannan_smart-llm_2024, wang_safe_2024, venkatesh_zerocap_2024, wang_dart-llm_2024} that tested architectures with multiple models, enabling comparative analysis of their performance.
\textbf{Falcon}, with its emphasis on practicality, is optimized for resource-constrained environments. For instance, the only study utilizing Falcon~\cite{lykov_llm-mars_2023} selected it as the preferred model due to computational limitations on each robot, which operated with microcomputers.
\textbf{PaLM} stands out for its multi-tasking and multi-modal capabilities, excelling in complex reasoning and cross-domain tasks such as translation and image processing. However, its partially closed-source nature and integration within Google's ecosystem make it predominantly utilized in Google DeepMind's research~\cite{ahn_vader_2024}.
Moreover, several VLMs have been explored in recent studies~\cite{ahn_vader_2024}, including PaLI, CLIP, and ViLD. PaLI, developed by Google, is a multi-modal model tailored for multilingual and cross-visual tasks such as image captioning and visual question answering, leveraging extensive multi-modal data. Similarly, CLIP, created by OpenAI, aligns images and text within a shared embedding space through contrastive learning, making it particularly effective for zero-shot tasks like image classification and retrieval. In contrast, ViLD, another model from Google, specializes in zero-shot object detection by integrating vision features with CLIP-style language alignment, enabling it to precisely identify unseen object categories.

\subsection{Simulation Environments} 
 We have summarized the simulation platforms used in related works, highlighting their contributions to evaluating and advancing the field.
\textbf{AI2-THOR} has been adapted for MRS in \cite{chen_scalable_2024, kannan_smart-llm_2024, wang_safe_2024, xu_scaling_2024} to evaluate embodied AI agents operating in complex indoor environments \cite{ai2thor}. While originally designed for single-agent tasks such as object manipulation and scene understanding, recent research extends its use to MRS scenarios, including cooperative object retrieval, shared perception, and collaborative planning in constrained environments. The physics-enabled interactions allow researchers to test LLM-driven coordination strategies in dynamic and physically grounded environments, where multiple agents must navigate, manipulate objects, and resolve conflicts dynamically.
\textbf{PyBullet} is an open-source physics engine widely used for simulating robotic systems, including articulated manipulators, wheeled robots, and multi-agent interactions \cite{coumans2021}. It provides real-time physics simulations, supporting tasks like collision detection, rigid body dynamics, and reinforcement learning in robotics. In the context of MRS, PyBullet enables accurate modeling of decentralized collaboration, object manipulation, and dynamic environment interactions \cite{yu_mhrc_2024}.
\textbf{BEHAVIOR-1K}, utilized by Liu \etal~\cite{liu_coherent_2024}, serves as the foundation for the COHERENT framework, which focuses on large-scale, heterogeneous multi-robot collaboration \cite{li2023behavior}. This platform facilitates training and evaluation in complex household-like environments where different types of robots (e.g., manipulators, mobile bases) must coordinate to accomplish everyday tasks such as table setting, object handoff, and multi-step assembly processes. The benchmark ensures that LLM-enhanced systems can handle dynamic task dependencies and ambiguous role assignments.
The \textbf{Pygame} platform is a cross-platform Python module set designed for video game writing. Robots are modeled as point-mass entities, focusing on formation control, decentralized consensus algorithms, and motion coordination without obstacle avoidance. This platform is particularly useful for analyzing emergent behaviors in swarms, where LLM-based controllers guide self-organized formations through simple local interactions \cite{venkatesh_zerocap_2024}.
\textbf{Habitat-MAS}, an extension of Habitat, introduces explicit multi-agent communication for indoor navigation and exploration \cite{habitat19iccv, szot2021habitat, puig2023habitat3}. Unlike the single-agent focus of its predecessor, Habitat-MAS enables studies on cooperative search, simultaneous localization and mapping (SLAM), and inter-agent strategy adaptation, crucial for deploying multi-robot exploration teams in disaster response and service robotics \cite{chen_emos_2024}.
\textbf{ROS}-based simulation is a middleware framework widely used for MRS, enabling inter-robot communication \cite{lampe2023robotkube}
%  ROS FOR MRS
, decentralized control, and real-time data sharing. It provides essential tools for swarm coordination, collaborative mapping, and distributed task allocation. With built-in simulation environments like Gazebo and RViz, ROS allows researchers to develop and test MRS strategies for exploration, target tracking, and cooperative manipulation \cite{wu_hierarchical_2024}.
\textbf{VR} platform introduces immersive simulations for human-robot collaboration and reinforcement learning. These environments are used to test human-in-the-loop control strategies for heterogeneous robot teams, such as coordinating robotic arms and mobile robots in warehouse logistics through natural language instructions \cite{lakhnati_exploring_2024}.
\textbf{GAMA} offers a multi-agent modeling environment suited for large-scale robot interactions \cite{gupte_rebel_2024}. It supports evaluations of distributed swarm intelligence, multi-agent task negotiation, and behavior adaptation in unstructured environments, making it ideal for testing decentralized LLM-driven controllers in logistics and autonomous fleet management.
\textbf{SimRobot}, utilized by Brienza \etal~\cite{brienza_llcoach_2024}, is specialized for multi-robot teamwork in robotic soccer. The LLCoach framework, trained using SimRobot, enhances robot coordination and strategic planning by processing match data and optimizing multi-agent role assignments dynamically.
\textbf{ARGoS}, chosen by Strobel \etal~\cite{strobel_llm2swarm_2024}, is a scalable platform for swarm robotics research. It enables controlled experiments on decentralized control mechanisms, including aggregate-then-disperse behaviors, leader election, and emergent self-organization. LLMs integrated into ARGoS are evaluated on their ability to generate adaptive communication protocols and handle task partitioning in dynamic environments.
These diverse platforms provide essential tools for evaluating LLM-driven MRS across different scales, from small collaborative teams to large autonomous swarms. By leveraging these environments, researchers refine multi-agent coordination, communication, and decision-making strategies, advancing the integration of LLMs in MRSs for real-world applications.
\subsection{Benchmarks} 
Benchmarks are essential for evaluating LLM-driven MRS, providing standardized environments to measure coordination, adaptability, and performance across diverse scenarios. They enable consistent comparisons, helping identify strengths, limitations, and the effectiveness of MRS in real-world applications.
\textbf{RoCoBench}, introduced by Mandi \etal~\cite{mandi_roco_2024}, focuses on human-robot collaboration in fine-grained manipulation tasks. While primarily designed for single-robot control, RoCoBench also provides insights into multi-robot collaboration, particularly in tasks requiring shared manipulation, coordinated actions, and real-time adjustments to changing conditions. The benchmark provides detailed metrics on precision, task success rates, and robustness under unpredictable physical conditions, making it valuable for evaluating LLM-assisted multi-robot cooperation in human-shared workspaces.
\textbf{ALFRED}, utilized in Xu \etal~\cite{xu_scaling_2024}, integrates language and vision benchmarks to test agents' ability to follow natural language instructions and execute multi-step tasks in household environments. Though originally focused on single-agent evaluations, ALFRED's framework can be extended to multi-robot task coordination, testing MRS on collaborative planning, sequential action execution, and efficient division of labor in domestic or service robotics applications.
\textbf{BOLAA}, proposed by Liu \etal~\cite{liu_bolaa_2023}, introduces a multi-agent orchestration benchmark specifically designed for LLM-augmented autonomous agents (LAAs). Unlike conventional evaluations that focus on individual agents, BOLAA assesses how LLMs manage multi-agent interactions, optimizing task distribution, decision-making, and real-time adaptability. This makes it a useful benchmark for LLM-driven MRS, where autonomous robots benefit from effective communication and collaboration to tackle complex, long-horizon objectives.
\textbf{COHERENT}-Benchmark, developed by Liu \etal~\cite{liu_coherent_2024}, is specifically designed for heterogeneous multi-robot collaboration in dynamic and realistic scenarios. Built on the BEHAVIOR-1K platform, this benchmark evaluates MRS across diverse environments, requiring coordinated task execution among robots with distinct capabilities—such as quadrotors for aerial mapping, robotic dogs for agile mobility, and robotic arms for precise manipulation. Key evaluation metrics include task allocation efficiency, inter-robot communication, and collaborative problem-solving, making it a comprehensive benchmark for testing LLM-driven coordination strategies in MRS.
