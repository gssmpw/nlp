\section{Conclusion and Future Work}
\label{sec:conclusion}
In this work, we explored Visual Instruction Rewriting as a lightweight, privacy-preserving approach to multimodal interaction on AR, VR, and smartphone devices. With a strong emphasis on dataset development, we present a diverse collection of 39,000+ examples covering 14 domains, enabling robust training for on-device instruction rewriting. Our approach ensures that text-only inference is more secure in privacy-sensitive settings by \textbf{eliminating the need to send personal vision-related images to the server}, reducing data exposure risks. Additionally, rewriting removes the necessity of storing images, making multimodal AI systems more efficient and privacy-focused. Our experimental results show that even an 8-bit quantized model maintains strong performance while significantly reducing memory requirements. For future work, we aim to expand data coverage by incorporating more diverse real-world multimodal instructions and introducing multilingual support to enhance accessibility. Furthermore, improving deixis resolution with bounding box annotations and localized image region training will enhance reference grounding while integrating gaze tracking and tactile input can further refine contextual understanding in on-device AI assistants.