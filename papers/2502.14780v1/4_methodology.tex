\section{Developing a Baseline Rewriter VLM}
\label{sec:method}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\columnwidth, height=6cm]{images/diagram2.png}
  \caption{Revision Model architecture}
  \label{fig:diagram}
\end{figure}

We develop a lightweight vision-language model (shown in Figure \ref{fig:diagram}) tailored for instruction-following tasks by integrating a pretrained vision encoder with an instruction-tuned language model, following the popular multimodal fusion approach \cite{zhang2024vision}. Since vision encoders and instruction-tuned language models operate in different embedding spaces, a multimodal projector \cite{liu2023llava} is used to align the encoded image features with the token embedding space of the language model. Our approach is similar to PaLI-Gemma \cite{beyer2024paligemma}, where an image encoder based on the SigLIP architecture \cite{zhai2023sigmoid} extracts $D$-dimensional image encodings for $N$ patches from a single input image, say $V_1, V_2, ..., V_N$).  Building on \newcite{laurenccon269587869matters}, who demonstrated that using a sampling technique to extract the most relevant $M$ patch encodings from a larger set of $N$ samples improves efficiency, we employ \textit{Perceiver Sampler} \cite{jaegle2021perceiver} to downsample the $N$ patch embeddings into $M$ D-dimensional encodings. These image encodings are then mapped into a shared embedding space via a linear multimodal projector, ensuring compatibility with the language model’s $H$-dimensional token embeddings. We fix $K$ at 64. The projected image embeddings ($H_1, H_2, ..., H_M$) are concatenated with the token embeddings extracted from the tokenized textual input ($H_1, H_2, ..., H_K$), where $K$ represents the number of input tokens. The combined embeddings are then processed by the language model to generate responses. To ensure consistency in input representation, we apply image preprocessing, tokenization, and chat template formatting, making the model familiar with structured multimodal input formats. 

Although large-scale vision-language models typically involve hundreds of millions of parameters, our focus is on designing a compact and efficient model capable of running on-device. To maintain a parameter budget under 250M, we select a small SigLIP encoder \cite{zhai2023sigmoid} (\texttt{google/siglip-base-patch16-256}), which processes images of size $256 \times 256$ by dividing them into $16 \times 16$ patches, with 768 dimensions in hidden layers. The language model is a 150M-parameter instruction-tuned model from OuteAI\footnote{\url{https://www.outeai.com}} (\texttt{OuteAI/Lite-Mistral-150M-v2-Instruct}) based on the Mistral architecture \cite{jiang2023mistral}, featuring a vocabulary size of 32,768 and a hidden dimension of 768. Since the hidden dimensions of both the vision encoder and the language model are identical, the projector acts purely as a dimensional transformer without altering the shape of the embeddings. While the model's limited size may impact its ability to handle multi-turn conversations, it is well-suited for single-turn multimodal instruction rewriting tasks. Additionally, since the model is designed for multimodal deixis resolution, it may not be effective for resolving text-only references in extended conversations\cite{ates2023marrs}.  

\subsection{Model Pretraining}

To pretrain the model, we adopt an end-to-end training strategy, leveraging datasets from three key sources: (a) LLaVA-ReCap-CC3M, (b) LLaVA-Pretrain, and (c) LLaVA-CC3M-Pretrain-595K. These datasets are curated from large-scale image-text corpora, including LAION \cite{schuhmann2021laion400mopendatasetclipfiltered}, Conceptual Captions (CC) \cite{sharma-etal-2018-conceptual}, and SBU \cite{NIPS2011_5dd9db5e}, which are filtered for balanced concept distribution and enhanced with synthetic captions generated via BLIP to improve vision-language alignment \cite{lmms2023llava, liu2023pretrain, liu2023cc3m}. Specifically, LLaVA-ReCap-CC3M focuses on re-captioning images to improve concept coverage, while LLaVA-Pretrain consists of 558K image-caption pairs, forming a strong foundational dataset for multimodal alignment. The LLaVA-CC3M-Pretrain-595K dataset, derived from Conceptual Captions 3M, provides a rich set of image-text pairs to enhance model robustness. The total number of examples is thus a little more than 4M. Despite some redundancy in images across datasets, we ensure sufficient data diversity and scale to instill basic image-text alignment capabilities in our pretrained model.  

For pretraining, we use the following configurations: a batch size of 16, trained for 2 epochs, using the AdamW optimizer with a learning rate of $2\times10^{-5}$ and a linear learning rate schedule. The training was conducted on consumer-grade GPUs (NVIDIA RTX 3090) over 3 days, using PyTorch and Hugging Face’s Transformers library for implementation. We refer to our pretrained model as \texttt{ReVision-250M-64-16}.

\subsection{Model Fine-Tuning}
For the instruction rewriting task, we conduct fine-tuning under multiple configurations, trained on our dataset (\ref{sec:datasets}). We will refer to the rewritten prompts from this dataset as the ``reference'' prompts. Below, we describe the fine-tuning setups and the rationale behind integrating metadata-driven enhancements to improve performance on text-dense images.

\begin{itemize}
\item \textbf{ReVision-BL}: This is the baseline fine-tuned model. The input consists of an image, a rewrite prompt, and an instruction, while the model generates a rewritten version of the instruction in response.

\item \textbf{ReVision-Metadata}: In this, we augment the input with ``metadata'', namely the image caption and an external OCR-extracted text. To differentiate the rewrite prompt and instruction from the auxiliary metadata, we prefix the prompt-instruction and metadata sections with \texttt{<task>} and \texttt{<data>}, respectively. Collectively, the input consists of an image, a prefixed rewrite prompt and instruction, and a prefixed caption and OCR text and the output is a rewritten instruction. 
\end{itemize}

The motivation for integrating metadata arises from the limitations of small-scale vision-language models (VLMs). Despite being optimized for rewriting tasks, small VLMs struggle with extracting embedded text from images. OCR is a specialized capability distinct from traditional vision-language alignment \cite{lamm2024can, nagaonkar2025benchmarking}. However, most modern devices are equipped with built-in OCR and image description capabilities, making it practical to supplement the model with external text recognition systems. To systematically evaluate this approach, we present two different metadata extraction:

\begin{itemize}
\item \textbf{GPT-4o\_Caption+OCR}: We use GPT-4o to generate both captions and OCR-extracted text, simulating a high-end device equipped with an advanced OCR and captioning system.
\item \textbf{Self\_Caption+EasyOCR}: We use rewriter models to generate captions themselves using the simple prompt: \textit{“Caption this:”}. For OCR, we employ EasyOCR\footnote{\url{https://github.com/JaidedAI/EasyOCR}}, a lightweight text extraction model based on the CRAFT algorithm \cite{baek2019character}, simulating a low-resource on-device setting.
\end{itemize}

The fine-tuning procedure follows a similar framework as pretraining but with optimized hyperparameters for smaller-scale adaptation. The vision encoder is frozen during fine tuning and the number of training epochs is increased from 2 to 5 to compensate for the smaller dataset size. The batch size remains at 16, but gradient accumulation steps are reduced from 4 to 1, allowing for more frequent model updates. The learning rate remains stable at $2\times10^{-5}$ with the same linear rate schedule, maintaining a conservative optimization approach. Additionally, the number of warm-up steps is lowered from 100 to 10, reflecting the shorter training duration. To simulate a realistic fine-tuning environment where such models could be updated on-device, we conduct fine-tuning on a consumer-grade desktop equipped with an NVIDIA GeForce RTX 2070 SUPER (8GB VRAM). Each fine-tuning run took approximately 5.5 to 6 hours.

For baseline comparisons, we evaluate our model against state-of-the-art small-scale VLMs: PaliGemma-v2 (10B) and QwenVL-v2 (7B), known for strong performance in OCR, captioning, and multimodal reasoning. However, deploying these models on-device is impractical without high-end GPUs. To ensure a fair comparison, we assess them as-is with optimized prompting but without fine-tuning, reflecting real-world constraints. While fine-tuning could improve accuracy, their size and hardware demands make them unsuitable for mobile applications, thus highlighting the need for lightweight models like ours.

To further assess on-device deployment feasibility, we evaluated the \textit{8-bit quantized} version of our fine-tuned models. This approach reduces memory by up to fourfold, lowering computational demands while maintaining competitive performance. Though quantization may slightly reduce accuracy, the simplicity of the rewriting task makes this trade-off worthwhile. We examine whether an 8-bit model can efficiently handle multimodal instruction rewriting while staying lightweight for real-world use.

\begin{table*}[t]
    \centering
    \footnotesize
    \begin{tabular}{lccccc|cc}
        \toprule
        \textbf{Model} & \multicolumn{2}{c}{\textbf{ROUGE-N}} & \textbf{ROUGE-L} & \textbf{BLEU} & \textbf{MET-} & \textbf{Intent} & \textbf{Arg} \\
                       & N=1              & N=2              &                  &               & \textbf{EOR}  & \textbf{Acc}   & \textbf{MJS} \\
        \midrule
        BL\textsubscript{1a}: PaliGemma2-10B\textsubscript{vanilla} & 3.4 & 0.5 & 3.3 & 0.03 & 2.3 & 16.2 & 42.7 \\
        BL\textsubscript{1b}: Qwen2-VL-7B\textsubscript{vanilla}   & 43.7 & 24.7 & 40.8 & 12.3 & 39.5 & 50.3 & 65.2 \\
        BL\textsubscript{2a}: PaliGemma2-10B\textsubscript{Self\_Caption+EasyOCR} & 11.1 & 2.5 & 11.1 & 0.03 & 4.5 & 19.3 & 30.0 \\
        BL\textsubscript{2b}: Qwen2-VL-7B\textsubscript{Self\_Caption+EasyOCR} & 41.3 & 24.0 & 38.7 & 8.4 & 39.1 & 61.2 & 67.0 \\
        \hline
        ReVision-BL                         & 56.9 & 41.4 & 55.4 & 27.7 & 61.4 & 56.5 & 68.8 \\
        ReVision-Metadata\textsubscript{GPT-4o\_Caption+OCR} & 72.4 & 60.6 & 71.5 & 49.9 & 74.4 & 62.4 & 73.7 \\
        ReVision-Metadata\textsubscript{Self\_Caption+EasyOCR}     & \textbf{79.3} & \textbf{70.0} & \textbf{78.4} & \textbf{61.5} & \textbf{80.2} & \textbf{71.5} & 74.5 \\
        ReVision-Metadata\textsubscript{Self\_Caption+EasyOCR(8bit)} & \textit{79.2} & \textit{69.9} & \textit{78.3} & \textit{61.3} & \textit{80.1} & \textit{67.6} & \textbf{\textit{79.5}} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation Results for Baseline and RV Models as a Percentage. BL = Baseline; ROUGE-N = N-grams between the system and reference summaries; ROUGE-L = Longest common subsequence-based statistics; BLEU = BiLingual Evaluation Understudy; METEOR = Metric for Evaluating Translation with Explicit Ordering; Intent Acc = Intent Accuracy; Arg MJS = Argument Mean Jaccard Similarity.}

    \label{tab:evaluation_results}
\end{table*}


\section{Evaluation Metrics}
To evaluate our models in Visual Instruction Rewriting, we use standard NLG metrics (BLEU, METEOR, ROUGE) \cite{sharma2017nlgeval} alongside task-specific semantic parsing evaluations. While NLG metrics assess linguistic similarity, they do not capture functional quality in downstream AI systems. Effective rewriting must ensure instructions remain interpretable by semantic parsers extracting intent and arguments \cite{louvan2020recent}. In the absence of a baseline parser for our ontology (Figure \ref{fig:intent_argument_box}), we use GPT-4 as a proxy parser to classify intents and extract arguments, simulating an on-device parser. We compare GPT-4-generated parses for reference and model rewrites to verify preservation of intent and structured arguments. We measure the following metrics:

\begin{itemize}
    \item \textbf{Intent Accuracy:} Exact match of intent labels between reference and model-generated rewrites, assessing task-specific intent preservation. 
    
    \item \textbf{Argument Similarity:} Mean Jaccard Similarity (MJS) between argument labels from reference and model rewrites, ensuring retention of key task-related arguments.
\end{itemize}

\begin{figure*}[t]
  \includegraphics[width=\textwidth, height=4cm]{images/F1.pdf}
  \caption{Class-wise F1 Scores for Intent Classification}
  \label{fig:F1}
\end{figure*}