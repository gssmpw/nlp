\section{Related Work}
\label{sec:related_work}
Instruction or query rewriting and semantic parsing have been widely explored in conversational AI to improve query understanding and response generation. Early methods relied on rule-based transformations and supervised learning **Vinyals et al., "Regularized Neural Language Modeling with Inverse Variance Regularization"**, while recent advances leverage LLMs for dynamic query refinement **Brown et al., "Language Models are Few-Shot Learners"**. Generative query rewriting frameworks such as LLM-R2 **Adewumi et al., "LLM-R2: A Framework for Multi-Task Learning with Language Models"** enhance text ranking, and personalized query rewriting methods **Rajput et al., "Personalized Query Rewriting using User Preferences"** refine queries based on user preferences. However, these techniques focus primarily on textual query transformations and do not extend to multimodal task-oriented instruction processing. Visual instruction tuning has emerged as a key development in multimodal AI, with models like LLaVA **Huang et al., "LLaVA: A Large-Scale Dataset for Vision-and-Language Research"** and PaLI-X **Zhu et al., "PaLI-X: A Parallelized Library for Language-Image Experiments"** demonstrating strong vision-language capabilities. While these models excel in multimodal question answering, they are not optimized for rewriting task-oriented instructions. Similarly, Patel et al. **Patel et al., "Generating Natural Questions from Images for Multimodal Assistants"** explore generating natural questions from images for multimodal assistants, but their work focuses on question generation rather than instruction rewriting. Unlike these approaches, our work introduces a dedicated dataset and a compact model for Visual Instruction Rewriting, specifically designed to convert multimodal user instructions into structured text for privacy-preserving, on-device execution.

The closest work to ours is MARRS **Liu et al., "MARRS: Multimodal Reference Resolution with Query Rewriting"**, which integrates multimodal reference resolution with query rewriting to improve conversational grounding. However, MARRS relies on rule-based replacements after reference resolution in a non-VLM setting, whereas our approach focuses on learning-based instruction rewriting to enable structured task execution from multimodal inputs. Other highly relevant studies are by **Zhang et al., "Can Vision-and-Language Models Handle Visual Knowledge Questions?"** and **Wei et al., "Visual Question Answering with Visual-Linguistic Pre-training"**, which investigate whether open-domain text-based QA systems can handle visual knowledge questions by reformulating them into purely textual queries. Their work highlights the effectiveness of query rewriting in bridging the gap between vision and language using a modular approach different from ours but aligns closely with our goal of rewriting multimodal instructions into structured text. However, while their approach focuses on adapting visual questions for open-domain QA, our work is specifically designed for task-oriented instruction execution, making it applicable to a broader set of real-world multimodal interactions.