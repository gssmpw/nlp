\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth, height=10cm]{images/mindmap2.pdf} 
  \caption{Mindmap showing Data Collection and Rewrite Desiderata}
  \label{fig:mindmap}
\end{figure*}
% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\textwidth]{images/process.pdf} 
%   \caption{Dataset Creation Pipeline}
%   \label{fig:process}
% \end{figure*}
\section{Constructing a Dataset for Visual Instruction Rewriting}
\label{sec:datasets}

Task-oriented conversational AI systems rely on a semantic parser to interpret user intent and extract structured arguments \cite{louvan2020recent,aghajanyan2020conversational}. For example, when a user says,\textit{ "Add the team meeting to my calendar for Friday at 3 PM"}, the system must parse the intent (\textit{CreateCalendarEvent}) and extract arguments such as the \textit{EventTitle} (``team meeting''), \textit{EventDate} (``Friday''), and \textit{EventTime} (``3 PM'') to schedule the event correctly. Unlike purely text-based interactions, multimodal instructions, particularly those directed at conversational AI assistants on AR/VR devices (\textit{e.g.,} Apple's Siri for Apple Vision Pro), introduce additional challenges such as ellipsis and coreference resolution. For instance, a user may look at a book cover and ask, \textit{“Who wrote this?”} or point at a product in an AR interface and say, \textit{“How much does this cost?”} Traditional text-based semantic parsers struggle with such instructions since critical visual context is missing. Thus, to bridge the gap between multimodal input and existing conversational AI stacks, we introduce a dataset specifically designed for \textit{rewriting multimodal instructions} into structured text that can be processed by standard text-based semantic parsers. Figure \ref{fig:mindmap} illustrates a representation of the dataset collection requirement, highlighting the transformation of multimodal inputs into text-based rewrites.

To construct our dataset, we first define an ontology of intents and arguments, as existing ontologies in conversational AI and semantic parsing are often proprietary and unavailable for research use. We take inspiration from \newcite{goel2023presto} for ontology and extend it to accommodate multimodal task-oriented interactions. Figure \ref{fig:intent_argument_box} (ref. Appendix) presents an overview of the intents and arguments in our ontology. Next, we curate a diverse set of images covering various real-world multimodal interaction scenarios, including book covers, product packaging, paintings, mobile screenshots, flyers, signboards, and landmarks. These images are sourced from publicly available academic datasets, such as OCR-VQA\footnote{\url{https://ocr-vqa.github.io/}}, CD and book cover datasets, Stanford mobile image datasets\footnote{\url{http://web.cs.wpi.edu/~claypool/mmsys-dataset/2011/stanford/}}, flyer OCR datasets\footnote{\url{https://github.com/Skeletonboi/ocr-nlp-flyer.git}}, signboard classification datasets\footnote{\url{https://github.com/madrugado/signboard-classification-dataset}}, Google Landmarks\footnote{\url{https://github.com/cvdfoundation/google-landmark}}, and Products-10K\footnote{\url{https://products-10k.github.io/}}.

\begin{table}[t]
    \centering
    \scriptsize
    \label{tab:dataset_statistics}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Category} & \textbf{Total} & \textbf{Train} & \textbf{Test} \\
        \midrule
        Book              & 485 / 500                               & 386 / 399                               & 101 / 101                               \\
        Business Card     & 26 / 960                                & 26 / 772                                & 26 / 188                                \\
        CD               & 27 / 1,020                              & 27 / 835                                & 27 / 185                                \\
        Flyer & 159 / 5,940                             & 159 / 4,742                             & 159 / 1,198                             \\
        Landmark         & 511 / 19,274                            & 511 / 15,420                            & 511 / 3,854                             \\
        Painting & 27 / 980                                & 27 / 774                                & 27 / 206                                \\
        Product          & 499 / 10,349                            & 499 / 8,276                             & 492 / 2,073                             \\
        \midrule
        \textbf{Total}   & \textbf{1,734 / 39,023}                 & \textbf{1,635 / 31,218}                 & \textbf{1,343 / 7,805}                  \\
        \bottomrule
    \end{tabular}
    \caption{Number of Images/Instructions per Category}
    \label{tab:sources}
\end{table}
\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{l  c}
        \toprule
         \textbf{Annotator}& \textbf{Percentage of Correct Captions}\\ 
         \midrule
         Annotator 1	& 90.62\%\\ 
         Annotator 2	& 87.23\%\\
         Annotator 3	& 86.35\%\\
         \midrule
         \textbf{At least two }& \textbf{92.18}\%\\
         \midrule
         \textit{All three }& \textit{74.63}\% \\
         \bottomrule
    \end{tabular}
    \caption{GPT-4 Instruction Rewriting Validation Results from Amazon Mechanical Turk }
    \label{tab:annotator_data}
\end{table}
\begin{figure}[t]
\includegraphics[width=\columnwidth]{images/intent.png}
  \caption{Dataset Distributions By Intent}
  \label{fig:intent}
\end{figure}
Upon identifying and verifying the images, we employ the GPT-4 model from OpenAI \cite{achiam2023gpt} to systematically generate and refine multimodal instructions into rewritten text-based instructions. The process begins with a bootstrap phase, where GPT-4 is prompted to generate 20 direct questions per image by explicitly referencing visible objects or textual elements while adhering to the intent list defined in Figure \ref{fig:intent_argument_box}. A second prompting phase then validates the generated questions against the corresponding image, filtering out ambiguous or irrelevant instructions to ensure alignment with the visual context. 

In the rewriting phase, GPT-4 is tasked with paraphrasing the validated instructions, ensuring that the transformed questions are fully self-contained and interpretable without requiring the image. This transformation is crucial for enabling multimodal conversational AI systems to process instructions using purely text-based stacks. Finally, a verification phase prompts the model to assess the rewritten questions in relation to both the original instruction and the image, ensuring semantic fidelity and eliminating inconsistencies. This multi-stage prompting strategy resulted in a dataset of 39,023 original-rewritten instruction pairs, derived from 1,734 images, with an 80\%-20\% train-test split. Table \ref{tab:sources} provides a breakdown of image sources.

While automated validation ensures consistency across different stages, human evaluation remains critical for verifying the dataset’s reliability. To this end, we conducted an annotation task via Amazon Mechanical Turk (AMT) to validate rewritten instructions within the test set for indirect image-based instructions. Each annotation task followed a structured validation guideline, where annotators reviewed an image, its original multimodal instruction, and the rewritten text-only instruction, determining whether the reformulation preserved the intent and meaning of the original instruction. Annotators were instructed to select "Accept" if the rewritten instruction was correct or "Reject" if it failed to capture the original meaning. Annotators are incentivized appropriately for this binary grading task. Agreement analysis, as shown in Table \ref{tab:annotator_data}, indicates that in 92.2\% of cases, at least two annotators agreed on "Accept," while 74.6\% of instructions achieved full consensus across all three annotators. Despite a Fleiss' Kappa score of 0.278—suggesting fair inter-annotator agreement—the high rate of majority consensus supports the dataset’s reliability for real-world use. Given these results, we publicly release the full dataset along with raw AMT responses, enabling further analysis, filtering, and refinements by the research community.

Figure \ref{fig:intent} presents the distribution of intents in our dataset, categorized into training and test splits. The distribution reflects practical usage patterns in real-world multimodal conversational AI systems, with a higher occurrence of general QA and web search, alongside diverse task-oriented intents such as reminders, messaging, and navigation, ensuring coverage of frequent user interactions.



% In this study, we utilize a comprehensive multimodal dataset curated from various sources to facilitate research in multimodal instruction rewriting using compact models. Table~\ref{tab:dataset_statistics} provides an overview of the dataset's composition, detailing the number of images and corresponding instructions sourced from different domains. This diverse dataset is designed to challenge models in interpreting and rewriting instructions based on both visual and textual information embedded within images.

% The dataset is organized into a single TSV file, \texttt{all\_data.tsv}, which consolidates all the data for streamlined processing and analysis.

% The dataset is publicly accessible and can be downloaded from our Hugging Face repository:
% \url{https://huggingface.co/datasets/utischoolnlp/multimodal_instruction_rewrites}.

% \begin{table}[h]
%     \centering
%     \caption{Dataset Statistics}
%     \label{tab:dataset_statistics}
%     \resizebox{0.5\textwidth}{!}{%
%         \begin{tabular}{|l|l|c|c|}
%             \hline
%             \textbf{Data Source} & \textbf{Type} & \textbf{Number of Images} & \textbf{Number of instructions} \\ \hline
%             \href{https://github.com/gulvarol/grocerydataset}{Grocery Store Dataset} & Grocery Dataset & 287 & 5,945 \\ \hline
%             \href{https://amazon-berkeley-objects.s3.amazonaws.com/index.html}{Amazon Berkeley Objects} & Amazon Dataset & 187 & 3,890 \\ \hline
%             \href{https://products-10k.github.io/}{Products-10K} & E-commerce Dataset & 23 & 472 \\ \hline
%             \href{https://www.kaggle.com/datasets/vikashrajluhaniwal/fashion-images}{Fashion Images} & Fashion Clothing Dataset & 2 & 42 \\ \hline
%             \textbf{Total} & & \textbf{499} & \textbf{10,349} \\ \hline
%         \end{tabular}
%     }
% \end{table}


% \subsection*{Additional Dataset Statistics}

% To provide a deeper understanding of the dataset's characteristics, we present the following statistics derived from \texttt{all\_data.tsv}:

% \begin{itemize}
%     \item \textbf{Prompt Length}:
%     \begin{itemize}
%         \item \textbf{Average Prompt Length}: 80.99 tokens
%         \item \textbf{Maximum Prompt Length}: 160 tokens
%         \item \textbf{Minimum Prompt Length}: 28 tokens
%     \end{itemize}
    
%     \item \textbf{Rewritten Question Length}:
%     \begin{itemize}
%         \item \textbf{Average Rewritten Question Length}: 56.94 tokens
%         \item \textbf{Maximum Rewritten Question Length}: 160 tokens
%         \item \textbf{Minimum Rewritten Question Length}: 28 tokens
%     \end{itemize}
% \end{itemize}

% These statistics highlight the complexity and variability of the prompts and their corresponding rewritten questions, providing a robust foundation for training and evaluating multimodal instruction rewriting models.

% \subsection*{Dataset Composition}

% The dataset is consolidated into a single TSV file, \texttt{all\_data.tsv}, which includes all image-instruction pairs. This unified format simplifies data handling and ensures consistency across training and evaluation phases. The structure of \texttt{all\_data.tsv} is as follows:


% \begin{itemize}
%     \item \textbf{Columns}:
%     \begin{itemize}
%         \item \texttt{Image\_ID}: Unique identifier for each image.
%         \item \texttt{Image\_URL}: Direct link to the image file.
%         \item \texttt{Prompt}: Original instruction associated with the image.
%         \item \texttt{Rewritten\_Question}: Reformulated version of the original instruction.
%     \end{itemize}
% \end{itemize}

% \subsection*{Dataset Accessibility}

% Researchers and practitioners can access the dataset and its associated resources through our Hugging Face repository:
% \url{https://huggingface.co/datasets/utischoolnlp/multimodal_instruction_rewrites}.

% The dataset is organized in a structured format, including:
% \begin{itemize}
%     \item \texttt{all\_data.tsv}: Consolidated dataset containing all image-instruction pairs.
%     \item \texttt{images.zip}: Compressed archive of all dataset images.
%     \item \texttt{README.md}: Detailed instructions and metadata descriptions for dataset usage.
% \end{itemize}

% \subsection*{Discussion}

% The diversity of data sources, ranging from grocery items to fashion clothing, ensures that the dataset covers a wide array of visual and textual contexts. This variety is crucial for training models that are robust and generalizable across different domains. The substantial number of instructions relative to images indicates that each image is associated with multiple instructions, providing ample data for effective model training and evaluation.

% By consolidating all data into a single TSV file, we streamline the data processing pipeline, facilitating easier integration with various modeling frameworks and tools. The comprehensive statistics on prompt and rewritten question lengths further underscore the dataset's complexity, challenging models to handle a wide range of instruction formulations.

% \section*{Conclusion}

% Our multimodal instruction rewriting dataset offers a comprehensive resource for researchers aiming to develop and evaluate models in this domain. By providing a diverse and sizeable dataset, we aim to facilitate advancements in multimodal understanding and contribute to the broader field of artificial intelligence.

% \section*{References}

% \begin{itemize}
%     \item \href{https://github.com/gulvarol/grocerydataset}{Grocery Store Dataset}
%     \item \href{https://amazon-berkeley-objects.s3.amazonaws.com/index.html}{Amazon Berkeley Objects}
%     \item \href{https://products-10k.github.io/}{Products-10K}
%     \item \href{https://www.kaggle.com/datasets/vikashrajluhaniwal/fashion-images}{Fashion Images Dataset}
% \end{itemize}

% \label{sec:dataset}