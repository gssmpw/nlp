 % This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{natbib}
% Remove the "review" option to generate the final version.
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{authblk}
\usepackage{ACL2025}
\usepackage{hyperref}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{array} % required for text wrapping in tables


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{4.2cm}
%
% and set <dim> to something 5cm or larger.

\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\usepackage[labelfont=bf]{caption}
\captionsetup{labelfont=bf}

\usepackage{adjustbox}


\title{\textit{ReVision:} A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting}  
% \author{Authors - TBD}
% \author {
%     % Authors
%    \textbf{ Abhijit Mishra}, \textbf{Richard  Noh},\textbf{ Hsiang Fu},, \textbf{Mingda Li}$\dagger$, \textbf{Minji Kim }
% }
% \affiliations {
%     % Affiliations
%     School of Information, University of Texas at Austin \\
%     \{abhijitmishra, rjnoh, seanfu, minji.kim\}@utexas.edu \\
%     \textsuperscript Department of Statistics and Data Science, Yale University \\
%     mingdali@utexas.edu
% }

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% For several authors from the same institution:
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
  
\author {
    % Authors
    \textbf{ Abhijit Mishra$^*$}, \textbf{Richard  Noh$^*$},\textbf{ Hsiang Fu$^*$}, \textbf{Mingda Li}$\dagger$, \textbf{Minji Kim }\\
    School of Information, University of Texas at Austin \\
    \{abhijitmishra, rjnoh, seanfu, minji.kim\}@utexas.edu \\
    \textsuperscript{$\dagger$}Department of Statistics and Data Science, Yale University \\
     mingda.li@yale.edu
}

\begin{document}
\maketitle

\def\thefootnote{*}\footnotetext{Equal Contribution}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores \textit{Visual Instruction Rewriting}, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.
\end{abstract}

\input{1_introduction}
\input{2_relatedwork}
\input{3_dataset}
\input{4_methodology}
\input{5_results}
\input{6_conclusion}

\section*{Limitations}
While our approach demonstrates strong performance in Visual Instruction Rewriting, several limitations remain. First, image downsampling to $256 \times 256$ resolution can lead to the loss of fine-grained text details, affecting instructions that rely on small-font information, such as nutritional labels or product specifications. Second, deictic reference resolution remains challenging, especially in images with multiple similar objects where the model lacks explicit localization cues. The absence of bounding box annotations in our dataset limits the model’s ability to disambiguate references, leading to errors in object-grounded instructions. Additionally, while our model is lightweight and optimized for on-device execution, it still lags behind larger VLMs in handling complex multimodal instructions requiring deep reasoning and external world knowledge. Lastly, our dataset, while diverse across 14 domains, is monolingual, limiting applicability to multilingual and culturally varied settings. Future work can address these challenges by increasing dataset coverage, incorporating localized image region processing, and adding bounding box annotations to improve reference resolution and multimodal grounding.

\section*{Ethics Statement}
This work prioritizes privacy and ethical considerations by designing a lightweight, on-device Visual Instruction Rewriting system that eliminates the need to transmit personal vision-related data to external servers. By converting multimodal instructions into text-only commands, our approach reduces data exposure risks and ensures secure, user-controlled inference. Our dataset is sourced from publicly available and academic-use image collections, ensuring compliance with fair use and licensing policies. However, we acknowledge potential biases in data distribution and the need for greater multilingual and cultural inclusivity. Future efforts will focus on expanding dataset diversity, improving fairness in multimodal understanding, and ensuring responsible AI deployment in real-world applications.

Additionally, we acknowledge the use of OpenAI’s ChatGPT-4 system solely for enhancing writing efficiency, generating LaTeX code, and aiding in error debugging. No content related to the survey's research findings, citations, or factual discussions was autogenerated or retrieved using Generative AI-based search mechanisms. Our work remains grounded in peer-reviewed literature and ethical academic standards.
% \section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{acl2025}
\bibliographystyle{acl_natbib}


\input{8_appendix}

% \newpage
% \pagebreak







\end{document}
