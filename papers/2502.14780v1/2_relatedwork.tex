\section{Related Work}
\label{sec:related_work}
Instruction or query rewriting and semantic parsing have been widely explored in conversational AI to improve query understanding and response generation. Early methods relied on rule-based transformations and supervised learning \cite{semantic_parsing_survey}, while recent advances leverage LLMs for dynamic query refinement \cite{ye2023large, mo2023convgqr}. Generative query rewriting frameworks such as LLM-R2 \cite{llm-r2} enhance text ranking, and personalized query rewriting methods \cite{cho-etal-2021-personalized} refine queries based on user preferences. However, these techniques focus primarily on textual query transformations and do not extend to multimodal task-oriented instruction processing. Visual instruction tuning has emerged as a key development in multimodal AI, with models like LLaVA \cite{liu2023visual} and PaLI-X \cite{chen2023pali} demonstrating strong vision-language capabilities. While these models excel in multimodal question answering, they are not optimized for rewriting task-oriented instructions. Similarly, Patel et al. \cite{patel2020generating} explore generating natural questions from images for multimodal assistants, but their work focuses on question generation rather than instruction rewriting. Unlike these approaches, our work introduces a dedicated dataset and a compact model for Visual Instruction Rewriting, specifically designed to convert multimodal user instructions into structured text for privacy-preserving, on-device execution.  

The closest work to ours is MARRS \cite{ates2023marrs}, which integrates multimodal reference resolution with query rewriting to improve conversational grounding. However, MARRS relies on rule-based replacements after reference resolution in a non-VLM setting, whereas our approach focuses on learning-based instruction rewriting to enable structured task execution from multimodal inputs. Other highly relevant studies are by \newcite{zhang2022can} and \newcite{wei2021visual}, which investigate whether open-domain text-based QA systems can handle visual knowledge questions by reformulating them into purely textual queries. Their work highlights the effectiveness of query rewriting in bridging the gap between vision and language using a modular approach different from ours but aligns closely with our goal of rewriting multimodal instructions into structured text. However, while their approach focuses on adapting visual questions for open-domain QA, our work is specifically designed for task-oriented instruction execution, making it applicable to a broader set of real-world multimodal interactions.