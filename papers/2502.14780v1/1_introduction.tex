\section{Introduction}
\label{sec:intro}
The increasing adoption of conversational AI in augmented reality (AR), virtual reality (VR), and modern smartphones has heightened the need for multimodal AI systems that seamlessly process text, images, speech, and gestures. Devices like \textit{Meta Ray-Ban Smart Glasses} and \textit{Apple Vision Pro} are transforming human-computer interaction, enabling users to issue spoken commands while interacting with digital and physical environments. For instance, a user wearing smart glasses might say, \textit{"Call this number"} while looking at a business card, or \textit{"Add this to my calendar"} while viewing an event flyer. Handling such task-oriented multimodal commands requires AI models capable of interpreting visual context, rewriting instructions into structured text, and executing them within a conversational AI framework, all while maintaining user privacy. Notably, a key challenge in these systems is privacy -- many interactions involve sensitive information that should ideally be processed on-device rather than being sent to cloud servers.

While large vision-language models (VLMs) such as PaLI-X, LLaVA, and Qwen-VL \cite{chen2023pali, liu2023llava, qwen-vl} have demonstrated impressive multimodal capabilities, they are often impractical for on-device deployment due to their size, requiring cloud-based inference. Also, transmitting private visual and textual data to external servers raises security risks and compromises user privacy. On the other hand, smaller models suitable for local execution often lack the broad world knowledge embedded in larger models \cite{small_models_limitations}, making them less effective in complex multimodal understanding.

To address this, we propose \textit{ReVision}, an approach based on \textit{Visual Instruction Rewriting} that converts multimodal instructions into text-only commands, allowing privacy-preserving on-device execution. By transforming complex visual interactions into structured text, existing lightweight conversational AI models can efficiently process user instructions without sending sensitive visual data to external servers. We introduce a curated dataset consisting of \texttt{$\langle$ image, original instruction, rewritten instruction $\rangle$} triplets, covering diverse real-world tasks. A freshly built compact 250 Million parameters vision-language model \cite{liu2023llava} is fine-tuned on this dataset and evaluated using NLG metrics (such as BLEU, METEOR, ROUGE) and semantic parsing accuracy.

Our findings demonstrate that our compact model achieves an acceptable level of rewriting capabilities, and performs better compared to popular baselines such as PaliGemma-v2 \cite{steiner2024paligemma} and Qwen2VL \cite{wang2024qwen2} in zero-shot settings. Additionally, even an 8-bit quantized version of our model (<500MB on storage disk) achieves effective instruction rewrites while maintaining a small computational footprint. We strongly believe this approach bridges the gap between large-scale multimodal AI and privacy-centric, on-device execution, ensuring secure, real-time interaction with AR/VR and smartphone interfaces.

The contributions of this paper are as follows:
\begin{itemize}
    \item Introduction of a novel dataset for Visual Instruction Rewriting, encompassing over 15 distinct intent domains and over 1,700 personal images and 39,000 examples.
    \item Development of a baseline small-scale vision-language model (250M parameters), pretrained on image captioning datasets and fine-tuned on the proposed rewriting dataset.
    \item Experimental validation using various NLG and semantic parsing metrics to demonstrate the effectiveness of the Visual Instruction Rewriting approach.
\end{itemize}

% The Code\footnote{\url{https://anonymous.4open.science/r/ReVision-8F0F/}}, Dataset\footnote{\url{https://huggingface.co/datasets/anonymoususerrevision/multimodal_query_rewrites}} and Models\footnote{\url{https://huggingface.co/anonymoususerrevision}} have been released for academic use. 
The Code\footnote{\url{https://github.com/abhijitmishra/visual_instruction_rewriting}}, Dataset\footnote{\url{https://huggingface.co/datasets/hsiangfu/multimodal_query_rewrites}} and Models\footnote{\url{https://huggingface.co/hsiangfu/ReVision-250M-256-16-baseline}} have been released for academic use. 

% The rapid advancement of technology has led to an increasingly intertwined physical and digital world, necessitating the development of sophisticated multimodal machine learning solutions capable of processing and interpreting data from various modalities, including text, images, audio, and user attention signals. This integration is particularly crucial in scenarios where users interact with digital interfaces through multiple sensory inputs.

% Consider a scenario where a user, while viewing a business card, issues the command, "Call this number." To effectively respond, a visual instruction processor must analyze data from multiple modalities: interpreting the visual content (the business card), understanding the spoken or textual instruction, and possibly considering user attention cues such as gaze direction. In mixed reality environments, additional modalities like motion and virtual touch gestures further enrich the interaction landscape.

% Visual Instruction Processing has emerged as a pivotal domain within multimodal generative AI, focusing on enabling systems to comprehend and execute instructions derived from visual inputs. Recent advancements have seen the development of large visual instruction-tuned models that combine vision encoders with large language model text encoders and decoders. Notable examples include PaLI-X, LLaVA, and Qwen-VL, which have demonstrated significant capabilities in this area \cite{chen2023pali, liu2023llava, qwen-vl}.

% However, many existing systems are primarily designed for question-answering scenarios and may not be optimized for task-oriented visual instructions. For instance, executing a command like "Call this number" while viewing a business card requires the system to generate a semantic parse of the instruction, a process traditionally utilized in conversational AI frameworks \cite{semantic_parsing_survey}. While large vision-language models possess the capacity for such tasks, deploying them presents challenges. These models are often too large to operate on-device and are typically hosted on servers, which raises privacy concerns, especially when tasks could be executed locally without server interaction. Moreover, smaller models, though more suitable for on-device deployment, often lack the extensive world knowledge embedded in their larger counterparts, presenting additional challenges \cite{small_models_limitations}.

% In this paper, we propose a middle-ground approach termed Visual Instruction Rewriting. This method aims to transform visual instructions into text-only commands that can be processed by existing text-based systems, such as popular conversational AI platforms or large language models. We introduce a novel dataset, meticulously collected and validated, encompassing a wide array of task-specific instructions—including questions and commands—across various personal and web-based domains. The curated dataset comprises examples formatted as $\langle$ image, original instruction, rewritten instruction $\rangle$.

% We conduct experiments using a baseline vision-language model with approximately 500 million parameters, pretrained on established vision-language pretraining datasets and fine-tuned on our Visual Instruction Rewriting dataset. The evaluation of the model's performance is carried out through two primary methods: (a) computing various Natural Language Generation (NLG) metrics such as BLEU, METEOR, ROUGE, and GPT-4-based fluency and adequacy scores by comparing the original rewrites with the model-generated outputs; and (b) analyzing semantic parse matches using a fixed third-party system that generates parses from both the original and model-rewritten instructions to assess intent and argument alignment.

% Our results indicate that the curated dataset is effective in training a foundational system. Notably, even in its quantized form (8-bit), which significantly reduces the memory footprint to approximately 200MB, the model is capable of generating acceptable rewrites.

% The contributions of this paper are as follows:

% 1. Introduction of a novel dataset for Visual Instruction Rewriting, encompassing 14 distinct domains and over 12,000 examples.
% 2. Development of a baseline small-scale vision-language model, pretrained on image captioning datasets and fine-tuned on the proposed rewriting dataset.
% 3. Experimental validation using various NLG and semantic parsing metrics to demonstrate the effectiveness of the Visual Instruction Rewriting approach.

% The code, dataset, and additional resources are available at \url{https://anonymized.io}.

