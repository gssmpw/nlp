\section{Results and Discussion}
\label{sec:results}

Table \ref{tab:evaluation_results} presents the evaluation results for both baseline models (BL) and our proposed ReVision models across Language Generation (NLG) metrics (ROUGE, BLEU, METEOR) and semantic parsing performance (Intent Accuracy and Argument Mean Jaccard Similarity). We also provide anecdotal examples in Figure \ref{fig:anecdotal_examples} in the Appendix to illustrate the strengths and limitations of various models.

As we can see, the baseline models struggle significantly in the rewriting task, not because they are inherently weak, but because they are not explicitly tuned for rewriting. While PaliGemma2-10B and QwenVL-7B have demonstrated strong performance in various vision-language tasks, they are not optimized to follow meta-instructions like rewriting. This is evident in their vanilla versions (BL\textsubscript{1a}, BL\textsubscript{1b}), where ROUGE-1 scores remain low (3.4\% and 43.7\%), BLEU is nearly negligible (0.03\% and 12.3\%), and Intent Accuracy is poor (16.2\% and 50.3\%). A key issue is that these models misinterpret the task, often either responding to the instruction directly or attempting to autocomplete the instruction instead of rewriting it. Since many input instructions are imperative, task-oriented instructions, they frequently refuse to generate a rewrite (e.g., replying "I can't help with that") or incorrectly complete the instruction, significantly degrading both NLG and parsing metrics. Moreover, since they are relatively small models (<10B parameters), they lack the necessary world knowledge and instruction-following capabilities to recognize and execute rewriting as a structured transformation task effectively. Additionally, prompting these models with Self\_Caption+EasyOCR metadata (BL\textsubscript{2a}, BL\textsubscript{2b}) helps slightly, especially for QwenVL-7B, where Intent Accuracy improves from 50.3\% to 61.2\%. However, the models still struggle with ROUGE and BLEU scores, reinforcing that generic VLMs require dedicated instruction tuning to handle the rewriting task effectively.

In contrast, our proposed ReVision models, specifically trained for rewriting, significantly outperform all baselines. Even without metadata enhancements, ReVision-BL already outperforms the input-augmented baseline models, achieving ROUGE-1 of 56.9\%, BLEU of 27.7\%, and Intent Accuracy of 56.5\%. This confirms that explicit tuning for rewriting is essential and that even a compact, instruction-tuned VLM can surpass larger models that lack task-specific optimization. These observations are also corroborated by the intent category-wise F1 scores reported in Figure \ref{fig:F1}. 

With metadata, the performance further improves. ReVision-Metadata with GPT4-derived captions and OCR Text achieves 72.4\% ROUGE-1, 49.9\% BLEU, and 62.4\% Intent Accuracy, showing that supplementing the input with extracted text helps models disambiguate multimodal instructions and produce more accurate rewrites. The best-performing model, ReVision-Metadata\textsubscript{Self\_Caption+EasyOCR}, achieves the highest scores across all metrics, confirming that even lightweight OCR and captioning models can be leveraged to improve rewriting quality. Lastly, the 8-bit quantized version of the best-performing model offers competitive performance to its full-precision counterpart, with only a minor drop in Intent Accuracy (67.6\% vs. 71.5\%) but a slight improvement in Argument Similarity (79.5\%). This demonstrates that 8-bit models can be effectively deployed on resource-constrained devices.

Despite the strong performance of our \textit{ReVision} model variants, certain limitations prevent even higher accuracy. One major challenge is the loss of fine-grained text details due to image downsampling to $256 \times 256$ resolution, making it difficult for the model to capture small but critical information, such as ingredient lists or nutritional facts on product packaging. Additionally, the lack of explicit reference localization in the dataset restricts the modelâ€™s ability to map user intent to specific image regions, leading to errors in object disambiguation and instruction alignment. To address these challenges, future work can incorporate bounding box annotations to provide spatial grounding cues for better reference resolution. Processing localized image regions instead of entire downsampled images could reduce information loss, especially for text-heavy visual instructions. This aligns with Pali-Gemma's short-resolution increase technique \cite{beyer2024paligemma}, which enhances fine-grained visual understanding. Despite these limitations, our results confirm that task-specific instruction tuning and metadata augmentation significantly enhance multimodal rewriting, ensuring scalable and efficient on-device deployment.

%the model sometimes struggles with deictic reference resolution, particularly in ambiguous cases where multiple visually similar objects exist. For instance, when a user asks, "What is the model of this green car?" in an image containing multiple cars of different colors, the model may fail to identify the intended reference. Lastly, 
 %Also, bounding box supervision could enable gaze tracking or tactile selection, making the model more effective for wearable AI applications like AR glasses and smart assistants. 