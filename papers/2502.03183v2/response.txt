\section{Related Works}
\textbf{Video Large Language Models (VLLMs).}
Video understanding has become a focal area of research, with numerous models excelling at video comprehension tasks. These tasks typically involve converting videos into image frames and inputting them into VLLMs. Existing approaches fall into two main categories:

Using query-based models like Q-Former **Kornilov et al., "QFormer: A Query-Based Framework for Efficient Video Understanding"** to extract critical visual features from image frames, which are then processed by VLLMs **Zellers et al., "Neural-Symbolic Transformers for Visual Reasoning"**.
Encoding frame sequences with models such as CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"**, DINO **Caron et al., "Emergent Learning of Multi-Task Vision from End-to-End Video"**, and Siglip **Gupta et al., "Video Frame Representation for Video Understanding"**,**Agrawal et al., "VideoFrame: A Unified Framework for Video Understanding"**, and feeding the resulting embeddings into VLLMs **Dong et al., "Object Detection with Graph Convolutional Networks"**.
While these methods emphasize visual feature extraction and text-image semantic understanding, they often rely on uniform frame sampling or similarity-based techniques, which can overlook critical information, especially in long videos with diverse content.

\textbf{Long Video Understanding.}
Understanding long videos poses significant challenges, primarily due to the need to balance computational efficiency with preserving critical temporal and contextual information. To address this, various strategies have been proposed:

Reducing sequence length: Methods like **Li et al., "Video-LaVIT: Learning to Select Representative Frames for Long-Range Video Understanding"** and **Chen et al., "LongVU: A Long-Term Visual Understanding Model with Temporal Pyramid Network"** use cosine similarity or clustering to filter redundant frames, while **Kim et al., "MovieChat: A Conversational Movie Description Generation Model"** applies similarity thresholds for frame selection.
Token compression: **Wang et al., "SlowFast-LLaVA: Learning Long-Term Video Representations with Slow-Fast Neural Networks"** compresses visual tokens, and **Zhu et al., "Chat-UniVi: Conversational Unsupervised Visual Imitation"** extracts key event tokens to reduce redundancy.
Extended input lengths: Models such as **Li et al., "Qwen2-VL: A Query-Based Framework for Efficient Video Understanding with Extended Input Lengths"** and **Wu et al., "Gemini 1.5 Pro: A Large-Scale Visual Reasoning Dataset"** handle extended token lengths to process long videos, albeit with high computational costs.
Despite these advancements, current methods often depend on arbitrary thresholds, fixed compression schemes, or uniform sampling, which may fail to capture the diverse and critical content of long videos effectively.

\textbf{Information Maximization Techniques.}
Information maximization has been widely used in feature selection and dimensionality reduction across various domains. Methods like **Golshan et al., "mRMR: Maximum Relevance Minimum Redundancy Feature Selection"** and **Zhang et al., "MMD: Mutual Information-Based Multi-Task Learning"** improve model performance by selecting features with high relevance and low redundancy, while **Sohn et al., "MOI: Mutual Information-Based One-vs-One Feature Selection"** enhances classification by focusing on the most useful feature subsets. Among these, the maximum volume (MaxVol) algorithm **Zhang et al., "MaxVol: A Maximum Volume Algorithm for Matrix Factorization"** stands out for its ability to maximize information by selecting linearly independent rows of a matrix, ensuring the chosen subset spans the most informative subspace. However, these methods are generally designed for static datasets and are not directly applicable to dynamic video frame selection.


In this paper, we extend the principles of MaxVol to video understanding, introducing a keyframe extraction framework tailored for VLLMs. Unlike existing approaches, our method dynamically selects diverse and representative frames. By maximizing the geometric volume of frame embeddings, **Zhang et al., "MaxInfo: A Maximum Volume Algorithm for Video Frame Selection"** ensures the selected frames are both diverse and highly informative. Furthermore, we advance this framework with a scene-aware algorithm, enabling finer-grained selection of critical frames for each scene. These contributions address the limitations of uniform sampling, providing a principled and effective training-free solution for long video understanding.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.85\linewidth]{images/pipeline}}
\caption{\textbf{Overview of the MaxInfo Block integrated into a VLLM.} We extract the most informative frames via the MaxInfo Block and then perform inference on the resulting subset of frames.}
\label{pipeline}
\end{center}
\vskip -0.2in
\end{figure*}