\section{Related Works}
\textbf{Video Large Language Models (VLLMs).}
Video understanding has become a focal area of research, with numerous models excelling at video comprehension tasks. These tasks typically involve converting videos into image frames and inputting them into VLLMs. Existing approaches fall into two main categories:

Using query-based models like Q-Former ____ to extract critical visual features from image frames, which are then processed by VLLMs ____.
Encoding frame sequences with models such as CLIP ____, DINO ____, and Siglip ____, and feeding the resulting embeddings into VLLMs ____.
While these methods emphasize visual feature extraction and text-image semantic understanding, they often rely on uniform frame sampling or similarity-based techniques, which can overlook critical information, especially in long videos with diverse content.

\textbf{Long Video Understanding.}
Understanding long videos poses significant challenges, primarily due to the need to balance computational efficiency with preserving critical temporal and contextual information. To address this, various strategies have been proposed:

Reducing sequence length: Methods like Video-LaVIT ____ and LongVU ____ use cosine similarity or clustering to filter redundant frames, while MovieChat ____ applies similarity thresholds for frame selection.
Token compression: SlowFast-LLaVA ____ compresses visual tokens, and Chat-UniVi ____ extracts key event tokens to reduce redundancy.
Extended input lengths: Models such as Qwen2-VL ____ and Gemini 1.5 Pro ____ handle extended token lengths to process long videos, albeit with high computational costs.
Despite these advancements, current methods often depend on arbitrary thresholds, fixed compression schemes, or uniform sampling, which may fail to capture the diverse and critical content of long videos effectively.

\textbf{Information Maximization Techniques.}
Information maximization has been widely used in feature selection and dimensionality reduction across various domains. Methods like mRMR ____ and MMD ____ improve model performance by selecting features with high relevance and low redundancy, while MOI ____ enhances classification by focusing on the most useful feature subsets. Among these, the maximum volume (MaxVol) algorithm ____ stands out for its ability to maximize information by selecting linearly independent rows of a matrix, ensuring the chosen subset spans the most informative subspace. However, these methods are generally designed for static datasets and are not directly applicable to dynamic video frame selection.


In this paper, we extend the principles of MaxVol to video understanding, introducing a keyframe extraction framework tailored for VLLMs. Unlike existing approaches, our method dynamically selects diverse and representative frames. By maximizing the geometric volume of frame embeddings, MaxInfo ensures the selected frames are both diverse and highly informative. Furthermore, we advance this framework with a scene-aware algorithm, enabling finer-grained selection of critical frames for each scene. These contributions address the limitations of uniform sampling, providing a principled and effective training-free solution for long video understanding.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.85\linewidth]{images/pipeline}}
\caption{\textbf{Overview of the MaxInfo Block integrated into a VLLM.} We extract the most informative frames via the MaxInfo Block and then perform inference on the resulting subset of frames.}
\label{pipeline}
\end{center}
\vskip -0.2in
\end{figure*}