@inproceedings{Chat-univi,
  title={Chat-univi: Unified visual representation empowers large language models with image and video understanding},
  author={Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13700--13710},
  year={2024}
}

@article{MMD,
  title={Feature selection by maximum marginal diversity},
  author={Vasconcelos, Nuno},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}

@article{MOI,
  title={Feature selection in MLPs and SVMs based on maximum output information},
  author={Sindhwani, Vikas and Rakshit, Subrata and Deodhare, Dipti and Erdogmus, Deniz and Principe, Jos{\'e} Carlos and Niyogi, Partha},
  journal={IEEE transactions on neural networks},
  volume={15},
  number={4},
  pages={937--948},
  year={2004},
  publisher={IEEE}
}

@article{Rectmax,
  title={Rectangular maximum-volume submatrices and their applications},
  author={Mikhalev, Aleksandr and Oseledets, Ivan V},
  journal={Linear Algebra and its Applications},
  volume={538},
  pages={187--211},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{Siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11975--11986},
  year={2023}
}

@inproceedings{blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{dino,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}

@incollection{goreinov2010find,
  title={How to find a good submatrix},
  author={Goreinov, Sergei A and Oseledets, Ivan V and Savostyanov, Dimitry V and Tyrtyshnikov, Eugene E and Zamarashkin, Nikolay L},
  booktitle={Matrix Methods: Theory, Algorithms And Applications: Dedicated to the Memory of Gene Golub},
  pages={247--256},
  year={2010},
  publisher={World Scientific}
}

@inproceedings{internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@article{longvu,
  title={Longvu: Spatiotemporal adaptive compression for long video-language understanding},
  author={Shen, Xiaoqian and Xiong, Yunyang and Zhao, Changsheng and Wu, Lemeng and Chen, Jun and Zhu, Chenchen and Liu, Zechun and Xiao, Fanyi and Varadarajan, Balakrishnan and Bordes, Florian and others},
  journal={arXiv preprint arXiv:2410.17434},
  year={2024}
}

@inproceedings{mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22195--22206},
  year={2024}
}

@article{omnifusion,
  title={Omnifusion technical report},
  author={Goncharova, Elizaveta and Razzhigaev, Anton and Mikhalchuk, Matvey and Kurkin, Maxim and Abdullaeva, Irina and Skripkin, Matvey and Oseledets, Ivan and Dimitrov, Denis and Kuznetsov, Andrey},
  journal={arXiv preprint arXiv:2404.06212},
  year={2024}
}

@article{peng2005feature,
  title={Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
  author={Peng, Hanchuan and Long, Fuhui and Ding, Chris},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={27},
  number={8},
  pages={1226--1238},
  year={2005},
  publisher={IEEE}
}

@article{qwen2-vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{slowfast,
  title={Slowfast-llava: A strong training-free baseline for video large language models},
  author={Xu, Mingze and Gao, Mingfei and Gan, Zhe and Chen, Hong-You and Lai, Zhengfeng and Gang, Haiming and Kang, Kai and Dehghan, Afshin},
  journal={arXiv preprint arXiv:2407.15841},
  year={2024}
}

@inproceedings{song2024moviechat,
  title={Moviechat: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18221--18232},
  year={2024}
}

@article{sozykin2022ttopt,
  title={TTOpt: A maximum volume quantized tensor train-based optimization and its application to reinforcement learning},
  author={Sozykin, Konstantin and Chertkov, Andrei and Schutski, Roman and Phan, Anh-Huy and Cichocki, Andrzej S and Oseledets, Ivan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26052--26065},
  year={2022}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{video-lavit,
  title={Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization},
  author={Jin, Yang and Sun, Zhicheng and Xu, Kun and Chen, Liwei and Jiang, Hao and Huang, Quzhe and Song, Chengru and Liu, Yuliang and Zhang, Di and Song, Yang and others},
  journal={arXiv preprint arXiv:2402.03161},
  year={2024}
}

@article{xenos2024vllms,
  title={VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning},
  author={Xenos, Alexandros and Foteinopoulou, Niki Maria and Ntinou, Ioanna and Patras, Ioannis and Tzimiropoulos, Georgios},
  journal={arXiv preprint arXiv:2404.07078},
  year={2024}
}

