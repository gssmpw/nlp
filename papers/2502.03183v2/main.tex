%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

% Colors
\usepackage[table,dvipsnames]{xcolor}
\definecolor{lightlightgray}{gray}{0.9}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
% \usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MaxInfo: A Training-Free Key-Frame Selection Method Using Maximum Volume for Enhanced Video Understanding}

\begin{document}

\twocolumn[
\icmltitle{MaxInfo: A Training-Free Key-Frame Selection Method Using Maximum Volume for Enhanced Video Understanding}


\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pengyi Li}{equal,yyy,comp}
\icmlauthor{Irina Abdullaeva}{equal,comp}
\icmlauthor{Alexander Gambashidze}{equal,yyy,comp}
\icmlauthor{Andrey Kuznetsov}{comp}
\icmlauthor{Ivan Oseledets}{yyy,comp}

\end{icmlauthorlist}

\icmlaffiliation{yyy}{Skolkovo Institute of Science and Technology, Moscow, Russia}
\icmlaffiliation{comp}{Artificial Intelligence Research Institute (AIRI), Moscow, Russia}

\icmlcorrespondingauthor{Pengyi Li}{pengyi.li@skoltech.ru}
\icmlcorrespondingauthor{Ivan Oseledets}{i.Oseledets@skoltech.ru}

\icmlkeywords{Training-Free, Plug-and-Play, VLLMs, Video Understanding}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Modern Video Large Language Models (VLLMs) often rely on uniform frame sampling for video understanding, but this approach frequently fails to capture critical information due to frame redundancy and variations in video content. We propose MaxInfo, a training-free method based on the maximum volume principle, which selects and retains the most representative frames from the input video. By maximizing the geometric volume formed by selected embeddings, MaxInfo ensures that the chosen frames cover the most informative regions of the embedding space, effectively reducing redundancy while preserving diversity. This method enhances the quality of input representations and improves long video comprehension performance across benchmarks. For instance, MaxInfo achieves a \textbf{3.28\% improvement} on LongVideoBench 
and a \textbf{6.4\% improvement} on EgoSchema for LLaVA-Video-7B. 
It also achieves a \textbf{3.47\% improvement} for LLaVA-Video-72B. The approach is simple to implement and works with existing VLLMs without the need for additional training, making it a practical and effective alternative to traditional uniform sampling methods.

\end{abstract}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\linewidth]{images/maxinfo}}
\caption{Reasons for not being able to answer the correct answer at long video benchmark (Video-MME).}
\label{video-mme-fig}
\end{center}
\vskip -0.2in
\end{figure*}


\section{Introduction} \label{Introduction}
Large language models (LLMs) such as GPT \cite{gpt-3, gpt4}, LLaMA \cite{llama2, llama3}, Qwen \cite{qwen, qwen2}, and Mistral \cite{jiang2023mistral} have revolutionized tasks like text generation, summarization, and reasoning. Recent advancements in multimodal large language models (MLLMs) \cite{llava, omnifusion} have extended these capabilities to include processing images, videos, and audio, enabling responses across diverse modalities. Video understanding, in particular, has garnered significant attention due to its complex, multi-dimensional nature and broad range of applications.

While models like LLaVA-Video \cite{llava-video}, VideoLLaMA 2 \cite{cheng2024videollama2}, MiniCPM-V 2.6 \cite{yao2024minicpmv}, and InternVL \cite{internvl} have made strides in video understanding, they struggle with long videos due to the diversity and redundancy of video content. Uniform frame sampling, a widely used approach, often fails to capture the most informative frames, leading to missing critical details and  model performance decrease. As in \cref{video-mme-fig} illustrate this challenge using the Video-MME \cite{video-mme} benchmark. As shown, uniform sampling can overlook key information essential to understanding the video, as frames critical to the answer may not be selected.

Existing approaches attempt to address these challenges by either increasing input sequence length or compressing video information. Models like LongVU \cite{longvu} and MovieChat \cite{song2024moviechat} compress tokens per frame, while others, such as Qwen2-VL \cite{qwen2-vl} and Gemini 1.5 Pro \cite{team2024gemini}, process longer sequences, supporting up to 32K and 10 million tokens, respectively. However, these solutions either incur substantial computational overhead or risk losing critical temporal information. Balancing the trade-off between efficiency and accuracy remains a key challenge in long video understanding.

% \begin{quote}
% “Nature is pleased with simplicity. And nature is no dummy” ― Isaac Newton
% \end{quote}

To address this, we propose \textbf{MaxInfo} -- a training-free, plug-and-play method for dynamically selecting the most informative frames. Unlike uniform sampling, MaxInfo ensures that the input sequence maximizes information content and diversity. Our approach identifies and retains the most representative frames using the maximum volume principle on the matrix of frame embeddings and selects a subset of embeddings that span the most informative subspace. This ensures that redundant frames are removed while the retained frames span the most meaningful subspace of the video content.

Our contributions are as follows:

\begin{enumerate}
    \item A novel framework to enhance frame diversity and informativeness. MaxInfo improves upon uniform sampling by selecting the most critical frames from a video, ensuring a more meaningful representation for VLLMs.

    \item An advanced scene-aware extension. We extend our framework with a scene-aware algorithm that further refines frame selection by identifying key frames within individual scenes, improving performance on tasks requiring temporal coherence.

    \item Training-free and plug-and-play integration: MaxInfo requires no retraining or fine-tuning and can be seamlessly applied to any VLLM, making it a highly practical solution for long video understanding.
\end{enumerate}

Experimental results demonstrate that MaxInfo significantly improves the performance of existing models, such as LLaVA-Video \cite{llava-video}, InternVL2 \cite{internvl} and Qwen2-VL \cite{qwen2-vl}, achieving better results on benchmarks like Video-MME \cite{video-mme} and EgoSchema \cite{egoschema}. By overcoming the limitations of uniform sampling, MaxInfo enables more accurate and robust video understanding, ensuring that critical information from long videos is effectively captured and utilized in multimodal applications.

\section{Related Works}

\textbf{Video Large Language Models (VLLMs).}
Video understanding has become a focal area of research, with numerous models excelling at video comprehension tasks. These tasks typically involve converting videos into image frames and inputting them into VLLMs. Existing approaches fall into two main categories:

Using query-based models like Q-Former \cite{blip2} to extract critical visual features from image frames, which are then processed by VLLMs \cite{mvbench, xenos2024vllms}.
Encoding frame sequences with models such as CLIP \cite{clip}, DINO \cite{dino}, and Siglip \cite{Siglip}, and feeding the resulting embeddings into VLLMs \cite{longvu, Chat-univi, video-lavit, qwen2-vl, internvl, omnifusion}.
While these methods emphasize visual feature extraction and text-image semantic understanding, they often rely on uniform frame sampling or similarity-based techniques, which can overlook critical information, especially in long videos with diverse content.

\textbf{Long Video Understanding.}
Understanding long videos poses significant challenges, primarily due to the need to balance computational efficiency with preserving critical temporal and contextual information. To address this, various strategies have been proposed:

Reducing sequence length: Methods like Video-LaVIT \cite{video-lavit} and LongVU \cite{longvu} use cosine similarity or clustering to filter redundant frames, while MovieChat \cite{song2024moviechat} applies similarity thresholds for frame selection.
Token compression: SlowFast-LLaVA \cite{slowfast} compresses visual tokens, and Chat-UniVi \cite{Chat-univi} extracts key event tokens to reduce redundancy.
Extended input lengths: Models such as Qwen2-VL \cite{qwen2-vl} and Gemini 1.5 Pro \cite{team2024gemini} handle extended token lengths to process long videos, albeit with high computational costs.
Despite these advancements, current methods often depend on arbitrary thresholds, fixed compression schemes, or uniform sampling, which may fail to capture the diverse and critical content of long videos effectively.

\textbf{Information Maximization Techniques.}
Information maximization has been widely used in feature selection and dimensionality reduction across various domains. Methods like mRMR \cite{peng2005feature} and MMD \cite{MMD} improve model performance by selecting features with high relevance and low redundancy, while MOI \cite{MOI} enhances classification by focusing on the most useful feature subsets. Among these, the maximum volume (MaxVol) algorithm \cite{goreinov2010find, Rectmax, sozykin2022ttopt} stands out for its ability to maximize information by selecting linearly independent rows of a matrix, ensuring the chosen subset spans the most informative subspace. However, these methods are generally designed for static datasets and are not directly applicable to dynamic video frame selection.


In this paper, we extend the principles of MaxVol to video understanding, introducing a keyframe extraction framework tailored for VLLMs. Unlike existing approaches, our method dynamically selects diverse and representative frames. By maximizing the geometric volume of frame embeddings, MaxInfo ensures the selected frames are both diverse and highly informative. Furthermore, we advance this framework with a scene-aware algorithm, enabling finer-grained selection of critical frames for each scene. These contributions address the limitations of uniform sampling, providing a principled and effective training-free solution for long video understanding.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.85\linewidth]{images/pipeline}}
\caption{\textbf{Overview of the MaxInfo Block integrated into a VLLM.} We extract the most informative frames via the MaxInfo Block and then perform inference on the resulting subset of frames.}
\label{pipeline}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Method}
We propose the \textbf{MaxInfo Block}, a plug-and-play, training-free module designed for long-video understanding tasks. It ensures both \textit{diversity} of frames and \textit{comprehensive} semantic coverage from a video by selecting only the most informative frames. As illustrated in \cref{pipeline}, the MaxInfo Block can be easily integrated into any VLLMs, enhancing the quality and diversity of visual information fed into the model.

\subsection{Overview}
Given a video, we uniformly sample $n$ frames. For example, sampling at 1~fps reduces the risk of losing important content, but still may yield a large number of frames, many of which could be redundant. This both increases computational cost and does not guarantee capturing the \textit{most} informative or diverse frames. Hence, we seek a small, representative subset of frames.

Let the sampled frames be
$\mathbf{I} \;=\; \{\, i_1,\; i_2,\;\ldots,\; i_n \}.$
We extract each frame’s visual representation via a CLIP-based ViT \cite{clip} and retain only the [CLS] token. Stacking these tokens yields

\begin{equation}
        \mathbf{Q} \;=\; \begin{bmatrix}
            q_1 \\[4pt]
            q_2 \\[4pt]
            \vdots \\[2pt]
            q_n
        \end{bmatrix}
        \;\in\;\mathbb{R}^{n\times d},
\end{equation}

where $q_n\in\mathbb{R}^d$ is the flattened [CLS] feature from the $n$-th frame.

\subsection{Dimensionality Reduction}
Handling all $n\times d$ features may still be computationally expensive when $n$ and $d$ are large. To mitigate this, we perform a truncated SVD on $\mathbf{Q}$:

\begin{equation}
    \mathbf{Q} \;=\; U\,\Sigma\,V^T \quad\rightarrow\quad \mathbf{Q}_s \;=\; U_{(:,1:s)},
\end{equation}

where $U\in \mathbb{R}^{n\times n}$, $\Sigma\in \mathbb{R}^{n\times d}$, and $V\in\mathbb{R}^{d\times d}$. By retaining the first $s$ singular vectors (the top $s$ columns of $U$), we obtain:

\begin{equation}
    \mathbf{Q}_s\;\in\;\mathbb{R}^{n\times s},
\end{equation}

which captures the principal visual variation among frames while drastically reducing dimensionality.

\subsection{Rectangular MaxVol Frame Selection}
On the next step we identify the “most informative” subset of rows of $\mathbf{Q}_s$, i.e., a set of frames which corresponding rows span the overall distribution of frames. To do this, we use the \emph{rectangular MaxVol} algorithm \cite{Rectmax} to evaluate a submatrix of maximal volume in $\mathbf{Q}_s$.

For a rectangular matrix $\mathbf{A}\in\mathbb{R}^{p\times q}$, the \emph{rect-volume} can be defined (up to transformations) as

\begin{equation}
    \text{rect-vol}(\mathbf{A}) \;=\;\sqrt{\det(\mathbf{A}\,\mathbf{A}^T)}
\end{equation}


Maximizing $\text{rect-vol}(\mathbf{A})$ with respect to the selection of rows corresponds to identifying the subset of frames that best preserves the variation in the data. We denote the selected row indices as

\begin{equation}
    \mathbf{r} \;=\; \arg\max_\mathbf{r}\,\text{rect-vol}\bigl(\mathbf{Q}_s(\mathbf{r},:)\bigr)
\end{equation}

These indices $\mathbf{r}$ then specify the frames $i_n$ we deem most representative.

\paragraph{Resulting Frame Subset.}
Let $r=|\mathbf{r}|$ be the number of selected frames. The final submatrix

\begin{equation}
    \mathbf{S} \;=\; \mathbf{Q}_s(\mathbf{r},:)
\end{equation}

is an $r\times s$ matrix containing the “diverse, high-information” frames. We feed only these $r$ frames into the downstream VLLM:

\begin{equation}
    \mathbf{A}_{\text{MaxInfo}} \;=\; \text{VLLM}\bigl(\text{Instruction},\;\mathbf{S},\;\text{Questions}\bigr),
\end{equation}

as opposed to using all $n$ frames (which might be computationally prohibitive or redundant):

\begin{equation}
    \mathbf{A}_{\text{Init}} \;=\; \text{VLLM}\bigl(\text{Instruction},\;\mathbf{Q},\;\text{Questions}\bigr).
\end{equation}

Since $r\ll n$ in most cases, this yields a more efficient inference process without sacrificing essential visual context.  In practice, VLLMs typically process a fixed number of frames \textit{k} (e.g., every 1, 32, 64, 128 frame). Instead of directly selecting \textit{k} frames or reducing the number of frames the network processes, we propose two versions of MaxInfo implementation: fast and slow. In the fast version, we apply MaxVol to the default number of frames and reduce it (e.g., reducing 256 frames to 180). In the slow version, we apply MaxVol to a larger set of frames (e.g., starting with 1024 frames and reducing it to 256), ensuring a richer pool of information for frame selection. We then apply CLIP embeddings and the MaxVol algorithm to intelligently refine this selection, maximizing the diversity and informativeness of the final k frames passed to the network. This ensures that VLLMs receive the most representative frames, improving their ability to understand long-form video content efficiently.

\subsection{Fast and Slow Version}

Given the differing objectives of the fast and slow versions of MaxInfo, we conducted a comparison to determine the most suitable option for each experimental setup. Additionally, we compared the two variants of MaxInfo, each designed to address different computational constraints:

\begin{enumerate}
    \item \textbf{Fast Version.}  
    MaxInfo is applied directly to the same number of frames as the original uniform sampling.  
    For instance, if the base model uses $n$ frames, we retain $n$ frames and rely on MaxInfo to identify 
    the most informative subset. This incurs minimal computational overhead and provides a quick improvement 
    without altering the model’s default settings.

    \item \textbf{Slow Version.}  
    A larger pool of frames ($N \gg n$) is initially sampled to ensure extensive coverage.  
    The MaxInfo Block is then applied to select the most diverse frames.  
    If the resulting set $x$ exceeds the model’s maximum input limit $n$, 
    we uniformly downsample $x \to n$.  
    This approach offers potentially higher gains by starting with more frames, 
    albeit at the cost of additional embedding computations.
\end{enumerate}

\subsection{Scene-Aware MaxInfo}

Videos often contain multiple scenes with similar visual content per scene, making Chunk-Based MaxInfo selection suboptimal. If applied globally, MaxInfo may discard crucial frames due to artificial embedding similarities across different scenes.

To address this, we introduce \textbf{Scene-Aware MaxInfo}, a very simple and effective modification. Instead of performing complex scene detection, we uniformly split the video into 32 equal-sized chunks and apply MaxVol independently within each chunk. This ensures that each segment is well-represented while maintaining computational simplicity.

\paragraph{Chunk-Based MaxVol Selection}
Given $n$ uniformly sampled frames, we split them into 32 contiguous chunks:

\begin{equation}
    \mathbf{I} = \bigcup_{i=1}^{32} \mathbf{I}^{(i)}, \quad \mathbf{I}^{(i)} = \{i_j \mid j \in \text{chunk } i\}.
\end{equation}

For each chunk, we extract CLIP embeddings, apply SVD for dimension reduction, and run MaxVol to select the most representative frames.

This approach is deliberately simple but demonstrates that even Chunk-Based scene segmentation can further enhance MaxInfo’s effectiveness. It highlights the potential for more refined scene-aware selection in future work.

\subsection{Summary}

Our pipeline can be summarized with an \cref{MaxInfo}. Because our method is \emph{training-free} and focuses purely on data selection, it can be plugged into diverse video-focused VLLMs to reduce cost and improve performance on long-video tasks.

\begin{algorithm}[tb]
    \caption{MaxInfo Block: SVD + MaxVol for Keyframe Selection}
    \label{MaxInfo}
\begin{algorithmic}[1]
    \STATE \textbf{Input:} A set of $n$ frames \(\mathbf{I} = \{i_1, i_2, \dots, i_n\}\)
    \STATE \textbf{Embedding:} Convert each frame \(i_j\) into a [CLS] embedding:
    \[
    q_n = \mathrm{flatten}\bigl(\mathrm{clip\_model}(i_n)\bigr),
    \mathbf{Q} = 
    \begin{bmatrix}
        q_1 \\ q_2 \\ \vdots \\ q_n
    \end{bmatrix} \in \mathbb{R}^{n\times d}.
    \]
    \STATE \textbf{SVD Reduction:} Perform truncated SVD on \(\mathbf{Q}\): 
    \[
    \mathbf{Q} \approx \mathbf{U}_r \, \mathbf{\Sigma}_r \, \mathbf{V}_r^\top \quad \rightarrow \quad
    \mathbf{Q_s} = \mathbf{U}_r \in \mathbb{R}^{n \times r}.
    \]
    \STATE \textbf{MaxVol Selection:} Run \(\text{rect\_maxvol}(\mathbf{Q_s}, \mathrm{tol})\) to find pivot indices:
    \[
    \mathrm{piv} = \text{rect\_maxvol}(\mathbf{Q_s}, \mathrm{Tol}),
    \]
    identifying rows (frames) that span the reduced embedding space.
    \STATE \textbf{Output:} Indices \(\mathrm{piv}\) of the most informative keyframes.
\end{algorithmic}
\end{algorithm}

\section{Experiments}

\subsection{Implementation Details}

In order to evaluate the contribution of MaxInfo to video understanding, we employed widely-used video understanding benchmarks, covering short-video tasks (such as MVBench \cite{mvbench}) and medium-to-long video tasks (such as EgoSchema \cite{egoschema}, Video-MME \cite{video-mme}, and LongVideoBench \cite{longvu}). For complete fairness, we only compared improved versions of the model against itself without MaxVol with freezing generation parameters, seed and prompt. We mostly focus on longer videos because better frame selection plays a bigger role in longer, more complex videos, whereas shorter ones intuitively work well with uniform sampling due to their lower information content and complexity.

We ensured that the resulting sequence length of a set of visual and textual tokens did not exceed the maximum sequence length for this LLM. When evaluating models using MaxInfo, we limited the number of selected frames so that they did not exceed the maximum allowed for the context of the estimated VLLM. For the evaluation on all benchmarks, we have set the generation temperature to 0.

For the general multiple-choice question-answering evaluation, we follow the official guidelines to construct the instructions using the provided questions and options. We added a prompt to the question and options like \textit{"Respond with only the letter (A, B, C, or D) of the correct option."} for LongVideoBench \cite{longvideobench}, Video-MME \cite{video-mme}, MLVU \cite{mlvu} and MVBench \cite{mvbench} or \textit{"Answer with the option's letter from the given choices directly and only give the best option."} for EgoSchema \cite{egoschema}. We follow the original benchmarks setup to calculate the final scores, and we also align our evaluation protocols with other evaluation toolkits, such as lmms-eval \cite{zhang2024lmmseval}.

To ensure the reproducibility of our results, we have included the main hyperparameters used for all benchmarks and estimated models in the results tables, such as tolerance and rank for the MaxInfo algorithm, the number of sampled frames, and the number of initial frames (before MaxInfo).


\subsection{Main Results}

\textbf{Overall Performance.}  
\cref{proof-maxinfo2}, \cref{proof-maxinfo1} and \cref{compare-details} present the performance gains achieved by integrating MaxInfo into existing InternVL \cite{internvl}, Qwen2-VL \cite{qwen2-vl} and LlaVA-Video \cite{llava-video} models. The experimental results show that MaxInfo Block exhibits significant performance improvement on a wide range of models. In particular, in the LLaVA-Video-7B and Qwen-VL-2B models, the introduction of MaxInfo Block improves the accuracy by 0.9\%/1.7\%, 6.4\%, 3.3\% and 1.4\%/1.2\%, 2.3\%, 1.5\% in VideoMME, EgoSchema, and LongVideoBench, respectively, which is significantly better than the versions without the block. This improvement not only validates the effectiveness of MaxInfo Block, but also provides new ideas for future research on video understanding tasks.

\input{tables/1_comparison_with_self_table_mian}
\input{tables/1_comparison_with_self_table_intern}
\input{tables/4_compare_details}

\textbf{MLVU Benchmark.} \cref{mlvu_aware} evaluates both MaxInfo and the benefits of Chunk-Based Scene-Awareness on the MLVU benchmark, demonstrating consistent improvements over uniform sampling. While these improvements are modest, it is important to highlight that the method is exceptionally simple and entirely training-free. This simplicity underscores the potential of even basic scene-awareness to enhance performance, suggesting that more sophisticated scene detection techniques could lead to further gains. Such results indicate that scene-awareness is a promising direction for improving video understanding tasks.


\input{tables/8_mlvu_awareness}


\subsection{Performance Analysis: MaxInfo vs. Uniform Sampling}

To better understand the strengths and trade-offs of MaxInfo, we analyzed per-task accuracy across multiple benchmarks.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.99\linewidth]{images/maxinfo-video-mme.pdf}}
\caption{Accuracy comparison between Uniform Sampling and MaxInfo on Video-MME (Qwen2-VL-2B).}
\label{details-video-mme}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.99\linewidth]{images/maxinfo-lvb.pdf}}
\caption{Accuracy comparison between Uniform Sampling and MaxInfo on LongVideoBench (Qwen2-VL-2B).}
\label{details-longvideobench}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.85\linewidth]{images/maxinfo-MVLU.pdf}}
\caption{Accuracy comparison between Uniform Sampling and MaxInfo on MLVU (Qwen2-VL-7B) with 64 final and 1024 initial frames.}
\label{details-mlvu}
\end{center}
\vskip -0.2in
\end{figure}

Our results, as shown in \cref{details-video-mme}, \cref{details-longvideobench}, and \cref{details-mlvu}, indicate that MaxInfo outperforms uniform sampling on tasks requiring high information density (e.g., counting, summarization, and spatial reasoning). However, for tasks requiring fine-grained temporal reasoning (e.g., object recognition and action perception), uniform sampling retains a slight advantage. These findings highlight a fundamental trade-off between information maximization and temporal continuity, suggesting potential directions for hybrid selection methods.

\subsection{Fast vs.\ Slow Version Comparisons} \label{subsec:fast-slow}

As \cref{slow-fast-version} shows, the slow version outperforms the fast version in most cases, particularly when processing longer videos. The initial oversampling mechanism in the slow version provides MaxInfo with a richer selection space, which significantly improves performance. However, experimental results indicate that the fast version may outperform the slow version in specific cases, depending on the benchmark type and the number of initial samples. Nevertheless, in scenarios with limited computational resources or strict latency requirements, the fast version remains a practical and reliable choice due to its efficiency. This flexibility allows the two versions to adapt to different task requirements, offering users more options.

\input{tables/5_slow_ann_fast_versions}

\subsection{Ablation Study}

To further evaluate the effectiveness of MaxInfo, we conducted a series of ablation experiments focusing on two key aspects: the impact of the choice of visual encoder and the influence of key hyperparameters in the MaxInfo module, particularly tolerance and rank.

\subsubsection{Vision Encoder Impact}

\cref{different-encoder} presents an ablation study on different visual encoders used for frame representation in MaxInfo.


\input{tables/7_ablation_experiments_visual_enc}

We evaluated CLIP, DINOv2, and SigLIP as visual encoders. DINOv2 performed comparably to CLIP, despite lacking vision-language alignment. However, SigLIP outperformed both CLIP and DINOv2, likely due to its stronger language-vision connectivity. Interestingly, the larger SigLIP model underperformed, suggesting that more complex encoders require careful hyperparameter tuning (e.g., tolerance and rank) for optimal frame selection.

\subsubsection{Impact of MaxInfo Block Hyperparameters}

This section examines the effect of MaxInfo parameters (rank R and tolerance Tol) as well as the initial number of samples on the final model accuracy.

\textbf{Effect of Different Tolerances and Ranks on the Model with Fixed Sampling.} We conducted a series of tests with fixed benchmarks, model settings, and an initial pool of frames (e.g., \(n^*=96\)). As shown in \cref{un-fixed-r-tol}, the best observed result achieved a \textbf{3.3\% improvement} over the base LLaVA-Video-7B model with \(\text{Tol} = 0.3\) and \(\text{R} = 8\).

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\linewidth]{images/R-and-Tol-Un-Fixed}}
\caption{Effect of initial sampling on MaxInfo performance for LlaVa-Video 7B model (fixed R=8, Tol=0.15).}
\label{un-fixed-r-tol}
\end{center}
\vskip -0.2in
\end{figure}

From these experiments, we derived the following guidelines:

\begin{enumerate}
    \item \textbf{Large Rank, Moderate Tol:}  
    When the Rank \(R\) is large, a moderate Tolerance (\(\text{Tol}\)) helps avoid selecting overly similar frames, optimizing the trade-off between diversity and redundancy.

    \item \textbf{Balanced Selection:}  
    An excessively large \(R\) combined with a very small \(\text{Tol}\) may cause the frame set to grow excessively, potentially exceeding the model’s capacity. Thus, \(R\) and \(\text{Tol}\) should be selected based on the maximum feasible input frame count for a given LLM.

    \item \textbf{Performance Sensitivity:}  
    Our experiments show that performance is most sensitive to \(\text{Tol}\) values in the range \(\text{Tol} \in [0.1, 0.6]\). Beyond this range, improvements plateau or regress due to over-pruning or under-pruning.

    \item \textbf{Convergence:}  
    When setting \(R = 12\) and varying \(\text{Tol}\) values to 0.30, 0.45, and 0.60, the best result converged at 61.1\%.
\end{enumerate}


\textbf{Effect of Sampling on Accuracy (Fixed Rank and Tolerance).} We analyzed how varying the initial number of sampled frames impacts accuracy, while keeping the hyperparameters fixed (R=8, Tol=0.15). As shown in \cref{fixed-r-tol}, accuracy initially improves with the addition of more frames, but beyond a certain threshold, it begins to plateau or slightly decline. Our key observations are as follows:

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\linewidth]{images/R-and-Tol-Fixed}}
\caption{Comparing the effect of initial sampling on the MaxInfo Block, n$^*$ represents the initial number of frames sampled, and after passing through the MaxInfo Block, the number of frames is increased to a maximum of 64.}
\label{fixed-r-tol}
\end{center}
\vskip -0.2in
\end{figure}

\begin{enumerate}
    \item Increasing the initial number of frames provides more diverse information, enabling MaxVol to select better keyframes.   
    \item Beyond a certain number of frames, accuracy slightly declines due to excessive redundancy, which hampers MaxVol's efficiency in keyframe selection.  
    \item There exists an optimal trade-off between the initial frame count and computational cost. In our tests, 128 as initial frames yielded the best accuracy for the LLaVA-Video-7B model.
\end{enumerate}

\section{Conclusion}

In this work, we introduced \textbf{MaxInfo} -- a training-free method for selecting the most informative frames from videos, improving VLLMs inference. Our results consistently demonstrate that informative frame selection outperforms uniform sampling, leading to improved performance of state of the art VLLMs (LLaVA-Video, InternVL and Qwen2-VL with different sizes) across multiple benchmarks. However, the improvements are relatively small, which is expected given the training-free nature and the simplicity of our approach. 

Beyond demonstrating empirical gains, we believe our work will encourage the community to focus more on frame selection strategies, an often-overlooked aspect of video understanding. Additionally, we have shown that even minimal refinements, such as chunk-wise MaxVol in our Scene-Aware MaxInfo, can further enhance results, demonstrating that simple adjustments can lead to meaningful improvements.

Finally, we hypothesize that training VLLMs with informative frame sampling, rather than simple uniform frame selection, could further enhance their capabilities when later used with inference-time MaxInfo techniques. We hope this work serves as a foundation for future research into more efficient, information-aware video sampling strategies for large-scale multimodal learning.

\section*{Impact Statement}


This work introduces a training-free framework for improving frame sampling in Vision-Language Large Models (VLLMs), enhancing video understanding tasks. Such advancements are crucial for applications in education, accessibility, and public safety, where accurate and efficient video analysis can have a meaningful impact.

However, improved video analysis capabilities may also raise ethical concerns, including potential misuse in surveillance, privacy violations, or biases in automated systems. Ensuring responsible deployment with a focus on fairness and transparency is essential to mitigate these risks.

In summary, while our approach offers significant benefits, its adoption must adhere to ethical principles to promote equitable and responsible use.

\bibliography{main}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{Appendix}

\subsection{Visualisation of MaxInfo Block results}
\input{images/appendix_maxvol/input_images}

As shown \cref{maxvol-visualisation}, MaxInfo Block can help us to reduce the redundancy of video information, better understand the video content and improve the model performance by selecting the most informative and representative key frames among multiple frames.

\subsection{Additional comparisons with other models}

We compared the performance of VLLMs of the same 7B size, as shown in \cref{same-size-sota}, such as Open Source Model, Proprietary Models and Train-Free Model or Key Frames selection, on Video-MME Benchmark.

\input{tables/2_currently_sota_and_training_free}

\subsection{Impact of MaxInfo Block's Tol and Rank on Models.}
Overview of the details of how Tol and R affect the accuracy of the model, as \cref{effect-tol-rank}, I.Frame is the number of frames input to the MaxInfo module, and F.Frame is the maximum number of frames after going through the MaxInfo Block. The effect of the initial number of samples on the model as shown in \cref{effect-fixed-tol-rank}.

\input{tables/9_appendix}


\end{document}
