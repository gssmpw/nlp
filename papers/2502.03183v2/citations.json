[
  {
    "index": 0,
    "papers": [
      {
        "key": "blip2",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "mvbench",
        "author": "Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others",
        "title": "Mvbench: A comprehensive multi-modal video understanding benchmark"
      },
      {
        "key": "xenos2024vllms",
        "author": "Xenos, Alexandros and Foteinopoulou, Niki Maria and Ntinou, Ioanna and Patras, Ioannis and Tzimiropoulos, Georgios",
        "title": "VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dino",
        "author": "Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\\'e}gou, Herv{\\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand",
        "title": "Emerging properties in self-supervised vision transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Siglip",
        "author": "Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas",
        "title": "Sigmoid loss for language image pre-training"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "longvu",
        "author": "Shen, Xiaoqian and Xiong, Yunyang and Zhao, Changsheng and Wu, Lemeng and Chen, Jun and Zhu, Chenchen and Liu, Zechun and Xiao, Fanyi and Varadarajan, Balakrishnan and Bordes, Florian and others",
        "title": "Longvu: Spatiotemporal adaptive compression for long video-language understanding"
      },
      {
        "key": "Chat-univi",
        "author": "Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li",
        "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding"
      },
      {
        "key": "video-lavit",
        "author": "Jin, Yang and Sun, Zhicheng and Xu, Kun and Chen, Liwei and Jiang, Hao and Huang, Quzhe and Song, Chengru and Liu, Yuliang and Zhang, Di and Song, Yang and others",
        "title": "Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization"
      },
      {
        "key": "qwen2-vl",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      },
      {
        "key": "internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      },
      {
        "key": "omnifusion",
        "author": "Goncharova, Elizaveta and Razzhigaev, Anton and Mikhalchuk, Matvey and Kurkin, Maxim and Abdullaeva, Irina and Skripkin, Matvey and Oseledets, Ivan and Dimitrov, Denis and Kuznetsov, Andrey",
        "title": "Omnifusion technical report"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "video-lavit",
        "author": "Jin, Yang and Sun, Zhicheng and Xu, Kun and Chen, Liwei and Jiang, Hao and Huang, Quzhe and Song, Chengru and Liu, Yuliang and Zhang, Di and Song, Yang and others",
        "title": "Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "longvu",
        "author": "Shen, Xiaoqian and Xiong, Yunyang and Zhao, Changsheng and Wu, Lemeng and Chen, Jun and Zhu, Chenchen and Liu, Zechun and Xiao, Fanyi and Varadarajan, Balakrishnan and Bordes, Florian and others",
        "title": "Longvu: Spatiotemporal adaptive compression for long video-language understanding"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "song2024moviechat",
        "author": "Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others",
        "title": "Moviechat: From dense token to sparse memory for long video understanding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "slowfast",
        "author": "Xu, Mingze and Gao, Mingfei and Gan, Zhe and Chen, Hong-You and Lai, Zhengfeng and Gang, Haiming and Kang, Kai and Dehghan, Afshin",
        "title": "Slowfast-llava: A strong training-free baseline for video large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Chat-univi",
        "author": "Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li",
        "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "qwen2-vl",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "team2024gemini",
        "author": "Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "peng2005feature",
        "author": "Peng, Hanchuan and Long, Fuhui and Ding, Chris",
        "title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "MMD",
        "author": "Vasconcelos, Nuno",
        "title": "Feature selection by maximum marginal diversity"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "MOI",
        "author": "Sindhwani, Vikas and Rakshit, Subrata and Deodhare, Dipti and Erdogmus, Deniz and Principe, Jos{\\'e} Carlos and Niyogi, Partha",
        "title": "Feature selection in MLPs and SVMs based on maximum output information"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "goreinov2010find",
        "author": "Goreinov, Sergei A and Oseledets, Ivan V and Savostyanov, Dimitry V and Tyrtyshnikov, Eugene E and Zamarashkin, Nikolay L",
        "title": "How to find a good submatrix"
      },
      {
        "key": "Rectmax",
        "author": "Mikhalev, Aleksandr and Oseledets, Ivan V",
        "title": "Rectangular maximum-volume submatrices and their applications"
      },
      {
        "key": "sozykin2022ttopt",
        "author": "Sozykin, Konstantin and Chertkov, Andrei and Schutski, Roman and Phan, Anh-Huy and Cichocki, Andrzej S and Oseledets, Ivan",
        "title": "TTOpt: A maximum volume quantized tensor train-based optimization and its application to reinforcement learning"
      }
    ]
  }
]