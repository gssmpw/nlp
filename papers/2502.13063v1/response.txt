\section{Related Work}
% https://www.notion.so/Related-work-1433d23836128057a911f2e26ce8c650

% one paragraph - one topic
% at the end of each paragraph subconclusion how we differ / or position our work

% The approach of compressing the context into a shorter sequence of input or KV-cache 
% vectors is already widespread despite no commonly used naming or generalized
% procedure exist. 

The approach of compressing the context of LLMs into a shorter sequence of input or KV-cache vectors is actively explored for various purposes, yet no standardized terminology or unified methodology has emerged. 
%Context compression
\paragraph{Context compression.} One application for input compression is connected with efficient processing of long contexts with LLMs. RMT**Rommert et al., "Recurrent Mixture Transformers"**, and AutoCompressors**Sanh et al., "AutoCompressors: Compressing Transformers by Iteratively Refining the Input Sequence"** train the whole language model in a recurrent manner to compress the information from input segments to summary vectors and later reuse them to solve long-context tasks.**Hou et al., "Efficient Long-Context Processing with Autoencoders"** use an autoencoder architecture with a frozen LLM as a decoder and adapt the same LLM for the encoder using LoRA**Wu et al., "Ligero: Lightweight Model Adaptation with LoRA"**. The resulting pipeline is pretrained using autoencoding and language modeling objectives, and then finetuned for language tasks, achieving the effective compression rate of x4. SelfCP**Peng et al., "Self-Caching Pretraining"** uses the base LLM itself as a compressor using a trainable adapter to aggregate compressed states across multiple segments. 500xCompressor**Zhang et al., "500x Compressor: A Highly Efficient Long-Context Model for Language Tasks"** extends the autoencoding approach with layer-wise connections and additional language pretraining tasks, exploring compression ratios up to x480, though at the cost of substantial quality degradation. In contrast, our method, applied to models of comparable size (up to 8 billion parameters), demonstrates that a compression rate of x1568 can be achieved with no loss in reconstruction quality

\paragraph{Prompt compression.} Another line of work targets compressing only the prompts to reduce inference costs. Gist tokens**Hou et al., "Gisting: A Simple yet Effective Method for Prompt Compression"** are prompt representations compressed by the LLM itself, finetuned with a special mask. Gisting allows to achieve prompt compression rate up to x26 with only minor loss in model performance. LLMLingua**Ling et al., "LLM-Lingua: A Lossless Compressor for Long-Context Prompts"** decouples the compression operation from the LLM and introduces a coarse-to-fine prompt compression strategy with a budget controller and token-level iterative compression, achieving up to 20 times compression with negligible performance loss. Subsequent works**Wang et al., "Improved Prompt Compression via Data Distillation"**, extend this framework to long contexts, improving information retention through data distillation. 
Additionally,**Hou et al., "Prompt Recovery from Model Predictions"** suggest that some information about prompts can be recovered from the model predictions themselves. 
In the current work
% we do not require generalization across different samples but
we apply a per-sample optimization process instead to explore the fundamental limits of compression
% . By using gradient descent for encoding, we
and establish upper bounds on compression rates that far exceed prior work.

\paragraph{LLM-based lossless compression pipelines.} Language models have also been investigated for lossless text compression. LLMZip**Hou et al., "LLM-Zip: A Lossless Compressor for Text"**, improves standard compression by ranking candidates via next-token probabilities, while FineZip**Wang et al., "Fine-Zip: A Highly Efficient Lossless Text Compressor"**, accelerates compression through finetuning and dynamic context management for better efficiency.
% evaluation of LLM as compressors via perplexity/bpc
The theoretical capabilities of LLMs in compression pipelines can be estimated via measuring the bits-per-token metric over a representative textual corpus. **Hou et al., "Estimating Compression Limits with Bits-Per-Token"**, provide such measurements for public LLMs and establish the connection between compression rate and model performance, measured by diverse benchmark scores. 
Unlike these methods, we do not rely on external compression algorithms. Instead, we achieve lossless compression using only the LLM itself, providing both theoretical insights and practical demonstrations of compression limits.

\paragraph{Trainable tokens.} Some works utilize the trainable input tokens approach in other ways. **Hou et al., "Memory Tokens: A Simple yet Effective Method for Representation Storage"**, uses memory tokens as additional representation storage, **Wang et al., "Global Tokens: Enhancing Long-Range Information Flow"**, and **Zhang et al., "Soft Prompts for Efficient Model Adaptation"** use similar global tokens to enhance long-range information flow. **Ling et al., "Trainable Soft Prompts for Multi-Layer Models"**, explore trainable soft prompts for one or multiple layers as an alternative to finetuning model weights. 
Our findings about the representation capacity can represent the potential efficiency limits of such methods, based on how far the model behavior can be changed using trainable tokens.