@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@misc{burtsev2021memory,
      title={Memory Transformer}, 
      author={Mikhail S. Burtsev and Yuri Kuratov and Anton Peganov and Grigory V. Sapunov},
      year={2021},
      eprint={2006.11527},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chevalier2023adapting,
  title={Adapting language models to compress contexts},
  author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14788},
  year={2023}
}

@article{gao2024selfcp,
  title={SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself},
  author={Gao, Jun},
  journal={arXiv preprint arXiv:2405.17052},
  year={2024}
}

@article{ge2023incontext,
  title={In-context autoencoder for context compression in a large language model},
  author={Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  journal={arXiv preprint arXiv:2307.06945},
  year={2023}
}

@article{guo2024ranking,
  title={Ranking LLMs by compression},
  author={Guo, Peijia and Li, Ziguang and Hu, Haibo and Huang, Chao and Li, Ming and Zhang, Rui},
  journal={arXiv preprint arXiv:2406.14171},
  year={2024}
}

@article{huang2024compression,
  title={Compression represents intelligence linearly},
  author={Huang, Yuzhen and Zhang, Jinghan and Shan, Zifei and He, Junxian},
  journal={arXiv preprint arXiv:2404.09937},
  year={2024}
}

@inproceedings{jiang2023llmlingua,
    title = "{LLML}ingua: Compressing Prompts for Accelerated Inference of Large Language Models",
    author = "Jiang, Huiqiang  and
      Wu, Qianhui  and
      Lin, Chin-Yew  and
      Yang, Yuqing  and
      Qiu, Lili",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.825/",
    doi = "10.18653/v1/2023.emnlp-main.825",
    pages = "13358--13376",
}

@article{jiang2023longllmlingua2,
  title={Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression},
  author={Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.06839},
  year={2023}
}

@inproceedings{lester2021power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243/",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
}

@inproceedings{li2021prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353/",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
}

@article{li2024500xcompressor,
  title={500xCompressor: Generalized Prompt Compression for Large Language Models},
  author={Li, Zongqian and Su, Yixuan and Collier, Nigel},
  journal={arXiv preprint arXiv:2408.03094},
  year={2024}
}

@inproceedings{liu2022ptuning,
    title = "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.8/",
    doi = "10.18653/v1/2022.acl-short.8",
    pages = "61--68",
}

@article{mittu2024finezip,
  title={Finezip: Pushing the limits of large language models for practical lossless text compression},
  author={Mittu, Fazal and Bu, Yihuan and Gupta, Akshat and Devireddy, Ashok and Ozdarendeli, Alp Eren and Singh, Anant and Anumanchipalli, Gopala},
  journal={arXiv preprint arXiv:2409.17141},
  year={2024}
}

@article{morris2023language,
  title={Language model inversion},
  author={Morris, John X and Zhao, Wenting and Chiu, Justin T and Shmatikov, Vitaly and Rush, Alexander M},
  journal={arXiv preprint arXiv:2311.13647},
  year={2023}
}

@article{mu2023learning,
  title={Learning to compress prompts with gist tokens},
  author={Mu, Jesse and Li, Xiang and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={19327--19352},
  year={2023}
}

@article{pan2024llmlingua,
  title={Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression},
  author={Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2403.12968},
  year={2024}
}

@inproceedings{rmt_2022,
 author = {Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {11079--11091},
 publisher = {Curran Associates, Inc.},
 title = {Recurrent Memory Transformer},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{valmeekam2023llmzip,
  title={Llmzip: Lossless text compression using large language models},
  author={Valmeekam, Chandra Shekhara Kaushik and Narayanan, Krishna and Kalathil, Dileep and Chamberland, Jean-Francois and Shakkottai, Srinivas},
  journal={arXiv preprint arXiv:2306.04050},
  year={2023}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

