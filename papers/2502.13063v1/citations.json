[
  {
    "index": 0,
    "papers": [
      {
        "key": "rmt_2022",
        "author": "Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail",
        "title": "Recurrent Memory Transformer"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chevalier2023adapting",
        "author": "Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi",
        "title": "Adapting language models to compress contexts"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ge2023incontext",
        "author": "Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu",
        "title": "In-context autoencoder for context compression in a large language model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hu2022lora",
        "author": "Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "Lo{RA}: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "gao2024selfcp",
        "author": "Gao, Jun",
        "title": "SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2024500xcompressor",
        "author": "Li, Zongqian and Su, Yixuan and Collier, Nigel",
        "title": "500xCompressor: Generalized Prompt Compression for Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "mu2023learning",
        "author": "Mu, Jesse and Li, Xiang and Goodman, Noah",
        "title": "Learning to compress prompts with gist tokens"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "jiang2023llmlingua",
        "author": "Jiang, Huiqiang  and\nWu, Qianhui  and\nLin, Chin-Yew  and\nYang, Yuqing  and\nQiu, Lili",
        "title": "{LLML}ingua: Compressing Prompts for Accelerated Inference of Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jiang2023longllmlingua2",
        "author": "Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili",
        "title": "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression"
      },
      {
        "key": "pan2024llmlingua",
        "author": "Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\\\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others",
        "title": "Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "morris2023language",
        "author": "Morris, John X and Zhao, Wenting and Chiu, Justin T and Shmatikov, Vitaly and Rush, Alexander M",
        "title": "Language model inversion"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "valmeekam2023llmzip",
        "author": "Valmeekam, Chandra Shekhara Kaushik and Narayanan, Krishna and Kalathil, Dileep and Chamberland, Jean-Francois and Shakkottai, Srinivas",
        "title": "Llmzip: Lossless text compression using large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "mittu2024finezip",
        "author": "Mittu, Fazal and Bu, Yihuan and Gupta, Akshat and Devireddy, Ashok and Ozdarendeli, Alp Eren and Singh, Anant and Anumanchipalli, Gopala",
        "title": "Finezip: Pushing the limits of large language models for practical lossless text compression"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "huang2024compression",
        "author": "Huang, Yuzhen and Zhang, Jinghan and Shan, Zifei and He, Junxian",
        "title": "Compression represents intelligence linearly"
      },
      {
        "key": "guo2024ranking",
        "author": "Guo, Peijia and Li, Ziguang and Hu, Haibo and Huang, Chao and Li, Ming and Zhang, Rui",
        "title": "Ranking LLMs by compression"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "burtsev2021memory",
        "author": "Mikhail S. Burtsev and Yuri Kuratov and Anton Peganov and Grigory V. Sapunov",
        "title": "Memory Transformer"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zaheer2020big",
        "author": "Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others",
        "title": "Big bird: Transformers for longer sequences"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2021prefix",
        "author": "Li, Xiang Lisa  and\nLiang, Percy",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      },
      {
        "key": "lester2021power",
        "author": "Lester, Brian  and\nAl-Rfou, Rami  and\nConstant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      },
      {
        "key": "gao2024selfcp",
        "author": "Gao, Jun",
        "title": "SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself"
      },
      {
        "key": "liu2022ptuning",
        "author": "Liu, Xiao  and\nJi, Kaixuan  and\nFu, Yicheng  and\nTam, Weng  and\nDu, Zhengxiao  and\nYang, Zhilin  and\nTang, Jie",
        "title": "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"
      }
    ]
  }
]