% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

%user imported packages
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage[capitalise]{cleveref}
\usepackage{lipsum}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{adjustbox}

\definecolor{gray1}{gray}{0.85}
\definecolor{gray2}{gray}{0.75}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{You Can Cram 1500 Tokens into a Single Vector}

% \title{Cramming 1568 Tokens into a Single Vector: There and Back Again in Embedding Space}
\title{Cramming 1568 Tokens into a Single Vector and Back Again:\\ Exploring the Limits of Embedding Space Capacity}

% inspiration from:
% “Hobbit, or There and Back Again”
% and
% “You can’t cram the meaning of a whole *** sentence into a single *** vector.”
% by Raymond Mooney

% another ideas:
% - Efficient Token Compression and Reconstruction in Embedding Space
% - On the Limits of Hidden State Compression: Quantifying the Capacity of Trainable Input Vectors in Language Models
% - On the Capacity Limits of Input Embedding Spaces: An Empirical Study of Text-to-Vector Compression
% - Cramming 1568 Tokens into a Single Vector: An Empirical Study of Embedding Space Capacity Limits

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
Yuri Kuratov$^{1,2}$\quad
Mikhail Arkhipov$^3$\quad
Aydar Bulatov$^{1,2}$\quad
Mikhail Burtsev$^4$\\\\
$^1$AIRI, Moscow, Russia\\
$^2$Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia \\
$^3$Independent Researcher, Amsterdam, Netherlands \\
$^4$London Institute for Mathematical Sciences, London, UK\\
\\
 \small{
   \textbf{Correspondence:} \href{mailto:yurii.kuratov@phystech.edu}{yurii.kuratov@phystech.edu}
 }
}

% \author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
% \\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
% \\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
% }

\begin{document}
\maketitle
\begin{abstract}
A range of recent works addresses the problem of compression of sequence of tokens into  a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches allow to reduce the amount of compute in existing language models. Despite relying on powerful models as  encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size.  In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. 
The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.
\end{abstract}

\section{Introduction}

% llms input space size is large ranging from 1024 for 1B models to 8192 for 70B models and to 16384 for 405B, however this input space is used to encode only single input token.
Most large language models (LLMs) are built on the Transformer architecture~\citep{vaswani2017attention} and have demonstrated remarkable performance as their parameters scale~\citep{radford2019gpt2,brown2020language,kaplan2020scaling,chinchilla_scaling}. As model sizes increase, so does the dimensionality of their input embeddings. However, despite this growth, each embedding still represents only a single token, e.g., for a series of Llama models embeddings size is growing from 2,048 in 1B-parameter models to 16,384 float numbers in 405B-parameter models~\citep{grattafiori2024llama}. Remarkably, even a 2,048-dimensional vector of 16-bit floats has a theoretical capacity of 32,768 bits, which is sufficient to encode roughly 1,931 tokens from a vocabulary of size 128,256. \emph{This observation motivates us to explore whether language models can utilize the latent capacity of input vectors more effectively, potentially encoding and processing multiple tokens with a single vector.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/results_brief.pdf}
    \caption{
    \textbf{How many tokens fit into a single input vector?} We estimate maximum number of tokens that can be decoded from a single input vector across various language models. %Each dot shows the maximum length of texts that can be accurately decoded from single input vector.% based on evaluations on texts from PG-19.
    % and a discrete resolution grid (32, 64, 96, 128, 160, 192, 256, 512, 768, 1024, 1568, 2048).
    }
    \label{fig:results_brief}
\end{figure}

Encoding multiple tokens or even entire texts into a compact latent representation has been a longstanding challenge in natural language processing. It includes approaches, such as sentence embeddings~\citep{pmlr-v32-le14,NIPS2015_f442d33f,cer2018USE,wang2024e5multilingual} for semantic search and retrieval, and text autoencoders~\citep{bowman-etal-2016-generating,pmlr-v48-miao16,montero-etal-2021-sentence}, aimed to capture the essential meaning of texts in a dense representations.

In the context of LLMs, the challenge of encoding prompts and long contexts is particularly important because of the quadratic computational cost of the self-attention mechanism in Transformers. Several works have explored the possibility of replacing token-based prompts with a smaller set of dense vectors~\citep{lester2021power,li2021prefix,gao2024selfcp,li2024500xcompressor}, thereby shortening the input sequence and reducing the computational budget. These methods have demonstrated token-to-vector lossy compression ratios on the order of x500 with 8B-parameter models, indicating that it is possible to retain the critical information in a significantly reduced number of vectors. However, lossless compression is still limited by approximately factor of 10.

% embeddings are operational space of models, memory models, reasoning in latent space (coconut), thus vectors capacity is important for expressivity and power of those methods.

Furthermore, another important aspect of embeddings is that they also constitute the operational space in which the model's computations take place through residual stream.
In memory-augmented architectures~\citep{weston2014memory,sukhbaatar2015endtoend, burtsev2021memory}, these embeddings can act as additional storage or as a recurrent state for passing information between time steps~\citep{dai2019transformerxl,rmt_2022,chevalier2023adapting,behrouz2024titans}, essentially serving as an episodic memory. Moreover, recent approaches have explored the power of latent space reasoning~\citep{hao2024coconut} where high-capacity embeddings enable models to perform complex multi-step tasks directly in latent space. Consequently, the capacity of these vectors is crucial not only for efficient input representation, but also for increasing the overall expressiveness and computational power of models~\citep{merrill2023parallelism,strobl2024formal, sanford2024representational}.
Better understanding of the latent capacity of input vectors, could significantly help to improve encoding and retrieving of contextual information, episodic memory, as well as complex reasoning within large language models.

In this work, we investigate the limits of such input representations, exploring their capacity to encode and reconstruct long text sequences. By systematically quantifying how much additional information these vectors can capture, we provide insights into the efficiency and potential of latent representations in LLMs. Our main contributions:


1. We empirically study  capacity limits of LLM's input representations by compressing texts into trainable \texttt{[mem]} vectors.

2. We establish a direct connection between the latent capacity of input vectors and text cross-entropy, providing a quantitative measure of the information each vector can encode.

% 3. We systematically measure the compression limits across LLMs of different architectures, sizes, and release years on diverse text domains.

% 4. We demonstrate that each model's capacity limit remains consistent across text lengths and origins, including randomly generated texts.

3. We show that the capacity limits remain consistent across different text lengths and domains, including natural text and randomly generated word sequences.%, highlighting the generalizability of our findings.

% 3. We systematically measure compression limits across LLMs of different architectures, sizes, and release years, and demonstrate that these capacity limits remain consistent across different text domains, including natural and randomly generated text.

% 5. We introduce the ''pure capacity'' metric by decoupling compression and text generation capabilities of the model. 

% 6. We show the near-linear compression scaling with the number of trainable vectors. For example, Llama-3.2-1B compresses 7168 tokens into just 16 vectors. 

4. We introduce a set of metrics that decouple the capacity of trainable input vectors from the language model's inherent prediction abilities. Using these metrics, we demonstrate a nearly linear scaling of compression capacity with the number of trainable vectors (e.g., Llama-3.2-1B compresses 7,168 tokens into just 16 vectors).

\vspace{1em}
Our code is available at this \href{https://github.com/yurakuratov/hidden_capacity}{URL}.


\section{Related Work}

% https://www.notion.so/Related-work-1433d23836128057a911f2e26ce8c650

% one paragraph - one topic
% at the end of each paragraph subconclusion how we differ / or position our work

% The approach of compressing the context into a shorter sequence of input or KV-cache 
% vectors is already widespread despite no commonly used naming or generalized
% procedure exist. 

The approach of compressing the context of LLMs into a shorter sequence of input or KV-cache vectors is actively explored for various purposes, yet no standardized terminology or unified methodology has emerged. 
%Context compression
\paragraph{Context compression.} One application for input compression is connected with efficient processing of long contexts with LLMs. RMT~\cite{rmt_2022} and AutoCompressors~\cite{chevalier2023adapting} train the whole language model in a recurrent manner to compress the information from input segments to summary vectors and later reuse them to solve long-context tasks.~\citet{ge2023incontext} use an autoencoder architecture with a frozen LLM as a decoder and adapt the same LLM for the encoder using LoRA~\cite{hu2022lora}. The resulting pipeline is pretrained using autoencoding and language modeling objectives, and then finetuned for language tasks, achieving the effective compression rate of x4. SelfCP~\cite{gao2024selfcp} uses the base LLM itself as a compressor using a trainable adapter to aggregate compressed states across multiple segments. 500xCompressor~\cite{li2024500xcompressor} extends the autoencoding approach with layer-wise connections and additional language pretraining tasks, exploring compression ratios up to x480, though at the cost of substantial quality degradation. In contrast, our method, applied to models of comparable size (up to 8 billion parameters), demonstrates that a compression rate of x1568 can be achieved with no loss in reconstruction quality

\paragraph{Prompt compression.} Another line of work targets compressing only the prompts to reduce inference costs. Gist tokens~\cite{mu2023learning} are prompt representations compressed by the LLM itself, finetuned with a special mask. Gisting allows to achieve prompt compression rate up to x26 with only minor loss in model performance. LLMLingua~\cite{jiang2023llmlingua} decouples the compression operation from the LLM and introduces a coarse-to-fine prompt compression strategy with a budget controller and token-level iterative compression, achieving up to 20 times compression with negligible performance loss. Subsequent works~\cite{jiang2023longllmlingua2, pan2024llmlingua} extend this framework to long contexts, improving information retention through data distillation. 
Additionally,~\citet{morris2023language} suggest that some information about prompts can be recovered from the model predictions themselves. 
In the current work
% we do not require generalization across different samples but
we apply a per-sample optimization process instead to explore the fundamental limits of compression
% . By using gradient descent for encoding, we
and establish upper bounds on compression rates that far exceed prior work.

\paragraph{LLM-based lossless compression pipelines.} Language models have also been investigated for lossless text compression. LLMZip~\cite{valmeekam2023llmzip} improves standard compression by ranking candidates via next-token probabilities, while FineZip~\cite{mittu2024finezip} accelerates compression through finetuning and dynamic context management for better efficiency.
% evaluation of LLM as compressors via perplexity/bpc
The theoretical capabilities of LLMs in compression pipelines can be estimated via measuring the bits-per-token metric over a representative textual corpus. \citet{huang2024compression, guo2024ranking} provide such measurements for public LLMs and establish the connection between compression rate and model performance, measured by diverse benchmark scores. 
Unlike these methods, we do not rely on external compression algorithms. Instead, we achieve lossless compression using only the LLM itself, providing both theoretical insights and practical demonstrations of compression limits.

\paragraph{Trainable tokens.} Some works utilize the trainable input tokens approach in other ways. \citet{burtsev2021memory} uses memory tokens as additional representation storage, \citet{beltagy2020longformer} and \citet{zaheer2020big} use similar global tokens to enhance long-range information flow. \citet{li2021prefix,lester2021power,gao2024selfcp,liu2022ptuning} explore trainable soft prompts for one or multiple layers as an alternative to finetuning model weights. 
Our findings about the representation capacity can represent the potential efficiency limits of such methods, based on how far the model behavior can be changed using trainable tokens.


\section{Method}

% We call the representation of a token sequence ideal if all tokens of the sequence are predicted correctly with temperature 0 (greedy decoding) by LM prefixed  with this  representation including end of sequence token. 

We propose a simple approach for compressing a sequence of tokens into a small set of "memory" vectors. Then with this method we analyze how many tokens can be stored and decoded from a small set of resulting vectors. \cref{fig:compression_schema} provides an overview of our setup.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\columnwidth]{imgs/compression_schema.drawio.png}
    \caption{\textbf{Compressing text into a \texttt{[mem]} vector.} The pre-trained LLM is frozen, and we only finetune one or multiple \texttt{[mem]} vectors to decode the sequence of tokens $[t_1, t_2, \ldots, t_N]$. \texttt{[mem]} vectors are trained for each text separately.}% In most experiments, we use a single \texttt{[mem]} vector.}
    \label{fig:compression_schema}
\end{figure}

Trainable \texttt{[mem]} vectors are inspired by Memory Transformers~\cite{burtsev2021memory}, but here these vectors are designed to encode an entire text sequence. The training method is borrowed from Prompt Tuning~\cite{lester2021power}, with only a set of special input embeddings optimized while all parameters of the language model are frozen.

Formally, given a token sequence $[t_1,t_2,\dots, t_N]$, we introduce a set of trainable vectors $\texttt{[mem]} = [m_1,\dots,m_K]$ that are prepended to the text. 
These \texttt{[mem]} vectors are optimized to encode $[t_1,t_2,\dots, t_N]$.
During training, the frozen language model processes $[m_1,\dots,m_K,t_1,t_2,\dots, t_i]$ as the input context for predicting next token $t_{i+1}$. The \texttt{[mem]} vectors are optimized by minimizing the standard next-token prediction cross-entropy loss. As a result, each text sequence is associated with a unique set of \texttt{[mem]} vectors. At inference time, we start generation with the learned \texttt{[mem]} tokens and let LM to decode the text.

Let estimate an upper bound on the number of tokens that can be generated from a single input vector by a language model. The input vector has a dimension $d_{model}$, with each element represented by $b$ bits, so that the total information content is approximately $d_{model} \times b$ bits. Given a vocabulary of size $|\mathcal{V}|$, where each token carries at most $\log_2 |\mathcal{V}|$ bits of information, the maximum number of tokens $L$ that can be generated is bounded by:

\begin{equation}
\label{eq:theoretical_capacity}
     L \leq \frac{d_{model} \times b}{\log_2 |\mathcal{V}|}.
\end{equation}

%We use the described compression setup to evaluate capacity of trainable input vectors via three measures: \emph{Max Tokens Encoded}, \emph{Gain in Correctly Decoded Tokens}, and \emph{Cross-Entropy Reduction}. Additionally, we use token-level next-token prediction accuracy (\emph{Acc}) as a helper metric. \emph{Acc} is computed using teacher-forcing when text is decoded both with and without \texttt{[mem]}.

Our goal is to quantify the capacity of trainable input vectors (denoted as \texttt{[mem]}) in terms of the amount of information they can encode and later decode. From an information-theoretic standpoint, we interpret this capacity as the ability to reduce uncertainty in the generated text. To this end, we define the following metrics.

%—\emph{Max Tokens Encoded}, \emph{Gain in Correctly Decoded Tokens}, and \emph{Cross-Entropy Reduction}—along with token-level next-token prediction accuracy (\emph{Acc}) as a supplementary measure. 

% mb change metrics names to:
% Max Text Length, Capacity(tokens), Capacity(Entropy)

%  ? Decoding Capacity (in Tokens), Token‐Level Accuracy Gain,     Information Gain


\textbf{Decoding Capacity (in Tokens)}:  
    From an information-theoretic perspective, this metric represents the maximum number of tokens that can be reliably reconstructed from the compressed representation in the \texttt{[mem]} vectors. It is defined as the longest text sequence length \(L\) for which the token-level accuracy exceeds a predefined threshold:
    \begin{multline}
    \label{eq:L_max}
    L_{\text{max}} = \max \big\{L \mid \\
    \text{Acc}\big(\text{LM}(t_{[1:L]} \mid \texttt{[mem]})\big) > \text{thr} \big\},
    \end{multline}
    here, \emph{Acc} is computed via teacher-forcing when decoding text both with and without the \texttt{[mem]} vectors. This measure reflects the effective storage limit (in tokens) imposed by the fixed capacity of the memory vector.

\textbf{Token Gain}:  
    This metric estimates the additional number of tokens that can be correctly decoded due to the presence of the \texttt{[mem]} vector, relative to the baseline performance of the language model (LM) without it. Formally, if \(C^{\text{LM+[mem]}}_{\text{tokens}}\) is the count of tokens correctly predicted when using the memory vector and \(C^{\text{LM}}_{\text{tokens}}\) is the count without it, then the gain is given by
    \begin{align}
    \label{eq:capacity_tokens}
    C_{\text{tokens}} &= C^{\text{LM+[mem]}}_{\text{tokens}} - C^{\text{LM}}_{\text{tokens}} \notag \\
    &= \sum_{i=1}^{N} \mathbbm{1}\Big( t_i = \text{LM}(t_{[1:i-1]} \mid \texttt{[mem]}) \Big) \notag \\
    &\quad - \sum_{i=1}^{N} \mathbbm{1}\Big( t_i = \text{LM}(t_{[1:i-1]}) \Big).
    \end{align}
    Viewed through an information-theoretic lens, this difference quantifies the number of tokens' worth of information (i.e., discrete units) that the memory vector adds to the decoding process.

  \textbf{Information Gain}:  
    Cross-entropy measures the uncertainty or the average number of bits required to encode a sequence under a given model. The \emph{Information Gain} quantifies how much the \texttt{[mem]} vector reduces this uncertainty. Let $H_{\text{LM}} = H\big( P_\theta(t_{[1:N]}) \big)$
    be the cross-entropy (in bits) when decoding without the memory vector, and $
    H_{\text{LM+[mem]}} = H\big( P_\theta(t_{[1:N]} \mid \texttt{[mem]}) \big)$
    be the cross-entropy with the memory vector. Then, the reduction is given by
    \begin{align}
    \label{eq:capacity_entropy}
        \text{CE-reduction} &= C_{\text{H}} = H_{\text{LM}} - H_{\text{LM+[mem]}}.
    \end{align}
    This measures how many fewer bits are needed to represent the text, thus reflecting the additional information provided by the memory vector.

Collectively, these metrics enable us to characterize the capacity of the trainable input vectors both in terms of discrete tokens (\(C_{\text{tokens}}\)) and entropy (\(C_{\text{H}}\)), while \(L_{\text{max}}\) provides an upper bound on the length of text that can be accurately reconstructed. In our experiments, these measures are computed over a curated set of texts and averaged to obtain robust estimates. We note that the absolute values of \emph{Information Gain} depend on the underlying vocabulary, and therefore should not be directly compared across models with different vocabularies.

%This allows us to estimate the capacity of trainable input vectors in terms of tokens ($C_\text{tokens}$), entropy ($C_\text{H}$), and the maximum number of tokens that can be decoded \emph{Max Tokens Encoded} ($L_{\text{max}}$) for different language models. All these metrics are computed on a selected set of texts and are averaged to get more stable results. We note that the absolute values of \emph{CE Reduction} should not be directly compared between models with different vocabularies.

\section{Experiments and Results}



We evaluate capacity of trainable input vectors of the same size as dimension of input embeddings for different language models on texts from different sources.

\paragraph{Models}
We use models from Pythia suite (160M, 410M, 1.4B, 2.8B)~\cite{biderman2023pythia}, OPT-1.3B~\cite{zhang2022opt}, OLMo-1B~\cite{groeneveld-etal-2024-olmo}, Sheared-LLaMA-1.3B~\cite{xia2024sheared}, and Llama-3 models (1B, 3B, 8B)~\cite{grattafiori2024llama}.
List of all used models with links to HuggingFace Hub are in Appendix~\ref{app:models}.

\paragraph{Data}
As a source of texts for compression, we use texts from the PG-19 dataset~\cite{rae2019compressive}, which consists of books extracted from the Project Gutenberg library. Given that PG-19 is publicly available and contains books, it is highly plausible that these texts were included in the pre-training data of LLMs. Notably, PG-19 is part of the Pile dataset~\cite{pile}, which was used to train Pythia models.

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{%
\begin{tabular}{llllllll}
\toprule
       & & \textbf{Pythia-160M} & \textbf{Pythia-410M} & \textbf{Pythia-1.4B} & \textbf{Llama-3.2-1B} & \textbf{Llama-3.2-3B} & \textbf{Llama-3.1-8B} \\
\midrule
\multirow{3}{*}{\textbf{PG-19}} & Max, tokens &                      80 &                      96 &                       160 &                       512 &                      1024 &                      1568 \\
      &  \cellcolor{gray1}Gain, tokens & \cellcolor{gray1}70.9\tiny$\pm$11.0 & \cellcolor{gray1}81.3\tiny$\pm$12.0 & \cellcolor{gray1}158.0\tiny$\pm$29.1 & \cellcolor{gray1}426.2\tiny$\pm$79.2 & \cellcolor{gray1}720.3\tiny$\pm$80.2 & \cellcolor{gray1}1094.1\tiny$\pm$127.6 \\
       & \cellcolor{gray2}Information Gain & \cellcolor{gray2}396.4\tiny$\pm$46.0 & \cellcolor{gray2}431.4\tiny$\pm$51.6 & \cellcolor{gray2}792.8\tiny$\pm$143.4 & \cellcolor{gray2}2119.9\tiny$\pm$364.8 & \cellcolor{gray2}3292.2\tiny$\pm$320.0 & \cellcolor{gray2}4865.7\tiny$\pm$546.6 \\
\cline{1-8}
\multirow{3}{*}{\textbf{Fanfics}} & Max, tokens &                      80 &                      96 &                       192 &                       512 &                      1024 &                      1568 \\
       & \cellcolor{gray1}Gain, tokens & \cellcolor{gray1}70.9\tiny$\pm$10.5 & \cellcolor{gray1}81.2\tiny$\pm$11.6 & \cellcolor{gray1}152.9\tiny$\pm$28.0 & \cellcolor{gray1}449.6\tiny$\pm$83.7 & \cellcolor{gray1}734.1\tiny$\pm$85.0 & \cellcolor{gray1}1071.8\tiny$\pm$168.6 \\
       & \cellcolor{gray2}Information Gain & \cellcolor{gray2}378.1\tiny$\pm$45.9 & \cellcolor{gray2}429.8\tiny$\pm$46.2 & \cellcolor{gray2}776.9\tiny$\pm$132.5 & \cellcolor{gray2}2213.8\tiny$\pm$365.8 & \cellcolor{gray2}3354.5\tiny$\pm$344.9 & \cellcolor{gray2}4768.9\tiny$\pm$622.6 \\
\cline{1-8}
\multirow{3}{*}{\textbf{Random}} & Max, tokens &                      65 &                      72 &                       139 &                       316 &                       460 &                       792 \\
       & \cellcolor{gray1}Gain, tokens & \cellcolor{gray1}61.3\tiny$\pm$6.6 & \cellcolor{gray1}76.9\tiny$\pm$8.7 & \cellcolor{gray1}144.4\tiny$\pm$17.5 & \cellcolor{gray1}294.9\tiny$\pm$64.8 & \cellcolor{gray1}456.9\tiny$\pm$72.1 & \cellcolor{gray1}623.2\tiny$\pm$97.3 \\
       & \cellcolor{gray2}Information Gain & \cellcolor{gray2}500.8\tiny$\pm$38.9 & \cellcolor{gray2}630.4\tiny$\pm$65.2 & \cellcolor{gray2}1108.2\tiny$\pm$136.2 & \cellcolor{gray2}2265.2\tiny$\pm$498.7 & \cellcolor{gray2}3382.6\tiny$\pm$585.2 & \cellcolor{gray2}4541.2\tiny$\pm$758.6 \\
\bottomrule
\end{tabular}
}%
\caption{
  \textbf{Compression capacity across different text sources and models.}
  We report \emph{Decoding Capacity (in Tokens)} ("Max, tokens" in the Table), \emph{Token Gain}, and \emph{Information Gain} for texts from \emph{PG-19}, \emph{fanfics}, \emph{random}.
  Notably, \emph{Information Gain} remains similar across all text sources for each model (except \emph{random} for Pythia).
  For \emph{PG-19} and \emph{fanfics}, LMs leverage their ability to predict natural language, so the \emph{Decoding Capacity (in Tokens)} generally exceeds the \emph{Token Gain}. Furthermore, we find no evidence that the models benefit from potentially having PG-19 in their pre-training data, as their performance on \emph{PG-19} is not significantly better than on \emph{fanfics} published after October 2024.
  In contrast, random text offers no predictable structure, making these two metrics nearly identical. This allows us to distinguish how many tokens model can predict by itself compared to decoding from trainable input vector.
  Larger models consistently show greater compression capacity across all metrics.
}
\label{tab:compression_pg_fanfics_random}
\end{table*}

To assess the compression of texts that models have not encountered during pre-training, we collected fanfiction texts published online after October 2024 from the AO3 fanfics library\footnote{\url{https://archiveofourown.org/}}. Details of this collection process are provided in Appendix~\ref{app:data_fanfics}.



Both the \emph{PG-19} and \emph{fanfics} consist of natural language texts, where language models can predict some tokens based on prior context and model parameters. To isolate the capacity of the trainable input vectors without the influence of the knowledge of natural language by language model itself, we also employed \emph{random} texts. \emph{Random} texts were generated by randomly sampling words from the top 100,000 words from the GloVe vocabulary\footnote{\url{https://nlp.stanford.edu/data/glove.6B.zip}}.


%\paragraph{Hyperparameters}
We train only a set of $M$ vectors that are prepended to the model's input. In most of the experiments, we use only one trainable vector, if not stated otherwise. 
%Trainable vectors are initialized randomly. We use the AdamW optimizer~\cite{loshchilov2018adamw} with a learning rate of $0.01$, $\beta_1$, and $\beta_2$ both set to $0.9$, and a weight decay of $0.01$. Training proceeds for a maximum of 5,000 steps, with early stopping if the text is compressed losslessly, i.e., achieving a token-level accuracy of 1.0. All models are loaded from the HuggingFace Transformers library with the PyTorch framework.


\subsection{Decoding Capacity of a Single Token}
\label{sec:compression_in_tokens}

We find that a \emph{single} trainable vector can enable language models to produce surprisingly long, \emph{targeted} text sequences. We estimate \emph{Decoding Capacity (in Tokens)} (\cref{eq:L_max}) on 50 texts from PG-19 for each length.
We set a token-level accuracy threshold of $0.99$ and evaluate across the following length grid: [64, 80, 96, 128, 160, 192, 256, 384, 512, 768, 1024, 1280, 1568, 2048, 2560, 3072].

\cref{fig:results_brief} presents the results for the evaluated models. Notably, Llama-3.1-8B can accurately reconstruct texts of up to $1568$ tokens from just a \emph{single} input vector. Interestingly, among models with around 1B parameters (Pythia-1.4B, OPT-1.3B, OLMo-1B, Sheared-LLaMA-1.3B, and Llama-3.2-1B) we observe compressive capacity that ranges from $128$ to $512$ tokens. Pythia-2.8b, despite its larger size, has poor compression of just $128$ tokens compared to smaller 1B models. 


%\subsection{Analysis of Capacity across Different Sources of Texts}

\subsection{Memorization, Natural Language Understanding and Episodic Memory}

Generation from the \texttt{[mem]} vector involves combining information from both the pre-trained language model parameters and memory about text specific sequence. To analyze contributions of these different types of memory, we use  \emph{Token Gain} (\cref{eq:capacity_tokens}) which measure the extra number of tokens predicted correctly, and \emph{Information Gain} (\cref{eq:capacity_entropy}) showing the decrease in cross-entropy when decoding from memory vector. In contrast to \emph{Decoding Capacity}, these two metrics more directly isolate the capacity contributed by the \texttt{[mem]} vector itself.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{imgs/entropy_vs_compressed_entropy.pdf} 
  \caption{
  \textbf{Information gain of text compression to \texttt{[mem]} vector doesn't depend on language understanding capabilities of models.} Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (\emph{Information Gain}, $C_H$).
  For texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).
Larger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (\emph{PG-19}) and unnatural \emph{random} texts consisting of random word sequences.
  }
  \label{fig:entropy}
\end{figure*}

In addition to texts from \emph{PG-19} that may have been seen by LMs during pre-training, we consider: (1) texts from ~\emph{fanfics} to factor out memorization as they were published after release of the models, and (2)~\emph{random} sequenses of words to exclude learned natural language understanding capabilities.%, which ensures that the language model's own parameters have no inherent advantage in predicting these texts.

\emph{Decoding Capacity (in Tokens)} for texts from \emph{PG-19} and \emph{fanfics} was evaluated on the following length grid: [64, 80, 96, 128, 160, 192, 256, 384, 512, 768, 1024, 1280, 1568, 2048, 2560, 3072]. 



\cref{tab:compression_pg_fanfics_random} summarizes the results for each model and text source. We have two main observations. 
%First, although \emph{Decoding Capacity} typically exceeds the \emph{Token Gain}, the latter offers a more ``pure'' estimate of how many tokens are being reconstructed solely from the \texttt{[mem]} vector --- in addition to what the model already predicts on its own. Second, 
The metrics for both \emph{PG-19} and \emph{fanfics} are remarkably similar across all models tested. This similarity implies that the presence of \emph{PG-19} in the pre-training data does not provide much of an advantage. Thus, compression performance does not appear to be driven by direct memorization of the dataset. Notably, even for \emph{random} texts, larger models such as Llama-3.1-8B still exhibit substantial compressing power, reliably reconstructing sequences of up to 792 tokens. This result demonstrates the impressive capacity of learnable input embeddings to control LLM generation. In particular, a single learned vector is sufficient to guide generation of nearly 800 random tokens.

% conclusion
A key takeaway from these results is that the model's compression ability does not depend on familiarity with specific texts or knowledge of natural language gained during pre-training. Instead, the single trainable vector itself provides language agnostic substantial capacity, allowing to store completely novel texts or random sequences of words.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{imgs/results_vary_input_tokens.pdf}
  \caption{
  \textbf{Compression scales linearly with the number of trainable \texttt{[mem]} vectors.} 
  Dashed lines represent ideal linear scaling, and shaded regions indicate $\pm1$ std.
  Pythia-160m reaches its maximum input context length of 2048 tokens and can successfully encode texts of up to 2016 tokens into 32 \texttt{[mem]} input vectors. Llama-3.2-1B can perfectly decode texts of 7168 tokens from just 16 input vectors.
}
\label{fig:scale_mem}
\end{figure*}

\subsection{Sensitivity of Compression to Text Complexity}
\label{sec:compression_in_entropy}

Decoding capacity might depend on the complexity of the input text for a language model. In this section, we study how compression changes uncertainty of the model about the text.% cross-entropy estimated by model itself with teacher-forcing and examine how it changes when the same text is decoded from a learned \texttt{[mem]} vector.

% PG-19
For 50 text samples from the \emph{PG-19} at each target length (ranging from 8 up to 1568 tokens, and to 3072 for larger models) we measured cross-entropy both before ($H_{\text{LM}}$) and after ($H_{\text{LM+[mem]}}$) compression (see  \cref{eq:capacity_entropy}). %Each colored dot in \cref{fig:entropy} represents one text from \emph{PG-19}.
Figure~\ref{fig:entropy} compares results across multiple models, and full results are provided in \cref{app:entropy_all_models}. 

On \cref{fig:entropy} the models demonstrate linear relationship between cross-entropy before and after compression for not perfectly compressible texts (i.e., lying above the red dotted line), indicating constant value of information gain (or, reduction in cross-entropy). Texts with cross-entropy smaller than a model's information gain are perfectly reconstructed.

% random texts
To verify that this also holds for arbitrary texts, we used \emph{random} word sequences and observed a similar pattern: as long as cross-entropy of a sample remains below the model-specific cutoff, it can be perfectly reconstructed. Notably, these random texts (black dots in~\cref{fig:entropy}) lie very close to the same linear trend as the \emph{PG-19} texts, showing that similar compression laws apply regardless of the nature of the sequence. Thus, $\texttt{[mem]}$ works as an episodic memory storing sequence specific information independent of natural language knowledge the model has.

%Overall, these results suggest that compared to decoding capacity an information gain offers a more direct measure of how much textual information can be stored in a single trainable vector.

\subsection{Scaling Compression with More Trainable Vectors}
% intro

%The previous sections analyzed the capacity of a single trainable input vector $\texttt{[mem]} = [m_1]$. 
To explore how compression scales with the number of input vectors $\texttt{[mem]} = [m_1,\dots,m_K]$ we use the same training process as before but for different numbers of trainable vectors, from 1 to 16 for the Llama-3.2-1B model and from 1 to 32 for Pythia-160M.

% results
The results of this series of experiments are presented in~\cref{fig:scale_mem}, demonstrating that input vector capacity scales almost linearly with the number of trainable \texttt{[mem]} vectors. This trend holds consistently across all measures of capacity, whether expressed in terms of tokens or text entropy. In particular, Pythia-160M successfully decodes texts up to 2016 tokens in length using 32 \texttt{[mem]} vectors, effectively reaching its maximum context length. Similarly, LlaMA-3.2-1B achieves perfect reconstruction for sequences as long as 7168 tokens with just 16 input vectors. However, scaling behavior for LlaMA-3.2-1B deviates from the linear trend, suggesting potential inefficiencies in the compression process or inherent model limitations in exploiting an increasing number of input vectors for information storage and extraction.

% hobbit 120k tokens, 1568 tokens in 8B model - should fit into 128 (77) tokens, 512 in 1B model- should fit into 256 tokens (234)
Extrapolating from these trends, we estimate that an entire text such as "The Hobbit, or There and Back Again" (approximately 120,000 tokens) could be compressed into only 128 input vectors using Llama-3.1-8B and into 256 vectors using Llama-3.2-1B.

% short conclusion
These results demonstrate that increasing the number of trainable \texttt{[mem]} vectors significantly enhances compression capacity, with linear scaling observed across the evaluated models. Notably, using a small number of additional vectors introduces minimal computational overhead while enabling the reconstruction of substantially longer texts.
% However, deviations from ideal scaling in larger models suggest potential bottlenecks in either the compression process or the model’s ability to extract information efficiently from additional input vectors.

\subsection{Embedding Capacity Utilization}
\label{sec:capacity_utilization}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{imgs/capacity_utilization_v.pdf}
    \caption{
    \textbf{Only fraction of learned input embedding information capacity can be utilized.} \textit{Top.} Maximum token capacity (see~\cref{eq:theoretical_capacity}) against gain in correctly decoded tokens shows differences in utilization of learned memory embedding for studied models.
    \textit{Bottom.} Capacity utilization for natural and random texts.
    %We derive theoretical capacity from the size of input vector in bits and models' vocabulary size: $d_{\text{model}} \cdot b \,/\, \log_2 |\mathcal{V}|$. We define capacity utilization as the ratio of empirical capacity in tokens (\emph{Token Gain}) to this theoretical limit. 
    %Two distinct groups of models emerge: (1) Pythia models and OPT underuse their embedding space, (2) OLMo and Llama models show higher utilization. These differences may reflect variations in the pre-training quality of these model families.
    }    \label{fig:capacity_utilization}
\end{figure}

To measure how effectively each model uses its input embedding space, we compare the empirically measured capacity in tokens (\emph{Token Gain}) to a theoretical maximum derived from embedding size and vocabulary size (see~\cref{eq:theoretical_capacity}). We define \emph{capacity utilization} as the ratio of these two quantities.

In~\cref{fig:capacity_utilization}~(top), when comparing all models with roughly 1B parameters, there are two groups: older models (e.g., OPT and Pythia) show lower capacity utilization, whereas newer models (e.g., Llama, ShearedLlama, and OLMo) demonstrate higher utilization despite having the same theoretical capacity. This disparity indicates that the quality of pre-training (data, compute budget) influences the extent to which a model can exploit its input vectors capacity.

For models within the Llama family, we observe that the theoretical capacity is identical for both the 3B and 8B variants. However, in practice, the 8B model exhibits significantly higher capacity utilization. This might be fully attributed to better language understanding, gained by the larger model during pre-training, but capacity utilization for random text is higher for 8B model as well. This result suggests that the overall number of parameters plays an important role in determining effective capacity not only via LM capabilities but also due to better utilization of embedding space for episodic information storage.

In~\cref{fig:capacity_utilization}~(bottom), the Pythia models show an interesting trend: as model size increases, capacity utilization decreases. This pattern suggests that the larger Pythia models may be under-trained relative to their theoretical potential. In contrast, Llama and OLMo models show higher capacity utilization. Based on these observations, we hypothesize that capacity utilization could serve as an indicator of the pre-training status and guide further training.


\section{Discussion and Conclusions}

In this work, we introduced a simple yet effective way to compress entire text sequences into a small set of trainable \texttt{[mem]} vectors without any information loss. We used this method to analyze how far we can push the latent capacity of large language models compared to its theoretical limits.

By systematically evaluating different models, we find that a surprising amount of text can be compressed to a single token, and this capacity scales linearly with the number of tokens. 
This highlights significant potential in practical compression pipelines and long-context processing. We demonstrate that our compression outperforms neural models as a compression method, suggesting a more efficient approach to representing information. However, significantly more compute is needed due to optimization nature of the proposed method.

We establish a direct link between representation capacity and cross-entropy, showing that it remains independent of text length, domain, or familiarity. However, the exact model characteristics that determine capacity remain an open question. The hidden state dimension and model size play an important role along with general performance, however further analysis is required to determine the exact scaling laws for capacity.

Compression ability serves as a strong indicator of an LLM’s potential. Since transformers operate entirely within their representation space, its capacity fundamentally constrains reasoning, intermediate computations, and large-scale information processing. All textual and soft prompts ultimately reside in this space, meaning its limits define how effectively models can be steered and conditioned. By mapping these boundaries, we gain deeper insight into the fundamental constraints of current architectures and the possibilities for more powerful future models.

Moreover, our findings hold significant promise for memory-augmented architectures. The ability to compress long sequences into a compact set of memory vectors shows the way for integrating efficient external memory modules that can store and retrieve detailed episodic information, potentially enhancing reasoning, long-term dependency handling, and overall model performance. We believe that incorporating such optimized memory representations could lead to novel architectures that are both computationally efficient and more capable of complex information processing.

We believe our findings present an important stepping stone to understanding the limits of modern LLMs and building more powerful models in the future.

\section*{Limitations}
% - analysis of trained vectors (their representaions) is only preliminar
% - only transformer-based lms
% - we use words, not tokens -- if word is split on multiple tokens model can use its own params, this can lead to overestimation for random texts.
% models only up to 8b cause of compute limits

While our experiments push the boundaries of compression with LLMs and offer insights into their upper capacity limits, the nature of the obtained representations remains largely unclear.
We have analyzed the structure of the space of trained \texttt{[mem]} vectors in~\cref{app:compressed_vectors_analysis}, but more in-depth analysis is needed to determine the semantic properties of the vectors and their potential value in downstream tasks. 
Our findings are limited to Transformer-based models with up to 8 billion parameters due to computational constraints. Investigating the representation space of larger models, as well as exploring alternative architectures such as recurrent and memory-augmented models, remains an important avenue for future research.
In our study with different text sources, we generate random text by sampling words from a dictionary. While this approach simplifies the analysis, it may slightly overestimate model capacity compared to sampling directly from a tokenizer’s vocabulary, as dictionary words can be split on multiple tokens by model. 

\section*{Broader Impact}
We train a set of \texttt{[mem]} vectors so that arbitrary texts can be accurately reconstructed from them. This process not only allows us to analyze the capacity of these vectors, but also demonstrates that any kind of text can be compressed into compact latent representations and later decoded. Such a capability may have far-reaching implications: it could lead to more efficient methods for storing and transmitting text, while also raising important considerations regarding the potential misuse of compressed information and issues related to data security, intellectual property, and altering the behavior of aligned models.

% \section*{Acknowledgments}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\clearpage

\appendix

\section{Models and Training Details}
\label{app:models}

We provide list of all models that we used in our experiments in~\cref{tab:hf_models}.

Trainable vectors are initialized randomly. We use the AdamW optimizer~\cite{loshchilov2018adamw} with a learning rate of $0.01$, $\beta_1$, and $\beta_2$ both set to $0.9$, and a weight decay of $0.01$. Training proceeds for a maximum of 5,000 steps, with early stopping if the text is compressed losslessly, i.e., achieving a token-level accuracy of 1.0. All models are loaded from the HuggingFace Transformers library with the PyTorch framework.

Each compression experiment was run on a single A100 80GB GPU. The time required to compress text using 5,000 optimization steps ranged from a dozen of seconds for small models and short contexts to 10--20 minutes for larger models and longer contexts. We used up to 4 GPUs to run several experiments in parallel.


\begin{table*}[htbp!]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{llccc}
\toprule
\textbf{Model Name} & \textbf{Link to HuggingFace} & \textbf{Params (B)} & \textbf{Input Hidden Size} & \textbf{Vocabulary Size} \\
\midrule
Pythia-160M & \href{https://huggingface.co/EleutherAI/pythia-160m}{EleutherAI/pythia-160m} & 0.16 & 768 & 50304 \\
Pythia-410M & \href{https://huggingface.co/EleutherAI/pythia-410m}{EleutherAI/pythia-410m} & 0.41 & 1024 & 50304 \\
Pythia-1.4B & \href{https://huggingface.co/EleutherAI/pythia-1.4b}{EleutherAI/pythia-1.4b} & 1.4 & 2048 & 50304 \\
Pythia-2.8B & \href{https://huggingface.co/EleutherAI/pythia-2.8b}{EleutherAI/pythia-2.8b} & 2.8 & 2560 & 50304 \\
OPT-1.3B & \href{https://huggingface.co/facebook/opt-1.3b}{facebook/opt-1.3b} & 1.3 & 2048 & 50272 \\
OLMo-1B & \href{https://huggingface.co/allenai/OLMo-1B-0724-hf}{allenai/OLMo-1B-0724-hf} & 1.0 & 2048 & 50304 \\
Sheared-LLaMA-1.3B & \href{https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B}{princeton-nlp/Sheared-LLaMA-1.3B} & 1.3 & 2048 & 32000 \\
Llama-3.2-1B & \href{https://huggingface.co/meta-llama/Llama-3.2-1B}{meta-llama/Llama-3.2-1B} & 1.0 & 2048 & 128256 \\
Llama-3.2-3B & \href{https://huggingface.co/meta-llama/Llama-3.2-3B}{meta-llama/Llama-3.2-3B} & 3.0 & 4096 & 128256 \\
Llama-3.1-8B & \href{https://huggingface.co/meta-llama/Llama-3.1-8B}{meta-llama/Llama-3.1-8B} & 8.0 & 4096 & 128256 \\
\bottomrule
\end{tabular}
}
\caption{List of used language models and their parameters.}
\label{tab:hf_models}
\end{table*}




\section{Collecting Texts from the Fanfics Library}
\label{app:data_fanfics}

We used the AO3 fanfiction library \url{https://archiveofourown.org/} as a source of texts that were not present in the language models' pre-training data. To ensure novelty, we manually downloaded 21 fanfics from various fandoms (including Harry Potter, Star Wars, Transformers, Lord of the Rings, and others) that contained more than 20,000 words and were published after October 2024. 

We preprocessed the HTML pages to extract only the main text content, removing any irrelevant elements. We then sampled passages from these texts to evaluate the capacity of trainable input vectors. Throughout our experiments, we refer to this dataset as \emph{fanfics}.

From each of the \emph{PG-19} and \emph{fanfics}, we sampled texts and set their lengths to match the desired token counts. We ensured that each text began with complete sentences to maintain coherence. 
As a result, to estimate the capacity of the input vectors, we used 50 texts for each length.

\section{Results of Evaluating Text Compression for All Models}
\label{app:entropy_all_models}
Here we provide results for all evaluated models in~\cref{fig:entropy_all_models}. Results are discussed in~\cref{sec:compression_in_entropy}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{imgs/entropy_vs_compressed_entropy_all_models.pdf} 
  \caption{
\textbf{Information gain of text compression to \texttt{[mem]} vector doesn't depend on language understanding capabilities of models.} Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (\emph{Information Gain}, $C_H$). %Texts with CE below $C_H$ are losslessly decompressed.
  %, indicating that text entropy, rather than text length, directly defines compression. Each point represents a text, with its length indicated by the color gradient (except \emph{random}). 
  For texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).
Larger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (\emph{PG-19}) and unnatural \emph{random} texts consisting of random word sequences.
  }
  \label{fig:entropy_all_models}
\end{figure*}

\section{Understanding the Structure of Compressed Vectors}
\label{app:compressed_vectors_analysis}
% markhipov
To better understand the structure of the space formed by the learned
embeddings, we collect a dataset of embeddings for 64-token sequences
from the GovReport dataset~\cite{huang-etal-2021-efficient}. The
optimization is performed until a reconstruction accuracy of 1.0 is
achieved. Additionally, for each sequence, we compute multiple embeddings
using different random initializations of the \texttt{[mem]} vectors.

First, we observe that the optimization process can yield different
solutions; the resulting vectors for the same text may lie in completely
different parts of the space. To visualize this phenomenon, we plot
histograms of cosine similarity between embeddings of the same text (intra-sample) and between embeddings of different texts
(inter-sample) in~\cref{fig:cosine_sim}. Notably, almost no high cosine
similarities (above 0.8) are observed in the intra-sample case. Moreover,
the intra-sample similarities significantly overlap with the inter-sample
ones, implying that the embeddings are considerably scattered throughout
the space.

Although the embeddings appear to be spread out, one might hope they form
a basin in which all linear interpolations between vectors would yield
perfect reconstruction. To test this, we computed the reconstruction accuracy
along linear interpolation trajectories between embeddings of the same
sequence. However, in all cases we examined, errors were present along the
interpolation trajectory (see~\cref{fig:interp}). Thus, the embeddings
obtained by the proposed procedure do not form a continuous basin.

These observations have several implications:
\begin{enumerate}
    \item A lossless compression algorithm ideally assigns a unique decoding
    to each vector; multiple valid embeddings for the same object limit the
    achievable compression rate.
    \item The spread and entangled structure of the embeddings may render them
    less useful as representations.
    \item This non-unique, scattered structure could make it more challenging
    to extract important information when these compressed representations are
    used as context in an LM.
\end{enumerate}


% We observe a number of phenomena that potentially cause difficulties in learning
% compressed representations of token sequences by an encoder. 

% First of all, we found that the optimization can lead to different solutions. 
% With random initialization of the memory vector the optimization procedure 
% results in different ideal vectors. To check how similar they are,
% we fit a number of ideal vectors from different random initializations
% and observe cosine similarity between them. We show histograms of cosine
% similarity between ideal vectors of the same sequence of tokens and 
% different ones in Figure \ref{fig:cosine_sim}. Interestingly,
% almost no high similarities observed for intra-sample case. Furthermore,
% intra-sample similarities have significant overlap with inter-sample ones. 
% This implies that ideal vectors might be scattered over the space of all
% possible vectors. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{imgs/cosine_sims.pdf}
    \caption{\textbf{Intra/inter-sample embeddings cosine similarity.} Empirical
        probability densities of cosine similarity between intra-sample and inter-sample
        embeddings. Intra-sample similarities are measured between
         of the same sequence of tokens, while inter-sample between different
        ones. Measured on GovReport \cite{huang-etal-2021-efficient} and Sheared-Llama-1.3B \cite{xia2024sheared}.
    }
    \label{fig:cosine_sim}
\end{figure}

% Despite the ideal vectors appears to be spread they might form a basin where 
% all linear interpolations between ideal Grey are also ideal. However, all
% among all interpolations we checked there was no cases without errors trhrough
% out the interpolation trajectory \ref{fig:interp}. We hypothesise that such disjoint
% and spread structure is hard to learn.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{imgs/interps.pdf}
    \caption{\textbf{Intra-sample Interpolation Accuracies.} Interpolation lines 
    are provided for all pairs between 32 embeddings of the same input sequence. 
    All interpolation lines are printed with high transparency 
    to show denser regions. Grey lines depict minimums and maximums of the 
    accuracy for a given interpolation parameter $\theta$.
    }
    \label{fig:interp}
\end{figure}

% Another possible problem is the large number of optimization steps required to reach 
% ideal compression. 

% TOCOMPUTE: distribution of optimization steps needed versus sequence length
% THOUGHT: may be the number of optimization steps needed is proportional to the seqlen



% NOTE: define ideal compression


% The current struggle of the compression models in encoding sequence of tokens into 
% shorter sequence of vectors might be caused 
% \subsection{Solution Space Disconnectivity}
% \begin{itemize}
%     \item DONE. Solution Space Disconnectivity
%     \item Optimization Steps
%     \item Comparison with full-parameters fine-tuning
% \end{itemize}


\end{document}
