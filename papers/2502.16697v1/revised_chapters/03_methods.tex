\subsection{Construction of a Heterogeneous Graph Representation}

We establish a heterogeneous graph representation for OCTA images incorporating biological domain knowledge. This representation models biological elements found in retinal images, specifically vessel segments, intercapillary areas, and the FAZ. Moreover, we preserve neighborhood information through edge connections, which are subsequently leveraged by graph convolutional algorithms. The process of constructing the heterogeneous graph is depicted in Figure \ref{fig:construction}. Figure \ref{fig:schematic} provides a detailed illustration of the graph's edges and nodes.

\subsubsection{Vessel Segmentation}
We generate vessel segmentation maps using the method by Kreitner et al. \cite{kreitner2023detailed, menten2023synthetic}. This method leverages synthetic training data to create high-fidelity label maps of the retinal vasculature. The resulting label maps include even the smallest capillary vessels and are highly continuous, which is important to generate continuous vessel and intercapillary area graphs in the next step.

\subsubsection{Vessel Graph}
Our representation incorporates vessel graphs \cite{paetzold2023geometric, todorov2020machine} to model the vasculature in imaged eyes. Biomarkers such as vessel complexity and diameter are also encapsulated in our vessel graphs. Each vessel segment between two bifurcation points is designated as a node, and edges between vessel nodes are established using bifurcation points. The vasculature segmentation serves as input for a graph extraction algorithm \cite{drees2021scalable}, which extracts the vessel graph and provides feature descriptors detailing properties such as volume, length, curvature, and surface-distance-based values for the centerline. These features serve as embeddings for the nodes in the vessel graph. For detailed descriptions of the extracted geometric features, refer to \cite{drees2021scalable} and our github repository. Figure \ref{fig:schematic} depicts the vessel segments as nodes $v_{ves}$ and the edges between adjacent vessel segments as $e_{ves}$. We use the open-source Voreen framework, built to visualize and explore scientific data to run the graph extraction algorithm \cite{meyer2009voreen}.

\subsubsection{Intercapillary Area Graph}
Intercapillary areas are indicators for disease progression in DR \cite{schottenhamml2016automatic, terada2022intercapillary}. Essentially, they are the counterparts of segmented vessels in OCTA images. We create the nodes of the intercapillary area graph using connected component labeling of the inverted segmentation map. The nodes of the intercapillary area graph are enriched with geometric descriptors such as total area, perimeter length, and eccentricity. We rely on the skeleton of the vessel segmentation to establish the edges in the intercapillary area graph. In the skeletonized segmentation map, we find edge candidates by traversing all skeleton pixels and checking if directly adjacent pixels are part of separate background components of the skeletonized segmentation map. Using the injective map of background components in the segmentation map to background components in the skeletonized segmentation map, edges are then introduced between the intercapillary areas. In Figure \ref{fig:schematic}, the intercapillary area nodes $v_{\scriptscriptstyle ICA}$ represent the green areas (blue nodes), and neighborhood information is encoded in the $e_{\scriptscriptstyle ICA}$ edge type.



\subsubsection{Foveal avascular zone} 
Because of its significance as a biomarker \cite{sun2021optical}, we assign a separate node type and unique edge types to the FAZ. The FAZ is identified as the central intercapillary area of the connected component labeling. Our used datasets are FAZ-centered during preprocessing, note that this is a necessary prerequisite for correctly identifying the FAZ. The separate node type for the FAZ adds an inductive bias for downstream algorithms. The FAZ is represented through the node $v_{\scriptscriptstyle FAZ}$ (see Figure \ref{fig:schematic}).

\subsubsection{Heterogeneous Edges}

In addition to the diverse node types, our representation includes heterogeneous edge connections between nodes of different types. Edges link intercapillary areas and vessels ($e_{\scriptscriptstyle ves-ICA}$) if an edge forms part of the border of an intercapillary area. For the FAZ, edges connect to vessels along its border ($e_{\scriptscriptstyle F-ves}$). Edges from the FAZ to intercapillary areas are created when a single vessel acts as a bisector to an intercapillary area ($e_{\scriptscriptstyle F-ICA}$). \\

Collectively, this biology-inspired graph representation introduces bias for downstream tasks while enhancing human interpretability by abstracting from gray values to intuitive concepts like vessels. Additionally, the graph representation preserves spatial information through positional coordinates and neighborhood information through edges. Positional coordinates can be embedded via x- and y-coordinates for intercapillary areas and FAZ, as well as x- and y-coordinates for the start and endpoints of vessel segments. Our graph representation can be used as a foundation for various prominent prediction tasks on OCTA images (see Section \ref{sec:method:staging} and \ref{sec:discussion}). 

\begin{figure}[!t]
    \centering
    % left, bottom, right, top
    \includegraphics[trim={8.5cm 4.8cm 7.65cm 5.3cm},clip,width = \linewidth]
    {figures/figure3_graph_schematic.pdf}
    \caption{Schematic illustration of our heterogenous graph representation of an OCTA image. Raw pixels in the image are abstracted into either a vessel node ($v_{\scriptscriptstyle ves}$), an intercapillary area node ($v_{\scriptscriptstyle ICA}$), or the FAZ ($v_{\scriptscriptstyle FAZ}$). Neighborhood information is preserved in the homogeneous (\textcolor{ves_col}{$e_{\scriptscriptstyle ves}$}, \textcolor{ica_col}{$e_{\scriptscriptstyle ICA}$}) and heterogeneous edges (\textcolor{vesica_col}{$e_{\scriptscriptstyle ves-ICA}$}, \textcolor{fica_col}{$e_{\scriptscriptstyle F-ICA}$}, \textcolor{fves_col}{$e_{\scriptscriptstyle F-ves}$)}. Interpretable geometric descriptors and intensity statistics are encoded in the node embeddings. }
    \vspace{-0.3cm}
    \label{fig:schematic}
\end{figure}


% Johannes: 2. Solving the disease classification as a graph classification on a heterogenous graph (here, we could also briefly formalize the classification target again) 
\subsection{Graph Learning for DR Staging}
\label{sec:method:staging}
We utilize our heterogeneous graph representation to predict the DR stage of the imaged eye, which we frame as a graph classification task. We use the Pytorch Geometric framework to implement our GNN model \cite{fey2019fast}. Specifically, we use a GNN architecture, as depicted in Fig. \ref{fig:arch}, tailored to our presented heterogeneous graph structure:

\begin{enumerate}
    \item First, we preprocess the different node types separately using a block of two fully connected layers for each type. Each layer is followed by a batch norm layer for regularization and a ReLU activation function, see Fig \ref{fig:arch} (1). These preprocessing layers allow to adjust the node embeddings for the following message-passing layer. 
    \item In the next step, we refine the embeddings based on neighborhood information using one independent SAGE \cite{hamilton2017inductive} layer for each edge type, see Fig \ref{fig:arch} (2). The update using one specific type of neighborhood node, e.g. type $a$ is then defined as
    \begin{gather}
          h_{\mathcal{N}_a(v)}^k\leftarrow\mathrm{AGG.}_k(\{h_u^{k-1},\forall u\in\mathcal{N}_a(v)\}) \\
          h^k_{v_a}\leftarrow\sigma\left(W^k\cdot(h_v^{k-1},h_{\mathcal{N}_a(v)}^k)\right) .
    \end{gather}
    Notably, all our edges are bidirectional and allow message passing in both directions. This message-passing layer is used to refine the node embeddings based on the neighborhood information stored in the graph. Using heterogeneous edges, node embeddings can be refined using the information of all types of connected neighbors. The resulting embeddings from the refinement using distinct types of neighborhood nodes are then aggregated 
    \begin{equation}
          h^k_v\leftarrow\mathrm{AGG.}_k(\{h^k_{v_a}, h^k_{v_b}, ..., h^k_{v_z}).
    \end{equation}
    \item After the graph layers, another block of two fully connected layers postprocesses the individual representations of the different node types, see Fig \ref{fig:arch} (3). Again, batch norm layers and a ReLU activation function are applied following each layer. 
    \item Next, we use \textit{sum} and \textit{max} pooling to aggregate over the embeddings of all nodes of the same type. The results of both aggregations are then concatenated, followed by another concatenation of all aggregated embeddings of the different node types, see Fig \ref{fig:arch} (4). Using a sparse aggregation (\textit{max}) and a dense aggregation (\textit{sum}) allows the network to extract global information and, at the same time, pay special attention to single instances. 
    \item Finally, we use an MLP head consisting of four fully connected layers with dropout on the aggregated fixed-size feature vector for multiclass classification, see Fig \ref{fig:arch} (5). 
\end{enumerate}

\begin{figure*}[!ht]
    \centering
    % left, bottom, right, top
    \includegraphics[
    width = \textwidth]{figures/figure_rev_model_architecture.pdf}
    \caption{Illustration of the graph neural network architecture designed for the DR disease staging task. The architecture takes a heterogeneous graph as input and predicts the disease stage for the provided input graph. We generalize homogeneous message-passing architectures to the heterogeneous case by learning individual message-passing functions for each node type. After the message passing layers and linear layers, an aggregation for each individual node type is performed. Finally, the embeddings for all node types are aggregated and passed on to a classification head.}
        \vspace{-0.3cm}
    \label{fig:arch}
\end{figure*}

Our proposed network is restricted in its complexity compared to current CNNs and transformer models. Table \ref{table:model_params} shows a comparison of the number of parameters and the training time of the GNN model and other prominent network architectures. Notably, the graph construction step is a prerequisite for our approach. We observe that with non-runtime-optimized code the construction takes multiple seconds. Therefore, the benefit of faster training during hyperparameter tuning already outweighs the construction overhead. 

\input{revised_tables/num_params}

\subsection{Interpretability Framework for Retinal Graphs}
Our explainability framework consists of two components. First, an attribution method, and second, a localization method. We combine these methods to enhance the interoperability of our graph-based classifier predictions. 

\subsubsection{Attribution Method}
We utilize integrated gradients \cite{sundararajan2017axiomatic, mccloskey2019using} as attribution method to describe the impact of specific nodes and their characteristics on the algorithmic outcome, following the evaluations by Agarwal et al. \cite{agarwal2023evaluating}. The integrated gradients approach follows a straight line path in the input feature space from a baseline sample to the target sample. In computer vision, this baseline sample is typically a zero-image. Equivalent to baseline images, graph-based approaches require a baseline vector for each node in the graph. The attribution score $a_{v_i}$ of a feature $i$ of a node $v$ given a baseline $x_{bl}$ is then defined as 
\begin{align}
a_{v_i}::=(x_{v_i}-x^{bl}_i)\times\int_{\alpha=0}^{1}{\frac{\partial F(x^{bl}+\alpha\times(x-x^{bl}))}{\partial x_{v_i}}}d\alpha .   
\end{align}
Instead of using a constant baseline sample, such as a zero-vector, we propose a dynamic baseline \cite{sturmfels2020visualizing} that considers the unique characteristics of retinal vasculature, generating more fine-grained explanations. In contrast to natural images, the retinal vasculature adheres to a fixed structure that can be represented in the explanation process. Previous studies indicate variations in capillary density with increasing distance from the FAZ and across different retinal sectors \cite{lavia2020retinal}. This implies that distinct characteristics should be considered normal for intercapillary areas and vessels in different areas of the retinal vasculature. Therefore, we select the k-nearest same-type neighbor nodes from the training set and use their feature-wise median as the dynamic baseline vector $x^{bl}$ for each node in a graph. Formally, the baseline vector $x_v^{bl}$ for a node $v$ is defined as

\begin{align}
    %x_{k_i}^{bl} = \frac{1}{l} \sum_{v_{k_j}}^{\mathcal{N}_l(v_{k_i})} x_{k_j} 
    x_{v}^{bl} = \text{{median}}\left(\left\{ x_{u} \, \middle| \, u \in \mathcal{N}_l(v) \right\}\right).
\end{align}

Here $\mathcal{N}_k(v)$ is the $k$ neighborhood of the node $v$. We define the $k$ neighborhood as the $k$ nodes of \textit{the same type} from the training set with the closest spatial correspondence to node $v$ and $x_u$ is the feature vector of $v_u$ after Z-score normalization. Using the k-nearest-neighbors for creating the baseline vector accommodates local variations at various distances from the fovea. We employ search trees to efficiently identify relevant neighbors in the training data.

\subsubsection{Localization Method}
For clinical utility, it is critical that graph nodes that have been identified as important (e.g., individual vessel segments) can be visualized in the corresponding OCTA image. These visualizations can be used to verify whether the extracted retinal characteristics align with the underlying image or if they arise from representation errors, such as image artifacts. We store the corresponding segmentation map for each node in our graph representation to achieve such a localization. 

\subsubsection{Explanation Generation}
To generate our explanations, we first identify critical nodes using our attribution method by summing over the integrated gradients for the feature vector. The identified critical nodes are then spatially highlighted using the described localization method. Lastly, we identify the decisive characteristics of each critical node using the feature-wise integrated gradients. Examples of the generated spatial and feature-wise explanations are described in Section \ref{sec:results} and visualized in Figure \ref{fig:full_explanations}.

\subsection{Datasets}
\subsubsection{Proprietary Clinical Dataset}
Our OCTA dataset contains 1268 OCTA images of the DVC. For some patients, images of both the right and left eye are contained in the dataset. Beyond that, no patients are included more than once. DR grades are assigned on a patient level with the stages of \textit{healthy} (847 samples), diabetes mellitus (\textit{DM}) without DR (133 samples), early non-proliferate DR (\textit{early NPDR}, 134 samples), \textit{late NPDR} (62 samples), and \textit{PDR} (92 samples). The images are 3 mm $\times$ 3 mm, with a resolution of 304$\times$304 pixels.

We perform a stratified data split into six parts for our experiments. As sometimes both eyes of a single patient are imaged, we strictly keep both samples in the same split to avoid information leakage. One of the splits is kept as a test set, while the other five splits are used for cross-validation. In each split, \textit{healthy} and \textit{DM} are pooled into a single \textit{healthy} class, and \textit{early NPDR} and \textit{late NPDR} are pooled into a single \textit{NPDR} class. As is typical in a real-world setting, our datasets are imbalanced towards \textit{healthy} subjects. For these reasons, we evaluate and select our models based on balanced accuracy.

\input{revised_tables/results1}

\subsubsection{Public Dataset}
We use the OCTA-500 \cite{li2020octa} dataset as an external validation dataset for our method. For that purpose, we use the 200 3 mm $\times$ 3 mm images with a resolution of 304$\times$304 pixels and remove samples that contain labels other than \textit{DR} or \textit{heatlhy}. This results in a total of 189 images, of which we use the ILM-OPL projection maps. Notably, these projections contain SVC and DVC vessels, resulting in a slight domain shift compared to our proprietary dataset. Of the 189 images, 160 are healthy samples and 29 samples are in the DR class. We apply each algorithmic class's best-performing checkpoints to classify the OCTA-500 samples. Predictions of \textit{NPDR} and \textit{PDR} are pooled into a single DR class for comparison with the ground truth labels.\\