
The DR stage label is assigned based on the International Clinical Disease Severity Scale \cite{wilkinson2003proposed} relying on color fundus photography. We want to stress that this feature-based grading approach does not always align with the microvasculature changes visible in OCTA images. Recognizable changes in the microvasculature can occur e.g. before the presence of microaneurisms that result in the NPDR stage. Therefore, the achieved classification scores deviate strongly from perfect classification in terms of the International Clinical Disease Severity Scale. 

\subsection{Baseline Methods}
We compare our approach to the most prominent and well-performing approaches for DR staging in the literature. These methods include manual biomarker extraction, and DL approaches such as CNNs directly operating on the information-rich raw OCTA images. Furthermore, we compare our method to a recently published foundational model that has not previously been employed for this task on OCTA images. Finally, we benchmark against traditional ML methods on graphs that employ global feature aggregation followed by classic ML classifiers without using graph convolutions.

\input{revised_tables/results2}

\subsubsection{Manual Biomarker Extraction}
We extract traditional biomarkers from the image segmentations as a means of comparison. We use RF and SVM as classifiers for the DR stage prediction due to their excellent performance in a wide range of applications for tabular data. The biomarkers that we extract and use as classification features are namely: 
\begin{enumerate}
    \item FAZ biomarkers: area, maximal diameter, mean diameter, acircularity index, STD4, and NR300. The reader is referred to \cite{lu2018evaluation} for detailed descriptions of the last three biomarkers. 
    \item Vessel and intercapillary area biomarkers: vessel density, vessel perimeter, and fractal dimension as vessel complexity measure. 
\end{enumerate}

\subsubsection{Vision Models}
We compare against well-established CNN architectures previously used for the task at hand \cite{qian2023drac}. We implemented EfficientNet \cite{tan2019efficientnet}, ResNet \cite{xie2017aggregated}, and VGG \cite{simonyan2014very} models in combination with an optimized data augmentation pipeline. We use all models with weights pre-trained on ImageNet. Beyond these networks, we use the more recent ConvNeXt \cite{liu2022convnet}, Efficient Net V2 \cite{tan2021efficientnetv2}, and SwinTransfomer \cite{liu2021swin} architectures. We use cross-entropy loss for CNNs and all other neural network architectures. 

\subsubsection{RETFound Foundational Model}
The RETFound model \cite{zhou2023foundation} is built on a vision transformer architecture. To initialize this complex architecture's weights, the model is pre-trained in a self-supervised fashion on more than 900,000 color fundus photography images before being fine-tuned on our training set. We decided to use the color fundus pre-trained model instead of the OCT fine-tuned models due to better performance. This is likely caused by the larger domain shift from en-face OCTA to OCT than to color fundus photography.  

\subsubsection{Traditional ML on Node Embeddings}
Traditional ML on graphs methods are applied to assess the feature quality of node embeddings in our heterogeneous graph. Global aggregation generates a single feature vector for vessel and intercapillary area nodes and the FAZ node. These aggregates are then concatenated and used as input for ML classifiers, with RF and SVM chosen for their effectiveness. 

\subsection{Results}

\subsubsection{Comparison of Graph Learning with Baselines}
Our biology-informed heterogeneous graph representation, combined with a shallow GNN model, outperforms all baseline algorithms with a balanced accuracy of 69.7\% (see Table \ref{table:holdout}). 
It also obtains the highest ROC-AUC, surpassing all alternative approaches. Among vision models, the highest ROC-AUC scores were achieved by the ConvNeXt-s and VGG11 architectures. In comparison, the finetuned RETFound model only achieves a performance of 63.8\%, which can be attributed to the large domain gap between the pretraining (color fundus images) and fine-tuning data (OCTA). Although the model learns about retinal characteristics during pretraining, it cannot transfer this knowledge to OCTA images, which capture fine-grained capillary structures and exhibit different colors and contrasts. Additionally, the complex underlying vision transformer architecture has a high data demand during training, which the available fine-tuning dataset does not satisfy. This observation is further supported by the comparably low performance of the SwinTransformer model. In contrast, our graph representation enables using a lightweight architecture (see Table \ref{table:model_params}) that achieves peak performance even with limited data. Furthermore, the relative performance gap between the best CNN model and the best biomarker-based approach of $\sim$ 3\% supports earlier findings that biomarker-based approaches cannot extract all relevant information necessary to achieve optimal performance. 



\begin{figure*}[!ht]
   \centering
    \includegraphics[trim={0cm 0cm 0cm 0cm},
    clip,width = \linewidth]{figures/figure4_full_explanations.pdf}
    \caption{Explanations for the decision-making of our proposed disease staging utilizing graph representations and graph classification. Rows \textbf{a)} and \textbf{b)} represent two separate samples that were correctly classified as \textit{NPDR} and \textit{PDR} respectively. The left-most images represent the original OCTA images. The pictures to the right spatially highlight the \textcolor{ves_spatial}{vessels} (red) and \textcolor{ica_spatial}{intercapillary areas} (blue) that are most important to the classification. The \textcolor{faz_spatial}{FAZ} (green) is highlighted if it is among the most important nodes in the graph. In the neighboring barplot, the most important properties of the respective entities are displayed according to their relative importance. The right-most information about the deviation from the mean indicated if an increased or decreased value for the characteristics is causing the model to predict the stated disease stage.}
    \vspace{-0.3cm}
    \label{fig:full_explanations}
\end{figure*}

\subsubsection{Comparison of Random Forest on Node Embeddings to Graph Learning}
Notably, a simple random forest classifier that operates on all aggregated node embeddings yields excellent results and even surpasses some CNNs and all traditional biomarker-based baselines. The ROC-AUC for the RF on the node embeddings (0.828) is significantly higher compared to the RF on the biomarkers (0.796), with a p-value $<0.01$. This observation shows that the nodes' feature embeddings, consisting of geometric descriptors and intensity statistics, contain valuable information for downstream tasks. Furthermore, it supports the finding that tree-based methods outperform neural networks on tabular data tasks in various settings \cite{grinsztajn2022tree}. However, our method performs superiorly because the graph learning framework exploits the graph representation's message-passing capabilities.


\subsubsection{Comparison to SVC and Segmentation Maps}
A performance comparison for using the SVC instead of the DVC using the same CNN architecture shows similar staging performance for using either the DVC or the SVC layer. Moreover, we experimented with directly using segmentation masks as input to the CNN classifier. Interestingly, this results in comparable performance to raw images and opens an interesting avenue for enhancing explainability in classification with CNN models. 

\subsubsection{Generalization to OCTA-500}
We use the best-performing checkpoint from the 5-fold cross-validation on the proprietary dataset for evaluation on the OCTA-500 dataset.  Our proposed graph learning method performs best in this 0-shot evaluation on the OCTA-500 dataset (see Table \ref{table:octa500}).  This highlights the robustness of our approach to minor domain changes induced by different hardware (OCTA scanners) and preprocessing algorithms (e.g., layer segmentation and intensity clipping). The performance between the CNN models and the RETFound model differs strongly, probably due to differences in the high-level concepts used by the vision models. Due to their lack of interpretability, the underlying mechanisms that cause the model to fail cannot be identified. The simple random forest models perform excellently in this generalization task. VGG11 and ResNet18 show high AUC ROC values compared to their relatively low balanced accuracy. This indicates that while these models are successful in differentiating the disease level, the classification threshold does not generalize to the new dataset. 