\begin{figure*}[!ht]
    \centering
    % left, bottom, right, top
    \includegraphics[
    width = \textwidth]{figures/graph_construction_snip.png}
    \caption{Processing pipeline for generating a heterogeneous graph that models an OCTA image's most relevant biological concepts: vessels, intercapillary areas, and the FAZ. A vasculature segmentation \cite{kreitner2023detailed} is used to create a vessel graph and intercapillary area graph and identify the FAZ. Finally, these components are merged into a single interconnected heterogeneous graph representation.}
        \vspace{-0.3cm}
    \label{fig:construction}
\end{figure*}


\subsection{Machine Learning for DR Staging in OCTA} 
In recent years, the improvement of instrumentation \cite{kim2011vivo}  has made OCTA available to an increasing number of ophthalmologists. Since then, many studies have investigated DR with OCTA. Approaches to staging DR with OCTA can be broadly categorized into two groups. 
The first group comprises feature-based methods, which are based on prior knowledge of indicative biomarkers, the quantitative assessment of these markers, and decision-making based on the extracted information \cite{le2021machine, alam2019supervised, sandhu2020automated, alam2020quantitative}. Traditionally utilized biomarkers include BVD, blood vessel caliber (BVC), blood vessel tortuosity, vessel perimeter index, vessel complexity index, FAZ area, and FAZ contour irregularity. These biomarkers serve as inputs for classification models such as neural networks, support vector machines (SVM), or tree-based methods such as random forests (RF). Despite their widespread acceptance and proven significance for DR disease staging, feature-based approaches have limitations. They can only rely on a priori defined biomarkers, established based on human expertise and laborious verification procedures. In the classification setting, these biomarkers are usually evaluated as global metrics. Although localization of pathological changes is theoretically possible, it is not used for explanation generation in clinical decision support. 


The second group comprises DL models directly operating on the raw OCTA images. Previous studies adopted well-established CNN architectures such as ResNet \cite{xie2017aggregated}, VGG \cite{simonyan2014very}, and EfficientNet \cite{tan2019efficientnet} for the task and achieved high accuracy. Zhou et al. proposed a transformer-based foundation model for retinal images \cite{zhou2023foundation}, which outperformed other state-of-the-art approaches for DR staging on color fundus images. Although the network was not directly trained on OCTA images, the model has a detailed understanding of abstract retinal concepts by training on a large amount of training data.
DL-based methods have shown favorable performance compared to the aforementioned traditional approaches. This performance gap indicates that DL methods can extract information that is, to this point, not formalized as an established, known biomarker. They can be combined with explainability tools, such as CAM \cite{zhou2016learning} and grad-CAM \cite{selvaraju2017grad}, to highlight critical image regions \cite{heisler2020ensemble, ryu2021deep, zang2022diabetic}. However, the human-in-the-loop cannot overcome the semantic gap between highlighted gray values and the abstract concepts established later in the CNN network. Clinicians cannot verify and interpret the algorithmic outcomes beyond the localization of influential regions. Moreover, the biomarkers utilized by the CNN networks cannot be identified and formalized. 



\subsection{Graph Neural Networks as Explainable Models} % in Biomedical Imaging Data
With the advent of graph neural networks (GNNs), convolutions were generalized towards graph-structured data; oftentimes including the concept of message-passing \cite{velickovic2017graph, kipf2016semi, hamilton2017inductive}. 
This has led to the widespread application of GNNs across diverse fields, including chemistry, physics, medicine, and social sciences, where graph-structured data is abundant \cite{zhou2020graph}. In biomedical image analysis, successful applications of GNNs to discriminative tasks include brain disorder classification from functional magnetic resonance images \cite{zhang2022classification}, multiple sclerosis disease activity prediction from MR images \cite{prabhakar2023self}, vessel hierarchy classification in microscopic images \cite{paetzold2021whole} and pathology image classification \cite{wang2023ccf}. 

In tandem with the evolution and utilization of GNN architectures, the methods for explaining their predictions advanced. For example, gradient-based approaches, proven successful in computer vision and CNNs, effectively provided explanations for graph-based molecular property prediction \cite{pope2019explainability}. Specific perturbation-based techniques tailored for graph-structured data, such as GNNExplainer \cite{ying2019gnnexplainer}, have been developed. Notably, GraphLime \cite{huang2022graphlime}, drawing inspiration from LIME \cite{ribeiro2016should}, stands out as a surrogate-based explainability method that uses local perturbations. For a comprehensive overview of other gradient-, perturbation-, or surrogate-based explanation approaches, refer to \cite{yuan2022explainability}.
Agarwal et al. \cite{agarwal2023evaluating} analyzed various explanation methods tailored for graph classification tasks. Their study showed that integrated gradients is the most faithful and accurate explanation method, particularly on real-world datasets with known ground truth explanations.