\begin{table}[t]
\centering
\scriptsize
\setlength\tabcolsep{3pt}
\caption{Comparison of model complexities and training times for 100 epochs with a fixed batch size of 16 on a single NVIDIA RTX A6000. The heterogeneous GNN is more than 100 times smaller than the smallest evaluated CNN model. The * indicates that the model is fine-tuned without adapting all learnable weights. Pretraining required around two weeks with eight NVIDIA Tesla A100 \cite{zhou2023foundation}.}
\label{table:model_params}
\resizebox{\linewidth}{!}{
\begin{tabular}{c||c|c|r|r}

\multicolumn{1}{c||}{\textbf{Architecture}} 
& \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Data\\ Representation\end{tabular}}} 
& \multicolumn{1}{c|}{\textbf{Model}} 
& \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\ Parameters\end{tabular}}} 
& \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Train\\ Time \end{tabular}}} 
\\ \hline \hline
\rule{0pt}{3ex}
\multirow{6}{*}{\textbf{CNN}}
& \multirow{6}{*}{\textbf{Image}}
& VGG11 & 133 Mio. & 13 min\\ 

& & ResNet18 & 12 Mio.  &  9 min\\

& & EfficientNet B0 & 5 Mio. & 9 min\\ 

& & ConvNeXt s & 50 Mio. & 27 min \\

& & ResNet50 & 25 Mio. & 9 min \\

& & EfficientNet V2 s & 21 Mio. & 11 min \\ [5pt]



\hline
\rule{0pt}{3ex}
\multirow{2}{*}{\textbf{Transformer}}
& \multirow{2}{*}{\textbf{Image}}
& RetFound & 87 Mio. &  *40 min
\\ 
& & Swin t & 28 Mio & 35 min \\ [5pt]

\hline
\rule{0pt}{3ex}
\textbf{\begin{tabular}[c]{@{}c@{}}Heterogeneous GNN\end{tabular}} & \textbf{Graph} & Ours & \textbf{0.04 Mio}. &  \textbf{2 min}
\\ [5pt]

\end{tabular}
}
\vspace{-0.3cm}
\end{table}


