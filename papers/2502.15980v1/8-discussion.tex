\section{Discussion}

\subsection{Design Implications}

{\tool} demonstrates significant performance improvements compared to manual annotation and using ChatGPT. Based on participants' feedback, we attribute the improvement to the four key design ideas of {\tool}: (1) Structured workflow, (2) Data suggestion, (3) Error detection and repair, and (4) Data visualization. 

First, manual annotation and AI-assisted approaches lack structure and standards. In contrast, {\tool} structures the workflow into concrete steps and provides clear guidance (e.g., the next component blinks in the UI).
P11 wrote, ``\textit{Overall, the tool is extremely useful and facilitates the annotation, which is an admittedly open task. It adds structure to an unstructured task.}''
Second, the grammar-based suggested SQL queries and LLM-based NL questions serve as unbiased, reliable starting points for annotation. It enables annotators to avoid the most challenging and biased step. Instead of creating data from scratch, annotators only need to refine the suggested ones.
P3 wrote, ``\textit{Even though ChatGPT can give me plenty of data, I need to manually check its output. I feel connecting its output SQL to the schema is challenging given the limited time.}''
P5 wrote, ``\textit{It is useful for generating synthetic question and SQL pairs.}''
P1 wrote, ``\textit{This is almost like someone gives you a draft that you
can just revise vs. gives you an empty doc for you to start from scratch in writing.}''
Third, {\tool} highlights potential errors according to the misalignment between SQL and NL, saving effort on error checking and correction. 
P6 wrote, ``\textit{I could see how the alignment feature could be useful for complex queries.}''
{\tool} then provides suggestions to fix the misalignments, which further reduces error correction efforts.
P8 wrote, ``\textit{It is helpful to edit by simply clicking buttons.}''
These two features effectively incorporate human knowledge into the system, providing a straightforward way for humans to collaborate with the LLM.
Fourth, {\tool} reduces mental effort in understanding complex database schemas and dataset composition through visualization.
The visualization helps improve human control throughout the annotation.
P2 wrote, ``\textit{I really like the schema page, I think it helps visualize the structure of the table a lot better than seeing it in a JSON format.}''
P6 wrote, ``\textit{The data analysis helps me understand the data and where I should go on.}''

While {\tool} is designed for text-to-SQL, these four design ideas can be applied to other relevant domains.
\edit{For example, in semantic parsing for mathematical expressions, the system could suggest formulas based on structural patterns (e.g., equations with specific operators or the number of variables) and allow humans to verify mathematical relationships while visualizing expression trees. The system could detect inconsistencies between the natural language description and mathematical notation, suggesting corrections based on common mathematical writing conventions.}
\edit{In semantic parsing for other formal languages like Python, the system could suggest code snippets in a configurable manner by code properties, such as test cases to indicate semantic patterns and control flow graphs to indicate syntactic patterns.}
\edit{In image captioning, the system could generate structured captions following predefined templates like ``A [subject] is [action] near a [object]''. The system would render the scene using predefined objects. This visualization helps humans quickly verify if the caption accurately describes the key objects and their spatial relationships in the image.}

\edit{{\tool} also ensures fairness and reduces bias in data annotation by providing features such as confidence scoring and diversity checking. These features enable users to identify and correct biases introduced by AI systems, keeping the annotation process transparent and under human control.}



\subsection{Limitation and Future Work}
While {\tool} shows significant effectiveness in text-to-SQL data annotation, participants suggested several improvements for future work.
P1 and P7 pointed out the suggested NL question may be inaccurate when the SQL query is complex, which requires a more advanced SQL-to-text approach in the backend.
P7 mentioned that generating the suggested NL question can take seconds and hopes for optimization of this generation time.
We believe this can be optimized by generating subsequent SQL queries and NL questions in the backend while users process previous data. Users will not have to wait for the system to dynamically generate suggested data.
P3 and P11 suggested that adding a search function for entity names within complex schema visualizations would be helpful.
To further increase controllability, P4 suggested adding options to control the properties in generated SQL rather than making it completely random.
We think all of these suggestions are valuable for future improvements in {\tool}.

