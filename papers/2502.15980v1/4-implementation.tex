\section{Implementation}

% \todo{You need to get rid of sentences about schema customization and repurpose your writing to schema comprehension. I don't think the schema editor is a novel IUI contribution. An undegrad can build such an editor using modern UI frameworks like Material UI. You may feel that every feature you have developed in the tool needs to be elaborated in the paper. It's not the case. You need to highlight the features and interface design choices that are relevant to the scientific problem solved by this work and downplay other features even though these other features are look visually nice. Then you will get a coherent scientific story. A research paper is not to sell an engineering piece. It's about presenting a general idea/design/method to solve a scientific problem and the findings on how well this idea/design/method works in an evaluation setting.}
% {\tool} comprises multiple UI components, corresponding to a step in text-to-SQL dataset annotation and address certain user needs, including schema comprehension, database population, SQL query and NL question generation, error detection and repair, and dataset diversity analysis.

% \begin{enumerate}
%     \item \textit{\textbf{Schema Editor}} (Fig.~\ref{fig:page1}) enables users to understand and customize a database schema.
%     \item \textit{\textbf{Database Synthesizer}} (Fig.~\ref{fig:page2}) allows users to swiftly populate a database based on the provided schema.
%     \item \textit{\textbf{Query Annotator}} (Figs.~\ref{fig:page3_A} and \ref{fig:page3_B}), the core component of {\tool}, facilitates efficient text-to-SQL dataset annotation with minimized bias, errors, and effort.
%     \item \textit{\textbf{Dataset Analyzer}} (Fig.~\ref{fig:page4}) provides dynamic visualization of dataset properties, allowing users to monitor and refine the annotation process.
% \end{enumerate}

% In this section, we describe the implementation for each component in detail.

{\tool} comprises multiple UI components, each corresponding to a step in text-to-SQL dataset annotation and addressing specific user needs mentioned in Section~\ref{sec:user_needs}. 
We describe each component in this section. 
Additional implementation details, such as algorithms and prompt design, are discussed in Appendix~\ref{app:pcfg} and Appendix~\ref{app:prompt}.

% \subsection{Schema Editor}
\subsection{Schema Visualization}

% Text-to-SQL dataset synthesis is intrinsically tied to a specific database schema. The initial step in creating these datasets depends on the customization of the database schema. However, our interview indicates that customizing database schemas is a non-trivial task without an effective interface, especially when the schema gets complex.
% To bridge this gap, {\tool} includes \textit{schema editor} at the first step.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/page1.png}
  \caption{The user interface for schema visualization. \edit{Each node represents a database table, while each cell represents a column in the table. The blue cell marked with ``PK'' represents the primary key. The dashed gray edge represents a foreign key reference relationship between two tables. Users can (a) add a new table, (b) add a new column, (c) add a reference relationship, (d) define the data type for a column, (e) add a description for columns and tables, and (f) remove, upload, or download the database schema.}}
  \label{fig:page1}
\end{figure*}




Annotating text-to-SQL datasets requires annotators to understand the corresponding database schema. However, practical database schemas can be complex and challenging to comprehend. To facilitate user comprehension of schema, {\tool} enables users to visualize the schema in an interactive graph, as shown in Figure~\ref{fig:page1}.
Users can upload the schema by either dragging a schema file onto the canvas or using the upload button. Each table is visualized as a box, with columns listed as rows within it.
The primary keys are colored blue and marked with ``PK''. 
Reference relationships between columns in different tables are rendered as dashed lines, with a flow animation indicating the reference direction.
To inspect details, users can hover over columns and tables to view data types or entity descriptions. The interface allows for dragging tables, zooming in and out, and panning across the view.
 
The graph is editable, allowing users to update the schema as needed. To add new tables, users can click the ``ADD TABLE'' button in the top-left corner (Fig.~\ref{fig:page1} \circled{a}). Hovering over a table allows users to add new columns or designate primary keys (Fig.~\ref{fig:page1} \circled{b}).
Users can specify the data type for each column and directly link two columns to establish foreign key relationships (Fig.~\ref{fig:page1} \circled{c}). To remove columns, users can click the trash icon that appears on hover. 
Tables and reference links can be removed using the backspace key on the keyboard.

Given that practical database entities often use abbreviations, clear documentation can help LLMs better interpret the schema and provide more accurate annotation suggestions in subsequent steps. {\tool} encourages users to add descriptions to tables and columns (Fig.~\ref{fig:page1} \circled{e}).
For better schema management, users can quickly remove the entire schema, as well as download or upload it as a JSON file (Fig.~\ref{fig:page1} \circled{f}).

% Users can specify the data type of each column. Users can directly link two columns to specify the foreign key relationship (Fig.~\ref{fig:page1} \circled{c}).
% To remove columns, users can click the red trash icon that appears on hover. Tables and reference links can be deleted using the keyboard's delete key.
% Since practical database entities are often named using abbreviations, maintaining clear documentation can help LLMs interpret the schema and provide more accurate annotation suggestions in subsequent steps. {\tool} encourages users to add descriptions to tables and columns before annotation (Fig.~\ref{fig:page1} \circled{e}).
% Users can quickly remove the entire schema, and download or upload the schema as a JSON file (Fig.~\ref{fig:page1} \circled{f}).

% To facilitate better schema management, users can clear the current schema, and download or upload the schema as a JSON file (Fig.~\ref{fig:page1} \circled{f}).

% This page facilitates complex database schema comprehension and customization. 
% Once users finalize the schema, they can proceed to the second page to synthesize database records based on this customized schema.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.85\textwidth]{figure/page2.png}
  \caption{The user interface for database population. \edit{Users can (a) populate the database with a specified number of records, (b) switch table views, and (c) upload or download synthesized records.}}
  \label{fig:page2}
\end{figure*}

\subsection{Database Population}

% \todo{It is not clear what the rules are, how many data types your synthesizer supports, and where the values for each data type is sampled from. For example, if a column requires social security numbers in a specific format, how do you synthesize these social security numbers in the right format? In the example below, student ID often has a fixed number of digits. How do you inform the synthesizer to generate student IDs with such a constraint?}
% A SQL query often refers to concrete values in the database.
% For example, ``\texttt{\textcolor[RGB]{172,41,0}{SELECT} student.id \textcolor[RGB]{172,41,0}{FROM} student \textcolor[RGB]{172,41,0}{WHERE} student.name = "Bob"}'' refers the value ``\textit{Bob}'' in database.
% A SQL query often refers to concrete values in the database but there are often no records in the database to annotate.
% To obtain reliable SQL queries, {\tool} allows users to instantly populate the database with numerous diverse values (Fig.~\ref{fig:page2}). 
% This database will be used for retrieving values and validating the correctness of the annotated SQL queries.
% This component ensures that annotated SQL queries align accurately with both the schema and the concrete data values within the database.
SQL queries often reference specific values in the database. 
However, there are often no existing records in the database to reference during annotation. 
To address this limitation, {\tool} enables users to instantly create a sandbox database populated with diverse values (Fig.~\ref{fig:page2}). This sandbox database serves two purposes: (1) it provides a source for retrieving values, and (2) it allows for executing the annotated SQL queries to validate their correctness.


% Users can instantly populate a specified number of diverse records with a single click
To populate the database, users can specify the desired number of records and create them with a single click (Fig.~\ref{fig:page2} \circled{a}). 
{\tool} employs a rule-based method to randomly synthesize records based on data types, which currently supports eight data types, including \texttt{text}, \texttt{boolean}, \texttt{int}, \texttt{timestamp}, \texttt{float}, \texttt{double}, \texttt{decimal}, and \texttt{enum}.
We design random generation rules for each data type. 
For example, in Figure~\ref{fig:page2}, ``apt\_id'' is a text field, and {\tool} generates values by taking ``\textit{apt\_id}'' as the prefix and append a random UUID to it;
``\textit{status\_date}'' is a timestamp field, generating values like ``\textit{2022-05-01T06:04:32}'';
``\textit{is\_available}'' is a boolean field, so it only yields either ``\textit{True}'' or ``\textit{False}'' in the records. 


When generating random values, we also consider primary and foreign key relationships to be the constraints.
For example, ``\textit{View\_Unit\_Status.apt\_id}'' is a foreign key referencing another column, ``\textit{Apartments.apt\_id}'', so {\tool} reuses existing values from the referenced column.
The probability of generating repetitive records can be easily adjusted by users in a configuration file, with a default probability of $0.3$.
Users can navigate between different tables via a drop-down menu (Fig.~\ref{fig:page2} \circled{b}). 
Users can save current database records or upload existing ones in JSON format (Fig.~\ref{fig:page2} \circled{c}).

% While our populated data does not follow real world distribution, text-to-SQL models are not sensitive to theses values.
% These values are placeholders used for data annotation and used to validate the correctness of annotated SQL queries.
% Text-to-SQL models are trained to learn the SQL structures and not sensistive to these values.

While these rule-based synthetic values may not reflect real-world distributions, this does not affect the annotation process. 
These values primarily act as placeholders for annotators to ensure that the annotated SQL queries can be executed with the desired behavior. 
Furthermore, values are used for reference purposes and do not alter the query's underlying structure or meaning. Models are not directly trained on these values. 
% Thus, the synthetic values will not compromise utility of annotated text-to-SQL data.


% While our rule-based approach for value generation may differ from real-world databases, we find it suitable for annotating text-to-SQL datasets. Unlike entity-linking tasks that require meaningful database values, text-to-SQL tasks focus primarily on understanding the structure and execution of SQL queries, independent of specific content within the database. With adequate diverse records, the execution results are meaningful and can validate query correctness.

{\tool} distinguishes itself from existing methods that only incorporate dummy values in SQL queries without providing an executable environment.
All SQL queries created by {\tool} are associated with their execution results for users to better understand the query behaviors.






% \subsection{Query Annotator}

% \begin{figure*}[ht]
%   \centering
%   \includegraphics[width=\textwidth]{figure/page3_core.png}
%   \caption{Query Annotator (Part I, Page 3)}
%   \label{fig:page3_A}
% \end{figure*}

% With the prepared database containing numerous records under the provided schema, users can now create text-to-SQL datasets. 
% \textit{Query Annotator} includes the core functionality of {\tool}. We describe each feature within this page in detail.


\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/page3_core.png}
  \caption{The user interface for data generation, error detection, and repair. \edit{Users can (a) generate a suggested SQL query, (b) check the query result, (c) read the step-by-step explanation in natural language, (d) generate the corresponding suggested NL question, (e) check similar gold data, (f) hover on each step to highlight the corresponding SQL component, NL question chunk, and sub-question, (g) build alignments among SQL, question, and steps, (i) identify and remove redundant text in the question, (j) update the question by emphasizing a certain step, (h) identify a misaligned step, and (k) collect annotated data.}}
  \label{fig:page3_A}
\end{figure*}

\subsection{\textbf{SQL Query Generation}}
\label{sec:sql_sampling}
Creating unbiased SQL queries is challenging, particularly when dealing with a new and complex database schema. 
To address this challenge, {\tool} provides a suggested SQL query (Fig.~\ref{fig:page3_A} \circled{a}) that is randomly sampled using SQL grammar and values in the populated database.
Specifically, {\tool} utilizes a pre-defined probabilistic context-free grammar (PCFG) tailored for SQL queries. This PCFG can be easily modified in a configuration file, as exemplified in Appendix~\ref{app:pcfg}.
While users can configure the grammar manually, {\tool} also supports automatically learning keyword probability distributions from an imported dataset.
Users can directly edit the suggested SQL query to meet specific needs and check the execution result via the ``EXECUTE'' button (Fig.~\ref{fig:page3_A} \circled{b}). 
% We provide more algorithmic details in the Appendix.

Compared to using LLMs to generate SQL queries directly, our PCFG-based approach offers more fine-grained control over query diversity and correctness. It mitigates issues such as bias or hallucination introduced by LLMs. This is the rationale behind our decision to first generate the SQL query and then translate it into natural language (NL).
An alternative approach could be generating the NL question first and then generating the SQL query. However, this method has limitations compared to ours. 
First, generating a large amount of diverse NL questions from scratch is challenging. The lack of fixed syntax in NL diminishes the control over data diversity. 
\edit{In contrast, our approach ensures data diversity by directly controlling SQL patterns.}
Second, generating the SQL query from NL essentially implies solving the text-to-SQL task. In this scenario, we can only generate SQL queries by models, which may hallucinate and introduce generation errors in SQL queries~\cite{ning_empirical_2023, tiis_sql}.
\edit{In contrast, the SQL queries generated by our approach are guaranteed to be syntactically correct.}

\subsection{\textbf{Natural Language Question Generation}}
Based on the SQL query, {\tool} provides a suggested NL question by translating the SQL query using GPT-4 Turbo\footnote{All the LLMs mentioned in this paper refer to GPT-4 Turbo (\url{https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4}).} (Fig.~\ref{fig:page3_A} \circled{d}).
For more accurate translation, {\tool} employs retrieval-augmented generation  (RAG)~\cite{rag1, rag2}. 
It retrieves similar examples from a text-to-SQL data pool, which collects previously annotated data and 1,500 real-world text-to-SQL pairs. 
\edit{Unlike commonly used retrievers in RAG, such as dense retriever~\cite{dense_retrieve} and BM25~\cite{bm25}, we develop an AST-based retriever tailored for SQL queries.}
\edit{Specifically, {\tool} calculates similarity scores between SQL queries by measuring the tree edit distance between their abstract syntax trees, retrieving the top five examples with scores above $0.5$.}  
Using these examples, {\tool} performs few-shot learning to translate the annotation SQL query into an NL question.
\edit{Figure~\ref{fig:prompt_question} shows the prompt.}
Furthermore, users can view the top similar examples by clicking the ``SIMILAR EXAMPLES'' button (Fig.~\ref{fig:page3_A} \circled{e}). These real-world examples also help users better assess the quality of the LLM-suggested NL question.
The NL question is editable, allowing users to make any necessary adjustments.



\subsection{Error Detection \& Repair}

\subsubsection{SQL Step-by-step Explanation in NL}
To enhance user comprehension of SQL queries and detect potential errors, {\tool} explains the SQL query step by step in NL (Fig.~\ref{fig:page3_A} \circled{c}).
We reuse the rule-based explanation generation approach from STEPS~\cite{STEPS}, which parses the SQL query and translates each part of the query to an NL description based on templates.
{\tool} enhances this approach using an LLM in two ways. 
First, if a SQL query cannot be fully covered by the translation rules, {\tool} prompts the LLM to generate the step-by-step explanation via few-shot learning from rule-based explanation examples. 
Second, {\tool} prompts the LLM to paraphrase the generated explanations based on the schema and documentation for better fluency and naturalness. \edit{Figure~\ref{fig:prompt_explanation} shows the prompt.} To further improve the readability, {\tool} identifies and highlights columns, tables, and values in different colors in the explanation. 
Furthermore, for each step, {\tool} renders a corresponding sub-question on the left as users hover the mouse over this step (Fig.~\ref{fig:page3_A} \circled{f}). 
% To generate the sub-question, {\tool} synthesizes a simple SQL query that only involves this step, which the LLM then translates to the question. 
The sub-question is translated from a simple SQL query that only involves this step.

\subsubsection{Visual Correspondence among SQL query, NL question, and Step-by-step Explanation}
The step-by-step explanation serves as a bridge between the SQL query and the NL question. When users click the ``CHECK ALIGNMENT'' button (Fig.~\ref{fig:page3_A} \circled{g}), {\tool} creates a triple-linkage among these elements.
First, since the step-by-step explanation is grammar-based, there is a one-to-one mapping between SQL components and explanation steps.
Second, {\tool} employs an LLM to align the step-by-step explanation with the NL question. For each explanation step, the LLM pinpoints related substrings in the NL question, maintaining a one-to-many mapping.
When users hover over an explanation step (Fig.~\ref{fig:page3_A} \circled{f}), {\tool} highlights the corresponding SQL component and related question substrings in yellow.
This triple-linkage helps users mentally connect the SQL query and the suggested NL question, enhancing user understanding of the data and aiding in the detection of potential errors.

\subsubsection{Misalignment Detection \& Correction}

While {\tool} guarantees the syntactic correctness of SQL queries sampled by PCFG, the NL question generated by the LLM can include errors or ambiguity.
{\tool} proposes a novel interactive error detection and correction strategy by aligning the NL question with the SQL query through the step-by-step explanation.
\edit{Motivated by research~\cite{multi_agent_collaboration} showing that multi-agent collaboration enhances generation accuracy, {\tool} accomplishes this task through a two-step prompting.}
\edit{We include our prompt design in Figures~\ref{fig:prompt_alignment_analysis} and~\ref{fig:prompt_alignment_map}, with further details discussed in Appendix~\ref{app:prompt}.}
If any substring in the NL question fails to map to any step in the explanation, the substring will be highlighted in red (Fig.~\ref{fig:page3_A} \circled{i}), suggesting that this text may be irrelevant to this SQL query.
Users can focus on the red text and consider removing them.
Similarly, if a certain explanation step does not map to any partial text in the NL question, this step will be highlighted in red (Fig.~\ref{fig:page3_A} \circled{h}), indicating the content mentioned in this step may be missing in the NL question.
In this case, users can update the NL question by clicking the ``INJECT'' button on the corresponding step (Fig.~\ref{fig:page3_A} \circled{j}). Then, the LLM is prompted to update the current NL question by amplifying the content mentioned in this step.
\edit{Figure~\ref{fig:prompt_inject} shows the prompt.}


\subsubsection{Confidence Scoring}
To help users better assess the quality of annotated data, {\tool} offers a post-annotation analysis via the ``POST-ANNOTATION ANALYSIS'' button (Fig.~\ref{fig:page3_B} \circled{a}). Recent research has demonstrated that LLMs can determine semantic equivalence between SQL queries~\cite{llm_sql_equivalence} and generate accurate confidence scores through self-reflection~\cite{calibration_and_correctness, tian2023justaskcalibrationstrategies}.
Based on these findings, {\tool} prompts the LLM to provide a final report and score indicating the quality and correctness of the data.  \edit{The prompt used in this step is shown in Figure~\ref{fig:prompt_equivalence}.}
The score is averaged after multiple rounds of analysis to ensure stability. 
This score serves as a confidence level, directing users to focus more on checking data with lower scores, as these data may contain potential issues.


Based on the analysis provided by {\tool}, users can choose to accept or reject the data (Fig.~\ref{fig:page3_B} \circled{b}). Accepted data is collected in the right panel (Fig.~\ref{fig:page3_A} \circled{k}), where users can review and download the dataset at any time.


\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/page3_2.png}
  \caption{The user interface for post-synthesis analysis \& automated annotation. \edit{Users can (a) generate an analysis report and scoring for annotating the current data pair, (b) accept or reject the current data pair, and (c) start automated data annotation without human intervention.}}
  \label{fig:page3_B}
\end{figure*}


\subsection{Automated Dataset Annotation}
While {\tool} enables users to annotate text-to-SQL datasets in an interactive manner, {\tool} also supports fully automated data annotation without humans in the loop (Fig.~\ref{fig:page3_B} \circled{c}). This is useful when users need a large amount of data without a perfect dataset quality (e.g., for fine-tuning LLMs).
Users can specify how many queries to synthesize and start by one click.
All generated data will be automatically collected on the right (Fig.~\ref{fig:page3_A} \circled{k}).

\subsection{Dataset Diversity Analysis}
% \todo{The description in this subsection is too handwavy. You should draw connection to the goal of generating diverse datasets and helping users to assess the diversity of the generated data}


To ensure diversity and eliminate potential biases in the annotated dataset, {\tool} allows users to monitor dataset composition and property distributions.
Users can upload their dataset via drag-and-drop (Figure~\ref{fig:page4} \circled{a}). {\tool} then renders various property distributions in pie charts, bar charts, or line charts, in terms of SQL structure, keyword, clause number, column usage, etc. (Figure~\ref{fig:page4} \circled{b}).
For example, users can monitor the number of referenced values in a bar chart.
If users find that SQL queries with a sufficient number of referenced values are underrepresented in the current dataset, they can adjust the PCFG probabilities to generate SQL queries with more values. And they can selectively accept only those queries that contain an adequate number of values.
In addition to ensuring diversity, this UI component generally improves human control during collaboration with the LLM, enabling users to better manage annotation pace and focus.


\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/page4.png}
  \caption{The user interface for dataset diversity analysis. \edit{Users can (a) upload existing annotated text-to-SQL datasets and (b) monitor various property distributions.}}
  \label{fig:page4}
\end{figure*}

