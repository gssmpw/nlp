\section{Usage Scenario}

Bob is a data scientist at a rapidly growing technology company. 
Now, his task is to create a high-quality text-to-SQL dataset for training and evaluating the natural language (NL) interface of the company's recently updated database system.
Bob faces several challenges that make this task particularly daunting.
First, the company has just completed a major update to its database schema, introducing new tables and relationships to accommodate its expanding business needs. 
This update makes previous datasets obsolete and incompatible with the current schema. 
Consequently, it is impossible to accurately evaluate the performance of the NL interface based on the updated database.
Adding to the complexity, the schema now becomes highly complex, with numerous tables and reference relationships. Manually updating previous datasets to reflect these changes would be impractical. Bob realizes that he needs a solution that can handle this complexity efficiently and accurately.
Furthermore, Bob needs to create diverse, unbiased SQL queries and their corresponding NL questions at scale. Doing this manually would be prohibitively time-consuming and challenging, especially given the complexity of the new schema.
Recognizing these challenges, Bob decides to use the newly developed text-to-SQL data annotation tool, {\tool}, to streamline his workflow and ensure the creation of a controllable, high-quality dataset.


\textbf{Schema Comprehension.}
Bob begins by uploading a JSON file containing the company's updated database schema to {\tool}. As the schema loads onto the drag-and-drop canvas, Bob is immediately impressed by how {\tool} transforms the complex JSON structure into an intuitive visual representation. Tables appear as clearly defined boxes with columns listed inside, while relationships between tables are displayed as animated dashed lines.
The visual layout allows Bob to quickly grasp the overall structure of the database, saving him hours of time that would have been spent mentally parsing the JSON file.
Using the intuitive interface, Bob makes necessary adjustments. He double-clicks to edit table names, drags lines to establish reference relationships, and documents the meaning of an abbreviated column name. The ability to zoom in and out further helps Bob navigate the complex structure. 
As he works, Bob realizes the significant improvement in efficiency compared to editing the schema through the original schema definition file directly. What might have taken hours of painstaking work is now being accomplished in minutes, with much greater accuracy and confidence. Finally, Bob downloads the updated schema. He feels confident that this new well-documented schema will serve as a valuable foundation for future projects.

\textbf{Database Population.}
For more convenient data annotation, Bob populates the database with synthetic records with {\tool}. He specifies a need for 1,000 employee records. Upon clicking the SYNTHESIZE button, {\tool} instantly creates these records. Bob reviews the generated data and notices that the synthetic employee names look diverse. He proceeds to generate records for the other tables. Satisfied with the data generation, Bob downloads the database for future use and moves on to the next step.


\textbf{Data Annotation.}
Given the database, Bob is ready to annotate text-to-SQL data by creating SQL queries and their corresponding NL questions.
% He navigates to the \textit{Query Synthesizer} page to begin this process.
% ## Generating SQL Queries
Bob finds manually creating a SQL query from scratch challenging, so he decides to generate a random SQL query by {\tool}:

\begin{center}
\texttt{\textcolor[RGB]{172,41,0}{SELECT} Employees.name} \texttt{\textcolor[RGB]{172,41,0}{FROM} Employees} \\
\texttt{\textcolor[RGB]{172,41,0}{WHERE} Employees.department\_id = 5 \textcolor[RGB]{172,41,0}{AND} Employees.salary > 50000}
\end{center}



% ## Step-by-step explanation generation
Bob finds the SQL query reasonable. To confirm his understanding, Bob clicks the ANALYZE SQL button, and {\tool} shows a step-by-step analysis:

\begin{enumerate}
    \item \texttt{\textcolor[RGB]{172,41,0}{FROM} Employees} $\rightarrow$ \textit{Which data source should we care about?} 
          \\ \colorbox[rgb]{0.95,0.95,0.95}{In employees} 
        
    
    \item \texttt{\textcolor[RGB]{172,41,0}{WHERE} Employees.department\_id = 5} $\rightarrow$ \textit{Which department are employees from?} 
          \\ \colorbox[rgb]{0.95,0.95,0.95}{Filter employees from department 5} 
          
    
    \item \texttt{\textcolor[RGB]{172,41,0}{AND} Employees.salary > 50000} $\rightarrow$ \textit{What salary range do we care about?} 
          \\ \colorbox[rgb]{0.95,0.95,0.95}{Keep employees with salary exceeding \$50,000} 
          
    
    \item \texttt{\textcolor[RGB]{172,41,0}{SELECT} Employees.name} $\rightarrow$ \textit{What information should be returned?} 
          \\ \colorbox[rgb]{0.95,0.95,0.95}{Return the names of employees} 
        
\end{enumerate}

As Bob hovers over each step, a corresponding sub-question is rendered in the tooltip and the corresponding SQL component is highlighted.
{\tool} then generates a suggested NL question for this query:
\begin{center}
``\textit{\textbf{Who are the employees in the marketing department with a salary higher than \$50,000 and have been with the company for over 5 years?}}''
\end{center}
However, Bob notices that this question does not perfectly match the SQL query and decides to use the alignment feature to refine it.
Bob clicks the CHECK ALIGNMENT button, eager to see how well the generated question matches the SQL query.
He is immediately drawn to a phrase in the question highlighted in red: ``\textit{marketing department}'', suggesting there is no corresponding element in the SQL query.
Bob realizes this information is irrelevant and needs to be removed.
To better understand the quality of this suggested query, Bob hovers over the step-by-step explanation. To his surprise, {\tool} further visually corresponds each explanation step to sub-strings of the NL question through simultaneous highlighting.
He notices one explanation step, ``\textit{Filter employees from department 5}'', is highlighted in red.
This visual cue tells Bob that this step is not reflected in the current question.

Bob decides to address these issues one by one. 
First, he removes the irrelevant information by deleting the red-highlighted phrase ``\textit{marketing}'' and the unrelated condition ``\textit{and have been with the company for over 5 years}'' from the question.
Next, he turns his attention to the missing information about the department. He hovers over the red-highlighted explanation step, ``{\em Filter employees from department 5}'', and an INJECT button appears. Bob clicks this button and the current NL question is updated by incorporating this step.
The question now becomes:
\begin{center}
\textit{\textbf{Who are the employees in Department 5 with a salary higher than \$50,000?}}
\end{center}
Excited to see the results of his edits, Bob clicks the CHECK ALIGNMENT button again. 
This time, Bob notices that there is no red highlight in either the explanation or the NL question.
% Each step successfully maps to a SQL component and relevant sub-strings in the NL question.
As a final check, Bob hovers over the explanation steps. He watches with satisfaction as each step successfully maps to a SQL component and sub-strings in the NL question.


\begin{table*}[htbp]
\centering
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{0.33\textwidth}>{\raggedright\arraybackslash}p{0.33\textwidth}>{\raggedright\arraybackslash}X}
\toprule
\textbf{Explanation Step} & \textbf{SQL Query Component} & \textbf{Question Sub-string} \\
\midrule
(1) In employees & \texttt{\textcolor{brown}{FROM} Employees} & \textit{... the employees ...} \\
(2) Filter employees from department 5 & \texttt{\textcolor{brown}{WHERE} Employees.department\_id = 5} & \textit{... in department 5 ...} \\
(3) Keep employees with salary exceeding \$50,000 & \texttt{\textcolor{brown}{AND} Employees.salary > 50000} & \textit{... with a salary higher than \$50,000...} \\
(4) Return the names of employees & \texttt{\textcolor{brown}{SELECT} Employees.name} & \textit{Who are the employees ...} \\
\bottomrule
\end{tabularx}
\label{tab:mapping}
\end{table*}


% ## Final Review and Acceptance
According to the visual alignment, Bob is confident that the NL question matches the SQL query. He further validates it by clicking the POST-SYNTHESIS button. 
{\tool} then reports the equivalence analysis in a short paragraph, along with a high confidence score of 98. Pleased with the result, Bob accepts this text-to-SQL instance and collects it to the right panel. He appreciates how the interactive alignment feature and intuitive triple-linkage visualization help him efficiently identify and correct misalignments with high confidence in the data's quality.

As Bob progresses, he periodically uses {\tool} to analyze the dataset composition to ensure he creates a diverse and balanced dataset. He notices that queries involving the newly added tables and relationships are underrepresented, so he adjusts the query generation parameters to increase their frequency.
By the end of the day, Bob creates a substantial, high-quality text-to-SQL dataset that accurately reflects the company's updated database schema. This new dataset will be invaluable for both training and evaluating their natural language interface, something that was not possible before due to the lack of relevant evaluation data.
Bob feels a sense of accomplishment. He successfully updates and documents a complex schema that would have been extremely time-consuming and error-prone to modify manually. He creates a dataset specific to the company's current database schema, including new tables, columns, and relationships. More importantly, the dataset provides a strong evaluation benchmark for the updated schema, allowing the team to accurately evaluate the performance of their NL interface.
The interactive nature of the tool allows Bob to leverage his domain knowledge while benefiting from automated generation and analysis features. He appreciates how the tool transforms a typically tedious and challenging process into an efficient and engaging one, ultimately contributing to the improvement of the company's data interaction capabilities.