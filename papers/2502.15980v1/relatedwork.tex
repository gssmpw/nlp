\section{Related Workd}
\subsection{SQL Generation from Natural Language}

% Due to the huge needs and similarity to translation tasks, translating a natural language question to a SQL query has gained much traction in recent years. Given a database schema and the natural language, an NL2SQL model aims to generate a corresponding SQL query. The generated SQL query will be used to retrieve or manipulate data in the database, and the query result will answer the users' NL question.


Developing a natural language interface for databases has been a long-standing problem since the 1970s. In 1972,  LUNAR~\cite{lunar} was proposed to enable geologists to query the chemical analysis data of lunar rocks. 
Early effort in this domain focuses on logic-based~\cite{logic1, logic2} and rule-based~\cite{rule1, SQLizer, ATHENA, Semantic-Tractability, construct_interface} approaches. However, these approaches require significant human efforts to create the translation rules and are limited to a definite set of queries~\cite{where-are-we, Semantic-Tractability}.

The emergence of many public text-to-SQL datasets~\cite{spider, wikisql, SQLizer, bird} propel researchers to develop learning-based text-to-SQL models~\cite{smbop, STEPS, ratsql, editsql, picard, sqlova}.
Early text-to-SQL models mainly leverage the encoder-decoder architecture~\cite{lin2020bridging, zhong-etal-2020-grounded, seq2sql}, where the encoder maps NL question and database schema to latent state and decoder maps the latent state to a SQL query. 
Recently, more work~\cite{xiyan, dailsql, distillery} has focused on leveraging large language models (LLMs).
These LLMs are pre-trained on large corpora and exhibit general ability in semantic parsing. 
They can effectively solve text-to-SQL tasks in a zero-shot setting with prompts or a few demonstrations~\cite{thorpe2024dubosqldiverseretrievalaugmentedgeneration, -dialsql}.
In the meanwhile, some works fine-tune existing LLMs to achieve better performance LLMs~\cite{codes, finetune_sql_llm, sqlcoder, NSText2SQL}. However, these approaches are optimized for general text-to-SQL capabilities and may not perform well on specialized schemas. {\tool} bridges this gap by enabling instant text-to-SQL dataset acquisition for a specific schema, thereby facilitating focused fine-tuning and evaluation.


\subsection{Text-to-SQL Dataset Creation}

% \subsubsection{\textbf{Database Schema Customization}}
% Allowing users to customize the database schema is an essential step when creating a database.
% Many commercial database management systems, such as Microsoft SQL Server,
% MySQL Workbench\footnote{\url{https://www.mysql.com/products/workbench/}} and PostgreSQL pgAdmin\footnote{\url{https://www.pgadmin.org/}}, provide user interfaces for users to modify a schema.
% Despite their utility, these systems encapsulate the schema within a self-contained environment, which poses challenges in integrating them in the text-to-SQL data annotation workflows.
% Additionally, these tools often bind to specific databases and demand a steep learning curve due to their complex functionalities.

% In contrast, {\tool} introduces a general and lightweight web-based schema editor. This editor is agnostic to database variations and SQL dialects, allowing users to focus solely on the ontology of the database schema. Moreover, {\tool} enables users to conveniently document the purpose and usage of each database entity—a crucial feature for data annotation and text-to-SQL generation, as it helps LLMs better understand the schema. Users can easily upload or download a schema through a JSON file, without dependency on any database files.

\subsubsection{Human-annotated Text-to-SQL Datasets}
There has been a growing number of text-to-SQL datasets~\cite{atis, scholar, restaurants, geoquery, academic, yelp_and_IMDB, advising, wikisql, spider, spider_syn, spider_dk, dusql, kaggledbqa, bird}. 
For example, WikiSQL~\cite{wikisql} contains 80,654 hand-annotated examples of questions and SQL queries across 24,241 Wikipedia tables. Spider~\cite{spider} is a cross-domain text-to-SQL dataset annotated by 11 college students. It contains 10,181 questions and 5,693 SQL queries across 200 databases, spanning 138 different domains. Another notable dataset, BIRD~\cite{bird}, includes 12,751 queries over 95 databases spanning 37 professional domains created by crowd-sourcing.

While these datasets attempt to incorporate real-world query scenarios, they are limited by the number of schemas. In practice, different applications may use significantly different schema architectures and query traffic, leading to significant disparities compared to existing datasets. Moreover, while each dataset contains thousands of queries in total, the number of queries per database remains insufficient to cover all query types, due to the high cost of human annotation.
% \todo{condense and move to user needs}



% To reduce the high cost of human-annotated datasets, data augmentation~\cite{nlp_data_annotate, nlp_synthetic_1, nlp_synthetic_2, nlp_synthetic_3, nlp_synthetic_4} is widely adopted for instant dataset acquisition without human efforts. 

% This approach is especially popular in semantic parsing tasks~\cite{semantic_parsing_data_augmentation_1, semantic_parsing_data_augmentation_2, semantic_parsing_data_augmentation_3, semantic_parsing_data_augmentation_4} such as text-to-SQL.
% For example, SyntaxSQLNet~\cite{syntaxsqlnet} generates cross-domain augmentation data using query templates from the Spider dataset.
% CONDA~\cite{conda} augments SParC~\cite{sparc} and CoSQL~\cite{cosql} by modifying SQL dialogue states to generate different queries and questions.

% % % However, data augmentation cannot handle text-to-SQL domain adaptation.
% % \todo{"domain adaptation" is not clear.} 
% % However, data augmentation cannot effectively handle situations where the database schema changes because it makes existing questions obsolete and is error-prone in editing free-form questions using data augmentation.




% A text-to-SQL dataset essentially consists of a large number of SQL queries and corresponding NL question pairs, with each pair corresponding to a database schema.

\subsubsection{Text-to-SQL Data Augmentation \& Synthesis} Compared with manual annotation, there have been a lot of efforts in automating the dataset construction process. The first line of research focuses on data augmentation from existing datasets, which transforms or expands an existing dataset to a new dataset~\cite{syntaxsqlnet, conda, nlp_synthetic_1, nlp_synthetic_2, nlp_synthetic_3, nlp_synthetic_4}.
For example, SyntaxSQLNet~\cite{syntaxsqlnet} generates cross-domain data using query templates from the Spider dataset. CONDA~\cite{conda} augments SParC~\cite{sparc} and CoSQL~\cite{cosql} by modifying SQL dialogue states to generate different queries and questions.
However, such data augmentation is limited to the existing dataset, making it challenging to significantly increase the diversity of the data.

To address the diversity limitation of data augmentation, the second line of research focuses on  sampling various queries based on the SQL grammar and then translating them into NL questions~\cite{conda, simiar_algorithm}. 
While grammar-based sampling is feasible, translating SQL queries back to NL questions remains a challenge. Existing methods are either template-based~\cite{STEPS, sqlucid, logos, explaininnl, SQL-to-text, diy, NaLIR} or model-based~\cite{IRnet, sql_to_text2, sql_to_text3, PCFG_SQL_synthesize}, both of which have limitations. Template-based methods translate formal queries into NL questions based on pre-defined templates, which lack diversity and naturalness in generated questions. Model-based methods train or employ a pre-trained model to generate the NL question from the formal query. 
For example, \cite{PCFG_SQL_synthesize} trained a BART-based~\cite{bart} translation model that maps a SQL to an NL question. 
However, such translation models can introduce errors and make the dataset unreliable.
%Language models are observed to occasionally hallucinate~\cite{llm_hallucination} and a recent study~\cite{collapse} shows that continuous training on synthetic data can lead to poor performance.

Recently, research has demonstrated that LLMs can work as effective data annotators~\cite{llm_data_annotation, annollm, llm_annotator_1, chatgpt_outperform_crowd_workers, self_instruct, oss_instruct, evol_instruct}. 
While LLMs can be prompted to generate text-to-SQL data, they are not well-suited for producing large amounts of diverse data from scratch. 

To address these issues, {\tool} first samples a SQL query based on grammar rules and translates it into an NL question using an LLM. To handle potential translation errors, {\tool} adopts a human-in-the-loop inspection and repair method. It detects and highlights any potential misalignment between the SQL query and NL questions for user inspection. It then allows users to fix the alignment issue by injecting missing information and removing redundant information in collaboration with the LLM.



% First, LLMs tend to generate hallucinated data~\cite{llm_hallucination}. A recent study~\cite{collapse} reveals that continuous training on synthetic data can lead to poor performance. 

% Second, while techniques like diverse beam search~\cite{diverse_beam_search} exist, LLMs are trained to minimize loss. They are not inherently designed to generate diverse data, thus lacking guarantees that the obtained data is sufficiently varied. Finally, due to limited context windows and model attention, it's challenging for models to remember all previously generated data without repetition—let alone in a controllable manner.


% 1. high learning curve
%     intuitive
%     web-based
%     download/upload JSON format file
% 2. database-dependent
%     general to databases
% 3. not for data annotation
% 4. doesn't support documentation


\subsection{Interactive Data Annotation}

While pure synthetic datasets are not reliable, human annotation is time-consuming and labor-intensive.
To strike a balance, interactive data annotation methods~\cite{semi_automatic_data_annotation, ARAIDA, INCEpTION, INCEpTION2, interactive_video_object_mask_annotation, freeal, fitannotator} were introduced to reduce annotation effort while maintaining annotation quality. 
These approaches typically employ an annotation model that provides suggestions for human reviewers to approve or correct.

Compared to manual annotation, interactive annotation offers a significant advantage in that human annotators need only verify model-generated annotations rather than creating data from scratch. This can significantly accelerate the annotation process while reducing human effort.
A common strategy of interactive annotation is utilizing active learning \cite{active_learning1, active_learning2, active_learning3, fitannotator, INCEpTION}, which strategically selects the most informative data points for annotation. This approach aims to optimize the model's performance incrementally with the least amount of human-annotated data.
However, a major limitation of active learning is its tendency to reinforce data biases. Although this approach selectively samples data points believed to be most valuable for model improvement, it may inadvertently focus on atypical examples that do not represent the full spectrum of the dataset. Consequently, the model trained on such datasets may develop a skewed understanding, resulting in poor performance.

Facilitating efficient collaboration between intelligent systems and humans has long been a central theme in HCI research, initially introduced in the seminal work on man-computer symbiosis~\cite{man-computer}. Nowadays, the imperfections of AI models in high-stakes domains underscore the need for enhanced human-AI collaboration. Interactive data annotation exemplifies this type of collaboration, aiming for more accurate and trustworthy outcomes. However, to the best of our knowledge, no effective interactive text-to-SQL annotation tools existed prior to this work.




% \subsection{Human-AI collaboration}


% Promoting efficient collaboration between intelligent systems and humans has been a long-standing research topic in HCI research. This concept was first introduced in the seminal work on man-computer symbiosis~\cite{man-computer}. 
% Nowadays, the inaccuracy of AI models in high-stake domains further necessitates collaboration between humans and AI, especially in interactive text-to-SQL~\cite{diy, misp, sqlucid}.

% There is no text-to-SQL annotation interfaces

% In that work, Licklider proposed that computers could perform routine tasks to pave the way for human insights, while human users could utilize their domain knowledge to make decisions that computers are not capable of making.

% Nowadays, the inaccuracy of AI models in high-stake domains further necessitates collaboration between humans and AI. However, the lack of interpretability and communication convenience presents a significant challenge to effective human-AI collaboration~\cite{blackbox, XAI}. Even though humans have the potential to complement AI, they often struggle to understand AI’s states and effectively express their thoughts~\cite{liao2020questioning, bansal2021does, eiband2019people, kocielnik2019will, luger2016like}. Specifically, if a user does not understand where the error is and what causes the error, they may find it difficult to provide effective instructions on fixing the error~\cite{dbtalkback}. 

% To help humans understand the behavior of AI, a large body of work on Explainable AI (XAI) has been proposed in recent years~\cite{molnar2020interpretable}. For example, LIME~\cite{lime} explains the predictions of any classifier.
% Sundararajan et al.~\cite{attribution-for-DNN} propose an approach to attributing the prediction of a deep network to its input features.
% Furthermore, various types of explanations, such as heatmaps~\cite{heatmap1, heatmap2} and search traces~\cite{search-trace}, have been studied in different domains.
% In addition to text-to-SQL, research from other domains has also utilized natural language to explain the system behavior~\cite{flashPog, Rationalization, Generation-NL-Explanations, nl-explanation-visual-question}. For example, FlashProg~\cite{flashPog} translates the data extraction program generated by a synthesizer into an English-like description to help users understand the generated program. 
% Ghosh et al.~\cite{nl-explanation-visual-question} propose an approach to generate NL explanation for visual question answering.
% With an understandable explanation, humans can easily detect erroneous behaviors of AI, thereby paving the way for further instructing AI.

% To facilitate effective human-AI collaboration, {\tool} enables humans to understand and instruct AI coherently.