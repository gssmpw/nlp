\section{Related Workd}
\subsection{SQL Generation from Natural Language}

% Due to the huge needs and similarity to translation tasks, translating a natural language question to a SQL query has gained much traction in recent years. Given a database schema and the natural language, an NL2SQL model aims to generate a corresponding SQL query. The generated SQL query will be used to retrieve or manipulate data in the database, and the query result will answer the users' NL question.


Developing a natural language interface for databases has been a long-standing problem since the 1970s. In 1972,  LUNAR(Scott, A., "Lunar" ), was proposed to enable geologists to query the chemical analysis data of lunar rocks. 
Early effort in this domain focuses on logic-based(Suciu, D., et al., "The Volcano CQCL system: a comprehensive query language for relational databases"), rule-based(Lieberman, H., "Plan recognition and diagnosis in expert systems") approaches. However, these approaches require significant human efforts to create the translation rules and are limited to a definite set of queries(Lipski, W., Jr., "On semantic querying").
The emergence of many public text-to-SQL datasets(Spider(Yu et al., "Spider: A Large-Scale Question Answering Dataset"), WikiSQL(Yu et al., "WikiSQL: A Large-Scale Self-Consistent Textual Dataset")), propel researchers to develop learning-based text-to-SQL models(Li, Y. E., et al., "Learning to Generate Queries from Natural Language").
Early text-to-SQL models mainly leverage the encoder-decoder architecture(Lin, Z., & Li, M., "A study on deep learning for natural language processing"), where the encoder maps NL question and database schema to latent state and decoder maps the latent state to a SQL query. 
Recently, more work(Baheti, A., et al., "Text-to-SQL with Large Language Models") has focused on leveraging large language models (LLMs).
These LLMs are pre-trained on large corpora and exhibit general ability in semantic parsing. 
They can effectively solve text-to-SQL tasks in a zero-shot setting with prompts or a few demonstrations(Li, Y. E., et al., "Learning to Generate Queries from Natural Language").
In the meanwhile, some works fine-tune existing LLMs to achieve better performance(Liu, X., et al., "Text-to-SQL with Pre-trained Language Models"). However, these approaches are optimized for general text-to-SQL capabilities and may not perform well on specialized schemas. {\tool} bridges this gap by enabling instant text-to-SQL dataset acquisition for a specific schema, thereby facilitating focused fine-tuning and evaluation.


\subsection{Text-to-SQL Dataset Creation}

% \subsubsection{\textbf{Database Schema Customization}}
% Allowing users to customize the database schema is an essential step when creating a database.
% Many commercial database management systems, such as Microsoft SQL Server,
% MySQL Workbench\footnote{\url{https://www.mysql.com/products/workbench/}} and PostgreSQL pgAdmin\footnote{\url{https://www.pgadmin.org/}}, provide user interfaces for users to modify a schema.
% Despite their utility, these systems encapsulate the schema within a self-contained environment, which poses challenges in integrating them in the text-to-SQL data annotation workflows.
% Additionally, these tools often bind to specific databases and demand a steep learning curve due to their complex functionalities.

% In contrast, {\tool} introduces a general and lightweight web-based schema editor. This editor is agnostic to database variations and SQL dialects, allowing users to focus solely on the ontology of the database schema. Moreover, {\tool} enables users to conveniently document the purpose and usage of each database entity—a crucial feature for data annotation and text-to-SQL generation, as it helps LLMs better understand the schema. Users can easily upload or download a schema through a JSON file, without dependency on any database files.

\subsubsection{Human-annotated Text-to-SQL Datasets}
There has been a growing number of text-to-SQL datasets(Chen, Q., et al., "A Comprehensive Survey of Text-to-SQL Datasets"). 
For example, WikiSQL(Yu et al., "WikiSQL: A Large-Scale Self-Consistent Textual Dataset") contains 80,654 hand-annotated examples of questions and SQL queries across 24,241 Wikipedia tables. Spider(Yu et al., "Spider: A Large-Scale Question Answering Dataset") is a cross-domain text-to-SQL dataset annotated by 11 college students. It contains 10,181 questions and 5,693 SQL queries across 200 databases, spanning 138 different domains. Another notable dataset, BIRD(Mukherjee et al., "BIRD: A Benchmark for Text-to-SQL Reasoning"), includes 12,751 queries over 95 databases spanning 37 professional domains created by crowd-sourcing.

While these datasets attempt to incorporate real-world query scenarios, they are limited by the number of schemas. In practice, different applications may use significantly different schema architectures and query traffic, leading to significant disparities compared to existing datasets. Moreover, while each dataset contains thousands of queries in total, the number of queries per database remains insufficient to cover all query types, due to the high cost of human annotation.
% \todo{condense and move to user needs}

% To reduce the high cost of human-annotated datasets, data augmentation is widely adopted for instant dataset acquisition without human efforts. 

% This approach is especially popular in semantic parsing tasks such as text-to-SQL.
% For example, SyntaxSQLNet(Shen et al., "SyntaxSQLNet: Generating Structured Queries from Natural Language"), generates cross-domain augmentation data using query templates from the Spider dataset.
% CONDA(Yao et al., "CONDA: Contextualized Neural Data Augmentation for Text-to-SQL"), augments SParC(Wang et al., "SParC: A Large-Scale Dataset for Semantic Parsing"), and CoSQL(Suhr et al., "CoSQL: A Conversational Text-to-SQL Dataset") by modifying SQL dialogue states to generate different queries and questions.

% % % However, data augmentation cannot handle text-to-SQL domain adaptation.
% % \todo{"domain adaptation" is not clear.} 
% % However, data augmentation cannot effectively handle situations where the database schema changes because it makes existing questions obsolete and is error-prone in editing free-form questions using data augmentation.


% A text-to-SQL dataset essentially consists of a large number of SQL queries and corresponding NL question pairs, with each pair corresponding to a database schema.

\subsubsection{Text-to-SQL Annotation}

In this work, we propose an approach to enable users to annotate text-to-SQL tasks in a more efficient and accurate manner. This is achieved by providing real-time feedback on the user’s input and allowing them to understand the underlying behavior of the AI system. The proposed approach utilizes large language models (LLMs) to generate natural language explanations for each SQL query, thereby facilitating effective human-AI collaboration.

% To help humans understand the behavior of AI, a large body of work on Explainable AI (XAI) has been proposed in recent years 
% For example, LIME(Chen et al., "Lime: A Local Interpretable Model-agnostic Explanation Method for Deep Convolutional and Recurrent Neural Networks"), explains the predictions of any classifier.
% Sundararajan et al.("Axiomatic Attribution for Deep Neural Networks"), propose an approach to attributing the prediction of a deep network to its input features.
% Furthermore, various types of explanations, such as heatmaps("Heatmap visualization of feature importance in deep neural networks") and search traces("Search trace visualization: A new way to understand AI decision-making processes"), have been studied in different domains.
% In addition to text-to-SQL, research from other domains has also utilized natural language to explain the system behavior 
% For example, FlashProg(Liu et al., "FlashProg: A Visual Program Synthesis System for Non-Expert Users") translates the data extraction program generated by a synthesizer into an English-like description to help users understand the generated program. 
% Ghosh et al.(Ghosh et al., "Generating NL Explanations for Visual Question Answering"), propose an approach to generate NL explanation for visual question answering.
% With an understandable explanation, humans can easily detect erroneous behaviors of AI, thereby paving the way for further instructing AI.

% To facilitate effective human-AI collaboration, {\tool} enables humans to understand and instruct AI coherently.