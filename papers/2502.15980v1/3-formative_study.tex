\section{Formative Study}

To understand the specific requirements for text-to-SQL dataset annotation, we conducted a formative study by interviewing 5 engineers from Adobe. These interviewees have experienced annotating text-to-SQL datasets in their work.
We describe our interview process in Section~\ref{sec:interview}. Based on these interviews, we identified five major user needs in Section~\ref{sec:user_needs}. 
Finally, we discuss our design rationale in Section~\ref{sec:design}, aiming to address the user needs.

\subsection{Interview}
\label{sec:interview}

We conducted 20-minute semi-structured interviews with each interviewee through a conversational and think-aloud process. 
During these interviews, we first asked about the \textbf{motivation} for text-to-SQL annotation in their use cases, specifically about the schemas they worked on and why obtaining more data was important.
Interviewees reported that when deploying a new service, companies often needed to introduce new entities and restructure the original schema.
However, after updating the schema, they typically found that model performance dropped dramatically. Their regression tests showed an overall accuracy drop of 13.3\% for newly added columns and 9.1\% for new tables. As the schema was further updated, performance continued to decline. Moreover, as the schema changed significantly, they needed a large amount of new data on the updated schema to ensure a robust evaluation.

Second, we asked about their detailed \textbf{workflow} and whether they used any tools to assist with data annotation. Interviewees reported that they did not use any specific tool for annotation, although they sometimes asked ChatGPT to generate initial data. 
Additionally, they often leveraged previous datasets by adapting previous queries to the new schema, such as replacing an outdated column name with a new one.
After annotation, their colleagues performed peer reviews to check and refine the data.





Third, we asked about \textbf{challenges} they had met and the speed of their dataset annotation. Overall, they considered annotation to be very expensive. 
Interviewees mentioned that one engineer could only annotate 50 effective SQL and NL pairs per day in their use case. 
They often lost track and felt overwhelmed during annotation. 
Despite the peer review, they still felt a lack of confidence in the quality of the annotated data. 
They pointed out that randomness existed throughout the entire procedure. 
We summarize more challenges as user needs in Section~\ref{sec:user_needs}.






\subsection{User Needs}
\label{sec:user_needs}

\noindent \textbf{\textit{N1: Effective Schema Comprehension.}} 
Text-to-SQL annotation assumes that users can easily understand the database schema specified in a certain format (e.g., Data Definition Language). However, our interviews indicate that it is cumbersome and error-prone for users to navigate and comprehend complex schemas from such a specification format.



\noindent \textbf{\textit{N2: Creating New Queries.}}
Creating SQL queries requires a deep understanding of both database schema and SQL grammar. When creating a text-to-SQL dataset, users need to continually come up with new, diverse SQL queries. However, it is challenging for them to break free from preconceptions shaped by existing queries they have seen before.

\noindent \textbf{\textit{N3: Detecting Errors in the Annotated Data.}} 
An annotated dataset may include errors, which can deteriorate model performance and evaluation results.
Our interviews suggest that annotators need an effective mechanism for detecting potential errors or ambiguity in the constructed queries.


\noindent \textbf{\textit{N4: Efficiently Correcting the Detected Errors.}} After identifying errors, users need an efficient way to correct these errors to ensure the accuracy and reliability of the dataset. They need to ensure the SQL query is syntactically correct, and the NL is semantically equivalent to the SQL query.
%However, correcting errors in the annotated data is often as labor-intensive as annotating new data.


\noindent \textbf{\textit{N5: Improve Dataset Diversity.}}
Dataset diversity is crucial for improving model performance and ensuring rigorous evaluation.
Human annotation can easily introduce biases due to individual knowledge gaps and a lack of holistic understanding of the dataset composition. 
Thus, interviewees reported the need for an effective way to improve diversity and eliminate biases in the dataset. 




\subsection{Design Rationale}
\label{sec:design}


To support \textbf{N1}, our approach visualizes the database schema as a dynamic, editable graph. This enables users to quickly grasp the overall structure of the database and the relationships between entities. Users can explore detailed information such as data type through further interactions with the graph.


To support \textbf{N2}, our approach alleviates the burden of manually creating new SQL queries. We design an algorithm to randomly sample SQL query templates based on SQL grammar, then fill out this template with entities and values retrieved from the database. We make the SQL generation highly configurable---users can manually adjust keyword probability, or automatically tune the probability by an existing dataset.

To support \textbf{N3}, our approach renders the alignment between the SQL query and the NL question via a step-by-step analysis. Our approach then prompts the LLM to highlight potential misalignments to users.
Subsequently, our approach performs a textual analysis to check the equivalence of the SQL query and NL question and offers users a confidence score about their consistency.

To support \textbf{N4}, we handle two common errors---missing information and including irrelevant information in the NL question. Our approach allows users to fix errors by injecting missing information or removing irrelevant details based on LLM-generated suggestions.

To support \textbf{N5}, our approach first enables users to sample SQL queries based on a probability distribution learned from real-world data rather than creating them manually. 
Furthermore, our approach supports visualizing various dataset compositions through diagrams. 
For example, users can view a bar chart displaying the distribution of column counts in SQL queries. This feature allows users to monitor dataset composition during annotation, maintaining control over the annotation direction and improving data diversity.

