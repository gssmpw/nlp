\section{User Study}
To investigate the usability and effectiveness of {\tool}, we conducted a within-subjects user study with 12 participants. The study compared {\tool} with manual annotation and the use of a conversational AI assistant. 



\subsection{Participants}
We recruited 12 participants (4 female, 8 male) from Adobe. They worked in different roles including Machine Learning Engineers, Research Scientists, Data Scientists, and Product Managers.
Their works were directly or indirectly related to querying data in the database.
All of them had either Master's or PhD degrees.
Participants self-rated their proficiency in SQL (\textit{3 Beginner}, \textit{3 Basic}, 4 \textit{Intermediate}, \textit{2 Advanced}) and usage frequency of LLMs (6 \textit{Yearly}, 2 \textit{Monthly}, 2 \textit{Weekly}, 3 \textit{Daily}).

\subsection{Tasks}

% \noindent \textbf{Task 1: Text-to-SQL Creation.} 
We randomly sampled 9 schemas on the widely used text-to-SQL benchmark, Spider~\cite{spider}. 
% This pool includes 3 simple schemas, 3 medium schemas, and 3 complex schemas, categorized based on the number of entities and references in each schema.
We provided these schemas in JSON format, whose syntax was comprehensible to all participants.
Based on the schema, participants were asked to annotate text-to-SQL data while optimizing both the data quantity and quality.


% \noindent \textbf{Task 2: Schema Customization.} To assess schema customization performance, we created a pool of schema editing tasks. For each sampled schema, we manually created 30 tasks requiring edits over the existing schema, e.g., "\textit{Add a new column 'Founded' (date) to the 'airlines' table.}"
% Participants were expected to complete these tasks sequentially, as some tasks depended on the completion of previous ones. 
% We maintained a consistent distribution of task types (e.g., the number of "add column" tasks) across different schemas. 
% \todo{Does this task only require editing the schema or does it also require the regeneration of the text-to-SQL data? If the former, I don't find it relevant to the goal of this work.}


\subsection{Comparison Baselines}
To the best of our knowledge, no text-to-SQL data annotation tools were readily available for comparison at the time of the user study. Thus, we compared {\tool} with two commonly applied scenarios for text-to-SQL dataset annotation in the industry, manual annotating and using an AI assistant. 

\noindent \textbf{Manual.} We asked participants to manually review and customize the schema, create SQL queries, and write corresponding NL questions. They recorded the results in an Excel sheet. 

%\vspace{1.5mm}
\noindent \textbf{AI Assistant.} We gave participants access to using a state-of-the-art conversational AI assistant, ChatGPT (GPT-4). 
For example, participants could directly upload the entire schema file in their conversation with ChatGPT and request the generation of sufficient text-to-SQL data.
We did not impose any restrictions on how participants should use ChatGPT.

\subsection{Protocol}
Each study began with a demographic survey and study introduction. Participants then watched a 4-minute tutorial video of {\tool} and spent 3 minutes practicing to get familiar with it. Meanwhile, we collected quality feedback from users.

After participants were familiar with {\tool}, they proceeded to annotate in the assigned condition (i.e., Manual, AI assistant, {\tool}). Each task consisted of three 5-minute sessions, one for each condition. 
We randomized the order of assigned conditions to mitigate learning effects.
For each session, participants were provided with a database schema and asked to annotate as many text-to-SQL datasets as possible. We asked participants to focus on not only the quantity but also the quality of annotated data.

After each session, participants completed a post-task survey to rate their experience with the assigned condition. The survey included the System Usability Score (SUS)~\cite{sus} and NASA Task Load Index (TLX)~\cite{NASA-TLX} questionnaires, using 7-point Likert scales to assess their perceptions. At the end of the study, participants filled out a final survey sharing their experiences, opinions, and thoughts. The entire study took approximately 70 minutes.

