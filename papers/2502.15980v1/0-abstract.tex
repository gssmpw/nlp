\begin{abstract}
    Text-to-SQL models, which parse natural language (NL) questions to executable SQL queries, are increasingly adopted in real-world applications. 
    However, deploying such models in the real world often requires adapting them to the highly specialized database schemas used in specific applications. 
    We find that existing text-to-SQL models experience significant performance drops when applied to new schemas, primarily due to the lack of domain-specific data for fine-tuning. This data scarcity also limits the ability to effectively evaluate model performance in new domains.
    Continuously obtaining high-quality text-to-SQL data for evolving schemas is prohibitively expensive in real-world scenarios.   
    To bridge this gap, we propose {\tool}, a human-in-the-loop text-to-SQL data annotation system.
    {\tool} streamlines the creation of high-quality text-to-SQL datasets through human-LLM collaboration in a structured workflow.
    A within-subjects user study comparing {\tool} with manual annotation and ChatGPT shows that {\tool} significantly accelerates text-to-SQL data annotation, reduces cognitive load, and produces datasets that are more accurate, natural, and diverse. \edit{Our code is available at \url{https://github.com/adobe/nl_sql_analyzer}.}
\end{abstract}