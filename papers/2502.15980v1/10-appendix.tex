\appendix


\section{Additional User Study: Database Customization}
\label{app:additional_study}

The text-to-SQL data annotation process can be divided into two stages: (1) database schema customization and (2) text-to-SQL data annotation based on a provided schema.
As the core contribution of {\tool}, we primarily focused on evaluating the text-to-SQL data annotation stage in the main study. 
In this section, we reported the evaluation of database schema customization as an additional study.
The participants and conditions were the same as the main study.

For the results, we began by analyzing participants' schema customization performance across different conditions, focusing on both accuracy and speed. 
Then, we reported user perceptions of the various conditions, including their confidence and cognitive load.

\subsection{Tasks}
To assess schema customization performance, we created a pool of schema editing tasks. For each sampled schema, we manually created 30 tasks requiring edits over the existing schema, e.g., ``\textit{Add a new column 'Founded' (date) to the 'airlines' table.}''
Participants were expected to complete these tasks sequentially, as some tasks depended on the completion of previous ones. 
We maintained a consistent distribution of task types (e.g., the number of ``add column'' tasks) across different schemas.

\subsection{Protocol}
There were three sessions corresponding to three conditions.
For each session, participants were given a database schema and 30 tasks describing how to edit the schema. The three sessions corresponded to three database schemas. Given the excessive number of tasks, participants were asked to complete as many as possible within the 5-minute time limit.

After each session, participants completed a post-task survey to rate their experience with the assigned condition. The survey included the System Usability Score (SUS)~\cite{sus} and NASA Task Load Index (TLX)~\cite{NASA-TLX} questionnaires, using 7-point Likert scales to assess their perceptions.




\subsection{Schema Customization Performance}
We manually review the correctness of participants' completed tasks. For each task, we mark it as correct if the participant met all requirements; otherwise, we mark it as wrong. Table~\ref{tab:schema_edit_performance} reports the number of completed tasks and their accuracy for each condition.

The results show that using {\tool} helps users achieve the highest customization speed (10.73) and accuracy (96.86\%) within 5 minutes. When manually customizing the schema in JSON file, users achieve the lowest customization speed (7.18) and accuracy (81.27\%). When using ChatGPT, users achieve intermediate speed (9.73) and accuracy (67.48\%).

When using ChatGPT, we observe that some participants directly copy and paste the entire schema and task descriptions into the prompt. This approach works when the schema is simple and includes only a few tables and columns. However, as the schema becomes more complex, ChatGPT may make mistakes due to hallucination~\cite{llm_hallucination}.

When using {\tool}, while there is no LLMs in the backend, participants complete schema customization tasks very quickly by directly editing the graph. Users can make precise edits and immediately detect errors through the visualization. In fact, except for one participant who misunderstand entity names, all other participants achieve 100\% task completion accuracy.

\begin{table}[htb]
    \centering
    \caption{Schema Customization Speed and Accuracy.}
    \vspace{-2.5mm}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccc}
        \toprule
              & \textbf{Completed Tasks} & \textbf{Correct Tasks} & \textbf{Accuracy} \\
        \midrule
        Manual  & 7.18  & 5.82 & 81.27\% \\
        AI-assistant   & 9.73   & 6.64 & 67.48\% \\
        {\tool} & \textbf{10.73}   & \textbf{10.45} & \textbf{96.86\%} \\
        \bottomrule
    \end{tabular}
    }
    
    \label{tab:schema_edit_performance}
\end{table}



\subsection{User Cognitive Load \& Usability Rating}
% Figure~\ref{fig:cognitive2} demonstrates participantsâ€™ ratings on the five cognitive load factors from the NASA TLX questionnaire. The ANOVA test demonstrates that the mean differences are all statistically significant\footnote{$\alpha = 0.05$} ($p$-value=2.95e-3, 3.31e-4, 4.76e-2, 1.81e-2, 3.49e-2 respectively).

Figure~\ref{fig:cognitive2} illustrates participants' ratings on the five cognitive load factors from the NASA TLX questionnaire. The ANOVA test reveals statistically significant ($\alpha = 0.05$) differences in means for all factors: Mental Demand ($p = 2.95 \times 10^{-3}$), Temporal Demand ($p = 3.31 \times 10^{-4}$), Performance ($p = 4.76 \times 10^{-2}$), Effort ($p = 1.81 \times 10^{-2}$), and Frustration ($p = 3.49 \times 10^{-2}$).

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/task2_cognitive.png}
  \caption{NASA Task Load Index Ratings of Schema Customization}
  \label{fig:cognitive2}
\end{figure*}


Participants appreciate visualizing the schema as an editable graph, which facilitates both comprehension and modification.
P2 report, ``\textit{I really like the schema editing page, I think it helps visualize the structure of the table a lot better than seeing it in a JSON format.}''
P11 write, ``\textit{Schema editing was easier and the ability to visualize the edits was very useful.}''
P9 comment, ``\textit{I definitely found the tool much more handy to make quick edits and understand the dependencies between different tables.}''

% P6 commented, ``\textit{The visualization of the relationship between tables was definitely useful}''

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figure/sus2.png}
  \caption{SUS Scores of Schema Customization}
  \label{fig:sus2}
\end{figure*}

Figure~\ref{fig:sus2} presents SUS scores for Task 2, revealing highly positive user perceptions across all dimensions. 
Participants agree or strongly agree that {\tool} help them understand schemas. 
They also find updating schemas easier and faster, with high confidence in the results.




\begin{table*}[!h]
\centering
\caption{PCFG Grammar for SQL Query Generation}
\label{tab:sql-pcfg}
\begin{tabular}{l@{\ }c@{\ }l}
\toprule
Non-terminal && Production Rule (Probability) \\
\midrule
$\langle$Query$\rangle$ & $\rightarrow$ & $\langle$SelectClause$\rangle$ $\langle$FromClause$\rangle$ [$\langle$WhereClause$\rangle$] [$\langle$GroupByClause$\rangle$] [$\langle$OrderByClause$\rangle$]  (1.0) \\[1ex]
$\langle$SelectClause$\rangle$ & $\rightarrow$ & \texttt{SELECT} $\langle$ColumnList$\rangle$  (0.9) \\
& $|$ & \texttt{SELECT DISTINCT} $\langle$ColumnList$\rangle$  (0.1) \\[1ex]
$\langle$ColumnList$\rangle$ & $\rightarrow$ & $\langle$Column$\rangle$  (0.6) \\
& $|$ & $\langle$Column$\rangle$\texttt{,} $\langle$ColumnList$\rangle$  (0.4) \\[1ex]
$\langle$Column$\rangle$ & $\rightarrow$ & $\langle$TableName$\rangle$\texttt{.}$\langle$ColumnName$\rangle$  (0.7) \\
& $|$ & $\langle$AggregateFunction$\rangle$\texttt{(}$\langle$TableName$\rangle$\texttt{.}$\langle$ColumnName$\rangle$\texttt{)}  (0.3) \\[1ex]
$\langle$FromClause$\rangle$ & $\rightarrow$ & \texttt{FROM} $\langle$TableName$\rangle$  (0.4) \\
& $|$ & \texttt{FROM} $\langle$TableName$\rangle$ $\langle$JoinClause$\rangle$  (0.6) \\[1ex]
$\langle$JoinClause$\rangle$ & $\rightarrow$ & $\langle$JoinType$\rangle$ \texttt{JOIN} $\langle$TableName$\rangle$ \texttt{ON} $\langle$JoinCondition$\rangle$  (0.7) \\
& $|$ & $\langle$JoinType$\rangle$ \texttt{JOIN} $\langle$TableName$\rangle$ \texttt{ON} $\langle$JoinCondition$\rangle$ $\langle$JoinClause$\rangle$  (0.3) \\[1ex]
$\langle$JoinType$\rangle$ & $\rightarrow$ & \texttt{INNER}  (0.4) \\
& $|$ & \texttt{LEFT}  (0.3) \\
& $|$ & \texttt{RIGHT}  (0.2) \\
& $|$ & \texttt{FULL}  (0.1) \\[1ex]
$\langle$JoinCondition$\rangle$ & $\rightarrow$ & $\langle$Column$\rangle$ \texttt{=} $\langle$Column$\rangle$  (0.8) \\
& $|$ & $\langle$JoinCondition$\rangle$ \texttt{AND} $\langle$JoinCondition$\rangle$  (0.2) \\[1ex]
$\langle$WhereClause$\rangle$ & $\rightarrow$ & \texttt{WHERE} $\langle$Condition$\rangle$  (1.0) \\[1ex]
$\langle$Condition$\rangle$ & $\rightarrow$ & $\langle$Column$\rangle$ $\langle$Operator$\rangle$ $\langle$Value$\rangle$  (0.5) \\
& $|$ & $\langle$Condition$\rangle$ \texttt{AND} $\langle$Condition$\rangle$  (0.3) \\
& $|$ & $\langle$Condition$\rangle$ \texttt{OR} $\langle$Condition$\rangle$  (0.2) \\[1ex]
$\langle$GroupByClause$\rangle$ & $\rightarrow$ & \texttt{GROUP BY} $\langle$ColumnList$\rangle$  (1.0) \\[1ex]
$\langle$OrderByClause$\rangle$ & $\rightarrow$ & \texttt{ORDER BY} $\langle$ColumnList$\rangle$ [\texttt{ASC} $|$ \texttt{DESC}]  (1.0) \\[1ex]
$\langle$AggregateFunction$\rangle$ & $\rightarrow$ & \texttt{COUNT}  (0.3) \\
& $|$ & \texttt{SUM}  (0.2) \\
& $|$ & \texttt{AVG}  (0.2) \\
& $|$ & \texttt{MIN}  (0.15) \\
& $|$ & \texttt{MAX}  (0.15) \\[1ex]
$\langle$Operator$\rangle$ & $\rightarrow$ & \texttt{=}  (0.3) \\
& $|$ & \texttt{<}  (0.1) \\
& $|$ & \texttt{>}  (0.1) \\
& $|$ & \texttt{<=}  (0.1) \\
& $|$ & \texttt{>=}  (0.1) \\
& $|$ & \texttt{<>}  (0.1) \\
& $|$ & \texttt{LIKE}  (0.1) \\
& $|$ & \texttt{IN}  (0.1) \\[1ex]
$\langle$Value$\rangle$ & $\rightarrow$ & $\langle$Number$\rangle$  (0.4) \\
& $|$ & $\langle$String$\rangle$  (0.4) \\
& $|$ & $\langle$Column$\rangle$  (0.2) \\[1ex]
\multicolumn{3}{l}{$\langle$TableName$\rangle$, $\langle$ColumnName$\rangle$, $\langle$Number$\rangle$, $\langle$String$\rangle$ represent database-specific values or literals} \\
\bottomrule
\end{tabular}
\end{table*}






\section{Example of PCFG in SQL Sampling}
\label{app:pcfg}

Table~\ref{tab:sql-pcfg} exemplifies the Probabilistic Context-Free Grammar (PCFG) used for random sampling in Section~\ref{sec:sql_sampling}.
Note that these probabilities are illustrative examples and not derived from real-world query distributions. In practice, these probabilities can be tuned by a large collection of real-world SQL queries. To protect customer data privacy, we do not display the actual probability distribution. 

The grammar begins with the $\langle$Query$\rangle$ non-terminal, which expands into the fundamental components of an SQL query, including $\langle$SelectClause$\rangle$, $\langle$FromClause$\rangle$, and optional $\langle$JoinClause$\rangle$, $\langle$WhereClause$\rangle$, $\langle$GroupByClause$\rangle$, and $\langle$OrderByClause$\rangle$. Each clause is further defined by its constituent parts. For instance, the $\langle$SelectClause$\rangle$ can be either a simple \texttt{SELECT} or \texttt{SELECT DISTINCT}, followed by a $\langle$ColumnList$\rangle$. The $\langle$ColumnList$\rangle$ can be a single $\langle$Column$\rangle$ or recursively defined as $\langle$Column$\rangle$\texttt{,} $\langle$ColumnList$\rangle$, allowing for multiple column selections. Columns can be either simple references ($\langle$TableName$\rangle$\texttt{.}$\langle$ColumnName$\rangle$) or aggregations ($\langle$AggregateFunction$\rangle$ \texttt{(}$\langle$TableName$\rangle$\texttt{.}$\langle$ColumnName$\rangle$\texttt{)}). The grammar also defines common SQL operations and functions, such as comparison operators (\texttt{=}, \texttt{<}, \texttt{>}, etc.) and aggregate functions (\texttt{COUNT}, \texttt{SUM}, \texttt{AVG}, etc.). 
Probabilities associated with each production rule guide the random sampling process. 
For example, a $\langle$SelectClause$\rangle$ has a 90\% chance of being a simple \texttt{SELECT} and a 10\% chance of being \texttt{SELECT DISTINCT} in our example. 

\section{Prompts Used in {\tool}}
\label{app:prompt}

\edit{In this section, we present the prompts used in {\tool} for different functionalities.Each prompt is carefully designed to guide the language model in generating specific outputs while maintaining consistency and quality. The prompt includes relevant context, objective, and structured output format for the following parsing.}


\edit{Figure~\ref{fig:prompt_explanation} shows the prompt for generating step-by-step explanations and sub-questions. Note that this prompt is invoked and the LLM is called only when the rule-based explanation generation fails. Figure~\ref{fig:prompt_question} displays the prompt for natural language question generation. Figure~\ref{fig:prompt_subquestion} illustrates the prompt for sub-question generation. Figures~\ref{fig:prompt_alignment_analysis} and~\ref{fig:prompt_alignment_map} show the prompts for alignment analysis and mapping generation, respectively. 
As a core functionality in {\tool}, we decompose this task into two smaller subtasks: free-form text analysis and structured output generation. This decomposition is motivated by research~\cite{multi_agent_collaboration} showing that multi-agent collaboration leads to improved generation accuracy.
Figure~\ref{fig:prompt_inject} presents the prompt for updating natural language questions by emphasizing specific steps. 
Finally, Figure~\ref{fig:prompt_equivalence} shows the prompt for analyzing and scoring NL-SQL equivalence.}




\vspace*{\fill}
\begin{figure*}[ht]
  \centering
\includegraphics[width=\textwidth]{figure/prompt/explanation_prompt.png}
  \caption{Prompt of Step-by-step Explanation \& Sub-Question Generation.}
  \label{fig:prompt_explanation}
\end{figure*}
\vspace*{\fill}

\vspace*{\fill}
\begin{figure*}[ht]
  \centering
\includegraphics[width=0.65\textwidth]{figure/prompt/question_prompt.png}
  \caption{Prompt of Natural Language Question Generation.}
  \label{fig:prompt_question}
\end{figure*}
\vspace*{\fill}

\vspace*{\fill}
\begin{figure*}[ht]
  \centering
\includegraphics[width=0.65\textwidth]{figure/prompt/sub_question_prompt.png}
  \caption{Prompt of Sub-question Generation.}
  \label{fig:prompt_subquestion}
\end{figure*}
\vspace*{\fill}

\vspace*{\fill}
\begin{figure*}[ht]
  \centering
\includegraphics[width=0.65\textwidth]{figure/prompt/alignment1_prompt.png}
  \caption{Prompt of Alignment Analysis Generation}
  \label{fig:prompt_alignment_analysis}
\end{figure*}
\vspace*{\fill}

\vspace*{\fill}
\begin{figure*}[ht]
  \centering
\includegraphics[width=0.65\textwidth]{figure/prompt/alignment2_prompt.png}
  \caption{Prompt of Alignment Mapping Generation}
  \label{fig:prompt_alignment_map}
\end{figure*}
\vspace*{\fill}

\vspace*{\fill}
\begin{figure*}[ht]
  \centering
\includegraphics[width=0.65\textwidth]{figure/prompt/inject_question_prompt.png}
  \caption{Prompt of Updating NL question by emphasizing a certain step}
  \label{fig:prompt_inject}
\end{figure*}
\vspace*{\fill}

\vspace*{\fill}
\begin{figure*}[ht]
  \centering
\includegraphics[width=0.65\textwidth]{figure/prompt/equivalence_score_prompt.png}
  \caption{Prompt of NL-SQL Equivalence Analysis and Scoring}
  \label{fig:prompt_equivalence}
\end{figure*}
\vspace*{\fill}