\section{Introduction}

%Developing natural language (NL) interfaces to databases has been a long-standing goal in human-computer interaction \cite{NLI1, NLI2, NLI3}.  
Natural language interfaces to databases can significantly democratize complex data analysis and decision-making processes for end-users by allowing them to express their intent through NL.
% \todo{Simplify the writings in the following two sentences. They look verbose and messy.}
% These NL interfaces are majorly powered by text-to-SQL models~\cite{smbop, picard, dinsql} that automatically translate a natural language question into an SQL query, which is then executed on the database and returns the query results.
These interfaces are mainly powered by text-to-SQL models that convert NL questions into SQL queries~\cite{smbop, picard, dinsql}. The resulting SQL queries are then executed on the database to retrieve the results for users.
Recent advancements, notably in large language models (LLMs), have substantially enhanced NL interfaces and opened a promising market that is expected to grow around three times in five years~\cite{market1, market2, market3}.


% Data querying using natural language (NL) has been a long-standing goal in human-computer interaction and database research. 
% Recent advancements, notably in large language models (LLMs), have substantially enhanced NL interfaces for databases.
% These NL interfaces are powered by text-to-SQL models~\cite{smbop, picard, dinsql} that can automatically translate an NL question into an executable SQL query, democratizing access to complex data for non-expert users.

Despite these advancements, the accuracy and reliability of text-to-SQL models in real-world applications remain suboptimal, especially in high-stakes domains such as finance and healthcare where an error can cause severe consequences. 
One root cause of this inaccuracy is the domain shift, which is a common challenge in machine learning~\cite{domain_adaption1, domain_adaption2, domain_adaption3}. 
% \todo{Change the tone in the following sentences. Don't call it "academic objectives" since it may offend some reviewers since it sounds like academic people work on useless objectives. Use the challenge of domain shift to motivate your work. This is a common and fundamental issue in machine learning and it's more acceptable to reviewers.}
Specifically, deploying an NL interface to a real-world database requires the underlying text-to-SQL model to understand the database schema, which is often highly specialized with unique architectures and data contexts that public datasets such as Spider~\cite{spider} and BIRD~\cite{bird} do not cover. %However, when the domain shifts, the performance of text-to-SQL models trained on public or original datasets can drop dramatically. 
Our formative study with engineers from Adobe reveals a text-to-SQL accuracy drop of at least 13.3\% for newly added columns and 9.1\% for new tables. Moreover, as the database schema evolves over time, the performance of the text-to-SQL model continues to decline. To fully unleash the computational power of text-to-SQL models in the new domain, there is a demand for annotating sufficient pairs of text and SQL data under a target schema for fine-tuning a text-to-SQL model to the domain. %Specifically, our formative study shows that anseveral industrial engineer could only annotate 50 complex queries daily.
% \todo{Seriously? This sounds surprisingly low. 50 effective instances are 50 pairs of text description and SQL queries, right? An undergrad can do that in one or two hours.}.

% \todo{Why suddenly mention LLM-based data augmentation? There is no context.}

Recent studies have shown that  LLMs can be used as data annotators in domains such as image captioning, code generation, and topic detection~\cite{llm_data_annotation, annollm, llm_annotator_1, chatgpt_outperform_crowd_workers, self_instruct, oss_instruct, evol_instruct}.
While we could use LLMs to annotate text-to-SQL datasets, it introduces new challenges such as hallucination~\cite{llm_hallucination}. %and performance degradation after prolonged training on synthetic data~\cite{collapse}. 
Furthermore, LLMs are inherently not designed or trained to generate diverse data.
They frequently generate repetitive data and struggle to maintain consistency in large datasets due to the limited attention in a long context~\cite{SPA, longformer}. Therefore, we believe that when harnessing LLMs for curating text-to-SQL datasets, it is essential to incorporate human knowledge and improve human control. To the best of our knowledge, no prior work has investigated human-in-the-loop data annotation methods in the domain of text-to-SQL generation.

% An alternative approach is data augmentation, such as prompting LLMs to create synthetic data. However, data augmentation has several limitations. First, LLMs or other learning-based models may generate hallucinated data~\cite{llm_hallucination}. A recent study~\cite{collapse} shows that continuous training on synthetic data can lead to poor performance. Second, LLMs are not inherently designed to generate diverse data, meaning there is no guarantee that the synthetic dataset will cover all cases. Finally, due to limited context windows and model attention, it is challenging for models to remember all previously generated data without repetition---let alone in a controllable manner.


\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figure/overview.png}
  \caption{Illustration of traditional text-to-SQL annotation v.s. using {\tool}. \edit{Green arrows indicate dataset creation steps, gray arrows represent supportive data flows, and blue dashed arrows show user interactions with the interface. {\tool} includes three interactive features in the blue box: {\em schema visualization}, {\em error detection and repair}, and {\em diversity analysis}. Annotators can leverage these features to efficiently control the data annotation process.}}
  \label{fig:overview}
\end{figure*}


To bridge this gap, we propose {\tool}, an interactive text-to-SQL data annotation system designed to streamline the process of creating high-quality text-to-SQL datasets on a customized schema. 
Our insight is to automate labor-intensive procedures (e.g., initialize SQL queries and NL questions) while providing effective error detection and repair mechanisms, allowing annotators to efficiently verify and refine the data in collaboration with the LLM.
Figure~\ref{fig:overview} compares the pipeline of traditional text-to-SQL annotation with the pipeline when using {\tool}. 
% \todo{Why mention DDL here? Is it necessary? Reviewers may feel that your approach is limited to DDL input. Does your approach work with other data schema specification languages? Can you describe it in a more abstract way like "Given a target database schema, our approach first ...."}
% Data Definition Language (DDL) file \footnote{Data Definition Language (DDL) file includes the code snippet that defines the structure and properties of a database. It specifies the data types and relationships of entities, such as the designation of primary and foreign keys. This file provides the blueprint for creating and organizing a database's architecture.} 
Traditionally, given a database schema, annotators have to understand the schema architecture and manually create new SQL queries and NL questions from scratch. This process is often mentally demanding and prone to biases, as personal assumptions can affect query patterns and phrasing.
% \todo{why biased?}

In contrast, {\tool} visualizes the database schema as an interactive editable graph to facilitate comprehension. Based on the customized schema, {\tool} leverages a rule-based method to populate a database with numerous diverse records. Then, it uses this populated database to sample SQL queries using Probabilistic Context-Free Grammar (PCFG).
% Leveraging an LLM, {\tool} first translates the SQL query into an NL question, then aligns the SQL query with the NL question through step-by-step query analysis. 
% Based on an \todo{Is this in-depth analysis and alignment automatically done by {\tool} or manually done by users?}in-depth analysis and alignment, 
{\tool} leverages both a grammar-based approach and an LLM to analyze SQL queries step by step. Based on the analysis, it then translates the SQL query into a corresponding NL question. The LLM automatically aligns the NL question with the SQL query through this step-by-step analysis. Based on this alignment, {\tool} identifies potential errors and allows annotators to efficiently rectify these issues by instructing the LLM through a structured workflow.
Specifically, annotators can inject missing information from misaligned steps or delete redundant text in the NL question.
Finally, {\tool} analyzes the annotated data, providing dynamic dataset composition on property distributions such as statistics for specific referenced entities in the current dataset. This feature enables annotators to iteratively monitor and refine the dataset's diversity and quality.


% \todo{This paragraph is too handwavy.}
We evaluated {\tool} through a within-subjects study with 12 participants. Compared to manual annotation and using ChatGPT, participants using {\tool} annotated 4.4X and 2.3X more data,  with 87\% and 84\% fewer errors, respectively.
We evaluated the naturalness of annotated data using both the Flesch-Kincaid Score and human ratings. The results indicate that NL questions annotated by {\tool} are more natural compared to those produced by ChatGPT.
To evaluate dataset diversity, we analyze the number of clauses, tables, columns, and values in SQL queries. The results show that {\tool} achieves a higher Simpson's Diversity Index, suggesting better overall diversity.
Furthermore, during the annotation, participants self-reported experiencing less cognitive load and greater confidence when using {\tool}.

In summary, this work makes the following contributions:%\footnote{Our code will be publicly available.}:

\begin{itemize}
    \item A \textbf{formative study with industrial practitioners} identifies five major users' needs of text-to-SQL annotation in practice.
    
    \item A \textbf{comprehensive interactive text-to-SQL annotation system}, {\tool}, streamlines the annotation of text-to-SQL datasets. It facilitates database comprehension, database population, text-to-SQL data generation, error detection and repair, and dataset analysis.
    
    %\item A \textbf{general Human-LLM collaborative annotation workflow} where formal code is sampled using PCFG. LLMs then summarize the code into NL and align it with the formal code through a step-by-step analysis. Potential errors are identified by misalignments, which users can fix in collaboration with the LLM.
    
    \item A \textbf{comprehensive evaluation} assesses the usability and effectiveness of {\tool}, demonstrating significant improvements in annotation speed, accuracy, naturalness, and dataset diversity compared to both manual efforts and conversational AI assistants like ChatGPT.
\end{itemize}

