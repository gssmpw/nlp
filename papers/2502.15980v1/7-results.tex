\section{Results}

This section describes the results of our user study. 
We first present the analysis of user annotation performance in different conditions. We measure annotation speed and annotation quality.
Then, we report user perception of different conditions, e.g., their confidence level of annotated data and the perceived cognitive load. 

\subsection{Annotation Speed}
%A faster speed suggests a less expensive annotation.

\begin{table}[htb]
    \centering
    \caption{Number of Annotated Data (5 minutes)}
    \vspace{-2.5mm}
    \resizebox{0.8\linewidth}{!}{%
    \begin{tabular}{lcc}
        \toprule
              & \textbf{Completed Annotation} & \textbf{SD} \\
        \midrule
        Manual & 2.00 & 0.91 \\
        AI Assistant & 3.75 & 2.09 \\
        {\tool} & \textbf{8.75} & 2.74 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:annotation_speed}
\end{table}


Since each session has a fixed annotation time period, we use the average number of annotated tasks to represent the annotation speed. 
We compare the number of tasks completed across three conditions: Manual, AI assistant, and {\tool}. Table~\ref{tab:annotation_speed} presents the average annotation count completed within 5 minutes of the task session for each condition.
When using {\tool}, participants annotated the most tasks ($Mean = 8.75, SD = 2.74$), followed by using the AI assistant ($Mean = 3.75, SD = 2.09$), and manually annotating ($Mean = 2.00, SD = 0.91$). The ANOVA test shows that the mean differences are statistically significant ($p$-value = 1.96e-8).

The substantial improvement in task achievement with {\tool} suggests that {\tool} could enhance productivity in real-world applications.
Compared to ChatGPT, {\tool} provides a better explanation for participant engagement and validation.
P3 said, ``\textit{Even though ChatGPT can give me plenty of data, I need to manually check its output. I feel that connecting its output SQL to the schema is challenging in the limited time.}''
Furthermore, {\tool} serves as an interactive interface, allowing users to iteratively refine the data.
Both P6 and P9 appreciated this utility.
P9 said, ``\textit{It's easier and quicker to test and iterate on these [data] using the tool}'',


\subsection{Annotation Quality}
We define the quality of a text-to-SQL dataset into (1) \textbf{Correctness} (whether there is a syntax error, and whether the NL question matches the SQL query), (2) \textbf{Naturalness} (whether the NL question is natural enough as a human daily question), and (2) \textbf{Diversity} (whether the dataset has comprehensive coverage of different entities and query types, without any biases).

\subsubsection{\textbf{Correctness}}

To evaluate the correctness of participants' annotations, we manually review all data collected during the user study. We evaluate two types of errors in the annotated data. 
First, we look for SQL syntax errors or the misuse of entities and references in the database schema. 
We identify this type of error by executing the SQL query in a sandbox database that is adequately populated from the schema.
The error leads to execution failures or the return of an empty result.
Second, we evaluate the equivalence between the SQL query and the NL question. While the SQL might be syntactically correct on the schema, it may not accurately represent the intent of the NL question. In this case, we manually evaluate their equivalence.
Table~\ref{tab:correctness_annotation} shows the two types of error rates and the overall accuracy for each condition.

We observe different reasons for errors in manual annotation compared to using the AI assistant. During manual annotation, participants who were less proficient in SQL often made grammatical mistakes (29.24\%). However, since they tended to write simple SQL queries, the equivalence error was less (5.15\%).
The AI assistant, ChatGPT, rarely introduces syntax errors. However, it tends to generate more complex SQL queries (e.g., multiple JOINs) than manual annotation. Despite participant refinement, these queries often fail to match the complex schema due to hallucination, leading to SQL execution errors (18.73\%). Moreover, these complex queries pose greater challenges in maintaining equivalence with the NL question, resulting in the highest error rate of equivalence ((8.34\%).

Compared to manual annotation and using the AI assistant, using {\tool} achieves the highest accuracy (95.56\%). For SQL errors, queries sampled by {\tool} are guaranteed to be syntactically correct. Regarding NL-SQL equivalence, {\tool} aligns SQL and NL through step-by-step analysis, achieving the best equivalence after human refinement. However, we observe cases (4.44\%) where they are not fully equivalent, suggesting room for further improvement in NL generation accuracy.



\begin{table}[htb]
    \centering
    \caption{Correctness of Annotated Data}
    \vspace{-2.5mm}
    \resizebox{1\linewidth}{!}{%
    \begin{tabular}{lccc}
        \toprule
              & \textbf{SQL Error} & \textbf{Equivalence Error} & \textbf{Accuracy} \\
        \midrule
        Manual  & 29.24\%  & 5.15\% & 65.61\% \\
        AI Assistant   & 18.73\%   & 8.34\% & 72.93\% \\
        {\tool} & \textbf{0}   & \textbf{4.44\%} & \textbf{95.56\%} \\
        \bottomrule
    \end{tabular}
    }
    
    \label{tab:correctness_annotation}
\end{table}



\subsubsection{\textbf{Naturalness}}
In addition to correctness, the naturalness of the NL question is crucial for the quality of text-to-SQL data.
While an NL question may accurately match its SQL query, it might be verbose. In the real world, people tend to ask concise questions that follow certain natural language patterns. To evaluate naturalness, we first calculate the \textit{Flesch-Kincaid Readability Score}~\cite{flesch_score}, an automatic metric measuring text readability on a scale from 0 to 100.
To better assess naturalness, we further manually rate all annotated questions from 1 to 7 after masking the conditions.\footnote{{\tool} references synthetic values in a sandbox database, which can be easily identified by the human raters. We replace all these values with commonly used values for fair evaluation.}

\begin{table}[htb]
    \centering
        \centering
        \caption{Flesch-Kincaid Readability Score of Annotated Questions (0-100).}
        \vspace{-2.5mm}
        \resizebox{0.69\linewidth}{!}{%
        \begin{tabular}{lcc}
            \toprule
                  & \textbf{Flesch-Kincaid Score} & \textbf{SD} \\
            \midrule
            Manual & 76.94 & 13.15 \\
            AI Assistant & 56.14 & 18.62 \\
            {\tool} & 72.32 & 16.54 \\
            \bottomrule
        \end{tabular}
        }
        \label{tab:Flesch-Kincaid}
\end{table}

\begin{table}[htb]
    \hspace{0.06\textwidth}
        \centering
        \caption{Manual Rating of Naturalness of Annotated Questions (0-7).}
        \vspace{-2.5mm}
        \resizebox{0.65\linewidth}{!}{%
        \begin{tabular}{lcc}
            \toprule
                  & \textbf{Human Rating} & \textbf{SD} \\
            \midrule
            Manual & 6.25 & 0.52 \\
            AI Assistant & 6.02 & 0.58 \\
            {\tool} & 6.19 & 0.44 \\
            \bottomrule
        \end{tabular}
        }
        \label{tab:natural_rating}
\end{table}


\begin{table*}[htb]
    \centering
    \caption{SQL Query Component Diversity Analysis in Annotated Datasets.}
    \vspace{-2.5mm}
    \resizebox{0.75\linewidth}{!}{%
    \begin{tabular}{l cc|cc|cc|cc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Clause}} & \multicolumn{2}{c}{\textbf{Table}} & \multicolumn{2}{c}{\textbf{Column}} & \multicolumn{2}{c}{\textbf{Value}} \\
        \cmidrule(lr){2-9}
        \textbf{Method} & Diversity & Mean & Diversity & Mean & Diversity & Mean & Diversity & Mean \\
        \midrule
        Manual       & 0.48 & 2.40 & 0.41 & 1.29 & 0.17 & 1.09 & 0.44 & 0.33 \\
        AI Assistant & 0.64 & 5.10 & 0.81 & 5.75 & \textbf{0.64} & 3.73 & 0.62 & 1.25 \\
        {\tool}      & \textbf{0.69} & 3.18 & \textbf{0.83} & 2.54 & 0.59 & 1.71 & \textbf{0.66} & 0.85 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:diversity}
\end{table*}


\begin{figure*}[htb]
  \centering
\includegraphics[width=0.85\textwidth]{figure/task1_cognitive.png}
  \caption{NASA Task Load Index Ratings of Text-to-SQL Data Annotation}
  \label{fig:cognitive1}
\end{figure*}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.92\textwidth]{figure/sus1.png}
  \caption{SUS Scores of Text-to-SQL Data Annotation}
  \label{fig:sus1}
\end{figure*}



As shown in Table~\ref{tab:Flesch-Kincaid} and Table~\ref{tab:natural_rating}, the \textit{Flesch-Kincaid Readability Score} is consistent with human ratings. While manual annotation achieves the highest score (Flesch-Kincaid Score: 76.94, Human Rating: 6.25) as expected, the NL questions annotated through {\tool} achieve a comparable score (Flesch-Kincaid Score: 73.32, Human Rating: 6.19). We observe that using ChatGPT achieves the worst naturalness in questions (Flesch-Kincaid Score: 56.14, Human Rating: 6.02). Notably, ChatGPT-generated questions often include SQL keywords or follow SQL query patterns (e.g., \textit{Return all student names that are grouped by grades.}). 
Participants often accept ChatGPT's generation without modifications. While {\tool} is also built upon ChatGPT, it employs step-by-step analysis and in-context learning from real-world questions, thereby better incorporating real question patterns into the generated questions. 
Moreover, {\tool} offers a better interface and provides helpful suggestions for refining questions. Participants report that they are more willing to polish the LLM-generated data when using {\tool}.
P8 said, ``\textit{I really enjoy the UI in this tool. It suggests how to polish the data. And I really enjoy playing with the alignment feature to see what I can do with existing data.}''



\subsubsection{\textbf{Diversity}}
To evaluate the diversity and potential biases in the annotated dataset, we analyze the composition of participant-annotated data across four dimensions, including the number of clauses, columns, tables, and values involved. We measure the diversity of each dimension using Simpson's Diversity Index~\cite{simpson_diversity}, which is used to quantify the level of heterogeneity of a certain property.





Table~\ref{tab:diversity} shows the diversity and mean values for each method and dataset property. {\tool} demonstrates better diversity in the generated SQL for most dimensions, except for columns generated by the AI assistant. 
This is because ChatGPT tended to join excessive tables (mean = 5.75) in \texttt{FROM} clauses and include excessive columns (mean = 3.73) in  \texttt{SELECT} clauses.
In contrast, {\tool} learned the distribution from real-world datasets, resulting in a more reasonable distribution. For manual annotation, participants typically wrote simple SQL queries. For instance, they rarely used JOIN clauses, leading to low diversity.





\subsection{User Cognitive Load \& Usability Rating}

% Reducing cognitive load during data annotation is crucial since it directly make the annotation process cheaper.
% Figure~\ref{fig:cognitive1} presents participants’ ratings on the five cognitive load factors from the NASA TLX questionnaire~\cite{NASA-TLX}. The ANOVA test demonstrates that the mean differences are all statistically significant ($p$-value=6.7e-5, 6e-6, 1.9e-5, 1.5e-5, 1.3e-4 respectively).

Reducing cognitive load during data annotation is crucial, since it directly makes this process more cost-effective. Figure~\ref{fig:cognitive1} illustrates participants' ratings on the five cognitive load factors from the NASA TLX questionnaire. The ANOVA test reveals statistically significant ($\alpha = 0.001$) differences in means for all factors: Mental Demand ($p = 6.7 \times 10^{-5}$), Temporal Demand ($p = 6.0 \times 10^{-6}$), Performance ($p = 1.9 \times 10^{-5}$), Effort ($p = 1.5 \times 10^{-5}$), and Frustration ($p = 1.3 \times 10^{-4}$).



The result demonstrates that {\tool} can significantly reduce users' cognitive load compared to manual annotation and using ChatGPT.
P1 said, ``\textit{Generating NL2SQL data felt effortless with the help of the tool.}''.
We believe this is achieved by a smoother collaboration between the huamn and the LLM.
P1 also comprehensively discussed how {\tool} reduces users' cognitive load,
``\textit{The system allows me to generate a variety of SQLs without any cognitive efforts. This was great because I didn't have to think about SQL syntax, different queries one might ask about the dataset. I also like that the system can generate corresponding NL questions for each SQL. While the generated NL question wasn't always accurate, the system already provided me something which I could iterate on. This is almost like someone gives you a draft that you can just revise vs.~gives you an empty doc for you to start from scratch in writing.}''





% \todo{discuss the comparison with ChatGPT}

Figure~\ref{fig:sus1} displays SUS scores reported by participants, showing consistently positive feedback across all dimensions. Notably, no participants disagreed with any dimension. While a few participants expressed neutral opinions about SQL queries and NL questions suggested by {\tool}, most viewed them positively. All participants expressed high confidence in the quality and quantity of data annotated using {\tool}.