\section{Discussion}
\label{sec:disc}
The last decade has seen significant advancements in modelling the observed galaxy distribution. However, current methods often face challenges when incorporating non-linear scales due to the complexity of the required physics simulators to model the data. Our novel learning-to-optimize approach allows for the direct inclusion of complex and accurate non-linear physics models without requiring differentiability. In the following, we extend the discussion on the importance of non-linear modelling and how to optimally select loss or likelihood functions to use most of the data. 

\subsection{The importance of modelling non-linear scales}
\label{sec:discuss_non_linear}

Non-linear modelling is essential because the observed objects are inherently non-linear, having formed through gravitational collapse. As we have also demonstrated, incorporating non-linear scales is crucial for accurately recovering the large-scale linear initial conditions, aligning with the findings in \citet{Doeser2024}. In the context of inferring the initial conditions from galaxy clustering data, as discussed in \citet{Stopyra2023}, a highly accurate non-linear model is required during inference to obtain accurate posteriors of present-day structures. 

Reconstructions of the initial conditions of our Universe can be leveraged to run constrained simulations that evolve primordial fluctuations to the particular observable large-scale structure distribution of galaxies \citep[see e.g.][]{Wang2016,Sorce2018,Gottl,Libeskind2020,Mcalpine2022,Sawala2020,Hutt2022,Ata2022,Byrohl2024,Wempe2024}. These simulations enables investigating how structures in our observable Universe formed, thus creating a laboratory to search for model consistencies between $\Lambda$CDM and observations. Recent work includes, for example, analysing the spatial distribution of elliptical galaxies and spiral galaxies \citep{Sawala2023}, intricate properties of clusters \citep{Sorce2018,Jasche2019,Mcalpine2022,Hutt2022}, fate of clusters \citep{Ata2022}, modified gravity \citep{Bartlett2021}, galaxy intrinsic alignment \citep{Tsaprazi2021}, large-scale environment of supernovae \citep{Tsaprazi2022}, gamma-ray emission from dark matter particle annihilation \citep{Kostic2023}, magnetic fields on cosmological scales \citep{Hutschenreuter2018}, massive neutrinos \citep{Elbers2023}, and the formation of our Local Group \citep{Wempe2024,Wempe2025}. Re-simulations of our observable Universe also provide access to the complete dynamical evolution of non-linear peculiar velocities, which are essential for peculiar velocity corrections in both Hubble constant measurements using supernova data \citep[e.g.,][]{Kenworthy2022, Riess2022, Peterson2022} or using gravitational wave sources \citep{Mukherjee2021}. 

As an increasing number of scientific objectives rely on cosmic initial conditions, improving their accuracy through enhanced non-linear data modelling is becoming critical. In this work, we advance non-linear modelling by incorporating non-differentiable components in the form of a halo-identification algorithm, enabling more accurate and reliable reconstructions of initial conditions.

\subsection{Choosing appropriate data likelihoods}
\label{sec:discuss_data_likelihood}

Throughout this work, our focus has been on optimizing and refining the search mechanism for accurate reconstructions using non-linear and non-differentiable physics simulators. Entering the non-linear regime also necessitates careful modeling of the loss functions and likelihoods. Specifically, in this study, where halo catalogs are used as data, we have explored approaches to model count data. Previous studies have examined the additional information provided by mass-weighted halo count fields as compared to count fields \citep{Seljak2009,Hamaus2010,Liu2021}. In this work, we chose to model the halo count data using both a halo overdensity (with counts only) and a mass-weighted halo overdensity, and applying a Gaussian likelihood between the reconstruction and the data. 

To evaluate the individual contributions of the halo overdensity and mass-weighted halo overdensity, we conducted experiments using each field independently, both for training the neural optimizer and as input data during optimization. The reconstructed halo mass functions for these cases are presented in Fig~\ref{fig:mw_vs_nw}. We observed that omitting either the count or mass constraints leads to worse reconstructions. For instance, in the case of only using the mass-weighted halo overdensity, we see that the power of the reconstructed initial conditions overshoots the true cosmological power, thus prioritizing the creation of massive objects. While using only the mass-weighted field resulted in reconstructing more high-mass haloes, using only the halo overdensity resulted in reconstructing more of the numerous low-mass haloes. 

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/various_likelihood.pdf}
    \vspace{-0.65em}
    \caption{Reconstructions using different halo fields as data $\mathbf{d}$, as generated by the same halo catalog. By only using either the mass-weighted halo overdensity $\boldsymbol{\delta}_{\mathrm{mw}}^{\mathrm{halo}}$ or halo overdensity $\boldsymbol{\delta}_{\mathrm{count}}^{\mathrm{halo}}$ in the data likelihood, the initial conditions (left), and hence the present-day halo mass function (right), are not accurately reconstructed. In particular, only using the mw-field results in an overestimation of high-mass haloes and hence a larger power in the initial conditions. On the contrary, using the count overdensity results in accurately recovering only the most numerous (low-mass) haloes, and a slightly lower power in the initial conditions. Using both the halo counts as well as their masses enables a more accurate reconstruction of the initial conditions and, in turn, the halo mass function.}
    \label{fig:mw_vs_nw}
\end{figure*}

Incorporating both counts and mass breaks these degeneracies, enabling more accurate reconstructions. Similarly, a recent study by \citet{Bayer2023b} demonstrated that adding halo velocity data alongside halo positions further enhances the reconstruction of initial conditions. These findings highlight the potential of incorporating additional information to achieve even more accurate reconstructions. 

While halo or galaxy clustering serves as one key data source, from which we have demonstrated the importance of careful modelling, other cosmological data sources can also be considered. Field-level inference or reconstruction of initial conditions has been explored using various datasets, including Lyman-alpha forests \citep{Horowitz2019,Horowitz2022,Porqueres2020,Porqueres2021}, cosmic shear measurements \citep{Porqueres2021,Porqueres2022,Porqueres2023}, photometric redshifts \citep{Tsaprazi2023}, and peculiar velocity tracers \citep{Prideaux-Ghee2022}. Our approach naturally allows for the simultaneous incorporation of multiple sources of cosmological data, which offers a pathway to more comprehensive and accurate reconstructions of initial conditions in the future.

\subsection{Flexibility and computational performance of \texttt{LULO}}
\label{sec:compcost}

We emphasize that our framework \texttt{LULO} is compatible with any simulation pipeline. Alternative forward models offering other levels of fidelity or involving different output quantities to match new datasets can be seamlessly integrated as long as input-output pairs can be generated. The computational cost therefore depends on the desired accuracy and can be adjusted to prioritize speed with approximate simulators or higher fidelity at an increased computational expense.

In this work, to demonstrate the successful integration of non-differentiable models without requiring modifications to the simulators, we used the \texttt{Gadget-IV} code \citep{Springel2021} and the \texttt{AHF} halo identification algorithm \citep{Gill2004, Knollmann2009}. The combined computational cost of these with $128^3$ simulated particles within a $250h^{-1}$ Mpc box was approximately $5.5$ CPU-hours per simulation, leading to a total of around $27.5$k CPU-hours for generating the training data. The exact number of simulations required for the training data was not explored in this work but remains an area of interest for future investigations. For this setup and having $5000$ training pairs, a single model required approximately $120$ GPU-hours for training.

During optimization, as also visualized in Figure~\ref{fig:opt_process}, each step requires on average $4.3$ simulations for the line search. In total, calling the simulation pipeline approximately $96$ times is needed for $22$ optimization steps, which corresponds to a total wall time of $11$ hours. This may be a conservative estimate, as convergence is approached after approximately $30$ simulations as seen in Fig.~\ref{fig:opt_process}. We note that predicting the update direction on the GPU takes less than $2$ seconds per optimization step, making this cost negligible in comparison to the simulation costs.

In future work, higher-resolution simulations could be incorporated to capture finer cosmological structures. Forward models that include hydrodynamical or other dynamical processes could also be integrated. Field-level emulators, such as the ones developed by \citet{Jamieson2022b,Jamieson2024} for simulating non-linear structure formation, could also be leveraged. As demonstrated in \citet{Doeser2024} within Bayesian inference of initial conditions, these emulators can reduce computational costs by more than a factor of $100$ compared to full $N$-body simulations, with only a minor trade-off in accuracy. The flexibility of \texttt{LULO} to incorporate the latest advancements from the community in structure formation and galaxy biasing modelling, without requiring differentiability, ensures it remains adaptable to evolving methodologies.

\section{Conclusions}
\label{sec:conc}

With next-generation galaxy surveys approaching, accurate data models are crucial for maximizing information from small-scale clustering. Extracting cosmological insights at non-linear scales is particularly challenging due to the complex non-linear modelling of large-scale structures and the relationship between the observed galaxies and the underlying dark matter distribution, often involving numerical simulators and non-differentiable data models. To tackle these challenges, we develop \textit{Learning the Universe by Learning to Optimize} (\texttt{LULO}), a framework for reconstructing 3D cosmic initial conditions by fitting state-of-the-art non-differentiable simulators to cosmological data at the field-level. 

By fitting explicit physics models that span from the initial conditions to the data, field-level inference surpasses traditional approaches based on statistical summaries while also enabling detailed causal analysis \citep[see e.g.][]{Jasche2019,Mcalpine2022,Wempe2024}. While most field-level approaches rely on differentiable physics simulators, we follow a complementary approach that goes beyond the requirement of differentiability by accelerating the optimization algorithm itself. \texttt{LULO} also separates the neural optimization pipeline from the physics simulator, allowing any simulator to be integrated without extra development.

As opposed to recent uses of deep learning that directly predicts the input or output of a physics model, our neural optimizer is tasked with learning the dynamical behaviour of a simulator by learning how differences between the data and the model prediction map to updates in the initial conditions. During optimization, the optimizer is then tasked to predict a search direction in which the initial conditions should be updated to minimize the data discrepancy. Specifically, our framework keeps high-fidelity physics simulators in the loop, allowing for ongoing validation of the updated initial conditions throughout the process and providing full explainability, thus addressing common remarks of black-box deep learning \citep[e.g.][]{Angulo2021, Huertas-Company2022}. 

In this work, we have demonstrated that \texttt{LULO} accurately reconstructs the initial conditions from $M_{200\mathrm{c}}$ halo catalogues. These catalogues are generated using the high-fidelity, dark-matter-only simulation code \texttt{Gadget-IV} and the spherical overdensity algorithm \texttt{AHF}. In particular, we show that the transfer function and cross-correlation of the initial conditions are accurately recovered in the linear regime. When forward simulated, we recover non-linear structures at the present epoch to scales $k\gtrsim1h$ Mpc$^{-1}$. Our work highlights the critical role of mode coupling in gravitational structure formation for modelling non-linear structures to $k\sim1h$ Mpc$^{-1}$ at the present epoch. Accurately describing linear scales ($k < 0.1h$ Mpc$^{-1}$) in the initial conditions, where it can be compared to linear theory, thus necessitates accounting for non-linear scales.

In summary, our approach integrates machine learning as an optimizer while preserving full physics simulations in the loop. By streamlining the development cycle and enabling the use of any high-fidelity physics simulator, our approach ensures both high scalability and physical fidelity. While our results demonstrate the potential of \texttt{LULO} for field-level inference, several challenges remain. Future work will explore its scalability to larger simulations, generalization to different physics models, and inclusion of observational systematics and survey characteristics in the modelling pipeline. In conclusion, \texttt{LULO} demonstrates a powerful framework for leveraging complex, non-differentiable simulators to model next-generation cosmological data at non-linear scales, paving the way for more precise and comprehensive cosmological field-level inference.
