\section{Introduction}
Harnessing the full potential of data from imminent Stage-IV galaxy surveys, including DESI \citep{DESICollaboration2016}, Euclid \citep{Laureijs2011,Amendola2018}, LSST \citep{LSSTScienceCollaboration2009,LSSTDarkEnergyScienceCollaboration2012,Ivezic2019}, SPHEREx \citep{Dore2014,Dore2018}, and Subaru Prime Focus Spectrograph \citep{Takada2012}, requires increasingly complex data models capable of accurately capturing all features in the observed galaxy distribution. Because galaxies are non-linear gravitationally collapsed objects, a non-linear model is required to establish a causal connection between observational data and cosmological theory. Over the past decades, the cosmology community has therefore been on a program of developing tools to probe increasingly smaller, non-linear scales, where a significantly larger number of observable modes promises an unprecedented amount of constraining information. 

Modern analysis of the large-scale structure of the Universe has been driven by advances in numerical resources, with $N$-body simulations emerging as the most advanced technique for modelling the full non-linear structure formation of the Universe \citep[see reviews by][]{Vogelsberger2020,Angulo2021}. However, incorporating these directly in (iterative) statistical inference frameworks for extracting physics information has not been feasible because of their high computational cost. Efforts were therefore put into developing approximate simulators, which aim to accurately reproduce non-linear structures for a lower cost \citep[see e.g][]{1995A&A...296..575B,Tassev2013,Leclercq2015,Feng2016,Rampf2024}. The development of modern advanced inference technologies was further made possible by algorithmic advancements, most notably the creation of fully differentiable non-linear physics simulators, including differentiable higher-order Lagrangian perturbation theory \citep{Jasche2012}, particle-mesh approaches \citep{Dai2018,Jasche2019,Modi2021,Ll2022} and neural network enhanced models \citep{He2019,AlvesDeOliveira,Bernardini2020,Kaushal,Jamieson2022b, Jamieson2024, Doeser2024, Bartlett2024}.

Performing cosmological inference from data to learn about fundamental physics involves exploration through high-dimensional spaces. These spaces consist of cosmological parameters, determining the average evolution of our Universe, and the field of primordial density fluctuations, from which observations originate. From the requirement of modelling non-linear physics comes the challenge of exploring the corresponding parameter spaces, an often numerically expensive task. To tackle this problem, two approaches are possible:
\begin{enumerate}
    \item Develop fast and differentiable models that accurately approximate a high-fidelity physics model.
    \item Develop optimal search algorithms to maintain full physics predictions in the loop, without requiring differentiability.
\end{enumerate}
In this work, we aim to push forward strategy (ii), while recognizing and leveraging the significant breakthroughs achieved through strategy (i). In particular, differentiable physics simulations allow for rapid exploration of parameter spaces through gradient-based optimization or Hamiltonian Monte Carlo \citep[HMC,][]{Duane1987} sampling facilitated by back-propagation. This has enabled recent data analysis approaches to move beyond perturbative modelling and traditional summary statistics to utilize numerical structure formation models during inference.

Instead of directly modelling the observed galaxy distribution, one can reframe the task as a statistical problem of inferring the initial conditions of the Universe \citep{Jasche2012,Kitaura2013,Wang2014}. By incorporating physical models of non-linear gravitational structure formation into a Bayesian framework, this approach establishes a direct link between the observed non-linear cosmic structures and the initial density fields from which they evolved \citep{Jasche2019}. The interest in initial conditions also stems from the crucial role that the initial conditions, along with the laws of physics, play in determining the dynamical evolution of the Universe and the formation of large-scale structures, including galaxies and galaxy clusters. Accurately inferring the initial conditions is thus essential for interpreting observational data, testing cosmological theories, and probing the nature of dark matter and dark energy. 

In the context of field-level inference or reconstruction of cosmic initial conditions, several methods have emerged to handle the complex and high-dimensional datasets provided by next-generation cosmological surveys, including Bayesian forward modelling \citep{Jasche2012, Kitaura2013, Wang2014,  Ata2014, Jasche2015, Jasche2017, Jasche2019, Schmidt2018, Lavaux2019, Kitaura2019, Bos_2019, Ata2020, Doeser2024}, maximum-a-posteriori reconstructions \citep{Seljak2017, Modi2018,Feng2018,Horowitz2019,Modi2021a,Horowitz2023}, and machine-learning enhanced approaches \citep{Modi2018, Modi2021a, Chen2023, List2023, Legin2023, Shallue2023, Jindal2023, Bayer2023, Doeser2024, Savchenko2024, Savchenko2025}. Field-level inference of initial conditions has also shown promise in joint inference with cosmological parameters \citep{Ramanah2018,Kostic2022,Porqueres2023,Chen2024,Andrews2022,Andrews2024,Nguyen2024, Krause2024}. 

A key challenge in accurately modelling observations lies in capturing the connection between matter and haloes or galaxies, commonly referred to as galaxy bias \citep[e.g.][]{Desjacques2016}. For example, \citet{Bartlett2024a} showed that non-linear but local galaxy models are insufficient. As high-dimensional field-level inference methods incorporating non-linear structure formation and perturbative or phenomenological galaxy bias models have now become computationally feasible, the next pressing challenge involves how to integrate non-differentiable models in the data analysis. These include halo population models, galaxy formation models, and hydrodynamical simulations, which offer high-fidelity modelling of, e.g. baryonic physics, massive neutrinos, and feedback-reactions \citep[see, e.g.][]{Villaescusa-Navarro2020a,Schaye2023,Pakmor2023}. Currently, no differentiable algorithms exist for these models, and the implementation and testing of these methods for science-ready applications will require significant development effort and resources to be invested by the community. This is also true for efforts to circumvent traditional non-differentiable models, such as fast differentiable bias models \citep{Modi2018, Charnock2020, Ding2024, Pandey2024a, Pandey2024} to replace algorithms such as spherical overdensity halo finders \texttt{AHF} \citep{Knollmann2009} and \texttt{ROCKSTAR} \citep{Behroozi2013}. Similar efforts also aim to directly learn the mapping between dark matter and galaxy fields through high-fidelity hydrodynamic simulations \citep{Villaescusa-Navarro2020a,Sether2024,Bourdin2024,Horowitz2025}. 

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{fig/schematic.pdf}
    \vspace{-2.5em}
    \caption{High-level overview of \texttt{LULO} (Learning the Universe by Learning to Optimize) which aims to fit complex models to data by reconstructing the three-dimensional initial conditions. The process consists of two components: 1) applying a high-fidelity physics simulator $\mathscr{S}$, and 2) updating the initial conditions to minimize discrepancies $\Delta \ve{d}$ between the simulator output and the data. Importantly, any simulator model, including fully non-linear and non-differentiable ones, is supported. The neural optimizer, pre-trained via a supervised approach to learn how to map data discrepancies to updates in the initial conditions, proposes an update direction $\Delta \ve{x}$ across all initial condition voxels simultaneously. In the current implementation, the step size $\gamma_t$ is optimized via a line search algorithm that requires running the simulator (i.e., $\gamma_t=\gamma_t(\mathscr{S}, \Delta \mathbf{x}_t)$; see details in sections~\ref{sec:update_direction}). The iterative process continues until the simulation output aligns with the data, as shown in the $2d$-slices after eight optimization steps. The slices show the evolved non-linear dark matter density field in a cubic box with side length $250h^{-1}$ Mpc and the corresponding dark matter halo field as produced by a non-differentiable spherical overdensity algorithm.}
    \label{fig:L20_schematic}
\end{figure*}

To address these challenges of having to develop precise and accurate differentiable physics models, we propose a novel approach that optimizes cosmological data without requiring simulator differentiability. As strategy (ii) states, instead of accelerating the physical modelling through efficient approximations, we aim to accelerate the search process itself. This means that alternatives to full-scale simulations designed to reduce computational costs no longer need to be constrained by the requirement of differentiability, thereby establishing a new foundation for future model development. Our strategy is based on recent advances in \textit{learning to optimize} (L2O), as first proposed by \citet{Andrychowicz2016,Chen2016}, which uses machine learning to design and/or improve optimization methods \citep[also see reviews by][]{Chen2021a,Hospedales2022}. This so-called meta-learning or learning-to-learn, addresses the task of improving the learning algorithm itself. The machine-learning-based optimizer serves exclusively as a search engine, distinctly separated from the physics predictions, which are generated by state-of-the-art simulators, as illustrated in Fig.~\ref{fig:L20_schematic}. We thus address the common remark (see e.g \citet{Huertas-Company2022}) that physics knowledge should be in the physics models and not in the machine learning model, leading to higher interpretability and explainability. This also aligns with recent efforts in promoting the safe use of machine learning for research applications, as seen in e.g. \citet{Bartlett2024, Holzschuh2024}.

It is worth stressing that our approach differs conceptually from recent advances in generative modelling, including flow-based models and conditional diffusion models \citep{Legin2023,Holzschuh2024}. These aim to sample plausible initial conditions by learning the underlying distribution constrained by the data. This learning process relies on a training dataset consisting of an ensemble of initial and final condition pairs from simulations, a set also used in the training of field-level emulators of structure formation \citep{He2019,AlvesDeOliveira,Bernardini2020,Kaushal,Jamieson2022b, Jamieson2024}. In contrast to the aforementioned techniques, we aim to learn the complete dynamical behavior of the simulator by understanding how discrepancies $\Delta \ve{d}$ in data space relate to changes $\Delta \ve{x}$ in the initial conditions. In other words, while generative models and emulators use a neural network as a model (falling under strategy (i)), we use it as part of an optimization algorithm (strategy (ii)). As illustrated in Fig.~\ref{fig:L20_schematic}, our incremental approach then entails having the neural network propose an update direction $\Delta \ve{x}$ in the initial conditions space based on the current data prediction, progressively refining the initial condition reconstruction at each iteration. We highlight that unlike learned iterative reconstruction approaches \citep[see e.g.][]{Putzky2017,Adler2017} that have been explored in the context of large-scale structure analysis \citep{Modi2021}, our approach eliminates the need for the gradient of an explicit data likelihood term, enabling the use of arbitrarily complex data models without requiring further development cycles.

We name our algorithm \textit{Learning the Universe by Learning to Optimize} (\texttt{LULO}) and release this work as part of the Learning the Universe collaboration\footnote{\href{https://learning-the-universe.org/}{https://learning-the-universe.org/}}, which seeks to learn the cosmological parameters and initial conditions of the Universe by simultaneously leveraging machine learning, Bayesian forward modelling, and state-of-the-art simulations. The manuscript is structured as follows. In Section~\ref{sec:method}, we define the general problem and present our approach of using a neural optimizer to optimize black-box simulator functions, irrespective of differentiability. Section~\ref{sec:L20_cosmo} details the adaptation for cosmological initial conditions reconstructions, including training data design, choice of simulators with non-differentiable components, and training procedure for the neural optimizer. In section~\ref{sec:demonstration} we apply \texttt{LULO} and show the high quality of the reconstructed initial conditions. In section~\ref{sec:disc}, we further discuss the significance of non-linear modelling and data likelihoods, followed by a discussion on the computational cost and flexibility of \texttt{LULO} to incorporate arbitrary forward models. We end by presenting our conclusions in section~\ref{sec:conc}.
