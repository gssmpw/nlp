\section{Related work on learning to optimize}
\label{sec:DFO}

While many optimization algorithms exist, the no-free-lunch theorem \citep{Wolpert1997} states there is no universally best method for all objectives. Achieving improved performance requires task-specific algorithm design, which is time-intensive and demands significant manual effort to tune and validate pipelines, architectures, and hyperparameters. Learning to Optimize (L2O) has recently emerged as a promising approach to alleviate this challenge for complicated tasks by automating and accelerating the optimization procedure.

The foundation of L2O, as introduced by \citet{Li2016} and \citet{Andrychowicz2016}, is to replace traditional hand-crafted update rules with learned update rules, referred to as the \textit{optimizer}. Various approaches to using learned optimizers have been developed \citep{Chen2016, Li2017, Metz2020, Chen2021a, Zheng2022, Premont-Schwarz2022, Heaton2023}. These methods have, however, predominantly been applied to low-dimensional problems, whereas our task of inferring cosmological initial conditions involves jointly optimising millions of parameters of the primordial white noise field. In this work, we draw inspiration from these strategies to develop our high-dimensional \textit{Learning the Universe by Learning to Optimize} framework.

\section{Method: Gradient-free optimization}
\label{sec:method}
Our goal is to address the inverse problem of reconstructing the Universe's initial conditions from volumetric datasets, such as dark matter halo or galaxy count data. This task poses significant challenges due to the inclusion of complex, non-linear, and often non-differentiable cosmological simulations to casually connect the initial conditions with data. For this work, we explore handling non-differentiability by seeking a maximum likelihood estimate. 

After introducing the general inverse problem we want to solve in section~\ref{sec:problem}, we discuss in section~\ref{sec:ml_dfo} how to leverage machine learning to design a gradient-free optimizer capable of handling non-differentiable models and three-dimensional data. How to train such a neural optimizer to provide update directions in high-dimensional parameter spaces is then introduced in section~\ref{sec:update_direction}. 

\subsection{Problem Overview}
\label{sec:problem}
Our specific task is to learn a machine learning algorithm that can optimize a black-box function $f: \mathbb{R}^{n}\rightarrow\mathbb{R}$, given by $\ve{x} \mapsto f(\ve{x})$, where $\ve{x} \in \mathbb{R}^{n}$ is the input parameters. We assume that the function $f$ does not have a closed-form representation, is costly to evaluate, and does not allow the computation of gradients. This means we can only query the function at a given point $\ve{x}$ and obtain a response $y = f(\ve{x})$, but we have no information on its gradient or analytic form. 

For the purposes of this work, we can identify $f$ as the composition of two parts as
\begin{equation}
    f(\ve{x}) = \mathcal{L} \circ \mathscr{S} (\ve{x}) = \mathcal{L}(\mathscr{S} (\ve{x})),
\end{equation}
where $\mathscr{S}$ is an algorithm in the form of a simulation that translates between the input $\ve{x}$ and output $\mathscr{S}(\ve{x})$, and $\mathcal{L}$ is a likelihood evaluation between the output and the data $\ve
{d}$. 

We seek to find the maximum likelihood estimate $\ve{x}^*$ such that
\begin{equation}
    \ve{x}^*= \underset{\ve{x} \in \mathbb{R}^{n}}{\arg\max} f(\ve{x})
\end{equation}
with a limited number of function calls to $f$. The aim is to train an optimizer $h_{\mathrm{opt}}$ with parameters $w_{\mathrm{opt}}$, such that given a query point $\ve{x}_{t}$ at optimization step $t$ and corresponding query answers $y_{t}$, $h_{\mathrm{opt}}$ proposes the next query point $\ve{x}_{t+1}$ as
% under a budget constraint of $T$ queries, i.e., $t \leq T - 1$:
\begin{equation}
    \ve{x}_{t+1} = h_{\mathrm{opt}}(\ve{x}_{t}, y_{t}; w_{\mathrm{opt}}).
    \label{eq:sec_updates}
\end{equation}

\subsection{Incorporation of ML to learn black-box optimization}
\label{sec:ml_dfo}
In this work, we focus on a task categorized as \textit{learning to learn without gradient descent by gradient descent} \citep{Chen2016}, i.e. learning a parametric function such as a neural network \textit{by} gradient descent to learn how to optimize a black-box function $f(\ve{x})$. A key distinction to \citep{Chen2016} is that we bypass the requirement on back-propagation through $f(\ve{x})$ during the training phase, enabling our optimizer to be trained even when gradient information is not available. 

We seek to learn an alternative to the vanilla, and hand-crafted, gradient-based optimization scheme \citep[see e.g. review by][]{Ruder2016}, which follows
\begin{equation}
\label{eq:update}
\ve{x}_{t+1} = \ve{x}_{t} - \gamma\, \nabla_{\ve{x}_{t}} f(\ve{x}_{t})
\end{equation}
where $\gamma \in \mathbb{R}_+$ is the learning rate. For the data discrepancy we choose a voxel-wise mean-squared likelihood
\begin{equation}
    \mathcal{L}(\ve{x}_t) = \frac{1}{2}(\ve{d}-\mathscr{S}(\ve{x}_t))^\top\Sigma (\ve{d}-\mathscr{S}(\ve{x}_t)),
    \label{eq:mse}
\end{equation}
with $\Sigma$ chosen to an identity covariance matrix. As $\nabla_{\ve{x}_{t}} f(\ve{x}_{t}) \propto \nabla_{\ve{x}_{t}} \mathcal{L}(\ve{x}_t) \propto \nabla_{\ve{x}_{t}} \mathscr{S}(\ve{x}_t)$ and $\nabla_{\ve{x}_{t}} \mathscr{S}(\ve{x}_t)$ is not accessible, we introduce a machine learning neural network (NN) model to learn the mapping from the data discrepancy $\Delta \ve{d}_{t}$ to the update direction $\Delta \ve{x}_{t}$ for the initial conditions. 
In particular, we then have
\begin{equation}
    \nabla_{\ve{x}_{t}} \mathcal{L}(\ve{x}_t) \propto \underbrace{\underbrace{\left(\ve{d} - \mathscr{S}(\ve{x}_{t})\right)}_{\textstyle \Delta \ve{d}_{t}} \nabla_{\ve{x}_{t}} \mathscr{S}(\ve{x}_{t})}_{\textstyle \mathrm{NN}(\Delta \ve{d}_{t})=\Delta \ve{x}_{t}},
    \label{eq:grad}
\end{equation}
where $\Delta \ve{d}_{t}$  is computable, while $\nabla_{\ve{x}_{t}} \mathscr{S}(\ve{x}_{t})$ is not. Experiments on including $\ve{x}_{t}$ as an additional input to NN did not show any improvement and we leave further investigation to future studies. 
% We also experimented with using $\ve{x}_{t}$ as additional input to NN and did not find it to work better, leaving further investigations in this avenue to future studies.
% \LDcom{One might note that using $\ve{x}_{t}$ as additional input would provide additional information; in practice, this increases the sensitivity to out-of-distribution predictions and we therefore leave further investigations to future studies.} 
We also note there can be $n_d$ quantities in the data space that are informative about the initial state. Thus, in general, we have
\begin{equation}
    \Delta \ve{d}_{t} = [\Delta \ve{d}_{t}^1,\dots,\Delta \ve{d}_{t}^{n_d}].
    \label{eq:multi_data}
\end{equation}
We also leverage the exploration-exploitation philosophy central to evolutionary computation techniques by incorporating two competing terms: one deterministic and one stochastic \citep{Holland1975}. Similar to e.g. \citet{Fornasier2021,Riedl2023} we introduce the latter by a new term $f(\ve{x}_{t})\mathcal{N}_{t}$ to Eq.~\eqref{eq:update} for some function $f(\ve{x}_{t})$ that vanishes as we approach the ground truth and a Gaussian random vector $\mathcal{N}_{t}$ with zero mean and unit diagonal covariance matrix. We choose an $\ve{x}$-independent stochastic function $f(\ve{x}_{t}) = 0.1 \times 0.9^t$. While the deterministic term from the machine-learning optimizer guides the optimization towards promising regions, the stochastic term introduces a diffusion mechanism that injects randomness into the dynamics to increase the exploration across the search space. The combined update and query rule can now be defined as:
\begin{align}
    \ve{x}_{t+1} & = \ve{x}_{t} - \gamma_t \Delta \ve x_t = \ve{x}_{t} - \gamma_t [\mathrm{NN}(\Delta \ve{d}_{t}) + f(\ve{x}_{t})\mathcal{N}_{t}] 
    \label{eq:update_rule1} \\
    \Delta \ve{d}_{t+1} & = \ve{d} - \mathscr{S}(\ve{x}_{t+1}).
    \label{eq:update_rule2}
\end{align}
We next introduce how to determine the update direction $\mathrm{NN}(\Delta \ve{d}_{t})$ and the amplitude $\gamma_t$, which in the current implementation depends on the optimization step $t$ through $\Delta \ve x_t$. 

\subsection{Predicting update direction from data discrepancy}
\label{sec:update_direction}

Training our neural optimizer $\mathrm{NN}$ to learn the relationship between discrepancies in data space $\Delta \ve{d}$ to updates in the initial conditions $\Delta \ve{x}$ requires choosing an adequate neural architecture and loss function. 

\subsubsection{Neural architecture design}
We design the neural optimizer to a U-Net/V-Net architecture \citep{Ronneberger,Milletari2016}, similar to the one described in \citet{Jamieson2022b}, as these networks inherently capture fine spatial details in volumetric data. This is achieved through the use of convolutional operators working on multiple levels of resolution connected in a U-shape, first by several downsampling layers and then by the same amount of upsampling layers. The architecture is particularly suitable for accurately representing cosmological large-scale structures thanks to its sensitivity to information at different scales, which is maintained through the different levels as well as the skip connections between the down- and upsampling paths. To avoid information bottlenecks the number of filters were doubled during each downsampling step \citep{Szegedy2016,Ibtehaz2020}, resulting in a higher computational demand and increased performance. The convolutional network also naturally allows for multi-channel input, such as required when the data consists of several quantities as in Eq.~\eqref{eq:multi_data}. 

\subsubsection{Neural loss function}
To ensure convergence in iterative optimization, it is sufficient for updates to consistently move in the correct half-space, as this guarantees reaching the target for convex or quasi-convex problems (see Appendix~\ref{app:opt_guarantees}). We therefore choose a loss function that ensures that the predicted direction $\Delta \ve{x}_{\mathrm{pred}}=\mathrm{NN}(\Delta \ve{d})$ is maximally recovered. This is provided by the cosine similarity loss (CSL),
\begin{equation}
L_{\mathrm{csl}} = 1 - \cos(\Delta \ve{x}_{\mathrm{true}}, \Delta \ve{x}_{\mathrm{pred}}) = 1 - \frac{\Delta \ve{x}_{\mathrm{true}}^\top \Delta \ve{x}_{\mathrm{pred}}}{||\Delta \ve{x}_{\mathrm{true}}|| \cdot ||\Delta \ve{x}_{\mathrm{pred}}||},
\label{eq:cossim}
\end{equation}
which measures the similarity between the predicted and target vectors; a loss of zero would indicate perfect similarity. The choice of training data is also vital, but since it depends on the specific problem setup, we refer the reader to section~\ref{sec:training_data} for further details.

We have also explored a neural loss function based on mean squared error (MSE) alone and a hybrid loss combining MSE and CSL, but both alternatives resulted in poorer optimization performance. While MSE-based objectives ensure that the model effectively recovers the correct amplitude, they are prone to directional errors. Using the cosine similarity loss achieves a higher alignment, which is necessary for iterative optimization. 

The cosine similarity loss ensures accurate update directions but does not constrain the amplitude. To address this, we employ a systematic line search approach, iteratively adjusting the step size $\gamma_t \in \mathbb{R}$ along the predicted update direction $\Delta \ve{x}_t$. This process involves sequentially evaluating $\mathscr{S}(\ve{x}_t - \gamma_t\Delta \ve{x}_t)$, approximately four times for our set-up, to minimize the discrepancy to the data $\ve{d}$. We provide more details on the line search method in Appendix~\ref{app:line_search}. 

\section{Learning the Universe by Learning to Optimize (\texttt{LULO})}
\label{sec:L20_cosmo}

Our optimization strategy now needs to be adapted for the task of reconstructing cosmological initial conditions. This involves training the neural optimizer using a problem-specific dataset $\{\Delta \ve{x}, \Delta \ve{d}\}$ with $\ve{x}\mapsto \mathscr{S}(\ve{x})=\ve{d}$, where $\ve{x}$ represents the cosmic initial conditions as a white noise field, $\mathscr{S}$ is a black-box physics simulator that may not be differentiable, and $\ve{d}$ refers to the data, which can be either simulated or observed. Throughout this work, we use cubic three-dimensional fields of $128^3$ voxels both for the initial conditions and fields in data space. We discuss the process of generating the training data in section~\ref{sec:training_data}, the simulator model in section~\ref{sec:cosmo_sims}, and the specifics of the neural training procedure, including its performance, in section~\ref{sec:neural_training}.

\subsection{Training data generation}
\label{sec:training_data}

We extend the approach of \citet{Sarafian2020} who samples pairs of inputs and outputs of a black-box function and then averages the numerical directional derivatives over small volumes to obtain the mean-gradient of the true gradient. Our high-dimensional input consisting of $128^3 \sim 2\times10^6$ white noise field voxels does, however, not allow for perturbing one input parameter at the time. Instead, we simultaneously and stochastically perturb all voxels.

\subsubsection{Perturbing cosmological initial conditions}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/training_data.pdf}
    \caption{Learning the relationship between changes in the output of a cosmological simulator and corresponding changes in the initial conditions involves two key steps: generating training data (left) and training a machine learning model with a specific architecture (right). We perturb the initial conditions using an operator $\mathcal{P}$ that preserves the statistical properties of the Gaussian initial conditions, namely zero mean and unit variance. The resulting set of difference fields $\{\Delta \ve{d}\}$, where $\Delta \ve{d} = \ve{d}_a - \ve{d}_b$, serves as input to the neural optimizer model during training. The neural optimizer is tasked with predicting the corresponding update, $\Delta \ve{x} = \ve{x}_a - \ve{x}_b$, in the initial conditions. The chosen architecture is a convolutional V-Net model, which effectively handles structures and their correlations across multiple scales. To prioritize learning accurate update directions we use a cosine similarity loss during training.}
    \label{fig:training_data_schematic}
\end{figure*}

Let us assume $\ve{x}_a$ is a particular vector representing the three-dimensional initial white noise field of a cosmological volume. In our cosmological setting, it is drawn from a multivariate normal Gaussian with zero mean and diagonal unit co-variance. The pair of initial and final conditions ($\ve{x}_a,\ve{d}_a$) is created through our physics simulator $\mathscr{S}: \ve{x}_a \rightarrow \ve{d}_a$. Now we create a second pair, ($\ve{x}_b,\ve{d}_b$), by perturbing the initial conditions as
\begin{equation}
    \ve{x}_b = \ve{x}_a \cos{\phi}  + \ve{\epsilon} \sin{\phi}  \, ,
    \label{eq:perturbing_ics}
\end{equation}
where the phase variable $\phi \curvearrowleft U(0,b)$ is drawn from a uniform random distribution and  $\ve{\epsilon} \curvearrowleft \mathcal{N}(\ve{0},\mathds{1})$ is another random Gaussian white noise field. This procedure ensures that the perturbed $\ve{x}_b$ has the same unit variance as $\ve{x}_a$. We choose $b=2\pi$ which allows for $\ve{x}_b$ to not only be a perturbation to $\ve{x}_a$, but possibly a completely new or even a fully anti-correlated realization. We then evaluate the corresponding simulation at $\ve{x}_b$ and obtain the corresponding predicted data $\ve{d}_b$. The vector differences $\Delta \ve{x}$ and $\Delta \ve{d}$ can now be formulated as
% as can be trivially shown through the trigonometric identity. 
\begin{align}
    \Delta \ve{x} & = \ve{x}_a -\ve{x}_b\\ 
    \Delta \ve{d} & = \ve{d}_a-\ve{d}_b
\end{align}
We visualize this training generation process in Fig.~\ref{fig:training_data_schematic}. We construct the training ensemble by randomly creating $\ve{x}_a$ and $\ve{x}_b$ and repeating the above procedure. In this fashion, the set of $\{\Delta \ve{d}, \Delta \ve{x}\}$ pairs provide the training data, with $\Delta \ve{d}$ being the input and $\Delta \ve{x}$ the output. The network is then trained in a supervised fashion using the cosine similarity loss function (Eq.~\eqref{eq:cossim}). 

\subsection{Non-linear, non-differentiable cosmological simulator}
\label{sec:cosmo_sims}
To translate between the initial conditions $\ve{x}$ and the data space, we employ a high-fidelity $N$-body simulation coupled with a non-differentiable spherical halo finder. The simulations are performed with $128^3$ particles within a cubic volume with a side length of $250h^{-1}$ Mpc, i.e. at a Lagrangian resolution of $1.95h^{-1}$ Mpc. The resulting particles and haloes are discretized onto grids of $128^3$ elements using a cloud-in-cell mass assignment scheme to obtain the density contrasts. The dark matter overdensity and halo overdensity thus maintain a resolution of $1.95h^{-1}$ Mpc, which enables accurate representation of non-linear cosmic structures.

\subsubsection{$N$-body simulation}
$N$-body simulations are currently the most advanced numerical technique for modeling the full non-linear structure formation of the Universe \citep{Vogelsberger2020,Angulo2021}. In this work, we use the $N$-body simulation code \texttt{Gadget-IV} \citep{Springel2021}. From the white-noise field, the simulator initial conditions of particles are generated at $z=127$ using \texttt{MUSIC} \citep{Hahn2011} together with transfer function from Eisenstein \& Hu \citep{Eisenstein1998,Eisenstein1999}. The cosmological parameters used are $\Omega_\mathrm{m} = 0.3175$, $\Omega_\mathrm{b} = 0.049$, $h = 0.6711$, $n_\mathrm{s} = 0.9624$, $\sigma_8 = 0.834$, and $w=-1$ consistent with latest constraints by Planck \citep{Aghanim2018} and equivalent with the fiducial cosmology of the Quijote simulations \citep{Villaescusa-Navarro2020}. We compile and run the dark-matter-only and TreePM code version of \texttt{Gadget-IV} with $\mathrm{PMGRID}=256$. Although in theory differentiable, achieving this would require storing the full phase-space information at every time step of the simulator integrator—on the order of thousands—which memory-wise is infeasible.

\subsubsection{Dark matter haloes}
\label{sec:amiga}
To identify haloes in our $N$-body simulations, we use the spherical-overdensity-based algorithm \citep{Press1974,Warren1992,Lacey1994} as implemented by the Amiga Halo Finder (\texttt{AHF}) \citep{Gill2004,Knollmann2009}. We require at least $20$ particles per halo and adopt the \( M_{200\mathrm{c}} \) mass definition, which represents the mass enclosed within a spherical region where the average density is $200$ times the critical density. This definition is particularly stringent, as it necessitates precise resolution of the density profile down to small scales. We generate haloes in $10$ mass bins logarithmically separated between $1\times 10^{13}M_{\odot}h^{-1}$ and $2\times 10^{15}M_{\odot}h^{-1}$, which for our particle mass of $6.57 \times 10^{11} M_{\odot}h^{-1}$ ensures all found haloes are incorporated. We emphasize that \texttt{AHF} uses a non-differentiable algorithm to find haloes from the positions and velocities of simulated particles.

For the haloes, we want to retain some mass information during the assignment, as otherwise a low mass halo would be regarded as significant as a massive halo. We do this by applying a mass weighting to the final halo overdensity field $\boldsymbol \delta^{\mathrm{halo}}$ as
\begin{equation}
    \boldsymbol \delta^{\mathrm{halo}} = \frac{\sum_iN_if(M_i)\boldsymbol \delta_i}{\sum_i N_if(M_i)},
    \label{eq:overdensity_weighting}
\end{equation}
with $N_i$ being the number of haloes, $f(M_i)$ a function of the average mass of haloes, and $\boldsymbol \delta^{\mathrm{halo}}_i$ the over-density field in halo mass bin $i$. In this work, we choose to make use of two distinct mass functions. The first one is a mass-weighted field through
\begin{equation}
    f(M_i) = \frac{M_i}{1+\sqrt{M_i}}
    \label{eq:mass_weighting}
\end{equation}
with $M_i$ expressed in units of $10^{14}h^{-1}M_{\odot}$. The denominator in the mass-weighting was introduced by \citet{Seljak2009} to down-weight the most massive haloes, as their contribution is otherwise too dominant. While more optimal weighting schemes have been proposed \citep{Hamaus2010, Liu2021}, we adopt Eq.~\eqref{eq:mass_weighting} and leave further exploration for future studies. The second function we adopt is a halo count overdensity, defined by $f(M_i) = 1$ in Eq.~\eqref{eq:overdensity_weighting}, which is equivalent to directly creating an overdensity field from all haloes in one single wide mass bin. We choose the data to be these two halo fields
\begin{equation}
    \ve{d} = [\boldsymbol{\delta}^{\mathrm{halo}}_{\mathrm{mw}}, \boldsymbol{\delta}^{\mathrm{halo}}_{\mathrm{count}}],
\end{equation}
where $\boldsymbol{\delta}^{\mathrm{halo}}_{\mathrm{mw}}$ is the mass-weighted (mw) halo count overdensity field, and $\boldsymbol{\delta}^{\mathrm{halo}}_{\mathrm{count}}$ is the halo count overdensity.

\subsection{Neural training procedure}
\label{sec:neural_training}

In Fig.~\ref{fig:training_data_schematic}, we present a schematic overview of the training process for the neural optimizer. After generating the training data via perturbations with the cosmological simulator $\mathscr{S}$, the resulting pairs of difference fields are used as input (${\Delta \ve{d}}$) and output ($\Delta \ve{x}$) for supervised training of the neural network. For details on the computational cost of training data generation and neural network training, we refer the reader to section~\ref{sec:compcost}.

\subsubsection{Size of training data}
We generate $1000$ random initial conditions, run the simulation pipeline, and use Eq.~\eqref{eq:perturbing_ics} to perturb each initial condition $5$ times. For each simulation, we store the initial white noise field $\ve{x}$, the redshift $z=0$ snapshot from \texttt{GADGET-IV} as well as the halo catalog provided by \texttt{AHF}. In total, we create $5000$ difference fields for the initial conditions $\Delta \ve{x}$ as well as for the data $\Delta \ve{d} = [\Delta \boldsymbol{\delta}^{\mathrm{halo}}_{\mathrm{mw}}, \Delta \boldsymbol{\delta}^{\mathrm{halo}}_{\mathrm{count}}]$. 

During training, we further augment the training data set through rotations and flips, similar to the training procedures in \citet{He2019,Jamieson2022b}. Additional combinations of simulations from the set of $5000$ could have been generated. Tests of incorporating such pairs showed no improvement, suggesting that these fully uncorrelated fields do not add any additional information. 

\subsubsection{Training set-up}
We use the framework \texttt{map2map}\footnote{\href{https://github.com/eelregit/map2map}{github.com/eelregit/map2map}} for field-to-field emulators, based on \texttt{PyTorch} \citep{Paszke2019}, to train the neural network. We allocate most simulation pairs to training, reserving only $2\%$ ($100$ random pairs) for validation to monitor the cosine similarity loss and prevent overfitting. As each application of the trained neural optimizer in \texttt{LULO} will be performed on new initial conditions distinct from the ones used during training (hence effectively being the test set), we only split the data into a training and a validation set.  

Parallel processing of large volumes into sub-boxes is enabled. In particular, we crop the $128^3$ fields to $64^3$ due to memory requirements and use a batch size of $16$ over multiple \texttt{NVIDIA} A$100$ $40$GB Tensor Core GPUs. Through periodic padding of the input to the network by $48$ voxels on each side and the rotation and flip augmentations our implementation preserves both translation and (approximately) rotational symmetry. We use an initial learning rate of $4 \times 10^{-4}$ with the AdamW optimiser \citep{Loshchilov2019}, which is an extension to the Adam optimiser \citep{Kingma2014} using decoupled weight decay. We use a decay coefficient $\lambda=1\times 10^{-4}$ and parameters $\beta_1=0.9$ and $\beta_2=0.99$. At a plateau, if the loss is not reduced by a factor $0.999$ over $3$ epochs, we half the learning rate. To achieve high performance, we primarily monitor the cosine similarity loss for the training and validation sets, but the performance after incorporation within the full optimization process is also evaluated.

\subsubsection{Performance}
As an initial assessment of the trained model's performance, we evaluate it on the validation set, while acknowledging that the true evaluation will occur on the \textit{test set}, that is when the neural optimizer is applied in \texttt{LULO} to reconstruct the initial conditions. The cosine similarity ($1-\mathrm{loss}$) achieved on the validation set is $0.087 \pm 0.024$. Applying a Gaussian smoothing kernel with a scale of $10h^{-1}$ Mpc, corresponding to $k \sim 0.3h$ Mpc$^{-1}$, to both the predicted $\Delta \ve{x}_{\mathrm{pred}}$ and the true $\Delta \ve{x}_{\mathrm{true}}$, reveals a higher alignment of cosine similarity $0.944 \pm 0.036$ at these larger scales. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/trained_neural_optimzer.pdf}
    \vspace{-1.5em}
    \caption{The performance of the neural optimizer is evaluated through the cross-correlation between the predicted update direction $\Delta \ve{x}_{\mathrm{pred}}$ in the initial conditions and the ground truth $\Delta \ve{x}_{\mathrm{true}}$. The mean and standard deviation of the cross-correlation across all samples in the validation set are shown. A high alignment between the prediction and the truth down to scales of approximately $0.3h$ Mpc$^{-1}$ is obtained, with the correlation decreasing at smaller scales. This provides insight into which scales in the initial conditions are crucial for correcting data discrepancies at our particular resolution.} \label{fig:optimizer_performance}
\end{figure}

In Figure~\ref{fig:optimizer_performance} we quantify the output of the trained model across all scales using the cross-correlation between the predicted $\Delta \ve{x}_{\mathrm{pred}}$ and the true $\Delta \ve{x}_{\mathrm{true}}$. The higher correlation observed at larger scales, with a decrease at smaller scales, can be attributed to the fixed data resolution and the hierarchical nature of structure formation. Specifically, due to gravitational collapse, larger regions in the initial conditions evolve into smaller structures in the data \citep{Gunn1972}. As the resolution of the initial conditions matches that of the data in our setup, small-scale structures in the initial conditions collapse below the data resolution, where information cannot be accessed, as also discussed in e.g. \citet{Doeser2024}. This coupling also means that to explain non-linear scales in the data, linear scales in the initial conditions need to be accurately recovered. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{fig/recon_viz.pdf}

    \caption{The reconstructed initial conditions (bottom row; left), and the corresponding forward simulation output at $z=0$ in terms of dark matter overdensity (centre left), the Lagrangian velocity field (centre right), and mass-weighted halo count overdensity field (right) are compared with the ground truth (top row). The initial conditions have been smoothed at $10h^{-1}$ Mpc to reveal the visual alignment. A high correlation over these scales suffices for a highly accurate reconstruction in the data space, even at the smallest non-linear scales. In particular, the density and velocity fields, despite only being by-products and not used to constrain the initial conditions, are well reconstructed. Note that for the mass-weighted field, all haloes as found by the non-differentiable \texttt{AHF} algorithm have been included. Low-mass haloes, being the most numerous, are often not accurately reproduced because of the low number of particles per halo (see section~\ref{sec:halo_reconstruction}). The majority of haloes are nonetheless observed to be accurately recovered.}
\label{fig:reconstruction_viz}
\end{figure*}


\section{Initial conditions reconstruction with \texttt{LULO}}
\label{sec:demonstration}

To test \texttt{LULO} we start by generating a $M_{200\mathrm{c}}$ dark matter halo catalogue using the forward simulation pipeline given in section~\ref{sec:cosmo_sims}. We emphasize the non-linear and non-differentiable nature of our setup, which employs $128^3$ particles within a cubic volume spanning $250h^{-1}$ Mpc per side. Throughout the optimization process, we consistently use the same pipeline and use $128^3$ voxels to grid density, velocity, and halo fields. The ground truth initial conditions are not part of the training or the validation data sets, ensuring the neural optimiser's generalizability is tested. From the halo catalogue, we create a mass-weighted halo count overdensity and a halo count overdensity that together makes the data $\mathbf{d} = [\boldsymbol \delta^{\mathrm{true}}_{\mathrm{mw}},\boldsymbol \delta^{\mathrm{true}}_{\mathrm{count}}]$. 

After describing the initialization of the optimization process in section~\ref{sec:towards_optimum}, we showcase the high accuracy of the reconstructed initial conditions and the late-time dark-matter and halo fields in section~\ref{sec:reconstructions}. While most of this section focuses on quantifying the reconstruction quality using a single set of ground truth initial conditions and corresponding data, section~\ref{sec:various_data} presents additional examples of reconstructions with other ground truths.



\subsection{Initialization}
\label{sec:towards_optimum}
The initial guess for the initial conditions is chosen as a zero field. Given the high-dimensional space of $128^3 \sim 10^6$ voxels and the Gaussian nature of the true initial conditions, characterized by a zero mean and unit variance, a zero field is closer to the truth than a randomly generated white noise field. Due to the reduced initial cosmological power, the first simulation yields no haloes. This means the first input to the neural optimizer is $\Delta \ve{d} = \ve{d}$, from which the first search direction $\Delta \ve{x}$ is predicted. The first mapping initiates the optimization process of iteratively updating the initial conditions per Eqs.~\eqref{eq:update_rule1}–\eqref{eq:update_rule2} such that they, after forward simulation, increasingly match the data. The full monitoring of the subsequent optimization progress and iterative improvements is presented in Appendix~\ref{app:towards_opt}. 

\subsection{Reconstructed initial conditions}
\label{sec:reconstructions}
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/recon_powcross.pdf}
    \vspace{-1.8em}
    \caption{The quality of the reconstruction is quantified through the power spectra (top) and cross-correlation (bottom) of the reconstructed initial conditions (left), the dark matter overdensity (center), the mass-weighted halo count overdensity and halo count overdensity (right). In the residual panels, we display the transfer function, i.e. the square root of the ratio of the power spectra of the reconstructions with the ground truth. With initial conditions matching to within $10\%$ down to scales of $\sim 0.3h$ Mpc$^{-1}$, the final density field and the halo fields match down to highly non-linear scales of $\sim 2.1h$ Mpc$^{-1}$ and $\sim 2.7h$ Mpc$^{-1}$, respectively. The cross-correlations further demonstrate that high correlation in the initial conditions at large scales ($80\%$ at $\sim0.23h$ Mpc$^{-1}$) is sufficient for an $80\%$ correlation in the dark-matter overdensity and halo fields beyond scales of $1h$ Mpc$^{-1}$. This reflects the gravitational collapse of proto-structures into final structures as discussed in section~\ref{sec:reconstruction_powspec_etc}.}
    \label{fig:reconstruciton_powspec}
\end{figure*}

In Fig.\ref{fig:reconstruction_viz}, we visualize the reconstruction after $22$ optimization steps, equivalent to running the full simulation pipeline $96$ times, by which point the data discrepancy has converged (as shown in Appendix~\ref{app:towards_opt}). We show the initial conditions and the corresponding derived fields at redshift $z=0$, including the dark-matter overdensity field, one component of the Lagrangian velocity field (i.e., particles are gridded as their ordering in the initial conditions such that each voxel corresponds to a particle velocity), and the mass-weighted halo count overdensity. To visualize the correlation in the initial conditions we smooth the field over $10h^{-1}$ Mpc with a Gaussian kernel. 

The sole objective of the optimizer is to update the initial conditions such that the data discrepancy in the halo fields is reduced. In the mass-weighted halo count overdensity, we see that most haloes are accurately reconstructed in the correct regions. The underlying density and velocity fields, arising as by-products, are also well reconstructed. We note that in the dark matter overdensity field, regions of high density are reconstructed with greater accuracy. This outcome is expected, as these regions correspond to the more massive haloes, which provide stronger constraints. The Lagrangian velocity field, while presenting a particularly stringent test as individual simulated particle velocities are compared, demonstrates notable alignment, capturing large-scale coherent flows in the correct regions. 

\subsubsection{Reconstruction accuracy and temporal mode coupling}
\label{sec:reconstruction_powspec_etc}

Next, we quantitatively compare the reconstructed initial conditions and the corresponding evolved fields with their ground truths. We quantify the reconstruction through the spatial correlation of structures in terms of the power spectra and cross-correlation. We make use of the \texttt{PYLIANS3}\footnote{\href{https://github.com/franciscovillaescusa/Pylians3}{https://github.com/franciscovillaescusa/Pylians3}} package \citep{Pylians} to compute these quantities for the initial conditions, the dark matter overdensity, the mass-weighted halo overdensity and the halo overdensity. We show the result in Fig.~\ref{fig:reconstruciton_powspec} and also include the transfer function (the square root of the ratio of the power spectra of the reconstructions with the ground truth). For comparison and to highlight the non-linear scales involved in this reconstruction, we display the linear $z=0$ power spectrum computed with \texttt{CLASS} \citep{Blas2011}.

The transfer function of the initial conditions matches to within $10\%$ for scales $k \leq 0.33h$ Mpc$^{-1}$, after which it drops. The final density field still matches within $10\%$ to highly non-linear scales of $k = 2.09h$ Mpc$^{-1}$. Similarly, the halo fields stay within $12\%$ for all scales $k \leq 2.69h$ Mpc$^{-1}$. For comparison, the Nyquist frequency of the box is $k_N = 2.79h$ Mpc$^{-1}$, indicating that the agreement extends close to the resolution limit. We note that to explain the data at non-linear scales and obtain $80\%$ correlation in the dark-matter overdensity field, mass-weighted halo overdensity and halo overdensity to scales of $1h$ Mpc$^{-1}$, $1.28h$ Mpc$^{-1}$, and $1.15h$ Mpc$^{-1}$, respectively, the method requires high cross-correlations in the initial conditions at large scales ($80\%$ for $k\leq0.23h$ Mpc$^{-1}$). This also connects back to the cross-correlation in Fig.~\ref{fig:optimizer_performance} for $\Delta \ve{x}_{\mathrm{pred}}$ vs $\Delta \ve{x}_{\mathrm{true}}$, where we identified that linear scales in the initial conditions need to be updated to minimize the data discrepancy at non-linear scales.

This coupling of scales naturally follows from hierarchical growth of structures, with primordial objects collapsing through gravitational interactions into smaller regions \citep{Gunn1972}. The amount of information in the initial conditions at the field-level required to account for highly non-linear structures observed today was also discussed in \citet{Doeser2024}, where it was demonstrated that modelling of late-time dark-matter structures to non-linear scales of $2.79h$ Mpc$^{-1}$ is needed to recover initial conditions with a cross-correlation of $80\%$ to scales of approximately $0.35h$ Mpc$^{-1}$.

The transfer function and cross-correlation furthermore show that the mass-weighted halo overdensity is slightly more accurately recovered than the halo overdensity. This is to be expected, as the halo overdensity treats all haloes equally regardless of their mass. In contrast, the mass-weighted halo field incorporates mass information by assigning greater weight to more massive haloes through Eq.~\eqref{eq:mass_weighting}, making this halo field more informative and thus constraining. 

\subsubsection{Peculiar velocities of simulated particles}
By reconstructing the initial conditions, we gain access to the complete dynamical evolution of structures, including their velocities at non-linear scales. This is a valuable by-product of our method as peculiar velocities provide a critical tool for testing and differentiating cosmological models, including for example the growth rate of the large-scale structure \citep[see e.g.][]{Koda2014,Howlett2017,Boruah2020}. Reconstructions of non-linear peculiar velocities can further enhance the precision of such tests and are also vital for peculiar velocity corrections in local measurements of the expansion rate of the Universe, i.e. the Hubble constant \citep[e.g.][]{Mukherjee2021,Boruah2021,Kenworthy2022,Riess2022,Peterson2022}. The impact of peculiar velocities on the measurement of $H_0$ from gravitational waves and megamasers was studied in \citet{Boruah2021}.  

In Fig.~\ref{fig:dm_vel}, we show the power spectra, transfer function and cross-correlation of the Eulerian momentum field and the Lagrangian velocity field. For both fields, we average the power spectra and the cross-correlation over the three spatial components of the velocities. Since each particle in the simulation has the same mass, to obtain the momentum field we weigh the particle positions by their velocity when gridding the particles. For the Lagrangian velocity field, we order the particles according to their position in the $3$D grid of the initial conditions and store their velocity in each component. 

In both the Eulerian and Lagrangian case, we achieve a high level of accuracy, with transfer functions within $10\%$ down to $k=1.9h$ Mpc$^{-1}$ and $k=2.7h$ Mpc$^{-1}$, respectively. The cross-correlation is $80\%$ down to scales of $k=0.59h$ Mpc$^{-1}$ and $k=0.24h$ Mpc$^{-1}$, respectively. The Eulerian momentum is expected to show a higher cross-correlation, as it also depends on the positions of the particles. In contrast, the Lagrangian velocity field only involves the velocities of individual simulated particles, which is more challenging to reconstruct.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/recon_velocities.pdf}
    \caption{The reconstructed $z=0$ momenta and velocities of the individual simulation particles are quantified through the power spectra (top) and cross-correlation (bottom). For both the Eulerian momentum and Lagrangian velocity field, the power spectra and cross-correlations of the individual spatial components have been averaged over. The power spectra show high accuracy as displayed by the transfer function well into the non-linear regime. }
    \label{fig:dm_vel}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/recon_hmf.pdf}
    \vspace{-1em}
    \caption{The halo mass function shows that most of the $M_{200\mathrm{c}}$ halo masses are accurately recovered, with a slight over-estimation in the number of low-intermediate mass haloes and an under-estimation in the intermediate-high mass haloes. The mass resolution limit at $130$ particles, below which the mass is less accurately resolved, is displayed in red. Above the limit, the reconstruction is consistent with $\Lambda$CDM (yellow) within $3\sigma$, as derived from the $1000$ independent simulations in the training data. The residual panel shows the square root of the ratio between the reconstruction and the ground truth, and between the $\Lambda$CDM band and the truth.}
    \label{fig:reconstruction_hmf}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/recon_halovel.pdf}
    \caption{The reconstructed distributions of the halo velocity dispersion and the halo peculiar velocity of all haloes above the mass resolution limit. Velocity information was not included in the data, but the reconstructions in these properties still show a high degree of accuracy.}
    \label{fig:halo_vel}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/all_bispec.pdf}
    \vspace{-1em}
    \caption{The reduced bispectra in two distinct triangular configurations are used to assess the reconstruction accuracy: a configuration defined by fixed wave vector magnitudes as a function of the separation angle $\theta$ (left) and an equilateral triangle configuration with equal wave vector magnitudes to examine all scales (right). The three panels display the results for the final density field, the mass-weighted halo overdensity, and the halo overdensity, along with residuals comparing the reconstructed fields to the ground truth. We also show cosmic variance ($1\sigma$, $2\sigma$, and $3\sigma$ for $\Lambda$CDM) derived from the $1000$ unperturbed, and hence uncorrelated, training simulations (shaded regions). In particular, we note that the largest scales are dominated by cosmic variance. In all six bispectra shown, the reconstructions match the underlying theory and ground truth to within $5\%$ (left) and $10\%$ (right) over most angles and scales.}
    \label{fig:recon_bispec}
\end{figure*}

\subsubsection{Halo mass function}
\label{sec:halo_reconstruction}
The halo mass distribution, i.e. the number of haloes as a function of mass, serves as an important probe for testing the underlying cosmological model \citep[see, e.g.,][]{Jenkins2001a,Artis2021a,Stopyra2021}. Despite our method operating within a non-differentiable structure formation framework, our reconstruction demonstrates accurate recovery across most mass ranges. In Fig.~\ref{fig:reconstruction_hmf}, we show the halo mass function of the reconstruction compared with the ground truth and the theoretical $\Lambda$CDM prediction using \texttt{Tinker} \citep{Tinker}. We also leverage the $1000$ independent simulations from the training set to estimate the scatter within $\Lambda$CDM in each mass bin. In the study by \citet{Mansfield2020}, it was shown that at least $N=130$ particles are required to accurately describe the density profile and mass of $M_{200\mathrm{c}}$ dark matter haloes. For our chosen set of cosmological parameters and number of simulated particles, this corresponds to $8.53\times 10^{13} M_{\odot}h^{-1}$. We show this mass resolution limit with the red shaded region in Fig.~\ref{fig:reconstruction_hmf}. While this limit is crucial for recovering accurate density profiles, we note that it is a conservative choice for obtaining an accurate halo mass function. In our case, deviations from the theoretical prediction only begin at $\sim 4 \times 10^{13} M_{\odot}h^{-1}$. It is important to note that this arises from the current simulation setup and that our method can also be applied to higher-resolution simulations (see section~\ref{sec:compcost}).

Our reconstructed halo catalogue aligns well with the theoretical prediction and only deviates from the ground truth near $5\times 10^{14}h^{-1}M_{\odot}$. In total, we identify $2823$ haloes in the reconstructed field, with $432$ above the mass resolution limit. While this total is lower than the $3475$ haloes in the true field, it slightly surpasses the $404$ well-resolved haloes in the true field. The deviations identified are likely an effect of the specific data choice of halo fields (one mass-weighted and one counts only) and the loss function in Eq~\eqref{eq:mse}. We leave the investigation on more optimally selected mass weighting schemes and the inclusion of more mass bins, both shown beneficial for information extraction in \citet{Hamaus2010} and \citet{Liu2021}, to future studies. In section~\ref{sec:discuss_data_likelihood}, we further discuss the importance of choosing appropriate data vectors.

\subsubsection{Halo velocities}
In the current reconstruction, only the position and mass information of haloes were used in the data. As the full halo catalogue provided by \texttt{AHF} contain additional properties that have not been used as constraints, we can check how well these have been reconstructed. In Fig.~\ref{fig:halo_vel}, we show the velocity dispersion $\sigma_v$ as well as the peculiar velocity $V=(V_x^2+V_y^2+V_z^2)^{1/2}$ for all haloes above the mass resolution limit. While it is evident that the reconstruction contains slightly more haloes, it demonstrates a strong overall alignment with the true distribution in both velocity properties. To further increase the alignment between these distributions, one could consider adding velocity information to the data (also see the discussion in section~\ref{sec:discuss_data_likelihood}). 

\subsubsection{Higher-order statistics}
\label{sec:higher}
As the quality of cosmological data from large-scale galaxy redshift surveys improves, the need for statistical tools capable of extracting more non-linear information is needed. While the power spectrum is still an important probe for Gaussian information contained in the large-scale structure, higher $N$-order statistics are essential to extract non-Gaussian information. We therefore choose to compute the third order statistics, namely the bispectrum, also using \texttt{PYLIANS} \citep{Pylians}. 

The bispectrum captures spatial correlations in the matter distribution over various triangular configurations. In Fig.~\ref{fig:recon_bispec}, we display two different configurations of the reduced bispectrum $Q$, which has a weaker dependence on cosmology and scale, and is defined as
\begin{equation}
    Q\left(k_1, k_2, k_3\right) \equiv \frac{B\left(k_1, k_2, k_3\right)}{P_1 P_2+P_1 P_3+P_2 P_3},
\end{equation}
where $P$ is the power spectrum, $k_i$ are the three wave modes corresponding to the three sides of the triangle, and $B$ is the bispectrum. 

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/various_data.pdf}
    \vspace{-0.55em}
    \caption{We quantify the reconstruction quality from ten different optimizations with the transfer function (top) and cross-correlation (bottom). The data in each case is generated from new initial conditions and the corresponding halo catalogue, from which the initial conditions are reconstructed with \texttt{LULO} with $22$ optimization steps in all cases. We display the reconstruction from section~\ref{sec:reconstructions} in dashed black. Apart from minor fluctuations, we demonstrate accurate and consistent recovery of the initial conditions, density field and halo fields for all ground truths.}
    \label{fig:various_data}
\end{figure*}

Two different triangular configurations typically used \citep[see e.g.][]{Jamieson2022b,Doeser2024,Jamieson2024,Bartlett2024} are chosen to asses the reconstructions. First, we express the bispectrum through the magnitude of two wave vectors, chosen to $k_1=0.1h$ Mpc$^{-1}$ and $k_2=1.0h$ Mpc$^{-1}$ as this captures both large and small scale information, and the separation angle $\theta$, i.e. $Q\left(k_1, k_2, \theta \right)$. Second, the bispectrum is expressed through the magnitude of equal wave vectors forming an equilateral triangle, which enables analyzing the fields at all scales. We compute these two forms of the reduced bispectrum for the final over-density field, the mass-weighted halo overdensity and the halo overdensity. Notably, we remove all haloes below the mass resolution limit (see Fig.~\ref{fig:reconstruction_hmf}) before computing the bispectra.

We not only compare the reconstructed results with the ground truth but also assess the compatibility with the underlying cosmological model by displaying cosmic variance. Using the $1000$ simulations that were used to create the training dataset, we estimate the expected variance in the bispectra. Our reconstructions demonstrate consistency with these predictions as well as the ground truth across most scales. The largest discrepancies are observed at large scales, which is expected since cosmic variance is dominating at those scales. On the other hand, we achieve highly accurate recovery at small scales, particularly for the halo overdensity. In summary, these results demonstrate the capability of \texttt{LULO} to accurately reconstruct non-linear density fields and haloes, despite the challenges posed by a non-differentiable setup.


\subsection{Application on other ground truths }
\label{sec:various_data}

Up to this point, we have only applied \texttt{LULO} on a single reference simulation with unique initial conditions. In this section, we test the generalizability of our method by applying it to ten other reference simulations. We sample $10$ random white-noise fields and simulate the corresponding new halo catalogues, which are then used to generate the data in the form of the two halo fields. 

In Fig.~\ref{fig:various_data}, we quantify the reconstructions after $22$ optimization steps for each run (requiring between $93$ and $97$ simulations) in terms of the transfer function and cross-correlation. By also comparing the reconstructions with the results from the previous section, we demonstrate that our method accurately and consistently traverses high-dimensional space to estimate the initial conditions from different halo catalogues. We note that the reconstructions all show similar behaviour, such as slightly undershooting the halo overdensity field $\boldsymbol{\delta}_{\mathrm{count}}^{\mathrm{halo}}$ at $k \sim 0.2h$ Mpc$^{-1}$ and overshooting the mass-weighted halo overdensity $\boldsymbol{\delta}_{\mathrm{mw}}^{\mathrm{halo}}$ at $k \sim 1h$ Mpc$^{-1}$. 

In Fig.~\ref{fig:various_data_hmf}, we further show the reconstructed halo mass functions as compared with the respective truth and the $\Lambda$CDM scatter as estimated from the $1000$ training simulations (see section~\ref{sec:halo_reconstruction}). \texttt{LULO} consistently recovers halo mass functions that agree with the theory, while typically under-predicting the number of low-mass haloes ($M<0.4\times 10^{14}M_{\odot}h^{-1}$; below mass resolution limit) and high-mass haloes ($M>3\times10^{14}M_{\odot}h^{-1}$) and over-predicting the number of low-intermediate mass haloes. To improve the reconstruction quality, future work will focus on refining the architecture, training strategy, and data likelihoods (see discussion in section~\ref{sec:discuss_data_likelihood}). 

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/various_data_hmf.pdf}
    \caption{Reconstructions of the halo mass function using different ground truth data. The method tends to predict fewer low- and high-mass haloes and a higher number of intermediate-sized haloes than the respective ground truths. In most optimizations and for most masses, the final halo mass function agrees with the underlying theory prediction of $\Lambda$CDM (yellow band) above the mass resolution limit (red band).}
    \label{fig:various_data_hmf}
\end{figure*}