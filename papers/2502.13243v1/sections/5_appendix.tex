\appendix

\section{Optimization Guarantees}
\label{app:opt_guarantees}
For convex and certain quasi-convex high-dimensional problems, \citet{Golovin2020} showed that a random walk optimization strategy will converge within an $\epsilon$-ball around the optimum in a finite number of function evaluations. We draw inspiration from their geometrical perspective to claim that walking in the correct half-space is sufficient to reach the optimum. \\

\noindent
\textbf{Theorem}: If $f$ is a convex function and the search direction $d$ points to the correct half-space, then a more optimal solution can be found by performing a line search along $d$.

\noindent
\begin{proof}
Let $x$ be the current iterate, $d$ be the search direction, and $\gamma > 0$ be the step size. Consider the function $g(\gamma) = f(x + \gamma d)$, which is a convex function of $\gamma$ since $f$ is convex. By convexity, we have
\begin{equation}
g(\gamma) \leq g(0) + \gamma g'(0),
\label{eq:app_1}
\end{equation}
where $g'(0)$ is the derivative of $g$ at $\gamma=0$. Note that $g(0) = f(x)$ and $g'(0) = \nabla f(x)^\top d$, since $g$ is just the restriction of $f$ to the line $x + \gamma d$. Substit
uting these values into Eq.~\eqref{eq:app_1}, we get
\begin{equation}
f(x + \gamma d) \leq f(x) + \gamma \nabla f(x)^\top d.
\label{eq:app_2}
\end{equation}
If $d$ points to the correct half-space, then $\nabla f(x)^\top d > 0$. Thus, for $\gamma>0$, the second term on the right-hand side of Eq.~\eqref{eq:app_2} is positive, which means that $x+\gamma d$ is a better solution than $x$. Therefore, we can perform a line search along $d$ to find the optimal solution.
\end{proof}

\section{Approximate exact line search}
\label{app:line_search}
As each function evaluation in the line search can be computationally heavy – as in our case with a full simulation – we aim for a minimal amount of evaluations. We pick an effective line search algorithm called Approximate Exact Line Search (\texttt{AELS}), which is an adaptive variant of golden section search that comes with convergence guarantees \citep{Fridovich-Keil2020}. Applying \texttt{AELS}, as seen in Appendix~\ref{app:towards_opt} and Fig.~\ref{fig:opt_process} in particular, requires on average $4.3$ simulations per line search.

As part of the development, we also used the bisection bracketing method to benchmark and verify the slightly higher performance of the AELS method. We highlight that AELS works without specifying a search interval and, while being approximate, can be tuned to satisfy accuracy requirements. The hyperparameters that need to be chosen are the initial step size guess $T$, and the adjustment factor $\beta$. We note that having the line search method applied at each optimization step requires setting $T$ at each iteration $t$. We find empirically that $T_{t+1} = \gamma_t$ if $\alpha < 1$ (see Algorithm 2 in \citet{Fridovich-Keil2020}) and $T_{t+1} =(\beta^2 \gamma_t + T_t)/2$ if $\alpha > 1$ are effective choices, where $\gamma_t$ is the picked step size at iteration $t$. We initialize with $T_0=0.01$ and $\beta = 0.7$, which slightly differs from the $\beta = 2/(1+\sqrt{5})$ chosen in \citet{Fridovich-Keil2020}. 

Increasing the value of $\beta$ results in a lower exploration efficiency but a heightened level of sensitivity. We found that a higher $\beta$ together with the addition of a patience parameter $p_{\mathrm{LS}}$, which stops the line search if a $f_{\mathrm{LS}}$ fractional improvement of the data discrepancy has not occurred for $p_{\mathrm{LS}}$ iterations, worked better for our purposes. Similarly, between the optimization steps, we introduce $p_{\mathrm{OPT}}$ and $f_{\mathrm{OPT}}$ that checks for fractional improvements. We picked $p_{\mathrm{LS}}=3$, $p_{\mathrm{OPT}}=5$, and $f_{\mathrm{LS}}=f_{\mathrm{OPT}}=0.001$. 

\renewcommand{\thefigure}{ C\arabic{figure}}
\begin{figure*}
    \centering
    \includegraphics{fig/plot_optimization_mwnw_to100.pdf}
    \vspace{-1em}
    \caption{The data discrepancy, as measured by the mean squared error $\mathcal{L}(\ve{x}_t)$ defined in Eq~\eqref{eq:mse} between the data and the forward simulated initial conditions in terms of the mass-weighted halo overdensity $\boldsymbol{\delta}_{\mathrm{mw,opt}}^{\mathrm{halo}}$ and the halo overdensity $\boldsymbol{\delta}_{\mathrm{count,opt}}^{\mathrm{halo}}$, is decreasing as a function of optimization steps. At each optimization step (upper x-axis), the line search requires a certain number of simulations (lower x-axis). The non-monotonic decrease in the data discrepancy results from the line search, which requires a few simulations to find the step size in the given search direction. In monitoring the improved reconstructions, we show for a few steps the initial conditions (upper left), and the corresponding forward simulation output at $z=0$: dark matter overdensity (upper right), Lagrangian velocity field (lower left), and mass-weighted halo field (lower right). Note the optimization is the same as in section~\ref{sec:reconstructions} but here with new 2d-slices. A visual comparison with the data $\mathbf{d}$ (right-most inset) shows similarities already after one and two optimization steps. Although convergence is approached after $\sim 8$ steps, we let it fully converge and run $22$ optimization steps ($96$ simulations).}
    \label{fig:opt_process}
\end{figure*}

\section{Towards optimum}
\label{app:towards_opt}

In Fig.~\ref{fig:opt_process} we monitor the mean squared error of the mass-weighted and the count-weighted halo fields with their respective ground truths as a function of the number of simulations (lower x-axis) and the number of optimization steps (upper x-axis). The figure displays the optimization process leading to the reconstruction analyzed in seciton~\ref{sec:reconstructions}. On average, each line search within one optimization step requires $5$ simulations. Only $4$ optimization steps, corresponding to $20$ simulations, are required to reduce the error, after which further optimization only marginally improves the reconstructions. 

In the inset figures in Fig.~\ref{fig:opt_process}, we display four different fields and their improvement as a function of optimization steps. In the upper left, we show the initial conditions smoothed over $5$ voxels ($\sim10h^{-1}$ Mpc) with a Gaussian kernel. Because of temporal mode coupling between large scales in the initial conditions and small scales in the data, to be discussed in more detail in section~\ref{sec:reconstructions}, a high correlation between the reconstruction and the truth down to roughly $10h^{-1}$ Mpc is sufficient to obtain forward predictions that match the data at non-linear scales of $\sim 1h^{-1}$ Mpc. This is seen in the lower right plot, where we show the mass-weighted halo field. Notably, the sole objective of the optimizer is to update the initial conditions such that the data discrepancy for the halo fields is reduced. As the number of optimization steps taken increases, the visual alignment increases.

\subsection{Initialization and generalizability}
While we leave tests of other initialization strategies to future studies, one may note that initializing from a zero field results in the first mapping being $\Delta \ve{x} = \ve{x}_{\mathrm{true}}$ and $\Delta \ve{d} = \ve{d}$. This setup effectively tasks the neural optimizer with estimating the true initial conditions directly from the data. This resembles e.g. \citet{Shallue2023,Jindal2023,Chen2023,Legin2023} who use neural networks to map non-linear data to the initial density field. These predictions could be used as the initial guess in our framework. We stress, however, the fundamental difference between these neural models and our neural optimizer, which does not aim to make any predictions directly but to propose a search step in the initial conditions space given the data discrepancy. This enables always testing the proposed initial conditions within the full physics model and computing the new data likelihood, before providing the next update direction. 

We note that zero initialization falls outside the range of the training dataset. The neural optimizer has only been trained on data differences originating from pairs of initial conditions each having unit variance. While the power mismatch is addressed through the line search, the neural optimizer determines the update direction, a task it performs with high accuracy even in these cases. It thus demonstrates a general understanding of the underlying dynamical behaviour, which results in efficient searches through high-dimensional spaces. 