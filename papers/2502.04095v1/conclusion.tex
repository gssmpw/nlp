

\chapter{Conclusion and Future Work}

\section{Conclusion}

This research was driven by a need to support companies in navigating IFRS reporting standards for the preparation of compliant sustainability reports. Two major gaps were identified in this domain that impeded the creation of an AI assistant for this task: (1) the lack of a high-quality question-answer dataset, and (2) the lack of a RAG system customised for question-answering in this domain. This work addresses both of these gaps. \\

This project utilises the official reports published by IFRS to construct, from scratch, a dataset and two question-answering systems in the sustainability reporting domain. A novel LLM-based pipeline is developed to generate and evaluate domain-specific synthetic question-answer pairs. This pipeline is used to produce a dataset of 1,063 diverse, well-referenced, domain-specific questions and answers. \\

The dataset was used to evaluate RAG methods for a domain-specific chatbot and design two custom architectures that improve on standalone RAG methods. Specifically, the dataset was used to fine-tune a model for question-answering and evaluate all the methods tested on various levels of complexity. The dataset was also used to train a range of industry classifiers, though ultimately a prompt-based classifier displayed the best performance. The prompt-based industry classifier and fine-tuned generation model were incorporated into two pipelines - a RAG-based pipeline and a fully LLM-based architecture. The fully LLM-based architecture performed best, although both implementations showed significant improvements in performance over the baseline. \\

Nevertheless, the methods presented in the project present some limitations:

\begin{itemize}
    \item \textbf{QA generation prompts:} All prompts in the project are designed using generalisable techniques such few-shot and chain of thought prompting, and templates are used throughout to act as LLM ``functions" to standardise the overall pipeline. Nevertheless, to obtain high quality questions for the sustainability reporting domain, the details in the prompts had to be carefully crafted and are heavily related to the underlying context of the chatbot. To adapt the QA generation pipeline to other specialised domains, the prompts will likely need to be edited to accommodate domain-specific concerns.
    \item \textbf{QA evaluation:} The QA evaluation methods are constructed using LLMs, which may fail to evaluate questions at the human level or may themselves hallucinate. Question quality was checked manually throughout the project as methods were iterated, though this was not done on all 1,063 questions presented in the final dataset.
    \item \textbf{QA post-processing:} Due to cost considerations, the custom post-processing functions - quality improvement and question generalisation - were implemented qualitatively on a small sample rather than applied to the entire dataset. Nevertheless, the generated dataset displays high quality even before post-processing.
    \item \textbf{Chatbot evaluation:} The knowledge of the RAG system was tested and the chatbot was constrained to its specific domain. However, the chatbot itself has not been evaluated through the lens of user experience and long-context dialogue.
\end{itemize}


\section{Future Work}

Future work can build on the contributions of this project in several ways to address the limitations identified as well as to expand on the methods proposed. The following work is proposed.
\begin{itemize}
    \item \textbf{Adapting methods to different domains:} The designed architecture can be adapted to different domains by editing the prompts where domain-specific context is most relevant.
    \item \textbf{Human QA evaluation:} Human evaluation on the question dataset can be conducted to verify the quality of the generated questions and answers. This can be done by domain experts and/or people with an understanding of IFRS sustainability reporting standards. Evaluation must be done systematically, with each question being graded on a set of quality metrics.
    \item \textbf{Mass post-processing:} The investigation of the custom post-processing functions showed that they are effective, and would merit an application across the entire dataset on questions that do score below the threshold on quality metrics.
    \item \textbf{Human chatbot evaluation:} Human evaluation of the chatbot can be conducted. This would involve defining the key metrics on which the chatbot must perform well. Some examples include usefulness, context awareness, factual accuracy, and politeness, measured on Likert or other numerical scales to increase comparability across human evaluators. An alternative to human evaluation of the chatbot is to prompt other LLMs to act as typical users and instruct them to rate their experience from this perspective.
    \item \textbf{Cross-industry multi-query:} A custom RAG pipeline can be designed to leverage multi-query transforms only on cross-industry questions, where it displays advantages over the baseline.
    \item \textbf{Leveraging the QA dataset:} The dataset can be used to train and evaluate further advanced RAG systems and other models within the sustainability reporting domain. 
    \item \textbf{Generating more questions:} The QA generation and evaluation pipeline can be employed to generate questions within this domain on a larger scale.
\end{itemize}
