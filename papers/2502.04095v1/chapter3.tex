\chapter{Synthetic Question-Answer Dataset }

This chapter presents the data and methods used to arrive at the final question-answer (QA) generation pipeline that is used to create a synthetic dataset of 1,063 QA pairs for evaluation of the RAG-based knowledge assistant. We start by introducing the domain and presenting the data collection and preprocessing pipeline. Next, we describe the methodologies adopted to generate and filter questions. First, we outline the requirements of our synthetic QA dataset and present the dataset structure created to meet these requirements. Then, we motivate our QA generation approach based on existing techniques, and describe the iterative development of the LLM architectures used, leveraging prompt engineering techniques like Chain of Thought (CoT) reasoning and Few-shot learning. Subsequently, we present a custom evaluation framework for QA quality, using both standard and novel metrics, and introduce three post-processing functions to improve the quality of the dataset. Finally, we present the results of our different question generation methods on the evaluation framework, define the final QA generation pipeline, and present a new dataset of QA pairs to be used for RAG training and evaluation in the sustainability reporting domain.




\section{Data collection and preparation}\label{data_collection}

This work adopts Sustainability Reporting as the knowledge domain, using data published by the International Financial Reporting Standards (IFRS) \cite{ifrsorgc9:online}, which provides international sustainability-related standards used by over 29,000 listed companies globally \cite{ifrs_companies}. These standards undergo periodic review and have recently been modified from previous versions in an effort to unify global reporting standards. The standards comprise 72 documents over 975 pages as described below:

\begin{enumerate}
    \item \textit{IFRS S1 General Requirements for Disclosure of Sustainability-related Financial Information (with an additional `accompanying guidance' document)}. This is the baseline financial-related reporting standard with the objective to ``require an entity to disclose information about its sustainability-related risks and opportunities that is useful to primary users of general purpose financial reports in making decisions relating to providing resources to the entity" \cite{ifrs_s1}. 
    \item \textit{IFRS S2 Climate-related Disclosures (with an additional `accompanying guidance' document)}. This is the baseline climate-related reporting standard with the objective to ``require an entity to disclose information about its climate-related risks and opportunities that is useful to primary users of general purpose financial reports in making decisions relating to providing resources to the entity" \cite{ifrs_s2}.
    \item \textit{Appendix Bâ€”Industry-based Disclosure Requirements}. This is the appendix to IFRS S2, comprising 68 separate industry-specific documents grouped into 11 different industry categories. These documents ``set out the [draft] requirements for identifying, measuring and disclosing information related to significant climate-related risks and opportunities associated with particular industries" \cite{ifrs_appendix}. It is important to note that the industry classification of companies is not always clear - IFRS states that entities can be ``associated with an industry through [their] business model, economic activities, production processes or other actions" \cite{ifrs_appendix}.
\end{enumerate}

The documents were downloaded in PDF format from the IFRS website at \cite{ifrs_s1,ifrs_s2,ifrs_appendix}, and the reports were manually examined to understand their structures and content. It was noted that, while S1 and S2 have custom structures, all Appendix reports follow the same structure. They start with identical title, disclaimer, and introduction pages, which are deleted from all but one report to avoid repetition. The material content in each report starts with an `Industry Description' section, followed by a `Sustainability Disclosure Topics \& Metrics' section. The latter contains one or two tables: all reports contain `Table 1. Sustainability Disclosure Topics \& Metrics', and most, though not all, contain `Table 2. Activity Metrics'. The columns of these tables are standardised across all reports, displaying the topics, with respective metrics, that must be reported for the particular industry, as well as as metric codes, categories, and units of measurement. The rest of the report contains detailed information on the topics and metrics outlined in the tables, structured as one section per topic, with subsections pertaining to each topic metric. \\

Finally, it is important to note that, having recently undergone modifications from previous versions, the standards contain many sections of crossed-out text that have been removed from prior versions, often followed by sections of underlined text that have been added. Crossed-out and newly added text is present in both tables as well as free-text sections. As outlined in Section \ref{data_processing}, crossed-out text is removes from the data, and underlined text is kept.

\subsubsection{Data preprocessing} \label{data_processing}

Leveraging the structured nature of the IFRS reports, a custom multi-modal pipeline (Figure \ref{fig:pdf_parse}) was designed to parse the text and table content into markdown format, preserving the hierarchy of report sections. A multi-modal model was chosen for parsing PDFs as it can accurately differentiate between crossed-out and underlined text chunks in the reports, and only extract the underlined ones. Other PDF parsing packages and OCR techniques were tested and were unsuccessful at accurately extracting only the underlined text and not the crossed-out text. The prompt used for text and table extraction is displayed in Appendix \ref{lst:pdf_parse_prompt}. The prompt is designed with particular attention towards crossed-out text, as well as to preserve the hierarchical structure of the report sections. \\

The multi-modal parsing pipeline follows four steps:

\begin{enumerate}
    \item PDF to image conversion: Each page of the PDF is converted into an image using a zoom factor of 2.5 to increase the resolution of the images for more accurate content parsing. The PyMuPDF package is used for this step.
    \item Table presence check: Each image is passed through a multi-modal model which checks if a table is present on the page. The model used is Claude 3.5 Sonnet.
    \item Conditional processing: Pages without tables are inserted into a text extraction prompt which, combined with a JSON schema for structuring the output, is passed through a multi-modal model to extract the text content and associated page number into a JSON file. Pages with tables present are inserted into a table and text extraction prompt, along with a more complex JSON schema, to extract tabular data with custom table columns (alongside any other text content and the page number). 
    \item JSON to markdown conversion: The JSONs for each page are combined into a structured markdown file that maintains section headers and hierarchies, and isolates the tables from the free-text content.
\end{enumerate}

The resulting 72 markdowns are saved to be used in QA generation as well as in the RAG system development described in Chapter 4.


\begin{figure}[H]
    \centering
    \includegraphics[width = 0.8\textwidth]{figures/pdf_parse.pdf}
    \caption{Multi-modal PDF to markdown parsing pipeline.}
    \label{fig:pdf_parse}
\end{figure}




\newpage
\section{Methods}



\subsection{QA design}\label{qa_design}

The ultimate aim of the knowledge assistant is to answer user queries correctly based on the source data. To ensure this, the RAG system must be evaluated on questions that are (1) representative of real user queries and (2) useful for testing the capabilities of the RAG system. The QA dataset is designed based on these two principles.\\

To design questions that are representative of real-life queries, several sources were used. First, the contents of the IFRS reports were manually analysed to identify key topics that are often repeated or heavily emphasised, as well as smaller but critical details that a person likely wants to know. Second, a range of real corporate sustainability reports from SASB (now IFRS) reporting companies were downloaded from \cite{sasb_reports} and studied to identify the ways in which companies use IFRS information to prepare sustainability reports. In most cases, companies prepare tables that explicitly provide the metrics required by IFRS, meaning they are very likely to ask detailed questions about metrics. Third, inspiration for typical user queries was taken from FinQA \cite{chen2022finqa} and Financebench \cite{islam2023financebench} - two prominent expert-annotated QA datasets that are based on corporate reporting. Both the questions themselves as well as their preparation methods were analysed. From the actual Financebench and FinQA question datasets, 20 questions were selected and adapted to the Sustainability Reporting domain. From the Financebench preparation methodology, the idea of preparing ``domain-relevant questions" that are ``generically relevant" to the user's task was adopted. In the case of Financebench, the user's task is defined as the financial analysis of a publicly traded company \cite{islam2023financebench}, whereas the user's task defined for this project is the use of IFRS standards to prepare corporate sustainability reports. It is important to note that these methods were used by experts to manually create questions, whereas this project adapts these methods to LLMs using prompt engineering. \\

Based on these sources, a list of 10-18 user-representative sample questions was created for each question category - Multiple-choice (MCQ) Local, MCQ Cross-industry, Free-text Local, Free-text Cross-industry - to be used for few-shot prompting techniques in the question generation pipeline as discussed in Section \ref{QA_gen}. Six examples are displayed in Listing \ref{lst:qa_structures_example} below, and the full list of sample questions can be found in Appendix \ref{lst:qa_structures}.

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:qa_structures_example},caption={Select user-representative question structures}]
Local single-hop:
"Can you provide for me the unit of measure I should use for the xxx metric in the xxx industry?",
"On what page can I find details about xxx for the xxx industry?"

Local multi-hop:
"Can you provide for me the unit of measure I should use for each of the metrics for 'xxx' topic(s) in the xxx industry?",
"What is the category of xxx metric in the xxx industry and how should this metric be calculated?"

Cross-industry single-hop:
"What topics should I report on for xxx and xxx (and xxx...) industries?",
"Give me all the metrics from Table 1 that are in the category 'Discussion and Analysis' for xxx and xxx (and xxx...) industries."

Cross-industry multi-hop:
"Give me the codes for all the 'Quantitative' metrics in Table 1 for xxx and xxx (and xxx...) industries.",
"How does the calculation of xxx-related metrics differ between xxx and xxx (and xxx...) companies?"
\end{lstlisting}


To design questions that are useful for testing the RAG system, questions were generated on three dimensions: question span, question complexity, and answer structure.\\

\textbf{1. Question span:} testing the RAG's ability to retrieve information from and reason over multiple source documents. The options are:
\begin{itemize}
    \item Single industry (`Local'): Questions that cover only one industry-related source document. They are designed to test retrieval abilities from one document.
    \item Two industries (`Cross-industry'): Questions that cover a pair of industries, in a way that compares/contrasts or gathers information from both industries. Industry pairs were chosen by an LLM prompted to pair up industries that would realistically be compared based on the industry descriptions. These form a key part of the evaluation dataset, as many IFRS reporting companies span different industries, and are designed to test the RAG's ability to identify the correct documents and source the relevant chunks from both.
\end{itemize}

\textbf{2. Question complexity:} testing the RAG's ability to reason through a question and retrieve necessary data. The options are:
\begin{itemize}
    \item Single-hop: Questions where the answer can be extracted directly from the document. They are designed to evaluate the RAG system's ability to answer factual questions based on small snippets of the source document.
    \item Multi-hop: Questions that require information from multiple sections of one or more reports, or involve multiple logical steps, to be answered. They are designed to evaluate the RAG system's ability to reason in multiple steps and extract multiple information chunks to answer a question.
\end{itemize}

\textbf{3. Answer structure:} testing the RAG's knowledge as well as its ability to answer a question correctly and coherently. The options are:
\begin{itemize}
    \item Multiple-choice: Questions with five answer options and only one correct answer option. They are designed to test the accuracy of the RAG system in an objective way that does not rely on NLP metrics.
    \item Free-text: Questions that have a free-text answer. They are designed to test the accuracy and coherence of the RAG system using traditional NLP metrics such as BLEU and ROUGE.
\end{itemize}

These categories are combined into eight different question configurations: local single-hop multiple-choice questions (MCQ), local multi-hop MCQ, local single-hop free-text questions (FTQ), local multi-hop FTQ, cross-industry single-hop MCQ, cross-industry multi-hop MCQ, cross-industry single-hop FTQ, and cross-industry multi-hop FTQ. 


\subsection{QA Generation}\label{QA_gen}

The overall aim of this project is to generate questions that are as specific as possible to minimise the chance of hallucination, and then generalise the phrasing of the questions in post-processing. As discussed later in this chapter, LLMs tend to generate vague questions based on a given context. As such, steps are taken to guide the LLM to create questions and answers that are highly specific to the domain to maximise their faithfulness and relevance to the text. Post-processing functions are then designed to enable the adjustment of highly specific questions to better mimic natural human questions while maintaining their underlying materiality.\\

The question generation and evaluation pipeline is defined in Algorithm \ref{alg:final-question-generation}. The steps of the pipeline are as follows:
\begin{enumerate}
    \item For each question setting (or type) described in Section \ref{qa_design}, a certain number of questions are generated using LLM prompting. Three techniques are tested for this step: a baseline prompt, a Chain-of-Thought (CoT) prompt, and a prompt that uses both CoT and provides few-shot examples of sample questions.
    \item Each question is then evaluated using a number of custom metric implementations. The evaluation metrics designed and employed are faithfulness, relevance, and specificity. Free-text questions are also evaluated using BLEU and ROUGE-L.
    \item Questions that fail to pass certain evaluation thresholds may be improved in post-processing using a quality improvement function.
    \item Questions that are ``too" specific may be passed through a generalisation function to make them sound more vague while maintaining their factfulness.
    \item A similarity filter is used to exclude question duplicates.
    \item MCQs are checked to ensure they have a `single best answer' (SBA). Questions that fail this test are discarded.
\end{enumerate}
\begin{algorithm}[H]
\caption{Final Question Generation Pipeline}
\label{alg:final-question-generation}
\begin{algorithmic}[1]
\Require Set of industry markdowns $M$, Number of questions $N$, Question types $T$ (free-text, MCQ)
\Ensure Set of quality question-answer pairs $Q$
\State $Q \gets \emptyset$
\For{each question type $t \in T$}
    \For{$i = 1$ to $N$}
        \State Select $k \geq 1$ industry contexts $C = \{c_1, \ldots, c_k\}$ from $M$
        \State $q \gets \Call{GenerateQAPair}{C, t}$ 
        \Comment{using Claude 3.5 Sonnet}
        \State $metrics \gets \Call{EvaluateMetrics}{q}$
        \If{$\exists m \in metrics : m < \theta_m$} \Comment{$\theta_m$: metric threshold}
            \State $q \gets \Call{QualityImprovement}{q, metrics}$
        \EndIf
        \If{$q$ contains industry name}
            \State $q \gets \Call{QuestionGeneralization}{q}$
        \EndIf
        \State $is\_similar \gets \Call{SimilarityFilter}{q}$
        \If{$\lnot is\_similar$ and $\forall m \in metrics : m \geq \theta_m$}
            \If{$t$ is multiple-choice}
                \State $has\_single\_best\_answer \gets \Call{SingleBestAnswer}{q}$
                \If{$has\_single\_best\_answer$}
                    \State $Q \gets Q \cup \{q\}$
                \EndIf
            \Else
                \State $Q \gets Q \cup \{q\}$
            \EndIf
        \EndIf
    \EndFor
\EndFor
\State \Return $Q$
\end{algorithmic}
\end{algorithm}
To design the prompt architecture for QA generation, three approaches were tried. Firstly, a baseline prompt was used to generate questions based on the markdown content as a baseline. To reduce the chance of hallucination, a second approach was adopted using a more sophisticated prompt with Chain-of-Thought (CoT) reasoning. Finally, to ensure that the questions are representative of real user queries, the last prompt-based approach incorporates few-shot learning into the prompt. \\

All questions are structured to have the question, the answer, the reference text that the question and answer is based on, and the pages where the reference text is sourced from. Multiple-choice questions also have answer options A through E, where only one option is the correct answer. 

\subsubsection{Baseline Prompting}

In the baseline method, question-answer pairs are generated with an LLM function call using a prompt that gives the LLM context of the task at hand - generating questions based on the markdown from the perspective of a company preparing sustainability reports. However, it is given no guidance on how these questions should be generated. The prompt takes as inputs the markdown and the question complexity: single-hop or multi-hop. For questions comparing different industries, the prompt asks the LLM to generate questions based on all markdowns given. The prompt used is shown in Listing \ref{lst:naive_prompt}.\\
\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:naive_prompt},caption={Baseline prompt for `local' multiple-choice question generation}]
system = "You are a sustainability reporting expert that helps companies draft their corporate sustainability reports using the IFRS reporting standards. You are preparing some questions that a company might ask while preparing its sustainability report, for which the answer can be taken from the context given in the markdown below."

user =  f"""
Here is the markdown content:
{markdown_content}

Based on the markdown content, generate {n} multiple-choice questions of type {qa_type}.

Generate {n} QA pairs for the specified type and return them using the provided schema."""
\end{lstlisting}

LLM `tool use' is used to ensure the output is in the required format containing the question, answer, reference text, and pages (and answer options for MCQs). A JSON schema is used as the tool to structure the output, and this is shown in Appendix \ref{lst:mcq_schema}.



\subsubsection{Chain-of-Thought Prompting}

In the second approach, several additions are made to the prompt to reduce the possibility of hallucination and improve the style of questions generated. The main addition is CoT reasoning to guide the LLM on the steps to take to generate questions, adapting traditional question-generation methods. Specifically, the LLM is prompted to:

\begin{enumerate}
    \item Select a list of one or more sentences/snippets of the markdown content that can be used to form an answer to a question. This will form the reference text.
    \item Write a question that requires the reader to understand the content of the selected text to answer correctly. The question should be based only on the selected text and should not require any additional information.
    \item Write five answer options, one of which is correct and the other four are incorrect. The correct answer should be complete and taken verbatim from the selected section(s) of the markdown content.
\end{enumerate}


This process adapts the traditional question-generation methods to LLMs by first guiding it to select the text that serves as the response before generating the QA pair. This adaptation has the advantage that the LLM has full context of what the text is about and is thus able to jointly select pieces of reference text that are not only most relevant to the specific domain but also would make the ``best" questions. Furthermore, it can do so with a reinforced understanding of its role as a human that wants information specifically for a company preparing sustainability reports. \\

Further additions to the prompt for MCQs include making explicit that the questions should have a single best answer, which should require very specific information from the context to be answered, as well as ensuring that the other answer options are not so obviously wrong that they can easily be discarded. The prompt used to achieve this (for local multiple-choice questions) is shown in Listing \ref{lst:cot_prompt}. \\

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:cot_prompt},caption={CoT prompt for `local' multiple-choice question generation}]
system = [unchanged]

user = f"""
Here is the markdown content:
{markdown_content}

Based on the markdown content, generate {n} 'Single Best Answer' (SBA) questions that have only one correct answer out of five options. The correct answer should not be obvious and *should really require specific information from the source document to be able to be answered*. The incorrect answer options should not be so ridiculous or extreme that they are obviously wrong. The questions must be of the type: {qa_type_str}

The questions should be ones that could occur when a human wants information from the chatbot. They should be directly relevant to companies preparing their sustainability reports and reflect real-world scenarios that reporting teams might encounter.

To generate questions, follow these steps:
1. Select a list of one or more sentences/snippets of the markdown content that can be used to form an answer to a question. This will form the reference text. Remember this should be relevant to the human for drafting sustainability reports.
2. Write a question that requires the reader to understand the content of the selected text to answer correctly. The question should be based only on the selected text and should not require any additional information. Remember this should be the type of question a human would ask when drafting sustainability reports.
3. Write five answer options, one of which is correct and the other four are incorrect. The correct answer should be complete and taken verbatim from the selected section(s) of the markdown content.

Generate {n} unique and diverse QA pairs for the specified type and return them using the provided schema.
"""
\end{lstlisting}

\subsubsection{CoT + Few-shot Prompting}

The third approach introduces few-shot examples to the prompt using the sample questions curated from different sources as described in Section \ref{qa_design} (the list of sample questions can be found in Appendix \ref{lst:qa_structures}). The LLM is given 10-12 question styles and asked to follow the CoT question generation steps to create questions in the same style. It is asked to select the question styles at random and adapt any that do not directly make sense in the context of the provided markdown(s).

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:memprompt},caption={CoT + few-shot prompt for `local' multiple-choice question generation}]
system = [unchanged]

user = f"""
[unchanged]

Some example {qa_type} question structures are shown below. Please choose {n} structures at random but do not limit yourself to these types only. If some of these structures do not make sense given the content of the document, adapt them to the context as appropriate or choose ones that you think are appropriate.

Here are some question structures:
{question_structures_str}  
    
Generate {n} unique and diverse QA pairs for the specified type and return them using the provided schema.
"""
\end{lstlisting}


All of the prompting techniques were experimented with using four different temperature settings - 0.0, 0.2, 0.5, 1.0 - to test the impact of varying LLM creativity on the questions generated. 




\subsection{QA Evaluation}

\subsubsection{Faithfulness and Relevance}

After the QA pairs are generated, they are evaluated on three LLM-based metrics - faithfulness and relevance, as well as a custom `domain specificity' metric. Faithfulness is a measure of how accurately LLM-generated text is grounded in the provided context(s) \cite{es2023ragas}. A faithful question must be directly answerable from the given information without any need for external knowledge or inference, while a faithful answer is true based on the context. Relevance is a measure of how focused LLM-generated text is on the given content, containing as little irrelevant information as possible \cite{es2023ragas}. A relevant question precisely aligns with the specific context taken from the report, while a relevant answer addresses the actual question that was given. \\

Implementing QA evaluation is a more complex task than a simple evaluation of LLM-generated text since questions and answers that are not simply factual typically deviate in phrasing and/or semantics from the source document - this can be seen in the drastic drop of BLEU and ROUGE-L scores as questions span more industries (see Table \ref{tab:perindustry_results}). Typical LLM-based checks of faithfulness and relevance are designed to evaluate LLM-generated responses to queries, rather than the quality of a whole question-answer pair relative to a complete source document, making them unsuitable for a direct application to the QA evaluation task. This was tested using two popular open-source evaluation frameworks - LlamaIndex \cite{llamaindex_eval} and DeepEval \cite{deepeval} which use various prompting techniques, most notably chain of thought, to measure metrics on a binary (LlamaIndex) or continuous (DeepEval) scale. They were manually tested on a subset of questions and did not prove to be effective at identifying bad question quality (see Appendix \ref{lst:opensource_eval} for an example). The tools perform particularly poorly on evaluating the relevance of a question, as they rely on generic prompts that do not provide context on how to measure relevance as defined from the perspective of a company using the context to produce sustainability reports. Nevertheless, these tools are effective at evaluating simple (non-question) statements relative to source context \cite{deepeval_paper}. \\

This project combines open-source evaluation tools with a custom `question quality' evaluator into an evaluation pipeline shown in Figure \ref{fig:metrics_arch}. With this, the resulting tools are (1) able to evaluate full question-answer pairs and (2) targeted to the sustainability reporting domain. DeepEval (chosen as it provides scores on a continuous scale) is used to evaluate the faithfulness and relevance of the reference text relative to the entire report markdown, as well as of the answer relative to the reference text. A standard evaluation tool is suitable for these tasks because it evaluates text taken straight from the document without the need for additional context.\\

A custom evaluator is used to check the quality of the question itself relative to the reference text. This evaluator uses domain context and few-shot prompting to guide the LLM on grading the faithfulness and relevancy of the question. The custom evaluator used is shown in Listing \ref{lst:fewshot_prompt}. Note that, for relevance, the LLM is also asked to check whether the question relates to all the industries it is set to cover (e.g. if it is asked to create a question comparing two industries, but the question asks about only one of those industries, it is considered irrelevant). \\

A design choice is made to automatically exclude any QA pairs that have a reference text faithfulness or relevance below 0.7, to avoid falsely inflating the scores of the question and answer that are based on the reference text. This threshold was chosen from manual inspection of question quality to define what is generally deemed acceptable. The average faithfulness/relevancy score for the reference text, question, and answer is then taken. Note that the custom question evaluator works on a scale from 1 to 10 as, experimentally, this has helped the LLM be more nuanced in its scoring than using a 0-1 scale. Therefore, the DeepEval scores are re-based on a 1-10 scale before taking the average. \\


\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\textwidth]{figures/metrics_arch.pdf}
    \caption{Faithfulness and relevance QA evaluation pipeline}
    \label{fig:metrics_arch}
\end{figure}

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:fewshot_prompt},caption={Prompt for question faithfulness/relevance evaluation}]
system = "You are an expert question evaluator. Given a question, you assess its faithfulness and relevance relative to the reference text.

user = f"""
Critically evaluate the following question based on the provided context for the relevant {'industry' if len(industries) == 1 else 'industries'}. Be extremely rigorous and unforgiving in your assessment.

Faithfulness: Measure how accurately the question is grounded in the provided context{'s' if len(industries) > 1 else ''}. A faithful question must be directly answerable from the given information without any need for external knowledge or inference. Be extremely critical - even minor discrepancies or omissions should significantly impact the score.

Relevancy: Assess how precisely the question aligns with {'the' if len(industries) == 1 else 'each'} industry's specific context, challenges, and goals. A highly relevant question should directly address key aspects, metrics, or challenges unique to the industry. Be very strict - even slight deviations from industry-specific concerns should result in lower scores. ALSO: if the question does not cover all the industries required, it must be considered irrelevant and given a score of 1.

Score both metrics on a scale of 1 to 10, where:
1-2: Completely irrelevant or unfaithful
3-4: Major flaws in relevancy or faithfulness
5-6: Moderate issues, but still lacking
7-8: Generally good, with minor issues
9-10: Excellent, near-perfect alignment

Example Context:
"The Apparel, Accessories & Footwear industry faces significant sustainability challenges, particularly in raw materials sourcing. Key metrics include:
1. Percentage of raw materials third-party certified to environmental and/or social sustainability standards.
2. Priority raw materials: Description of environmental and social risks and/or hazards associated with priority raw materials used for products.
3. Environmental impacts in the supply chain: Percentage of (1) Tier 1 supplier facilities and (2) supplier facilities beyond Tier 1 that have completed the Sustainable Apparel Coalition's Higg Facility Environmental Module (Higg FEM) assessment or an equivalent environmental data assessment."

Examples:
Good Question  (Faithfulness: 10, Relevancy: 9):
"What percentage of raw materials in the Apparel, Accessories & Footwear industry should be third-party certified to environmental or social sustainability standards, according to the context?"
Explanation: This question directly addresses a specific metric mentioned in the industry context and can be answered solely based on the provided information.

Very Bad Question (Faithfulness: 1, Relevancy: 2):
"What is the average salary of a fashion designer in New York City?"
Explanation: This question is neither relevant to the industry's sustainability metrics nor answerable from the given context.
"""

\end{lstlisting}

\subsubsection{Domain Specificity}

Domain specificity is a newly introduced metric in this project to measure how specific the QA pair is to the sustainability reporting domain through the use of key concepts or trends found in the report. The question "What does Table 1 contain?" is generic, whereas "Can you provide me the unit of measure for the \{xxx\} metric in Table 1?" is more specific, as it gives more context for what the question should focus on from a domain perspective. Maintaining high question specificity ensures that the generated questions are representative of the domain and reduces the possibility of hallucination by guiding the LLM to the exact details it should be using from the source document to generate the question and answer. Furthermore, specific questions are less likely to have obvious answers that can be determined without reference to the source document (i.e. without RAG), so they are more useful for testing the system's true domain knowledge. \\

Domain specificity is measured on the question (plus answer options if it is an MCQ) relative to the whole markdown report (rather than just the reference text). This gives the LLM full context on what the report is about rather than a single text snippet, enabling it to identify keywords or ideas that are important in the report and therefore should be mentioned in the question. A scale of 1-10 is used, and the few-shot prompt is shown in Listing \ref{lst:qa_eval_prompt}. 


\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:qa_eval_prompt},caption={Few-shot prompt for domain specificity}]
system: = ""You are an expert question evaluator."

user = f"""
    Full content:
    {full_context}

    Evaluate the specificity of the following question based on the provided context and compared to the highly specific question examples provided.
    Score the specificity on a scale of 1 to 10, where 1 is extremely broad or general and 10 is very specific. 
    
    {"Consider both the question itself and the answer options given." if is_multiple_choice else "Consider the question and the expected level of detail in the answer."}
    
    If a question requires a very specific answer directly from a specific sentence/part of the document, it is considered more specific. If a question can be answered in multiple ways or is broad, it is considered less specific.

    To help you, here are some example questions below:

    Highest specificity questions that score 10:
    "What is the unit of measure for the 'Percentage of raw materials third-party certified to an environmental and/or social sustainability standard, by standard' metric in the Apparel, Accessories & Footwear industry (as listed in the relevant table)?"

    High specificity questions that score 8:
    "What topics are covered in the 'Raw Materials Sourcing' section of the Apparel, Accessories & Footwear document, and what are the key takeaways for a company writing its sustainability report in this industry?"

    Medium specificity questions that score 6:
    "A company in the Household & Personal Products industry is facing water scarcity issues in multiple manufacturing locations. What is the most comprehensive approach to address this challenge in their sustainability report?"

    Low specificity questions that score 3:
    "How might the increasing focus on energy efficiency certifications in the appliance industry influence future regulatory trends and consumer behaviour?"

    Lowest specificity questions that score 1:
    "What broader implications does the industry's focus on energy management have for environmental sustainability?"

    Now the question to be evaluated:
    Question: {question_data['question']}
    """
\end{lstlisting}

\subsubsection{BLEU and ROUGE-L}

Finally, for free-text questions, BLEU and ROUGE-L are calculated for the answer relative to the reference text. ROUGE-L is chosen (over ROUGE-N) as it is designed to evaluate the semantic similarity and content coverage of text regardless of word order. ROUGE-N on the other hand simply evaluates the grammatical correctness and fluency of the generated text, which is not a requirement of the answers - succinct and factually accurate answers are preferred.\\

MCQ answers are not evaluated using BLEU and ROUGE-L. This is because low BLEU and/or ROUGE-L scores are not indicative of a wrong answer. For example, the correct answer option for an MCQ may be ``All of the above", which is not a phrase found in the text, and so will score 0 on both metrics despite being correct. An extension of this problem is that, if all answer options were to be evaluated instead of just the single correct one, multiple answer options may score highly as the options are designed to be similar enough that the correct answer is not obvious. It then becomes difficult to discern between correct and incorrect options from metric scores.

\subsection{Post-processing}

A selection of post-processing functions was developed to make the questions as high quality as possible (i.e. free of hallucination, relevant, and domain-specific) while making them sound human-like. For this project, human-like questions are defined as having some degree of vagueness, particularly with regards to the industry being asked about. To achieve this aim, two functions were introduced - quality improvement, and question generalisation. Additionally, MCQs are verified to ensure they have a single best answer (SBA) among the answer options. Finally, a similarity filter is introduced to remove identical question copies (in case these arise).

\subsubsection{Quality Improvement Function}


The first post-processing step involves passing questions that fail to meet a threshold on any of the three quality metrics through a quality improvement function that is designed to address the identified shortcomings. The flagged questions are passed along with their relevant industry markdown(s) through a prompt that instructs the LLM to slightly adjust the question on the required aspect. For example, if a question scored low on faithfulness, the prompt instructs the model to reformulate the question to more closely align with the factual information from the report. Similarly, for questions with low relevance scores, the prompt guides the model to focus more tightly on the specific sustainability reporting context from the report. This approach targets improvements to address each question's weaknesses while preserving its original intent and structure. The prompt used for this function is shown in Appendix \ref{lst:quality_improvement_prompt}.\\


To avoid any chances of hallucination by repeated prompting (as the LLM "forgets" the initial goal), this function is applied only once, and the question metrics are checked again. If the improved question still fails to meet the threshold, or meets the threshold for the original weak metric but now fails on another metric, it is discarded. Additionally, if the original question fails to meet the threshold on more than one metric, it is discarded, as it would not provide the LLM enough ``starting material" for the improvement function to reformulate the question while preserving its initial intent. \\

There is no generally accepted choice of threshold for faithfulness and relevance, as thresholds are designed to be adapted to the aim of the LLM application. In principle, any score above 0.5 would suggest that the LLM is better than random and may be deemed acceptable. For QA evaluation, the thresholds are determined based on the results of the experiments shown in Table \ref{tab:qa_evals_methods} of Section \ref{qa_experiment_results}. 


\subsubsection{Question Generalisation Function}

The second post-processing function generalises questions that are ``too specific to be human". It alters any mentions of very specific industry names into more generic industry ``areas", mimicking the more natural conversational-style queries that a user would ask. For example, a question that originally asked ``What does the IFRS require companies to disclose regarding energy management for the Processed Foods industry?" might be rephrased to ``What does the IFRS require food companies to disclose regarding energy management?". This generalisation increases the difficulty of the questions, requiring the RAG system to have a deeper understanding of the content rather than relying on keyword matching. The efficacy of this function is tested qualitatively. The prompt used for this function is shown in Appendix \ref{lst:generalisation_prompt}.


\subsubsection{Single Best Answer Verification}

The questions are filtered to remove any MCQs that have more than one correct answer (i.e. that do not pass the SBA test). This is checked by giving a strong LLM (Claude 3.5 Sonnet) the full markdown context and asking it to choose all correct answer options based on it. If more than one option is chosen, the question fails the SBA test and is discarded. Finally, a similarity filter is created to remove identical questions. The prompt used for this function is shown in Appendix \ref{lst:sba_prompt}.

\subsubsection{Similarity Filter}

Finally, to avoid question repetition, the questions are passed through a similarity filter to remove identical questions. The nature of the question generation pipeline, which uses few-shot prompting to create very specific questions, means that many questions are likely to be very similar in their structures. As such, the filter is set to a threshold of 0.99\% similarity, and questions above this are discarded.

\section{Results}



\subsection{Question Embeddings}

The embeddings for a sample of local free-text questions from six industry categories are mapped using t-SNE (t-distributed Stochastic Neighbour Embedding), a dimensionality reduction technique for visualisation of high-dimensional data. For each industry category, questions are randomly selected from each industry. The questions used are generated using the CoT + few-shot method. The results are shown in Figure \ref{fig:question_embeddings}.

\begin{figure}[H]
    \centering
    \includegraphics[width = \textwidth]{figures/question_embeddings_tsne_sample.pdf}
    \caption{Sample t-SNE visualisation of question embeddings, coloured by industry group}
    \label{fig:question_embeddings}
\end{figure}




\subsection{Experimental Results}\label{qa_experiment_results}

The three question generation methods outlined in Section \ref{QA_gen} are used to generate a sample of questions for quality evaluation experiments. For each method, the questions are generated according to the settings described in Section \ref{qa_design}:
\begin{itemize}
    \item Question span: For local questions, eight industries are chosen at random, while for cross-industry questions, eight industry pairs are chosen out of the LLM-generated industry pairs (generation of these pairs is described in Section \ref{qa_design}). (Note that a subset of industry (pairs) is used for experimentation due to cost reasons. The full QA dataset will be generated on all industries and industry pairs.)
    \item Question complexity: Single- and multi-hop questions are generated.
    \item Answer structure: MCQ and free-text questions are generated.
    \item Temperature: All settings are tried with four temperatures: 0.0, 0.2, 0.5, 1.0.
\end{itemize}

To conduct experiments (before running the final question generation pipeline), 32 questions of each type and temperature are created and evaluated. Tables \ref{tab:qa_evals_methods}, \ref{tab:qa_evals_methods_free}, and \ref{tab:temperature-comparison} below show the average scores of question evaluations across different methods and temperatures.


\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
              & \multicolumn{3}{c}{MCQ Local}                                                & \multicolumn{3}{c}{MCQ Cross-industry}                                       \\ \cline{2-7} 
              & Baseline & CoT     & \begin{tabular}[c]{@{}c@{}}CoT +\\ Few-shot\end{tabular} & Baseline & CoT     & \begin{tabular}[c]{@{}c@{}}CoT +\\ Few-shot\end{tabular} \\ \cline{2-7} 
No. Questions & 64       & 64      & 64                                                       & 64       & 64      & 64                                                       \\
Faithfulness  & 6.17     & 7.25    & \textbf{9.45}                                                     & 5.58     & 6.06    & \textbf{7.69}                                                     \\
Relevance     & 6.25     & 7.78    & \textbf{9.68}                                                     & 3.46     & 5.93    & \textbf{7.86}                                                     \\
Specificity   & 5.61     & 7.47    & \textbf{9.24}                                                     & 5.04     & 6.31    & \textbf{7.69}                                                     \\
SBA (\%)      & 67.19\%  & 81.25\% & \textbf{92.19\%}                                                  & 68.76\%  & 75.01\% & \textbf{92.19\%}                                                  \\ \hline
\end{tabular}
\caption{Experimental results per method, averaged over single- and multi-hop MCQs. The experiment was done with questions generated on a selection of 8 industries (for local) and 8 industry pairs (for cross-industry) with a temperature of 0.5.}
\label{tab:qa_evals_methods}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
              & \multicolumn{3}{c}{Free-text Local}                                       & \multicolumn{3}{c}{Free-text Cross-industry}                              \\ \cline{2-7} 
              & Baseline & CoT  & \begin{tabular}[c]{@{}c@{}}CoT +\\ Few-shot\end{tabular} & Baseline & CoT  & \begin{tabular}[c]{@{}c@{}}CoT +\\ Few-shot\end{tabular} \\ \cline{2-7} 
No. Questions & 64       & 64   & 64                                                       & 64       & 64   & 64                                                       \\
Faithfulness  & 5.68     & 7.12 & \textbf{8.75}                                                    & 5.09     & 5.89 & \textbf{7.13}                                                     \\
Relevance     & 5.93     & 7.30  & \textbf{9.56}                                                     & 3.34     & 5.82 & \textbf{7.32}                                                     \\
Specificity   & 5.04     & 7.01 & \textbf{9.06}                                                     & 5.07     & 6.25 & \textbf{7.44}                                                     \\
BLEU          & 0.31     & 0.68 & \textbf{0.95}                                                     & 0.12     & 0.28 & \textbf{0.36}                                                     \\
ROUGE-L       & 0.35     & 0.71 & \textbf{0.95}                                                     & 0.29     & 0.41 & \textbf{0.49}                                                     \\ \hline
\end{tabular}
\caption{Experimental results per method, averaged over single- and multi-hop free-text questions. Experiment done with questions generated on a selection of 8 industries (for local) and 8 industry pairs (for cross-industry) with a temperature of 0.5.}
\label{tab:qa_evals_methods_free}
\end{table}


Tables \ref{tab:qa_evals_methods} and \ref{tab:qa_evals_methods_free} show significant improvements in question quality across the three methods for both MCQs and free-text questions - there is a steady increase in scores for faithfulness, relevance, and consistency, as well as, for MCQs, the percentage of questions that have a single best answer. On average (across faithfulness, relevance, and specificity), local and cross-industry MCQs generated by the CoT + few-shot method are 57.6\% and 72.5\% better than the baseline method, respectively. For free-text questions, these respective improvements are 65.0\% and 68.7\%. \\

The CoT + few-shot method yields the best questions for both local and cross-industry spans, though cross-industry questions consistently score lower than local questions, not scoring above 8 on average. Cross-industry free-text questions also have much lower BLEU and ROUGE-L scores on average than local questions. Free-text questions score slightly lower than MCQs in general, and the extent of the difference in quality varies - free-text local questions, for example, score 8.75 on faithfulness, which is significantly less than the 9.45 scored by MCQs, though the difference between their relevance and specificity is much smaller.


\begin{table}[H]
\centering
\small
\begin{tabular}{lcccccccc}
\hline
\multirow{2}{*}{} & \multicolumn{4}{c}{MCQ}              & \multicolumn{4}{c}{Free-text} \\ \cline{2-9} 
                  & 0.0     & 0.2     & 0.5     & 1.0     & 0.0    & 0.2   & 0.5   & 1.0   \\ \cline{2-9} 
Faithfulness      & \textbf{8.83}    & 8.58    & 8.57    & 8.04    & 7.21   & 6.93  & \textbf{7.94}  & 7.63  \\
Relevance         & \textbf{9.10}    & 8.92    & 8.77    & 8.21    & 7.95   & 7.68  & \textbf{8.44}  & 8.38  \\
Specificity       & \textbf{8.50}    & 8.36    & 8.47    & \textbf{8.50}    & 8.08   & 8.13  & \textbf{8.25}  & 8.00  \\
BLEU              & --      & --      & --      & --      & 0.49   & 0.42  & \textbf{0.66}  & \textbf{0.66} \\
ROUGE-L           & --      & --      & --      & --      & 0.60   & 0.53  & 0.72  & \textbf{0.75}  \\
SBA (\%)          & 92.19\% & \textbf{92.97\%} & 92.19\% & 87.50\% & -      & -     & -     & -     \\ \hline
\end{tabular}
\caption{Temperature sensitivity for free-text and MCQ generated using the CoT + Few-shot method on a selection of 8 industries (for local questions) and 8 industry pairs (for cross-industry questions). Results averaged across single-/multi-hop and local/cross-industry questions. SBA (\%) stands for the percentage of questions that have a `single best answer'.}
\label{tab:temperature-comparison}
\end{table}

Table \ref{tab:temperature-comparison} shows that question quality varies little with LLM temperature, though small trends are observed. MCQs tend to score better with lower temperatures, though this is not the case for specificity and SBA percentages, which show no real trend. Free-text questions seem to score best with higher temperatures - faithfulness, relevance, and specificity are highest with a temperature of 0.5, while ROUGE-L is highest with a temperature of 1.

\subsection{Post-processing}

The efficacy of the quality improvement function is tested on the questions generated using the CoT + few-shot method. There are no standard accepted thresholds for faithfulness and relevance within LLM text generation literature. Generally, any score above 0.5 can be deemed to be acceptable, as more than half the generated text is truthful and related to the context. For the quality improvement function, we therefore base our threshold selection on the experimental results for CoT + few-shot shown in Tables \ref{tab:qa_evals_methods} and \ref{tab:qa_evals_methods_free}. We set a conditional threshold - local questions must score at least 9 on all metrics, while cross-industry questions must score at least 7 (both metrics are out of 10). \\

The post-processing functions are cost-intensive operations. In particular, the quality improvement function involves passing the entire question and source document(s) through the quality improvement function as well as re-evaluating the question on all metrics. Therefore, the application of the quality improvement and generalisation functions to the entire dataset is left as future work. These functions were tested qualitatively, with two examples shown below.
\newpage
\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:quality_improvement},caption={Application of quality improvement post-processing on a cross-industry MCQ}]
Original question:
"question": "What are the metrics for Coal Operations and Apparel, Accessories & Footwear industries.",
    "optionA": "Production of thermal coal, Production of metallurgical coal, Number of Tier 1 suppliers and suppliers beyond Tier 1",
    "optionB": "Production of thermal coal, Number of Tier 1 suppliers and suppliers beyond Tier 1",
    "optionC": "Production of metallurgical coal, Number of Tier 1 suppliers and suppliers beyond Tier 1",
    "optionD": "Production of thermal coal, Production of metallurgical coal",
    "optionE": "Number of Tier 1 suppliers and suppliers beyond Tier 1",
"answer": "A"
Original specificity score: 5

Improved question:
"question": "List the activity metrics for Coal Operations and Apparel, Accessories & Footwear industries.",
    "optionA": "Production of thermal coal, Production of metallurgical coal, Number of Tier 1 suppliers and suppliers beyond Tier 1",
    "optionB": "Production of thermal coal, Number of Tier 1 suppliers and suppliers beyond Tier 1",
    "optionC": "Production of metallurgical coal, Number of Tier 1 suppliers and suppliers beyond Tier 1",
    "optionD": "Production of thermal coal, Production of metallurgical coal",
    "optionE": "Number of Tier 1 suppliers and suppliers beyond Tier 1",
"answer": "A"
New specificity score: 9
\end{lstlisting}

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:generalisation_example},caption={Application of generalisation post-processing on a cross-industry MCQ}]
"original question": "What is the code for the 'Gross global Scope 1 emissions, percentage covered under emissions-limiting regulations' metric in the Coal Operations industry?",
    "optionA": "EM-CO-110a.2",
    "optionB": "EM-CO-140a.1",
    "optionC": "EM-CO-110a.1",
    "optionD": "EM-CO-420a.3",
    "optionE": "EM-CO-000.A",
"answer": "C",

"refined question": "What is the code for the 'Gross global Scope 1 emissions, percentage covered under emissions-limiting regulations' metric in the fossil fuel operations industry?"
\end{lstlisting}

\subsection{Final Question Evaluation}


The final questions are generated using the CoT + few-shot method using all industries and industry pairs. 1,179 questions are generated using 68 industries (for local questions) and 44 industry pairs (for cross-industry questions) with question number per type shown in Table \ref{tab:final_qa_numbers}. The average quality results of these questions is shown in Tables \ref{tab:perindustry_results} and \ref{tab:per_hop}.

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\hline
           & \multicolumn{2}{c}{MCQ}                               & \multicolumn{2}{c}{Free-text}                         \\ \hline
           & \multicolumn{1}{l}{Local} & \multicolumn{1}{l}{Cross-industry} & \multicolumn{1}{l}{Local} & \multicolumn{1}{l}{Cross-industry} \\ \cline{2-5} 
Single-hop & 272                       & 93                        & 136                       & 88                        \\
Multi-hop  & 272                       & 94                        & 136                       & 88                        \\ \hline
\end{tabular}
\caption{Number of questions generated, per type, using the CoT + few-shot method.}
\label{tab:final_qa_numbers}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\hline
              & \multicolumn{2}{c}{MCQ}  & \multicolumn{2}{c}{Free-text} \\ \cline{2-5} 
              & Local   & Cross-industry & Local     & Cross-industry    \\ \cline{2-5} 
No. Questions & 544     & 187            & 272       & 176               \\
Faithfulness  & \textbf{9.40}    & 7.64           & 8.32      & 6.42              \\
Relevance     & \textbf{9.68}    & 7.65           & 8.78      & 6.88              \\
Specificity   & \textbf{9.06}    & 7.15           & 8.80      & 8.22              \\
BLEU          & -       & -              & \textbf{0.93}      & 0.34              \\
ROUGE-L       & -       & -              & \textbf{0.95}      & 0.46              \\
SBA (\%)      & \textbf{92.47\%} & 91.45\%        & -         & -                 \\ \hline
\end{tabular}
\caption{Results per question span. Results are averaged across single- and multi-hop. Questions are generated using a temperature of 0.5. SBA (\%) stands for the percentage of questions that have a `single best answer'.}
\label{tab:perindustry_results}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\hline
              & \multicolumn{2}{c}{MCQ} & \multicolumn{2}{c}{Free-text} \\ \cline{2-5} 
              & Single-hop  & Multi-hop & Single-hop     & Multi-hop    \\ \cline{2-5} 
No. Questions & 365         & 366       & 224            & 224          \\
Faithfulness  & \textbf{9.06}        & 7.98      & 7.52           & 7.23         \\
Relevance     & \textbf{8.84}        & 8.49      & 7.79           & 7.86         \\
Specificity   & 8.20         & 8.00         & \textbf{8.65}           & 8.35         \\
BLEU          & -           & -         & \textbf{0.76}           & 0.51         \\
ROUGE-L       & -           & -         & \textbf{0.82}           & 0.60          \\
SBA (\%)      & \textbf{93.43\%}     & 90.44\%   & -              & -            \\ \hline
\end{tabular}
\caption{Results per question complexity. Results are averaged across local and cross-industry. Questions are generated using a temperature of 0.5. SBA (\%) stands for the percentage of questions that have a `single best answer'.}
\label{tab:per_hop}
\end{table}

The average quality score (across faithfulness, relevance, and specificity) across all question types is 8.16. Local questions score 9 on average, while cross-industry ones score 7.32. Multiple-choice questions score 8.42 on average, while free-text questions score 7.9. Single-hop questions score 8.34 while multi-hop score 8 on average. \\

Based on these results, two general trends are observed: (1) `simpler' questions (i.e. local and single-hop) score better than more complex questions (i.e. cross-industry and multi-hop), and (2) MCQs score better than free-text questions. Among both MCQs and free-text questions, there is a bigger difference in quality between local and cross-industry questions than between single- and multi-hop questions. On average, single-hop MCQs score 6.7\% better than multi-hop MCQs, while local MCQs score 25.4\% better than cross-industry MCQs. For free-text questions, these differences are 2.2\% and 21.4\%, respectively. 


\section{Discussion}

\subsection{Question Embeddings}

Figure \ref{fig:question_embeddings} shows two key trends - the question embeddings across different industry groups are generally distinct, while the embeddings within industry groups are similar. This means that, while it is easy to distinguish between the industry groups a query relates to, it is more difficult to distinguish between industries within the group based on the query.



\subsection{Experimental Results} \label{qa_experiments_discussion}


Each question generation method offered additional advantages for question quality. Table \ref{tab:qa_ablation} below displays the incremental improvements attributed to each method relative to the previous one. Note that the percentages for `Few-shot vs CoT' are calculated on a larger baseline than the percentages for `CoT vs baseline', so although the contributions of each additional method may seem similar, the absolute contributions of Few-shot are larger. 

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\hline
                     & \multicolumn{2}{c}{Local}                                                                                              & \multicolumn{2}{c}{Cross-industry}                                                                                    \\ \cline{2-5} 
                     & \begin{tabular}[c]{@{}c@{}}CoT \\ vs baseline\end{tabular} & \begin{tabular}[c]{@{}c@{}}Few-shot\\ vs CoT\end{tabular} & \begin{tabular}[c]{@{}c@{}}CoT\\ vs baseline\end{tabular} & \begin{tabular}[c]{@{}c@{}}Few-shot\\ vs CoT\end{tabular} \\ \cline{2-5} 
\multicolumn{1}{c}{} & \multicolumn{4}{c}{MCQ}                                                                                                                                                                                                                        \\ \cline{2-5} 
Faithfulness         & 17.50\%                                                    & 30.34\%                                                   & 8.60\%                                                    & 26.90\%                                                   \\
Relevance            & 24.48\%                                                    & 24.42\%                                                   & 71.39\%                                                   & 32.55\%                                                   \\
Specificity          & 33.16\%                                                    & 23.69\%                                                   & 25.20\%                                                   & 21.87\%                                                   \\
\textbf{Average}     & \textbf{25.05\%}                                           & \textbf{26.15\%}                                          & \textbf{35.06\%}                                          & \textbf{27.10\%}                                          \\ \cline{2-5} 
\multicolumn{1}{c}{} & \multicolumn{4}{c}{Free-text}                                                                                                                                                                                                                  \\ \cline{2-5} 
Faithfulness         & 25.35\%                                                    & 22.89\%                                                   & 15.72\%                                                   & 21.05\%                                                   \\
Relevance            & 23.10\%                                                    & 30.96\%                                                   & 74.25\%                                                   & 25.77\%                                                   \\
Specificity          & 39.09\%                                                    & 29.24\%                                                   & 23.27\%                                                   & 19.04\%                                                   \\
\textbf{Average}     & \textbf{29.18\%}                                           & \textbf{27.70\%}                                          & \textbf{37.75\%}                                          & \textbf{21.96\%}                                          \\ \hline
\end{tabular}
\caption{Percentage improvements in question quality across different methods. Calculated based on data in Tables \ref{tab:qa_evals_methods} and \ref{tab:qa_evals_methods_free}}
\label{tab:qa_ablation}
\end{table}


The percentage contributions of each additional method to question quality are mixed. Integrating CoT reasoning into the prompt yields average quality improvements in the range of 25\% to 37.8\%, with cross-industry questions benefiting most from this method. This suggests that guiding the LLM through a three-step process for creating questions helps it to navigate through multiple documents and isolate relevant chunks from each of them to form a question. \\

Few-shot prompting yields additional benefits ranging from 22\% to 27.7\%, and these benefits are mixed across local and cross-industry questions. However, in absolute terms, Tables \ref{tab:qa_evals_methods} and \ref{tab:qa_evals_methods_free} show that local questions are higher quality than cross-industry ones, scoring an average of 9.29 (across both MCQ and free-text) versus 7.52. A similar observation is made when comparing MCQs with free-text questions - though they both benefit equally from the advanced methods, MCQs are (albeit to a lesser extent) higher quality in absolute terms, scoring an average of 8.60 versus 8.21. \\

These observations suggest that there exists a limit to LLMs' capabilities of generating questions using prompt methods. It may struggle with using multiple documents to create questions, as well as ensuring that free-text questions are as accurate as multiple-choice ones. \\

In the following subsections, we discuss the quality issues encountered with each method, providing examples that have contributed to their respective quality scores.

\subsubsection{Baseline Prompting} 

Several issues are encountered in the baseline question generation method, as it does not guide the LLM on how to create questions and does not provide any example structures it should aim for. Four examples are shown below to highlight the biggest issues with the baseline approach (in addition to standard faithfulness problems).

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:memprompt},caption={Question Example 1: Local single-hop free-text}]
"question": "How many incidents of non-compliance with water quality permits, standards, and regulations have occurred for coal operations?",
"answer": "The entity shall disclose the total number of instances of non-compliance, including violations of a technology-based standard and exceedances of quantity and/or quality-based standards.",
\end{lstlisting}

\textit{Issue \#1: Questions are not related to sustainability reporting.} This is depicted in the example above, which displays phrasing that is not in line with user queries about how to report on the metrics. This in turn makes the answer, which is in fact correct based on the source, irrelevant to the question. A more relevant phrasing for this question would be ``How should I disclose incidents of non-compliance with water quality permits, standards, and regulations in the Coal Operations industry?". This issue is captured and measured by the relevance metric.

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:memprompt},caption={Question Example 2: Local single-hop MCQ}]
"question": "According to the topic summary, how can companies in this industry be better positioned to meet consumer demand and reduce future liabilities?",
"optionA": "By focusing solely on product innovation",
"optionB": "By reducing production costs at all costs",
"optionC": "By addressing environmental concerns and enhancing product take-back",
"optionD": "By increasing marketing budgets",
"optionE": "By outsourcing all manufacturing processes",
"answer": "C. By addressing environmental concerns and enhancing product take-back",
\end{lstlisting}

\textit{Issue \#2: Questions are vague, making the correct answer obvious.} In the example above, there is no specific reference to what piece of the document, and what industry, the question refers to. Nevertheless, the question is easily answerable even without knowing the context, as options A, B, D, and E are very obviously wrong. This issue is measured by the domain specificity metric, which shows the lowest scores of all metrics for the baseline method, indicating that this is a particular issue with this method.


\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:memprompt},caption={Question Example 3: Cross-industry single-hop MCQ}]
"question": "Which of the following is a key strategy home builders can use to mitigate environmental legal risks?",
"optionA": "Ensuring all homes are certified to a third-party green building standard",
"optionB": "Implementing corrective actions in response to any legal proceedings related to environmental regulations",
"optionC": "Focusing development only in regions with low baseline water stress",
"optionD": "Both B and C",
"optionE": "All of the above",
"answer": "B",
"industries": ["b8-construction-materials","b35-home-builders"]
\end{lstlisting}

\textit{Issue \#3: Cross-industry questions do not cover all required industries.} In the example above, not all the required industries are covered by the question. This issue is captured and measured by the relevance metric, contributing to its low score.

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:memprompt},caption={Question Example 4: Cross-industry single-hop MCQ}]
"question": "Which of the following is a key risk for home builders related to climate change adaptation?",
    "optionA": "Increased costs associated with flood insurance for homes in 100-year flood zones",
    "optionB": "Difficulty obtaining permits for developments in water-stressed regions",
    "optionC": "Reduced long-term demand for homes in volatile climate regions",
    "optionD": "Need for more robust construction materials to withstand extreme weather events",
    "optionE": "Higher energy costs for cooling homes in regions experiencing rising temperatures",
"answer": "A"
"true correct answers": A, B, C
\end{lstlisting}


\textit{Issue \#4: Questions do not have a single best answer.} The example question above has three answer options that are correct, though the LLM has selected only one. Multiple-choice questions often have multiple correct answers listed in the options. This is driven by questions being vague, inviting multiple ways of answering them. The LLM simply chooses one of its generated answer options to be the `correct' one, however several of the other given options can also be deemed correct based on the source information. Having multiple correct answer options artificially deflates the accuracy of RAG systems tested on the MCQs. The lack of a single best answer affects one in three questions generated by the baseline method (see Table \ref{tab:qa_evals_methods}).


\subsubsection{CoT Prompting}

The CoT approach addresses some of the issues encountered in the baseline approach, and the highest quality improvements are seen for cross-industry questions, where the quality improved by an average of 36.3\% across all metrics (the average improvement for local questions is 27\%). Most notably, cross-industry questions become 72.8\% more relevant and 24.2\% more specific. Nevertheless, in absolute terms, these questions do not achieve a satisfactory level, with scores being only slightly above average. Some sample questions are displayed below to highlight persistent issues with question quality.

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:cot_example1},caption={Question Example 1: Local single-hop free-text}]
"question": "What is the percentage of the company's Scope 1 emissions that are covered under emissions-limiting regulations?",

"answer": "The percentage shall be calculated as the total amount of gross global Scope 1 GHG emissions (CO2-e) that are covered under emissions-limiting regulations divided by the total amount of gross global Scope 1 GHG emissions (CO2-e).",
\end{lstlisting}

\textit{Issue \#1: Questions are vague.} This question has no reference to what type of industry it is referring to and is therefore applicable to all industries. It would be nearly impossible for any RAG system to get this question correct with no further context.


\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:cot_example2},caption={Question Example 2: Cross-industry single-hop MCQ}]
"question": "Which industry discusses the percentage of products by revenue that contain IEC 62474 declarable substances?",
    "optionA": "Metals & Mining",
    "optionB": "Electrical & Electronic Equipment",
    "optionC": "Both industries",
    "optionD": "Neither industry",
    "optionE": "Not specified",
    "answer": "B"
    "industries": ["b10-metals-and-mining","b49-electrical-and-electronic-equipment"]
\end{lstlisting}

\textit{Issues \#2 and \#3: Questions do not cover all industries required, and are not representative of real-life user queries  for sustainability reporting.} This question is supposed to be comparative between two different industries. Instead, it lists industries as multiple-choice options, a format that is not user-typical. The question itself doesn't refer to both industries it is given. These issues contribute to lower relevancy and specificity. 




\subsubsection{CoT + Few-shot Prompting}

This is the final selected method for question generation, which displays the best quality metric scores and the highest percentage of questions that pass the SBA test out of all the methods. These improvements can be attributed to the precise wording and structuring of the few-shot examples provided to the LLM, as described in Section \ref{qa_design}. \\

Below are some sample questions that depict high-quality questions that are representative of real-life user queries.

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:few_example2},caption={Question Example 1: Local multi-hop MCQ}]
{
"question": "What is the category of the metric 'Total landfill gas generated' in the Waste Management industry and how should this metric be calculated?",
      "optionA": "Quantitative; calculated in millions of British Thermal Units (MMBtu)",
      "optionB": "Discussion and Analysis; calculated based on engineering estimates",
      "optionC": "Quantitative; calculated in metric tons (t)",
      "optionD": "Qualitative; based on expert interviews",
      "optionE": "Quantitative; calculated in gigajoules (GJ)",
"answer": "A",
"reference_text": [
    "1 The entity shall disclose (1) the total amount, in millions of British Thermal Units (MMBtu), of landfill gas generated from its owned or operated facilities.",
    "1.1 Landfill gas is defined as gas produced as a result of anaerobic decomposition of waste materials in the landfill."
      ],
"pages": ["401"]
\end{lstlisting}

\begin{lstlisting}[language=JSON,firstnumber=1,label={lst:few_example1},caption={Question Example 2: Cross-industry multi-hop MCQ}]
"question": "Give me the codes for all the 'Quantitative' metrics in Table 1 for Insurance and Real Estate Services industries.",
      "optionA": "FN-IN-410b.1, FN-IN-450a.1, FN-IN-450a.2, IF-RS-410a.1, IF-RS-410a.2, IF-RS-410a.3",
      "optionB": "FN-IN-410b.1, FN-IN-450a.1, FN-IN-450a.2, IF-RS-410a.1",
      "optionC": "FN-IN-410b.1, FN-IN-450a.1, IF-RS-410a.1, IF-RS-410a.2",
      "optionD": "FN-IN-450a.1, FN-IN-450a.2, IF-RS-410a.1, IF-RS-410a.3",
      "optionE": "FN-IN-410b.1, FN-IN-450a.1, IF-RS-410a.2, IF-RS-410a.3",
"answer": "A",
"reference_text": [
    "| Policies Designed to Incentivize Responsible Behavior | Net premiums written related to energy efficiency and low carbon technology | Quantitative | Reporting currency | FN-IN-410b.1 |",
    "| Environmental Physical Risk Exposure | Probable Maximum Loss (PML) of insured products from weather-related natural catastrophes 21 | Quantitative | Reporting currency | FN-IN-450a.1 |",
    "| Environmental Physical Risk Exposure | Total amount of monetary losses attributable to insurance payouts from (1) modeled natural catastrophes and (2) non-modeled natural catastrophes, by type of event and geographic segment (net and gross of reinsurance) 22 | Quantitative | Reporting currency | FN-IN-450a.2 |",
    "| Sustainability Services | Revenue from energy and sustainability services 55 | Quantitative | Reporting currency | IF-RS-410a.1 |",
    "| Sustainability Services | (1) Floor area and (2) number of buildings under management provided with energy and sustainability services | Quantitative | Square feet (ft\u00b2), Number | IF-RS-410a.2 |",
    "| Sustainability Services | (1) Floor area and (2) number of buildings under management that obtained an energy rating | Quantitative | Square feet (ft\u00b2), Number | IF-RS-410a.3 |"],
"pages": ["Page 156","Page 391"],
\end{lstlisting}




Nevertheless, these questions are not perfect, and there are limits to what types of questions can be generated using advanced prompting methods. One such limit was encountered when trying to generate questions spanning more than two industries, where the LLM was not able to capture information from all industries to create a question. Instead, it selected one or two of the given industries and made a question based on that. This is not a significant problem for our application, as such complicated questions may be less likely to occur in our domain, but presents some room for potential future work on question-generation methods.


\subsubsection{Temperature}

Table \ref{tab:temperature-comparison} shows the evaluation of questions generated using the CoT + few-shot method with different LLM temperature settings. The differences in evaluation scores across temperature values are small, showing that LLM temperature does not have a strong impact on question quality. This is because the LLM is guided to follow certain question styles through few-shot prompting, removing much scope for LLM creativity (which is what temperature dictates). Given the small variance, and to ensure all questions are created with consistent settings, the decision is made to use a temperature of 0.5 for all question generation. 

\subsection{Final Question Evaluation}


The results in Tables \ref{tab:perindustry_results} and \ref{tab:per_hop} show that the CoT + few-shot generation method produces high-quality questions across all formats. Nevertheless, table \ref{tab:final_ablation} shows that question span is still a significant driver of question quality, as is question complexity, although to a much lesser extent. This highlights that LLMs possess some ability to reason in multiple steps but struggle more with utilising data from multiple source documents. A manual evaluation of a subset of the questions show that they are high quality (as depicted in the examples shown in Section \ref{qa_experiments_discussion}), though conducting human evaluation on the entire dataset is left as future work.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\hline
             & Question Span           & Question Complexity     \\ \hline
             & Local vs Cross-industry & Single-hop vs Multi-hop \\ \cline{2-3} 
             & \multicolumn{2}{c}{MCQ}                           \\ \cline{2-3} 
Faithfulness & 23.04\%                 & 13.53\%                 \\
Relevance    & 26.54\%                 & 4.12\%                  \\
Specificity  & 26.71\%                 & 2.50\%                  \\
\textbf{Average}      & \textbf{25.43\% }                &\textbf{ 6.72\%}                  \\ \cline{2-3} 
             & \multicolumn{2}{c}{Free-text}                     \\ \cline{2-3} 
Faithfulness & 29.60\%                 & 4.01\%                  \\
Relevance    & 27.62\%                 & -0.89\%                 \\
Specificity  & 7.06\%                  & 3.59\%                  \\
\textbf{Average}      & \textbf{21.42\% }                & \textbf{2.24\%}                  \\ \hline
\end{tabular}
\caption{Percentage differences in question quality of simpler questions relative to more difficult questions, based on question span and question complexity. Calculated based on data in Tables \ref{tab:perindustry_results} and \ref{tab:per_hop}.}
\label{tab:final_ablation}
\end{table}

For the final dataset, 116 questions are removed as they do not have a single best answer, leaving 1,063 questions to be used for evaluating RAG systems.
