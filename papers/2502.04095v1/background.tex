

\chapter{Background and Related Work}

\section{Advancements in Natural Language Processing}

This section explores key developments in Natural Language Processing (NLP) and Large Language Models (LLMs) that have provided the foundation for domain-specific question-answering systems.

\subsection{Large Language Models}

The introduction of the Transformer architecture \cite{vaswani2023attention} marked a significant shift in NLP, moving away from recurrent neural networks towards models based on self-attention mechanisms. These mechanisms allow for efficient parallel processing and excel at capturing long-range dependencies in text, which are crucial for understanding complex language structures. This shift has enabled the development of sophisticated pre-trained models that can be fine-tuned for various tasks, significantly improving performance across the board.

\subsection{BERT}

BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin2019bertpretrainingdeepbidirectional} utilises the Transformer architecture's encoder to understand text context bidirectionally. It is pre-trained on tasks like Masked Language Modeling, where it predicts masked tokens, and Next Sentence Prediction, enhancing its ability to capture nuanced language features. BERT's architecture, with its deep bidirectional context understanding, makes it particularly effective for tasks requiring deep comprehension, such as sentiment analysis and text classification. Its design allows for straightforward adaptation to classification tasks by adding a task-specific output layer for fine-tuning. This flexibility and effectiveness have led to BERT's widespread adoption in various NLP applications, including named entity recognition and question answering.

\subsection{GPT}

The Generative Pre-trained Transformer (GPT) \cite{radford2018improving} is an auto-regressive language model that leverages the Transformer architecture's decoder. It processes text in a unidirectional manner, generating coherent text by predicting each word based on the preceding context. GPT is pre-trained on extensive datasets using unsupervised learning, optimising its parameters to capture complex language patterns. This pre-training equips GPT to perform various NLP tasks such as text completion, summarization, and more. The model's ability to generate human-like text has made it a popular choice for applications in creative writing, dialogue systems, and other generative tasks.

\subsection{Limitations of Large Language Models}

Despite their impressive capabilities, LLMs have inherent limitations. They can struggle with queries requiring current information beyond their training data and may generate inaccurate or nonsensical text, known as ``hallucinations" \cite{zhang2023siren}. Additionally, LLMs often lack true understanding and can find multi-step reasoning challenging. While larger models exhibit improved reasoning abilities, integrating genuine causal reasoning remains an active research area. The frequency and representation of specific documents or facts in their training data can significantly impact their ability to answer related questions accurately. Moreover, the computational resources required for training and deploying these models can be substantial, which can pose challenges for scalability and accessibility.


\section{Prompt Engineering}

This section provides an overview of state-of-the-art prompt engineering techniques used in LLM-based systems.

\subsection{Prompt Templates}

Large language models typically operate by generating text outputs in response to user-provided input prompts, which can be natural language instructions, questions, or other forms of content such as images or audio \cite{schulhoff2024promptreportsystematicsurvey}. Prompt engineering is a method of utilising LLMs for various tasks by crafting input prompts to guide LLMs towards desired outputs \cite{schulhoff2024promptreportsystematicsurvey}. It relies on the principle that LLMs generate responses based on patterns in their training data, so a carefully crafted prompt can activate relevant knowledge within the model's parameters. This process often involves providing context, specifying desired output formats, and framing tasks in ways that align with the model's training. Early work in prompt engineering focused on techniques such as including examples or explicit instructions within prompts \cite{brown2020language}. Later research focused on standardising prompt creation and improving reproducibility using methods such as meta-prompting \cite{reynolds2021prompt, zhou2022large} (prompting an LLM to generate or improve a prompt or prompt template) and using prompt templates \cite{shin2020autoprompt}. \\

Prompt templates are functions containing variables that can be replaced with specific content to create customised inputs - or template instances - for various tasks \cite{shin2020autoprompt}. Prompts may be composed of different components, the key one being the `directive' or `intent' - this is the instruction or question that forms the core of the prompt. Additional components include examples (also known as `shots'), output formatting (such as in markdown or JSON formats), style instructions (used to modify the style rather than the structure of the output), role (a persona given to the LLM that can improve writing style), and additional information that gives the LLM context for the text it is generating \cite{schulhoff2024promptreportsystematicsurvey}.\\

Prompt templates can be designed to incorporate further prompt engineering techniques, such as Chain-of-Thought and Few-shot learning.


\subsection{In-Context Learning (ICL)} 

In-context learning (ICL) refers to LLMs' ability to adapt to new tasks by utilising examples or instructions embedded in the input prompt, eliminating the need for model retraining or parameter adjustments \cite{brown2020language}. Two prominent ICL techniques are chain-of-thought and few-shot prompting.\\

Chain-of-Thought (CoT) prompting \cite{wei2022chain} is a technique for improving the reasoning capabilities of LLMs by prompting the model to generate intermediate steps or thoughts leading to a final answer, rather than directly producing the end result. The concept of CoT prompting was introduced to address the limitations of LLMs in tasks requiring multi-step reasoning or complex problem-solving and has been shown to improve LLM performance on tasks in domains such as mathematics, logic, and analytical reasoning. \\

A typical implementation of CoT prompting involves structuring the input prompt to include examples of step-by-step reasoning and either explicitly or implicitly encouraging the model to emulate a similar thought process in its response. Drawbacks of the approach include that factors such as model size and the complexity of the reasoning required can influence the success of this technique, while generating and validating appropriate CoT prompts can be challenging and time-consuming, particularly for specialised domains. Recent work has explored variations of CoT prompting, including zero-shot CoT \cite{kojima2023largelanguagemodelszeroshot}, which improves LLM performance by incorporating phrases such as ``Letâ€™s think step by step".\\


CoT is a special application of few-shot prompting \cite{brown2020language}, which is a technique whereby several examples are provided within the prompt to guide the language model's response. The principle behind few-shot prompting is to provide task-specific context through examples to help the model understand the task requirements and generate more accurate responses. The effectiveness of few-shot prompting has been demonstrated across various natural language processing tasks, including text classification, question answering, and language translation, and has shown particular promise in scenarios where task-specific training data is limited or when rapid adaptation to new tasks is required. Nevertheless, designing an effective few-shot prompt is a challenging task, as several factors such as exemplar ordering and format may affect outcomes \cite{schulhoff2024promptreportsystematicsurvey}.



\subsection{Prompting for LLM-based Evaluation}


Recent research has explored the use of LLMs as evaluators due to their ability to extract and reason about information, as well as to understand the intent of the users \cite{schulhoff2024promptreportsystematicsurvey}. The prompting techniques used incorporate ICL \cite{kocmi2023gemba}, role-based evaluation \cite{wu2023large}, and CoT prompting \cite{lu2023error}. Output frameworks for evaluation have also been shown to significantly impact the performance of LLMs as evaluators \cite{gao2023human}. A particularly significant component of the output framework is the styling of the output - such as in markdown or JSON format - which has been shown to make the LLM's judgment more accurate \cite{hada2023large, lin2023llm}. Other output formats for evaluation include linear scales, binary scores, and Likert scales \cite{schulhoff2024promptreportsystematicsurvey}.

\section{LoRA Fine-Tuning}



Low-Rank Adaptation (LoRA) \cite{hu2021loralowrankadaptationlarge} is a parameter-efficient fine-tuning method for LLMs. This technique addresses the computational challenges associated with fine-tuning large pre-trained models by introducing a small number of trainable parameters while keeping the majority of the model frozen.\\

The core idea behind LoRA is to approximate the weight updates during fine-tuning using low-rank decomposition. Instead of updating the full weight matrices, LoRA introduces smaller matrices that capture the changes in a low-dimensional space. These additional matrices are then used to modify the output of the original model layers through simple matrix multiplication.\\

Formally, for a pre-trained weight matrix $W \in \mathbb{R}^{d \times k}$, LoRA defines the update as:

\[
h = Wx + BAx
\]

where $A \in \mathbb{R}^{r \times d}$ and $B \in \mathbb{R}^{d \times r}$ are the low-rank matrices, and $r$ is the chosen rank, typically much smaller than $d$ and $k$. Only $A$ and $B$ are trained during the fine-tuning process, significantly reducing the number of trainable parameters.\\

LoRA has demonstrated comparable performance to full fine-tuning on various natural language processing tasks while requiring substantially less computational resources \cite{hu2021loralowrankadaptationlarge}, making it useful for adapting LLMs to specific domains or tasks. Nonetheless, its effectiveness can vary depending on the degree of domain shift from the pre-training data, the choice of which layers to adapt, and the rank of the decomposition.


\section{Retrieval Augmented Generation}

Retrieval Augmented Generation (RAG) \cite{lewis2021retrievalaugmented} is a method for enhancing the capabilities of LLMs by grounding them in external knowledge sources. At its core, RAG operates on the principle of retrieving relevant information from a knowledge base and using this information to augment the input to a language model. It is a hybrid approach that combines the strengths of retrieval and generation tasks in NLP to address limitations in traditional language models, such as hallucination and the inability to access proprietary and/or up-to-date information. This section describes the basic RAG pipeline and explores recent advancements and challenges in this field.\\


The basic RAG pipeline, shown in Figure \ref{fig:basic_rag}, consists of three main components - data indexing, retrieval, and generation. 



\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/basic_rag.pdf}
    \caption{Basic RAG pipeline.}
    \label{fig:basic_rag}
\end{figure}


\subsubsection{Data Indexing}

Data indexing is the first step in the RAG pipeline and involves preprocessing the documents, segmenting them, and embedding these segments to be stored in a vector database. The chunking methods and embedding models selected can have significant impacts on the performance of the RAG system.\\

Chunking defines the granularity of the retrieved context - coarse-grained segments potentially offer more comprehensive information but risk including irrelevant content, while fine-grained units may preserve semantic integrity but increase retrieval complexity. On this spectrum, many chunking strategies exist, from simple fixed-length chunks to more advanced semantic approaches, created to optimise retrieval granularity, which can significantly enhance both retrieval efficacy and downstream task performance \cite{gao2024retrievalaugmented}.\\

The second step involves embedding the chunks by converting the text information into dense vector representations. The embeddings capture semantic meaning in a compact form, allowing for rapid similarity comparisons \cite{gao2024retrievalaugmented}. Typically, the embedding process utilises pre-trained language models or specialised embedding models. For instance, embedding models trained by OpenAI \cite{openaiembeddingmodel} and Voyage AI \cite{voyageembeddingmodel} have gained popularity due to their ability to generate high-quality sentence embeddings (the latter model is used by Anthropic for their LLMs). \\

To index the data within the vector database, metadata can be appended to each chunk with relevant information about the content of the chunk, such as what source file and page number it is taken from. This allows subsequent filtering of the chunks based on this metadata to limit the scope of the retrieval \cite{gao2024retrievalaugmented}.

\subsubsection{Retrieval}

The retrieval component is responsible for identifying the most relevant information from the embedded knowledge base given a particular query or context. Traditional retrieval methods rely on embedding cosine similarity. Semantic cosine similarity \cite{rahutomo2012semantic} measures the angular proximity of non-zero vectors in a vector space, ranging from -1 to 1. It is predominantly used in text vector matching to assess both semantic and structural similarities between texts and is especially effective in high-dimensional spaces due to its focus on vector direction rather than magnitude. Beyond cosine similarity, dense retrieval methods have gained prominence in recent years. Karpukhin et al. \cite{karpukhin2020dense} introduced Dense Passage Retrieval (DPR), which uses separate encoders for queries and passages, trained end-to-end on relevant data. This approach has shown superior performance on complex queries where semantic understanding is crucial. More recent work has focused on hybrid retrieval systems that integrate keyword, semantic, and
vector searches \cite{gao2024retrievalaugmented}. \\

One common problem among these approaches is their reliance on the user's original query for retrieval, as the query may be poorly phrased or misrepresentative of the user's true intent \cite{gao2024retrievalaugmented}. To address this issue, several query optimisation techniques have been developed, including query expansion and transform methods like multi-query \cite{medium_rag_not_working}, sub-query \cite{zhou2023leasttomostpromptingenablescomplex}, and query re-writing using LLMs or smaller specialised models. Query optimisation aims to improve the relevance of the user query to the true context needed to answer it, thereby improving retrieval by similarity measures like embedding cosine similarity. 


\subsubsection{Generation}

Once context is retrieved, it is passed to a pre-trained model along with the initial query to generate a response to the query based on the context. Answer generation can vary depending on the specific task, the most important consideration being whether the model utilises its parametric knowledge or is constrained to the information provided in the retrieved context \cite{gao2024retrievalaugmented}. Fine-tuning is an approach to embed knowledge into the generator LLM to improve its performance on downstream tasks such as question-answering. 


\subsubsection{RAG versus Fine-tuning}

RAG and fine-tuning are two approaches to LLM optimisation. RAG utilises real-time knowledge retrieval, which is advantageous in domains where information is updated frequently and interpretability is valued. However, RAG typically incurs higher latency owing to all the steps required to retrieve context before generating a response. Fine-tuning, on the other hand, modifies model parameters, which allows deeper customization of model behaviour but requires retraining for updates and significant computational resources.\\

Empirical studies \cite{gao2024retrievalaugmented} indicate that RAG shows superior performance in knowledge-intensive tasks compared to unsupervised fine-tuning, particularly for new information, and that LLMs show limited ability to learn new facts through unsupervised fine-tuning alone. Nonetheless, the choice between RAG and fine-tuning depends on the specific requirements of the application, including data dynamics, the need for customisation, and the computational resources available. It is important to note that these methods are not mutually exclusive and can be combined to potentially enhance overall model performance.


\section{Evaluation of LLM Generated Text}

Evaluating the performance of domain-specific question-answering systems built using RAG methods presents unique challenges. This section discusses various evaluation metrics and the specific difficulties associated with assessing knowledge-intensive dialogue systems.

\subsection{NLP Evaluation Metrics}

Traditional NLP evaluation metrics have been crucial in assessing the quality of LLM-generated text across various tasks. Two of the most widely used metrics are BLEU and ROUGE.\\

BLEU (Bilingual Evaluation Understudy) \cite{papieni_bleu} is primarily used for quality evaluation in machine translation. It measures the precision of n-gram matches between the generated text and one or more reference texts. BLEU scores range from 0 to 1, with higher scores indicating better quality translations. The metric computes the geometric mean of n-gram precisions (typically up to 4-grams) and applies a brevity penalty to discourage very short translations. \\


\begin{equation}
    \text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^N w_n \log p_n\right)
\end{equation}

\noindent where $BP$ is the brevity penalty, $N$ is the maximum n-gram order (typically 4), $w_n$ are positive weights summing to 1, and $p_n$ is the modified n-gram precision. The brevity penalty is calculated as:

\begin{equation}
    BP = 
    \begin{cases}
        1 & \text{if } c > r \\
        e^{(1-r/c)} & \text{if } c \leq r
    \end{cases}
\end{equation}

\noindent where $c$ is the length of the candidate translation and $r$ is the effective reference length.\\

ROUGE (Recall-Oriented Understudy for Gisting Evaluation), \cite{lin2004rouge}, is commonly used in text summarisation tasks. Several ROUGE variants exist, with ROUGE-N and ROUGE-L being the most popular. ROUGE-N measures the recall of n-grams between the generated summary and reference summaries, while ROUGE-L considers the longest common subsequence (LCS) and calculates the precision, recall, and F1-score based on the length of the LCS. ROUGE-N focuses on the grammatical correctness and fluency of text, while ROUGE-L focuses on semantic similarity. ROUGE scores typically range from 0 to 1, with higher scores indicating better summarisation quality.\\


\begin{equation}
    \text{ROUGE-N} = \frac{\sum_{S \in \{\text{Reference Summaries}\}} \sum_{\text{gram}_n \in S} \text{Count}_\text{match}(\text{gram}_n)}{\sum_{S \in \{\text{Reference Summaries}\}} \sum_{\text{gram}_n \in S} \text{Count}(\text{gram}_n)}
\end{equation}

\noindent where $\text{Count}_\text{match}(\text{gram}_n)$ is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries, and $\text{Count}(\text{gram}_n)$ is the number of n-grams in the reference summaries.\\


\begin{equation}
    \text{ROUGE-L} = \frac{(1 + \beta^2)R_L P_L}{R_L + \beta^2 P_L}
\end{equation}
\noindent where $R_L$ is the LCS-based recall, $P_L$ is the LCS-based precision, and $\beta$ is a parameter to control the importance of recall and precision (usually $\beta = 1.2$).\\

\noindent The components $R_L$ and $P_L$ are defined as:
\begin{equation}
    R_L = \frac{\text{LCS}(X,Y)}{\text{len}(X)}
\end{equation}
\begin{equation}
    P_L = \frac{\text{LCS}(X,Y)}{\text{len}(Y)}
\end{equation}
\noindent where $\text{LCS}(X,Y)$ is the length of the Longest Common Subsequence between the reference summary $X$ and the candidate summary $Y$, and $\text{len}(X)$ and $\text{len}(Y)$ are the lengths of $X$ and $Y$ respectively.\\

These metrics have been widely used to assess the quality of generated text. However, they have limitations, particularly in the context of conversational AI and knowledge-intensive tasks. Their reliance on exact word matches or predefined synonyms can fail to capture more nuanced semantic similarities, especially in open-ended generation tasks.\\

More recent metrics like BERTScore \cite{zhang2019bertscore} and BLEURT \cite{sellam2020bleurt} leverage pre-trained language models to capture the semantic similarity between generated and reference texts, showing better correlation with human judgments compared to traditional n-gram-based metrics. BERTScore computes cosine similarities between BERT embeddings of generated and reference texts, while BLEURT fine-tunes BERT on human judgments to learn a task-specific metric. 

\subsection{RAG Evaluation}

Recent research has explored various approaches to evaluate the quality and factuality of text generated by LLMs based on a particular source context. One major focus has been on using LLMs themselves to detect hallucinations and estimate faithfulness. Zhang et al. \cite{zhang2023interpretable} suggested using few-shot prompting strategies for predicting factuality, while \cite{min2023factscore} proposed linking generated responses to external knowledge bases. LLMs have also been used to evaluate other aspects of generated text beyond factuality. Fu et al. \cite{fu2023gptscore} introduced GPTScore, which uses generative pre-trained models to score generated texts using prompting techniques such as zero-shot instruction. Wang et al. \cite{wang2023chatgpt} explored directly asking ChatGPT to provide numerical ratings for specific aspects of answers. \\

The evaluation of RAG architectures in particular is a challenging task, requiring many dimensions to be considered. These include the relevance and conciseness of selected context passages, the LLM's ability to accurately interpret and utilise these passages, and the overall quality of the generated output. Frameworks such as RAGAS \cite{es2023ragas} have been developed, proposing a suite of LLM-based evaluation metrics for RAG systems as a whole. \\

Nevertheless, LLM-based evaluation methods have limitations, such as being sensitive to factors like prompt design and presentation order \cite{wang2023large}. To address these limits, recent work by \cite{li2023halueval} has proposed a multi-dimensional evaluation framework for knowledge-intensive question-answering systems that combines automated metrics with human evaluation across dimensions such as factual accuracy, relevance, coherence, and user satisfaction.


\section{Synthetic Question-Answer Data Generation}

This section reviews the literature on the automatic generation of synthetic question-answer (QA) data. QA data is used for training and/or evaluating RAG systems that specialise in Open Domain Question Answering (ODQA) tasks. Automatic generation of synthetic QA data to improve these systems has been an active area of research for decades \cite{mitkov_mcq} \cite{rus2010first}, and is particularly useful in domains where QA datasets do not already exist and/or where curating a human (expert) annotated dataset would involve prohibitive time and monetary costs.\\

However, automatic QA generation is a highly complex task and is particularly difficult for domain-specific RAG systems, where questions must be tailored to test the quality of the system within the context of a specific use case. This is because the questions must be representative of queries that the target user might ask, while the answers must be representative of the type of response the user would expect to receive from their query while staying faithful to the source data. Furthermore, a fully automated QA generation system lacks human oversight of the generated data and therefore must incorporate automatic quality controls. These controls must test both objective metrics, as well as custom metrics that reflect the intended RAG use case within its domain. As a result, while many QA generation systems have been proposed for various use cases, no single system published to date can generate useful data for all domains. 


\subsection{Question-Answer Generation}

Automatic generation methods for QA data have evolved significantly since the introduction of transformers, and the methods proposed continue to evolve as LLM capabilities improve. Before transformers, proposals have been made to enhance sequence-to-sequence models for question generation using methods such as sentence- or paragraph-level information encoding \cite{du2017learning}, modelling question generation using answer embeddings \cite{sun2018answer}, and matching the answer with the passage before question generation \cite{song2018leveraging}. Ultimately, these have shown little success at improving downstream question-answering or reading comprehension tasks \cite{shakeri2020end}. Transformer-based methods have relied on training or fine-tuning of question generators within different architectures, where the more promising approaches \cite{alberti2019synthetic, puri2020training, liu2020asking, bartolo2021improving} adopted a three-step process: (1) answer detection using a pre-trained QA model (alongside custom heuristics), (2) question generation using a fine-tuned LLM to create questions conditioned on a passage and answer, and (3) question filtering using a model fine-tuned for reading comprehension to score questions. \\

The three-step system showed more success on downstream tasks than previous sequence-to-sequence approaches \cite{shakeri2020end}. However, an issue with the approach is that by first selecting candidate answers, it limits the scope of the content that any one single question can be based on, resulting in factual-style questions. It also provides less customisation optionality for the types of questions that are most important to target from a particular domain. Therefore, while these factual-style questions may be useful for benchmarking different RAG architectures or training basic RAG systems, they are less useful for testing a RAG system for a particular domain use case. Furthermore, the three-step system only generates free-text answers, which rely on semantic similarity metrics such as BLEU and ROUGE to be evaluated. A more concrete way of testing the RAG's knowledge is by asking it multiple-choice questions, where there is only one correct answer, and measuring its accuracy. Finally, multi-stage model training and fine-tuning is computationally expensive due to large transformer networks, and leaves room for errors to compound from stage to stage. \\


More recent methods have leveraged large generative models for synthetic data generation \cite{guo2024generativeaisyntheticdata}, including multi-lingual QA generation \cite{han2023multilingual}. Frameworks involving distillation and self-improvement, such as this one by DeepEval \cite{deepeval_qa_gen} and MLflow \cite{mlflow}, have been developed to generate questions and answers based on contexts using LLMs. These systems use simple prompts to ask the LLM to create a question based on a context paragraph \cite{mlflow}. The prompt includes instructions to ask a specific question and generate the answer using as much information as possible. However, these prompts are simple and generic, providing no instruction on what aspects of the context to focus on in the question, what specific details the question should include, or what the structure of the question should be. These issues make them unsuitable for generating domain-specific questions. Nevertheless, there is little to no literature on how to adapt LLM question generation methods to the requirements of a particular domain or generate diverse questions for testing various RAG abilities required for domain-specific question-answering. As such, this project utilises the MLflow \cite{mlflow} framework, adapted for the sustainability reporting domain, as a baseline to develop more sophisticated LLM question-generation methods.


\subsection{Question-Answer Evaluation}

Synthetic QA data generation techniques often employ an `overgenerate and filter' approach to improve question quality \cite{puri2020training}. Many evaluation metrics and methodologies exist to filter questions, and the choice and combination of evaluation metrics often depend on the specific use case of the RAG system and the characteristics of the target domain. One evaluation method is round-trip consistency, proposed by \cite{alberti2019synthetic}. This method involves using a question-answering model to predict an answer to a generated question, and then comparing this predicted answer to the original answer used to generate the question. This approach helps ensure that the generated questions are answerable and closely related to the intended answer. \\

Other fundamental aspects of QA evaluation include the relevance of questions (and answers) to the context as well as their factual accuracy. Diversity of questions is another key factor as different question structures and topics test different abilities of the RAG system. Nevertheless, little work has been done in using LLMs to evaluate other LLM-generated questions. 


\section{Related Work in LLMs for Domain-Specific \\Question-Answering}

Early methods for developing domain-specific question-answering systems involve training RAG architectures, or various components of the architecture, from scratch. These techniques embed knowledge directly into the language models underpinning the system. REALM \cite{guu2020realm} was an early system that pre-trained specific knowledge (in this case, Wikipedia) into the retriever. This was done by augmenting the pre-training with a latent knowledge retriever, allowing the model to access documents during pre-training, fine-tuning, and inference. A pre-training approach is also adopted in RETRO \cite{borgeaud2022improvinglanguagemodelsretrieving}, which uses training and fine-tuning of autoregressive models conditioned on retrieved document chunks. Two recent innovations are RAG-end2end \cite{siriwardhana2022improvingdomainadaptationretrieval}, which explores training the retriever and generator jointly to improve their adaptation to domain-specific question-answering, and RAFT \cite{zhang2024raftadaptinglanguagemodel}, which trains the model to ignore `distractor' documents that don't help in answering the query. \\


The adoption of the above methods to real-world application is hindered by their computationally intensive and inflexible nature. It takes a large amount of external knowledge and significant model adaptation to train or fine-tune models from scratch. Furthermore, once embedded, it is difficult to keep the information up-to-date over time, as that would require re-training \cite{gao2024retrievalaugmented}. As such, modern domain-specific applications of RAG use pipelines where knowledge is retrieved from the external database rather than being embedded into the LLM parameters. This method is less computationally intensive and easier to maintain over time, prompting widespread adoption of LLMs and RAG systems to a wide range of domains. A particularly popular domain is finance, where human/expert-annotated datasets like FinQA \cite{chen2022finqa} and Financebench \cite{islam2023financebench} have enabled the development of large-scale systems such as BloombergGPT \cite{wu2023bloomberggptlargelanguagemodel}. Other popular domains include medicine \cite{xiong2024benchmarkingretrievalaugmentedgenerationmedicine} and pharmaceuticals, where the latter even explored RAG for question-answering in pharmaceutical regulatory compliance \cite{kim2024ragqaragintegratinggenerative}. \\


Nonetheless, there is no standard approach utilised across all domains, as the requirements of specialised systems differ across different domains and tasks. Furthermore, the development of high-quality question-answering systems is more difficult in the absence of a dataset that can be used for training, fine-tuning, and evaluating the system. As a result, this project presents a novel implementation of LLMs and RAG to develop and evaluate a chatbot for the sustainability reporting domain.


\section{Legal, Social Ethical and Professional Considerations }

The domain-specific RAG system presented in this project is crafted to be truthful and trustworthy. This is done by taking steps to minimise hallucinations through sourcing all information, providing succinct answers, and asking guiding questions to the user where more details are necessary to answer a query accurately. Nevertheless, hallucinations may only be minimised within the limits posed by the current technological state of large language models. Therefore, while the system is a reliable source of guidance most of the time, due to the nature of the LLMs underpinning the assistant, there still exists the risk of misrepresentation of the source content through hallucination. In the context of assisting companies in preparing regulatory reports, such hallucinations could potentially result in regulatory violations and legal consequences for the companies that rely on the hallucinated content. It is therefore advisable to leverage the system as purely a guiding assistant rather than as the single source of truth on sustainability reporting standards, and to verify information against the official IFRS reports. \\

The data used to augment the LLM - the IFRS Sustainability Reporting Standards - is not under copyright and may be used by companies freely to aid in the preparation of their regulatory reports. However, the data that the LLMs are trained on may contain information from unverified online sources, potentially without regard towards factuality, copyright, or social and political issues like discrimination, prejudice, and other biases. There may also exist the risk of misuse of LLMs in harmful or dangerous ways. To address these issues, this project takes steps to constrain the system to its specific domain by constructing an agent which ensures that unrelated queries are not answered, basing all responses on the source context, and constraining the language and vocabulary to the style of the source documents. Furthermore, providers of the LLMs used in this project - OpenAI, Anthropic, and Meta - have taken many measures to ensure their models are safe to use.\\

Finally, there exist the social and ethical concerns regarding the environmental impacts of the use of LLMs, which consume large amounts of energy during training on billions of parameters. The providers of these LLMs do not provide a guarantee that they use clean energy sources to train their models, and so reliance on them may perpetuate the use of non-renewable and/or polluting energy sources. However, there is currently no alternative for training LLMs in a less energy-intensive way, and efforts must be made on a large scale within AI to mitigate environmental impacts by using more clean and renewable energy sources.