
\chapter{Domain-Specific Assistant}


This chapter presents the techniques implemented to design a useful domain-specific knowledge assistant. We start by defining the knowledge scope of the chatbot. Then, we describe the methods investigated to design such a chatbot, including RAG, fine-tuning, and classification techniques, and propose two novel architectures for the chatbot - a RAG-based pipeline and an LLM-based pipeline. The dataset developed in Chapter 3 is used along with the IFRS markdown texts to train the classification models, fine-tune LLMs, and conduct experiments to evaluate all techniques. We present the results of the experiments as well as the final performance of the proposed architectures on the QA dataset.


\section{Knowledge Scope}

The knowledge scope for the assistant is defined by the IFRS Sustainability Reporting standards detailed in Section \ref{data_collection}. To be `useful' for answering user queries based on this knowledge, the assistant must be able to retrieve, comprehend, and reason over this data to answer questions, while being constrained from answering out-of-scope queries.

\section{Methods} 

First, an initial investigation is conducted into different RAG techniques by conducting experiments using the QA dataset. Then, an LLM is fine-tuned and tested on the QA dataset. Next, several methods are explored to design an industry classifier, including training an NLP model, two machine learning (ML) methods, a multi-layer perceptron (MLP), and a prompt-based LLM classifier. Finally, two chatbot architectures are proposed - a RAG pipeline and an LLM-based pipeline.

\subsection{Retrieval Augmented Generation (RAG)}

This section explores a selection of RAG techniques for the domain-specific assistant. The steps of a basic RAG pipeline (shown in Figure \ref{fig:basic_rag} of Chapter 2) are outlined below, alongside the alternative methods tested for each step, which are described in detail in the following sections.

\begin{enumerate}
    \item \textbf{Indexing:} text is extracted from the documents and broken down into smaller chunks, which are embedded into vector representation using an embedding model and stored in a vector DB. \\
    \textit{Chunking methods tested:} fixed-size chunks of 256, 512, and 1024 tokens, page, rolling window, semantic chunking, and custom markdown chunking.
    \item \textbf{Retrieval:} When a user asks a question, the query is embedded (using the same model as the data) into a vector and is compared to the stored vectors to find the most similar matches. In the basic case, the query is embedded as is (with no transforms applied), and KNN is used as the retrieval technique.\\
    \textit{Retrieval techniques tested:} KNN and hybrid.\\
    \textit{Query Transformation techniques tested:} HyDE, multi-query.
    \item \textbf{Generation: }This is the final step, where the user's question and the best-matching information are combined and given to an LLM to create an answer.
    \textit{LLMs tested:} Google Gemma 2B, Metaâ€™s Llama 2 13B, Llama 3.1 8B, Llama 3 70B, Mixtral 8x7B, and fine-tuned Llama 3.1 8B.
\end{enumerate}


\subsubsection{Models}


To assess the impact of LLM size and capabilities on RAG performance, a selection of models is tested. The models included are smaller parameter models like Google's Gemma 2B and Meta's Llama 3.1 8B, mid-sized models such as Llama 2 13B, and larger models like Llama 3 70B and Mixtral 8x7B. The models differ in their training data, architecture, and parameter count, which can influence their reasoning capabilities. For instance, the impact of model scale on performance can be examined using the Llama family models (2 13B, 3.1 8B, and 3 70B) share a similar architecture but vary in size. Mixtral 8x7B, on the other hand, utilises a mixture-of-experts architecture \cite{shazeer2017outrageouslylargeneuralnetworks} and gives insight into how this compares to the other models. \\

The fine-tuned Llama 3.1 8B model described in Section \ref{finetuning} is also tested to evaluate the impact of domain-specific fine-tuning on performance.

\subsubsection{Data Chunking} \label{data_chunking_methods}

One limitation of LLMs is their restricted context window which limits their comprehension abilities for extensive documents \cite{liu2024lost, lmsys}. To prepare the markdown data extracted from the PDF reports for retrieval, it must be broken down into chunks - smaller, more focused segments that the LLM can process with greater precision. Data chunking can impact the performance of RAG systems by directly impacting the quality and relevance of retrieved context being inputted into the LLM for answer generation \cite{liu2024lost}. This project explores seven chunking strategies to determine the optimal approach for the Sustainability Reporting domain. The chunking strategies explored are fixed-size chunks of 256, 512, and 1024 tokens, page-based chunking, a rolling window approach, semantic chunking, and custom markdown chunking. \\

Fixed-size chunks of 512 tokens are a common baseline in RAG systems, balancing granularity with computational efficiency. To investigate the impact of chunk size on performance, 256- and 1024-token chunks are also evaluated. These larger chunks may provide less/more comprehensive context for complex queries but could potentially reduce retrieval precision by missing information or confusing the LLM.\\



One issue with fixed-length chunking strategies is that relevant information might be split across chunk boundaries. To address this, a rolling window approach with 512-token chunks and 10\% overlap is implemented to improve retrieval robustness by ensuring that key information is not inadvertently separated. \\

A page-based chunking strategy is implemented to preserve the original document structure and may be particularly helpful for queries referencing specific pages. Semantic chunking is also explored to create chunks based on topical coherence rather than arbitrary token counts. This method aims to preserve the semantic integrity of the content, potentially improving the relevance of retrieved chunks for complex queries. \\

Finally, this project leverages the markdown structure of the reports to implement a custom chunking strategy. Markdown chunking splits the reports by their headings at a chosen level of hierarchy (e.g. at the section/subsection level). Additionally, it chunks tables as a whole, separately from free-text content. Maintaining the integrity of tables reduces the risk of the retriever missing or misinterpreting relevant rows or columns of data. This method is akin to that introduced by Yepes et al \cite{yepes2024financial} for the finance domain. \\

The chunked data is embedded using the OpenAI embedding model text-embedding-3-small and stored in Pinecone, a vector database. Metadata is appended to each chunk containing the page number the text is taken from, the report name, the industry, and the type of information (free text or table) contained in the chunk. Chunking strategies are systematically evaluated on the diverse synthetic QA dataset to determine the most effective method for this domain. Results are shown in Section \ref{chunking_results}.



\subsubsection{Retrieval Techniques}

Retrieval techniques define how the most relevant context chunks for a query are determined, and choice of technique can significantly influence the effectiveness of RAG systems \cite{gao2024retrievalaugmented}. This study examines three widely used retrieval techniques: KNN and hybrid.\\

K-Nearest Neighbors (KNN) retrieves the K most similar chunks based on embedding cosine similarity. This is a simple and efficient method but may struggle with capturing complex relationships between queries and documents. Hybrid approaches combine embedding-based similarity with traditional text search techniques such as BM25 (Best Match), a search algorithm that selects documents according to their query relevance, using Term Frequency (TF), Inverse Document Frequency (IDF), and Document Length to compute a relevance score \cite{sawarkar2024blended}. This can be particularly effective for queries that contain both conceptual elements and key terms or phrases. These retrieval techniques are systematically evaluated to identify the most effective method for the sustainability reporting domain.  \\

Three further popular retrievers - SVM, linear regression, and Maximum Marginal Relevance (MMR) - are also tested, with results shown in Appendix \ref{retriever_appx}. 
 

\subsubsection{Query Transforms}


The study explores two query transformation techniques: Hypothetical Document Embeddings (HyDE), and multi-query expansion. Each method is an approach to modifying or expanding the original query to potentially improve retrieval performance. The baseline approach involves using the original query without any modifications. \\

Hypothetical Document Embeddings (HyDE) \cite{gao2022precise} is a query transformation technique that leverages LLMs to generate a hypothetical document that could answer the query, and then uses the embedding of this document for retrieval. The process involves inputting the original query to an LLM, which then generates a hypothetical document that could potentially answer the query. This hypothetical document is then embedded and used, instead of the original query, to retrieve real documents. HyDE aims to bridge the semantic gap between queries and documents by creating a more comprehensive representation of the user's information needs. Gao et al. \cite{gao2022precise} demonstrated that HyDE can improve retrieval performance across various domains, particularly for complex or ambiguous queries. However, the hypothetical document is fake and therefore may contain false details, introducing hallucinations into the retrieval process. \\

Multi-query expansion \cite{medium_rag_not_working} involves generating multiple variations or reformulations of the original query to capture different aspects of the information needed. This technique analyses the original query to identify key terms or concepts and formulates alternative formulations of the query, often using synonyms, related terms, or different phrasings. These multiple queries are then used for retrieval using parallel vector searches and intelligent reranking to aggregate results. The goal of multi-query expansion is to improve recall by addressing vocabulary mismatch problems and capturing a broader range of relevant documents.


\subsection{Fine-tuning}\label{finetuning}

Another method for integrating domain-specific data into an LLM is fine-tuning (as discussed in Chapter 2). The Llama 3.1 8B model was fine-tuned using the final MCQ QA pairs to enhance its performance on multiple-choice questions without RAG, by learning the specific information and thus reducing the hallucination rate. LoRA finetuning was implemented using Together.AI. The performance of the fine-tuned Llama 3.1 8B was compared to both the original Llama 3.1 8B and a larger Llama 3.1 40B model on local and cross-industry questions. 

 
\subsection{Industry Classification} 
Six models were implemented for multi-label industry classification: a natural language processing (NLP) approach, a machine learning (ML) approach, a Multi-Layer Perceptron (MLP) model, and two prompting-based approaches using GPT-4o and GPT-4o Mini. Each model was designed to output multiple industry labels per input question, accommodating up to 5 industries per question.

\subsubsection{NLP Model (DistilBERT)} 

The NLP model is based on DistilBERT \cite{sanh2020distilbertdistilledversionbert}, a smaller and more efficient version of BERT tailored for sequence classification. The model includes a pre-classifier layer and a final classifier with sigmoid activation, allowing for multi-label classification by outputting probabilities for each industry. Input text was tokenized using the distilbert-base-uncased tokenizer.

\subsubsection{ML Models (Random Forest and XGBoost)} 

The machine learning approach involved two models: a Random Forest classifier and an XGBoost model. Both models were trained on text embeddings generated using OpenAI's text-embedding-3-small model. The Random Forest model was composed of 200 decision trees, while the XGBoost model used 100 estimators. Each model was tuned to predict multiple industry labels per input, with a custom threshold applied to the output probabilities to determine the final industry predictions.

\subsubsection{MLP Classifier}

The Multi-Layer Perceptron (MLP) classifier was structured with two hidden layers and trained using the same embeddings as the ML models, using a sigmoid activation in the output layer to generate probabilities for each industry, allowing for multi-label classification. This model was selected to learn non-linear patterns in the input data.

\subsubsection{LLM Classifier} \label{llm_classifier}

The prompting-based approach used GPT-4o and GPT-4o Mini to classify industries using a custom prompt containing industry descriptions. The returned predictions were structured using a Pydantic model, allowing for variable industry outputs depending on the input query.


\subsubsection{Evaluation Metrics} 

The performance of each model was evaluated using the following metrics: \begin{itemize} \item Macro F1 Score: This metric calculates the harmonic mean of precision and recall for each industry label, treating each label equally, regardless of its frequency in the dataset. It provides a balanced measure of performance across all industry categories. \item Precision: Precision measures the proportion of correctly predicted industry labels out of all labels predicted by the model, indicating how accurate the model is when it makes a positive prediction. \item Recall: Recall calculates the proportion of correctly predicted industry labels out of all true industry labels, evaluating how well the model identifies all relevant industries. \item Hamming Loss: This metric quantifies the fraction of labels that are incorrectly predicted. It accounts for both false positives and false negatives, providing insight into how often the model makes incorrect label assignments. \end{itemize}


\subsection{Proposed Architecture}

For the final question-answering system, two architectures are proposed. The first one leverages the RAG techniques studied along with the GPT-4o Mini industry classifier. The second one uses LLMs throughout the entire pipeline. Both pipelines integrate a pre-processing step to stop the chatbot from answering queries that are not related to IFRS sustainability reporting standards. This is done by passing the user query through an LLM call to verify that it is relevant before proceeding. 

\subsubsection{Custom RAG Pipeline}

A custom RAG pipeline is designed that combines the strengths of the RAG methods tested. In particular, it integrates two novel elements - the prompt-based industry classifier and the fine-tuned Llama 3.1 8B model - with established RAG techniques to create a more targeted system for answering queries related to sustainability reporting standards. The pipeline is shown in Figure \ref{fig:rag_pipeline}.\\

User queries are checked for domain relevancy and passed into the LLM industry classifier described in Section \ref{llm_classifier}, which identifies the most likely industries the query is referring to. The chunks stored in the vector database, which were chunked by the custom markdown method, are then filtered using their metadata (since each chunk is labelled by industry as described in Section \ref{data_chunking_methods}) to constrain the database to only chunks from those industries. The top 5 most relevant chunks are then retrieved from the filtered database using KNN. These are passed to the fine-tuned Llama 3.1 8B model which uses them to generate a response.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/rag_pipeline.pdf}
    \caption{Custom RAG pipeline.}
    \label{fig:rag_pipeline}
\end{figure}

\subsubsection{Fully LLM-based Pipeline}

The second question-answering pipeline, shown in Figure \ref{fig:llm_rag}, is designed using LLMs throughout. After the query has been deemed relevant to the domain, the information retrieval and answer generation steps are as follows:
\begin{enumerate}
    \item The LLM classifier is used to output the most likely industries related to the query.
    \item The markdown report is selected for each industry. 
    \item For each industry separately, the associated industry markdown report is passed along with the user query to an LLM, which retrieves the chunks of context that are best suited to answer the question. The LLM used for this step is GPT-4o Mini as it has a long enough context length.
    \item The chunks for each industry are labelled with the industry, and all industry chunks are combined.
    \item The combined chunks are passed along with the user query to the fine-tuned Llama 3.1 8B model to generate an answer.
\end{enumerate}


This architecture displays two key differences relative to the RAG pipeline. Firstly, the system operates by passing inputs directly into LLMs, without embedding data or the query. This bypasses the need for similarity search methods for retrieving relevant chunks and instead leverages the reasoning abilities of the LLM to gauge relevance. Secondly, the data is not broken down into chunks beforehand. Instead, an LLM call takes as input the entire industry markdown to select the most relevant chunks based on the user query. This provides flexibility to tailor the number and content of the retrieved chunks to each query, rather than relying on a fixed chunking strategy and retrieval number. \\


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/llm_pipeline.pdf}
    \caption{Custom LLM pipeline.}
    \label{fig:llm_rag}
\end{figure}


\section{Results}

All methods are evaluated on both MCQ and free-text questions. Accuracy is used on MCQs, and BLEU and ROUGE-L are used for evaluation on free-text questions.

\subsection{RAG}

\subsubsection{Models}



\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
\multirow{3}{*}{Model} & \multicolumn{2}{c}{MCQ} & \multicolumn{4}{c}{Free text} \\ \cline{2-7} 
 & Local & Cross-Industry & \multicolumn{2}{c}{Local} & \multicolumn{2}{c}{Cross-Industry} \\  
 & Accuracy & Accuracy & BLEU & ROUGE & BLEU & ROUGE \\ \hline
Google Gemma 2B         & 12.90 & 17.64 & 0.18 & 0.36 & 0.05 & 0.14 \\
Llama 2 13B             & 38.71 & 17.64 & 0.17 & 0.34 & 0.01 & 0.03 \\
Llama 3.1 8B            & 80.65 & 52.94 & 0.41 & 0.60 & 0.06 & 0.23 \\
\begin{tabular}[c]{@{}l@{}}Llama 3.1 8B \\ Fine-tuned\end{tabular} & \textbf{83.87} & 53.16 & \textbf{0.49} & \textbf{0.65} & \textbf{0.07} & \textbf{0.24} \\
Llama 3 70B             & 80.65 &\textbf{ 60.72} & 0.42 & 0.62 & 0.04 & 0.19 \\
Mixtral 8x7B            & 80.65 & 37.25 & 0.18 & 0.38 & 0.05 & 0.02 \\ \hline
\end{tabular}
\caption{RAG results with different LLM and baseline settings: 512-token chunking, KNN retrieval with top K=5 chunks, no query transform, and LLM temperature of 0.5.}
\label{tab:models}
\end{table}


Table \ref{tab:models} shows that Llama 3.1 models generally outperform others on both local and cross-industry questions, and little variance is observed across model size between Llama 3.1 8B and 40B. However, a large gap exists between Llama 3.1 8B and Llama 2 13B, which is a much less capable model despite being larger. Llama 2's performance is more comparable to Google Gemma 2B, showing the same performance in MCQ accuracy (17.64\%). \\

The fine-tuned Llama 3.1 8B model shows the best performance across all metrics for local questions, as measured by both MCQ accuracy and free-text BLEU and ROUGE. In particular, it shows better performance than the significantly larger model Llama 3 70B. However, the fine-tuned model's outperformance over the standard Llama 3.1 8B model is smaller for cross-industry questions, performing worse on these than the larger 70B model. This suggests that fine-tuning (when coupled with RAG) improves the model's ability to handle highly specific `factual' queries, though provides limited benefit for more complex queries, where larger models can leverage their enhanced reasoning power.\\

All models show a noticeable drop in performance for cross-industry questions compared to local ones. Nevertheless, one notable observation is that, while cross-industry performance is low, it is still above average since a random answer selection strategy would result in a 20\% accuracy for MCQs with 5 answer options.\\

Finally, it is of note that BLEU and ROUGE are generally very low, particularly for cross-industry questions. This is discussed in Section \ref{models_discussion}.
 

\subsubsection{Data Chunking}\label{chunking_results}


\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
\multirow{3}{*}{Chunking} & \multicolumn{2}{c}{MCQ} & \multicolumn{4}{c}{Free text} \\ \cline{2-7} 
 & Local & Cross-Industry & \multicolumn{2}{c}{Local} & \multicolumn{2}{c}{Cross-Industry} \\   
 & Accuracy & Accuracy & BLEU & ROUGE & BLEU & ROUGE \\ \hline
Sentence 256    & 80.65 & 41.26 & 0.25 & 0.48 & \textbf{0.09} & \textbf{0.24} \\
Sentence 512    & 80.65 & 52.94 & \textbf{0.41} & \textbf{0.60} & 0.06 & 0.23 \\
Sentence 1024   & 38.71 & 31.09 & 0.28 & 0.49 & 0.06 & 0.22 \\
Sentence window & 81.32 & 54.55 & 0.04 & 0.19 & 0.05 & 0.15 \\
Page            & 65.12 & 37.45 & 0.03 & 0.16 & 0.06 & 0.17 \\
Semantic        & 83.30 & 63.90 & 0.33 & 0.55 & 0.06 & 0.23 \\
Custom Markdown & \textbf{84.52} & \textbf{69.10} & 0.04 & 0.19 & 0.04 & 0.15 \\ \hline
\end{tabular}
\caption{RAG results with different chunking methods and baseline settings: Llama 3.1 8B, KNN retrieval with top K=5 chunks, no query transform, and LLM temperature of 0.5.}
\label{tab:chunking}
\end{table}


Table \ref{tab:chunking} shows the performance of the Llama 3.1 8B model across different chunking strategies. For MCQ accuracy, the custom markdown chunking method performs best, achieving 84.52\% for local and 69.10\% for cross-industry questions. This is followed closely by semantic chunking at 83.30\% and 63.90\% respectively.\\

In the free text evaluation, sentence-based chunking strategies with 256 and 512 tokens generally outperform other methods, particularly for local scenarios. Sentence 512 chunking achieves the highest BLEU (0.41) and ROUGE (0.60) scores for local free-text responses. Performance drops significantly for cross-industry tasks across all chunking methods. The page-level chunking shows the poorest performance overall, particularly for free text tasks. \\

A notable result is that Sentence 1024 shows very poor performance for MCQs, but reasonable results on free-text questions. Reasons for this are discussed in Section \ref{chunking_discussion}. 


 

\subsubsection{Retrieval Techniques}
\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
\multirow{3}{*}{Retriever} & \multicolumn{2}{c}{MCQ} & \multicolumn{4}{c}{Free text} \\ \cline{2-7} 
 & Local & Cross-Industry & \multicolumn{2}{c}{Local} & \multicolumn{2}{c}{Cross-Industry} \\  
 & Accuracy & Accuracy & BLEU & ROUGE & BLEU & ROUGE \\ \hline
KNN     & \textbf{80.65} & \textbf{52.94} & \textbf{0.41} & \textbf{0.60} & \textbf{0.06} & \textbf{0.23} \\
Hybrid            & \textbf{80.65} & 47.05 & 0.37 & 0.58 & \textbf{0.06} & \textbf{0.23} \\ \hline
\end{tabular}
\caption{RAG results with different chunking methods and baseline settings: Llama 3.1 8B, retrieval of top K=5 chunks, no query transform, and LLM temperature of 0.5.}
\label{tab:retriever-comparison}
\end{table}


Table \ref{tab:retriever-comparison} presents the performance of the two retrieval techniques tested. The retrievers perform equally well on local MCQs, achieving 80.65\% accuracy, though KNN performs better on Cross-industry MCQs, achieving 52.94\% versus 47.05\%. KNN also displays the strongest performance on free-text questions, obtaining BLEU and ROUGE scores of 0.41 and 0.60 for local free text responses. Both retrievers exhibit a significant drop in performance when moving from local to cross-industry tasks. 


\subsubsection{Query Transforms}

\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
\multirow{3}{*}{Query Transform} & \multicolumn{2}{c}{MCQ} & \multicolumn{4}{c}{Free text} \\ \cline{2-7} 
 & Local & Cross-Industry & \multicolumn{2}{c}{Local} & \multicolumn{2}{c}{Cross-Industry} \\  
 & Accuracy & Accuracy & BLEU & ROUGE & BLEU & ROUGE \\ \hline
None        & 80.65 & 52.94 & \textbf{0.41} & \textbf{0.60} & 0.06 & 0.23 \\
HyDE        & \textbf{83.87} & 48.38 & 0.26 & 0.46 & 0.07 & 0.22 \\
Multi-Query & 48.39 & \textbf{54.83} & 0.18 & 0.35 & \textbf{0.11} & \textbf{0.25} \\ \hline
\end{tabular}
\caption{RAG results with different chunking methods and baseline settings: Llama 3.1 8B, KNN retrieval with top K=5 chunks, and LLM temperature of 0.5.}
\label{tab:query-transform-comparison}
\end{table}

Table \ref{tab:query-transform-comparison} illustrates the impact of different query transforms on the performance of the Llama 3.1 8B model across local and cross-industry tasks. The ``None" transform, representing no query modification, serves as a baseline, achieving 80.65\% accuracy for local MCQs and 52.94\% for cross-industry MCQs. It also shows relatively strong performance on local free text questions with BLEU and ROUGE scores of 0.41 and 0.60 respectively.\\

HyDE (Hypothetical Document Embeddings) demonstrates the best performance for local MCQs, improving accuracy to 83.87\%. However, it shows a slight decrease in cross-industry MCQ accuracy (48.38\%) compared to the baseline. HyDE's impact on free text metrics is mixed, with lower scores for local tasks but a marginal improvement in cross-industry BLEU.\\

The Multi-Query transform significantly underperforms in local MCQs (48.39\%), despite achieving the highest accuracy for cross-industry MCQs at 54.83\%. This suggests that the Multi-Query approach may be particularly effective for more complex queries requiring information synthesis across multiple domains (this is discussed further in Section \ref{query_transform_discussion}. Its free text performance is generally lower than the baseline, except for a slight improvement in cross-industry BLEU and ROUGE scores.


\subsection{Fine-tuning}

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\hline
& \begin{tabular}[c]{@{}c@{}}MCQ\ Local\end{tabular} & \begin{tabular}[c]{@{}c@{}}MCQ\ Cross-Industry\end{tabular} \\ \hline
Llama 3.1 8B  & 0.46  & 0.48\\
Llama 3.1 40B & \textbf{0.58} & 0.50   \\
Llama 3.1 8B fine-tuned & 0.49  & \textbf{0.52}\\ \hline
\end{tabular}
\caption{Accuracy of the Llama 3.1 8B, Llama 3.1 40B, and fine-tuned Llama 3.1 8B models on MCQs, without RAG.}
\label{tab:fine-tuning}
\end{table}

Table \ref{tab:fine-tuning} presents the accuracy of three Llama 3.1 models on local and cross-industry multiple-choice questions (MCQs) without using RAG. The Llama 3.1 40B model demonstrates the highest accuracy (0.58) on local MCQs, while the fine-tuned Llama 3.1 8B model performs best on cross-industry MCQs with an accuracy of 0.52. Notably, the fine-tuned 8B model outperforms the base 8B model on both local and cross-industry questions, suggesting that domain-specific fine-tuning can enhance model performance. The 40B model shows superior performance on local questions, indicating that it is able to use its reasoning capabilities to deduce the correct answer. Nevertheless, this result is much lower than the performance of Llama 3.1 8B with RAG.


\subsection{Industry Classification}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\hline
Model                    & Recall & Precision & Macro F1 score & Hamming Loss \\ \hline
RAG                      & 0.29   & 0.28      & 0.44     & 0.06         \\
Random Forest Classifier & \textbf{0.84}   & 0.45      & 0.59     & 0.02         \\
XGBoost                  & 0.62   & 0.77      & 0.69     & \textbf{0.01}         \\
MLP (Neural Network)     & 0.96   & 0.65      & 0.78     & \textbf{0.01}         \\
gpt-4o-mini              & 0.83   & \textbf{0.93}      & 0.86     & \textbf{0.01}         \\
gpt-4o                   & \textbf{0.84}   & \textbf{0.93}      & \textbf{0.87}     & \textbf{0.01}    \\ \hline     
\end{tabular}
\caption{Performance comparison of cross-industry classifiers.}
\label{tab:industry-classifiers}
\end{table}

Table \ref{tab:industry-classifiers} compares the performance of various industry classification models. The evaluation metrics include recall, precision, macro F1 score, and Hamming loss. The gpt-4o model achieves the highest overall performance with a macro F1 score of 0.87 and a Hamming loss of 0.01. The MLP (Neural Network) model demonstrates the highest recall (0.96), while gpt-4o and gpt-4o-mini show the highest precision (0.93). The RAG-based approach exhibits the lowest performance across all metrics, indicating that specialised classification models are more effective for this task. Among the machine learning models, XGBoost shows a balanced performance with high precision (0.77) and a low Hamming loss (0.01). These results suggest that both pre-trained language models and specialised machine learning approaches can be effective for industry classification, although LLM approaches are superior.

\subsection{Proposed Architecture}

\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
\multirow{2}{*}{Pipeline} & \multicolumn{2}{c}{MCQ} & \multicolumn{4}{c}{Free text} \\ \cline{2-7} 
 & Local & Cross-Industry & \multicolumn{2}{c}{Local} & \multicolumn{2}{c}{Cross-Industry} \\  
 & Accuracy & Accuracy & BLEU & ROUGE & BLEU & ROUGE \\ \hline
Baseline        & 80.65 & 52.94 & 0.41 & 0.60 & 0.06 & 0.23 \\
Custom RAG & 85.32 & 72.15 & 0.45 & 0.61 & 0.13 & 0.26 \\
Fully LLM-based & \textbf{93.45} & \textbf{80.30} & \textbf{0.47} & \textbf{0.66} & \textbf{0.15} & \textbf{0.30} \\ \hline
\end{tabular}
\caption{Performance comparison of the custom RAG and LLM-based pipelines on MCQ and free-text questions. The baseline approach uses Llama 3.1 8B, and KNN retrieval with top K=5.}
\label{tab:proposed-pipelines}
\end{table}

The table \ref{tab:proposed-pipelines} provides a comparative analysis of the performance metrics for two different pipelines: the custom RAG pipeline and the fully LLM-based pipeline.\\ 

For MCQs, the custom RAG pipeline achieves an accuracy of 85.32\% for local questions and 72.15\% for cross-industry questions. The fully LLM-based pipeline demonstrates higher accuracy, with 93.45\% for local questions and 80.30\% for cross-industry questions. Most notably, both approaches achieve much higher performance on cross-industry questions than the baseline, and the LLM-based pipeline achieves an accuracy on cross-industry MCQs that is similar to the baseline performance on local questions.\\

On free-text questions, the custom RAG pipeline records BLEU and ROUGE scores of 0.45 and 0.61 for local queries, and 0.13 and 0.26 for cross-industry queries. The fully LLM-based pipeline achieves BLEU and ROUGE scores of 0.47 and 0.66 for local queries, and 0.15 and 0.30 for cross-industry queries. BLEU and ROUGE scores of the custom pipelines show a lower improvement than accuracy on MCQs, though this is a result of the metrics themselves, as discussed in Section \ref{models_discussion}. \\

Overall, the fully LLM-based pipeline achieves the best performance across all types of questions.

\section{Discussion}

\subsection{RAG}
\subsubsection{Models}\label{models_discussion}

Model size and capability have a strong impact on RAG performance. The smallest model, Google Gemma 2B, is most often unable to answer the question posed, and when it does provide an answer it is usually wrong. Llama 2 13B is an older, less capable model, and although it is bigger than the newer 8B models, it struggles to answer questions directly and recites the source content instead, showing its weak reasoning capabilities. These issues are particularly pronounced for more complicated cross-industry questions.\\

Listing \ref{lst:models_qa_example} below shows a local multi-hop question example with the RAG responses given by each model. The correct answer must contain two parts: the first part requires extracting information from a Table, while the second is verbatim text from the source document. Most models (except Gemma 2B) got the second part correct. However, only the fine-tuned model gets the first part of the answer correct, showcasing that certain knowledge has been embedded into the fine-tuned model and is being used successfully when combined with RAG methods.\\

Also notable is that all models contain slightly different phrasings of the same answer, containing additional words/details to various extents. Even though multiple phrasings can be correct, BLEU and ROUGE metrics are sensitive to exact phrasing, resulting in low BLEU and ROUGE scores, particularly for more complicated cross-industry questions. Local questions, on the other hand, can more successfully be evaluated using BLEU and ROUGE as they are often shorter and more factual, so a correct answer will score above 0.9 on both metrics.

\begin{lstlisting}[language=json,firstnumber=1,label={lst:models_qa_example},caption={Example local multi-hop question with RAG answers using different models}]
Question: "What is the category of the 'Payload fuel economy' metric in the Health Care Distributors industry and how should this metric be calculated?"

Correct answer: "The category of the 'Payload fuel economy' metric is Quantitative. Payload fuel economy shall be calculated as: total gallons of fuel consumed / revenue tons miles (RTM), where revenue ton miles (RTM) = total weight of paid tonnage transported (payload) x total distance in miles goods were transported."

Google Gemma 2B answer: "The category of the 'Payload fuel economy' metric in the Health Care Distributors industry is **Transportation**.\n\nThe metric should be calculated as:\n\n```\nPayload fuel economy = total gallons of fuel consumed / revenue tons miles (RTM)\n```\n\nwhere:\n\n* **total gallons of fuel consumed** is the total amount of fuel consumed by the entity's transportation fleet.\n* **revenue tons miles (RTM)** is the total weight of paid tonnage transported (payload) multiplied by the total distance in miles goods were transported."
"bleu_score": 0.36
"rouge_l_score": 0.65

Llama 2 13B answer: "Based on the context information provided, the 'Payload fuel economy' metric in the Health Care Distributors industry falls under the category of 'Fleet Fuel Management' and should be calculated as the total gallons of fuel consumed divided by the revenue tons miles (RTM) of the delivery fleet, where RTM is the weight of paid tonnage transported (payload) multiplied by the total distance in miles goods were transported. This calculation should be aggregated for types of transportation, including air, marine, rail, and road."
"bleu_score": 0.30
"rouge_l_score": 0.50

Llama 3.1 8B answer: "The category of the 'Payload fuel economy' metric is HC-DI-110a.1. This metric should be calculated as total gallons of fuel consumed divided by revenue ton miles (RTM), where revenue ton miles (RTM) is the total weight of paid tonnage transported multiplied by the total distance in miles goods were transported."
"bleu_score": 0.60
"rouge_l_score": 0.75

Llama 3.1 8B fine-tuned answer: "The category of the 'Payload fuel economy' metric is Quantitative. This metric should be calculated as total gallons of fuel consumed divided by revenue ton miles (RTM), where revenue ton miles (RTM) is the total weight of paid tonnage transported multiplied by the total distance in miles goods were transported."
"bleu_score": 0.65
"rouge_l_score": 0.77

Llama 3 70B answer: "The category of the 'Payload fuel economy' metric in the Health Care Distributors industry is HC-DI-110a.1. This metric should be calculated as total gallons of fuel consumed divided by revenue tons miles (RTM), where revenue ton miles (RTM) equals total weight of paid tonnage transported (payload) multiplied by total distance in miles goods were transported."
"bleu_score": 0.61
"rouge_l_score": 0.74

Mixtral 8x7B answer: "The 'Payload fuel economy' metric in the Health Care Distributors industry falls under the category of Fleet Fuel Management. This metric should be calculated as the total gallons of fuel consumed divided by revenue tons miles (RTM), where RTM is equal to the total weight of paid tonnage transported (payload) multiplied by the total distance in miles goods were transported. Payload includes the weight of paid tonnage and excludes the vehicle weight. The entity should aggregate payload fuel economy for different types of transportation, including air, marine, rail, and road transportation."
"bleu_score": 0.31
"rouge_l_score": 0.56
\end{lstlisting}



\subsubsection{Data Chunking} \label{chunking_discussion}

A notable observation from Table \ref{tab:chunking} is that combining large fixed-sized chunks (Sentence 1024) with top-5 retrieval hurts performance on MCQs, but not on free-text questions. This suggests that the retrieved context confuses the LLM when it has to choose between five similar answer options, but not when the LLM writes its own response to the question in free text format. Interestingly, this is not a problem for semantic/markdown chunking, which also contains large information chunks. This is because, even though the chunks are large, they are semantically coherent, whereas fixed-size chunks do not contain complete information as text is simply cut off at fixed points. \\

Semantic and custom markdown chunking methods yield the best results, as the context given to the LLM is coherent. The custom markdown adds the additional benefit of isolating tables, so more specific context can be retrieved for questions that ask for specific information from tables.



\subsubsection{Retrieval Techniques}

While both KNN and hybrid retrievers yield similar performance, there is a notable drop in accuracy on cross-industry MCQs using the hybrid retriever. This could suggest that incorporating text-based keyword search does not provide a benefit for the sustainability reporting domain, and may indeed detract from the power of embedding similarity.


\subsubsection{Query Transforms}\label{query_transform_discussion}



Hyde exhibits 4\% better performance on local MCQ accuracy than the baseline, but 8.6\% lower performance on cross-industry performance. This suggests that the hypothetical generated document is often accurate for factual questions, but struggles for more complex queries based on multiple source documents. This is evident in Listing \ref{lst:query_transforms_example}, where HyDE's answer to a cross-industry question contains hallucinated content such as the ``FTSE Nareit Classification Structure". \\


Multi-query exhibits a 40\% reduction in accuracy on local answers and a 3.6\% improvement in cross-industry accuracy relative to baseline. Its superior ability to handle complex questions is shown in its highly accurate answer in Listing \ref{lst:query_transforms_example}. However, the process of rephrasing questions in multiple ways harms the response to specific factual questions that are sensitive to detail and phrasing. This is shown in Listing \ref{lst:query_transforms_local}, where the baseline query gets the correct answer, but rephrasing the query multiple times induces hallucination in the last part of the answer. The fall in quality of simple factual responses is too great to justify the advantages of multi-query transforms for answering complex questions.
\newpage
\begin{lstlisting}[language=json,firstnumber=1,label={lst:query_transforms_local},caption={Example local single-hop question with RAG answers using different query transforms}]
Question: "What are some examples of efforts to reduce the environmental impact of logistics for the Health Care Distributors industry?"

Correct answer: "Relevant efforts to describe include, but are not limited to, upgrades to fleet (fuel efficiency), usage of alternative and/or renewable fuels, optimized logistics routes, and idling reduction programs.",

Baseline answer: "Efforts to reduce the environmental impact of logistics for the Health Care Distributors industry may include upgrading to more fuel-efficient fleets, using alternative and/or renewable fuels, optimizing logistics routes, and implementing idling reduction programs."
"bleu_score": 0.34
"rouge_l_score": 0.57

Multi-query answer: "Efforts to reduce the environmental impact of logistics for the Health Care Distributors industry may include upgrading to more fuel-efficient fleets, using alternative and/or renewable fuels, optimizing logistics routes, implementing idling reduction programs, partnering with energy-efficient suppliers, and investing in smart logistics technologies to enhance route efficiency and decrease fuel consumption.",
"bleu_score": 0.13
"rouge_l_score": 0.28

HyDE answer: "Health care distributors can implement various strategies to reduce the environmental impact of their logistics operations. For instance, they can optimize their transportation routes and modes to minimize fuel consumption and emissions. This might involve using more fuel-efficient vehicles, such as electric or hybrid trucks, or switching to rail or sea transportation for longer distances. Additionally, distributors can work with suppliers to reduce packaging materials and implement sustainable packaging practices, such as using biodegradable or recyclable materials. They can also explore alternative fuel options, like biofuels or hydrogen fuel cells, to power their fleets. Furthermore, distributors can invest in route optimization software to reduce idling time and lower emissions. By implementing these strategies, health care distributors can not only reduce their environmental footprint but also improve their bottom line by reducing fuel costs and increasing efficiency.",
"bleu_score": 0.004
"rouge_l_score": 0.115
\end{lstlisting}

\begin{lstlisting}[language=json,firstnumber=1,label={lst:query_transforms_example},caption={Example cross-industry single-hop question with RAG answers using different query transforms}]
"question": "Which industry has more 'Quantitative' metrics to disclose in Table 1 between Real Estate Services and Agricultural Products industries?"

"correct_answer": "The Agricultural Products industry has more 'Quantitative' metrics to disclose, with seven metrics in Table 1 compared to the Real Estate Services industry, with three metrics."

No transform answer: "Based on the provided context, it appears that the Agricultural Products industry has more 'Quantitative' metrics to disclose in Table 1. This is evident from the detailed metrics listed under the \"Metrics\" section, which include specific calculations and thresholds for disclosure, such as FB-AG-440a.1 and FB-AG-110a.1. In contrast, there is no mention of specific metrics or disclosure requirements for the Real Estate Services industry in the provided context."

Multi-query answer: "The Agricultural Products industry has more 'Quantitative' metrics to disclose in Table 1 compared to the Real Estate Services industry."

HyDE answer: "The Real Estate industry has 4 'Quantitative' metrics to disclose in Table 1, specifically Leasable floor area, by property subsector, Percentage of indirectly managed assets, by property subsector, and Average occupancy rate, by property subsector. These metrics are aligned with the FTSE Nareit Classification Structure and are disclosed separately for each portion of the entity's portfolio where properties are classified into subsectors."
\end{lstlisting}

\subsubsection{Industry Retrieval}

A consistent theme observed in the RAG results across all methods is that performance on cross-industry questions is lower than on local ones. This phenomenon was manually investigated by examining the chunks retrieved for cross-industry queries. It was found that the retriever gets chunks from multiple industries that could be related to the query, thus confusing the LLM that generates the answer. One such example question is shown in Listing \ref{lst:industry_query_example} below along with the retrieved industries using the baseline RAG method, as well as those classified by the LLM industry classifier. The baseline RAG selects five chunks from four different related industries, where the target is two industries. The LLM classifier identifies the correct industries as being the highest probability ones intended by the user.


\begin{lstlisting}[language=json,firstnumber=1,label={lst:industry_query_example},caption={Example cross-industry single-hop question}]
"question": "What are the activity metrics for the car transport and air transport industries?"
"target industries": "b61-airlines", "b63-automobiles"
"industries retrieved by baseline RAG": "b62-auto-parts" (x2), "b63-automobiles", "b60-air-freight-and-logistics", "b61-airlines"
"industries output by the LLM classifier": "b63-automobiles", "b61-airlines"
\end{lstlisting}


\subsection{Proposed Architecture}

Both custom pipelines demonstrate significant improvements in accuracy on cross-industry questions, and this can be largely attributed to the integration of the LLM industry classifier, which constrains the domain of knowledge from which information is retrieved for a query, thus increasing the likelihood of retrieving the correct chunks. \\

Only the LLM-based pipeline, however, reaches performance on cross-industry questions that is comparable to performance on local ones, despite both architectures using the same industry classifier. This is explained by the difference in chunking and retrieval methods adopted in the two approaches. The RAG pipeline still utilises information that has been `generically' segmented, rather than chunks that are customised to the query itself. Furthermore, top-5 retrieval is used rather than allowing an LLM to decide on the appropriate number of chunks based on their context. While the top-5 retrieval hurdle can be tackled using more sophisticated techniques, such as by setting a similarity threshold and selecting all chunks above this, selecting this threshold is complex, requiring thorough experimentation, and there is unlikely to be a single threshold that is appropriate for all query types. Finally, the RAG-based pipeline relies on embedding similarity to select chunks, which was the best-performing retriever tested.\\

The LLM-based pipeline retrieves the most customised context for answering the specific query, lessening the burden on the generator LLM of utilising its own reasoning capabilities to understand the information and generate an answer. It also minimises the chances of irrelevant context being retrieved (which is a problem when a specific number of chunks must be retrieved at each query). Nevertheless, this architecture is not without weaknesses. Most notably, the increased number of LLM calls may increase the chance of hallucination of the retrieved content. Furthermore, there seems to be a limit to its performance on complex questions still, as the accuracy on cross-industry questions is 80.30\% rather than being closer to the 93.25\% accuracy on local MCQs. This suggests there exist limits to the reasoning capabilities of the generator LLM, despite being provided with improved context.
