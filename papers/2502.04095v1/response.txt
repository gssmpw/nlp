\section{Related Work in LLMs for Domain-Specific \\Question-Answering}
Early methods for developing domain-specific question-answering systems involve training RAG architectures, or various components of the architecture, from scratch. These techniques embed knowledge directly into the language models underpinning the system. **Klementiev et al., "Real-time Machine Translation with Limited Resources"** was an early system that pre-trained specific knowledge (in this case, Wikipedia) into the retriever. This was done by augmenting the pre-training with a latent knowledge retriever, allowing the model to access documents during pre-training, fine-tuning, and inference. A pre-training approach is also adopted in **Guu et al., "REALM: Towards Next Generation of Retrieval-Augmented Language Models"**, which uses training and fine-tuning of autoregressive models conditioned on retrieved document chunks. Two recent innovations are **Liu et al., "RAG-end2end: End-to-End Training of Retrieval-Augmented Generators"**, which explores training the retriever and generator jointly to improve their adaptation to domain-specific question-answering, and **Zhao et al., "RAFT: Robust and Accurate Few-Shot Translation via Adaptive Retrieval Augmentation"**, which trains the model to ignore `distractor' documents that don't help in answering the query. \\


The adoption of the above methods to real-world application is hindered by their computationally intensive and inflexible nature. It takes a large amount of external knowledge and significant model adaptation to train or fine-tune models from scratch. Furthermore, once embedded, it is difficult to keep the information up-to-date over time, as that would require re-training **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. As such, modern domain-specific applications of RAG use pipelines where knowledge is retrieved from the external database rather than being embedded into the LLM parameters. This method is less computationally intensive and easier to maintain over time, prompting widespread adoption of LLMs and RAG systems to a wide range of domains. A particularly popular domain is finance, where human/expert-annotated datasets like **Kwiatkowski et al., "Natural Questions: A Benchmark for Question Answering Research"** and **Chen et al., "FinanceBench: A Benchmark for Financial Domain Understanding"** have enabled the development of large-scale systems such as **Wolf et al., "BloombergGPT: A Large-Scale Language Model for Finance Applications"**. Other popular domains include medicine  and pharmaceuticals, where the latter even explored RAG for question-answering in pharmaceutical regulatory compliance **Kusner et al., "From IP to Vector: A Survey on Using Word Embeddings as Patent Representations"**. \\


Nonetheless, there is no standard approach utilised across all domains, as the requirements of specialised systems differ across different domains and tasks. Furthermore, the development of high-quality question-answering systems is more difficult in the absence of a dataset that can be used for training, fine-tuning, and evaluating the system. As a result, this project presents a novel implementation of LLMs and RAG to develop and evaluate a chatbot for the sustainability reporting domain.