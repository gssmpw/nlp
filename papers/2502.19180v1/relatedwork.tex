\section{Related Work}
\label{sec:related}
% the related work has to focus on how our approach differs from what was done previously -- the focus right now is missing.
% Be explicit about how the related work relates to your work. E.g.,
% expect that our approach Y should perform better”
%Our work develops a benchmark for drift compensation, which is solved using techniques from the AutoML literature, which is why both will be reviewed in short.
\paragraph{Automated Machine Learning (AutoML)}
Machine Learning has succeeded in countless applications~\cite{TangWangWuChenPangSunFengWang+2023, TangWangWuLiuZhangZhuChenSunWang+2022+849+872,Schaller2023LiquorHGNNAH, schaller2024modeconvnovelconvolutiondistinguishing}, raising the demand for automated and streamlined solutions. The field of AutoML, which aims to find well-performing models automatically, has been receiving increased attention~\cite{AutoML:SMAC}. To ensure the flexibility and robustness needed for our sensor drift task, different AutoML techniques are applied, such as dynamic feature selection, model tuning, and adaptation to new drift patterns without extensive manual intervention, addressing the limitations of prior approaches that require fixed models and extensive domain knowledge.

Several frameworks, such as SMAC3 or auto-sklearn implement these techniques and are used in many different use cases~\cite{AutoML:auto-sklearn, AutoML:SMAC, AutoML:CASH}. Many AutoML frameworks resort to approaches such as Bayesian optimization to guide the non-trivial search for strong hyperparameters given a specific model~\cite{AutoML:SMAC, AutoML:auto-sklearn}. The problem of algorithm selection (AS) aims to find the most suitable algorithm for a given task. Other fields, such as Neural Architecture Search (NAS) aim to find new neural network architecture and topologies, to solve new tasks~\cite{AutoML:NAS}. 


\paragraph{Drift Compensation}\label{sec:drift_compensation}
%For completeness we review commonly used methods for sensor drift compensation. 
Prior drift compensation methods can be categorized into five types: component correction, adaptive methods, sensor signal preprocessing, attuning methods, and machine learning approaches.

\emph{Component correction methods} use methods such as Principal Component Analysis (PCA) or Independent Component Analysis (ICA) to identify and eliminate drift components~\cite{artursson2000drift, DINATALE2002158}. For dynamically evolving data sets, which regularly change due to drift, these component correction methods would need continual retraining to consider current statistics—making them labor-intensive and inefficient in comparison to systems designed to compensate dynamically without regular re-training. Furthermore methods like PCA, primarily a linear dimensionality reduction method, assumes that the main variability in the data can be captured in a reduced orthogonal space. This works well for stable datasets but can underperform if variability is erratic, time-dependent, or non-linear. ICA finds components that are statistically independent, which might not align with how drift manifests over time. Drift often appears as correlated sequential data changes not fully captured by static independence assumptions. In comparison our AutoML Drift Compensation framework allows a flexible adaptation by learning patterns, updating as the data evolves without the need for constant retraining from scratch, unlike static PCA/ICA frameworks.

%However, CCPCA assumes uniform behavior across all classes during drift, which may not always be accurate~\cite{artursson2000drift}.
%In contrast, Di Natale et al.~\cite{DINATALE2002158} addressed sensor drift using an attuning method that employs Independent Component Analysis (ICA) to separate signal and disturbance components.
%This method focuses on eliminating components more correlated with disturbances, effectively removing drift caused by external factors.

\emph{Adaptive methods} include evolutionary algorithms that optimize a multiplicative correction factor for incoming samples. These algorithms, like the one proposed by Di Carlo et al.~\cite{DICARLO20111594}, continuously adapt the correction factor through linear transformations within a restricted time window. Although Evolutionary algorithms can find optimal solutions within complex, high-dimensional spaces, the multiplicative correction factor assumes drift can be corrected through simple linear scaling, which might not suffice for nonlinear drift patterns. The focus on short-term optimization can also lead to overfitting to noise or transient anomalies in the data hindering adaptation to sustained nonlinear drift dynamics. In comparison our AutoML ensemble methods might capture multi-faceted drift patterns by combining models that individually address different components of the drift. AutoML-DC can also include model evaluation strategies that balance fitting the data while avoiding over-adjustment to noise.

\emph{Preprocessing methods} involve baseline manipulation and filtering strategies. Baseline manipulation transforms sensor signals based on initial values using differential, relative, or fractional transformations. Filtering strategies, such as the Discrete Wavelet Transform (DWT), mitigate drift by discarding low-frequency components associated with drift and reconstructing the signal from the remaining components~\cite{HUI2003354}. Nevertheless, preprocessing techniques generally assume that drift patterns, such as baselines or low-frequency components, remain constant over time. This constancy allows them to calibrate and correct the data based on fixed parameters. Thus it is not useful for dynamically adjusting to new drift patterns or evolving drift like in our usecase, as it is used in a rather static manner. With the Auto-ML DC model, we instead combine and learn different preprocessing parameters dynamically according to the temporal drift patterns, that are learnable. Choosing and combining preprocessing strategies alongside model configurations also allows more immediate responses to evolving drifts. 

\emph{Attuning methods} aim to correct drift components without relying on calibration samples, instead deducing drift directly from training data. Orthogonal Signal Correction (OSC) is one such method, which removes non-correlated variance in sensor-array data~\cite{PADILLA201028}.Methods like Orthogonal Signal Correction (OSC) remove components orthogonal to the drift, thus eliminating the non-correlated variance in the data set. Thus, They rely on previously seen drift effects being representative for current and future drift compensation. Attuning methods often rely on the assumption that drift manifests in identifiable components (e.g., orthogonality) that are separated and compensated. Unlike attuning methods that are preset to correct only previously identified drift components, AutoML-DC can learn from broader, potentially evolving drift patterns within and beyond initial training data, which is shown in the extensive experiments with different training strategies. In cases where drift doesn’t appear as (e.g. orthogonal) component, AutoML-DC might also recognize shifts in sensor behavior dynamically across the operational data range.

\emph{Machine learning approaches} initially focused on adaptive drift correction using neural networks~\cite{kohonen}. These methods, however, demand a substantial number of training samples and are tightly integrated with specific algorithms. To address flexibility, Vergara et al.~\cite{VERGARA2012320} introduced an ensemble drift compensation method, utilizing features such as steady-state and normalized responses and employing classifiers like SVMs. 
Various machine learning models have been proposed, often using random train-test splits or cross-validation, and they are thus trained in a setting other than our proposed drift compensation setting. While Machine Learning models typically require large volumes of training data to accurately model the drift, especially when using neural networks or complex models to ensure convergence and generalization, we use meta-learning strategies to start from an informed initial configuration, reducing the need for exhaustive training data. While methods like those in Vergara et al.'s ensemble often rely on pre-selected models and handcrafted feature sets, AutoML-DC offers dynamic adaptability by exploring a broad range of models and hyperparameter settings automatically, selecting combinations that best capture the current drift patterns.