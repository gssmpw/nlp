
\subsection{Evaluating Benefits from Sparsity}

Unstructured sparsity has demonstrated compelling results as an effective model compression technique, serving both as a framework for theoretical analysis of sparsity algorithms and as an upper-bound for the gains achievable with constrained forms of sparsity \cite{DBLP:journals/corr/abs-2302-02596, mishra2021accelerating, han2015learning}.
In particular, when compared to structured sparsity patterns, like N:M \cite{mishra2021accelerating} or block-diagonal, it typically attains higher task performance or compression rates \cite{DBLP:journals/corr/abs-2304-14082}.
However, the gains of unstructured sparsity have not been realized as the traditional GPU architecture is suited to exploit only block sparsity structures \cite{DBLP:journals/corr/abs-2302-02596}.
Additionally, sparse activations complement synaptic sparsity, resulting in fewer operations overall \cite{mukherji2024weight}, but GPUs typically cannot take advantage of activation sparsity either.
% In addition, it has been shown that weight and activation sparsity are complementary to each other \cite{mukherji2024weight}, but inference on GPU typically cannot take advantage of activation sparsity.
Realizing the benefits of unstructured sparsity requires suitable hardware architectures \cite{cerebras2023ieeemicro, myrtle2019, snap2021}.
% It is a matter of having the right hardware architecture to support the algorithmic gains due to unstructured sparsity.
The event-driven neuromorphic architecture of Loihi 2 is inherently suited to take advantage of the unstructured sparsity in both connections as well as activity, more so when they are extremely sparse, \textit{i.e.,} $\geq 90\%$. Therefore, we choose to compare the benefits of efficiency gained from sparsity on Loihi 2 with equivalent dense networks on an edge GPU.

Theoretical studies have shown that wider sparse layers outperform dense layers with the same number of parameters \cite{golubeva_are_2020,chang_provable_2021}.
Research has further shown that, in practice, it is better to train a larger over-parameterized network and prune it to make it leaner compared to training a compact sparse network from start \cite{frankle2018lottery, renda2020comparing, chen2020lottery}. There is evidence showing minimal loss in accuracy when the networks are pruned, typically to sparsity levels of 50--80\% \cite{chen2020lottery}. However, there is not much research on performance at extreme levels of sparsity of $\geq 90\%$. % i.e.\ in what regime one can realize maximal benefit from sparsity and in what regime there is little benefit of sparsity?
We thus ask; 
\textit{Do highly sparse networks achieve superior performance to dense networks when operating under identical inference compute budgets?
How does the performance benefit of sparsity vary with increased compute budget?}

% However, previous research on unstructured sparsity 

% - 

% - Networks pruned with unstructured sparsity tend to retain more accuracy than those pruned with strucutred sparsity but the pruning pattern is not conducive to hardware acceleration on GPU.\cite{mishra2021accelerating} % . Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks.
%   The need for right hardware and algorithm match


% Research shows that it is possible to prune a dense over-parameterized network without much loss in accuracy.
% % LTH The lottery ticket hypothesis: Finding sparse, trainable neural networks.
% % Comparing rewinding and fine-tuning in neural network pruning.
% % The lottery ticket hypothesis for pre-trained bert networks.
% But the fall-off is naturally expected at extreme levels of sparsity.

% \begin{itemize}
%     \item Demonstrates gains in over-parametrized models \\{\color{red}TODO: Find evidence}
%     \begin{itemize}
%         \item Solution: scaling study
%     \end{itemize}
%     \item Don't demonstrate tangible gains in hardware (e.g.\ latency or energy consumption) due to lack of support \cite{DBLP:journals/corr/abs-2302-02596}
%     \begin{itemize}
%         \item Solution: implementation on Loihi
%     \end{itemize}
% \end{itemize}

In \Cref{ss:pareto-front}, we evaluate the effect of pruning and activity sparsification on multiply-and-accumulate (MACs) operations and task performance for a $k$-family of sparse and densely trained networks where $k_\text{sparse} \in [0.5, 3.0], \ k_\text{dense} \in [0.25, 1.0]$ is the width scaling factor of the networks.
In linear layers, which account for most of the computation in the S5 architecture, MACs scale linearly with weight and pre-activation sparsity. The detailed MAC calculation is reported in \Cref{supp:macs}.
Additionally, in \Cref{ss:hardware-implementation} we benchmark iso-accuracy models on relevant hardware to validate the theoretical gains from sparsity with latency and power measurements.

\subsection{Model Compression}

\paragraph{Synaptic pruning}

Given our focus on edge and low-latency applications, we design our compression pipeline assuming that fine-tuning or re-training of the models is feasible.
Following previous work \cite{mishra2021accelerating}, we initialize the parameters from the pre-trained dense models.
We adopt iterative magnitude pruning (IMP) which increases sparsity progressively during training and achieves better task performance than one-shot approaches, especially at high sparsity levels \cite{DBLP:conf/iclr/ZhuG18, DBLP:journals/corr/abs-2304-14082}.
% Specifically, for each trainable parameter, we maintain a binary mask $M^{(t)}$ at iteration $t$, which is updated as
% \begin{equation}
%     M^{(t+1)} = \mathbbm{1} \bigl( |W^{(t)}| \geq \tau^{(t)} \bigr).
%     \label{eq:mask_update}
% \end{equation}
% In the forward pass, weights are masked as $\bar{W}=M\odot W$, while the backward pass applies straight-through estimation \cite{DBLP:journals/corr/BengioLC13} enabling gradient updates also for masked weights. 
% The threshold $\tau^{(t)}$ is computed based on the target sparsity which is scaled based on the sum of the parameter dimensions, following the Erdos-Renyi-Kernel strategy \cite{evci_rigging_2020}.
% Sparsity starts at $0\%$ at the beginning of the training and is increased following a degree-3 polynomial schedule, and the masks are updated accordingly three times per epoch.
% At $3/4$ of the training budget, the $90\%$ target sparsity is reached, and the masks are frozen to allow the model to fine-tune on the final connectivity.

Specifically, we train for $E$ epochs with $T$ update steps in total. Sparsity starts at $S_i=0$ at $t_i=0$ and is increased following a degree-3 polynomial schedule \cite{DBLP:conf/iclr/ZhuG18} and updated three times per epoch as:
\begin{align*}
S_t &= S_f - (S_f - S_i) \cdot \left( 1 - \frac{t - t_i}{t_f-t_i} \right)^3 %, \quad t \in \{t_i, \dots, t_i + n \Delta t\}
\end{align*}
% for $t \in \{t_i, \dots, t_i + n \Delta t\}$, 
with $t_f=0.75 T$.
%
Given the total sparsity $S_t$ and weights $W_t^\ell \in \mathbb{R}^{N^\ell \times M^\ell}$ at time $t$ and position $\ell$ in the network, we scale the sparsity $s^\ell_t$ for each weight according to the Erd√∂s-Renyi-Kernel (ERK) strategy \cite{evci_rigging_2020,mocanu_scalable_2018} to compute the mask $M_t^\ell$:
%s
\begin{align*}
s_t^\ell &= s_t \cdot \frac{N^\ell + M^\ell}{N^\ell \cdot M^\ell} \\
% \end{align}
% %
% We then create a mask $M_t^\ell$ that induces sparsity as: 
% % keeps only the top-$k_t^\ell$ values where $k_t^\ell = c$:
% \begin{align}
M_t^\ell &= \mathbbm{1} \left( |W_t^\ell| \geq \tau_t^\ell \right) \\
% \tau_t^\ell &= \min \left[ \text{TopK} \left( |W_t^\ell|, k_t^\ell \right) \right]
\tau_t^\ell &= \min \left[ \text{TopK} \left( |W_t^\ell|, s_t^\ell N^\ell M^\ell \right) \right]
\end{align*}
where $\tau_t^\ell$ is the calculated threshold for $W_t^\ell$ to reach sparsity $s_t^\ell$ and $\text{TopK}(W, k)$ gives the top-$k$ values from $W$.
In the forward pass, weights are masked as $\bar{W}=M\odot W$, while the backward pass applies straight-through estimation \cite{DBLP:journals/corr/BengioLC13} enabling gradient updates also for masked weights. 
% Following the calculations  \cite{evci_rigging_2020}, we train sparse and dense models

\paragraph{Activity sparsification}

Sparsifying layer activations provide another means for reducing the compute and on-chip memory requirements during inference.
In particular, sparse pre-activations of linear layers can significantly reduce the number of MACs required for the associated matrix-vector multiplication (MVM), if appropriately supported by the hardware backend.
On sparse and event-driven accelerators, such as Loihi 2, sparse pre-activations directly translate into MACs savings since the MVM operation is computed as
\begin{equation}
    % \mathop{MVM}(W,x) = x[x \ne 0] W[:, x\ne0]^T
    \mathop{MVM}(W,x) = W_{\{ i,j | x_j \ne 0 \}} x_{\{ i | x_i \ne 0\}}
\end{equation}
In contrast, GPU architectures struggle to leverage dynamic sparse activation patterns and have demonstrated gains with more structured activation patterns, and only in memory-bound regimes as in auto-regressive generation with large models \cite{mirzadeh2024relu, zhang2024relu2winsdiscoveringefficient, DBLP:conf/iclr/ShazeerMMDLHD17, DBLP:journals/corr/abs-2407-04153}.

Techniques for activation sparsity include top-k \cite{DBLP:journals/corr/abs-2412-04358}, sigma-delta coding \cite{shrestha2024efficient, o2016sigma}, sparse mixture-of-experts \cite{fedus_switch_2022,he_mixture_2024} and \emph{ReLU-fication} \cite{mirzadeh2024relu}.
We base our methodology on the latter of these. Since ReLU is a fully element-wise operation, it doesn't require synchronization across channels which would complicate implementation in compute-memory integrated platforms, such as Loihi 2.
Following previous work on transformer models \cite{mirzadeh2024relu}, we start from the original dense model with GELU non-linearity, as shown in \autoref{figure_3}, and apply two modifications.
First, we replace the GELU activation with a ReLU, sparsifying pre-activations of the linear layer in the GLU block.
Second, we insert additional ReLU activations after the residual add in the GLU block and to the real component of the S5 hidden layer, further increasing the pre-activation sparsity of linear operators.
Both model surgeries are applied to the pre-trained model at the beginning of the iterative pruning procedure, enabling accuracy recovery from both weight and activation pruning without extra training budget.


\paragraph{Quantization and fixed-point computation}

Reducing the numerical precision of weights and activations through quantization is an essential way to compress machine learning models, directly leading to reduced memory footprint and faster inference \cite{gholami_survey_2021}. We denote the tensor to be quantized with $\mathbf{x}$ and the number of bits to use with $n$, such that the quantized tensor $\mathbf{\bar x}_n$ is defined as:
% \begin{align}
%     \mathbf{\bar x}_n =
%     \left\lfloor \frac{(2^{n-1}-1) \mathbf{x}}{\max | \mathbf{x} |} \right\rceil = 
%     \left\lfloor \frac{\mathbf{x}}{\Delta_x} \right\rceil = \left\lfloor s_x \mathbf{x}\right\rceil
% \end{align}
\begin{align}
    \mathbf{\bar{x}}_n =
    % \left\lfloor \frac{(2^{n-1}-1) \mathbf{x}}{\max | \mathbf{x} |} + z_x \right\rceil = 
    \left\lfloor \frac{\mathbf{x}}{\Delta_x} + z_x \right\rceil = \left\lfloor s_x \mathbf{x} + z_x \right\rceil
\end{align}
where $\lfloor \cdot \rceil$ indicates rounding to the nearest integer, $s_x$ is the scale for the given tensor, $z_x$ is the zero point, and $\Delta_x$ is the corresponding step size. For simplicity, we choose $s_x = (2^{n-1}-1) (\max |\mathbf{x}|)^{-1}$ and $z_x = \mathbf{0}$, \textit{i.e.}, we use symmetric quantization based on the absolute maximum.

% There are primarily two types of quantization strategies: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) \cite{nagel_white_2021}. 
Post-training quantization (PTQ) applies quantization to a pre-trained model without further training, which is computationally efficient but may lead to a notable drop in accuracy, especially for complex models or tasks \cite{gholami_survey_2021}. Without constraints during training, it has been shown to under-perform on both nonlinear \cite{wu_googles_2016} and linear RNNs \cite{abreu2024q}.
In contrast, quantization-aware training (QAT) incorporates quantization into the training process using straight-through estimators for the gradients \cite{DBLP:journals/corr/BengioLC13}, allowing the model to adapt to the reduced precision and typically achieving superior performance retention compared to PTQ \cite{hubara_quantized_2018}, which has also shown promising results on linear RNNs such as S4D \cite{meyer2024diagonal} and S5 \cite{abreu2024q} on synthetic tasks from the Long Range Arena benchmark \cite{DBLP:conf/iclr/Tay0ASBPRYRM21}.
%
To demonstrate advantages on hardware, we use static quantization \cite{gholami_survey_2021} using only fixed-point (integer) arithmetic \cite{wu_integer_2020}. Whereas in dynamic quantization, scales $s_x$ are computed dynamically on incoming data (and therefore requiring floating-point operations), static quantization pre-computes scales for all weights and activations in the neural network and ``freezes'' these scales so that the network can be converted to use only fixed-point arithmetic.

Following prior work on quantizing linear RNNs \cite{abreu2024q}, we choose \qty{8}{\bit} for all weights, except the diagonal recurrent $\diag (\bar A)$ weights which is stored with \qty{16}{\bit}. All activations are quantized to \qty{16}{\bit}. We denote this quantization recipe with W8A16. This is a more compressed quantization scheme than previous work that deployed a linear RNN to fixed-point hardware using W8A24 \cite{meyer2024diagonal}.
% 
% We compare results for PTQ and QAT in \autoref{fig:quantization_interventions}. 
For the linear RNNs that are deployed to the Loihi 2 chip, we combine QAT with sparse training. 
% For our implementation of QAT, we use the AQT library \cite{aqt}.% with JAX which slows down our neural network training by a factor of 2--3.


\subsection{Porting S5 to Loihi 2}

Running S5 on Loihi 2 requires a range of adjustments, to fully leverage the neuromorphic architecture and to adhere to its constraints. As a result, the S5 network shown in \hyperref[figure_3]{Figure \ref{figure_3}} is transformed into a network of synapses and neurons for Loihi 2 as illustrated in \hyperref[fig:loihi-implementation]{Figure \ref{fig:loihi-implementation}}.
In general, a state vector of dimension $\mathbb{R}^{M}$ is encoded by M neurons. Matrix-vector multiplications are hardware accelerated by the synaptic layers, which take a vector of neuron activities, multiply it with the matrix of synaptic weights, and pass the output to the next layer of neurons.
Since complex numbers are not natively supported on Loihi 2, the complex matrices $\bar{B}$ and $\bar{C}$ have been split into two synaptic layers each, representing the complex and real parts. Similarly, the complex state $x_k$ is stored by two neuronal states.
The remaining operations are performed within the assembly-programmable neurons.

A single layer of programmable neurons can efficiently fuse many operations on the vector it encodes. This applies to all element-wise operations where each neuron must operate only on its local states.
The neuronal layers thus implement ReLUs, BatchNorm, Hadamard products, residual add, and multiplications of a state vector with a diagonal matrix.
Applying this layer fusion, the full S5 architecture only requires one neuron group for the encoder, one for the decoder, and three for each S5 block. 
The detailed mapping of operations to neuron groups is illustrated in \autoref{fig:loihi-implementation},