Linear Recurrent Neural Networks (RNNs) have recently emerged as powerful primitives for sequence modeling, both in isolation or hybridized with self-attention, achieving impressive results in language modeling \cite{DBLP:conf/icml/PoliTNPDKSHER0M24}, audio generation \cite{DBLP:conf/icml/GoelGDR22}, and genomics \cite{DBLP:conf/nips/NguyenPFTWBMPRB23}, among other domains.
This success has been ignited by advances in initialization \cite{DBLP:conf/nips/GuDERR20}, parametrization \cite{DBLP:conf/iclr/GuGR22}, and parallelization \cite{DBLP:conf/nips/GuJGSDRR21,DBLP:journals/corr/abs-2312-00752,DBLP:conf/iclr/SmithWL23} of these models, which, combined, enabled efficient large-scale training on GPUs.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/figure_1.pdf}
    \caption{Model compression and acceleration pipeline for linear RNNs, tailored to the Intel Loihi 2 chip.}
    \label{figure_1}
\end{figure}

At inference time, linear RNNs iteratively compress the input sequence into a finite-dimensional representation whose dimensionality does not depend on the sequence length.
Their memory requirements remain constant regardless of sequence length, and runtime scales linearly with sequence length. In contrast, transformer architectures \cite{DBLP:conf/nips/VaswaniSPUJGKP17} exhibit linear memory growth and quadratic runtime scaling as sequence length increases.
This advantageous scaling makes linear RNNs especially well-suited for real-time long-range sequence modeling on edge devices that require low latency, a small form factor, and are subject to weight and power constraints, as common for applications like audio denoising \cite{Timcheck_2023}, keyword spotting \cite{DBLP:journals/corr/abs-1804-03209}, or perception-and-control \cite{DBLP:conf/nips/0001SGPF0B23}.
% Crucially, model optimization and compression techniques to accelerate the inference of linear RNNs remain under-explored.
Although model optimization and compression are essential for enabling efficient edge machine learning by reducing resource demands, their application to accelerate the inference of linear RNNs remains under-explored.

%% NOTE: move this to related work somewhere (background?)
% Standard quantization-aware training (QAT) has been shown to work well for state space models on many, but not all, tasks from the Long Range Arena (LRA) \cite{abreu2024q} and post-training quantization for sequential image processing tasks has been shown to work well, too \cite{meyer2024diagonal}. Post-training quantization has been shown to be difficult for selective state space models \cite{pierro2024mamba} though methods are being investigated to tackle these difficulties \cite{chiang2024quamba}. 

% With their temporal dynamics, linear RNNs are a promising match for \textit{neuromorphic} processors, which can efficiently update stateful neurons due to a tight integration of massively parallel compute and memory. Neuromorphic processors are an emerging class of brain-inspired hardware architectures, with notable examples like IBM’s NorthPole \cite{modha2023}, SpiNNaker 2 \cite{DBLP:journals/corr/abs-1911-02385}, Tianjic \cite{Pei2019}, and Intel’s Loihi 2 \cite{orchard2021}. In a recent proof-of-concept, we have shown that Loihi 2 is faster and more energy efficient than a GPU to run inference of small artificial workloads using a linear RNN \cite{meyer2024diagonal}. Beyond parallelism and compute-memory integration, different neuromorphic processors offer unique sets of further computational features, including event-driven compute and messaging, low-precision arithmetic, and support for unstructured sparse weight matrices. These sets of features offer unique opportunities to optimize and compress linear RNNs for real-world applications. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/Figure_S5_Diagram.pdf}
    \caption{Overview of the S5 architecture. Symbols are shown as defined by equations in \Cref{ss:linear-rnns}.}%\eqref{eq:x_k}-\eqref{eq:glu}.}
    \label{figure_3}
\end{figure*}

Linear RNNs are a promising match for \textit{neuromorphic} processors, which can efficiently update stateful neurons due to a tight integration of massively parallel compute and memory. Neuromorphic processors are an emerging class of brain-inspired hardware architectures, with notable examples like IBM’s NorthPole \cite{Modha2023}, SpiNNaker 2 \cite{DBLP:journals/corr/abs-1911-02385}, Tianjic \cite{Pei2019}, and Intel’s Loihi 2 \cite{DBLP:conf/sips/OrchardFRSSSD21}. Beyond parallelism and compute-memory integration, different neuromorphic processors offer unique sets of further computational features, including event-driven compute and messaging, low-precision arithmetic, and support for unstructured sparse weight matrices. These sets of features offer unique opportunities to optimize and compress linear RNNs for real-world applications.

In this work, we explore the potential of unstructured sparsity--in weights and activations--and fixed-point quantization for the compression of linear RNNs and acceleration on neuromorphic hardware as illustrated in \autoref{figure_1}.
Specifically, we explore four key research questions:
\textit{
\textbf{1)} Can we train linear RNNs with high synaptic and activation sparsity while retaining high performance?
\textbf{2)} Do highly sparse linear RNNs outperform dense linear RNNs across different inference compute budgets?
\textbf{3)} Can fixed-point quantization compress sparse linear RNNs without damaging the network's performance?
\textbf{4)} Can unstructured sparsity and fixed-point quantization be translated into latency and energy advantages on neuromorphic hardware?
}
% \begin{enumerate}
%     \item \label{rq1}\textit{Can we train linear RNNs with high synaptic and activation sparsity while retaining high performance?} % How do weight and activations sparsity interact?
%     % ANSWER: IMP/dense2sparse-IMP + ReLU + SDNN (maybe topk?) // they compete with each other? -> fig5 on act sparsity
%     \item \label{rq2}\textit{Do highly sparse linear RNNs outperform dense linear RNNs across different inference compute budgets?} 
%     % ANSWER: yes, but diminishing returns when moving to high-accurayc regime 
%     \item \label{rq3}\textit{Can fixed-point quantization compress sparse linear RNNs without damaging the network's performance?}
%     % ANSWER: QAT is enough, then careful calculation of scales and fxp simulation
%     \item \label{rq4}\textit{Can unstructured sparsity and fixed-point quantization be translated into latency and energy advantages on neuromorphic hardware?}
%     % ANSWER: hell yeah, RIP Jetson
%     %%%% old questions:
%     % \item \textit{Is it possible to exploit unstructured sparsity to enable sequence modeling at the edge on neuromorphic accelerators?} 
%     % \item \textit{How does the sparsity structure affect the expressivity of linear RNNs?} 
%     % How do power and performance compare to GPU solutions?}
% \end{enumerate}

We provide definite positive answers to questions \qty{1}{} and \qty{4}{}, and present positive evidence for questions \qty{2}{} and \qty{3}{}.
% We answer questions 1 and 4 with a definite yes, and questions 2 and 3 with a qualified yes. 
