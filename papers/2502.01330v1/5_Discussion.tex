
% \begin{itemize}
%     \item Summarize findings
% \end{itemize}

In this work, we explored the Pareto front of efficiency and performance for a streaming audio processing task, comparing dense and sparsified variants of a linear RNN based on the S5 architecture.
We showed that combining activation sparsity and unstructured weight pruning results in a significant reduction in compute requirements, up to $3.2\times$, and memory footprint, $5.7\times$, without accuracy degradation.
In addition, we validated these theoretical gains with a hardware-accelerated implementation on a compute-memory integrated coarse accelerator, the Intel Loihi 2 neuromorphic chip.
When quantized and deployed on Loihi 2, sparse models deliver $42 \times$ lower latency and $149\times$ lower energy consumption in token-by-token processing, compared to the iso-accuracy dense models on the Jetson Orin Nano GPU.
% Furthermore, we analyze the interaction between weight sparsity and activation sparsity, observing an increase in activity for higher levels of weight sparsity.

%We quantize and map sparsified S5 networks to the neuromorphic chip Loihi 2 in order to realize its theoretical efficiency advantages while minimizing the performance degradation with QAT.
%
% We study several quantization interventions, finding quantization-aware training highly beneficial toward maintaining audio denoising performance.
% %; indeed, a very small degradation in performance is observed going from FP32 to static quantization.
%
% However, we do find that audio denoising performance degrades further as we transition from static quantization to a fixed-point simulation and Loihi hardware with integer 8 bit weights and 16 bit activations, revealing an opportunity to more closely capture subtleties in integer-only arithmetic in QAT. 
%
%Finally, we evaluate latency and throughput-optimized implementations of S5's audio denoising performance and efficiency on both Loihi 2 and Jetson Orin Nano. %We demonstrate that for real-time processing, sparse linear RNNs on suitable hardware are $42 \times$ faster while using up $149\times$ less energy compared to a dense linear RNNs running on an edge GPU.

% \paragraph{Future Work}
In conclusion, our work demonstrates that sparse event-driven accelerators, such as neuromorphic processors, can provide state-of-the-art accuracy on high-frequency signal processing tasks, with orders of magnitude gains in latency and energy efficiency.
This possibility opens up several research directions to further materialize these gains in real-world applications.
In particular, future work should investigate how the efficiency-performance Pareto front scales up to larger models and more complex tasks, such as language and multimodal modeling.
In this setting, the scalability of multi-chip neuromorphic processors \cite{kudithipudi_neuromorphic_2025} and high-frequency execution could power the growing need for large-scale inference compute \cite{DBLP:journals/corr/abs-2408-03314}.
Finally, improvements to our fixed-point conversion methodology and the use of advanced data types (e.g.\ FP8), could help close the gap between simulation and hardware deployment.
% In addition, in this work, we only consider ReLUification as a source of activation sparisty. Other techniques, such as top-k \cite{DBLP:journals/corr/abs-2412-04358} and sigma-delta coding \cite{shrestha2024efficient, o2016sigma} are promising directions toward  further activation sparsity and potential efficiency gains on suitable hardware.
%
% Scaling the hardware implementations of state-space models with activation and weight sparsity is also of keen interest. 
% %
% Neuromorphic computer architectures such as Loihi 2 are inherently scalable to multi-chip configurations, and evaluating efficiency at increased scale is important toward demonstrating the nature of and extent to which theoretical gains are realizable.
%
