\subsection{Experimental Setup}

\input{Figures/ndns_performance_efficiency_plot}

\paragraph{Software}
We implemented our methodology in JAX 0.4.30, building on top of the original S5 codebase \cite{DBLP:conf/iclr/SmithWL23}, with JaxPruner \cite{DBLP:journals/corr/abs-2304-14082} for the pruning algorithms and the AQT library \cite{aqt} for quantization-aware training. We implemented static quantization and a fixed-point model ourselves using only JAX.
% The implementation on the Intel Loihi 2 is based on NxKernel 0.2.0 and all characterization results are produced on a single-chip Oheo Gulch N3C1 board (accessible only to Intel Neuromorphic Research Community members).
% The implementation on the NVIDIA Jetson Orin Nano 8GB is running Jetpack 6.2, CUDA 12.4, JAX 0.4.32 and using the MAXN SUPER power mode. Power on the Jetson is reported as only CPU\_GPU\_CV through jtop 4.3.0.
% Performance results are based on testing as of Jan 2025 and may not reflect all publicly available security updates. Results may vary.

\paragraph{Audio denoising task}

We evaluated our approach on the Intel Neuromorphic Deep Noise Suppression Challenge \cite{Timcheck_2023}.
%
% AP: We should give a general understanding of the task, the pre-/post-processing steps, the acceptable latency, and the SI-SNR metric.
The objective of the Intel N-DNS Challenge is to enhance the clarity of human speech recorded on a single microphone in a noisy environment.
%
The Intel N-DNS Challenge utilizes data from the Microsoft DNS Challenge,  encompassing clean human speech audio samples and noise source samples.  \cite{reddy2020interspeech, reddy2021icassp, reddy2021interspeech, dubey2024icassp}.
Clean human speech and noise samples are mixed to produce noisy human speech with a ground truth clean human speech goal.

To train our models, we used the default Intel N-DNS Challenge training and validation sets, each consisting of \qty{60000}{} noisy audio samples of \qty{30}{\s} each, and a test set with \qty{12000}{} samples. 
%
We encoded and decoded each audio sample using the Short-Time Fourier Transform (STFT) and Inverse Short-Time Fourier Transformer (iSTFT) \cite{grochenig2013foundations}. 
%
Following the N-DNS baseline solution, NsSDNet \cite{shrestha2024efficient}, we adopted a \qty{32}{\milli\s} window length and a \qty{8}{\milli\s} hop length for the STFT/ISTFT.
%
This resulted in a nominal real-time audio processing latency of \qty{32}{\milli\s}, which allows ample time (\qty{8}{\milli\s}) for denosing network inference, as \qty{40}{\milli\s} is the standard for an acceptable latency as recognized in the Microsoft N-DNS Challenge. 

We evaluated the denoising quality of our model using the scale-invariant signal-to-noise ratio (SI-SNR)
\begin{equation}
    \text{SI-SNR} = 10\log_{10}\frac{\norm{s_\text{target}}^2}{\norm{e_\text{noise}}^2}.
\end{equation}
Importantly, SI-SNR provides a volume-agnostic measure of audio cleanliness relative to the ground truth signal. 



\subsection{Pareto Front of Performance and Efficiency}
\label{ss:pareto-front}

We studied the performance-efficiency Pareto front of dense and sparse models across inference compute budgets.
Starting from the S5 architecture \cite{DBLP:conf/iclr/SmithWL23}, we trained a family of dense models of increasing size by linearly scaling the model dimensions (i.e.\ model width and size of the SSM hidden state), while keeping the depth fixed to three S5 layers.
Similarly, we trained a family of sparse models, i.e., pruned and ReLU-fied, according to our methodology discussed above, with $90\%$ of weights pruned by the end of training (further details on the model dimensions are provided in \Cref{app:model-params}).
The results, reported in \autoref{fig:ndns_performance_efficiency}, compare de-noising performance (SI-SNR) and computational efficiency as measured by effective MACs and memory footprint (see \Cref{supp:macs}).
%
%Furthermore, we applied a hyperparameter search {\color{red} TODO: details of the hyperparameter search methodology?} to ensure a fair representation of the best performance at each network size and in either sparse or dense configuration.
%
%We computed a proxy measure of efficiency for each model by calculating the effective Multiply-And-Accumulate operations (MACs) per time step and the memory footprint (model size).

The results show that sparsification significantly degrades performance when applied to under-parametrized dense models (e.g., sparsifying dense-\qty{3}{} reduces SI-SNR by $7.3\%$).
However, task performance is recovered with increased model dimensions and the accuracy of dense models is matched by larger sparse ones, with fewer MACs and lower memory requirements.
This gives empirical support to theoretical work on the capacity of sparse-and-wide neural networks \cite{golubeva_are_2020}.
For example, sparse-\qty{8}{} model requires \textbf{$\mathbf{2}\boldsymbol{\times}$ lower compute} and \textbf{$\mathbf{36}\boldsymbol{\%}$ lower memory} than the dense-\qty{3}{} model, \textbf{while achieving the same level of accuracy}.
Overall, sparse models constitute the Pareto front of task performance and computational efficiency across compute budgets.

In terms of absolute task performance, we find that the S5 architecture provides state-of-the-art results on audio denoising out of the box.
When compared to Spiking-FullSubNet-XL \cite{10605482}, the Track 1 winner of the Intel N-DNS Challenge with \qty{15.2}{\dB} SI-SNR, our sparse-\qty{11}{} S5 model requires \textbf{$\mathbf{3.2}\boldsymbol{\times}$ lower compute} and \textbf{$\mathbf{5.37}\boldsymbol{\times}$ lower memory} \textbf{iso-accuracy}.
This finding is in line with previous research on audio modeling with state space models \cite{DBLP:conf/icml/GoelGDR22}, and provides additional evidence on the suitability of these architectures for signal processing.
%
%The XL version of the Spiking-FullSubNet network achieves \qty{15.2}{\dB} SI-SNR on the Intel N-DNS Challenge test set, as noted by the horizontal dashed line in \autoref{fig:ndns_performance_efficiency}. 
%
%Our S5 models can achieve \qty{15.2}{\dB} SI-SNR with modest computational cost and memory footprints.
%In comparison, Spiking-FullSubNet XL uses $8.4 \times 10^5$ effective MACs per \qty{8}{\milli\s} timestep and has a memory footprint of \qty{7.02}{\mega\byte}; these computational cost and memory points are much larger than those of our S5 networks shown in \autoref{fig:ndns_performance_efficiency}---beyond the domain displayed in our plots---suggesting strong competitiveness from our S5 networks, especially under consideration of resource constraints.
% 
%We note that the Spiking-FullSubNet network was trained using a loss function that includes other terms in addition to SI-SNR, catering to other audio quality metrics.
%Therefore, Spiking-FullSubNet's results in the Intel N-DNS Challenge does not represent the maximum achievable SI-SNR for the Spiking-FullSubNet architecture.
%
%Nevertheless, Spiking-FullSubNet's results provide an excellent point of comparison, as SI-SNR was one of the main metrics for which Spiking-FullSubNet was optimized.



\paragraph{Interaction of weight and activation sparsity}

An interesting question is what is the interaction between the two types of sparsity, in weights and activations.
\autoref{fig:activation_sparsity} reports the pre-activation sparsity for different layers across the model depth for two ReLU-fied models of the same size (model variant \qty{6}{}), with and without synaptic sparsity.
%
We observe that the synaptic-sparse model exhibits lower activation sparsity across the board, a finding that is consistent across model sizes.
%
In addition, activation sparsity significantly decreases with model depth, both for dense and sparse models.
These phenomena, previously observed in other models \cite{mukherji2024weight}, suggest that, during training, the model compensates the reduced information flow caused by pruning with increased levels of activation.
Additional research on more advanced activation functions would allow for the optimal allocation of MACs, especially those that provide explicit control over sparsity without cross-channel synchronization (e.g.,\ approximate top-k \cite{DBLP:journals/corr/abs-2412-04358}).
% Nonetheless, weight and activation sparsity combine constructively to result in overall effective MAC reductions greater than that of activation sparsity or weight sparsity alone.

%{\color{red} TODO: would need an ablation study or additional Pareto curves to support this statement. Not necessarily necessary to include the curves in the paper, but that we know it is true would be helpful and could be written in in some way.}

%{\color{red} TODO: may wish to move interpretation to discussion} 

\input{Figures/activation_sparsity_plot}


\subsection{Hardware Implementation} 
\label{ss:hardware-implementation}

%In order to validate the sparsity gains on real-time inference performance, we implemented our S5 variants on the Intel Loihi 2 neuromorphic chip.

\input{Figures/quantization_plot}

\paragraph{Impact of fixed-point conversion}

Since Loihi 2 only supports fixed-point (FXP) arithmetic, as presented in \Cref{sec:methodology}, we quantized the weights and activations of our model and implemented the network dynamics in FXP arithmetic. The effect of our quantization methodology is presented in \autoref{fig:quantization_interventions}.
%
Starting from a 32-bit floating-point (FP32) model, we apply static quantization, which rounds weights and activations using fixed scales, but still performs the actual computation in FP32. Notably, Quantization-Aware Training (QAT) is very effective in maintaining test performance (SI-SNR) from FP32 to static quantization, compared to Post-Training Quantization (PTQ).
%
The frozen scales from static quantization are imported into our FXP model implemented in JAX, which uses only int32 types and fixed-point arithmetic to compute the forward pass of the model.
We observe further performance degradation in the FXP simulation, which we analyze in more detail in \Cref{appendix:fxp-sim-mismatch}. 
%
We finally map the FXP model to Loihi 2 and perform inference on the chip, again finding a degradation in SI-SNR, which is likely due to subtle differences in the integer arithmetic performed by the FXP simulation and Loihi 2 implementation with fused layers. Another source of mismatch is that the FXP model in simulation handles overflows by clipping to the maximum value, whereas Loihi 2 ``wraps around'' the value, resulting in a sign inversion.
%
The size of the model decreases by about a factor of 4 when transitioning from FP32 weights to INT8 weights, as shown on the right side of \autoref{fig:quantization_interventions}.



\paragraph{Power and Performance}

\input{Tables/pnp_table}

To measure the empirical efficiency benefits afforded by the sparse S5 model on neuromorphic hardware, we profile inference on Loihi 2 using the fixed-point S5 model, in particular, configuration sparse-\qty{8}{} from \autoref{fig:ndns_performance_efficiency}.
%
To compare to conventional hardware, we profile the smallest dense model that achieves equivalent performance on Jetson Orin Nano\footnote{Our W8A16 fixed-point model in JAX does not provide a speedup over the FP32 model on the Jetson Orin Nano, therefore we profile the FP32 model.}, which is configuration dense-\qty{3}{} from \autoref{fig:ndns_performance_efficiency}.
%
There exist a variety of modes in which to execute a model on Loihi and Jetson, each exhibiting different tradeoffs in terms of latency, throughput, and energy.
Therefore, we present different modes for a comprehensive characterization and comparison.
We summarize our profiling results in \autoref{tab:pnp}. More details on the different execution modes on Loihi 2 are presented in \Cref{app:exmode}.

In real-time, token-by-token processing on a single input sequence, Loihi 2 processes a single STFT frame $\mathbf{35\times}$ \textbf{faster} and with $\mathbf{1200\times}$ \textbf{less energy} than the Jetson Orin Nano. % (Token-by-token; Loihi 2 Fall-Through and Jetson Orin Nano Recurrent 1-step (b=1) in \autoref{tab:pnp}). 
When the Jetson Orin Nano processes ``chunks'' of multiple time steps, its utilization increases, and energy per token improves. With the largest chunks that fit the real-time requirement of latency $\leq$\qty{8}{\milli\sec}, Loihi 2 is \textbf{$\mathbf{42}\times$ faster} and uses \textbf{$\mathbf{149} \times$ less energy} per token.

In offline processing, when many STFT frames are buffered to process in succession (or in parallel), the energy efficiency and throughput of the Jetson Orin Nano improves. Loihi 2 performs offline processing with pipelining (see \Cref{app:exmode} for further explanation). When processing single sequences, \textit{i.e.} batch size $b=1$, Loihi 2 has \textbf{$\mathbf{3.7} \times$ higher throughput} with \textbf{$\mathbf{8}\times$ less energy} per sample. 

It is important to note that the Jetson Orin Nano is only fully utilized when processing \qty{256}{} sequences in parallel, and at this level, it shows significantly higher throughput while consuming less energy per sample, compared to Loihi 2. We include these results in the last row of \autoref{tab:pnp}.

\paragraph{Energy at real-time inference rate}
The latency budget for the neural network component of the audio denoising pipeline, running either on Loihi 2 or on the Jetson, is \qty{8}{\milli\s}.
Our Loihi 2 and Jetson implementations are well below 8ms for online inference.
%
Thus, to estimate the energy consumption in real-time settings, where subsequent tokens are actually \qty{8}{\milli\s} apart, we rescale the power as:
\begin{equation*}
    P_\text{total}^\text{real-time} =  P_\text{static} + \frac{t_\text{compute}}{\qty{8}{\milli\s}} P_\text{dynamic},
\end{equation*}
based on the power measurements in token-by-token processing.
In this setting, Loihi 2 achieves \qty{1128}{\micro\joule/\token} while the Jetson achieves \qty{36528}{\micro\joule/\token} for token-by-token processing and \qty{3720}{\micro\joule/\token} when processing chunks of 10 time steps at once. Loihi 2 remains at least $3 \times$ more energy efficient than the Jetson Orin Nano.

\paragraph{Limitations}

Our Jetson Orin Nano implementation is in FP32, while our Loihi 2 implementation is in W8A16. Our fixed-point model in JAX provides no improvements in runtime or energy. More competitive Jetson energy, latency, and throughput could potentially be obtained by developing a more optimized quantized implementation. 

% \paragraph{Energy and throughput for offline processing}

% {\color{red} TODO: Distill this discussion in the PnP comparison and remove this paragraph.

% If we move from online processing to offline processing, i.e., buffer several STFT frames to rapidly process in succession, Jetson energy efficiency and throughput improves (Jetson Orin Nano, Recurrent $n$-step or Recurrent scan).
% %
% However, this comes at the cost of buffering and additional processing latency. 
% %
% Jetson Orin Nano can also perform offline S5 utilizing a parallel scan routine (Sample-by-sample, Jetson Orin Nano, Scan); here, batch processing improves Jetson throughput and energy as well (b = max). 
% %
% Nonetheless, Loihi 2 performs offline processing in pipelined mode for batch size 1 with approximately $3.8\times$ increase in throughput and approximately $24\%$ increased energy cost compared to the most efficient batch size 1 Jetson implementation, which is the recurrent scan.
% %
% When using the maximal batch size than can fit on the Jetson's memory, however, the Jetson parallel scan implementation achieves the highest throughput of approximately 4.2M tokens per second, \textcolor{red}{yet at a substantial energy cost per token}.
% }