
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/Figure_Loihi2_Diagram.pdf}
    \caption{Diagram of S5 as implemented on Loihi 2. To leverage the neuromorphic hardware architecture, several adjustments are made in comparison to the original S5 model shown in \hyperref[figure_3]{Figure \ref{figure_3}}: First, complex numbers are split into real and complex components for processing. Second, ReLUs are introduced to increase activation sparsity. Third, multiple element-wise operations are fused into single neuromorphic neurons. Symbols are shown as defined in \Cref{ss:linear-rnns}.}% by equations \eqref{eq:x_k}-\eqref{eq:glu}.}
    \label{fig:loihi-implementation}
\end{figure*}


\subsection{Effective MACs computation for S5 architecture}
\label{supp:macs}

In this section, we detail the computation of effective multiply-accumulate operations (MACs) for different components of the S5 architecture. The total MAC count provides an estimate of the computational cost associated with each stage of the model. 
Below, we outline the individual contributions from key components of the architecture. The effective MACs for all model sizes--sparse and dense--in Figure \autoref{fig:ndns_performance_efficiency} are calculated based on the formulas below, summed over the entire network structure.

\paragraph{Notation:}
\begin{itemize}
    \item $N_\text{input}$: Input dimension
    \item $N_\text{model}$: Model dimension for activations outside of the linear RNN. 
    \item $N_\text{ssm}$: Dimension of the linear RNN's hidden state. 
    \item $N_\text{output}$: Output dimension (equal to the number of classes for classification)
    \item $d^\text{wgt}_x$: Density of weights for $x$
    \item $d^\text{act}_x$: Density of activations for $x$
\end{itemize}

where the density $d$ is calculated from the sparsity $s$ as $d=1-s$.

\paragraph{Breakdown of MAC Calculation per Component:}
\begin{itemize}
    \item \textbf{Encoder:} The MACs for the encoder depend on the input dimension, model size, and scale linearly with activation and weight densities:
        \begin{equation}    
            N_\text{input} N_\text{model} d^\text{wgt}_\text{encoder} d^\text{act}_\text{input}
        \end{equation}
    \item \textbf{Batch Normalization (BatchNorm):} A lightweight operation, requiring only element-wise scaling, leading to:
        \begin{equation}
            N_\text{model}
        \end{equation}
    \item \textbf{S5 Hidden Layer:} The hidden state update for the S5 model involves both matrix multiplications and element-wise operations:
        \begin{equation}
            2N_\text{model} N_\text{ssm} d^\text{wgt}_B d^\text{act}_\text{pre\_ssm} + 4 N_\text{ssm}
        \end{equation}
    \item \textbf{SSM Output Layer:} Computes the output transformation of the linear RNN:
        \begin{equation}
            2N_\text{ssm} N_\text{model} d^\text{wgt}_C d^\text{act}_\text{hidden} + N_\text{model} d^\text{act}_\text{pre\_ssm}
        \end{equation}
    \item \textbf{Gated Linear Unit (GLU):} The computation for the GLU involves matrix multiplications for the dense weight matrix, followed by an element-wise multiplication:
        \begin{equation}
            N_\text{model}^2 d^\text{wgt}_\text{GLU} d^\text{act}_\text{pre\_GLU} + N_\text{model}
        \end{equation}
    \item \textbf{Classification Head:} The final linear projection for classification:
        \begin{equation}
            N_\text{model} N_\text{output} d^\text{wgt}_\text{head} d^\text{act}_\text{pre\_hread}
        \end{equation}
    \item \textbf{Regression Head:} The regression head follows the same computation as the classification head:
        \begin{equation}
            N_\text{model} N_\text{output} d^\text{wgt}_\text{head} d^\text{act}_\text{pre\_hread}
        \end{equation}
\end{itemize}

Numerical operations such as the inverse square-root, sigmoid function, and others, are ignored from our MAC calculations, as is commonly done when calculating the MACs or floating point operations (FLOPs) of machine learning models \cite{evci_rigging_2020}.

\subsection{Experimental Details}

\paragraph{Model architecture}
\label{app:model-params}

Our linear RNN is based on the S5 architecture \cite{DBLP:conf/iclr/SmithWL23}, as described in Section \Cref{ss:linear-rnns}. We use the following dimensions for our base model with width scaling $k=1$ (\textit{i.e.} configuration \qty{4}{} in \autoref{fig:ndns_performance_efficiency}). We use three layers, the recurrent state vector is $\mathbf{x}_t \in \mathbb{R}^{256}$, we use a model dimension of $192$. 
Both input and output have dimension $257$.
The width scaling factors $k_i$ scale the model and recurrent state dimension linearly. In \autoref{fig:ndns_performance_efficiency}, we report results for a $k$-family of sparse and densely trained networks where $k_\text{sparse} \in [0.5, 3.0], \ k_\text{dense} \in [0.25, 1.0]$.

\paragraph{Training recipe}
We trained all models for \qty{50}{} epochs with the Adam optimizer. The parameters of the SSM block were updated with initial learning rate $0.002$, while the rest of the architecture used initial learning rate $0.008$ and weight decay $0.04$. All learning rates used cosine annealing and no warmup epochs.
The dropout was set to $0.1$.


\subsection{Additional Results}

\subsubsection{Loihi execution mode}
\label{app:exmode}

Loihi 2â€™s asynchronous architecture allows to trade off between throughput and latency, as illustrated in \hyperref[fig:time_per_step]{Figure \ref{fig:time_per_step}a}. For optimal throughput, new input is provided every time step and forwarded through the neuronal layers in a pipelined mode. For optimal latency, new input is injected only once the previous input has been processed by, or fallen through, the network as fast as possible. The pipelined and fall-through mode can be balanced by changing the rate of new input, to match the throughput of a given input stream while minimizing its processing latency.

As audio denoising is typically deployed in realtime in an online fashion where one STFT input frame in processed at a time, fall-through mode is appropriate, as one desires a corresponding output STFT frame immediately.

We see that Loihi 2 processes a single STFT frame $35\times$ faster and with $1200\times$ less energy than the Jetson Orin Nano (Token-by-token; Loihi 2 Fall-Through and Jetson Orin Nano Recurrent 1-step (b=1) in \autoref{tab:pnp}). 

\input{Figures/time_per_step_plot}

\subsubsection{Fixed-point model mismatch}
\label{appendix:fxp-sim-mismatch}

The mismatch in \autoref{fig:quantization_interventions} indicates that fixed-point implementation in JAX does not perfectly match the original FP32 model when using the scales computed through our static quantization step. Further investigations show that the mismatch between hidden activations is highest for the hidden states $\mathbf{x}_k$ of the linear RNN and its outputs $\mathbf{y}_k$, see \autoref{fig:app_fxp_mismatch}. 
This mismatch increases approximately linearly with model depth, indicating that quantization errors accumulate as information propagates through the network layers. This linear escalation of errors underscores a critical challenge in fixed-point quantization of recurrent models \cite{wu_googles_2016,abreu2024q,li_quantization_2021,pierro2024mamba}. Consequently, ensuring the fidelity of deeper Linear RNNs on fixed-point neuromorphic hardware may require advanced quantization techniques or error mitigation strategies to preserve the network's temporal dynamics and memory capacity effectively.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/app_fxp_mismatch.png}
    \caption{Layer-wise analysis of mismatch between the fixed-point model in JAX against the base model using floating-point weights and activations. The \textbf{left} and \textbf{right} side show the same data with a linear y-axis and log y-axis, respectively. The \textbf{top} panels show the mean absolute error $N^{-1}\sum_i^N|x_i-x_i'|$ for all components of the model while the \textbf{bottom} panels show the mean relative error $N^{-1}\sum_{\{i \ | \ i \in \{0, \ldots, N\} \ \wedge \ x_i \neq 0\}}^N|x_i-x_i'|/|x_i|$. For further explanation, see text.}
    \label{fig:app_fxp_mismatch}
\end{figure*}