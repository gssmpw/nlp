\begin{figure*}[t]
  \centering
  \subfigure[]{\includegraphics[width=0.46\linewidth]{Figures/Figure_Loihi_Processing.pdf}}
  \quad
  \subfigure[]{\includegraphics[width=0.5\linewidth]{Figures/Figure_Systems.pdf}}
  \caption{(a) Loihi 2 implements a network of neurons, which are processed by neuro-cores and communicate via an asynchronous network-on-chip. Parallel IO and \qty{10}{\giga\bit} Ethernet interfaces enable a Loihi 2 chip to communicate with other Loihi 2 chips and external hosts, respectively. Embedded microprocessors provide a flexible method of interaction with neuro-core registers, management, and communication. On a neuro-core, each neuron receives spike messages from other neurons via synapses with multiplicative weights $w_\textnormal{i}$, and sums them up by one or multiple dendritic accumulators. The input is used by a dendrite to update memory states that are local to the respective neuron. The neuron communicates with other neurons by sending spike messages. (b) Different Loihi 2 systems are available to cover a wide range of applications from the edge to HPC with up to \qty{1}{\billion} neurons.}
  \label{fig:loihi2}
  \vspace{-0.2cm}
\end{figure*}

\subsection{Linear Recurrent Neural Networks}
\label{ss:linear-rnns}

Recurrent neural networks (RNNs) are a class of neural networks designed for processing sequential data by maintaining hidden states that capture temporal dependencies.
Linear RNNs distinguish themselves through their linear dynamics, which enables parallelization over the sequence length and, therefore, efficient training.
Previous work has shown--both theoretically \cite{DBLP:conf/icml/OrvietoDGPS24} and empirically \cite{DBLP:conf/nips/GuG0R22}--that the network's recurrent weight matrix can effectively be diagonalized in the complex domain without loss of generality or model capacity.
We use this diagonal formulation of linear RNNs, such that the network's update equations for the state $\mathbf{x}_k \in \mathbb{C}^{N}$ and output $\mathbf{y}_k \in \mathbb{R}^{M}$ are given by:
% 
\begin{align}
    \label{eq:x_k}
    \mathbf{x}_{k} & = \diag(\bar{\mathbf{A}})\otimes\mathbf{x}_{k-1} + \bar{\mathbf{B}}^T\mathbf{u}_{k} \\
    \mathbf{y}_{k} & = \bar{\mathbf{C}}^T\mathbf{x}_{k} + \diag(\bar{\mathbf{D}})\otimes\mathbf{u}_{k}
\end{align}
%
where $\otimes$ denotes the Hadamard product, 
$\mathbf{u}_k \in \mathbb{R}^M$ is the input sequence, 
$\diag(\bar{\mathbf{A}}) \in \mathbb{C}^{N}$ are the diagonal recurrent weights, 
$\bar{\mathbf{B}}^T \in \mathbb{C}^{M \times N}$ are the input weights, 
$\bar{\mathbf{C}}^T \in \mathbb{C}^{N \times M}$ are the output weights, and 
$\diag(\bar{\mathbf{D}}) \in \mathbb{R}^{M}$ are the residual weights.
%
We follow the S5 model \cite{DBLP:conf/iclr/SmithWL23} for the initialization and parameterization of the linear RNN. 

Because of the RNN's linearity, the temporal mixing of the S5 block above is followed by a nonlinear channel mixing block. We use a particular variant of the GLU block \cite{DBLP:conf/icml/DauphinFAG17} where the linear RNN's output $\mathbf{y}_k \in \mathbb{R}^M$ is transformed as:
$\mathop{GLU}(y_k) = \sigma \left( W \tau(\mathbf{y}_k) \right) \otimes \tau(\mathbf{y}_k)$
% \begin{align}
%     \label{eq:glu}
%     \mathop{GLU}(y_k) = \sigma \left( W \tau(\mathbf{y}_k) \right) \otimes \tau(\mathbf{y}_k)
% \end{align}
where $\tau$ is an element-wise nonlinear function (we use either the Gaussian error linear unit (GELU) or the Rectified Linear Unit (ReLU)), $W \in \mathbb{R}^{M \times M}$ is a weight matrix, and $\sigma$ is the sigmoid function. 
% 
The full model architecture is illustrated in \autoref{figure_3}.

\subsection{Neuromorphic Computing with Intel Loihi 2}

Neuromorphic processors mimic computing principles of the brain, which excels in processing sequential data streams with just around \qty{20}{\watt} of power.
Loihi 2 is the second-generation of Intelâ€™s neuromorphic research processor \cite{DBLP:conf/sips/OrchardFRSSSD21} and implements a spiking neural network as illustrated in \autoref{fig:loihi2}.
The network is processed by massively parallel compute units, with 120 \textit{neuro-cores} per chip.
The neuro-cores compute and communicate asynchronously, but a global algorithmic time step is maintained through a barrier synchronization process.
The neuro-cores are co-located with memory and can thus efficiently update local states, simulating up to \qty{8192}{} stateful neurons per core.
Each neuron can be programmed by the user to realize a variety of temporal dynamics through assembly code.
Input from and output to external hosts and sensors is provided with up to \qty{160}{\million} 32 bit integer \unit{\messages/\second} \cite{shrestha_efficient_2024}.
Loihi 2 can scale to real-world workloads of various sizes with up to \qty{1}{\billion} neurons and \qty{128}{\billion} synapses, using fully-digital stacked systems shown in \autoref{fig:loihi2}.

The architectural features of Loihi 2 offer unique opportunities to compress and optimize deep learning models. Like GPUs, its neuro-cores benefit from model quantization, as it supports low-precision arithmetics, \qty{8}{\bit} for synaptic weights and up to \qty{32}{\bit} for spike messages. Unlike GPUs, Loihi 2 is optimized for computations local within neurons, a common focus of neuromorphic processors.
First, it allows fast and efficient updates of neuronal states with recurrent dynamics with minimal data movement, due to its tight compute-memory integration.
Second, the fully asynchronous event-driven architecture of Loihi 2 allows it to efficiently process unstructured sparse weight matrices.
Third, the neuro cores can leverage sparsified activation between neurons, as the asynchronous communication transfers only non-zero messages.
