\section{Related Work}
\label{gen_inst}
\paragraph{Zeroth-order method.} Zero-order (ZO) optimization techniques use forward passes for gradient estimation. Since backpropagation is not required during training, ZO methods reduce memory consumption significantly compared to a FO method. MeZO____ employed a memory-efficient ZO stochastic gradient descent (ZO-SGD) algorithm to efficiently fine-tune LLMs exceeding $60$ billion parameters, leveraging parameter-efficient approaches____ like LoRA____. Other ZO methods include ZO-SGD____ and ZO-Sign-SGD____ using sign-based gradient estimation, the ZO-Adam____ optimizer exploiting momentum information, and parameter-efficient methods like AdaZeta ____. Sparse MeZO____ employs a sparse perturbation for LLM fine-tuning. ${\rm FP}16$ ZO training____ performs well but still faces memory bottlenecks. Recent ZO quantization introduces fixed-point 16-bit but fails at 8-bit____. However, we overcome the challenges of lower-precision quantization and enable accurate fine-tuning of LLMs below 8-bit quantization.

% However, no prior ZO work has explored low-bit quantization such as 8-bit for hardware deployment. \zz{This is NOT true. I remember that Sijia Liu's benchmark paper includes some low-precision examples. Some recent work also utilized quantized ZO training. Please cite these papers and explain the difference with QuZO.}

\paragraph{Quantization of LLMs.} Various quantization methods have been developed to reduce the memory and computing cost of LLMs. LLM.int8()____ reduces the precision of model weights while keeping outliers in ${\rm FP}16$. SmoothQuant ____ introduces a fine-grained quantization method that supports ${\rm INT}8$ operations exclusively. QLLM____ addresses the outlier problem via employing an adaptive channel reassembly technique. LLM-QAT____ employs quantization-aware training with a data-free strategy to achieve $4$-bit quantization. Furthermore, the $4$-bit training____ and QLoRA____ methods leverage a Hadamard Transform and a novel NF4 datatype, respectively, to accelerate training while preserving performance. While prior quantized training methods rely on backpropagation for gradient updates, our QuZO method eliminates the error-prune STE-based back propagation and used low-bit inference for truly quantized fine-tuning.


%