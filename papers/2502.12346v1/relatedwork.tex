\section{Related Work}
\label{gen_inst}
\paragraph{Zeroth-order method.} Zero-order (ZO) optimization techniques use forward passes for gradient estimation. Since backpropagation is not required during training, ZO methods reduce memory consumption significantly compared to a FO method. MeZO~\citep{malladi2024fine} employed a memory-efficient ZO stochastic gradient descent (ZO-SGD) algorithm to efficiently fine-tune LLMs exceeding $60$ billion parameters, leveraging parameter-efficient approaches~\cite{yang2024loretta,liu2021p} like LoRA~\cite{hu2021lora}. Other ZO methods include ZO-SGD~\citep{ghadimi2013stochastic} and ZO-Sign-SGD~\citep{liu2018signsgd} using sign-based gradient estimation, the ZO-Adam~\citep{chen2019zo} optimizer exploiting momentum information, and parameter-efficient methods like AdaZeta \cite{yang2024adazeta}. Sparse MeZO~\cite{liu2024sparse} employs a sparse perturbation for LLM fine-tuning. ${\rm FP}16$ ZO training~\citep{zhang2024revisiting} performs well but still faces memory bottlenecks. Recent ZO quantization introduces fixed-point 16-bit but fails at 8-bit~\cite{feng2024stepping}. However, we overcome the challenges of lower-precision quantization and enable accurate fine-tuning of LLMs below 8-bit quantization.

% However, no prior ZO work has explored low-bit quantization such as 8-bit for hardware deployment. \zz{This is NOT true. I remember that Sijia Liu's benchmark paper includes some low-precision examples. Some recent work also utilized quantized ZO training. Please cite these papers and explain the difference with QuZO.}

\paragraph{Quantization of LLMs.} Various quantization methods have been developed to reduce the memory and computing cost of LLMs. LLM.int8()~\citep{dettmers2022gpt3} reduces the precision of model weights while keeping outliers in ${\rm FP}16$. SmoothQuant \citep{xiao2023smoothquant} introduces a fine-grained quantization method that supports ${\rm INT}8$ operations exclusively. QLLM~\citep{liu2023qllm} addresses the outlier problem via employing an adaptive channel reassembly technique. LLM-QAT~\citep{liu2023llm} employs quantization-aware training with a data-free strategy to achieve $4$-bit quantization. Furthermore, the $4$-bit training~\citep{xi2023training} and QLoRA~\citep{dettmers2024qlora} methods leverage a Hadamard Transform and a novel NF4 datatype, respectively, to accelerate training while preserving performance. While prior quantized training methods rely on backpropagation for gradient updates, our QuZO method eliminates the error-prune STE-based back propagation and used low-bit inference for truly quantized fine-tuning.


%