[
  {
    "index": 0,
    "papers": [
      {
        "key": "malladi2024fine",
        "author": "Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev",
        "title": "Fine-tuning language models with just forward passes"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yang2024loretta",
        "author": "Yang, Yifan and Zhou, Jiajun and Wong, Ngai and Zhang, Zheng",
        "title": "LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models"
      },
      {
        "key": "liu2021p",
        "author": "Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie",
        "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ghadimi2013stochastic",
        "author": "Ghadimi, Saeed and Lan, Guanghui",
        "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2018signsgd",
        "author": "Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi",
        "title": "signSGD via zeroth-order oracle"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2019zo",
        "author": "Chen, Xiangyi and Liu, Sijia and Xu, Kaidi and Li, Xingguo and Lin, Xue and Hong, Mingyi and Cox, David",
        "title": "Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yang2024adazeta",
        "author": "Yang, Yifan and Zhen, Kai and Banijamali, Ershad and Mouchtaris, Athanasios and Zhang, Zheng",
        "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024sparse",
        "author": "Liu, Yong and Zhu, Zirui and Gong, Chaoyu and Cheng, Minhao and Hsieh, Cho-Jui and You, Yang",
        "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2024revisiting",
        "author": "Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D and Yin, Wotao and Hong, Mingyi and others",
        "title": "Revisiting zeroth-order optimization for memory-efficient llm fine-tuning: A benchmark"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "feng2024stepping",
        "author": "Feng, Chen and Zhuo, Shaojie and Zhang, Xiaopeng and Ramakrishnan, Ramchalam Kinattinkara and Yuan, Zhaocong and Li, Andrew Zou",
        "title": "Stepping Forward on the Last Mile"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2023qllm",
        "author": "Liu, Jing and Gong, Ruihao and Wei, Xiuying and Dong, Zhiwei and Cai, Jianfei and Zhuang, Bohan",
        "title": "Qllm: Accurate and efficient low-bitwidth quantization for large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2023llm",
        "author": "Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas",
        "title": "Llm-qat: Data-free quantization aware training for large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "xi2023training",
        "author": "Xi, Haocheng and Li, Changhao and Chen, Jianfei and Zhu, Jun",
        "title": "Training transformers with 4-bit integers"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dettmers2024qlora",
        "author": "Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke",
        "title": "Qlora: Efficient finetuning of quantized llms"
      }
    ]
  }
]