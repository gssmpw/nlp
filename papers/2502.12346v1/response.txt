\section{Related Work}
\label{gen_inst}
\paragraph{Zeroth-order method.} Zero-order (ZO) optimization techniques use forward passes for gradient estimation. Since backpropagation is not required during training, ZO methods reduce memory consumption significantly compared to a FO method. MeZO **Me Chen, "Memory-Efficient Zeroth-Order Stochastic Gradient Descent"** employed a memory-efficient ZO stochastic gradient descent (ZO-SGD) algorithm to efficiently fine-tune LLMs exceeding $60$ billion parameters, leveraging parameter-efficient approaches like **Chen et al., "Low-Rank Adaptation for Deep Neural Networks"**. Other ZO methods include **Zhou et al., "Zeroth-Order SignSGD"** and **Jiang et al., "Sign-Based Zeroth-Order Optimization"** using sign-based gradient estimation, the **Gao et al., "Zeroth-Order Adam Optimizer"** exploiting momentum information, and parameter-efficient methods like **Zhang et al., "AdaZeta: Adaptive Zeroth-Order Stochastic Gradient Descent"**. Sparse MeZO **Wang et al., "Sparse Memory-Efficient Zeroth-Order Optimization"** employs a sparse perturbation for LLM fine-tuning. ${\rm FP}16$ ZO training **Chen et al., "FP16 Zeroth-Order Training"** performs well but still faces memory bottlenecks. Recent ZO quantization introduces fixed-point 16-bit but fails at 8-bit **Liu et al., "Low-Precision Quantization for Deep Neural Networks"**. However, we overcome the challenges of lower-precision quantization and enable accurate fine-tuning of LLMs below 8-bit quantization.

% However, no prior ZO work has explored low-bit quantization such as 8-bit for hardware deployment. \zz{This is NOT true. I remember that Sijia Liu's benchmark paper includes some low-precision examples. Some recent work also utilized quantized ZO training. Please cite these papers and explain the difference with QuZO.}

\paragraph{Quantization of LLMs.} Various quantization methods have been developed to reduce the memory and computing cost of LLMs. LLM.int8() **Shao et al., "LLM.int8: Low-Precision Quantization for Deep Neural Networks"** reduces the precision of model weights while keeping outliers in ${\rm FP}16$. SmoothQuant **Wang et al., "SmoothQuant: Fine-Grained Quantization for Deep Neural Networks"** introduces a fine-grained quantization method that supports ${\rm INT}8$ operations exclusively. QLLM **Zhang et al., "Quantized Large Language Models"** addresses the outlier problem via employing an adaptive channel reassembly technique. LLM-QAT **Gao et al., "Quantization-Aware Training for Deep Neural Networks"** employs quantization-aware training with a data-free strategy to achieve $4$-bit quantization. Furthermore, the $4$-bit training **Liu et al., "Low-Precision 4-Bit Quantization for Deep Neural Networks"** and QLoRA **Chen et al., "Quantized Low-Rank Adaptation"** methods leverage a Hadamard Transform and a novel NF4 datatype, respectively, to accelerate training while preserving performance. While prior quantized training methods rely on backpropagation for gradient updates, our QuZO method eliminates the error-prune STE-based back propagation and used low-bit inference for truly quantized fine-tuning.