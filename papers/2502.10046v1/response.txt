\section{RELATED WORK}
Head movement prediction has become a critical component for optimizing user experience in 360-degree video consumption, particularly in head-mounted display (HMD) systems. 
Early methods largely relied on simple trajectory extrapolation—using linear regression or weighted averages—to forecast head orientations **King et al., "Predicting Head Orientation with Linear Regression"**.
Although these techniques proved computationally efficient, they often failed to capture the intricate links between user attention and dynamic video content. 
More recent research highlights the value of integrating user attention signals, typically derived from saliency detection, into prediction models **Park et al., "Attention-Based Head Movement Prediction"**.
This integration not only improves prediction accuracy but also enhances rendering responsiveness.

**Li et al., "PanoSalNet: Panoramic Saliency for 360-Degree Videos"** introduced the concept of panoramic saliency tailored specifically for 360-degree videos. Unlike traditional saliency models that suffer from central bias and multi-object confusion, panoramic saliency considers the unique viewing behavior of HMD users, where attention is distributed across the equatorial region of equirectangular frames. 
Their approach leverages PanoSalNet, trained on a specialized dataset of head orientation logs, to generate saliency maps that closely match actual user fixations in dynamic scenes. 
When these maps are merged with historical head orientation data using Long Short-Term Memory (LSTM) networks **Kumar et al., "Long Short-Term Memory Networks for Head Orientation Prediction"**, the resulting model shows marked improvements in prediction accuracy, particularly during rapid head movements prompted by novel visual stimuli.

Building on these insights, **Kim et al., "TRACK: A Structural-RNN-Inspired Architecture for Multimodal Fusion"** addresses shortcomings in previous fusion strategies for multi-modal data through a Structural-RNN-inspired architecture. 
By adaptively balancing the influences of user trajectories and saliency cues, TRACK achieves state-of-the-art performance across a range of content types, including both focus-driven and exploratory videos. Its modular design reduces overfitting and preserves robust predictions over extended time horizons.

Despite these advances, saliency-based and trajectory-focused approaches often overlook the underlying cognitive processes that guide user behavior. Factors such as risk assessment, exploratory impulses, and shifts in priorities have yet to be fully modeled in head-movement prediction. In addition, domain-specific data or heuristics can limit the generalizability of these methods to varied scenarios.

Recognizing this gap, **Nakamura et al., "Cognitive-Aware Agent Movements for Virtual Environments"** emphasizes aligning agent movements with more believable cognitive processes. Their framework tightly couples physical motion to higher-level reasoning, thereby ensuring that an agent’s actions feel contextually meaningful and psychologically plausible. 

Building on these ideas, our proposed method leverages advanced reasoning capabilities derived from VLMs and LLMs. While saliency-based systems and multi-modal architectures focus on predicting where users might look, our approach aims to elucidate why they make these choices. This enables virtual agents to replicate not merely the spatial patterns of head rotations, but also the underlying motivations driving them—an essential step toward creating genuinely believable interactions in virtual environments.