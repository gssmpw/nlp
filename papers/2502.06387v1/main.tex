\documentclass[a4paper]{article}[12pts]

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\normalsize



%% Sets page size and margins
\usepackage[a4paper,top=1in,bottom=1in,left=1in,right=1in,marginparwidth=1in]{geometry}

%% Useful packages
\usepackage{natbib}
\usepackage{framed}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{amsthm}
\usepackage{amsfonts, mathtools}
\usepackage{algorithm,algpseudocode}
\usepackage{bm}
\usepackage{bbm}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{multirow, booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\hypersetup{colorlinks=true, allcolors=blue}

\usepackage{multirow}
\usepackage{textgreek}
\usepackage{adjustbox}
\usepackage{mdframed}

\definecolor{azure}{rgb}{0.9, 0.95, 1.0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\renewcommand{\labelenumi}{(\alph{enumi})}

%% Algorithm environments
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\shang}[1]{{\color{red} [Shang: {#1}]}} % just an example
\newcommand{\xc}[1]{{\color{blue} [XC: {#1}]}} 
\newcommand{\hz}[1]{{\color{green} [HZ: {#1}]}}




% just an example
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{How Humans Help LLMs: Assessing and Incentivizing Human Preference Annotators}
\author{Shang Liu$^{*,1}$, Hanzhao Wang$^{*,1}$, Zhongyao Ma$^{2}$, Xiaocheng Li$^{1}$}
\date{\small
Imperial College Business School, Imperial College London$^{1}$ \ \ \ \ Meta$^{2}$}

\begin{document}
\maketitle
\onehalfspacing

\def\thefootnote{*}\relax\footnotetext{Equal contribution. Corresponding to s.liu21@imperial.ac.uk and h.wang19@imperial.ac.uk.}

\begin{abstract}
Human-annotated preference data play an important role in aligning large language models (LLMs). In this paper, we investigate the questions of assessing the performance of human annotators and incentivizing them to provide high-quality annotations. The quality assessment of language/text annotation faces two challenges: (i) the intrinsic heterogeneity among annotators, which prevents the classic methods that assume the underlying existence of a true label; and (ii) the unclear relationship between the annotation quality and the performance of downstream tasks, which excludes the possibility of inferring the annotators' behavior based on the model performance trained from the annotation data. Then we formulate a principal-agent model to characterize the behaviors of and the interactions between the company and the human annotators. The model rationalizes a practical mechanism of a bonus scheme to incentivize annotators which benefits both parties and it underscores the importance of the joint presence of an assessment system and a proper contract scheme. From a technical perspective, our analysis extends the existing literature on the principal-agent model by considering a continuous action space for the agent. We show the gap between the first-best and the second-best solutions (under the continuous action space) is of $\Theta(1/\sqrt{n \log n})$ for the binary contracts and $\Theta(1/n)$ for the linear contracts, where $n$ is the number of samples used for performance assessment; this contrasts with the known result of $\exp(-\Theta(n))$ for the binary contracts when the action space is discrete.
Throughout the paper, we use real preference annotation data to accompany our discussions.
\end{abstract}


\section{Introduction}

Human-annotated preference data have been playing a critical role in aligning large language models (LLMs) and other multi-modal foundation models. Millions of preference samples annotated by human annotators are aggregated from public and private data sources, and then used in the post-training/alignment of the state-of-the-art LLMs, more specifically, in the stage of RLHF \citep{ouyang2022training} and DPO \citep{rafailov2024direct}. The literature on aligning LLMs has been focused on developing alignment algorithms and evaluating the performance of an aligned model. In alignment procedures, human-annotated data are often used as a golden standard. Sometimes people are aware of the presence of potential mistakes in human annotations, but do not make any special treatment for these mistakes; partly because these mistakes are hard to identify given the volume of the data. 

Once the human-annotated data is received from an upstream source, the things that one can do in the downstream training of machine learning models are probably limited. In this paper, we take an upstream perspective -- we raise and investigate the question of how to evaluate the performance of human annotators and accordingly incentivize them to produce high-quality annotations. In the first place, we do not assume that human annotators have a bad intention of providing low-quality data. However, low-quality annotations can commonly happen for reasons such as exhaustion of the annotators and lack of expertise. Without a monitoring system to assess the performance of the annotators, human nature will render them to provide random effort-saving annotations. In building such monitoring systems, one challenge is that the data to be annotated usually does not have a true answer, and this is in particular the case of annotating data for persona LLMs \citep{tseng2024two}. In our paper, we first address the question of how to build the monitoring system and then analyze effective mechanisms that can incentivize human annotators. We hope our work makes some first-step progress in answering the question of how humans help AI in the future of human-AI interaction.


%Ideally, incentivizing human annotators to provide high-quality annotations requires a careful design of the mechanism including both the annotation procedure and the payment system. A related question, which should be answered first, is how we can evaluate the quality of the annotated data produced by human annotators. Throughout this paper, we will discuss both of these two aspects (i) mechanism design and (ii) assessing human annotators.  


%To our knowledge, there is generally no monitoring system at present that assesses the performance of human annotators in the creation of the current annotation datasets. This void of monitoring system itself can be a cause for low-quality annotation data. 

From a modeling viewpoint, we develop a probability model (in Section \ref{sec:setup}) to characterize the behavior of annotators in preference data annotation and such a model complements the existing preference probability model which does not account for the human factors of the annotators. We also build utility models (in Section \ref{sec:pa_models}) for both the annotator (as an agent) and the company (as a principal) under a principal-agent model. These models not only lay the foundations for our discussions but can also be used for the future development of better annotation systems and for understanding the business models of data provider companies such as Scale AI, Appen, Outlier, etc.


In assessing the performance of human annotators, we note two new challenges arising in the context of language preference annotation compared to traditional data annotation: (1) Unlike tasks such as image classification where there is a \textit{true label} of the image (say being a dog image or a cat image), there are hardly true labels in human preference due to human annotators' heterogeneity; (2) The downstream performance cannot be effectively evaluated and it might be not directly related to the annotation data, which prevents assessing the annotation data's quality from the model performance. Keeping these challenges in mind, in Section \ref{sec:assess}, we study how to assess human annotators under the context of language preference annotation. We present two assessment methods: the classic expert-based monitoring and our proposed self-consistency monitoring. The first method hires experts to count the agreements between themselves and the annotators, which still suffers from annotators' heterogeneity. In contrast, the self-consistency method asks the human annotator herself/himself to label some data twice to check if the two labels are consistent, which overcomes the cross-annotator heterogeneity. Technically, we present information-theoretic bounds on the estimation errors for both methods and show that under the assumption that the two labels are consistent with high probability if the annotator fully commits, the self-consistency monitoring method has a lower error (in terms of minimax lower bounds). Numerical results on real preference data generation more intuitions to accompany our discussions. 

In Section \ref{sec:incentivizing}, we consider how to incentivize human annotators to produce high-quality annotations. We construct a principal-agent model: The principal (the LLM-developing company) first offers a contract for data annotation, and the agent (the annotator) is who decides the annotation quality $\eta \in [0, 1]$. The model is for the moral hazard problem that the agent does not act for the welfare of both parties but only for herself/himself. Due to imperfect monitoring and the risk-averse nature of agents, there is always a gap between the first-best (the ideal case) and the second-best (the real case) solutions. We prove that, under some mild assumptions, the gap is of $\Theta(1/\sqrt{n \log n})$ for the binary contracts where the payment is two-leveled (Theorem \ref{thm:binary}) and of $\Theta(1/n)$ for the linear contracts where the payment is a linear function of the average quality (Theorem \ref{thm:linear_contract}) if $n$ samples are tested for monitoring. The results justify both the binary contracts and the linear contracts as a proper mechanism to incentivize human annotators. Our technical result is different from the cases where the agent's action space is discrete of which the binary contracts' gap is proved to be $\exp(-\Theta(n))$ by \citet{frick2023monitoring}, better than the linear contracts. Technically, since the classic tools in large deviation theory fail in continuous cases, our analysis relies on a fine-grained analysis of the tail probability of binomial distributions under incentives, which provides a solution to the open question proposed in \citet{frick2023monitoring} and may be of independent interest.

\subsection{Related literature}

\paragraph{Annotation monitoring and management}\  

The ultimate goal of designing monitoring systems and mechanisms is to incentivize human annotators to provide high-quality annotations for reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). RLHF is a prominent framework for aligning large language models (LLMs) with human preferences. Under this framework, one first trains a reward model using preference data, then applies reinforcement learning to fine-tune the LLM to produce high-reward responses \citep{askell2021general,ouyang2022training,ziegler2019fine}.  For a detailed survey on RLHF, we refer readers to \cite{kaufmann2023survey}. 

\begin{comment}
    While canonical RLHF has been shown to improve preference alignment \citep{ouyang2022training}, its training process can be unstable \citep{choshen2019weaknesses, yuan2024rrhf,shen2023large}. Consequently, alternative methods have been proposed to align LLMs without explicitly relying on a reward model, such as Direct Preference Optimization (DPO) \citep{rafailov2024direct}.
\end{comment}

A crucial factor underlying RLHF is the quality of the preference data. Recent studies \citep{gao2024impact,chowdhury2024provably,wang2024secrets} demonstrate that alignment performance is highly sensitive to data quality and can deteriorate significantly in the presence of noisy samples or flipped labels. In practice, preference noise has been observed at rates exceeding 20\% in many datasets \citep{zhao2023slic,munos2023nash,cui2023ultrafeedback,touvron2023llama}. To mitigate the effects of such noise during RLHF or other optimization stages, various approaches have been introduced, including filtering out noisy data \citep{gao2024impact,liang2024robust}, applying label smoothing \citep{wang2024secrets}, and designing robust loss functions \citep{gao2024impact, wang2024secrets,liang2024robust}. Different from all these works, our paper takes the perspective of better designing the human annotator system to improve the data quality. 

Beyond these post-annotation techniques, many studies address data quality management during data annotation, which involves both quality estimation and improvement. Quality estimation methods verify whether the annotated data meets the required standards. They may involve expert- or AI-based monitoring \citep{pustejovsky2012natural,qian2021annotation,northcutt2021confident,klie2024efficient,ghosal2022cicero}, injecting annotation tasks with known answers to check consistency \citep{callison2010creating}, measuring inter-annotator agreement (IAA) through various metrics \citep{krippendorff1989content,krippendorff2004reliability,artstein2008inter,monarch2021human}, and designing contracts that compare annotations from different annotators in crowdsourcing (e.g., \cite{miller2005eliciting,bacon2012predicting,cai2015optimum,dasgupta2013crowdsourced}). There is also a line of works on analyzing the value of the quality estimation/monitoring \citep[e.g.,][]{holmstrom1979moral,Jewitt,singh1985monitoring,kim1995efficiency}. If the annotated data does not meet the required standards, improvement strategies should be adopted, such as re-annotation with updated guidelines or retrained annotators \citep{bareket2021neural,klie2024analyzing,ghosal2022cicero} and data filtering \citep{bastan2020author}. For a comprehensive discussion of annotation quality management, we refer to the survey \cite{klie2024analyzing}. 

However, to the best of our knowledge, these methods suffer from two aforementioned challenges when applied to preference data annotation. First, the intrinsic heterogeneity among annotators renders traditional approaches inapplicable, as there is no ground-truth label available for each data sample. Second, the unclear relationship between annotation quality and the performance of downstream tasks complicates the development of a comprehensive evaluation metric for the entire annotated dataset. Together, these challenges hinder the adoption of existing methods in the context of preference data annotation.

\paragraph{Contract Design}\ 

Contract design investigates how to formulate contracts that incentivize agents to pursue the principal's objectives, particularly under conditions of information asymmetry. As a powerful tool, contract design has been widely applied to various problems. For instance, in operations management, \cite{de2019designing,corbett1999designing,corbett2005optimal} propose different contracts for supply chain management with diverse objectives, \cite{adida2019contracts} study contract design for referral services in healthcare, and \cite{jain2013optimal} explore the design of optimal contracts for outsourcing repair and restoration services. In the realm of machine learning, \cite{goldwasser2021interactive} investigate interactive proof systems for PAC verification, while \cite{ivanov2024principal} focus on designing contracts to align the preferences of principals and agents in reinforcement learning. Moreover, \cite{ananthakrishnanknowledge} examine the achievability of optimal outcomes that a fully informed player could secure despite inherent uncertainties in strategic interactions. Another related line of research considers data markets and the pricing of data \citep[e.g.,][]{agarwal2019marketplace,chen2022selling,ho2014adaptive,acemoglu2022too,moscarini2002law}; for a survey, see \cite{bergemann2019markets}. These works primarily address the selling of annotated data to buyers, whereas our focus is on assessing and incentivizing annotators.

Recent papers \cite{ananthakrishnan2024delegating,saig2024delegated} propose simple-form contracts to address machine learning delegation problems under various assumptions about the principal’s utilities, proving the (near-)optimality of these contracts. In their frameworks, agent effort is represented by the quantity of collected data, each data point is assumed to be correctly annotated, and the principal’s utility depends on the accuracy of the resulting machine learning model; hence, the contract is based on accuracy. In contrast, we consider settings where the agent’s effort is reflected in annotation quality and the desired label is influenced by the annotator's own preference. Here, the principal’s utility depends on the quality of annotations, and the contract is based on (tested) annotation quality.

In the context of large language models (LLMs), several studies have explored contract- and mechanism-design approaches to address incentive-related challenges. For example, \cite{saig2024incentivizing} employ algorithmic contract design to enhance the quality of generated content from LLMs. In another direction, \cite{duetting2024mechanism} design auctions that aggregate outputs from multiple LLMs for advertising in an incentive-compatible manner, \cite{harris2023algorithmic} propose a Bayesian persuasion framework with generative AI simulating receiver behavior, and \cite{sun2024mechanism} develop mechanisms for fine-tuning LLMs that aggregate reward models from multiple agents. A recent work \cite{hao2024online} introduces an online learning mechanism that addresses strategic human annotators in RLHF by formulating a novel dynamic Bayesian game. In their setting, agents (annotators) adversarially misreport their preference probabilities to sway the principal's aggregation toward their own interests, while the principal seeks to learn the (unknown) most accurate agent through repeated interactions and minimize regret. In contrast, our agents randomly misreport labels to maximize the difference between the expected utility and their effort, and our principal's goal is to incentivize agents to produce high-quality annotations.



A stream of works has also focused on the theoretical analysis of contract theory \citep[e.g.,][]{dutting2021complexity,dutting2019simple,alon2022bayesian,10.1145/3670865.3673607}. For comprehensive overviews, see \cite{dutting2024algorithmic,NBERw13480}. Among these studies, our work is most closely related to the analysis of optimality in linear and binary contracts. For example, \cite{holmstrom1987aggregation,herweg2010binary,georgiadis2020optimal,lopomo2011knightian} investigate settings where binary contracts are optional, while \cite{holmstrom1987aggregation,walton2022general,carroll2015robustness,barron2020optimal} study linear contracts. In contrast to these works, we analyze the convergence rate to the first-best solution for both binary and linear contracts rather than their optimality.

The most relevant paper to our study is \cite{frick2023monitoring}, which examines the convergence rate of the principal's payoff to the first-best as the amount of data increases (thereby revealing the agent's effort). Specifically, they analyze the convergence rates of binary and linear contracts, showing that the binary contract can achieve the optimal convergence rate and the linear contract yields a suboptimal rate. They further provide a ranking of monitoring technologies (i.e., given the agent's effort, how the monitoring data is generated) by determining which technology achieves better principal utility. There are two main differences between their work and ours: (i) they assume a discrete action (effort) space for agents; we extend the analysis with a new proof scheme to the continuous space. (ii) Although they offer a ranking method to evaluate given monitoring technologies based on maximizing the principal's utility (and only feasible with a discrete action space), we design a novel monitoring technology specially tailored to the language preference data. We then analyze and compare it with other classical technologies from both the assessment perspective and the principal utility perspective, with both theoretical and numerical results.


\section{Problem Setup}

\label{sec:setup}

In this section, we first introduce an idealized setting for human annotators, which is commonly assumed as true and widely used as the backbone in developing reward/preference models for aligning LLMs. And then we propose a more practical setting to model the annotation behavior. Consider the task of preference (reward) modeling based on pairwise preference data. Each data sample consists of a tuple 
$$(x, y_1, y_2, Z)$$
where $x\in \mathcal{X}$ denotes a prompt/instruction, $y_1, y_2\in \mathcal{Y}$ are two candidate responses to $x$, and $Z\in\mathcal{Z}$ is a random variable that denotes the feedback indicating the preference between $y_1$ and $y_2$. %The feedback $Z$ can be viewed as a realized proxy of the marginal probability $\mathbb{P}(y_1 \succ y_2 | x)$ that $y_1$ is better than $y_2$ for the prompt $x$.
In the canonical setup (\cite{bai2022training, ouyang2022training} among others), the label $Z$ takes binary values, i.e., $\mathcal{Z} = \{0, 1\}$. Furthermore one assumes $Z$ is a Bernoulli random variable such that
\begin{equation}
\mathbb{P}(Z=1) = 1-\mathbb{P}(Z=0) = \mathbb{P}\left(y_{1} \succ y_{2}\ | \ x\right).
\label{eqn:Z_binary}
\end{equation}

\begin{assumption}
Assume the label $Z$ is produced by human annotators and it follows \eqref{eqn:Z_binary}.
\label{assm_basic}
\end{assumption}

This assumption has been (unconsciously) widely used as the backbone of training mainstream preference and reward models; some literature (e.g., \cite{gao2024impact,liang2024robust,wang2024secrets}) discusses the case of noisy or poisoned labels of $Z$ without \eqref{eqn:Z_binary}. Differently, our paper focuses on assessing whether human annotators meet such an assumption (and to what extent), and motivating human annotators to do so.

%The literature has mainly focused on the case where the annotation probability model \eqref{eqn:Z_binary} holds, yet  In comparison, we consider the case where the probability model \eqref{eqn:Z_binary} is violated by a certain proportion of low-quality (randomly labeled) annotations.

We denote the true preference probability as a function of $(x, y_1, y_2)$
\begin{equation}
p(x,y_1,y_2) \coloneqq \mathbb{P}(y_1 \succ y_2\ |\ x).  
\label{eqn:true_preference}
\end{equation}
Let $\mathcal{P}$ denote the probability distribution from which $(x,y_1,y_2)$ is sampled. Then, the probability value $p(x,y_1,y_2)$ can be viewed as a random variable accordingly. 

In the light of human annotation under Assumption \ref{assm_basic}, we can think of the probability $p(x, y_1, y_2)$ as the preference between $y_{i,1}$ and $y_{i,2}$ among the whole population, and $Z_i$, the label annotated by one human annotator, is a random draw from the population. We refer to \citep{sun2024rethinking, liu2024reward} for more discussion on the probability model behind the annotation procedure. Throughout the paper, we focus on this binary annotation setup and leave the more complicated annotation setups such as ranking and ordinal feedback for future studies. 

The downstream task of preference modeling thus refers to the learning of a probability model $\hat{p}_\theta$ (induced by a reward function) $r_{\theta}(x,y): \mathcal{X}\times \mathcal{Y}\rightarrow \mathbb{R}$ with parameter $\theta\in \Theta$ from 
an annotated dataset $$\mathcal{D}_{\mathcal{Z}} \coloneqq \{(x_i, y_{i,1}, y_{i, 2}, Z_i)\}_{i=1}^N,$$
where the preference probability $\hat{p}_\theta$ between $(x, y_1)$ and $(x, y_2)$ is modeled by the relative relationships of $r_\theta(x, y_1)$ and $r_\theta(x, y_2)$. For example, the prevalent way to relate the reward model with the preference probability is via the Bradley-Terry model \citep{bradley1952rank}
\[
\hat{p}_\theta(y_1 \succ y_2 | x) \coloneqq \frac{\exp\big(r_{\theta}(x, y_1)\big)}{\exp\big(r_{\theta}(x, y_1)\big) + \exp\big(r_{\theta}(x, y_2)\big)},
\]
where the probability is modeled by the softmax reward values on the right-hand side. Then the learned reward model $r_{\theta}(x,y)$ is used for aligning/post-training the LLMs.


% To be put in the literature review: gao2023scaling, cheng2024adversarial, dong2024rlhf, lin2024limited, wang2024secrets, xiong2024iterative, yang2024regularizing, zhang2024improving


\subsection{Annotator probability model}

While the above discusses an idealized setting for preference annotation, we introduce a more realistic probability model to characterize the behavior of annotators. Let a binary random variable $V\in\{0,1\}$ represent whether the annotator treats one sample carefully. Specifically, we consider the following model 
%Specifically, we define the event of low-quality annotation, $\mathcal{E}_{\text{low}}$, where the annotator randomly labels the feedback with equal probabilities under $\mathcal{E}_{\text{low}}$ while following the law \eqref{eqn:Z_binary} under $\mathcal{E}_{\text{low}}^\complement$:
\begin{equation}
\begin{split}
\mathbb{P}(Z=1|V=0, x, y_1, y_2) = \mathbb{P}(Z=0|V=0, x, y_1, y_2) &= \frac{1}{2},\\
\mathbb{P}(Z=1|V=1, x, y_1, y_2) = 1 - \mathbb{P}(Z=0|V=1, x, y_1, y_2) &= p(x, y_1, y_2).
\end{split}
\label{eqn:Z_annotation_quality}
\end{equation}
The probability model \eqref{eqn:Z_annotation_quality} reduces to \eqref{eqn:Z_binary} when $\mathbb{P}(V=0) = 0$. In the other extreme of $\mathbb{P}(V=0) = 1$, the annotations are produced by coin flips. Essentially, $V=1$ indicates that the annotator is fully committed to the sample, whereas $V=0$ indicates that the annotator assigns the label randomly.

Throughout the paper, we consider a simplified case where
\begin{equation}
\mathbb{P}(V=1|x,y_1,y_2) \equiv \eta \text{ \ for all \ } (x,y_1,y_2).
\label{eqn:annotation_eta}
\end{equation}
In other words, the random variable $V$ is a Bernoulli variable independent from $(x,y_1,y_2),$ and for all the samples, the chance that the annotator fully commits ($V=1$) is always $\eta$. Most of the results in our paper still hold under a more complicated model where $V$ and $(x,y_1,y_2)$ are dependent, say, for a more difficult sample $(x,y_1,y_2),$ the annotator probably has a lower chance of full commitment ($V=1$). We choose this simplified setting mainly for notation simplicity and to better generate intuitions. In the light of \eqref{eqn:annotation_eta}, $\eta$ is a key parameter that reflects the annotator's commitment level and consequently the quality of the annotation. The company aims to assess the annotators by estimating $\eta$, and on the other hand, the annotator chooses $\eta$ as a decision variable according to the design of the annotation system. We will discuss these two aspects in the following two sections, respectively.

%We simplify the problem to the case where all the $\mathcal{E}_{\text{low}}$'s are independent.

%We further define the annotation quality as the complement probability of the low-quality annotation event:
%\[
%\eta(x,y_1,y_2) \coloneqq 1 - \mathbb{P}(\mathcal{E}_{\text{low}}\ | \ x,y_1,y_2) = \mathbb{P}(\mathcal{E}_{\text{low}}^\complement \ | \ x,y_1,y_2).
%\]
%From the definition, we can see that the higher the $\eta$, the lower the probability of $\mathcal{E}_{\text{low}}$. Different from the previously mentioned literature, we view the annotation quality $\eta$ as a decision by the annotator. For example, the low-quality annotations may be a consequence of fatigue, and the annotator can choose a lower $\eta$ to save the effort yet resulting in lower-quality annotations. In our paper, we aim to discuss how the annotation system can be designed to incentivize human annotators to improve the annotation quality $\eta$. We restrict to the simplest yet inspiring enough case where $\eta$ remains constant almost surely
%\[
%\eta(x,y_1,y_2) \equiv \eta, \quad \text{a.s.}
%\]
%and leave other more complicated cases for the future.


\section{Assessing Human Annotators}
\label{sec:assess}

In this section, we study the problem of assessing human annotators. Before we proceed, we note two challenges of this assessment problem. First, the intrinsic heterogeneity among annotators: For a sample $(x,y_1,y_2)$ to be annotated, some people may think $y_1$ is better while others prefer $y_2$ (see Appendix \ref{appx:example} for a few examples). As a result, we cannot say for sure that an annotator makes a mistake when they give a certain label to a data sample. Second, the relationship between the annotation quality and the performance of the downstream alignment is unclear. For example, if one randomly flips 10\%-20\% of the annotated data, the downstream alignment performance does not drop significantly \citep{gao2024impact}. Also, in practice, the human-annotated data will be combined with data generated from other sources (such as other annotators or AI-generated data) in the alignment phase, and this causes an additional attribution issue. These factors combined make it impossible to assess the performance of an annotator from the downstream alignment performance. 

With these two challenges in mind, we discuss two assessment methods: the canonic expert-based monitoring and our proposed self-consistency monitoring.  


%Before diving into the question of how the annotation system affects the behavior of the annotators, we first examine how the behavior of the annotators can be evaluated and monitored. In this section, we introduce expert-based monitoring as a commonly adopted practice and give a brief statistical analysis of the errors. We later propose a novel self-monitoring method that overcomes the limitations of expert-based monitoring.

\subsection{Expert-based monitoring}

We first consider the expert-based monitoring system, commonly used in traditional production quality control. An expert can be a manager, an inspector, or a senior-level annotator who inspects (a proportion of) the annotated data carefully to evaluate the quality of the labeled data produced by one annotator. We formulate the problem as a hypothesis testing problem:
$$H_0: \eta\le \eta_0, \ \ \ \ H_1: \eta\ge \eta_1$$
where $0 \leq \eta_0 < \eta_1 \leq 1$. Here $\eta_1$ can be some target level that the annotator is expected to achieve, say, a full commitment on $\eta_1=95\%$ of the samples. Meanwhile, $\eta_0$ is some penalty threshold; if the annotator does not fully commit on $\eta_0=80\%$ of the samples, they will face some penalty.

\begin{algorithm}[ht!]
\centering
\caption{Expert-based monitoring}
\label{alg_expert}
\begin{algorithmic}
\State \textbf{Input:} A set of $N$ samples $\mathcal{D}_{\mathcal{Z}}$ produced by one annotator 
\State \textbf{Output:} Assert $\eta\le \eta_0$ or $\eta\ge \eta_1$ 
\State Randomly select a subset of $n\le N$ samples $\mathcal{D}_{\text{test}}\subseteq \mathcal{D}_{\mathcal{Z}}$ without replacement and let
\[
\mathcal{D}_{\text{test}} \coloneqq \{(x_i, y_{i, 1}, y_{i, 2}, Z_i)\}_{i=1}^n.
\]
\State The expert's monitoring decision is based on some testing function $\Psi: \mathcal{D}_{\text{test}}\rightarrow \{0,1\}$
\State \textbf{Return} $\Psi(\mathcal{D}_{\text{test}})$
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg_expert} describes the generic procedure of expert-based monitoring. The first step is to sample a subset $\mathcal{D}_{\text{test}}$ of $n$ annotations from the whole data $\mathcal{D}_{\mathcal{Z}}.$ Without loss of generality, we let the test data $\mathcal{D}_{\text{test}}$ be the first $n$ annotations in the overall $N$ annotations. The expert's testing decision can be viewed as a function of $\mathcal{D}_{\text{test}}$ indicating the acceptance ($\Psi(\mathcal{D}_{\text{test}})=0$)  or rejection ($\Psi(\mathcal{D}_{\text{test}})=1$)  of $H_0.$


\begin{proposition}
\label{prop:info_lower_bound}
The following inequality holds for any $0 \leq \eta_0 < \eta_1 \leq 1,$
%For arbitrary $0 \leq \eta_0 < \eta_1 \leq 1$, one has to decide $\mathcal{D}_{\text{test}}$ is from either $\eta \leq \eta_0$ or $\eta \geq \eta_1$ from $n$ i.i.d. test samples. If we are using testing function $\Psi$'s where $\Psi = 0$ implies inferring $\eta \leq \eta_0$ and $\Psi = 1$ implies inferring $\eta \geq 1$, then the sum of two types of errors is lower bounded as follows:
\[
\inf_{\Psi}\left\{ \mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 1 \middle | \eta \leq \eta_0\right) + \mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 0 \middle | \eta \geq \eta_1\right) \right\} \geq \frac12 \cdot\exp(- n D_{\mathrm{kl}}(\mathcal{P}_{\eta_0} \| \mathcal{P}_{\eta_1})),
\]
where the infimum over $\Psi$ is taken with respect to any measurable function and the probability $\mathbb{P}(\cdot)$ on the left hand side is with respect to the law of \eqref{eqn:Z_annotation_quality} and \eqref{eqn:annotation_eta}. Here $\mathcal{P}_{\eta_0}$ and $\mathcal{P}_{\eta_1}$ on the right hand side refer to the joint distribution of $(x,y_1,y_2,Z)$ under the law of \eqref{eqn:Z_annotation_quality} and \eqref{eqn:annotation_eta}, with $\eta=\eta_0$ and $\eta=\eta_1$, respectively. 
\end{proposition}

The proposition gives a lower bound for the sum of the two types of errors, and it follows from a standard application of Le Cam's method. We note that there is basically no restriction imposed on $\Psi$, and $\Psi$ can even utilize the knowledge of $p(x,y_1,y_2)$ -- true probability \eqref{eqn:true_preference}. This means the lower bound is (probably far) more optimistic than what people can achieve in a real-world scenario. 

To provide more intuitions on the lower bound, Figure \ref{fig:main_hist_LB} plots the lower bound on four different preference datasets. Specifically, for each dataset, we either use its original preference probability $p(x,y_1,y_2)$ or calibrate its corresponding open-sourced preference model and use it as an estimate of the true preference probability. In some sense, we can think that the preference models have a paramount performance in comparison to human experts on the annotation task, and hence it is a legitimate proxy of $p(x,y_1,y_2)$  (more details on the datasets and calibrations deferred to Appendix \ref{appx:fig_hist_LB}). For the three preference datasets of PKU, Helpsteer, and Ultra, it takes from 100 to 500 samples to reduce the sum of two types of errors to a reasonable level. The explanation is that for these datasets, the preference between $y_{i,1}$ and $y_{i,2}$ is vague for most of the samples (See examples in Appendix \ref{appx:example}). For the preference dataset of Skywork, the sample number is much smaller as the preference between $y_{i,1}$ and $y_{i,2}$ is mostly clear. To interpret the result, let's say each annotator produces 1000 annotations per week, and if we perform a quality assessment every week, the result implies that we need to examine a few hundred samples for each annotator, which makes this expert-based monitoring infeasible. If the company has the budget to hire experts to examine these many samples, the company can directly hire these experts to annotate the samples, and then this causes another layer of the problem -- how to assess the expert. 


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKU__histogram-eps-converted-to.pdf}
 
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Helpsteer__histogram-eps-converted-to.pdf}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Ultra__histogram-eps-converted-to.pdf}

  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/sky__histogram-eps-converted-to.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKUeta_2_0_mode_1__LB-eps-converted-to.pdf}
    \caption{PKU}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Helpsteereta_2_0_mode_1__LB-eps-converted-to.pdf}
    \caption{Helpsteer}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Ultraeta_2_0_mode_1__LB-eps-converted-to.pdf}
    \caption{Ultra}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/skyeta_2_0_mode_1__LB-eps-converted-to.pdf}
    \caption{Skywork}
  \end{subfigure}
  \caption{\small How expert-based monitoring fails on real preference data. Upper four plots: histograms of $\mathbb{P}(y_{\text{chosen}} \succ y_{\text{rejected}} \mid x)$ ($y_{\text{chosen}}$ and $y_{\text{rejected}}$ represent the chosen/preferred and rejected responses, respectively). Lower four plots: the lower bound of the sum of two types of errors against the number of tested annotations $n$ at different $\eta_0$ with $\eta_1=1$ (see Proposition \ref{prop:info_lower_bound}). The observations align with Proposition \ref{prop:info_lower_bound}: the lower bound (i) decreases monotonically with $n$ and increases with $\eta_0$, and (ii) depends on the underlying distribution of preference probabilities. Note that the PKU dataset, where preference probabilities are mostly around 1/2, faces higher errors in assessing annotation quality than datasets (e.g., Skywork) where preference probabilities deviate further from 1/2. See Appendix \ref{appx:fig_hist_LB} for the setup and additional results with $\eta_1<1$.}
\label{fig:main_hist_LB}
\end{figure}


\subsubsection{Agreement-based test -- a practical monitoring algorithm}


Proposition \ref{prop:info_lower_bound} presents a lower bound and some negative numerical results on the practical feasibility (even in the best and most optimistic case) of expert-based monitoring. Now we turn to a more practical scenario by specifying the test function $\Psi$, which provides additional insights and a finer characterization of the test error. Consider an agreement-based test as follows, which is commonly used in the literature of data quality control \citep{krippendorff1989content,krippendorff2004reliability,artstein2008inter,monarch2021human}. For each sample $(x_i,y_{i,1},y_{i,2})$, let $Z_i$ denote the annotator's label and $Z_{i}^{(e)}$ denote the label of the expert. Accordingly, we define the agreement variable 
\[
A_i \coloneqq \begin{cases}
1, \quad & \text{if } Z_i = Z_i^{(e)}, \\
0, \quad & \text{otherwise}.
\end{cases}
\]

%Previous results focus on the lower bound of the errors of any test while applying no restrictions on the feasibility of it. To develop a practical algorithm, we suggest that an expert could infer the annotation quality $\eta$ by checking the coincidences between her/himself and the annotator. On observation of the test data, the expert makes her/his own recommended annotation $Z_i^{(e)}$ as a consequence of each $(x_i, y_{i,1}, y_{i,2})$. Then the expert reports the \textit{coincidences} with the annotator, denoted as

It is very natural to suppose that $Z_i$ and $Z_i^{(e)}$ are conditionally independent based on the observation of each $(x_i, y_{i,1}, y_{i,2})$. Let the preference probability of the expert
$$p_{e}(x,y_1,y_2) \coloneqq \mathbb{P}\left(Z_{i}^{(e)}=1\Big|(x,y_1,y_2)\right).$$
Then we have the probability of agreement
$$\mathbb{P}(A_i=1|(x_i,y_{i,1},y_{i,2})) = 2\eta\left(p(x,y_{i,1},y_{i,2})-1/2\right)\left(p_e(x,y_{i,1},y_{i,2})-1/2\right)+1/2.$$


Define
\[
c(x, y_1, y_2) \coloneqq 4\left(p(x,y_{i,1},y_{i,2})-1/2\right)\left(p_e(x,y_{i,1},y_{i,2})-1/2\right)
\]
to quantify the degree of (underlying) agreement between the agent's and the principal's preferences. Intuitively, the sign of $c(x, y_1, y_2)$ indicates whether the preferences between the annotator and the expert are aligned (positive sign) or misaligned (negative sign), while its absolute value reflects the strength of the alignment or misalignment.

Moreover, denote
\[
c_i \coloneqq c(x_i, y_{i,1}, y_{i,2}) \quad \text{and} \quad \bar{c} \coloneqq \mathbb{E}[c_i]
\]
where the expectation is taken with respect to the sample $(x_i, y_{i,1}, y_{i,2})$.

Consider an expert-based monitoring system as Algorithm \ref{alg_expert} that uses the collection $\bm{A} \coloneqq \{A_i\}_{i=1}^n$ for the test $\Psi$. Then the following proposition shows that a test based on the average of the agreement variables $$\bar{A}\coloneqq \frac{1}{n}\sum_{i=1}^n A_i$$ is uniformly most powerful. It also relates the distribution of $\bar{A}$ with the quantity of $\bar{c}.$
\begin{proposition} We have
$n \cdot \bar{A}$ follows a binomial distribution $\mathrm{Binomial}\left(n, \frac{1+\bar{c} \eta}{2}\right)$.
If $\bar{c}$ is known, then $\bar{A}$ is a sufficient statistic of $\eta$. In addition, one can build a Neyman-Pearson uniformly most powerful test for $\eta$ based $\bar{A}$.
\label{prop:UMP}
\end{proposition}

Proposition~\ref{prop:info_lower_bound} gives a lower bound for the error of a hypothesis testing problem with respect to $\eta$. The following proposition considers the estimation problem, and one can yield a similar lower bound by applying Le Cam's two-point method. 

%In the previous section, we have presented Le Cam's two-point bound for the case where a minimum distance exists between the null hypothesis $H_0$ and the alternative hypothesis $H_1$ in . For the general estimation of $\eta$ in the continuous case, Le Cam's two-point method is also applicable, yielding the following result.


\begin{proposition}
\label{prop:lecam_lower_bound}
Denote $\bm{A} = \{A_1, \dots, A_n\}$'s joint distribution under $\eta$ by $\mathcal{Q}_{\eta}^{\otimes n}$. Denote $\hat{\eta}$ as any estimator of $\eta$ based on $\bm{A}$, we have
\begin{equation}
\begin{split}
\inf_{\hat{\eta}} \sup_{\eta \in [0, 1]} \mathbb{E}\left[|\hat{\eta} - \eta|\right]
&\geq \sup_{\eta_0, \eta_1 \in [0, 1]} \frac12 \cdot |\eta_0 - \eta_1| \cdot \left(1-\mathrm{TV}(\mathcal{Q}_{\eta_0}^{\otimes n}, \mathcal{Q}_{\eta_1}^{\otimes n})\right)\\
&\geq \sup_{\eta_0, \eta_1 \in [0, 1]} \frac14 \cdot |\eta_0 - \eta_1| \cdot \exp\left(-nD_{\mathrm{kl}}(\mathcal{Q}_{\eta_0} \| \mathcal{Q}_{\eta_1})\right)
= \Omega\left(\frac{1}{|\bar{c}|\sqrt{n}}\right).
\end{split}
\label{eq:lecam_continuous_lower_bound}
\end{equation}
\end{proposition}

% In the discrete case where a minimum distance exists between the null hypothesis and the alternative hypothesis, one can derive Le Cam's two-point lower bound again as presented in Proposition~\ref{prop:info_lower_bound}. However, the annotation quality % $\eta$ itself is continuous. We provide a more general analysis that deals with continuity based on mutual information. Mutual information implies the minimax performance of the statistical tests: the larger the mutual information, the better the risk's lower bound of the estimation. For example, we present Fano's method as a typical mutual information method (MIM). 

% For any prior distribution $P_{\eta}$ of $\eta$, denote the mutual information under the Kullback-Leibler divergence between $\eta$ and $\bar{A}$ by $I(\eta; \bar{A})$.

% \begin{proposition}
% \label{prop:Fano_lower_bound}
% Denote $\hat{\eta}$ as any estimator based on $\bar{A}$, we have
% \[
% \inf_{\hat{\eta}}\sup_{\eta\in[0,1]}
% \mathbb{E}\left[\left|\hat{\eta}(\bar{A})-\eta\right| \right] \geq  \sup_{\epsilon>0} \frac{\epsilon}{2} \cdot \left(1 - \frac{\sup_{P_{\eta} \in \mathcal{P}_{[0, 1]}} I(\eta; \bar{A}) + \log2}{\log (1+1/\varepsilon)}\right).
% \]
% where $\mathcal{P}_{[0, 1]}$ is the set of all distributions of which the support is $[0, 1]$. Furthermore, if $\xi \sim \mathrm{Binomial}(n, p)$, then
% \[
% \sup_{P_{\eta} \in \mathcal{P}_{[0, 1]}} I(\eta; \bar{A}) = \sup_{P_p \in \mathcal{P}_{[1/2, (1+|\bar{c}|)/2]}} I(p; \xi) \leq \sup_{P_p \in \mathcal{P}_{[0, 1]}} I(p; \xi) = (1+o(1)) \cdot \frac{1}{2} \log n.
% \]
% As a consequence, we have for any positive $r > 0$,
% \[
% \inf_{\hat{\eta}}\sup_{\eta\in[0,1]}
% \mathbb{E}\left[\left|\hat{\eta}(\bar{A})-\eta\right| \right] = \Omega\left(n^{-\frac12 - r}\right).
% \]
% \end{proposition}

% From the fact that $[1/2, (1+|c_1|)/2] \subset [1/2, (1+|c_2|)/2]$ for any $|c_1| \leq |c_2|$, we note that $\sup_{P_{\eta} \in \mathcal{P}_{[0, 1]}} I(\eta; \bar{A})$ is monotonically increasing with respect to $|\bar{c}|$. 
From the proposition, a larger $|\bar{c}|$ can reduce the minimax lower bound of the error for any possible estimator of $\eta$. And $|\bar{c}|$ is large if the sample $(x, y_1, y_2)$ has an apparent preference meaning in the eyes of both the annotator and the expert; in such a case, both $|p(x,y_1,y_2) - 1/2|$ and $|p_e(x,y_1,y_2) - 1/2|$ are large on expectation and share the same sign for most of the samples (or disagree with each other on most samples, which is unlikely in the real cases). From Proposition~\ref{prop:info_lower_bound} and Figure \ref{fig:main_hist_LB}, we know that the lower bound in the previous proposition will be reduced when $|p(x,y_1,y_2) - 1/2|$ is large on expectation. Now in this specialized setting, we additionally expect $|p_e(x,y_1,y_2) - 1/2|$ to be large. The quantity $|p_e(x,y_1,y_2) - 1/2|$ will be maximized when the expert has the exact knowledge of $p(x,y_1,y_2)$ which is hardly practical. 

%Compared to previous results that only require the term $|p - 1/2|$ to be large, the additional requirement on $p_e$ has some concerns. It requires that the expert be very confident and an efficient estimation is only possible if the expert has the same preference as the agent. Such a requirement may violate the diversity of human preferences under some subjective cases. 



\subsection{Self-consistency monitoring}

Now we present the second assessment method which we call \textit{self-consistency monitoring}. It aims to address the heterogeneity among different annotators, and it also saves the additional time and money cost of expert-based monitoring. The idea is to duplicate some samples in the dataset for the annotator to label them twice and then to check if the two labels are consistent. Such an idea is very natural and often adopted in survey design. The full procedure is described in Algorithm \ref{alg_self}. 



\begin{algorithm}[ht!]
\centering
\caption{Self-consistency monitoring}
\label{alg_self}
\begin{algorithmic}
\State \textbf{Input:} A set of $N+n$ samples $\mathcal{D}_{\mathcal{Z}}'$ produced by one annotator. Among the $N+n$ samples, there are $n$ duplicated samples, i.e., $n$ random samples from the original $\mathcal{D}_{\mathcal{Z}}$ where each appears twice in $\mathcal{D}_{\mathcal{Z}}'$. Let 
$Z_i$ and $Z_{i}'$ denote the annotated labels for these $n$ samples for their first and second appearance, and define the self-consistency variable as
$$A_{i} \coloneqq \begin{cases}
1, \quad & \text{if } Z_i = Z_i', \\
0, \quad & \text{otherwise}.
\end{cases} $$
\State \textbf{Output:} Assert $\eta\le \eta_0$ or $\eta\ge \eta_1$ 
\State Based on the self-consistency variables, define
$$\bm{A} = \{A_{i}\}_{i=1}^n$$
\State The self-consistency monitoring decision is based on some testing function $\Psi: \bm{A} \rightarrow \{0,1\}$
\State \textbf{Return} $\Psi(\bm{A})$
\end{algorithmic}
\end{algorithm}

In comparison with expert-based monitoring, self-consistency monitoring employs the annotators themselves as the experts to assess their annotations. We can augment the previous probability model \eqref{eqn:Z_annotation_quality} and \eqref{eqn:annotation_eta} to capture the annotator's behavior in the second annotation of the same sample. Specifically, for a sample $(x,y_1,y_2)$, let $Z$ and $Z'$ be the two annotations produced by the same annotators. Consider the following probability model 
\begin{equation}
\begin{split}
\mathbb{P}\left(Z=Z'|V=0, x, y_{1}, y_{2}\right) &=\frac{1}{2},\\
\mathbb{P}(Z=Z'|V=1, x, y_1, y_2)& = 1 -\delta.
\end{split}
\label{eqn:Z_annotation_quality_new}
\end{equation}
where $\delta\in[0,1]$ is the disagreement probability under full commitment. To interpret the model, when the annotator assigns the label randomly for the sample, then the two annotations agree with 1/2 probability. When the annotator fully commits on the sample, it may still happen that the two annotations are different, and we model this disagreement probability with $\delta$. In the ideal case, the annotator memorizes precisely their preference over all the samples and $\delta\equiv 0$; in practice, even under a full commitment, $\delta$ can still be positive but it should be reasonably small. Then under the probability model of \eqref{eqn:Z_annotation_quality}, \eqref{eqn:annotation_eta} and \eqref{eqn:Z_annotation_quality_new}, the agreement probability on the $i$-th duplicate sample
\begin{equation}
\mathbb{P}\left(A_i = 1|x_{i},y_{i,1},y_{i,2}\right) = \eta(1-\delta)/2 + 1/2.
\label{eqn:agreement_self_consist} 
\end{equation}

\begin{comment}
\begin{assumption}
If the first-time annotation $Z$ is of high quality, then the re-annotation result $Z^\prime$ remains the same with probability $1 - \delta$; otherwise, the re-annotation result $Z^\prime$ is independent of the first-time result $Z$. That is to say,
\[
\mathbb{P}(Z_i^\prime = Z_i \ | \ \mathcal{E}_{\text{low}}) = \frac12,
\quad
\mathbb{P}(Z_i^\prime = Z_i \ | \ \mathcal{E}_{\text{low}}^\complement) = 1 - \delta.
\]
\end{assumption}

We can then define the coincidence similarly
\[
C_i^\prime \coloneqq \mathbbm{1}\{Z_i = Z_i^\prime\}.
\]
\end{comment}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_PKUeta0.8_thr1.0_eta21__LB-eps-converted-to.pdf}
    \caption{PKU, $\eta_0=0.8$}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_PKUeta0.9_thr1.0_eta21__LB-eps-converted-to.pdf}
    \caption{PKU, $\eta_0=0.9$}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_Helpsteereta0.8_thr1.0_eta21__LB-eps-converted-to.pdf}
    \caption{Helpsteer, $\eta_0=0.8$}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_Helpsteereta0.9_thr1.0_eta21__LB-eps-converted-to.pdf}
    \caption{Helpsteer, $\eta_0=0.9$}
  \end{subfigure}
  \caption{\small Comparison between self-consistency monitoring (upper bound) and expert-based monitoring (lower bound). For the sum of two types of errors, we plot the \textbf{upper bound} for self-consistency monitoring with various values of $\delta$ (blue, thick line) and the \textbf{lower bound} for expert-based monitoring (red, dashed line), evaluated at $\eta_0 \in \{0.8, 0.9\}$ and $\eta_1 = 1$ for two datasets. Even with a nontrivial disagreement probability $\delta$, self-consistency monitoring outperforms expert-based monitoring over a wide range of $n$, especially when the average preference probability is near $1/2$ (e.g., PKU). See Appendix~\ref{appx:fig_self_UB} for details on the experimental setup and additional results with $\eta_1 < 1$.}
\label{fig:main_self_UB}
\end{figure}

Figure \ref{fig:main_self_UB} plots the realized error of self-consistency monitoring against the lower bound of Proposition \ref{prop:info_lower_bound}. The fact that the realized bound is better than the theoretical lower bound is not a contradiction, but it is because of the introduction of duplicated samples in Algorithm \ref{alg_self}, and the additional structure of \eqref{eqn:agreement_self_consist}. Another benefit of self-consistency monitoring is that its performance is contingent on the parameter $\delta$ but not on the underlying distribution $p(x,y_1,y_2).$ Thus it resolves the challenge of annotator heterogeneity in performance assessment, and also saves the additional costs in hiring experts. To some extent, we can think of self-consistency monitoring as having the annotators themselves as the experts to assess their own annotation performance. In this light, the annotator uses their second label to the same sample to judge their full commitment to the first label, and as the judgment is from the same annotator, there is no heterogeneity among annotators involved. 

 \begin{proposition}
\label{prop:KL_compare}
Let $\hat{\eta}$ be any estimator of $\eta$ based on $\bm{A}$. The minimax lower bound of the estimation error for the self-consistency monitoring method is
\begin{equation}
\inf_{\hat{\eta}} \sup_{\eta \in [0, 1]} \mathbb{E}\left[|\hat{\eta} - \eta|\right] = \Omega \left(\frac{1}{(1-\delta) \sqrt{n}}\right)
\end{equation}
where the expectation is taken with respect to the randomness of $\bm{A}$ in Algorithm \ref{alg_self}.
\end{proposition} 


% \begin{proposition}

% For the KL-divergence term in Proposition \ref{prop:info_lower_bound}, if $1-\delta > 2\mathbb{E}[|p(x,y_1,y_2)-1/2|]$, then the self-consistency monitoring method has a value larger than (any) expert-based method, and thus enjoys a better lower bound for the sum of the two types of errors. Particularly, if $1-\delta> \bar{c}$, then the KL divergence under the self-consistency monitoring method has a larger value than the expert agreement-based test. 

% \end{proposition}   

While Figure \ref{fig:main_self_UB} gives some numerical evidence on the advantage of self-consistency monitoring, Proposition \ref{prop:KL_compare} states that the advantage is also exemplified in an improved lower bound against 
% Proposition \ref{prop:info_lower_bound} and 
Proposition \ref{prop:lecam_lower_bound}. When $1-\delta \geq \bar{c} = \mathbb{E}[4|p(x,y_1,y_2) - 1/2|\cdot |p_e(x,y_1,y_2)- 1/2|]$, then the above lower bound is smaller than that for the expert-based monitoring in Proposition \ref{prop:lecam_lower_bound}. Such a condition is easy to meet in that a fully committed annotator will have a consistent preference over time. Therefore, the two labels of the same sample should agree with a high probability, and thus $\delta$ should be very close to 1.



% The following result is a direct consequence of Lemma \ref{prop:mutual_info}.
% \begin{proposition}
% \label{prop:MutualInf_compare}
% If $1-\delta \geq c(x, y_1, y_2)$ almost surely, then the mutual information $I(\eta; \bm{A})$ of the self-consistency monitoring is no smaller than that of the expert-based monitoring.
% \end{proposition}
% Using Proposition \ref{prop:Fano_lower_bound}, we provide another way to show the superiority of self-monitoring.





\section{Incentivizing Human Annotators}
\label{sec:incentivizing}

%In this section, we study the mechanisms to incentivize the human annotator 


We consider the classic principal-agent model \citep{harris1979optimal, holmstrom1979moral, grossman1992analysis, laffont2009theory, frick2023monitoring} to characterize the dynamics between the company (principal) and the annotator (agent). The goal of mechanism design under the principal-agent model is to have the principal design contracts so that the agent's behavior is bound by the contract and thus incentivized to behave desirably for the interests of both parties. A proper contract design avoids the so-called \textit{moral hazard}; for the context of data annotations, moral hazard arises when the annotators provide low-quality annotations but do not bear the full consequence of these low-quality annotations. The principal-agent model is a ``leader-follower'' game (also known as the Stackelberg game) where the principal moves first and the agent acts after observing the principal's move. Here the LLM company acts as the principal who designs a contract, and the annotator acts as the agent who determines the annotation quality if the contract is accepted.

%It is well known that moral hazards may arise when individuals engage in private actions that affect the global outcome. The annotation quality is an example of which the Pareto-optimum is hard to achieve since the individuals are not properly incentivized. One of the most classic models of this problem is the principal-agent model . 

The source of moral hazard or undesirable agent behavior is the information asymmetry between the two parties. The principal-agent model sets up the asymmetry in the sense that the agent's action is not directly observed by the principal. In our case, the information asymmetry is the annotation quality $\eta$ (see the probability model \eqref{eqn:Z_annotation_quality} and \eqref{eqn:annotation_eta}). The agent is aware of their annotation quality $\eta$ but the principal cannot directly access this quality without an assessment system such as the ones in the previous section. To this end, the goal of incentivizing the annotators to produce high-quality annotation (with a large $\eta$) entails a joint study of the assessment system together with the contract design, i.e., how the company pays the annotator for annotating the data. This is the focus of this section. 

%A natural solution to mitigate this is to investigate efforts to monitor the agent's action and consider that when designing contracts. In simple cases where complete monitoring may be possible, the first-best solution can be achieved by forcing a penalty on the misbehavior of the agent. Yet full observations are usually impossible or prohibitively costly in real cases. 

\subsection{Principal-agent model for data annotation}

\label{sec:pa_models}

We first present the agent's utility function. As noted earlier, the action of the agent is the annotation quality $\eta \in [0, 1]$. It doesn't hurt to consider the total annotation number $N$ to be fixed. The utility function of the agent is modeled by 
\[
U_a(w_a, \eta) \coloneqq G_a(w_a) - E(\eta).
\]
Here $w_a$ is the wealth/money payment received by the agent for annotating $N$ samples, and $w_a$ is jointly determined by the contract (will be defined shortly) and the annotated data. The function $G_a: \mathbb{R}\rightarrow \mathbb{R}$ is the monetary utility of the agent and is strictly increasing as the payment $w_a$ increases. The effort function $E:[0,1]\rightarrow \mathbb{R}$ is a function of $\eta$ (given the total annotation number $N$ is fixed). 

\begin{assumption}
\label{assm:basic_assm_1}
Assume that the monetary utility $G_a$ is twice continuously differentiable, monotonically increasing, and strictly concave and that the effort $E$ is purely a function of $\eta$ (for fixed $N$ annotations) that is twice continuously differentiable, monotonically increasing, and convex, i.e.,
\[
\frac{\mathrm{d} G_a}{\mathrm{d} w_a} > 0, \quad \frac{\mathrm{d}^2 G_a}{\mathrm{d} w_a ^{\ 2}} < 0;
\]
\[\frac{\mathrm{d} E}{\mathrm{d} \eta} > 0, \quad \frac{\mathrm{d}^2 E}{\mathrm{d} \eta^2} \geq 0.
\]
\end{assumption}

Assumption \ref{assm:basic_assm_1} imposes some basic assumptions on the functions $G_a$ and $E$. The increasingness of $G_a$ is natural, and the concavity of $G_a$ is also known as the risk aversion of the agent. In the risk-neutral case of a linear $G_a$, the discussion of contract design under the principal-agent model becomes trivial (as well known in the literature \citep{harris1979optimal}). The assumption of $E$ simply reflects human nature: to maintain a high annotation quality (large $\eta$) requires a large effort, and the paid effort becomes even larger when the agent wants to achieve an even higher quality (say, $\eta = 0.96$) from an already high quality (say, $\eta = 0.95$). The major implication of Assumption \ref{assm:basic_assm_1} for our discussions is to guarantee that (one of) the optimal mixed strategy of the agent must be a pure strategy, so we can focus on the pure strategy only when considering the agent's action.



%$U_a(w_a, E)$ where the agent receives a payoff of wealth $w_a$ and makes effort $E$. We assume that the agent's utility can be factorized as
%$U_a(w_a, E) = G_a(w_a) - E,$
%where $G_a(w_a)$ is the monetary utility of the agent and is strictly increasing as $w_a$ increases (to guarantee the existence of $G_a^{-1}$). 
%Then the agent's utility function can be expressed as

%As is well known in the literature (for example, \citet{harris1979optimal}), the first-best solutions can be achieved if the agent is risk-neutral. Following the literature, we assume that the agent is risk-averse such that . Thus,  

Next, we model the principal's utility function by
\[
U_p(w_a, \eta) \coloneqq -w_a + \mu(\eta).
\]
Here $w_a$ is the payment made by the principal (company) to the agent (annotator) and $\eta$ is the annotation quality -- a decision made by the annotator. The function $\mu: [0,1] \rightarrow \mathbb{R}$ models the utility function of the company with respect to the data quality $\eta$, for which we make the following assumption.

\begin{assumption}
Assume that $\mu$ is a twice continuously differentiable, increasing, and concave function of $\eta$, i.e.,
\begin{equation*}
 \frac{\mathrm{d} \mu}{\mathrm{d} \eta} \geq 0, \quad \frac{\mathrm{d}^2 \mu}{\mathrm{d} \eta^2} \leq 0.
\end{equation*}    
\label{assm:basic_assm_2}
\end{assumption}
The increasingness of $\mu$ is intuitive in that better data quality leads to large utility. The concavity of $\mu$ is widely acknowledged and verified by the downstream performance of an ML/LLM model under noisy data \citep{gao2024impact,chowdhury2024provably,liang2024robust}.   


We note that an important quality appearing in both the agent and the principal utility functions is the payment $w_a.$ Under the principal-agent model, $w_a$ is determined through a (sequence of) contract(s) $F_n \in \mathcal{F}_n$. Each possible $F_n$ is a function that maps the assessment dataset $\mathcal{D}_n$ to the payment to the annotator,
$$w_a \coloneqq F_n(\mathcal{D}_n).$$
The assessment dataset $\mathcal{D}_n$ is a dataset of $n$ samples used to assess the performance of the annotator. For example, $\mathcal{D}_n$ can be the dataset $\mathcal{D}_{\text{test}}$ in expert-based Algorithm \ref{alg_expert} or the dataset $\bm{A}$ in self-consistency Algorithm \ref{alg_self}. The function $F_n$ prescribes how much money $w_a$ is paid to the annotator based on the $\mathcal{D}_n$. It is a \textit{sequence} of contracts in that it is indexed by $n$; for a different sample size $n$, the function can be different. The contract class $\mathcal{F}_n$ is determined by the principal, and then the agent's decision of $\eta$ is made in the knowledge and under the constraint of $\mathcal{F}_n$. In this light, $\mathcal{F}_n$ provides a handle for the principal to regulate the agent's behavior. In the following two sections, we further specify two classes of possible contracts, binary contract and linear contract. 

%from the space of test annotations $\mathcal{D}_{\text{test}}$ of size $n$ (abbreviated as $\mathcal{D}_n$) to a real number (as the wage paid to the agent).




%We assume the principal to be risk-neutral to simplify the analysis, implying that the monetary utility of the principal $G_p(w_p)$ can be set to $G_p(w_p) \equiv w_p$. The principal's utility function is defined as
%$
%U_p(w_p, \mu) \coloneqq w_p + \mu,
%$
%where $\mu$ is the data utility gained from the model trained by those collected annotations.
%\shang{Discussions: justify using scaling laws or some other papers or LLMs are hard to evaluate.} 


With the definition of utility functions and contracts, the dynamics between the principal and the agent can be formulated as a constrained optimization problem. The idealized scenario is called the \textit{first-best} solution which corresponds to the following program
\begin{equation}
\begin{split}
\mathcal{C} \coloneqq \max_{F_n \in \mathcal{F}_n, \eta \in [0, 1]} \, & \mathbb{E}\left[U_p(F_n(\mathcal{D}_n), \eta)\right], \\
\text{s.t. } & \mathbb{E}\left[U_a(F_n(\mathcal{D}_n), \eta)\right] \geq U_0
\end{split}
\label{eq:first_best}
\end{equation}
where the expectation is taken with respect to the randomness of the annotation data $\mathcal{D}_n$. Here the right-hand side of the constraint $U_0$ is called the leisure utility which denotes the utility the agent gains by rejecting the contract (say, not to work for the company). The constraint requires the agent's expected utility under the contract to be above this threshold $U_0$. Such a constraint is often referred to as the \textit{individual rationality} (IR). The objective function is to maximize the expected utility of the principal. We say it is an idealized scenario in that the problem treats both the contract $F_n$ and the annotation quality $\eta$ as the decision variables. Equivalently, under the first-best formulation, we can think of the principal has a perfect monitoring system and thus an exact knowledge of $\eta$; and consequently, the principal can design the contract in a way to control $\eta$. However, in reality, the principal can only optimize the contract $F_n$ but cannot even observe $\eta$, not to say to optimize it. In this sense, the program \eqref{eq:first_best} characterizes the best-achievable utility for the principal for the case that the principal can even control $\eta$.

\begin{assumption}
\label{assm:basic_assm_3}
The wage is bounded, i.e., there exist constants $\underline{w}, \overline{w}\in \mathbb{R}$ such that
\[
F_n(\mathcal{D}_n) \in [\underline{w}, \overline{w}]
\]
almost surely for all $F_n \in \mathcal{F}_n$. 
Besides, the wage range is rich enough to cover all the efforts (plus the leisure utility), i.e.
\[
E([0, 1]) + U_0 \subset \mathrm{interior}\left(G_a([\underline{w}, \overline{w}])\right),
\]
\end{assumption}
The first part of the assumption simply requires the wage $w_a$ paid to the agent to be bounded. This is without loss of generality in that if the wage $w_a$ is too low (say, lower than $U_0 + E(0)$), then the agent would quit the contract; if $w_a$ is too high (say, higher than $\mu(1) - \mu(0)$), then the principal would choose not to provide the contract. The second part of the assumption is to consider a rich enough wage range to ensure that (under perfect monitoring/the first-best setting) the principal could offer a contract to make the agent achieve any effort option $\eta \in [0, 1]$.


\begin{proposition}
\label{prop:first_best_sol}
Under Assumptions \ref{assm:basic_assm_1}, \ref{assm:basic_assm_2}, and \ref{assm:basic_assm_3}, the first-best problem \eqref{eq:first_best} has a unique optimal solution. In addition, the optimal annotation quality
\[
\eta^* \coloneqq \argmax_{\eta \in [0, 1]} \, -G^{-1}_a(E(\eta) +U_0 ) + \mu(\eta),
\]
and the principal pays the agent $w_a^* \coloneqq G^{-1}_a(E(\eta^*) + U_0 )$ units of wealth. Here $G^{-1}_a$ is the inverse function of $G_a$, the existence of which is guaranteed by Assumption \ref{assm:basic_assm_1}.
\end{proposition}

The proposition characterizes the optimal quality $\eta^*$ and the optimal payment amount $w_a^*$ under the first-best program. 


%However, in the principal-agent game, the agent only cares for their own utility. The above solution is usually impossible to achieve. Instead, the program changes to the following form.

A more practical setting is that the principal decides the contract $F_n$ and the agent decides the annotation quality $\eta$. This corresponds to the so-called second-best solution which corresponds to the following bi-level optimization problem
\begin{equation}
\begin{split}
\mathcal{C}_n \coloneqq \max_{F_n \in \mathcal{F}_n} \, & \mathbb{E}\left[U_p(F_n(\mathcal{D}_n), \eta_a(F_n))\right], \\
\text{s.t. } & \mathbb{E}\left[U_a(F_n(\mathcal{D}_n), \eta_a(F_n))\right] \geq U_0, \\
& \eta_a(F_n) \in \argmax_{\eta \in [0, 1]} \mathbb{E}\left[U_a(F_n(\mathcal{D}_n), \eta)\right].
\end{split}
\label{eq:second_best}
\end{equation}
Here, as before, the expectation is taken with respect to the randomness of the annotation data $\mathcal{D}_n$. Compared to \eqref{eq:first_best}, there is an additional last constraint in \eqref{eq:second_best}. The constraint says that the annotation quality is determined by the annotator maximizing their expected utility, known as \textit{incentive compatibility} (IC). As a result, the outer program \eqref{eq:second_best} contains only one decision variable $F_n$ for the principal to optimize over. We note that the annotation quality $\eta_a(F_n)$ chosen by the annotator depends on the contract $F_n$, and this is determined by the nature of the game that the principal first offers the contract, and then the agent makes their decision upon seeing the contract. 

Another more restricted definition of second-best solutions examined in some literature \citep{frick2023monitoring} requires that $\eta_a(F_n) \equiv \eta^*$ (See Proposition \ref{prop:first_best_sol} for the definition of $\eta^*$).
\begin{equation}
\begin{split}
\tilde{\mathcal{C}}_n \coloneqq \max_{F_n \in \mathcal{F}_n} \, & \mathbb{E}\left[U_p(F_n(\mathcal{D}_n), \eta^*)\right], \\
\text{s.t. } & \mathbb{E}\left[U_a(F_n(\mathcal{D}_n), \eta^*)\right] \geq U_0, \\
& \eta^* \in \argmax_{\eta \in [0, 1]} \mathbb{E}\left[U_a(F_n(\mathcal{D}_n), \eta)\right].
\end{split}
\label{eq:restricted_second_best}
\end{equation}

For the above three programs \eqref{eq:first_best}, \eqref{eq:second_best}, and \eqref{eq:restricted_second_best}, we know 
%As there is one more constraint such that $\eta_a(F_n) = \eta^*$ in \eqref{eq:restricted_second_best} compared to \eqref{eq:second_best}, the optimal value of \eqref{eq:restricted_second_best} is no greater than that of \eqref{eq:second_best}.
\begin{equation*}
%\label{eqn:snd_best_rank}
\mathcal{C} \geq \mathcal{C}_n \geq \tilde{\mathcal{C}}_n.
\end{equation*}

While ${\mathcal{C}}_n$ and $\tilde{\mathcal{C}}_n$ represent a more practical setting, the question is how large the gap is between these two and the ideal case of $\mathcal{C}.$ The analysis of the gap sheds light on how we should design the contract class $\mathcal{F}_n$ -- the domain of $F_n$. A proper $\mathcal{F}_n$ leads to a smaller gap which means an effective regulation of the agent's behavior in the choice of $\eta$, and the regulation is achieved implicitly by the design of $\mathcal{F}_n$. In the following two subsections, we present two classes of contracts, both of which enjoy a small gap and are simple to use in practice. 


%In the following subsections, we will discuss the gap between the first-best and the (restricted) second-best solutions under different cases of $\mathcal{F}$'s.

\subsection{Binary contracts}
Binary contracts refer to those contracts $F_n$'s that map the assessment dataset $\mathcal{D}_n$ onto only two values. We describe the structure of binary contracts in Algorithm \ref{alg_binary_contr}. A binary contract $F_n$ is specified by three components, a test $\Psi$, base salary $w$, and bonus $w_b$. The test $\Psi$ takes the data $\mathcal{D}_n$ as the input and gives a binary output indicating where the annotator passes the quality assessment. If so, the company pays the annotator the sum of the base salary and the bonus $w+w_b$; if not, the company only pays the base salary. 

\begin{algorithm}[ht!]
\centering
\caption{Binary contract}
\label{alg_binary_contr}
\begin{algorithmic}
\State \textbf{Input:} A dataset $\mathcal{D}_n$ used to assess the annotator performance and a binary contract $F_n = (\Psi, w, w_b)$
\State \textcolor{blue}{\%\% $\Psi(\mathcal{D}_n)\in \{0,1\}$ represents some test as the ones in Algorithm \ref{alg_expert} and Algorithm \ref{alg_self}}
\State \textcolor{blue}{\%\% $w$ is the base salary and $w_b$ is the bonus for passing the performance assessment}
\State Perform the test $\Psi(\mathcal{D}_n)$
\If{$\Psi(\mathcal{D}_n)=0$, i.e., the annotator fails the performance test}
\State The company pays the annotator $w_a = w$
\Else \ \ (the annotator passes the performance test)
\State  The company pays the annotator $w_a = w+w_b$
\EndIf
\end{algorithmic}
\end{algorithm}


%Binary contracts are those contracts that map onto only two values. Defining the value domain of $\mathcal{F}_n$ to be $\mathcal{V}(\mathcal{F}_n)$, we have:
%\[
%\mathcal{F}_n^{\text{bin}} \coloneqq \{F_n: |\mathcal{V}(\mathcal{F}_n)| = 2\}.
%\]
In the finite-action case where the agent can only choose from a finite set of possible actions, \citet{frick2023monitoring} shows that the gap between $\mathcal{C}_n$ and $\mathcal{C}$ is of an optimal order $\exp(-\Theta(n))$. However, their analysis's large deviation theory tools no longer apply to the continuous case, and the authors leave the continuous case as an open question therein. For the problem of human annotation, the annotation quality $\eta$ is naturally a continuous variable that the annotator chooses as a decision, and there are infinite possibilities for the choice of $\eta$.

For our analysis, we consider a simple setting in quality monitoring/assessment that is also general enough to cover many application contexts including ours. Each inspected sample $D_i\in \mathcal{D}_n$ ($i=1,...,n$) passes the assessment with a certain probability, and the probability is a function of the effort: the more effort the agent spends on the sample, the higher the chance that the sample passes the assessment. For the preference data annotation, if the LLM company adopts our proposed self-consistency monitoring, then each sample $A_i$ is the agreement variable defined in Algorithm \ref{alg_self}. Then when the two labels agree, $A_i=1$ and the sample passes the assessment; the pass probability $P(A_i=1)=\frac{1+c\eta}{2}$ for $c=1-\delta$ (see \eqref{eqn:agreement_self_consist}). We show that in contrast to the discrete case, binary contracts of the (restricted) second-best problem no longer bear an exponential convergence rate to the first-best. Instead, the convergence rate is far worse in the continuous case: if we inspect $n$ samples, then the gap between the first-best and the second-best is $\Theta(1/\sqrt{n \log n})$. 


\begin{assumption}
\label{assm:basic_assm_4}
The unique first-best solution $\eta^*$ is bounded away from 0 and 1:
\[
\eta^* \in (0, 1).
\]
\end{assumption}
The assumption is to avoid trivial solutions such as $\eta^* = 0$ or those solutions that have no randomness in the outcome (say, $\eta^* = 1$ and $c = 1$ so the agreement/consistency happens with probability $\frac{1+c\eta^*}{2} = 1$).


\begin{theorem}
\label{thm:binary}
Under Assumptions \ref{assm:basic_assm_1}, \ref{assm:basic_assm_2}, \ref{assm:basic_assm_3}, and \ref{assm:basic_assm_4}, we have
\[
\mathcal{C} - \tilde{\mathcal{C}}_n^{\text{bin}} = \Theta\left(1/\sqrt{n \log n}\right);
\quad
\mathcal{C} - \mathcal{C}_n^{\text{bin}} = \Theta\left(1/\sqrt{n \log n}\right)
\]
where $n$ is the number of samples in the assessment dataset $\mathcal{D}_n.$
\end{theorem}

Theorem \ref{thm:binary} shows the gap between the first-best and the second-best solutions for binary contracts. To our knowledge, it is the first such result for the principal-agent model where the agent's decision is continuous. The key to our analysis is (i) to use the first-order condition of the agent's utility optimization problem and (ii) a careful analysis of the tail probability of binomial distribution. It justifies the effectiveness of the simple and easy-to-implement binary contract, also known as the bonus scheme in practice. In the following, we outline the proof sketch and we note that the upper bound proof is constructive, which characterizes that the optimal contract is a threshold contract. 


\subsubsection*{Proof Sketch of Theorem \ref{thm:binary}}
The proof of the theorem does not fully follow the standard method in literature, so we provide a proof sketch here and defer the full details to Appendix \ref{subsec:proof_of_binary}.

The proof consists of two parts: the upper bound and the lower bound.

\textbf{Upper bound.} We prove the upper bound by considering a subclass of binary contracts: \textit{threshold contracts}. Suppose the principal pays the agent a bonus $w_b$ if $\sum_{i=1}^n A_i \geq k$ for some $k = \Theta(n)$ as the decision threshold. To interpret the contract under self-consistency monitoring, it means if for $k$ out of the $n$ duplicated samples, the annotator gives consistent labels, then the annotator can win a bonus $w_b.$ The analysis of the threshold contract consists of four steps as follows.

\textbf{Step 1. Reduce the problem to tail probability estimation.} Show the gap between the first-best and the second-best is approximately the payoff's variance due to the agent's risk-averse nature. This step is standard in the literature \citep{laffont2009theory, frick2023monitoring}.

\textbf{Step 2. Examine the curve of the marginal incentive.} Show that the marginal incentive $w_b \cdot \frac{\partial \mathbb{P}(\sum_{i=1}^n A_i \geq k)}{\partial \eta}$ for the agent to increase the quality $\eta$ is approximately a bell-shaped curve centered near the decision threshold $k/n$. The term ``approximately a bell-shaped curve'' means that the marginal incentive behaves similarly to a normal random variable with mean around $k/n$ and variance $\Theta(1/n)$.

\textbf{Step 3. Estimate the agent's decision via the first-order condition.} From the first-order condition of the agent's utility maximization, we know the agent's choice of $\eta_a$ happens at the intersection between the bell-shaped marginal incentive curve and the marginal effort curve $\frac{\nabla E(\eta)}{\nabla \eta}$. The two curves intersect twice at some $\eta_1$ and $\eta_2$ such that $p(\eta_1) < \frac{k-1}{n-1} < p(\eta_2)$, and the agent's choice to maximize their utility must be the one on the right. Then we prove that the intersection point $\eta_2$ happens at a distance of $\Theta(\sqrt{\log n/n})$ from the contract's threshold $k/n$ due to the bell-curve approximation.

\textbf{Step 4. Estimate the tail probability that the agent does not get the bonus.} By a careful examination of the properties of binomial distributions and the first-order condition, the probability that the agent exactly reaches the decision threshold $\mathbb{P}(\sum_{i=1}^n A_i = k | \eta_2)$ is of $\Theta(1/n)$. Then we estimate the ratio $\frac{\mathbb{P}(\sum_{i=1}^n A_i = j | \eta_2)}{\mathbb{P}(\sum_{i=1}^n A_i = j-1 | \eta_2)}$. Finally, we use the sum of geometric sequences to derive bounds for the tail probability (and henceforth the variance), which is of $\Theta(1/\sqrt{n \log n})$.


\textbf{Lower bound.} As noted in Step 1 of the upper bound proof, the gap is approximately the payoff's variance. The lower bound proof reduces to showing that the contract that minimizes the payoff's variance must be a threshold scheme.

\textbf{Step 1. Convexify the combinatorial rejection region of the monitoring test.} The original test that decides whether the agent receives the bonus can be expressed by the rule that if $n\cdot \bar{A} \notin \mathcal{K}$, then the agent receives the bonus otherwise not. The set $\mathcal{K} \subset \{0,1,...,n\}$ is the rejection region. We convexify this monitoring test by a rule that if $n\cdot \bar{A} = k$, then with probability $w_k \in [0,1]$ the agent gets the bonus for $k=1,...,n$. Then the rejection rule is parameterized by $w_k$'s and we can optimize $w_k$'s as a handle to minimize the variance.

\textbf{Step 2. Formulate the optimization problem of $w_k$'s to minimize the variance.} The objective is to minimize $\text{Bonus}^2 \cdot \mathrm{Var}(\mathbb{P}(\text{Bonus}))$. The first-order condition requires that the bonus times the marginal incentive at $\eta = \eta^*$ must be a constant, implying that the $\text{Bonus}$ term can be substituted by the inversion of $\sum_{k} w_k \cdot \frac{\partial \mathbb{P}(n \cdot \bar{A} = k)}{\partial \eta}\vert_{\eta = \eta^*}$. The term $\mathbb{P}(\text{Bonus})$ is $\sum_{k} w_k \cdot \mathbb{P}(n \cdot \bar{A} = k|\eta^*)$. The problem is now formulated as an optimization problem of different $w_k$'s.

\textbf{Step 3. Inspecting the local minima to show that any optimal binary contract must be thresholding.} By calculating the derivatives, show that all local minima must be $w_k = 0$ for $k < k_0$ and $w_k = 1$ for $k > k_0$. Then the minimizer must be some threshold scheme (up to one single term $k = k_0$). For large enough $n$, any single term $\mathbb{P}(n \cdot \bar{A} = k_0)$ does not change the estimation. From the discussions on the threshold scheme for the upper bound analysis, we know that the variance is of $\Theta(1/\sqrt{n \log n})$.


\subsection{Linear contracts}

Under linear contracts, each contract $F_n$ is specified by a function $f_n$ that maps from each assessment sample $D_i$ to a positive value and pays the agent the average of $f_n(D_i).$ The procedure is described by Algorithm \ref{alg_lin_contr}. For example, under self-consistent monitoring, each sample $D_i$ corresponds to the agreement variable $A_i$ in Algorithm \ref{alg_self}. Then the payment $w_a$ is equal to the proportion of consistent labels annotated by the annotator. 

\begin{algorithm}[ht!]
\centering
\caption{Linear contract}
\label{alg_lin_contr}
\begin{algorithmic}
\State \textbf{Input:} A dataset $\mathcal{D}_n=\{D_1,...,D_n\}$ used to assess the annotator performance and a linear contract $F_n = (f_n)$
\State \textcolor{blue}{\%\% $f_n(D_i)\in \mathbb{R}$ maps each assessment data sample $D_i$ to a payment amount (say, based on its quality)}
\State  The company pays the annotator $$w_a = \frac1n \sum_{i=1}^n f_n(D_i)$$
\end{algorithmic}
\end{algorithm}

\begin{theorem}
\label{thm:linear_contract}
Under Assumptions \ref{assm:basic_assm_1}, \ref{assm:basic_assm_2}, \ref{assm:basic_assm_3}, and \ref{assm:basic_assm_4}, if \eqref{eq:restricted_second_best} is feasible, then we have
\[
\mathcal{C} - \tilde{\mathcal{C}}_n^{\text{lin}} = \Theta\left(1/n\right); \quad 
\mathcal{C} - \mathcal{C}_n^{\text{lin}} = \Theta\left(1/n\right)
\]
where $n$ is the number of samples in the assessment dataset $\mathcal{D}_n.$
\end{theorem}

Theorem \ref{thm:linear_contract} gives the gap between the first-best and second-best solutions under linear contracts. Compared to the case of binary contracts, the analysis here is standard. As in the previous analysis, the gap is approximately the payoff’s variance, and then, for linear contracts, the variance decays as the order of $1/n$. This basically describes the crux of the proof; we defer the full proof to Appendix \ref{apd:proof_of_linear}.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/PKUexp_para_1_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
    \caption{PKU}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Helpsteerexp_para_1_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
    \caption{Helpsteer}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Ultraexp_para_1_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
    \caption{Ultra}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/skyexp_para_1_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
    \caption{Skywork}
  \end{subfigure}
  \caption{\small Normalized principal utility gap ($\mathcal{C}-\mathcal{C}_n$ and $\mathcal{C}-\tilde{\mathcal{C}}_n$) under different monitoring and contract settings. In these experiments, we set $U_0=0$, $\delta=0.02$, $\mu(\eta)=1/2\eta^{4/5}$, $G_a(w_a)=1-\exp(-w_a)$, and $E(\eta)=0.18\eta^2$ (see Appendix \ref{appx:fig_contract_rank} for further details and additional configurations). (i) The self-consistency monitoring consistently outperforms the expert-based monitoring given the same second-best formulation and contract type. (ii) The performance of the expert-based monitoring depends on the underlying distribution of preference probabilities and may perform poorly in some cases (e.g., PKU). (iii) The numerical results validate Theorems \ref{thm:binary} and \ref{thm:linear_contract}: the linear contract closes the gap at a faster rate than the binary contract in $n$. For instance, in PKU under $\tilde{\mathcal{C}}_n$ with expert-based monitoring (red square line), the binary contract initially exhibits a lower utility gap than the linear contract, but when $n\geq 100$, the linear contract achieves a lower utility gap.} 
\label{fig:main_contract_rank}
\end{figure}

Figure \ref{fig:main_contract_rank} illustrates the performance of linear contracts and binary contracts under the monitoring methods of expert-based and self-consistency. We make the following observations: first, self-consistency monitoring has a uniformly better performance than expert-based monitoring. Second, both contracts give a converging gap as the number of monitored samples $n$ increases. Third, linear contracts have a smaller gap than binary contracts (under the same monitoring method); however, binary contracts may be easier and more friendly to implement in practice since the agent's behavior must be near the decision threshold.

\section{Conclusion}

In this paper, we study the problem of assessing and incentivizing human annotators in the context of human preference annotation, which is an emerging problem in the era of large language models and generative AI. In the face of two new challenges (1) the lack of a ``true label'' due to human annotators' heterogeneity and (2) the hardness in evaluating downstream performance and tracing back the data quality, we propose a self-consistency monitoring approach to assess human annotators, which requires the human annotators to re-label a random subset of the samples. This approach addresses both concerns and enjoys a smaller error as well compared to classic expert-based monitoring (in terms of mini-max lower bounds). To solve the problem of designing better contracts to incentivize human annotators, we give a new analysis under the framework of the principal-agent model. The analysis of the continuous action space shows that the binary contract's gap between the first-best and the second-best is $\Theta(1/\sqrt{n \log n})$, inferior to the linear contract's $\Theta(1/n)$. This result partly answers an open question proposed by \citet{frick2023monitoring}, where the discrete case is analyzed such that the binary contract is of $\exp(-\Theta(n))$ gap. Our result also underlines the importance of reducing the monitoring costs (to increase test number $n$), which further supports the superiority of self-consistency monitoring. The analysis is of independent interest since traditional tools such as large deviation theory fail in the continuous case. Our results are not restrictive: (1) our self-consistency method applies to not only binary preference data but also other types of preference data, such as ranking or best-of-$K$ selection in preference annotation; and (2) while our proof in Theorem \ref{thm:binary} assumes the binomial distribution, the framework can also be extended to other distributions such as normal distributions, which we leave as future work.


% \subsection{Moral hazard without monitor}
% \label{sec:fixed}
% The basic payment mechanism is to pay the annotator a salary of $S$ every week/month for completing the annotation task of $N$ samples. In other words, each annotator is assigned $N$ samples every week/month, and upon finishing annotating these $N$ samples, the annotator receives $S$ compensation. 
% Then the utility of the agent is
% $
% U_a(S, \eta) = G_a(S) - E(\eta),
% $
% and that of the principal is
% $
% U_p(S, \eta) = -S + \mu(\eta).
% $
% We shall see from the following proposition that under this type of payment mechanism, the annotators are not incentivized to ensure good-quality annotations, and consequently obtaining good-quality annotations relies on some \textit{code of honor}.
% \begin{proposition}
% If $\mathcal{F} = \mathcal{F}_{\text{no monitor}}$ that only consists of a fixed payment without monitoring, the annotation quality chosen by the agent is always zero. In other words,
% \[
% \eta_a(F) = 0, \quad \forall F \in \mathcal{F}_{\text{no monitor}}.
% \]
% \end{proposition}

% \subsection{Bonus under monitor}
% There are two necessary conditions to incentivize the annotators to provide high-quality annotations. First, the principal must spend some extra costs to inspect and monitor the annotation quality $\eta$. Second, the principal must design a contract that provides good incentives for the agent to perform well. In this subsection, we explore the simplest case. We consider a contract with a bonus if the annotations pass some test (also known as binary contracts) to incentivize the agent. We investigate the coincidence-based tests discussed in Section \ref{sec:assess}. The principal specifies a statistical test using some statistic $T(\mathcal{D}_{\text{test}})$ of the annotated samples $\mathcal{D}_{\text{test}}$ of size $n$ and some rejection region $\mathcal{R}$. The principal must also pay an extra monitoring cost of $M n$ for inspecting $n$ samples. For self-consistency monitoring methods, the cost can be viewed as paying the agent to annotate extra $n$ labels. An agent will be rewarded a bonus $B$ if the agent is not rejected ($T \notin \mathcal{R}$). Now the contract $F$ consists of $(T, \mathcal{R}, S, B, n)$, and we denote this type of contract as $\mathcal{F}_{\text{bonus under monitor}}$. Recall that
% $
% U_a(w_a, \eta) = G_a(w_a) - E(\eta),
% $
% and
% $
% U_p(w_p, \eta) = w_p + \mu(\eta).
% $
% In this case, the wealth given to the agent is
% \[
% w_a = S + B \cdot \mathbbm{1}\left\{T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} \right\}.
% \]
% As for the principal, an additional monitoring cost rises up in addition to the payment to the agent
% \[
% w_p = - M n  - \left[S + B \cdot \mathbbm{1}\left\{T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} \right\}\right].
% \]
% We now start to characterize how to reach a near-optimal solution in the coincidence-based monitoring case. Throughout this section, we only consider the sample average of the coincidences. The annotations will be rejected if the coincidences are fewer than a threshold.
% % \begin{assumption}
% % Limited liability. That is to say, the money only flows from the principal to the agent. In other words,
% % \[
% % S \geq 0, \quad B \geq 0.
% % \]
% % \end{assumption}
% % \begin{assumption}
% % Fixed budget. That is to say, the overall payment has an upper bound.
% % \[
% % S+B \leq \overline{B}.
% % \]
% % \end{assumption}
% % Discussions on $S$: only to compensate the agent such that the agent's utility exceeds some threshold (leisure utility). Not critical.

% \begin{assumption}
% Assume that the function values and the first-order derivatives of the effort function $E$ are uniformly bounded by $\overline{E}$, and those of the data utility function $\mu$ are uniformly bounded by $\overline{\mu}$. Assume that the first-order derivatives of the monetary utility function of the agent $G_a$ are uniformly bounded between $\underline{G}$ and $\overline{G}$.
% \end{assumption}

% \begin{theorem}
% Assume the coincidence happens with probability $\frac{1 + (1 - \delta) \eta}{2}$ for some $\delta < 1$. Assume $\eta^* \in (0, 1)$. For any moderately large $n = \Theta((1- (1 - \delta)\eta^*)^{-1})$, by designing a contract $F(T, \mathcal{R}, S, B, n)$ of
% \begin{align*}
% B & \leftarrow \overline{E}, \quad S \leftarrow G_a^{-1}(U_0 + E(\eta^*)) - B + \tilde{\Theta}(\underline{G}^{-1} (U_0 + \overline{E}) n^{-\frac12}),\\
% T & \leftarrow \sum_{i=1}^n C_i, \quad \mathcal{R} \leftarrow \left\{T \leq \left\lceil \frac{1+ (1-\delta) \eta^*}{2} \cdot (n-1)\right\rceil\right\},
% \end{align*}
% we have the choice of the agent $\eta_a(F) = \argmax_{\eta \in [0, 1]} \mathbb{E}[U_a(w_a, \eta)]$ satisfying
% \[
% \eta^* \leq \eta_a(F) \leq \eta^* + \mathcal{O}(1/\sqrt{n}),
% \quad
% \mathbb{P}\left(T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} \middle| \eta = \eta_a(F) \right) \geq 1 - \tilde{\mathcal{O}}(1/\sqrt{n}).
% \]
% Hence, if $1 - (1-\delta)\eta^* = \Omega\left((\overline{E} + \overline{\mu})^{-\frac23} M^{\frac23}\right)$, by choosing
% \[
% n \leftarrow \tilde{\Theta}\left((\underline{G}^{-1}(U_0 + \overline{E}) + \overline{\mu})^{\frac23} M^{-\frac23}\right),
% \]
% we have
% \[
% \mathrm{OPT} - \mathbb{E}\left[U_p(F)\right] \leq  \mathcal{O}\left(M^{\frac13} (\underline{G}^{-1}(U_0 + \overline{E}) + \overline{\mu})^{\frac23}\right),
% \]
% where $\mathrm{OPT}$ is the first-best solution shown in \eqref{eq:first_best}.
% \end{theorem}
% Discussions: a larger unit monitoring cost of $M$ means worse social welfare. Expert-based monitoring is too costly; self-monitoring is cheap.


% Discussions on choosing a moderately large $n$: controlling the output to be slightly above the set rejection threshold. For linear contracts, larger $n$ helps reduce the risk. Recall the agent is risk-averse. The principal now still needs to choose a moderately large $n$ even if the estimator is unbiased to reduce the risk of the testing.


% Discussions on assuming $\eta^* < 1$: if $(1 - \delta)\eta^* = 1$, one can choose $\mathcal{R}$ to be $\{T < 1\}$. The optimum must be $\eta^* = 1$. The choice of the agent is $\eta_a = 1 - \mathcal{O}(1/n)$, which is even better compared to the case $\eta_a - \eta_* = \mathcal{O}(1/\sqrt{n})$. Thus we can keep choosing such a moderately large $n$ and the conclusion also holds with $n \leftarrow \tilde{\Theta}\left((\underline{G}^{-1}(U_0 + \overline{E}) + \overline{\mu})^{\frac12} M^{-\frac12}\right)$ and the bound is now $\mathrm{OPT} - \mathbb{E}\left[U_p(F)\right] \leq  \mathcal{O}\left(M^{\frac12} (\underline{G}^{-1}(U_0 + \overline{E}) + \overline{\mu})^{\frac12}\right)$.

% \shang{Discussions on assuming the knowledge of $\eta^*$.}


% Discussions on assuming the knowledge of $\overline{E}$: We do not assume direct access of the principal to the effort function of the agent. The effort function is only known to the agent and should be viewed as an information advantage of the agent. We only assume knowing the upper bound of the effort and the marginal effort, which means our results generalizes to the case where the principal is providing the same contract to multiple agents with different effort functions. In fact, for very large data utility $\mu$, the value of $\overline{E}$ does not affect the choice of the test sample number $n$ and nearly bears no influence on the choice of $\eta_a$ when the test is set and the test sample number is moderately large.

% \subsection{Designing statistical tests}
% \shang{This subsection needs an entire re-construction. Should include discussions on linear/binary contracts and so on.}
% From previous sections, we derive the results under the self-monitoring case. From the analysis, we can see that the quantity $\frac{\partial \mathbb{P}(T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} | \eta)}{\partial \eta}$ plays a vital role in incentivizing human annotators. We now investigate how one can design statistical tests that provide stronger incentives in a more general sense.


% Consider a family of statistical tests. If the statistical tests are well-posed in the sense that they faithfully reflect the annotation quality in the extreme cases, such that
% \[
% \mathbb{P}(T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} | \eta) \rightarrow 0, \quad \text{as } \eta \rightarrow 0,
% \]
% and
% \[
% \mathbb{P}(T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} | \eta) \rightarrow 1, \quad \text{as } \eta \rightarrow 1,
% \]
% Then the integration must be constant
% \[
% \int_0^1 \frac{\partial \mathbb{P}(T_1(\mathcal{D}_{\text{test}}) \notin \mathcal{R}_1 | \eta)}{\partial \eta} \cdot \mathrm{d}\eta = 1,
% \]
% which tells us that no statistical tests could provide \textit{uniformly stronger} incentives on all the intervals of $\eta$ than other ones. In other words, one can only hope that one test is stronger than the other on \textit{some} interval of $\eta$ and that interval covers the agent's potential decision $\eta_a$. We now investigate how to maximize the incentive
% $\frac{\partial \mathbb{P}(T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} | \eta)}{\partial \eta}$
% on a certain point.
% \begin{proposition}
% \[
% \frac{\partial \mathbb{P}(T(\mathcal{D}_{\text{test}}) \notin \mathcal{R} | \eta^\prime)}{\partial \eta^\prime} \bigg\vert_{\eta^\prime = \eta} \leq \lim_{\Delta \eta \rightarrow 0} \frac{\mathrm{TV}(\mathbb{P}(\cdot | \eta + \Delta \eta), \mathbb{P}(\cdot | \eta))}{\Delta \eta} \eqqcolon \tau(\eta).
% \]
% The equal sign holds only if $T$ is a sufficient statistic of $\eta$.
% \end{proposition}

% \begin{proposition}
% For any fixed $\eta \in (0, 1)$, $\tau(\eta)$ monotonically increases as the test sample number $n$ grows. Furthermore, for coincidence-based monitoring cases such that coincidence happens w.p. $\frac{1+c\eta}{2}$, we have
% % \begin{equation}
% % \tau(\eta) = \begin{dcases}
% % \frac12 \cdot \left(\frac{1+\eta}{2}\right)^{n-1}, \quad & \text{if } \frac{1 + \eta}{2} > \frac{n-1}{n},\\
% % \frac{1}{\sqrt{2\pi}} \cdot \sqrt{\frac{n}{1-(\lceil n \cdot \frac{(1+\eta)}{2} \rceil / n)^2}} + o(\sqrt{n}), \quad & \text{if } \frac{1 + \eta}{2} \leq \frac{n-1}{n}.
% % \end{dcases}
% % \end{equation}
% \begin{equation}
% % \tau(\eta) = 
% % \frac{1}{\sqrt{2\pi}} \cdot \sqrt{\frac{n}{1-(\lceil n \cdot \frac{(1+\eta)}{2} \rceil / n)^2}} + o(\sqrt{n}), \quad \text{as } n \rightarrow \infty.
% \tau(\eta) = 
% \frac{c}{\sqrt{2\pi}} \cdot \sqrt{\frac{n}{1-(\lceil n \cdot \frac{(1+c\eta)}{2} \rceil / n)^2}} + o(\sqrt{n}), \quad \text{as } n \rightarrow \infty.
% \end{equation}
% % \shang{Cite "Survey of Distances between the Most Popular Distributions" Proposition 6.}
% \end{proposition}
% Messages: The incentive of the optimal $T = T^*$ and $\mathcal{R} = \mathcal{R}^*$ monotonically increases as the test sample number $n$ increases; The marginal effect of increasing the test sample number $n$ decreases as $n$ grows larger.



% \begin{figure}[ht!]
%   \centering

%   \begin{subfigure}[b]{0.23\textwidth}
%     \centering
% \includegraphics[width=\textwidth]
% {fig/PKU_incentive_n_sample-eps-converted-to.pdf}
%     \caption{PKU}
%   \end{subfigure}
%     \hfill
%   \begin{subfigure}[b]{0.23\textwidth}
%     \centering
% \includegraphics[width=\textwidth]
% {fig/Helpsteer_incentive_n_sample-eps-converted-to.pdf}
%     \caption{Helpsteer}
%   \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%     \centering
% \includegraphics[width=\textwidth]
% {fig/Ultra_incentive_n_sample-eps-converted-to.pdf}
%     \caption{UltraFeedback}
%   \end{subfigure}
%   \hfill  
%   \begin{subfigure}[b]{0.23\textwidth}
%     \centering
% \includegraphics[width=\textwidth]
% {fig/sky_incentive_n_sample-eps-converted-to.pdf}
%     \caption{Skywork}
%   \end{subfigure}
%   \caption{\small \hz{TBD: change the caption to align Section 4} Incentive (under the oracle design) vs. the number of tested annotations $n$ for four datasets.  We highlight two key observations on the advantages of the self-monitoring method: (i) The incentive of expert-based monitoring heavily depends on the dataset while the self-monitoring always keeps the same high level--it remains near zero for PKU due to its preference scores clustering around 1/2 (see Figure \ref{fig:main_hist_LB}), making it difficult to distinguish low-quality even from adequate annotations, and thus low incentive; (ii) For all datasets, at the same $\eta$ and $n$, self-monitoring consistently yields higher incentives than expert monitoring. See Appendix XXX for additional details.}
% \label{fig:main_incentive}
% \end{figure}


% \section{Other Assessments}

% \subsection{Cross-annotator quality control}

% \subsection{AI-based quality control} 
% References
\bibliographystyle{informs2014}
\bibliography{main}

\newpage
\appendix


\section{Proofs and Additional Discussions}

\subsection{Proofs and Discussions for Section \ref{sec:assess}}
\subsubsection{More Discussions on Proposition \ref{prop:info_lower_bound}}

For the Kullback--Leibler divergence appearing on the right-hand side of the lower bound, we can decompose it into two terms:
\[
D_{\mathrm{KL}}(\mathcal{P}_{\eta_0} \,\|\, \mathcal{P}_{\eta_1}) = \mathrm{CrossEntropy}(\mathcal{P}_{\eta_0}, \mathcal{P}_{\eta_1}) - \mathrm{Entropy}(\mathcal{P}_{\eta_0}).
\]
Recall that a smaller \(D_{\mathrm{KL}}(\mathcal{P}_{\eta_0} \,\|\, \mathcal{P}_{\eta_1})\) induces a larger/better lower bound on the sum of the two types of errors. This observation has two implications:
\begin{itemize}
    \item A larger \(\mathrm{CrossEntropy}(\mathcal{P}_{\eta_0}, \mathcal{P}_{\eta_1})\) is favorable. Denote $c = 2(p - 1/2)$, so that we have
    \[
    \mathrm{CrossEntropy}(\mathcal{P}_{\eta_0}, \mathcal{P}_{\eta_1}) = -\frac{1+\eta_0 c}{2}\log\left(\frac{1+\eta_1 c}{2}\right)
    -\frac{1-\eta_0 c}{2}\log\left(\frac{1-\eta_1 c}{2}\right).
    \]
    It can verify that this expression is increasing in \(\eta_1\) for \(\eta_1 > \eta_0\). Intuitively, a larger gap between the target level \(\eta_1\) and the penalty threshold \(\eta_0\) makes the hypothesis testing problem less challenging.
    
    \item A smaller \(\mathrm{Entropy}(\mathcal{P}_{\eta_0})\) is favorable. Intuitively, this corresponds to a distribution \(\mathcal{P}_{\eta_0}\) with less uncertainty, meaning that the annotator's low-annotation behavior adheres to a more predictable, ``fixed pattern'', thereby facilitating the detection.
\end{itemize}



\subsubsection{Proof of Proposition \ref{prop:info_lower_bound}}
We first define the total variation distance with two lemmas.
\begin{definition}
For any two distributions $\mathcal{Q}$ and $\mathcal{P}$ over the measurable space $(\Omega, \mathcal{F})$, the total variation distance is defined as
\[
\mathrm{TV} \coloneqq \sup_{A \in \mathcal{F}} \{|\mathcal{Q}(A) - \mathcal{P}(A)|\}.
\]
\end{definition}

\begin{lemma}[Le Cam's Lemma \citep{le2012asymptotic}]
\label{lemma:le_cam}
For any two distributions $\mathcal{Q}$ and $\mathcal{P}$ over the space $(\Omega, \mathcal{F})$, and denote $\Psi$ as a measurable function from $\Omega$ to $\{0,1\}$. Then
\[
\inf_{\Psi} \big\{\mathcal{Q}(\Psi(\omega) = 0) + \mathcal{P}(\Psi(\omega) = 1) \big\} = 1 - \mathrm{TV}(\mathcal{Q}, \mathcal{P}).
\]
Furthermore, such an infimum is met with the following function
\[
\Psi^*(s) \coloneqq \mathbbm{1}\Big\{\frac{\mathrm{d}\mathcal{Q}}{\mathrm{d}\mathcal{P}}(s) \geq 1\Big\}.
\]
\end{lemma}
\begin{lemma}[Bretagnolle-Huber's Inequality \citep{bretagnolle1978estimation}]
\label{lemma:BH_ineq}
For any two distributions $\mathcal{Q}$ and $\mathcal{P}$, we have
\[
\mathrm{TV}(\mathcal{Q}, \mathcal{P}) \leq \sqrt{1 - \exp(-D_{\mathrm{kl}}(\mathcal{Q}\|\mathcal{P}))} \leq 1 - \frac12 \cdot \exp(-D_{\mathrm{kl}}(\mathcal{Q}\|\mathcal{P})).
\]
\end{lemma}

For any two distributions $\mathcal{Q}$ and $\mathcal{P}$ over the measurable space $(\Omega, \mathcal{F})$, the total variation distance is defined as
\[
\mathrm{TV} \coloneqq \sup_{A \in \mathcal{F}} \{|\mathcal{Q}(A) - \mathcal{P}(A)|\}.
\]
\textbf{Proof of Proposition \ref{prop:info_lower_bound}.}
\begin{proof}
 Since each of $n$ samples from $\mathcal{D}_{\text{test}}$  is i.i.d. sampled under the law of \eqref{eqn:Z_annotation_quality} and \eqref{eqn:annotation_eta}, we denote their distributions as $\mathcal{P}_{\eta_0}^n,\mathcal{P}_{\eta_1}^n$ under  $\eta=\eta_0$ and $\eta=\eta_1$, respectively. By the definition of KL-divergence, we have (e.g., see Theorem 2.16 in \cite{Polyanskiy_Wu_2025}):
$$D_{\mathrm{kl}}(\mathcal{P}_{\eta_0}^n\|\mathcal{P}_{\eta_1}^n)=n\cdot D_{\mathrm{kl}}(\mathcal{P}_{\eta_0}\|\mathcal{P}_{\eta_1}).$$

Then by directly using Lemma \ref{lemma:le_cam} and \ref{lemma:BH_ineq}, we have 
\begin{align*}
&\phantom{=}\ \inf_{\Psi}\left\{\mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 1 \middle | \eta \leq \eta_0\right) + \mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 0 \middle | \eta \geq \eta_1\right)\right\} \\
&\geq \inf_{\Psi}\left\{ \inf_{\eta\leq \eta_0, \eta^\prime \geq \eta_1} \Big\{\mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 1 \middle | \eta\right) + \mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 0 \middle | \eta^\prime\right) \Big\} \right\}\\
&= \inf_{\eta\leq \eta_0, \eta^\prime \geq \eta_1} \inf_{\Psi}\left\{ \mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 1 \middle | \eta\right) + \mathbb{P}\left(\Psi(\mathcal{D}_{\text{test}}) = 0 \middle | \eta^\prime\right) \right\}\\
&\geq \inf_{\eta\leq \eta_0, \eta^\prime \geq \eta_1} \frac12\cdot \exp(- n D_{\mathrm{kl}}(\mathcal{P}_{\eta} \| \mathcal{P}_{\eta^\prime}))\\
    &= \frac12 \cdot \exp(- n D_{\mathrm{kl}}(\mathcal{P}_{\eta_0} \| \mathcal{P}_{\eta_1})),
\end{align*}
where the first equality uses the exchangeability among taking infimum.
\end{proof}

\subsubsection{Proof of Proposition \ref{prop:UMP}}
\begin{proof}
By definition, given $\delta$, the distribution of each $A_i$ is Bernoulli with success probability
\[
p(\eta) = \frac{\eta \bar{c}+1}{2}.
\]
To see this, we note that for any $\eta$, $\frac{1+c\eta}{2}$ is a linear function of $c$, such that
\begin{align*}
    \mathbb{P}(A_i=1) & = \mathbb{E}\left[\frac{1+c(x, y_1, y_2)\eta}{2}\right] \\
    & = \frac{1 + \mathbb{E}[c(x, y_1, y_2)] \eta}{2}\\
    & = \frac{1 + \bar{c} \eta}{2}.
\end{align*}




Then the (joint) likelihood for the observations $\bm{A} = \{A_i\}_{i=1}^n$ is:
\[
L(\eta; \bm{A}) = \prod_{i=1}^n \left( \frac{1+\eta\bar{c}}{2} \right)^{A_i} \left( \frac{1-\eta\bar{c}}{2} \right)^{1-A_i} = \left( \frac{1+\eta\bar{c}}{2} \right)^{\sum_{i=1}^n A_i} \left( \frac{1-\eta\bar{c}}{2} \right)^{n-\sum_{i=1}^n A_i}.
\]
The likelihood purely depends on $\bar{A} = \frac1n \sum_{i=1}^n A_i$, which implies that $\bar{A}$ is a sufficient statistic. The likelihood ratio for two different parameter values $\eta_0$ and $\eta_1$ is given by
\[
\Lambda(\bm{A}) = \frac{L(\eta_1; \bm{A})}{L(\eta_0; \bm{A})} = \exp\Bigl\{ n\bar{A} \cdot \log\frac{1+\eta_1\bar{c}}{1+\eta_0\bar{c}}  + n(1-\bar{A})  \cdot \log\frac{1-\eta_1\bar{c}}{1-\eta_0\bar{c}}  \Bigr\}.
\]

Note that $\Lambda(\bm{A})$ is an increasing function of $\bar{A}$ for any given $\eta_1 \geq \eta_0$ because
\[
\log\frac{1+\eta_1\bar{c}}{1+\eta_0\bar{c}} \geq 0 \quad \text{and} \quad \log\frac{1-\eta_1\bar{c}}{1-\eta_0\bar{c}} \leq  0.
\]
This implies that the test based on $\bar{A}$ is uniformly most powerful by the Karlin--Rubin theorem \citep{karlin1956theory}.


\end{proof}

\subsubsection{Proof of Proposition \ref{prop:lecam_lower_bound}}
\begin{proof}
The first two inequalities are the direct consequence of the following Le Cam's two-point method and Bretagnolle-Huber inequality (Lemma \ref{lemma:BH_ineq}) by taking $\ell$ to be the $L_1$ distance. The proof of Le Cam's two-point method is standard and can be found in textbooks.
\begin{lemma}[Le Cam's two-point method, e.g. Theorem 31.1 in \citet{Polyanskiy_Wu_2025}]
Suppose the loss function $\ell: \Theta \times \Theta \rightarrow \mathbb{R}_+$ satisfies $\ell(\theta, \theta) = 0$ for all $\theta \in \Theta$ and the following $\alpha$-triangle inequality for some $\alpha > 0$: For all $\theta_0, \theta_1, \theta \in \Theta$,
\[
\ell(\theta_0, \theta_1) \leq \alpha\left(\ell(\theta_0, \theta) + \ell(\theta_1, \theta)\right).
\]
Then
\[
\inf_{\hat{\theta}} \sup_{\theta \in \Theta} \mathbb{E}[\ell(\theta, \hat{\theta})] \geq \sup_{\theta_0, \theta_1 \in \Theta} \frac{\ell(\theta_0, \theta_1)}{2\alpha} (1- \mathrm{TV}(\mathcal{Q}_{\theta_0}, \mathcal{Q}_{\theta_1})).
\]
\end{lemma}


Now we start to approximate the amount $\frac14 \cdot \sup_{\eta_0, \eta_1} |\eta_0 - \eta_1| \exp(-n D_{\mathrm{kl}}(\mathcal{Q}_{\eta_0} \| \mathcal{Q}_{\eta_1}))$. Note that $\mathcal{P}_{\eta} = \mathrm{Bernoulli}\left(\frac{1+\bar{c} \eta}{2}\right)$, we set $\eta_1 = 0$, $\eta_0 = 1/(\bar{c}\sqrt{n})$. Using the Taylor's expansion at $\eta = 0$ for the first argument $\eta_0$, we have
\[
D_{\mathrm{kl}}(\mathcal{Q}_{\eta_0} \| \mathcal{Q}_{\eta_1}) = \frac{1}{2n} + o(1/n).
\]
This gives our desired bound.
\end{proof}

\begin{comment}
    \subsubsection{Proof of Proposition \ref{prop:mutual_info}}
\begin{proof}
To begin with, we denote the entropy of a Bernoulli random variable with mean $\mu$ as 
$H(\mu)=-\mu\log \mu-(1-\mu)\log(1-\mu)$, where the derivation can be found in \cite{Polyanskiy_Wu_2025}.

By definitions, given any $c_i$ and $\eta$, we have 
$$\mathbb{P}(A_i=1|\eta)=\frac{1+c_i\eta}{2}, \quad \mathbb{P}(A_i=0|\eta)=\frac{1-c_i\eta}{2}.$$

Meanwhile, given any distribution $\rho$ such that $\eta\sim\rho$ , we have 
$$\mathbb{P}(A_i=1)=\mathbb{E}_{\eta}\left[\frac{c_i\eta+1}{2}\right]=\frac{1+c_i\bar{\eta}}{2},$$
where $\bar{\eta}=\mathbb{E}[\eta]$. Thus, $A_i$ follows a Bernoulli distribution with mean $\frac{1+c_i\bar{\eta}}{2}$.

Thus, by the definition of mutual information, we have 
\begin{align*}
    I_{\rho}(\eta;A_i)=&H(A_i)-H(A_i|\eta)\\
    =&H(\frac{1+c_i\bar{\eta}}{2})-\mathbb{E}_{\eta}\left[H(\frac{1+c_i\eta}{2}) \right]\\
=&\mathbb{E}_{\eta}\left[H(\frac{1+c_i\bar{\eta}}{2})-H(\frac{1+c_i\eta}{2}) \right].
\end{align*}


\noindent \textbf{(a)} We first show $I_{\rho}(\eta;A_i)$ is symmetric along $c$. By definition, we have 
\begin{align*}
    H(\frac{1-c_i\bar{\eta}}{2})=&-(\frac{1-c_i\bar{\eta}}{2})\log(\frac{1-c_i\bar{\eta}}{2})-(\frac{1+c_i\bar{\eta}}{2})\log(\frac{1+c_i\bar{\eta}}{2})\\
    =&H(\frac{1+c_i\bar{\eta}}{2}).
\end{align*}
And similarly, we can prove $\mathbb{E}_{\eta}\left[H(\frac{1-c_i\eta}{2}) \right]=\mathbb{E}_{\eta}\left[H(\frac{1+c_i\eta}{2}) \right]$. Thus we get the symmetry property.

We then show the monotonically increasing in $|c_i|$. Without loss of generality, we only prove the monotonically increasing in $c_i>0$ and the monotonically decreasing in $c_i<0$ can be similarly proved.  We first calculate the derivative of $I_{\rho}(\eta;A_i)$ w.r.t. $c_i$ and denote it as $g(c)$:
\begin{align*}
    g(c)=&\frac{\partial I_{\rho}(\eta;A_i)}{\partial c_i}\\
    =& \mathbb{E}_{\eta}\left[\frac{1}{2c_i}\left(-\bar{\eta}c_i\log\frac{1+c_i\bar{\eta}}{1-c_i\bar{\eta}}+\eta c_i\log\frac{1+c_i\eta}{1-c_i\eta}  \right)\right]\\
    =&\frac{1}{2c_i}\left(q(\mathbb{E}[c_i\eta])-\mathbb{E}[q(c_i\eta)]\right)
\end{align*}
where $q(x)=-x\log(\frac{1+x}{1-x})$ for $x\in[-1,1]$. By noting $q(x)$ is a strictly concave function for $x\in(0,1]$, by Jensen's inequality we have $g(c)> 0$ for $c>0$ and we can conclude the monotonically increasing property.


\noindent \textbf{(b)} Note (a) holds for any given $\rho$, thus (a) still holds under the supremum over $\rho$. Specifically, we have 
\begin{itemize}
    \item Symmetry: $$\sup_{\rho}\mathbb{E}_{\eta\sim\rho}\left[H(\frac{1-c_i\bar{\eta}}{2})-H(\frac{1+c_i\eta}{2}) \right]=\sup_{\rho}\mathbb{E}_{\eta\sim\rho}\left[H(\frac{1+c_i\bar{\eta}}{2})-H(\frac{1-c_i\eta}{2}) \right].$$
    \item Monotonically increasing: For any $|c'_i|> |c''_i|>0$, we have
    \begin{align*}
        \sup_{\rho}\mathbb{E}_{\eta\sim\rho}\left[H(\frac{1-c'_i\bar{\eta}}{2})-H(\frac{1+c'_i\eta}{2}) \right]
        &= \sup_{\rho}\mathbb{E}_{\eta\sim\rho}\left[H(\frac{1-|c'_i|\bar{\eta}}{2})-H(\frac{1+|c'_i|\eta}{2}) \right]\\
        &>\sup_{\rho}\mathbb{E}_{\eta\sim\rho}\left[H(\frac{1-|c''_i|\bar{\eta}}{2})-H(\frac{1+|c''_i|\eta}{2}) \right]\\
        &=\sup_{\rho}\mathbb{E}_{\eta\sim\rho}\left[H(\frac{1-c''_i\bar{\eta}}{2})-H(\frac{1+c''_i\eta}{2}) \right]
    \end{align*}
\end{itemize}


\noindent \textbf{(c)} Since we assume the conditional independence of $A_i$'s given $\eta$, by Theorem 6.1 in \cite{Polyanskiy_Wu_2025}, we can directly conclude the result. 


\noindent \textbf{(d)} By combining (b) and (c) we can conclude the result.
\end{proof}

\end{comment}

% \subsubsection{Proof of Proposition \ref{prop:Fano_lower_bound}}
% \label{appx:fano_lower_proof}

% The first inequality follows directly from an application of Fano's inequality. For example, see Theorem 31.3 in \cite{Polyanskiy_Wu_2025}: by taking $T=[0,1]$ and observing that the $\epsilon$-packing number with $L_1$ norm of $[0,1]$ is $1+1/\epsilon$, we can get the result.

% To establish the equality, first, by the definition of mutual information we have:
% \[
% \sup_{\rho} I(\eta;\bar{A}) = \sup_{\rho} I\Bigl(\eta;\sum_{i=1}^n A_i\Bigr).
% \]
% Then by a similar argument to that used in the proof of Proposition \ref{prop:UMP}, for any fixed $\eta\in[0,1]$, the random variable $\sum_{i=1}^n A_i$ follows a binomial distribution,
% \[
% \sum_{i=1}^n A_i \sim \mathrm{Binomial}\Bigl(n,\frac{1+\bar{c}\eta}{2}\Bigr).
% \]
% Hence, we can define a new random variable $\tilde{A}$ such that
% \[
% \tilde{A}\sim \mathrm{Binomial}(n,\tilde{\eta}),
% \]
% with
% \[
% \tilde{\eta}\in \mathcal{S}_{\bar{c}}\coloneqq  \left[\frac{1-|\bar{c}|}{2},\,\frac{1+|\bar{c}|}{2}\right].
% \]
% This change of variable implies
% \begin{equation}
%     \label{eqn:fano_proof_var_change}
%     \sup_{\rho: \ \text{Supp}(\rho)\subseteq [0,1] } I(\eta;\bar{A}) = \sup_{\tilde{\rho}: \  \text{Supp}(\tilde{\rho})\subseteq \mathcal{S}_{\bar{c}}} I(\tilde{\eta};\tilde{A}),
% \end{equation}
% where the distribution $\rho$'s support is within $[0,1]$, and the distribution $\tilde{\rho}$'s support is within $\mathcal{S}_{\bar{c}}$.

% By Theorem 3 in \cite{barletta2024binomial}, we have
% \[
% \sup_{\tilde{\rho}: \  \text{Supp}(\tilde{\rho})\subseteq \mathcal{S}_{\bar{c}}} I(\tilde{\eta};\tilde{A})\leq  \sup_{\tilde{\rho}: \  \text{Supp}(\tilde{\rho})\subseteq [0,1]} I(\tilde{\eta};\tilde{A})= \Theta(\log n).
% \]
% Thus, by choosing $\epsilon = 1/\sqrt{n\log n}$, we obtain
% \[
% \sup_{\epsilon>0} \frac{\epsilon}{2}\left(1 - \frac{\sup_{\rho} I(\eta; \bar{A}) + \log 2}{\log (1+1/\epsilon)}\right) \geq \frac{1}{2\sqrt{n\log n}} \left(1 - O\!\left(\frac{\log (\sqrt{n})}{\log (1+\sqrt{n\log n})}\right)\right) = \Theta\!\left(\frac{1}{\sqrt{n\log n}}\right).
% \]
% This completes the proof of the equality.

% Finally, to prove the monotonicity of $\sup_{\rho} I(\eta;\bar{A})$ on $|\bar{c}|$, note that for any $|c'|\leq |c''|$, we have
% \[
% \left[\frac{1-|c'|}{2},\,\frac{1+|c'|}{2}\right] \subseteq \left[\frac{1-|c''|}{2},\,\frac{1+|c''|}{2}\right].
% \]
% This inclusion shows that increasing $|\bar{c}|$ relaxes the constraint on the support of $\tilde{\eta}$, thereby enlarging the domain for the optimization in $\sup_{\tilde{\rho}: \  \text{Supp}(\tilde{\rho})\subseteq [0,1]}  I(\tilde{\eta};\tilde{A})$. Consequently, a larger value of $|\bar{c}|$ results in a larger maximum value. Thus, by \eqref{eqn:fano_proof_var_change}, the desired monotonicity follows.

\subsubsection{Proof of Proposition \ref{prop:KL_compare}}
\begin{proof}
The proof is almost the same as that of Proposition~\ref{prop:lecam_lower_bound}, which we omit for simplicity.
\end{proof}


\begin{comment}

\subsubsection{Proof of Proposition \ref{prop:KL_compare}}
\begin{proof}
For the KL-divergence of two Bernoulli distributions with mean $\frac{c\eta_0+1}{2}$ and $\frac{c\eta_1+1}{2}$, respectively, with the definition of the KL-divergence, we have 
$$D_{\mathrm{kl}}(\text{Bern}((c\eta_0+1)/2) \| \text{Bern}((c\eta_1+1)/2))=\log \frac{1-c \cdot \eta_0}{1-c \cdot \eta_1}+\frac{1+c \cdot \eta_0}{2} \cdot \log \frac{(1-c \cdot \eta_1)(1+c \cdot \eta_0)}{(1-c \cdot \eta_0)(1+c \cdot \eta_1)}.$$

For the expert-based monitoring and self-consistency monitoring, the $c$ equals to $4(p-1/2)(p_e-1/2)$ (or similarly, $2(p-1/2)$)   and $(1-\delta)$, respectively. Thus, to prove Proposition \ref{prop:KL_compare}, note $D_{\mathrm{kl}}(\text{Bern}((c\eta_0+1)/2) \| \text{Bern}((c\eta_1+1)/2))$ is symmetric at $c=0$,  we only need to show $D_{\mathrm{kl}}(\text{Bern}((c\eta_0+1)/2) \| \text{Bern}((c\eta_1+1)/2))$ is non-decreasing in $c>0$. 


With some algebra, $D_{\mathrm{kl}}(\text{Bern}((c\eta_0+1)/2) \| \text{Bern}((c\eta_1+1)/2))$'s derivative  with respect to $c$ is 
$$f(c)=\eta_0\log\left(\frac{(1+c\eta_0)(1-c\eta_1)}{(1-c\eta_0)(1+c\eta_1)}\right)+\frac{c\eta_1(\eta_1-\eta_0)}{(1-c\eta_1)(1+c\eta_1)}.$$

Since $\log(\cdot)$ is a concave function, we have 
\begin{align*}
    \log\left(\frac{(1+c\eta_0)(1-c\eta_1)}{(1-c\eta_0)(1+c\eta_1)}\right)&\geq \frac{(1+c\eta_0)(1-c\eta_1)-(1-c\eta_0)(1+c\eta_1)}{(1+c\eta_0)(1-c\eta_1)}\\
    &=\frac{c(\eta_0-\eta_1) }{(1+c\eta_0)(1-c\eta_1)}. 
\end{align*}

Thus, we have 
\begin{align*}
    f(c)&\geq \frac{c\eta_0(\eta_0-\eta_1) }{(1+c\eta_0)(1-c\eta_1)}+\frac{c\eta_1(\eta_1-\eta_0)}{(1-c\eta_1)(1+c\eta_1)}\\
    &=\frac{c(\eta_1-\eta_0)}{(1-c\eta_1)}\left(\frac{\eta_1-\eta_0}{(1+c\eta_1)(1+c\eta_0)}\right)\\
    &\geq 0   
\end{align*}
when $c>0$, $c\eta_1<1$ and $\eta_1\geq \eta_0$. Thus, $D_{\mathrm{kl}}(\text{Bern}((c\eta_0+1)/2) \| \text{Bern}((c\eta_1+1)/2))$ is non-decreasing in $|c|$ and we can conclude the result.
\end{proof}    
\end{comment}

% \subsubsection{Proof of Proposition \ref{prop:MutualInf_compare}}
% \begin{proof}
%     The result is a direct consequence of Proposition \ref{prop:mutual_info}. 
% \end{proof}


\subsection{Proofs and Discussions for Section \ref{sec:incentivizing}}
\subsubsection{Proof of Proposition \ref{prop:first_best_sol}}
\begin{proof}
\medskip
\noindent\textbf{The Optimal Payment is Constant.} \\
Recall that the agent's utility is given by
\[
U_a(w_a,\eta) = G_a(w_a) - E(\eta),
\]
where \(G_a\) is strictly increasing and strictly concave (reflecting risk aversion), and \(E(\eta)\) is an increasing effort function. Suppose the principal offers a contract in which the wage depends on the realization of the test annotations, that is, a random variable \(F_n(\mathcal{D}_n)\) with
\[
\mathbb{E}\Bigl[U_a(F_n(\mathcal{D}_n), \eta)\Bigr] = \mathbb{E}\bigl[G_a(F_n(\mathcal{D}_n))\bigr] - E(\eta).
\]
By Jensen's inequality and the strict concavity of \(G_a\),
\[
\mathbb{E}\bigl[G_a(F_n(\mathcal{D}_n))\bigr] < G_a\Bigl(\mathbb{E}\bigl[F_n(\mathcal{D}_n)\bigr]\Bigr)
\]
whenever \(F_n(\mathcal{D}_n)\) is not almost surely constant. Therefore, any randomization in the wage reduces the agent's expected utility. As a result, if a random wage contract satisfies the individual rationality (IR) constraint, then the principal can instead offer a constant wage
\[
w_a = \mathbb{E}\bigl[F_n(\mathcal{D}_n)\bigr]
\]
and achieve a strictly higher utility for the agent without increasing the expected cost of the principal (note the cost $G_p(w_p)\equiv w_p$ is risk-neutral). Hence, the optimal contract has a constant payment.

\medskip

\noindent\textbf{The IR Constraint is Binding.}\\
When the wage is constant, we can remove the expectation operator and the agent's utility becomes
\[
U_a(w_a,\eta) = G_a(w_a) - E(\eta),
\]
and the IR constraint is now
\[
G_a(w_a) - E(\eta) \ge U_0.
\]

To show the IR constraint must be binding under the optimal solution, suppose there is an optimal solution $(w'_a,\eta')$ where $G_a(w'_a) - E(\eta') > U_0$, then by the monotonicity of $G_a$, we can always find another feasible solution $(w''_a,\eta')$ such that $w''_a<w'_a$ and $G_a(w''_a) - E(\eta') = U_0$. However, we have the objective function now satisfying 
$$-w''_a + \mu(\eta')> -w'_a + \mu(\eta'),$$
which contradicts that $(w'_a,\eta')$ is the optimal solution.  Thus we can conclude the IR constraint must bind.

\medskip

\noindent\textbf{Optimal Solution.}\\
Therefore, we have at the optimum,
\[
G_a(w_a) - E(\eta) = U_0.
\]
Then for any given $\eta$, by the monotonicity (and invertibility) of \(G_a\), we can solve for the corresponding \(w_a\) which satisfies the binding constraint:
\[
w_a = G_a^{-1}\Bigl(E(\eta) + U_0\Bigr).
\]

Substituting this expression into the principal's utility, we obtain
\[
U_p = -G_a^{-1}\Bigl(E(\eta) + U_0\Bigr) + \mu(\eta).
\]
Thus, the principal's problem reduces to choosing the annotation quality \(\eta \in [0, 1]\) to maximize
\[
\max_{\eta\in [0,1]} \Bigl\{-G_a^{-1}\Bigl(E(\eta) + U_0\Bigr) + \mu(\eta)\Bigr\}.
\]
Denote the maximizer by \(\eta^*\). Then, the corresponding optimal wage is
\[
w_a^* = G_a^{-1}\Bigl(E(\eta^*) +U_0\Bigr).
\]

\medskip
\noindent\textbf{Uniqueness.} \\
To show the uniqueness, by noting the strict monotonicity of $G^{-1}_a$, $w^*_a$ is unique if $\eta^*$ is unique. Thus it is sufficient to prove the uniqueness of $\eta^*$.

We first show $G^{-1}_a$ is convex. To see this, let \(y_1, y_2\) be in the range of \(G_a\) and define \(x_1=G_a^{-1}(y_1)\) and \(x_2=G_a^{-1}(y_2)\). For any \(\lambda\in[0,1]\), by the concavity of \(G_a\),
\[
G_a\bigl(\lambda x_1+(1-\lambda)x_2\bigr) \ge \lambda G_a(x_1) + (1-\lambda) G_a(x_2) = \lambda y_1 + (1-\lambda)y_2.
\]
Since \(G_a\) is strictly increasing, applying \(G_a^{-1}\) (which preserves order) yields
\[
\lambda x_1+(1-\lambda)x_2 \ge G_a^{-1}\Bigl(\lambda y_1 + (1-\lambda)y_2\Bigr),
\]
whish shows the convexity for \(G_a^{-1}\).

Thus, by the preservation of convexity in the composition and affine function (Section 3.2 in \cite{boyd2004convex}), since $E(\eta)$ is convex and non-decreasing, and $\mu$ is concave, we have 
$$-G_a^{-1}\Bigl(E(\eta) + U_0\Bigr) + \mu(\eta)$$
is a concave function.

Hence, if \eqref{eq:first_best} is feasible, then the first-best solution is unique to this concave optimization problem:
\[
\eta^* = \arg\max_{\eta \in [0, 1]} \Bigl\{-G_a^{-1}\Bigl(E(\eta) + U_0\Bigr) + \mu(\eta)\Bigr\},
\]
with the agent receiving the wage
\[
w_a^* = G_a^{-1}\Bigl(E(\eta^*) + U_0\Bigr).
\]
\end{proof}



\subsubsection{Discussions on Feasibility of Second-Best Problems}
For the unrestricted second-best problem \eqref{eq:second_best}, it is straightforward to see that at least one feasible solution exists by paying the agent enough salary. Consider those 
$F_n$'s defined on the space of measurable functions of $\mathcal{D}_n$ and using the $L_1$ norm, for example. Then, due to the continuity of $E$ and $\mu$, the feasible set must be closed. Furthermore, it is without loss of generality to assume that we are considering only a uniformly bounded subset of $F_n$; otherwise, the agent would quit (due to low salary) or the payment is unnecessarily high for the principal to afford. Then, the feasible set is compact. Since the target function is continuous, it must have a maximum on the compact feasible set.


The core argument is that any non-empty compact set must have a maximum for a continuous function. Thus, for the restricted second-best problem \eqref{eq:restricted_second_best}, the conclusion could also be derived in the same way. It is easy to check that all the other requirements (closed feasible set, continuous target function, and bounded feasible set) are met in \eqref{eq:restricted_second_best}. However, the key point is: is the feasible set of \eqref{eq:restricted_second_best} non-empty? We will prove the claim for the consistency/agreement-based tests based on $\bm{A} = (A_1, \dots, A_n)$ and the binary contract case $\mathcal{F}_n = \mathcal{F}_n^{\text{bin}}$. Such a case covers all the discussions in Theorem \ref{thm:binary}.

\begin{proposition}
\label{prop:feasible_binary}
Assume $n$ is large enough. For $\mathcal{F}_n = \mathcal{F}_n^{\text{bin}}$ and $\mathcal{F}_n^{\text{bin}}(\mathcal{D}_n) = \mathcal{F}_n^{\text{bin}}(\bm{A})$, the problem \eqref{eq:restricted_second_best} is feasible.
\end{proposition}
\begin{proof}
Recall that each consistency happens with probability $p(\eta) \coloneqq \frac{1+c\eta}{2}$. We consider a particular subclass of contracts based on
\[
\bar{A} \coloneqq \frac1n \sum_{i=1}^n A_i
\]
or equivalently,
\[
\hat{\eta} \coloneqq \frac{2\bar{A} - 1}{c}.
\]
The agent gets a higher wealth if $\bar{A}$ is no less than a threshold $\frac{k}{n}$ with monetary utility $S_1$ with probability $\mathbb{P}(\bar{A} \geq \frac{k}{n})$. 
Denotes the probability by $P(\eta, n, k)$. 
Otherwise, the agent gets a monetary utility of $S_0$. 



From Lemma \ref{lemma:binomial_properties} (f) and (g), we see that if we set $k = \frac{1+c \eta^*}{2} \cdot n - \Theta(1/\sqrt{n \log n})$ and the difference between two wages $S_1 - S_0$ moderately large (for example, larger than $E(1)$), then the induced $\eta_a$ must be the right intersection between $(S_1 - S_0) \cdot \frac{\partial }{\partial \eta} P(\eta, n, k)$ and $E(\eta)$; the intersection happens very close to $\eta^*$ by a distance of at most $\Theta(1/\sqrt{n \log n})$. Suppose we adjust $S_1 - S_0$ by multiplying a factor of $(1+\gamma)$. By the first-order condition, we have
\[
(1+\gamma) \cdot (S_1 - S_0) \cdot \frac{\partial }{\partial \eta} P(\eta, n, k)\bigg\vert_{\eta = \eta_a} - \frac{\mathrm{d} E}{\mathrm{d} \eta}\bigg \vert_{\eta = \eta_a} = 0.
\]
By the implicit function theorem, we can write $\eta_a$ as a function of $\gamma$ with
\[
(S_1 - S_0) \cdot \frac{\partial }{\partial \eta} P(\eta, n, k)\bigg\vert_{\eta = \eta_a} + (1+\gamma) \cdot (S_1 - S_0) \cdot \frac{\partial^2 }{\partial \eta^2} P(\eta, n, k)\bigg\vert_{\eta = \eta^*} \cdot \frac{\partial \eta_a}{\partial \gamma} - \frac{\mathrm{d}^2 E}{\mathrm{d} \eta^2}\bigg \vert_{\eta = \eta_a} \cdot \frac{\partial \eta_a}{\partial \gamma} = 0.
\]
By the fact that $P(\eta, n, k)$ is concave and monotonically increasing at $\eta_a$ (Lemma \ref{lemma:binomial_properties} (e) and (b)) and the assumption that $E$ is convex, we have that
\[
\frac{\partial \eta_a}{\partial \gamma} = \Omega\left(\frac{1}{1+\gamma}\right) > 0,
\]
where $\Omega(\cdot)$ hides the dependence on $n, P,$ and $E$. We can always find an $\gamma = \exp\left(-\mathcal{O}\left(\sqrt{n\log n}\right)\right) = o(1)$ such that the $\eta_a$ is adjusted to $\eta_a = \eta^*$.
\end{proof}



\subsubsection{Proof of Theorem \ref{thm:binary}}
\label{subsec:proof_of_binary}
We follow the convention in the principal-agent model that the gap between the first-best and the second-best solutions is due to the concavity of the agent's monetary utility. Such a concave function induces a Jensen's gap, which can be approximated by the variance when $n$ is large (see \citet{frick2023monitoring} for an example). Then, all the efforts could be reduced to estimating the payoff variance for large enough $n$'s. We summarize them into the following:\\
\textbf{Step 1. First-best/second-best gap is Jensen's gap; which approximately is the variance when $n$ is large enough.}\\
\textbf{Step 2. Estimating binary contracts' variance. The variance is approximately $\Theta(1/\sqrt{n \log n})$ when $n$ is large enough.}


During our discussions on the variance, the only required property of the yielding outputs $\eta_a(F_n)$'s is that
\begin{equation}
\exists \varepsilon, N > 0, \text{ s.t. } \forall n \geq N, \eta_a(F_n) \in [\varepsilon, 1-\varepsilon].
\label{eq:uniformly_bounded_away}
\end{equation}
In other words, the yielding outputs $\eta^*$'s are uniformly bounded away from 0 and 1 for large enough $n \geq N$. This requirement is to make sure that the agent's output at least induces some randomness in the outcome (a counter-example is that if $\eta_a = 1$ and the agreement probability $p(\eta) = \frac{1+c \eta}{2} = 1$ for some $c=1$, then the outcome is deterministic, and the variance of the outcome is zero). This property is satisfied by the restricted program \eqref{eq:restricted_second_best} by the constraint that $\eta_a = \eta^*$ and $\eta^* \in (0, 1)$. For the unrestricted second-best program \eqref{eq:second_best}, the property is also satisfied in the sense that the optimal solution will converge to $\eta^*$ (later proved in Proposition \ref{prop:second_converge_to_first}). 


Now, we start to derive Theorem \ref{thm:binary} for the restricted second-best \eqref{eq:restricted_second_best}'s gap from the first-best. The proof also holds for the unrestricted program \eqref{eq:second_best} and will be omitted for simplicity.
\begin{proof}
\textbf{Part I: Upper bound.}
According to our discussions in Proposition \ref{prop:first_best_sol}, there is only one constraint (IR) in the first-best problem \eqref{eq:first_best}, and the constraint is binding. The first-best solution is to pay the agent a fixed amount of $G_a^{-1}(E(\eta^*) + U_0)$. 


For the restricted second-best problem \eqref{eq:restricted_second_best}, the paid amount must obey the IR constraint as well, induces
\[
\mathbb{E}[G_a(F_n) - E(\eta^*)] \geq U_0.
\]
For the binary contract, suppose the agent's monetary utility is one of the following two levels: a higher level $S_1$ and a lower level $S_0$. We can always adjust two levels of payment $G_a^{-1}(S_0)$ and $G_a^{-1}(S_1)$ in the following way: we keep the agent's monetary utility gap $\Delta S = S_1 - S_0$ between those two levels and only adjust the lower level $S_0$ of the agent's monetary utility. Such adjustments do not change the utility-maximizing choice of the agent, which separates the IC constraint from the IR. Recall that we want to prove that the IR constraint is binding. By adjusting $S_0$, the expected monetary utility of the agent changes according to
\[
\frac{\partial \mathbb{E}[G_a(F_n)]}{\partial S_0} = -\mathbb{P}(\text{agent gets }G_a^{-1}(S_0)) - \mathbb{P}(\text{agent gets }G_a^{-1}(S_0 + \Delta S)) = -1.
\]
The principal's expected paid amount changes accordingly:
\[
\frac{\partial \mathbb{E}[F_n]}{\partial S_0} = -\mathbb{P}(\text{agent gets }G_a^{-1}(S_0)) \cdot \frac{\partial G_a^{-1}}{\partial S} \bigg\vert_{S=S_0} - \mathbb{P}(\text{agent gets }G_a^{-1}(S_0 + \Delta S)) \cdot \frac{\partial G_a^{-1}}{\partial S} \bigg\vert_{S=S_0+\Delta S} < 0.
\]
Thus, the principal's optimal choice must be to adjust the payment such that the IR constraint is binding: the principal pays the minimal amount such that the agent accepts the contract. Due to the continuity, the adjustment is always achievable. Hence, we can now restrict our attention to the binding IR constraint cases:
\[
\mathbb{E}[G_a(F_n) - E(\eta^*)] = U_0.
\]


The expected paid amount in the restricted second-best problem is now
\[
\mathbb{E}[F_n] = \mathbb{E}[G_a^{-1}(U_0 + E(\eta^*))],
\]
which is larger than the first-best problem's optimal payment
\[
G_a^{-1}(E(\eta^*) + U_0) = G_a^{-1}\left(\mathbb{E}[E(\eta^*) + U_0]\right).
\]
By a sharpened version of Jensen's inequality \citep{liao2019sharpening}, we have
\[
\inf_{s} \frac{\partial^2 G_a^{-1}(s)}{\partial s^2} \cdot \mathrm{Var}(F_n(\mathcal{D}_n)) \leq \mathbb{E}[G_a^{-1}(U_0 + E(\eta^*))] - G_a^{-1}(E(\eta^*) + U_0) \leq \sup_{s} \frac{\partial^2 G_a^{-1}(s)}{\partial s^2} \cdot \mathrm{Var}(F_n(\mathcal{D}_n)).
\]
Since the agent decision $\eta_a \in [0, 1]$, without loss of generality assume that the amount paid to the agent must be almost surely bounded:
\[
F_n(\mathcal{D}_n) \in [G_a^{-1}(U_0 + E(0)), G_a^{-1}(U_0 + E(1))] \eqqcolon \mathcal{S},\quad  \text{a.s.}
\]
Then $\inf_{s \in \mathcal{S}} \frac{\partial^2 G_a^{-1}(s)}{\partial s^2}$ and $\sup_{s \in \mathcal{S}} \frac{\partial^2 G_a^{-1}(s)}{\partial s^2}$ must be bounded from below and above by Assumption \ref{assm:basic_assm_1} and Assumption \ref{assm:basic_assm_3}. We now conclude that the Jensen gap between the first-best and the second-best solutions is of the order of $\mathrm{Var}(F_n(\mathcal{D}_n))$:
\begin{equation}
\mathbb{E}[G_a^{-1}(U_0 + E(\eta^*))] - G_a^{-1}(E(\eta^*) + U_0) = \Theta(\mathrm{Var}(F_n(\mathcal{D}_n))).
\label{eq:Jensen_gap_variance}
\end{equation}


The task now reduces to estimating the variance of the payment $\mathrm{Var}(F_n(\mathcal{D}_n))$. Again, for the binary contracts, we assume that the agent's monetary utility is of two levels: a higher level $S_1$ and a lower level $S_0$. If we set two levels of payments to be $G_a^{-1}(S_1) > G_a^{-1}(S_0)$, the variance is
\[
\mathrm{Var}(F_n(\mathcal{D}_n)) = [G_a^{-1}(S_1) - G_a^{-1}(S_0)]^2 \cdot \mathbb{P}(\text{agent gets } G_a^{-1}(S_1)) \cdot \mathbb{P}(\text{agent gets} G_a^{-1}(S_0)).
\]
From the discussions in Proposition \ref{prop:feasible_binary}, we can see that the $\eta^*$ can be achieved by setting the gap $G_a^{-1}(S_1) - G_a^{-1}(S_0) = \mathcal{O}(E(1))$, setting a test based on average agreements $\bar{A}$
\[
\bar{A} = \frac{1}{n} \sum_{i=1}^n A_i,
\]
and paying the agent a higher amount if $\bar{A} \geq \frac{1+c\eta^*}{2} \cdot n - \Theta(1/\sqrt{n \log n})$. Under this setting, we shall see from Lemma \ref{lemma:binomial_properties} (f) that
\[
\mathbb{P}(\text{agent gets }G_a^{-1}(S_1)) = 1 - \Theta\left(1/\sqrt{n \log n}\right), \quad \mathbb{P}(\text{agent gets } G_a^{-1}(S_0)) = \Theta\left(1/\sqrt{n \log n}\right).
\]
We reach the conclusion that there is a feasible solution to \eqref{eq:restricted_second_best} such that
\[
\mathrm{Var}(F_n(\mathcal{D}_n)) = \Theta\left(1/\sqrt{n \log n}\right),
\]
which proves the upper bound.


\textbf{Part II: Lower bound.}
The consistencies happen according to $n$ independent Bernoulli distribution; or equivalently, the total consistency follows a binomial distribution. Fix the number 
$n$. Since the likelihood of Bernoulli/binomial distributions purely depends on the consistency probability $p(\eta) = \frac{1+c\eta}{2}$ for some $c > 0$, we, without loss of generality, assume that we consider only the contracts based on the consistencies/agreements $\bm{A}$ (such a consideration can be justified by the Proposition \ref{prop:UMP}).


From the proof of the upper bound, we observe that if $F_n^{\text{bin}}(\bm{A})$ follows a threshold scheme such that the agent gets a higher payoff when $\bar{A}$ exceeds a certain threshold and gets a lower payoff otherwise, then the gap between the first-best and the second-best solutions is of $\Theta(1/\sqrt{n \log n})$. It suffices to show that such a threshold scheme is indeed optimal in the sense of minimizing the Jensen's gap/variance. We will prove the claim for (a slightly extended version of) binary contracts $\mathcal{F}_n^{\text{bin}}$.


To prove the claim, we first extend $\mathcal{F}_n^{\text{bin}}$ to be measurable functions of not only $\bm{A}$ but also an additional uniform random variable $U$ on $[0, 1]$, which introduces some extra randomness for fixed $\bm{A}$. More specifically, the original binary contract $F_n^{\text{bin}}$ pays the agent a higher wealth $G_a^{-1}(S_1)$ (so that the agent's monetary utility is $S_1$) if the total agreements $n \cdot \bar{A} \in \mathcal{K} \subset [n]$ and pays a lower amount $G_a^{-1}(S_0)$ (so that the agent's monetary utility is $S_0$) otherwise. We can write down as
\[
\mathbb{E}\left[F_n^{\text{bin}}(\bm{A})\right] = G_a^{-1}(S_0) + \left(G_a^{-1}(S_1) - G_a^{-1}(S_0)\right) \cdot \sum_{k \in \mathcal{K}} \mathbb{P}\left(n \cdot \bar{A} = k\right).
\]
We can view the payment process as: if $n \cdot \bar{A} \in \text{or }\notin \mathcal{K}$, pay a bonus with probability one or zero. If we "soften" the probability from one/zero to a real number $w_k \in [0, 1]$ (by using the additional uniform random variable $U$ and setting the pay probability to be $\mathbb{P}(U \leq w_k)$), we get a richer class of contracts, including
\[
\mathbb{E}\left[\tilde{F}_n^{\text{bin}}(\bm{A})\right] \coloneqq G_a^{-1}(S_0) + \left(G_a^{-1}(S_1) - G_a^{-1}(S_0)\right) \cdot \sum_{k \in [n]} \mathbb{P}\left(n \cdot \bar{A} = k\right) \cdot w_k.
\]
We denote the extended class by $\tilde{\mathcal{F}}_n^{\text{bin}}$. We denote $\mathbb{P}\left(n \cdot \bar{A} = k\middle| \eta^*\right)$ by $p_k$ and the amount $\sum_{k \in [n]} \mathbb{P}\left(n \cdot \bar{A} = k\middle| \eta^*\right) \cdot w_k$ by $P = \sum_{k \in [n]} w_k p_k$. Since more information is included, the principal's second-best solution could not be worse (because at least the principal can ignore the additional $U$ to keep the original contract). Thus, it suffices to show the optimality of the threshold scheme in the extended class to prove the lower bound.


Before we dive into the derivations, we present a property of the binomial distribution: for any $p \in (0,1 )$ and $X_n(p) \sim \mathrm{Binomial}(n,p)$,
\[
\frac{\partial }{\partial p}\mathbb{P}\left(X_n(p) = k\right) = \frac{k-np}{p(1-p)} \cdot \mathbb{P}\left(X_n(p) = k\right).
\]
Denote $(\frac{k}{n} - p(\eta^*))$ by $u_k$. Then $u_k$ is strictly increasing with respect to $k$, and
\begin{equation}
\begin{split}
\frac{\partial }{\partial \eta}\mathbb{P}\left(n \cdot \bar{A} = k\middle| \eta\right)\bigg\vert_{\eta = \eta^*}
&=\frac{\partial }{\partial \eta}\mathbb{P}(X_n(p(\eta)) = k)\bigg\vert_{\eta = \eta^*}\\
&= \frac{c}{2}\cdot \frac{k-np(\eta^*)}{p(\eta^*)(1-p(\eta^*))} \cdot \mathbb{P}(X_n(p(\eta^*)) = k)\\
&= \frac{cn}{2p(\eta^*)(1-p(\eta^*))} \cdot u_k \cdot \mathbb{P}\left(n \cdot \bar{A} = k\middle| \eta^*\right),
\end{split}
\label{eq:lower_bound_eq_1}
\end{equation}
which induces
\begin{equation}
\frac{\partial P}{\partial \eta}\bigg\vert_{\eta = \eta^*} = \frac{cn}{2p(\eta^*)(1-p(\eta^*))} \cdot \sum_{k \in [n]} w_k u_k p_k.
\label{eq:lower_bound_eq_2}
\end{equation}


From the discussions in the upper bound, we have the observation that: (1) Jensen's gap between the first-best and the second-best solutions is of the order of $\Theta(\mathrm{Var}(F_n(\mathcal{D}_n)))$; and (2) at least one feasible solution has the property that $\mathrm{Var}(F_n(\mathcal{D}_n)) \rightarrow 0$ as $n$ grows to infinity. Hence, for the optimal solution to \eqref{eq:restricted_second_best}, it must also have $\mathrm{Var}(F_n(\mathcal{D}_n)) \rightarrow 0$.
By the delta's method, we have that
\[
\mathrm{Var}(\tilde{F}_n) = (1+o(1)) \cdot \frac{\partial ^2 G_a^{-1}}{\partial s^2}\bigg\vert_{s=(\mathrm{E[U_0 + E(\eta^*)]})}\cdot \mathrm{Var}(G_a(\tilde{F}_n)),
\]
meaning that the gap can also be approximated by $\mathrm{Var}(G_a(\tilde{F}_n))$ or equivalently, $\mathrm{Var}(U_a)$. We have
\begin{equation}
\mathrm{Var}(U_a) = (S_1 - S_0)^2 P (1-P).
\label{eq:lower_bound_eq_3}
\end{equation}
By the first-order condition of the agent's IC constraint, the agent's marginal expected utility must be zero at $\eta = \eta^*$, which means that the agent's monetary utility gap between two wages $S_1 - S_0$ must satisfy
\begin{equation}
(S_1 - S_0) \cdot \frac{\partial P}{\partial \eta}\bigg\vert_{\eta = \eta^*} - \frac{\mathrm{d} E}{\mathrm{d} \eta}\bigg\vert_{\eta = \eta^*} = 0.
\label{eq:FOC_lower_bound}
\end{equation}
Substituting \eqref{eq:FOC_lower_bound} into \eqref{eq:lower_bound_eq_3}, we have
\begin{equation}
\mathrm{Var}(U_a) = \frac{P (1-P) \Big(\frac{\mathrm{d} E}{\mathrm{d} \eta}\big\vert_{\eta = \eta^*}\Big)^2}{\Big(\frac{\partial P}{\partial \eta}\big\vert_{\eta = \eta^*}\Big)^2}.
\label{eq:lower_bound_eq_4}
\end{equation}
If our target is to minimize the variance, it is equivalent to maximizing $\frac{\big(\frac{\partial P}{\partial \eta}\big\vert_{\eta = \eta^*}\big)^2}{P (1-P)}$, which means that our target is also equivalent to:
\begin{equation}
\max_{w_1,\dots,w_n} \ \frac{\left(\sum_{k \in [n]} w_k u_k p_k\right)^2}{\left(\sum_{k\in[n]} w_k p_k\right)\left(1 - \sum_{k\in[n]} w_k p_k\right)} \eqqcolon g(w_1,\dots,w_n).
\label{eq:lower_bound_target}
\end{equation}
Taking the derivatives with respect to each $w_k$, we have
\begin{equation}
\frac{\partial g}{\partial w_k} = \frac{\left(\sum_{j \in [n]} w_j u_j p_j\right) p_k \cdot\left[2u_k P(1-P) - \left(\sum_{j \in [n]} w_j u_j p_j\right)\cdot (1-2P)\right]}{P^2(1-P)^2}.
\label{eq:lower_bound_derivative}
\end{equation}
We can see that for any value of $\left(\sum_{k \in [n]} w_k u_k p_k\right)$ and $\left(\sum_{k \in [n]} w_k p_k\right)$, the sign of the derivative purely depends on the value of $u_k = \frac{k}{n} - p(\eta^*)$. Thus, there must exist some $k_0 \in [n]$, such that
\begin{equation}
\mathrm{sgn}\left(\frac{\partial g}{\partial w_k}\right) = \begin{cases}
-1, \quad \text{if }k < k_0,\\
+1, \quad \text{if } k > k_0,
\end{cases}
\end{equation}
which implies that any maximum must be taken at
\[
w_k = 
\begin{cases}
0, \quad \text{if }k < k_0,\\
1, \quad \text{if }k > k_0.
\end{cases}
\]
We have proved that any optimal contract in the extended case must be of the threshold form (except for one single $k_0$). Since $n$ is large enough and $p(\eta^*)$ is uniformly bounded away from 0 and 1, any single term $p_{k_0}$ does not have much influence over the entire contract; adding one term or removing one term does not change the structure of the problem. Therefore, it does not hurt to consider only those threshold contracts by letting $w_{k_0} = 0$ or $w_{k_0} = 1$. From Lemma \ref{lemma:binomial_properties}, we can see that the first-order condition is met at two points: one $p_1$ on the left side of $\frac{k_0}{n}$, and one $p_2$ on the right side of $\frac{k_0}{n}$. But the left one $p_1$ is not a local maximum of the expected utility with respect to the agent's IC constraint. In conclusion, the only possible way is to set $k_0$ such that $p_2 = p^*$. In that case,
\[
\mathrm{Var}(U_a) = \Theta\left(1/\sqrt{n \log n}\right).
\]
\end{proof}


We now give a proof to show that the optimal solution to \eqref{eq:second_best} must be close to that of \eqref{eq:restricted_second_best} when $n$ is large enough.
\begin{proposition}
\label{prop:second_converge_to_first}
Denote the optimal solution to \eqref{eq:second_best} by $\eta_n^*$. Then, under the condition of Theorem \ref{thm:binary}, we have that $\eta_n^*$ converging to $\eta^*$ with
\[
\left(\eta_n^* - \eta^*\right)^2 = \mathcal{O}\left(1/\sqrt{n \log n}\right).
\]
\end{proposition}
\begin{proof}[Proof of Proposition \ref{prop:second_converge_to_first}]
For any $\eta \in [0, 1]$, define two optimization problems in analogy to the first-best/second-best problems:
\begin{equation}
\begin{split}
\mathrm{F}(\eta) \coloneqq \max_{F_n} \ &\mathbb{E}[U_p(F_n(\mathcal{D}_n), \eta)]\\
\text{s.t. }& \mathbb{E}[U_a(F_n(\mathcal{D}_n), \eta)]\geq U_0. \quad \text{(IR)}
\end{split}
\label{eq:first_best_auxilliary}
\end{equation}
\begin{equation}
\begin{split}
\mathrm{S}(\eta) \coloneqq \max_{F_n^{\text{bin}}} \ &\mathbb{E}[U_p(F_n^{\text{bin}}(\mathcal{D}_n), \eta)],\\
\text{s.t. }& \mathbb{E}[U_a(F_n^{\text{bin}}(\mathcal{D}_n), \eta)]\geq U_0, \quad \text{(IR)}\\
& \mathbb{E}[U_a(F_n^{\text{bin}}(\mathcal{D}_n), \eta)]\geq \sup_{\eta^\prime} \mathbb{E}[U_a(F_n^{\text{bin}}(\mathcal{D}_n), \eta^\prime)]. \quad \text{(IC)}
\end{split}
\label{eq:second_best_auxilliary}
\end{equation}
Note that the maximum to the first-best problem \eqref{eq:first_best} equals $\mathrm{S}(\eta^*)$. Also, the maximum to the second-best problem \eqref{eq:second_best} equals $\mathrm{F}(\eta_n^*)$ and the maximum to the restricted second-best problem \eqref{eq:restricted_second_best} equals $\mathrm{F}(\eta^*)$. We have
\begin{align*}
\mathrm{F}(\eta_n^*) & \geq \mathrm{S}(\eta_n^*)\\
& \geq \mathrm{S}(\eta^*)\\
& \geq \mathrm{F}(\eta^*) - \Theta\left(1/\sqrt{n \log n}\right).
\end{align*}
Here, the first inequality is due to \eqref{eq:first_best_auxilliary}'s removing the IC constraint and enlarging the contract class could only induce an optimum no smaller than \eqref{eq:second_best_auxilliary}, the second inequality is due to the optimality of $\eta_n^*$ subject to the original second-best problem \eqref{eq:second_best}, and the last from the proof of Theorem \ref{thm:binary} for the restricted second-best problem.


Now, as we have already proved the optimal contract form for the first-best problem must be a constant wage that exactly compensates the efforts made by the agent $E(\eta)$ plus the leisure utility $U_0$ (see that in the proof of Proposition \ref{prop:first_best_sol}), we have
\[
\mu(\eta_n^*) - G_a^{-1}(U_0 + E(\eta_n^*)) \geq \mu(\eta^*) - G_a^{-1}(U_0 + E(\eta^*)) - \Theta\left(1/\sqrt{n \log n}\right).
\]
From the assumption made in Theorem \ref{thm:binary}, the function $G_a^{-1}(U_0 + E(\eta))$ must be strictly convex for $\eta$. Also, $\mu(\eta)$ is concave. Thus, the function $\mu(\eta) - G_a^{-1}(U_0 + E(\eta))$ must be strictly concave. From Taylor's expansion at $\eta=\eta^*$, we have
\[
\left(\eta_n^* - \eta^*\right)^2 = \mathcal{O}\left(1/\sqrt{n \log n}\right).
\]
\end{proof}

\subsubsection{Proof of Theorem \ref{thm:linear_contract}}
\label{apd:proof_of_linear}
Similar to the binary contract case, the only difference between the proof for the restricted second-best \eqref{eq:restricted_second_best} and the unrestricted \eqref{eq:second_best} in our proof is that for the restricted case, we directly assume $\eta^* \in (0, 1)$ such that \eqref{eq:uniformly_bounded_away} is satisfied (so there is at least some randomness in the outcome), while for the unrestricted case, we need to prove that $\eta_n^*$ converges to $\eta^*$ such that for large enough $n$'s, the decisions are also uniformly bounded away from 0 and 1 (proved later in Proposition \ref{prop:second_converge_to_first_linear}). Thus, we only prove Theorem \ref{thm:linear_contract} for the restricted case and do not repeat for the unrestricted case. 
\begin{proof}
Suppose we are considering some contract $F_n^{\text{lin}}(\bm{A}) = \frac{1}{n}\sum_{i=1}^n f_n(A_i)$, where the agent gets a lower payment $f_n(0)$ per $A_i = 0$ and a higher payment $f_n(1)$ per $A_i = 1$. From previous discussions in the proof of Theorem \ref{thm:binary}, we know that the gap between the first-best and the second-best is the Jensen's gap due to the risk-averse nature of the agent. This gap is of the same order as $\mathrm{Var}(F_n(\mathcal{D}_n))$, where
\begin{align*}
\mathrm{Var}(F_n(\mathcal{D}_n)) & = \mathrm{Var}\left(\frac1n \sum_{i=1}^n f_n(A_i)\right) \\
& = \frac{1}{n} \mathrm{Var}(f_n(A_1)).
\end{align*}
It suffices to show that $\mathrm{Var}(f_n(A_1)) = \Theta(1)$. 


\textbf{Upper bound.}
By Assumption \ref{assm:basic_assm_3}, the wage is uniformly bounded, thus
\[
\mathrm{Var}(f_n(A_1)) = \mathcal{O}(1).
\]


\textbf{Lower bound.}
If $\mathrm{Var}(f_n(A_1)) = \Omega(1)$ does not hold, then we must have $\mathrm{Var}(f_n(A_1)) \rightarrow 0$ as $n \rightarrow \infty$. Since $p(\eta^*) \in (0, 1)$ (due to $\eta^* \in (0, 1)$), we have $\mathrm{Var}(A_1) = \Theta(1)$. It implies that
\[
f_n(1) - f_n(0) \rightarrow 0, \quad \text{as }n \rightarrow \infty.
\]
Then the agent's utility at $\eta = \eta^*$ satisfies
\begin{align*}
\mathbb{E}[U_a(F_n^{\text{lin}}(\mathcal{D}_n), \eta^*)] & = \mathbb{E}\left[G_a\left(\frac1n \sum_{i=1}^n f_n(A_i)\right)\middle| \eta = \eta^*\right] - E(\eta^*) \\
& < G_a\left(\mathbb{E}[f_n(A_1)|\eta = \eta^*]\right) - E(\eta^*) \quad \rightarrow G_a(f_n(0)) - E(\eta^*).
\end{align*}
Similarly, the agent's utility at $\eta = 0$ also has the property that
\begin{align*}
\mathbb{E}[U_a(F_n^{\text{lin}}(\mathcal{D}_n), 0)] & = \mathbb{E}\left[G_a\left(\frac1n \sum_{i=1}^n f_n(A_i)\right)\middle| \eta = 0\right] - E(0) \\
& < G_a\left(\mathbb{E}[f_n(A_1)|\eta = 0]\right) - E(0) \quad \rightarrow G_a(f_n(0)) - E(0).
\end{align*}
However, the fact that $\eta^* > 0$ and $E$ is strictly monotonically increasing due to Assumption \ref{assm:basic_assm_4} and Assumption \ref{assm:basic_assm_1} implies that
\[
\mathbb{E}[U_a(F_n^{\text{lin}}(\mathcal{D}_n), \eta^*)] < \mathbb{E}[U_a(F_n^{\text{lin}}(\mathcal{D}_n), 0)], \quad \text{for large enough }n.
\]
This contradicts the IC constraint that $\eta^*$ maximizes the agent's utility.
\end{proof}


The convergence of $\eta_n^*$ could also be proved once we have proved the gap between the first-best and the restricted second-best solutions, which we present without proof for simplicity. The proof is almost the same as that in Proposition \ref{prop:second_converge_to_first} except for the convergence rate.
\begin{proposition}
\label{prop:second_converge_to_first_linear}
Denote the optimal solution to \eqref{eq:second_best} by $\eta_n^*$. Then, under the condition of Theorem \ref{thm:linear_contract}, we have that $\eta_n^*$ converging to $\eta^*$ with
\[
\left(\eta_n^* - \eta^*\right)^2 = \mathcal{O}\left(1/n\right).
\]
\end{proposition}

\subsubsection{Technical Lemmas}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
\includegraphics[width=0.7\textwidth]
{fig/plot.001.png}
    \caption{Illustration for~Lemma \ref{lemma:binomial_properties} (e) (f). The blue-colored curve shows the ``bell curve'' nature of $\frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k)$. The curve's peak value is of $\Theta(\sqrt{n})$. Two intersections with some $\Theta(1)$-Lipschitz function $f$ take place: one on the left, one on the right. Both are at a distance of $\Theta(\sqrt{\log n/n})$ to the center.}
  \end{subfigure}
    \hfill
    \vspace{0.5cm}
  \begin{subfigure}[b]{\textwidth}
    \centering
\includegraphics[width=0.7\textwidth]
{fig/plot.002.png}
    \caption{Illustration for~Lemma \ref{lemma:binomial_properties} (g). Both the red-colored and the blue-colored bars are probability mass for different $\mathbb{P}(X_n(p_2) = m)$'s. By the first-order condition, the probability at the threshold $\mathbb{P}(X_n(p_2) = k)$ is of $\Theta(1/n)$. The tail probability (the red part) can be then estimated by the sum of geometric sequences, which is of $\Theta(1/\sqrt{n\log n})$.}
  \end{subfigure}
  \caption{Illustration for~Lemma \ref{lemma:binomial_properties}.}
\label{fig:tech_lemma_illu}
\end{figure}


\begin{lemma}
\label{lemma:binomial_properties}
Suppose $X_n(p) \sim \mathrm{Binomial}(n, p)$. Then for any integer $k$ such that $1 < k < n$,
\begin{enumerate}
\item \[
\mathbb{P}(X_n(p) \geq k) = n\int_0^p \mathbb{P}(X_{n-1}(u) = k - 1) \mathrm{d} u
\]
\item \[
\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k) = n \mathbb{P}(X_{n-1}(p) = k - 1).
\]
\item \[
\frac{\partial^2}{\partial p^2} \mathbb{P}(X_n(p) \geq k) = \frac{n-1}{(1-p) p} \cdot (\tilde{p} - p) \cdot \frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k),
\]
where $\tilde{p} = \frac{k-1}{n-1}$.
\item If $a \leq (1-p) p \leq b$, then \[
\exp\left(-\frac{n-1}{a} (p - \tilde{p})^2\right) \leq \frac{\frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k)}{\frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k) \vert_{p=\tilde{p}}} \leq \exp\left(-\frac{n-1}{b} (p - \tilde{p})^2\right),
\]
where $\tilde{p} = \frac{k-1}{n-1}$.
In other words, the curve of $\frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k)$ is like a bell curve centered at $\tilde{p}$.
\item $\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k)$ monotonically increases for $p < \tilde{p}$ and monotonically decreases for $p > \tilde{p}$. If $k = cn + \mathcal{O}(1)$ for some $c \in (0, 1)$, then $\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k)$ reaches its maximum of order $\Theta(\sqrt{n})$ at $\tilde{p} = \frac{k-1}{n-1}$.
\item For any $\mathcal{O}(1)$-Lipschitz function $f$ independent of $n$ on $[0, 1]$ with $f(p) > 0$, for large enough $n$ and $k = cn + \mathcal{O}(1)$ for some $c \in (0, 1)$, $f$ intersects with $\frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k)$ at two points $p_1$ and $p_2$, with
\[
p_1 = \tilde{p} - \Theta\left(\sqrt{\frac{(1-\tilde{p})\tilde{p}}{n}} \cdot \sqrt{\log n}\right),
\]
and
\[
p_2 = \tilde{p} + \Theta\left(\sqrt{\frac{(1-\tilde{p})\tilde{p}}{n}} \cdot \sqrt{\log n}\right).
\]
\item \[
\mathbb{P}(X_n(p_1) \geq k) = \Theta\left(\frac{1}{\sqrt{n \log n}}\right),
\]
and
\[
\mathbb{P}(X_n(p_2) \geq k) = 1 - \Theta\left(\frac{1}{\sqrt{n \log n}}\right).
\]
\end{enumerate}
\end{lemma}
\begin{proof}
(a) This could be done via iterative integration by parts. For the completeness, we present the process here:
\begin{align*}
\int_0^p \mathbb{P}(X_{n-1}(u) = k - 1) \mathrm{d} u & = \int_0^p \frac{(n-1)!}{(k-1)!(n-k)!} \cdot u^{k-1}(1-u)^{n-k} \mathrm{d} u\\
& = \frac{(n-1)!}{k!(n-k)!} \cdot p^{k}(1-p)^{n-k} + \int_0^p \frac{(n-1)!}{(k)!(n-k-1)!} \cdot u^{k}(1-u)^{n-k-1} \mathrm{d} u\\
& = \frac{(n-1)!}{k!(n-k)!} \cdot p^{k}(1-p)^{n-k} + \frac{(n-1)!}{(k+1)!(n-k-1)!} \cdot p^{k+1}(1-p)^{n-k-1} \\
& \phantom{=} + \int_0^p \frac{(n-1)!}{(k+1)!(n-k-2)!} \cdot u^{k+1}(1-u)^{n-k-2} \mathrm{d} u \\
& = \cdots\\
& = \frac{1}{n}\sum_{j=k}^{n} \mathbb{P}(X_n(p) = j).
\end{align*}


(b) Taking the derivative with respect to $p$ on both sides of (a) yields the result.


(c) Taking the derivative with respect to $p$ on both sides of (b), we have
\begin{align*}
\frac{\partial^2}{\partial p^2} \mathbb{P}(X_n(p) \geq k) &= n \cdot \frac{(n-1)!}{(k-1)!(n-k)!} \cdot \frac{\partial}{\partial p} (p^{k-1} (1-p)^{n-k})\\
& = n \cdot \frac{(n-1)!}{(k-1)!(n-k)!} \cdot \left[(k-1)(1-p) - (n-k)p\right] \cdot p^{k-2}(1-p)^{n-k-1} \\
& = n \cdot \frac{(n-1)!}{(k-1)!(n-k)!} \cdot \left[(k-1) - (n-1)p\right] \cdot p^{k-2}(1-p)^{n-k-1} \\
& = n \cdot \frac{(k-1) - (n-1)p}{p(1-p)} \cdot \mathbb{P}(X_{n-1}(p) = k-1)\\
& = \frac{(k-1) - (n-1)p}{p(1-p)} \cdot \frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k).
\end{align*}


(d) By the assumption that $a \leq (1-p)p$ and (c), for $p \geq \tilde{p}$ we have
\[
\frac{\partial^2}{\partial p^2} \mathbb{P}(X_n(p) \geq k) \geq \frac{n-1}{a} \cdot (\tilde{p} - p) \cdot \frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k).
\]
Multiplying both sides with $\exp(\frac{n-1}{a}(p - \tilde{p})^2)$, we have
\[
\frac{\partial }{\partial p} \left[\exp\left(\frac{n-1}{a}(p - \tilde{p})^2\right) \cdot \frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k)\right] \geq 0.
\]
Thus,
\[
\exp\left(\frac{n-1}{a}(p - \tilde{p})^2\right) \cdot \frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k) \geq \frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k) \bigg\vert_{p=\tilde{p}}.
\]
For the part $p \leq \tilde{p}$, the conclusion could be derived similarly. As for the second inequality, it follows from the assumption $(1-p)p \leq b$ and a similar argument.


(e) The monotonicity follows from (c). If $\frac{k}{n} = c + \mathcal{O}(\frac{1}{n})$ for some $c \in (0, 1)$, then the maximum can be computed as
\begin{align*}
\frac{\partial }{\partial p} \mathbb{P}(X_n(p) \geq k)\bigg\vert_{p = \tilde{p}} & = n \cdot \frac{(n-1)!}{(k-1)!(n-k)!} \cdot \tilde{p}^{k-1} (1-\tilde{p})^{n-k} \\
& = n \cdot \frac{(n-1)!}{(n-1)^{n-1}} \cdot \frac{(k-1)^{k-1}}{(k-1)!} \cdot \frac{(n-k)^{n-k}}{(n-k)!}\\
& = (1+o(1)) \cdot n \cdot \sqrt{2\pi n} \cdot \frac{1}{\sqrt{2\pi c n}} \cdot \frac{1}{\sqrt{2\pi(1-c)n}} \quad \text{by Stirling's approximation}\\
& = (1+o(1)) \cdot \frac{\sqrt{n}}{\sqrt{2\pi c(1-c)}}.
\end{align*}


(f) From the fact that the peak value of $\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k)$ is of order $\Theta(\sqrt{n})$ and (d)'s bell curve approximation, we have the following fact:\\
If $\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k)$ is of $\Theta(1)$ value, then $|p - \tilde{p}|$ must be of order $\Theta(\frac{\sqrt{(1-\tilde{p}){\tilde{p}} \log n}}{\sqrt{n}})$. In other words, if $\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k)$ intersects with a Lipschitz function $f$ independent of $n$, then the intersection must happens at a distance of $\Theta(\frac{\sqrt{(1-\tilde{p}){\tilde{p}} \log n}}{\sqrt{n}})$ from $\tilde{p}$.


From part (c), we see that the derivative of $\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k)$ (that is, the second-order derivative $\frac{\partial^2}{\partial p^2} \mathbb{P}(X_n(p) \geq k)$) must be of order $\Theta(\sqrt{n \log n})$, either positive if $p < \tilde{p}$ or negative if $p > \tilde{p}$. Since the derivative of $f$ is at most $\mathcal{O}(1)$, the intersection must happen at most twice (once on the left side of $\tilde{p}$ and once on the right).


We now prove that the intersection must happen at least twice. First, observe that
\[
\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k) \bigg\vert_{p = 0} = 0 < f(0),
\]
and
\[
\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k) \bigg\vert_{p = 1} = 0 < f(1).
\]
For large enough $n$, 
\[
\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k) \bigg\vert_{p = \tilde{p}} = \Theta(\sqrt{n}) > f(\tilde{p}).
\]
By the continuity of $\frac{\partial}{\partial p} \mathbb{P}(X_n(p) \geq k)$ and $f$, we must have at least two intersections.


(g) First, observe that
\begin{align*}
\mathbb{P}(X_n(p_1) = k) & = p_1 \cdot \mathbb{P}(X_{n-1}(p_1) = k-1) + (1-p_1) \cdot \mathbb{P}(X_{n-1}(p_1) = k)\\
& = p_1 \cdot \mathbb{P}(X_{n-1}(p_1) = k-1) + (1-p_1) \cdot \frac{p_1}{1-p_1} \cdot \frac{n-k}{k} \cdot \mathbb{P}(X_{n-1}(p_1) = k-1)\\
& = \frac{p_1 n}{k} \cdot \mathbb{P}(X_{n-1}(p_1) = k-1)\\
& = \frac{p_1}{k} \cdot \frac{\partial }{\partial p}\mathbb{P}(X_n(p_1) \geq k)\\
& = \frac{p_1}{k} \cdot f(p_1) = \Theta\left(\frac{1}{n}\right).
\end{align*}
For $k \leq j \leq k + \lceil C\sqrt{n \log n}\rceil$, we have
\[
\frac{\mathbb{P}(X_n(p_1) = j+1)}{\mathbb{P}(X_n(p_1) = j)} = \frac{p_1}{1-p_1} \cdot \frac{n-j}{j+1} = 1 - \Theta\left(\sqrt{\frac{\log n}{n}}\right).
\]
Hence,
\begin{align*}
\sum_{j=k}^{n} \mathbb{P}(X_n(p_1) = j) & \geq \sum_{j=k}^{k + \lceil C\sqrt{n \log n}\rceil} \mathbb{P}(X_n(p_1) = j)\\
& \geq \frac{\mathbb{P}(X_n(p_1) = k) \cdot \left(1-\left(\sup_{k\leq j\leq k + \lceil C\sqrt{n \log n}\rceil - 1} \frac{\mathbb{P}(X_n(p_1) = j+1)}{\mathbb{P}(X_n(p_1) = j)}\right)^{\lceil C\sqrt{n \log n}\rceil + 1}\right)}{1-\inf_{k\leq j\leq k + \lceil C\sqrt{n \log n}\rceil - 1} \frac{\mathbb{P}(X_n(p_1) = j+1)}{\mathbb{P}(X_n(p_1) = j)}}\\
& = \frac{\mathbb{P}(X_n(p_1) = k) \cdot (1-\exp(-\Theta(1)))}{\Theta\left(\sqrt{\frac{\log n}{n}}\right)}\\
& = \Theta\left(\frac{1}{\sqrt{n \log n}}\right).
\end{align*}
For the upper bound, it is easy to see that
\begin{align*}
\sum_{j=k}^{n} \mathbb{P}(X_n(p_1) = j) 
& \leq \frac{\mathbb{P}(X_n(p_1) = k)}{1-\sup_{k\leq j\leq n -1} \frac{\mathbb{P}(X_n(p_1) = j+1)}{\mathbb{P}(X_n(p_1) = j)}}\\
& = \Theta\left(\frac{1}{\sqrt{n \log n}}\right).
\end{align*}
The conclusion can also be similarly derived for $p_2$, which we omit for simplicity.
\end{proof}


\section{Appendix for Numerical Experiments}




\subsection{Experiments Setup and Additional Results}
\subsubsection{Dataset and Preference Probability}
\label{appx: dataset}
Throughout this work, we use four preference datasets: PKU-SafeRLHF (PKU) \citep{ji2024pku}, HelpSteer \citep{wang2023helpsteer}, UltraFeedback (Ultra) \citep{cui2023ultrafeedback}, and Skywork-Reward-Preference-80K-v0.2 (Skywork) \citep{liu2024skywork}:

\begin{itemize}
\item \textbf{PKU:} \textit{PKU-SafeRLHF} consists of 83K preference entries annotated along two dimensions: harmlessness and helpfulness. In our experiments, we use the preference model \textit{beaver-7b-v1.0-reward} \citep{dai2024safe} to annotate data pairs and compute the preference probability.

\item \textbf{HelpSteer:} \textit{HelpSteer} contains 37K samples. Each response is annotated by humans on five attributes ranging from 0 to 4, with higher scores indicating better performance. We use the average score across these five attributes as the chosen score to compute the preference probability.

\item \textbf{Ultra:} \textit{UltraFeedback} is annotated by GPT-4 using a fine-grained instruction covering four aspects: instruction-following, truthfulness, honesty, and helpfulness. GPT-4 provides a chosen score for each response, which we directly use to compute the preference probability.
\item \textbf{Skywork:} \textit{Skywork-Reward-Preference-80K-v0.2} is a curated subset of publicly available preference data and contains 77K samples. In our experiments, we use the preference model \textit{Skywork-Reward-Gemma-2-27B-v0.2} \citep{liu2024skywork} to annotate data pairs and derive the preference probability.
\end{itemize}

\noindent \textbf{Preference Model Calibration} To address the potential overconfidence of the predicted preference probabilities from open-source preference models \citep{guo2017calibration}, we calibrate these models using \textit{Histogram Binning} \citep{zadrozny2001obtaining} with half of the dataset. Specifically, we create 30 bins and define their boundaries such that each bin contains an equal number of samples. We then set the calibrated preference probability for each bin to the average proportion of positive samples (i.e., $y_1$ is the preferred response) within that bin. Figure \ref{fig:appx_cal} compares (out-of-sample) performance before and after calibration for the PKU and Skywork datasets. For more details on calibration methods, we refer readers to the survey \cite{silva2023classifier}.






\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.46\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKU_calibration-eps-converted-to.pdf}
    \caption{PKU}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.46\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/sky_calibration-eps-converted-to.pdf}
    \caption{Skywork}
  \end{subfigure}
  \caption{\small Calibration for two datasets.  
(Top row) Empirical preference probability $p(x,y_1,y_2)$ vs. the predicted probability before and after calibration. The dashed line ($x=y$) represents perfect alignment between predictions and empirical observations.  
(Bottom row) Histogram of the (predicted) preference probability $p(x,y_1,y_2)$ before and after calibration. We can see the calibration procedure improves alignment between the predicted probabilities and the empirical observations for both datasets. }
\label{fig:appx_cal}
\end{figure}
\subsubsection{Setup and more experiments for Figure \ref{fig:main_hist_LB}}
\label{appx:fig_hist_LB}

\textbf{Setup:} We use half of each (calibrated) dataset (specifically, the part not used during the calibration step or randomly sampled data if there is no calibration)  to plot the histogram of the probability 
\[
\mathbb{P}\bigl(y_{\text{chosen}} \succ y_{\text{rejected}} \mid x\bigr),
\]
which is equivalent to 
\[
\mathbb{P}\bigl(y_{1} \succ y_{2} \mid x\bigr)
\]
if we always order the pair so that $\mathbb{P}\bigl(y_{1} \succ y_{2} \mid x\bigr) \ge 1/2$. In addition, we plot the lower bound on the sum of the two types of errors, as stated in Proposition~\ref{prop:info_lower_bound}, for various choices of $\eta_0$, $\eta_1$, and $n$.\\

\noindent \textbf{More results:} Figure~\ref{fig:appx_sup_1} complements Figure~\ref{fig:main_hist_LB} by further computing the lower bound from Proposition~\ref{prop:info_lower_bound} for $\eta_1 \in \{0.98,\, 0.96\}$. The results show that a lower $\eta_1$ yields a higher lower bound. Intuitively, a lower $\eta_1$ (or a higher $\eta_0$) causes the induced distributions on $\mathcal{D}_{\text{test}}$ corresponding to $\eta_1$ and $\eta_0$ to be more similar, thereby complicating the hypothesis testing.



\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKUeta_2_0.02_mode_1__LB-eps-converted-to.pdf}
    \caption{PKU, $\eta_1=0.98$}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Helpsteereta_2_0.02_mode_1__LB-eps-converted-to.pdf}
    \caption{Helpsteer, $\eta_1=0.98$}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Ultraeta_2_0.02_mode_1__LB-eps-converted-to.pdf}
    \caption{Ultra, $\eta_1=0.98$}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/skyeta_2_0.02_mode_1__LB-eps-converted-to.pdf}
    \caption{Skywork, $\eta_1=0.98$}
  \end{subfigure}

  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKUeta_2_0.04_mode_1__LB-eps-converted-to.pdf}
    \caption{PKU, $\eta_1=0.96$}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Helpsteereta_2_0.04_mode_1__LB-eps-converted-to.pdf}
    \caption{Helpsteer, $\eta_1=0.96$}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Ultraeta_2_0.04_mode_1__LB-eps-converted-to.pdf}
    \caption{Ultra, $\eta_1=0.96$}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/skyeta_2_0.04_mode_1__LB-eps-converted-to.pdf}
    \caption{Skywork, $\eta_1=0.96$}
  \end{subfigure}
  \caption{\small Additional results for Figure \ref{fig:main_hist_LB}: Lower bound of the sum of two types of errors from Proposition \ref{prop:info_lower_bound} vs. the number of tested annotations $n$ at different  $\eta_0$ with $\eta_1\in \{0.98, 0.96\}$.}
\label{fig:appx_sup_1}
\end{figure}

\subsubsection{Setup and more experiments for Figure \ref{fig:main_self_UB}}
\label{appx:fig_self_UB}
\textbf{Setup:} The lower bound on the two types of errors for the expert-based model is derived in the same manner as in Figure~\ref{fig:main_hist_LB}. For the upper bound of the self-consistency monitoring, we simulate the two types of errors using a likelihood-ratio test with a threshold of 1. Specifically, the test compares  $\mathbb{P}(\bm{A}\mid\delta_0)$ and $\mathbb{P}(\bm{A}\mid\delta_1)$.
If $
\mathbb{P}(\bm{A}\mid\delta_0) \ge \mathbb{P}(\bm{A}\mid\delta_1),$
we decide that $\bm{A}$ is generated from $H_0$, i.e., $\eta\leq \eta_0$
otherwise, we decide that it is generated from $H_1$, i.e., $\eta\geq \eta_1$.

To compute the two types of errors in our experiments, given $n$, $\eta_0$, $\eta_1$, and $\delta$, we perform $M = 10000$ trials as follows. In each trial, we sample $
\bm{A}^0 = \bigl(A^{0}_1, \ldots, A^{0}_n\bigr)$ with each $A_i^0$
i.i.d.\ from a Bernoulli distribution with mean $\eta_0(1-\delta)/2 + 1/2$. We then compute the frequency (over $M$ trials) with which 
$\mathbb{P}(\bm{A}^0\mid\delta_0) < \mathbb{P}(\bm{A}^0\mid\delta_1)$, and designate this frequency as the (simulated) Type-I error. Similarly, we perform another $M = 10000$ trials, where in each trial we sample $\bm{A}^1 = \bigl(A^{1}_1, \ldots, A^{1}_n\bigr)$ with $A^1_i$ i.i.d.\ from a Bernoulli distribution with mean $\eta_1(1-\delta)/2 + 1/2$. We then compute the frequency with which $
\mathbb{P}(\bm{A}^1\mid\delta_0) \ge \mathbb{P}(\bm{A}^1\mid\delta_1)$,
and designate this frequency as the (simulated) Type-II error. The sum of these two error frequencies yields the upper bound on the overall error for the self-consistency monitoring approach.

\textbf{More results:} Figure~\ref{fig:appx_self_UB} complements Figure~\ref{fig:main_self_UB} by further comparing self-consistency monitoring (upper bound) with expert-based monitoring (lower bound) at $\eta_1 = 0.95$. The results indicate that self-consistency monitoring continues to outperform expert-based monitoring over a wide range of $n$, underscoring its potential advantages in various settings.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_PKUeta0.8_thr1.0_eta20.95__LB-eps-converted-to.pdf}
    \caption{PKU, $\eta_0=0.8$}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_PKUeta0.9_thr1.0_eta20.95__LB-eps-converted-to.pdf}
    \caption{PKU, $\eta_0=0.9$}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_Helpsteereta0.8_thr1.0_eta20.95__LB-eps-converted-to.pdf}
    \caption{Helpsteer, $\eta_0=0.8$}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/LRT_Helpsteereta0.9_thr1.0_eta20.95__LB-eps-converted-to.pdf}
    \caption{Helpsteer, $\eta_0=0.9$}
  \end{subfigure}
  \caption{\small Additional results for Figure \ref{fig:main_self_UB}. Self-consistency monitoring (upper bound) vs. expert-based monitoring (lower bound). For the sum of two types of errors, we plot the \textbf{upper bound} of the self-monitoring with different $\delta$ (blue, thick line) and the \textbf{lower bound} of the expert-based monitoring (red, dashed line), at $\eta_0\in\{0.8,0.9\}$ and $\eta_1=0.95$ for two datasets.}
\label{fig:appx_self_UB}
\end{figure}
\begin{comment}
    \subsubsection{Experiments for Proposition  \ref{prop:Fano_lower_bound}}
\label{appx:fig_for_fano_lower}
Figure~\ref{fig:appx_Fano_main} displays the lower bound of 
\[
\mathbb{P}\Bigl(\|\hat{\eta}(\bm{A})-\eta\|_2 \ge \epsilon\Bigr)
\]
as stated in Proposition~\ref{prop:Fano_lower_bound}. Here, without loss of generality, we assume that 
$p(x,y_1,y_2) \geq 1/2$, and the lower bound is numerically computed as shown in Appendix \ref{appx:fano_lower_proof}.

We consider three types of expert behavior/preference:

\begin{itemize}
    \item \textbf{Constant behavior:} 
    \[
    p_e(x,y_1,y_2) \equiv p_e,\quad \text{with } p_e\in\{0.8,1\},
    \]
    as shown in the top row. Note that by Proposition~\ref{prop:mutual_info}, setting $p_e=1$ attains the upper bound of $\sup_{\rho} I(\eta;\bm{A})$ (with respect to $p_e$), thereby minimizing the lower bound. Intuitively, a larger value of $p_e$ indicates a more “assured” answer from the expert, which eases the testing of low-quality annotations by reducing uncertainty in the expert's preference.

    \item \textbf{Aligned behavior:} 
    \[
    p_e(x,y_1,y_2)= p(x,y_1,y_2),
    \]
    as depicted in the middle row. This case represents a scenario in which the expert's preferences are perfectly aligned with those of the annotator.

    \item \textbf{Noisy behavior:} 
    \[
    p_e(x,y_1,y_2)= \operatorname{Proj}_{[0,1]}\Bigl(p(x,y_1,y_2)+\epsilon\Bigr),
    \]
    as shown in the bottom row. Here, $\epsilon$ is an i.i.d. noise term sampled from the normal distribution $\mathcal{N}(0,0.1)$, which captures the heterogeneity between the expert's and the annotator's preferences.
\end{itemize}

\noindent \textbf{Result:} Figure \ref{fig:appx_Fano_main} indicates that: (i) The lower bound is highly dependent on the average confidence $\bar{c}$, which in turn is influenced by the annotator's preference $p(x,y_1,y_2)$ (e.g., see PKU vs.\ Skywork), the expert's behavior $p_e(x,y_1,y_2)$ (e.g., see top row vs.\ bottom row), and the number of tested samples $n$. (ii) To obtain a nontrivial estimation error on datasets such as PKU, a large number of tested samples is required.



\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKU_MI-eps-converted-to.pdf}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Helpsteer_MI-eps-converted-to.pdf}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Ultra_MI-eps-converted-to.pdf}

  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/sky_MI-eps-converted-to.pdf}

  \end{subfigure}
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKUmode_random_MI-eps-converted-to.pdf}
  
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Helpsteermode_random_MI-eps-converted-to.pdf}

  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Ultramode_random_MI-eps-converted-to.pdf}

  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/skymode_random_MI-eps-converted-to.pdf}

  \end{subfigure}
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/PKUmode_noisy_MI-eps-converted-to.pdf}
    \caption{PKU}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Helpsteermode_noisy_MI-eps-converted-to.pdf}
    \caption{Helpsteer}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/Ultramode_noisy_MI-eps-converted-to.pdf}
    \caption{Ultra}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/skymode_noisy_MI-eps-converted-to.pdf}
    \caption{Skywork}
  \end{subfigure}
  \caption{\small Lower bound in Proposition  \ref{prop:Fano_lower_bound}. Lower bound of $\mathbb{P}(\|\hat{\eta}(\bm{A})-\eta\|_2\geq \epsilon)$ vs. the number of tested annotations $n$ at different $\epsilon$ with $p(x,y_1,y_2)=p(x,y_1,y_2)$ (without loss of generality we assume $p(x,y_1,y_2)\geq 1/2)$. (i) $p_e(x,y_1,y_2)\equiv p_e$ (top row), (ii) $p_e(x,y_1,y_2)= p(x,y_1,y_2)$ (middle row), and (iii) $p_e(x,y_1,y_2)= \text{Proj}_{[0,1]}\left( p(x,y_1,y_2)+\epsilon \right)$, where $\epsilon$ is i.i.d. sampled from a Normal distribution $\mathcal{N}(0,0.1)$  to represent a noise term (i.e., heterogeneity among
humans) of the expert (bottom row).}
\label{fig:appx_Fano_main}
\end{figure}

\end{comment}

\subsubsection{Setup and Additional Experiments for Figure \ref{fig:main_contract_rank}}
\label{appx:fig_contract_rank}

\textbf{Setup:} In Figure \ref{fig:main_contract_rank} (and also Figure \ref{fig:appx_contract_rank}), we approximately solve the optimization problems $\mathcal{C}$, $\mathcal{C}_n$, and $\tilde{\mathcal{C}}_n$ as follows. Given $\delta$, $\mu(\eta)$, $G_a(w_a)$, and $E(\eta)$, we discretize the effort space and the contract space. In particular, we discretize the effort space into
\[
\mathcal{S}_\eta = \{0,0.01,\ldots,1\}.
\]
For the binary contract, we set
\[
\tilde{\mathcal{F}}^{\text{bin}}_n = \Bigl\{ w_0 + w_1\,\mathbbm{1}\Bigl\{\bar{A}\geq \tau\Bigr\} :\, w_0\in \{-10,-9.95,\ldots,9.95,10\},\; w_1\in \{0,0.05,\ldots,10\},\; \tau \in \{0,0.01,\ldots,1\} \Bigr\},
\]
where, by the proof of Theorem~\ref{thm:binary}, the optimal binary contract must follow a threshold contract form as above (although the space should be continuous).

For the linear contract, we set
\[
\tilde{\mathcal{F}}^{\text{lin}}_n = \Bigl\{ w_0 + w_1\,\bar{A} :\, w_0\in \{-10,-9.95,\ldots,9.95,10\},\; w_1\in \{0,0.05,\ldots,10\} \Bigr\}.
\]

We then compute the principal's and the agent's utilities for each pair of effort and contract design, and select the pair that maximizes the principal's utility while satisfying the corresponding constraints in $\mathcal{C}$, $\mathcal{C}_n$, and $\tilde{\mathcal{C}}_n$ as the approximate solution. In our experiments, we observed that the discretization can induce infeasibility in the restricted problem $\tilde{\mathcal{C}}_n$. To address this issue, we relax the constraint to
\[
\eta^* \in \Bigl[-0.01+\argmax_{\eta \in \mathcal{S}_\eta} \mathbb{E}\left[U_a\bigl(F_n(\mathcal{D}_n), \eta\bigr)\right],\; 0.01+\argmax_{\eta \in \mathcal{S}_\eta} \mathbb{E}\left[U_a\bigl(F_n(\mathcal{D}_n), \eta\bigr)\right]\Bigr],
\]
where $\eta^*\in \mathcal{S}_\eta$ is the approximate solution of $\mathcal{C}$. This relaxation further induces a non-decreasing gap between $\mathcal{C}$ and $\tilde{\mathcal{C}}_n$, as shown in our results.

In Figure \ref{fig:main_contract_rank}, we set $U_0=0$, $\delta=0.02$, $\mu(\eta)=\frac{1}{2}\eta^{4/5}$, $G_a(w_a)=1-\exp(-w_a)$, and $E(\eta)=0.18\eta^2$. In Figure \ref{fig:appx_contract_effort}, we plot the corresponding agent utility under the optimal solutions, which shows that the agent's utility closely matches the leisure utility $U_0$. This further numerically validates the binding condition of the leisure utility constraint proved in Proposition \ref{prop:first_best_sol} and Theorems \ref{thm:binary} and \ref{thm:linear_contract}.

In Figure \ref{fig:appx_contract_rank}, we further test alternative settings: $\delta=0$ (top row), $\tilde{\mu}(\eta)=\frac{1}{3}\eta^{4/5}$ (middle row), which has a smaller derivative (and value) than $\mu(\eta)$, and $\tilde{G}_a(w_a)=\frac{1}{2}-\frac{1}{2}\exp\Bigl(-\frac{1}{2}w_a\Bigr)$, representing a derivative that is less than that of $G_a$, i.e., the agent is less sensitive to the payment. The results continue to match our theoretical results and demonstrate the advantage of the self-consistency monitoring over the expert-based approach, especially for the PKU dataset.






\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/agent/PKUexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{PKU}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/agent/Helpsteerexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Helpsteer}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/agent/Ultraexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Ultra}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/agent/Skyexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Skywork}
  \end{subfigure}
  \caption{\small Agent utility under the optimal solution, where we set the leisure utility $U_0=0$. For all datasets, monitoring method, contract type, and second-best formulation, the resulted agent utility matches the leisure utility, i.e., the corresponding constraint is binding.}
\label{fig:appx_contract_effort}
\end{figure}




\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/PKUexp_para_1_mu_high_U0_0_delta_0-eps-converted-to.pdf}
\caption{PKU, $\delta=0$}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Helpsteerexp_para_1_mu_high_U0_0_delta_0-eps-converted-to.pdf}
\caption{Helpsteer, $\delta=0$}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Ultraexp_para_1_mu_high_U0_0_delta_0-eps-converted-to.pdf}
\caption{Ultra, $\delta=0$}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Skyexp_para_1_mu_high_U0_0_delta_0-eps-converted-to.pdf}
\caption{Skywork, $\delta=0$}
  \end{subfigure}
 \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/PKUexp_para_1_mu_low_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{PKU, $\tilde{\mu}(\eta)$}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Helpsteerexp_para_1_mu_low_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Helpsteer, $\tilde{\mu}(\eta)$}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Ultraexp_para_1_mu_low_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Ultra, $\tilde{\mu}(\eta)$}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Skyexp_para_1_mu_low_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Skywork, $\tilde{\mu}(\eta)$}
  \end{subfigure}
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/PKUexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{PKU, $\tilde{G}_a(w_a)$}
  \end{subfigure}
    \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Helpsteerexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Helpsteer, $\tilde{G}_a(w_a)$}
  \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Ultraexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Ultra, $\tilde{G}_a(w_a)$}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
\includegraphics[width=\textwidth]
{fig/gap/Skyexp_para_0.5_mu_high_U0_0_delta_0.02-eps-converted-to.pdf}
\caption{Skywork, $\tilde{G}_a(w_a)$}
  \end{subfigure}
  \caption{\small More principal utility gap results for Figure \ref{fig:appx_contract_rank} under different configurations: (i) $\delta=0$; (ii) $\tilde{\mu}(\eta)=1/3\eta^{4/5}$; (iii) $\tilde{G}_a(w_a)=1/2-1/2\exp(-1/2w_a)$.}
\label{fig:appx_contract_rank}
\end{figure}



\subsection{Examples for hard-to-choose responses}
\label{appx:example}
In the following, we present a few examples from HelpSteer \citep{wang2023helpsteer} for which we think it is unclear to say which response is better. In this case, annotators may assign different preference labels for this sample, even when they are fully committed. This is what we refer to as the \textit{heterogeneity} among human annotators. Hence we cannot tell if an annotator makes a mistake whatever label they assign.

\begin{framed}
\textbf{Prompt:}\\

What is the history of the bed? \\

\textbf{Response 1:}\\

The history of the bed is a long and complex one, with many different cultural and historical influences. Beds have been used for thousands of years, and their design and function have evolved significantly over time. In ancient times, beds were often simple platforms made of wood or stone, and were used primarily for sleeping and rest. However, as societies became more complex and wealthy, beds became more elaborate and comfortable, with mattresses, pillows, and even canopy beds. In medieval times, beds became even more luxurious, with mattresses stuffed with feathers, and canopy beds draped in fine fabrics. As the Industrial Revolution brought new manufacturing processes and materials, beds became more mass-produced and affordable, and new designs and features such as adjustable beds and water beds were introduced. Today, beds come in a wide variety of styles, materials, and sizes, and are used not only for sleeping but also for relaxing, reading, and other activities. \\

\textbf{Response 2:}\\

The bed is one of the most important inventions in human history, providing a place of comfort, relaxation, and safety for people around the world. In this question, we will explore the history of the bed and how it has evolved over time. The earliest beds were simple platforms or mats made of leaves, branches, or other natural materials. These beds were used by prehistoric humans and were often placed on the ground or in a cave. Over time, beds became more sophisticated, with the addition of cushions, pillows, and even mattresses made of straw or other materials. In ancient civilizations such as Egypt and Greece, beds were symbols of status and wealth, with wealthy individuals owning ornate beds made of wood and adorned with gold and other precious materials. In medieval Europe, beds became even more elaborate, with some beds featuring canopy frames and curtains for privacy. As materials and manufacturing processes advanced, beds became more affordable and accessible to the general public. In the 20th century, the bed became an important symbol of comfort and relaxation, with the invention of the adjustable bed and the development of the modern mattress. Today, the bed remains an essential piece of furniture in homes around the world, with a wide variety of styles, sizes, and materials available to suit different needs and preferences. From simple futons to luxurious canopy beds, the bed continues to play a crucial role in providing comfort and relaxation for people of all ages and backgrounds.\\
\end{framed}

\begin{framed}
\textbf{Prompt:}\\

What are the three most important things to consider when deciding what technology to use to build an assist device to help an elderly person with basic needs? \\

\textbf{Response 1:}\\

There are many different types of assistive devices that can be used to help an elderly person with their basic needs. When deciding what technology to use to build an assistive device, it is important to consider the following three factors: 1. The needs of the elderly person: The first and most important factor to consider is the specific needs of the elderly person who will be using the assistive device. It is important to understand their physical limitations, as well as their daily routine and activities, in order to design an assistive device that will be effective and useful for them. 2. The capabilities of the technology: It is also important to consider the capabilities of the technology that will be used to build the assistive device. For example, if the elderly person has limited vision, it may be important to use a technology that provides auditory feedback or haptic feedback in addition to visual feedback. 3. The ease of use of the technology: Finally, it is important to consider the ease of use of the technology. The assistive device should be designed in a way that is easy for the elderly person to understand and use, even if they have limited dexterity or cognitive abilities. This may involve using simple controls, providing clear instructions, or using a natural language interface. By considering these three factors, it is possible to build an assistive device that is effective, useful, and easy to use for the elderly person. \\                

\textbf{Response 2:}\\

 When deciding what technology to use to build an assist device to help an elderly person with basic needs, there are three most important things to consider: 1. The specific needs of the elderly person: The assist device should be tailored to the individual's specific needs and limitations. For example, if the person has limited mobility, the device should focus on assisting with mobility. 2. The ease of use: The assist device should be easy to use and understand, with clear and simple instructions. The elderly person should be able to operate the device without assistance. 3. The reliability of the technology: The assist device should be reliable and robust, with minimal risk of malfunction or failure. The technology should be tested thoroughly to ensure it is safe and effective.   \\
 
\end{framed}


\begin{framed}
\textbf{Prompt:}\\

List down some tips to make the most out of attending a seminar. \\

\textbf{Response 1:}\\

Sure! Here are some tips to make the most out of attending a seminar: 1. Prepare in advance by reading the seminar's material and any suggested readings. 2. Arrive on time and stay for the duration of the seminar. 3. Engage with the speaker and other attendees by asking questions and participating in discussions. 4. Take notes and actively listen to the speaker and other attendees. 5. Connect with other attendees to expand your network and share ideas. 6. Reflect on the seminar's content and consider how it applies to your own work or interests. 7. Follow up with the speaker or organizers to continue the conversation or ask further questions.        \\

\textbf{Response 2:}\\

Before attending a seminar, research the topic and speakers to understand the purpose and goals. Choose a comfortable seat near the front to improve your focus and engagement. Arrive early to settle in and mingle with other attendees. Use the event app or program to connect with other attendees and speakers. Take notes and ask questions to enhance your learning experience. Don't be afraid to introduce yourself to others and engage in conversations. Stay for the entire event, even if it's longer than expected, to reap the full benefits. Don't be afraid to ask for business cards and follow up with people after the event.            \\

\end{framed}







\end{document}
