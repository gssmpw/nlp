

\section{Related Work}
\label{sec:related-work}

\noindent\textbf{White-box Adversarial Attacks}.
L-BFGS~\cite{szegedy2014intriguing} was the first proposed adversarial attack that demonstrated how simple perturbations could affect the DNNs performance.
Fast Gradient Sign Method (FGSM)~\cite{goodfellow2015explaining} is a one-step method that uses the model cost function, the gradient, and the radius epsilon to search for perturbations.
Jacobian-based Saliency Maps (JSM)~\cite{papernot2016limitations} explore the forward derivatives and construct the adversarial saliency maps.
Gradient Aligned Adversarial Subspace (GAAS)~\cite{tramer2017space} estimates the dimensionality of the adversarial subspace using the first-order approximation of the loss function.
Sparse and Imperceivable Adversarial Attacks (SIAA)~\cite{croce2019sparse} create sporadic and imperceptible perturbations by applying the standard deviation of each color channel in both axis directions.
DeepFool~\cite{moosavi2016deepfool} is an iterative attack that stops when the minimal vector orthogonal to the hyperplane representing the decision boundary is found.
SmoothFool (SF)~\cite{dabouei2020smoothfool} is an iterative algorithm that uses DeepFool to calculate the initial perturbation and smoothly rectifies the resulting perturbation until the adversarial example fools the classifier.
Projected Gradient Descent (PGD)~\cite{madry2018towards} is an iterative attack that uses saddle point formulation to find a strong perturbation.
Momentum Iterative FGSM (MI-FGSM)~\cite{dong2018boosting} introduces momentum into the Iterative FGSM (I-FGSM).
Auto-Attack~\cite{croce2020reliable} is a set of attacks to evaluate the networks, proposing the APGD-CE (i.e., PGD using Cross-Entropy (CE)), and APGD-DLR (i.e., PGD using Difference of Logits Ratio (DLR)) attacks. These techniques are are combined with Fast Adaptive Boundary (FAB)~\cite{croce2020minimally}, used to minimize the norm of the adversarial perturbations, and the Square Attack~\cite{andriushchenko2020square}, a query-efficient black-box attack. LISArD proposes using white-box attacks against models with the same architecture and data as the target, but without assuming the attacker can access this target model, making our approach more suitable to deal with realistic scenarios.



\textbf{Adversarial Distillation}.
Defensive Distillation (DD)~\cite{papernot2016distillation}, and its extension~\cite{papernot2017extending}, were the first methods to demonstrate the usefulness of distillation to defend against adversarial examples.
Robust Self-Training (RST)~\cite{carmon2019unlabeled} uses a standard supervised approach to obtain pseudo-labels and feed them into another network that targets adversarial robustness.
Adversarially Robust Distillation (ARD)~\cite{goldblum2020adversarially} performs distillation using an adversarially trained network as the teacher.
Introspective Adversarial Distillation (IAD)~\cite{zhu2021reliable} evaluates the robustness of the teacher network considering both the student and teacher labels.
Robust Soft Label Adversarial Distillation (RSLAD)~\cite{zi2021revisiting} uses robust soft labels produced by a teacher network to supervise the student training on natural and adversarial examples.
Low Temperature Distillation (LTD)~\cite{chen2021ltd} considers low temperature in the teacher network and generates soft labels that can be integrated into existing works.
Robustness Critical Fine-Tuning (RiFT)~\cite{zhu2023improving} introduces the module robust criticality metric to fine-tune the less robust modules to adversarial perturbations.
Adaptive Adversarial Distillation (AdaAD)~\cite{huang2023boosting} involves the teacher model in the optimization process by interacting
with the student model to search for the inner
results adaptively.
Information Bottleneck Distillation (IBD)~\cite{kuang2024improving} uses soft-label distillation to increase the mutual information between latent features and predictions and transfers relevant knowledge from the teacher
to the student to reduce the mutual information between the input and latent features.
Fair Adversarial Robustness Distillation (FairARD)~\cite{yue2024revisiting} ensures robust fairness of the student by increasing the weights for naturally more difficult classes.
PeerAiD~\cite{jung2024peeraid} trains a peer network on
the adversarial examples generated for the student network, simultaneously training the student and peer network.
Dynamic Guidance Adversarial Distillation (DGAD)~\cite{park2025dynamic} corrects teacher and student misclassification on clean and adversarially perturbed images. LISArD also does not include additional models during the inference phase, without involving Adversarial Training (AT) and larger previously trained models, thus being a more reliable approach for various domains.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/approaches_v2.png}
    \caption{Types of approaches commonly used to defend against adversarial attacks. The Teacher Model refers to a previously trained model, usually bigger than the Student Model, that aids the latter by providing soft labels. The DDPM refers to a Denoising Diffusion Probabilistic Model (a generative model) that uses noise and denoise to produce a ``purified'' image.}
    \label{fig:approaches}
\end{figure*}



\textbf{Adversarial Purification}.
Yoon \textit{et al.}~\cite{yoon2021adversarial} propose using an Energy-Based Model with Denoising Score-Matching to purify perturbed images quickly.
For the first time, diffPure~\cite {nie2022diffusion} uses DDPM to remove the adversarial perturbations from the input images.
Guided Diffusion Model for Adversarial Purification (GDMAP)~\cite{wu2022guided} gradually denoises pure Gaussian noise with guidance to an adversarial image.
APuDAE~\cite{kalaria2022towards} uses Denoising AutoEncoders~\cite{vincent2008extracting} to purify the adversarial examples in an adaptive way, improving the accuracy of target networks.
DensePure~\cite{chen2022densepure} uses different random seeds to get multiple purified images, which are fed to the classifier, and its final prediction is based on majority voting.
Wang \textit{et al.}~\cite{wang2023better} uses better diffusion models~\cite{karras2022elucidating} to demonstrate that higher efficiency and quality diffusion models translate into better robust accuracy.
Lee \textit{et al.}~\cite{lee2023robust} propose a gradual noise-scheduling strategy that improves the robustness of diffusion-based purification.
Feature Purification Network (FePN)~\cite{cao2023fepn} is an adversarial learning mechanism that learns robust features by removing non-robust features from inputs while reconstructing high-quality clean images.
DifFilter~\cite{chen2024diffilter} uses a score-based method to improve the data distribution of the clean samples.
DiffAP~\cite{zhang2024random} uses conditional guidance to ensure prediction consistency between the purified and clean images. 
MimicDiffusion~\cite{song2024mimicdiffusion} approximates the purification process of adversarial examples and clean images by using Manhattan distance and two guidances. Adversarial Purification is the most efficient defense approach for DNNs, but it comes at the cost of high computational resources, while LISArD is able to protect different architectures in various setups without requiring additional training overhead.