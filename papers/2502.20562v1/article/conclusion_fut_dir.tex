

\section{Conclusion}
\label{sec:conclusion}

\noindent This paper describes an evaluation framework based on the \emph{gray-box setting} that is more realistic than the typically used \emph{white-box} scenario, where the existing models do not perform reliably. We propose an adversarial defense mechanism for this setting (LISArD), which is simultaneously robust against white-box attacks, and does not depend on the inclusion of adversarial samples. This mechanism uses image similarity to instruct the model to recognize that images pairs regard the same object, while simultaneously inferring class information. The experiments show the vulnerability of pre-trained and scratch-trained networks to gray-box adversarial samples and point to the effectiveness of LISArD in increasing resilience against this type of samples. Also, state-of-the-art \textit{Adversarial Distillation} models cannot perform in white-box settings without the inclusion of AT. In the future, the injection of other types of noise (\textit{e.g.}, using fractional Gaussian noise with persistence, instead of white noise) will be subject of our further analysis. Finally, for realism purposes, we suggest evaluating the models in scenarios in which the attacker only knows the training data and the type of architecture.
