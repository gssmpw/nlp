

\section{Experiments}

\subsection{Experimental Setup}

\noindent\textbf{Datasets}. 
The datasets are based on the most recent papers addressing adversarial defenses and their performance. 
The models are evaluated on CIFAR-10~\cite{krizhevsky2009learning} and CIFAR-100~\cite{krizhevsky2009learning}, consisting of 50 000 training and 10 000 testing images, and Tiny ImageNet~\cite{le2015tiny}, which has 100 000 training and 10 000 validation images, and is a subset of ImageNet comprising only 200 classes. These datasets are widely adopted in Adversarial Attacks in Object Recognition specialized literature.

\textbf{Gray-box Attacks.} Each selected architecture was trained in the CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning}, and Tiny ImageNet~\cite{le2015tiny} datasets and typical models with the best clean accuracy were selected. 
The weights of these models are then used to generate the adversarial images. For all the attacks, the perturbation constraint was set to $\epsilon = 8/255$ for CIFAR-10 and CIFAR-100 and $\epsilon = 4/255$ for Tiny ImageNet. The considered attacks were FGSM~\cite{goodfellow2015explaining}, 10 steps PGD~\cite{madry2018towards} with a step size of $2/255$, and AA~\cite{croce2020reliable} using the $L_\infty$ norm and standard version.

\textbf{Evaluation}. We use accuracy on natural test samples, denominated Clean Accuracy, and accuracy on adversarial test samples, represented by the attack name, to measure the performance of the model. The attacked datasets used to train the proposed approach are different from the ones used to evaluate LISArD.

\input{article/tables/graybox_scenario}
\input{article/tables/diff_networks}

\subsection{Gray-box Settings}

\noindent\textbf{Gray-box Adversarial Attack Impact.} We start by studying if the gray-box scenario is also an issue for typical models. We report the gray-box accuracy for ResNet18 architecture using different training methods in Table~\ref{tab:graybox_scenario}. This scenario uses a ResNet18 model trained on the CIFAR-10 dataset to generate the adversarial images using the different attacks. Then, these images are given to other ResNet18 models to evaluate their robustness. Both models with and without pretrain are vulnerable to gray-box attacks, raising awareness for this more realistic type of attack. LISArD significantly helps to diminish the effect of gray-box attacks, which highlights the importance of image similarity for robust model defense. 


\textbf{Vulnerability of Different Architectures.} Since we are proposing a new training mechanism to diminish the effect of gray-box attacks, we need to evaluate if it can be applied to different networks. The gray-box accuracy for different architectures is presented in Table~\ref{tab:diff_networks}, ranging from 3.5M (MobileNetv2) to 143.7M (VGG19) parameters. The proposed method effectively helps to protect against gray-box attacks for multiple models with different architectures and number of parameters. Figure~\ref{fig:Comparison_distribution} highlights the LISArD approach of clean and perturbed images representing the same concept, translated by the overlap of the distributions of clean and attacked image embeddings. To objectively quantify this overlap of information, we use the decidability measure~\cite{daugman2000biometric} ($\textit{d}'$), which shows that LISArD effectively approximates the distributions of clean and attacked images ($\textit{d}'$ close to 0), increasing protection against the gray-box attacks without additional training effort. 


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{imgs/distribution_CIFAR10_ResNet_LISAD.png}
    \caption{Comparison of the distributions for clean (blue) and attacked (red) images when considering a ResNet (left) and LISArD (right) for CIFAR-10. $\textit{d}'$ refers to the decidability measure, where values closer to 0 mean greater overlap between distributions.}
    \label{fig:Comparison_distribution}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{imgs/comparison_CIFAR_Normal_LISAD.png}
    \caption{Clean and PGD$_{10}$ images and the effect of adversarial attacks on ResNet and LISArD trained networks for CIFAR-10 and CIFAR-100 datasets. Red and Green refer to incorrect and correct classifications, respectively.}
    \label{fig:comparison_CIFAR_Normal_LISAD}
\end{figure}

\input{article/tables/SOTA_comparison_CIFAR}

\textbf{Gray-box Adversarial Examples}. To further understand the impact of gray-box settings, we demonstrate scenarios where a typical network fails to correctly classify the object, in Figure~\ref{fig:comparison_CIFAR_Normal_LISAD}. A typical network has difficulty in rightly classifying images with clearly outlined objects or with almost no background, only by adding perturbations that do not impair Human decision (third column in the figure). This confirms the hypothesis that typical networks are also vulnerable to gray-box attacks, which is mitigated when using LISArD, where the networks can now correctly classify the same images (fourth column in the figure), highlighting the importance of image similarity-based training to provide increased robustness.



\subsection{Adversarial Robustness}

\noindent To the best of the authors' knowledge, no previous works in the literature propose evaluating the networks in the gray-box scenario. Therefore, images solely for the purpose of evaluation were generated to ensure a fair comparison between the different approaches, effectively assuring that the models had not previously seen these images. The adversarial robustness is evaluated on CIFAR-10 and CIFAR-100 in Table~\ref{tab:SOTA_comparison_CIFAR} and on Tiny ImageNet in Table~\ref{tab:sota_comparison_TinyImageNet}.

\textbf{AT Effect on Model Performance}. State-of-the-art models typically include adversarial samples in training to increase model robustness, which gives an inherent advantage in white-box settings. To assess the resilience in both scenarios (gray-box and white-box) and to make a fair comparison with LISArD, we consider the settings with and without AT for the different models in our experiments. Table~\ref{tab:SOTA_comparison_CIFAR} compares LISArD and \textit{Adversarial Distillation} approaches, with and without AT during the training phase, showing that the models are highly dependent on AT examples to perform in white-box settings and are not as resilient in gray-box settings without these samples. The results also show that to improve resilience against white-box attacks, the Adversarial Distillation approaches are highly reliant on including AT and not on proposing a different type of approach.


\textbf{Gray-box \textit{vs.} White-box}. 
The difference in Clean accuracy between the gray-box and white-box evaluation for AdaAD, PeerAiD, and DGAD is: 1) the former is obtained from models trained according to the author's available implementations, and 2) the latter were obtained directly from the paper reported experiments~\cite{huang2023boosting,jung2024peeraid,park2025dynamic}.
In Table~\ref{tab:SOTA_comparison_CIFAR}, when comparing the results referring to gray-box attacks (4th, 5th, and 6th columns) with the ones from white-box attacks (8th, 9th, and 10th columns), we can note that the pattern regarding the strength of the attack is the same. In both scenarios, AA is the strongest, followed by PGD, and FGSM, respectively, suggesting that the effectiveness in the white-box scenario is also transferred to the gray-box scenario. This shows that the selected settings are representative of strong attacks, and the gray-box scenario has a difficulty aligned with the white-box one.


\textbf{State-of-the-art Methods}.
When evaluating the gray-box scenario, AdaAD is the second most resilient defense when removing the AT due to their reduced reliance on the labels from adversarial samples. Additionally, the same model is robust against FGSM in the white-box scenario, suggesting that learning from a teacher might be reliable for single-step attacks.
The remaining Adversarial Distillation methods rely heavily on including adversarial samples during the training phase to provide robustness against both gray-box and white-box attack scenarios.
PeerAiD without including AT performs similarly to a model with random predictions, which suggests that this defense is highly (or completely) dependent on the inclusion of AT to train a resilient model, justified by the removal of the ground-truth labels during the training phase. The results show that all analyzed models underperform greatly when removing AT and/or moving to gray-box settings, highlighting the limitations of existing approaches and their lack of robustness to perform in various conditions (aside from white-box settings).


\textbf{Overall LISArD Perfomance}.
LISArD is the least time-consuming method whilst offering the best overall resilience against attacks in a gray-box scenario due to its similarity learning relying solely on mathematical operations without including additional models.
For both datasets, LISArD shows a decrease in accuracy for all the attacks when including the AT approach in the gray-box scenario, suggesting that including adversarial samples in the training phase weakens the generalization capability.
Nevertheless, the inclusion of AT diminishes the clean accuracy of all models, which does not happen with the same impact when using the proposed defense.
This shows that LISArD performs the best in gray-box settings and is the most resilient defense in white-box settings when the adversarial samples are removed from the training stage, as shown by the $\Delta$ for both gray-box and white-box.



\input{article/tables/SOTA_comparison_TinyImageNet}

\textbf{Tiny ImageNet}. To demonstrate the applicability of LISArD to larger and diversified datasets, we display the results for Tiny ImageNet in Table~\ref{tab:sota_comparison_TinyImageNet}. It was not possible to provide the results for AdaAD, PeerAiD, and DGAD, because the available implementations did not provide enough details on how to train for Tiny ImageNet. Nonetheless, we compare the proposed approach with a standardly trained network, showing that the former has a greater capacity to resist gray-box attacks despite the increase of data and labels. 


\input{article/tables/generation_ablation_studies}
\input{article/tables/loss_ablation_studies}

\subsection{Ablation Studies}

\noindent The loss function displayed in equation~\ref{eq:final_loss} was altered in multiple ways to find the adequate method for both learning image similarity and classification, considering Random as the image generation mechanism. The results for the ablation studies are displayed in Tables~\ref{tab:approximation_using_diff_imgs},~\ref{tab:loss_ablation_studies}, and~\ref{tab:components_ablation_studies}, and Figure~\ref{fig:Failures_CIFAR_LISAD} illustrates some scenarios where LISArD is unable to correctly classify the object.

\textbf{Different Image Generation Mechanisms}. Since LISArD intends to train a model to learn to approximate the noisy images to the clean images, the first ablation consists of evaluating the mechanism used to generate the noise images. Table~\ref{tab:approximation_using_diff_imgs} indicates the results for these evaluations, considering the FGSM, PGD, and AA attacks and adding Gaussian Noise (Random), with the loss function according to the one in equation~\ref{eq:final_loss}. As can be observed, the FGSM image generation fails to provide robustness against PGD and AA, suggesting the former is unsuccessful against multiple-step attacks. Although the PGD image generation improves the resilience against AA and PGD, it still performs less than AA, with the latter not significantly increasing the time cost. The increased performance observed in AA might be related to including multiple-step and black-box attacks in the image generation process, increasing the model generalization capability. Finally, the results show that adding white noise (Random) grants the best generalizing capability to the models for all the considered attacks whilst being the least resource-consuming because the images are generated without accessing the model gradients.


\textbf{Loss Function and Optimizer}. We start by evaluating the adequate optimizer for the main objectives of LISArD and if all the terms in the previously mentioned equation are necessary, as shown in Table~\ref{tab:loss_ablation_studies}. Since LISArD uses a training batch of 2048, we first explored the Layer-wise Adaptive Rate Scaling (LARS), which is an optimizer commonly used in greater-dimension batch sizes. However, the results demonstrate that using LARS is not the most effective means to make the classification component learn, leading to a performance significantly lower than a standard-trained network in clean accuracy (4$^{th}$ row in Table~\ref{tab:loss_ablation_studies}). Therefore, we opted to use the Stochastic Gradient Descent (SGD) as an optimizer, which is typically used in the literature to train the models (specifically for object recognition) and demonstrated overall better results than LARS. When considering only classifying the noisy images, the model performs better in the attack scenario but decreases performance for the clean images. On the other hand, solely classifying clean images demonstrates better results in clean accuracy. Thus, we opted for a conjunction between clean and noisy image classification, which exhibits the best results in overall accuracy.

\textbf{Loss Component Variation}. LISArD considers gradual learning throughout the 200 epochs, with greater initial importance given to the image similarity part and gradually increasing the importance of classification through the inclusion of $\alpha$. Additionally, temperature ($\tau$) was included in LISArD due to its proven increase in classification accuracy, specifically for \textit{adversarial distillation}~\cite{chen2021ltd,huang2023boosting}. Table~\ref{tab:components_ablation_studies} displays the results for balancing the classification and similarity components and including a temperature element. It is possible to observe that $\alpha$ significantly impacts the resilience of the model against adversarial attacks, reinforcing the significance of the image similarity component in that matter. The temperature $\tau$ is relevant to improve the results in both clean and attacked scenarios, as previously shown in the literature.

\input{article/tables/components_ablation_studies}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/Failures_CIFAR_LISAD.png}
    \caption{LISArD misclassification for CIFAR-10 and CIFAR-100 datasets, showing the difficulty to correctly classify objects that are blended with the background. Red refers to incorrect classifications.}
    \label{fig:Failures_CIFAR_LISAD}
\end{figure}

\textbf{Approach Limitations}. We display examples of scenarios that are challenging for LISArD in Figure~\ref{fig:Failures_CIFAR_LISAD}. LISArD fails to resist adversarial attacks when the pictures have the object masked in the image background. These scenarios are already hard to classify in a clean context, and their difficulty is exacerbated by adding perturbations to these images. As such, the seen underperformance of LISArD relates to the resemblance between the background and the object, making the classification inherently challenging, even when classifying the clean images.
