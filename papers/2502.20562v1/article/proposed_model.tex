


\section{LISArD Methodology}
\label{sec:proposed_model}

\subsection{Adversarial Context and Preliminary}

\noindent\textbf{White-box Issues}. The white-box attacks are the strongest attacks for a specific model, yet if its training method slightly diverges, the same perturbations no longer have the identical effect as the model that was used to generate the adversarial samples. Furthermore, the white-box scenario requires that the attacker has access to the implementation/code of the model, which might not be realistic in most cases, since the attacker will rarely have access to the code of deployed models.


\textbf{Black-box Problems}. The black-box attacks are mainly focused on generating perturbations based on a low amount of knowledge, reducing the effect of the adversarial samples when compared to the white-box. However, the former attacks are more viable since the attacker only needs to know pairs of images and answers given by the target model to generate the perturbations. The black-box scenario can be considered as the most generic nowadays, since it does not require any information about the model, being potentially applicable to any available system exposing an DNN. Nevertheless, in this scenario, the attacker does not benefit from additional details of the target model, which hinders the probability of success.


\textbf{Proposed Solution}. We propose an alternative scenario in which the attacker knows the architecture of the model and dataset used to train it but does not have access to the gradients of the model. This information is usually accessible in papers or descriptive pages of the model, which can help the attacker to achieve stronger perturbations. This scenario is more realistic than white-box by compromising the amount of knowledge needed and the influence of the perturbations on the target model. LISArD considers using white-box attacks to generate adversarial samples against a model that uses the same architecture and dataset as the target model.


\textbf{Types of Approaches}. The two approaches described in the specialized literature and the approach proposed herein are summarized in Figure~\ref{fig:approaches}, which highlights the increased resources for performing \textit{Adversarial Distillation} and \textit{Purification} compared to LISArD. \textit{Adversarial Distillation} requires the usage of an additional previously trained model (Teacher) to aid in teaching the resilient network (Student), and \textit{Adversarial Purification} involves the training of a generative model (DDPM) to remove the adversarial noise during the inference phase (Purification). LISArD considers its attack scenario as a gray-box, meaning that the attacker only has partial knowledge about the target model. Thus, training to perceive noisy images (created by adding random Gaussian noise) similar to clean images can aid in defending against this type of attack.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/learning_image_similarity.png}
    \caption{Overview of the conversion from embeddings to a matrix in the Learning Image Similarity component. $E$ refers to the size of the embeddings, which vary depending on the selected model.}
    \label{fig:learning_image_similarity}
\end{figure}



\subsection{Image Similarity and Importance}

\noindent\textbf{Motivation}. Learning Image Similarity (LIS) is based on the idea that an image containing a reduced amount of noise does not affect the object represented in that image. Barlow Twins~\cite{zbontar2021barlow} proposes a procedure to reduce the redundancy between a pair of identical networks in the context of self-supervised learning. LISArD utilizes the redundancy reduction approach to teach the model to identify the noisy and clean images as similar and improve robustness against gray-box and white-box attacks. 

\textbf{Embeddings to Matrix Conversion}. An overview of the LIS component, explaining the conversion process from embeddings to a matrix, which is used to achieve redundancy reduction between images is provided in Figure~\ref{fig:learning_image_similarity}. The embeddings with size $E$ are extracted before being fed to the classification layer, and each clean embedding is multiplied by each noisy embedding to obtain the cross-correlation matrix. Then, this cross-correlation matrix is approximated to the diagonal matrix to achieve a perfect correlation.

\textbf{Weighted Training}. LISArD focuses on two main approaches: learning that two images are similar and simultaneously learning to classify the images, which motivates the usage of weighted training. Figure~\ref{fig:weighted_loss} explains how LISArD relates the LIS component with the classification one. As previously explained, the embeddings obtained from clean and noisy images are used in LIS while simultaneously being forwarded to the classification layer. The predictions are used in the (losses) $\mathcal{L}_{C}$ and $\mathcal{L}_{R}$ for the clean and noisy images, respectively, to train the classification component (the losses are better explained below). With this approach, we intend that the model initially concentrates on learning that two images represent the same object, but the final task is the classification of that object, justifying the initially increased importance of LIS and the gradually increasing importance of classification toward the end of training.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/weighted_loss.png}
    \caption{Overview of the LISArD~architecture. The clean and noisy images are fed to the model, and the inner product is calculated using their respective embeddings. Both clean (orange) and noisy embeddings (green) are used to predict each class using an adaptive weight loss between $\mathcal{L}_{C}$ and $\mathcal{L}_{R}$ and $\mathcal{L}_{S}$.}
    \label{fig:weighted_loss}
\end{figure}


\subsection{Loss Function}

\noindent The LISArD consists of a new defense mechanism that does not cost significantly more than the standard training but provides the networks with robustness against gray-box and white-box adversarial attacks. It starts by generating random images for every batch, according to the following equation:

\begin{equation}
x_{R} = x_{C} + \sqrt{\mu} \cdot x_{N},
\end{equation}
%
where $x_{R}$ refers to the random image, $x_{C}$ refers to the clean image, and $\mu$ is the maximum amount of perturbation to be added to the image (simulating the $\epsilon$ from adversarial attacks). $x_{N}$ refers to the Gaussian noise with the same size as the clean image. Since we have two images that are given as input to the model, we have a classification loss for each of them. Formally, this loss is defined as the comparison between the predicted label and the ground truth via Cross-Entropy:

\begin{equation}
\mathcal{L}_{\{C,R\}} = (y~\text{log}(p) + (1 - y)~\text{log}(1 - p)),
\end{equation}
%
where $\mathcal{L}_{\{C,R\}}$ refers to either the clean image loss or random image loss, $p$ are the predicted labels for the images batch, and $y$ are the ground truth labels for the images batch. Another part of the loss function consists of the approximation between the embeddings of each input image. The following equation translates this process:

\begin{equation}
    \mathcal{L}_{S} = \sum_i (1 - M_{ii})^2 + \lambda \sum_i \sum_{j \neq i} M_{ij}^2,
\end{equation}
%
where $\lambda$ is a positive constant that balances the importance of the terms and $M$ is the cross-correlation matrix obtained by the embeddings of the two images along the batch:

\begin{equation}
    M_{ij} = \frac{ \sum_b z^A_{b,i} z^B_{b,j} }{ \sqrt{\sum_b (z^A_{b,i})^2} \sqrt{\sum_b (z^B_{b,j})^2}},
\end{equation}
%
where $b$ is the index for the batch samples and $i$, $j$ are the indexes for the elements of the matrix. $M$ is a square matrix with a size equal to the network output. Finally, the complete loss function is expressed by:

\begin{equation}
\begin{aligned}
\mathcal{L}_{} = \alpha~(\mathcal{L}_{C} + \mathcal{L}_{R}) + (1-\alpha)~\left(\frac{\mathcal{L}_{S}}{\tau}\right), 
\end{aligned}
\label{eq:final_loss}
\end{equation}
%
where $\mathcal{L}_{C}$, $\mathcal{L}_{R}$, and $\mathcal{L}_{S}$ refer to the losses for clean images, random images (defined in equation 2), and similarity approximation (defined in equation 3). $\tau$ refers to the temperature and $\alpha$ refers to the weight for classification, with $\alpha$ starting at 0.5 and incrementing to 1 throughout training, as follows:

\begin{equation}
\begin{aligned}
\alpha = \alpha_{0} + \delta(\varepsilon-1),
\end{aligned}
\end{equation}
%
where $\alpha_{0}$ is the starting coefficient, defined as 0.5, $\delta$ is the decay degree, set to $\frac{1}{400}$ and $\varepsilon$ refers to the training epoch.



\subsection{Selected Attacks and State-of-the-art}

\noindent FGSM~\cite{goodfellow2015explaining}, PGD~\cite{madry2018towards}, and AA~\cite{croce2020reliable} attacks are selected to evaluate LISArD and compare it with state-of-the-art. FGSM is a one-step adversarial attack that uses the gradients of the model, being a weaker white-box adversarial attack. PGD is a strong attack that many defenses still fail to overcome and has multiple iterations that increase its strength. AA consists of an ensemble of attacks containing white-box and black-box variants, allowing an evaluation in both settings, which increases the scope of our evaluation. AdaAD~\cite{huang2023boosting}, PeerAiD~\cite{jung2024peeraid}, and DGAD~\cite{park2025dynamic} are the approaches selected to compare with LISArD since these \textit{Adversarial Distillation} models achieve state-of-the-art performance in white-box settings and have available implementations.



\subsection{Implementation Details}

\noindent\textbf{Hardware.} The experiments were performed in a multi-GPU server containing seven NVIDIA A40 and an Intel Xeon Silver 4310 \symbol{`@} 2.10 GHz, with the Pop!\_OS 22.04 LTS operating system. The models were trained using a single NVIDIA A40 GPU without additional models running on the same GPU when presenting the total time or time per epoch results.

\textbf{Models}. In order to be as comprehensive as possible regarding the multiple proposal of architectures, we selected ResNet18~\cite{he2016deep}, ResNet50~\cite{he2016deep}, ResNet101~\cite{he2016deep}, WideResNet28-10~\cite{zagoruyko2016wide}, VGG19~\cite{simonyan2014very}, MobileNetv2~\cite{sandler2018mobilenetv2}, and EfficientNetB2~\cite{tan2019efficientnet} as our backbones. 
For all the datasets, the networks were trained using an SGD optimizer with a learning rate of 0.001, a momentum of 0.9, and a weight decay of 0.0005 during 200 epochs. 
We disregarded the training of Inceptionv3~\cite{szegedy2016rethinking} due to its need to increase the image size to 299x299, which would not be the same training and evaluation settings as other models.

\textbf{Ablation Studies}. Models were trained for 200 epochs using ResNet18 as the backbone architecture for all ablation studies and evaluated on the CIFAR-10 clean, FGSM, PGD, and AA datasets. The last three datasets were generated by applying the respective attack to a previously trained ResNet18 on CIFAR-10 clean.