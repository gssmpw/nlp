


\section{Introduction}
\label{sec:intro}

\IEEEPARstart{D}{eep} Neural Networks (DNNs) have achieved remarkable performance in multiple areas, such as Medical Imaging~\cite{thirunavukarasu2023large,patricio2023coherent}, Natural Language Processing~\cite{touvron2023llama2openfoundation,costa2022predicting}, and Active Speaker Detection~\cite{roxo2023exploring,roxo2024bias,roxo2024asdnb}. This accomplishment led to the wide adoption of Artificial Intelligence in the daily lives of many people, either in work or leisure scenarios, increasing the attractiveness and susceptibility of DNNs to attackers.
The study of DNN security is still in its early stages. with Szegedy \textit{et al.}~\cite{szegedy2014intriguing} demonstrating, for the first time, that Convolutional Neural Networks (CNNs) fail to generalize and are vulnerable to carefully crafted perturbations (consisting of noise imperceptible to the Human eye) that when added to the original images create the so-called \textit{adversarial examples}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{imgs/attack_types_v2.png}
    \caption{Comparison between the information available to an attacker when considering the different types of attacks. \textit{Image/Predictions Pairs} refers only accessing a set of images given to the model and the respective prediction, \textit{Data and Architecture} refers to knowing the target model architecture and dataset used to train it, and \textit{Model Gradients} refers to controlling the model loss function.}
    \label{fig:attack_types}
\end{figure*}

\textit{Adversarial Distillation}~\cite{papernot2016distillation,goldblum2020adversarially,zhu2021reliable} and \textit{Adversarial Purification}~\cite{yoon2021adversarial,wu2022guided,chen2022densepure} are two of the most studied methods to develop models that are robust against white-box attacks. Although \textit{Adversarial Distillation} can help defend against white-box attacks, it requires including adversarially perturbed images during the training process. \textit{Adversarial Purification} includes a Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020denoising} between the inputted image and the target network. This defense mechanism requires the use of a high-parameter model and time to purify each image that is given to the target network. Both previously mentioned defense mechanisms focus on white-box attacks, which are the most explored in the literature and significantly impact the performance of DNNs.

Assuming that the attacker can access the model parameters to generate the perturbed images is unrealistic in many cases. Furthermore, a work by Katzir and Elovici~\cite{katzir2020gradients} found that the ability to defend against white-box attacks comes at the cost of losing the ability to learn. Therefore, this paper proposes a more realistic adversarial scenario that assumes the attacker only knows the network architecture and the dataset used during the training process without accessing model gradients, named the \textbf{gray-box scenario}. Figure~\ref{fig:attack_types} summarizes the differences between white-, gray-, and black-box scenarios, clearly displaying the amount of information the attacker can access in each of them. In this sense, the gray-box scenario assumes a compromise between what the attacker knows and the effect of the attacked images.

\IEEEpubidadjcol
This paper also presents an approach to duly defend against the proposed gray-box scenario, named Learning Image Similarity Adversa\textit{r}ial Defense (LISArD), which can also be applied to white-box settings without depending on adversarial examples. LISArD relates the similarity between clean and perturbed images by calculating the cross-correlation matrix between the embeddings of these images and using the loss to approximate this matrix to the identity while teaching the model to classify objects correctly. The goal of this approach is to reduce the effect of perturbations, motivating the model to recognize the clean and perturbed images as similar. This paper contributions can thus be summarized as follows:
\begin{itemize}
    \item It introduces the first gray-box testing framework, solely based on the architecture and data used to train a network, which is more realistic than white-box scenarios;
    \item It presents a defense mechanism that helps standard networks to be robust against gray-box attacks, without additional training epochs, parameters, and adversarially attacked images;
    \item Ablation studies and experimental evaluation demonstrate LISArD is the most robust against gray-box attacks, without increasing training cost, and has the least performance decrease in white-box scenarios.
\end{itemize}

The remaining of the paper is structured as follows: section~2 discusses the related works, namely \textit{Adversarial Distillation} and \textit{Adversarial Purification}; section~3 describes preliminary concepts, provides LISArD formal definition, and justifies the attack selection; section~4 reports the experimental setup, ablation studies and performance analysis, accompanied by a discussion; finally, Section~5 concludes the paper.
