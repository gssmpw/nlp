\section{Related Works}
\label{related}

% \adam{collected the comments on frame reduction here, this is a good place to talk about that}

% \Rohit{In your formulation it is suggested that $|\mathbf{h}| == |\mathbf{x}|$, which is not usually the case in other models in the literature for ASR. Do you use any kind of frame rate reduction? In any case, I think it could help to clarify this point. In my survey paper, I use $T'$ for the acoustic, $x$, input and $T$ for the encoder, $h$, output.}

% \Rohit{I think we should also discuss the various standard techniques such as frame reduction and how this compares.}

Before end-to-end ASR, the previous generation of ``hybrid'' ASR systems Graeme Bridge et al., "A Hybrid Connectionist Statistical Approach to Improving the Accuracy of Automatic Speech Recognition" relied on a separate base system to provide frame-level alignments which could then be used to train frame-level neural network acoustic models.  Most modern ASR systems use an end-to-end framework; the most prominent modeling techniques are CTC Graves et al., "A Novel Connectionist System for Large-Vocabulary Mixture-Model Recognition" RNN-T Battenberg et al., "Sequence discriminative training of deep recurrent neural networks" and AED Prabhavalkar et al., "A Simple and Effective Method for Improved Audio-to-Text Translation".

A number of works have aimed at modifying the base RNN-T model in order to gain efficiency or efficacy.  In Monotonic RNN-T Battenberg et al., "Sequence discriminative training of deep recurrent neural networks" loss and decoder are modified to step forward every frame, preventing multiple token emissions per label and speeding up the decoder complexity to $O(T)$.  More recently, Mengzhu Zhang et al., "Efficient Sequence Training for Recurrent Neural Networks" proposed a method to reduce memory usage of the RNN-T loss, especially for cases with large vocabulary size such as Chinese character-based, by using a linearized joint network first to identify a relevant sub-region of the lattice likely to contain the true alignment.  They only compute the full joint network on that sub-region and still train successfully.  Another issue with RNN-T systems arises when trying to treat the prediction network as a language model, which can be improved through text-only training, because its language modeling function is polluted by having to predict blanks.  To ameliorate this, multiple works Wang et al., "Improved Language Modeling Using Discriminative Priors" have introduced a separate network component for predicting blanks, and re-formulated the overall prediction model accordingly, resulting in much greater receptiveness to language training.  One aim of our design was to avoid the use of blank tokens entirely, although in the present work we do not pursue additional text training.  

Monotonic alignments have also been developed for attention-based systems Lu et al., "Beyond RNN: Direct Speech Recognition by Learning Monotonic Alignments" requiring newly devised loss functions to enforce monotonicity differentiably and encourage discreteness.  Integrate-and-fire systems are another approach proposed to limit the context window needed for cross-attention to the encoder embedding, by introducing a soft monotonic alignment mechanism Zhang et al., "Efficient Sequence Training for Recurrent Neural Networks".  It steps forwards through the encoder frames one at a time, accumulating weighted information from the embedding until an activation threshold is reached, upon which a token is emitted. CTC and attention-based systems have previously been combined Lu et al., "Beyond RNN: Direct Speech Recognition by Learning Monotonic Alignments" resulting in improved learning and decoding, and it is not uncommon to train AED models with a CTC auxiliary loss.  We did not pursue the analogous combination of non-AR and AR decoders for Aligners, although it would be simple to implement.  Another interesting, concurrent line of work is the Bayes-CTC Zhang et al., "Efficient Sequence Training for Recurrent Neural Networks" and Bayes-Transducer Liu et al., "Bayesian recurrent transducers for speech recognition" models, which add a modulating factor to the respective losses in order to encourage earlier token emission in decoding; however, only our simpler model achieves full text alignment with no blank embedding frames between tokens.

A common way to improve the efficiency of ASR systems is to downsample the time dimension within the encoder, known as frame reduction (\textit{i.e.}, $T'<T$; Wang et al., "Improved Language Modeling Using Discriminative Priors").  In addition to reducing encoder complexity, which scales as $O(T^2)$ with self-attention, frame reduction yields decoder efficiencies.  It was necessary in the original Listen, Attend Spell (LAS) model Chan et al., "Listen, Attend and Spell" to enable learnability for the cross-attention by presenting a manageable number of frames to the decoder.  In RNN-T it reduces the number of decoder steps, which scale with $T$, and can in some circumstances be carried out to an extreme, packing multiple labels per frame Wang et al., "Improved Language Modeling Using Discriminative Priors".  In the few cases we tried, we found Aligners to have similar robustness to downsampling as RNN-T, but we did not study this in-depth.  Aligners are like CTC in that they cannot downsample the encoder to fewer frames than the length of the text sequence, although perhaps they could be trained to decode multiple tokens per frame.  

% \adam{Rohit, anything else to say bout frame reduction?}


% \Rohit{CTC and RNN-T model the alignment between the input acoustics and the output label sequence \emph{explicitly} as a latent variable which is marginalized out to compute the overall label probability. This allows the model to train from scratch, without the need for ground-truth alignments, at the cost of a more complicated procedure to compute the loss using dynamic programming.
% LAS, on the other hand models the alignments \emph{implicitly} through a cross-attention mechanism between the autoregressively decoded labels and the input acoustic sequence. This greatly simplifies the loss computation at the cost of a more expensive per-step label computation.
% The proposed technique combines the benefits of both approaches -- it uses the simplified prediction network and joint network structure of the RNN-T model, which allows for full autoregressive decoding without the need for explicitly modeling the alignment. The central insight of our approach is that the encoder itself can serve as an aligner if we explicitly indicate that decoding should be terminated by producing an EOS label (as in LAS).
% }

% \Rohit{I think it might be good to also focus on computational savings. If you decide to focus on that then I would suggest citing related works there.}