\section{Related Work}
\subsection{Motion Capture}
Many applications such as gaming, bio-mechanical analysis, movie production, and emerging human-computer interaction paradigms such as Virtual and Augmented Reality (VR/AR) require a means to human motion capture (MoCap). Such applications impose three challenging constraints on pose reconstruction: (i) precise MoCap in real-time, (ii) work in everyday settings including in/outdoors and easy to access, (iii) minimally intrusive in terms of user experience \cite{zhou2021monocular, yi2021transpose, moeslund2006survey}.

We conducted a detailed discussion on the advantages and disadvantages of four different paradigms of MoCap methods, as outlined below, and summarized our findings in Fig. \ref{fig:related}. In summary, previous methods relied on a single type of sensor, limiting the potential for accurate motion capture. In contrast, our method, FIP, integrates both IMUs and flex sensors into loose-fitting clothing, achieving both accuracy and comfort in motion capture.

\subsubsection{Vision-based MoCap} Most commonly, the MoCap task is achieved using commercial marker-based systems like 
Vicon \cite{vicon_website}. These systems \cite{optiTrack_website, chatzitofis2021democap, ghorbani2021soma} achieve high accuracy by tracking markers on the body. However, they require costly infrastructure and physical markers placed on users, limiting accessibility in everyday contexts. Markerless single RGB or RGB-D cameras methods have gained attraction \cite{alldieck2018video, lin2023design, saini2023smartmocap, xu2022vitpose, li2022mhformer, shin2024wham, feng2023diffpose, lu2024rtmo, you2023co, dwivedi2024tokenhmr, ye2023distilpose, yoshiyasu2023deformable, xu2023vitpose++, yu2021function4d, pesavento2024anim, lu20233d, mehta2020xnect}, but similar to multi-camera systems \cite{zhang2021direct, wu2021graph, choudhury2023tempo, shuai2023reconstructing, Liao_2024_CVPR}, they are still highly sensitive to occlusions, lighting variations, and the actor's appearance. Additionally, many of these assume a static camera setting, limiting capture space and reducing effectiveness in dynamic or outdoor environments. These constraints hinder the broader application of optical motion capture. 


\subsubsection{Inertial-based MoCap} 
Inertial Measurement Units (IMUs) provide efficient and compact measurement for the orientation and acceleration of the human skeleton, facilitating accurate motion capture \cite{kamboj2024survey}. Commercial systems like Noitom \cite{noitom_website} and Xsens \cite{movella_website} achieve precise capture by embedding dense IMU arrays into tight-fitting bandages. However, the intrusive nature of these setups significantly restricts user comfort and accessibility. To address these challenges, recent research has focused on sparse-IMU solutions to balance functionality and user comfort. SIP reduced the IMU count to six, but limited to offline \cite{von2017sparse}. DIP introduced real-time performance using deep recurrent neural networks, but was constrained to local pose estimations \cite{huang2018deep}. TransPose improved global translations by incorporating foot-ground contact constraints \cite{yi2021transpose}. PIP \cite{PIPCVPR2022} and TIP \cite{jiang2022transformer} refine TransPose further: PIP ensures physical plausibility through physics-based optimization, while TIP enhances tracking accuracy using transformer architectures. PNP \cite{yi2024physical} improved the accuracy and robustness by compensating for fictitious forces in non-inertial frames. Furthermore, UIP \cite{armani2024ultra} combined IMUs with UWB to constrain drift and jitter, improving accuracy. IMUPoser \cite{mollyn2023imuposer} estimated pose by IMUs in devices like smartphones, smartwatches, and earbuds, eliminating the need for external sensors and further improving comfort and accessibility. However, its accuracy may decline when users lack or do not wear the required devices.

Despite these innovations, sparse-IMU systems still struggle with pose ambiguity, often misclassifying similar movements like sitting and standing, leading to errors in elbow and knee tracking. Moreover, they rely on tight straps for accuracy, compromising long-term comfort and wearability.

\subsubsection{Flexible Sensor-based MoCap}
Flexible sensors are emerging as promising solutions for long-term monitoring of human activities due to their insusceptibility to issues like lighting conditions, integration drift, or occlusions \cite{nag2017wearable, luo2022digital, li2024intelligent}. In addition, their inherent bio-compatibility, high stretchability, and lightweight design make them particularly suitable for monitoring of human physical status at extended duration, enhancing the overall user experience \cite{luo2021learning, luo2023technology}. Despite their comfort, flexible sensors may not yet match the precision of IMUs, as they are unable to directly measure the 3-axis orientation of bones, posing a less accurate full-body motion capture compared to inertial systems \cite{chen2023full}. However, their strength of directly measuring joint angles through skin deformation, renders them particularly accurate for joints with noticeable deformation, such as the elbow \cite{chen2023dispad, zuo2023self, jiaweisuda} and knee joints \cite{bergmann2013attachable, papi2018flexible, sheeja2023design}.

In general, their seamless integration into clothing ensures enhanced comfort and long-term wearability. Although flexible sensors have not yet surpassed IMUs in full-body motion capture accuracy, their exceptional capabilities in tracking knee and elbow joints offer a promising complement to inertial MoCap systems, potentially enhancing both accuracy and user satisfaction.


\subsubsection{Clothes-based MoCap}

Clothing has been indispensable to everyday life since humans first donned garments for warmth and protection during the Ice Age \cite{kittler2003molecular}. In the modern world, the always-on nature and portability of clothing make it an ideal medium for interacting with digital realities \cite{weiser1991computer}. Therefore, sensors are increasingly being embedded into garments to enable long-term MoCap. To obtain accurate data, mainstream approaches typically involve integrating either inertial sensors or flexible sensors at key joint locations within tight-fitting clothing. For instance, the TESLASUIT incorporates a dense array of IMUs (n=14) into its tight-fitting garment \cite{teslasuit_website}. On the other hand, Chen et al. demonstrate the use of six flexible sensors on an elbow pad to predict joint angles \cite{chen2023dispad}.

While tight-fitting clothing-based motion capture has shown promise, solutions for loose-fitting garments remain underexplored. LIP demonstrates robust accuracy by integrating a sparse set of IMUs (n=4) into a loose-fitting jacket \cite{zuo2024loose}, but its performance on the elbow joint is limited due to the sparse sensor placement. Other approaches, such as MoCaPose \cite{zhou2023mocapose} and SeamPose \cite{yu2024seampose}, incorporate capacitive sensors into loose garments for motion capture. However, these methods rely on a single sensor type, restricting their accuracy and adaptability. In general, as shown in Tab. \ref{tab:capacitive}, our approach achieves accurate and comforatble motion capture paradigm at a smooth 60 Hz by leveraging both IMUs and flex sensors in everyday clothing. This multimodal integration is intuitive, as garments naturally serve as an ideal interface for sensor fusion.


% \subsection{Further Comparison with Loose-fitting Clothes-based MoCap Methods}
% \paragraph{Capacitive-based} 
% Tab. \ref{tab:capacitive} compares FIP with capacitive sensor-based methods, such as MoCaPose and SeamPose. Due to differences in sensors and datasets, the comparison serves as a rough estimate. Results indicate that FIP offers advantages in accuracy, higher frame rates, and fewer sensors. Additionally, compared to capacitive sensors made from conductive fabric or yarn, the commercial IMUs and flex sensors we use are more stable and consistent, which could offer advantages for future large-scale applications.

% \paragraph{Inertial-based}
% Our method introduces two key innovations compared to LIP: 1) Sensor Fusion: We combine flex and IMU sensors with PIC and PFP methods to improve MoCap accuracy while maintaining comfort, as shown in Tab. \ref{tab:compare}. 2) IMU Displacement Generator: Unlike LIP's model, which assumes a close match between simulated and real data, our DLDM uses LDM to generate IMU displacements with greater fidelity. This enables our method to outperform LIP even without the flex sensor, as demonstrated in Tab. \ref{tab:woflex}.

\begin{table}[h]
\caption{Comparison with other methods in loose-fitting clothing. Our FIP method demonstrates similar, or even superior, motion capture performance with higher frame rates.}
\label{tab:capacitive}
\begin{tabular}{ccccc}
\toprule
    Method & Sensor Types & FPS & Positional Error (cm) \\ \midrule
    MoCaPose & capacitive sensors    & 30* & 8.80 \\ 
    SeamPose & capacitive sensors    & 30* & 8.60 \\ 
    LIP      & IMUs                  & 30  & 9.99 ± 4.19 \\ 
    Ours     & IMUs and flex sensors & 60  & 8.06 ± 2.87 \\ \bottomrule
\multicolumn{4}{l} {\footnotesize *: According to the dataset.}
\end{tabular}
\end{table}


\begin{figure*}[ht]
% \begin{figure}[bp]
% \centering
% \hspace{=0.01cm}
\centering
% \flushleft
\includegraphics[width=1.0\linewidth]{figure/Fig_related_work2.png}
\caption{Comparison of related MoCap methods.}
\label{fig:related}
% \setlength{\abovedisplayskip}{0pt}
% \setlength{\belowdisplayskip}{0pt}
\end{figure*}




\subsection{Algorithms for Sensor Displacements}
Sensor displacements occur when sensors shift from their intended configuration, resulting in altered positioning that can challenge pre-trained pattern recognition models. To address this, researchers have employed transfer learning techniques \cite{wang2018deep, qin2019cross,kamboj2024fusion}, such as domain adaptation \cite{chang2020systematic, zhao2020local} and self-supervised learning \cite{jain2022collossl, deldari2022cocoa, haresamudram2022assessing}, to extract invariant features from displaced sensor data \cite{kunze2008dealing, wu2018orientation}. For IMUs, static pose calibrations, like A-pose and T-pose \cite{jiang2022transformer, yi2021transpose,noitom_website}, are commonly used to eliminate the initial relative rotation bias between the body and sensors, referred to as \textit{Primary Displacement}. On another front, LIP generates synthetic loose IMU data to address \textit{Real-time Displacement}. However, LIP relies on the assumption that the distributions of loose and tight IMU data are sufficiently similar, which may lead to the generated displacement distribution diverging from real-world conditions \cite{zuo2024loose}.

For flexible sensors, Zuo et al. proposed a self-adaptive algorithm that fine-tunes and aligns distributions to handle \textit{Real-time Displacement} \cite{zuo2023self}. DisPad leverages fuzzy entropy to identify the closest matching distribution to the current displacement, effectively managing both circular and lateral sensor shifts \cite{chen2023dispad}. Meanwhile, SuDA introduced a Sim2Real approach that aligns the support of source and target domains, rather than their distributions, achieving results comparable to supervised learning without the requirements for real labeled data \cite{jiaweisuda}.

To sum up, while these methods have been successful in addressing single-sensor displacement, handling displacement across multiple sensors remains a significant challenge. In this work, we propose targeted algorithms for both flex and inertial sensors, designed to account for their distinct displacement characteristics and ensure robust performance in everyday garments.

\subsection{Generative AI}


% Diffusion Models (DMs) are powerful generative models which applied to various areas.Traditionally, DMs are introduced in a discrete-step fashion: Given samples  $\mathbf x_0 \sim q(\mathbf x_0)$ from a data distribution, DMs use a Markovain fixed forward diffusion process defined as 
% \begin{equation}
% \begin{aligned}
% q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right):=\prod_{t=1}^{T} 
% q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right), 
% \quad q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t_1}\right):=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{1-\beta_{t}} \mathbf{x}_{t-1}, \beta_{t} \boldsymbol{I}\right)    
% \end{aligned}
% \end{equation}

% where $T $ denotes the number of steps and $q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)$ is a Gaussian transition kernel, which gradually adds noise to the input with a variance schedule $\beta_1,...,\beta_T$. The $\beta_t$ are chosen such that the chain approximately converges to a standard Gaussian distribution after $T$ steps, $q(\mathbf x_T)=\mathcal{N}\left(\mathbf{x}_{T} ; \mathbf0, \boldsymbol{I}\right)$. DMs learn a parameterized reverse process (model parameters $\boldsymbol{\theta} $ ) that inverts the forward diffusion:

% \begin{equation}
% \begin{aligned}
% \label{eq:sm}
% p_{\boldsymbol{\theta}}(\mathbf{x}_{0:T}):=p(\mathbf{x}_{T})\prod_{t=1}^{T}p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}|\mathbf{x}_{t}), \quad p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}|\mathbf{x}_{t}):=\mathcal{N}(\mathbf{x}_{t-1};\mu_{\boldsymbol{\theta}}(\mathbf{x}_{t},t),\rho_{t}^{2}\boldsymbol{I})
% \end{aligned}
% \end{equation}

% This generative reverse process is also  Markovian with Gaussian transition kernels, which use fixed variances $\rho_{t}^{2}$. DMs can be interpreted as latent variable models, where $\mathbf x_1,...,\mathbf x_T$ are latents, and forward process $q(\mathbf x_{1:T}|\mathbf x_0)$ acts as a fixed approximate posterior, to which the generative $p_{\boldsymbol{\theta}}(\mathbf x_{0:T})$ is fit. DMs are trained by minimizing the variational upper bound on the negative log-likelihood of the data $\mathbf x_0$ under $p_{\boldsymbol{\theta}}(\mathbf x_{0:T})$. Up to irrelevant constant terms, this objective can be expressed as 

% \begin{equation}
% \begin{aligned}
% \min\limits_{\boldsymbol{\theta}}\mathbb{E}_{t\sim U\{1,T\},
% \mathbf{x}_{0}\sim p(\mathbf{x}_{0}),\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\boldsymbol{I})}
% \left[w(t)||\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\alpha_{t}\mathbf{x}_{0}+\sigma_{t}\boldsymbol{\epsilon},t)||_{2}^{2}\right], w(t)=\frac{\beta_{t}^{2}}{2\rho_{t}^{2}(1-\beta_{t})(1-\alpha_{t}^{2})},
% \end{aligned}
% \end{equation}

% where $\alpha_t = \sqrt{\prod_{s=1}^t(1-\beta_s)}$ and $\sigma_{t} = \sqrt{1-\alpha_{t}^{2}}$ are the parameters of the tractable diffused distribution after $t$ steps $q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{t};\alpha_{t}\mathbf{x}_{0},\sigma_{t}^{2}\boldsymbol{I})$. Furthermore, Eq. (3) employs the widely used parameterization  $\mu_{\boldsymbol{\theta}}(\mathbf{x}_{t},t):= {\frac{1}{\sqrt{1-\beta_{t}}}}\left(\mathbf{x}_{t}-{\frac{\beta_{t}}{\sqrt{1-\alpha_{t}^{2}}}}\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)\right)$. It is common practice to set $w(t)=1$, instead of the one in Eq. (3), which often promotes perceptual quality of the generated output. In the objective of Eq. (3), the model $\boldsymbol {\epsilon_\theta}$ is  for all possible steps $t$ along the diffusion process, effectively trained to predict the noise vector $\boldsymbol \epsilon$ that is necessary to denoise an observed diffused sample $\mathbf{x_t}$. After training, the DM can be sampled with ancestral sampling in an interactive fashion:

% \begin{equation}
% \label{eq:sample}
% \mathbf{x}_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}^{2}}}\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t))+\rho_{t}\boldsymbol{\eta},
% \end{equation}

% where $\boldsymbol\eta\sim\mathcal{N}(\boldsymbol{\eta};\boldsymbol{0},\boldsymbol{I}).$ This sampling chain is initialized from a random sample $\mathbf{x}_{T}\sim\mathcal{N}(\mathbf{x}_{T};\mathbf{0},\boldsymbol{I}).$ 
Generative AI has widespread applications across various fields, including image \cite{openai_dalle3, openai_dalle2}, speech \cite{cao2024survey}, and text generation \cite{openai_chatgpt}, with profound impact on the Human-Computer Interaction (HCI) community \cite{muller2022genaichi}. Variational Autoencoders (VAEs) \cite{kingma2013auto} and Diffusion Models (DMs) \cite{ho2020denoising}, both of which are designed to learn underlying data distributions and generate new samples from them. 

For VAEs, the model optimizes the Evidence Lower Bound (ELBO), formulated as:
\begin{equation}
\label{eq:sample}
\mathcal{L}(\xi, \phi; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\xi(x|z)] - D_{KL}(q_\phi(z|x) \parallel p(z)),
\end{equation}
where \(q_\phi(z|x)\) is the approximate posterior, \(p_\xi(x|z)\) is the likelihood of generating the data, and \(D_{KL}\) is the Kullback-Leibler divergence that regularizes the latent space to follow a prior distribution.

Diffusion Models, in contrast, work by modeling the reverse process of a gradual noise addition. The forward process adds Gaussian noise to the data over \(T\) timesteps, modeled as:
\begin{equation}
q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I)
\end{equation}
where \(\beta_t\) controls the noise schedule. The model learns the reverse process, recovering the original data by predicting the noise at each step:
\begin{equation}
\label{eq:sample}
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
\end{equation}
By minimizing the difference between the added noise and the predicted noise, the model can generate data by starting from pure noise and iteratively denoising it.

Both VAEs and Diffusion Models are pivotal techniques in generative AI, driving advancements in generating high-quality, complex data across various domains. In our application, we seek to harness the capabilities of generative AI to create a diverse range of IMU displacements, which could enhance the robustness of pose estimation models.