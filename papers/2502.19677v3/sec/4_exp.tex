\section{Experiments}

In this section, we outline the experimental settings and present both qualitative and quantitative comparisons between DHNet and other state-of-the-art methods. We then conduct ablation studies to highlight the effectiveness of our approach. The best and second-best results in the tables are indicated in \textbf{bold} and \underline{underlined} formats, respectively.



\subsection{Experimental Settings}

\subsubsection{\textbf{Training details}}
We train separate models for different tasks, and unless otherwise specified, the following parameters are utilized.  In our model, $N_{i \in [1,2,3,4,5,6,7,8,9]}$ are set to $\{1,1,1,28,1,1,1,1,1\}$. In terms of VBlock, we set $Q = 4$ (Eq.~\ref{vb2}).
In terms of DDRE, we set $ S = 5$ for the number of experts and $T = 4$ for the number of routers. 
We use the Adam optimizer~\cite{2014Adam} with parameters $\beta_1=0.9$ and $\beta_2=0.999$. The initial learning rate is set to $5 \times 10^{-4}$ and gradually reduced to $1 \times 10^{-6}$ using the cosine annealing strategy~\cite{2016SGDR}. The batch size is chosen as $32$, and patches of size $256 \times 256$ are extracted from training images. Data augmentation includes both horizontal and vertical flips. Given the high complexity of image motion deblurring, we set the number of channels to 32 for DHNet and 64 for DHNet-B. 
In DHNet, we do not use a pre-trained model in the DDRE, whereas DHNet-B utilizes UFPNet~\cite{UFPNetFang_2023_CVPR}.

\begin{figure*} % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{mgopr.jpg}
	\caption{Image motion deblurring comparisons on the GoPro dataset~\cite{Gopro}. Our DHNet recovers image with clearer details. }
	\label{fig:blurm}
\end{figure*}

\subsubsection{Datasets}


We validate the effectiveness of our method using the GoPro dataset~\cite{Gopro}, which consists of 2,103 training image pairs and 1,111 evaluation pairs, in line with recent approaches~\cite{FSNet}. To evaluate the generalizability of our model, we  apply the GoPro-trained model to the HIDE~\cite{HIDE} dataset, containing 2,025 images specifically designed for human-aware motion deblurring. Both the GoPro and HIDE datasets are synthetically generated. Additionally, we assess our method's performance on real-world images using the RealBlur~\cite{realblurrim_2020_ECCV} dataset, which contains 3,758 training image pairs and 980 testing pairs, divided into two subsets: RealBlur-J and RealBlur-R.




\begin{table}
\centering
\caption{Quantitative evaluations of the proposed approach against state-of-the-art motion deblurring methods. Our DHNet and DHNet-B are trained only on the GoPro dataset. \label{tb:deblurgh}}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
    \hline
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{GoPro}  & \multicolumn{2}{c}{HIDE} 
    \\
   Methods & PSNR $\uparrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & SSIM $\uparrow$   
    \\
    \hline\hline
    MPRNet~\cite{Zamir2021MPRNet} & 32.66 & 0.959 & 30.96 & 0.939 
    \\
    Restormer~\cite{Zamir2021Restormer} & 32.92 & 0.961 & 31.22 & 0.942 
    \\
     Uformer~\cite{Wang_2022_CVPR} &32.97 & 0.967 &30.83 &\underline{}{0.952} 
     \\
       NAFNet-32~\cite{chen2022simple}&32.83&0.960&-&-
    \\
    NAFNet-64~\cite{chen2022simple}&33.62&0.967&-&-
    \\
    IRNeXt~\cite{IRNeXt} &33.16 &0.962 &- & - 
    \\
    SFNet~\cite{SFNet} &33.27 &0.963 &31.10 & 0.941 
    \\
    DeepRFT+~\cite{fxint2023freqsel} &  33.23 &0.963 &31.66 &0.946
    \\
    UFPNet~\cite{UFPNetFang_2023_CVPR} &34.06 &0.968 &31.74 &0.947
    \\
    MRLPFNet~\cite{MRLPFNet} &34.01 &0.968 &31.63 &0.947
    \\
     MambaIR~\cite{guo2024mambair}&33.21 &0.962 &31.01 &0.939
     \\
     ALGNet-B~\cite{alggao2024learning} &34.05 &0.969 &31.68 &\underline{0.952}
     \\
     MR-VNet~\cite{MR-VNet} & 34.04 & 0.969 & 31.54 & 0.943
     \\
    FSNet~\cite{FSNet} &33.29&0.963 &31.05 & 0.941 
    \\
    AdaRevD-B~\cite{AdaRevD} & 34.50 &0.971 &32.26 &\underline{0.952}
    \\
    AdaRevD-L~\cite{AdaRevD} &\underline{34.60} &\underline{0.972} &\underline{32.35} &\textbf{0.953} 
    \\
    \hline
    \textbf{DHNet(Ours)}& 33.28	&0.964	&31.75	&0.948
    \\
    \textbf{DHNet-B(Ours)} & \textbf{34.75} & \textbf{0.973} & \textbf{32.37} & \textbf{0.953}
    \\
    \hline
\end{tabular}}
\end{table}




\begin{table}
\centering
\caption{ Quantitative evaluations of the proposed approach against
state-of-the-art methods on the real-word dataset RealBlur~\cite{realblurrim_2020_ECCV}. }
\label{tb:0deblurringreal}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
    \hline
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{RealBlur-R}  & \multicolumn{2}{c}{RealBlur-J} 
    \\
   Methods & PSNR $\uparrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & SSIM $\uparrow$   
    \\
    \hline\hline
    DeblurGAN-v2~\cite{deganv2} & 36.44 & 0.935& 29.69& 0.870
\\
    MPRNet~\cite{Zamir2021MPRNet} & 39.31 & 0.972 & 31.76 & 0.922
   \\
   DeepRFT+~\cite{fxint2023freqsel}&39.84 &0.972 &32.19 &0.931
\\
Stripformer~\cite{Tsai2022Stripformer} & 39.84 & 0.975 & 32.48 & 0.929
\\
FFTformer~\cite{kong2023efficient}&40.11& 0.973 &32.62 &0.932
\\
UFPNet~\cite{UFPNetFang_2023_CVPR} &40.61 &0.974 &33.35 &0.934 
\\
 MambaIR~\cite{guo2024mambair}& 39.92 & 0.972 & 32.44 & 0.928
 \\
 ALGNet~\cite{alggao2024learning} & 41.16 &0.981 &32.94 &0.946
 \\
 MR-VNet~\cite{MR-VNet} & 40.23 & 0.977 &32.71 & 0.941
 \\
 AdaRevD-B~\cite{AdaRevD} &41.09 &0.978 &33.84 &0.943
 \\
 AdaRevD-L~\cite{AdaRevD} &41.19 &0.979 &\underline{33.96} &0.944
 \\
 \hline
    \textbf{DHNet(Ours)}& \underline{41.30} & \textbf{0.984} & 33.78 & \underline{0.952}
    \\
     \textbf{DHNet-B(Ours)}& \textbf{41.33} & \underline{0.983} & \textbf{34.28} & \textbf{0.953}
    \\
    \hline
\end{tabular}}
\end{table}





\subsection{Experimental Results}
\subsubsection{ \textbf{Evaluations on the synthetic dataset.}}
We present the performance of various image deblurring methods on the synthetic GoPro~\cite{Gopro} and HIDE~\cite{HIDE} datasets in Table~\ref{tb:deblurgh}. Overall, our DHNet outperforms competing approaches, yielding higher-quality images with superior PSNR and SSIM values. Specifically, compared to our baseline model, NAFNet~\cite{chen2022simple}, DHNet improves performance by 0.45 dB and 1.13 dB with 32 and 64 channels, respectively. In comparison to the previous best method, AdaRevD-L~\cite{AdaRevD}, our DHNet-B shows an improvement of 0.15 dB on the GoPro~\cite{Gopro} dataset. Notably, although our model was trained exclusively on the GoPro~\cite{Gopro} dataset, it still achieves state-of-the-art results (32.37 dB in PSNR) on the HIDE~\cite{HIDE} dataset, demonstrating its excellent generalization capability.
Furthermore, Figure~\ref{fig:param} shows that DHNet not only achieves state-of-the-art performance but also reduces computational costs. The performance continues to improve with an increase in model size, highlighting the scalability of our approach. Finally, Figure~\ref{fig:blurm} presents deblurred images from the evaluation methods, where our model produces more visually pleasing results.

\begin{figure*} % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{mreal.jpg}
	\caption{Image motion deblurring comparisons on the RealBlur dataset~\cite{realblurrim_2020_ECCV}. Our DHNet recovers perceptually faithful images. }
	\label{fig:realm}
\end{figure*}

\subsubsection{ \textbf{Evaluations on the real-world dataset.}}
In addition to evaluating on synthetic datasets, we further assess the performance of our DHNet on real-world images from the RealBlur dataset~\cite{realblurrim_2020_ECCV}. As shown in Table~\ref{tb:0deblurringreal}, our method produces deblurred images with superior PSNR and SSIM scores. Specifically, compared to the previous best method, AdaRevD-L~\cite{AdaRevD}, our approach achieves improvements of 0.14 dB and 0.32 dB on the RealBlur-R and RealBlur-J datasets, respectively. Figure~\ref{fig:realm} demonstrates that DHNet produces clearer images with finer details and structures, outperforming the competing methods.




\begin{table}
    \centering
       \caption{Ablation study on individual components of the
proposed DHNet.}
    \label{tab:abl1}
    
    \begin{tabular}{ccccc}
    \hline
         Net&VBlock& DDRE  & PSNR & $\triangle$ PSNR
         \\
         \hline
         (a)& &  &    33.62 & -
         \\
         (b)& \ding{52}&     & 34.05 & +0.43
         \\
         (c)&  &  \ding{52}  &  34.32 & +0.70
         \\
         (d)& \ding{52}&    \ding{52}&  34.75  & +1.13
         \\
         \hline
    \end{tabular}
\end{table}



\subsection{Ablation Studies}
In this section, we first demonstrate the effectiveness of the proposed modules and then investigate the effects of different designs for each module. 

\subsubsection{Effects of Individual Components} 

To evaluate the effectiveness of each module, we use NAFNet~\cite{chen2022simple} as our baseline model and subsequently replace or add the modules we have designed. As shown in Table~\ref{tab:abl1}(a), the baseline achieves  33.62 dB PSNR. Each module combination leads to a corresponding performance improvement. Specifically, replacing NAFBlock~\cite{chen2022simple} with our VBlock enhances the performance by 0.43 dB (Table~\ref{tab:abl1}(b)). Adding the DDRE module to the original NAFNet~\cite{chen2022simple} can significantly boost the model's performance from 33.62 dB to 34.32 dB (Table~\ref{tab:abl1}(c)). When all modules are combined (Table~\ref{tab:abl1}(d)), our model achieves a 1.13 dB improvement over the original baseline.

\begin{figure*}[htb] % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{vsg.jpg}
	\caption{Effect of the proposed VBlock. Zoom in for the best view.}
	\label{fig:vbabl}
\end{figure*}

\begin{table}
    \centering
       \caption{Results of alternatives to VBlock.}
    \label{tab:vbl1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccc}
    \hline
         Net& SG~\cite{chen2022simple} &Volterra& Q  & PSNR & $\triangle$ PSNR &MACs(G)
         \\
         \hline
         (a)& \ding{52} &  &0  &34.12 & - & 106
         \\
         (b)& & \ding{52} &1    &34.25 & +0.13 &107
         \\
         (c)&  &  \ding{52} & 2  & 34.42 & +0.30 &109
         \\
         (d)& &    \ding{52}& 4 & 34.75 & +0.63 & 111
         \\
         (e)& &    \ding{52}& 8 &34.76 & +0.64  & 119
         \\
         \hline
    \end{tabular}}
\end{table}


\begin{figure*}[htb] % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{diffe.jpg}
	\caption{Effect of the proposed DDRE. Zoom in for the best view.}
	\label{fig:ddreabl}
\end{figure*}
\subsubsection{Design Choices for VBlock} 
VBlock facilitates the generation of nonlinear interactions through the interactions between image pixels. To evaluate the effectiveness of VBlock, we first use SG~\cite{chen2022simple} to replace the Volterra kernel and then examine the impact of varying the kernel rank. As shown in Table~\ref{tab:vbl1}, the SG~\cite{chen2022simple} achieves a PSNR of 34.12 dB, and performance improves when we implement our Volterra kernel. We further visualize the feature maps in Figure~\ref{fig:vbabl} to highlight the advantages of our Volterra kernel compared to SG~\cite{chen2022simple}. The results clearly demonstrate that features obtained with the Volterra kernel are more detailed (as indicated by the red box) and better suited for handling complex scenes. Additionally, we analyze the effect of the $Q$ value on the performance. As we increase the rank $Q$, performance consistently improves, but this also increases system complexity. To achieve a balance between efficiency and performance, we choose an experimental setting with $Q = 4$.  




\subsubsection{Design Choices for DDRE} 

To evaluate the effectiveness of DDRE, we conduct experiments with various model variants, as shown in Table~\ref{tab:ddrel1}. When we introduce experts (Table~\ref{tab:ddrel1} (b)) into the baseline model (Table~\ref{tab:ddrel1} (a)), performance improves from 34.05 to 34.26, highlighting the model's capability to address degradation in a differentiated manner. Further enhancement occurs when we incorporate external degradation pattern knowledge, leading to even greater performance gains (Table~\ref{tab:ddrel1} (c) and (d)). Additionally, we observe that different pre-trained models (NAFNet~\cite{chen2022simple}, UFPNet~\cite{UFPNetFang_2023_CVPR}) yield varying performance outcomes. We also investigate the impact of the number of routers and experts on the experimental results. Our findings indicate that the combination of $ S = 5$ and $ T = 4$ (Table~\ref{tab:ddrel1} (d)) yields the best results. A smaller number (Table~\ref{tab:ddrel1} (e) (g)) fails to adequately identify the degradation degree. While an excessive number (Table~\ref{tab:ddrel1} (f) (h)) tends to focus on the severely degraded regions, and the lightly degraded regions will receive relatively less attention, ultimately hindering their restoration. 
To further demonstrate the effectiveness of our DDRE module, we visualize the features of a routing path weighted over all experts, as shown in Figure~\ref{fig:ddreabl}. It is clear that each expert handles the inconsistent degradation region as well as  the degree of restored degradation.

\begin{table}
    \centering
       \caption{Results of alternatives to DDRE, where $S$ and $T$ denote the number of experts and routers, respectively.}
    \label{tab:ddrel1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccc}
    \hline
         \multirow{2}{*}{Net}&\multicolumn{2}{c}{Pre-trained}& \multirow{2}{*}{T}  & \multirow{2}{*}{S}& \multirow{2}{*}{PSNR} & \multirow{2}{*}{$\triangle$ PSNR}
         \\
         & NAFNet~\cite{chen2022simple}&UFPNet~\cite{UFPNetFang_2023_CVPR} & & & & 
         \\
         \hline
         (a)& &   &0  &0  & 34.05 & -
         \\
         (b)& &  &4  &5 &34.26 & +0.21
         \\
         (c)&\ding{52}  &  &4  & 5  & 34.47 & +0.35
         \\
         (d)& &  \ding{52}& 4 & 5 &34.75 & +0.70
         \\
         (e)& &  \ding{52}&4 &3 &34.56 & +0.51
         \\
         (f)& &  \ding{52}&4 &8 &34.61 & +0.56
          \\
         (g)& &  \ding{52}&2 &5 &34.51 & +0.46
         \\
          (h)& &  \ding{52}&6 &5 &34.57 & +0.52
         \\
         \hline
    \end{tabular}}
\end{table}

% 证明确实每个专家关注的区域不同