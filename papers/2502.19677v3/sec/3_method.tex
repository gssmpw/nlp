\begin{figure*}[htb] % use float package if you want it here
	\centering
	\includegraphics[width=1\textwidth]{network.jpg}
	\caption{The overall architecture of the proposed differential handling network (DHNet)  for image deblurring mainly consists of the differential handling block (DHBlock), which includes the Volterra block (VBlock) and the degradation degree recognition expert module (DDRE). DDRE is shown in the one-router case for clarity.}
	\label{fig:network}
\end{figure*}



\section{Method}
In this section, we begin with an overview of the entire DHNet pipeline. Following that, we delve into the details of the proposed differential handling block (DHBlock), which serves as the fundamental building unit of our method. This block primarily consists of two key components: the Volterra block (VBlock) and the degradation degree recognition expert module (DDRE).

\subsection{Overall Pipeline} 
Our proposed DHNet, depicted in Figure~\ref{fig:network}, employs a hierarchical encoder-decoder architecture to facilitate effective hierarchical representation learning. Each encoder-decoder includes a DHBlock, which consists of $N_{i \in [1,...,9]}$ VBlocks and a degradation degree recognition expert module (DDRE).
Given a degraded image $\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$, DHNet first applies convolution to extract shallow features $\mathbf{F_{s}} \in \mathbb{R}^{H \times W \times C}$ (where $H$, $W$, and $C$ represent the height, width, and number of channels of the feature map, respectively). 
These shallow features pass through a four-scale encoder sub-network, where the resolution is progressively reduced while the number of channels increases. The resulting in-depth features then move to a middle block, and the deepest features are processed by a four-scale decoder, which gradually restores them to their original size. Finally, we apply convolution to the refined features to generate a residual image $\mathbf{X}\in \mathbb R^{H \times W \times 3}$. This residual image is added to the degraded image to produce the restored image:  $\mathbf{\hat{I}} = \mathbf{X} +\mathbf{I}$.  



\subsection{Differential Handling Block}
While existing image deblurring methods have demonstrated commendable performance, they often rely on stacking numerous nonlinear activation functions to approximate nonlinear properties, neglecting the varying degradation degrees across different blurred regions. To address this issue, we design the differential handling block (DHBlock) as a feature extractor, enabling differential processing of diverse blur regions and utilizing Volterra kernel to capture complex input-output relationships. Formally, given the input features at the $(l-1)_{th}$ block $X_{l-1}$, the procedures of DHBlock can be defined as:
\begin{equation}
\begin{aligned}
\label{eq:dh1}
    X_l^{'} &= VBlock_{N_i}(...(VBlock_1(X_{l-1}))...)
    \\
    X_l &= DDRE(X_l^{'}) 
\end{aligned}
\end{equation}
where $N_i$ represents the number of VBlocks in the DHBlock, while  $X_l^{'}$ and $X_l$ denote the outputs from the   $N_i$  Volterra blocks (VBlocks) and the degradation degree recognition expert module (DDRE), which are detailed below.

\subsection{Volterra Block}
The strong performance and versatility of deep learning-based image deblurring models~\cite{FSNet,AdaRevD} can be attributed to their nature as universal approximators. They achieve this by stacking numerous nonlinear activation functions, allowing them to fit any nonlinear function. 
In order to reduce the system complexity caused by an excess of  nonlinear activation functions,  we design a Volterra block (VBlock) that utilizes the Volterra kernel to explore non-linearity within the network. Instead of relying on traditional nonlinear activation functions, VBlock employs higher-order convolutions to enhance linear convolution by enabling interactions between image pixels. As shown in Figure~\ref{fig:network}, unlike MR-VNet~\cite{MR-VNet}, which treats the Volterra filter as a complete block, our VBlock incorporates it as part of a block. The VBlock first captures local features using LN, 1x1, and 3x3 convolutions before entering the Volterra filter, and then applies two 3x3 convolutions for each order branch.  Specifically, our VBlock takes an input tensor $F$, first applies layer normalization (LN), and then encodes channel-wise context by utilizing 1×1 convolutions followed by 3×3 depth-wise convolutions to obtain the feature $F_1$ as follows:
\begin{equation}
\label{vb1}
    F_1 = W_d^0W_p^0LN(F)
\end{equation}
where $W_p^{(\cdot)}$ denotes the $1 \times 1$ point-wise convolution, and $W_d^{(\cdot)}$ represents the $3 \times 3$ depth-wise convolution. 

The resulting feature $F_1$ is then processed by the second-order Volterra kernel to capture complex input-output relationships through interactions between image pixels. In this paper, we compute the second-order Volterra kernel as a product of traditional correlation operations. This representation can achieve an approximation error with arbitrary precision. Moreover, the separability assumption of the kernels enables efficient computation, which is especially advantageous in network settings.
The process is as follows:
\begin{equation}
\begin{aligned}
\label{vb2}
    F_2 &=  X_1 \oplus X_2
    \\
    &= W_d^1 F_1 \oplus (\sum_{q=1}^Q X_2^{aq} \otimes X_2^{bq}) 
    \\
    &= W_d^1 F_1 \oplus (\sum_{q=1}^Q W_d^{2aq} F_1 \otimes W_d^{2bq} F_1)
\end{aligned}
\end{equation}
where $W_d^{2aq}$ and $W_d^{2bq}$ are learnable weight matrices, $Q$ represents the  desired rank of approximation.

\textbf{Theorem 1.}  Any continuous function can be approximated using a Volterra kernel. 

\textbf{Proof.}  Any continuous nonlinear function can be approximated by a polynomial. Similarly to VNNs~\cite{roheda2024volterra}, using a Taylor expansion at $x_0$, the nonlinear function $\sigma(\cdot)$ can be represented as:
\begin{equation}
\begin{aligned}
\label{tp1_1}
    \sigma_t(x) &= f(x_0) + ... + \frac{f^{n}(x_0)}{n!}(x-x_0)^n + R_n(x)
    \\ 
     &= \alpha_0 + \alpha_1x + ...+ \alpha_n x^n + R_n(x)
\end{aligned}
\end{equation}

For a memoryless higher-order Volterra kernel, it can be formulated as:
\begin{equation}
\label{tp1_2}
    \sigma_v(x)  = h_0 + h_1x + ...+ h_n x^n
\end{equation}
where $h_n$ represents the $n_{th}$ order weight, which is learned during the training process. The function $\sigma_v(x)$ can be considered an $n_{th}$ order approximation of $\sigma_t(x)$, with the error defined as:
\begin{equation}
\label{tp1_3}
    error = (\alpha_0 - h_0) + ...+  (\alpha_n - h_n)x^n + R_n(x)
\end{equation}

For a finite polynomial, the absolute error of the expansion can be quantitatively expressed using the Taylor Remainder as follows:
\begin{equation}
\label{tp1_4}
    |error| \leq R_n(x) =  \frac{f^{n+1}(\xi)}{(n+1)!} (x-x_0)^{n+1}
\end{equation}
where $\xi \in (x_0, x)$. Due to the complexity of higher-order Volterra kernels, this paper utilizes a cascade of second-order Volterra kernels to approximate the higher-order behavior (refer to supplementary material for the proof).

\textbf{Theorem 2.} A Volterra kernel provide more adaptive representation than that possible by activation functions.

\textbf{Proof.} For nonlinear activation functions such as ReLU, sigmoid, and tanh, the coefficients $\alpha_n$ in their Taylor expansions are predetermined\footnote{The term "predetermined" refers to a fixed value that remains constant.}. Specifically, a sigmoid activation can be approximated as:
\begin{equation}
\label{tp2_1}
    \sigma_s(x)  = \frac{1}{1+e^{-x}} = \frac{1}{2} + \frac{1}{4}x - \frac{1}{48}x^3 +...
\end{equation}
The expansion coefficient $h_n$ of the Volterra kernel can be continuously adjusted and learned during training, allowing for greater adaptability and more precise mapping of complex scenes. As a result, the feature $F_2$ in Eq.~\ref{vb2} exhibits nonlinear characteristics and offers greater flexibility compared to features obtained through  nonlinear activation functions (e.g. sigmoid). Finally, we apply channel attention to $F_2$ to obtain the final output features $F_3$ of VBlock as follows:
\begin{equation}
\label{vb3}
    F_3 = W_p^2(F_2 \otimes (W_p^1GAP(F_2))) \oplus F
\end{equation}



\subsection{Degradation Degree Recognition Expert Module}
While existing image deblurring methods~\cite{MR-VNet} have demonstrated strong performance, they overlook the inconsistency in the degradation degree across different regions. As illustrated in Figure~\ref{fig:ques}, the degree of degradation in a blurred image can differ significantly between areas.  Thus, it is clearly unreasonable for these methods to assume that all degraded areas share the same degree of degradation. 
In contrast to AdaRevD~\cite{AdaRevD}, which relies on a limited set of predefined categories and a fixed blur patch size, DDRE first incorporates prior knowledge from a well-trained model to estimate the spatially variable blur information. This enables DDRE to allow the router to map the learned degradation representation and assign weights to experts according to both the degree of degradation and the size of the regions.  

Different from the conventional mixture of experts~\cite{moeYu2024BoostingCL, Lifelong-MoE} that select only one router and a subset of experts for each router, our DDRE utilizes all routing paths, dynamically assigning weights to each expert along every router. Each expert comprises convolutions with varying receptive field sizes, allowing for the identification of degraded regions of different sizes. For simplicity, Figure~\ref{fig:network} illustrates a scenario with just one router. Given an input feature map $F_4$, we first integrate prior knowledge from a well-trained model and keeps it frozen during training for lower memory consumption as:
\begin{equation}
\label{ddre1}
    F_5 = W_p^3[F_4, P]
\end{equation}
where $P$ is the prior knowledge and $[\cdot]$ denotes the concatenation. The feature map $F_5$ that aggregate the degradation information is then input to each router, which assigns weights to each expert as follows:
\begin{equation}
\label{ddre2}
    F_6 = [W_j^k E_k(F_5)] 
\end{equation}
where $W_j^k$  represents the learnable weight coefficient assigned by each router $j \in \{1,2,...,T\}$ to each expert $k \in \{1,2,...,S\}$, and $E$ is the expert operations. Finally, the output of the DDRE is calculated by:
\begin{equation}
\label{ddre3}
    F_7 = W_p^4F_6 \oplus F_4 
\end{equation}

With this design, DDRE can adaptively handle varying degradation degrees across different sizes of degraded regions.