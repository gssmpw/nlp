\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Overview}
The supplementary material is composed of:

Motivation Analysis: Sec.\ref{sec:ma}

More Proof for VBlock: Sec.\ref{sec:mpvb}

Loss Function: Sec.\ref{sec:loss}

More Ablation Studies: Sec.\ref{sec:mas}

Additional Visual Results: Sec.\ref{sec:Visual}

\section{Motivation Analysis}
\label{sec:ma}
Although existing image deblurring methods~\cite{kong2023efficient, MR-VNet} have demonstrated strong performance, they fail to account for the varying degrees of degradation across different regions. As shown in Figure~\ref{fig:diffr}, the blur intensity differs across regions of the image. Additionally, Figure~\ref{fig:diffs} highlights that larger blurred regions are more challenging to recover. To address the first issue, AdaRevD~\cite{AdaRevD} introduces a classifier to assess the degradation degree of image patches. However, AdaRevD~\cite{AdaRevD} relies on a limited set of predefined categories and a fixed blur patch size, which restricts its ability to effectively adapt to different degradation degrees across varying patch sizes. This limitation prevents it from adequately solving the second problem. 

In this paper, we propose the degradation degree recognition expert (DDRE) module which enables the model to adaptively handle varying degrees of degradation in blurred regions. Unlike conventional mixture of experts methods~\cite{moeYu2024BoostingCL, Lifelong-MoE}, where each router selects only one expert and a subset of experts for processing, our DDRE first integrates prior knowledge from a well-trained model to estimate spatially varying blur information. It then utilizes all available routing paths, dynamically assigning weights to each expert along every path. As shown in Table~\ref{tab:ddreexp}, each expert is configured with convolutions of varying receptive field sizes, allowing it to identify degraded regions at different scales. To further validate the effectiveness of the DDRE module, we visualize the feature map in Figure~\ref{fig:ddrerr}. Compared to the initial feature map, the DDRE module recovers images with clearer details, such as the phone number on the white car advertisement. Moreover, the different routers exhibit varying abilities to handle blur degradation, demonstrating that our method can adaptively address blur with different degrees of degradation.

\begin{table}
    \centering
       \caption{The configuration of each expert.}
    \label{tab:ddreexp}

    \begin{tabular}{cc}
    \hline
        Expert &  Configuration
         \\
         \hline
         (1)& 1x1 depthwise separable convolution
         \\
         (2)& 3x3 depthwise separable convolution
         \\
         (3)&5x5 depthwise separable convolution
         \\
         (4)& 7x7 depthwise separable convolution
         \\
         (5)& 9x9 depthwise separable convolution
         \\
         \hline
    \end{tabular}
\end{table}

\begin{figure*}[htb] % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{ddrer.jpg}
	\caption{The internal features of DDRE.}
	\label{fig:ddrerr}
\end{figure*}


\begin{figure}[htb] % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{diffr.jpg}
	\caption{The ranked PSNR curve of the different blur region from GoPro~\cite{Gopro} test set.}
	\label{fig:diffr}
\end{figure}

\begin{figure}[htb] % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{diffs.jpg}
	\caption{The PSNR curve of the different blur size from GoPro~\cite{Gopro} test set.}
	\label{fig:diffs}
\end{figure}




\section{More Proof for VBlock}
\label{sec:mpvb}
Our theoretical proof ideas are borrowed from VNNs~\cite{roheda2024volterra}. A Volterra kernel with $ L $  terms, it can be expressed as:
\begin{equation}
\begin{aligned}
\label{sp1_1}
   y_k  = h_0 + \sum_{d=1}^n \sum_{r_1 = 0}^{L-1} ... \sum_{r_d = 0}^{L-1} h_d(r_1...r_d) \prod_{j=1}^dx(k-r_j)
\end{aligned}
\end{equation}
where $d$ represents the order, $n$ is  the maximum number of $b$, and $r_d$ denotes the memory delay.

For image data, the feature value at location $[x_1, x_2]$ in feature map $F$ is computed using a 2D version of the Volterra kernel, as expressed:
\begin{equation}
\begin{aligned}
\label{sp1_2}
 & F_z \left[ \begin{array}{c}
       x_1
        \\
        x_2
  \end{array}\right ] 
  = V_z(F_{z-1} \left[ \begin{array}{c}
       x_1 - p_1 : x_1 + p_1
        \\
        x_2 - p_2 : x_2 + p_2
  \end{array}\right ] )
  \\
  &= \sum_{r_{11}, r_{21}}h_1 \left[ \begin{array}{c}
       r_{11}
        \\
        r_{21}
  \end{array}\right ]x\left[ \begin{array}{c}
       x_1 - r_{11}
        \\
        x_2 - r_{21}
  \end{array}\right ]
  \\
  &+
  \sum_{\substack{ r_{11}, r_{21} \\ r_{12}, r_{22}}}h_2 \left[ \begin{array}{c}
       r_{11}
        \\
        r_{21}
  \end{array}\right ]\left[ \begin{array}{c}
       r_{12}
        \\
        r_{22}
  \end{array}\right ]
  x\left[ \begin{array}{c}
       x_1 - r_{11}
        \\
        x_2 - r_{21}
  \end{array}\right ]
   x\left[ \begin{array}{c}
       x_1 - r_{12}
        \\
        x_2 - r_{22}
  \end{array}\right ] 
  \\
  &+...
\end{aligned}
\end{equation}
where $V_z$ denotes the $z_{th}$ layer of the Volterra  kernel, $r_{1i} \in [-p_1,p_1]$ and $r_{2i} \in [-p_2,p_2]$ represent spatial translations in the horizontal and vertical directions, respectively. The  complexity of a $n_{-th}$ order Volterra kernel is computed as:
\begin{equation}
\begin{aligned}
\label{sp1_3}
  \sum_{d=1}^n ( L[2p_{1}+1][2p_{2}+1])^d
\end{aligned}
\end{equation}

The Volterra kernel described earlier is significantly more expressive because it captures higher-order relationships among inputs. However, its computation requires iterated integrals and does not have an efficient GPU implementation. To overcome this limitation, we approximate higher-order behavior by utilizing a convex combination of the first-order and second-order terms of the Volterra kernel.

\textbf{Theorem 3.} A cascade of second-order Volterra kernels can approximate the higher-order behavior.

\textbf{Proof.} The second-order Volterra kernel can be formulated as:
\begin{equation}
\label{sp1_4}
    \sigma_{v2}(x)  = h_0 + h_1x + h_2 x^2
\end{equation}

After feeding the output of the second-order Volterra kernel $\sigma_{v2}(\cdot)$  back into another second-order Volterra kernel, we obtain an output that reaches up to the fourth order:
\begin{equation}
\begin{aligned}
\label{sp1_4}
    \sigma_{v2}(\sigma_{v2}(x))  &= \sigma_{v2}(h_0 + h_1x + h_2 x^2)
    \\
    & = h_0^{'} + h_1^{'}x + h_2^{'} x^2 + h_3^{'}x^3 + h_4^{'}x^4
\end{aligned}
\end{equation}
where:
\begin{equation}
\begin{aligned}
\label{sp1_5}
   h_0^{'}  &=  h_0 + h_0^2h_2
   \\
   h_1^{'}& = h_0h_1 +2h_0h_1h_2
   \\
   h_2^{'} &=  h_1^{2}h_2 + h_1^2
   \\
   h_3^{'} &= 2 h_1h_2h_2^{2} + h_1h_2
   \\
    h_4^{'} &= h_2^3
\end{aligned}
\end{equation} 

Therefore, we can utilize $K$ instances of the second-order Volterra kernel to implement an $n_{-th}$ order Volterra kernel, where $n = 2^{2^{K-1}}$.


\textbf{Theorem 4.} The complexity of an $n_{-th}$ order Volterra kernel, implemented using cascaded second-order Volterra kernels, is calculated as follows:

\begin{equation}
\begin{aligned}
\label{sp1_6}
  \sum_{k=1}^K[(L_k[2p_{1k}+1][2p_{2k}+1]) + ( L_k[2p_{1k}+1][2p_{2k}+1])^2]
\end{aligned}
\end{equation}

\begin{figure*} % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{gopr.jpg}
	\caption{Comparison of image motion deblurring on the GoPro dataset~\cite{Gopro}.}
	\label{fig:mgopr}
\end{figure*}

\begin{figure*} % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{real.jpg}
	\caption{Comparison of image motion deblurring on the RealBlur dataset~\cite{realblurrim_2020_ECCV}.}
	\label{fig:mreal}
\end{figure*}

\begin{figure*} % use float package if you want it here
	\centering
	\includegraphics[width=1\linewidth]{mhide.jpg}
	\caption{Comparison of image motion deblurring on the HIDE dataset~\cite{HIDE} among MRLPFNet~\cite{MRLPFNet}, AdaRevD~\cite{AdaRevD}, Restormer~\cite{Zamir2021Restormer}, FSNet~\cite{FSNet}, MambaIR~\cite{guo2024mambair}, ALGNet~\cite{alggao2024learning}, and our DHNet.}
	\label{fig:mhide}
\end{figure*}

\textbf{Proof.}  It follows from Eq.~\ref{sp1_3} that, for a second-order Volterra kernel, the number of parameters required is
$[(L_k[2p_{1}+1][2p_{2}+1]) + ( L_k[2p_{1}+1][2p_{2}+1])^2]$. When we utilize $K$ times of the second-order Volterra kernel to implement an $n_{-th}$ order Volterra kernel, it will lead to Eq.~\ref{sp1_6}, which is significantly lower than Eq.~\ref{sp1_3}.

The first order Volterra kernel is similar to the convolutional layer in the conventional CNNs. The second order kernel may be approximated as the concept of separable kernels as :
\begin{equation}
\label{sp1_7}
    W_d^{2}  = \sum_{q=1}^Q W_d^{2aq}  \otimes W_d^{2bq} 
\end{equation}
where $Q$ represents the  desired rank of approximation, $W_d^{2} \in \mathbb{R}^{(2p_1+1)\times (2p_2+1) \times (2p_1+1) \times (2p_2+1)}, W_d^{2aq}\in \mathbb{R}^{(2p_1+1)\times (2p_2+1) \times 1}$ and $W_d^{2bq} \in \mathbb{R}^{1 \times (2p_1+1)\times (2p_2+1)}$. A larger $Q$ will provide a better approximation of the second order kernel. This is easier to implement with a convolutional library in the first place. Secondly, the complexity is reduced from  Eq.~\ref{sp1_6} to:
\begin{equation}
\begin{aligned}
\label{sp1_8}
  \sum_{k=1}^K[(L_k[2p_{1k}+1][2p_{2k}+1]) + 2Q( L_k[2p_{1k}+1][2p_{2k}+1])]
\end{aligned}
\end{equation}

Therefore, we can adjust the value of $Q$ to strike a balance between performance and acceptable computational complexity. We also illustrate the impact of different  $Q$ values on the model in the ablation experiments presented in the main text.



\section{Loss Function}
\label{sec:loss}
To optimize the proposed network DHNet by minimizing the following loss function: 
\begin{equation}
\begin{aligned}
\label{eq:loss1}
L &= L_{c}(\hat{I},\overline I)  + \delta L_{e}(\hat{I},\overline I) + \lambda L_{f}(\hat{I},\overline I))
\\
L_{c} &= \sqrt{||\hat{I} -\overline I||^2 + \epsilon^2}
\\
L_{e} &= \sqrt{||\triangle \hat{I} - \triangle \overline I||^2 + \epsilon^2}
\\
L_{f} &= ||\mathcal{F}(\hat{I})-\mathcal{F}(\overline I)||_1
\end{aligned}
\end{equation}
where  $\overline I$ denotes the target images and $L_{c}$ is the  Charbonnier loss with constant $\epsilon = 0.001$. $L_{e}$ is the edge loss, where $\triangle$ represents the  Laplacian operator. $L_{f}$  denotes the frequency domains loss, and $\mathcal{F}$ represents fast Fourier transform. To control the relative importance of loss terms, we set the parameters $\lambda = 0.1$ and $\delta = 0.05$  as in~\cite{Zamir2021MPRNet,FSNet}.



\section{More Ablation Studies}
\label{sec:mas}
We provide more ablation studies on the GoPro dataset~\cite{Gopro}.


\subsection{VBlock vs. MR-VNet~\cite{MR-VNet}}
Unlike MR-VNet~\cite{MR-VNet}, which treats the Volterra filter as a complete block, our VBlock incorporates it as part of a block. The VBlock first captures local features using LN, 1x1, and 3x3 convolutions before entering the Volterra filter, and then applies two 3x3 convolutions for each order branch. Additionally, by fusing the first- and second-order components and including a residual
connection for the preorder, our VBlock effectively avoids the issues of local feature loss. 

To validate the advantage of our VBlock over MR-VNet~\cite{MR-VNet}, we replace our VBlock with MR-VNet and present the results in Table 1. When replaced, the performance drops by 0.24 dB, demonstrating the superior performance of our VBlock. Additionally, MR-VNet~\cite{MR-VNet} is prone to gradient collapse during practical training.

\begin{table}
    \centering
       \caption{Results of alternatives to VBlock.}
    \label{tab:vbl1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \hline
         Net& MR-VNN~\cite{MR-VNet} &VBlock&  PSNR & $\triangle$ PSNR 
         \\
         \hline
         (a)& \ding{52} &   &34.51 & - 
         \\
         (b)& & \ding{52}   &34.75 & +0.24 
         \\
         \hline
    \end{tabular}}
\end{table}


\subsection{Resource Efficient}
We evaluate the model complexity of our proposed approach and other state-of-the-art methods in terms of running time and MACs. As shown in Table~\ref{tab:computational2222}, our method achieves the lowest MACs value while delivering competitive performance in terms of running time. 

\begin{table}
    \centering
    \caption{The evaluation of model computational complexity.}
    \label{tab:computational2222}
       \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \hline
         Method& Time(s) & MACs(G)  & PSNR$\uparrow$  & SSIM$\uparrow$ 
         \\
         \hline\hline
         MPRNet~\cite{Zamir2021MPRNet} & 1.148 & 777 & 32.66 &0.959
         \\
         Restormer~\cite{Zamir2021Restormer} & 1.218 & 140 & 32.92 & 0.961
         \\
         FSNet~\cite{FSNet} &\underline{0.362} & 111& 33.29 & 0.963
         \\
          MambaIR~\cite{guo2024mambair} &0.743 &439 &33.21 &0.962
         \\
         MR-VNet~\cite{MR-VNet} & 0.388 & \underline{96} & 34.04 & 0.969
         \\
         AdaRevD-L~\cite{AdaRevD} &0.761 & 460& \underline{34.60} & \underline{0.972}
         \\
         \hline
        \textbf{ DHNet(Ours)} &\textbf{0.256} &\textbf{32} & 33.28 & 0.964
         \\
        \textbf{DHNet-B(Ours)} & 0.499 &111 & \textbf{34.75} & \textbf{0.973}
         \\
         \hline
    \end{tabular}}
\end{table}

\section{Additional Visual Results}
\label{sec:Visual}
In this section, we present additional visual results alongside state-of-the-art methods to highlight the effectiveness of our proposed approach, as shown in Figures~\ref{fig:mgopr},\ref{fig:mhide},\ref{fig:mreal}.
It is clear that our model produces more visually appealing outputs for both synthetic and real-world motion deblurring compared to other methods.









