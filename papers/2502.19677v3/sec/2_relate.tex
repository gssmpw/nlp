\section{Related Work}
\label{sec:formatting}

\subsection{Image Deblurring}
The ill-posed nature of image deblurring leads many conventional approaches~\cite{2011Image,karaali2017edge} to rely on hand-crafted priors to limit the possible solutions. Although these priors can assist in blur removal, they often struggle to accurately model the degradation process and often lack generalization.

Rather than manually designing image priors, many methods focus on developing various deep CNNs~\cite{AdaRevD,Zamir2021MPRNet,Zamir2022MIRNetv2,FSNet,SFNet,focalnetcui2023focal,IRNeXt,chen2022simple,CascadedGaze,MR-VNet, DDAnet} to tackle image deblurring.
MPRNet~\cite{Zamir2021MPRNet} presents a multi-stage progressive approach that improves the exploration of spatial details and contextual information. 
MIRNet-V2~\cite{Zamir2022MIRNetv2} employs a multi-scale architecture that maintains spatially precise representations while also capturing complementary contextual insights. 
CGNet~\cite{CascadedGaze} integrates a global context extractor to effectively gather global contextual information. IRNeXt~\cite{IRNeXt} rethinks CNN design and introduces an efficient network. FSNet~\cite{FSNet} utilizes multi-branch and content-aware modules to dynamically select the most informative components. MR-VNet~\cite{MR-VNet} proposes a novel architecture that leverages Volterra layers for both image and video restoration. 
Nonetheless, the inherent characteristics of convolutional operations, such as local receptive fields, limit the models' ability to effectively address long-range degradation disturbances.

To tackle these limitations, Transformers~\cite{kong2023efficient, potlapalli2023promptir, DeblurDiNAT, Zamir2021Restormer, MRLPFNet, Wang_2022_CVPR, mt10387581, liang2021swinir, Tsai2022Stripformer} have been utilized in image deblurring. They effectively capture global dependencies through the adaptive weights of the self-attention mechanism, outperforming CNN-based methods. However, the quadratic time complexity of the self-attention mechanism with respect to input size increases the computational burden.
To mitigate this issue, Uformer~\cite{Wang_2022_CVPR}, SwinIR~\cite{liang2021swinir}, and U$^2$former~\cite{u2former} implement self-attention using a window-based approach. In contrast, Restormer~\cite{Zamir2021Restormer}, MRLPFNet~\cite{MRLPFNet}, and DeblurDiNAT~\cite{DeblurDiNAT} compute self-attention across channels rather than in the spatial dimension, achieving linear complexity in relation to input size. Additionally, FFTformer~\cite{kong2023efficient} leverages frequency domain properties to estimate scaled dot-product attention.

While the aforementioned methods have two main drawbacks: (1) they approximate nonlinear functions by stacking many nonlinear activation functions, and (2) they fail to account for the varying degrees of degradation in different blur regions.
Although NAFNet~\cite{chen2022simple} uses element-wise multiplication instead of nonlinear activation functions, it struggles with more complex scenes. AdaRevD~\cite{AdaRevD} introduces a classifier to assess the degradation degree of image patches, but it relies on a limited number of predefined categories and a fixed patch size, lacking adaptive flexibility.  
In this paper, we propose a differential handling network that incorporates two key components: the Volterra block (VBlock) and the degradation degree recognition expert module (DDRE). This network achieves nonlinear properties with fewer nonlinear activation functions while adaptively handling varying degradation degrees in blur regions of different sizes.


\subsection{Volterra Series}
The Volterra series is a model for nonlinear behavior that effectively captures "memory" effects~\cite{Volterra2005TheoryOF}. With its capability to learn nonlinear functions, Volterra Filters have been applied in deep learning~\cite{MR-VNet, Volter9247263, Volter8237772, roheda2024volterra}. To enhance non-linearity and complement traditional activation functions,~\cite{Volter8237772} introduces a single layer of Volterra kernel-based convolutions followed by standard CNN layers. VNNs~\cite{roheda2024volterra} proposes a cascaded approach of Volterra Filtering to significantly reduce the number of parameters. VolterraNet~\cite{Volter9247263} presents a novel higher-order Volterra convolutional neural network designed for data represented as samples of functions on Riemannian homogeneous spaces. MR-VNet~\cite{MR-VNet} uses Volterra layers to effectively introduce nonlinearities into the image restoration process. However, in MR-VNet, only the second-order nonlinearity is captured, while the first-order nonlinearity is overlooked. This results in reduced attention to local features and a higher risk of gradient explosion. In this paper, we design a VBlock that addresses these issues and approximates nonlinearity without the computational burden of stacking multiple nonlinear functions.



\subsection{Mixture of Experts}
Mixture of Experts (MoE)~\cite{moe6797059} consists of multiple experts and a routing network that combines their outputs using a weighted strategy. Its main goal is to enhance model performance by scaling up parameters while maintaining computational efficiency~\cite{moeYu2024BoostingCL,moe10.5555,moemustafa2022multimodal,moeruiz2021scaling,Lifelong-MoE,moehyChen_Pan_Lu_Fan_Li_2023,DRSformer,moedanYe2022TowardsES}.
HCTFFN~\cite{moehyChen_Pan_Lu_Fan_Li_2023} leverages MoE to highlight features related to spatially varying rain distribution. DRSformer~\cite{DRSformer} incorporates an MoE feature compensator for a unified exploration of joint data and content sparsity. DAN-Net~\cite{moedanYe2022TowardsES} employs MoE to extract degradation information from each input image.  Lifelong-MoE~\cite{Lifelong-MoE} uses pre-trained experts and gates to retain prior knowledge. 
In contrast to the methods mentioned above, we incorporate prior knowledge into DDRE from a well-trained model to estimate spatially variable blur information. This allows the router to adaptively allocate weights to different experts, enabling them to address the varying degradation degrees presented by different blur regions.