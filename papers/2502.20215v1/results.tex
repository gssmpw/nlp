\section{Results}
\label{sec:results}

This section presents both qualitative and quantitative experimental results.
All results were obtained on a laptop computer with 64 GB of RAM, a Core
i7-13850HX CPU (8 cores at 5.3 GHz and 12 cores at 3.8 GHz), and a RTX 2000 GPU.
We implemented our method in C++ (with the libraries OpenMP, LibTorch
\cite{pytorch} and CGAL
\cite{fabri_cgal_2009}) as modules for TTK~\cite{tierny2017topology, masood2021overview}.
We used as encoder (resp. decoder) a fully connected network with two hidden layers of size 128 and 32 (resp. 32 and 128), with ReLU activation functions, and batch normalization.
For each experiment, we performed 1000 iterations of the Adam optimizer~\cite{kingma2017adam} with a learning rate fixed at 0.01.

\subsection{Test data}
\label{sec:test_data}

We present results on synthetic, three-dimensional datasets in
\autoref{fig:tableSyntheticData}, which have been specifically designed
to include clearly salient topological features. The first one consists in 3
Gaussian clusters,
and features 3 significantly persistent \PH{0} pairs. The second one consists in
points sampled around an ellipse twisted along its major axis, and features one
significantly persistent \PH{1} pair. The third one consists in points sampled
around the edges of a tetrahedron (i.e., the complete $K_4$ graph which is
planar) and features 3 significantly persistent \PH{1} pairs.
We also consider a stress case in \autoref{fig:tableK5}, with a dense point cloud sampling the complete $K_5$ graph embedded in $\bbr^3$ (i.e., $5$ vertices in 3D, all pairwise connected by an edge).
Since $K_5$ is a non-planar graph, the resulting 3D
persistent generators (which follow the edges of $K_5$) cannot
be projected to the plane without intersections.
Then, it is not possible to project this stress case to the plane faithfully
with respect to \PH{1}, as the intersections of the projected 3D
generators create additional, short cycles in 2D.

We present results on real-life, acquired or simulated, high-dimensional, cycle-featuring
datasets in \autoref{fig:tableRealData}. COIL-20~\cite{nene_columbia_1996} is an
image dataset with 20 classes, each one containing 72 grayscale $64\times64$
images. Each class consists in a single object (e.g., a rubber duck)
viewed from 72 evenly spaced angles, so that the images within each class live
on a manifold that is homeomorphic to a circle.
This leads to significantly persistent pairs when computing the \PH{1} of the input endowed with the Euclidean metric between the images.
Cyclic features may also appear in motion capture
data~\cite{bei_wang_branching_2011}. Hence, we
also evaluate our approach on a dataset extracted from the
CMU motion capture database~\cite{noauthor_carnegie_nodate},
which records a human subject doing some periodic movement
(e.g., walking or running), each frame consisting in 62 skeleton joint
angles measurements.
Finally, data from biological processes can also feature meaningful cycles like in single-cell omics~\cite{saelens2019comparison}, a field that aims at sequencing single cells.
This enables the observation of gene expression within
individual cells at different stage of the cell cycle (resting, growth, DNA
replication, mitosis).
Specifically, we used the simulated dataset \emph{"cyclic\_2"}~\cite{cannoodt_single-cell_2018}, which features a significantly persistent
\PH{1} pair.
See \autoref{table:datasetSummary} for a summary of the presented datasets.
\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Dataset & Nature & Dimension ($\highDim$) & Size ($n$) & Fig. \\
		\hline
		\datathreeblobs & Synthetic & 3 & 800 & \multirow{3}{*}{\autoref{fig:tableSyntheticData}} \\
		\datatwist & Synthetic & 3 & 100 & \\
		\datakfour & Synthetic & 3 & 300 & \\
		\hline
		\datakfive & Synthetic & 3 & 500 & \autoref{fig:tableK5} \\
		\hline
		\datacoil & Acquired & 4096 & 72 & \multirow{3}{*}{\autoref{fig:tableRealData}} \\
		\datamocap & Acquired & 62 & 138 & \\
		\datasinglecell & Simulated & 1170 & 243 & \\
		\hline
	\end{tabular}
	\vspace{2mm}
	\caption{Summary of datasets.}
	\label{table:datasetSummary}
\end{table}

\subsection{Quantitative criteria}
\label{sec:criteria}

In order to evaluate the topological accuracy of the representations generated by
our approach, we compute the
$L_2$-Wasserstein distance (see \autoref{sec:persistentHomology}) between the
1-dimensional persistence diagrams of the input $\inputPointCloud$ and the
low-dimensional representation $\latentPointCloud$:
\[\metwasser(\inputPointCloud,\latentPointCloud)=\calw_2\bigl(\dgmrips{1}(\inputPointCloud),\dgmrips{1}(\latentPointCloud)\bigr).\]
This quantity, which we want to be as low as possible, summarizes in some measure the extent to which similar cycles exist in the two point clouds.
However, as persistence diagrams lose information -- they are in particular invariant to permutations in the point clouds --
it does not convey whether the set of vertices involved in these cycles are
the same, in contrast to TopoAE-like loss functions.

We also compute the metric distortion $\metdistor$, which should also be as low as possible, expressed as the root mean squared error ($\RMSE$) between the pairwise distances in $\inputPointCloud$ and those in $\latentPointCloud$ -- which is what MDS-like methods typically minimize:
\[\metdistor(\inputPointCloud,\latentPointCloud)=\sqrt{\frac{1}{n}\sum\limits_{i
<j}\bigl(\lVert X_i-X_j\rVert_2-\lVert Z_i-Z_j\rVert_2\bigr)^2}.\]

We refer the reader to \autoref{appendix:quantitativeDescription} for a
description of other quality scores often considered in the DR literature
(see \autoref{appendix:quantitativeObservations} for additional experiments
with these indicators).

\subsection{Competing approaches}
We compare our approach to global methods (PCA~\cite{pearson1901liii},
MDS~\cite{torgerson1952multidimensional}) that do
not take topology into account. We also compare to several locally
topology-aware methods (Isomap~\cite{tenenbaum_global_2000},
t-SNE~\cite{van2008visualizing}, UMAP~\cite{mcinnes2018umap}) that are able to
consider the local manifold structure of the data but that might fail to project
faithfully the overall structure.
Regarding previously documented
globally topology-aware methods, although TopoMap~\cite{doraiswamy2020topomap}
preserves exactly \PH{0} and TopoAE~\cite{moor2020topological} preserves \PH{0}
in the sense of \autoref{lemma:TopoAE0_bound}, both methods have no guarantee
about \PH{1}. In addition, we show the results of the method suggested
in~\cite{carriere2021optimizing}, which we denote $\carriereMethod$ and which
consists in adding $\metwasser(\inputPointCloud,\latentPointCloud)$
to the TopoAE loss $\ltopoae{0}$. Both terms in the loss are given the same weights.
It is then supposed to give the best results in terms
of $\metwasser(\inputPointCloud,\latentPointCloud)$ values and therefore make the cycles in $\inputPointCloud$ and $\latentPointCloud$ as similar (i.e., in number, in size) as possible.
However, as explained in \autoref{sec:criteria}, $\metwasser(\inputPointCloud,\latentPointCloud)$ is not sensible to which simplices are involved in
the cycles, and the cycles in $\latentPointCloud$ might be created between simplices that do not belong to a cycle in $\inputPointCloud$.

For PCA, MDS, Isomap and t-SNE, we used the implementations in
scikit-learn~\cite{pedregosa2011scikit} (with default parameters
except Isomap for which the number of neighbors is set to 8); for
UMAP, the Python package umap-learn
provided by the authors; and for TopoMap, the existing implementation in TTK.
For TopoAE-like approaches, we used our own implementation, with
Ripser~\cite{bauer_ripser_2021} as latent space \PH{1} algorithm for \tAE{} and
\carriereMethod{}, and \autoref{algo:2dpersistence} for \tAE++. For
non-deterministic methods (i.e., \tAE, \carriereMethod{} and \tAE++
which rely on a stochastic optimization of a neural network with a random
initialization), we performed 10 runs and kept the result with the best
$\metwasser(\inputPointCloud,\latentPointCloud)$.

\subsection{Result analysis}
\label{sec:results:analysis}
\input{comparisonTableK5}
\input{comparisonTableReal}

\subsubsection{Qualitative analysis}
Our visual validation approach involves computing a generator for each
significantly persistent pair in the input and observing their low-dimensional
projections. These generators should ideally be projected without
self-intersection and coincide with the cycles in the low-dimensional
representation. In such a case, the output would be considered to faithfully
represent the high-dimensional cycles.

In the depicted examples, \tAE++ succeeds in projecting the cycles from the
input in this visual sense, whenever possible
(with the exception of \datakfive{}, for which a faithful planar projection
is not possible, see \autoref{sec:test_data}).
It also tends to preserve the \textit{shape} of the cycles.
For example, \datatwist{} (\autoref{fig:tableSyntheticData}) is projected with
the original ellipse shape preserved, but without torsion.
Similarly, in \datacoil{} (\autoref{fig:tableRealData}), the
high-dimensional cycle has a stretched shape (since the rubber duck has a
similar silhouette when viewed from the front and back, but not when viewed from
the left and right), which \tAE++ takes into account (views from the left and
right corresponds to the "ends" of the stretched planar cycle).
\datamocap{} (\autoref{fig:tableRealData}) comes from the capture of a subject performing a
translation in addition to its periodic movement (running). It is therefore
projected to a spiral shape, where the longest edge of the projection of the
input cycle (appearing in lavender) corresponds to the same body configuration
during the running cycle, but at two different locations. Finally,
\datasinglecell{} (\autoref{fig:tableRealData}) is difficult to project in the plane as it presents points
that lie on a cycle but are distant from each other due to the high
dimensionality: \tAE++ takes that into account and its result can be
interpreted as a compromise between preserving \PH{0} and \PH{1}.

On contrary, global methods (PCA, MDS) do not guarantee the preservation of
these cycles (see, e.g., their results on \datatwist{},
\autoref{fig:tableSyntheticData}, and
\datacoil{}, \autoref{fig:tableRealData}), while locally
topology-aware methods sometimes fail to project
them correctly (e.g., t-SNE and UMAP break apart the cycles in
\datakfour{}, \autoref{fig:tableSyntheticData}, and
\datasinglecell{}, \autoref{fig:tableRealData}, when
projecting). In addition, even if the
cycle is visually correctly projected, these methods tend to forget the
\textit{shape} of the cycle (e.g., Isomap projects \datatwist{},
\autoref{fig:tableSyntheticData}, and
\datacoil{}, \autoref{fig:tableRealData}, as almost perfect circles).
Finally, as expected, previously documented
globally topology-aware methods generally fail to
faithfully project the cycles, as they only incorporate constraints on \PH{0}
(TopoMap, \tAE) or on the Wasserstein distance $\metwasser$ (\carriereMethod),
but not on \PH{1} (see the next paragraph).

\begin{figure}[b]
	\def\svgwidth{\linewidth}
	\input{figures/counter-ex-diags.pdf_tex}
	\caption{Topological accuracy comparison between projections obtained
	with the loss $\ltopoae{1}$ (a naive extension of \cite{moor2020topological}
to
	\PH{1}, \autoref{eq:topoae}) and our cascade distortion ($\lcascae{1}$,
	\autoref{eq:cascae}) on the counter-example
	$\inputPointCloud\in\bbr^3$ of \autoref{fig:counter-example}, producing the 2D
	embeddings $\latentPointCloud_\mathrm{TAE}$ and
$\latentPointCloud_\mathrm{CD}$
	respectively (left).
	In this example, the optimization achieved a virtually zero value for
	both losses. However, in the space of persistence diagrams,
	$\dgmrips{1}(\latentPointCloud_\mathrm{TAE})$ does not match
	$\dgmrips{1}(\inputPointCloud)$, while
	$\dgmrips{1}(\latentPointCloud_\mathrm{CD})$ coincides with it (right).}
	\label{fig:counter-example-diags}
\end{figure}

\subsubsection{Quantitative analysis}

From a more quantitative point of view, in the examples shown, \tAE++ generally produces the second best $\metwasser$ after \carriereMethod{}, both significantly better than all other methods for this metric.
However, as \carriereMethod{} directly optimizes $\metwasser$ and forgets the
simplices involved in \PH{1} pairs, the vertices involved in planar cycles
(i.e., corresponding to the points of $\dgmrips{1}(\latentPointCloud)$)
do not match those involved in input cycles (i.e., corresponding to the
points of $\dgmrips{1}(\inputPointCloud)$). This leads to a good $\metwasser$
metric but a poor projection.
For instance, in \datatwist{} (\autoref{fig:tableSyntheticData}),
\carriereMethod{} creates a cycle (the top one) that has the right birth and
death -- while the bottom loop has a persistence close to zero -- to cancel out
$\metwasser$; yet the original cycle is poorly projected, with a
self-intersection.
Similarly, on \datakfive{} (\autoref{fig:tableK5}), \carriereMethod{} places the points in order to have 6 persistent cycles with the right birth and death to cancel out $\metwasser$, while \tAE++ tries to preserve \PH{1}, which is impossible here due to the non-planarity of $K_5$.

The distortion metric is higher with TopoAE++, than in global methods
(PCA, MDS) which directly minimize it (by design for MDS).
It can be seen as a necessary compromise to faithfully project cycles
(e.g., \datatwist{}, \autoref{fig:tableSyntheticData},
requires distortion to be untwisted in 2D;
the cycle in \datacoil, \autoref{fig:tableRealData}, which is
folded up in high dimension, requires
distortion to be unfolded).
See \autoref{table:quantitative} of \autoref{appendix:quantitative} for
additional metrics commonly used in DR, along with a companion
discussion.



\begin{figure}[b]
	\centering
	\scriptsize{
	\begin{tabular}{|c|c|c|}
		\hline
		Input & $\ltopoae{1}$ & $\lcascae{1}$ \\
		\hline
		\raisebox{.25cm}{\datacoil}
		& \includegraphics[width=.1\linewidth]{results/figures_cropped/coil1_TopoAE1}
		& \includegraphics[width=.1\linewidth]{results/figures_cropped/coil1_TopoAE++}\\
		$\metwasser(\inputPointCloud,\latentPointCloud)$ & 3.2e-02 & 1.6e-02 \\
		$\timing$ & 5.9 (2.4) & 2.4 \\
		\hline
		\raisebox{.25cm}{\datamocap}
		& \includegraphics[width=.1\linewidth]{results/figures_cropped/mocap_09_07_TopoAE1}
		& \includegraphics[width=.1\linewidth]{results/figures_cropped/mocap_09_07_TopoAE++}\\
		$\metwasser(\inputPointCloud,\latentPointCloud)$ & 3.0e+01 & 1.2e+01 \\
		$\timing$ & 15 (3.3) & 3.4 \\
		\hline
		\raisebox{.25cm}{\datasinglecell}
		& \includegraphics[width=.1\linewidth]{results/figures_cropped/cyclic_2_TopoAE1}
		& \includegraphics[width=.1\linewidth]{results/figures_cropped/cyclic_2_TopoAE++}\\
		$\metwasser(\inputPointCloud,\latentPointCloud)$ & 1.3e+03 & 8.6e+02 \\
		$\timing$ & 26 (5.9) & 6.0 \\
		\hline
	\end{tabular}}
	\caption{Topological accuracy comparison
	($\metwasser(\inputPointCloud,\latentPointCloud)$)
	between projections obtained with the loss $\ltopoae{1}$
	and our cascade distortion $\lcascae{1}$, for our real-life datasets.
	For $\ltopoae{1}$, we report the runtime
	with the gold standard for Rips persistence computation~\cite{bauer_ripser_2021}
	as well as, in parentheses, with our novel planar Rips
	persistence computation algorithm (\autoref{sec:algorithms}).
	On average, projections using our novel loss ($\lcascae{1}$), combined with our fast planar Rips
	persistence algorithm, improve topological accuracy by $48\%$, with a runtime improvement of $71\%$.}
	\label{table:comparisonTAEwithCasc}
\end{figure}


\autoref{fig:counter-example-diags} provides further evaluations of the
internal aspects of our approach. Specifically, for the counter-example of
\autoref{fig:counter-example}, it compares projections obtained by minimizing
a naive extension of~\cite{moor2020topological} to \PH{1} ($\ltopoae{1}$,
\autoref{eq:topoae}) and our novel loss ($\lcascae{1}$,
\autoref{eq:cascae}). In this example, while the optimization achieves a
virtually zero value for both losses, a clear gap occurs between the
persistence diagrams
$\dgmrips{1}(\latentPointCloud_\mathrm{TAE})$ and
$\dgmrips{1}(\inputPointCloud)$, while
$\dgmrips{1}(\latentPointCloud_\mathrm{CD})$ coincides with
$\dgmrips{1}(\inputPointCloud)$.
This indicates that $\lcascae{1}$ enables a better preservation of the
persistent homology of the input $\latentPointCloud$. This gain in topological
accuracy is further evaluated in \autoref{table:comparisonTAEwithCasc}, which
compares the resulting Wasserstein distances
($\metwasser(\inputPointCloud,\latentPointCloud)$) for our real-life datasets.
In particular, we observe in this figure that,
while both losses do not produce
intersections in the projection of the high-dimensional generator (colored
curve), projections based on
our novel loss improve topological
accuracy by $48\%$ on average. This illustrates that our novel loss
produces projections which are more faithful to the input data, in
terms of the number, size and shape of its cyclic patterns.

% \subsubsection{\matteo{Comparison between $\ltopoae{1}$ and $\lcascae{1}$}}
%
% \matteo{
% The use of the 1-dimensional cascade distortion $\lcascae{1}$ instead of the original \tAE{} regularization $\ltopoae{1}$ improves the resulting $\metwasser$ by avoiding configurations similar to the counter-example shown in \autoref{fig:counter-example}. We show in \autoref{fig:counter-example-diags} possible executions of \tAE{} (with $\ltopoae{1}$ and \tAE++ on this counter-example. \tAE{} produces an error on $\dgmrips{}(\latentPointCloud)$ while \tAE++ successfully recovers it.
% \autoref{table:comparisonTAEwithCasc} shows the quantitative improvement on $\metwasser$ on real-life datasets.
% }

\subsection{Time performance}
\label{sec:performance}

\subsubsection{Fast 2D persistence computation}
\label{sec:performance:rips2d}

We compared our persistence algorithm (\autoref{algo:2dpersistence}) for point
clouds in $\bbr^2$ to the following more generic Rips \PH{0} and \PH{1}
implementations: \textit{Gudhi}~\cite{maria2014gudhi} (a generic implementation
that handles various filtrations), \textit{Ripser}~\cite{bauer_ripser_2021} (a
specialization to Rips filtrations that can be applied to any point
cloud in $\bbr^d$ or distance matrix), and \textit{Euclidean PH1}
\cite{koyama2023reduced} (that works theoretically for any point cloud in
$\bbr^d$, but implemented only for $d=2$ or $d=3$).
We observe that on uniformly sampled point clouds, the running time asymptotic
behavior is roughly cubic (with regard to the number of input points)
for \textit{Gudhi} and quadratic for \textit{Ripser},
while it remains close to being linear for \textit{Euclidean PH1} and our
algorithm. However, our approach offers a speedup of about 2
orders of magnitudes compared to \textit{Euclidean PH1} (see
\autoref{fig:benchmark-rips-uniform}).
The speedup seems even more noticeable with points clouds sampled next to a
circle which creates one high-persistence pair in the 1-dimensional persistence
diagram and particularly slows down \textit{Euclidean PH1} (see
\autoref{fig:benchmark-rips-ring}).
Finally, we have observed empirically that the use of
\autoref{lemma:bounded_rips_delrips} in \autoref{algo:findMMLedge}
(i.e., checking expandability only for the edges of length in
$\left[\frac{\sqrt{3}}{2}\delta_{\dr},\delta_{\dr}\right]$)
enables discarding $84\%$ of the possible polygon diagonals on average,
which significantly contributes to the reported speedup.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/benchmark_uniform}
	\caption{Comparison of Rips persistence computation times
	between \textit{Gudhi}, \textit{Ripser}, \textit{Euclidean PH1} and our algorithm
	(on a single thread, 1T, and on 20 threads, 20T), on point clouds randomly,
	uniformly distributed over $[0,1]^2$.}
	\label{fig:benchmark-rips-uniform}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/benchmark_ring}
	\caption{Comparison of Rips persistence computation times
	between \textit{Gudhi}, \textit{Ripser}, \textit{Euclidean PH1} and our algorithm
	(on a single thread, 1T, and on 20 threads, 20T), on a stress case (for
	\emph{Ripser} and \emph{Euclidean PH1}): a point cloud randomly
	distributed over the unit circle, with Gaussian noise ($\sigma=0.1$).}
	\label{fig:benchmark-rips-ring}
\end{figure}

\subsubsection{Overall DR approach}

\tAE++ can hardly compete with conventional DR methods (PCA, MDS, Isomap, t-SNE) in terms of computation time.
However, this must be put into perspective. First, we construct a complete
projection from the input space $\calx$ to the
latent space $\bbr^2$, and not just a projection of the point cloud
$\inputPointCloud$ (i.e., we can project a new point, at the low price of
one evaluation of the encoder network). Second,
we incorporate constraints from persistent homology, which is computationally
expensive by nature.

However, our fast 2D Rips \PH{} algorithm (\autoref{sec:algorithms})
manages to overcome the runtime overhead due to
the computation of \PH{1} of $\latentPointCloud$ at each iteration of the
optimization.
Indeed, comparing \tAE++ with \carriereMethod{} -- which also requires this
computation, implemented with \textit{Ripser} -- we observe a speedup
by a factor of $2$ to $20$ in the presented examples.
The experiments from \autoref{table:comparisonTAEwithCasc} (comparing
\tAE++ to a naive extension of \cite{moor2020topological} to \PH{1})
report compatible speedups ($3.7$ on average).
In addition, \tAE++ is sometimes even faster than \tAE{} although
the latter only computes \PH{0}, both initially for the
input $\inputPointCloud$ and at each optimization step for
$\latentPointCloud$.

\subsection{Limitations}
\label{sec:limitations}

The first obvious limitation is of a fundamental nature: there are
points clouds for which there is no planar embedding that is faithful with
regard to the \PH{1} (see \autoref{fig:tableK5} and its description,
\autoref{sec:test_data}).
Besides, like many other topological methods, we are limited in the size of the input due
to the initial \PH{} computation, if only in terms of memory. Specifically,
\textit{Ripser}, which is commonly considered as the gold
standard implementation for Rips filtrations in high dimensions, runs out
of memory on a 64 GB machine when computing the \PH{1} of around 50,000 points.
This limitation could be addressed by batch processing or subsampling approaches
(see, e.g.,~\cite{moor2020topological}). In addition, as with the original \tAE,
unsatisfactory results may occur when the minimization of the topological
regularization term is not successful (i.e., stuck in a "bad" local minimum).
In our experiments, as for \tAE{} and \carriereMethod{}, this was mitigated by
running 10 times the projection with a random initialization, and considering
as an output the projection minimizing
$\metwasser(\inputPointCloud,\latentPointCloud)$. More sophisticated strategies
could be considered in the future, in particular for an early detection of
potentially bad local minima.
Finally, we have no theoretical guarantee that our cascade distortion
loss function upper bounds the Wasserstein distance between the persistence diagrams, despite its
experimental ability to accurately preserve the cycles. However,
we have not found a counter-example yet, similar to the example
presented in \autoref{fig:counter-example}, and this question remains open for the moment.
