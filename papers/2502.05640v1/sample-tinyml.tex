\pdfoutput=1

%%
%% This is file `sample-tinyml.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `tinyml')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-tinyml.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[tinyml]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{rightsretained}
%\setcopyright{none}
\copyrightyear{2025}
\acmYear{2025}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}

%%
%% end of the preamble, start of the body of the document source.

\settopmatter{printacmref=false}

\usepackage{threeparttable}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{cases}
\usepackage{subfig}
\usepackage{xcolor,colortbl}
\usepackage{multirow}
\usepackage{url}
\usepackage{IEEEtrantools}

\usetikzlibrary{arrows.meta,positioning,automata}
\usetikzlibrary{backgrounds,positioning,shapes.gates.logic.US}
\usetikzlibrary{shapes.geometric}

% heatmap
\usepackage{collcell}
\usepackage{enumitem}

\setlength{\textfloatsep}{5pt}
\setlength{\intextsep}{5pt}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\textit{ETHEREAL}: Energy-efficient and High-throughput Inference using Compressed Tsetlin Machine}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\iffalse
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}
\fi

\author{Shengyu Duan}
\affiliation{%
  \institution{Newcastle University}
  \city{Newcastle upon Tyne}
  \country{UK}}
\email{shengyu.duan@newcastle.ac.uk}

\author{Rishad Shafik}
\affiliation{%
  \institution{Newcastle University}
  \city{Newcastle upon Tyne}
  \country{UK}}
\email{rishad.shafik@newcastle.ac.uk}

\author{Alex Yakovlev}
\affiliation{%
  \institution{Newcastle University}
  \city{Newcastle upon Tyne}
  \country{UK}}
\email{alex.yakovlev@newcastle.ac.uk}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The Tsetlin Machine (TM) is a novel alternative to deep neural networks (DNNs). Unlike DNNs, which rely on multi-path arithmetic operations, a TM learns propositional logic patterns from data literals using Tsetlin automata. This fundamental shift from arithmetic to logic underpinning makes TM suitable for empowering new applications with low-cost implementations. 

In TM, literals are often included by both positive and negative clauses within the same class, canceling out their impact on individual class definitions. This property can be exploited to develop compressed TM models, enabling energy-efficient and high-throughput inferences for machine learning (ML) applications.

We introduce a training approach that incorporates excluded automata states to sparsify TM logic patterns in both positive and negative clauses. This exclusion is iterative, ensuring that highly class-correlated (and therefore significant) literals are retained in the compressed inference model, ETHEREAL, to maintain strong classification accuracy. Compared to standard TMs, ETHEREAL TM models can reduce model size by up to 87.54\%, with only a minor accuracy compromise. We validate the impact of this compression on eight real-world Tiny machine learning (TinyML) datasets against standard TM, equivalent Random Forest (RF) and Binarized Neural Network (BNN) on the STM32F746G-DISCO platform. Our results show that ETHEREAL TM models achieve over an order of magnitude reduction in inference time (resulting in higher throughput) and energy consumption compared to BNNs, while maintaining a significantly smaller memory footprint compared to RFs.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Tsetlin Machine, Machine Learning, Model Compression, TinyML}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction} \label{sec:intro}
The ever increasing demand for deploying machine learning (ML) in low-energy, resource-constrained edge applications presents a significant challenge for deep neural network (DNN) implementations due to their high computational demands. This has led to efforts to identify alternative low-complexity ML algorithms. One such alternative is the Tsetlin Machine (TM), which is a novel ML algorithm that has been demonstrated with lower complexity than DNN, while achieving comparable accuracy across a range of ML datasets and exhibiting inherent interpretability \cite{granmo2018tsetlin}. A TM marks a fundamental shift from DNN by relying primarily on logic operations, which for example could outperform a multi-layer neural network (NN) in terms of accuracy \cite{tang2024adatm}, while eliminating hundreds of thousands of multiply-accumulate operations.

Figure \ref{fig:tm_structure} demonstrates a typical TM structure for supervised ML. The structure comprises three incremental processes:
\begin{itemize}
	\item [A.]\textit{Booleanization}: Before TM training and inference regimes, the input dataset is first expressed in the form of a set of literals, represented as Boolean data. These literals are derived through a data encoding process, known as Booleanization. A typical Booleanization process uses fixed or dynamic thresholds to generate Boolean literals as opposed to Binarized features from the raw data \cite{9923830}.
	\item [B.]\textit{Training}: Booleanized literals are given to a group of clauses, each learning a sub-pattern of some literals and performing AND operations to independently make a decision. Each clause learns these patterns through Tsetlin Automata (TAs), which decide whether a literal is included (above middle state) or excluded (below middle state), after a reinforcement learning process, see Section \ref{sec:learn} for further details. Half of all clauses have positive/negative polarity, capturing sub-patterns to support/oppose a classification. 
	\item [C.]\textit{Inference}: A binary classification is performed by a majority vote between the sum of outputs from positive and negative clauses. A multi-class classification requires as many pairs of positive-negative clauses as classes, where the overall classification is based on the one with the greatest class sum.
\end{itemize}

\input{TM_overview}

After training, TA array exhibits high sparsity; for example in the case of an MNIST dataset there are more than 99\% excludes. This property was leveraged to derive a compact model representation, REDRESS \cite{maheshwari2023redress}. The model only stores the information of includes as relative clauses and literal addressing. However, 
REDRESS, applied as a post-training compression, still follows the standard training process of the vanilla TM, resulting in a sparse form of training where the number of includes is not minimized, retaining less relevant or irrelevant context.
Reducing the number of includes is important in TMs, as their inherent sparse nature often incorporates literals with weak correlation to the target classes.  

It is possible to develop a more efficient TM by eliminating literals with weak correlations to a class, leading to minimal accuracy loss. 
{Though extensive research has been conducted on pruning weakly correlated features in DNNs, we emphasize that TM employs fundamentally different learning mechanism and data representation, and thus the pruning methods for DNNs are not applicable.}
In this work, we leverage the inherent interpretability of TM to identify weakly correlated literals, which are often included in both positive and negative clauses, due to their lack of strong association with a class. We propose a training approach to remove these literals, compressing TM models at the algorithm level, beyond REDRESS. This method, called \textbf{ETHEREAL}, enables \textbf{E}nergy-efficien\textbf{T}, \textbf{H}igh-throughput and accurate inf\textbf{E}rence through the practical implementation of a comp\textbf{RE}ssed tset\textbf{L}in m\textbf{A}chine. 

ETHEREAL introduces an additional exclusion process during training, to exclude literals shared by positive and negative clauses.
The exclusion is iteratively followed by standard training to restore important features. Results from eight real-world Tiny Machine Learning (TinyML) datasets show that ETHEREAL can realize up to an 87.54\% reduction in model size with a maximum accuracy loss of only 3.38\%, compared to a vanilla TM \cite{granmo2018tsetlin}. In some cases, accuracy even improves by eliminating some features that contribute noise. We use STM32F746G-DISCO micro-controller as the platform to implement ETHEREAL alongside REDRESS TM \cite{maheshwari2023redress}, a Random Forest (RF) and a Binarized Neural Network (BNN) \cite{courbariaux2016binarized, geiger2020larq}. The TM implementations can provide up to an order of magnitude reduction in inference time and energy compared to BNN, and 7$\times$ lower memory footprints than RF, while giving comparable accuracy. ETHEREAL further improves these design metrics, commensurate with the model size reductions achieved over REDRESS TMs. 

In this paper, we make the following key \textbf{\textit{contributions}}:
\begin{itemize}
	\item Empirical evidence revealing the inefficiency of vanilla TM in including less correlated (and thereby insignificant) literals.
	\item A training approach with additional exclusion, effectively compressing TM model and ensuring high accuracy.
	\item Validation with TinyML benchmarks on STM32 micro-controller, validating improved throughput, energy and memory usage produced by ETHEREAL. 
\end{itemize}

%We describe the TM learning process in Section \ref{sec:learn}. The ETHEREAL model compression method is introduced and evaluated in Sections \ref{sec:ethereal} and \ref{sec:eva}, respectively. We conclude our work in Section \ref{sec:conc}.

\section{TM Learning Dynamics} \label{sec:learn}
A TM is trained to capture the sub-pattern supporting or opposing a proposition by adjusting the TA states, which determine inclusion or exclusion of literals, driven by Type I and Type II feedback.

Figure \ref{fig:tm_feedbacks} explains the conditions under which each type of feedback is initiated. For a TA with 2N states, all TA states are initially set to either N or N+1 at random ($i.e.$, near the confusion state). During training, feedback is probabilistically activated for each datapoint; each specific type of feedback as well as the TAs being reinforced are determined by the training outcomes at the class and clause levels. 
%In Figure \ref{fig:tm_feedbacks}, $y$ indicates the label of a sample: for binary classification, $y$=1 or 0 suggests the sample belongs to the class or not, respectively; for multiclass classification and a TM for a certain class, $y$=1 or 0 suggests the sample belongs to the class or any other classes, respectively. 
%At a datapoint, each clause independently receives its feedback, the type of which is determined by both $y$ and the clause polarity: 
%\todo{This line repeats what you said earlier with a bit more specificity -- keep one; you actually repeat again in the following paragraph}
Type I/II feedback is activated for all positive/negative clauses when $y$=1, while an opposite reaction occurs when $y$=0.

\input{TM_feedbacks}

Figure \ref{fig:tm_feedbacks} shows both types of feedback are triggered with probability $\mathcal{P}$, determined by a hyperparameter $T$, as in (\ref{eq:feedback_prob}):
\vspace{-0.2cm}
\begin{IEEEeqnarray}{rCl}
	\mathcal{P} =\dfrac{T+(-1)^y\times\textrm{clip}(\sum\limits_{j=1}^{M} p_j C_j, -T, T)}{2T}
	\label{eq:feedback_prob}
\end{IEEEeqnarray}
where $M$ is the number of clauses; $p_j$ and $C_j$ are the polarity and output, respectively, for a specific clause. According to (\ref{eq:feedback_prob}), the farther the class sum is from $T$/$-T$ when $y$=1/0, the more likely the feedback is triggered, potentially calibrating more clauses to cast correct votes. On the other hand, feedback is withheld if the class sum becomes greater/smaller than $T$/$-T$, when $y$=1/0. Therefore, $T$ reveals the confidence in distinguishing between different classes. 

Figure \ref{fig:type_i_ii} illustrates the mechanism of both types of feedback.
%In Type I feedback (Figure \ref{fig:type_i_ii} (a)), the TA state of a literal tends to increase, if the literal for the current training sample is `1' and the clause with positive/negative polarity manages to support for the fact that $y$=1/0 by producing the clause output as `1'. 
%Thus, in general, a clause that correctly produces the output is likely to include more literals equaling `1' at the datapoint, through Type I feedback. 
In Type I feedback (Figure \ref{fig:type_i_ii} (a)), a clause that correctly supports or opposes the class (by producing an output of `1') is likely to include more literals that equal `1' at the datapoint.
%\todo{avoid potentially -- use probabilistically or don't use at all.}
This enables it to continue making the right decision by using a more fine-grained sub-pattern. On the other hand, the TA state of a literal equal to `0' is decreased to prevent it from overturning the correct output. 

Finally, a clause that fails to support the correct class (by producing an output of `0') may cause a false negative.
As a result, all TA of the clause are penalized by decreasing their states. 
In other words, Type I feedback combats false negatives by denying established sub-patterns and regenerating them in later learning process.

\input{TM_type_i_ii}

The probability of changing a TA state is determined by another hyperparameter, $s$, which indicates the probability of including a literal. The larger the value of $s$, the more/less likely a literal is to be included/excluded through Type I feedback. So far, the optimal values for both $T$ and $s$ are determined based on extensive trials aimed at achieving optimal accuracy~\cite{maheshwari2023redress, tarasyuk2023systematic}. 

If a clause incorrectly supports a class proposition, a false positive may occur. For instance, a positive/negative clause output is `1', when $y$=0/1. False positives are minimized by Type II feedback (Figure \ref{fig:type_i_ii} (b)). This type of feedback increases the TA states for the literals equaling `0', which potentially modifies the incorrect clause output of `1'. The TA states of a clause with output as `0' keeps unchanged, to avoid being trapped by local minima.

\section{ETHEREAL Model Compression} \label{sec:ethereal} 
%This section presents evidence of inefficiency in vanilla TM due to the inclusion of insignificant literals based on a case study, followed by the ETHEREAL to address this issue.
\subsection{Literal Significance in Learning Dynamics}
The TM feedback mechanism given in Figure \ref{fig:type_i_ii}
ensures faster convergence during the training regime, through the interactions between both types of feedback. 
In addition, accuracy generally improves as more literals are included to capture fine-grained sub-patterns, as described in Section \ref{sec:learn}. However, this training process overlooks the significance or the correlation of individual literals to the target class. For example, a literal that consistently equals `1' does not provide useful information for classification, yet it can still be included in many clauses without adversely affecting accuracy.
%Nevertheless, as the training proceeds, a TM always keeps expanding by including more literals. This is because: 1) it is only possible to exclude a literal from a clause through Type I feedback, and 2) in Type I feedback, the probability of increasing a TA state will be greater than that of decreasing it, if $s$ is greater than 2, which is unfortunately the case for nearly all reported TM models with good accuracy.

We conduct an exploratory experiment to demonstrate how a TM model expands during training. In our experiment, the TM is trained to classify MNIST handwritten digits~\cite{deng2012mnist}, chosen as a case study for its simplicity in visualizing such an image classification task for our later analysis. We set the number of clauses per class, $T$ and $s$ to 100, 10 and 3, respectively, and Booleanize the dataset by applying a threshold of 75 to all grayscale values. Figure \ref{fig:TM_acc_size} shows resulting test accuracy and model size. As can be seen, the accuracy tends to increase with more training epochs, which is accompanied by a large increment on number of includes. This trend of model expansion is seen to hold across datasets and hyperparameters, as more TAs are included through random selection of automata reinforcements through $s$ and $T$ parameters explained above.

\input{training}

%By definition, including more literals would be helpful by capturing more fine-grained sub-patterns related to the target, potentially improving the accuracy. However, such a learning process is conducted, disregarding the significance of a literal to the target. For example, a literal, consistently equaling `1', apparently does not provide useful information for the classification, but it is still possible to be included in many clauses, while not degrading the accuracy at all.

To investigate which literals are included during training, 
we visualize all complemented features in the image coordinate for a specific class (Figure \ref{fig:mnist_vanilla}). 
A notable observation from Figure \ref{fig:mnist_vanilla} (b) is that the features near digit outlines are more likely to be included in either positive or negative clauses, while those near the borders tend to be included in both types of clauses. This occurs because the border features do not effectively distinguish between classes, and can appear in samples from any class. Consequently, we conclude that insignificant literals are more likely to be included in both positive and negative clauses. Such observation is used to identify and exclude the insignificant literals, as described in Section \ref{sec:proposed}.     

\input{mnist_vanilla}

\vspace{-0.3cm}
\subsection{ETHEREAL Training} \label{sec:proposed}
The ETHEREAL training process consists of the following alternating steps,
repeated until the entire training is complete:
\begin{itemize}
	\item[1)] Conduct a specific number of standard training epochs, which is crucial for restoring any incorrectly excluded literals, as will be explained later.
	\item[2)] Identify all potentially insignificant literals, where a literal is considered as less insignificant if it is included in both positive and negative clauses.
	\item[3)] Exclude all potentially insignificant literals by adjusting their TA states, in exclusion process.
\end{itemize}

%To address the above objectives, initially 
Specifically, a TM is initially trained for a certain number of epochs, using the standard training process, enabling it to identify preliminary sub-patterns. Subsequently, literals shared by positive and negative clauses (denoted by $l_i$) are identified, followed by an exclusion process, as shown in Figure \ref{fig:exclude_overview}. 

\input{exclude_overview}

%We propose to exclude $l_i$ by properly adjusting their TA states, given in Figure \ref{fig:exclude_overview}.
%We consider a literal that is included in both positive and negative clauses as a potentially insignificant literal. Nevertheless, a significant literal may also be included in the two types of clauses, but in different proportions. Thus, there is no straightforward way to accurately evaluate literal significance. 
%We thereby propose the training-excluding procedure, in Figure \ref{fig:exclude_overview}, which excludes all potentially insignificant literals and then exploits a standard TM training process to discover the literal significance by itself. Specifically, after the first epoch, literal excluding and standard training are alternately performed till the end of training. During an excluding process, any literals that are simultaneously included in positive and negative clauses are identified, denoted by $l_i$ in Figure \ref{fig:exclude_overview}. These literals are then excluded from all clauses. We propose to exclude such literals by properly adjusting their TA states: 
%\todo{break large para into smaller para}
For any clause including $l_i$, the TA states of $l_i$ are reduced by N, assuming each TA has a total of 2N states. This scheme ensures that $l_i$ is completely excluded from all clauses, while preserving its relative TA state: 
a ``strong include" (indicating a relatively high TA state) becomes a ``weak exclude", and a ``weak include" becomes a ``strong exclude".
For clauses that do not contain $l_i$, TA states remain unchanged, allowing the excluded $l_i$ with a TA state near the middle to possibly be restored in later training epochs.
Finally, literals that appear only in positive or negative clauses remain as they are, treated as crucial literals with strong correlation to target. 

A relatively significant literal may be predominantly included in one of the two types of clauses, but also appears in the other. Such literals may be improperly excluded. However, they are expected to have many candidate clauses with TA states near the middle state, allowing them to be restored after one or more training epochs. 

%An excluding process is always followed by at least one training epoch, to restore some literals based on a self-exploration for literal significance. In specific, a relatively significant literal may be excluded during excluding process, but we expect it has plenty of candidate clauses with their TA states fairly close to the middle state, so that it can be easily restored after a or a few training epochs.

Figure \ref{fig:exclude_training} depicts the compressed TM model for MNIST. As can be seen, the model improves accuracy with fluctuations, while ETHEREAL results in a slower growth in the number of includes compared to the vanilla TM.
%the number of includes gradually increases
%\todo{This important message is not clear -- please elaborate nicely}. 
This gives a 46.6\% reduction in model size, with only a slight accuracy drop. An even greater reduction in model size is expected with additional training epochs.

%During each epoch, the excluding process firstly causes a reduction on both accuracy and model size, as it overly excludes some relatively important literals. These important literals are then restored in the following training epoch, which restores the accuracy but increases the model size. However, the overall accuracy keeps improving, accompanied by fluctuations, while the number of includes slowly increases. In such a case, compared with the vanilla TM, the training-excluding procedure causes a 0.78\% reduction on accuracy (from 95.77\% to 94.99\%), but realizes a 46.57\% compressed model, where the average number of includes per clause is reduced from 36.31 to 19.40. An even greater reduction on model size can be expected for more epochs.

\input{exclude_training}

In Figure \ref{fig:mnist_diet}, we visualize the complemented features in the image coordinate for the ETHEREAL TM. The less significant features are largely excluded, while the more significant ones are retained, ensuring minimal loss in accuracy.

\input{mnist_diet}

\section{Evaluation} \label{sec:eva}
%This section starts with an off-platform evaluation of ETHEREAL, focusing on model complexity and accuracy, followed by an on-platform evaluation, assessing performance metrics on a micro-controller.
\subsection{Experimental Setup}
To validate the proposed inference model, a ML pipeline (Figure \ref{fig:pipeline}) is applied to produce both vanilla and ETHEREAL TM models, encoded with REDRESS \cite{maheshwari2023redress} and deployed on STM32F746G-DISCO micro-controller via Micropython. 

\input{pipeline}

%\todo{refere to table I (dataset) from here???}
Eight real-world TinyML datasets (Table \ref{tab:dataset}) are selected from \cite{banbury2020benchmarking}, including electromyography (EMG) based gesture recognition \cite{lobov2018latent}, gas sensor array drift \cite{rodriguez2014calibration}, gesture phase segmentation (GPS) \cite{madeo2013gesture}, human activity recognition (HAR) \cite{anguita2013public}, mammographic mass \cite{elter2007prediction}, sensorless drive diagnosis \cite{dataset_for_sensorless_drive_diagnosis_325}, sport activity \cite{altun2010comparative}, and statlog (vehicle silhouette) \cite{mowforth1987}. 
To the best of our knowledge, there has been no prior work exploring TM models on these datasets. 
{Consequently, we determine all hyperparameters through trial and error, to achieve TM models with accuracy comparable to other reported ML algorithms \cite{karnam2022emghandnet, rodriguez2014calibration, madeo2013gesture, anguita2013public, elter2007prediction, jiang2016fault, altun2010comparative, king1995statlog}.
By definition, it is possible to further improve accuracy with more clauses, at the cost of greater computational resources \cite{granmo2018tsetlin,tarasyuk2023systematic}.} 
Relevant information about the datasets and hyperparameters is provided in Table \ref{tab:dataset}. Additional details and the source code for the entire pipeline are openly available at: \url{https://github.com/nsd5g13/TM4TinyML}. 

\input{dataset}

\vspace{-0.5cm}
\subsection{Off-Platform Evaluation}
For off-platform evaluation, we assess model complexity and accuracy.
For the vanilla and ETHEREAL TMs, we report the best test accuracy achieved in the total number of epochs along with the model sizes corresponding to this accuracy, in Table \ref{tab:acc_size}. As can be seen, ETHEREAL significantly decreases the number of includes by 39.29-87.54\%, except for mammographic mass and statlog, while resulting in a small reduction (0.78-3.38\%) in accuracy.
It is most notable that ETHEREAL achieves equal or even slightly improved accuracy with fewer literals for mammographic mass and statlog. 

\input{accuracy_size}

{The above results suggest that the performance of ETHEREAL is determined by the given features and target:}
for datasets with a large amount of noisy features, accuracy could remain the same or improve by retaining significant features and excluding noise. Conversely, for datasets that rely on interactions among numerous similarly significant features, ETHEREAL results in a slight decrease in accuracy as some features are excluded.
{The results are unrelated to the dataset or model scales, by comparing Tables \ref{tab:dataset} and \ref{tab:acc_size}.}

For each dataset, 
{we train a RF model using the raw features with the number of trees (5–30) and maximum depth (2–20) selected via grid search for the hightest test accuracy.}
We also train two single hidden layer BNN models using the Boolean features with 256 and 512 fully connected neurons (FC256 and FC512) using Larq \cite{geiger2020larq}. 
All the algorithms are capable of achieving comparable accuracy. As the accuracy of each algorithm can be improved with further tuning, we do not directly compare the accuracy; instead, we will evaluate them based on accuracy and other design metrics in Section \ref{sec:stm32}.
%While both the accuracy of the RF and BNN can be improved with further tuning, we do not directly compare the accuracy of RFs, BNNs and TMs; instead, we will evaluate them based on accuracy and other design metrics in Section \ref{sec:stm32}.

We evaluate the trade-off between accuracy and model size for vanilla and ETHEREAL TMs, in Figure \ref{fig:trade_off}, based on metrics from each training epoch. The results show that ETHEREAL consistently offers a better trade-off by producing models with fewer includes while maintaining comparable accuracy. This is most notable in Figure \ref{fig:trade_off} (d), (e) and (h). Although ETHEREAL does not always reach the highest accuracy as the vanilla TM, it still exhibits a superior trade-off at lower accuracy levels.  

\input{trade_off}

\vspace{-0.2cm}
\subsection{On-Platform Evaluation using STM32F746G-DISCO} \label{sec:stm32}
In the on-platform evaluation, both the vanilla and ETHEREAL TM are encoded using REDRESS and deployed on the micro-controller. ETHEREAL is expected to deliver shorter inference time, lower energy consumption, and a reduced memory footprint, compared to the REDRESS TM ($i.e.,$ the REDRESS-encoded vanilla TM).

\input{stm32_results}

Table \ref{tab:stm32_results} presents the inference time, memory footprint and energy obtained from the micro-controller for the RF, FC256 BNN, REDRESS TM, and ETHEREAL TM. The inference time and energy are averaged per datapoint, with energy measured using a Keithley DC power supply.
{Generally, RF provides the fastest inference and lowest energy consumption compared to BNN and TM, but it has the highest memory footprint due to its use of floating-point data representation. In contrast, BNN and TM result in more compact models by primarily using binary or Boolean values for logic operations, where ETHEREAL TM offers a 7$\times$ around reduction in memory footprint compared to RF, specifically for the case of mammography mass,}
This efficiency allows most ETHEREAL TM models to be deployed on state-of-the-art micro-controllers with up to 512 kB SRAM and 2 MB Flash \cite{lin2020mcunet}, unlike many other models in the comparison that exceed these limits.
Comparing the BNN and REDRESS TM across all datasets, both models generally exhibit similar memory footprints. Notably, TMs demonstrate significantly shorter inference time than BNNs for most datasets, with TMs achieving over 10$\times$ faster inference for mammographic mass and statlog.
This reduction in inference time also results in more than 10$\times$ lower energy consumption.
This is due to the high sparsity of a TA array, where most features are excluded from a TM after training, as described in Section \ref{sec:intro}. Consequently, these features are not used during inference, significantly reducing inference time and energy. In contrast, a BNN must consider all features during its inference process. 
While reducing the number of neurons or layers in a BNN could lower inference time and energy, it would also lead to a further decline in accuracy. Notably, the FC256 BNN has already demonstrated lower accuracy compared to the vanilla TMs, as shown in Table \ref{tab:acc_size}.
Finally, compared to REDRESS TMs, ETHEREAL TMs demonstrate reductions across all metrics, corresponding with the percentage decrease in the number of includes presented in Table \ref{tab:acc_size}.
This can be expected as ETHEREAL utilizes fewer includes during inference, which reduces both inference time and energy consumption, while also decreasing runtime memory usage. Furthermore, since REDRESS retains only the information of included literals, ETHEREAL further reduces the memory required to store the model.

\section{Conclusion} \label{sec:conc}
We introduced ETHEREAL, a model compression method for TM. ETHEREAL excludes insignificant literals based on their occurrences in both positive and negative clauses. 
This exclusion is facilitated by a modified training regime. Compared to the vanilla TM, ETHEREAL TM achieves up to an 87.54\% reduction in number of includes, while resulting in only a 3.38\% decrease in accuracy across eight TinyML applications. The reduction in model size leads to proportional reductions in inference time, memory footprint and energy, for micron-controller based implementations. 
{The reduction of accuracy is a reasonable compromise for substantial gains in inference speed and reduced resource consumption.} 
Compared to BNNs, ETHEREAL TM offers over 10$\times$ less inference time and energy; compared to RF, ETHEREAL TM provides up to 7$\times$ less memory usage. In summary, ETHEREAL enhances the trade-off between accuracy and model size, promoting efficient TM implementations with low cost, high speed and trustworthy behavior.

%Future work will focus on investigating the impact of hyperparameters on ETHEREAL and evaluating its performance across various implementations, including Application-Specific Integrated Circuits (ASICs).

\begin{acks}
This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) under Grant EP/X039943/1 and Grant EP/X036006/1.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}

\end{document}
\endinput
%%
%% End of file `sample-tinyml.tex'.
