\section{\emph{\color{Orange}Ola}: Omni-Modal Understanding}
\label{sec:method}

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{figs/fig1-new.pdf} 
% \fbox{\rule{0pt}{2.5in} \rule{0.9\linewidth}{0pt}}
\caption{\textbf{Illustrations of the \textit{Ola} Progressive Modality Alignment. }We visualize the relationships among modalities in the left part. Speech acts as the connection between language and audio knowledge, while video constructs the bridge with highly relevant visual and audio information. Therefore, we design the progressive alignment training strategy from primary to periphery. Furthermore, we design the cross-modality video-audio data to better capture the relationships among modalities. }
\label{fig:training}
\end{figure*}

We put main efforts into three aspects to obtain an omni-modal understanding for \textit{Ola}, capable of reading, hearing, seeing, typing, and speaking arbitrarily. 1) The \textit{Ola} architecture introduced in Sec.~\ref{sec:method_arch} supports omni-modal inputs and streaming outputs for both text and speech. 2) We design the progressive training strategy in Sec.~\ref{sec:method_train} to bridge the modality gaps between language and vision from primary to periphery. 3) Effective omni-modal training data in Sec.~\ref{sec:method_data} provide strong performance across all the benchmarks, especially with our proposed cross-modal video data that stresses to learn audio-relevant information from videos. 

\subsection{\textit{\textbf{\color{Orange}Ola}} Architecture} \label{sec:method_arch}

A general view of \textit{Ola} architecture is illustrated in Fig.~\ref{fig:sota}. The encoding part accepts omni-modal inputs in text, image, video, and audio formats with modal-specific encoders or embeddings. Subsequently, the joint alignment operations process all the inputs in a unified manner, fusing and concatenating all the sequences into flattened tokens for the core \textit{Ola} Large Language Model. The LLM generates text tokens in serial, and we adopt the speech decoder for streaming speech decoding.

\paragrapha{Omni-Modal Inputs Encoding. }We encode visual, audio, and text inputs separately based on the successful practice of the previous text-to-one-modal Large Language Models. For visual inputs that include images $I$, videos $V_{1, 2, \cdots, n}$ with $n$ frames, we follow vision-language models~\citep{liu2024llava,li2023blip2} to use multi-modal visual encoder $\mathcal{E}_{I,V}(I, V_{f_1, f_2, \cdots})$ for encoding. Note that we preserve the original aspect ratio of each image or frame for the arbitrary resolution vision encoder OryxViT~\citep{liu2024oryx} initialized from SigLIP-400M~\citep{zhai2023siglip}, as OryxViT performs a more natural solution for visual inputs. We obtain the image features $f_{I},$ and the video features for each frame $[f_{V_{1}},f_{V_{2}}, \cdots, f_{V_{n}}]$ based on the image patches. 

For audio encoding, we propose the dual encoder approach for the audio input. Specifically, we use Whisper-v3~\citep{radford2022whisper} as the speech encoder and BEATs~\citep{Chen2022beats} as the music encoder for better alignment between audios and texts, providing richer audio information. The music encoder takes the original wav audio $A$ as inputs, while the speech encoder takes the wav transformed into Mel spectrogram representation $A_{(mel)}$ as inputs. Note that the Whisper encoder only supports a certain length of audio inputs, therefore we fix the sample rate as 16000Hz and cut the overlong audio into segments of 30 seconds $A_1, A_2, \cdots, A_n$ and conduct the encoder operation in batches $[f_{A_1},f_{A_2},\cdots,f_{A_n}]=\mathcal{E}_A([A_1, A_2, \cdots, A_n])$. The embedding features of the speech and music encoders are concatenated across channel dimensions for the comprehensive audio features $f_A$. 

For the text inputs, we use the carefully designed tokenizer and the well-trained embedding layers from the pre-trained Large Language Model for the text tokens $t_T$ directly.

\paragrapha{Joint Alignment for Vision and Audio. }The alignment modules act as the converter from specific-modal spaces to the text embedding space, which is an essential part of omni-modal Large Language Models. To reduce the token length of visual features for higher efficiency, we obtain one step forward based on the motivation of structural downsampling in previous works~\citep{li2024minigemini}, and propose the \textit{Local-Global Attention Pooling} layer for better downsampled features with less information loss. Specifically, for image or video frame feature in spatial shape $H\times W$ and channel $C$, we adopt the bilinear interpolation for 2x downsampling to obtain $f^{\text{global}}$, which contains the global information of the downsampled region. We combine the original and global features for local-global embeddings and use Softmax to predict the importance $\pi$ of each downsampled spatial region:
\begin{equation}
    f=\text{Concat}[f,f^{\text{global}}], \ \ \pi=\text{Softmax}(\text{MLP}(f))
\end{equation}
The downsampled feature $f^{\text{global}}$ determines the weight of each previous region based on the score $\pi$ with the Hadamard product.

We apply the simple yet effective two-layer non-linear MLP connectors $\text{MLP}_{A},\text{MLP}_{V}$ following the previous works~\citep{liu2024llava,liu2024llava15} to project the specific modal features $[f_I,f_V,f_A]$ into unified tokens $[t_I,t_V,t_A]$. We define visual and audio start, separate, newline, and end tokens to indicate special positions for inputs. The omni-modal tokens $[t_I,t_V,t_A]$ are concatenated with text tokens $t_T$ in free combination for LLM decoding. 

\paragrapha{Streaming Speech Generation. }We adopt CosyVoice~\citep{du2024cosyvoice} as a high-quality speech decoder for speech generation. To support user-friendly streaming decoding, we detect the generated text tokens in real time and truncate the sentence once punctuation is encountered. Afterward, the previous sentence is fed into the speech decoder for audio synthesis. Therefore, \textit{Ola} does not need to wait for the whole sentence to finish while supporting streaming decoding. Though several attempts~\citep{fang2024llamaomni,xie2024miniomni2} have been made to train the speech generation module end-to-end, the external text-to-speech decoder is a more efficient, high-quality, and training-free solution for omni-modal models.

\subsection{Progressive Omni-Modal Alignment} \label{sec:method_train}

\paragrapha{Rethinking Modality Gaps among Language, Vision and Audio. }From our exploration, we recognize two critical issues in omni-modal training. 1) \textit{Modal balancing.} As illustrated in Fig.~\ref{fig:progressive}, directly combining data from all modalities negatively affects benchmark performance. Therefore, we propose a rational training procedure that progressively equips the sense organ to the \textit{Ola} model. We assert that texts and images are the core modalities in omni-modal learning, while speeches and videos are variants of texts and images, respectively. Learning to recognize texts and images ensures the model's basic cross-modal ability, so we prioritize these harder cases. Subsequently, we gradually incorporate video, audio, and speech into the training for the omni-modal LLM. 2) \textit{Connections between audio and vision. } Another problem lies in building connections between audio and vision, which has been overlooked by previous works. However, jointly learning audio and vision data can yield surprising results in omni-modal learning by providing a more comprehensive view across different modalities. For the \textit{Ola} model, we consider video as the bridge between audio and vision, as videos contain natural, abundant, and highly relevant information between frames and the accompanying audio. We test our hypothesis by optimizing the training pipeline and preparing targeted training data, as introduced below.

\paragrapha{Stage 1: Text-Image Training. }The \textit{Ola} training starts from a pre-trained Large Language Model, where we use Qwen2.5-7B~\citep{qwen2.5} in our implementation for better trade-offs for model sizes and performance. The \textit{Ola} text-image training includes MLP alignment, large-scale pre-training, and supervised fine-tuning following common practice in large-scale multi-modal learning~\citep{tong2024cambrian,li2024llavaov}. We initialize the visual MLP adapter and freeze other parameters in MLP alignment with the image captioning task. Subsequently, we unfreeze all the parameters including the vision encoder in the pre-training and supervised fine-tuning phase. The downsampling module is well-trained in the text-image training stage to hold the 2x compression for visual data including images and videos.

\paragrapha{Stage 2: Continuous Training for Images and Videos. }Based on a strong text-image multi-modal LLM, we continuously extend the capability for \textit{Ola} with video data. We keep most of the experimental setting for supervised fine-tuning while freezing the vision encoder in this stage as the encoder is already fully trained beforehand. We mix the previous image data and the video data to preserve the original text-image performance. 

\paragrapha{Stage 3: Bridging Vision and Audio with Videos. }The audio-relevant training is included in stage 3. We follow the training strategy for the visual MLP adapter while initializing the audio MLP adapter with a basic audio-speech recognition (ASR) task. Then we mix up the text \& speech understanding, text \& music understanding, audio \& video joint comprehension, and the foremost text-image multi-modal tasks together for the formal training. \textit{Ola} concentrates on learning audio recognition and identifying the relationships between vision and audio in this stage, resulting in a comprehensive image, video, and audio understanding model after training. 

%\paragrapha{Stage 4: Learning to Speak. }In the last stage, we freeze all the trunk parameters while training the speech decoder $\mathcal{D}_{\text{speech}}$ sorely. We generate the targeting speech tokens for supervision with a pre-trained HuBERT~\citep{hsu2021hubert} encoder and cluster the continuous features into $N$ discrete units with K-Means~\citep{hartigan1979kmeans} following previous works~\citep{zhang2023speechgpt}. As the predicted speech tokens from $\mathcal{D}_{\text{speech}}$ cannot form one-to-one correspondence with labels, we integrate the widely-used CTC Loss~\citep{graves2006ctc} to perform vague cross-entropy alignment. 


\subsection{Data} \label{sec:method_data}

The training data of \textit{Ola} includes the general supervised fine-tuning data collected from open-source academic datasets in image, video, and audio categories. Additionally, we design a pipeline to generate cross-modal video-audio data for omni-modal alignment. 

\paragrapha{Image Data. }We follow the simple setting in~\citep{liu2024llava15} for image MLP alignment. The MLP alignment data includes 800k image captioning pairs from the LAION dataset~\citep{schuhmann2021laion}. For the large-scale pre-training phase, we collect around 20M text-image data pairs from open-sourced and in-house data to build the basic capability of the model. For text-image supervised fine-tuning data, we collect abundant data from various tasks including captions, conversations, OCR, etc. The source of the training data involves the mixture of LLaVA-OneVision~\citep{li2024llavaov}, Cauldron~\citep{laurençon2024cauldron}, Cambrian-1~\citep{tong2024cambrian}, MAmmoTH-VL~\citep{guo2024mammoth}, PixMo~\citep{deitke2024molmo}, etc., resulting in 7.3M image training data in total. 

\paragrapha{Video Data. }For text-video training data, we collect useful video datasets from LLaVA-Video-178K~\citep{zhang2024videoinstructiontuningsynthetic}, VideoChatGPT-Plus~\citep{maaz2023videochatgpt}, LLaVA-Hound~\citep{zhang2024hound}, and Cinepile~\citep{rawal2024cinepile}, with 1.9M video conversation pieces in total. We randomly sample 2/3 video-language data pairs from LLaVA-Video-178K, resulting in 1.2M high-quality training data, and we use the full set of other data sources. In stage 2 for multi-image training, we randomly sample 0.8M image data from stage 1 and mix it with the video datasets for continuous training to maintain the basic performance. 

\paragrapha{Audio Data. }We prepare audio training data for comprehensive speech and music understanding. For text-speech understanding, we design multiple tasks including ASR from LibriSpeech~\citep{panayotov2015librispeech} and GigaSpeech~\citep{chen2021gigaspeech} datasets, audio captioning from AudioCaps~\citep{kim2019audiocaps} and Clotho~\citep{drossos2020clotho} datasets, speech question answering from LibriSpeech~\citep{panayotov2015librispeech} datasets, audio question answering from WavCaps~\citep{mei2024wavcaps} and AudioCaps~\citep{kim2019audiocaps} datasets. For text-music understanding, we collect tasks including music captioning from MusicCaps~\citep{agostinelli2023musiclm}, music question answering from MillionSong~\citep{mcfee2012millionsong} and MusicNet~\citep{thickstun2016musicnet}. The overall audio training data involves 1.1M samples. The relevant text question-answering representations are collected from SALMONN~\citep{tang2023salmonn}.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
  \centering
  \caption{\textbf{Main Results across Image, Video, and Audio Understanding Benchmarks. }We select representative benchmarks among image, video, and audio benchmarks, and select mainstream state-of-the-art open-source large language models in each modality. We also include open-source omni-modal LLMs for comparison. In the table, "$-$" indicates the model is capable of solving the tasks theoretically, while the result is lacking. "\xmark" indicates that the model is not capable of the task. $\downarrow$ indicates that lower score is better. * LLaMA-Omni is not optimized for ASR and thus cannot produce reasonable results on this task.}
  \adjustbox{width=\linewidth}{
    \begin{tabular}{llcccccccccccc}
    \toprule
    \multirow{2}[2]{*}{Model} & \multirow{2}[2]{*}{Size} & \multicolumn{7}{c}{Image Benchmarks} & \multicolumn{3}{c}{Video Benchmarks} & \multicolumn{2}{c}{Audio Benchmarks} \\
    \cmidrule(lr){3-9}\cmidrule(lr){10-12}\cmidrule(lr){13-14}
          &       & \rotatebox{45}{MMBench-1.1} & \rotatebox{45}{MMStar} & \rotatebox{45}{MMMU} & \rotatebox{45}{MathVista} & \rotatebox{45}{HalluBench} & \rotatebox{45}{AI2D} & \rotatebox{45}{OCRBench} & \rotatebox{45}{VideoMME} & \rotatebox{45}{LongVideoBench} & \rotatebox{45}{MVBench} & \rotatebox{45}{LibriSpeech$\downarrow$} & \rotatebox{45}{AIR-Bench} \\
    \midrule
    \multicolumn{14}{l}{\textit{Image LLMs}} \\
    \midrule
    Cambrian-1~\citep{tong2024cambrian} & 8B & 68.2 & 50.7 & 41.8 & 48.1 & 30.6 & 74.6 & 614 & \xmark & \xmark & \xmark & \xmark & \xmark \\
    Pixtral~\citep{agrawal2024pixtral} & 12B & 72.7 & 54.5 & 51.1 & 56.3& 47.0 & 79.0 & 685 & \xmark & \xmark & \xmark & \xmark & \xmark \\
    \midrule
    \multicolumn{14}{l}{\textit{Video LLMs}} \\
    \midrule
    VideoCCAM~\citep{fei2024videoccam} & 9B   &$-$  &$-$   &$-$  &$-$   &$-$    &$-$    &$-$    & 53.9  &$-$    & 64.6  & \xmark & \xmark \\
    LLaVA-Video~\citep{zhang2024videoinstructiontuningsynthetic} & 7B  &$-$  &$-$  &$-$    &$-$  &$-$   &$-$    &$-$ & 63.3 & 58.2 & 58.6  & \xmark & \xmark \\
    \midrule
    \multicolumn{14}{l}{\textit{Vision Comprehensive LLMs}} \\
    \midrule
    LLaVA-OneVision~\citep{li2024llavaov} & 7B & 80.9 & 61.9 & 47.9 & 62.6 & 31.6 & 82.4 & 622 & 58.2  & 61.3  & 59.4  & \xmark & \xmark \\
    MiniCPM-V 2.6~\citep{yao2024minicpm} & 8B & 78.0 & 57.5 & 49.8 & 60.8 & 48.1 & 82.1 & 852 & 60.9  &$-$    &$-$    & \xmark & \xmark \\
    %Qwen2-VL~\citep{qwen2vl} & 7B & 81.0 & 60.7 & 53.7 & 61.6 & 50.4 & 83.0 & 843 & 63.3 & $-$ &$-$    & \xmark & \xmark  \\
    InternVL2.5~\citep{chen2024intern25} & 8B  & 82.5 & 63.2 & 56.2 & 64.5 & 49.0 & 84.6 & 821 & 64.2 & 60.0 & \textbf{72.0}   & \xmark & \xmark   \\
    Qwen2.5-VL~\citep{Qwen2.5-VL} & 7B & 82.6 & 64.1 & 56.2 & 65.8 & \textbf{56.3} & 84.1 & \textbf{877} & 65.1 & 54.7 & 69.6 &  \xmark & \xmark \\
    \midrule
    \multicolumn{14}{l}{\textit{Audio LLMs}} \\
    \midrule
    SALMONN~\citep{tang2023salmonn} & 13B  & \xmark & \xmark  & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & 3.5   & 6.12  \\
    Qwen2-Audio~\citep{chu2024qwen2audio} & 7B   & \xmark & \xmark & \xmark  & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \textbf{2.5}   & \textbf{6.93}  \\
    \midrule
    \multicolumn{14}{l}{\textit{Omni-Modal LLMs}} \\
    \midrule
    LLaMA-Omni~\citep{fang2024llamaomni} & 8B & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & 120.4$^*$ & 4.70  \\
    Mini-Omni2~\citep{xie2024miniomni2} & 0.5B  & 32.1 &$-$ & 24.9 &$-$ &$-$ &$-$ & 6 &$-$    &$-$    &$-$    & 7.2   & 3.20  \\
    VITA-1.5~\citep{fu2025vita1_5}  & 7B & 76.8 & 60.2 & 52.6 & 66.2 & 44.6 & 79.2 & 741 & 56.1 & $-$ & 55.4 & 5.4 & 4.54 \\
    IXC2.5-OmniLive~\citep{internlmxcomposer2_5_OL} & 8B & 79.4 & 59.9 & 42.9 & 64.0 & 43.1 & 81.6 & 686 & 60.6 & $-$ & 68.7 & 4.4 & 1.67\\
    \midrule
    \rowcolor{Lightorange}\textit{\textbf{Ola}} & 7B & \textbf{84.3} & \textbf{70.8} & \textbf{57.0} & \textbf{68.4} & 53.5 & \textbf{86.1} & 827 & \textbf{68.4} & \textbf{61.4} & 66.3 & 3.1 & 6.41 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:main}%
\end{table*}%


\paragrapha{Generating Cross-Modal Video Data. }Most existing video training data are annotated or synthesized solely from frame inputs, often overlooking the valuable information in accompanying audio. To address this, we designed a pipeline to generate cross-modal video data, aiming to uncover the intrinsic relationships between video and audio. This guides an omni-modal large language model in learning cross-modality information. Specifically, we developed two tasks for cross-modal learning: video-audio question answering and video speech recognition. We collected videos from the academic dataset LLaVA-Video-178k~\citep{zhang2024videoinstructiontuningsynthetic} and the open-ended video datasets from FineVideo~\citep{Farré2024FineVideo}. Due to the lack of subtitles in the academic datasets, we used Whisper-v3~\citep{radford2022whisper} to generate subtitles from the video audio and conducted a language-based cleaning procedure. We then employed a large language model to assess whether the subtitles were complete and informative. We gathered 41k pure videos from LLaVA-Video-178k, along with the original 42k videos from FineVideo. Subsequently, we used Qwen2-VL-72B~\citep{qwen2vl} to generate questions and answers based on the video and corresponding subtitles. The model was instructed to focus on the subtitle inputs while using the videos as supplementary information. We created three question-answer pairs for each video, resulting in 243k cross-modal video-audio data points. Additionally, we included the original video subtitling tasks with 83k training data to help the model maintain its ASR ability in noisy environments. During training, the models processed multiple image frames, audio, and text inputs simultaneously, significantly enhancing their cross-modal capabilities.

The prepared 324k cross-modal video data is mixed with audio data implemented above for stage 3 training. We mix all the 1.1M pure text-audio training data and 324k cross-modal video-audio data for the comprehensive training stage. Additionally, we sample 400k image data from stage 1 to maintain the basic ability and create 200k image data with voice instructions to equip the model with interaction capability. 