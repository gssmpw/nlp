\appendix
\section*{Appendix}

\section{More Details}

We provide more details that are not implemented in the main paper. Specifically, we provide the detailed architecture for the model, more training details about \textit{Ola}'s progressively modality alignment, and details of the data preparation procedure. 

\subsection{Model Details}

The visual encoder for \textit{Ola} is based on the SigLIP-400M~\citep{zhai2023siglip} backbone and is further fine-tuned for native-resolution visual inputs. The patch size for the visual encoder is set to 16, the hidden dimension is 1152, and the MLP hidden dimension is 4304. The SigLIP-400M model consists of 27 transformer blocks and 16 attention heads. The audio encoder for \textit{Ola} is built on the whisper-v3-large~\citep{radford2022whisper} model. The length of the input audio tensor for the whisper model is fixed at 480,000; therefore, we chunk the entire audio tensor into pieces and concatenate the audio features. The mel size for the whisper-v3-large model is set to 128, and the hidden dimension for the speech features after the whisper-v3-large model is 1280.

For the connector layer, we utilize a 2-layer MLP for feature projection. We initialize two separate MLPs for visual and audio features, respectively. The input dimension matches that of the visual or audio encoder, and the output dimension matches the LLM dimension. For the \textit{Local-Global Attention Pooling} layer, we use a predictor to calculate the score based on the concatenated features. Therefore, the dimension for the predictor is 2$\times$ to 1$\times$ dimension.

We integrate Qwen-2.5-7B~\citep{qwen2.5} into the \textit{Ola} large language model. The Qwen-2.5-7B model has a hidden LLM dimension of 3,584 and an intermediate size of 18,944. It consists of 28 transformer layers. The basic architecture for the speech decoder mirrors that of the Qwen-2.5-7B LLM, but we use only a 2-layer transformer block for the speech decoder. During generation, the maximum number of classification categories for the unit speech tensor is set to 1,000. We use a pre-trained speech vocoder to convert the unit speech tensor into speech waveforms. During inference, speech outputs are generated whenever a punctuation mark is detected, ensuring that the features of streaming outputs are preserved.

\subsection{Training Details}

As stated in the main paper, the progressive modality alignment procedure is conducted in four stages. The first image-text stage involves adapter pre-training and supervised fine-tuning. The adapter pre-training stage is conducted on 808k image captioning data collected from the LAION datasets. During pre-training, we unfreeze the parameters for the connector while keeping other parameters frozen. We set the training batch size to 256 and the overall learning rate to 1e-3. The supervised fine-tuning stage is conducted on 7.3 million image-text data pairs. During image-text training, input images are maintained at their original aspect ratio, with the maximum image size restricted to 1536×1536. We set the batch size to 128 and the overall learning rate to 2e-5. The stage 1 experiment is conducted on 64 NVIDIA A800 GPUs.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/fig-supp1.pdf}
\caption{\textbf{Showcases on Text and Audio Understanding.}}
\label{fig:supp1}
\end{figure*}

Stage 2 integrates both image and video data for supervised fine-tuning, following most of the training strategies from Stage 1. The total amount of training data is 2.7 million, comprising 800k image-text pairs sampled from Stage 1 and 1.9M video data collected from open-source datasets. We set the training batch size to 256 and the overall learning rate to 2e-5. The model's maximum sequence length is set to 16k. The maximum number of frames is set to 64. The Stage 2 experiment is conducted on 64 NVIDIA A800 GPUs. 

Stage 3 involves joint training in the audio domain. We conduct a projector alignment procedure similar to the image pre-training for initializing the speech adapter. During the speech adapter pre-training, we unfreeze the parameters of the speech adapter while freezing the other parameters. We set the batch size to 256 and the overall learning rate to 1e-3. The pre-training phase is conducted on the 370k LibriTTS~\citep{zen2019libritts} dataset. After pre-training, we integrate audio-video joint alignment by combining image data, video data, and pure audio data. We use 600k image data, 1.1M audio data, and 243k video-audio training data. The training batch size is set to 128, and the overall learning rate is set to 1e-5. We maintain the original audio data for inputs in the audio and video data and append the necessary prompts for instructions. Specifically, for the ASR tasks, we set the ASR prompt as \textit{"Please give the ASR results of the given speech."} For the audio instruction tasks, we set the instruction-following prompt as \textit{"Please directly answer the questions in the user's speech."} We maintain the maximum frame number for the video and set the maximum speech chunk number to 20. The Stage 3 experiment is conducted on 64 NVIDIA A800 GPUs.

\subsection{Data Collection Details}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/fig-supp2.pdf}
\caption{\textbf{Showcases on Video Understanding.}}
\label{fig:supp2}
\end{figure*}

We provide details on collecting video-audio relevant data. Our data comes from two sources with high-quality raw videos: LLaVA-Video-178k~\citep{zhang2024llavanextvideo}, which contains 178k raw videos, and FineVideo~\citep{Farré2024FineVideo}, which contains 42k raw videos. For the open-sourced video data from LLaVA-Video-178k, we first use the Whisper~\citep{radford2022whisper} model to generate subtitles. We find that the videos include content in other languages and videos without valid audio, so we design a filtering method for better results. Specifically, we first assess the ratio of English words in the generated subtitles and discard those with a lower ratio, indicating subtitles in other languages. We also discard extremely short subtitles. Then, we use a large language model, Qwen-2.5-72B, to further filter the subtitles. The model is asked to identify meaningless sentences with the following prompt: \textit{"I will give you a subtitle generated from a video. Identify whether the subtitle is complete, fluent, and informative. Answer directly with yes or no and do not add other explanations."} After this procedure, we gathered 41k valid videos. For the videos in FineVideo, as they are already well-processed, we directly use the subtitles for the following steps. We utilize Qwen-2-72B to generate audio-relevant question-answer pairs based on the given videos and subtitles. The prompt for the vision-language model is: \textit{"Please generate at least three questions and answers based on the information in the subtitle. You can refer to the video for additional context. The questions and answers must be highly relevant to the subtitle and video and should not include fabricated content."} We then generate 243k cross-modal video-audio data points from the 81k collected videos. This data is used for stage 3 training for omni-modal alignment.

\section{More Showcases}

\subsection{Text and Audio Understanding}

In this subsection, we provide more practical text-audio understanding samples for visualizations. The inputs of the text-audio understanding are a mixture of audio and text instructions, which can strongly test the cross-modal capability for \textit{Ola} model. Results are shown in Fig.~\ref{fig:supp1}, we provide results on music-related, speech-related, and sound-related audio inputs and \textit{Ola} excels at all the circumstances with a strong performance on mixed audio and text understanding. 

\subsection{Video Understanding}

In this subsection, we provide more results on video understanding tasks and provide comparisons with state-of-the-art vision LLM. Results are shown in Fig.~\ref{fig:supp2}. With the capability to recognize video, audio, and text jointly, \textit{Ola} can gather more information from the video. 
