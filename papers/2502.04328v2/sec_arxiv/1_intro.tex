\section{Introduction}
\label{sec:intro}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/fig0-new.pdf} 
\caption{\textbf{\textit{Ola} Architecture. }\textit{Ola} supports omni-modal inputs including text, image, video, and audio, capable of processing the inputs simultaneously with competitive performance on understanding tasks for all these modalities. Meanwhile, \textit{Ola} supports user-friendly real-time streaming decoding for texts and speeches thanks to the text detokenizer and the speech decoder. }
\label{fig:sota}
\end{figure*}

Multi-Modal Large Language Models are drawing increasing attention owing to their strong instruction-following capabilities and abundant knowledge of handling complex inputs including texts, images, videos, and audio. Based on the strong performance of open-sourced large language models~\citep{qwen2,young2024yi}, extensive research has been done on connecting specific modalities with language responses~\citep{tong2024cambrian,li2024llavaov,chen2024internvl,qwen2vl,lin2023videollava,qian2024streaming}. Recently, the success of GPT-4o~\citep{GPT4o} and Gemini~\citep{reid2024gemini} aiming at supporting more modalities in Large Language Models inspires researchers to take one important steps towards omni models that understand all the inputs in one model. 

% % Table generated by Excel2LaTeX from sheet 'Sheet3'
% \begin{table}[t]
%   \centering
%   \caption{\textbf{Comparisons among Multi-Modal Family. } We compare \emph{Ola} with existing open omni-modal models and representative multimodal models on their abilities for image, video and audio understanding as well as the streaming speech generation function.  For fair comparisons, we select around 7B versions of existing MLLMs, except VITA and Mini-Omni2 which only have 8$\times$7B and 0.5B sizes. * VITA can generate speech while not supporting streaming. ** LLaMA-Omni is not optimized for ASR and thus cannot produce reasonable results on this task. \emph{Ola} can achieve competitive performance in all modalities thanks to our progressive alignment strategy. }\vspace{-8pt}
%   \adjustbox{width=\linewidth}{
%     \begin{tabular}{llcccc}
%     \toprule
%     Model & \makecell{LLM\\Type}  & \makecell{Image QA \\ \emph{MMBench}} & \makecell{Video QA \\ \emph{VideoMME}} & \makecell{ASR \\ \emph{LibriSpeech} $\downarrow$} &  \makecell{Streaming \\ Speech Gen} \\
%     \midrule
%     Cambrian-1~\citep{tong2024cambrian} & Image & 75.3 & \xmark & \xmark &  \xmark \\
%     Video-CCAM~\citep{fei2024videoccam} & Video & \xmark & 53.9 & \xmark &  \xmark \\
%     LLaVA-OneVision~\citep{li2024llavaov} & Vision & 80.8 & 58.2 & \xmark & \xmark \\
%     Qwen2-Audio~\citep{chu2024qwen2audio} & Audio & \xmark & \xmark & 2.5  & \xmark \\
%     \midrule
%     VITA~\citep{fu2024vita}  & Omni  & 71.8 &55.8 & 12.7 & * \\
%     LLaMA-Omni~\citep{fang2024llamaomni} & Omni  & \xmark & \xmark & ** & \cmark \\
%     Mini-Omni2~\citep{xie2024miniomni2} & Omni  & 32.1  & \xmark & 7.2 & \cmark \\
%     \midrule
%     \emph{\textbf{\color{orange}Ola}} & Omni  & 80.5 & 65.8 & 4.0 & \cmark \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:teaser}%
%   }\vspace{-10pt}
% \end{table}%

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figs/fig2.pdf} 
\caption{\textbf{Progressive modality alignment helps to learn better omni-modal models. } We compare our progressive alignment strategy with two baseline training pipelines on Image QA(MMBench~\citep{liu2023mmbench}), Video QA(VideoMME~\citep{fu2024videomme}), and ASR(LibriSpeech~\citep{panayotov2015librispeech}): 1) direct mixing where all instruction tuning data is merged and trained in a single stage, and 2) balanced sampling where we upsample certain sources to make the training data more balanced among modalities. The experiment is conducted on a subsampled training set for efficiency and we train models for the same number of steps for fair comparisons. The score is normalized based on the score of progressive alignment to calculate the relative score and the ASR score is inverted as lower is better for the WER metric.}
\label{fig:progressive}
\end{figure}


% % Table generated by Excel2LaTeX from sheet 'Sheet3'
% \begin{table}[t]
%   \centering
%   \caption{\textbf{Progressive modality alignment helps to learn better omni-modal models. } We compare our progressive alignment strategy with two baseline training pipelines: 1) direct mixing where all instruction tuning data is merged and trained in a single stage, and 2) balanced sampling where we upsample certain sources to make the training data more balanced among modalities. The experiment is conducted on a subsampled training set for efficiency and we train models for the same number of steps for fair comparisons.}
%   \adjustbox{width=\linewidth}{
%     \begin{tabular}{L{120pt}C{65pt}C{65pt}C{65pt}}
%     \toprule
%     Training Strategy & \makecell{Image QA \\ \emph{MMBench}} & \makecell{Video QA \\ \emph{VideoMME}} & \makecell{ASR \\ \emph{LibriSpeech} $\downarrow$} \\
%     \midrule
%     Direct Mixing & \textbf{81.0} & 57.3 & 6.5 \\
%     Balanced Sampling & 73.2 & 58.0  & \textbf{3.8} \\
%     Progressive Alignment (\emph{\textbf{\color{orange}Ola}}) & \textbf{80.5} & \textbf{65.8}  & \textbf{4.0} \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:teaser}%
%   }
% \end{table}%

The core challenges in training omni-modal Large Language Models lie in the modeling of modalities in various distributions, and the design of an effective learning pipeline to achieve competitive, balanced performance on all the supported tasks. Several attempts have been made to overcome the difficulty of omni-modal models~\citep{fu2024vita,fang2024llamaomni,xie2024miniomni2}, where we illustrate the mainstream works and the state-of-the-art MLLMs~\citep{tong2024cambrian,zhang2024videoinstructiontuningsynthetic,li2024llavaov,chu2024qwen2audio} in specific domains in Fig.~\ref{fig:teaser}. Impressive performance and modality breadth are contradicted in previous works, while existing open-sourced omni-modal solutions still have a large performance gap between state-of-the-art specialized LLMs, making a strong barrier between the concept of omni-modal and real-world applications. Moreover, the lack of capability in specific domains or tasks, the mass data demand, the delay for user interaction, and the inadequate alignment across modalities show the suboptimal for existing omni-modal models. 

In this paper, we propose the \textit{Ola} model, exploring the solution for training an omni-modal Large Language Model with comparable performance with state-of-the-art specific LLMs, real-time interaction, and high efficiency on alignment data. The core design of the \textit{Ola} model is the progressive modality alignment strategy. To build connections between language and vision, we start from two fundamental and separated modalities, image and text, to build the basic knowledge for omni-modal models. Subsequently, we gradually expand the training sets to equip the model with an extended ability including video frames that strengthen the visual understanding capability, speech data that connects the language and audio knowledge, and the video with audio that mixes up the information from language, video, and audio comprehensively. The progressively learning strategy makes omni-modal learning easier by disassembling the complex training procedure into small steps, therefore maintaining a small size of cross-modal alignment data and making it easier to start from existing achievements in vision-language models. As shown in Fig.~\ref{fig:progressive}, the performance of Ola largely benefits from our progressive training pipeline, leading to more balanced and competitive results on all modalities.

To cooperate with the training strategy, important improvements have been made to the architecture and data domains. 1) The \textit{Ola} architecture supports omni-modal inputs and streaming text and speech generation with extendable and concise architecture design. We design the joint alignment module for vision and audio, fusing the visual inputs with a local-global attention pooling layer, and make free combinations for visual, audio, and text tokens. Moreover, we integrate the sentence-wise streaming decoding module for high-quality voice synthesis. 2) Beyond the collected fine-tuning data in vision and audio aspects, we deeply excavate the relationships between video and the corresponding audio to construct the bridge between visual and audio modality. Specifically, we collect raw videos from academic and open-ended web sources, design separated cleaning pipelines, and then utilize vision-language models to generate question-answering pairs based on the subtitles and video content.

We evaluate \textit{Ola} under the complete omni-modal benchmarks including image, video, and audio aspects. With only 7B parameters, \textit{Ola} achieves competitive performance across mainstream multi-modal benchmarks. On Image Benchmarks, \textit{Ola} excels at general understanding and specific-task understanding, with an overall mean accuracy of 72.6\% on the challenging OpenCompass benchmark~\citep{duan2024vlmevalkit}, 84.3\% average scores on MMBench-1.1~\citep{liu2023mmbench}, 57.0\% average scores on MMMU~\citep{yue2024mmmu}, etc. On the challenging VideoMME~\citep{fu2024videomme} multiple-choice benchmark ranging from videos within 30 seconds to 1 hour, \textit{Ola} achieves the impressive accuracy of 68.4\% with video and audio inputs. \textit{Ola} also excels at audio understanding tasks such as audio-speech recognition and chat evaluation, achieving 3.1 mean WER on LibriSpeech~\citep{panayotov2015librispeech} and 6.41 GPT-eval score on AIR-Bench~\citep{yang2024airbench}. Results on the benchmarks show a giant promotion compared with existing omni-modal LLMs and even outperforming the performance of state-of-the-art specialized LLMs.

