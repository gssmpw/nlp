\section{Related Works}
\label{sec:related_works}

\paragrapha{Large Vision-Language Models. }Inspired by the success of AI assistants and large language models~\citep{GPT35,GPT4V,OpenAI_GPT4_2023}, research has increasingly focused on vision-language multi-modal large language models. Significant advancements have been made in architecture design~\citep{alayrac2022flamingo,li2023blip2,liu2024llava,dong2024insight,liu2024efficient}, training strategies~\citep{chen2024internvl,liu2024chain}, model scaling~\citep{li2024llavaov}, and data curation~\citep{lauren√ßon2024cauldron,lu2024deepseek,liu2024chain,yang2024octopus}. Furthermore, models are evolving beyond static images to support video~\citep{lin2023videollava,maaz2023videochatgpt,chen2024sharegpt4video,liu2024oryx}, 3D~\citep{hong20233d,liu2024coarse}, and mixed visual inputs~\citep{ranzinger2024radio,qwen2vl}. However, extending these models to effectively integrate audio modalities while maintaining balanced and robust performance remains an area that has not been fully explored.


\paragrapha{Large Audio-Text Models. }Large Language Models, mainly focused on text inputs and outputs, have a foundational link to speech, with pioneering efforts integrating speech inputs through adapter-based modifications~\citep{chen2023xllm,wu2023decoder,fathullah2024prompting}. The challenge of LLM-based speech generation has been addressed with the development of speech decoders~\citep{zhang2023speechgpt,rubenstein2023audiopalm}, marking a significant step towards omni-modal models. Beyond speech, research is expanding into audio-based LLMs that encompass the understanding of music, events, and more. Notable examples include AudioGPT~\citep{huang2024audiogpt} and SALMONN~\citep{tang2023salmonn}, which explore these audio dimensions, while models like Qwen2-Audio~\citep{chu2024qwen2audio} demonstrate enhanced understanding capabilities.

\paragrapha{Towards Large Omni-Modal Models. }Recent advancements in large language models~\citep{GPT4o,reid2024gemini} have spurred interest in developing omni-modal models that can handle multiple modalities simultaneously.  Notable examples include SpeechGPT~\citep{zhang2023speechgpt} and LLaMA-Omni~\citep{fang2024llamaomni}, which integrate audio-text understanding with speech generation.  The Mini-Omni~\citep{xie2024miniomni2} series addresses challenges in speech streaming generation through parallel decoding techniques.  VITA~\citep{fu2024vita} extends this capability by unifying audio, image, video, and text understanding.  While these models excel in understanding tasks, efforts are also being made to tackle both understanding and generation tasks~\citep{xie2024show,wang2024emu3}.  However, existing omni-modal models often fall short in managing the full spectrum of input modalities and output formats, or they suffer from significantly poorer performance.  Ola aims to address these limitations by enhancing the capability and efficiency of omni-modal models with better architecture, training strategy, and targeting data preparation.