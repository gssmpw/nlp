\section{Experiments}
\label{sec:exp}



We conduct all-sided benchmarking in Sec.~\ref{sec:exp_main}  to evaluate the all-powerful \textit{Ola} model, including the representative benchmarks in image, video, and audio understanding. Subsequently, we conduct detailed results on critical benchmarks in Sec.~\ref{sec:exp_analysis} to demonstrate the effectiveness of our design on motivation, training, and data preparations. 

\subsection{Implementation Details}

The \textit{Ola} model builds upon the Qwen-2.5-7B~\citep{qwen2.5} framework, incorporating OryxViT~\citep{liu2024oryx} as the vision encoder initialized from SigLIP-400M~\citep{zhai2023siglip}, Whisper-V3-Large~\citep{radford2022whisper} as the speech encoder, and BEATs-AS2M(cpt2)~\citep{Chen2022beats} as the music encoder. Initially, we employ a relatively high learning rate of 1e-3 for MLP adapter pre-training. During supervised fine-tuning, the learning rate is gradually reduced from 2e-5 for text-image and multi-image training to 1e-5 for video-audio training. We utilize a batch size of 256 for fine-tuning, leveraging 64 NVIDIA A800 GPUs to conduct our training. We adopt 10$\times$ downsampled rate for audio features to reduce the token length, resulting in 300 tokens per minute. During training and inference, the maximum token length is set to 16384 and the maximum number of audio trunks is set to 25. 

\subsection{Results on Omni Understanding} \label{sec:exp_main}

\paragrapha{Benchmarks. }We conduct extensive comparisons across image, video, and audio understanding benchmarks to demonstrate the omni-modal capabilities of the \textit{Ola} model. For image benchmarks, we utilize comprehensive understanding datasets including MMBench-1.1~\citep{liu2023mmbench}, MMMU~\citep{yue2024mmmu}, MMStar~\citep{chen2024mmstar}, MathVista~\citep{lu2023mathvista}, Hallusion Bench~\citep{guan2024hallusionbench}, AI2D~\citep{kembhavi2016ai2d}, and OCRBench~\citep{liu2023ocrbench}. In the video domain, we evaluate using VideoMME~\citep{fu2024videomme}, which involves multiple-choice questions on videos of varying lengths, and LongVideoBench~\citep{wu2024longvideobench} for assessing performance on extremely long video content, MVBench~\citep{li2024mvbench} for the general recognition ability. For audio benchmarks, we focus on two primary tasks relevant to audio LLMs. Librispeech~\citep{panayotov2015librispeech} serves as a traditional audio-speech recognition (ASR) dataset, testing the model's ability to accurately transcribe spoken language. AIR-Bench~\citep{yang2024airbench} provides a comprehensive evaluation of audio question-answering capabilities, incorporating speech, sound, and music inputs. The responses are evaluated with a GPT-based~\citep{OpenAI_GPT4_2023} scorer against ground truth answers. We report the mainstream evaluation metric on image and video benchmarks and report the mean metric in Tab.~\ref{tab:audio} for ASR and audio understanding tasks for simplicity. 

\paragrapha{Baselines. }We selected a range of state-of-the-art multi-modal large language models across different modalities for comparison and reference. We categorized vision-language models into three groups: image-centric LLMs, video-centric LLMs, and comprehensive LLMs capable of handling both images and videos.  For image understanding, we utilized Cambrian-1~\citep{tong2024cambrian} and Pixtral-12B~\citep{agrawal2024pixtral}.  For video understanding, VideoCCAM~\citep{fei2024videoccam} and LLaVA-Video~\citep{zhang2024videoinstructiontuningsynthetic} are employed. Comprehensive models included LLaVA-OneVision~\citep{li2024llavaov}, MiniCPM-V 2.6~\citep{yao2024minicpm}, InternVL2.5~\citep{chen2024intern25}, and Qwen2.5-VL~\citep{Qwen2.5-VL} which excel across various visual benchmarks. In the audio domain, we compared our work with state-of-the-art models such as SALMONN~\citep{tang2023salmonn} and Qwen-2 Audio~\citep{chu2024qwen2audio}. As an Omni-modal LLM, our model, \textit{Ola}, was compared with state-of-the-art open-source omni-modal LLMs like Mini-Omni2~\citep{xie2024miniomni2}, VITA-1.5~\citep{fu2024vita}, InternLM-XComposer2.5-OmniLive~\citep{internlmxcomposer2_5_OL}, which support image, audio, and text inputs.  Additionally, LLaMA-Omni~\citep{xie2024miniomni2}, an audio-text omni model, was noted for its strong speech generation capabilities.



% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[t]
  \centering
  \caption{\textbf{Analysis Results on Audio Benchmarks. }We report the WER rate on test-clean, test-other, dev-clean, dev-other subsets of LibriSpeech, and the scores on AIR-Bench. In the table, "$-$" indicates the model is capable of solving the tasks, while the result is lacking. "\xmark" indicates that the model is not capable of the task.}
  \adjustbox{width=\linewidth}{
    \begin{tabular}{lccccccccc}
    \toprule
    \multirow{2}[2]{*}{Model} & \multicolumn{4}{c}{ASR on LibriSpeech$\downarrow$} & \multicolumn{5}{c}{Chat on AIR-Bench} \\
    \cmidrule(lr){2-5}\cmidrule(lr){6-10}
          & test-c & test-o & dev-c & dev-o & speech & sound & music & mix & avg\\
    \midrule
    \textit{Audio Models} \\
    \midrule
    SpeechGPT~\citep{zhang2023speechgpt} & \xmark & \xmark & \xmark & \xmark & 1.57  & 0.95  & 0.95  & 4.13 & 1.90\\
    Whisper-small~\citep{radford2022whisper} & 4.4   & 10.1  & 4.6   & 10.3  & \xmark & \xmark & \xmark & \xmark & \xmark\\
    SALMONN~\citep{tang2023salmonn} & 2.1   & 4.9   &$-$    &$-$    & 6.16  & 6.28  & 5.95  & 6.08 & 6.12\\
    Qwen2-Audio~\citep{chu2024qwen2audio} & 1.6   & 3.6   & 1.3   & 3.4   & 7.18  & 6.99  & 6.79  & 6.77 & 6.93\\
    \midrule
    \textit{Omni-Modal LLMs} \\
    \midrule
    LLaMA-Omni~\citep{fang2024llamaomni} & \xmark & \xmark & \xmark & \xmark & 5.22  & 5.25  & 4.33  & 4.00 & 4.70 \\
    Mini-Omni2~\citep{xie2024miniomni2} & 4.8   & 9.8   & 4.7   & 9.4   & 3.58  & 3.54  & 2.62  & 3.08 & 3.20\\
    VITA-1.5~\citep{fu2025vita1_5} & 3.3 & 7.2 & 3.4 & 7.5 & 4.83 & 5.48 & 4.91 & 2.92 & 4.54\\
    IXC2.5-OmniLive~\citep{internlmxcomposer2_5_OL} & 2.5 & 5.7 & 2.6 & 5.8 & 1.60 & 1.77 & 1.74 & 1.58 & 1.67\\
    \midrule
    \textbf{\textit{Ola}} (Pure audio) & 2.1 & 4.7 & 2.1 & 4.6 & 6.32 & 5.43 & 5.76 & 5.83  & 5.84\\
    \rowcolor{Lightorange}\textbf{\textit{Ola}} & \textbf{1.9} & \textbf{4.4} & \textbf{1.9} & \textbf{4.2} & \textbf{7.34} & \textbf{6.40} & \textbf{5.90} & \textbf{6.01} & \textbf{6.41}\\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:audio}%
\end{table}%


\paragrapha{Results. }We present the comprehensive results in Table~\ref{tab:main}, highlighting \textit{Ola}'s competitive performance across major multi-modal benchmarks when compared to state-of-the-art specialist-modal LLMs. Specifically, in image benchmarks, \emph{Ola} achieves 84.3\% on MMBench-1.1~\citep{liu2023mmbench}, 70.8\% on MMStar~\citep{chen2024mmstar}, 57.0\% on MMMU~\citep{yue2024mmmu}, 68.4\% on MathVista~\citep{lu2023mathvista}, 86.1\% on AI2D~\citep{kembhavi2016ai2d} and 827 on OCRBench~\citep{liu2023ocrbench}, surpassing all the relative multi-modal LLMs in similar number of parameters. In video benchmarks, \textit{Ola} attains an impressive 68.4\% on VideoMME~\citep{fu2024videomme}, showcasing its robust capability to handle both video and audio inputs simultaneously, and setting a new state-of-the-art performance among 7B models on the VideoMME benchmark. \emph{Ola} also maintains a leading position compared to mainstream video LLMs including LLaVA-Video~\citep{zhang2024videoinstructiontuningsynthetic} and VideoCCAM~\citep{fei2024videoccam} on LongVideoBench~\citep{wu2024longvideobench} and MVBench~\citep{li2024mvbench}. In audio benchmarks, \textit{Ola} demonstrates strong audio-speech recognition and conversational abilities, with 3.1\% mean WER rate on LibriSpeech~\citep{panayotov2015librispeech} and 6.41 mean score on AIR-Bench~\citep{yang2024airbench}, outperforming existing omni-modal LLMs, including LLaMA-Omni~\citep{fang2024llamaomni}, which focuses on audio understanding. These results indicate a significant advancement over current omni-modal LLMs, underscoring the effectiveness of \textit{Ola}'s training approach.

\subsection{Analysis} \label{sec:exp_analysis}

For the analysis part, we report the detailed results on audio benchmarks to illustrate the fine-grained performance. We also demonstrate our designs on training and cross-modal training data with ablations on critical benchmarks. At last, we perform qualitative showcases of \textit{Ola}. 


% Table generated by Excel2LaTeX from sheet 'Sheet4'
\begin{table}[t]
  \centering
  \caption{\textbf{Analysis on Omni-Modal Training. }We conduct analysis for the performance before/after omni-modal learning, and show the performance gain with audio inputs on mainstream video benchmarks. The highlighted row indicates the final accepted strategy. }
  \adjustbox{width=\linewidth}{
    \begin{tabular}{ccccccc}
    \toprule
    Omni-Stage & \multirow{2}[4]{*}{~Audio~} & \multirow{2}[4]{*}{Subtitle} & \multicolumn{4}{c}{VideoMME} \\
\cmidrule{4-7}  Training  &       &       & ~~Short~~ & ~~Medium~~ & ~~Long~~  & ~~Overall~~  \\
    \midrule
    \xmark & \xmark & \xmark &  75.9 & 61.2 & 54.3 & 63.8 \\
    \cmark & \xmark & \xmark & 76.4  & 61.9  & 54.8  & 64.4   \\
    \cmark & \xmark & \cmark & 78.4  & 66.6  & 56.4  & 67.1   \\
    \rowcolor{Lightorange} \cmark & \cmark & \xmark & 78.7  & 68.3  & 58.3  & 68.4 \\
    \cmark & \cmark & \cmark & 78.8  & 68.8  & 60.3  & 69.3 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:video}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'Sheet4'
\begin{table}[t]
  \centering
  \caption{\textbf{Analysis on Cross-Modal Training Data.} We analyze our data mixture for the cross-modal video-audio alignment data about sources from academic or open-ended videos. The highlighted row indicates the final accepted strategy. The experiment is conducted on a subsampled training set.}
  \adjustbox{width=\linewidth}{
    \begin{tabular}{cccccc}
    \toprule
    Acadamic & Open-End & ~MMMU~  & VideoMME & LongVideo & LibriSpeech$\downarrow$ \\
    \midrule
    \xmark & \xmark & 48.2  & 59.0 & 56.4  & 4.5 \\
    \cmark & \xmark & \textbf{48.3}  & 64.2  & 56.8  & 4.1  \\
    \rowcolor{Lightorange} \cmark & \cmark & 48.1  &\textbf{ 65.7}  & \textbf{57.4}  & \textbf{4.0}  \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:data}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'Sheet4'
\begin{table}[t]
  \centering
  \caption{\textbf{Analysis on Progressive Modality Learning.} We evaluate the basic performance on image and video understanding for the intermediate models during the training stage. The highlighted row indicates the final accepted strategy.}
  \adjustbox{width=\linewidth}{
    \begin{tabular}{ccccccc}
    \toprule
    Stage1 & Stage2 & Stage3 & MMBench-1.1 & MMMU & OCRBench & VideoMME \\
    \midrule
    \cmark & \xmark & \xmark & 83.5  & \textbf{57.5} & \textbf{832}  & \xmark \\
    \cmark & \cmark & \xmark & 83.8 & 57.2 & 820  & 63.8 \\
    \rowcolor{Lightorange} \cmark & \cmark & \cmark & \textbf{84.3} & 57.0 & 827 & \textbf{68.4} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:training}%
\end{table}%



\paragrapha{Analysis on Audio Benchmarks. }To demonstrate the effectiveness of our approach on audio and speech tasks, we conducted experiments using the LibriSpeech~\citep{panayotov2015librispeech} and AIR-Bench~\citep{yang2024airbench} datasets, and we illustrate our results on Tab.~\ref{tab:audio}. Specifically, we report the Word Error Rate (WER) on the test-clean, test-other, dev-clean, and dev-other subsets of LibriSpeech.   Additionally, we present the GPT-4-eval scores on speech, sound, music, and mix sub-metrics in AIR-Bench.   Our model, \textit{Ola}, is compared against state-of-the-art audio models and omni-modal LLMs.

Notably, \textit{Ola} demonstrates a significant advantage over existing omni-modal models, achieving a 1.9 WER on the test-clean subset of LibriSpeech and a 6.41 average score on AIR-Bench. This is in contrast to the previous state-of-the-art omni-modal models, which achieved a 2.5 WER and a 4.70 score, respectively. \textit{Ola}'s performance is even approaching that of audio-specific models, highlighting its strong universality. 

Furthermore, we evaluated \textit{Ola} under two situations. \textit{Ola} (Pure audio) indicates that we omit video-audio data in stage 3 and replace the same amount of data with pure audio inputs, where we can observe a consistent performance gain with cross-modal joint learning. Despite the significant distribution gap between video audio and speech-relevant datasets, this improvement indicates the robust connections between video and speech modalities.


\paragrapha{Effectiveness of Omni-Modal Training. }In exploring the relationships between video and audio, we examined the effectiveness of omni-modal training and its impact on audio within videos, both during training and in benchmark outcomes. The analysis results are shown in Tab.~\ref{tab:video}.  By comparing results before and after omni-modal training (i.e., stage 3 of the progressive training strategy), we observed performance improvements from 63.8\% to 64.4\% on VideoMME~\citep{fu2024videomme}.  Additionally, incorporating audio modalities alongside raw video resulted in significant performance gains, increasing scores from 64.4\% to 68.4\% on VideoMME~\citep{fu2024videomme}. These findings suggest that audio contains valuable information that enhances overall recognition performance. Notably, the multiple-choice accuracy for \textit{Ola} with omni-modal training and audio inputs even surpasses the results with original text subtitles, with 68.4\% overall performance compared with 67.1\% overall performance. The results indicate that audio data may include more information beyond the original text information in certain benchmarks.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figs/viz.pdf}
\caption{\textbf{Generative results on speech and visual understanding tasks. }We illustrate results on speech and video understanding and show the strong ability of omni-modal \textit{Ola} compared with conventional vision-language models.}
\label{fig:showcase}
\end{figure}

\paragrapha{Effectiveness of Progressive Modality Learning. }To evaluate the effectiveness of the proposed training strategy, we evaluate the basic performance of the intermediate model in each stage (Stage-1 for Ola-Image, Stage-2 for Ola-Video and Stage-3 for the final \textit{Ola} model). Specifically, we adopt the representative MMBench-1.1~\citep{liu2023mmbench}, MMMU~\citep{yue2024mmmu}, and OCRBench~\citep{liu2023ocrbench} for image performance and VideoMME~\citep{fu2024videomme} for video performance. Results are shown in Tab.~\ref{tab:training}. We can observe that the progressive modality training from image, video to audio can maximally preserve the previously learned capability. Additionally, we can also observe a performance gain on MMBench-1.1 and VideoMME, revealing the superiority of joint learning. 

\paragrapha{Ablations on Cross-Modal Training Data. }In our implementations, we collect the cross-modal video-audio data for modality alignment from multiple sources including academic datasets and open-ended videos from YouTube. While the data distribution and the processing pipeline vary for the two sources, we conduct ablation analysis on the combination of dual video sources. Results are shown in Tab.~\ref{tab:data} Our baseline model excluded video-audio training, focusing solely on audio-relevant data in Stage 3.  Results indicate that video-audio training minimally affects image benchmarks, suggesting stable image understanding post-text-image training.  For video benchmarks, we observed consistent performance improvements: 59.0\% without video training, rising to 64.2\% with academic data and 65.7\% with open-ended data in VideoMME~\citep{fu2024videomme}. Furthermore, ASR performance on LibriSpeech~\citep{panayotov2015librispeech} improved with video-audio data, likely due to the challenging subtitling tasks in complex environments, enhancing speech recognition capabilities.

\paragrapha{Showcases.} We present the qualitative generation results of the \textit{Ola} model on both speech and visual understanding tasks in Fig.~\ref{fig:showcase}. For speech understanding, we utilize sources from the AIR-Bench~\citep{yang2024airbench} benchmark, where \textit{Ola} demonstrates precise speech recognition and effective emotional analysis, as well as reasoning about the questions posed. \textit{Ola} tries to find out the answer for the instruction based on the audio inputs. In visual understanding tasks, we analyze an interview with a famous tennis player after the Olympic Games. We compare our approach with the state-of-the-art vision-language model~\citep{li2024llavaov}. The large vision-language models without audio inputs exhibit significant information loss due to the absence of audio input and recognition capabilities for audio. In contrast, \textit{Ola}, with its omni-modal inputs, provides more accurate responses regarding nationality, context, and background from the speaker's dialogue.