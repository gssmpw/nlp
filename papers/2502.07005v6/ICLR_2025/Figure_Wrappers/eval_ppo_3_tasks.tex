\begin{figure*}[t]
    \makebox[\textwidth][c]{
    \input{ICLR_2025/Figure_Wrappers/Legend/ppo_four_methods}
    }
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{ICLR_2025/Figures/eval_ppo_3_tasks_5_seeds/smooth_eval_ppo_Isaac-Rigid-Insertion-Multi-v0_eval_consistent.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{ICLR_2025/Figures/eval_ppo_3_tasks_5_seeds/smooth_eval_ppo_Isaac-Rope-Shaping-v0_eval_all.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\textwidth]{ICLR_2025/Figures/eval_ppo_3_tasks_5_seeds/smooth_eval_ppo_Isaac-Cloth-Hanging-Multi-v0_eval_all.pdf}
    \end{subfigure}
    
    
    \caption{Performance comparison between HEPi and Transformer models with TRPL and PPO over 10 seeds. TRPL shows stable performance across all tasks, while PPO struggles in tasks requiring high exploration, especially in 3D environments like \textit{cloth-hanging-3D}. In tasks with a lower-dimensional action space (e.g., 2D tasks), both methods perform comparably when carefully tuned.
    % We evaluated each method over 5 seeds, performing a grid search for the best PPO parameters across an additional 5 seeds
    }
    \vspace{-0.2cm}
    \label{fig:eval_trpl_ppo}
\end{figure*}
