\paragraph{Problem Statement} We aim to solve robotic manipulation problems using an on-policy actor-critic reinforcement learning approach. To address the symmetries present in the state and action spaces of the Markov Decision Process (MDP), we leverage \textit{equivariant policies}, ensuring that transformations applied to the state space are consistently reflected in the action space. To handle the complexities of robotic manipulation, where actuators and objects play distinct roles, we propose the \emph{Heterogeneous Equivariant Policy (HEPi)}, which comprises three key components:
\begin{itemize}
    \item \textit{Equivariant MPN backbone}: An efficient and expressive \gls{empn} capable of exploiting environment symmetries, thereby significantly reducing the search space complexity.
    \item \textit{Heterogeneous graph design and update rules}: A graph structure with distinct actuator and object nodes, with tailored message-passing rules to handle the system's heterogeneity.
    \item Employing a \emph{principled trust-region method} to stabilize training in complex, high-dimensional environments.
\end{itemize}


\subsection{Equivariant MPN Backbone}

An Equivariant Message Passing Network (\gls{empn}) can be constructed \citep{brandstetter2022geometric, duval2023hitchhikers, bekkers2024fast} by enforcing equivariance in the functions $\phi$ and $\psi$ in Equation~\ref{eq:mpnn}, ensuring that their inputs and outputs are steerable and transform consistently under the group $G$, as described in Definition~\ref{def:eq_and_inv}. Constructing such functions for high-dimensional steerable features is challenging and typically requires spherical harmonics embeddings \citep{duval2023hitchhikers}, where matrix-vector multiplications are carried out using Clebsch-Gordan tensor products, followed by steerable activation functions \citep{brandstetter2022geometric, bekkers2024fast}. While these operations guarantee equivariance, they also introduce high computational complexity, making them impractical for reinforcement learning settings. To mitigate this issue, \citet{bekkers2024fast} introduced the PONITA framework, an efficient equivariant message-passing approach. We use it as our \gls{empn} backbone and refer to it as \gls{empn} throughout the rest of the paper for consistency.

Consider the message function in Equation~\ref{eq:mpnn}, where $\psi(\mathbf{f}_v^{(k)}, \mathbf{f}_u^{(k)}, \mathbf{e}_{uv}) = k(\vx_u - \vx_v) \mathbf{f}_u$ is defined as a linear function with steerable feature $\mathbf{f}_u$, and summation is used as the aggregation function, $\bigoplus = \sum$. Here, $k$ is a convolution kernel, and $(\vx_u - \vx_v)$ represents the relative position between nodes $u$ and $v$. This leads to the convolutional message-passing update rule, $\mathbf{f}'_v = \sum_{u \in N(v)} k (\vx_u - \vx_v) \mathbf{f}_u$. 
On a regular grid, e.g., an image, each relative position $(\vx_u - \vx_v)$ has a corresponding weight $\mW_{u,v}$ and is stored in one single matrix $\mW$. 
However, this does not apply to non-uniform grids, e.g. point clouds, $k(\vx_u, \vx_v)$ in this case can be parameterized by a neural network, resulting in the formulation
$
\mathbf{f}'_v = \int_{\mathcal{X}} k_\theta(\vx_u, \vx_v) \mathbf{f}_u d{\vx_u}
$.
Under this general formulation, \citet{bekkers2024fast} showed that $k$ can be made $SE(3)$ equivariant by ``lifting" the input domain from $\gX = \mathbb{R}^3$ to $\mathcal{X}^\uparrow = \mathbb{R}^3 \times \mathcal{S}^2$. Specifically, for every position $\vp \in \mathbb{R}^3$, an associated orientation $\vo \in \mathcal{S}^2$ is introduced. This allows features in \gls{empn} to be embedded in both spatial and orientation spaces, leading to the following convolutional form:
$$
\mathbf{f}'_v = \int_{\mathbb{R}^3} \int_{\mathcal{S}^2} k_\theta ([(\vp_u, \vo_u), (\vp_v, \vo_v) ]) \mathbf{f}_u d{\vp_u} d{\vo_u}.
$$ 
Furthermore, to improve computational efficiency, the kernel function is factorized as:
$$
k_\theta ([(\vp_u, \vo_u), (\vp_v, \vo_v) ]) = K_\theta^{(3)} \, k_\theta^{(2)}(\vo_v^\top \vo_u) \, k_\theta^{(1)}(\vo_v^\top (\vp_u - \vp_v), \| \vo_v \perp (\vp_u - \vp_v) \|),
$$
where $k^{(1)}$ handles spatial interactions based on the relative position $(\vp_u - \vp_v)$ and the perpendicular component $\| \vo_v \perp (\vp_u - \vp_v) \|$, $k^{(2)}$ manages orientation-based interactions via dot products $\vo_v^\top \vo_u$, and $K^{(3)}$ performs channel-wise mixing across features. 
This formulation preserves the universal approximation property of equivariant functions while being significantly more computationally efficient and not requiring specialized network structures. Furthermore, $\vo$ can be sampled on a uniform grid over $\mathcal{S}^2$, making \gls{empn} only approximately equivariant. However, in practice, it achieves strong performance even with a limited number of samples, as discussed in Appendix~\ref{appx:further_exp}.

\subsection{Heterogeneous Equivariant Policy}
\label{alg:hepi}

In robotic manipulation tasks, actuators and objects play fundamentally distinct roles. The graph is defined as $\gG = (\gV, \gE)$, where $\gV = \gV_\text{act} \cup \gV_\text{obj}$ represents disjoint node sets for actuators and objects. Our approach captures these roles by first processing local information within the object and actuator clusters and then aggregating it globally to the actuators via directed, fully-connected inter-edges, as shown in Figure~\ref{fig:hepi_diagram}. This design distinguishes object-to-object, actuator-to-actuator, and object-to-actuator interactions, allowing the system to separate local processing from global information exchange.

The updates for both object and actuator nodes can be expressed as
\begin{equation}
\begin{aligned}
\mathbf{f}_v^{\text{obj, new}} &= \phi_\text{obj} \left( \mathbf{f}_v^\text{obj}, \sum_{u \in N(v)_\text{obj}} k(x^{\text{obj}}_u, x^{\text{obj}}_v; \theta_\text{obj-obj}) \mathbf{f}_u^\text{obj} \right), \quad v \in \gV_\text{obj}, \\
\mathbf{f}_v^{\text{act, new}} &= \phi_\text{act-local} \left( \mathbf{f}_v^\text{act}, \sum_{w \in N(v)_\text{act}} k(x^{\text{act}}_w, x^{\text{act}}_v; \theta_\text{act-act}) \mathbf{f}_w^\text{act} \right), \quad v \in \gV_\text{act}, \\
\mathbf{f}_v^{\text{act, final}} &= \mathbf{f}_v^{\text{act, new}} + \phi_\text{act-global} \left( \mathbf{f}_v^{\text{act}}, \sum_{u \in {\cal V}_\text{obj}} k(x^{\text{obj}}_u, x^{\text{act}}_v; \theta_\text{obj-act}) \mathbf{f}_u^{\text{obj, new}} \right), \quad v \in \gV_\text{act}.
\end{aligned}
\label{hepi_update}
\end{equation}
Here, $\mathbf{f}_v^{\text{obj, new}}$ represents the updated object features after local object-to-object interactions, $\mathbf{f}_v^{\text{act, new}}$ refers to the updated actuator features after local actuator-to-actuator interactions, and $\mathbf{f}_v^{\text{act, final}}$ is the final feature for actuator nodes after aggregating information from both the objects and its actuator neighbors. Here each of the kernels $k(\cdot, \cdot; \theta_\text{obj-obj})$, $k(\cdot, \cdot; \theta_\text{obj-act})$, and $k(\cdot, \cdot; \theta_\text{act-act})$, has it own learnable parameters, allowing them to specialize the learning process for each interaction type.

Moreover, each node $v \in \gV$ encodes its \texttt{node\_type} as a one-hot scalar-vector, along with normalized position vectors $\mathbf{p}_v$ and velocities $\mathbf{v}_v$. For object nodes, the feature vector also includes the relative distance to the target, $\mathbf{d}_{v, \text{target}}$, embedding target information directly without the need for an additional target node. For actuator nodes, the output consists of both a scalar $c$ and a vector $\mathbf{v}_\text{out}$, where the final output vector is computed as $\mathbf{v}_\text{out} = c \cdot \mathbf{v}$. This setup ensures flexibility for diverse tasks while maintaining consistency with the geometric properties of the system.

\textbf{Value Function} We employ DeepSets \citep{deepsets} with the same input structure as the policy to preserve permutation invariance of the node features, while keeping the architecture both simple and computationally efficient, similar to the prior work \cite{simm2021symmetryaware}. The value function is computed as $V(s) = \text{MLP}_{\text{outer}} \left( \sum_{v \in \gV} \text{MLP}_{\text{inner}} (s_v) \right)$,
where $s_v$ represents the feature of node $v$. These node features may differ from those used in the policy network to capture task-specific observations. For example, in the \textit{Cloth-Hanging} task, the value function considers features from all nodes, while the policy network focuses only on the hole boundary nodes. Full details of the input features used for the value function are provided in the Appendix~\ref{appx:task_details}. 

% \subsection{Trust-region Projection Layers}
% \label{sec:trpl}
\textbf{Trust-Region Projection Layers} Standard on-policy reinforcement learning approaches such as Proximal Policy Optimization (PPO) \citep{ppo}, learn a policy by optimizing the surrogate objective
$$
\theta_{k+1} = \arg \max_\theta \mathbb{E}_{(s,a) \sim \pi_{\theta_k}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a) \right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_\theta || \pi_{\theta_k}) \leq \delta,
$$
where $A^{\pi_{\theta_k}}(s,a)$ is the advantage function. Here, $D_{\text{KL}}(\pi_\theta || \pi_{\theta_k})$ is the KL-divergence between the new policy $\pi_\theta$ and the old policy $\pi_{\theta_k}$, constrained by $\delta$ to ensure stable updates and prevent overly large policy changes. 
PPO approximates this trust region by clipping the importance sampling ratio to limit updates. This, however, requires careful hyperparameters turning to make it work stably, as pointed out by \cite{andrychowicz2021what}. We will show later in the Results Section~\ref{sec:results}, this is also applied to graph-based policy. On the other hand, Trust Region Projection Layers (TRPL) \citep{otto2021trpl} adopt a more principled approach. TRPL projects policy parameters onto trust region boundaries using a differentiable convex optimization, ensuring stability by projecting both the mean and variance of the Gaussian policy to satisfy trust region constraints. 

In this paper, we adopt TRPL to ensure stable policy updates, and we will show in the Result Section~\ref{sec:results}, TRPL consistently outperforms PPO, with little hyperparameter tuning required, especially in tasks requiring complex exploration, as also observed in the prior works \citep{otto23bbrl, li2023open, celik2024acquiring}.



\subsection{Theoretical Justification}
\label{sec:main_theorem}

HEPi is inspired by adding global Virtual Nodes ($\text{VN}_G $) to Message Passing Neural Networks, ($\text{MPNNs}$). 
For example, \citet{southern2024understanding} proposed $\text{MPNN}$ + $\text{VN}_G$ which seperates local and global updates. 
Here, the local updates are equivalent to our object node updates, and the global ones correspond to our actuator node update. Based on this interpretation, we show that locally connecting actuator nodes to only k-nearest object nodes can not capture relevant relation between object and actuator nodes, while treating the actuator nodes as VN that connects to all object nodes can. We name the graph network with locally connected actuators and object nodes as $\text{MPNN}$ + $\text{VN}_{\text {Local}} $. We show that for HEPi any two actuator and object nodes can exchange information, while this is not the case for $\text{MPNN}$ + $\text{VN}_{\text {Local}} $. 

\begin{proposition}
    For $\text{MPNN}$ + $\text{VN}_{\text {Local}} $, the Jacobian $\partial \mathbf{f}_v^{\text{act}}/\partial \mathbf{f}_u^{\text{obj}}$ is independent of $u$ whenever object node $u$ and actuator node $v$ are separated by more than 2 hops. In contrast, HEPi with node connections and updates as described in Section \ref{alg:hepi} can exchange information between any actuator and object nodes after a single layer.
\end{proposition}
The proof is provided in Appendix \ref{app_proof}. This result implies that HEPi's connection design allows the actuators to receive relevant information to predict actions w.r.t changes at object nodes. In contrast, for $\text{MPNN}$ + $\text{VN}_{\text {Local}} $ the actuators could fail to predict relevant actions to changes at object node $u$. We provide experimental ablations across various values of $k$ to clearly emphasize this distinction in Appendix \ref{appx:further_exp}.