\paragraph{Message Passing Neural Networks (MPNN)}

Consider a graph $\gG=(\gV,\gE)$, where $\gV$ represents the nodes and $\gE$ the edges. In a standard Graph Neural Network (GNN) \citep{battaglia2018relational}, each node $v \in \gV$ updates its feature representation by aggregating information from its neighbors $N(v)$. This process is formalized as
\begin{equation} \label{eq:mpnn}
    \mathbf{f}_v^{(k+1)} = \phi \left( \mathbf{f}_v^{(k)}, \bigoplus_{u \in N(v)} \psi \left( \mathbf{f}_v^{(k)}, \mathbf{f}_u^{(k)}, \mathbf{e}_{uv} \right) \right),
\end{equation}
where $\mathbf{f}_v^{(k)}$ is the feature vector of node $v$ at iteration $k$, $\mathbf{e}_{uv}$ is the edge feature between nodes $u$ and $v$, $\phi$ and $\psi$ are often deep neural networks, and $\bigoplus$ represents an aggregation function like summation, mean or max.

\paragraph{$SE(3)$ Equivariance and Invariance}

In this work, we focus on geometric graphs $\gG = (\gV, \gE, \gX)$, where each node $v$ is associated with coordinates $\vx_v \in \gX = \mathbb{R}^3$ and steerable geometric features $\mathbf{f}_v$. A feature is considered \emph{steerable} if it transforms consistently under the action of a group $g \in G$ through a group representation $\rho$. For instance, a vector $\vv \in \mathbb{R}^3$ under a rotation $\mR \in SO(3)$ transforms as $\vv' = \mR \vv$. In this case, $\vv$ is a finite-dimensional vector, and its transformation is described by an invertible matrix representation with determinant $1$, $\rho(g) = \mR$.

Equivariance and invariance are two fundamental concepts in this context, can be formalized as follows, using the notation from \citet{brandstetter2022geometric}:
\begin{definition}
    \label{def:eq_and_inv}
    Let $G$ be a group with representations $\rho^{\gX}$ and $\rho^{\gY}$. A function $f: \gX \to \gY$ is \textit{equivariant} if
    \begin{equation*}
        \rho^{\gY}(g)[f(x)] = f(\rho^{\gX}(g)[x]), \quad \forall g \in G, x \in \gX,
    \end{equation*}
    and \textit{invariant} if
    \begin{equation*}
        f(x) = f(\rho^{\gX}(g)[x]), \quad \forall g \in G, x \in \gX.
    \end{equation*}
\end{definition}

In simpler terms, equivariance guarantees that applying a transformation $g$ to the input space $\gX$ and then applying the function $f$ produces the same result as applying $f$ first and then transforming the output space $\gY$. On the other hand, invariance implies that the function $f$ remains unchanged when the input undergoes a transformation in $\gX$.

\paragraph{Symmetries in MDPs}
A Markov Decision Process (MDP) is defined by the tuple $(S, A, P, R, \gamma)$, where $S$ is the set of states, $A$ the actions, $P(s'|s,a)$ the transition probability, $R(s,a)$ the reward function, and $\gamma \in [0,1]$ the discount factor. The goal is to find a policy $\pi: S \to A$ that maximizes the expected discounted reward $\mathbb{E}_\pi[\sum_{t=0}^\infty \gamma^t R(s_t, a_t)]$.

In MDPs with symmetries, both the transition distribution $P(s'|s,a)$ and policy distribution $\pi(a|s)$ are invariant under group transformations $g \in G$ via \emph{left-regular representation} $L_g$ and $K_g$ for state and action, respectively, resulting in the following conditions:
% \[
% L_g[T(s,a)] = T(L_g[s], K_g^s[a]), \quad K_g^s[\pi(s)] = \pi(L_g[s])
% \]
% or in the stochastic form:
\[
P(L_g[s']|L_g[s], K_g^s[a]) = P(s'|s,a), \quad \pi(K_g^s[a]|L_g[s]) = \pi(a|s),
\]
and similarly for the reward function: $R(L_g[s], K_g^s[a]) = R(s, a)$. This allows leveraging symmetries to reduce the complexity of learning, potentially improving sample efficiency and generalization, as it results in a group-structured MDP homomorphism \citep{van2020mdp}.



