\section{Introduction}

\begin{figure}[h!]
\setlength\tabcolsep{0pt}
\setlength{\abovecaptionskip}{0.1cm}
    \centering
    \includegraphics[width=\columnwidth]{figures/teaser.pdf}
    \caption{\textbf{An illustration of multimodal inconsistency reasoning on a webpage.} An agent examines a webpage where the brand “IKEA AB” is mentioned, but other elements clearly refer to “Lorell.” Detecting this brand identity misattribution requires the ability to compare text fields across different sections of the page and reconcile them with accompanying images or context—an inherently multimodal reasoning task.
    }
    % \vspace{-10pt}
    \label{fig:teaser}
\end{figure}

Recent advances in Large Language Models (LLMs) have demonstrated impressive reasoning abilities across a variety of tasks~\cite{openai2024gpto1card, guo2025deepseek, kojima2022large, chain-of-thought}. Building on pre-trained LLMs, Multimodal Large Language Models (MLLMs) are fast evolving. However, they usually face greater challenges as they need to reason across different modalities, especially when inconsistencies (i.e., mismatched or contradictory contents) exist. We find that, being primarily trained and evaluated on consistent visual-textual inputs, existing MLLMs are largely untested in scenarios where the input contains misaligned or contradictory information—a situation that is common in real-world scenarios. For example, in Figure~\ref{fig:teaser}, a user presents a web page containing conflicting visual and textual elements, asking the model to identify errors.

%Recent advances in Large Language Models (LLMs) have produced significant improvements in their reasoning capabilities \cite{openai_o1, guo2025deepseek, kojima2022large, chain-of-thought}. With pre-trained LLMs as backbones, Multimodal Large Language Models (MLLMs) are fast evolving in various reasoning tasks. Existing MLLMs are primarily trained and evaluated on consistent visual-textual inputs where little to less contradictory contents exist. However, they overlook critical real-world scenarios where inconsistencies, i.e., misaligned or contradictory contents, are inherent to the task. For example, as shown in Figure~\ref{fig:teaser}, a user may ask the model to identify errors in a given slide, which significantly challenges the model's comprehensive reasoning capabilities.


To comprehensively evaluate the ability of MLLMs in reasoning over multimodal inconsistency, we introduce the \textbf{Multimodal Inconsistency Reasoning Benchmark (MMIR)}. MMIR is the first framework dedicated to evaluating how effectively MLLMs can reason about and identify semantic mismatches within complex, layout-rich content with interleaved image and text components. Our benchmark is built on a diverse collection of real-world artifacts (e.g. websites, slides, posters) which have been augmented with \textbf{synthetic inconsistencies}—realistic inconsistency errors injected into their original structures. These inconsistency errors span a range of reasoning-heavy categories: \textit{Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy}, and \textit{Temporal/Spatial Incoherence}—posing a next-level reasoning challenge for models. For example, resolving a \textit{Identity Misattribution} involves verifying entity alignment across modalities, while \textit{Quantitative Discrepancy} requires cross-referencing chart data with textual claims. By challenging models to detect such inconsistencies, MMIR forces them to perform intricate reasoning that goes well beyond simple pattern recognition. This benchmark not only exposes the limitations of current MLLMs in handling real-world challenges of reasoning over multimodal content with inconsistency, but also provides a platform for developing more robust multimodal reasoning systems.
% MMIR targets reasoning-intensive inconsistencies that demand: cross-modal grounding (accurately aligning text with corresponding visual elements), Contextual integration (resolving conflicts across interdependent elements), and Hierarchical verification (combining low-level perception with high-level logical analysis and reasoning).

% MMIR introduces 534 challenging test samples spanning three artifact categories: Web, Office, and Poster. MMIR spans five reasoning-heavy error categories: \textit{Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy}, and \textit{Temporal/Spatial Incoherence}, each demanding distinct reasoning skills. For example, resolving a \textit{Identity Misattribution} involves verifying entity alignment across modalities, while \textit{Quantitative Discrepancy} requires cross-referencing chart data with textual claims.

% MMIR employs two evaluation settings to dissect MLLMs’ capabilities: 1) \texttt{Open-ended}: Models analyze raw screenshots and generate free-form answers (LLM-as-judge evaluation), testing unsupervised reasoning with raw perception. 2) \texttt{Multiple-choice (MCQ)}: Models select inconsistencies from natural-language descriptions of elements in the artifacts, focusing on content understanding.\eric{is this paragraph necessary? How it helps the storytelling?}

In our experiments, we evaluated the advanced multimodal reasoning model o1~\cite{openai2024gpto1card} and five other state-of-the-art MLLMs: GPT-4o~\cite{openai2024gpt4ocard}, Qwen2.5-VL~\cite{Qwen2.5-VL}, LLaVA-NeXT~\cite{liu2024llavanext}, InternVL2.5~\cite{Chen2024ExpandingPB} and Phi-3.5-Vision~\cite{abdin2024phi3technicalreporthighly} using MMIR's 534 test samples. %The data in MMIR spans a broad spectrum of inconsistency types in complex, layout-rich content.
The results overall underscore that current MLLM models struggle with multimodal inconsistency reasoning. Specifically, there is a stark contrast between proprietary and open-source models. The open-source models evaluated only reach less than 25\% accuracy. o1 with strong reasoning capability achieves the overall best performance with over 50\% accuracy.% while GPT-4o being 33.14\%.
%When natural language descriptions of elements are provided as additional textual input, GPT-4o's accuracy improved significantly—rising by 14.61\% and narrowing the performance gap with o1 to 4.4\%—though open-source models showed moderate to little benefit.

To further understand the benchmarking results, we conduct analysis based on the inconsistency category, modality, and layout complexity of the artifact. We find the proprietary models excel in identifying factual contradiction and identity misattribute types of inconsistency and pairwise inconsistency, either inter-modality or intra-modality.
Last but not least, we investigate some approaches to enhance the model performance in our probing experiment. The results indicate that text-based Chain-of-Thought prompting and visual-based prompting (Set-of-Mark annotations) offer minimal and sometimes adverse effects, whereas an iterative multimodal interleaved reasoning strategy shows promising gains. Overall, these results highlight a critical bottleneck in the ability of MLLMs to perform robust, integrated reasoning—a key challenge for future research.

Our contributions are threefold: 
\begin{itemize} 
\item We introduce MMIR, a novel benchmark that targets the critical yet underexplored task of multimodal inconsistency reasoning in layout-rich content. 
\item We perform a comprehensive evaluation of one leading multimodal reasoning model and five state-of-the-art MLLMs, revealing significant gaps in their ability to detect inconsistency errors with detailed error analyses across multiple error types, modalities, and layout complexities.
\item We provide detailed probing analyses that expose key challenges—from perceptual shortcomings to reasoning bottlenecks—and propose a framework that iteratively refines predictions by jointly leveraging visual and textual modalities.
\end{itemize}