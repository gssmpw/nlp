\section{Experiments and Analysis}

\input{sections/tables/main_result}

We first evaluate the advanced multimodal reasoning model o1~\cite{openai2024gpto1card} and five other state-of-the-art MLLMs: GPT-4o~\cite{openai2024gpt4ocard}, Qwen2.5-VL~\cite{Qwen2.5-VL}, LLaVA-NeXT~\cite{liu2024llavanext}, InternVL2.5~\cite{Chen2024ExpandingPB} and Phi-3.5-Vision~\cite{abdin2024phi3technicalreporthighly} on the MMIR benchmark. We implement open-source models using their default settings and select the 1217 version of o1 and the 1120 version of GPT-4o for evaluation. Model implementation details are provided in Appendix~\ref{appendix:sec:model_detail}.
We then examine error patterns across different inconsistency types and layout complexities and finally explore how prompting strategies affect multimodal reasoning under the open-ended setting.

\subsection{Main Results}

As shown in Table~\ref{tab:main_eval_result}, proprietary models (o1 and GPT-4o) significantly outperform open-source alternatives, though all models exhibit substantial room for improvement. Appendix~\ref{appendix:sec:example_main settings} shows a qualitative example with question-answer and model response.

\noindent\textbf{Performance Gap Between Reasoning, Proprietary and Open-Source Models.} In both open-ended and MCQ settings, the reasoning o1 model substantially outperforms the rest, surpassing all open-source models by over 30\%. The other proprietary model GPT-4o, although missing the explicit reasoning ability of o1, outperforms open-source alternatives, reflecting stronger multimodal alignment and reasoning capabilities.

\noindent\textbf{Impact of Semantic Cues.} GPT-4o sees a 14.61\% accuracy boost in the MCQ setting with additional element descriptions as options, narrowing its gap with o1 from 18.26\% to just 4.4\%. This indicates that GPT-4o relies heavily on semantic context when available. 

\noindent\textbf{Inconsistent Gains for Open-Source Models.} Most open-source models gain moderate or little accuracy when provided with MCQ-style prompts. Phi-3.5-Vision-4B experiences a 9.93\% drop, suggesting weaker reasoning capacity and less effective use of textual cues. The gap between proprietary and open-source models widens further in MCQ (from 27.08\% to 35.21\%), highlighting the persistent challenge of integrating perceptual grounding with logical inference. 

\subsection{Error Analysis}



\begin{figure*}[h]
\setlength\tabcolsep{0pt}
\setlength{\abovecaptionskip}{0.1cm}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ablation_error.pdf}
    \caption{Fine-grained analysis of model performance.}
    % \vspace{-10pt}
    \label{fig:ablation_error}
\end{figure*}

\subsubsection{Results Across Inconsistency Categories and Modalities}
To investigate how different types of inconsistencies affect model performance, we show the results across the category and modality of inconsistency in Figure~\ref{fig:ablation_error}. 

\paragraph{Inconsistency Categories}
Figure~\ref{fig:ablation_error}(a) breaks down accuracy by the five inconsistency error categories. Proprietary models (o1, GPT-4o) outperform open-source models across the board, but the gap is particularly pronounced for \emph{Factual Contradictions} and \emph{Identity Misattribution}, implying that high-capacity models may have stronger factual grounding and entity recognition. 
Interestingly, \emph{Quantitative Discrepancy} poses a substantial challenge for all models, highlighting a limitation in reasoning about numerical misalignment.

\paragraph{Inconsistency Modalities}
In Figure~\ref{fig:ablation_error}(b), we examine how accuracy varies by the modality of the inconsistency. Overall, inter-modality errors yield the highest performance, with image-image inconsistencies proving especially tractable. 
Next in difficulty are intra-modality errors (image-text), which require partial cross-modal integration but can still leverage textual anchors. 
Finally, single modality (those involving only one text or image field) inconsistencies pose the greatest challenge, as they demand more advanced visual understanding and the ability to reconcile contextual inconsistency without contradiction with another element.
These findings highlight that while models cope relatively well with pairwise conflicts, their capacity for deep visual or contextual reasoning remains underdeveloped.

\subsubsection{Impact of Layout Complexity}

\begin{figure}[t]
\setlength\tabcolsep{0pt}
\setlength{\abovecaptionskip}{0.1cm}
    \centering
    \includegraphics[width=\columnwidth]{figures/ablation_count.pdf}
    \caption{Model performance on layout complexity.}
    % \vspace{-10pt}
    \label{fig:ablation_count}
\end{figure}

We further examine the relationship between model accuracy and the number of elements in an artifact. To ensure statistical significance, we only include data points where at least 10 samples share the same element count. As shown in Figure~\ref{fig:ablation_count}, the overall trend suggests that handling visually dense, information-rich artifacts remains a major challenge for current MLLMs.
(1) Performance declines sharply as the number of elements increases, highlighting the difficulty in parsing cluttered layouts.
(2) Proprietary models maintain higher accuracy in simpler layouts but degrade similarly in highly dense artifacts, indicating limitations in spatial reasoning. Open-source models struggle even in low-complexity settings, reinforcing the gap in perception and layout-aware inference.

\subsection{Probing on Prompting Methods}

\input{sections/tables/ablation_cot_som}

We further investigate whether textual or visual prompts can alleviate the reasoning bottleneck. Table~\ref{tab:ablation_cot_som} compares \emph{Chain-of-Thought (CoT)} prompting~\cite{chain-of-thought} and \emph{Set-of-Mark (SoM)} visual augmentation~\cite{Yang2023SetofMarkPU}, as well as their combination. We also explored an interleaved multimodal reasoning strategy, which we term \emph{Multimodal Interleaved CoT (MM-CoT)} to further integrate and refine reasoning across both visual and textual modalities.

\subsubsection{Chain-of-Thought (CoT) Prompting}

To assess whether explicit reasoning instructions can enhance performance, we apply CoT prompting~\cite{chain-of-thought} to the four open-sourced models (benchmarked proprietary models have API guides to not include additional CoT prompting).

As shown in Table~\ref{tab:ablation_cot_som}, CoT prompting yields negligible or even negative effects on accuracy. This suggests that simply injecting explicit reasoning steps is insufficient when the underlying model lacks strong cross-modal alignment or robust logical inference mechanisms.

\subsubsection{Set-of-Mark (SoM) Prompting}

\begin{figure}[h]
\setlength\tabcolsep{0pt}
\setlength{\abovecaptionskip}{0.1cm}
    \centering
    \includegraphics[width=\columnwidth]{figures/som.pdf}
    \caption{Example of original artifact in MMIR (left) and artifact annotated with Set-of-Mark in the probing analysis (right).}
    % \vspace{-10pt}
    \label{fig:som}
\end{figure}

We next examine the effect of SoM visual prompting~\cite{Yang2023SetofMarkPU}. By overlaying bounding boxes onto the artifact screenshots (example in Figure~\ref{fig:som}), we aim to enhance the models’ ability to perceive and localize elements.

The result shows that these additional visual cues yield moderate improvements for GPT-4o (5.34\%) yet confuse the rest of the models, leading to little or even slightly degraded performance, likely because the additional visual cues interfere with the model’s initial perception.

When combined with CoT prompting, SoM provides little gains for some open-source models but remains largely inconsistent or even detrimental for others. This indicates that simply stacking CoT and SoM techniques does not guarantee improved performance, underscoring the need for more sophisticated strategies to unify visual cues with explicit reasoning steps.

\subsubsection{Multimodal Interleaved CoT (MM-CoT)}

Our previous analyses indicate that single-modality prompts (CoT or SoM) often yield minimal or even detrimental gains in the open-ended setting when models receive no textual hints about which elements might be inconsistent. We hypothesize that MMIR tasks demand \textit{iterative} reasoning that tightly integrates both visual and textual modalities. To address this, we propose \emph{Multimodal Interleaved CoT (MM-CoT)}, a two-stage approach explicitly designed to weave visual cues into a step-by-step reasoning process:

\paragraph{Stage 1: Initial Candidate Generation}  
The model receives the same input in Stage 1 as in the open-ended setting, generating its top five predictions (along with associated reasoning). Using o1-mini (0912) to interpret these responses, we map each prediction back to one or a pair of element IDs from the artifact’s metadata \(C_i\). We then highlight the bounding boxes of those elements on the artifact image, producing an SoM-annotated version to be used in the next stage.

\paragraph{Stage 2: Multimodal Refinement}  
The model is subsequently given the SoM-annotated artifact from Stage 1, alongside the textual reasoning it generated previously. This additional visual context helps the model refine its earlier predictions, integrating both the visual bounding-box annotations and the initial textual reasoning to arrive at a final answer.

\paragraph{Results}  
As shown in Table~\ref{tab:ablation_cot_som}, MM-CoT outperforms all other prompting methods. GPT-4o, for example, improves by 4.40\% over its vanilla baseline, while open-source models gain an average of around 2\% improvements. These findings underscore the importance of iterative cross-modal reasoning: once textual inferences guide which visual elements to focus on, SoM annotations become more informative, and the overall reasoning process becomes more accurate. Although the bounding boxes used for SoM are derived from ground-truth references, this probing experiment demonstrates that \emph{interleaved} multimodal interaction is a promising direction for closing the reasoning gap in challenging, inconsistency-heavy scenarios.