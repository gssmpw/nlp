\section{MMIR}
% \subsection{Overview}

\begin{figure*}[t]
\setlength\tabcolsep{0pt}
\setlength{\abovecaptionskip}{0.1cm}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/examples.pdf}
    \vspace{-20pt}
    \caption{There are five inconsistency categories in the MMIR benchmark, posing diverse challenges.}
    % \vspace{-10pt}
    \label{fig:MMIR_examples}
\end{figure*}

The MMIR benchmark is designed to assess how effectively MLLMs can detect and localize semantic mismatches within complex, layout-rich artifacts. Unlike conventional benchmarks that assume coherent visual–textual inputs, MMIR challenges models with realistic errors that require deep, cross-modal reasoning. In MMIR, errors are defined and categorized along five semantic dimensions:

\textit{A. Factual Contradiction}: Direct conflict between two elements (text–text, text–image, or image–image) within the modified content.

\textit{B. Identity Misattribution}: Mislabeling of entities (objects, locations, brands, people) that conflict with other elements.
    
\textit{C. Contextual Mismatch}: Tonal, thematic, or situational incompatibility between elements.

\textit{D. Quantitative Discrepancy}: Numerical or statistical inconsistencies between elements.

\textit{E. Temporal/Spatial Incoherence}: Implied timelines, dates, or spatial relationships that are impossible or conflicting.

% \textit{Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy}, and \textit{Temporal/Spatial Incoherence}.
Figure~\ref{fig:MMIR_examples} provides one example from each error type across web, office, and poster artifacts, illustrating the diverse challenges MMIR poses. 
% Detailed definitions and examples of inconsistency error types can be found in~\ref{appendix:sec:error category}.

\subsection{Data Curation}

MMIR’s data is curated through a four-stage pipeline (Figure~\ref{fig:data_filter}), ensuring high-quality, diverse, and challenging test cases.

\begin{figure}[htbp]
\setlength\tabcolsep{0pt}
\setlength{\abovecaptionskip}{0.1cm}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/data_filter.png}
    \caption{MMIR Data filtering process.}
    % \vspace{-10pt}
    \label{fig:data_filter}
\end{figure}

\paragraph{Artifact Collection and Parsing}
We begin by manually selecting a total of 521 original artifacts from two domains: 349 webpages (sub-categories: shopping, classifieds, wiki) from VisualWebArena~\cite{Koh2024VisualWebArenaEM} and 172 presentations from Zenodo~\cite{zenodo}, categorized into Office (sub-categories: slides, charts, diagrams) and Posters. 
Each artifact $A_i$ is parsed using either using Document Object Model (DOM) or the \texttt{python-pptx} library to extract a set of elements $E_i = \{e_j\}_{j=1}^{n_i}$, where each element $e_j$is assigned a unique ID \(\mathtt{id}_j\) and labeled with its type, content, and a bounding box showing location information. Additionally, each artifact is paired with a Set-of-Marks (SoM) annotation $A_{i}^{\text{SoM}}$ derived from $E_i$. This structured metadata forms the basis for subsequent error injection and question-answer curation.

\paragraph{Synthetic Inconsistency Generation}
To simulate real-world errors, we prompt an MLLM, o1-1217~\cite{openai2024gpto1card}, as a generator with the annotated artifact and its element set $\{A_{i}^{\text{SoM}}, E_i\}$.
The generator produces 2,534 proposals,
% . Each proposal $P_m = \{\mathtt{edit}_m, \mathtt{GT}_m, \mathtt{category}_m, \mathtt{rationale}_m\}$ 
each comprising a formatted edit instruction,
% (with fields ${\mathtt{id}, \mathtt{type}, \mathtt{content}, \mathtt{new\_content}}$)
the ground-truth element or element pair introducing the inconsistency:
$$\mathtt{GT} \in \{\mathtt{id}_j\} \cup \{(\mathtt{id}_j, \mathtt{id}_k) | j \neq k\},$$
the inconsistency error type, and the accompanying rationale. Following a self-evaluation loop (details in Appendix~\ref{appendix:sec:generator}), 2,446 valid proposals are retained.

\paragraph{Automated Editing and Human Verification}
An auto-verification process then filters these proposals based on format and backend constraints (e.g., ensuring the target elements are editable), reducing the candidate set to 1,273, and saves low-level edit details, such as the path of the new image for an image edit, as inputs to the editor. 

An automated editor-implemented using the Chrome DevTools Protocol (CDP) for web pages and \texttt{python-pptx} for presentations-executes the approved edits, generating for each successful operation a modified pair: $\{A'_i, E'_i\}$ where $A'_i$ represents the modified artifact and $E'_i$ contains the updated element metadata after the edit.
For each pair, a descriptive caption set $C_i$ is generated, where each caption within $C_j$ details the element ID, location, and content summary of $e'_j$. These captions serve as references for later evaluation.
More details on the verifier and editor are provided in Appendix~\ref{appendix:sec:verifier and editor}.

\input{sections/tables/statistics}

Finally, human experts review 747 edited samples, resulting in a final dataset of 534 validated quintuples: 
$D_{MMIR} = \{S'_i, E'_i, \mathtt{GT}_i, \mathtt{category}_i, \mathtt{rationale}_i\}_{i=1}^{534}$, ensuring that only realistic and challenging samples remain. Table~\ref{tab:statistics} provides a detailed breakdown by artifact type, subcategory, and error type. For example, webpages are further divided into shopping, wiki, and classifieds, each with its average number of elements, while errors are distributed across the five defined categories. Notably, the average word count in multiple-choice questions is 382.6, whereas open-ended responses are fixed at 59 words.

\subsection{Evaluation}

MMIR assesses a model's ability to \emph{detect inconsistency}, i.e., identifying and localizing semantic mismatches where elements deviate from their expected roles within an artifact. To assess the model's performance comprehensively, each of the 534 test samples is provided to models under two distinct settings:

\paragraph{Open-Ended Setting}
Models receive the artifact $A'_i$ with a fixed prompt $Q_\text{open\_ended}$ and generate a free-form response that identifies the semantic mismatch. 
% Their responses are compared with $C_i$ and mapped back to the ground-truth element IDs $\mathtt{GT}_i$ using o1-mini (0912)~\cite{openai2024gpto1card} as a judge. 
This formulation evaluates the model's ability to detect inconsistencies without relying on predefined answer options, thereby testing its unsupervised perception and reasoning.

\paragraph{Multiple-Choice Setting}
Models receive the artifact $A'_i$, but now with a combined prompt 
$
Q_{\text{MCQ}} = (Q_\text{open\_ended}, C_i).
$
Each candidate in $C_i$ is a textual description of an element. The model must select, from these options, the element(s) corresponding to the introduced inconsistency.

\paragraph{Evaluation Setup} 
For the MCQ setting, we utilize regular expressions to compare the MLLM's predicted answers against the ground truth, using accuracy as our metric. 
For the open-ended setting, o1-mini (0912) is employed as an LLM judge~\cite{hsu2023gpt,hackl2023gpt,liu2023g} to map the model's free-form response back to the most likely ground-truth element IDs.
The predicted IDs are then compared against $\mathtt{GT}_i$ to calculate accuracy.
