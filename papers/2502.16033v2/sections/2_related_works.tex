\section{Related Work}
% \eric{cite and discuss more relate work.}
    % # reasoning LLM/MLLM
    % # reasoning benchmark - MLLM, explicit reasoning (reasoning requirement)
    % # multipanel VS/CoT

\paragraph{Multimodal Understanding and Reasoning} 
Multimodal Large Language Models (MLLMs) process multimodal inputs by first processing visual inputs with pre-trained vision encoders such as CLIP \cite{radford2021learning} to extract features, and then projecting them into the textual representation space with adapters \cite{liu2024improved,li2023blip}. Significant efforts have been made to bridge the gap between vision and text modalities via integrating more cross-modality data such as interleaved image-text sequences and visual grounding data \cite{alayrac2022flamingo,chen2023shikra, peng2023kosmos}. Also, some recent works develop MLLMs with improved nuanced multimodal abilities, such as Optical Character Recognition (OCR) \cite{bai2023qwen,liu2024llavanext}, layout understanding \cite{feng2024layoutgpt, fan2024read}, Graphic User Interface (GUI) interpretation \cite{liu2024harnessingwebpageuistextrich, Qwen2.5-VL}.

As MLLMs typically leverage pre-trained large language models (LLMs) as the backbone, they inherent strong textural reasoning abilities from the advanced LLMs\cite{floridi2020gpt,touvron2023llama, bai2023qwen,taori2023stanford,chowdhery2023palm, openai2024gpt4ocard, geminiteam2024geminifamilyhighlycapable}. To further enhance the reasoning ability of MLLMs, increasing efforts have focused on improving MLLMs in multimodal reasoning. The proprietery model, o1 \cite{openai2024gpto1card} first realize strong multimodal reasoning with reasoning process similar to the Chain-of-Thought \cite{chain-of-thought} and other following works have also explored the multimodal reasoning either through training \cite{wu2024v,qi2024cogcom,shao2024visual} or prompting \cite{zhang2023makes,zhang2024prompt,zheng2023ddcot}.
% \eric{o1 should be discussed separately.}
%As a result, MLLMs have expanded their multimodal reasoning scope beyond natural images to structured and composite visual formats, such as documents, GUI screenshots, posters, and diagrams, where models needs to deal with layout, interleaved image and text, graphical elements, etc. 
    

%Building on these developments, our work investigates the ability of MLLMs to detect and localize misalignments—such as contradictory text-image pairs or inconsistent visual-textual semantics—in diverse multimodal content.
    % \eric{(Visual) CoT}
\paragraph{Multimodal Reasoning Benchmarks} To evaluate the reasoning capabilities of MLLMs, numerous benchmarks have been developed with various focuses. Broad-coverage benchmarks such as MM-Bench \cite{liu2024mmbench}, MMMU \cite{yue2023mmmu} and MM-Vet \cite{yu2024mmvetevaluatinglargemultimodal} cover comprehensive reasoning challenges in real life scenarios, offering holistic insights into model performance. Others are developed with focuses on specific perspectives, such as TextVQA \cite{singh2019towards}, POPE \cite{li2023evaluatingobjecthallucinationlarge} and MATHVERSE \cite{zhang2024mathversedoesmultimodalllm} respectively challenge models with tasks in domains of reasoning about text, objects, mathematics in multimodal contexts. Recently, additional benchmarks have emerged targeting artificially created multipanel images—such as posters and screenshots—that combine several subfigures in structured layouts \cite{fan2024muffin,hsiao2025screenqalargescalequestionanswerpairs}, which require models to analyze spatial relationships and hierarchical structures in complex visual contexts.
However, current multi-modal benchmarks assume visual-text alignment, overlooking detecting critical errors of vision-language inconsistency in the input - a key challenge in real-world scenarios. Instead, we evaluate MLLMs’ ability to detect and localize such inconsistency via the proposed MMIR benchmark.
% \eric{Multi-panel Visual Understanding?}

\paragraph{Inconsistency Checking}
Existing works on tasks related to checking or verifying inconsistency in the input are primarily in the language domain. For example, fact-checking \cite{thorne2018fever} requires a model to first retrieve evidence and then decide if a claim is supported, where the model must reason if contradictive information existed in the retrieved corpus. One step further, summary inconsistency detection \cite{laban2022summac} focuses on flagging any errors in summaries that create contradictions regardless of correctness, including incorrect use or hallucination of entities. As modern language models prosper, inconsistencies are found existing within their outputs \cite{ravichander2020systematicity} and across different outputs of paraphrased queries \cite{elazar2021measuring}, and efforts have been made towards the evaluation of those inconsistencies \cite{fabbri2021qafacteval,wang2020asking,lattimer2023fast}. In our research, we lead efforts in detecting inconsistencies in the field of vision and language.