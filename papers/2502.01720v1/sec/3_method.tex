




\section{SynCD: Synthetic Customization Dataset}
Training tuning-free customization models requires a diverse dataset of different objects, each with multiple images in different contexts. 
To address the data shortage and collection challenge, we introduce an automatic data curation pipeline for synthesizing diverse, high-quality image corpora.

This section outlines our data generation pipeline with two main components: (1) Creating a set of $N$ prompts per object and (2) Creating $N$ \emph{coupled} images with a consistent object given the prompts. For the objects, we sample approximately $75, 000$ rigid category assets from Objaverse~\cite{deitke2023objaverse} and $16$ deformable categories mainly belonging to pet animals, as personalizing on pets is a common use case.
We use the Instruction-tuned LLama3~\cite{dubey2024llama} and the Flux model~\cite{flux} for generating these coupled images with the same object, each with distinct backgrounds and pose variations. 

\subsection{LLM assisted prompt generation}\lblsec{llm}
We design each prompt to have a detailed description of both the object and the background, as having a detailed description of the object already helps enhance consistency. In the case of Objaverse, Cap3D~\cite{luo2024scalable} provides detailed captions for each asset, e.g., {\menlo a large metal drum with blue and pink stripes}. For deformable objects, we instruct the LLM to generate descriptive captions, e.g., {\menlo The Russian blue cat has a thick plush coat}. Based on the object description, we instruct the LLM~\cite{dubey2024llama} to generate plausible background scene descriptions. 
We then combine a common object description with multiple background descriptions for multi-image generation, as shown in \reffig{dataset_generation}. We provide the instructions used to prompt the LLM in \refapp{details}. 

\subsection{Multi-image consistent-object generation} 
Using the LLM-suggested prompts in the previous step, we aim to generate multiple images of a common object. To enforce object consistency for both deformable and rigid objects, we share the latent features across the images during the denoising process via a Masked Shared Attention (MSA) mechanism~\cite{tewel2024training,shi2023zero123++}. For rigid objects, we further leverage depth and multi-view correspondence derived from Objaverse 3D assets. We explain both steps below.  


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/msa.pdf}
    \caption{{\textbf{Feature warping and Masked Shared Attention (MSA) for object consistency.}  For rigid objects, we first warp corresponding features from the first image to the other. Then, each image feature attends to itself, and the foreground object features in other images. We show an example mask, $\mathbf{M}_1$, used to ensure this for the first image when generating two images with the same object. 
    }
    }
    \lblfig{msa}
    \vspace{-15pt}
\end{figure}

\myparagraph{Masked Shared Attention (MSA).} We modify the attention block of the diffusion model such that each image attends to itself as well as the foreground object regions of the other images. Given query, key, and value features, $\q_i, \k_i, \v_i \in \mathbb{R}^{n \times d'} $, of the $i^{th}$ image, a shared attention block performs the following operation:
\begin{equation}
    \begin{aligned}
    & \text{MSA}(\{\q_i, \k_i, \v_i\}_{i=1}^{N}) \equiv \\ &  \Big \{ \text{Softmax}\Big(\frac{\q_i [\k_1 \cdots \k_N]^T}{\sqrt{d'}} + \mathbf{M}_i \Big)[\v_1 \cdots \v_N] \Big \}_{i=1}^{N},\\
    \end{aligned}\lbleq{msa}
\end{equation}



where $d'$ is the feature dimension and $n$ is the sequence length. Each $\q_i$ attends over the $N \times n$ features, and the `mask', \ie attention bias matrix $\mathbf{M}_i \in \mathbb{R}^{n \times (Nn)}$ ensures that the $i$-th image feature only attends to the object region of other images and ignores their background. Since DiT model~\cite{peebles2023scalable} consists of joint text and image attention, the mask $\mathbf{M}_i$ is initialized so that text tokens of one image do not attend to other image tokens, as shown in \reffig{msa}.

MSA enables us to generate  with similar visual features among all the images. However, MSA does not explicitly enforce 3D multi-view consistency, as qualitatively shown in the Appendix, \reffig{dataset_ablation2}. Therefore, for rigid objects with available 3D datasets like Objaverse, we use these assets to ensure multi-view consistency, as described next.






\myparagraph{Rigid object generation with MSA and 3D consistency.} Given an Objaverse asset, we render it from $N$ varying camera poses and feed the rendered depth map and captions generated in \refsec{llm} to a depth-conditioned Flux model. During denoising timesteps, Masked Shared Attention (MSA) is applied across all the images using the ground truth masks from the rendered depth map. Depth guidance ensures 3D shape consistency of the object across the images, while MSA encourages similar visual appearance. %
Given the 3D mesh and relative camera poses, we establish pixel-wise cross-view correspondence for locations visible in both views. 
We then enhance multi-view consistency further by warping the corresponding features from the first image to the other images, as shown in \reffig{msa}. Thus, given latent features $f_i  \in \mathbb{R}^{(h \times w) \times d}$, $i \in \{1 \cdots N\}$, the query, key, and value features input to MSA are calculated as:
\vspace{-10pt}
\begin{equation}
    \begin{aligned}
   & \hat{f}_i(h,w) = \alpha f_1 (C_{i}(h,w)) + (1 - \alpha) f_i(h,w), \\
   & \hat{\q}_i = W_{\q} \hat{f}_i, \; \hat{\k}_i = W_{\k} \hat{f}_i, \; \hat{\v}_i = W_{\v} \hat{f}_i,
\end{aligned}\lbleq{msa_objaverse}
\end{equation}
where for a given pixel $(h,w)$ in the $i^{th}$ image, $C_{i}(h,w)$ denotes its corresponding location in the first image, $\alpha$ is the visibility mask, $f_1 (C_{i}(h,w))$ is the corresponding feature in the first image computed with bilinear interpolation, and $W_{\q}$, $W_{\k}$, $W_{\v}$ are query, key, value projection matrics. We only apply this warping during the initial diffusion timesteps. This increases multi-view consistency without introducing warping artifacts and allows flexibility for lighting variations. 




\reffig{dataset_generation} shows the overall framework of our dataset generation pipeline, where we use $N$ as $3$. We also use aesthetic score~\cite{aesthetic} and object similarity via DINOv2~\cite{oquab2023dinov2} feature space to remove low-quality and inconsistent images to get a final dataset of $95K$ objects with $2$-$3$ images per object, uniformly distributed among rigid and deformable categories.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/method.pdf}
    \vspace{-18pt}
    \caption{{\textbf{Training Method.} We condition the model on reference images via two pathways -- (1) Masked Shared Attention, similar to \reffig{msa}, on fine-grained reference features extracted from the pre-trained text-to-image model with the same timestep, $t$, forward noise added to it as target $\x_1^t$ and (2) Decoupled image cross-attention~\cite{ye2023ip} on a global image feature extracted from CLIP~\cite{radford2021learning}.
    }
    }
    \lblfig{methoddiagram}
    \vspace{-15pt}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/results_1input.pdf}
    \vspace{-25pt}
    \caption{{\textbf{Results.} We compare our method qualitatively against other leading tuning-free baselines with a single reference image as input. We can successfully incorporate the text prompt while preserving the object identity similar to or higher than the baseline methods. We pick the best out of $4$ images for all methods. More qualitative samples are shown in Figure~\ref{fig:results_comparison_1ref_1} and \ref{fig:results_comparison_1ref_2} in the Appendix.
    }}
    \lblfig{results_comparison1}
    \vspace{-10pt}
\end{figure*}

\myparagraph{Discussion.} Our key insight is that synthesizing such a dataset with consistent object identities, using internal feature sharing and external 3D guidance, is far more scalable than collecting real-world data. Moreover, generating such data is also more tractable than the task of model customization with real images, where access to the internal features and the object's true 3D geometry is not easily available.

\section{Our Customization Model}

\subsection{Diffusion Learning Objective}
A text-to-image diffusion model aims to learn the distribution of images given a text prompt, i.e., $p(\x | \c)$, where $\x$ is a real image corresponding to the text prompt $\c$. The model consists of a forward Markov process in which the real sample is gradually transformed to random noise $\x^T \sim \mathcal{N} (\mathbf{0}, \mathbf{I})$ in $T$ timesteps. During training, the model learns to denoise the input noisy image $\x^{t} = \sqrt{\alpha^t}\x + \sqrt{1 - \alpha^t}\epsilon $ given the text prompt $\c$ via the following loss function: 
\begin{equation}
    \begin{aligned}
        \mathbb{E}_{\x^t,t,\mathbf{c}, \epsilon \sim \mathcal{N} (\mathbf{0}, \mathbf{I})} [w^t||\epsilon - \epsilon_{\theta} (\x^t, t, \mathbf{c}) ||],
    \end{aligned}
\end{equation}
where $\epsilon$ is the input noise, $t$ is the current timestep, $\sqrt{\alpha_t}$ determines the noising ratio, $\epsilon_{\theta}$ is the predicted noise, and $w^t$ is a weighing function. Other formulations of the training objective also exist for more stable training. Specifically in this work, we use the $\mathbf{v} \equiv \sqrt{\alpha_t}\epsilon - \sqrt{1-\alpha_t}\x $ prediction objective instead of $\epsilon$ as proposed by Salimans~\etal~\cite{salimans2022progressive}.

\subsection{Architecture Design and Training}

A tuning-free image customization method aims to learn $p(\x | \c, \{(\x_i, \c_i)\}_{i=1}^K)$, i.e., the distribution of images aligned with both the input text prompt and object identity as shown in the $K$ reference images. To train such a model, we use the $N$ coupled images of an object generated in the previous step and consider one of them as the target and the rest as references to condition the generation process.  %

\myparagraph{Reference image conditioning.} Our base model is an image-conditioned text-to-image diffusion model~\cite{ye2023ip}, which performs image cross-attention with a \emph{global feature} extracted from CLIP~\cite{radford2021learning} for conditioning on an image.

To condition the generation on multiple reference images, motivated by our dataset generation pipeline, we propose to additionally employ Masked Shared Attention with \emph{fine-grained features} of the reference images. Specifically, we use the forward Markov process to add the same timestep noise to the reference images as the target image and extract their features from the base model. Similar to the dataset curation step in \reffig{msa}, 
reference features are then concatenated with the target image features along the sequence dimension in each MSA block. The query features of the target image are subsequently updated by attending to both itself and all foreground object features within the reference images. The overall framework is as shown in \reffig{methoddiagram}.




\myparagraph{Training details.} 
We fine-tune two pretrained U-Net-based latent diffusion models (with 1B and 3B parameters) using a noise schedule with zero-SNR at the terminal timestep~\cite{lin2024common}. 
We initialize our model with the pre-trained IP-Adapter Plus~\cite{ye2023ip} and randomly select one of the reference images as the global image condition. We only fine-tune LoRA~\cite{hu2021lora} layers in the self-attention blocks of diffusion U-Net and key, value projection matrices in image cross-attention layers of IP-Adapter. Additionally, we incorporate random text-image pairs in $10\%$ of training steps from a licensed image-text dataset and drop text, image, or both conditions with $5\%$ probability to enable classifier-free guidance. More training hyperparameter details are provided in \refapp{our_method_details}.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/results_3input.pdf}
    \vspace{-15pt}
    \caption{{\textbf{Results with 3 input images.} Here, we show qualitative samples of our method and JeDi~\cite{zeng2024jedi}, which can take multiple reference images as input. Though JeDi maintains high object identity alignment, the background and lighting can often be incoherent in the generated images. Comparatively, our method maintains higher image fidelity while following image and text conditions. We pick the best out of $4$ images for both methods. Zoom in for more details. We show more qualitative samples in \reffig{results_comparison_3ref} in the Appendix.
    }}
    \lblfig{results_comparison2}
    \vspace{-10pt}
\end{figure*}

\subsection{Tuning-Free Inference}
\label{sec:tuning_free_inference}
During inference, given $K$ reference images, $I=\{\x_i \}_{i=1}^K$, at each time step, we add forward diffusion noise to all the reference images and extract their fine-grained features for masked shared attention. One of the reference images is randomly selected for the global feature. We then combine the text and image guidance using the formulation of Brooks~\etal ~\cite{brooks2023instructpix2pix}. Empirically, we observed that the image guidance vector is typically of a much higher magnitude than the text guidance vector, especially at higher guidance scales. This often leads to over-exposure issues in the generated image. To mitigate this, we propose normalizing image and text guidance vectors. This helps us achieve better image alignment with the reference object while still following the text prompt. Our final inference is
\begin{equation}
    \begin{aligned}
    & \epsilon_{\theta}(\x^t, I, \varnothing) + \lambda_{I}\frac{||g||}{||g_{I}||} \cdot g_{I}  + \lambda_{\c}\frac{||g||}{||g_{c}||} \cdot g_{\c}, \\
 \text{where } & g_{I} = \epsilon_{\theta}(\x^t, I, \varnothing)  - \epsilon_{\theta}(\x^t, \varnothing, \varnothing), \\
   & g_{\c} = \epsilon_{\theta}(\x^t, I, \mathbf{c})  - \epsilon_{\theta}(\x^t, I, \varnothing), \\
   & ||g|| = \min (||g_I||, ||g_{\c}||),
    \end{aligned}\lbleq{inference}
\end{equation}
where $t$ is the denoising timestep, $\epsilon_{\theta}$ is the diffusion U-Net output, $g_{I}$ and $g_{c}$ are the guidance vectors towards image and text condition, and $\lambda_{I}$ and $\lambda_{\c}$ represent the guidance strength for the image and text. We scale the norm of the two guidance vectors to the minimum norm, allowing only $\lambda_{I}$ and $\lambda_{\c}$ to vary the relative strength of the image and text guidance. Note that the number of reference images during inference can be different than the training time.





