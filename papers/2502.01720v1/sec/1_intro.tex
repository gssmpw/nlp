\section{Introduction}
\label{sec:intro}
Text-to-image models are capable of generating high-fidelity and realistic images given only a text prompt~\cite{peebles2023scalable,saharia2022photorealistic,rombach2022high,esser2024scaling}. Yet, text often falls short of describing rich visual details of real-world objects, such as the unique toy in \reffig{teaser}. What if the user wishes to generate images of the toy in new scenarios? This has given rise to the emerging field of model customization or personalization~\cite{ruiz2022dreambooth,gal2022image,kumari2023multi,chen2023subject,wei2023elite}, allowing us to generate new compositions of the object via text prompts, e.g., {\menlo the toy in a wheat field}, as shown in \reffig{teaser}. Early works~\cite{gal2022image,dreamboothimpl,kumari2023multi} require many optimization steps on user-provided images of every single new object, a process both costly and slow. 
To address this, several encoder-based methods
~\cite{wei2023elite,chen2023subject,li2023blip,ye2023ip,song2024moma} train the model with reference images as an additional input. During inference, these tuning-free methods can generate new compositions of the reference object in a single forward pass without expensive per-object optimization.




However, the lack of a dataset comprising multiple images of the same object in diverse poses, backgrounds, and lighting conditions has been a major bottleneck in developing these methods. Collecting such large-scale datasets from the internet is difficult, as real images are often not annotated with object identity. 
In this work, we aim to address this data shortage challenge using a new synthetic dataset generation method. %
This is challenging, as we need to maintain the object identity while generating multiple images with varying contexts. %
To achieve this, our first idea is to employ shared attention among foreground object regions in multiple generated images, ensuring visual consistency across different images. %
Furthermore, to ensure 3D multi-view consistency for rigid objects, we use Objaverse~\cite{deitke2024objaverse} assets as a 3D prior. Specifically, we use depth guidance and %
cross-view correspondence between different renderings to promote object consistency further. Finally, we automatically filter out any low-quality and inconsistent object images. 



Given our synthetically curated dataset, \emph{SynCD}, we propose new training and inference methods for tuning-free customization. We borrow the idea of shared attention from our dataset generation pipeline and incorporate it in the training method as well. This conditions the generation on fine-grained features of input reference images, improving object identity preservation. During inference with classifier-free guidance, we propose a normalization technique that better incorporates both text and image conditions without overexposure issues. Extensive experiments show that our full method using the new dataset outperforms  tuning-free customization baselines like JeDi~\cite{zeng2024jedi}, Emu-2~\cite{Emu2}, and IP-Adapter~\cite{ye2023ip}. Compared to standard tuning-based methods, it performs on par with identity preservation while better following the input text prompt.





