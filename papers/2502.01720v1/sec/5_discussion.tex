\section{Limitations and Discussion}
In this work, we focus on tuning-free model customization and propose advancements to address current limitations. To overcome the lack of training data, we have created a synthetic dataset by generating multiple images with consistent objects using Masked Shared Attention and 3D asset priors. We then propose an improved model architecture and inference technique. Our approach outperforms existing tuning-free methods while being on par with existing time-consuming tuning-based approaches. %

While promising, our method has limitations, e.g., it struggles with intricate textures and can have limited pose variability. Integrating recent advances in text-to-3D and video generative models can enhance the quality of the generated dataset, ultimately leading to improved performance in downstream applications.
