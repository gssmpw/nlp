\section{Related Works}
\myparagraph{Text-to-image models.}
With recent advancements in training methods~\cite{ho2020denoising,dhariwal2021diffusion,sauer2023stylegan,karras2022elucidating,karras2023analyzing,liu2022flow,yu2022scaling}, model architectures~\cite{peebles2023scalable,rombach2022high,esser2024scaling,ramesh2022hierarchical,kang2023scaling}, and datasets~\cite{schuhmann2021laion}, text-conditioned generative models have excelled at photorealistic generation while adhering to text prompts. Primarily among them are diffusion~\cite{lu2022dpm,rombach2022high} and flow~\cite{esser2024scaling,flux} based models, commonly trained in an encoded latent space~\cite{rombach2022high,esser2021taming}.  
Their impressive generalization capability has enabled diverse  applications~\cite{parmar2023zero,richardson2023conceptlab,hertz2022prompt,mokady2023null,meng2021sdedit,hertz2024style,huang2024creativesynth,gu2024swapanything,ge2023expressive}. However, text as a modality can often be imprecise. This has given rise to various works on improving text alignment~\cite{chefer2023attend,ge2023expressive,liu2022compositional} and increasing user-control via additional image conditions~\cite{zhang2023adding,chen2022re}. 


\myparagraph{Image-conditioned generation}%
aims to enhance control of the generation process by incorporating additional inputs, such as depth and segmentation maps~\cite{zhao2024uni,zhang2023adding,avrahami2023spatext,densediffusion} or layout conditions~\cite{li2023gligen,phung2023grounded,Zheng_2023_CVPR,bhat2024loosecontrol} to control the spatial structure. 
Another use-case is to condition the generation on context images~\cite{chen2022re,najdenkoska2024context}, either retrieved from a dataset or given by the users themselves. These images provide relevant context apart from the text prompt to help guide the generation. 



\myparagraph{Customizing text-to-image models.} A particular case of image-conditioned generation is the task of model customization or personalization~\cite{ruiz2022dreambooth,gal2022image,kumari2023multi}, which aims to precisely learn the concept shown in reference images, such as pets or personal objects, and compose it with the input text prompt. 
Early works in model customization fine-tune a subset of model parameters~\cite{kumari2023multi,han2023svdiff,hu2021lora,tewel2023key} or text token embeddings~\cite{gal2022image,voynov2023p+,zhang2023prospect,alaluf2023neural} on the few user-provided reference images with different regularization~\cite{ruiz2022dreambooth,kumari2023multi}. However, this fine-tuning process for every new concept is both time-consuming and computationally expensive. 
Thus, many works now focus on training tuning-free methods, which do not require per-object optimization during inference.  


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/datapipeline.pdf}
    \caption{{\textbf{Dataset Generation Pipeline.} \textit{Top:} For deformable objects like cats, we use an object description along with a set of background descriptions, both suggested by an LLM, as input to generate multiple images with a common object. \textit{Bottom:} For rigid objects, we use depth from multi-view rendered images of the 3D asset~\cite{deitke2023objaverse}, object description from Cap3D~\cite{luo2024scalable}, and background prompts suggested by the LLM as input to our method consisting of a depth-conditioned~\cite{zhang2023adding} text-to-image model. 
    We use Masked Shared Attention (MSA) and warping (in the case of rigid objects) to promote object consistency, as shown in \reffig{msa}. 
    }}
    \lblfig{dataset_generation}
    \vspace{-10pt}
\end{figure*}


\myparagraph{Tuning-free methods for customization} add an additional image condition to the diffusion model in addition to the text prompt. To achieve this, many of the methods use pre-trained feature extractors to embed reference images into visual embeddings~\cite{li2023blip,song2024moma,wei2023elite,chen2024anydoor,xiao2024fastcomposer,parmar2025object}, which are then mapped to a text token embedding space. Some recent methods have also proposed learning a mapper between multimodal autoregressive models~\cite{touvron2023llama} and generative models to incorporate reference images as visual prompts~\cite{pan2023kosmos,Emu2}. Another commonly adopted design is the decoupled text and image cross-attention~\cite{ye2023ip,wei2023elite,ma2024subject}. Our training method is also motivated by this, but we also propose to insert fine-grained features via shared self-attention.

However, most existing methods still rely on single-image or multi-view datasets for training, with the same or limited background diversity. The reference images are encoded in a compact feature space to prevent overfitting~\cite{li2023blip,song2024moma}, hurting identity preservation. To address this, we propose new methods for creating a synthetic dataset containing multiple images of the same object while having background and pose diversity. %
Our method is motivated by recent works in consistent character~\cite{tewel2024training,zhou2024storydiffusion} and multi-view generation~\cite{shi2023mvdream,deng2024flashtex,shi2023zero123++}, but tailored for the model customization task. While recently Zeng~\etal~\cite{zeng2024jedi} also aims to create a synthetic dataset, they rely on text prompting alone to generate collage images with similar objects.
In contrast, our dataset curation method uses explicit constraints for object consistency, resulting in higher-quality training data. %


