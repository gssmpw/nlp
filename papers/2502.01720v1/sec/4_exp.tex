
\section{Experiments}


\myparagraph{Evaluation dataset.} Consistent with prior works~\cite{zeng2024jedi,song2024moma,pan2023kosmos}, we use DreamBooth~\cite{ruiz2022dreambooth} dataset consisting of $30$ objects with $4$-$5$ images each and 25 evaluation text prompts.


\myparagraph{Baselines.} We compare our method with the leading tuning-free customization baselines, which include JeDi~\cite{zeng2024jedi}, IP-Adapter~\cite{ye2023ip}, Emu-2~\cite{Emu2}, Kosmos~\cite{pan2023kosmos}, BLIP-Diffusion~\cite{li2023blip}, and MoMA~\cite{song2024moma}.

\myparagraph{Evaluation metric.} The goal of the text-conditional image customization task given one or a few reference images is to follow the input prompt while maintaining object identity and image fidelity. To measure the text alignment of generated images with the input prompt, we use CLIPScore~\cite{radford2021learning} and TIFA~\cite{hu2023tifa}. 
To evaluate the alignment of the object in generated images with the reference object, we compute similarity to reference images in DINOv2~\cite{oquab2023dinov2} feature space. Following recent works~\cite{zeng2024jedi,song2024moma}, we compute this similarity using a cropped and background-masked version of the image, denoted as MDINOv2-I, where the mask is computed by pre-trained object detectors~\cite{kirillov2023segment,zhou2022detecting,ren2024grounded}. Given the inherent tradeoff between text and image alignment metrics, we combine the two into a single metric, Geometric score~\cite{yan2023motion}, by taking the geometric mean of TIFA and MDINOv2-I. It is shown in~\cite{yan2023motion} that this geometric mean score is aligned better with the overall human preferences. 
In addition, we also conduct human evaluation to compare to prior works. 









\begin{table*}[!t]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{@{\extracolsep{4pt}}ll c  cc c  c@{} }
\toprule
& \textbf{Method}
 &  \multicolumn{2}{@{} c}{\textbf{MDINOv2-I}$\uparrow$}
& \textbf{CLIPScore} $\uparrow$ & \textbf{TIFA} $\uparrow$ 
& \textbf{GeometricScore} $\uparrow$ \\
\cmidrule{3-4}
&  & \shortstack[c]{Background\\ change prompt} & \shortstack[c]{Property\\ change prompt}  & & \\
\midrule
\multirow{9}{*}{1-input} & \textbf{Kosmos}~\cite{pan2023kosmos} &  0.636&0.638 & 0.287 &  0.729 & 0.679 \\
& \textbf{BLIP-Diffusion}~\cite{li2023blip} &0.658&0.643    & 0.294 & 0.782 & 0.714 \\
& \textbf{MoMA}~\cite{song2024moma}  & 0.616&0.620    & 0.320 & 0.867 & 0.730 \\
& \textbf{JeDi}~\cite{zeng2024jedi}  & 0.684& \textbf{0.690} & 0.303 &  0.833  & 0.754 \\
& \textbf{Ours (1B)}  & \textbf{0.744} & 0.671  & \textbf{0.310} & \textbf{0.850} & \textbf{0.781} \\
\cdashline{2-7}
& \textbf{IPAdapter}~\cite{ye2023ip}  & 0.718 & 0.702 & 0.283 & 0.701 & 0.704\\
& \textbf{IPAdapter Plus}~\cite{ye2023ip}  &0.744& \textbf{0.737} & 0.270 &   0.615 & 0.675 \\
& \textbf{Emu-2}~\cite{Emu2}   & 0.750& 0.736& 0.283 &  0.741  & 0.740  \\
& \textbf{Ours (3B)}   & \textbf{0.777}& 0.708  & \textbf{0.319} &   \textbf{0.902}  & \textbf{0.825} \\
\cdashline{1-7}
\multirow{3}{*}{3-input} & \textbf{JeDi}~\cite{zeng2024jedi} & 0.771&0.775  & 0.292 &   0.789 & 0.780 \\
& \textbf{Ours (1B)}   & 0.806&0.773  & 0.303 &   0.830 & 0.801 \\
& \textbf{Ours (3B)} & \textbf{0.822} &\textbf{0.789}  & \textbf{0.313} &  \textbf{0.863}  & \textbf{0.838}  \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{ \textbf{Quantitative comparison}. We compare our method against other tuning-free methods with similar model scales on image alignment and text alignment metrics. Our method performs better than other baselines on the combined GeometricScore metric. For reference, the all-pairwise MDINOv2 similarity between reference images themselves using ground-truth masks is $0.851$. }
\label{tbl:metrics}
\vspace{-8pt}
\end{table*}

\subsection{Comparison to Prior Works}

\subsubsection{Quantitative Comparison}
\myparagraph{Automatic scores.} \reftbl{metrics} compares our method against tuning-free baselines. For MDINOv2-I metrics, we measure it on two subsets separately -- prompts that only change the background vs. prompts that modify the appearance, e.g., {\menlo cube-shaped or wearing sunglasses}, with the latter subset expected to yield lower image similarity in comparison. We evaluate our method using either $1$ or $3$ input reference images. Our approach outperforms all the baselines in the overall Geometric Score, last column in \reftbl{metrics}, when compared with baselines of similar model scales. This indicates that we maintain a good balance between object identity preservation and following the input prompt. 

 \noindent We also compare our method against tuning-based approaches in Table~\ref{tbl:tuningbased_metrics}. For a single input image, we compare with Break-a-Scene~\cite{avrahami2023break}, which also uses one image. With $3$ reference images, we benchmark against LoRA~\cite{loraimplementation,dreamboothimpl}. As shown in \reftbl{tuningbased_metrics}, our method achieves comparable performance in image alignment while improving text alignment, suggesting reduced overfitting to the reference images. We show qualitative samples in \refapp{appendix_result}.
 
\myparagraph{Human evaluation.}
For a comprehensive evaluation, we also conduct a pairwise user study.
In each study, participants view two generated images (from our method and a baseline) alongside the text prompt and $3$ reference images. We ask them to select the preferred image based on three criteria: (1) Consistency with the reference object (image alignment), (2) Alignment with the text prompt (text alignment), and (3) Overall quality and photorealism (quality). They also indicate the specific criterion or criteria for their selection. \reftbl{human_eval} shows the results compared to two competing methods from \reftbl{metrics}, i.e., Emu-2~\cite{Emu2} and JeDi~\cite{zeng2024jedi}. Our method is preferred over both baselines according to all evaluation criteria. To ensure valid responses, participants complete a practice test, and only those with correct responses are considered. We gather approximately $300$ valid responses per comparison. We provide further details in \refapp{appendix_eval_details}. 


\begin{table}[!t]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{ll cc c  c}
\toprule
\multirow{3}{*} & \textbf{Method}
& \multicolumn{2}{@{} c}{\textbf{MDINOv2-I}$\uparrow$} 
& \textbf{TIFA} $\uparrow$ 
&  \textbf{Geometric} $\uparrow$ \\
& &  Background &  Property & & \textbf{Score} \\
& & change prompt & change prompt & & \\

\midrule
\multirow{3}{*}{1-input} & Break-a-Scene~\cite{avrahami2023break}  & 0.765&0.752 & 0.823 & 0.791 \\
& Ours (1B) & 0.744 & 0.671  &  0.850 & 0.781 \\
& Ours (3B)  & 0.777 & 0.708 & 0.898  & \textbf{0.825} \\
\cdashline{1-6}
\multirow{3}{*}{3-input} & LoRA~\cite{loraimplementation} &0.795&0.776 & 0.760 & 0.774 \\
& Ours (1B) & 0.806 & 0.773 & 0.830  & 0.801 \\
& Ours (3B) & \textbf{0.822} & 0.789 & \textbf{0.863}  & \textbf{0.838} \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{ \textbf{Comparison with tuning-based methods}. Our method remains competitive against tuning-based methods, with better text alignment and comparable image alignment. }

\label{tbl:tuningbased_metrics}
\vspace{-5pt}
\end{table}


\begin{table}[!t]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{ll cccc  }
\toprule
& \textbf{Method} & \multicolumn{4}{@{} c}{\textbf{Human preference} (in $\%$)$\uparrow$}
\\
& &   \shortstack[c]{Overall\\preference}  & \shortstack[c]{Text\\alignment} &  \shortstack[c]{Image\\alignment} &  Quality \\
\midrule
\multirow{2}{*}{1-input} & Ours (1B) vs JeDi & 71.91  & 73.27 & 73.41 & 74.13 \\
& Ours (3B) vs Emu-2 & 66.74  & 70.49 & 66.88  & 64.66 \\
\cdashline{1-6}
\multirow{2}{*}{3-input} & Ours (1B) vs JeDi & 68.19 & 69.51 & 63.05  & 80.89 \\
  & Ours (3B) vs JeDi & 72.14 & 81.40 & 64.02 & 75.13  \\ %
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{ \textbf{Human preference study.} Here, we compare the pairwise preference of our method against the competing methods from \reftbl{metrics}, i.e., Emu-2~\cite{Emu2} with the same model scale and JeDi~\cite{zeng2024jedi}. The standard error for all is within $\pm 5\%$. %
}

\label{tbl:human_eval}
\vspace{-12pt}
\end{table}




\subsubsection{Qualitative Comparison}
We show sample comparisons of our method against other tuning-free methods in \reffig{results_comparison1} and \ref{fig:results_comparison2}. Our method more effectively incorporates the text prompt while keeping the object identity and image fidelity, e.g., the blue house in the background in $1^{\text{st}}$ row of \reffig{results_comparison1}. In contrast, most baseline methods either ignore the text prompt or have low object identity preservation. With $3$ reference images as input in \reffig{results_comparison2}, although JeDi~\cite{zeng2024jedi} achieves high identity preservation, it can result in reduced image quality, with inconsistency in lighting and background scene. 












\subsection{Model Ablation Study}\lblsec{model_ablation}

\myparagraph{Architecture design.} We evaluate the impact of key components of our model, specifically: (1) masked shared attention, (2) mask usage in shared attention during training, and (3) global feature injection. All the experiments are with our 3B model. \reftbl{ablation} shows that removing global feature significantly affects performance with $1$ image as input. Adding masked shared attention allows effective use of multiple reference images during inference, improving performance as we increase the number of reference images. We show qualitative samples in \reffig{model_ablation_fig} in the Appendix. 





\myparagraph{Modified guidance inference.} We compare our inference approach (\refsec{tuning_free_inference}) to guidance rescale~\cite{lin2024common} with our trained model. As shown in \reffig{sample_inf}, increasing the guidance strength in our method preserves image fidelity while incorporating the text and image conditions. Guidance rescale was also proposed to mitigate image saturation but in vanilla text-to-image generation pipeline. We also evaluate the baseline IP-Adapter Plus~\cite{ye2023ip} with our modified inference. As shown in \reftbl{ablation}, this improves its TIFA score from $0.615$ to $0.816$, with only a minor decrease in image alignment. 

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{ll cc c c}
\toprule
\multirow{3}{*} & \textbf{Method}
& \multicolumn{2}{@{} c}{\textbf{MDINOv2-I}$\uparrow$} 
& \textbf{TIFA} $\uparrow$ 
&  \textbf{Geometric} $\uparrow$ \\
& &  Background &  Property & & \textbf{Score} \\
& & change prompt & change prompt & & \\
\midrule
\multirow{6}{*}{1-input} 
& IPAdapter Plus  &0.744& 0.737 &   0.615 & 0.675 \\
& + our inference & 0.719 & 0.668 &  0.816 & 0.756 \\
\cdashline{2-6} 
& Ours & \textbf{0.777} & \textbf{0.708} & 0.902 &  \textbf{0.825} \\
& w/o mask in MSA & 0.763 & 0.693  & 0.901 & 0.817 \\
& w/o global feature &0.709&0.679 & \textbf{0.908} & 0.795 \\
& w/o MSA  & 0.766  & 0.695   & 0.901 & 0.819 \\
\cdashline{1-6} 
\multirow{3}{*}{3-input} & Ours & \textbf{0.822} & 0.789 & 0.863  & \textbf{0.838}  \\
& w/o mask in MSA & 0.804 & 0.747 & \textbf{0.865} & 0.825 \\
& w/o global feature & 0.797 & \textbf{0.792} & 0.821 & 0.807 \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{ \textbf{Model ablation}. We show the contribution of different components of our model architecture and inference method. MSA enables the effective use of multiple reference images as input, thus significantly helping with image alignment, whereas, with only one image as input, having the global feature is more crucial.
}

\label{tbl:ablation}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/inference.pdf}
    \vspace{-15pt}
    \caption{{\textbf{Inference} with ours and guidance rescale~\cite{lin2024common} technique. The caption is {\menlo A stuffed toy with a blue house in the background}. As we increase guidance, our inference better follows the text prompt while increasing the image similarity.
    }}
    \lblfig{sample_inf} 
    \vspace{-15pt}
\end{figure}


\subsection{Dataset Ablation Study}

\myparagraph{Dataset curation.} We ablate different steps of the dataset generation to analyze their respective contributions. We compute the average intra-cluster similarity using DINOv2 features, where a cluster is composed of images coupled by the same object, as well as the filtering ratio using our automatic filtering step. \reftbl{dataset_ablation} shows that MSA consistently improves intra-cluster similarity and, thereby, the filtering ratio. For rigid object generation, while feature warping minimally affects DINOv2 feature similarity, we find it beneficial in promoting multi-view consistency between the object in the images, e.g., the consistent cup colors in $1{\text{st}}$ row (left column) of \reffig{sample_images_2}. For deformable objects, providing descriptive prompts in addition to MSA proves crucial.




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/objaverse_data.pdf}
     \vspace{-15pt}
    \caption{{\textbf{Dataset generation ablation.} \textit{Top:} our synthetic training images. \textit{Middle:} removing warping reduces multi-view consistency, e.g., colors of the center cup. \textit{Bottom:} removing both warping and MSA
    further hurts visual consistency. Zoom in for details.
    }}
    \vspace{-8pt}
    \lblfig{sample_images_2} 
\end{figure}

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l lcc}
\toprule
& \textbf{Method} & \textbf{DINOv2-I}$\uparrow$ & \textbf{Filtering $\%$} $\uparrow$   \\
\midrule
\multirow{3}{*}{\shortstack[l]{Rigid\\ categories}} & Ours & \textbf{0.595} & \textbf{15.64} \\
& w/o Warping &  0.591 & 15.10 \\
& w/o MSA  &  0.534 & 14.44 \\
\cdashline{1-4}
\multirow{3}{*}{\shortstack[l]{Deformable\\ categories} } & Ours & \textbf{0.700} & \textbf{39.84}  \\
& w/o MSA & 0.626 & 36.44 \\
& w/o Detailed description & 0.564 & 27.32 \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{ \textbf{Dataset curation ablation.} MSA consistently enhances intra-cluster DINOv2-I similarity. While warping does not impact DINOv2-I scores, its qualitative benefits are shown in \reffig{sample_images_2}. 
}

\label{tbl:dataset_ablation}
\vspace{-12pt}
\end{table}















\reftbl{ablation} shows the effectiveness of our training data.
Our model without MSA is comparable to an IP-Adapter Plus model trained on our $95K$ dataset. A direct comparison between the 1-input samples generated by our model without MSA (row 6) and IP-Adapter Plus using our inference (row 2) highlights the improvement due to our 95K training set, while using similar models and inference protocols.

In \refapp{appendix_result} and \ref{sec:appendix_ablation}, we show more quantitative and qualitative results, comparisons on other evaluation datasets~\cite{kumari2023multi}, and effect of dataset size vs. category diversity.












