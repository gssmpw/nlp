\renewcommand{\thefootnote}{\arabic{footnote}}
\clearpage
\noindent{\Large\bf Appendix}
\vspace{5pt}


In \refapp{appendix_result} and \ref{sec:appendix_ablation}, we show more qualitative samples of our method, its comparison to the baselines, and more ablation studies. Then, in \refapp{details}, we provide implementation details related to our dataset generation, model training, and inference. Finally, in \refapp{limitation}, we discuss our work's limitations and societal impact. 




\section{Additional Comparison with Prior Works}\lblsec{appendix_result} 


\myparagraph{CustomConcept101 Benchmark.}
Though DreamBooth~\cite{ruiz2022dreambooth} is a widely used evaluation dataset, CustomConcept101~\cite{kumari2023multi} is more diverse with $101$ unique concepts. Here, we also compare our model (3B) with open-source baseline models of similar scale, i.e., Emu-2~\cite{Emu2} and IP-Adapter~\cite{ye2023ip}, on this dataset. As shown in \reftbl{customconcept101}, our method performs better in identity preservation compared to both baselines while also yielding higher text alignment as indicated by CLIPScore~\cite{radford2021learning} and TIFA~\cite{hu2023tifa} metrics. We show a qualitative comparison in \reffig{customconcept_compare} and more samples in \reffig{customconcept_sample}. 

\myparagraph{Additional Qualitative Comparison.} 
Figures~\ref{fig:results_comparison_1ref_1} - \ref{fig:results_comparison_3ref} show more visual comparison against the tuning-free baselines with single and three reference images as input, respectively. Similarly, \reffig{tuning_based_samples1} and \ref{fig:tuning_based_samples2} show sample comparisons of our method with tuning-based approaches Break-a-Scene~\cite{avrahami2023break} and LoRA~\cite{hu2021lora,loraimplementation}. When compared to tuning-based approaches, our method performs on par in identity preservation while better following the text prompt. 









\section{Ablation Study}\lblsec{appendix_ablation}

\myparagraph{Model ablation.} 
Here, we show a qualitative comparison of our model ablation experiments reported in \refsec{model_ablation}, i.e., when training w/o global features and w/o masks in MSA.  As shown in \reffig{model_ablation_fig}, removing global features significantly degrades performance, especially when the text prompt is ambiguous, e.g., toy instead of a rubber duck. Similarly, MSA helps the model capture fine details, e.g., the specific color pattern of the shoe in the last row of \reffig{model_ablation_fig}. 


\noindent We also perform a more detailed analysis of our proposed inference technique here. \reftbl{inference_supp} compares our technique with the default inference of Brooks~\etal ~\cite{brooks2023instructpix2pix} and guidance rescale~\cite{lin2024common} on the IP-Adapter Plus~\cite{ye2023ip} baseline. For all, we use the same text and image guidance scale as ours. The default inference technique of Brooks~\etal ~\cite{brooks2023instructpix2pix} and adding guidance rescale to it do not affect the final performance significantly. Meanwhile, with our normalization technique, the text alignment improves significantly with a comparatively minor drop in image alignment. The sample comparisons in \reffig{inference_supp} also show the same trend. 


\begin{table}[!t]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l cc cc c}
\toprule
\multirow{3}{*} \textbf{Method}
& \multicolumn{2}{@{} c}{\textbf{MDINOv2-I}$\uparrow$} 
& \textbf{CLIPScore} $\uparrow$ 
& \textbf{TIFA} $\uparrow$ 
&  \textbf{Geometric} $\uparrow$ \\
&  Background &  Property & & & \textbf{Score} \\
& change prompt & change prompt & & \\
\midrule
\textbf{1-input} & & \\
IPAdapter Plus~\cite{ye2023ip}   & 0.618 & 0.626 &  0.261 & 0.569 & 0.595 \\
Emu-2~\cite{Emu2}  & 0.604 & 0.619 & 0.284 &  0.701 & 0.655 \\
Ours (3B)  &  0.645 & 0.609 & \textbf{0.315} & \textbf{0.809} & \textbf{0.712} \\
\cdashline{1-6} 
\textbf{3-input} \\
Ours (3B) & \textbf{0.689}  & \textbf{0.666} & 0.304 & 0.749 & \textbf{0.712} \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{ \textbf{Results on CustomConcept101~\cite{kumari2023multi}}. Our method outperforms both Emu-2~\cite{Emu2} and IP-Adapter~\cite{ye2023ip} on the overall Geometric Score~\cite{yan2023motion} metric while being on par regarding image alignment. The Geometric Score is computed by taking the geometric mean of MDINOv2-I and TIFA scores, both of which are in the 0-1 range.  
}

\label{tbl:customconcept101}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/break_a_scene_compare.pdf}
    \vspace{-20pt}
    \caption{{\textbf{Comparison with Break-a-Scene~\cite{avrahami2023break} given 1-input image.} Break-a-Scene can sometimes ignore the text prompt, e.g., {\menlo A dog in police outfit } in the last row. Our method follows the text prompt better while having on-par image alignment.
    }}
    \lblfig{tuning_based_samples1} 
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/lora_compare.pdf}
    \vspace{-20pt}
    \caption{{\textbf{Comparison with LoRA~\cite{loraimplementation} given 3-input image.} Our method works similarly to LoRA regarding image alignment while being better on text alignment, e.g., the snow texture in the first row. Please zoom in for details.
    }}
    \lblfig{tuning_based_samples2} 
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/model_ablation.pdf}
    \vspace{-20pt}
    \caption{\textbf{Qualitative samples of ablation studies}. \textit{$1^{\text{st}}$ column}: Not training on rigid object categories leads to worse shape and identity preservation on rigid category objects during inference. \textit{$2^{\text{nd}}$ column}: Not using the global feature hurts the performance overall, with the most visible effect on categories with ambiguous text description, e.g., rubber duck as a toy in $1^{\text{st}}$ row. \textit{ $3^{\text{rd}}$column}: Not having MSA leads to fine details being missed, e.g., the specific color pattern of the shoes in $3^{\text{rd}}$ row. Please zoom in for details.
    }
    \vspace{-10pt}
    \lblfig{model_ablation_fig} 
\end{figure}

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l cc cc c}
\toprule
\multirow{3}{*} \textbf{Method}
& \multicolumn{2}{@{} c}{\textbf{MDINOv2-I}$\uparrow$} 
& \textbf{CLIPScore} $\uparrow$ 
& \textbf{TIFA} $\uparrow$ 
&  \textbf{Geometric} $\uparrow$ \\
&  Background &  Property & & & \textbf{Score} \\
& change prompt & change prompt & & \\
\midrule
IPAdapter Plus   & \textbf{0.744} & \textbf{0.737} & 0.270 & 0.615 & 0.675 \\
+ Guidance rescale~\cite{lin2024common} (0.6)  &0.722 & 0.699 & 0.276 & 0.707 & 0.710 \\
+ Vanilla Img + Text~\cite{brooks2023instructpix2pix} & 0.722 &  0.711 & 0.270 & 0.681 & 0.699 \\
+ Our inference & 0.719 & 0.668 & \textbf{0.298} &  \textbf{0.816} & \textbf{0.756} \\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{ \textbf{Our inference}. We compare our inference technique with vanilla image and text guidance technique~\cite{brooks2023instructpix2pix} as well as guidance rescale~\cite{lin2024common} with the same inference hyperparameters across all. 
}

\label{tbl:inference_supp}
\vspace{-12pt}
\end{table}


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/inference_supp.pdf}
    \vspace{-20pt}
    \caption{{\textbf{Qualitative comparison of our inference.} Our modified inference technique helps increase text alignment while minimally affecting the object identity. In comparison, the inference technique of Brooks~\etal~\cite{brooks2023instructpix2pix} or additional guidance rescale~\cite{lin2024common} has less effect on the final outputs. Please zoom in for details.
    }}
    \lblfig{inference_supp} 
\end{figure}





\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/dataset_size.pdf}
    \vspace{-15pt}
    \caption{{\textbf{Dataset size.} Increasing training samples from $100$ to $1K$, $10K$, and $95K$ yields gradual improvements in object identity preservation, especially in finer details. As our model is initialized from IP-Adapter Plus~\cite{ye2023ip}, it can already generate a similar-looking object with as few as $1K$ samples. Zoom in for details.
    }}
    \lblfig{dataset_size} 
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/data_cat_plot.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Dataset ablation.} We plot the MDINOv2-I metric with increased sample size and category diversity. Given the same sample size of 1K objects, increasing category diversity from $3$ to $16$ and $200$ gradually improves image alignment. Regarding sample size, we observe that the performance plateaus, with similar performance on $10K$ as $95K$ samples based on both quantitative metrics, as shown here, and the human preference study. The above metrics are calculated with $3$ images as input.
    }}
    \lblfig{data_cat_plot} 
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.88\linewidth]{images/dataset_ablation_cat.pdf}
    \vspace{-15pt}
    \caption{{\textbf{Effect of dataset category diversity on performance.} As we increase the number of unique categories from  $1$ to $3$ to $16$ and $200$ (with a fixed sample size of 1K), performance improves with the model capturing finer details of the object, e.g., the unique pattern in front of the toy car in $1^{\text{st}}$ row or the frills of the boot in $4^{\text{rth}}$ row. \textbf{Please zoom in for details.}
    }}
    \lblfig{dataset_ablation}
    \vspace{-10pt}
\end{figure*}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.88\linewidth]{images/3dguidance.pdf}
    \vspace{-8pt}
    \caption{\textbf{Rigid object generation w/ vs. w/o 3D Asset guidance.} We compare our final rigid object generation results with that of removing 3D asset guidance and only using MSA. Removing depth and warping guidance from the dataset generation pipeline reduces multi-view and shape consistency. 
    }
    \lblfig{dataset_ablation2}
\end{figure*}

\myparagraph{Dataset diversity and size.} 
We examine the impact of training dataset size vs. category diversity on the final model performance by creating various subsets of the data by adjusting these two factors and training our model on each subset. \reffig{dataset_size} shows qualitative samples from models trained on progressively larger datasets, showing that increased sample size enables capturing fine object details. However, we observe that reducing the category diversity has a larger impact than the sample size, as shown in \reffig{data_cat_plot}. We plot the image alignment MDINOv2-I metric for different subsets, and given the same sample size, e.g., 1K samples, higher category diversity leads to better performance. Similarly, given the same category diversity, performance quickly plateaus with sample size, with similar performance when trained on $10K$ vs. $95K$ samples. We also perform a human preference study for our final model vs. the models trained on $1K$ and $10K$ samples, with the preference for our model being $62.16\%$ and $50.59\%$, respectively. 




\myparagraph{Rigid object generation.} \reffig{dataset_ablation2} here shows that only having MSA for rigid objects fails to maintain the same shape and multi-view consistency across different views. Whereas guiding the generation using 3D assets from datasets like Objaverse~\cite{deitke2023objaverse} leads to more consistent objects. We also analyze the importance of including such categories for training. \reffig{model_ablation_fig} shows qualitative samples from the model trained only on deformable object categories, consisting of pet animals, which leads to degraded performance on rigid objects, e.g., the duck toy or sunglasses and shoes. The average MDINO-v2-I metric on the DreamBooth evaluation dataset also reduces from $0.81$ to $0.78$.









\section{Implementation details}
\lblsec{details}

\subsection{Dataset Generation}\lblsec{details_dataset_gen}

\myparagraph{LLM instruction details.}
To get a set of prompts in our dataset generation, we use Instruction-tuned LLama3~\cite{dubey2024llama}. The input instruction to the LLM always consists of the prompt shown below, which is modified from Esser~\etal~\cite{esser2024scaling}: 
\begin{tcolorbox}
\textbf{Role}: system, \textbf{Content}: You are a language model expert in suggesting image captions for different object categories.\\
\textbf{Role}: user, \textbf{Content}: suggest ten captions for images of a [\textcolor{blue}{object description/ category}]. The caption should provide a [\textcolor{blue}{TASK}]. DO NOT add any unnecessary adjectives or emotional words in the caption. Please keep the caption factual and terse but complete. DO NOT add any unnecessary speculation about the things that are not part of the image, such as ``the image is inspiring to viewers'' or ``seeing this makes you feel joy''. DO NOT add things such as ``creates a unique and entertaining visual'', as these descriptions are interpretations and not a part of the image itself. The description should be purely factual, with no subjective speculation.
\end{tcolorbox}

Where in the case of rigid object generation, we provide the \textcolor{blue}{object description} from CAP3D~\cite{luo2024scalable} and the \textcolor{blue}{TASK} is ``a description of the background''. We also provide two sample descriptions, as shown below: 
\begin{tcolorbox}
Follow this guidance for the captions: 
\begin{enumerate}
    \item Generate captions of [\textcolor{blue}{object description}] in different backgrounds and scenes. 
    \item Generate captions of [\textcolor{blue}{object description}] with another object in the scene. 
\end{enumerate}

Example captions for ``White plastic bottle'' are:
\begin{enumerate}
    \item A white plastic bottle on a roadside cobblestone with stone bricks.
    \item A white plastic bottle is placed next to a steaming cup of coffee on a polished wooden table.
\end{enumerate}

Example captions for a ``blue truck'' are:
\begin{enumerate}
    \item A blue tank in a military storage facility with metal walls.
    \item A blue tank on a desert battlefield ground, with palm trees in the background.
\end{enumerate}

\end{tcolorbox}

In the case of deformable object generation, we prompt the LLM once, with the \textcolor{blue}{category} name, e.g., cat, and \textcolor{blue}{TASK} as ``detailed visual information of the category, including color and subspecies''. We append the below instruction as well to the LLM: 

\begin{tcolorbox}
Example caption descriptions for the category ``cat'':
\begin{enumerate}
    \item  The Siamese cat has blue almond-shaped eyes and cream-colored fur with dark chocolate points on the ears, face, paws, and tail. 
    \item The white fluffy Maine Coon cat with a long and bushy tail spread out beside it, and its thick fur has a mix of brown, black, and white stripes. 
    \item The Bengal cat with a marbled coat features a pattern of vivid orange and black spots.
\end{enumerate}
\end{tcolorbox}

We prompt the LLM again with the same \textcolor{blue}{category} name and \textcolor{blue}{TASK} as `` a description of the background''. We append the below instruction as well to the LLM: 
 \begin{tcolorbox}
Follow this guidance for the captions: 
\begin{enumerate}
    \item Generate captions of [\textcolor{blue}{category}] in different backgrounds and scenes. 
    \item Generate captions of [\textcolor{blue}{category}] with another object in the scene. 
    \item Generate captions of [\textcolor{blue}{category}] with different stylistic representations. 
\end{enumerate}

Example captions for the category ``cat'' are:
\begin{enumerate}
    \item Photo of a cat playing in a garden. The garden is filled with wildflowers. 
    \item A cat is sitting beside a book in a library. 
    \item Painting of a cat in watercolor style.
\end{enumerate}
\end{tcolorbox}


\newpage
\myparagraph{Masked Shared Attention (MSA).} When performing MSA in DiT-based text-to-image models, we modify the rotational positional encoding~\cite{su2024roformer} to be $N H \times W$ image resolution for generating the $N$ images of $H \times W$ resolution. Further, during sampling, each image attends to everything in the other image at the first time step, and the mask is then used in subsequent timesteps. The final training dataset is filtered with a mean intra-cluster DINOv2 similarity threshold of $0.7$ and aesthetics score above $6.0$ out of $10.0$. More specific details related to rigid and deformable object generation are provided below. 

\myparagraph{Rigid object generation.} We select approximately $75K$ assets from the Objaverse~\cite{deitke2023objaverse}, which is a subset of LVIS and high-quality assets shared by Tang~\etal~\cite{tang2025lgm}. For the depth images, we used the Cap3D dataset~\cite{luo2024scalable}, which renders the assets from different camera poses. We select the $3$ views with a minimum $10\%$ pair-wise overlap in rendered images and pre-calculate the cross-view pixel correspondence to be used in the dataset generation pipeline. We use the ground truth rendered depth images as input to a depth-conditioned FLUX model~\cite{xflux}. During dataset generation, we also use negative prompts, such as {\menlo 3d render, low resolution, blurry, cartoon}. The feature warping is performed only on the first $20\%$ of denoising timesteps. We perform sampling with $50$ steps at $512$ resolution, with text guidance of $3.5$ and depth control strength of $0.8$.  

\myparagraph{Deformable object generation.} 
In the case of deformable objects, during Masked Shared Attention (MSA), we compute the mask of the foreground object region via text cross-attention~\cite{hertz2022prompt}, which is updated at every diffusion timestep. Additionally, once generated, we remove the object description from the prompt in the dataset. We perform sampling with $50$ timesteps and standard text guidance of $3.5$ at $1$K resolution. 



\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/practice_test.pdf}
    \vspace{-20pt}
    \caption{{\textbf{Sample practice test for human preference study.} We show $3$ practice questions to each participant that test their ability to select the images based on the three criteria that we care about, i.e., identity preservation or image alignment, text alignment, and overall quality. 
    }}
    \lblfig{practice_fig} 
    \vspace{-10pt}
\end{figure*}


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/limitation.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Limitation.} Our method can still struggle with detailed textures, e.g., the cartoon-like dog design on the bag and its flaps in the first row or the stickers on the red backpack in the second row. 
    }}
    \lblfig{limitation_fig} 
    \vspace{-10pt}
\end{figure}

\subsection{Our Method}\lblsec{our_method_details}
\myparagraph{Training.}
We train our model with a batch size of $32$, learning rate $1\times 10^{-5}$ for $20K$ iterations. We sample $3$ images of each object during training, with $2$ as references and $1$ image as target. For objects with only $2$ images, we horizontally flip one image as the third image in the set.

\myparagraph{Inference.}
During inference, we use the text-guidance scale of $7.5$ and an adaptive image-guidance scale, $\lambda_{I}$ in \refeq{inference}, starting from $8.0$ for background change prompts and $6.0$ for property/shape change prompts and linearly increasing it by $5.0$ during the $50$ sampling steps. For the global feature, the IP-Adapter scale is always set to its default value of $0.6$ during inference. For sampling, we use the Euler Discrete scheduler~\cite{karras2022elucidating}. Our inference time for sampling an image is $19$ seconds compared to $3$ seconds for the base pretrained model in mixed-precision $bfloat16$ on H100 GPU. The overhead is because of the longer sequence length in the masked shared attention combined with making the forward call to the model twice at every step to extract reference features. 





\subsection{Baselines}
Here, we mention the implementation details of baseline methods. For baselines with recommended hyperparameters, we always followed those while keeping the sampling step consistent across all to $50$ and the text guidance scale of $7.5$ if not mentioned. 

\myparagraph{Kosmos-G~\cite{pan2023kosmos}.} We follow their open-source code to sample images on the DreamBooth evaluation dataset.

\myparagraph{BLIP Diffusion~\cite{li2023blip}.} According to the recommended technique, we modify each prompt to be an (image, category name, instruction) tuple where instruction is modified from the input prompt, e.g., ``{\menlo toy in a junle} ''$\rightarrow$ ``{\menlo in a jungle}'' or ``{\menlo a red toy}'' $\rightarrow$ ``{\menlo make it red}''. Additionally, we use the negative prompts provided in their open-source code. 

\myparagraph{IP-Adapter~\cite{ye2023ip}.} In the case of IP-Adapter~\cite{ye2023ip}, we use the IP-Adapter Plus with a U-Net-based diffusion model of the same parameter scale as Ours (3B). We use the recommended $0.6$ IP-Adapter scale.

\myparagraph{MoMA~\cite{song2024moma}.} We use their open-source code with the maximum strength parameter of $1$ for increased object identity preservation. 

\myparagraph{JeDi~\cite{zeng2024jedi}.} We use the generated images on the DreamBooth evaluation dataset shared by the authors. 

\myparagraph{Emu-2~\cite{Emu2}}. We use their open-source code with the recommended guidance of $3$. Additionally, as mentioned in their paper, we modify each prompt to be an (image, instruction) tuple where instruction is modified from the input prompt, e.g., ``{\menlo toy in a junle}'' $\rightarrow$ ``{\menlo in a jungle}'' or ``{\menlo a red toy}'' $\rightarrow$ ``{\menlo make it red}''.

\myparagraph{Break-a-Scene~\cite{avrahami2023break}.} We use the open-source code of Break-a-Scene and learn $2$ assets, one corresponding to the object and another for the background. During inference, we only use the learned asset for the object.

\myparagraph{LoRA~\cite{hu2021lora,loraimplementation}.} We follow the hyperparameters from the HuggingFace implementation~\cite{loraimplementation} and finetune a U-Net-based diffusion model of the same parameter scale as Ours (3B). Additionally, we enable class regularization with generated images to prevent overfitting, as suggested in DreamBooth~\cite{ruiz2022dreambooth}. 

\subsection{Evaluation}~\lblsec{appendix_eval_details}

\myparagraph{MDINOv2-I metric.} To compute this, we first detect and segment the object. For detection, we use Detic~\cite{zhou2022detecting} and Grounding DINO~\cite{liu2023grounding} in case Detic fails. For object detection, we modify the category names to be more descriptive, e.g., ``{\menlo rubber duck}'' instead of ``{\menlo toy}'',  ``{\menlo white boot}'' instead of ``{\menlo boot}'', or  ``{\menlo toy car}'' instead of ``{\menlo toy}''. We then use the detected bounding box as input to SAM~\cite{kirillov2023segment} for segmentation. Once segmented, we mask the background and crop the image around the mask for both reference and generated images. Additionally, for reference images, we manually correct the predicted mask using the SAM interactive tool to be ground truth. 


\myparagraph{Human preference study details.}
For each human preference study, we randomly sample $750$ images, with one image per object-prompt combination. We use Amazon Mechanical Turk for our study. During the study, participants first complete a practice test consisting of three questions that test their ability to select an obvious ground truth image based on alignment to the text prompt, reference object similarity, and image quality. A sample set of practice questions is shown in \reffig{practice_fig}. The study has a similar setup, except the two images are now from ours and a baseline method. We only consider responses from participants who answered the practice questions correctly.  

\section{Limitations and Societal Impact}
\lblsec{limitation}
Here, we provide examples to show the limitations of our model and discuss its broader societal implications. One notable limitation of our method is that it can still fail to capture fine texture details as shown in \reffig{limitation_fig}. This can be attributed to our dataset often having simple, uniform textures. Despite this, our method improves upon current leading tuning-free customization methods by proposing advancements in dataset collection and training architecture. 

We hope this will empower users in their creative endeavors to generate ever-new compositions of concepts from their personal lives. However, the potential risks of generative models, such as creating deepfakes or misleading content, extend to our method as well. Possible ways to mitigate such risks are technologies for watermarking~\cite{fernandez2023stable} and reliable detection of generated images~\cite{wang2020cnn,corvi2022detection,cazenavette2024fakeinversion}.

\clearpage

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/supp_1ref_1.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Results.} We compare our method qualitatively against other leading tuning-free baselines with a single reference image as input. We can successfully incorporate the text prompt while preserving the object identity similar to or higher than the baseline methods. We pick the best out of $4$ images for all methods. In comparison, Emu-2 and JeDi often have low fidelity, and IP-Adapter Plus overfits on the input image. MoMa, though it has a reasonable performance on pet animals like dogs and cats, fails on more unique objects like the shoe in $3^{\text{rd}}$ row. \textbf{Please zoom in for details.}
    }}
    \lblfig{results_comparison_1ref_1}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/supp_1ref_2.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Results.} We compare our method qualitatively against other leading tuning-free baselines with a single reference image as input. We can successfully incorporate the text prompt while preserving the object identity similar to or higher than the baseline methods. We pick the best out of $4$ images for all methods. \textbf{Please zoom in for details.}
    }}
    \lblfig{results_comparison_1ref_2}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/supp_3ref_1.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Results with $3$ input images.} We compare our method qualitatively against JeDi~\cite{zeng2024jedi}, which can also take multiple images as input. Compared to JeDi, our method more coherently incorporates the text prompt with higher image fidelity while being similar in performance on image alignment, e.g., the missing firefighter outfit in $2^{\text{nd}}$ row or low fidelity sunglasses in $4^{\text{rth}}$ row. We pick the best out of $4$ images for all methods. \textbf{Please zoom in for details.}
    }}
    \lblfig{results_comparison_3ref}
    \vspace{-10pt}
\end{figure*}



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{images/customconcept101_compare.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Qualitative comparison on CustomConcept101~\cite{kumari2023multi} dataset with $1$ input image.} We compare our method against baselines of similar scale models, i.e., Emu-2~\cite{Emu2} and IP-Adapter Plus~\cite{ye2023ip}. We observe that both baselines can sometimes overfit on the input image. Whereas our method can better incorporate the text prompt while respecting the object's identity. \textbf{Please zoom in for details.}
    }}
    \lblfig{customconcept_compare}
    \vspace{-10pt}
\end{figure*}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/customconcept_samples.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Samples on CustomConcept101~\cite{kumari2023multi} dataset with $3$ input images.} We show more samples of our method given $3$ reference images of the object.
    }}
    \lblfig{customconcept_sample}
    \vspace{-10pt}
\end{figure*}
