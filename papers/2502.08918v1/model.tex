\section{\ourmethod}

We proposed a model named \ourmethod to bridge the gap between pre-training and downstream tasks on heterogeneous graphs. Our model has three main modules, including pre-training, cluster prompt, and meta-path template. The overall framework is shown in Figure \ref{figure::framework}.

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{figure/model.pdf}
	\caption{Overall framework of \ourmethod. The pre-training module learns structural and semantic features of heterogeneous graphs within a contrastive framework. The prompt module introduces cluster prompts to learn clustering features, also trained within the same contrastive framework. The template module integrates prompts into meta-paths to enhance prompt learning with high-order semantics. Parameters are transferred from the pre-training module to the prompt module and template module.}
	\label{figure::framework}
\end{figure}

\subsection{Pre-training on Heterogeneous Graph}\label{sec::pretrain}

Heterogeneous graphs contain not only structural information of node connections but also semantic information of meta-paths. To capture both structural and semantic features, we propose a self-supervised heterogeneous graph learning framework that separately acquires these features and fuses them using contrastive learning.

Due to the heterogeneity of the graph, the features of different types of nodes are distributed in different spaces. For easier message-passing between different types of nodes, we design type-specific transformation matrix $\bm M_{\phi}$ to project features into the common space:
\begin{equation}\label{equ::transformation}
    \bm h_i = \bm M_\phi \bm x_i,
\end{equation}
where $\bm x_i$ and $\bm h_i$ are the original and projected features of node $v_i$, respectively.

Assume that the target nodes of type $T_\tau$ are connected to neighbors of types $\{T_1,T_2,\cdots,T_K\} \in \mathcal{T}$. To aggregate node features from different types of neighbors, we present graph convolutional networks with the following type-wise propagation rule:
\begin{equation}
    \bm H_\tau^k = \sigma \left(\tilde{\bm A}_k \bm H_k \bm W_k\right),
\end{equation}
where $\tilde{\bm A}_k \in \mathbb{R}^{N_\tau \times N_k}$ is the normalized adjacency matrix from the given type $T_k$ to the target type $T_\tau$, $N_\tau$ and $N_k$ are the number of nodes of the corresponding type, $\bm H_k$ represents the set of node features of type $T_k$, and $\bm W_k$ is a type-specific trainable weight matrix. After aggregating features from different types, we employ the attention mechanism to measure the contribution of each type of nodes to the target nodes:
\begin{equation}\label{equ::structure}
    \bm H_\tau^{\mathcal{G}} = \sum_{k=1}^{K} \bm\alpha_k \bm H_\tau^k,
\end{equation}
where $\bm\alpha_k$ is the attention weight of type $T_k$:
\begin{equation}
    \alpha_k = \frac{\exp(\text{LeakyReLU}(\bm a^\top [\bm h_i||\bm h_i^k]))}{\sum_{l \in \mathcal{T}} \exp(\text{LeakyReLU}(\bm a^\top [\bm h_i||\bm h_i^l]))}.
\end{equation}
Here, $\bm a \in \mathbb{R}^{2d \times 1}$ is a learnable attention vector and $||$ denotes the concatenate operation.

In addition to learning structural information, we also utilize meta-paths to capture high-order semantic information of heterogeneous graphs. Given a set of meta-paths $\{\mathcal{P}_1,\mathcal{P}_2,\cdots,\mathcal{P}_M\}$, we can get a meta-path based adjacency matrix $\bm A_\tau \in \mathbb{R}^{N_\tau \times N_\tau}$ for the target node type. For neighbors that exist in multiple meta-paths (i.e., $v_i,v_j \in \mathcal{P}_1,\mathcal{P}_2$), we set the element of adjacency matrix to the corresponding count (i.e., $\bm A_{ij}=2$). Then we utilize a GCN layer to obtain the semantic feature of nodes:
\begin{equation}\label{equ::semantics}
    \bm H_\tau^{\mathcal{P}} = \sigma \left(\tilde{\bm A}_\tau \bm H_\tau \bm W_\tau\right),
\end{equation}
where $\tilde{\bm A}_\tau$ is the normalized adjacency matrix, $\bm W_\tau$ is a trainable weight matrix, and $\sigma$ is the sigmoid activation function.

After obtaining node representations of two views $\bm H_\tau^\mathcal{G}$ and $\bm H_\tau^\mathcal{P}$, we use a shared projection head $g(\cdot)$ to map these representations into the contrastive space:
\begin{equation}\label{equ::project}
	\bm Z = g(\bm H), \; \bm H \in \{\bm H_\tau^\mathcal{G},\bm H_\tau^\mathcal{P}\},
\end{equation}
where $g(\cdot)$ can be fully connected layers with activation functions. A contrastive loss $\mathcal{L}_{pre}$ is applied to pre-train the parameters by maximizing the agreement between $\bm Z^{\mathcal{G}}$ and $\bm Z^{\mathcal{P}}$.
\begin{equation}\label{equ::pretrain}
    \mathcal{L}_{pre} = -\sum_{i=1} \log \frac{\exp(\text{sim}(\bm z_i^{\mathcal{G}},\bm z_i^{\mathcal{P}}))}{\sum_{j=1}\exp(\text{sim}(\bm z_i^{\mathcal{G}},\bm z_j^{\mathcal{P}}))},
\end{equation}
where $z_i^{\mathcal{G}} \in \bm Z^{\mathcal{G}}$ and $\bm z_i^{\mathcal{P}} \in \bm Z^{\mathcal{P}}$, and $\text{sim}(\cdot,\cdot)$ is a similarity function (e.g., cosine similarity or Minkowski distance).

\subsection{Cluster Prompt}\label{sec::prompt}

The prompt aims to guide the pre-trained model to understand and predict the downstream task. In natural language processing, prompts are mostly in the form of cloze and prefix. Such prompts work well because words can naturally constitute phrases or sentences with suggestive semantics. For graphs, there are fixed relations and structures between nodes, the semantics of which cannot be expressed easily. Instead, existing homogeneous graph prompting methods \cite{GPPT,All} introduce learnable tokens to build prompts automatically. Nevertheless, this is infeasible for heterogeneous graphs due to their multiple node/edge types and complex structure.

To bridge the gap between pre-training and downstream tasks, we consider learning the node class based on structural clustering. As shown in Figure \ref{figure::prompt}, each cluster is treated as a learnable prompt token and fully connected to the target node with a trainable adjacency matrix. Thus, the node classification task can be viewed as a link prediction task between the target node and the prompt token.

In line with the structure of the heterogeneous graph, the prompt tokens are assigned a new node type $T_\rho$, and the type set is updated as $\mathcal{T} \leftarrow \mathcal{T} \cup \{T_\rho\}$. The clustering feature of target nodes is calculated as: 
\begin{equation}\label{equ::cluster}
	\bm H_\tau^\rho = \tilde{\bm A}_\rho \bm H_\rho,
\end{equation}
where $\bm H_\rho$ is the representation of prompt tokens and $\bm A_\rho \in \mathbb{R}^{N^\tau*N^\rho}$ is the adjacency matrix between prompt tokens and target nodes.

Consistent with the pre-training stage, we map the clustering feature into the contrastive space via the trained projection head $g(\cdot)$ and learn representations for the prompt tokens using the contrastive loss:
\begin{equation}
	\bm Z^{\rho} = g(\bm H_\tau^\rho),
\end{equation}
\begin{equation}\label{equ::prompt}
	\mathcal{L}_1 = -\sum_{i=1} \log \frac{\exp(\text{sim}(\bm z_i,\bm z_i^\rho))}{\sum_{j=1}\exp(\text{sim}(\bm z_i,\bm z_j^\rho))},
\end{equation}
where $\bm Z \in \{\bm Z^\mathcal{G},\bm Z^\mathcal{P}\}$ is the pre-trained projection of target nodes.

Moreover, in order to guarantee that the clusters are separated from each other, we utilize orthogonal constraint \cite{GPPT} to initialize and continuously learn prompt tokens:
\begin{equation}\label{equ::orthogonal}
    \mathcal{L}_0 = ||\bm H_\rho \bm H_\rho^\top - \bm I||_2^2,
\end{equation}
where $\bm I$ is the identity matrix.

\subsection{Meta-path Template}\label{sec::template}

In addition to direct neighbor relationships, meta-paths describe high-order semantic relationships. To further enhance node clustering with semantic information, we design a meta-path prompt template with an adjacency-guided discriminator. Specifically, given a meta-path ``XXTXX'' with the target node ``T'', we duplicate ``T'' and insert the prompt token ``P'' to construct the prompt template ``XX\underline{TPT}XX'', which also conforms to the original meta-path pattern. For instance, in a citation network, the meta-path ``ATA'' (i.e., author-paper-author, where paper is the target node type) represents the semantics of co-authorship. In order for the prompt token to understand this semantics, we generate an extended meta-path ``A\underline{TPT}A'' based on the prompt template.

After constructing the meta-path template, we introduce a discriminator to measure the correctness of generated meta-paths. Considering the specific connection pattern in meta-paths, we employ a one-dimensional convolutional neural network (Conv1d) to capture path semantics since it has a strong capability to process sequential data. A multilayer perceptron (MLP) with a sigmoid function serves as a probability distribution function to calculate the validity probability of the generated meta-paths:
\begin{equation}\label{equ::discriminator}
     \mathcal{D}(\mathcal{P}) = \sigma(\text{MLP}(\text{Conv1d}(\mathcal{P}))).
\end{equation}

For an original meta-path containing the target node, there are $N^\rho$ generated template paths to be discriminated, where $N^\rho$ is the number of prompt tokens. Here, we introduce an adjacency-based sampling method that measures the impact coefficient of different template paths. Specifically, the prompt token with the highest connection probability to the target node is regarded as the ground truth and the others are the negative samples. The loss function is given as:
\begin{equation}\label{equ::template}
    \mathcal{L}_2 = - \left( \sum_{\mathcal{P}_+} q_{\mathcal{P}_+} \log\mathcal{D}(\mathcal{P}_+) + \sum_{\mathcal{P}_-} q_{\mathcal{P}_-} \log(1-\mathcal{D}(\mathcal{P}_-)) \right),
\end{equation}
where $\mathcal{P}^+$ and $\mathcal{P}^-$ are positive and negative template paths, and the coefficient $q_{\mathcal{P}}$ is the connection probability between the prompt token and the target node in terms of the normalized adjacency matrix $\tilde{\bm A}_p$. By considering the impact of different paths, we train the prompt token in a self-adversarial manner, which improves the robustness and effectiveness of the model.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figure/2.pdf}
%     \caption{Meta-path template. For example, ``627'' is an original meta-path and ``62827'' is an extended meta-path of that.}
%     \vspace{-2mm}
%     \label{figure::template}
% \end{figure}

\subsection{Overall Learning Process}

Our prompting method can work under zero-shot or few-shot settings. In the pre-training stage, we learn node representations and model parameters using a contrastive loss. In the prompting stage, we fix the pre-trained parameters and optimize the prompt tokens $\bm H_\rho$ and the adjacency matrix $\bm A_\rho$. The loss function of zero-shot learning is given as:
\begin{equation}\label{equ::overall}
	\mathcal{L}_{prompt} = \alpha \mathcal{L}_1 + (1-\alpha) \mathcal{L}_2 + \beta \mathcal{L}_0,
\end{equation}
where $\alpha,\beta$ are hyper-parameters.

With regard to few-shot learning, the downstream tasks are given limited labels. We minimize the distance between the target node $v_i$ and its ground truth label $y_i$ while maximizing the distance between negative pairs:
\begin{equation}
	\mathcal{L}_{label} = \sum_i \left( d(\bm h_i^\tau,\bm h_{y_i}^\rho) - \mathbb{E}_{\{\phi(y) = T_\rho, y \neq y_i\}} d(\bm h_i^\tau,\bm h_{y}^\rho) \right),
\end{equation}
where $\bm h_i^\tau \in \bm H_\tau$ is the representation of the target node, $\bm h_{y_i}^\rho \in \bm H_\rho$ is the representation of the corresponding prompt token, and $d(\cdot,\cdot)$ is the Euclidean distance. Finally, the loss function of few-shot learning is given as: $\mathcal{L}_{few} = \mathcal{L}_{prompt} + \gamma \mathcal{L}_{label}$, where $\gamma$ is a hyper-parameter.

% \begin{algorithm}
% 	\caption{Overall Learning Process}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% 	\renewcommand{\algorithmicensure}{\textbf{Output:}}
% 	\begin{algorithmic}[1]
% 		\REQUIRE heterogeneous graph $\mathcal{G} = \{\mathcal{V}, \mathcal{E}, \mathcal{T}, \mathcal{R}\}$, node features $\bm X$, meta-path set $\{\mathcal{P}\}$, cluster number $C$
% 		\ENSURE prompt tokens $\bm H_\rho$, adjacency matrix $\bm A_\rho$
% 		\FOR{$i \in \mathcal{T}$}
% 		\STATE Conduct type-specific transformation via Eq. \ref{equ::transformation}
% 		\ENDFOR
% 		\STATE Learn structural node features $\bm H_\tau^{\mathcal{G}}$ via Eq.\ref{equ::structure}
% 		\STATE Learn semantic node features $\bm H_\tau^{\mathcal{P}}$ via Eq.\ref{equ::semantics}
% 		\STATE Map node features into the contrastive space via Eq. \ref{equ::project}
% 		\STATE Pre-train $\mathcal{G}$ by contrastive learning via Eq. \ref{equ::pretrain}
% 		\STATE Initialize prompt tokens $\bm H_\rho$ and adjacency matrix $\bm A_\rho$
% 		\WHILE{not done}
% 		\STATE Learn clustering node features $\bm H_\tau^\rho$ via Eq. \ref{equ::cluster}
% 		\STATE Generate template meta-paths "XX\underline{PTP}XX" and calculate the probability via Eq. \ref{equ::discriminator}
% 		\STATE Update prompt tokens $\bm H_\rho$ and adjacency matrix $\bm A_\rho$ via Eq. \ref{equ::overall}
% 		\ENDWHILE
% 	\end{algorithmic}
% 	\label{algorithm}
% \end{algorithm}
