% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{xspace}
\newcommand{\ourmethod}{CLEAR\xspace}

\begin{document}
%
\title{CLEAR: Cluster-based Prompt Learning \\ on Heterogeneous Graphs}
%
\titlerunning{Prompt Learning on Heterogeneous Graphs}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Feiyang Wang$^1$, Zhongbao Zhang$^1$, Junda Ye$^1$, Li Sun$^2$, Jianzhong Qi$^2$}
\institute{$^1$Beijing University of Posts and Telecommunications, Beijing, China \\
$^2$North China Electric Power University, Beijing, China\\
\{fywang, zhongbaozb, jundaye\}@bupt.edu.cn, ccesunli@ncepu.edu.cn\\
jianzhong.qi@unimelb.edu.au
}
%
 \authorrunning{Wang, F., et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%

\begin{abstract}
Prompt learning has attracted increasing attention in the graph domain as a means to bridge the gap between pretext and downstream tasks. Existing studies on heterogeneous graph prompting typically use feature prompts to modify node features for specific downstream tasks, which do not concern the structure of heterogeneous graphs. Such a design also overlooks information from the meta-paths, which are core to learning the high-order semantics of the heterogeneous graphs. To address these issues, we propose \ourmethod, a \underline{C}luster-based prompt \underline{LEAR}ning model on heterogeneous graphs. We present cluster prompts that reformulate downstream tasks as heterogeneous graph reconstruction. In this way, we align the pretext and downstream tasks to share the same training objective. Additionally, our cluster prompts are also injected into the meta-paths such that the prompt learning process incorporates high-order semantic information entailed by the meta-paths. Extensive experiments on downstream tasks confirm the superiority of \ourmethod. It consistently outperforms state-of-the-art models, achieving up to 5\% improvement on the F1 metric for node classification.

\keywords{Heterogeneous graph \and Prompt learning \and Graph clustering.}
\end{abstract}
%
%
%
\input{introduction}
\input{relatedwork}
\input{preliminary}
\input{model}
\input{experiment}
\input{conclusion}

\bibliographystyle{splncs04}
\bibliography{ref}

\end{document}
