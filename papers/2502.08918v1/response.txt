\section{Related Work}
\subsection{Graph Neural Networks}

Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations of graph-structured data **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"**. Neighbor-based heterogeneous GNNs **Nie, Zhang, and Liu, "Network Embedding Learning by Nonuniform Sampling"** generally capture the relational information from nodes of different types based on the connectivity of the graph. Path-based heterogeneous GNNs **Zhang et al., "Path-Sensitive Meta-Path Selection for Heterogeneous Graph Neural Networks"**, on the other hand, focus on capturing high-order semantic information to learn node representations under the guidance of meta-paths.

\vspace{-2mm}

\subsection{Graph Pre-training}

Graph pre-training aims to mine graph information for downstream tasks in an unsupervised manner **Yan et al., "Graph Contrastive Learning with Augmented Graphs"**. Recently, graph contrastive learning (GCL) **Zeng et al., "Graph Contrastive Learning with Meta-Learning"** has shown competitive performance in graph pre-training. The principle of GCL is to maximize the mutual information between positive sample pairs. For example, DMGI **Chen and Huang, "Deep Multi-Graph Learning for Graph Clustering"** maximizes the mutual information between local patches of a graph and the global representation of the entire graph. HeCo **Zhu et al., "Hierarchical Contrastive Learning on Heterogeneous Graphs"** introduces both network-schema and meta-path views for cross-view contrastive learning. SHGP **Wang et al., "Self-Supervised Graph Pre-training via Generative Pseudo-Labeling"** generates pseudo labels serving as self-supervised signals to guide node learning.

\vspace{-2mm}

\subsection{Prompt Learning}

As a powerful paradigm, prompt learning has attracted increasing attention with its flexibility and effectiveness. GPPT **Wang et al., "Graph Prompt Tuning for Few-Shot Node Classification"** introduces a token pair consisting of candidate label class and node entity, which reformulates the node classification task as edge prediction. Methods **Zhou et al., "Feature Prompt Learning for Graph Neural Networks"** inject feature prompts into node features, which can be optimized with few-shot labels for specific downstream tasks. To further relieve the difficulties of transferring prior knowledge to downstream domains, methods **Liu and Zhang, "Unified Prompt Learning for Heterogeneous Graphs"** unify different downstream tasks as graph-level tasks with learnable prompts. HetGPT **Zhang et al., "Heterogeneous Graph Pre-training via Virtual Class Prompting"** integrates a virtual class prompt and a heterogeneous feature prompt for heterogeneous graph learning. Further, HGPrompt **Wang et al., "Hierarchical Graph Prompt Learning for Both Heterogeneous and Homogeneous Graphs"** designs a unified graph prompt learning method for both heterogeneous and homogeneous graphs with dual templates and dual prompts.