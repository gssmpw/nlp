\section{Experiments}

\begin{table}[b]
    \caption{Statistics of the datasets}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \setlength{\tabcolsep}{4mm}
    \begin{tabular}{c|c|c|c|c}
        \hline
        Dataset&\# Nodes&\# Edges&Meta-path&\# Classes\\ \hline
        ACM&\makecell{Paper: 4019\\Author: 7167\\Subject: 60}&\makecell{P-A: 13407\\P-S: 4019}&\makecell{PAP\\PSP}&\makecell{Paper\\3}\\ \hline
        DBLP&\makecell{Author: 4057\\Paper: 14328\\Term: 7723\\Conference: 20}&\makecell{P-A: 19645\\P-T: 85810\\P-C: 14328}&\makecell{APA\\APCPA\\APTPA}&\makecell{Author\\4}\\ \hline
        IMDB&\makecell{Movie: 4278\\Director: 2081\\Actor: 5257}&\makecell{M-D: 4278\\M-A: 12828}&\makecell{MAM\\MDM}&\makecell{Movie\\3}\\ \hline
    \end{tabular}}
    \label{tab::statistics}
\end{table}

\subsection{Experimental Setup}

\noindent\textbf{Datasets.} We evaluate \ourmethod on three benchmark datasets: ACM, DBLP, and IMDB \cite{HAN,MAGNN}. Detailed statistics of these datasets are given in Table \ref{tab::statistics}.

\noindent\textbf{Baselines.} We evaluate \ourmethod against the state-of-the-art models that are grouped into three main categories as follows. (1) Supervised learning: HetGNN \cite{HetGNN}, HAN \cite{HAN}, HGT \cite{HGT}, and MAGNN \cite{MAGNN}. (2) Pre-training: DMGI \cite{DMGI}, HeCo \cite{HeCo}, and SHGP \cite{SHGP}. (3) Prompt learning: HGPrompt \cite{HGPrompt} and HetGPT \cite{HetGPT}.

\begin{table}[t]
    \centering
    \caption{Evaluation results on node clustering}
    \resizebox{0.75\textwidth}{!}{
    \setlength{\tabcolsep}{3mm}
    \begin{tabular}{c|cc|cc|cc}
        \hline
        Dataset&\multicolumn{2}{c|}{ACM}&\multicolumn{2}{c|}{DBLP}&\multicolumn{2}{c}{IMDB}\\
        \hline
        Metric&NMI&ARI&NMI&ARI&NMI&ARI\\
        \hline
        DMGI&45.64&40.58&63.25&65.10&3.41&2.72\\
        HeCo&49.20&46.91&67.49&71.74&4.18&3.63\\
        SHGP&51.14&47.83&70.02&73.56&4.31&3.87\\
        HGPrompt&47.36&43.11&67.62&70.13&3.05&2.49\\
        HetGPT&52.58&49.37&70.42&72.61&4.13&3.65\\
        \textbf{\ourmethod}&\textbf{55.16}&\textbf{52.62}&\textbf{74.49}&\textbf{76.56}&\textbf{4.82}&\textbf{4.63}\\
        \hline
    \end{tabular}}
    \label{tab::clustering}
\end{table}

\begin{table}[t]
    \centering
    \caption{Evaluation results on 1-shot node classification}
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c|c|cc|cc|cc}
        \hline
        \multirow{2}{*}{Category}&Dataset&\multicolumn{2}{c|}{ACM}&\multicolumn{2}{c|}{DBLP}&\multicolumn{2}{c}{IMDB}\\
        &Metric&Macro-F1&Micro-F1&Macro-F1&Micro-F1&Macro-F1&Micro-F1\\
        \hline
        \multirow{4}{*}{Supervised Learning}&HetGNN&32.73$\pm$3.1&43.10$\pm$2.9&44.05$\pm$2.5&48.66$\pm$2.4&20.16$\pm$1.6&27.72$\pm$1.4\\
        &HAN&37.92$\pm$3.2&48.54$\pm$3.1&49.25$\pm$2.8&51.33$\pm$2.5&25.06$\pm$1.7&36.17$\pm$1.4\\
        &MAGNN&40.67$\pm$1.9&51.10$\pm$2.3&54.21$\pm$1.3&52.75$\pm$2.0&34.30$\pm$1.2&37.68$\pm$1.3\\
        &HGT&47.49$\pm$3.4&55.30$\pm$3.3&60.81$\pm$2.9&63.41$\pm$2.7&28.39$\pm$2.2&35.05$\pm$1.9\\
        \hline
        \multirow{3}{*}{Pre-training}&DMGI&44.10$\pm$1.5&50.35$\pm$1.4&74.89$\pm$2.2&75.46$\pm$2.0&30.02$\pm$1.8&33.21$\pm$1.7\\
        &HeCo&54.76$\pm$1.8&57.50$\pm$1.7&82.32$\pm$1.3&82.41$\pm$1.3&29.13$\pm$1.9&34.57$\pm$1.6\\
        &SHGP&59.01$\pm$1.3&63.29$\pm$1.3&84.74$\pm$1.2&85.32$\pm$1.1&36.83$\pm$1.2&38.25$\pm$1.3\\
        \hline
        \multirow{3}{*}{Prompting}&HGPrompt&60.35$\pm$1.9&62.92$\pm$2.2&82.54$\pm$1.8&84.76$\pm$2.0&34.31$\pm$1.6&37.25$\pm$1.7\\
        &HetGPT&63.97$\pm$1.6&65.24$\pm$1.8&85.14$\pm$1.2&85.60$\pm$1.2&39.19$\pm$1.9&40.06$\pm$1.8\\
        &\textbf{\ourmethod}&\textbf{69.19}$\pm$1.4&\textbf{71.43}$\pm$1.6&\textbf{88.86}$\pm$0.8&\textbf{89.07}$\pm$1.1&\textbf{44.50}$\pm$0.9&\textbf{45.48}$\pm$1.0\\
        \hline
    \end{tabular}}
    \label{tab::classification}
\end{table}

\noindent\textbf{Settings.} We consider two downstream tasks, i.e., node clustering and node classification. The node clustering task is learned under a zero-shot setting. We use the clustering feature of target nodes as the node embedding, and apply $K$-means algorithm to cluster the target nodes. The clustering quality is measured by normalized mutual information (NMI) and adjusted rand index (ARI). The node classification task is configured in $k$-shot learning, where $k$ ranges in $\{1,5,10,20,50\}$. We adopt Macro-F1 and Micro-F1 as the evaluation metrics.

\subsection{Main Results}

\noindent\textbf{Node Clustering}.
The result of zero-shot node clustering is reported in Table \ref{tab::clustering}. We omit the supervised models for comparison due to their data leakage during training. We can see that \ourmethod achieves the best results consistently over all datasets. Compared with pre-training models, \ourmethod gains an improvement of approximately 5\% on both metrics for the ACM and DBLP datasets, which demonstrates the effectiveness of our prompting model.

\noindent\textbf{Node Classification}.
The result of one-shot node classification is reported in Table \ref{tab::classification}. Compared with state-of-the-art prompting models, \ourmethod achieves an improvement of 5.6\%, 3.7\%, and 5.3\% in Macro-F1 and 6.2\%, 3.5\%, and 5.4\% in Micro-F1, respectively, demonstrating the superiority of our model. Under the one-shot learning setting, prompting models generally perform better than pre-training models and significantly better than supervised models. This indicates that when training labels are extremely scarce, pre-training and prompting models can better exploit inherent graph information. Moreover, the prompting models bridge the gap between pre-training and downstream tasks, leading to substantial performance gains.

% \begin{figure}
%     \centering
%     \subfigure[zero-shot node clustering]{\includegraphics[width=0.22\textwidth]{figure/NMI.pdf}}
%     \subfigure[one-shot node classification]{\includegraphics[width=0.225\textwidth]{figure/MaF1.pdf}}
%     \vspace{-3mm}
%     \caption{Performance of \ourmethod at different stages}
%     \label{fig::ablation}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\textwidth]{figure/ACM.pdf}
    \includegraphics[width=0.3\textwidth]{figure/DBLP.pdf}
    \includegraphics[width=0.3\textwidth]{figure/IMDB.pdf}
    \caption{Model performance w.r.t. number of shots on node classification task}
    \label{fig::classification}
\end{figure}

\subsection{Shot Performance}
We vary the number of shots and plot the result in Figure \ref{fig::classification}. \ourmethod consistently outperforms other models. With fewer shots, the performance gap is greater, demonstrating the robustness of \ourmethod for few-shot learning. Compared with the ``pre-train, fine-tune'' paradigm, our ``pre-train, prompt'' model successfully bridges the gap between node classification and the pretext task.

\begin{figure}[t]
    \centering
    \subfigure[raw features]{\includegraphics[width=0.3\textwidth]{figure/t-SNE0.pdf}}
    \subfigure[pre-trained features]{\includegraphics[width=0.3\textwidth]{figure/t-SNE1.pdf}}
    \subfigure[clustering features]{\includegraphics[width=0.3\textwidth]{figure/t-SNE2.pdf}}
    \vspace{-3mm}
    \caption{Visualization of node embeddings under the zero-shot setting on DBLP}
    \label{fig::visualization}
\end{figure}

\subsection{Visualization}

To better understand the effectiveness of our prompting framework under the zero-shot setting, we plot nodes and prompt tokens in 2-dimensional Euclidean space using the t-SNE algorithm in Figure 4. Nodes are colored by their ground-truth labels, and prompt tokens are denoted by stars. We observed that after pre-training, nodes with different labels have been preliminarily separated, but there is still a gap in application to downstream tasks. In contrast to pre-trained features, the clustering features are orthogonally distributed and the prompt tokens are located within the clusters of nodes, which simplifies the application of downstream tasks.