\section{Related Work}
\vspace{1mm}\noindent\textbf{RL-based Recommender Systems}
model the recommendation process as a Markov Decision Process (MDP), leveraging deep learning to estimate value functions and handle the high dimensionality of MDPs~\cite{mahmood2007learning}. \citet{chen2021generative} proposed InvRec, which uses inverse reinforcement learning to infer rewards directly from user behavior, enhancing policy learning accuracy. Recent efforts have focused on offline RLRS. \citet{wang2023causal} introduced CDT4Rec, which incorporates a causal mechanism for reward estimation and uses transformer architectures to improve offline RL-based recommendations. Additionally, \citet{10.1145/3637528.3671750} enhanced this line of research by developing a max-entropy exploration strategy to improve the decision transformerâ€™s ability to "stitch" together diverse sequences of user actions, addressing a key limitation in offline RLRS. 
\citet{gao2023alleviating} developed a counterfactual exploration strategy designed to mitigate the Matthew effect, which refers to the disparity in learning from uneven distributions of user data. 



\vspace{1mm}\noindent\textbf{Causal Recommendation.}
The recommendation domain has recently seen significant advancements through the integration of causal inference techniques, which help address biases in training data. For example, \citet{zhang2021causal} tackled the prevalent issue of popularity bias by introducing a causal inference paradigm that adjusts recommendation scores through targeted interventions. Similarly, \citet{li2024removing} proposed a unified multi-task learning approach to eliminate hidden confounding effects, incorporating a small number of unbiased ratings from a causal perspective.
Counterfactual reasoning has also gained traction in recommender systems. \citet{chen2023intrinsically} developed a causal augmentation technique to enhance exploration in RLRS by focusing on causally relevant aspects of user interactions. \citet{wang2023plug} introduced a method to generate counterfactual user interactions based on a causal view of MDP for data augmentation. In a related vein, \citet{li2023should} explored personalized incentive policy learning through an individualized counterfactual perspective.
Further studies have focused on the use of causal interventions. \citet{wang2022causalint} proposed CausalInt, a method inspired by causal interventions to address challenges in multi-scenario recommendation. Additionally, \citet{he2023addressing} tackled the confounding feature issue in recommendation by leveraging causal intervention techniques. These efforts collectively demonstrate the growing importance of causal inference and intervention in improving recommendation performance and addressing biases.