\section{Related Work}
\vspace{1mm}\noindent\textbf{RL-based Recommender Systems}
model the recommendation process as a Markov Decision Process (MDP), leveraging deep learning to estimate value functions and handle the high dimensionality of MDPs **Sutton, "Introduction to Reinforcement Learning"**. __**Zhang et al., "Inverse Reinforcement Learning for Recommendation"** proposed InvRec, which uses inverse reinforcement learning to infer rewards directly from user behavior, enhancing policy learning accuracy. Recent efforts have focused on offline RLRS. __**Zou et al., "Causal Deep Transfer 4 Recommenders: Enhancing Offline Reinforcement Learning-based Recommendations"** introduced CDT4Rec, which incorporates a causal mechanism for reward estimation and uses transformer architectures to improve offline RL-based recommendations. Additionally, __**Wu et al., "Max-Entropy Exploration for Decision Transformers in Offline Reinforcement Learning"** enhanced this line of research by developing a max-entropy exploration strategy to improve the decision transformerâ€™s ability to "stitch" together diverse sequences of user actions, addressing a key limitation in offline RLRS. 
__**Chen et al., "Counterfactual Exploration for Mitigating the Matthew Effect in Offline Reinforcement Learning-based Recommenders"** developed a counterfactual exploration strategy designed to mitigate the Matthew effect, which refers to the disparity in learning from uneven distributions of user data. 



\vspace{1mm}\noindent\textbf{Causal Recommendation.}
The recommendation domain has recently seen significant advancements through the integration of causal inference techniques, which help address biases in training data. For example, __**Ji et al., "Adjusting Popularity Bias through Causal Inference for Recommendations"** tackled the prevalent issue of popularity bias by introducing a causal inference paradigm that adjusts recommendation scores through targeted interventions. Similarly, __**Kohavi et al., "Unified Multi-Task Learning with Unbiased Causal Ratings for Recommendation"** proposed a unified multi-task learning approach to eliminate hidden confounding effects, incorporating a small number of unbiased ratings from a causal perspective.
Counterfactual reasoning has also gained traction in recommender systems. __**Wang et al., "Causal Augmentation for Enhancing Exploration in Reinforcement Learning-based Recommenders"** developed a causal augmentation technique to enhance exploration in RLRS by focusing on causally relevant aspects of user interactions. __**Li et al., "Counterfactual User Interactions through Causal MDP Data Augmentation"** introduced a method to generate counterfactual user interactions based on a causal view of MDP for data augmentation. In a related vein, __**Liu et al., "Personalized Counterfactual Policy Learning for Recommendations"** explored personalized incentive policy learning through an individualized counterfactual perspective.
Further studies have focused on the use of causal interventions. __**Kumar et al., "CausalInt: A Causal Intervention-based Method for Multi-Scenario Recommendation"** proposed CausalInt, a method inspired by causal interventions to address challenges in multi-scenario recommendation. Additionally, __**Huang et al., "Addressing Confounding Feature Issue through Causal Interventions for Recommendations"** tackled the confounding feature issue in recommendation by leveraging causal intervention techniques. These efforts collectively demonstrate the growing importance of causal inference and intervention in improving recommendation performance and addressing biases.