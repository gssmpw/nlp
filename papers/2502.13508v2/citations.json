[
  {
    "index": 0,
    "papers": [
      {
        "key": "chung2022scalinginstructionfinetunedlanguagemodels",
        "author": "Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei",
        "title": "Scaling Instruction-Finetuned Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ouyang2022traininglanguagemodelsfollow",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "touvron2023llamaopenefficientfoundation",
        "author": "Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\u00e9e Lacroix and Baptiste Rozi\u00e8re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gu_mamba_2024",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "awadalla2023openflamingo",
        "author": "Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt",
        "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2023blip2bootstrappinglanguageimagepretraining",
        "author": "Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi",
        "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2024llamaadapterefficientfinetuninglanguage",
        "author": "Renrui Zhang and Jiaming Han and Chris Liu and Peng Gao and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Yu Qiao",
        "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "laurencon_obelics_2023",
        "author": "Lauren\u00e7on, Hugo and Saulnier, Lucile and Tronchon, L\u00e9o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander M. and Kiela, Douwe and Cord, Matthieu and Sanh, Victor",
        "title": "{OBELICS}: {An} {Open} {Web}-{Scale} {Filtered} {Dataset} of {Interleaved} {Image}-{Text} {Documents}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "karamcheti_prismatic_2024",
        "author": "Karamcheti, Siddharth and Nair, Suraj and Balakrishna, Ashwin and Liang, Percy and Kollar, Thomas and Sadigh, Dorsa",
        "title": "Prismatic {VLMs}: {Investigating} the {Design} {Space} of {Visually}-{Conditioned} {Language} {Models}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2023visualinstructiontuning",
        "author": "Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee",
        "title": "Visual Instruction Tuning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhao_cobra_2025",
        "author": "Zhao, Han and Zhang, Min and Zhao, Wei and Ding, Pengxiang and Huang, Siteng and Wang, Donglin",
        "title": "Cobra: {Extending} {Mamba} to {Multi}-{Modal} {Large} {Language} {Model} for {Efficient} {Inference}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang_video-llama_2023",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "Video-{LLaMA}: {An} {Instruction}-tuned {Audio}-{Visual} {Language} {Model} for {Video} {Understanding}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "cheng_videollama_2024",
        "author": "Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and Bing, Lidong",
        "title": "{VideoLLaMA} 2: {Advancing} {Spatial}-{Temporal} {Modeling} and {Audio} {Understanding} in {Video}-{LLMs}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu_pite_2024",
        "author": "Liu, Yang and Ding, Pengxiang and Huang, Siteng and Zhang, Min and Zhao, Han and Wang, Donglin",
        "title": "{PiTe}: {Pixel}-{Temporal} {Alignment} for {Large} {Video}-{Language} {Model}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "lin_video-llava_2024",
        "author": "Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li",
        "title": "Video-{LLaVA}: {Learning} {United} {Visual} {Representation} by {Alignment} {Before} {Projection}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "li_llava-next-interleave_2024",
        "author": "Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan",
        "title": "{LLaVA}-{NeXT}-{Interleave}: {Tackling} {Multi}-image, {Video}, and {3D} in {Large} {Multimodal} {Models}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "radford2021learningtransferablevisualmodels",
        "author": "Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2022blipbootstrappinglanguageimagepretraining",
        "author": "Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi",
        "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "han_imagebind-llm_2023",
        "author": "Han, Jiaming and Zhang, Renrui and Shao, Wenqi and Gao, Peng and Xu, Peng and Xiao, Han and Zhang, Kaipeng and Liu, Chris and Wen, Song and Guo, Ziyu and Lu, Xudong and Ren, Shuai and Wen, Yafei and Chen, Xiaoxin and Yue, Xiangyu and Li, Hongsheng and Qiao, Yu",
        "title": "{ImageBind}-{LLM}: {Multi}-modality {Instruction} {Tuning}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "lu_unified-io_2024",
        "author": "Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha",
        "title": "Unified-{IO} 2: {Scaling} {Autoregressive} {Multimodal} {Models} with {Vision}, {Language}, {Audio}, and {Action}"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "openai2024gpt4",
        "author": "OpenAI",
        "title": "GPT-4: Generative Pre-trained Transformer 4"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "team_gemini_2024",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac",
        "title": "Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "fu_vita_2024",
        "author": "Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and Zheng, Xiawu and He, Ran and Ji, Rongrong and Wu, Yunsheng and Shan, Caifeng and Sun, Xing",
        "title": "{VITA}: {Towards} {Open}-{Source} {Interactive} {Omni} {Multimodal} {LLM}"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "driess2023palmeembodiedmultimodallanguage",
        "author": "Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence",
        "title": "PaLM-E: An Embodied Multimodal Language Model"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "saycan2022arxiv",
        "author": "Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng",
        "title": "Do As I Can and Not As I Say: Grounding Language in Robotic Affordances"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "liang2023codepolicieslanguagemodel",
        "author": "Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng",
        "title": "Code as Policies: Language Model Programs for Embodied Control"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "brohan2023rt2visionlanguageactionmodelstransfer",
        "author": "Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alexander Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich",
        "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "li2024visionlanguagefoundationmodelseffective",
        "author": "Xinghang Li and Minghuan Liu and Hanbo Zhang and Cunjun Yu and Jie Xu and Hongtao Wu and Chilam Cheang and Ya Jing and Weinan Zhang and Huaping Liu and Hang Li and Tao Kong",
        "title": "Vision-Language Foundation Models as Effective Robot Imitators"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "kim2024openvlaopensourcevisionlanguageactionmodel",
        "author": "Moo Jin Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "ding_quar-vla_2024",
        "author": "Ding, Pengxiang and Zhao, Han and Zhang, Wenjie and Song, Wenxuan and Zhang, Min and Huang, Siteng and Yang, Ningxi and Wang, Donglin",
        "title": "{QUAR}-{VLA}: {Vision}-{Language}-{Action} {Model} for {Quadruped} {Robots}"
      },
      {
        "key": "tong_quart-online_2024",
        "author": "Tong, Xinyang and Ding, Pengxiang and Wang, Donglin and Zhang, Wenjie and Cui, Can and Sun, Mingyang and Fan, Yiguo and Zhao, Han and Zhang, Hongyin and Dang, Yonghao and Huang, Siteng and Lyu, Shangke",
        "title": "{QUART}-{Online}: {Latency}-{Free} {Large} {Multimodal} {Language} {Model} for {Quadruped} {Robot} {Learning}"
      },
      {
        "key": "yue2024deer",
        "author": "Yue, Yang and Wang, Yulin and Kang, Bingyi and Han, Yizeng and Wang, Shenzhi and Song, Shiji and Feng, Jiashi and Huang, Gao",
        "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution"
      },
      {
        "key": "zhang2025gevrm",
        "author": "Zhang, Hongyin and Ding, Pengxiang and Lyu, Shangke and Peng, Ying and Wang, Donglin",
        "title": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "belkhale_rt-h_2024",
        "author": "Belkhale, Suneel and Ding, Tianli and Xiao, Ted and Sermanet, Pierre and Vuong, Quon and Tompson, Jonathan and Chebotar, Yevgen and Dwibedi, Debidatta and Sadigh, Dorsa",
        "title": "{RT}-{H}: {Action} {Hierarchies} {Using} {Language}"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "cai2024spatialbotprecisespatialunderstanding",
        "author": "Wenxiao Cai and Iaroslav Ponomarenko and Jianhao Yuan and Xiaoqi Li and Wankou Yang and Hao Dong and Bo Zhao",
        "title": "SpatialBot: Precise Spatial Understanding with Vision Language Models"
      },
      {
        "key": "zhen_3d-vla_2024",
        "author": "Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang",
        "title": "{3D}-{VLA}: {A} {3D} {Vision}-{Language}-{Action} {Generative} {World} {Model}"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "shah_mutex_2023",
        "author": "Shah, Rutav and Mart\u00edn-Mart\u00edn, Roberto and Zhu, Yuke",
        "title": "{MUTEX}: {Learning} {Unified} {Policies} from {Multimodal} {Task} {Specifications}"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "fu_vita_2024",
        "author": "Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and Zheng, Xiawu and He, Ran and Ji, Rongrong and Wu, Yunsheng and Shan, Caifeng and Sun, Xing",
        "title": "{VITA}: {Towards} {Open}-{Source} {Interactive} {Omni} {Multimodal} {LLM}"
      }
    ]
  }
]