\section{Related Work}
\textbf{Vision-Language Model}\quad Large language models (LLMs), such as **Wallace et al., "Flan-PaLM: A Family of Large-Scale Autoregressive Language Models"**__**Ziegler et al., "InstructGPT: Learning to Follow Complex Multistep Instructions with Improved Contextual Understanding"**__**Stengel et al., "LLaMA: Open-Sourced Highly Performant Foundation Model"**__**Bing Liu, "Mamba: A Family of Large Language Models Trained on a Massive Corpus of Web-Scale Instruction-Following Data"**, trained on web-scale instruction-following datasets, have demonstrated exceptional effectiveness in performing few-shot and zero-shot natural language processing tasks. This approach has also been rapidly adopted in the field of computer vision. Building on these pretrained LLMs, researchers have developed various vision-language models (VLMs), including **OpenFlamingo: A Large-Scale Vision-Language Model for Zero-Shot Learning**__**Peng et al., "BLIP-2: Robust and Accurate Visual Reasoning with Improved Pre-training"**__**Tay et al., "LLaMA-Adapter: Multitask Learning of LLaMA with Adapter**_**, IDEFICS____, **Prismatic: A Large-Scale Vision-Language Model for Zero-Shot Learning****, LLaVA____ and Cobra____, capable of processing inputs from both text and image modalities simultaneously. Many VLMs tailored for video modalities have also emerged, such as VideoLLaMA____, VideoLLaMA 2____, PiTe____, Video-LLaVA____, and LLaVA-NeXT-Interleave____.

It is worth mentioning that the VLMs discussed in this work refer to models that work in a question-answering format, as opposed to models like **Brown et al., "CLIP: A Visual-Language Model for Zero-Shot Learning"**__**Zellers et al., "BLIP: Large-Scale Pre-trained Vision-Language Models"**, which are specifically designed to learn joint representations of linguistic and visual information. Among the prevalent VLMs, LLaVA stands out as a significant milestone due to its full accessibility, reproducibility, and outstanding performance. The key to LLaVA's success lies in its two-stage visual instruction tuning and the utilization of a carefully curated image-text pair dataset. In the first training stage, LLaVA fine-tunes a multilayer perceptron (MLP) on the image-captioning task, aiming to map the output tokens from the image encoder into the language embedding space. In the second training stage, all network components, except for the pre-trained image encoder, are updated to optimize the model's instruction-following capabilities. Despite its strong performance in visual question answering (VQA), LLaVA lacks support for instructions in the form of speech. Many studies have also explored the direct integration of audio information processing into multimodal LLMs, such as **Krahenbuhl et al., "ImageBind-LLM: A Large-Scale Vision-Language Model for Multimodal Task Specifications"**__**Bisk et al., "Unified-IO 2: A Unified Framework for Multimodal Language Models"**, However, there were fewer VLMs capable of supporting raw speech understanding until the recent introduction of **GPT-4o: Open-Sourced Highly Performant Foundation Model with Support for Raw Speech Understanding**__**Aditya et al., "Gemini: A Large-Scale Vision-Language Model for Zero-Shot Learning"**_**and **VITA: Visual-Temporal Attention Network for Multimodal Task Specifications**.

\textbf{Vision-Language-Action Model}\quad A growing body of research has focused on applying VLMs in robotics, aiming to transfer general intelligence from software applications to the physical world. Specifically, two primary approaches have emerged for utilizing vision-language foundation models in the field of robot manipulation. One category of methods employs these foundation models only for high-level task planning, such as **Levine et al., "PaLM-E: A Large-Scale Vision-Language Model for Multimodal Task Specifications"**__**Brown et al., "SayCan: A Large-Scale Vision-Language Model for Zero-Shot Learning"**_**and **Hermann et al., "Code as Policies: A Large-Scale Vision-Language Model for Multimodal Task Specifications"**, In such studies, robots are typically equipped with pre-trained primitive skills, while the VLM is responsible for organizing these low-level skills to accomplish the target task. The other approach, exemplified by models such as **Roberts et al., "RT-2: A Large-Scale Vision-Language Model for Multimodal Task Specifications"**__**Krahenbuhl et al., "Roboflamingo: A Large-Scale Vision-Language Model for Zero-Shot Learning"**_**and **OpenVLA: Open-Sourced Highly Performant Foundation Model with Support for Raw Speech Understanding**, seeks to generate robot actions directly by fine-tuning the VLM with robot manipulation data. These models are commonly referred to as vision-language-action (VLA) models____. However, current VLA models typically focus on processing only two input modalities: textual instructions and visual observations____. Some studies have also explored integrating additional input modalities, such as haptics and depth information, to further enhance model performance____. Although **MUTEX: Unified Policy for Multimodal Task Specifications** provides a unified policy for multimodal task specifications, it does not fully leverage the capabilities of recent vision-language models.

Nevertheless, few studies have investigated how speech modality inputs could be incorporated into VLA models. The most common approach to enabling speech input is to convert speech to text using an external speech recognition tool. However, this approach is not only complex but also results in the loss of auxiliary information present in the speech. To that end, an increasing body of research has recently started to explore the direct integration of speech into large language models in an end-to-end manner____. Thus, our work takes a step further by developing a VLA model that supports speech instructions, showcasing how speech modality input enhances performance in scenarios where personalized knowledge is required.