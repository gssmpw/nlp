\section{Related Work}
\textbf{Vision-Language Model}\quad Large language models (LLMs), such as FLAN-PaLM~\citep{chung2022scalinginstructionfinetunedlanguagemodels}, InstructGPT~\citep{ouyang2022traininglanguagemodelsfollow}, LLaMA~\citep{touvron2023llamaopenefficientfoundation}, and Mamba~\citep{gu_mamba_2024}, trained on web-scale instruction-following datasets, have demonstrated exceptional effectiveness in performing few-shot and zero-shot natural language processing tasks. This approach has also been rapidly adopted in the field of computer vision. Building on these pretrained LLMs, researchers have developed various vision-language models (VLMs), including OpenFlamingo~\citep{awadalla2023openflamingo}, BLIP-2~\citep{li2023blip2bootstrappinglanguageimagepretraining}, LLaMA-Adapter~\citep{zhang2024llamaadapterefficientfinetuninglanguage}, IDEFICS~\citep{laurencon_obelics_2023}, Prismatic~\citep{karamcheti_prismatic_2024}, LLaVA~\citep{liu2023visualinstructiontuning} and Cobra~\citep{zhao_cobra_2025}, capable of processing inputs from both text and image modalities simultaneously. Many VLMs tailored for video modalities have also emerged, such as VideoLLaMA~\citep{zhang_video-llama_2023}, VideoLLaMA 2~\citep{cheng_videollama_2024}, PiTe~\citep{liu_pite_2024}, Video-LLaVA~\citep{lin_video-llava_2024}, and LLaVA-NeXT-Interleave~\citep{li_llava-next-interleave_2024}.

It is worth mentioning that the VLMs discussed in this work refer to models that work in a question-answering format, as opposed to models like CLIP~\citep{radford2021learningtransferablevisualmodels} and BLIP~\citep{li2022blipbootstrappinglanguageimagepretraining}, which are specifically designed to learn joint representations of linguistic and visual information. Among the prevalent VLMs, LLaVA stands out as a significant milestone due to its full accessibility, reproducibility, and outstanding performance. The key to LLaVA's success lies in its two-stage visual instruction tuning and the utilization of a carefully curated image-text pair dataset. In the first training stage, LLaVA fine-tunes a multilayer perceptron (MLP) on the image-captioning task, aiming to map the output tokens from the image encoder into the language embedding space. In the second training stage, all network components, except for the pre-trained image encoder, are updated to optimize the model's instruction-following capabilities. Despite its strong performance in visual question answering (VQA), LLaVA lacks support for instructions in the form of speech. Many studies have also explored the direct integration of audio information processing into multimodal LLMs, such as ImageBind-LLM~\citep{han_imagebind-llm_2023} and Unified-IO 2~\citep{lu_unified-io_2024}. However, there were fewer VLMs capable of supporting raw speech understanding until the recent introduction of GPT-4o~\citep{openai2024gpt4}, Gemini~\citep{team_gemini_2024} and VITA~\citep{fu_vita_2024}.

\textbf{Vision-Language-Action Model}\quad A growing body of research has focused on applying VLMs in robotics, aiming to transfer general intelligence from software applications to the physical world. Specifically, two primary approaches have emerged for utilizing vision-language foundation models in the field of robot manipulation. One category of methods employs these foundation models only for high-level task planning, such as PaLM-E~\citep{driess2023palmeembodiedmultimodallanguage}, SayCan~\citep{saycan2022arxiv} and Code as Policies~\citep{liang2023codepolicieslanguagemodel}. In such studies, robots are typically equipped with pre-trained primitive skills, while the VLM is responsible for organizing these low-level skills to accomplish the target task. The other approach, exemplified by models such as RT-2~\citep{brohan2023rt2visionlanguageactionmodelstransfer}, Roboflamingo~\citep{li2024visionlanguagefoundationmodelseffective}, and OpenVLA~\citep{kim2024openvlaopensourcevisionlanguageactionmodel}, seeks to generate robot actions directly by fine-tuning the VLM with robot manipulation data. These models are commonly referred to as vision-language-action (VLA) models~\citep{ding_quar-vla_2024, tong_quart-online_2024, yue2024deer, zhang2025gevrm}. However, current VLA models typically focus on processing only two input modalities: textual instructions and visual observations~\citep{belkhale_rt-h_2024}. Some studies have also explored integrating additional input modalities, such as haptics and depth information, to further enhance model performance~\citep{cai2024spatialbotprecisespatialunderstanding,zhen_3d-vla_2024}. Although MUTEX~\citep{shah_mutex_2023} provides a unified policy for multimodal task specifications, it does not fully leverage the capabilities of recent vision-language models.

Nevertheless, few studies have investigated how speech modality inputs could be incorporated into VLA models. The most common approach to enabling speech input is to convert speech to text using an external speech recognition tool. However, this approach is not only complex but also results in the loss of auxiliary information present in the speech. To that end, an increasing body of research has recently started to explore the direct integration of speech into large language models in an end-to-end manner~\citep{fu_vita_2024}. Thus, our work takes a step further by developing a VLA model that supports speech instructions, showcasing how speech modality input enhances performance in scenarios where personalized knowledge is required.