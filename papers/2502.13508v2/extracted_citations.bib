@article{awadalla2023openflamingo,
  title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models},
  author={Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@misc{belkhale_rt-h_2024,
	title = {{RT}-{H}: {Action} {Hierarchies} {Using} {Language}},
	shorttitle = {{RT}-{H}},
	url = {http://arxiv.org/abs/2403.01823},
	doi = {10.48550/arXiv.2403.01823},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Belkhale, Suneel and Ding, Tianli and Xiao, Ted and Sermanet, Pierre and Vuong, Quon and Tompson, Jonathan and Chebotar, Yevgen and Dwibedi, Debidatta and Sadigh, Dorsa},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01823 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, /unread},
}

@misc{brohan2023rt2visionlanguageactionmodelstransfer,
      title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}, 
      author={Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alexander Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
      year={2023},
      eprint={2307.15818},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2307.15818}, 
}

@misc{cai2024spatialbotprecisespatialunderstanding,
      title={SpatialBot: Precise Spatial Understanding with Vision Language Models}, 
      author={Wenxiao Cai and Iaroslav Ponomarenko and Jianhao Yuan and Xiaoqi Li and Wankou Yang and Hao Dong and Bo Zhao},
      year={2024},
      eprint={2406.13642},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.13642}, 
}

@misc{cheng_videollama_2024,
	title = {{VideoLLaMA} 2: {Advancing} {Spatial}-{Temporal} {Modeling} and {Audio} {Understanding} in {Video}-{LLMs}},
	shorttitle = {{VideoLLaMA} 2},
	url = {http://arxiv.org/abs/2406.07476},
	doi = {10.48550/arXiv.2406.07476},
	language = {en-US},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and Bing, Lidong},
	month = oct,
	year = {2024},
	note = {arXiv:2406.07476},
}

@misc{chung2022scalinginstructionfinetunedlanguagemodels,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.11416}, 
}

@inproceedings{ding_quar-vla_2024,
	address = {Berlin, Heidelberg},
	title = {{QUAR}-{VLA}: {Vision}-{Language}-{Action} {Model} for {Quadruped} {Robots}},
	isbn = {978-3-031-72651-4},
	shorttitle = {{QUAR}-{VLA}},
	url = {https://doi.org/10.1007/978-3-031-72652-1_21},
	doi = {10.1007/978-3-031-72652-1_21},
	urldate = {2025-02-18},
	booktitle = {Computer {Vision} – {ECCV} 2024: 18th {European} {Conference}, {Milan}, {Italy}, {September} 29–{October} 4, 2024, {Proceedings}, {Part} {V}},
	publisher = {Springer-Verlag},
	author = {Ding, Pengxiang and Zhao, Han and Zhang, Wenjie and Song, Wenxuan and Zhang, Min and Huang, Siteng and Yang, Ningxi and Wang, Donglin},
	month = oct,
	year = {2024},
	pages = {352--367},
}

@misc{driess2023palmeembodiedmultimodallanguage,
      title={PaLM-E: An Embodied Multimodal Language Model}, 
      author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
      year={2023},
      eprint={2303.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.03378}, 
}

@misc{fu_vita_2024,
	title = {{VITA}: {Towards} {Open}-{Source} {Interactive} {Omni} {Multimodal} {LLM}},
	shorttitle = {{VITA}},
	url = {http://arxiv.org/abs/2408.05211},
	doi = {10.48550/arXiv.2408.05211},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and Zheng, Xiawu and He, Ran and Ji, Rongrong and Wu, Yunsheng and Shan, Caifeng and Sun, Xing},
	month = sep,
	year = {2024},
	note = {arXiv:2408.05211 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{gu_mamba_2024,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {https://openreview.net/forum?id=tEYskw1VY2#discussion},
	language = {en},
	urldate = {2025-02-18},
	booktitle = {First {Conference} on {Language} {Modeling}},
	author = {Gu, Albert and Dao, Tri},
	month = aug,
	year = {2024},
}

@misc{han_imagebind-llm_2023,
	title = {{ImageBind}-{LLM}: {Multi}-modality {Instruction} {Tuning}},
	shorttitle = {{ImageBind}-{LLM}},
	url = {http://arxiv.org/abs/2309.03905},
	doi = {10.48550/arXiv.2309.03905},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Han, Jiaming and Zhang, Renrui and Shao, Wenqi and Gao, Peng and Xu, Peng and Xiao, Han and Zhang, Kaipeng and Liu, Chris and Wen, Song and Guo, Ziyu and Lu, Xudong and Ren, Shuai and Wen, Yafei and Chen, Xiaoxin and Yue, Xiangyu and Li, Hongsheng and Qiao, Yu},
	month = sep,
	year = {2023},
}

@inproceedings{karamcheti_prismatic_2024,
	title = {Prismatic {VLMs}: {Investigating} the {Design} {Space} of {Visually}-{Conditioned} {Language} {Models}},
	shorttitle = {Prismatic {VLMs}},
	url = {https://proceedings.mlr.press/v235/karamcheti24a.html},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Karamcheti, Siddharth and Nair, Suraj and Balakrishna, Ashwin and Liang, Percy and Kollar, Thomas and Sadigh, Dorsa},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {23123--23144},
}

@misc{kim2024openvlaopensourcevisionlanguageactionmodel,
      title={OpenVLA: An Open-Source Vision-Language-Action Model}, 
      author={Moo Jin Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},
      year={2024},
      eprint={2406.09246},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2406.09246}, 
}

@misc{laurencon_obelics_2023,
	title = {{OBELICS}: {An} {Open} {Web}-{Scale} {Filtered} {Dataset} of {Interleaved} {Image}-{Text} {Documents}},
	shorttitle = {{OBELICS}},
	url = {http://arxiv.org/abs/2306.16527},
	doi = {10.48550/arXiv.2306.16527},
	publisher = {arXiv},
	author = {Laurençon, Hugo and Saulnier, Lucile and Tronchon, Léo and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander M. and Kiela, Douwe and Cord, Matthieu and Sanh, Victor},
	month = aug,
	year = {2023},
	note = {arXiv:2306.16527},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
}

@misc{li2022blipbootstrappinglanguageimagepretraining,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      eprint={2201.12086},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.12086}, 
}

@misc{li2023blip2bootstrappinglanguageimagepretraining,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2301.12597}, 
}

@misc{li2024visionlanguagefoundationmodelseffective,
      title={Vision-Language Foundation Models as Effective Robot Imitators}, 
      author={Xinghang Li and Minghuan Liu and Hanbo Zhang and Cunjun Yu and Jie Xu and Hongtao Wu and Chilam Cheang and Ya Jing and Weinan Zhang and Huaping Liu and Hang Li and Tao Kong},
      year={2024},
      eprint={2311.01378},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2311.01378}, 
}

@misc{li_llava-next-interleave_2024,
	title = {{LLaVA}-{NeXT}-{Interleave}: {Tackling} {Multi}-image, {Video}, and {3D} in {Large} {Multimodal} {Models}},
	shorttitle = {{LLaVA}-{NeXT}-{Interleave}},
	url = {http://arxiv.org/abs/2407.07895},
	doi = {10.48550/arXiv.2407.07895},
	language = {en-US},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07895},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{liang2023codepolicieslanguagemodel,
      title={Code as Policies: Language Model Programs for Embodied Control}, 
      author={Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng},
      year={2023},
      eprint={2209.07753},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2209.07753}, 
}

@inproceedings{lin_video-llava_2024,
	address = {Miami, Florida, USA},
	title = {Video-{LLaVA}: {Learning} {United} {Visual} {Representation} by {Alignment} {Before} {Projection}},
	shorttitle = {Video-{LLaVA}},
	url = {https://aclanthology.org/2024.emnlp-main.342},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {5971--5984},
}

@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}

@inproceedings{liu_pite_2024,
	address = {Berlin, Heidelberg},
	title = {{PiTe}: {Pixel}-{Temporal} {Alignment} for {Large} {Video}-{Language} {Model}},
	isbn = {978-3-031-72651-4},
	shorttitle = {{PiTe}},
	url = {https://doi.org/10.1007/978-3-031-72652-1_10},
	doi = {10.1007/978-3-031-72652-1_10},
	urldate = {2025-02-18},
	booktitle = {Computer {Vision} – {ECCV} 2024: 18th {European} {Conference}, {Milan}, {Italy}, {September} 29–{October} 4, 2024, {Proceedings}, {Part} {V}},
	publisher = {Springer-Verlag},
	author = {Liu, Yang and Ding, Pengxiang and Huang, Siteng and Zhang, Min and Zhao, Han and Wang, Donglin},
	month = oct,
	year = {2024},
	pages = {160--176},
}

@inproceedings{lu_unified-io_2024,
	address = {Seattle, WA, USA},
	title = {Unified-{IO} 2: {Scaling} {Autoregressive} {Multimodal} {Models} with {Vision}, {Language}, {Audio}, and {Action}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350353006},
	shorttitle = {Unified-{IO} 2},
	url = {https://ieeexplore.ieee.org/document/10657364/},
	doi = {10.1109/CVPR52733.2024.02497},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha},
	month = jun,
	year = {2024},
	pages = {26429--26445},
}

@misc{openai2024gpt4,
  author       = {OpenAI},
  title        = {GPT-4: Generative Pre-trained Transformer 4},
  year         = {2024},
  howpublished = {\url{https://openai.com/index/hello-gpt-4o/}}
}

@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@inproceedings{saycan2022arxiv,
    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},
    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
    booktitle={arXiv preprint arXiv:2204.01691},
    year={2022}
}

@inproceedings{shah_mutex_2023,
	title = {{MUTEX}: {Learning} {Unified} {Policies} from {Multimodal} {Task} {Specifications}},
	shorttitle = {{MUTEX}},
	url = {https://proceedings.mlr.press/v229/shah23b.html},
	language = {en},
	urldate = {2024-12-08},
	booktitle = {Proceedings of {The} 7th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Shah, Rutav and Martín-Martín, Roberto and Zhu, Yuke},
	month = dec,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {2663--2682},
}

@misc{team_gemini_2024,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac},
	month = jun,
	year = {2024},
	note = {arXiv:2312.11805},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tong_quart-online_2024,
	title = {{QUART}-{Online}: {Latency}-{Free} {Large} {Multimodal} {Language} {Model} for {Quadruped} {Robot} {Learning}},
	shorttitle = {{QUART}-{Online}},
	url = {http://arxiv.org/abs/2412.15576},
	doi = {10.48550/arXiv.2412.15576},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Tong, Xinyang and Ding, Pengxiang and Wang, Donglin and Zhang, Wenjie and Cui, Can and Sun, Mingyang and Fan, Yiguo and Zhao, Han and Zhang, Hongyin and Dang, Yonghao and Huang, Siteng and Lyu, Shangke},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15576 [cs]},
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@article{yue2024deer,
  title={DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution},
  author={Yue, Yang and Wang, Yulin and Kang, Bingyi and Han, Yizeng and Wang, Shenzhi and Song, Shiji and Feng, Jiashi and Huang, Gao},
  journal={NeurIPS},
  year={2024}
}

@misc{zhang2024llamaadapterefficientfinetuninglanguage,
      title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention}, 
      author={Renrui Zhang and Jiaming Han and Chris Liu and Peng Gao and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Yu Qiao},
      year={2024},
      eprint={2303.16199},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.16199}, 
}

@article{zhang2025gevrm,
  title={GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation},
  author={Zhang, Hongyin and Ding, Pengxiang and Lyu, Shangke and Peng, Ying and Wang, Donglin},
  journal={arXiv preprint arXiv:2502.09268},
  year={2025}
}

@inproceedings{zhang_video-llama_2023,
	address = {Singapore},
	title = {Video-{LLaMA}: {An} {Instruction}-tuned {Audio}-{Visual} {Language} {Model} for {Video} {Understanding}},
	shorttitle = {Video-{LLaMA}},
	url = {https://aclanthology.org/2023.emnlp-demo.49},
	doi = {10.18653/v1/2023.emnlp-demo.49},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Hang and Li, Xin and Bing, Lidong},
	editor = {Feng, Yansong and Lefever, Els},
	month = dec,
	year = {2023},
	pages = {543--553},
}

@misc{zhao_cobra_2025,
	title = {Cobra: {Extending} {Mamba} to {Multi}-{Modal} {Large} {Language} {Model} for {Efficient} {Inference}},
	shorttitle = {Cobra},
	url = {http://arxiv.org/abs/2403.14520},
	doi = {10.48550/arXiv.2403.14520},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Zhao, Han and Zhang, Min and Zhao, Wei and Ding, Pengxiang and Huang, Siteng and Wang, Donglin},
	month = jan,
	year = {2025},
	note = {arXiv:2403.14520 [cs]},
}

@inproceedings{zhen_3d-vla_2024,
	title = {{3D}-{VLA}: {A} {3D} {Vision}-{Language}-{Action} {Generative} {World} {Model}},
	shorttitle = {{3D}-{VLA}},
	url = {https://proceedings.mlr.press/v235/zhen24a.html},
	language = {en},
	urldate = {2024-09-30},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {61229--61245},
}

