\section{Related Work}
% A lot of previous work have delved into using extended reality (XR) (a gather word of virtual reality (VR), AR, and mixed reality (MR)) techniques into robotics in numerous aspects. In this section, we revisit the current landscape of VR, AR, and MR in robotics, and AR for HRI.

% \subsection{Virtual Reality in Robotics}
% Employing XR technologies for facilitating robotics research has become a popular topic over past decades. VR has been widely used for simulating the robots due its virtuality and precision \cite{sethi2009validation,lee2012validation,matsas2017design,whitney2019comparing}. Burghardt et al. \cite{burghardt2020programming} developed a VR system with digital twins to capture human movements in a virtual environment, enabling real industrial robots to replicate complex human actions that are challenging to automate. Togias et al. \cite{togias2021virtual} explored a VR-based teleoperation method for reprogramming industrial robots, enabling flexible and remote process design, which supported hybrid human-robot collaboration to adapt production lines for different tasks. However, in most cases, VR has been harnessed together with robotics for healthcare, specifically, for rehabilitation \cite{albani2007virtual,wade2011virtual,brutsch2011virtual}. Over three decades ago, Ojha \cite{ojha1994application} acknowledged the potential of VR existing in medicine and surgery. The author proposed an application in evaluating the performance of impaired hand. which can be used to monitor patient's progress in therapy and to devise better treatment strategies. Mirelman et al. \cite{mirelman2009effects} investigated the effect of using VR for recovery for individuals after stroke, proved that using a robotic device coupled with VR improved the walking ability of them. Similarly, Frisoli et al. \cite{frisoli2007arm} explored the use of a robotic exoskeleleton in VR for arm rehabilitation, demonstrating the advantage of the inclusion of this technique.

% \subsection{Augmented Reality for Human Robot Interaction}
% In addition, harnessing AR (MR is usually referred to AR) for effective HRI and robotic teleoperation \cite{solanes2020teleoperation,marin2002very,brizzi2017effects} is a particularly well-established field. During past years, there is trend of using AR for effective robotic teleoperation due to its ability in visualization and information richness \cite{zhang2021supporting,nowak2021augmented}. For instance, Hedeyati et al. \cite{hedayati2018improving} explored how AR enhances robot teleoperation by providing intuitive visual feedback. By prototyping AR-based interfaces for aerial robots and evaluating them in a user study, the research demonstrated improved usability and performance over traditional systems that divide user attention. In addition, Hashimoto et al. \cite{hashimoto2011touchme} introduced TouchMe, a touch-based AR interface for intuitive robot teleoperation from a third-person view. Unlike traditional joystick controls, users can directly manipulate robot parts by touching them on a live camera feed, reducing the learning curve. For industrial scenarios, Michalos et al. \cite{michalos2016augmented} proposed an AR tool for hybrid human-robot workplaces, enhancing operator support through AR visualizations, instructions, and real-time production updates. It improves safety, acceptance, and workflow integration using AR immersion. For human-robot communication, Walker et al. \cite{walker2018communicating} proposed AR-based cues for conveying robot motion intent, enhancing human-robot interaction for non-anthropomorphic robots. A user study showed AR designs improve task efficiency and influence perceptions of robot teamwork. For AR puppeteer, as aforementioned, Van et al. \cite{van2024puppeteer} ideated and propsoed the first-ever robotic puppeteering conceptualization and system using optical see-through HMD AR, that a controller was necessitated to manipulated the virtual robot manually. Our work, filled the gap of controller-free AR robotic puppeteering, as well as bringing voice commanding by introducing LLM-driven methods.