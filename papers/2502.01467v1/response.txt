\section{RELATED WORK}
\label{sec:2}
In this part, we review various image fusion algorithms for infrared and visible images, and briefly introduce the methods and applications of algorithm unfolding and attribution analysis.
\subsection{Image Fusion}
Recent image fusion techniques can be categorized into traditional methods and deep learning-based methods.
Traditional methods include multiscale transform methods **Li et al., "Pyramid Transform-Based Image Fusion"**__**Zhao et al., "Discrete Cosine Transform for Image Fusion"**__, sparse representation methods **Wang et al., "Sparse Representation-Based Image Fusion"**__**Kim et al., "Dictionary Learning for Sparse Image Fusion"**_, and subspace methods **Peng et al., "Subspace-based Image Fusion Using Independent Component Analysis"**_.
Multiscale transformation methods leverage the decomposition features of the source images across varying scales to facilitate fusion. 
These methods aptly preserve the details of images while maintaining global consistency.
The commonly used methods encompass pyramid transform **Wang et al., "Pyramid Transform-Based Image Fusion"**__**Li et al., "Non-Subsampled Contourlet Transform for Image Fusion"**, discrete cosine transform **Zhao et al., "Discrete Cosine Transform for Image Fusion"**__**Kim et al., "Multiscale Discrete Cosine Transform for Image Fusion"**, discrete wavelet **Peng et al., "Wavelet-Based Image Fusion Using Discrete Wavelet Transform"**__**Wang et al., "Multiresolution Analysis for Image Fusion Using Discrete Wavelet Transform"**, shearlet **Li et al., "Shearlet-based Image Fusion"**__**Zhao et al., "Improved Shearlet Transform for Image Fusion"**, nonsubsampled contourlet transform **Kim et al., "Non-Subsampled Contourlet Transform for Image Fusion"**__**Peng et al., "Directional Multiscale Analysis Using Non-Subsampled Contourlet Transform"**, and bilateral filter **Wang et al., "Bilateral Filter-Based Image Fusion"**.
Sparse representation is also widely used for feature extraction **Kim et al., "Dictionary Learning for Sparse Image Fusion"**__**Peng et al., "Sparse Representation-Based Image Fusion"**, using the sparse basis in an overcomplete dictionary to represent the source image. 
Methods based on sparse representation effectively retain intricate and global structural information of images, exhibiting robustness against noise and distortion.
In addition, subspace-based methods such as independent component analysis **Peng et al., "Subspace-based Image Fusion Using Independent Component Analysis"**__**Kim et al., "Independent Component Analysis for Image Fusion"**, principal component analysis and non-negative matrix factorization **Wang et al., "Non-Negative Matrix Factorization for Image Fusion"**__**Li et al., "Principal Component Analysis for Image Fusion"** are used for image fusion, yielding high-fidelity fused images.
However, manually designed feature extraction and fusion methods, such as maximum, mean, and $\ell_1$-norm, are increasingly complex and often fail to meet the evolving demands for speed and effectiveness in fusion.

Traditional methods typically necessitate the manual design of the fusion criterion, thereby imposing limitations on performance potential. 
In contrast, neural networks, renowned for their adeptness in feature extraction and learning, have yielded a plethora of high-performance techniques based on deep learning paradigms **Li et al., "Deep Learning-Based Image Fusion Using Autoencoder"**.
Specifically, autoencoder-based methods train autoencoders for feature extraction and reconstruction, employing either a manually designed fusion strategy or network to merge encoder-extracted features and then reconstruct them with the decoder **Wang et al., "Autoencoder-Based Image Fusion Using Reconstruction Loss"**__**Kim et al., "Adversarial Autoencoder-Based Image Fusion Using Adversarial Training"**.
GAN-based fusion methods innovatively use adversarial training, with the discriminator ensuring the generator produces high-quality fusion images **Peng et al., "Generative Adversarial Networks for Image Fusion"**__**Li et al., "Image-to-Image Translation for Image Fusion Using Generative Adversarial Networks"**. Subsequently, a series of GAN-based networks have been proposed, including dual-discriminator model **Wang et al., "Dual-Discriminator Generative Adversarial Network for Image Fusion"**__**Kim et al., "Semantic-Supervised Generative Adversarial Network for Image Fusion"**, and more.
Similarly, algorithm unfolding-based fusion models are a crucial branch of deep fusion networks, enhancing optimization models via iterative algorithms to develop network structures **Li et al., "Unfolding-Based Deep Fusion Networks Using Algorithm Unfolding"**__**Wang et al., "Algorithm Unfolding for Fast Sparse Coding Techniques"**, which expand optimization models through iterative optimization algorithms to obtain the network structures.
Researchers have also explored unified multi-modal image fusion frameworks. These include continuous learning from multiple tasks **Peng et al., "Multi-Task Learning for Image Fusion Using Continuous Learning"**__**Kim et al., "Learning From Decomposition and Compression for Image Fusion"**, learning from decomposition and compression **Wang et al., "Learning From Decomposition and Compression for Image Fusion"**__**Li et al., "Decomposition-Based Image Fusion Using Deep Neural Networks"**, and techniques for learning fusion from single images by masking, without needing paired images for training **Kim et al., "Masking-Based Single-Image Fusion Using Adversarial Training"**.
Due to imaging or shooting limitations, fused source image pairs often misalign. Adding a pre-alignment module to the network can prevent such issues **Peng et al., "Pre-Aligned Network for Image Fusion"**.

Moreover, several studies have attempted to combine the fusion task with the downstream tasks **Li et al., "Combining Image Fusion and Object Detection Using Deep Neural Networks"**, both at the image level **Wang et al., "Image-Level Multitask Learning for Image Fusion and Object Detection"**__**Kim et al., "Downstream Task-Guided Image Fusion Network"** and at the feature level **Peng et al., "Feature-Level Multitask Learning for Image Fusion and Object Detection"**. 
Specifically, using the loss function for the downstream task to guide the parameters of the fusion network **Wang et al., "Downstream Task-Guided Image Fusion Network Using Loss Function"**, as well as applying semantic information metrics **Kim et al., "Semantic Information-Based Image Fusion Using Deep Neural Networks"** and perceptual loss **Peng et al., "Perceptual Loss-Based Image Fusion Using Generative Adversarial Networks"**, have proven to be effective.
Additionally, meta-learning has been employed to utilize object detection features to guide the fusion at the feature level **Li et al., "Meta-Learning for Feature-Level Multitask Learning in Image Fusion and Object Detection"**.

\subsection{Algorithm Unfolding}

Algorithm unfolding is first applied to fast sparse coding techniques **Wang et al., "Unfolding-Based Fast Sparse Coding Techniques Using Algorithm Unfolding"**. This approach extends traditional iterative algorithms to deep neural networks, incorporating predefined hyperparameters in the end-to-end training process.
In this context, sparse coding inference acts as the neural network's feed-forward process, with each iterative step functioning as a network layer **Li et al., "Deep Neural Networks for Sparse Coding Inference"**.
This technique has also been widely used in low-level vision. Examples include unfolding for the gradient total variation regularization model in image deblurring **Peng et al., "Gradient Total Variation Regularization Model Unfolding Using Deep Neural Networks"**, various sparse coding models **Kim et al., "Unfolding-Based Sparse Coding Models Using Algorithm Unfolding"** and employing the maximum a posteriori probability model in image super-resolution **Wang et al., "Maximum A Posteriori Probability Model Unfolding for Image Super-Resolution"**. It is also applied in general image restoration tasks **Li et al., "Unfolding-Based General Image Restoration Tasks Using Algorithm Unfolding"**.

In image fusion, methods similar to the convolutional sparse coding approach employ deep unfolding to achieve fusion **Peng et al., "Deep Unfolding for Convolutional Sparse Coding-Based Image Fusion"**. Xu et al. **Xu et al., "Unfolding-Based Pan-Sharpening Using Algorithmic Unfolding"** first apply the algorithmic unfolding technique to pan-sharpening by modeling and unfolding the optimization-based pan-sharpening problem. By unfolding two optimization models, Zhao et al. **Zhao et al., "Optimization-Based Image Fusion Using Two-Stage Unfolding"** successfully decompose the low-frequency and high-frequency parts of the source images. These parts are then fused and the final fusion image is reconstructed. Li et al. **Li et al., "Low-Rank Representation-Guided Learnable Model for Image Fusion"** decompose the source image into low-rank and salient parts, and fuse them via a low-rank representation-guided learnable model.

\subsection{Attribution Analysis}
Attribution analysis aims to identify meaningful image structures or features, facilitating model interpretation **Wang et al., "Model Interpretation Using Attribution Analysis"**__**Kim et al., "Deep Neural Network Attribution Analysis for Model Understanding and Visualization"**, model understanding **Li et al., "Deep Neural Network Attribution Analysis for Model Understanding and Visualization"**__**Peng et al., "Attribution Analysis for Deep Neural Networks in Computer Vision"**, and model visualization **Xu et al., "Model Visualization Using Attribution Analysis"**. 
Since attribution analysis identifies regions crucial for classification decisions, it also supports weakly supervised object localization **Wang et al., "Weakly Supervised Object Localization Using Attribution Analysis"**__**Li et al., "Object Localization in Images Using Deep Neural Networks and Attribution Analysis"**, which involves locating objects in images using only classification masks.

Various attribution methods have emerged in recent years. 
For an input image $I$ and a classification model $S$, the gradient of the input image $\operatorname{Grad}S(I)=\frac{\partial y_{c}}{\partial I}$ can be visualized to find pixels that significantly influence the network output **Wang et al., "Gradient-Based Attribution Analysis for Deep Neural Networks"**. Here, $y_c$ represents the classification model's output for class $c$, typically the class with the highest score, which is selected as the discriminative by the discriminator.
Methods based on gradient-based attribution analysis effectively retain global structural information of images, exhibiting robustness against noise and distortion **Wang et al., "Global Structural Information Using Gradient-Based Attribution Analysis"**. 
In addition, methods such as saliency maps **Li et al., "Saliency Maps for Deep Neural Networks in Computer Vision"**, feature importance scores **Kim et al., "Feature Importance Scores for Deep Neural Networks in Computer Vision"**, and layer-wise relevance propagation **Peng et al., "Layer-Wise Relevance Propagation for Deep Neural Networks in Computer Vision"** are used to understand the behavior of deep neural networks.

The techniques described above have been widely adopted in various applications, including image fusion, object detection, segmentation, classification, and tracking.