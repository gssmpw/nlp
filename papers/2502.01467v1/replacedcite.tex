\section{RELATED WORK}
\label{sec:2}
In this part, we review various image fusion algorithms for infrared and visible images, and briefly introduce the methods and applications of algorithm unfolding and attribution analysis.
\subsection{Image Fusion}
Recent image fusion techniques can be categorized into traditional methods and deep learning-based methods.
Traditional methods include multiscale transform methods____, sparse representation methods____, and subspace methods____.
Multiscale transformation methods leverage the decomposition features of the source images across varying scales to facilitate fusion. 
These methods aptly preserve the details of images while maintaining global consistency.
The commonly used methods encompass pyramid transform____, discrete cosine transform____, discrete wavelet____, shearlet____, nonsubsampled contourlet transform____ and bilateral filter____.
Sparse representation is also widely used for feature extraction____, using the sparse basis in an overcomplete dictionary to represent the source image. 
Methods based on sparse representation effectively retain intricate and global structural information of images, exhibiting robustness against noise and distortion.
In addition, subspace-based methods such as independent component analysis____, principal component analysis and non-negative matrix factorization____ are used for image fusion, yielding high-fidelity fused images.
However, manually designed feature extraction and fusion methods, such as maximum, mean, and $\ell_1$-norm, are increasingly complex and often fail to meet the evolving demands for speed and effectiveness in fusion.

Traditional methods typically necessitate the manual design of the fusion criterion, thereby imposing limitations on performance potential. 
In contrast, neural networks, renowned for their adeptness in feature extraction and learning, have yielded a plethora of high-performance techniques based on deep learning paradigms____.
Specifically, autoencoder-based methods train autoencoders for feature extraction and reconstruction, employing either a manually designed fusion strategy or network to merge encoder-extracted features and then reconstruct them with the decoder____.
GAN-based fusion methods innovatively use adversarial training, with the discriminator ensuring the generator produces high-quality fusion images____. Subsequently, a series of GAN-based networks have been proposed, including dual-discriminator model____, semantic-supervised model____, and more.
Similarly, algorithm unfolding-based fusion models are a crucial branch of deep fusion networks, enhancing optimization models via iterative algorithms to develop network structures____, which expand optimization models through iterative optimization algorithms to obtain the network structures.
Researchers have also explored unified multi-modal image fusion frameworks. These include continuous learning from multiple tasks____, learning from decomposition and compression____, and techniques for learning fusion from single images by masking, without needing paired images for training____.
Due to imaging or shooting limitations, fused source image pairs often misalign. Adding a pre-alignment module to the network can prevent such issues____.

Moreover, several studies have attempted to combine the fusion task with the downstream tasks____, both at the image level____ and at the feature level____. 
Specifically, using the loss function for the downstream task to guide the parameters of the fusion network____, as well as applying semantic information metrics____ and perceptual loss____, have proven to be effective.
Additionally, meta-learning has been employed to utilize object detection features to guide the fusion at the feature level ____.

\subsection{Algorithm Unfolding}

Algorithm unfolding is first applied to fast sparse coding techniques____. This approach extends traditional iterative algorithms to deep neural networks, incorporating predefined hyperparameters in the end-to-end training process.
In this context, sparse coding inference acts as the neural network's feed-forward process, with each iterative step functioning as a network layer____.
This technique has also been widely used in low-level vision. Examples include unfolding for the gradient total variation regularization model in image deblurring____, various sparse coding models____, and employing the maximum a posteriori probability model in image super-resolution____. It is also applied in general image restoration tasks____.

In image fusion, methods similar to the convolutional sparse coding approach employ deep unfolding to achieve fusion____. Xu et al.____ first apply the algorithmic unfolding technique to pan-sharpening by modeling and unfolding the optimization-based pan-sharpening problem. By unfolding two optimization models, Zhao et al.____ successfully decompose the low-frequency and high-frequency parts of the source images. These parts are then fused and the final fusion image is reconstructed. Li et al.____ decompose the source image into low-rank and salient parts, and fuse them via a low-rank representation-guided learnable model.

\subsection{Attribution Analysis}
Attribution analysis aims to identify meaningful image structures or features, facilitating model interpretation____, model understanding____ and model visualization____. 
Since attribution analysis identifies regions crucial for classification decisions, it also supports weakly supervised object localization____, which involves locating objects in images using only classification masks.

Various attribution methods have emerged in recent years. 
For an input image $I$ and a classification model $S$, the gradient of the input image $\operatorname{Grad}S(I)=\frac{\partial y_{c}}{\partial I}$ can be visualized to find pixels that significantly influence the network output____. Here, $y_c$ represents the classification model's output for class $c$, typically the class with the highest score, which is selected as the discriminative result by the network.
The element-wise product of the input and its gradient $I \odot \frac{\partial S(I)}{\partial I}$, addresses the gradient saturation issue____. 
Then, CAM____ and Grad-CAM____ use the gradient of the convolutional features in the last layer of the model to locate the regions and pixels that are most relevant to the output. 
Integrated Gradients (IG)____ introduces a baseline image $I^{\prime}$ and integrates the gradient along a path that gradually transitions from the baseline image to the target image $\left(I-I^{\prime}\right) \cdot \int_0^1 \frac{\partial S\left(I^{\prime}+\alpha\left(I-I^{\prime}\right)\right)}{\partial I} d \alpha$ to obtain the attribution map. 
The gradient is the network output relative to the intermediate image of the path. Attribution analysis methods such as Grad-CAM____ used for classification can also be extended to pattern recognition tasks. 
The attribution map for a semantic segmentation network____ is computed by replacing the class output $y_c$ in the classification model with $\sum_{(i, j) \in \mathcal{M}} y_{ij}^c$, analyzing pixels $(i,j)$ within set $\mathcal{M}$, where $\mathcal{M}$ is the set of pixels to analyze and $y_{i j}^c$ is the $c$-th class score on pixel $(i,j)$. 
Similarly, the objectivity and classification scores in the output of the object detection network are subjected to the attribution operator, and the detection network can be likewise analyzed____. 
For tasks like super-resolution, which do not involve classification, attribution analysis is performed by analyzing local gradients $D_{x y}(I)=\sum_{i \in[x, x+l], j \in[y, y+l]} \nabla_{i j} I$ in the generated images____, where $i,j$ are the coordinates and $l$ is the region size.




\begin{figure*}[!]
	\centering
	\includegraphics[width=\linewidth]{Figure/workflow.pdf}
	\caption{{Illustration of UAAFusion.} UAAFusion consists of multiple stages, each stage contains three parts, \textit{Iterative Fusion Module} (blue) for feature extraction as well as processing, the \textit{Attribution Analysis Module} (green) for calculating the attribution weight and attribution map, and the \textit{Memory Augmentation Module} (orange) for enhancing the flow of information between stages.}
	\label{fig:AUIF_net}
\end{figure*}