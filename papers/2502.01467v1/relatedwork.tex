\section{RELATED WORK}
\label{sec:2}
In this part, we review various image fusion algorithms for infrared and visible images, and briefly introduce the methods and applications of algorithm unfolding and attribution analysis.
\subsection{Image Fusion}
Recent image fusion techniques can be categorized into traditional methods and deep learning-based methods.
Traditional methods include multiscale transform methods~\cite{DBLP:journals/sigpro/LiuJWSD14,DBLP:journals/ijon/LiuMD17,DBLP:journals/isci/0019LLM020}, sparse representation methods~\cite{DBLP:journals/tip/LiWK20,DBLP:journals/spl/LiuCWW16}, and subspace methods~\cite{DBLP:conf/icip/CvejicLBC06}.
Multiscale transformation methods leverage the decomposition features of the source images across varying scales to facilitate fusion. 
These methods aptly preserve the details of images while maintaining global consistency.
The commonly used methods encompass pyramid transform~\cite{BULANON200912}, discrete cosine transform~\cite{JIN20181, DBLP:journals/mta/EKR19}, discrete wavelet~\cite{DBLP:journals/sigpro/LiuJWSD14}, shearlet~\cite{ 
DBLP:journals/ijon/LiuMD17}, nonsubsampled contourlet transform~\cite{XIANG201553} and bilateral filter~\cite{DBLP:journals/inffus/HuL12}.
Sparse representation is also widely used for feature extraction~\cite{DBLP:journals/tip/LiWK20,DBLP:journals/spl/LiuCWW16}, using the sparse basis in an overcomplete dictionary to represent the source image. 
Methods based on sparse representation effectively retain intricate and global structural information of images, exhibiting robustness against noise and distortion.
In addition, subspace-based methods such as independent component analysis~\cite{DBLP:conf/icip/CvejicLBC06}, principal component analysis and non-negative matrix factorization~\cite{DBLP:conf/icip/ZhangWMW04} are used for image fusion, yielding high-fidelity fused images.
However, manually designed feature extraction and fusion methods, such as maximum, mean, and $\ell_1$-norm, are increasingly complex and often fail to meet the evolving demands for speed and effectiveness in fusion.

Traditional methods typically necessitate the manual design of the fusion criterion, thereby imposing limitations on performance potential. 
In contrast, neural networks, renowned for their adeptness in feature extraction and learning, have yielded a plethora of high-performance techniques based on deep learning paradigms~\cite{DBLP:conf/aaai/Xu0LJG20,DBLP:journals/inffus/ZhangLSYZZ20}.
Specifically, autoencoder-based methods train autoencoders for feature extraction and reconstruction, employing either a manually designed fusion strategy or network to merge encoder-extracted features and then reconstruct them with the decoder~\cite{DBLP:journals/tip/LiW19,DBLP:journals/inffus/LiWK21,DBLP:journals/tim/LiWD20,DBLP:conf/ijcai/ZhaoXZLZL20,DBLP:journals/corr/abs-2211-14461}.
GAN-based fusion methods innovatively use adversarial training, with the discriminator ensuring the generator produces high-quality fusion images~\cite{DBLP:journals/inffus/MaYLLJ19,DBLP:journals/inffus/MaLYCGWJ20}. Subsequently, a series of GAN-based networks have been proposed, including dual-discriminator model~\cite{DBLP:journals/tip/MaXJMZ20}, semantic-supervised model~\cite{DBLP:journals/tmm/ZhouWZML23}, and more.
Similarly, algorithm unfolding-based fusion models are a crucial branch of deep fusion networks, enhancing optimization models via iterative algorithms to develop network structures~\cite{DBLP:journals/tcsv/ZhaoXZLZL22,DBLP:journals/pami/0002D21,DBLP:journals/corr/abs-2005-08448}, which expand optimization models through iterative optimization algorithms to obtain the network structures.
Researchers have also explored unified multi-modal image fusion frameworks. These include continuous learning from multiple tasks~\cite{DBLP:journals/pami/XuMJGL22}, learning from decomposition and compression~\cite{DBLP:journals/ijcv/ZhangM21}, and techniques for learning fusion from single images by masking, without needing paired images for training~\cite{DBLP:conf/eccv/LiangJLM22}.
Due to imaging or shooting limitations, fused source image pairs often misalign. Adding a pre-alignment module to the network can prevent such issues~\cite{DBLP:conf/eccv/HuangLFLZL22,DBLP:conf/ijcai/WangLFL22,DBLP:conf/cvpr/Xu0YLL22}.

Moreover, several studies have attempted to combine the fusion task with the downstream tasks~\cite{DBLP:conf/cvpr/LiuFHWLZL22}, both at the image level~\cite{DBLP:journals/inffus/TangYM22,DBLP:conf/cvpr/LiuFHWLZL22,DBLP:journals/tmm/ZhouWZML23} and at the feature level~\cite{zhao2023metafusion}. 
Specifically, using the loss function for the downstream task to guide the parameters of the fusion network~\cite{DBLP:journals/inffus/TangYM22}, as well as applying semantic information metrics~\cite{DBLP:journals/tmm/ZhouWZML23} and perceptual loss~\cite{DBLP:journals/inffus/LiWK21,DBLP:journals/tip/LiW19,DBLP:journals/inffus/ZhangLSYZZ20}, have proven to be effective.
Additionally, meta-learning has been employed to utilize object detection features to guide the fusion at the feature level ~\cite{zhao2023metafusion}.

\subsection{Algorithm Unfolding}

Algorithm unfolding is first applied to fast sparse coding techniques~\cite{DBLP:conf/icml/GregorL10}. This approach extends traditional iterative algorithms to deep neural networks, incorporating predefined hyperparameters in the end-to-end training process.
In this context, sparse coding inference acts as the neural network's feed-forward process, with each iterative step functioning as a network layer~\cite{DBLP:conf/cvpr/ZhaoZXLP22}.
This technique has also been widely used in low-level vision. Examples include unfolding for the gradient total variation regularization model in image deblurring~\cite{DBLP:journals/tci/LiTGME20}, various sparse coding models~\cite{deng2019deep,DBLP:journals/tip/MarivaniTCD20}, and employing the maximum a posteriori probability model in image super-resolution~\cite{yang2022memory,DBLP:conf/cvpr/ZhangGT20}. It is also applied in general image restoration tasks~\cite{DBLP:conf/cvpr/ZhangZGZ17,DBLP:journals/pami/DongWYSWL19,DBLP:conf/iccv/LiuPRS19,DBLP:journals/ijcv/ZhouYPRXC23}.

In image fusion, methods similar to the convolutional sparse coding approach employ deep unfolding to achieve fusion~\cite{DBLP:journals/pami/0002D21,DBLP:journals/corr/abs-2005-08448}. Xu et al.~\cite{DBLP:conf/cvpr/Xu0ZSL021} first apply the algorithmic unfolding technique to pan-sharpening by modeling and unfolding the optimization-based pan-sharpening problem. By unfolding two optimization models, Zhao et al.~\cite{DBLP:journals/tcsv/ZhaoXZLZL22} successfully decompose the low-frequency and high-frequency parts of the source images. These parts are then fused and the final fusion image is reconstructed. Li et al.~\cite{li2023lrrnet} decompose the source image into low-rank and salient parts, and fuse them via a low-rank representation-guided learnable model.

\subsection{Attribution Analysis}
Attribution analysis aims to identify meaningful image structures or features, facilitating model interpretation~\cite{DBLP:conf/cvpr/ShenGTZ20}, model understanding~\cite{DBLP:conf/iclr/BauZSZTFT19} and model visualization~\cite{DBLP:journals/corr/SimonyanVZ13,DBLP:journals/corr/SpringenbergDBR14,DBLP:conf/eccv/ZeilerF14}. 
Since attribution analysis identifies regions crucial for classification decisions, it also supports weakly supervised object localization~\cite{DBLP:conf/cvpr/ZhouKLOT16,DBLP:conf/iccv/SelvarajuCDVPB17,DBLP:journals/corr/abs-1805-11393}, which involves locating objects in images using only classification masks.

Various attribution methods have emerged in recent years. 
For an input image $I$ and a classification model $S$, the gradient of the input image $\operatorname{Grad}S(I)=\frac{\partial y_{c}}{\partial I}$ can be visualized to find pixels that significantly influence the network output~\cite{DBLP:journals/jmlr/BaehrensSHKHM10,DBLP:journals/corr/SimonyanVZ13}. Here, $y_c$ represents the classification model's output for class $c$, typically the class with the highest score, which is selected as the discriminative result by the network.
The element-wise product of the input and its gradient $I \odot \frac{\partial S(I)}{\partial I}$, addresses the gradient saturation issue~\cite{DBLP:conf/icml/SundararajanTY17}. 
Then, CAM~\cite{DBLP:conf/cvpr/ZhouKLOT16} and Grad-CAM~\cite{DBLP:conf/iccv/SelvarajuCDVPB17} use the gradient of the convolutional features in the last layer of the model to locate the regions and pixels that are most relevant to the output. 
Integrated Gradients (IG)~\cite{DBLP:conf/icml/SundararajanTY17} introduces a baseline image $I^{\prime}$ and integrates the gradient along a path that gradually transitions from the baseline image to the target image $\left(I-I^{\prime}\right) \cdot \int_0^1 \frac{\partial S\left(I^{\prime}+\alpha\left(I-I^{\prime}\right)\right)}{\partial I} d \alpha$ to obtain the attribution map. 
The gradient is the network output relative to the intermediate image of the path. Attribution analysis methods such as Grad-CAM~\cite{DBLP:conf/iccv/SelvarajuCDVPB17} used for classification can also be extended to pattern recognition tasks. 
The attribution map for a semantic segmentation network~\cite{DBLP:conf/aaai/VinogradovaDM20} is computed by replacing the class output $y_c$ in the classification model with $\sum_{(i, j) \in \mathcal{M}} y_{ij}^c$, analyzing pixels $(i,j)$ within set $\mathcal{M}$, where $\mathcal{M}$ is the set of pixels to analyze and $y_{i j}^c$ is the $c$-th class score on pixel $(i,j)$. 
Similarly, the objectivity and classification scores in the output of the object detection network are subjected to the attribution operator, and the detection network can be likewise analyzed~\cite{DBLP:journals/corr/abs-2211-12108}. 
For tasks like super-resolution, which do not involve classification, attribution analysis is performed by analyzing local gradients $D_{x y}(I)=\sum_{i \in[x, x+l], j \in[y, y+l]} \nabla_{i j} I$ in the generated images~\cite{DBLP:conf/cvpr/GuD21}, where $i,j$ are the coordinates and $l$ is the region size.




\begin{figure*}[!]
	\centering
	\includegraphics[width=\linewidth]{Figure/workflow.pdf}
	\caption{{Illustration of UAAFusion.} UAAFusion consists of multiple stages, each stage contains three parts, \textit{Iterative Fusion Module} (blue) for feature extraction as well as processing, the \textit{Attribution Analysis Module} (green) for calculating the attribution weight and attribution map, and the \textit{Memory Augmentation Module} (orange) for enhancing the flow of information between stages.}
	\label{fig:AUIF_net}
\end{figure*}