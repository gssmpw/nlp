\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{ragged2e}
\usepackage{multirow}
\usepackage[symbol]{footmisc}
\newcommand\mytodo[1]{\textcolor{red}{#1}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{defn}{Definition}
\newtheorem{conj}{Conjecture}
\newtheorem{exmp}{Example}
\newtheorem{rem}{Remark}

\usepackage{algorithm}
\usepackage{algorithmic}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\usepackage{newfloat}
\usepackage{placeins}
\usepackage{listings}



\title{A Survey on Active Feature Acquisition Strategies}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{\hspace{1mm}Arman Rahbar$^*$ \\
	Chalmers University of Technology and University of Gothenburg\\
	Gothenburg, Sweden\\
	\texttt{armanr@chalmers.se} \\
	\And
	\hspace{1mm}Linus Aronsson$^*$ \\
	Chalmers University of Technology and University of Gothenburg\\
	Gothenburg, Sweden\\
	\texttt{linaro@chalmers.se} \\
	\AND
        \hspace{1mm}Morteza Haghir Chehreghani \\
	Chalmers University of Technology and University of Gothenburg\\
	Gothenburg, Sweden\\
	\texttt{morteza.chehreghani@chalmers.se} \\
}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{A Survey on Active Feature Acquisition Strategies}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work.}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
Active feature acquisition studies the challenge of making accurate predictions while limiting the cost of collecting complete data. By selectively acquiring only the most informative features for each instance, these strategies enable efficient decision-making in scenarios where data collection is expensive or time-consuming. This survey reviews recent progress in active feature acquisition, discussing common problem formulations, practical challenges, and key insights. We also highlight open issues and promising directions for future research.
\end{abstract}
\section{Introduction}
Most machine learning algorithms, such as those used for classification, rely on features for making predictions. Many real-world datasets are high-dimensional, containing a large number of features. In such cases, obtaining complete information for every data point can be impractical due to constraints such as cost or time. For example, in the context of medical diagnosis, acquiring all possible diagnostic tests for every patient may not only be expensive and time-consuming but could also cause unnecessary discomfort to patients. Another example is personalized marketing, where users are classified into various categories based on their behavior. Collecting comprehensive data on every user's behavior can be costly and may also raise privacy concerns.
Active feature acquisition (AFA), or dynamic feature selection, addresses this challenge by sequentially selecting a subset of features tailored to each instance, thereby balancing acquisition costs against prediction accuracy. Unlike traditional static feature selection, which selects a fixed subset of features for all instances, AFA tailors the feature acquisition process to each instance by dynamically selecting the most informative features to query. To determine the next feature to query, a selection \emph{policy} is required (see Section \ref{sec:problem_formulation}).  
Various approaches have been proposed to learn this policy for the AFA problem. A natural formulation of this problem is as a Markov Decision Process (MDP), where reinforcement learning (RL) methods are employed. In these methods, each data point is treated as an episode of RL. Several studies have adopted this RL-based framework to optimize feature acquisition policies.  
Alternatively, other approaches focus on designing greedy policies that aim to optimize an objective function at each step of the feature acquisition process. Additionally, another line of research embeds feature selection directly into the model's inference phase, integrating the selection process with the prediction task.  
In this paper, we discuss and compare these different approaches.

This paper is structured as follows:  
Section \ref{sec:problem_formulation} presents a formal definition of the AFA problem. Sections \ref{section:greedy}-\ref{sec:rl} discuss the key approaches addressing the AFA problem. Finally, Section \ref{sec:conclusion} compares these approaches and outlines future research directions.


\section{Problem Formulation} \label{sec:problem_formulation}
We begin with a general formulation of the AFA problem. The formulation presented here is mainly adapted from \citep{dulac2011datum, gadgil2024estimating, li2024distributionguidedactivefeature}. 

Consider a data point $\mathbf{x}$\footnote{In this paper, we represent random variables using bold symbols, while their possible values are shown in regular font. A data point $\mathbf{x}$ consists of $n$ features that are not known a priori. Thus, $\mathbf{x}$ is treated as a random vector of $n$ random variables.}, represented as a vector of $n$ features, i.e., $\mathbf{x} = (\mathbf{x}_1, \dots, \mathbf{x}_n)$. In AFA, the goal is to select a low-cost subset of features (as acquiring the value of a feature incurs some cost) from $\mathbf{x}$ to make an accurate target prediction $y$ for the given instance. The prediction is made using a predictor $f$, which operates on a selected subset of features from $\mathbf{x}$. Specifically, if $S\subseteq \{1, \dots , n\}$ denotes a subset of the $n$ features, the prediction is given by $f(\mathbf{x}_S)$, where $\mathbf{x}_S$ represents the instance restricted to the features in $S$, i.e., $\mathbf{x}_S = \{\mathbf{x}_i \mid i \in S\}$. This is different from the (static) feature selection, where features are provided for the \emph{entire} dataset in advance rather than on a per-instance basis. Ultimately, in AFA, the objective is to find a solution to the following optimization problem (across many data points):
%
\begin{equation} \label{eq:afa}
    \min_{S \subseteq \{1,\dots,n\}} \, l(f(\mathbf{x}_S), \mathbf{y}) + \alpha c(S),
\end{equation}
%
\noindent where \( \mathbf{y} \) denotes the random variable representing the true value of the target, \( l \) is a loss function measuring the accuracy of the target value predicted by \( f \), \( c(S) = \sum_{i \in S} c_i \) (with \( c_i \) being the cost of obtaining the value for feature \( i \)), and \( \alpha \) is a balancing constant. Since the set of selected features \( S \) can vary from one instance to another, the prediction function \( f \) must be capable of producing accurate predictions using any arbitrary subset of features. Moreover, \( f \) may either be pre-trained or optimized jointly with the feature selection process. In the latter case, Eq.~(\ref{eq:afa}) is an optimization problem w.r.t. both \( f \) and the selection of features \( S \). The specific formulation depends on the chosen approach and will be discussed in detail throughout the survey.

%Since the set of selected features $S$ can vary for each instance, the prediction function $f$ must be able to make predictions with arbitrary subsets. In addition, $f$ may either be assumed to be pre-trained or as a part of the optimization problem in Eq. (\ref{eq:afa}). In this case, the optimization is w.r.t. both $f$ and the selection of features $S$. This depends on the approach, and will be discussed in detail throughout the survey.

To select the subset $S$ for each instance, we require a policy $\pi$ (e.g., through a policy neural network) that takes the currently selected features and outputs the next feature (i.e., $\pi(\mathbf{x}_S) \in [n]$), continuing until sufficient information is available to make an accurate prediction using $f$. The primary distinction among the methods proposed in the literature lies in the design of their feature selection policies. 
In the following sections, we provide a detailed overview of these methods, highlighting their underlying principles and comparative advantages.


\section{Greedy Active Feature Acquisition} \label{section:greedy}

The greedy approach to the AFA problem involves defining an objective function that quantifies the informativeness of a feature in the context of a prediction task \footnote{When features have non-uniform costs, there is a trade-off between the cost and the informativeness of a feature, which should be balanced during the AFA process. See Section \ref{sec:ec2}
for more details.}. For each data point, the aim is to optimize this objective function using a greedy strategy.

We show the objective function by $F$ which takes a feature index and the previously observed feature values as input, and produces a real number that represents the informativeness of the feature corresponding to that index, given the observed features. Importantly, since AFA is performed sequentially for each data point, the function $F$ considers the previously observed features, represented by $x_S$, which is a realization of the random vector $\mathbf{x}_S$. Using this function, the selection policy $\pi$ can be expressed as:
%
\begin{equation}
\pi(x_S) = \argmax_{i\in[n]} F(i, x_S).
\end{equation}
%
This policy selects the feature index $i$ that maximizes the objective function $F$, ensuring that the most informative feature is acquired at each step. Here, we assume that high values of $F$ correspond to more information in the feature. 
% If this is not the case, the policy should maximize $-F$. 
The acquisition process continues until there is no budget left or a predefined threshold on informativeness is met.
In what follows, we go over two important objective functions that have been used in the literature.

\subsection{Conditional Mutual Information}
One of the most common objective functions for the AFA problem is conditional mutual information (CMI). In CMI, we measure the mutual information between the features and the target variable. Formally, the ideal selection policy is a policy that maximizes the mutual information between the target variable $\mathbf{y}$ and a feature $\mathbf{x}_i$ given the already observed features $x_S$. This conditional mutual information, denoted as $I(\mathbf{y}; \mathbf{x}_i \mid x_S)$, is defined using the following KL-divergence \citep{cover2012elements, covert2023learning}:
%
\begin{equation} 
\label{eq:kl}
I(\mathbf{y}; \mathbf{x}_i \mid x_S) = D_{\text{KL}} \big(p(\mathbf{x}_i, \mathbf{y} \mid x_S) \,\big\|\, p(\mathbf{x}_i \mid x_S)p(\mathbf{y} \mid x_S)\big).
\end{equation}
%
Then, the selection policy will be:
%
\begin{equation}
\label{policy:cmi}
\pi(x_S) = \argmax_{i\in[n]} I(\mathbf{y}; \mathbf{x}_i \mid x_S).
\end{equation}
%
In the noise-free setting, where feature values are deterministic functions of the target variable, it can be shown that the greedy policy in Eq.~\eqref{policy:cmi} is near-optimal (see, for instance, \cite{dasgupta2004analysis}).

To our knowledge, the first theoretical analysis of the performance of greedy policy in Eq.~\eqref{policy:cmi} under noisy feature values is presented in \cite{chen2015sequential}. Specifically, the authors in \cite{chen2015sequential} show that the greedy policy behaves near-optimally under common assumptions about noise and provide a lower bound on the expected amount of information that the greedy policy gives about the target variable upon completing feature selection.

In practice, implementing the ideal policy described in Eq.~\eqref{policy:cmi} is challenging. The reason is that, at each step, the execution of this policy requires computing $I(\mathbf{y}; \mathbf{x}_i \mid x_S)$ for all the feature indices $i$. This computation depends on access to the distributions of the target variable and the features, conditioned on the observed feature values. 
One common method %to estimate the distributions required for computing $I(\mathbf{y}; \mathbf{x}_i \mid x_S)$ involves making
is to make simplifying assumptions about the underlying data distributions. For example, \cite{geman1996active} employ a CMI policy for the task of tracking roads in satellite images, where the authors assume the conditional independence of features given the target variable, allowing them to compute the required distributions from the data.

\paragraph{Generative methods.} An alternative approach to implementing the CMI policy is through \emph{generative} modeling. In \cite{ma2019eddi}, a novel Variational AutoEncoder (VAE) capable of handling partial inputs is proposed. This framework, named EDDI, involves training the partial VAE on a given dataset. Once trained, the partial VAE approximates the required data distributions, such as \( p(\mathbf{x}_i \mid x_S) \), by sampling a latent variable from the observed features, which is then used to sample \(\mathbf{x}_i\). This approximation often enables analytical computation of the CMI, for instance, using Gaussian parameterization. When analytical computation is not feasible, Monte Carlo sampling is employed as an alternative. 
In \cite{chattopadhyay2022interpretable}, a similar approach has been used to implement the CMI policy, which also uses a VAE together with an MCMC algorithm.

The work of \cite{RangrejC21} adapts the partial VAE proposed in \cite{ma2019eddi} to perform prediction on images. In particular, in their work, the prediction on images is performed using a sequence of partial observations of an image. A hard attention model \citep{pmlr-v37-xuc15}  is proposed to perform prediction on the sequence of observed scenes. A hard attention model makes predictions about images by attending to specific parts of an image, and in \cite{RangrejC21}, these image parts are selected in an active manner starting from a random location, with the next location found using a partial VAE.


One limitation of the framework proposed in \cite{ma2019eddi} is the necessity of a training phase prior to performing active feature selection. To address scenarios where obtaining training data is challenging or costly, a related method is introduced in \cite{NEURIPS2019_c055dcc7}. This method incorporates training-time active acquisition, wherein training data features are selected actively during the training process. To achieve this, the authors propose a Bayesian generative model that employs a Bayesian approach for learning the weights, thereby enabling active acquisition throughout the training phase. 
Another limitation of EDDI is the computational cost involved in the estimation of CMI with the partial VAE. In \cite{bsoda2022}, a Product-of-Experts (PoE) encoder \citep{wu2018multimodal} is employed instead of the partial VAE. The PoE encoder leverages the final predictor model to approximate the posterior distribution of the latent variable. Utilizing the PoE encoder for posterior inference allows for a reduction in the computational cost of estimating CMI, particularly in scenarios with feature sparsity (e.g., disease diagnosis using a sequence of symptoms).

\paragraph{Discriminative methods.} The generative models proposed to estimate the CMI are usually expensive to train, and thus some of the recent works have suggested directly predicting the next feature (i.e., the feature with maximum CMI) to query. This is usually referred to as \emph{discriminative} modeling in the literature \citep{chattopadhyay2023variational, gadgil2024estimating} (compare to generative and discriminative classification models \citep{ng2001discriminative}). 

In \cite{chattopadhyay2023variational}, a variational perspective on CMI is proposed, which enables the removal of the need for training generative models. In essence, their framework minimizes the KL-divergence between the conditional distribution of the target variable and its posterior distribution (the distribution of the target variable given the observed features along with the new feature) by optimizing over various policies parameterized by deep networks. They also show that the solution to their variational optimization problem recovers the policy in Eq.~\eqref{policy:cmi}. A classifier network is used to estimate the posterior distribution, and thus the optimization is performed over both the policy and classifier networks. 

Concurrent with \cite{chattopadhyay2023variational}, a similar variational approach was proposed in \cite{covert2023learning}. In the variational perspective of \cite{covert2023learning}, an optimization problem is defined based on the accuracy of one-step-ahead prediction achieved by the policy. Concretely, if $i$ is the next feature selected by the policy given $x_{S}$, they aim to minimize the expected value of $l(f(x_{S},\mathbf{x}_i), \mathbf{y})$, where $l$ is a loss function measuring the discrepancy between the target and the prediction. They show that the predictor $f$ that minimizes this objective function is the Bayes classifier, i.e., $f^*(x_{S}) = p(\mathbf{y}|x_{S})$, and for this classifier, the policy that minimizes the expected one-step-ahead loss is the policy in Eq.~\eqref{policy:cmi}. They solve the optimization problem using amortized optimization \citep{amor2022}. They first cast their variational perspective as an objective function and then optimize it using a deep neural network.

More recently, another method was proposed in \cite{gadgil2024estimating} to estimate the CMI policy. Their work suggests directly estimating the CMI value (i.e., Eq.~\eqref{eq:kl}) in a discriminative manner. Based on variational methods introduced in \cite{chattopadhyay2023variational, covert2023learning}, they propose a novel learning objective and show that the optimal solution to their optimization problem yields the true value of CMI. 
To implement their method in practice, they utilize two deep networks: a predictor network and a value network. The value network is designed to predict the CMI value for unobserved features. Once trained, the value network can be used to select the next feature to query. The value and predictor networks are trained jointly, leveraging their result that when the predictor is a Bayes classifier, the expected improvement in the cross-entropy loss of prediction when adding feature $i$ corresponds to the CMI value for feature $i$.
This method also allows for variable budgets across different samples as well as non-identical feature costs, which were not supported by earlier discriminative methods.

% TODO: create a table for comparison
% Add ec2 and related works

\subsection{Equivalence Class Edges Cut}
\label{sec:ec2}
In various practical applications, different features in the test data points may have varying acquisition costs. In such cases, one should balance the incurred cost and the information gained by querying the value of a feature. Then, the feature selection policy for the CMI objective can be defined as 
$\pi(x_S) = \argmax_{i\in[n]} I(\mathbf{y}; \mathbf{x}_i \mid x_S)/c_i$,
where $c_i$ is the cost of querying the feature with index $i$ \citep{gadgil2024estimating}. However, performance guarantees (in relation to an optimal policy) do not exist for this modified policy in the noisy setting. However, a similar greedy policy achieves near-optimal performance for an objective function that is \emph{adaptive submodular} \citep{golovin2011adaptive}. 

In \cite{golovin2010near}, a novel objective function is introduced that satisfies adaptive submodularity. This objective function relies on the notion of \emph{hypothesis}. In the AFA problem, a hypothesis corresponds to a full realization of all features, e.g., for $n$ binary features, there are $2^n$ hypotheses. In the noisy setting, each value of the target variable $y$ corresponds to a subset of all possible hypotheses, where this subset is referred to as an \emph{equivalence class} in \cite{golovin2010near}. To identify the underlying target variable (or equivalence class), the Equivalence Class Edge Cutting (EC$^2$) algorithm is proposed. This algorithm operates on a graph where the nodes represent hypotheses, and there is an edge between two hypotheses if they belong to different equivalence classes. When a feature value is queried, all hypotheses that are inconsistent with the observed feature value are removed, along with the edges connected to those hypotheses (i.e., the query cuts those edges). Each edge between two hypotheses is assigned a weight based on the probability of those hypotheses. The EC$^2$ objective is then defined as the sum of the weights of edges that are cut during the AFA procedure. This objective function is shown to be adaptive submodular, and thus greedily maximizing it ensures near-optimal performance. 

Similar to the CMI policy, implementing the EC$^2$ policy requires access to the probability distributions of features and target variables (e.g., to compute the probability of a hypotheses). Another limitation of EC$^2$ is that it is only applicable to problems with a finite number of feature values (to enumerate the hypotheses). 
In \cite{uaivoi2017}, EC$^2$ is utilized within a framework for \emph{online} troubleshooting where data points are received in a stream. 
To achieve this, they employ a Thompson sampling \citep{thompson1933likelihood} scheme for online learning. Specifically, they assume a prior distribution over the parameters of feature %(test outcome) 
distributions conditioned on the target variable, as well as a prior distribution over the target variable itself. At each time step in the data stream, parameters are sampled from their posterior distributions, and the EC$^2$ algorithm is executed using the sampled parameters. At the end of each time step, the posterior distributions of the parameters are updated using the feature values queried during that time step. They also provide a theoretical analysis of the performance of their framework. 
% by formulating the online learning problem as a Partially Observable Markov Decision Process (POMDP).
In \cite{ijcai2023p463}, a similar online framework is proposed to perform AFA with EC$^2$ in general classification problems.
Their online learning framework enables them to handle concept drift, where the dependency of features on the target variable changes over time. Additionally, they propose extensions of the EC$^2$ policy to handle features with infinite possible values (real-valued features). 
Finally, a novel approach is proposed in \cite{rahbar2025costefficient}, where online AFA is formulated as a combinatorial multi-armed bandit (CMAB) problem \citep{cesa2012combinatorial}. This CMAB formulation allows them to address a more general problem where the cost of acquiring feature values is stochastic and depends on both the feature value itself and the target value. They also provide a theoretical analysis of their framework, which provides an upper bound on its performance that is linear in the number of features.

% Explain EC2, its extensions together with ijcai, tmlr papers.

\section{Embedded Methods for Active Feature Acquisition}
\label{sec:embedded}
Another significant line of work in AFA involves methods that learn to actively select features as part of the learning algorithm. These methods are designed to embed the feature acquisition process within the prediction process, allowing the algorithm to decide which features to acquire at each step based on their potential contribution to improving predictive performance. In what follows, we review some of the important works in this category.

One of the earliest works in this category of AFA methods is the work of \cite{ling2004decision}, in which the authors propose a training and prediction framework for decision trees that simultaneously minimizes misclassification and test costs. In classical decision tree learning methods like C4.5 \citep{quinlan1993c4.5}, the cost of obtaining the value of a feature is not considered during training. In contrast, the decision tree learning method proposed in \cite{ling2004decision} incorporates the cost of obtaining feature values into the training process and, based on that, chooses the next feature value to query. In particular, at each step, their method selects the feature that minimizes the total cost, i.e., the sum of feature and misclassification costs (instead of minimizing entropy in C4.5). Their training procedure is also capable of handling datasets with missing features. 

Another cost-sensitive method based on decision trees was proposed in \cite{miser2012}. In their approach, the final classifier is built from multiple decision trees. The method seeks to optimize a global objective function that is non-continuous and depends on feature costs during training, which in turn minimizes prediction costs. The global objective is designed to balance the trade-off between feature costs and classification accuracy. To implement this algorithm, the authors relax the objective into a continuous loss function and optimize it using a greedy optimization strategy. 

AFA has also been applied to random forests. In \cite{2015forest}, a random forest learner was proposed to minimize prediction error while considering the \emph{average} cost of acquiring features, as specified by the user. In classical random forest learners, maintaining a diverse set of final trees is desirable, as diversity enhances classifier performance. However, high diversity often leads to increased feature costs since different trees within the forest tend to utilize different features. To address this, \cite{2015forest} proposes learning decision trees using minimax cost-weighted impurity splits to reduce cost. The authors also show that their algorithm achieves near-optimal cost. Later,
in \cite{nan2016pruning}, pruning is used to minimize feature cost during prediction. Their algorithm starts by training a random forest with cost-adaptive impurity functions, similar to the one introduced in \cite{2015forest}. The trained trees are then jointly pruned to ensure compliance with constraints on feature cost.


CSTC (Cost-Sensitive Tree of Classifiers), proposed by \cite{2013cstc}, addresses the AFA problem via constructing a tree of classifiers. It employs a probabilistic tree-traversal framework to compute the expected cost of the CSTC tree during prediction. CSTC is trained using a global loss function, where the cost component is a relaxed version of the expected cost. 
An extension of CSTC, called the Approximately Submodular Tree of Classifiers (ASTC), was introduced in \cite{kusner2014feature}. ASTC incorporates approximate submodularity to train a near-optimal tree of classifiers. Specifically, the optimization in CSTC is formulated as an approximately submodular set function optimization problem, enabling its solution through greedy methods.


A related cost-sensitive method is proposed in \cite{chai2004test} to train a Naive Bayes classifier. Their method can be used in both sequential and batch modes (obtaining multiple feature values together). In their method, the training procedure is similar to traditional Naive Bayes, which involves estimating the distributions of each feature given the target class, along with the prior distributions for the target classes. During prediction, the next feature to query is selected by maximizing a utility function defined based on the cost of features as well as the reduction in expected misclassification cost. When no feature has a positive utility, the prediction is made using the observed features.

In \cite{nan2014fast}, a margin-based classification algorithm\footnote{Margin is used as a confidence measure in various classification algorithms.} is proposed that is capable of selecting the next unobserved feature based on training data with fully observed features and labels. The feature selection policy is determined by considering training instances in the neighborhood of a test instance. The main challenge in this context is identifying the nearest neighbors based on partially observed features. In \cite{nan2014fast}, this challenge is addressed by learning the margin (instead of the label) of a test instance's features from the training instances in the partial neighborhood of that instance. This margin is then used to estimate the probability of correct classification upon acquiring the value of an unobserved feature, and the estimated probability is maximized within the feature selection policy. A related method, based on the distance to training instances, is proposed in \cite{mirzaei2023fast}. In their method, features are selected using their Fisher scores based on the training data. Then, training instances that have a large distance from the test instance are removed, and the process continues (the distances are computed only using the already selected features).


\section{MDP-Based Active Feature Acquisition}
\label{sec:rl}

Many of the approaches discussed so far share a common limitation: they rely on greedy informativeness criteria to select features, which can result in myopic decisions and suboptimal long-term performance. Given the inherently sequential nature of the AFA problem, modeling it as a Markov Decision Process (MDP) provides a more principled framework. This formulation enables the use of Reinforcement Learning (RL) to learn non-myopic policies $\pi$ that can outperform greedy strategies by selecting features that optimize long-term outcomes. %However, it has been observed that greedy methods can sometimes outperform RL-based approaches, despite the additional complexity in RL.

We begin by describing the standard MDP formulation of the AFA problem as presented in existing work \citep{10.5555/645531.656020, 10.1007/978-3-642-25832-9_14, NEURIPS2018_e5841df2, pmlr-v139-li21p, li2024distributionguidedactivefeature}. In its simplest form, the state space, action space, and reward function are described as follows.
%
\begin{equation} \label{eq:mdp}
\begin{aligned}
    s &= [S,\, x_S] \\
    a &\in U \cup \{\phi\}, \quad U = \{1,\dots,n\}\setminus S \\
    r(s, a) &= -l\big(f(x_S), y\big)\, \mathbb{I}(a = \phi) + \alpha\, c(a)\, \mathbb{I}(a \neq \phi)
\end{aligned}
\end{equation}
%
Here, $y$ denotes the true label of the current data point. The state $s$ consists of the set of acquired features $S \subseteq \{1, \dots, n\}$, and their corresponding values $x_S$. The action space includes the features not yet selected, denoted by $U$, as well as a special termination action $\phi$. Thus, each action specifies either the next feature value to observe or the decision to terminate the acquisition process (i.e., end the episode) and trigger the prediction $f(x_S)$. The reward function $r(s, a)$ penalizes the acquisition cost $c(a)$ when a feature is selected (i.e., when $a \neq \phi$) and, upon termination ($a = \phi$), penalizes the prediction loss $l\big(f(x_S), y\big)$. The hyperparameter $\alpha$ determines the balance between prediction loss and acquisition cost. It is common to assume $\alpha = 1$ and uniform cost across all features.

Some studies cast the problem as a Partially Observable Markov Decision Process (POMDP). In this formulation, the state $s$ is typically enriched with additional information beyond the raw features. For instance, properties of the current prediction model $f$ may be included as part of an observation that encodes the augmented state\footnote{A POMDP is an extension of an MDP in which the agent has limited access to the full state. Instead of receiving complete state information, the agent only obtains an incomplete observation derived from the current state.}. This will be discussed further in the following.

\cite{dulac2011datum} demonstrated an equivalence between reward maximization in an MDP and minimization of an $L_0$-regularized loss, thereby providing a direct optimization objective for their model. They introduced a scoring function to guide both feature selection and classification, which was trained using approximate policy iteration with rollouts. Building on this work, \cite{10.1609/aaai.v33i01.33013959, DBLP:journals/ml/JanischPL20} explored more advanced RL techniques, such as Deep Q-Networks (DQN) \citep{mnih2015human} and its variants. Additionally, \cite{DBLP:journals/ml/JanischPL20} introduced a modified MDP framework that explicitly incorporates constraints on feature acquisition costs, including both average-case and hard budget limits.

\cite{10.1007/978-3-642-25832-9_14} formulated the problem as a POMDP. They consider a classification setting, and the state is represented by the class probabilities $p(y_1|x_S),\dots,p(y_K|x_S)$ predicted by the current model $f$ given the currently observed features $x_S$ (where $K$ is the number of classes). They further introduced an action selection mechanism that ensures features are chosen without replacement, and applied fitted Q-Iteration (FQI) with function approximation to learn an optimal feature selection policy.

\cite{NEURIPS2018_e5841df2} proposed a novel order-invariant set encoding to represent acquired feature subsets, effectively reducing the search space. Their approach jointly trains an RL agent and a classifier, ensuring that the feature selection policy is optimized in alignment with classification objectives. To solve the underlying MDP, they employ DQNs, enabling efficient policy learning in high-dimensional feature spaces.

\cite{kachuee2018opportunistic} introduced an alternative reward function that simultaneously accounts for the cost of acquiring a feature and the expected reduction in model uncertainty, quantified via Monte Carlo dropout sampling \citep{DBLP:conf/icml/GalG16}. They leverage DQNs to solve the underlying MDP. To improve efficiency, the authors propose sharing representations between the prediction model $f$ and the value function estimator network (i.e., the policy), which allows joint optimization. Furthermore, the framework is fully online and stream-based, making it well-suited for real-world applications with limited feature availability (we further discuss online vs. offline RL for AFA below).

%Unlike previous methods that tune the trade-off between feature cost and prediction accuracy during training, their approach adapts dynamically at prediction time.

\cite{zannone2019odin} introduced ODIN, a model-based RL framework for AFA. ODIN utilizes a Partial Variational Autoencoder (PVAE) \citep{ma2019eddi} to model the conditional distribution of unobserved features, \( p(\mathbf{x}_i \mid x_S) \) for \( i \notin S \). Since the transition dynamics of the MDP in this setting can be determined by \( p(\mathbf{x}_i \mid x_S) \), ODIN uses model-based RL techniques using an approximation of this distribution. Concretely, they generate rollouts based on \( p(x_i \mid \mathbf{x}_S) \) given a pre-trained PVAE. This significantly improves data efficiency and effectively handles missing features. A key contribution is its order-sensitive reward function, which optimally balances feature acquisition cost and sequence, ensuring that the most informative features are selected first. To solve the underlying MDP, ODIN employs Proximal Policy Optimization (PPO) \citep{DBLP:journals/corr/SchulmanWDRK17}. 

\cite{pmlr-v139-li21p, li2024distributionguidedactivefeature} also introduced a model-based RL framework, called Generative Surrogate Models for RL (GSMRL). Using a generative surrogate model, GSMRL captures dependencies between features and provides intermediate rewards and auxiliary information, thus mitigating challenges associated with sparse rewards and high-dimensional action spaces. More concretely, the framework employs arbitrary conditioning flow models (ACFlow) \citep{pmlr-v119-li20a} to model the distribution of features and target labels conditioned on arbitrary features, $p(\mathbf{y}, \mathbf{x}_i \mid x_S)$. This modeled distribution is then used to augment both the state and reward, leading to better overall performance.

%We now summarize the differences between existing methods that solve the AFA problem using RL, and provide a unified view of the different approaches. From the above discussion, we observe that the approaches are similar and typically differ in design choices w.r.t. one or more of the following components: (i) the state representation, (ii) the reward function, (iii) the RL algorithm used, (iv) whether a generative surrogate model is used (effectively leading to a model-based vs. model-free approach, discussed more below), (v) whether the prediction model $f$ is pre-trained and/or jointly trained with the policy $\pi$, (vi) whether the approach operates online or offline (discussed below). 

From the above discussion, we see that existing RL-based approaches for the AFA problem share many similarities and typically differ in design choices w.r.t. one or more of the following components: (i) the state representation, (ii) the reward function, (iii) the choice of RL algorithm, (iv) whether a generative surrogate model is used (which leads to a model-based vs. model-free approach, discussed more below), (v) whether the prediction model \(f\) is pre-trained and/or jointly trained with the policy \(\pi\), and (vi) whether the method operates online or offline (discussed below).

\paragraph{Model-Free vs. Model-Based RL for AFA.} As described by \cite{li2024distributionguidedactivefeature}, RL methods can be broadly categorized into model-based and model-free approaches, depending on whether a transition model is employed \citep{DBLP:journals/corr/Li17b}. Model-based methods tend to be more data-efficient but may suffer from significant bias if the dynamics are misspecified. In contrast, model-free methods can handle arbitrary dynamic systems but typically require substantially more data. In the context of AFA, model-based RL can be used by modelling the distribution $p(\mathbf{y}, \mathbf{x}_i \mid x_S)$ (e.g., using a generative model as discussed) in order to (i) generate synthetic trajectories of the transition dynamics \citep{zannone2019odin}, and/or (ii) augment the state and reward with richer information \citep{pmlr-v139-li21p, li2024distributionguidedactivefeature}.


\paragraph{Offline vs. Online RL for AFA.} RL-based AFA methods can operate offline or online. In the offline setting, a complete dataset $D$ (with all features and labels) is available, allowing both the prediction model $f$ and the policy $\pi$ to be pre-trained before deployment. This setting is the most common for the greedy approaches discussed in Section \ref{section:greedy} \citep{covert2023learning, chattopadhyay2023variational}. Notably, model-based RL approaches are particularly advantageous in this setting because training a generative model to approximate the transition dynamics is often computationally expensive and data-intensive. In contrast, online methods learn $f$ and $\pi$ incrementally as new data arrives. In this context, the reward function must be designed either to be independent of the true label or under the assumption that the true label of each new data point is revealed at the end of an episode (as in \citep{kachuee2018opportunistic}, similar to the setting in \citep{ijcai2023p463} discussed in Section \ref{sec:ec2}). In principle, the offline and online approaches can be combined by pre-training on a dataset $D$ offline and then continuing with online updates after deployment.



%In Algorithm \ref{alg:rl_procedure}, we present a typical procedure for applying RL to the AFA problem. Note that this algorithm is not meant to exactly represent any single approach; rather, it provides an intuitive, generic overview of the design choices that differentiate various methods. In an online setting, the dataset $D$ is a continuous stream of data points. Additionally, the algorithm takes as input the policy $\pi$, predictor $f$, and surrogate model $G$, indicating that these components may be pre-trained on a separate data source before executing the algorithm.
%
%The procedure begins by optionally pre-training the predictor $f$ and surrogate model $G$ on the dataset $D$ (if working in an offline setting). Next, an experience buffer $\mathcal{B}$ is initialized, and the algorithm then iterates through a series of episodes. Each episode starts by sampling a new data point from $D$. In the online setting, features and the true label are not available immediately; rather, they are incrementally revealed as the episode progresses. 
%
%The \texttt{InitState} call initializes the basic state as described in Eq. \ref{eq:mdp}, while the \texttt{AugmentState} function enriches the state with additional information from $f$ (and from $G$, if available). A concrete example is given soon. The \texttt{SelectAction} step then determines the next action based on the current state $s$. This decision process depends entirely on the chosen RL algorithm (e.g., on-policy or off-policy); in model-based settings, the next action may be generated using the distribution $p(x_i \mid \mathbf{x}_S)$ modeled by the surrogate model $G$.
%
%Subsequently, the reward is computed, either as specified in Eq. \ref{eq:mdp} or using a reshaped version that leverages currently available information. For instance, \citep{pmlr-v139-li21p, li2024distributionguidedactivefeature} augments the intermediate reward for acquiring a new feature with:
%
%\begin{equation} \label{eq:shapereward}
%r_m(s, a) := H\left(y \mid \mathbf{x}_S\right) - \gamma\, H\left(y \mid \mathbf{x}_S, x_a\right).
%\end{equation}
%
%Equation \ref{eq:shapereward} is the mutual information defined in Eq. \ref{eq:kl}, with the discount factor $\gamma$ applied to the second term. This reward is estimated using the generative surrogate model $G$, as discussed in Section \ref{section:greedy}. Consequently, if the acquired feature $x_a$ effectively reduces uncertainty about the true label $y$, a larger reward is granted. This mechanism introduces intermediate rewards that help mitigate the well-known credit assignment problem in RL \citep{DBLP:journals/ml/Sutton88}.
%
%After computing the reward, the current state is updated to $s'$ based on the selected action (i.e., the acquired feature $a$). The new state is then augmented with additional information from $f$ and $G$. For example, as in \citep{pmlr-v139-li21p, li2024distributionguidedactivefeature}, the state may be enriched with (i) uncertainty estimates and imputations for unobserved features, (ii) predictions of the expected information gain from future acquisitions, and (iii) uncertainty regarding the target output. Finally, the tuple $(s, a, r, s')$ is stored in the experience buffer $\mathcal{B}$, and the policy $\pi$ and predictor $f$ are periodically updated based on the collected episodes. The specifics of these update steps depend on the particular RL algorithm employed (and whether $\pi$ and $f$ are jointly trained).
%

\paragraph{Search methods.} \cite{10.5555/645531.656020} formulated the problem as an MDP and use the AO* heuristic search algorithm to find the optimal classification policy that minimizes expected cost. They introduced an admissible heuristic that significantly reduces the search space, particularly when feature costs are high, and propose a statistical pruning technique that removes policies that are statistically indistinguishable, further improving efficiency. They estimate the transition dynamics of the MDP based on a training set of available features/labels.

\paragraph{Imitation Learning.} \cite{DBLP:conf/nips/HeDE12, DBLP:journals/corr/HeMK16} also formulate the AFA problem as an MDP. However, they adopt an imitation learning approach by training the agent to mimic a reference policyâ€”specifically, the greedy policy of an oracle that exploits the true labels and the classifier. This dependence on the oracle's potentially suboptimal performance can limit the agent's overall effectiveness.




\section{Conclusion and Future Directions}
\label{sec:conclusion}

In this paper, we surveyed active feature acquisition methods and organized them into three main categories: (i) \emph{Greedy} methods, which includes both generative and discriminative approaches based on conditional mutual information as well as the Equivalence Class Edge Cutting technique; (ii) \emph{Embedded} methods that integrate feature acquisition within the training or inference process; and (iii) \emph{MDP-based} methods, which are further subdivided into model-free RL, model-based RL, imitation learning, and search-based approaches. Some of these approaches are designed for online adaptation. Notably, the online methods can belong to different main categories, such as RL-based approaches or those
\begin{table*}[t!]
\centering
\small
\begin{tabular}{p{3.0cm} p{3.2cm} p{4.0cm} p{4.0cm}}
\toprule
\textbf{Category} & \textbf{Representative Work(s)} & \textbf{Approach / Method} & \textbf{Key Contributions / Notes} \\
\midrule
\textbf{Greedy: CMI (Generative)} & 
\citep{ma2019eddi,NEURIPS2019_c055dcc7,RangrejC21,chattopadhyay2022interpretable,bsoda2022} &
Use partial generative models (e.g., partial VAE, Product-of-Experts encoder) to approximate conditional distributions \(p(\mathbf{x}_i \mid x_S)\) and compute conditional mutual information. &
Enables CMI-based feature selection by learning a surrogate for missing data; requires pre-training and can be computationally intensive. \\
\midrule
\textbf{Greedy: CMI (Discriminative)} & 
\citep{chattopadhyay2023variational,covert2023learning,gadgil2024estimating} &
Adopt a variational formulation to directly estimate the CMI without explicit generative modeling. &
Jointly optimize the prediction model and selection policy; supports variable budgets and non-uniform feature costs. \\
\midrule
\textbf{Greedy: Equivalence Class Edge Cutting (EC\(^2\))} & 
\citep{golovin2010near, uaivoi2017,ijcai2023p463,rahbar2025costefficient} &
Formulate feature acquisition as an edge-cutting problem in a graph where nodes represent complete feature realizations and edges connect hypotheses from different equivalence classes. &
Adaptive submodular objective that guarantees near-optimal performance; extended to online settings. \\
\midrule
\textbf{Embedded Methods} & 
\citep{ling2004decision,chai2004test,miser2012,2013cstc,kusner2014feature,nan2014fast, 2015forest, nan2016pruning, mirzaei2023fast} &
Integrate feature acquisition directly into the training/inference process (e.g., decision trees, random forests, Naive Bayes) by modifying splitting criteria or loss functions to account for cost. &
Balances accuracy and cost within a unified framework while handling missing features. However, its reliance on specific model architectures limits flexibility across different domains.\\
\midrule
\textbf{MDP: Model-Free RL} & 
\citep{dulac2011datum,10.1007/978-3-642-25832-9_14,NEURIPS2018_e5841df2,kachuee2018opportunistic,10.1609/aaai.v33i01.33013959, DBLP:journals/ml/JanischPL20} &
Formulate the problem as a (PO)MDP with states defined by the set of acquired features. Apply model-free RL methods (e.g., DQN, PPO) to learn sequential policies directly from experience. &
Optimizes long-term rewards by balancing prediction loss and acquisition cost. \\
\midrule
\textbf{MDP: Model-Based RL} & 
\citep{zannone2019odin,pmlr-v139-li21p,li2024distributionguidedactivefeature} &
Similar to above category, but also employs generative surrogate models to model transition dynamics \(p(\mathbf{x}_i \mid x_S)\) and augment the state/reward with richer information. &
Provides richer state representations and improved data efficiency, contingent on accurate surrogate modeling. \\
\midrule
\textbf{MDP: Imitation Learning} & 
\citep{DBLP:conf/nips/HeDE12,DBLP:journals/corr/HeMK16} &
Train an agent to mimic a reference (often greedy) oracle policy for feature acquisition, thereby reducing exploration challenges. &
Simplifies policy learning via expert trajectories, though performance depends on the quality of the oracle. \\
\midrule
\textbf{MDP: Search} & 
\citep{10.5555/645531.656020} &
Use heuristic search (AO* algorithm) to solve the (PO)MDP for optimal feature acquisition policies. &
Provides an alternative to RL by directly searching for the optimal policy. Can be very inefficient. \\
\midrule
\textbf{Online Methods} &
\citep{uaivoi2017,kachuee2018opportunistic,ijcai2023p463,rahbar2025costefficient} &
Adopt an online learning paradigm where policies are updated incrementally as new data arrives. &
Enables real-time adaptation in dynamic environments. Notably, does not assume access to an offline dataset $D$ with features and labels for pre-training. \\
\bottomrule
\end{tabular}
\caption{Overview of active feature acquisition approaches, categorized into three main categories: (i) \emph{Greedy} methods maximize conditional mutual information via generative or discriminative models or optimize adaptive submodular objectives (e.g., EC\(^2\)), offering theoretical guarantees. (ii) \emph{Embedded} methods integrate feature acquisition into model training or inference (e.g., decision trees, random forests, Naive Bayes) to balance accuracy and cost. (iii) \emph{MDP-based} methods use reinforcement learning (model-free, model-based, or imitation learning) or search-based strategies. Some methods operate in an online setting (last row). The online methods also belong to one of the other categories above (not necessarily the same).}
\label{tab:afa_summary}
\end{table*}

%\begin{table}
%    \centering
%    \begin{tabular}{lrr}
%        \toprule
%        Scenario  & $\delta$ (s) & Runtime (ms) \\
%        \midrule
%        Paris     & 0.1          & 13.65        \\
%                  & 0.2          & 0.01         \\
%        New York  & 0.1          & 92.50        \\
%        Singapore & 0.1          & 33.33        \\
%                  & 0.2          & 23.01        \\
%        \bottomrule
%    \end{tabular}
%    \caption{Booktabs table}
%    \label{tab:booktabs}
%\end{table}
% \appendix
\clearpage % Forces all floats to be placed
\FloatBarrier
optimizing the EC\(^2\) objective. Table~\ref{tab:afa_summary} provides an overview of these categories along with representative works, core approaches, and key contributions.

While significant progress has been made, several promising research directions remain. First, recent greedy methods that directly estimate CMI through discriminative approaches \citep{covert2023learning, gadgil2024estimating} have been shown to outperform model-free RL methods, yet model-based RL techniques \citep{li2024distributionguidedactivefeature} can surpass greedy methods under certain conditions. This disparity calls for a comprehensive benchmark to rigorously compare these methods. Second, most existing studies assume an offline setting with readily available feature/label datasets, which contradicts the very premise of AFA where features are costly to acquire. In addition, they rarely provide theoretical performance guarantees of their methods. In contrast, approaches based on adaptive submodular objectives (such as EC\(^2\)) often operate in an online setting and offer theoretical guarantees, highlighting the need for more work in this direction. Third, both generative greedy CMI methods and model-based RL approaches stand to benefit from advances in deep generative modeling for more accurate estimation of arbitrary conditional distributions \(p(\mathbf{x}_i\mid x_S)\). Finally, as noted by \cite{li2024distributionguidedactivefeature}, future work should also focus on enhancing the explainability of active feature acquisition systems and developing robust methods capable of detecting out-of-distribution instances.

%\begin{algorithm}[tb]
%\caption{Unified RL-based AFA}
%\label{alg:unified_afa}
%\begin{algorithmic}[1]
%
%\REQUIRE $D_{\text{observed}}, D_{\text{unobserved}}$
%\vspace{0.3em}
%\STATE $f, G \gets \text{initialize randomly}$
%
%\vspace{0.2em}
%\STATE \textbf{Offline Training (Optional):}
%\STATE \quad \text{(1) Pretrain $f$ on $D_{\text{observed}}$.}
%\STATE \quad \text{(2) Train $G$ on $D_{\text{observed}}$.}
%\STATE \quad \text{(3) }$\pi, f \gets \mathrm{RL}\bigl(D_{\text{observed}},\,f,\,G\bigr)$
%
%\vspace{0.2em}
%\STATE \textbf{Online Training (Optional):}
%\STATE \quad $\pi, f \gets \mathrm{RL}\bigl(D_{\text{unobserved}},\,f,\,G\bigr)$ 
%
%\vspace{0.3em}
%\STATE \textbf{Deployment:} 
%\STATE \quad Use $\pi$ and $f$ for feature acquisition and prediction. (\emph{No further updates or label observation.})
%
%\RETURN $\pi, f$
%
%\end{algorithmic}
%\end{algorithm}

%\begin{algorithm}[tb]
%\caption{RL for AFA}
%\label{alg:rl_procedure}
%\begin{algorithmic}[1]
%
%\REQUIRE 
%  Dataset $D$ (offline or online), 
%  policy $\pi$, 
%  predictor $f$, 
%  (optional) surrogate $G$, 
%  number of episodes $N$.
%
%\ENSURE Updated policy $\pi$ and predictor $f$.
%\vspace{0.3em}
%\STATE Optionally pre-train $f$ and/or $G$ on dataset $D$ (if offline)
%\STATE Initialize an experience buffer $\mathcal{B}$.
%
%\FOR{\text{episode} = 1 \dots N}
%  \STATE $(x, y) \gets \texttt{GetSample}(D)$
%  \STATE $s \gets \texttt{InitState}(x)$
%  \STATE $s \gets \texttt{AugmentState}(s, f, G)$
%  \WHILE{\texttt{NotTerminated}(s)}
%    \STATE $a \gets \texttt{SelectAction}(s, \pi, f, G)$
%    \STATE $r \gets \texttt{ShapeReward}(a, s, f, G, y)$
%    \STATE $s' \gets \texttt{UpdateEnvironment}(a, s, x, y)$
%    \STATE $s' \gets \texttt{AugmentState}(s, s', f, G)$
%    \STATE \texttt{Store} $(s, a, r, s')$ in buffer $\mathcal{B}$
%    \STATE $s \gets s'$
%  \ENDWHILE
%
%  \STATE \textbf{[Optionally, every $K$ episodes:]}
%  \STATE \quad $\pi \gets \texttt{RLupdate}(\pi, \mathcal{B})$
%  \STATE \quad $f \gets \texttt{UpdatePredictor}(f, \mathcal{B})$
%  \STATE \quad \texttt{Clear} or \texttt{SampleAndRetain} buffer $\mathcal{B}$ as desired
%\ENDFOR
%
%\vspace{0.3em}
%\RETURN $\pi, f$
%
%\end{algorithmic}
%\end{algorithm}
% \begin{table*}[t!]
% \centering
% \small
% \begin{tabular}{p{3.0cm} p{3.2cm} p{4.0cm} p{4.0cm}}
% \toprule
% \textbf{Category} & \textbf{Representative Work(s)} & \textbf{Approach / Method} & \textbf{Key Contributions / Notes} \\
% \midrule
% \textbf{Greedy: CMI (Generative)} & 
% \citep{ma2019eddi,NEURIPS2019_c055dcc7,RangrejC21,chattopadhyay2022interpretable,bsoda2022} &
% Use partial generative models (e.g., partial VAE, Product-of-Experts encoder) to approximate conditional distributions \(p(\mathbf{x}_i \mid x_S)\) and compute conditional mutual information. &
% Enables CMI-based feature selection by learning a surrogate for missing data; requires pre-training and can be computationally intensive. \\
% \midrule
% \textbf{Greedy: CMI (Discriminative)} & 
% \citep{chattopadhyay2023variational,covert2023learning,gadgil2024estimating} &
% Adopt a variational formulation to directly estimate the CMI without explicit generative modeling. &
% Jointly optimize the prediction model and selection policy; supports variable budgets and non-uniform feature costs. \\
% \midrule
% \textbf{Greedy: Equivalence Class Edge Cutting (EC\(^2\))} & 
% \citep{golovin2010near, uaivoi2017,ijcai2023p463,rahbar2025costefficient} &
% Formulate feature acquisition as an edge-cutting problem in a graph where nodes represent complete feature realizations and edges connect hypotheses from different equivalence classes. &
% Adaptive submodular objective that guarantees near-optimal performance; extended to online settings. \\
% \midrule
% \textbf{Embedded Methods} & 
% \citep{ling2004decision,chai2004test,miser2012,2013cstc,kusner2014feature,nan2014fast, 2015forest, nan2016pruning, mirzaei2023fast} &
% Integrate feature acquisition directly into the training/inference process (e.g., decision trees, random forests, Naive Bayes) by modifying splitting criteria or loss functions to account for cost. &
% Balances accuracy and cost within a unified framework while handling missing features. However, its reliance on specific model architectures limits flexibility across different domains.\\
% \midrule
% \textbf{MDP: Model-Free RL} & 
% \citep{dulac2011datum,10.1007/978-3-642-25832-9_14,NEURIPS2018_e5841df2,kachuee2018opportunistic,10.1609/aaai.v33i01.33013959, DBLP:journals/ml/JanischPL20} &
% Formulate the problem as a (PO)MDP with states defined by the set of acquired features. Apply model-free RL methods (e.g., DQN, PPO) to learn sequential policies directly from experience. &
% Optimizes long-term rewards by balancing prediction loss and acquisition cost. \\
% \midrule
% \textbf{MDP: Model-Based RL} & 
% \citep{zannone2019odin,pmlr-v139-li21p,li2024distributionguidedactivefeature} &
% Similar to above category, but also employs generative surrogate models to model transition dynamics \(p(\mathbf{x}_i \mid x_S)\) and augment the state/reward with richer information. &
% Provides richer state representations and improved data efficiency, contingent on accurate surrogate modeling. \\
% \midrule
% \textbf{MDP: Imitation Learning} & 
% \citep{DBLP:conf/nips/HeDE12,DBLP:journals/corr/HeMK16} &
% Train an agent to mimic a reference (often greedy) oracle policy for feature acquisition, thereby reducing exploration challenges. &
% Simplifies policy learning via expert trajectories, though performance depends on the quality of the oracle. \\
% \midrule
% \textbf{MDP: Search} & 
% \citep{10.5555/645531.656020} &
% Use heuristic search (AO* algorithm) to solve the (PO)MDP for optimal feature acquisition policies. &
% Provides an alternative to RL by directly searching for the optimal policy. Can be very inefficient. \\
% \midrule
% \textbf{Online Methods} &
% \citep{uaivoi2017,kachuee2018opportunistic,ijcai2023p463,rahbar2025costefficient} &
% Adopt an online learning paradigm where policies are updated incrementally as new data arrives. &
% Enables real-time adaptation in dynamic environments. Notably, does not assume access to an offline dataset $D$ with features and labels for pre-training. \\
% \bottomrule
% \end{tabular}
% \caption{Overview of active feature acquisition approaches, categorized into three main categories: (i) \emph{Greedy} methods maximize conditional mutual information via generative or discriminative models or optimize adaptive submodular objectives (e.g., EC\(^2\)), offering theoretical guarantees. (ii) \emph{Embedded} methods integrate feature acquisition into model training or inference (e.g., decision trees, random forests, Naive Bayes) to balance accuracy and cost. (iii) \emph{MDP-based} methods use reinforcement learning (model-free, model-based, or imitation learning) or search-based strategies. Some methods operate in an online setting (last row). The online methods also belong to one of the other categories above (not necessarily the same).}
% \label{tab:afa_summary}
% \end{table*}

% %\begin{table}
% %    \centering
% %    \begin{tabular}{lrr}
% %        \toprule
% %        Scenario  & $\delta$ (s) & Runtime (ms) \\
% %        \midrule
% %        Paris     & 0.1          & 13.65        \\
% %                  & 0.2          & 0.01         \\
% %        New York  & 0.1          & 92.50        \\
% %        Singapore & 0.1          & 33.33        \\
% %                  & 0.2          & 23.01        \\
% %        \bottomrule
% %    \end{tabular}
% %    \caption{Booktabs table}
% %    \label{tab:booktabs}
% %\end{table}
% % \appendix
% \clearpage % Forces all floats to be placed
% \FloatBarrier
% \section*{Ethical Statement}
% There are no ethical issues.
\section*{Acknowledgments}
This work was partially supported by the Wallenberg AI, Autonomous Systems and
Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.

% keywords can be removed
% \keywords{First keyword \and Second keyword \and More}

\bibliographystyle{unsrtnat}
\bibliography{refs}

\end{document}
