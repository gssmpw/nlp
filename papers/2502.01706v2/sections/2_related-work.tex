\section{Related Work}

\paragraph{Bio hashing.}
The mushroom body of the fruit fly is considered to be the center of olfactory learning and memory formation \cite{doi:10.1126/science.8303280}. 
It is where projection neurons (PN) make synaptic connections with Kenyon cells (KC).
In contrast to PNs, KCs have much more odor-selective responses \cite{turner2008olfactory, honegger2011cellular}.
In other words, only a small proportion of them becomes activated for a given odor due to inhibition \cite{lin2014sparse, papadopoulou2011normalization}.
Therefore, the KCs represent odors as sparse and distributed hash codes \cite{lin2014sparse}.
These codes are high-dimensional representations that the mushroom body output neurons (MBONs) use to form memories.
The sparsity and high specificity of these KC responses are thought to support the accuracy of this process\cite{laurent2002olfactory}. 

\par
The \flyvec\cite{flyvec} model repurposes bio hashing into learning word embeddings from raw text.
Using a one-layer neural network from PNs to KCs, it creates representations of a binarized input of context and target words.
The synaptic weights from PNs to KCs are learned with a biologically plausible unsupervised learning algorithm \cite{hopfield} inspired by Hebbian learning.
During inference, the $k$-Winner-Takes-All ($k$-WTA) mechanism creates sparse, binary hash codes.

\par
Our method, \methodname, uses a similar architecture and algorithm.
However, we augment the inputs with positional information and extend the loss and parameters accordingly.
Specifically, \methodname generalizes \flyvec, enabling bio hashes to represent sequences.

\paragraph{Text embeddings.}
The main goal of embedding text is to produce vectors that have a semantic correspondence to characters, words, or arbitrary sequences of text elements (tokens).
Examples are \texttt{word2vec} \cite{word2vec}, \texttt{GloVe} (Global Vectors) \cite{glove}, and \bert (Bidirectional Encoder Representations from Transformers) \cite{bert}.
The former two are examples of static word embeddings, which are constant regardless of context.
In contrast, \bert learns a function that can yield representations conditioned on the context: a contextual word embedding.

Transformer-based encoder models like \bert learn language representations using self-attention and masked language modeling among other objectives, which are not necessarily motivated by biological mechanisms.
These representations are \enquote{contextual}, i.e., they capture the ambiguity of words (sentences) given the context in which they appear.

The \flyvec model highlights how a biologically inspired architecture is also capable of learning contextual \textit{word} embeddings (binary hash codes).
In our work, we extend this model to \textit{sentence} representations.
We do this by learning positions as a complementary distributional property of text. 

\paragraph{Positional encoding.}
Recurrent Neural Networks (RNNs) \cite{rnn}, e.g., Long Short-Term Memory (LSTM) \cite{lstm, gers}, inherently capture positional information given that their state (and output) is always conditioned on the sequential input.
Similarly, Convolutional Neural Networks (CNNs) can implicitly learn to incorporate positional information \cite{cnn_position}.
Although self-attention \cite{attention} is designed to focus on different parts of the sequence depending on the task, it does not infer positional information.
When introducing Transformers, the work \cite{vaswani} explicitly incorporates positional encodings in an additive manner.
In fact, this component is crucial for the success of these architectures \cite{pos-information-crucial}.
Closer to our work, \texttt{RoPe} \cite{roformer} expresses positional information in Transformer models through a multiplicative approach.
Namely, an embedding vector of a token is \textit{rotated} based on its position.
Although this approach and ours are both non-parametric and leverage the complex plane to implement such rotations, we differ by explicitly expressing the input in the complex field and learning a complex parameter matrix.

\paragraph{Complex weights.}
Complex-valued Neural Networks (CVNNs) offer advantages in specific domains, as they can handle both amplitudes and phases of the signals.
Examples of such areas are image processing, and more generally signal processing where a datum is inherently complex.
For a comprehensive overview see, e.g., \cite{cvnn-survey}.
Close to our work, the method in \cite{complex_embeddings} uses a learned complex-valued map to enrich text embeddings with positional information.
In our work, using the semantics of the complex field yields a simple mathematical extension of the \flyvec model.
While CVNNs focus on end-to-end functions in the complex field, we use complex numbers only as a way of efficiently representing the order in sequences learned in neurons.
However, we do \textit{not} use complex activation functions, our embeddings are purely real and are constructed with the $k$-WTA mechanism.