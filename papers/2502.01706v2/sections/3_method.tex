\section{Methods}\label{sec:method}
\subsection{\flyvec\cite{flyvec}.}
This model adapts a single parameter matrix $\vect{W} \in \R^{K\times2\nvoc}$, modeling the PNs$\to$KCs synapses (projection neurons to Kenyon cells) and using a biologically inspired mechanism.
Here, $K$ is the number KCs, and $\nvoc$ is the size of the word vocabulary.
For training, every input sentence is split into sliding text windows of fixed length $\omega\in\N$ disregarding positions, thus modeling sets (bag-of-words).
The inputs $\vect{v} \in \R^{2\nvoc}$ are crafted as follows: the first $\nvoc$ components are the sum of one-hot encoded vectors of context tokens surrounding a \textit{target word}, and the second $\nvoc$ entries are a single one-hot vector corresponding to the \textit{target word}, see \cref{fig:bag-of-words}.
\begin{figure}[tb]
    \centering
    \includegraphics[width=6.5cm]{images/bag_of_words.drawio.pdf}
    \caption{Sample input vector $\vect{v}\in\R^{2\nvoc}$ to the \flyvec model. The two blocks (context and target) of length $\nvoc$ are separated by the long vertical line in the middle. Here, \enquote{more} is the \textit{target word} and sliding window $\omega=3$. Adapted from \cite{flyvec}, Figure 2, p.3.}
    \label{fig:bag-of-words}
\end{figure}
The parameter matrix $\vect{W}$ is learned by minimizing the energy function
\begin{equation}
E \coloneqq - \sum_{\vect{v} \in \inputs}
                       {
                         \frac
                              { 
                                  \langle
                                  \vect{W}_{\widehat{\mu}} , \vect{v}/\vect{p}
                                  \rangle 
                              }
                              { \langle
                                  \vect{W}_{\widehat{\mu}},\vect{W}_{\widehat{\mu}}
                                \rangle^{1/2} 
                              }
                       },
\label{eq:E}
\end{equation}
where
\begin{equation}
\widehat\mu \coloneqq \underset{\mu\in\{0,\dotsc,K-1\}}{\operatorname{argmax}} \ \langle \vect{W}_{\mu},\vect{v} \rangle,
\label{eq:argmax}
\end{equation}
$\vect{v}/\vect{p}$ is the input vector as described above divided by the word frequencies $\vect{p}\in\N^{\nvoc}$ in the whole training corpus, and $\inputs$ is the set of preprocessed inputs. 
The model emphasizes learning through lateral inhibition, i.e., during training, there is only a single neuron $\widehat\mu$ (a row $\vect{W}_{\widehat\mu}$ in the $\vect{W}$ matrix), which is adapted for a single sample.

\par
\subsection{\methodname}
Our extension revolvese around capturing the sequential nature of text data through added positional information.
We leverage the semantics of complex numbers and keep the same biologically inspired mechanisms, namely sparsity through $k$-Winner-Takes-All ($k$-WTA) activations.
We aim to preserve the properties of \flyvec regarding efficiency, simplicity, and interpretability.
Thus, we constrain ourselves to a single-parameter matrix.
Additionally, we focus on an extension that considers a complex-valued input, a single complex parameter matrix $\vect{W}$, and a compatible energy function $E$.

\paragraph{Complex sequential input.}\label{ss:complexinput}
For each \textit{sentence} expressed as a concatenation of one-hot encoded vectors, we multiply each vector by $e^{i\pi\frac{l}{L}}$, where $L$ is the length of the sentence, $l \in \{0,1,\dotsc,L-1\}$ denotes the position of the respective word in the sentence and $i$ is the imaginary unit, see \cref{fig:complex-sequence}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=7.95cm]{images/complex_weights_sequence.drawio.pdf}
    \caption{Sample input $\vect{z}\in\C^{L\times\nvoc}$ to our \methodname model, where $L=4$ is the length of the sample sentence \enquote{\textit{Fly high, fly free!}}.
    Positions are encoded as the complex $2L$th roots of one in the upper half-plane.}
    \label{fig:complex-sequence}
\end{figure}
We denote this preprocessed input by $\vect{z} \in \C^{L\times\nvoc}$ and its complex conjugate by $\compconj{\vect{z}}$.
Note that this modification, compared to \flyvec, expands the dimension of the input by $L$, and reduces the length of each word vector by half since we do not have a target word as in \cref{fig:bag-of-words}.
Consequently, our parameter matrix $\vect{W}$ is in $\C^{K \times \nvoc}$ and has the same number of parameters as \flyvec ($2K\nvoc$), since for each complex parameter two real numbers are required.

With this modified input, an immediate extension of \cref{eq:E} with the Hermitian inner product $\langle\cdot,\cdot\rangle_H$ is not directly applicable, since for a sentence the quantity
\begin{equation}\label{eq:newquantity}
\sum_{l \in L}\langle \vect{W}_{\mu},\vect{z}_l \rangle_H = \sum_{l\in L}\vect{W}_{\mu}^T \compconj{\vect{z}}_l \ \in \C
\end{equation}
is complex and $\C$ is unordered, hence finding the $\operatorname{argmax}$ as in \cref{eq:argmax} is not possible.
Thus, we adapt $E$ to find a coherent maximally activated neuron $\widehat\mu$.

\paragraph{Adapting $E$.}
Due to the complex-valued nature of the Hermitian inner product, an alternative extension must be considered.
If we disregard word repetition, we can think of \cref{eq:newquantity} as $\langle \vect{W}_{\mu}, \tilde{\vect{z}}\rangle_H$, where $\tilde{\vect{z}} \coloneqq \sum_{l \in L}\vect{z}_l$ is a multi-hot encoded version of the input (similar to \flyvec).
We can decompose $\langle W_{\mu}, \tilde{\vect{z}}\rangle_H$ into two steps, element-wise multiplication and aggregation, and yield real values that are proportional to neuron activation and can be sorted:
\begin{itemize}
    \item \textit{Element-wise multiplication}: For every word and neuron, we perform a complex multiplication, which translates to scaling absolutes and subtracting phases. 
    The former indicates the correlation between the input \textit{intensity} and the neuron's output, while the latter gives us a measure of the distance (\textit{temporal difference}) between the word in neuron $\vect{W}_\mu$ and the sentence.
    \item \textit{Aggregation}:
    Simply adding these complex numbers does not retain positional difference information, since $e^{i\theta} = e^{i2\pi\theta}$ for any $\theta\in\R$.
    Hence, we instead sum the magnitudes and absolute values of phase differences to capture both the effect of maximum word activation (as in the \flyvec model) and the word position distance.
\end{itemize}
\Cref{eq:E-complex} reflects these modifications to the energy function $E$.
Note that the energy of the original \flyvec model (see \cref{eq:E}) is a special case of our approach when $\vect{W}$ is real and the inputs are preprocessed as in \cref{fig:bag-of-words}.
We slightly abuse notation by writing $L$ and $\{1,\dotsc,L\}$ interchangeably, and define
\begin{equation}
\begin{aligned}
E \coloneqq - \sum_{\vect{z} \in \inputs}\biggl(&\frac{ 
                               \sum_{l \in L}{
                                   \left\lvert \left\langle\vect{W}_{\widehat{\mu}}, \vect{z}_{l}/\vect{p}_{s_l}\right\rangle_H \right\rvert
                                   }
                             }
                             % denominator
                             {  
                                \left\langle
                                \vect{W}_{\widehat{\mu}},\vect{W}_{\widehat{\mu}}
                                \right\rangle^{1/2}_H 
                             } \\
                            &+ 
                            \sum_{l \in L}{
                             \left\lvert \operatorname{Arg}\left\langle\vect{W}_{\widehat{\mu}}, \vect{z}_l/\vect{p}_{s_l} \right\rangle_H\right\rvert
                             } % end sum
                        \biggr),
\label{eq:E-complex}
\end{aligned}
\end{equation}
where
\begin{equation}
\widehat\mu \coloneqq \underset{\mu\in\{0,\dotsc,K-1\}}{\operatorname{argmax}} \sum_{l \in L}{\left\lvert \left\langle\vect{W}_{\mu}, \vect{z}_{l} \right\rangle_H \right\rvert + \left\lvert \operatorname{Arg}\left\langle\vect{W}_{\mu}, \vect{z}_l\right\rangle_H\right\rvert, }
\label{eq:mu-complex}
\end{equation}
$\operatorname{Arg}$ is the function that extracts the phases from the complex numbers, and $s_l$ is the index in the vocabulary of the $l$th word in a sentence $[s_0,...,s_{L-1}]$.
\Cref{eq:E-complex} favors learning complex weights for words in one half of the complex plane, since the absolute angle difference $\lvert \operatorname{Arg}\cdot\rvert$ is maximized, and its maximum is at $\pi$.
If we replaced the additions in both \cref{eq:E-complex,eq:mu-complex} with subtractions, the synapses would be learned on the same half-plane as the input vectors $\vect{z}$.
Either option is of no consequence for the model and we choose the additive expression for our experiments.
Furthermore, we highlight that the two summands in \cref{eq:mu-complex} are not of the same scale; the former, $\lvert \langle\vect{W}_{\mu}, \vect{z}_{l} \rangle_H\rvert$, is in $[0, \infty)$, whereas the latter, $\lvert \operatorname{Arg}\langle\vect{W}_{\mu}, \vect{z}_l\rangle_H\rvert$, is in $[0, \pi]$.
To consider this, we experimented with replacing addition ($+$) by multiplication ($\cdot$) in \cref{eq:E-complex} and \cref{eq:mu-complex}. 
This however resulted in instability leading to model overfitting (multiple neurons learn the same sequences) and, more prominently, would not constitute a generalization of the original energy presented in \cref{eq:E}.
Nevertheless, in the case of inference, we also test the multiplicative approach. 

\par
Note that, as depicted in \cref{fig:complex-sequence}, we limit the phases in the input $\vect{z}$ to lie in $[0,\pi)$ to enforce learning phases in $\vect{W}$ that also only span half of the complex plane.
This resolves the ambiguity of having distinct positions in word phases that would be mapped equivalently in the phase distance component of our energy function, e.g., a word appearing in $\frac{-\pi}{2}$ and $\frac{\pi}{2}$ and the weight of that word in $\vect{W}_{\mu}$ with phase $0$.

\par
To summarize, the semantics and operations of complex numbers naturally give us a framework to generalize the objective of minimizing $E$, while capturing positional information.

\paragraph{Toy example.}\label{ss:toy}
We illustrate the mechanics of the loss presented in \cref{eq:E-complex} with a toy example that uses only four KCs ($K=4$) and ten words (digits) ($\nvoc=10$) for two sentences (sequences): $1,2,\dotsc,9$ and $9,8,\dotsc,1$. The model is trained with backpropagation following \cref{eq:E-complex}.
\Cref{fig:toy} shows the initial state in the complex plane for each neuron, as well as the converging state after learning.
Both sequences are explicitly learned in the network in the opposite half-plane of the input (that is, with angles in $[-\pi;0)$). 
The $1$-WTA activation (\cref{eq:E-complex}) successfully creates a sparse representation of the two sequences \textit{without interference}. 
This can be observed since the remaining neurons are unchanged and the two sequences are learned (\textit{imprinted}) in two distinct neurons.

\begin{figure*}[tb]
    \centering
    \includegraphics[width=1.0\linewidth]{images/toy.pdf}
    \caption{Toy example of training with two sequences $1,2,\dotsc,9$ and $9,8,\dotsc,1$ modified as in \cref{fig:complex-sequence} with phases in $[0, \pi)$.
    Top row: The randomly initialized complex weights of a model with four Kenyon cells.
    Bottom row: Weights after learning with \cref{eq:E-complex}. The loss makes the model memorize the sequences in the lower half of the complex plane (phases in $[-\pi,0)$) in the first two neurons.
    The other neurons remain unchanged.}
    \label{fig:toy}
\end{figure*}

\paragraph{Binary hashes.}
\label{ss:hashes}
We construct binary feature vectors for \methodname from the order of activations of the KCs.
Specifically, as in \cite{biohash}, the output vector $\vect{h} \in \{0,1\}^K$ of an input $\vect{z}$ is defined for all $\mu \in\{1,\dotsc,K\}$ as
\begin{equation}
    \vect{h}_\mu \coloneqq
    \begin{cases}
        1, & \text{if } \sum_{l \in L}{\left\lvert \left\langle\vect{W}_{\mu}, \vect{z}_{l} \right\rangle_H \right\rvert + \left\lvert \operatorname{Arg}\left\langle\vect{W}_{\mu}, \vect{z}_l\right\rangle_H\right\rvert} \\ & \hphantom{if }\text{is in the top-$k$ of all KC activations,}\\
        0 & \text{otherwise,}
    \end{cases}
    \label{eq:hash}
\end{equation}
where $k \in \N$, the hash length, is a hyperparameter.
Since the two summands are, as noted earlier, of different scales, we also experiment with a version we call \methodnametwo that does multiplication ($\cdot$) instead of addition ($+$) in \cref{eq:hash} above.