\section{Conclusions}\label{sec:conclusion}
We present \methodname, an approach to improve learning of sequences by introducing positional information to a biologically-motivated model of the mushroom body of a fruit fly.
We achieve this by a novel loss that leverages representations in the complex field $\mathbb{C}$ and generalizes \flyvec \cite{flyvec} to learning sentences.
Our experiments show that \methodname improves sequence representations without adding more parameters.
In sentence similarity tasks we significantly bridge the gap between these biologically-inspired models and larger Transformer encoders like \bert. 
We show that the resource efficiency of our method makes it suitable for challenging settings like Reinforcement Learning (RL), where it outperforms a Transformer baseline.
Our analysis expands on the mechanism of the loss we introduce and the interpretability of the learned feature representations.

\section{Limitations and future work}\label{sec:future}
\paragraph{Tasks and modalities.}
We limit our experiments to pretraining our model on large general-purpose corpora.
Including the objective of sequence similarity as in sentence Transformers \cite{s_bert} would expand the comparison to such models.
Although our work presents results on the textual modality for comparability with previous work, we believe that our method is applicable to different modalities.
Thus a more extensive evaluation of, e.g., different RL environments or time-series tasks is needed.

\paragraph{Loss and model design.}
Word repetition is supported in our model in a limited way.
Namely, the phases of repeated words are averaged by the aggregation (see \cref{eq:E-complex}) during training.
Improvements in our method could stem from incorporating word repetition as a factor when learning sub-sequences across neurons as we show qualitatively in \cref{fig:neuron_ex}.
Furthermore, for the sake of comparability, we do not extend our evaluation to larger models (larger $K$). 
Scaling induces problems such as \textit{dead neurons}, so strategies like the GABA switch as proposed by \cite{sdm} or similar extensions could be needed.
Thus, future work would involve a study on the effect of scale on our approach.
Finally, the inherent benefit of adding positional information into a learned representation yields the potential for sequence generation.
We do not explore this in our work, but we see making the model generative as a future direction of research.