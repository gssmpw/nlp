\section{Experiments}\label{sec:experiments}
We evaluate our methods in sentence semantic similarity tasks, as well as in a text-based Reinforcement Learning (RL) environment.

\subsection{Baselines}
We focus on \flyvec since it is the architecture we extend.
For the sentence similarity tasks, we also compare against \bert embeddings which are contextual representations based on the Transformer architecture.
We use the \texttt{bert-base-uncased} weights publicly available on \texttt{Hugging Face}.\footnote{\url{https://huggingface.co/google-bert/bert-base-uncased}}
Likewise, we use the publicly available weights of \flyvec. \footnote{\url{https://pypi.org/project/flyvec/}}
Following the evaluation in \cite{flyvec}, we constrain the tokenizer of \bert to the top \numprint{20000} word indices for a fair comparison against \flyvec and our models.

\par
In the RL evaluation, we compare against a \flyvec model that we pretrain on PubMed (\pflyvec), and the best performing Transformer-based encoder model in \cite{ddxgym}, denoted as \transmlm.
This architecture provides a policy with an auxiliary language modeling objective that is crucial to stabilize Transformer-encoder learning in online RL settings.
We use this approach for both \pflyvec and \methodname.

\subsection{Data and model training}\label{ss:pretraining}
We use large text corpora to train the models for our experiments.
Sentences are the largest sequence structure that we model with \methodname (see \cref{ss:complexinput}) and we create batches accordingly.
Although the sliding window approach is only relevant for \flyvec (see \cref{fig:bag-of-words}), we use it in our implementation only to construct batches of the same shape (phases of the input are independent of this), and guarantee that \methodname sees tokens as often as \flyvec. 
We pretrain our models on two different text corpora depending on the domain of the target task:

\paragraph{Open Web Corpus (OWC) \cite{openweb}.}
For the sake of comparability with \cite{flyvec}, we use the OWC dataset to train \methodname.
It consists of roughly eight million web documents filtered to be longer than 128 tokens in English, resulting in about 40GB of text files.
We preprocess and encode these text data with the \flyvec tokenizer provided in \cite{flyvec}.
It maps every word to a single integer in a vocabulary of \numprint{20000} IDs, resulting in approximately 6.5B tokens.
This model is evaluated against the semantic similarity tasks.

\paragraph{PubMed.}
We construct a dataset from the PubMed database\footnote{\url{https://pubmed.ncbi.nlm.nih.gov/download/}} keeping only abstracts of articles as in \cite{pubmedbert}.
Their model, \texttt{PubMedBERT}, highlights that in-domain abstracts are sufficient to achieve competitive results in biomedical tasks.
We filter for abstracts in English and use this tokenizer to yield approximately 6.9B tokens.
We pretrain both \pflyvec and \methodname with these data.
The resulting models have the same number of parameters and we use them in the RL evaluation. 

\subsection{Semantic similarity evaluation}
We emphasize that \methodname creates representations for sentences.
We are interested in evaluating the pretrained embeddings learned in an unsupervised manner \textit{without further fine-tuning}.
Hence, we select the subset of tasks compiled in the Massive Text Embedding Benchmark (MTEB) \cite{mteb} that targets sentence embeddings without additional models (classification heads or clustering algorithms).
Furthermore, we focus on tasks comprising English sentences. 
This results in the majority of the Semantic Textual Similarity (STS), as well as Pair Classification tasks.
\par
The STS tasks are the SemEval workshop tasks STS12-17 in addition to BIOSESS, SICK-R, and STSBenchmark.
In these tasks, the similarity between two sentences is assessed based on the embeddings produced by a model.
For \flyvec and \methodname, we construct their respective contextual feature hashes and for \bert we use the average of all token embeddings.
We report Spearman rank correlation based on cosine similarity as in \cite{reimers-etal-2016-task}.

\par
In the Pair Classification (PC) tasks, a model predicts whether two sentences are duplicates in a binary setting.
The metric reported for these PC tasks is the average precision based on cosine similarity.
These tasks are SprintDuplicateQuestions, TwitterSemEval2015, and TwitterURLCorpus.

\par
Finally, we include the Words in Context (WiC) \cite{wic} task for comparability with the accuracy scores reported in \cite{flyvec}.
Here, the goal is to disambiguate, in a binary sense, whether a specific word appearing in two sentences conveys the same meaning, given their context.
We closely follow the evaluation protocol explained in \cite{flyvec} when evaluating our models.

\par
For all tasks, we conduct a 5-fold cross-validation analysis.
Following \cite{flyvec}, we use only one fold to determine the hyperparameters for testing on the remaining four folds.
For the MTEB tasks, we optimize the hash length $k$. 
For WiC, we optimize the same hyperparameters as in \cite{flyvec}.

\subsection{Reinforcement Learning (RL) evaluation}
In practice, learning policies for RL environments require a significantly large number of environment interactions and training iterations.
We argue that \pflyvec and \methodname are computationally efficient, and thus, well-suited for this scenario.
RL involves simulation environments phrased as Markov Decision Processes (MDP) that involve a very large number of learning iterations.
For our evaluation, we use DDxGym \cite{ddxgym}, which is an RL environment focused on differential diagnosis -- the process of elimination that doctors employ to determine the diagnoses of patients.
This environment generates sequences of text describing the state of a patient.
Here, an agent learns a policy to choose \textit{diagnosis} or \textit{treatment} actions to cure a patient.
The \textit{diagnosis} actions uncover state information (e.g., symptoms) about the patient, while the \textit{treatment} actions treat them. 
Since not all the information about the state (symptoms, severity, and overall health condition) is present in each observation, this is a partially observable MDP (POMDP).
The environment is based on a knowledge graph of 111 diseases and an action space of 330 actions.
We compare the \textit{mean reward} and \textit{mean episode length} of all models in this environment. 
Both metrics show the level of success to diagnose and treat a simulated patient given a textual representation of their state. 

\par
All evaluated models use the auxiliary language modeling objective described in \cite{ddxgym}.
In the case of \pflyvec and \methodname, this objective is based on the energy in \cref{eq:E} and \cref{eq:E-complex}, respectively.
For the action and value function prediction, we use a projection layer on the hashes, ignoring gradient computation for $\vect{W}$.
This means that $\vect{W}$ is only fine-tuned by the auxiliary language modeling objective.
To compare with the results in \cite{ddxgym}, we use \texttt{IMPALA} \cite{espeholt2018impala} to optimize our policy for a maximum of 80M steps.
This algorithm is a state-of-the-art actor-critic method to parallelize the optimization of a policy across multiple workers (GPUs).

\subsection{Hyperparameters and implementation}
When compared, we use the same number of parameters for \flyvec and \methodname in all experiments.
Namely, 400 Kenyon cells ($K=400$) and their respective tokenizers: \numprint{20000}-word indices for OWC and \numprint{30522} for PubMed, i.e., $\nvoc=\numprint{20000}$ and $\nvoc=\numprint{30522}$, respectively.
We follow \flyvec to pretrain our models for 15 epochs with a learning rate of $4\times10^{-4}$, which is linearly annealed towards $0$.

\par
Our models are developed with \texttt{PyTorch} \cite{paszke2019pytorch}.
For multiprocess parallelization and RL, we use \texttt{Ray} \cite{ray} and \texttt{RLlib}\cite{liang2018rllib}.
For the WiC task, we conduct hyperparameter tuning using the \texttt{hyperopt}\cite{hyperopt} scheduler.
For training, we use \texttt{Adam} \cite{adam} as optimizer.
The sparse matrix multiplications in our model are mainly indexing operations, which we favor over the sparse\footnote{\url{https://pytorch.org/docs/stable/sparse.html}} abstractions in \texttt{PyTorch}, due to a much lower memory footprint. 

\par
For pretraining, we use eight A100 GPUs and a batch size of 0.8 \textit{million} samples.
In contrast, the sentence similarity evaluation on \flyvec and \methodname is run on 64 CPU cores to compute all hashes.
For \bert we use one V100 GPU and a batch size of 8 in the evaluations.
In the RL evaluation, for \pflyvec and \methodname we use four P100 GPUs and the same learning rate as in pretraining.
For \transmlm we report the results stated in \cite{ddxgym}.
