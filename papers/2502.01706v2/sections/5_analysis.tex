\section{Results and Discussion}
In the following sections, we present the results of our experiments in both semantic similarity and Reinforcement Learning (RL) tasks.

\subsection{Semantic similarity}
\Cref{tab:mteb} presents our results for the semantic similarity tasks.
\begin{table*}[tb]
    \tiny
    \centering
    \caption{Five-fold cross-evaluation of Semantic Textual Similarity (STS) (top), Pair Classification (middle), and Words in Context (WiC) result for \bert, \flyvec, \methodname and \methodnametwo with the best hash length $k$ for each task.
    Except for STS12, our method outperforms \flyvec using the same number of parameters.
    \methodname also outperforms \bert in seven out of the 13 tasks.
    In terms of evaluation time, \flyvec, \methodname and \methodnametwo perform similarly and broadly outperform \bert even though the latter was evaluated on a GPU.
    The best score per task is highlighted in \textbf{bold}, and the best times are \underline{underlined}.
    The results with an asterisk* are taken from \cite{flyvec}.
    }
    \begin{tabular}{lcccc|cccc}
    \toprule
         & \multicolumn{4}{c}{Score mean $\pm$ std ($\uparrow$)} & \multicolumn{4}{c}{Evaluation time [s] mean $\pm$ std ($\downarrow$)} \\
         \midrule
         Task & \bert & \flyvec & \methodname & \methodnametwo &\bert & \flyvec & \methodname & \methodnametwo \\
        \midrule
        BIOSSES & \textbf{44.4$\pm$4.64} & 18.4$\pm$4.07 & 28.7$\pm$7.49 & 26.7$\pm$8.11 & \underline{0.28$\pm$0.01} & 0.49$\pm$0.01 & 0.45$\pm$0.02 & 0.76$\pm$0.11 \\
        SICK-R & 51.4$\pm$1.78 & 42.0$\pm$1.97 & \textbf{53.9$\pm$1.59} & 52.7$\pm$1.87 & 11.32$\pm$0.26 & \underline{3.43$\pm$0.11} & 3.68$\pm$0.35 & 4.25$\pm$0.35 \\
        STS12 & 33.8$\pm$7.04 & \textbf{46.3$\pm$7.49} & 45.5$\pm$7.33 & 42.4$\pm$7.10 & 4.27$\pm$0.19 & 1.59$\pm$0.18 & 1.67$\pm$0.08 & \underline{1.59$\pm$0.16} \\
        STS13 & 48.8$\pm$3.76 & 42.1$\pm$2.13 & 51.8$\pm$1.70 & \textbf{52.7$\pm$1.99} & 2.56$\pm$0.50 & 1.19$\pm$0.07 & 1.23$\pm$0.08 & \underline{1.13$\pm$0.09} \\
        STS14 & 44.3$\pm$3.52 & 42.1$\pm$2.61 & \textbf{53.6$\pm$2.25} & 51.9$\pm$2.68 & 6.69$\pm$1.24 & \underline{1.72$\pm$0.16} & 1.83$\pm$0.06 & 1.74$\pm$0.13 \\
        STS15 & 57.3$\pm$2.88 & 53.5$\pm$2.76 & \textbf{60.4$\pm$1.24} & 59.5$\pm$0.90 & 5.54$\pm$0.41 & \underline{1.57$\pm$0.10} & 1.73$\pm$0.07 & 1.69$\pm$0.09 \\
        STS16 & \textbf{60.2$\pm$6.21} & 45.5$\pm$6.30 & 49.7$\pm$4.53 & 47.1$\pm$4.35 & 2.26$\pm$0.17 & 1.08$\pm$0.08 & 1.10$\pm$0.09 & \underline{1.04$\pm$0.04} \\
        STS17 & 64.5$\pm$2.26 & 52.3$\pm$6.93 & \textbf{67.7$\pm$1.45} & 59.5$\pm$12.04 & \underline{0.23$\pm$0.01} & 0.74$\pm$0.06 & 0.63$\pm$0.07 & 0.95$\pm$0.07 \\
        STSBenchmark & \textbf{45.4$\pm$3.19} & 33.8$\pm$4.90 & 43.3$\pm$2.32 & 41.0$\pm$2.71 & 1.83$\pm$0.02 & \underline{1.14$\pm$0.07} & 1.27$\pm$0.10 & 1.42$\pm$0.45 \\
        \midrule
        SprintDuplicateQuestions & 31.6$\pm$0.90 & 40.4$\pm$0.70 & \textbf{56.2$\pm$1.02} & 55.0$\pm$1.09 & 11.98$\pm$0.18 & \underline{7.78$\pm$0.11} & 8.22$\pm$0.10 & 8.07$\pm$0.35 \\
        TwitterSemEval2015 & \textbf{51.3$\pm$1.57} & 38.4$\pm$1.74 & 51.2$\pm$2.66 & 50.8$\pm$2.00 & 9.90$\pm$0.13 & \underline{2.75$\pm$0.08} & 2.98$\pm$0.19 & 3.03$\pm$0.17 \\
        TwitterURLCorpus & 71.1$\pm$1.08 & 66.0$\pm$0.93 & \textbf{75.4$\pm$1.14} & 74.6$\pm$1.23 & 84.89$\pm$5.15 & \underline{7.24$\pm$0.84} & 8.00$\pm$0.54 & 7.64$\pm$0.23 \\

        \midrule
        WiC                      & \textbf{61.2$\pm$0.22}* & 57.7$\pm$0.27* & 57.08$\pm$1.10 & 58.16$\pm$1.59 & - & - & - & - \\
        \bottomrule
    \end{tabular}
    \label{tab:mteb}
\end{table*}


\paragraph{Task scores.}
The scores show that on seven out of 13 datasets, \methodname outperforms all other methods.
Except for STS12, \methodname consistently outperforms \flyvec by a large margin, signaling that for these tasks the added positional information is beneficial.
\bert outperforms on five out of 13 tasks. 
However, in two of these, TwitterSemEval2015 and STSBenchmark, \methodname is very close in second place.
\flyvec outperforms in STS12, where \methodname follows closely.
Similarly, \methodnametwo outperforms \flyvec and \bert in six tasks, however falls short of \methodname on all tasks except for STS13.
Nevertheless, these results show that \methodnametwo is an additional viable method to compute sentence hashes.

\par
It is remarkable that \methodname manages to bridge the performance gap and improve on \bert with only 14.56\% of the parameters, and that this is achieved solely by incorporating positional information.
Note that only unsupervised pretraining is involved and no post-hoc methods of model compression \cite{compression} are employed.
In practice, the latter is the most popular avenue to achieving significantly smaller models with comparable performance.

\paragraph{Task evaluation time.}
The evaluation times of \flyvec and \methodname on CPUs are significantly smaller than those of \bert on GPU.
BIOSSES and STS17 are the only tasks for which \bert shows a comparatively shorter time.
We argue that this is solely due to the small number of samples in these tasks. 
This effect is confirmed with datasets like TwitterURLCorpus, where the number of samples is significantly larger.
Here, the efficiency benefits of \flyvec and \methodname variants become evident (only ${\approx}7$ compared to ${\approx}85$ seconds of evaluation time).
We argue that the differences in efficiency observed in the evaluation time of \flyvec, \methodname, and \methodnametwo are mainly due to tokenization, hardware optimization, and implementation details.
Still, in terms of the hash computation both \methodname and \methodnametwo are mathematically more complex as can be inferred from the times of larger tasks, such as TwitterURLCorpus.

\par
This is reinforced when considering pretraining time.
\methodname takes approximately 15 minutes per epoch, which is significantly higher than the reported time for \flyvec on the OWC corpus -- eight minutes on less powerful hardware ($8\times\text{A100}$ vs. $3\times\text{V100}$) \cite{flyvec}.
This difference comes from our choice of implementation.
We favor an automatic differentiation framework, while \cite{flyvec} computes analytically a learning rule to fit the parameters.
Even in this setting, our training resources of ${\approx}4$ hours on $8\times\text{A100}$ are significantly inferior to the ${\approx}96$ hours of pretraining of \bert on 16 TPUs reported in \cite{bert}. 
When we pretrain \pflyvec for the RL evaluation with the PubMed corpus with our implementation (optimizing \cref{eq:E}), we report that \pflyvec and \methodname take roughly 12 and 15 minutes per epoch, respectively, highlighting the added complexity of our energy proposed in \cref{eq:E-complex}.

\paragraph{Hash length.}
Additionally, we survey the relationship between the mean test performance in the MTEB tasks and the chosen hash length $k$.
We present this in \cref{fig:hash-lengths} comparing \flyvec and \methodname.
\begin{figure*}[tb]
    \centering
    \includegraphics[width=\linewidth]{images/k_decomposition.pdf}
    \caption{Test metric averaged over five folds with respect to the hash length $k$ on the STS and PC tasks.
    \methodname shows a better and more regular performance with respect to $k$, plateauing at around $k=150$, while \flyvec tends to perform better on either sparse (low $k$) or dense (high $k$) hashes for these tasks.}
    \label{fig:hash-lengths}
\end{figure*}
Notably, the scores of \methodname are not only higher, but also more regular and concave, allowing for a hash length $k$ that is optimal with sparsity in the neighborhood of $150$ KCs.
In contrast, \flyvec tends to have an optimal $k$ either with very low or high values (very high and low sparsity).
Minimizing the energy of \methodname in \cref{eq:E-complex} can be interpreted as memorizing sequences.
Hence, we argue that the relationship between the scores and $k$ is due to these sequences being partially distributed across multiple neurons.
Although the energy optimized in \flyvec could accomplish the same (due to the sliding window, and as confirmed with the tasks that perform better with very dense hashes), we argue that presenting the additional positional information explicitly in $\C$ creates more distinct hashes.
We observe these partially distributed sequences in \methodname and present a qualitative example in \cref{fig:neuron_ex} of the most activated neuron conditioned on a specific sentence taken from \cite{flyvec}.
This neuron shows contextually relevant short sequences that can be extracted due to the relative position preserved in the phases of the words.
Examples of such sequences can be found in the caption.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.45\linewidth]{images/complex_neuron_example.pdf}
    \caption{Highest activated neuron on the input sentence \enquote{\textit{Senate majority leader discussed the issue with the members of the committee}} as in \cite{flyvec}. The neuron shows signs of memorizing (overlapping) of short sequences like $[\texttt{house}, \texttt{representative}]$, $[\texttt{plays}, \texttt{major}, \texttt{role}]$, and $[\texttt{senate}, \texttt{bill}]$, among others. These are meaningful to the context of the input and likely to appear in the pretraining corpus.}
    \label{fig:neuron_ex}
\end{figure}

\subsection{Reinforcement Learning (RL)}
In \cref{fig:rl_results}, we present the results of the evaluation on DDxGym \cite{ddxgym}.
\begin{figure}[tb]
    \centering\includegraphics[width=0.45\linewidth]{images/rl_results.pdf}
    \caption{Mean episode reward (top, higher is better) and mean episode length (bottom, lower is better) in DDxGym. \methodname outperforms and continues to improve, while the Transformer baseline converges/peaks at around 15M episodes.}
    \label{fig:rl_results}
\end{figure}
Both mean episode reward and mean episode length estimate how well the models learn to solve this environment.
For the Transformer baseline both metrics are taken from \cite{ddxgym}.
\methodname outperforms the Transformer baseline roughly after 15M environment steps, and it continues to improve until the 80M mark.
The Transformer converges to a mean reward of 400 after roughly 10M steps.
Since the theoretical maximum for the reward in DDxGym is at about 1200, there is significant room for improvement.
The mean episode length shows a similar behavior -- here a lower value means that the episode ends successfully with the diagnosis both uncovered and treated. 
While the Transformer converges at around 18 steps per episode, \methodname improves until the experiment stops at 80M episodes with around 16 steps per episode.
Although \pflyvec initially reaches a reward of almost 0, it fails to improve further and shortly collapses in performance.
This further confirms that capturing positional information significantly improves the performance of \methodname, and makes the method more competitive to larger sequence-to-sequence models like Transformers.
