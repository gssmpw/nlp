\section{Introduction}\label{sec:introduction}
A large variety of deep learning models have centered on creating distributional language representations in recent years.
Among these, Transformer-based architectures such as \bert \cite{bert} have consistently set the state-of-the-art and are now considered baselines for Natural Language Processing (NLP) tasks.
However, better performance in metrics has come at the expense of a seemingly ever-growing computational complexity.
Naturally, there is a competing interest in developing efficient architectures, and often, these are inspired by intelligent biological systems that are efficient by nature.
Recently, \flyvec was proposed as a simple approach for creating contextual word embeddings motivated by the olfactory circuit of the fruit fly \cite{flyvec}.
These embeddings are binary vectors (hash codes) that are formed through neural inhibition via the $k$-Winner-Takes-All ($k$-WTA) activation.
While \flyvec is competitive with traditional word embedding methods like \texttt{word2vec} \cite{word2vec} and \texttt{GloVe} \cite{glove}, it is still superseded by contextual embedding methods such as \bert \cite{bert}.
Although \flyvec demonstrates an elegant approach that is more computationally efficient, it motivates the question of whether the performance can still be improved, while maintaining the same scale and architectural simplicity.

\par
We attempt to tackle this by, yet again, gathering inspiration from how fruit flies learn to identify odors.
Previous research has focused on the spatial bundling and modularity of neurons to create high-dimensional representations.
However, olfactory processing also involves processing temporal patterns, e.g., firing delays and duration. 
We highlight two ways in which fruit flies utilize temporal information.

\par
\textit{Relevance of odor arrival time}: 
The way fruit flies perceive a mixture of odors hints at how the arrival order of stimuli leads to better segregation and classification capabilities.
Multiple odors can be emitted either from the same source or from different sources.
Odors emitted from a single source have an approximately simultaneous arrival time due to the physical process of turbulent diffusion, as opposed to odors propagating from multiple sources, which results in distinct arrival times.
Several studies have shown that fruit flies \cite{10.1371/journal.pone.0036096, hopfield1991olfactory, SEHDEV2019113}, as well as other insects \cite{baker1998moth, saha2013spatiotemporal, nikonov2002peripheral, witzgall1991wind, andersson2011attraction}, likely use this asynchronous arrival time to perceptually segregate mixed odors.

\par
\textit{Using time as a medium to encode odors}:
Olfactory receptor neurons (ORNs) respond in a temporally heterogeneous manner that depends on both the type of odor and the type of ORNs \cite{raman2010temporally}.
This heterogeneity encompasses the firing rate and the time course (latency and duration), which results in odor identification irrespective of stimulus concentration \cite{stopfer2003intensity}.
Mutant flies that are ablated of spatial coding retain odor discriminatory capacity only through the temporal responses of ORNs \cite{dasgupta2008learned, olsen2007excitatory}.

\par
These two perspectives regarding time illustrate how spatial encoding (in the sense of neuron collocation) is \textit{not the only process} employed to generate odor embeddings.
We investigate the impact of integrating this temporal information in the quality of the embeddings.
Our method, \methodname, generalizes \flyvec to incorporate time information (word positions) in the analogy of mirroring text sequences (sentences) to odor segregation.
This is accomplished by expressing the input with phase differences and exploiting the semantics of complex numbers. 
Consequently, we learn a complex-valued parameter matrix through an extension of an unsupervised energy function.
We subject our models to a comprehensive evaluation in semantic sentence similarity tasks, as well as a text-based Reinforcement Learning environment.
Our experiments show how the features generated by \methodname boost the performance significantly, while preserving the parameter complexity, computational efficiency, and biological plausibility of \flyvec.
The main contributions of this work can be summarized as follows:
\begin{itemize}
    \item We introduce a novel architecture that can produce sparse encodings (binary hashes) incorporating positional information, while preserving computational efficiency and significantly boosting performance.
    \item Through a comprehensive quantitative evaluation of the unsupervised representations, we show the enhanced quality of the produced embeddings, being competitive with significantly larger architectures in NLP.
    \item We showcase qualitatively that the simplicity of our approach, \methodname, retains interpretable properties of \flyvec, while extending to the temporal dimension of sequences.
\end{itemize}
We make the source code\footnote{\href{https://github.com/DATEXIS/comply}{https://github.com/DATEXIS/comply}} available to reproduce the results of our experiments.