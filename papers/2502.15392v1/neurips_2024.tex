\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024
\usepackage{svg}

% ready for submission
% \usepackage{neurips_2024}
\usepackage[final]{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{subfigure}
\usepackage{subcaption}


\title{Chitrarth: Bridging Vision and Language \\ for a Billion People}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{Shaharukh Khan, Ayush Tarun, Abhinav Ravi *, Ali Faraz, Akshat Patidar \\ \textbf{Praveen Pokala *, Anagha Bhangare, Raja Kolla, Chandra Khatri *, Shubham Agarwal *} \\ \\
% \\
% \textbf{ }\\ \\
Krutrim AI, Bangalore, India\\
\texttt{* Senior Contributors}\\
\textsuperscript{Contact: \{shaharukh.khan, abhinav.ravi, shubham.agarwal1\}@olakrutrim.com} 
}


% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }


\begin{document}


\maketitle


\begin{abstract}
Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages.
% , such as the Indian languages. 
To address this limitation, we introduce \textit{Chitrarth} (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data.
% , collected through semi-automated translation framework. 
Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating
% and enhancing 
VLMs across various Indian languages, ultimately contributing to more diverse and effective AI systems. Our model achieves SOTA results for benchmarks across low resource languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation to facilitate future advancements in this arena. 
\end{abstract}

\section{Introduction}

% Para 1: Motivation -- India centric
% Para 2: Multimodal LLMs
% Para 3: LLMS
% Para 4: Multilingual LLMS
% Para 5: Contribution

% Focus on "making AI accessible and sustainable for billions of people" and inclusiveness

% In recent years, multimodal foundation models have demonstrated impressive capabilities in integrating and interpreting both visual and textual information~\citep{laurenccon2024matters,tong2024cambrian}. 
% To this end, we propose ....

With the success and demonstrated effectiveness  of \textit{Visual instruction tuning} ~\citep{liu2024visual,liu2024improved}, recent years witnessed a surge of
interest in developing general purpose multimodal conversational agents. These unified foundation models excel at algorithmic reasoning and generic perception tasks like image captioning, visual question answering, text-based image retrieval, etc.~\citep{lu2024deepseek,laurenccon2024matters,tong2024cambrian,xue2024xgen}, and more specialized frameworks, for instance, converting Scalable Vector Graphics (SVGs) to code~\citep{rodriguez2023starvector}. Often, these models rely on pre-trained Large Language Models (LLMs)~\citep{brown2020language,touvron2023llama,achiam2023gpt,chiang2023vicuna,touvron2023llama2,team2023gemini,jiang2024mixtral, team2024gemma, dubey2024llama} as the transformer~\citep{vaswani2017attention} backbones, primarily trained on English or high resource European languages.  
% dubey2024llama
% Webpage screenshots or laurenccon2024unlocking

This work is driven by two main motivations: 1. \textit{Language diversity gap:} Most Vision Language Models (VLMs) are predominantly trained on English datasets, overlooking the linguistic needs of non-English languages, particularly from the Indian subcontinent. 2. \textit{Lack of low resource language benchmarks:} Absence of corresponding VLM benchmarks hinders the progress for these low resource Indic languages. We aim to address these issues through our research and serve a broader audience, encompassing billions of people.
 
% exploring whether an efficient, Bharat-centric VLM can effectively manage the linguistic diversity and complexity of the Indian linguistic landscape.

Few LLMs have been developed specifically for Indic languages ~\citep{gala2024airavata, NavarasaTeluguLLMLabs, balachandran2023tamilllama, kohli2023building}, most of which extend and fine-tune text-only English-centric LLMs. Naturally, they fail to fully capture the nuances of the language, with the exception of models like~\citep{team2024krutrim,bendale2024sutra}, trained from scratch. Our model builds upon the recent success of Krutrim LLM~\citep{team2024krutrim}, which supports English and 10 other languages including Hindi, Bengali, Telugu, Tamil, Marathi, Gujarati, Kannada, Malayalam, Odia, and Assamese, representing a significant portion of the cultural and linguistic diversity in India. 
% n subcontinent.

Another key challenge is the limited availability of low resource data with Indic languages significantly under-represented in Common Crawl despite India (or Bharat) making up 18\% of the global population. For instance, Hindi, in spite of being the third most spoken, does not appear among the top 20 languages~\citep{buck2014n,penedo2023refinedweb}. To enhance our model's cross-lingual generalization abilities, we translate the open-source multimodal training datasets into the 10 Indic languages natively supported by the backbone LLM. Developing this multilingual dataset is a substantial endeavor aimed at addressing the disparity between high-resource 
% languages The construction of this multilingual dataset represents a significant effort in bridging the gap between high-resource languages like English 
and relatively low-resource Indian languages in the context of vision-language models. 


% To enhance our model's ability to generalize across languages, we translate the open-source training datasets into the 10 Indic languages natively supported by the backbone LLM. The construction of this multilingual dataset represents a significant effort in bridging the gap between high-resource languages like English and relatively lower-resource Indic languages in the context of vision-language models. 


% However, these models have predominantly been trained on English-language datasets, which significantly limits their effectiveness for non-English languages. Consequently, a considerable void persists in the development of multilingual-multimodal comprehension tasks apart from English. This limitation is particularly pronounced in linguistically diverse regions such as India, where over one billion people, accounting for nearly 16\% of worlds population, speak a multitude of languages. The disparity in language representation impedes the ability of these models to engage with and support the cultural and linguistic diversity. Our motivation for this study stems from bridging this gap. 


% To further enhance the model's ability to generalize across languages, we translated the open-source datasets into the 10 Indic languages natively supported by our LLM, using an in-house machine translation system. These languages include Hindi, Bengali, Telugu, Tamil, Marathi, Gujarati, Kannada, Malayalam, Odia, and Punjabi, representing a significant portion of the linguistic diversity in the Indian subcontinent. The inclusion of these translations was aimed at providing diverse linguistic inputs, fostering better generalization across languages as done by Krutrim \citep{team2024krutrim}. This approach aligns with recent trends in multilingual model development, which emphasize the importance of incorporating a wide range of languages during pre-training to improve cross-lingual transfer and performance. The dataset comprises high-quality image-caption pairs that cover a wide range of topics, including world knowledge, object properties, spatial relationships, and aesthetic evaluations. The captions are characterized by their descriptive richness, often including detailed observations about the images, contextual information, and nuanced interpretations. This diversity in content helps in training a model that can understand and generate sophisticated descriptions across various domains and visual scenarios. To construct a balanced multi-lingual pre-training dataset, we sampled translations across different languages, including English, ensuring that the pre-training data was capped at 1.2M data points. This sampling strategy was designed to maintain a balance between linguistic diversity and computational efficiency. Half of these data points were retained as original English data, resulting in a robust, multilingual dataset for pre-training. This approach ensures that the model maintains strong performance in English while also developing capabilities in the target Indic languages. The balanced nature of the dataset helps in mitigating potential biases towards any single language and promotes equitable performance across all supported languages.\\
%  By incorporating these translations, we aim to develop a model that can effectively process and generate text in multiple languages, making it more accessible and useful for a diverse global audience. 


% There are various aspects that motivated this work are summarized as: 1. {\bf Language Diversity Gap:} Most Vision Language Models (VLMs) are trained on English-centric datasets, neglecting the needs of other languages, particularly Indian languages. Our model leverages this linguistic distribution to enhance accessibility and usability across different regions catering to over 1 billion people across India. The incorporation of these languages ensures that the model can serve a broader audience, supporting multilingual communication in both textual and visual contexts. 
% 2. {\bf Lack of Benchmarks:} There's an absence of benchmarks that accurately evaluate VLM performance in the context of Indian languages, hindering their development. Thus, we would like to address the following question in this work: {\it Could one develop an efficient Bharat centric VLM that accounts for diversity and complexity of Bharat lingustic landscape?} 
% The accompanying plot \ref{fig:lan} illustrates the prevalence of these languages, highlighting the need for a multilingual approach in vision-language systems. 

In this paper, we present our multimodal LLM, which employs the Krutrim multilingual LLM backbone ~\citep{team2024krutrim} in conjunction with a pre-trained visual image encoder ~\citep{alexey2020image}. Figure\ref{fig:teaser} demonstrates the multi-lingual capability of our model across major Indian languages. A brief summary of our contribution is provided below: 

% [noitemsep]
\begin{itemize}

    \item We introduce Chitrarth (Chitra: Image; Artha: Meaning), a Multimodal LLM model which leverages images and language modalities for a range of visual tasks such as image captioning, visual question answering in the multilinugal context. We further present optimal training recipe including data composition and architecture configuration. 
    % We further present optimal training recipe encompassing data composition, hyper-parameter and architecture configuration. 
    \item We also present \textit{BharatBench}, a comprehensive evaluation benchmark suite designed for 10 under-resourced Indic languages across 3 tasks, which we will make available to the research community upon acceptance.
    % \item We also present BharatBench, a unified evaluation benchmark suite, targeting 10 low-resource Indic languages across 3 tasks which we will release for the research community.     
    \item Finally, we evaluate Chitrarth and prior baselines on both existing English academic datasets as well as the proposed evaluation framework and demonstrate the effectiveness of our model, using different training strategies and ablations, achieving SOTA results on 3 out of 5 English datasets and propose benchmark results on the derived multi-lingual datasets.
    % \item {\bf Bharat-Centric Data:} Training data comprises of state-of-the-art datasets in $10$ major indian languages, which covers the multilingual landscape of Bharat.
    % \item {\bf Bharat Evaluation Benchmarks:} Finally, our evaluation includes: 1. Qualitative benchmark for Indian use cases and 2. Quantitative evaluation of VLMs in Bharat languages.
\end{itemize}


\begin{figure}[htbp]
  \centering
\centerline{\includegraphics[scale=0.27]{resources/_multiLingual_capability.drawio.pdf}}
\caption{\textbf{Multi lingual capability of Chitrarth model across major Indian languages.} For the same underlying image, we present question-answer pairs in English and several Indian languages - Gujarati, Kannada, Hindi, Marathi, Telugu, Tamil, Malayalam, and Bengali (in order). Questions are highlighted in purple, and responses are shown in orange (provided with English translations). The model accurately understands and identifies the `image of a saint writing a book with a feather' and correctly addresses related questions in different languages.}
\label{fig:teaser}
\end{figure}



The remainder of the paper is structured as: Section \ref{sec:relwork} reviews recent related research on VLMs. Section \ref{sec:model} provides a detailed description of our Chitrarth model with information about training data mix in Section \ref{sec:dataset}. Section \ref{sec:bench} introduces the BharatBench evaluation framework that we propose, while Section \ref{sec:exp} presents the experimental results. Finally, Section \ref{sec:conclusion} offers concluding remarks.


\section{Related Work}
\label{sec:relwork}

\subsection{English-centric VLMs}
% Numerous studies \citep{} have investigated English-centric VLMs that connect language models, image encoders, and adapters. 
Recent studies \citep{laurenccon2024matters, laurenccon2024building,tong2024cambrian} have investigated design strategies for multi-stage training pipelines in contemporary VLMs. Typically, these models rely on pre-trained LLMs; however, there are some exceptions where models are trained from scratch \citep{team2024chameleon, lu2024unified}. Prior works like Flamingo\citep{alayrac2022flamingo} leverage a Perceiver Resampler \citep{jaegle2021perceiver} to inject visual features into the language model through cross-attention, promoting quick adaptation to various tasks with few labeled examples. The LLaVA family models \citep{liu2024visual,liu2024improved}, including LLaVA-1.5 and LLaVA-1.6, demonstrated intriguing multimodal comprehension capabilities by integrating advanced language models with vision encoders through visual instruction tuning. 
% PALO \citep{maaz2024palo} showed that LLaVA-1.5 underperforms on multiple languages apart from English. 
PaliGemma~\citep{beyer2024paligemma}, optimized for tasks that require deep integration of visual and textual data, is designed to excel in scenarios where English is the primary language. Florence-2~\citep{xiao2024florence} focuses on handling diverse tasks from simple text prompts addressing the complexity of spatial hierarchy and semantic granularity. The Idefics family~\citep{laurenccon2024matters,laurenccon2024building} is focused on substantially enhancing capabilities around OCR, document interpretation and visual reasoning functionalities. CogVLM~\citep{wang2023cogvlm} drives an intricate fusion of language and vision features unlike other 
% recent 
VLMs, which rely on the shallow alignment method. PALI models~\citep{chen2022pali} on the other hand explored contrastive pretraining and higher resolution training for the VLM tasks. 


\subsection{Multi-lingual VLMs}
Qwen-VL \citep{bai2023qwen} is a multilingual VLM, trained on English and Chinese data, supporting diverse instructions and multi-image context analysis. InternVL 1.5 \citep{chen2024far} proposed an enhanced vision encoder and a superior bilingual dataset, i.e., English and Chinese. Phi-3 family ~\citep{abdin2024phi} offer multilingual, multimodal, and long-context support in 11 languages, including English, across the world but do not cover Indian languages. PALO \citep{maaz2024palo} is the closest VLM to our research, however supporting only 3 Indian languages Hindi, Urdu, and Bengali apart from the other high-to-medium resource language offerings. To our knowledge, no other open-source multimodal LLMs include low-resource Indic languages in the training mix. In contrast, our work introduces a multilingual VLM system that supports ten Indian languages.


% GPT-4 \citep{achiam2023gpt}, the latest model in the GPT models, supports enhanced multilingual, contextual, and reasoning abilities, though its efficacy and coverage of Indian linguistic landscape is not well-documented. 
% The sub optimal performance of existing SOTA models in Indic contexts further motivates our efforts to enhance the model's capabilities as illustrated in fig \ref{fig:indic_context}


%%%%%%%%%%%%%%%
\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
\centerline{\includegraphics[width=0.87\linewidth]{resources/model-neurips.pdf}}
%  \vspace{2.0cm}
  % \centerline{(a) Result 1}\medskip
\end{minipage}
\caption{\textbf{Chitrarth model features a fully autoregressive architecture with a two-stage training process.} In Stage 1, the model is trained using images and their descriptions, aligning visual and linguistic embeddings through image-caption pairs. In Stage 2, model is fine-tuned on multimodal instruction-following and domain-specific academic datasets.}
\label{fig:model}
\end{figure}



\section{Chitrarth: A Multilingual Vision-Language Model}
\label{sec:model}
% In this section, we discuss our proposed Chitrarth VLM architecture.  belongs to the class of auto-regressive VLMs wherein input image is tokenized into visual tokens and concatenated with text tokens then routed as the input to LLM. Proposed model (Cf. Fig.~\ref{fig:model}) draws inspiration from LLaVA framework owing to its adaptability and broad acceptance, and will briefly cover the included modules below. Our \textit{Chitranuvad} model architecture borrows heavily from LLaVA-like models~\citep{liu2024visual,liu2024improved}, where we use pre-trained Krutrim LLM~\citep{team2024krutrim} instead, as the autoregressive multi-lingual LLM backbone. 

In this section, we outline the architecture of our proposed Chitrarth model. Chitrarth is an autoregressive VLM where the input image is tokenized into visual tokens, combined with textual instruction tokens and fed into the large language model (LLM). Inspired by the versatile and widely followed LLaVA~\citep{liu2024visual,liu2024improved} framework, our model incorporates several key components, as illustrated in Figure~\ref{fig:model}, where we use pre-trained Krutrim LLM~\citep{team2024krutrim} instead, as the autoregressive multi-lingual LLM backbone. 

For multimodal training, we start by encoding images using a vision encoder. The modality projection layer (adapter/connector) maps the vision embeddings into the LLM embedding space, producing a sequence of visual tokens. The multilingual LLM then generate responses based on these visual tokens. 
% Krutrim LLM has been pretrained on 10 languages (Hindi, Bengali, Telugu, Malayalam, Kannada, Assamese, Tamil, Sanskrit, Marathi, Gujarati alongwith English) along with custom Indic tokeniser and embeddings. 
The Krutrim LLM~\citep{team2024krutrim} supports a context length of 4096 tokens, of which 576 tokens (14X14 patch size results in 729 tokens) are allocated for image representation after the modality projection. We explore different configurations for the projection layer, including a single-layer projection~\citep{liu2024visual,liu2024improved} and a two-layer MLP vision-language connector with non-linearity~\citep{liu2024visual}. Additionally, we experiment with various vision encoders, including the pre-trained CLIP ViT-L/14@336px~\citep{radford2021learning} and SigLIP-SO400M~\citep{zhai2023sigmoidlosslanguageimage}. Our model is trained in multiple stages:
% , detailed below:

\textbf{Stage 1: Pre-Training (PT) for Feature Alignment.} In this stage, we conduct pre-training using image-text pairs, with the projector layer being trained while keeping the vision encoder and LLM fixed. Each sample is treated as a single-turn conversational instruction for tuning.

\textbf{Stage 2: Instruction Tuning (IT).} In this stage, we maintain the vision encoder in a frozen state, following the approach used in LLaVA models ~\citep{liu2024visual,liu2024improved}. However, unlike the previous stage, we also update the weights of the LLM in addition to tuning the modality projection layer. The objective of this stage is to develop a general-purpose multimodal agent (chatbot) capable of comprehending and executing complex instructions across multiple conversational turns. We describe the datasets used in both the stages in the next section. 


% \begin{itemize}
%     \item {\bf Vision Encoder:} We have explored multiple options to arrive at best-performing model for vision encoder. After thorough evaluation, we selected architecture, namely SigLIP-$400$M \citep{zhai2023sigmoidlosslanguageimage} as the vision encoder in {\it Chitrarth}.
%     \item {\bf Modality Projecter:} We have investigated both a single-layer projection and a two-layer MLP vision-language connector with non-linearity to select the best one within our VLM framework. Best modality projecter turns out to be a two-layer MLP vision-language connector with non-linearity in {\it Chitrarth}.
   
%     \item {\bf Krutrim LLM:} {\it Chitrarth} is powered by the {\it Krutrim} large language model \citep{team2024krutrim}, which has been pretrained on $10$ languages, including key Indic languages like Hindi, Bengali, and Malayalam along with custom Indic tokeniser and embeddings.  {\it Krutrim} LLM model supports a context length of 4096 tokens, out of which 576 tokens are used for the image representation, obtained after the modality projector layer.
          
                
                 
% \end{itemize}
% '''
% The
% Krutrim LLM model supports a context length of
% 4096 tokens, out of which 576 tokens are used for
% the image representation, obtained after the modal-
% ity projector layer. For the projection layer, we
% experiment with both single layer projection (Liu
% et al., 2023b) as well as a two-layer MLP vision-
% language connector with non-linearity
% we use pre-trained Krutrim LLM (Team, 2024b)
% instead, as the autoregressive multi-lingual LLM
% backbone. Our Krutrim LLM is trained across 10
% languages and natively supports all the 3 Indic lan-
% guages (Hindi, Bengali, Malayalam) used as part
% of the shared task
%  We have explored options for vision encoder and such as SIGLIP-$400$M \citep{}, ViT-CLIP $14$ Large \citep{}, ViT-$400$M-448px \citep{}, and ViT-$6$B-$448$px \citep{} for vision encoder. After thorough evaluation, we selected architecture, namely SIGLIP-$400$M \citep{} as the best-performing encoder in {\it Chitrarth}.
% '''

\section{Dataset}
\label{sec:dataset}

Figure \ref{fig:data} illustrates the language distribution of our data mix for both the training stages, which we describe in more detail below: 

\textbf{Stage 1:} For Stage 1 adapter Pre-Training (PT), we use the 1.2 million-sample ShareGPT4V-PT dataset~\citep{chen2023sharegpt4v}, which demonstrated consistent superior performance compared to other PT datasets, such as LLaVA-Pretrain-LCS-558K~\citep{liu2024visual}, in our preliminary experiments. This dataset was subsequently translated into the ten Indic languages supported by the Krutrim LLM. Specifically, we use the open-source model, IndicTrans2~\citep{gala2023indictrans} for this text-only translation task. IndicTrans2 outperformed other translation services (Yandex, ChatGPT, Google Translate, and Bard) in small-scale in-house qualitative human evaluation (win rates 93\% and 80\% for Bengali and Marathi respectively). 
% \footnote{We compared against multiple translation services (Yandex, ChatGPT, Google Translate, and Bard) and IndicTrans2 consistently performed best through small-scale qualitative human evaluation, with win rates like 93\% for Bengali, 80\% for Marathi, and 74\% for Kannada.}. 
% To maintain the dataset's original size of 1.2 million samples, we ensured an equal distribution of English and translated data, achieving a balanced multilingual dataset. 
We ensure the pre-training data remained at 1.2M points, with half of the data in English, and sample translations across different languages in an equal ratio to create a balanced multilingual dataset.
This approach was designed to preserve linguistic diversity and computational efficiency, thereby ensuring robust performance in English while developing capabilities in the Indic languages. The balanced dataset mitigates potential biases towards any single language, fostering equitable performance across all supported languages.


% In our initial experiments, we used the LLaVA-Pretrain-LCS-558K dataset for Stage 1 pre-training. Recent work \citep{tong2024cambrian} indicates that larger adapter datasets, such as the 1.2M ShareGPT4V-PT \citep{chen2023sharegpt4v} image-captioning dataset, improve model performance. 
% We sample translations across different languages (including English) in an equal ratio and ensure that PT data limits to 1.2M data points in our final data mix.
% We also experiment with LLaVA-Pretrain-LCS-558K in our preliminary experiments but found that more adapter data is beneficial for the model~\citep{tong2024cambrian}.  

\textbf{Stage 2:} The Stage 2 Instruction Tuning (IT) dataset is notably more intricate. The core element of this dataset is the complete English version of LLaVA-1.5-665K~\citep{liu2024improved}. Additionally, we translate LLaVA-Instruct-150K~\citep{liu2024visual} into ten languages using the methodology outlined in Stage 1. Our dataset also incorporates the Cauldron dataset~\citep{laurenccon2024matters}, which includes 50 academic vision-language tasks along with its corresponding in-house translations. Furthermore, we add a substantial collection of images reflecting Indian cultural diversity comprising prominent personalities, monuments, artwork, culinary dishes, and more;  transformed into multilingual pluralistic instruction tuning data, analogous to the open-source English-based LLaVA-IT datasets. Lastly, our dataset features high-quality, text-only English proprietary data. The final composition of the dataset includes approximately 880K English and 90K samples in multiple languages, ensuring a balanced and diverse dataset. This comprehensive range of content supports the development of a model capable of generating and understanding complex descriptions across various domains and visual scenarios, thereby enhancing its reasoning capabilities.


% For the second stage instruction tuning, eliciting visual reasoning abilities, we experiment with both LLaVA-Instruct-150K~\citep{liu2024visual} and LLaVA-1.5-665K~\citep{liu2024improved} where we find continued improvements with the 665K version. Similar to pre-training data, we also translated the LLaVA-1.5-665K into multiple languages. 
% Recently released Cauldron dataset~\citep{laurenccon2024matters} is a collection of 50 academic Vision-language tasks. In our final submission, we also include the translated versions and the original English language based Cauldron apart from the proprietary multi-modal dataset in the training mix. This diversity in content helps in training a model that can understand and generate sophisticated descriptions across various domains and visual scenarios, requiring reasoning abilities. 
% This dataset enabled us to focus specifically on instruction generation tasks that require visual reasoning abilities. 

\begin{figure}
    \centering
    \subfigure[]    {\includegraphics[width=0.4\linewidth]{resources/pt_data_chart_svg-tex.pdf}}
    % {\includesvg[width=0.4\textwidth]{resources/pt_data_chart.svg}}
    \subfigure[]
    % {\includesvg[width=0.4\textwidth]{resources/sft_data_chart.svg}} 
    {\includegraphics[width=0.4\linewidth]{resources/sft_data_chart_svg-tex.pdf}}    
    \caption{\textbf{Language distribution in data mix.} (a) Stage 1 data consists of 1.2M ShareGPT4V in the original English version (650K) and remaining Indian language translations (65K each) (b) Stage 2 data involves 879K samples in English and 88K for each respective language, discussed in Section \ref{sec:dataset}.}
    % : Llava 665k SFT + proprietary:  Translations in 10 Indian languages (in thousands) (12k is llava translations + 26K is cauldron translations+ 50K is proprietary)}
    \label{fig:data}
\end{figure}


\section{BharatBench Evaluation Suite}
\label{sec:bench}
% \subsection{BharatBench Evaluation Suite}

Although recent efforts have advanced text-only multilingual evaluation~\citep{ahuja2023mega, singh2024indicgenbench}, there is still a lack of evaluation framework for multimodal multilingual scenarios. We introduce BharatBench, a benchmark designed to assess the image understanding capabilities of multilingual Vision-Language Models (VLMs). 
Expanding upon LLaVa-Bench (In-the-Wild) \citep{liu2024visual}, initially adapted for Hindi and Bengali by \citep{maaz2024palo}, we further broadened the benchmark to cover eight additional low resource languages. This extension now forms part of our comprehensive benchmark suite. Furthermore, we include translated versions of prominent VLM evaluation datasets, such as MMVet \citep{yu2023mm} and POPE~\citep{li2023evaluating}
covering all ten languages in our study, in addition to English.

% for Indic languages. 

% , and LLaVA-Bench (In-the-Wild) \citep{liu2024visual}, 


% Additionally, we propose Indic versions of well-known VLM evaluation benchmarks like MMVet, POPE, and GQA for all ten Indian languages apart from English.
% We present LLaVa-Bench-In-Wild in eight Indian languages, building on the Hindi and Bengali extensions previously introduced by PALO \citep{maaz2024palo}. Additionally, we propose Indic versions of well-known VLM evaluation benchmarks like MMVet, POPE, and GQA for all ten Indian languages apart from English. These Indic benchmarks contribute to advancing the evaluation of VLMs within India's linguistic landscape. 

In essence, we intentionally chose to extend existing benchmarks through translation, which 
% We consciously, extended existing benchmarks through translation, which in essence, 
not only facilitates the creation of valuable multi-way parallel data but also addresses data scarcity issues and leverages the inherent quality of established evaluation frameworks~\citep{singh2024indicgenbench}. This methodology enhances our ability to evaluate and advance multimodal models in a multilingual context. We followed similar guiding principles while creating the training datasets described earlier. 





\section{Experiments}
\label{sec:exp}
% In this section, we discuss the efficacy of the proposed model on various Indic benchmarks while summarizing the vision-language datasets employed during training. In addition, we present ablations on hyper-parameters configuration and vision encoder in our training setup, achieving the best results for the {\it Indian} landscape.
% This section outlines our experimental setup and provides the results of our comparative analyses.

\subsection{Implementation}

We use PyTorch~\citep{paszke2019pytorch} based HuggingFace Transformers~\citep{wolf2019huggingface} for our experiments. Our Stage 1 and 2 tuning use hyperparameters consistent with those of the LLaVA model~\citep{liu2024visual}, unless otherwise specified. Particularly, we train the model for 1 epoch in both the stages with an overall batch
size of 256 in Stage 1 and 128 in Stage 2. We used cosine LR scheduler with Adam optimizer and a learning rate of 2e-3 and 2e-5 in both the stages respectively. We consider IDEFICS 2 ~\citep{laurenccon2024matters} and PALO~\citep{maaz2024palo} as respective English and multi-lingual baselines and report results from their published work. All our models are trained on 8 $\times$ H100 GPUs which takes around 8 hours for Stage 1 and 18 hours for Stage 2 tuning.  

% We consider PALO~\citep{maaz2024palo} as a multi-lingual multi-modal baseline and use the code provided with the repository\footnote{\url{https://github.com/mbzuai-oryx/PALO}}. Our Stage 1 and Stage 2 tuning follow similar hyperparameters as the LLaVA model~\citep{liu2024visual} unless specified otherwise. We conducted multiple experiments for hyper parameter search of learning rate (1e-3, 1e-4, and 1e-5); as well as multiple epochs (1, 2 and 3). We observed rapid over fitting after only one epoch while a learning rate of 1e-4 yielded the highest overall performance. All our further experiments are reported based on this configuration. All our models are trained on 8 $\times$ H100 GPUs which takes around 8 hours for Stage 1 and 18 hours for Stage 2 tuning.  


\begin{figure}[htbp]

\begin{minipage}[b]{1.0\linewidth}
  \centering
\centerline{\includegraphics[scale=0.6]{resources/vlm_capabilities-compressed.pdf}}
\end{minipage}
\caption{\textbf{Multilingual VLM Capabilities.} Our model demonstrates robust performance across various languages in: a) Creative writing, b) Fine-grained attribute extraction, c) Explaining scientific diagrams, d) Screen reading/OCR, e) Anomaly and hazard detection, and f) Real-time accident and incident monitoring.}
    \label{fig:outputs}
\end{figure}

% \begin{figure*}
%     \centering
% \includegraphics[width=1.0\linewidth]{resources/features-compressed.pdf} \caption{Outputs.}
%     \label{fig:outputs}
% \end{figure*}



\subsection{English academic benchmarks}

We also evaluate our model using a range of English academic benchmarks, including VQA-v2~\citep{goyal2017making} and GQA~\citep{hudson2019gqa} for visual perception, VizWiz~\citep{gurari2018vizwiz} for zero-shot generalization on questions posed by visually impaired users, and TextVQA~\citep{singh2019towards} for text-rich visual question answering. We also use POPE~\citep{li2023evaluating} to assess hallucination tendencies, MME~\citep{fu2023mme} for yes/no question responses, and LLaVA-Bench (In-the-Wild) \citep{liu2024visual} and MM-Vet \citep{yu2023mm} for visual conversation capabilities. Evaluation scores are reported following prior works.
% established prior methodologies.
% We evaluate our model using a diverse set of benchmarks, including both academic task-oriented and recent instruction-following frameworks. VQA-v2 \citep{goyal2017making} and GQA \citep{hudson2019gqa} assess visual perception through open-ended short question-answer pairs, while VizWiz \citep{gurari2018vizwiz} tests zero-shot generalization with questions posed by visually impaired users. TextVQA ~\citep{singh2019towards} focuses on text-rich visual question answering. Additionally, POPE \citep{li2023evaluating} measures the model’s tendency to hallucinate, and MME \citep{fu2023mme} evaluates visual perception using yes/no questions. We also use LLaVA-Bench-in-the-Wild \citep{liu2024visual} and MM-Vet \citep{yu2023mm} to assess the model's capabilities in visual conversations by GPT-4. 
% % , concentrating on response accuracy and helpfulness as evaluated by GPT-4. 
% We followed the previous literature to report the evaluation scores for different benchmarks. 

\begin{figure}[htb]

\begin{minipage}[b]{1.0\linewidth}
  \centering
\centerline{\includegraphics[scale=0.27]{resources/radar_hin_bn.pdf}}

% \centerline{\includegraphics[width=8.1cm]{resources/radar_hin_bn.pdf}}
%  \vspace{2.0cm}
  % \centerline{(a) Result 1}\medskip
\end{minipage}
%
\caption{\textbf{Performance against SOTA VLMs on different academic multimodal tasks.} Our model consistenly outperforms IDEFICS 2 (7B) and PALO 7B on different benchmarks while remaining competitive on TextVQA and Vizwiz.}
\label{fig:radar_res}
% Radar graph for comparison against similarly sized (7B) SOTA model Idefics-2 and PALO 7B.
\end{figure}

% \begin{table*}[htbp]
% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% \centering
%     \resizebox{0.99\linewidth}{!}{
% \begin{tabular}{@{}lrrrrrrrrrrlr@{}}
% \toprule
% \textbf{Bench}       & \multicolumn{1}{l}{\textbf{Telugu}} & \multicolumn{1}{l}{\textbf{Hindi}} & \multicolumn{1}{l}{\textbf{Bengali}} & \multicolumn{1}{l}{\textbf{Malayalam}} & \multicolumn{1}{l}{\textbf{Kannada}} & \multicolumn{1}{l}{\textbf{Assamese}} & \multicolumn{1}{l}{\textbf{Tamil}} & \multicolumn{1}{l}{\textbf{Sanskrit}} & \multicolumn{1}{l}{\textbf{Marathi}} & \multicolumn{1}{l}{\textbf{Gujarati}} & \textbf{Odia} & \multicolumn{1}{l}{\textbf{English}} \\ 
% \hline
% \midrule
% \textbf{POPE} & & & & & & & & & & & \\
% CLIP
% & 56.97 & 86.04 & 53.57 & 63.30 & 66.61 & 55.59 & 57.14 & 65.62 & 53.14 & 61.68 & 77.94  & 74.63 \\
% SigLIP
% & - & - & - & - & - & - & - & - & - & - & - & -                         
% \\
% \hline 
% \textbf{LLAVA Bench} & & & & &  \\
% CLIP 
% & 54.80                                & 51.50                               & 53.70                                 & 55.50                                   & 58.10                                 & 59.10                                  & 58.30                               & 58.70                                  & 52.80                                 & 55.90                                  & 62.80          & 67.90                                 \\ 
% SigLIP
% & - & - & - & - & - & - & - & - & - & - & - & - \\
% \hline 
% \textbf{MMVet} & & & & &  \\
% CLIP & 16.30 & 19.60 & 18.30 & 20.69 & 17.01 & 15.19 & 16.01 & 15.01 & 15.39 & 12.18 & 13.39 & 30.49 \\
% SigLIP & - & - & - & - & - & - & - & - & - & - & - & - \\
% \bottomrule
% \end{tabular}
% }
% \caption{\textbf{Performance of Chitrarth on BharatBench.} We provide baseline results across different languages for different datasets.}
% \label{tab:indic_evals}
% \end{table*}

\begin{table*}[htbp]
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\centering
    \resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\textbf{Bench}       & \multicolumn{1}{l}{\textbf{Telugu}} & \multicolumn{1}{l}{\textbf{Hindi}} & \multicolumn{1}{l}{\textbf{Bengali}} & \multicolumn{1}{l}{\textbf{Malayalam}} & \multicolumn{1}{l}{\textbf{Kannada}} & \multicolumn{1}{l}{\textbf{Assamese}} & \multicolumn{1}{l}{\textbf{Tamil}} & \multicolumn{1}{l}{\textbf{Marathi}} & \multicolumn{1}{l}{\textbf{Gujarati}} & \textbf{Odia} & \multicolumn{1}{l}{\textbf{English}} \\ \midrule
\textbf{POPE}   & 79.9 & 78.68 & 83.24 & 85.29 & 85.52    & 55.59  & 83.28  & 79.17  & 84.75 & 82.03  & 87.63   
\\
\textbf{LLaVA-Bench} & 54.8 & 51.5 & 53.7 & 55.5 & 58.1 & 59.1 & 58.3  & 52.80  & 55.90 & 62.80 & 67.90 \\ 

\textbf{MMVet} & 43.76 & 38.85 & 33.24 & 25.36 & 46.19 & 37.29 & 34.31 & 40.96 & 39.03 & 19.67 & 30.49 \\


\bottomrule
\end{tabular}
}
\caption{\textbf{Performance of Chitrarth on BharatBench Evaluation framework.} Our model is unique in its ability to handle all included languages, setting a baseline for future research.}
\label{tab:indic_evals}

\end{table*}


\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
\centerline{\includegraphics[width=0.6\linewidth]{resources/Indic_context.drawio.pdf}}
% \centerline{\includegraphics[width=8.5cm]{resources/Indic_context.pdf}}
\end{minipage}
\caption{\textbf{Performance on images with Indian context.} Chitrarth is able to better understand the images of Image context such as the prominent lady figure (Late Indian Prime Minister Indira Gandhi) in left as well as historical artwork compared to generic and incorrect responses from GPT-4o.}
\label{fig:indic_context}
%
\end{figure}


\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.49\textwidth]{resources/vision_encoder_ablation.pdf}} 
    \subfigure[]
    % {\includesvg[scale=0.3]{resources/siglip_pretraining_loss.svg}} 
{\includegraphics[scale=0.3]{resources/siglip_pretraining_loss_svg-tex.pdf}}
    
    % width=0.43\textwidth
    \caption{\textbf{Ablation on visual encoder choice.} a) SigLIP as the vision encoder consistently performs better than CLIP in the same training regime. b) SigLIP based model also achieve faster convergence as depicted in Stage 1 loss curve. Stage 2 follows a similar pattern.}
    \label{fig:encoder_ablation}
\end{figure}
% width=0.48\textwidth



\begin{figure}[htb]

% \begin{minipage}[b]{1.0\linewidth}
  \centering
\centerline{\includegraphics[scale=0.36]{resources/data_ablation.pdf}}

% \centerline{\includegraphics[width=8.1cm]{resources/data_ablation.pdf}}
%  \vspace{2.0cm}
  % \centerline{(a) Result 1}\medskip
% \end{minipage}
%
\caption{\textbf{Impact of Multi-lingual training Data.} Expanding the number of languages in the training data enhances multilingual capabilities but results in decreased scores on academic English datasets.}
\label{fig:data_ablation}
% Ablation on adding more languages in the training data leading to dip in scores across benchmarks.

\end{figure}


\subsection{Results}

On the English academic datasets, our model depicts State-of-the-art (SOTA) results for POPE, VQAv2 and GQA compared to the baseline models, while remaining competitive on TextVQA and Vizwiz (see radar graph in Figure \ref{fig:radar_res}). On the LLaVA-Bench (Bengali) our model outperforms the multi-lingual baseline PALO and achieves SOTA results of 53.7 points. Table \ref{tab:indic_evals} presents results on BharatBench across various languages, demonstrating that ours is the only model capable of handling all included languages, establishing baseline results for future research. Figure \ref{fig:outputs} showcases selected outputs from our top-performing Multimodal LLM across various languages. The model excels in tasks such as creative writing, fine-grained attribute extraction, explaining scientific diagrams, and screen reading/OCR, while also demonstrates strong capabilities in anomaly and hazard detection, as well as real-time accident and incident monitoring. In our manual qualitative evaluation, we observe that our model is able to better understand the images of Indian context such as the prominent lady figure in Figure \ref{fig:indic_context}, compared to generic and incorrect responses from GPT-4o. This could be attributed to the inclusion of high quality culturally rich images in Stage 2. A further quantitative analysis around this would be interesting but out of scope of this work.  

We conducted an ablation study evaluating various vision encoders and found that SigLIP-SO400M consistently outperforms CLIP ViT-L/14@336px across all English benchmarks, achieving faster convergence (see Figure \ref{fig:encoder_ablation}
). Notably, SigLIP-SO400M yields improvements of 11 points on TextVQA and 13 points on LLaVA-Bench compared to CLIP ViT-L/14@336px. Figure \ref{fig:data_ablation} explores the impact of multilingual training data on the English academic benchmarks. We compare our model's performance when trained with only English, bilingual, and multilingual data across both stages. Consistent with the findings of ~\citep{scao2022language}, expanding the range of languages in the training data improves multilingual capabilities but leads to decreased performance on academic English datasets. This underscores a key challenge in balancing cross-lingual performance.


% On the English academic datasets, our model depicts State-of-the-art (SOTA) results for POPE, VQAv2 and GQA while remaining competitive on TextVQA and Vizwiz (see Figure \ref{fig:radar_res}). Figure \ref{fig:outputs} showcases selected outputs from our top-performing Multimodal LLM across various languages. The model excels in tasks such as creative writing, fine-grained attribute extraction, explaining scientific diagrams, and screen reading/OCR, while also demonstrates strong capabilities in anomaly and hazard detection, as well as real-time accident and incident monitoring. 

% We performed an ablation study whereby we experiment with different vision encoders. We find that SigLIP-SO400M consistenly outperforms CLIP ViT-L/14@336px as the image encoder across all English benchmarks. Notably, we find an uplift of 11 points on TextVQA and 13 points on LLaVA-bench on the existing English benchmarks.   

% Consistent with the observations of ~\citep{scao2022language}, we also note a decline in performance for English when integrating multiple languages into our large language models (LLMs). Initially, using only English data yielded the highest scores. However, when Hindi was introduced alongside English, there was a marginal dip in performance. This trend continued as additional languages were included, resulting in a further decline in scores as illustrated in \ref{fig:data_ablation}. This highlights a significant challenge in maintaining cross-lingual performance. 




% \subsection{Ablation Study}
% We provide ablations that lead the optimum configuration for {\it Chitrarth}, which relies upon {\it Krutrim LLM}, in terms of hyperparameters setting and vision encoder.
% \subsubsection{Vision Encoder}
% We experiment with SIGLIP, Vit-CLIP 14 Large, VIT-400m-448px (from InternVL) and VIT-6B 448px (from InternVL) are the 4 vision encoders on which we did the ablations. VIT-6B 448px  is very bad on Pope (Classification) and MM-Vet (49 and 13.5 respectively). Hence we did not do any further evaluation here.
% \subsubsection{Data ablation}
% We observe that expanding the language set during training leads to a decline in performance across various English benchmarks. Initially, using only English data yielded the highest scores. However, when Hindi was introduced alongside English, there was a marginal dip in performance. This trend continued as additional languages were included, resulting in a further decline in scores as illustrated in \ref{fig:data_ablation}. The observed pattern suggests that increasing the diversity of languages in the training data, while essential for multilingual capabilities, introduces complexities that can challenge model performance on certain benchmarks.



\section{Conclusion}
\label{sec:conclusion}
This paper presents Chitrarth, a multilingual multimodal LLM that is able to have image grounded conversations in English as well as across multiple Indian languages. Our model encodes images using a pre-trained vision encoder~\citep{alexey2020image} and autoregressively generates response using a pre-trained multi-lingual LLM. Empirically, our model outperforms previous baselines for different multimodal tasks. As part of this work, we also introduce BharatBench, a multimodal evaluation framework and provide benchmark results for low resource languages. We anticipate that our research will significantly contribute to the advancement of VLMs for Indian languages, thereby providing substantial benefits to a population exceeding one billion people. 

% \subsection{Limitations and Future Work} 
\textbf{Limitations and Future Work:}
We use an automated translation pipeline for creating multi-lingual training data
% as well as evaluation framework 
which may introduce biases from large language models (LLMs), potentially leading to misrepresentations of cultural symbols and gestures, impacting content accuracy. Addressing these biases requires additional evaluation and targeted training, which we plan to address in the future work. Building on our promising results across 10 low-resource languages, we plan to broaden the language scope in the future research to enhance linguistic diversity and inclusivity in our Vision-Language Models (VLMs). In our current training pipeline, we keep the vision encoder frozen throughout both training stages. However, recent research \citep{laurenccon2024matters, tong2024cambrian} suggests that unfreezing the vision encoder could enhance representation learning. We plan to investigate this approach in future work with higher resolution vision encoders, along with expanding our model's ability to interpret multiple images within a conversational context.
 
% Furthermore, the process may not fully capture cultural nuances, impacting content accuracy, 
% Despite our promising results, 
% In our current training pipeline, we freeze the vision encoder during both stages of training, while recent works have shown that unfreezing the vision encoder helps learn better representations~\citep{laurenccon2024matters,tong2024cambrian}. We plan to explore this in the future along with extending the capabilities to understand multiple images in a conversational turn. 

\section*{Acknowledgements}
We thank Bhavish Aggarwal and the rest of the Krutrim team which helped with model development at various stages. Our models were trained with generous support from Krutrim cloud using Krutrim credits. We also thank the reviewers for their valuable feedback and suggestions. 


\bibliographystyle{plainnat}
\bibliography{anthology-neurips}
% \bibliographystyle{neurips_2024.sty}

% \section*{References}


% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip


% {
% \small


% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.


% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.


% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
% }


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix

% \section{Appendix / supplemental material}


% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
% SA: Removing checklist for arxiv
% \input{checklist}

\end{document}