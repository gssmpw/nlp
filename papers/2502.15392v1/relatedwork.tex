\section{Related Work}
\label{sec:relwork}

\subsection{English-centric VLMs}
% Numerous studies \citep{} have investigated English-centric VLMs that connect language models, image encoders, and adapters. 
Recent studies \citep{laurenccon2024matters, laurenccon2024building,tong2024cambrian} have investigated design strategies for multi-stage training pipelines in contemporary VLMs. Typically, these models rely on pre-trained LLMs; however, there are some exceptions where models are trained from scratch \citep{team2024chameleon, lu2024unified}. Prior works like Flamingo\citep{alayrac2022flamingo} leverage a Perceiver Resampler \citep{jaegle2021perceiver} to inject visual features into the language model through cross-attention, promoting quick adaptation to various tasks with few labeled examples. The LLaVA family models \citep{liu2024visual,liu2024improved}, including LLaVA-1.5 and LLaVA-1.6, demonstrated intriguing multimodal comprehension capabilities by integrating advanced language models with vision encoders through visual instruction tuning. 
% PALO \citep{maaz2024palo} showed that LLaVA-1.5 underperforms on multiple languages apart from English. 
PaliGemma~\citep{beyer2024paligemma}, optimized for tasks that require deep integration of visual and textual data, is designed to excel in scenarios where English is the primary language. Florence-2~\citep{xiao2024florence} focuses on handling diverse tasks from simple text prompts addressing the complexity of spatial hierarchy and semantic granularity. The Idefics family~\citep{laurenccon2024matters,laurenccon2024building} is focused on substantially enhancing capabilities around OCR, document interpretation and visual reasoning functionalities. CogVLM~\citep{wang2023cogvlm} drives an intricate fusion of language and vision features unlike other 
% recent 
VLMs, which rely on the shallow alignment method. PALI models~\citep{chen2022pali} on the other hand explored contrastive pretraining and higher resolution training for the VLM tasks. 


\subsection{Multi-lingual VLMs}
Qwen-VL \citep{bai2023qwen} is a multilingual VLM, trained on English and Chinese data, supporting diverse instructions and multi-image context analysis. InternVL 1.5 \citep{chen2024far} proposed an enhanced vision encoder and a superior bilingual dataset, i.e., English and Chinese. Phi-3 family ~\citep{abdin2024phi} offer multilingual, multimodal, and long-context support in 11 languages, including English, across the world but do not cover Indian languages. PALO \citep{maaz2024palo} is the closest VLM to our research, however supporting only 3 Indian languages Hindi, Urdu, and Bengali apart from the other high-to-medium resource language offerings. To our knowledge, no other open-source multimodal LLMs include low-resource Indic languages in the training mix. In contrast, our work introduces a multilingual VLM system that supports ten Indian languages.


% GPT-4 \citep{achiam2023gpt}, the latest model in the GPT models, supports enhanced multilingual, contextual, and reasoning abilities, though its efficacy and coverage of Indian linguistic landscape is not well-documented. 
% The sub optimal performance of existing SOTA models in Indic contexts further motivates our efforts to enhance the model's capabilities as illustrated in fig \ref{fig:indic_context}


%%%%%%%%%%%%%%%
\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
\centerline{\includegraphics[width=0.87\linewidth]{resources/model-neurips.pdf}}
%  \vspace{2.0cm}
  % \centerline{(a) Result 1}\medskip
\end{minipage}
\caption{\textbf{Chitrarth model features a fully autoregressive architecture with a two-stage training process.} In Stage 1, the model is trained using images and their descriptions, aligning visual and linguistic embeddings through image-caption pairs. In Stage 2, model is fine-tuned on multimodal instruction-following and domain-specific academic datasets.}
\label{fig:model}
\end{figure}