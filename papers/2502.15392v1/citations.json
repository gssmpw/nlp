[
  {
    "index": 0,
    "papers": [
      {
        "key": "",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "laurenccon2024matters",
        "author": "Lauren{\\c{c}}on, Hugo and Tronchon, L{\\'e}o and Cord, Matthieu and Sanh, Victor",
        "title": "What matters when building vision-language models?"
      },
      {
        "key": "laurenccon2024building",
        "author": "Lauren{\\c{c}}on, Hugo and Marafioti, Andr{\\'e}s and Sanh, Victor and Tronchon, L{\\'e}o",
        "title": "Building and better understanding vision-language models: insights and future directions"
      },
      {
        "key": "tong2024cambrian",
        "author": "Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others",
        "title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "team2024chameleon",
        "author": "Team Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      },
      {
        "key": "lu2024unified",
        "author": "Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha",
        "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jaegle2021perceiver",
        "author": "Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao",
        "title": "Perceiver: General perception with iterative attention"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "liu2024improved",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "maaz2024palo",
        "author": "Maaz, Muhammad and Rasheed, Hanoona and Shaker, Abdelrahman and Khan, Salman and Cholakal, Hisham and Anwer, Rao M and Baldwin, Tim and Felsberg, Michael and Khan, Fahad S",
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "beyer2024paligemma",
        "author": "Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others",
        "title": "Paligemma: A versatile 3b vlm for transfer"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xiao2024florence",
        "author": "Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu",
        "title": "Florence-2: Advancing a unified representation for a variety of vision tasks"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "laurenccon2024matters",
        "author": "Lauren{\\c{c}}on, Hugo and Tronchon, L{\\'e}o and Cord, Matthieu and Sanh, Victor",
        "title": "What matters when building vision-language models?"
      },
      {
        "key": "laurenccon2024building",
        "author": "Lauren{\\c{c}}on, Hugo and Marafioti, Andr{\\'e}s and Sanh, Victor and Tronchon, L{\\'e}o",
        "title": "Building and better understanding vision-language models: insights and future directions"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2023cogvlm",
        "author": "Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others",
        "title": "Cogvlm: Visual expert for pretrained language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2022pali",
        "author": "Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others",
        "title": "Pali: A jointly-scaled multilingual language-image model"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "{Qwen-VL}: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "chen2024far",
        "author": "Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others",
        "title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "abdin2024phi",
        "author": "Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others",
        "title": "Phi-3 technical report: A highly capable language model locally on your phone"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "maaz2024palo",
        "author": "Maaz, Muhammad and Rasheed, Hanoona and Shaker, Abdelrahman and Khan, Salman and Cholakal, Hisham and Anwer, Rao M and Baldwin, Tim and Felsberg, Michael and Khan, Fahad S",
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  }
]