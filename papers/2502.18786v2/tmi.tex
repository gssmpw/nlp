\newcommand{\ours}{NeuroTree}

\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{graphicx} % Ensure image handling
\usepackage{tmi}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{booktabs}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\usepackage{subfigure}  % 或改用 \usepackage{subcaption}
\usepackage{multirow}
\usepackage{lineno} 
\usepackage{hyperref}
\usepackage{algorithm,algpseudocode}
\usepackage{tablefootnote}
\usepackage{pifont}
\usepackage{threeparttable}
\usepackage{makecell}
\algnewcommand{\Inputs}[1]{%
  \State \textbf{INPUT:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\algnewcommand{\Initialize}[1]{%
  \State \textbf{Initialize:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\algnewcommand{\ALGORITHM}[1]{%
  \State \textbf{ALGORITHM:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\algnewcommand{\OUTPUT}[1]{%
  \State \textbf{OUTPUT:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \markboth{\journalname, VOL. XX, NO. XX, XXXX 2023}
\markboth{, VOL. XX, NO. XX, XXXX 2023}
% %\markboth{The target journal: IEEE Transactions on Medical Imaging}
% {Yuanyuan \MakeLowercase{\textit{et al.}}: Score-based Diffusion Models With Self-Supervised Learning For Accelerated 3D multi-contrast Cardiac Magnetic Resonance Imaging}
% %\markboth{\journalname, VOL. XX, NO. XX, XXXX 2020}
% \begin{document}
% \pagewiselinenumbers
% \switchlinenumbers
% \title{Score-based Diffusion Models With Self-Supervised Learning For Accelerated 3D multi-contrast Cardiac Magnetic Resonance Imaging}
% \author{Yuanyuan Liu,  Zhuo-Xu Cui, Hairong Zheng, Dong Liang,\IEEEmembership{Senior Member, IEEE}, and Yanjie Zhu
% \thanks{This study was supported in part by the National Key R\&D Program of China no . 2020YFA0712200, National Natural Science Foundation of China under grant nos. 62201561, 62206273, 12226008, 62125111, 62106252, 81971611, and 81901736, the Guangdong Basic and Applied Basic Research Foundation under grant no. 2021A1515110540, China Postdoctoral Science Foundation under grant no. 2022M723302, and the Shenzhen Science and Technology Program under grant no. RCYX20210609104444089.(Corresponding author: Yanjie Zhu) }
% \thanks{Yuanyuan Liu,Zhuo-Xu Cui, and  Dong Liang are with Research Center for Medical AI, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences(e-mail:  $\left \{\text{liuyy; zx.cui; dong.liang}\right \}$@siat.ac.cn).}
% \thanks{ Hairong Zheng, Yanjie Zhu are with Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China $\left \{\text{hr.zheng; yj.zhu; dong.liang}\right \}$@siat.ac.cn).}
% % \thanks{Yuxin Yang, Chentao Cao, Jing Cheng, Caiyun Shi, Haifeng Wang, and Yanjie Zhu are with Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences (e-mail: $\left \{\text{yx.yang; ct.cao; jing.cheng; cy.shi; hf.wang1; yj.zhu}\right \}$@siat.ac.cn).}
{Jun-En Ding \MakeLowercase{\textit{et al.}}: NEUROTREE: Hierarchical Functional Brain
Pathway Decoding in Mental Health Disorders}
%\markboth{\journalname, VOL. XX, NO. XX, XXXX 2020}
\begin{document}
%\pagewiselinenumbers
\switchlinenumbers
\title{\ours: Hierarchical Functional Brain
Pathway Decoding for Mental Health Disorders}
\author{Jun-En Ding, Dongsheng Luo, Anna Zilverstand, Feng Liu \thanks{J. Ding and F. Liu are with the Department of Systems and Enterprises at Stevens Institute of Technology in Hoboken, NJ 07030, United States. J. Ding and F. Liu are also with Semcer Center for Healthcare Innovation at Stevens Institute of Technology. Corresponding author: F. Liu, Email: fliu22@stevens.edu.}
\thanks{D. Luo is with the Department of Computing and Information Sciences at the University of Florida International, Miami, FL 33199, United States.}
\thanks{A. Zilverstand is with the Department of Psychiatry and Behavioral Sciences at the University of Minnesota, Minneapolis, MN 55414, United States.}}

\maketitle
\begin{abstract}
Analyzing functional brain networks using functional magnetic resonance imaging (fMRI) is crucial for understanding psychiatric disorders and addictive behaviors. While existing fMRI-based graph convolutional networks (GCNs) show considerable promise for feature extraction, they often fall short in characterizing complex relationships between brain regions and demographic factors and accounting for interpretable variables linked to psychiatric conditions. We propose {\ours} to overcome these limitations, integrating a $k$-hop AGE-GCN with neural ordinary differential equations (ODEs). This framework leverages an attention mechanism to optimize functional connectivity (FC), thereby enhancing dynamic FC feature learning for brain disease classification.
Furthermore, {\ours} effectively decodes fMRI network features into tree structures, which improves the capture of high-order brain regional pathway features and enables the identification of hierarchical neural behavioral patterns essential for understanding disease-related brain subnetworks. Our empirical evaluations demonstrate that {\ours} achieves state-of-the-art performance across two distinct mental disorder datasets and provides valuable insights into age-related deterioration patterns. These findings underscore the model's efficacy in predicting psychiatric disorders and elucidating their underlying neural mechanisms.
\end{abstract}

\begin{IEEEkeywords}
fMRI, graph neural networks, mental health disorders, 
neural ODE, brain tree
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{I}{n} recent years, neuropsychiatric disorders and addiction have emerged as substantial public health challenges, with their impact on brain function becoming a critical research focus in neuroscience~\cite{sahakian2015impact,hollander2020beyond,borumandnia2022trend}. Substance use disorder (SUD) is widely studied in the field of mental disorders, enabling the investigation of structural brain alterations among SUD users across various factors (e.g., age or gender)~\cite{niklason2022explainable,ding2024spatial}. The development of functional magnetic resonance imaging (fMRI), which measures blood oxygen level-dependent (BOLD) signals, has provided unprecedented opportunities to analyze patterns of brain network activity in mental disorders~\cite{zhang2016neural,li2022resting,niklason2022explainable}. 

Moreover, research on the clustering of connectivity in brain regions using fMRI has demonstrated that the brain does not operate statically but instead exhibits a dynamic, multi-level hierarchical organization~\cite{meunier2009hierarchical,betzel2023hierarchical}. In particular, hierarchical network analysis has been shown to effectively distinguish these abnormal patterns and identify brain regions associated with specific pathological mechanisms in psychiatric disorders~\cite{boisvert2024patterns,segal2023regional}.



Currently, traditional brain network analysis has successfully represented regions of interest (ROIs) as nodes with functional connectivity (FC) as edges~\cite{kulkarni2023interpretable,xu2024abnormal,xu2016network}. Recent research on graph neural networks (GNNs) has shown promising results, demonstrating strong predictive capabilities for neuropsychiatric disorders by learning patterns across brain networks~\cite{tong2023fmri,kazi2019inceptiongcn}.  

Differential equations can capture the dynamic processes underlying changes in brain activity over time. Several studies have employed conventional network-based and dynamic causal modeling (DCM) approaches to capture neural-hemodynamic relationships~\cite{havlicek2015physiologically,nag2023dynamic}. CortexODE further advances these approaches by combining ordinary differential equations (ODEs) with deep learning methods to model MRI images as point trajectories, thereby enabling the reconstruction of complex cortical surfaces~\cite{ma2022cortexode}. Moreover, STE-ODE~\cite{tang2024interpretable} combines the advantages of ODEs with continuous-time fMRI to model dynamic brain networks while leveraging GNN architectures for embedding learning. However, these approaches have several limitations. First, although graph convolutional networks (GCNs) demonstrate excellent node feature learning capabilities in graph structures, they are generally constrained by limited interpretability—particularly regarding the explicability of auxiliary factors in mental disorders. Second, graph-based networks for human brain functional connectivity are restricted to feature learning from dynamic graph sequences, overlooking the brain's hierarchical clustering of network activity. Consequently, there is a lack of effective methodologies for predicting functional brain alterations across different age stages in psychiatric disorders.


To address these limitations, we leverage the properties of ODEs for fMRI modeling like STE-ODE's continuous-time dynamics methods. In this study, we propose {\ours}, a framework designed to characterize brain network patterns through hierarchical, tree-structured pathways. Drawing from neuroscience research indicating that psychiatric disorders affect brain regions in hierarchical patterns, we represent the fMRI network as tree structures. This approach decodes brain network features and facilitates visualization of disorder propagation through brain pathways. Additionally, we develop a $k$-hop ODE-GCN to capture complex interactions between adjacent and distant brain regions and to measure the convergence of various mental disorder subtypes. We incorporate demographic features like age into node representations and message passing to enhance interpretability. Moreover, we implement contrastive masking optimization to identify key functional connectivity patterns. Our main contributions are as follows:

\begin{itemize}
    \item  We integrate interpretable variables into static and dynamic causal graph convolutional modeling through $k$-hop convolution.
    \item We investigate two types of brain disorder datasets using fMRI brain networks, constructing disease-specific brain trees while employing trainable $k$-hop graph convolutions to identify optimal high-order tree paths.
    \item  Our research not only achieves state-of-the-art graph classification performance but also effectively predicts and interprets FC brain age change patterns associated with addiction and mental disorders.
\end{itemize}
% \ding{172} \ding{173}  \ding{174}

\begin{figure*}
\centering

\includegraphics[width=1\textwidth]{images/fig1.png}
\caption{Overview of {\ours} framework.
}
\label{fig:fusion_method}

\end{figure*}



\section{Related Work}

\subsection{High-Order GCNs for Brain Disease Classification.}

In the auxiliary diagnosis of brain diseases, feature extraction and classification methods based on functional connectivity networks (FCNs) have received widespread attention. Early research primarily focused on low-order functional connectivity (LOFC), constructing networks by calculating temporal correlations between brain regions~\cite{gupta2021obtaining,chen2016high}. However, considering only pairwise connections between nodes may overlook information about higher-level interactions among brain regions. Therefore, researchers introduced high-order functional connectivity networks (HOFC), which capture more complex brain network characteristics by calculating correlations between functional connectivity distribution sequences~\cite{zhang2016topographical,zhao2018diagnosis}. Moreover, recent work has shown that traditional first-order GCNs rely on direct neighbor information but struggle to model the long-range dependencies critical for understanding brain functional networks. For instance, one study proposed a multi-channel fusion GCN (MFGCN) that integrates both topographical and high-order functional connectivity for depressive disorder (MDD) classification~\cite{liu2024fusing}. 

%and provide reference to the study that proposed MFGCN. 

The design of higher-order neighbor matrices—exemplified by HWGCN—defines $k$-hop neighbor matrices based on shortest path distances to learn weighted matrices, effectively integrating first-order and higher-order neighborhood information~\cite{liu2019higher}. In fMRI studies of brain disorders, learned masks have been utilized to construct hyperedges and compute weights for each hyperedge in hypergraph construction, revealing maximum information minimum redundancy (MIMR) higher-order relationships among brain regions~\cite{qiu2023learning}.



\vspace{-2mm}
\subsection{Hierarchical and Path-Aware Learning in GNNs.}

Some studies have found that noise connectivity issues in brain networks and the difficulty of stacking deep layers in traditional GCN models lead to over-smoothing problems. Researchers have proposed hierarchical GCNs to capture richer feature representations~\cite{li2022te}. For example, HFBN-GCN designed a coarse-to-fine hierarchical brain feature extraction process with two branches: one focusing on local information transmission within the brain's hierarchical structure and another processing the entire brain network to preserve the original information flow~\cite{mei2022modular}.

Unlike hierarchical GCN approaches, GCKN computes node distances using kernel functions along vertex-connected paths~\cite{chen2020convolutional}. Additionally, PathNNs propose an end-to-end approach for extracting multiple types of paths and updating node representations by encoding paths originating from nodes into fixed-dimensional vectors through LSTM-based path aggregation~\cite{michel2023path}. The HEmoN model~\cite{huang2025identifying} has provided preliminary insights into human emotions through tree path analysis; however, it lacks interpretable representations of higher-order brain pathways. In this study, we propose a weighted higher-order brain tree in a hierarchical pathway framework that enhances pathway representation and effectively explains the predictions of age-related changes in FC networks associated with mental disorders across different age groups.


\section{Preliminaries}
\subsection{Problem Formulation and Notations.}
In this section, we formulate the mental disorder classification problem as a graph classification task, using fMRI networks constructed for each patient. We introduce the temporal brain activity measurements by giving $N$ patients with fMRI data. Let $X_i(t) \in \mathbb{R}^{v}$ denote the blood oxygen level-dependent (BOLD) signals at time point $t$ for subject $i$, where $v$ represents the number of regions of interest (ROIs). The complete time series for each subject can be represented as $X_{i} \in \mathbb{R}^{v \times T}$, where $T$ is the length of the time series. The foundation of brain network analysis lies in representing these temporal patterns through graph structures. For each subject, we construct a functional connectivity (FC) graph $G(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ represents the set of vertices (i.e., brain regions) and $\mathcal{E}$ denotes the set of edges (i.e., functional connections). The functional connectivity between regions is typically computed using Pearson or partial correlation methods. Based on these temporal correlation patterns, we derive both a static adjacency matrix $A^{s}$ from the complete time sequence and time-varying dynamic FC matrices $A^{d}(t)$. The static matrices capture averaged temporal dependencies, while the dynamic matrices preserve the temporal evolution of neural interactions~\cite{zhang2016neural,li2022resting}.

\subsection{Graph Convolutional Networks.}\label{sec:GCN}

Traditional GCNs operate through neighborhood aggregation, whereby each node's representation is updated by aggregating features from its adjacent nodes. The classical GCN layer is formulated as \begin{equation}
    H^{(l)} = \sigma(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}H^{(l-1)}W^{(l-1)}).
\end{equation} where $A$ is the adjacency matrix, $D$ is the degree matrix, $H^{(l)}$ represents the node features at layer $l$, and $W^{(l)}$ contains learnable parameters. However, this formulation is limited to first-order neighborhoods and may not effectively capture the complex interactions among brain region~\cite{velivckovic2017graph}.

\subsection{Neural ODEs for Brain Dynamics.}
Recent work has demonstrated the effectiveness of ordinary differential equations (ODEs) in modeling continuous-time neural dynamics~\cite{tang2024interpretable}. The evolution of brain states can be described by the equation $\frac{dX(t)}{dt} = f(X(t), t)$, where $X(t)$ represents the brain state at time $t$ and $f(\cdot)$ is a neural network that learns the rules governing state transitions. This framework provides a natural way to model the continuous nature of brain dynamics while maintaining biological plausibility~\cite{havlicek2015physiologically}.

\section{The Proposed \ours}

% To address the problems discussed in the above subsection~\ref{sec:problem_formulation},
The proposed {\ours} framework, illustrated in Fig.~\ref{fig:fusion_method}, constructs dynamic FC graphs from subjects' time-series data. It encodes dynamic graph fMRI features using a $k$-hop ODE-GCN for graph embedding and FC strength computation. Subsequently, a CMFS optimization is performed to adjust the FC strength between similar and dissimilar brain regions, followed by tree pruning to create anatomically meaningful hierarchical brain network structures. The resulting brain tree structures are then employed to interpret differences between healthy controls and disease cohorts. Finally, NeuroTree predicts functional age-related changes and compares these with chronological age.

\subsection{$K$-hop ODE-Based Graph Embedding}


The ODE effectively describes the dynamic changes in effective connectivity between different ROI\cite{sanchez2019estimating}. Inspired by existing studies \cite{cao2019functional,wen2024biophysics,tang2024interpretable} that utilize ODEs to model blood-oxygen-level-dependent signals or biomarkers in Alzheimer's disease (AD) research, we extend the ODE modeling framework by incorporating a scalar parameter, $\theta \in \mathbb{R}^+$, representing the chronological age of subjects. This enhancement aims to improve the prediction and interpretability of mental disorders in fMRI analysis. 


\begin{equation}
    \dot{X} = f(X(t),u(t), \theta)
\end{equation}

where $u(t)$ denotes the external experimental stimuli. We consider the influence of an external parameter, $\theta$, on FC across the entire temporal time series $X(t)$, which is represented by adjacency matrix $A^{d}$. The following ODE models this relationship:


\begin{equation}\label{eq:eq2}
\frac{d X(t)}{d t} = \eta A^{d}(t) X(t) + \rho \cdot \theta X(t)  + Cu(t),
\end{equation}
where the matrix \( C \) encodes the reception of external stimuli \( u(t) \) and parameters $\eta$ and $\rho$. To simplify the analysis, we assume that \( Cu(t) = 0 \), indicating the absence of additional external stimuli.
By discretizing Eq. \ref{eq:eq2}, we derive the effective connectivity matrix $A^{d}(t)$. Its discrete representation is given by:
%\vspace{-1mm}
\begin{align}
A^{d}(t) &= \frac{1}{\eta} \left(\frac{d X(t)}{d t} \frac{1}{X(t)} - \rho \cdot  \theta\right)   \\
&\overset{\Delta t = 1}{\approx} \varphi \left( \frac{X(t+1) - X(t)}{X(t)} - \rho \cdot \theta \right)
\end{align}
where $\varphi=\frac{1}{\eta}$ and $\rho$ are a scaling factors ranging from 0 to 1. To address the limitations of first-order feature convolution filters in traditional GCNs, we propose an extended spatial feature filter that operates over a $k$-hop connectivity operator. This approach enables the spatial $k$-order brain network representation to be learned via a generalized graph convolution.

\begin{equation}\label{eq:k_hop_GCN}
\begin{aligned}
H^{(l+1)}(t) &= \sigma\Big(\sum_{k=0}^{K-1} \Phi_k(t)H^{(l)}(t)W_k^{(l)}\Big) 
\end{aligned}
\end{equation}
where $\sigma$ denotes the ReLU function, and $\Phi_k(t) = \hat{D}^{-\frac{1}{2}}\hat{A}_k(t)\hat{D}^{-\frac{1}{2}}$ represents the normalized $k$-hop connectivity adjacency operator. The adjacency operator can be defined as:
\begin{equation}\label{eq:equation7}
\hat{A}_k(t) = \Gamma  \odot A^s \odot  \underbrace{[\lambda A^d(t) + (1-\lambda)(A^d(t))^T]^k}_{\textbf{k-hop connectivity}}.
\end{equation}
where $\Gamma $ is a learnable parameter, the $k$-hop connectivity operator retains directional information, as evidenced by the fact that $\hat{A}_k(t) \neq (\hat{A}_k(t))^T$ when $\lambda \neq \frac{1}{2}$. This result is derived using the binomial theorem, as detailed in Appendix \ref{appendix_A}.



\textbf{Theorem I.\label{the:theorem3.2}} \textit{Suppose the $l^2$-norm of the $k$-hop connectivity adjacency operator $\|\hat{A}_k(t)\|_{2}$ can be derived from Eq.~\eqref{eq:equation7}. Then, as the $k$-hop approaches infinity ($k \to \infty$ ), the $\|\hat{A}_k(t)\|_{2}$  is bounded by the following inequality:}
\begin{equation}\label{eq:bound}
\lim_{k \to \infty} \|\hat{A}_k(t)\|_{2} \leq \|\Gamma\|_{2} \cdot \|A^s\|_{2} \cdot \max(\lambda, 1-\lambda)^k
\end{equation}
The detailed proof of Eq. \ref{eq:bound} is provided in Appendix \ref{the:theorem1}. To ensure numerical stability when computing powers of $\hat{A}_k(t)$, we establish the bounded condition: $\|\Phi_k(t)\|_2 \leq 1$, which holds for all values of $k$ and $t$. In proposition I, we demonstrate that $\Phi_k(t)$ exhibits smooth temporal evolution of the graph structure while maintaining a well-defined upper bound. 


\textbf{Proposition I.}\label{pro:convercence}
\textit{Let $\Phi_k(t)$ be the normalized adjacency operator that satisfies the Lipschitz condition~\cite{coddington1955theory} $\|\Phi_k(t_1) - \Phi_k(t_2)\|_2 \leq L\|t_1 - t_2\|_2$ for some constant $L \geq 0$. Then, for a Lipschitz continuous activation function $\sigma$ and bounded weights $W_k^{(l)}$, the $k$-hop ODE-GCN admits a unique solution. The proof from Picard's existence theorem can be found in Appendix }\ref{Proposition_3.2_proof}.

\textbf{Theorem II.}\label{theorem:3.3}~\cite{tang2024interpretable} \textit{Consider a series of brain networks $\{\mathcal{G}(t;\theta)\}_{t=0}^T$ parameterized by an age variable $\theta$. Let $Z(t)$ denote the embedding (or feature representation) of the STE-ODE network at time $t$. The continuous temporal evolution of the brain network can be expressed through the following age-modulated ODE-based graph convolution representation:}

\begin{equation}
Z(t + \Delta t) = Z(t) + \int_{t}^{t+\Delta t} F(Z(\tau), \tau, \theta) \, d\tau,
\end{equation}
\textit{where $F(\cdot)$ is the graph embedding function that captures the spatiotemporal and age-related information within the network.}

Specifically, the temporal evolution function is defined as:
\begin{equation}
F(Z(t), t, \theta) = \sum_{l=1}^L H^{(l)}(t,\theta),
\end{equation}
with $H^{(l)}(t, \theta)$ representing the age-modulated layer-wise transformation at time $t$. Using a first-order approximation,  the temporal dynamics described in Eq. \ref{eq:k_hop_GCN} and theorem theorem II can be discretized into the following form:
\begin{equation}
\begin{aligned}
Z(t + \Delta t) &\approx Z(t) + \Delta t \cdot F_\theta(Z(t)) \\
&= Z(t) + \Delta t \sum_{l=1}^L \sigma\Big(\sum_{k=0}^{K-1} \Phi_k(t) X_\theta^{(l)}(t) W_k^{(l)}\Big).
\end{aligned}
\end{equation}
Here, $X_\theta^{(l)}(t) = \Lambda\theta H^{(l)}(t)$ denotes the age-modulated features at layer $l$, where $\Lambda$ is a trainable parameter. In this context, $\Phi_k(t)$ represents the polynomial filter coefficients at time $t$, $H^{(l)}(t)$ is the feature matrix at layer $l$, and $W_k^{(l)}$ are the learnable weights associated with the $k$-th hop filter at layer $l$.

For $t \geq 1$, the temporal graph embedding update is expressed as:
\begin{equation}
\begin{aligned}
Z(t+1) &= Z(t) + \text{AGE-GCN}_k(\mathcal{G}^f(t+1), \theta) \\
&= Z(t) + \sigma\Big(\sum_{k=0}^{K-1} \Phi_k(t+1) X_\theta^{(L)}(t+1) W_k^{(L)}\Big)
\end{aligned}
\end{equation}
where $\text{AGE-GCN}_k(\mathcal{G}^f(t+1),\theta)$ denotes the age-modulated $k$-hop graph convolution applied to the graph $\mathcal{G}^f(t+1)$ at the final layer $L$.

\subsection{Attention-Based FC Strength Enhancing}

To enhance the FC between neighborhood nodes representations, we further implement an attention mechanism that computes attention for each timestamp $t \in \{1,2,..., T\}$ in the temporal networks $A^{d}(t)$ for each node $i$. Specifically, we encode the $t$th adjacency matrix $A^{d}$ into a features matrix: $\mathcal{H}=A^{d}W$ and then map it into three  embeddings: the query $\mathcal{H}W^{Q}_{i}$, key $\mathcal{H}W^{K}_{i}$, and value $\mathcal{H}W^{V}$ matrices, where $W^{Q}$,$W^{K}$, and $W^{V}$ $\in \mathbb{R}^{d \times d_{k}}$ are learnable matrices. A single attention head is computed as:


% the to ....  $\mathcal{H}=WA^{d}$, the we can formula attention as:

\begin{equation}
    \text{head}_i = \text{softmax} \left( \frac{\mathcal{H}W_i^Q (\mathcal{H}W_i^K)^T}{\sqrt{d}_{k}} \right) \mathcal{H}W_i^V
\end{equation}
where $d_{k}$ represents the dimensionality of each attention head. Next, we concatenate the outputs of all $h$ heads to form the multi-head attention (MHA):

\begin{equation}
    \mathcal{H}^{\text{MHA}} = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
\end{equation}
where \( W^O \in \mathbb{R}^{h d_k \times d} \) is the projection matrix. For simplicity, we use feature-wise averaging over the $d$-dimensional FC output:

\begin{equation}
C_i(t) = \frac{1}{d} \sum_{k=1}^{d} \mathcal{H}_{i,k}^{\text{MHA}}
\end{equation}
where $\mathcal{H}^{\text{MHA}}_{i,k}$ denotes the $k$-th dimension of the $i$-th node's representation in $\mathcal{H}^{\text{MHA}}$. 



\subsection{Optimization of Objective Function}

This section introduces a node-wise contrastive masked FC (CMF) loss function to enhance inter-FC and intra-FC strength distances.  For each node $i$ at time $t$, the mean connectivity value is defined as $\mu^C(t) = \frac{1}{|\mathcal{V}|} \sum_{i=1}^{|\mathcal{V}|} C_i(t)$. Subsequently, we generate positive and negative masks for connectivity pairs according to $M^{\pm}_{ij}(t) = \mathbb{I} \left( (C_i(t) \gtrless \mu^C(t)) \land (C_j(t) \gtrless \mu^C(t)) \right)$, where $\mathbb{I}(\cdot)$ is the indicator function.  The  positive and negative sample sets are then defined as $\mathcal{A}^{\pm}(t) = \{ (i,j) \mid M^{\pm}_{ij}(t) = 1 \}$ and the CMFS Finally, the CMF loss is expressed as:
\vspace{-1mm}
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{pos}} &= - \frac{1}{|\mathcal{A}^{+}|} 
\sum_{(i,j) \in \mathcal{A}^{+}} 
\log \frac{\exp(S_{ij}(t))}{\sum_{k \in \mathcal{V}} \exp(S_{ik}(t)) + \epsilon}, \\
\mathcal{L}_{\text{neg}} &= - \frac{1}{|\mathcal{A}^{-}|} 
\sum_{(i,j) \in \mathcal{A}^{-}} 
\log \left( 1 - \frac{\exp(S_{ij}(t))}{\sum_{k \in \mathcal{V}} \exp(S_{ik}(t)) + \epsilon} \right).
\end{aligned}
\end{equation}
where $S_{ij}(t) = \frac{\sum_{t=1}^T C_i(t)C_j(t)}{\sqrt{\sum_{t=1}^T C_i(t)^2} \sqrt{\sum_{t=1}^T C_j(t)^2}}$ computes the cosine similarity of FC between nodes $i$ and $j$ at time $t$ and  $\epsilon =  10^{-6} $ is added to prevent division by zero. To optimize the model, the final AGE-GCN output $Z$ was fed into simple multilayer perceptron (MLP) layers for our final two tasks: graph classification $\hat{y}=\text{MLP}(Z)$ and chronological age prediction $\hat{y}_{age}=\text{MLP}(Z)$. We minimized the CMF loss with positive $\mathcal{L}_{\text{pos}}$ and negative $\mathcal{L}_{\text{neg}}$ loss:

\begin{equation}
    \mathcal{L}_{CMF}=\mathcal{L}_{\text{pos}}+\mathcal{L}_{\text{neg}},
\end{equation}
And combine the cross-entropy loss for overall optimizations:
\begin{equation}
    \mathcal{L}=\mathcal{L}_{CMF} + \mathcal{L}_{b}
\end{equation}

% combine it with the binary loss $\mathcal{L}_{b} $ for the overall loss function $\mathcal{L}=\mathcal{L}_{CMF} + \mathcal{L}_{b}$.

\subsection{Node Score Predictor}

Regarding FC, distinct brain regions exhibit characteristic patterns that reflect their specialized roles. To enhance intrinsic functional brain networks, we leverage FC strength distances between brain regions while incorporating age-modulated parameters $\boldsymbol{\Theta} = \{\Lambda, \theta, W_k^{(l)}\}$. Specifically, using the dynamic FC matrices $A^d(t)$, we compute the multi-timestep average node weight $\mathcal{C}=\frac{1}{T}\sum_{t=1}^{T} C(t) \in \mathbb{R}^{|\mathcal{V}|}$ and combine it with the final age-aware $k$-hop graph embedding to generate predictive node scores for individual regions. The score for the $i$th node is expressed as follows:
\begin{equation}\label{eq:node_score}
    \mathcal{S}_{i} =  \mathcal{C}_{i} \cdot \zeta\Big(\frac{1}{\left| \mathcal{V}\right|} \sum_{j \in \mathcal{V}} Z_{j}(\boldsymbol{\Theta})^\top Z_{i}(\boldsymbol{\Theta})\Big), \quad i \in \{1, 2, \ldots, \left| \mathcal{V}\right|\}
\end{equation}
where $\zeta(\cdot)$ denotes a non-linear transformation function and $Z(\boldsymbol{\Theta}) \in \mathbb{R}^{\mathcal{|\mathcal{V}| } \times d}$ represents the node embeddings learned by AGE-GCN with the parameter set $\boldsymbol{\Theta}$.

\subsection{Minimum Spanning Tree}\label{def:MST}

Given a weighted undirected graph $G^u = (V,E,w^u)$, where $w^u(e_{ij})$ assigns a weight to the edge $e_{ij} \in E$ between vertices $i$ and $j$, we define the pruned weighted graph $\mathcal{T}(G^{u}) = (\mathcal{V},\mathcal{E}_{\mathcal{T}},w^u)$, where $\mathcal{E}_{\mathcal{T}} \subseteq E$, $|\mathcal{E}_{\mathcal{T}}| \ll |E|$ represents the set of edges retained in the minimum spanning tree (MST) after pruning. Our goal is to find the minimum spanning tree minimum spanning tree by solving
\begin{equation}
\min_{T \in \mathcal{T}(G^u)} \sum_{e_{ij} \in \mathcal{E}_T} w^u(e_{ij}).
\end{equation}
where $T$ is a spanning tree with edges $\mathcal{E}_T \subseteq \mathcal{E}$ that satisfies the diameter upper bound $\text{diam}(T) \leq \frac{(|\mathcal{V}|-1)(|\mathcal{V}|-2)}{2}$ as shown in \cite{spira1975finding}. In practice, Kruskal's algorithm \cite{kruskal1956shortest} can be applied to compute the minimum spanning tree efficiently.

\section{Hierarchical Brain Tree Construction}

To characterize the hierarchical structure of FC in brain networks associated with mental illness, we construct a weighted undirected graph \( G^u \) from fMRI signals. The edge weights $w^u(e_{ij})$ are derived from the FC strength measured for each ROI connectomee. Following Definition \ref{def:MST}, the pruning process ensures that the set $\mathcal{E}_{\mathcal{T}}$ retains only the significant edges that satisfy the optimization criterion. This approach constructs a hierarchical representation of brain connectivity that emphasizes the structural and functional relationships pertinent to mental illness.

\subsection{Optimal Weighted Tree Path}\label{sec:optimal_weight_path}
Given the pruned graph \(\mathcal{T}(G)\), we examine paths that integrate node importance and high-order FC across multiple tree depths. Let \(P = \{v_1, v_2, \dots, v_k\}\) represent a path of length \(k-1\) in \(\mathcal{T}(G)\), where each node \(v \in P\) is associated with a scalar score \(\mathcal{S}(v; \boldsymbol{\Theta}) \in \mathbb{R}\), computed from Eq.~\ref{eq:node_score}. The edge set of \(P\) is denoted by $E(P) = \{e_{ij} \mid v_i, v_j \in P(v_i, v_j) \in \mathcal{T}(G) \}$, where each edge \(e_{ij} \in E(P)\) represent the path of neighbor connectivity. Let a single learnable node weight be defined: 

\begin{equation}
\mathcal{F}_{ij} = \frac{1}{T} \sum_{t=1}^{T} \sigma \left(\sum_{v_j \in P} A_{ij}^{d}(t) w_j(t) \right),
\end{equation}
where \(A_{ij}^{d}(t)\) represents the adjacency relationship between nodes \(v_i\) and \(v_j\)  at time \(t\), and \(w_j(t)\) is the learnable weight for node \(v_j\). The composite path weight \(\mathcal{W}(P)\) is expressed as:

\begin{equation}\label{eq:all_path} 
\begin{aligned} 
\mathcal{W}(P) = \alpha \kern-0.3cm & \underbrace{\sum_{v \in P} \mathcal{S}(v; \boldsymbol{\Theta})}_{\textbf{Node Score Contribution}} 
\kern-0.3cm + (1 - \alpha) \kern-0.2cm \underbrace{\sum_{s=1}^{S} \sum_{(v_i,v_j) \in E(P)} \mathcal{F}_{ij}^{(s)}}_{\textbf{High-Order FC Contribution}}.
\end{aligned} 
\end{equation}
Here, \(\mathcal{F}_{ij}^{(s)}\) represents the \(s\)-th order FC between nodes \(v_i\) and \(v_j\) along the path and balance parameter $\alpha \in [0,1]$. To solve Eq. \ref{eq:all_path}, we identify the path that minimizes \(\mathcal{W}(P)\) by adjusting the parameter $\alpha$ to balance the contributions of the scoring function \(\mathcal{S}(v;\boldsymbol{\Theta})\) and the connectivity strengths along the path. Additional experimental results can be found in Fig. \ref{fig:tree_weight_path}. According to theorem \ref{theorem:optimal_path} in the Appendix, the optimal path can be determined when $\alpha^{*}$ exist in the interval $[\alpha_{L},\alpha_{U}]$~\cite{xue2000primal}.


\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{images/tree_path.png}
\caption{\textbf{Aggregation for hierarchical neighborhood paths} Fig. (a) illustrates the brain tree structure with connectivity strengths derived from the pruned graph, providing an overall view of the hierarchical organization of functional connectivity. In Fig.(b), the zero-order path is depicted; here,  aggregation initiates at the highest-scoring node, which integrates its immediate neighborhood by combining edges $e_{aj}$ and $e_{ah}$. Fig. (c) shows the aggregation of higher-order paths, where isolated nodes are connected along the shortest weighted paths, thereby capturing more complex connectivity patterns beyond immediate neighbors.}
\label{fig:high_order_path_tree}
\end{figure*}

\subsection{High-Order Tree Path Aggregation}

In high-order tree path traversal, our objective is to aggregate information about the connectivity of tree paths within the pruned graph \(\mathcal{T}(G)\). In practice, we aggregate connectivity information from neighboring edges at each node, forming a weighted tree path. We define the second term in Eq. \ref{eq:all_path}, which represents the high-order FC contribution for an $s$-order connectivity along the \(p\)-th path, as follows:

\begin{equation}\label{eq:high_order_decomp}
\mathcal{F}_{ij}^{(s)} = \mathcal{F}_{ij}^{(0)} + \sum_{p \in \mathcal{P}_s(i,j)} P_s(p)
\end{equation}
where \( \mathcal{F}_{ij}^{(0)}= A^{d}_{ij}\) represents the direct average FC of a 0-th order path, and \(\mathcal{P}_s(i,j)\) denotes the set of all possible s-order paths between nodes \(i\) and \(j\) in \(\mathcal{T}(G)\). For each $s$-order path \(p = (i, k_1, k_2, \ldots, k_s, j)\), where all intermediate nodes are distinct (\(k_m \neq k_n \; \forall m \neq n\)), its contribution is calculated as $P_s(p) = \sum_{v \in p} \mathcal{F}^{(s)}_v$. For a path of order \(s\), its extended connectivity at order \(s+1\)-th through the neighboring node \(k\) can be computed as $\mathcal{F}_{ij}^{(s+1)} = \sum_{k \in \mathcal{N}(i,j)} \left(  \mathcal{F}_{ik}^{(s)} + \mathcal{F}_{kj}^{(s)} \right)$, where \(\mathcal{N}(i,j)\) represents the set of common neighbors between nodes \(i\) and \(j\), and \(\mathcal{F}^{(s)}_{ik}\) denotes the $s$-th order path connectivity from node \(i\) to node \(k\). This formulation allows us to capture direct connections and higher-order structural relationships within the brain network, as illustrated in Fig.~\ref{fig:high_order_path_tree}.


\subsection{Fine-Grained Tree Trunk}

We aim to construct a hierarchical tree structure comprising a trunk and branches distributed across different levels in \(\mathcal{T}(G)\). Our method groups nodes with similar scores into the same hierarchical level integrates the most relevant paths into the $l$-th trunk \(\mathcal{T}_l\) and excludes unrelated paths. For the $l$-th level connected component \(G^* = (V_l, E_l)\), the numbers of nodes and edges decrease monotonically as the level increases—that is, \(|V_l| \leq |V_{l-1}|\) and \(|E_l| \leq |E_{l-1}|\). Following Section \ref{sec:optimal_weight_path}, we identify the shortest path set \(P(v_{start}, v_{end}) = \{v_{start}, v_2, \ldots, v_{end}\}\) that incorporates the weighted tree path and the optimal trunk set \(\mathcal{T^{\prime}} = \{\mathcal{T}_1, \mathcal{T}_2, \ldots, \mathcal{T}_k\}\), as presented in algorithm \ref{fig:tree_algorithm}.


\begin{algorithm}[h]
\caption{Hierarchical Tree Trunk Extraction with Weighted Tree Path Learning}
\begin{algorithmic}[1]
\State \textbf{Input:} Node scores $\{\mathcal{S}(v;\boldsymbol{\Theta})\}_{v \in \mathcal{V}}$, 
pruned graph $\mathcal{T}(G)$ with nodes $\mathcal{V}$ and edges $\mathcal{E}_{\mathcal{T}}$, 
maximum levels $L_{\mathrm{max}}$, scaling factor $\alpha$.
\State \textbf{Output:} Hierarchical trunks $\mathcal{T}^{\prime} = \{\mathcal{T}_1, \mathcal{T}_2, \ldots, \mathcal{T}_{L_{\mathrm{max}}}\}$
\State Initialize $\mathcal{T} \leftarrow \emptyset$.
\State Let $\mathcal{G}(\mathcal{V}, \mathcal{E}_{\mathcal{T}})$ be the pruned graph using MST.
\For{$l \leftarrow 1$ to $L_{\mathrm{max}}$}
    \State  Identify connected components $\{\mathcal{K}_1, \mathcal{K}_2, \ldots, \mathcal{K}_m\}$ in \indent $\mathcal{G}$.
    \For{each component $\mathcal{K}_j$}
        \State Find $\nu_{\mathrm{start}} = \arg\max\limits_{\nu \in \mathcal{K}_j} \mathcal{S}(\nu)$.
        \State Compute shortest paths $\mathcal{P}(\nu_{\mathrm{start}}, \nu)$ for all $\nu \in \mathcal{K}_j$ with weighted path:
        \begin{equation}
        \mathcal{W}(P) = \alpha \sum_{v \in P} \mathcal{S}(v;\boldsymbol{\Theta}) + (1-\alpha) \sum_{s=1}^{S} \sum_{(v_i,v_j) \in E(P)} \mathcal{F}_{ij}^{(s)}
        \end{equation}
        \State Let $\nu_{\mathrm{end}}$ maximize $\mathcal{W}(P)$, and define  $P^* =  \mathcal{P} (\nu_{\mathrm{start}}, \nu_{\mathrm{end}})$;
        \State Append $P^*$ to $\mathcal{T}_l$.
    \EndFor
    \State Remove edges in $\mathcal{T}_l$ from $\mathcal{G}$ and update the graph.
    \State Combine paths: $\mathcal{T} \leftarrow \mathcal{T} \cup \mathcal{T}_l$.
\EndFor
\State \textbf{Return} $\mathcal{T}^{\prime}$.
\label{fig:tree_algorithm}
\end{algorithmic}
\end{algorithm}




\begin{table*}[h]
\caption{Evaluating graph classification performance using five-fold cross-validation. For each method, we computed the most competitive baseline. We then compared the second-best methods (highlighted in blue) and calculated the improvement rate, denoted as "Improv. (\%)".}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{4}{c|}{\textbf{Cannabis}} & \multicolumn{4}{c|}{\textbf{COBRE }} \\
 \hline
\textbf{Model} & \textbf{AUC} & \textbf{Acc. } & \textbf{Prec. } & \textbf{Rec. } & \textbf{AUC} & \textbf{Acc. } & \textbf{Prec. } & \textbf{Rec. } \\
\hline
$Pearson$ GCN & $0.67 \scriptstyle{\pm 0.06}$ & $0.55 \scriptstyle{\pm 0.07}$ & $0.59 \scriptstyle{\pm 0.13}$ & $0.55 \scriptstyle{\pm 0.06}$ & $0.54 \scriptstyle{\pm 0.11}$ & $0.55 \scriptstyle{\pm 0.10}$ & $0.61 \scriptstyle{\pm 0.12}$ & $0.55 \scriptstyle{\pm 0.10}$ \\
$k$-NN GCN & $0.64 \scriptstyle{\pm 0.03}$ & $0.62 \scriptstyle{\pm 0.03}$ & $0.63 \scriptstyle{\pm 0.03}$ & $0.63 \scriptstyle{\pm 0.03}$ & $0.66 \scriptstyle{\pm 0.07}$ & $0.62 \scriptstyle{\pm 0.08}$ & $0.63 \scriptstyle{\pm 0.08}$ &  $0.63 \scriptstyle{\pm 0.08}$ \\
GAT~\cite{velivckovic2017graph} & $0.72  \scriptstyle{\pm0.05}$  &  $0.67  \scriptstyle{\pm0.04}$ & $0.70  \scriptstyle{\pm0.06}$ & $0.67  \scriptstyle{\pm0.04}$ & $0.67 \scriptstyle{\pm 0.08}$ & $0.60 \scriptstyle{\pm 0.11}$ & $0.57 \scriptstyle{\pm 0.21}$ & $0.60 \scriptstyle{\pm 0.11}$ \\
BrainGNN~\cite{li2021braingnn} & $0.67 \scriptstyle{\pm 0.13}$ & $0.59 \scriptstyle{\pm 0.16}$ & $0.51 \scriptstyle{\pm 0.28}$& $0.59 \scriptstyle{\pm 0.12}$ & $0.55  \scriptstyle{\pm 0.11}$ & $0.50  \scriptstyle{\pm 0.02}$ & $0.31  \scriptstyle{\pm 0.11}$ & $0.50  \scriptstyle{\pm 0.02}$ \\
BrainUSL~\cite{zhang2023brainusl}  & $0.63 \scriptstyle{\pm 0.11}$ & $0.65 \scriptstyle{\pm 0.06}$ & $0.62 \scriptstyle{\pm 0.13}$ & $ 0.63 \scriptstyle{\pm 0.11}$ & $0.57 \scriptstyle{\pm 0.10}$ & $0.54  \scriptstyle{\pm 0.04}$ & $0.41 \scriptstyle{\pm 0.18}$ & $0.57  \scriptstyle{\pm 0.11}$ \\
BrainGSL~\cite{wen2023graph} & $0.59 \scriptstyle{\pm 0.11}$ &  $0.65 \scriptstyle{\pm 0.02}$ & $0.67 \scriptstyle{\pm 0.17}$ & $0.65 \scriptstyle{\pm 0.02}$ & $0.55 \scriptstyle{\pm 0.12}$ & $0.51 \scriptstyle{\pm 0.04}$ & $0.45 \scriptstyle{\pm 0.11}$ &  $0.51 \scriptstyle{\pm 0.04}$ \\

\hline
MixHop~\cite{abu2019mixhop} & $0.73 \scriptstyle{\pm 0.05}$ & $0.69 \scriptstyle{\pm 0.03}$ & $0.70 \scriptstyle{\pm 0.04}$ & $0.69 \scriptstyle{\pm 0.03}$ & $\textcolor{blue}{0.69 \scriptstyle{\pm 0.05}}$  & $0.61 \scriptstyle{\pm 0.06}$  & $0.62 \scriptstyle{\pm 0.07}$  & $0.61 \scriptstyle{\pm 0.06}$  \\
GPC-GCN~\cite{li2022brain} & $0.53 \scriptstyle{\pm 0.05}$ & $0.60 \scriptstyle{\pm 0.06}$ & $0.37 \scriptstyle{\pm 0.08}$ & $0.60 \scriptstyle{\pm 0.06}$ & $0.50  \scriptstyle{\pm 0.00}$ & $0.47  \scriptstyle{\pm 0.04}$ & $0.22  \scriptstyle{\pm 0.04}$ &  $0.47  \scriptstyle{\pm 0.04}$ \\
PathNN~\cite{michel2023path} & $0.70 \scriptstyle{\pm 0.10}$ & $0.67 \scriptstyle{\pm 0.04}$ & $0.72 \scriptstyle{\pm 0.12}$ & $\textbf{0.83} \scriptstyle{\pm 0.16}$ &  $0.49 \scriptstyle{\pm 0.01}$ & $0.51 \scriptstyle{\pm 0.05}$ & $0.32 \scriptstyle{\pm 0.27}$ & $0.43 \scriptstyle{\pm 0.46}$ \\
%ST-GCN &  &  &  &  &  &  &  &  \\
\hline
Ours (w/o $\theta$)  & $0.50  \scriptstyle{\pm 0.00}$ & $0.60  \scriptstyle{\pm 0.06}$ & $0.37  \scriptstyle{\pm 0.07}$ & $0.60  \scriptstyle{\pm 0.06}$ & $0.50  \scriptstyle{\pm 0.00}$ & $0.47  \scriptstyle{\pm 0.04}$ & $0.22  \scriptstyle{\pm 0.01}$ & $0.47  \scriptstyle{\pm 0.04}$ \\
Ours (w/o $\mathcal{L}_{CMF}$ ) & $\textcolor{blue}{0.74  \scriptstyle{\pm 0.09}}$ & $\textcolor{blue}{0.72 \scriptstyle{\pm 0.06}}$ & $\textcolor{blue}{0.72  \scriptstyle{\pm 0.07}}$ & $0.72  \scriptstyle{\pm 0.06}$ & $0.63  \scriptstyle{\pm 0.01}$ & $\textcolor{blue}{0.61  \scriptstyle{\pm 0.11}}$ & $\textcolor{blue}{0.61 \scriptstyle{\pm 0.12}}$ &  $\textcolor{blue}{0.61  \scriptstyle{\pm 0.11}}$ \\
{\ours} & $\textbf{0.77}  \scriptstyle{\pm 0.07}$ &$\textbf{0.75} \scriptstyle{\pm 0.06}$ & $\textbf{0.75}  \scriptstyle{\pm 0.06}$  &  $\textcolor{blue}{0.74  \scriptstyle{\pm 0.06}}$& $\textbf{0.71}  \scriptstyle{\pm 0.10}$ & 

$\textbf{0.69}  \scriptstyle{\pm 0.05}$ & $\textbf{0.69}  \scriptstyle{\pm 0.05}$ & $\textbf{0.68}  \scriptstyle{\pm 0.05}$ \\
\hline

Improv. (\%) & $4.05\%$ & $4.16\%$ & $4.16\%$ & - & $2.89\%$ & $3.17\%$  & $3.12\%$ & $3.17\%$ \\
\hline
\end{tabular}
}
\label{table:overall_table}
% \vspace{-4mm}
\end{table*}

\section{Experiments}

In this section, we first evaluate the proposed {\ours} performance in disease classification (section \ref{sec:model_results}) and its application to predicting chronological age (section \ref{sec:age_prediction}). In section \ref{sec:convergence}, we analyze the convergence of the spectral norm for $k$-hop connectivity. Finally, we examine how hierarchical brain trees interpret FC patterns between brain regions and their corresponding seven subnetworks in section \ref{sec:brain_tree}. 

% Detailed experimental settings for {\ours} are provided in Appendix~\ref{appendix:exp}.

\begin{table} 
    \centering
    \caption{Summary statistics of demographics in Cannabis and COBRE} 
    \label{table:demographics} 
    \begin{tabular}{l l c}  
        \toprule
        Dataset & Sample size & \makecell{Age \\ (Mean ± SD)} \\ 
        \midrule
        \multirow{2}{*}{Cannabis} & HC (n=128) & 30.06 $\pm$ 10.47 \\  
                                   & Cannabis (n=195) & 27.23 $\pm$  7.73 \\  
        \midrule
        \multirow{2}{*}{COBRE} & HC (n=72) & 38.31 $\pm$  12.21 \\  
                                & Schizophrenia (n=70) & 36.04  $\pm$ 13.02 \\ 
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Datasets}

We validated two publicly available fMRI datasets —one focusing on cannabis use disorder and the other on schizophrenia. \ding{172} \textbf{Cannabis}~\cite{kulkarni2023interpretable}: The cannabis dataset comprises fMRI data from two distinct sources. The data were preprocessed from 3-Tesla fMRI acquisitions, and the mean time series for each subject was computed across 90 ROIs using the Stanford atlas parcellation~\cite{shirer2012decoding}. \ding{173} \textbf{COBRE}~\cite{calhoun2012exploring}: The Center for Biomedical Research Excellence (COBRE) dataset includes resting-state fMRI data collected from healthy controls and individuals diagnosed with schizophrenia. All MRI data were parcellated into 118 ROI regions using the Harvard-Oxford atlas. We summarized the statistics and demographics of both datasets in Table \ref{table:demographics}.

\textbf{Comparison Models:} Classical methods for constructing brain networks typically rely on Pearson correlation and $k$-nearest neighbor approaches combined with GCNs. The baseline methods evaluated include) the $Pearson$ GCN: This model constructs an fMRI graph using Pearson correlation coefficients to represent relationships between ROIs. 2) $k$-NN GCN: This model builds brain networks by connecting k-nearest neighbors based on similar input features among brain regions. 3) GAT~\cite{velivckovic2017graph}: This model extends the GCN framework by incorporating an attention mechanism that enhances feature aggregation and learning from neighboring nodes.
Addictionally, we also evaluating the performance of {\ours} against state-of-the-art (SOTA) GCN architectures including 4) BrainGNN~\cite{li2021braingnn}: BrainGNN is an interpretable GNN framework designed for analyzing fMRI data. This framework effectively identifies and characterizes significant brain regions and community patterns associated with specific neurological conditions. 5) BrainUSL~\cite{zhang2023brainusl}: BrainUSL learns the graph structure directly from BOLD signals while incorporating sparsity constraints and contrastive learning to capture meaningful connections between brain regions. Finally, we compare {\ours} with other methods that incorporate higher-order node information, specifically MixHop~\cite{abu2019mixhop} and graph path-based models such as GPC-GCN~\cite{li2022brain} and the tree path-based model PathNN~\cite{michel2023path}.


\subsection{Experiments setting}\label{sec:qualitative}
\vspace{-1mm}

In our experimental setup, we employed the following training parameters: scale parameter ($\rho$) of 0.1, 2-time segments ($T$), and a $k$-hop connectivity balance ($\lambda$) of 0.5 between dynamic FC matrices, and multi-head with four heads. We use Area Under the Curve (AUC), Accuracy (Acc.), Precision (Prec.), and Recall (Rec.) as metrics for graph classification, as well as the Mean Squared Error (MSE) metric for the regression task for age prediction. The comparative analysis was conducted using 5-fold cross-validation on two distinct datasets, benchmarking {\ours} against baseline models and SOTA models.

\begin{figure*}
    \centering
    % \vspace{-2mm}
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/k_order_converge.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/COBRE_k_order.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/k_order_converge_cannabis.png}
    \end{minipage}
    \caption{\textbf{Convergence analysis of $\Phi_k(t)$ over $k$-hop} The spectral norm of $\Phi_k(t)$ exhibits differential convergence rates across varying $k$-orders among different subtyping diseases. The cannabis dataset demonstrates a steeper convergence gradient than COBRE as the regularization parameter $\lambda$ increases.}
    \label{fig:spectral_norm}
    % \vspace{-3mm}
\end{figure*}

\subsection{Compare with Brain GCN Models}\label{sec:model_results}

We quantitatively assessed brain network construction using various metrics, employing Pearson correlation and KNN as connectivity measures. As shown in Table \ref{table:overall_table}, the GAT model effectively integrates attention mechanisms into fMRI node features, achieving mean AUC scores of 0.72 and 0.67. In contrast, the conventional GCN is limited to learning solely from topological structures. Furthermore, {\ours} enhances ACC performance in fMRI-based models, improving BrainGNN (ACC $\uparrow$ 27.11\%), BrainUSL (ACC $\uparrow$ 15.38\%), and BrainGSL (ACC $\uparrow$ 15.38\%) on the cannabis dataset. On the COBRE dataset, BrainGNN, BrainUSL, and BrainGSL further improve ACC by 38.00\%, 27.77\%, and 35.29\%, respectively. %The above of results demonstrate...

\subsection{Compare with Path-Based Models}

As shown in Table \ref{table:overall_table}, {\ours} outperforms existing high-connectivity view field-aware graph GCN models, such as the high-order concatenation layer-based MixHop, achieving over 5\% average performance improvement in AUC across all metrics on the cannabis dataset. Furthermore, compared to MixHop on the COBRE dataset, {\ours} not only achieved a 2.8\% increase in AUC, but also demonstrated a mean performance improvement of 13.1\%, 11.3\%, and 11.4\% in ACC, Precision, and Recall. Notably, although GPC-GCN can aggregate multi-level connectivity information by considering the concept of paths, it does not significantly improve classification performance on critical connectivity paths. Similarly, the PathNN framework aggregates path information starting from nodes to generate node representations but shows only limited improvements. In contrast, our {\ours} achieves a 10\% improvement in AUC on the cannabis dataset, while PathNN underperforms compared to MixHop on the COBRE dataset. The experimental results indicate that {\ours} is superior in capturing temporal functional connectivity patterns and predicting outcomes in addiction and psychiatric disorders.

\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{images/Brain_tree_plot.pdf}
\caption{The brain tree visualization illustrates psychiatric disorders structured into three hierarchical trunk levels. Panels (a-1) and (b-1) highlight the most significant nodes along the tree path. The $l$-1 pathways represent regional connectivity, corresponding to the level three brain maps on the right. Panels (a-2) and (a-3) depict the number of connections using color gradients across hierarchical levels.}
\label{fig:brain_tree_plot}
\end{figure*}

\section{Ablation Study}
\subsection{The Impact of Objective Function and Demographics}

In this section, we evaluate the efficacy of CMF loss integration within our proposed {\ours}. The quantitative analysis in Table  \ref{table:overall_table} demonstrates that without $\mathcal{L}_{CMF}$, the model only achieves an AUC of 0.74 on the cannabis dataset and 0.63 on the COBRE dataset. Notably, even without the CMF loss component, our approach still exhibited performance comparable to MixHop across other metrics, particularly in characterizing mental disorder feature classification. With CMF loss integration, {\ours} achieves superior results with AUC values of 0.77 and 0.71, respectively. This enhancement stems from combining the loss function with an attention mechanism optimization framework that simultaneously suppresses connectivity between dissimilar regions while amplifying patterns among similar neurological areas. Furthermore, our ablation experiments demonstrate that age serves as a critical modulator in AGE-GCN by adapting parameter $\theta$, substantially influencing the classification of changes in brain connectivity.


\subsection{Convergence Analysis}\label{sec:convergence}

To further examine the convergence speed of $k$-hop FC in various brain disorders, we analyze the spectral norm under the $k=1$ setting in the convergence of $\Phi_k(t)$. Although the COBRE dataset initially exhibits slower convergence, particularly with the boundary defined around order 15, the overall convergence in the cannabis dataset is faster than in COBRE across different $k$-hop values. Furthermore, when analyzing the convergence of $\Phi_k(t)$ under varying $\lambda$ values, as shown in Fig. \ref{fig:spectral_norm} (middle) and (right), the $\lambda$ values for cannabis are more concentrated and stable. In contrast, COBRE requires larger $\lambda$ values to achieve convergence. The empirical results effectively capture the relationship between dynamic and static FC of $\hat{A}_{k}(t)$. This indicates that addiction may lead to stronger brain connectivity signals compared to psychiatric disorders, as reflected in the affected brain regions.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{images/weighted_tree_path.png}
\caption{Average FC strength weights versus $\alpha$ in high-order tree paths, comparing (a) HC versus cannabis users and (b) HC versus schizophrenia patients. The results indicate reduced connectivity patterns with increasing $\alpha$ in both clinical groups compared to the control group.}
\label{fig:tree_weight_path}
\end{figure}

% \begin{figure}
% \centering
%  \includegraphics[width=0.5\textwidth]{images/Tree_weight_path.png}
% \caption{Average FC strength weights versus $\alpha$ in high-order tree paths, comparing (a) HC vs cannabis users and (b) HC versus schizophrenia patients. The results indicate reduced connectivity patterns with increasing $\alpha$ in both clinical groups compared to the control group.}
% \label{fig:tree_weight_path}
% \end{figure}



\begin{figure*}
\centering
\includegraphics[width=1\textwidth]{images/heatmaps.png}
\caption{Heatmaps illustrate connectivity alterations across Yeo's seven networks during age progression in (a) addiction and (b) schizophrenia. Below, each age stage maps the seven subnetworks in both hemispheres, shown in medial and lateral views.}
\label{fig:heatmaps}
\end{figure*}

\section{Brain Tree Analysis}\label{sec:brain_tree}

This section investigates brain disorders by analyzing hierarchical brain networks using decoded fMRI-derived connectomes. {\ours} deconstructs the brain network into a tree-structured component graph consisting of a main trunk and radial branches. We identify key regions associated with brain disorders within each trunk by analyzing optimal weighted high-order tree paths. Our analysis highlights the most relevant functional network regions determining critical pathways within the brain connectome tree.



\subsection{Exploring Hierarchical Regional Patterns in  Brain Disorders}

%, as illustrated in Fig. \ref{fig:brain_tree_plot}

To better understand how embedded brain features vary across different hierarchical levels of brain subnetworks, {\ours} predicts each ROI score and performs reranking, as illustrated in Fig. \ref{fig:fusion_method}. Higher levels represent trunk pathways of high-scoring nodes, strongly connected edges, and their associated brain ROIs. We mapped the atlas parcellations of both datasets (i.e., 90 ROIs and 118 ROIs) to Yeo's seven-network parcellation~\cite{yeo2011organization}, which includes the visual network (VN), somatomotor network (SMN), dorsal attention network (DAN), ventral attention network (VAN), limbic network (LIM), frontoparietal network (FPN), and default mode network (DMN). Atlas regions not belonging to these networks were classified as 'Others. 

Next, we discuss the addiction brain tree illustrated in Figs. \ref{fig:brain_tree_plot} (a-1) and (a-2), which features three trunk levels. At the primary trunk level (red pathway), the addiction brain tree predominantly exhibits connectivity among DMN (red), FPN (purple), and partial VAN (sky blue) nodes, similarly, in Figs. \ref{fig:brain_tree_plot} (b-1) and (b-2), the schizophrenia brain tree at the main trunk level primarily involves VN (light blue) node, VAN, and partial SMN (green) nodes. At the second hierarchical level (pink pathways), the addiction cohort exhibits predominant connections within DMN and FPN nodes.  In contrast, the schizophrenia group at this level demonstrates connectivity primarily among VAN and some DMN nodes. Finally, at the third hierarchical level (light purple pathways), the addiction group’s terminal branches show minimal connections with FPN, VAN, and DMN nodes. In contrast, the schizophrenia cohort displays connections among VN nodes. 
A key observation is that the addiction brain tree exhibits more distinct communities at different levels, characterized by groups of highly interconnected brain regions. In contrast, the schizophrenia brain tree has a more disordered topological structure, where different networks appear on the same main trunk, leading to a less structured node sequence.


\begin{figure}
%\vspace{-3mm}
\centering
\subfigure[Cannabis]{
\includegraphics[width=0.4\textwidth]{images/Cannabis_comparison.png}
}\hspace{-4mm}
\subfigure[COBRE]{
\includegraphics[width=0.4\textwidth]{images/COBRE_comparison.png}
}
% \vspace{-2mm}
\caption{Age-related changes in predicted FC values across different groups: (a) cannabis users versus healthy controls (HC) and (b) schizophrenia patients versus HC using the COBRE dataset. Asterisks indicate statistical significance ($^*p < 0.05$, $^{**}p < 0.01$, $^{***}p < 0.001$). Error bars represent the standard error of the mean.}
%\vspace{-3mm}
\label{fig:age_comparision}
\end{figure}

\begin{table}
    \centering
    \caption{{\ours} prediction for different age groups.}
    \renewcommand{\arraystretch}{1}
    \begin{tabular}{l|l|c|c}
        \hline
        \textbf{Datasets} & \textbf{Age Group} &  \textbf{Pearson (r)} & \textbf{MSE} \\
        \hline
        \multirow{4}{*}{\shortstack{Cannabis}} 
        & 18-25 (n=150) & $0.63 \scriptstyle{\pm 0.08}$ & $\textbf{7.8}  \scriptstyle{\pm 3.1}$  \\
        & 25-40 (n=127) & $0.77 \scriptstyle{\pm 0.06}$ &  $15.8  \scriptstyle{\pm 5.3}$ \\
        & 40-54 (n=41) & $0.72 \scriptstyle{\pm 0.05}$ & $46.6 \scriptstyle{\pm 22.8}$ \\
        \cline{2-4}
        & \textbf{Overall Prediction} & $\textbf{0.95}  \scriptstyle{\pm 0.01}$ &  $12.5  \scriptstyle{\pm 4.8}$  \\
        \hline
        \multirow{4}{*}{COBRE} 
        & 18-25 (n=35) & $0.92  \scriptstyle{\pm 0.02}$ & $\textbf{5.1}  \scriptstyle{\pm 1.9}$ \\
        & 25-40 (n=56) & $0.77  \scriptstyle{\pm 0.16}$ &  $26.5  \scriptstyle{\pm 16.3}$ \\
        & 40-66 (n=61) & $0.97  \scriptstyle{\pm 0.01}$ &  $19.6 \scriptstyle{\pm 22.0}$ \\
        \cline{2-4}
        & \textbf{Overall Prediction} & $\textbf{0.97}  \scriptstyle{\pm 0.07}$ &  $15.5 \scriptstyle{\pm 6.0}$ \\
        \hline
    \end{tabular}
    \label{table:age_prediction}
\end{table}

\section{Chronological Brain Age Prediction}\label{sec:age_prediction}

In this section, we employed {\ours} to predict mental health disorders and analyze their effects on brain FC changes across different age groups. Participants were stratified into three age-based cohorts, and brain age was predicted in the testing set. First, {\ours} predicted FC values across age groups. The results indicate that as age increases, the addiction and mental disorders cohorts exhibit substantial age-related changes in FC, indicating that cannabis use and schizophrenia have similar impacts on the brain FC, as illustrated in Fig. \ref{fig:age_comparision}. Second, we present Table \ref{table:age_prediction}, validating {\ours} predictions of brain age compared to chronological age.  

Overall, in {\ours} age prediction,  the cannabis cohort exhibited a lower MSE than the COBRE cohort. Additionally, we calculated the Pearson correlation coefficient (r) between predicted and actual ages, achieving values exceeding 0.95. Notably, for psychiatric disorder predictions, the MSE for the 18–25 age group was substantially lower, with values of 7.8 for cannabis and 5.1 for COBRE. This finding indicates that younger brains exhibit more stable development patterns, higher neuroplasticity, and more consistent characteristics with smaller individual variations. Furthermore, these brains are less impacted by long-term environmental factors. Specifically, the correlation coefficient (r) reached 0.92 for COBRE predictions related to psychiatric disorders in the 18–25 age group. This high accuracy in younger subjects is likely due to their more stable functional connectivity patterns, which are less affected by neurodegeneration. Additionally, our model achieved an MSE of 19.6 and a correlation coefficient of 0.97 when predicting psychiatric disorders among older adults (40–66 years). However, prediction accuracy for this age group requires consideration of additional variables due to the increasing complexity of psychiatric manifestations with advancing age.




\section{Discussion}
\vspace{-2mm}
In section \ref{sec:brain_tree}, our analysis of the tree-structured topology revealed distinct patterns of brain subnetwork alterations associated with addiction and schizophrenia. These changes were primarily concentrated in networks governing cognitive functions, including the DMN and the decision-making FPN. This network-specific disruption pattern aligns with previous findings~\cite{kleinhans2020fmri,kulkarni2023interpretable,ding2024spatial}. Additionally, cannabis use increases functional connectivity between the DMN and VAN, which substantially correlates with subjective feelings of being 'high'~\cite {ramaekers2022functional}.  Our findings also support previous research indicating that patients with schizophrenia exhibit considerably reduced FC in the left VAN and VN, with these connectivity abnormalities potentially contributing to perceptual and attentional dysfunction~\cite{hou2023spatiotemporal,zhu2023higher}.

Moreover, we identified age-related FC subnetwork changes associated with mental disorders, as shown in Fig. \ref{fig:heatmaps}. In addition, alterations primarily originate in the limbic system (LIM) and progressively extend to networks responsible for executive function (FPN), attention control (DAN, VAN), internal cognition (DMN), and motor-related functions (SMN) with increasing age. In contrast, schizophrenia initially affects the frontoparietal network and limbic system (FPN \& LIM) before expanding to broader higher-order and sensorimotor networks (VN, DMN, SMN, VAN, DAN). These findings align with Fig.~\ref{fig:age_comparision}, illustrating dynamic FC changes that evolve with age.
\vspace{-8mm}
\section{Conclusion}

This study proposed {\ours}, a novel hierarchical brain tree framework that integrates k-hop AGE-GCN with neural ODEs and an attention-based contrastive loss to decode functional brain pathways in psychiatric disorders. {\ours} effectively incorporates demographic factors to improve brain disease classification and enhances interpretability in predicting brain age while revealing distinct hierarchical connectivity patterns within the DMN, FPN, and VAN across different mental disorders. Additionally, these findings demonstrate that different subtypes of mental disorders exhibit varying trajectories of FC changes across age periods. These insights contribute to the neuroscientific understanding of functional alterations induced by mental disorders, potentially informing therapeutic interventions and pharmacological treatments.

% \newpage


%\section*{Appendix }





% \setcounter{table}{0}
% \setcounter{figure}{0}
% \renewcommand\thefigure{S\arabic{figure}}
% \renewcommand\thetable{S\arabic{table}} 

% \subsection{Motion Correction}
% The respiratory motion correction process consists of three main steps: 1. Image Acquisition: 2D image navigators (iNAVs) \cite{henningsson2012whole} are acquired prior to each volume by spatially encoding the 14 ramp-up pulses of the bSSFP readout in each heartbeat. These iNAVs are low-resolution 2D images in the coronal plane, as shown in Figure \ref{figure S1}(a). 2. Motion Estimation: The 2D translational motion, including foot-head (FH) and left-right (LR) directions, is estimated using a template-matching algorithm. A template (indicated by the white rectangle in \ref{figure S1}(a))  is manually selected around the heart, and FH and LR beat-to-beat translations are calculated using a mutual information similarity measure. An example of an estimated FH motion curve for an MC image is provided in Figure \ref{figure S1} (b). 3. Motion Correction: Motion correction is applied by modulating the k-space data with a linear phase shift to align it with a reference position at end-expiration. Zero-filling reconstructions of the undersampled 3D-MC-CMR datasets before and after motion correction are shown in Figures \ref{figure S1}(c) and \ref{figure S1}(d), respectively. 


% \bibliographystyle{unsrt}
% \bibliography{SMART_Bib}


\appendices
\section{Appendix: Proof Formula~\ref{eq:equation7} for K-hop Connectivity }\label{appendix_A}

(\textbf{K-hop Connectivity}) \label{sec:theorem_k_hop}
Given the $k$-hop connectivity adjacency operator:
\begin{equation}
\hat{A}_k(t) = \Gamma \odot A^s \odot \big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^k,
\end{equation}
the k-hop connectivity can be expressed as:
\begin{equation}
\begin{aligned}
    \big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^k 
    & \\
    = \sum_{i=0}^k \binom{k}{i} \lambda^i (1-\lambda)^{k-i} 
    \big(A^d(t)\big)^i \big((A^d(t))^T\big)^{k-i}.
\end{aligned}
\end{equation}

\vspace{4mm}
\begin{proof}
For \(k = 1\), the formula holds trivially:
\[
\big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^1 = \lambda A^d(t) + (1-\lambda)(A^d(t))^T.
\]
\end{proof}

Assume the formula holds for \(k = n\):

\begin{align*}
    \big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^n 
    = \sum_{i=0}^n & \binom{n}{i}\lambda^i(1-\lambda)^{n-i} \\
    & \kern-0.7cm \times \big(A^d(t)\big)^i\big((A^d(t))^T\big)^{n-i}.
\end{align*}

For \(k = n+1\), expand:

\begin{align*}
    \big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^{n+1} 
    & \\
    = & \big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^n \\
    & \kern-0.7cm \times \big[\lambda A^d(t) + (1- \lambda)(A^d(t))^T\big].
\end{align*}


Substituting the inductive hypothesis and applying Pascal’s identity:

\[
\binom{n+1}{i} = \binom{n}{i-1} + \binom{n}{i},
\]

We obtain:

\begin{align*}
    \big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^{n+1} 
    & \\
    = \sum_{i=0}^{n+1} & \binom{n+1}{i}\lambda^i(1-\lambda)^{n+1-i} \\
    &  \kern-0.7cm \times \big(A^d(t)\big)^i\big((A^d(t))^T\big)^{n+1-i}.
\end{align*}


Thus, by induction, the formula holds for all $k$. And we have following corollary in \ref{sec:connectivity_properties}
\vspace{4mm}
\begin{corollary}[Asymmetric Property of $K$-hop Operator]\label{sec:connectivity_properties}
The k-hop operator preserves directional information:
\begin{equation}
\hat{A}_k(t) \neq (\hat{A}_k(t))^T, \forall \lambda \neq \frac{1}{2}
\end{equation}

\begin{proof}
By the definition of the $k$-hop connectivity operator:
\begin{equation}
\hat{A}_k(t) = \gamma \odot A^s \odot \big[\lambda A^d(t) + (1-\lambda)(A^d(t))^T\big]^k
\end{equation}

Applying the binomial expansion from \ref{sec:theorem_k_hop}:
\begin{equation}
\begin{aligned}
\hat{A}_k(t) &= \Gamma \odot A^s \odot \sum_{i=0}^k \binom{k}{i}\lambda^i(1-\lambda)^{k-i}(A^d(t))^i((A^d(t))^T)^{k-i}
\end{aligned}
\end{equation}

Taking the transpose of $\hat{A}_k(t)$:
\begin{equation}
\begin{aligned}
(\hat{A}_k(t))^T 
&= \Gamma^T \odot (A^s)^T \odot \sum_{i=0}^k \binom{k}{i}\lambda^i(1-\lambda)^{k-i} \\
& \qquad \times ((A^d(t))^i)^T(((A^d(t))^T)^{k-i})^T \\
&= \Gamma \odot A^s \odot \sum_{i=0}^k \binom{k}{i}\lambda^i(1-\lambda)^{k-i} \\
& \qquad \times ((A^d(t))^T)^i(A^d(t))^{k-i}
\end{aligned}
\end{equation}

When $\lambda \neq \frac{1}{2}$, the weights $\lambda^i$ and $(1-\lambda)^{k-i}$ are asymmetric, meaning:
\begin{equation}
\lambda^i(1-\lambda)^{k-i} \neq \lambda^{k-i}(1-\lambda)^i
\end{equation}

Therefore, $\hat{A}_k(t) \neq (\hat{A}_k(t))^T$ unless all terms in the expansion are zero.
\end{proof}
\end{corollary}






\section{Appendix: Proof of Theorem ~\ref{the:theorem3.2}}\label{the:theorem1}
We prove that $\|\hat{A}_k(t)\|_{2}$ has an upper bound  as $k$-hop approaches infinity:

\begin{equation}
\lim_{k \to \infty} \|\hat{A}_k(t)\|_{2} \leq \|\Gamma\|_{2} \cdot \|A^s\|_{2} \cdot \max(\lambda, 1-\lambda)^k
\end{equation}

\begin{equation}
\lim_{k \to \infty} \|\hat{A}_k(t)\|_{2} \leq \|\Gamma\|_{2} \cdot \|A^s\|_{2} \cdot \max(\lambda, 1-\lambda)^k
\end{equation}

\begin{proof}
    Using matrix norm properties:  

    \begin{equation}
        \|A \odot B\|_{2} \leq \|A\|_{2}\cdot \|B\|_{2}
    \end{equation}
%We have:
Applying the triangle inequality when $\lambda \in [0,1]$, we obtain:
    \begin{equation}\label{eq:convex}
        \|\lambda A + (1-\lambda)B\|_{2} \leq \max(\lambda, 1-\lambda)(\|A\|_{2} + \|B\|_{2})
    \end{equation}
    
Applying the $l^2$-norm to $\hat{A}_k(t)$:


\begin{equation}
\|\hat{A}_k(t)\|_{2} = \|\Gamma \odot A^s \odot [\lambda A^d(t) + (1-\lambda)(A^d(t))^T]^k\|_{2}
\end{equation}

And we have:

\begin{equation}\label{eq:eq_29}
\|\hat{A}_k(t)\|_{2} \leq \|\Gamma\|_{2} \cdot \|A^s\|_{2} \cdot \|[\lambda A^d(t) + (1-\lambda)(A^d(t))^T]^k\|_{2}
\end{equation}

Using equation \ref{eq:convex}, we bound:

\begin{equation}
\begin{aligned}
\|[\lambda A^d(t) + (1-\lambda)(A^d(t))^T]\|_{2} 
   &\leq \max(\lambda, 1-\lambda) \\
   &\qquad \kern-0.7cm \times (\|A^d(t)\|_{2} + \|(A^d(t))^T\|_{2})
\end{aligned}
\end{equation}
Since \(\|(A^d(t))^T\|_{2} = \|A^d(t)\|_{2}\), we obtain:

\begin{equation}\label{eq:eq31}
\begin{split}
\|[\lambda A^d(t) + (1-\lambda)(A^d(t))^T]\|_{2} & \leq 2\max(\lambda, 1-\lambda)\|A^d(t)\|_{2} 
\end{split}
\end{equation}

Taking the power of $k$ on both sides of equation~\ref{eq:eq31}: $\|\hat{A}_{k}(t)\|_{2} \leq \|\hat{A}(t)\|_{2}^{k}$ and the following inequality holds:

\begin{equation}
\|[\lambda A^d(t) + (1-\lambda)(A^d(t))^T]^k\|_{2} \leq \max(\lambda, 1-\lambda)^k
\end{equation}

Substituting into equation \ref{eq:eq_29}, we get:

\begin{equation}
\|\hat{A}_k(t)\|_{2} \leq \|\Gamma\|_{2} \cdot \|A^s\|_{2} \cdot \max(\lambda, 1-\lambda)^k.
\end{equation}

Since $\max(\lambda, 1-\lambda) < 1$ for $\lambda \in (0,1)$, the series converges geometrically. This result indicates that \(\|\hat{A}_k(t)\|\) converges to zero as \(k \to \infty\), provided the spectral radius of \(A\), denoted by \(\lambda\), satisfies \(\max(\lambda, 1-\lambda) < 1\). Consequently, the rate of decay of \(\|\hat{A}_k(t)\|\) is governed by \(\max(\lambda, 1-\lambda)^k\), which demonstrates the dependence on the spectral properties of \(A\) and ensures convergence for sufficiently small \(\lambda\).  
\end{proof}

\section{Appendix: Proof for Proposition I}\label{Proposition_3.2_proof}
To prove the proposition I, we express the neural ODE system as:


\begin{equation}
    \frac{dH^{(l)}(t)}{dt} = F(H^{(l)}(t), t)
\end{equation}
where
\begin{equation}
    F(H^{(l)}(t), t) = \sigma\left(\sum_{k=0}^{K-1} \Phi_k(t)H^{(l)}(t)W_k^{(l)}\right)
\end{equation}
\begin{proof}
Assume $\sigma$ is a differentiable function and continuous in $t$. We show that $F$ satisfies Lipschitz continuity in $H^{(l)}$ and continuity in $t$. Let $H^{(l)}_{1}$ and $H^{(l)}_{2}$ be two output states:

\begin{align}
    &\|F(H_1^{(l)}, t) - F(H_2^{(l)}, t)\|_2 \notag \\
    &= \left\|\sigma\left(\sum_{k=0}^{K-1} \Phi_k(t)H_1^{(l)}W_k^{(l)}\right) - 
    \sigma\left(\sum_{k=0}^{K-1} \Phi_k(t)H_2^{(l)}W_k^{(l)}\right)\right\|_2
\end{align}
since $\|F(H_1^{(l)}, t) - F(H_2^{(l)}, t)\|_2$ is bounded by the Lipschitz constant of $\sigma$, denoted by $L_{\sigma}$, we obtain:

\begin{equation}
    \|F(H_1^{(l)}, t) - F(H_2^{(l)}, t)\|_2 \leq L_\sigma \left\|\sum_{k=0}^{K-1} \Phi_k(t)(H_1^{(l)} - H_2^{(l)})W_k^{(l)}\right\|_2
\end{equation}

Using matrix norm properties and $\|\Phi_k(t)\|_2 \leq 1$ and 
$W_k^{(l)}$ are bounding the norms. The bound equation can be written as:


\begin{equation}
    \|F(H_1^{(l)}, t) - F(H_2^{(l)}, t)\|_2 \leq L_\sigma \sum_{k=0}^{K-1} \|W_k^{(l)}\|_2 \|H_1^{(l)} - H_2^{(l)}\|_2
\end{equation}

Let  $M = \max_k \|W_k^{(l)}\|_2$. Then:

\begin{equation}
    \|F(H_1^{(l)}, t) - F(H_2^{(l)}, t)\|_2 \leq L_\sigma KM \|H_1^{(l)} - H_2^{(l)}\|_2
\end{equation}
Thus, $F$ is Lipschitz continuous in $H^{(l)}$ with Lipschitz constant $L_F = L_\sigma KM$.  For continuity in $t$, we use the Lipschitz condition of $\Phi_k(t)$:
\begin{equation}\label{eq:eq_30}
    \|\Phi_k(t_1) - \Phi_k(t_2)\|_2 \leq L\|t_1 - t_2\|_2
\end{equation}

Thus, Eq. \ref{eq:eq_30} ensures that $F$ is continuous in $t$. According to Picard's existence theorem, the $k$-hop ODE-GCN has a unique solution.
\end{proof}

\section{Appendix: Finding the Shortest Paths in a Weighted Brain Tree}

(\textbf{Optimal Path Extremes})\label{theorem:optimal_path}
Let \(\mathcal{W}(S) = \sum_{v \in P} S(v; \boldsymbol{\Theta})\) represent the node score contribution and \(\mathcal{W}(C) =\sum_{e_{ij} \in E(P)} \mathcal{C}_{i,j}\) represent the connectivity strength contribution in the pruned graph \(\mathcal{T}(G)\). For an arbitrary integer threshold $\lambda^{*}$, the optimal path \(P^{*}\) satisfies the following interval constraint $\mathcal{W}(S) \leq \lambda^{*} \leq\mathcal{W}(C)$.

In practice, the minimum interval can be determined using the bisection method within $[\alpha_{L},\alpha_{U}]$ using a bisection method such that $\mathcal{W}(S) \leq \mathcal{W}_{\alpha_{L}}(P^{*}) \leq \alpha^{*} \leq \mathcal{W}_{\alpha_{U}}(P^{*}) \leq\mathcal{W}(C)$~\cite{xue2000primal}.


\bibliography{Ref_Bib}
\bibliographystyle{IEEEtran}

\end{document}

