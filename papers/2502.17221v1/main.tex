% \documentclass[lettersize,journal]{IEEEtran}
\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}
\usepackage{amsmath,amsfonts}
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
%\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{paralist}
 
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}
\usepackage{algpseudocode}
\usepackage{balance}
\usepackage{multirow}
\usepackage{glossaries}
\usepackage{CJK}
\usepackage{booktabs}
\setacronymstyle{long-short}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{microtype} % reduce spaces
\usepackage[export]{adjustbox}
\usepackage{dblfloatfix}
\DeclareUnicodeCharacter{2212}{-}
\hyphenation{in-di-vi-duals}
\usepackage{array}
\usepackage{hyperref}  
\usepackage{microtype} % reduce spaces
 
\newcommand{\Tau}{\mathrm{T}}
 
% \hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
 
\begin{document}
 
\title{%Cent}
A Reinforcement Learning Approach to Non-prehensile Manipulation through Sliding}
 
\author{Hamidreza Raei$^{1,2}$, Elena De Momi$^{2}$, Arash Ajoudani$^{1}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^1$HRI$^2$ Lab, Istituto Italiano di Tecnologia, Genoa, Italy. {\tt\small hamidreza.raei@iit.it}}%
\thanks{$^{2}$ Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy.}
% \thanks{$^{3}$ Robotics and Mechatronics lab, Systems Engineering and Automation Department, University of Malaga, Malaga, Spain.}%
\thanks{This work was supported by the European Commission's Marie Skłodowska-Curie Actions (MSCA) Project RAICAM (GA 101072634) and Horizon Europe TORNADO (GA 101189557).}%
}
 
% The paper headers
% \markboth{}
% {Raei \MakeLowercase{\textit{et al.}}: A Reinforcement Learning Approach to Non-prehensile Manipulation through Sliding}
 
 
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.
 
 
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
% Deployment of robots in various applications, has emphasized the capability of rapid manipulation, and most current manipulation techniques rely on grasping which is precise but slow. non-prehensile manipulation makes fast interaction with the environment possible. This study has introduced an effective implementation DDPG reinforcement learning algorithm that is used in non-prehensile manipulation of sliding an object precisely. The effectiveness of the proposed method is illustrated both in simulation and experiment.





Although robotic applications increasingly demand versatile and dynamic object handling, most existing techniques are predominantly focused on grasp-based manipulation, limiting their applicability in non-prehensile tasks. To address this need, this study introduces a Deep Deterministic Policy Gradient (DDPG) reinforcement learning framework for efficient non-prehensile manipulation, specifically for sliding an object on a surface. The algorithm generates a linear trajectory by precisely controlling the acceleration of a robotic arm rigidly coupled to the horizontal surface, enabling the relative manipulation of an object as it slides on top of the surface. Furthermore, two distinct algorithms have been developed to estimate the frictional forces dynamically during the sliding process. These algorithms provide online friction estimates after each action, which are fed back into the actor model as critical feedback after each action. This feedback mechanism enhances the policy's adaptability and robustness, ensuring more precise control of the platform's acceleration in response to varying surface condition. The proposed algorithm is validated through simulations and real-world experiments. Results demonstrate that the proposed framework effectively generalizes sliding manipulation across varying distances and, more importantly, adapts to different surfaces with diverse frictional properties. Notably, the trained model exhibits zero-shot sim-to-real transfer capabilities.







\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

% Recent advancements in robotic manipulation have demonstrated significant progress in grasping dexterity and the ability to generalize these skills across diverse objects and various manipulation tasks. While these developments have enabled various applications, they primarily excel in handling slow, quasi-static tasks. In addition to grasp-based manipulation, humans often tend to perform non-prehensile manipulation, such as sliding a pizza from a peel into an oven or shoveling snow, requiring skill and experience. As robots are increasingly deployed in logistics, cooking, and food delivery, the development of capabilities for non-prehensile manipulation is gaining significant attention. Emerging research in this area presents a marked contrast to the static manipulation commonly employed in robotics (refa), as it does not assume no-slip constraints during manipulation, but rather a controlled sliding is used for manipulation (refb). 

For years, advancements in robotic manipulation have been concentrated on enhancing grasping dexterity and generalizing skills across a wide range of objects and tasks. These developments have significantly narrowed the gap between robotic manipulators and skilled humans, particularly in prehensile manipulation which are performed in slow and quasi-static operations. The ability to manipulate objects without grasping them is a decisive aspect of human dexterity. Despite the growing demand of involving robots in various applications where non-prehensile manipulation is the most effective way to perform the task, current algorithms developed for non-prehensile manipulation still fall short of achieving human-level performance. This limitation primarily stems from their dependence on oversimplified models and approximated physical parameters, which compromise the robustness of these methods when dealing with uncertainties, variations in object properties, and dynamic environmental conditions. Non-prehensile manipulation tasks are essential in various fields, such as logistics, cooking, and food delivery. Examples include sliding a pizza into an oven or flipping a burger in a pan. 

\begin{figure}[t]
\centering
    \includegraphics[trim=0.2cm 0.1cm 0.2cm 0.2cm, clip, width=0.42\textwidth, height=0.4\textheight]{images/starting_pic.pdf}
    \vspace{-10pt}
    \caption{Illustration of the non-prehensile manipulation explored in this study: A robotic arm sliding an object using a controlled maneuver by following determined and commanding cartesian velocity to the robotic arm controller.}
    \label{fig:concept}
% \vspace{-7mm}
% \vspace{-15pt}

\end{figure}


To address these limitations in non-prehensile manipulation, this study investigates precise sliding control of objects on a horizontal surface using a robotic arm. This approach differs from other studies on non-prehensile manipulation methods that focus on preventing objects from sliding across a horizontal surface during the transportation \cite{c18}, as well as from previous work that uses external barriers to slide objects on the manipulator’s palm \cite{c1}. Our study aims to achieve accurate sliding displacements by controlling surface acceleration and deceleration (see Fig. \ref{fig:concept}), similar to moving a glass on a tray, without requiring prior knowledge of the precise friction coefficient and only by maneuvering the tray. To perform this, an actor-critic reinforcement learning (RL) framework is implemented in a simulation environment. The RL framework generates sequences of actions in a continuous action space to create precise linear trajectories, achieving the desired sliding displacement. Once trained, the actor model is transferred to real-world setup to be evaluated.



\textcolor{black}{
However, deploying policies trained in simulation directly into real-world scenarios often presents a significant challenge due to the sim-to-real gap. One approach to mitigating this gap is the use of higher-fidelity simulation environments, which more accurately capture subtle real-world conditions \cite{c6}. In this study, we employ MuJoCo as our simulation environment due to its high physical accuracy. In robotic manipulation complex physical variables, such as surface roughness, and contact mechanics, are often challenging to measure accurately and can affect the performance of the algorithm's real world implementation. Yet, these factors are often approximated or overlooked altogether \cite{c2}. In non-prehensile manipulation, the dynamic friction between surfaces is particularly critical, in contrast to manipulation through grasping where static friction plays the key role. Most simulation environments utilize the coulomb friction model which is known for its inaccuracy in dynamic friction. Another common solution to bridge the gap between simulation and reality is domain randomization, which randomizes the dynamics of the environment and exposes the RL framework to a diverse set of environments in the training phase \cite{c5,c6}.}

In addition to measures taken to reduce the sim-to-real gap in the face of physical uncertainties, we further enhance robustness by estimating and incorporating real-time friction feedback into the control algorithm. To achieve this, we introduce two friction inference methods: an analytical approach and a data-driven Long Short-Term Memory (LSTM) model. In summary, the contributions of this work can be listed as follows:

% Beyond sim-to-real transfer challenges, handling variable surface roughness and friction is also critical. We propose an inference algorithm to infer friction using kinematic data from the platform and the sliding object. This inference of friction is performed in each action of the model, and its estimate is given to refine future actions of the model. In summary, the contributions of this work can be listed as follows:


\begin{itemize}
    \item Training an actor-critic RL model which is robust to surface friction and can slide an object on a surface by generating linear trajectories.

    \item Training an LSTM model to infer surface friction from time-series kinematic data, to infer the friction between the two surfaces.

    \item Developing an analytical friction inference approach using kinematic data to robustly estimate friction without learned models.

    \item Assessing the effect of domain randomization and friction inference methods on the performance of zero-shot sim-to-real transfer.
\end{itemize}

% Experimental results, both in simulation and on physical hardware, indicate that our proposed approach, supported by appropriate sim-to-real transfer strategies, achieves reliable and precise sliding displacement across diverse surfaces. In certain non-prehensile manipulation tasks, our approach can exceed human-level performance.





% In robotic manipulation complex physical variables, such as surface roughness, and contact mechanics which are often difficult to measure accurately can affect the performance of the algorithm in real world implementation. Consequently, in most cases these variables effect are approximated or disregarded altogether. In non-prehensile manipulation, the dynamic friction between surfaces is particularly critical, in contrast to manipulation through grasping where static friction plays the key role. Most simulation environments utilize the coulomb friction model which is known for its inaccuracy in dynamic friction. One of the common solution to bridge the gap between simulation and reality is domain randomization, which randomizes the dynamics of the environment and exposes the RL agent to a diverse set of environments in the training phase.


  % that is previously introduced and validated over deep learning models trained on images in simulation and tested in reality for grasping \cite{c5}. In this study we take inspiration from that to perform similar concept for randomization of surfaces friction during the learning process, as an attempt to decrease the gap between simulation and reality for non-prehensile manipulation. 



% In our study, we need to use simulation environment to train our proposed RL framework that can generate the desired trajectory for sliding the object, such as legged robots, where performance in locomotion of the legged robot is highly dependent on the friction between the robot's legs and the underlying surface. (ref..leg). The uncertainties stemmed from physical variables can hinder the adaptation of the developed algorithms in real world scenarios.


% talking about problem in friction and friction models and connection to simulation ............


% These barriers in adaptation is mainly stemmed from the uncertainty arisen from these variables, and particularly from friction between surfaces.



% Various robotic capabilities have been developed for non-prehensile manipulation that is limited to specific conditions 
% This primarily arises from the complex physical variables involved in these tasks, which are often difficult to predict and, in some cases, challenging to measure accurately, such as the friction between surfaces, and complex aerodynamic effect on aerial robots. These variables causes uncertainties in performing automated tasks and limits their adaptation in various applications. 

% Recent advancements in robotic manipulation have enhanced grasping dexterity and skill generalization across various objects and tasks. However, these improvements are mainly effective for slow, quasi-static tasks \cite{a2}. Non-prehensile manipulation, such as sliding a pizza into an oven or moving a glass on a tray, requires controlled sliding instead of no-slip constraints, which are commonly studied in robotic manipulation \cite{a1}. As robots become more involved in various application fields such as logistics, cooking, and food delivery, developing non-prehensile manipulation capabilities is increasingly important.




% \textcolor{red}{Coefficient of friction, for different materials can be affected by various factors such as speed of in contact surfaces, temperature, the force on the contact surfaces, the materials that are in contact, etc.(refd) As a result defining a constant coefficient of friction for only two surfaces, to be used in manipulation through sliding is not practical and even by change in environmental parameters such as temperature, the basis assumption in calculation is   in all c performing a predefined maneuver and for manipulating an object through sliding it, is incredibly hard and not possible}



% These studies have  is dedicated to various tasks ranging from tasks like flipping burgers (ref) to controlled sliding which is in contrast to static manipulation that assumes a no-slip constraints for the task (ref). \textcolor{red}{whilst there are various studies on this topic ...} 
% are mostly single contact and with the aid of external obstacles to be perfromed.

% Emerging research is increasingly dedicated to studies on robotic capabilities for non-prehensile manipulation techniques, tasks like flipping burgers (ref) and wo 

% This article presents an ongoing study on performing a non-prehensile manipulation task, where an object is controlled on a supporting horizontal surface attached to the end-effector of a robotic arm. The study focuses on achieving precise sliding displacement of the object by accurately controlling the acceleration and deceleration of the supporting surface, ensuring continuous contact with the object throughout the process. This task can resemble moving a glass on a tray. In contrast to human that learn this technique through real-world trial and error often performed with relatively constant friction coefficient, in this work, we aim to train a neural network model within a simulation environment using a reinforcement learning (RL) framework capable of determining a sequence of actions in a continuous action space to generate proper linear trajectories for the horizontal platform. The goal is to achieve the desired displacement of a sliding object without prior knowledge of the friction coefficient between the contact surfaces.










% \vspace{-5 mm}
% \textcolor{red}{
% This study addresses a task akin to transferring a pizza on peel into an oven, in contrast to humans who typically learn through real-world trial and error to perform this task under relatively constant friction coefficient, our objective is to train a neural network model in a simulation capable of sliding objects on various surfaces with different friction coefficients. Using a reinforcement learning algorithm, the model learns to select the right sequence of actions that achieves the required displacement of the sliding object with, considering scenarios where a single action may not suffice which is the case when the friction coefficient is unknown to the system. Importantly, the model operates within a continuous action space, leveraging feedback solely based on the distance to the target object position, without prior information regarding the environmental variables such as the friction coefficient.}  

% Our approach aims to offer a generalized solution for sliding objects on surfaces or manipulating items on the palm by determining the required acceleration of the affixed surface, thereby enabling various robots—from robotic arms to mobile platforms—to perform manipulation through sliding an object. By bridging the gap between simulation and real-world application, we address the challenges posed by dynamic frictional contacts. Unlike previous research, which relies on external objects to perform slip-stick maneuvers(refb), In this study slipping on the surface and sticking back to the surface relies only on the controlled maneuvers of the supporting surface. 

% Achieving our goal involves several challenges regarding the sim-to-real transfer, including optimizing the simulation environment to closely resemble real-world conditions, accounting for actuator limitations and restricted motion ranges, and applying domain randomization on friction coefficients to minimize the sim-to-real gap due to the inaccuracies in friction model inherent in the simulation environment. a





% and addressing challenges with the coulomb friction model. This model, effective for static friction, overlooks factors like sliding speed, temperature, and the forces between objects \cite{c2}. Implementing our solution on robots also requires considering the maximum acceleration limits of the robot actuators to ensure precise control of the sliding displacement within the operational constraints, which is crucial for real-world applications.

% on dynamic frictional forces. The coefficient of friction between materials varies with factors like contact speed, temperature, and applied force, making
\section{Related Works}


\subsubsection{\textbf{Non-prehensile manipulation}} Non-prehensile manipulation, or the ability to manipulate objects without grasping, is crucial for tasks involving hard-to-grasp objects or cluttered workspaces \cite{c7, c8}. This types of manipulation includes throwing \cite{c21}, pushing \cite{c20}, rolling \cite{c19}, and sliding \cite{c1} among others. Several studies have explored non-prehensile manipulation of an object on horizontal surfaces \cite{c1, c18, c9}. One of these studies \cite{c18} primarily focused on transporting an object on a horizontal surface while ensuring no sliding or slippage occurs during the task. In contrast, the objective of our work deliberately leverages sliding as the primary method for moving the object across the surface. The other study \cite{c1}, investigates the control approach of sliding an object on the surface and impressive controllability and performance were demonstrated. It is important to note that this study assumes the presence of external barriers to assist in performing the sliding task and incorporates precise knowledge of the surface friction coefficient (\textit{$\mu$}) into its model-based controller. While this setup enables impressive controllability and performance, it inherently limits the approach's ability to generalize to scenarios without external constraints or surfaces with unknown friction properties. Another study in this area \cite{c9}, avoided external barriers but lacked real-time friction adaptation, which our work addresses through on-line friction estimation and robust RL-based control.

% \textbf{Additionally, in this exeriment   In contrast, the approach in \cite{c9} employs linear acceleration and deceleration as their manipulation strategy which does not need external barrier. It uses a supporting platform where the sliding object is placed on it, However, this method relies on assumptions such as known static and dynamic friction coefficients and uniform mass distribution, which limit its adaptability to real-world scenarios where these parameters are often unknown.}



\subsubsection{\textbf{RL for continuous action space}} Reinforcement learning algorithms have long been studied and applied in simplified environments, such as the Atari games discussed in \cite{c10}. In these settings, they have achieved notable success, often surpassing human experts in performance. However, these environments rely on discrete action spaces, and for most control problems in real world applications, continuous action space is necessary. Several methods have been introduced to extend reinforcement learning to continuous action spaces. Among the most prominent are actor-critic models like Deep Deterministic Policy Gradient (DDPG) \cite{c11} and Proximal Policy Optimization (PPO) \cite{c12}, both of which are proven highly effective for handling continuous control tasks. Thus, these method have been utilized more frequently in recent studies in robotics such as training an actor-critic model for non-prehensile manipulation where the end-effector can interact with the object on an external surface, to perform the manipulation, in which they achieved a 50\% success in zero-shot sim-to-real transfer over unseen objects \cite{c7}.
Actor-critic RL models can be applied to other applications as well, such as robotic motion planning in dynamic environments \cite{c14} and drones learning to fly through gates autonomously \cite{c13}.


\section{Methodology}

In this study, we propose a discrete-time control strategy for sliding manipulation, inspired by the way humans perform rapid, step-wise adjustments to slide objects on a surface. Our approach leverages an iterative process that adapts to surface friction by incorporating feedback from past actions and real-time observations, particularly when surface friction estimation is inaccurate. Following this, we present the reinforcement learning framework, providing details about the simulation environment and the architecture of the actor and critic models. Finally, we introduce two distinct online algorithms for friction estimation: an analytical approach and a data-driven LSTM approach, both designed to infer friction based on the action given to the manipulator and the kinematic data of relative motion between the object and the surface.

\begin{figure}
\centering
    \includegraphics[trim=0.2cm 0.2cm 0.2cm 0.2cm, clip, width=0.16\textwidth, height=0.1\textheight]{images/figure_schematic.JPG}
    % \vspace{-10pt}
    \caption{A simple Schematic of the platform and the sliding object where \textit{$\Sigma_B$} is the coordinate system fixed on the sliding object and  \textit{$\Sigma_A$} is the coordinate system fixed on the platform.}
    \label{fig:Schematic}
% \vspace{-5 pt}
\end{figure}




\subsection{\textbf{Problem Statement and Assumptions} \label{sec:assumptions}} 

Initially, we define a strategy under which the sliding manipulation on a surface by linear maneuver can be mathematically formulated. In Fig. \ref{fig:Schematic} the parameters used in the formulation for sliding are illustrated. The condition for the object to start sliding on the surface, is the required inertial force being more than the maximum static frictional force, shown in (\ref{eq:conditional}), and once that is satisfied, the relative equation of motion for the object with respect to the surface is illustrated in (\ref{eq:motion}).
\begin{equation}
    F_{f_{max}} = \mu_s g \hspace{1cm} |\mathbf{{\ddot{X}}}| > \mu_s g,
    \label{eq:conditional}
    % \vspace{-10pt}
\end{equation}



\begin{equation}
    {\ddot{x}}_A^B = -\mathbf{\ddot{X}} - \frac{\dot{x}_A^B}{|\dot{x}_A^B|} \mu_k g.
    \label{eq:motion}
    % \vspace{-10pt}
\end{equation}







To simplify the problem of the sliding object, assumptions are made regarding the input command \textit{$\mathbf{\ddot{X}}$} which defines the linear acceleration of the supporting surface. The surface starts from a stationary position, accelerates, decelerates, and returns to its initial resting pose. This motion defines a step in the DDPG reinforcement learning framework. The relative displacement of object B with regards to the surface A is defined in (\ref{eq:displacement}),

% \begin{equation}
%     x_B^A = - \iint \mathbf{\ddot{X}}  dt - \iint \frac{\dot{x}_B^A}{|\dot{x}_B^A|} \mu_k g dt
% \label{eq:displacement}
% \end{equation}

\begin{equation}
x_A^B =
\begin{cases} 
0 &  |\ddot{X}| \leq \mu_s g \quad  \\ 
-\iint \ddot{X} \, dt - \iint \frac{|\dot{x}_A^B|}{\dot{x}_A^B} \mu_k\,g \, dt 
&  |\ddot{X}| > \mu_s g \quad 
\end{cases}.
\label{eq:displacement}
\end{equation}

\subsection{\textbf{Generated Action into Linear Trajectory} \label{sec:action_to_trajectory}} 

% \begin{figure}
% \centering
%     \includegraphics[trim=0.05cm 0.05cm 0.05cm 0.05cm, clip, width=0.48\textwidth, height=0.13\textheight]{images/single_step.JPG}
%     % \vspace{-10pt}
%     \caption{Single action output: Acceleration of the support surface and the sliding object depicted on the left side, and the velocity of the platform and the relative velocity of the object with regards to the platform is depicted on the right side. }
%     \label{fig:result}
% % \vspace{-10 pt}
% \end{figure}

Fig. \ref{fig:concept} shows the determined command for a desired trajectory, and the hardware setup used to implement the experiment. The action generated by the RL agent comprises three parameters which are two accelerations and one time duration defined in Fig. \ref{fig:concept} denoted as (\textit{$a_i, a_m, t_m$}). To elaborate the maneuver using these parameters, we need to write down the velocity of the surface as a function of time derived from this command, which is shown in \eqref{eq:velocity_time}. The maneuver is divided into four phases. The transition from one phase to another happens when there is a change of commanded acceleration or change of direction of the motion of the platform, as illustrated in the platform velocity versus time in Fig. \ref{fig:concept}. At \textit{$t = t_i + \frac{t_m}{2}$} the platform change its direction of motion and that means \textit{$\dot{x}_A(t) = 0$}.

By solving (\ref{eq:velocity_time}), for \textit{$t = t_i + \frac{t_m}{2}$}, the undetermined parameter \textit{$t_i$} in the action sequence can be explicitly calculated.  The resulting expression for \textit{$t_i$} is provided in (\ref{eq:time_d}). The end of the second phase marks the maximum distance from the starting pose which is crucial as the robotic arm has a limited range of motion. The required range of motion based on the generated action is obtained in (\ref{eq:distance}). This formulation presented for \textit{$t_i$} ensures that the robot begins and concludes the maneuver at the same pose, adhering to the initial assumption discussed in \ref{sec:assumptions}. The calculated \textit{$\dot{x_A}$} will be commanded to the franka cartesian velocity controller to be executed.



\begin{equation}
\scalebox{0.92}{$
\dot{x}_A(t)=
\begin{cases}
a_i\,t & 0 \leq t \leq t_i \\[3pt]

a_i\,t_i
+ {a_m}(t - t_i) & t_i \leq t \leq t_i+\frac{t_m}{2}\\[3pt]

a_i\,t_i
+ {a_m} \frac{t_m}{2} + {a_m}(t - t_i - \frac{t}{2}) & t_i+\frac{t_m}{2} \leq t \leq t_i + t_m\\[3pt]

a_i\,t_i
+ {a_m}\,t_m + a_i\, t & t_i+t_m \leq t \leq 2 t_i + t_m\\[3pt]
% \label{eq:phases}
\end{cases}
$}
\label{eq:velocity_time}
\end{equation}

\begin{equation}
    t_i = \frac{t_m}{2} \left| \frac{a_m}{a_i} \right|,
    \label{eq:time_d}
\end{equation}

\begin{equation}
    \int_{0}^{t_i + \frac{t_{_m}}{2}} \dot{x}_A(t) \, dt = \frac{a_i {t_i}^2}{2} + \frac{a_m\, t_m^2}{8}.
    \label{eq:distance}
\end{equation}


% are given by the actor model and the parameter $t_i$ is calculated in (\ref{eq:time_d}). 
 % Equation of the motion of the surface using the given action is stated in  As illustrated in Fig. \ref{fig:result} an step will make a linear trajectory for the supporting surface, that cause relative motion between the two surfaces. 

% \begin{equation}
%     x_A = \frac{1}{2} a_i {t_i}^2 + a_i t_i t - \frac{1}{2} a_m {t}^2 +
% \end{equation}


% By applying integral over (\ref{eq:motion}), having the input acceleration and deceleration for a determined time period, and the friction coefficient, the relative velocity of the object with regards to the surface denoted as \textit{$\dot{x}_B^A$} can be obtained, and similarly same can be done again to obtain the relative displacement denoted as \textit{$x_B^A$}.


% The maximum distance of the supporting surface from its initial pose during the maneuver is denoted in (\ref{eq:distance}) and the required time for the maneuver is (\textit{$2t_i + t$}). This parameter is critical as the robot has a limited range of motion, and the trained model in simulation is supposed to have high zero-shot accuracy in real world implementation.



% \vspace{-10pt}


% \begin{enumerate}
%     \item \textit{{Each action in this framework consists of a stroke, which involves applying various linear acceleration (\textit{$a_i$ and $a_m$}) to the surface.}}
%     \item \textit{{The supporting surface starts from stationary position, and comes to an stop while it returns to its initial pose.}}
%     \item \textit{{In the experiment, velocities of in contact surfaces and their effect on kinetic friction are considered negligible, and during the experiment, the temperature remained constant.}}
   
   
% \end{enumerate}



% \begin{equation}
%     t_i = \frac{a_m}{2 a_i} t
%     \label{eq:t_i}
% \end{equation}

% \begin{equation}
%     D = \frac{a_i {t_i}^2}{2} + \frac{a_m t^2}{8}
%     \label{eq:distance}
% \end{equation}

\subsection{\textbf{Reinforcement Learning Framework}}



To handle the system's actions in a continuous space, we utilize the DDPG framework, an actor-critic RL approach. For training, the MuJoco simulator \cite{c3} is selected, due to its high-fidelity physics simulation, and light-weight computational load.  The DDPG model is an off-policy, deterministic policy gradient algorithm \cite{c15}, that generates the same action for a given state during evaluation, making it more predictable than stochastic policy models. The deterministic nature of DDPG can aid sim-to-real deployment when combined with techniques like domain randomization and robust policy training, as demonstrated in \cite{c22, c23}. In this method, the objective function is defined as (\ref{eq:objective}). The actor network, represented as \textit{$\pi(s|\theta^\pi)$}, is updated by applying the gradient of the objective function \textit{$J(\theta^{\pi})$} with regards to its actor network parameters, denoted as \textit{$\theta^\pi$} shown in (\ref{eq:gradient}). This update involves two components: the gradient of the critic \textit{$Q(s, a|\theta^Q)$} with respect to the action \textit{$a$}, evaluated at the policy output \textit{$a = \pi(s|\theta^\pi)$}, and the gradient of the policy’s output \textit{$\pi(s|\theta^\pi)$} with respect to its parameters \textit{$\theta^\pi$}. The critic \textit{$Q(s, a|\theta^Q)$} learns to approximate the expected cumulative reward using the Bellman equation (\ref{eq:bellman}), which defines the relationship between immediate and future rewards. Together, these components ensure the policy is improved to maximize the expected return.
 
 % The reward function that is designed in a progressive approach, to decrease the training time and episodes. This part is illustrated in the \textit{\textbf{Reward function and RL framework training:}}. 

\begin{equation}
    J(\theta^\pi) = \mathbb{E}_{s \sim \mathcal{D}} \left[ Q(s, \pi(s|\theta^\pi) |\theta^Q) \right],
    \label{eq:objective}
\end{equation}

\begin{equation}
    \nabla_{\theta^\pi} J(\theta^\pi) = \mathbb{E}_{s \sim \mathcal{D}} \left[ \nabla_a Q(s, a | \theta^Q) \nabla_{\theta^\pi} \pi(s | \theta^\pi) \right],
    \label{eq:gradient}
\end{equation}

\begin{equation}
    Q(s, a) = r(s, a) + \gamma \mathbb{E}_{s'} \left[ Q(s', \pi(s')) \right].
    \label{eq:bellman}
\end{equation}

It is worth mentioning that this framework is designed for zero-shot sim-to-real transfer of the learned policy. Below, we detail the components of this RL framework. 





\subsubsection{\textbf{Data structure}}
We are using a symmetric implementation of the actor-critic model, where both have access to the same set of state from the environment. The state for this structure includes, desired remaining distance for displacement denoted as \textit{$D_{des}$}, up to three previous actions, (\textit{$A_{i-1}, A_{i-2}, A_{i-3}$}) and the relative displacement caused by each action (\textit{$D_{i-1}, D_{i-2}, D_{i-3}$}), and a guesstimate of the friction coefficient shown as \textit{$\mu_e$}. In simulation, all states are accurately available, whereas in real-world implementation, relative position is tracked via a motion capture system, and friction is estimated through extensive experimentation.

\subsubsection{\textbf{Action space}}

At each step, the learned policy generates a three-dimensional action that defines the trajectory using three parameters \textit{$a_i$} (initial acceleration), \textit{$a_m$} (maximum acceleration), and \textit{$t_m$} (time duration). This action turns into a linear trajectory as stated in \ref{sec:action_to_trajectory}. The generated action is constrained by acceleration and motion range limits to comply with the robotic arm's physical capabilities, as stated in \eqref{eq:action_space},
% At each step, the learned policy generates a three-dimensional action that defines the trajectory using three parameters \textit{$a_i$} (initial acceleration), \textit{$a_m$} (maximum acceleration), and \textit{$t_m$} (time duration). This action turns into a linear trajectory as stated in \ref{sec:action_to_trajectory}. The generated action is constrained by acceleration and motion range limits to comply with the robotic arm's physical capabilities. To impose these limitations we have bounded the \textit{$\left|a_i\right|$}, \textit{$\left| a_m \right|$} \textit{$\leq$} 4.2 \textit{$\frac{m}{s^2}$}, and \textit{$\left| t_m \right| < 2.0s$}. Additionally two accelerations have opposite directions, \textit{$a_i\,a_m \leq0$}. The limitations on action space were chosen to ensure transferability of the learned policies to the real world experiment.  
% \begin{equation}
% \text{Action Space} = 
% \begin{cases} 
% |a_i|, |a_m| \leq 4.2 \, \text{m/s}^2, \\
% t_m < 2.0 \, \text{s}, \\
% a_i a_m \leq 0
% \end{cases}
% \label{eq:action_space}
% \end{equation}
\begin{equation}
\text{Action Space} = 
\left\{
\begin{aligned}
&|a_i|, |a_m| \leq 4.2 \, \text{m/s}^2 \\
&t_m < 2.0 \, \text{s} \\
&a_i\, a_m \leq 0
\end{aligned}.
\right.
\label{eq:action_space}
\end{equation}




% Additionally, at each step, \eqref{eq:distance} is computed to evaluate the maximum distance from the starting pose. If this distance exceeds 30 \textit{$cm$} a penalty is applied to the reward function to discourage trajectories that violate the range-of-motion constraints.
\begin{figure}
\centering
    \includegraphics[trim=0.0cm 0.0cm 0.0cm 0.0cm, clip, width=0.45\textwidth, height=0.22\textheight]{images/DDPG_frame.pdf}
    % \vspace{-10pt}
    \caption{This figure illustrate, the states, the environment random initialization and domain randomization on friction parameter, in the DDPG training framework.}
    \label{fig:training_framework}
% \vspace{-5 pt}
\end{figure}


\subsubsection{\textbf{Reward function and training}}

By formulating the sliding task, assuming the accurate knowledge of the friction coefficient in Coulomb model, the optimal solution can be acquired. Although this optimal solution works perfect in simulation, it cannot satisfy the task in the real world implementation considering the prior assumptions. As a result a progressive \textit{\textbf{reward shaping}} approach \cite{c17} is implemented, and the training is divided into the several steps. First step is mimicking the analytical solution for the Coulomb friction model for a single distance that is reinforced by shaping the reward function to incentivize proximity of the action parameters to optimal values which are (\textit{$a_i = \mu_s\,g $}, \textit{$a_m = 4.2 \frac{m}{s^2}$}) and \textit{$t_m$} can be obtained from the expansion of \eqref{eq:displacement} which is illustrated in \eqref{eq:relative_vel} for the relative velocity and \eqref{eq:relative_motion} for the relative displacement from the start to the end of each phase for all of the four phases stated in \eqref{eq:velocity_time}. The resulting displacement is summation of relative displacement of all the four phases. If any of the acceleration is lower than the \textit{$ \mu_s\,g$}, the corresponding displacement of that phase is zero. The complexity of this maneuver is reflected in \eqref{eq:relative_acceleration}, where the object's acceleration varies within the same phase, even without changes in the surface's motion direction or acceleration.

% \begin{equation}
% \scalebox{0.90}{$
% {x}^{B}_A(t)=
% \begin{cases}
% \frac{a_i - \mu_k\,g}{2}\,{t}^2 & 0 \leq t \leq t_i \\[3pt]

% {(a_i - \mu_k\,g})\,{t_i}
% + (\frac{a_m - \mu_k\,g}{2})(t - t_i) & t_i \leq t \leq t_i+\frac{t_m}{2}\\[3pt]

% a_i\,t_i
% + {a_m} \frac{t_m}{2} + {a_m}(t - t_i - \frac{t}{2}) & t_i+\frac{t_m}{2} \leq t \leq t_i + t_m\\[3pt]

% a_i\,t_i
% + {a_m}\,t_m + a_i\, t & t_i+t_m \leq t \leq 2 t_i + t_m\\[3pt]
% % \label{eq:phases}
% \end{cases}
% $}
% \label{eq:relative_motion}
% \end{equation}

\begin{equation}
    \scalebox{0.8}{$
\ddot{x}^{B}_A(t)=
\begin{cases}
{a_i - \mu_k\,g}&  \, 0 \leq t \leq t_i \\[3pt]


{a_m -  \, \frac{\left| \dot{x}^B_A(t) \right| \mu_k\,g}{\dot{x}^B_A(t)}} & t_i < t \leq t_i + \frac{t_m}{2} \\[3pt]

{a_m -  \, \frac{\left| \dot{x}^B_A(t) \right| \mu_k\,g}{\dot{x}^B_A(t)}} & t_i + \frac{t_m}{2} < t \leq t_i + t_m \\[3pt]

{a_i - \frac{\left| \dot{x}^B_A(t) \right| \mu_k\,g}{\dot{x}^B_A(t)}} & t_i + t_m < t \leq 2t_i + t_m\\[3pt]
% \label{eq:phases}
\end{cases},
$}
\label{eq:relative_acceleration}
\end{equation}


\begin{equation}
\scalebox{0.8}{$
\dot{x}^{B}_A(t)=
\begin{cases}
({a_i - \mu_k\,g})\,{t} & 0 \leq t \leq t_i \\[3pt]

\dot{x}^{B}_A(t_i) + \,(\ddot{x}^{B}_A(t))\,({t - t_i}) & t_i < t \leq t_i + \frac{t_m}{2} \\[3pt]

\dot{x}^{B}_A(t_i + \frac{t_m}{2}) + (\ddot{x}^{B}_A(t)))\,(t - t_i +\frac{t_m}{2}) & t_i + \frac{t_m}{2} < t \leq t_i + t_m\\[3pt]

\dot{x}^{B}_A(t_i + t_m) + (\ddot{x}^{B}_A(t))\,(t-t_i-t_m) & t_i + t_m < t \leq 2t_i + t_m \\[3pt]
% \label{eq:phases}
\end{cases},
$}
\label{eq:relative_vel}
\end{equation}


\begin{equation}
\scalebox{0.8}{$
\Delta \,{x}^{B}_A=
\begin{cases}
\frac{(\ddot{x}^{B}_A(t))}{2}\,{t_i}^2 & \left |a_i\right | > \mu_s\,g \\[3pt]

{\dot{x}^{B}_A(t_i)}\,\frac{t_m}{2}
+ (\ddot{x}^{B}_A(t))\frac{t_m^2}{8} & \left |a_m\right | > \mu_s\,g \\[3pt]

{\dot{x}^{B}_A(t_i+\frac{t_m}{2})}\,\frac{t_m}{2}
+ (\ddot{x}^{B}_A(t))\frac{t_m^2}{8} & \left |a_m\right | > \mu_s\,g \\[3pt]

{\dot{x}^{B}_A(t_i+t_m)}\,t_i
+ \frac{(\ddot{x}^{B}_A(t))}{2}\,{t_i}^2 & \left |a_m\right | > \mu_s\,g\\[3pt]
% \label{eq:phases}
\end{cases}.
$}
\label{eq:relative_motion}
\end{equation}

As the performance of the actor model increases the reward function automatically switches to a performance based function, which takes into account the accuracy of displacement through sliding, maximum range-of-motion defined in \eqref{eq:distance}, the required time and number of steps for a desired displacement. In the second step, we give various distances for each episode ranging from 0.02 \textit{$m$} to 0.2 \textit{$m$}, and the reward function remains performance base. In the third step, not only various distances but also various friction coefficient (\textit{$\mu$}) is given to the simulation environment for each episode ranging from 0.05 to 0.45. 

% \textcolor{red}{The training structure is shown in the Fig. ()}

% The goal of this RL framework is not to train a model that can slide an object on a surface when it relies on the coulomb friction model, but rather a robust solution that regardless of the friction model, inaccuracy in sliding friction coefficient, and inaccuracies in system modeling in simulation ,it performs the task. Besides by considering perfect coulomb friction model
% Although the coulomb friction model is inaccurate for sliding, most simulation environment including MuJoco uses the same model. Considering the coulomb friction model, mathematically we can solve the equation for a desired relative displacement. However, to make the policies robust to factors such as inaccuracy of friction model, error in estimation of the sliding friction coefficient and inaccuracies in system modeling, and how the system can follow commanded acceleration, we want to implement actor-critic
% We implemented progressive learning, by initially training the network to perform the maneuver on a constant friction and for a constant distance.
% The model initially struggled on converging to a solution for the sliding tasks due to varying distance and friction. We performed two possible solution to reduce the training time. First, by implementing, a progressive \textit{\textbf{reward shaping}} algorithm \cite{c17}, that uses the mathematical formulation stated in (\ref{eq:displacement}) to determine the desired action, for the given state, was implemented. this reward function, could facilitate the early stage of the training by giving clear direction, for all three dimensions of the command input. However, as the performance of the model increases the reward function automatically switch into a performance based cost function where the value is defined by ,the range of motion stated in (\ref{eq:distance}), the accuracy of displacement through sliding, the step and time required to perform the displacement, are parameters that determine the reward.
% This technique enabled us to guide the model's sliding manipulation strategy while continuously refining it based on performance criteria.

% In addition to reward shaping, we performed, \textit{\textbf{transferred learning}}, which could increase stability during the training phase due to the deterministic policy in the implemented actor-critic model. The model was initially trained over a constant desired sliding distance, and once trained same actor and critic network would be trained over various distances, this approach showed more stability and faster convergence in training.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[trim=0 0.0cm 0 0.0cm, clip, width=0.9\textwidth, height=0.34\textheight]{images/block_diagram_main.pdf}
    % \includegraphics[width=1.0\textwidth, height=0.35\textheight]{Images/Block_Diagram_blend.pdf}
    \caption{A schematic of the evaluation framework. Connections between components for real-world experiments (solid lines) and simulations (dashed lines) are shown. The key difference lies in pose estimation: real-world setups rely on motion capture systems and inverse kinematics to determine the pose of the end-effector and the object, while simulations provide these parameters directly.}

    
    % \caption{This figure showcases the VIO performance using two sensors, Stereo and RGB-D, in two setups: wired (left column labeled as (a)) and wireless (right column labeled as (b)). The bottom plots in both columns display the absolute error in position estimation, with the red line representing RGB-D and the blue line representing stereo Setup.}
    \label{fig:block_diagram_evaluation}
\end{figure*}


\subsubsection{\textbf{Domain Randomization}}

Upon completing the training phase, where the model successfully generalizes its actions across varying distances and friction coefficients (\textit{$\mu$}), domain randomization is introduced to enhance robustness and sim-to-real transferability \cite{c6}. For this problem, friction is identified as the most critical parameter influencing the sliding task. By introducing variability in \textit{$\mu$} the goal is to enable the model to handle a wide range of frictional conditions and prepare it for deployment in real-world environments, where the Coulomb friction model may not fully capture the underlying dynamics. To achieve this, we added a noise according to \eqref{eq:noise} to the friction coefficient, provided to the state for the actor and the critic model in two stages. In the first stage, \textit{$\eta \in [0.05, 0.1]$} and in the second stage \textit{$\eta \in [0.05, 0.15]$}. As highlighted in \cite{c24}, excessive randomization can lead to overly conservative policies, which is why the range was incrementally adjusted. In Fig. \ref{fig:training_framework}, the training structure for the DDPG RL framework is depicted.
  

\begin{equation}
    \Bar{\mu} = \mu \pm \eta
    \label{eq:noise}
\end{equation}






% to perform tasks on surface where \textit{}
% Once the training on various desired sliding distance was completed, we performed another training on already trained actor and critic networks by randomizing the friction coefficient denoted as \textit{$\mu$} in the environment at each episode of the training session. As explained in the \textit{\textbf{Data structure}}, the friction coefficient was included in the state, \textit{\textbf{s}}, provided to both the actor and critic networks. To prevent overfitting and excessive sensitivity to friction coefficient, a random offset between 0.04 and 0.1 was added to the parameter reporting the friction coefficient of the environment to the \textit{\textbf{s}}. This served two key purposes: first, to reduce overfitting and improve the model's robustness to variations in friction, and second, to help the model handle uncertainty, preparing it for sim-to-real transfer through Domain Randomization \cite{c6}.



% At early stage of training in this study, our model failed to perform effectively for sliding, over various distances with varying level of friction coefficient. Thus, we decided to assume a constant value for the distance and the friction coefficient, and the model could effectively generalize the action. Then we performed transfer learning, by employing the already trained model in the previous condition, and re-train it in the same simulation environment with a varying distance for each episode which it showed improvement by decreasing the overall training time.\cite{c16} Although, this solution was effective in converging the learning algorithm, but still it was not capable of dealing with friction coefficient as a variable in the environment. too time-consuming and not all model during the training could converge. As a result we used the \textit{\textbf{reward shaping}}, to guide the model at early stage of the training by defining a reward function that is just trying to mimic the solution obtained from mathematical formulation illustrated in (\ref{eq:displacement}). As soon as it was able to slide at least 50\% of the required displacement, the reward function was replaced by a more sophisticated performance based function that would prioritize, precision of sliding, required time for the performance and the maximum range of the motion.

% Initially the implementation of the DDPG model, takes the distance from the sliding object to the desired position on the supporting surface as the only input to the actor model to generate desired action. This implementation can be sufficient to cope with constant kinetic friction coefficient. This has been implemented both in simulation and experiment, and the result is discussed in the next section. According to the DDPG framework shown in (\ref{eq:policy_gradient}) to maximize the objective function \textit{$J(\theta)$} the gradient of $J(\theta)$ with respect to the policy parameters $\theta$ is calculated. This process involves modifying the policy function $\pi_{\theta}$ that maps the state $s$ into an action $a$. The Q function estimate the expected value of taking a series of action $a$ in state $s$ and gradient of Q with respect to $a$ maximize the reward over an episode.\cite{c4} 
% \begin{equation}
%     \nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim D} \left[ \nabla_{\theta} \pi_{\theta}(s) \nabla_{a} Q_{\pi}(s, a) \Big|_{a = \pi_{\theta}(s)} \right]
%     \label{eq:policy_gradient}
% \end{equation}

% In definition of the reward function, the accuracy of the sliding object displacement, the required time for maneuver (\textit{$2t_i + t$}), and the maximum distance of the
% supporting surface from its initial pose shown in (\ref{eq:distance}) are considered.



% During the training phase in the DDPG framework, even in simulations, performance was highly unstable. To improve the starting phase of training, reward shaping was employed, guiding the model to achieve a baseline level of performance. Once the baseline performance was observed, the reward function was reshaped around its original goal that is controlling the object to the desired position. We observed that by performing the reward shaping, the training epochs for a determined friction coefficient was reduced by up to 50\%. Additionally, since we trained the same network for various friction coefficients, transferred learning was employed to re-train the model for new kinetic friction, that reduced training time and epochs effectively.



% we used already trained models, with closest friction coefficient to re-train, that showed a considerable reduce in training time and epochs. In this framework, an action (\textit{$a = [a_i, a_m, t]$}) consists of a stroke ,
% which involves applying various linear acceleration to the surface and an episode, is a series of actions that tries to manipulate the object to the desired position. 

% The initial step in this study was to achieve precise object manipulation on a surface with predefined friction coefficients. However, our ultimate goal was to apply this technique to real-world scenarios, which requires an algorithm that can generate desired action, independent of coefficient of friction. 

% In \textit{section IV} detail of possible approached to achieve our final goal is discussed. 






% Although, manipulation of an object on a surface using precise acceleration control on predefined friction coefficient was the first stride, required to perform manipulation through sliding on    of ourr goal, in order to employ this technique in real world scenario To be able to train a model in the simulation with similar friction coefficient, as the supporting surface and the sliding object on it, we performed inclined plane test.

% However, as discussed in the previous section the main goal of 

% It is worth mentioning, that within our designed framework, each each step, is an stroke, that has 
% \hspace{-50pt}


\subsection{\textbf{Friction Inference Algorithms}
\label{sec:algorithms}} 

This study introduces two algorithms that estimate friction using kinematic data of the relative motion and the actor model's commands, providing feedback on surface roughness to enhance the actor model's performance.

\subsubsection{\textbf{Analytical approach}} 
In this approach, the relative equations of motion stated in \eqref{eq:relative_vel} and \eqref{eq:relative_motion} are utilized to solve \textit{$\mu_k$} for the first three phases of the maneuver, given the known \textit{$x^B_A$} from the simulation data or motion capture system in real world experiment. With the generated action provided, the only unknown parameter is the friction coefficient, \textit{$\mu_k$}, as detailed in Algorithm \ref{alg:friction_estimation}.
\begin{algorithm}
\caption{Analytical Friction Estimation}
\label{alg:friction_estimation}
\textbf{Input:} Relative displacement \(x^B_A(t)\) (measured via OptiTrack or simulation), generated action (\(a_i, a_m, t_i, t_m\)) \\
\textbf{Output:} Friction coefficient \(\mu_e\)
\begin{algorithmic}[1]
% \State Measure \(x^B_A(t)\) using the OptiTrack system or simulation.
\If{\(x^B_A(t = t_i) > 1.0 \, \text{cm}\)}
    \State \textit{\textbf{$\mu_e \leftarrow$ Solve }}
    [$\frac{1}{2}\,(a_i - \mu_k \, g)\, t^2 = x^B_A(t_i)$] for \textit{$\mu_k$}
    

\ElsIf{\(x^B_A(t_i + t_m) - x^B_A(t_i) > 1.0 \, \text{cm}\)}
    \State \textit{\textbf{$\mu_e \leftarrow$ Solve}}
    [$\dot{x}^B_A(t_i)\,t_m + \frac{(a_m - \mu_k \, g )\, t_m^2}{2}= x^B_A(t_i + t_m) - x^B_A(t_i)$] for \textit{$\mu_k$}
    
\Else
    \State  
    $\mu_e \leftarrow 1.1 \, \frac{a_m}{g}$
\EndIf
\end{algorithmic}
\end{algorithm}


\subsubsection{\textbf{Data-driven LSTM approach}} 

In this approach, the training data for the LSTM were exclusively generated from the simulation environment. The input to the LSTM consists of a time series of commanded accelerations applied to the surface and the relative velocity between the object and the surface for a duration of 2.0\textit{s}. The target output is the friction coefficient corresponding to the simulation environment. The network consists of four recurrent layers with 1024, 512, 256, and 128 neurons, each utilizing kernels, recurrent kernels, and biases to process inputs and capture temporal dependencies. Fully connected layers follow, progressively reducing dimensionality, with the final output layer predicting a single friction coefficient value.











\subsection{\textbf{Evaluation Setup}}

The DDPG framework is evaluated in both simulation and real-world experiments, as shown in Fig. \ref{fig:block_diagram_evaluation}. Although in the simulation setup, all required parameters can be obtained through the simulation itself, for the real-world implementation, parameters regarding motion of the surface is obtained through inverse kinematics of the franka arm and the parameters regarding the relative motion of the object on the surface is obtained through motion capture system. The evaluation setup is capable of including or excluding the friction estimation algorithms, and its effect is examined in the following section.

% Simulation tests are performed on 3 different surfaces and the RL framework is evaluated with various estimated friction ranging from \textit{$(0.04, 0.32)$} to assess adaptability, while real-world tests use two surfaces to evaluate zero-shot sim-to-real transfer performance. Additionally, the accuracy of the friction inference algorithms is analyzed in both setups, with a comparative assessment to identify their performance differences, and their impact on the performance of the actor model, when facing significant error in initial friction estimation. 




\section{Experiment and Results}


The experiment in this study consists of two main phases to comprehensively evaluate the proposed framework. In the first phase, simulations are conducted to assess the actor network's ability to generalize the sliding task across varying environmental parameters, such as displacement distances, surface friction, and inaccuracies in estimated friction coefficients. This simulation phase benchmarks the RL model's performance against analytical solutions, highlighting its adaptability under uncertainty in conditions. Additionally, it evaluates the friction estimation algorithms, and its effect on overall proposed framework. The second phase transitions the trained model into real-world scenarios to evaluate its robustness and zero-shot sim-to-real transfer capabilities. This phase involves testing the model on hardware as shown in Fig. \ref{fig:concept} with varying surface properties, validating its capacity to replicate desired behaviors without additional fine-tuning.

\subsection{ Simulation Evaluation}
\label{sec:Sim_result}

\begin{figure}
\centering
    \includegraphics[trim=0.05cm 0.05cm 0.05cm 0.05cm, clip, width=0.46\textwidth, height=0.3\textheight]{images/Sim_one_test.pdf}
    % \vspace{-10pt}
    \caption{This plots illustrate performance of the actor network in sliding displacement under friction coefficient errors, compared to the analytically calculated optimal solution. Yellow highlights show the acceptable margin for errors, while gray highlights denote complete action failure to move the object.}
    \label{fig:DDPG_vs_optimal}
% \vspace{-10 pt}
\end{figure}

This experiment aims to evaluate how effectively the actor network performs a sliding task over a fixed distance when exposed to varying friction coefficients. The friction coefficients, provided as inputs to the network’s state (shown in Fig. \ref{fig:block_diagram_evaluation}). The evaluation focuses on analyzing the error in the initial action generated by the actor network and its dependency on the provided friction coefficient. Additionally, it examines the network's ability to generalize its performance across diverse friction conditions. 

To benchmark its performance, the DDPG model is compared against an analytical approach that provides optimal control for the sliding task, as defined by \eqref{eq:relative_motion}. The experiment is conducted across three surfaces with distinct friction coefficients (\textit{$\mu_k$}), where the friction values provided to the network's state range between \textit{$\mu_e$} = (0.04, 0.32). Fig. \ref{fig:DDPG_vs_optimal} highlights the comparative performance of the DDPG model and the analytical approach. The actor trained within the DDPG framework demonstrates robust performance, showing significantly reduced sensitivity to variations in friction coefficients compared to the analytical optimal solution. Additionally, the results indicate that across all three surfaces, for \textit{$D_{des} = 8\,cm$} the actor network is capable of performing the task completely regardless of the error between \textit{$\mu_k$} and \textit{$\mu_e$}, provided the discrepancy remains within a range of \textit{$|\mu_k - \mu_e| \leq 0.05$}. Notably, the actor model avoids "dead zones"—situations where the generated action fails to slide completely—even under challenging conditions. 

\begin{figure}
\centering
    \includegraphics[trim=0.01cm 0.01cm 0.01cm 0.15cm, clip, width=0.48\textwidth, height=0.13\textheight]{images/Sim_exp_two_a.png}
    % \vspace{-10pt}
    \caption{The performance of the actor model across different desired sliding distances (\textit{$D_{des}$}), evaluated under conditions of accurate and perturbed friction coefficient estimation.}
    \label{fig:DDPG_various_distances}
% \vspace{-10 pt}
\end{figure}



% It is evident that the actor trained, in DDPG framework, provides a robust action that is much less affected by the friction coefficient when compared to the optimal solution.
\begin{figure}
\centering
    \includegraphics[trim=0.01cm 0.01cm 0.01cm 0.15cm, clip, width=0.48\textwidth, height=0.13\textheight]{images/LSTM_ANALITICAL_SIM.png}
    % \vspace{-10pt}
    \caption{This plot illustrates how each friction estimation algorithm reduces the error in the estimated friction coefficient (\textit{$\mu_e$}), across a range of errors \textit{$|\mu_e - \mu_k|$} $\in  [0.07, 0.15]$. The $y$ axis  shows the percentage improvement in the accuracy of the estimated friction coefficient for each model (LSTM and Analytical), where higher values indicate better performance.}
    \label{fig:friction_estimate_acc}
% \vspace{-10 pt}
\end{figure}


The second experiment evaluates the model's ability to generalize across various desired displacements (\textit{$D_{des}$}) in a simulation environment with a friction coefficient of \textit{$\mu_k = 0.24$}. As shown in Fig. \ref{fig:DDPG_various_distances}, the results highlight the model's accuracy (\textit{$1 -\frac{\left | \Delta x^B_A - D_{des} \right |}{|D_{des}|}$}) and the absolute error (\textit{$\left | \Delta x^B_A - D_{des} \right |$}) for both accurate (\textit{$\mu_e = \mu_k$}) and slightly deviated (\textit{$\mu_e = \mu_k \pm 0.05$}) friction estimates. The model achieves high precision, with errors of \textit{$\left | \Delta x^B_A - D_{des} \right | \leq 1 cm$} for accurate \textit{$\mu_e$} and \textit{$\left | \Delta x^B_A - D_{des} \right | \leq 1.25 cm$} for slightly deviated \textit{$\mu_e$}. It is important to note that all results presented in Fig. \ref{fig:DDPG_vs_optimal} and Fig. \ref{fig:DDPG_various_distances} focus exclusively on evaluating the performance of the actor network during its initial step.

\begin{figure}
\centering
    \includegraphics[trim=0.01cm 0.01cm 0.01cm 0.15cm, clip, width=0.48\textwidth, height=0.16\textheight]{images/Sim_Estimation_Effect_cm.pdf}
    % \vspace{-10pt}
    \caption{Comparison of the impact of friction estimation algorithms (analytical and LSTM) on task performance when the actor network starts with significant friction estimation error in simulation experiments.}
    \label{fig:sim_estimation_effect}
% \vspace{-10 pt}
\end{figure}

Finally, the friction estimation algorithms are assessed for their accuracy and their influence on the overall performance of the evaluation framework. The assessment begins with the DDPG framework executing a sliding task with a desired displacement of \textit{$D_{des} = 8\,cm$} in an environment characterized by \textit{$\mu_k = 0.24$}. Following this, kinematic data, including the relative displacement (\textit{$x_A^B$}), relative velocity (\textit{$\dot{x}_A^B$}), and platform acceleration (\textit{$\ddot{x}_A$}), are utilized as per the methods described in \ref{sec:algorithms}. These data are applied to minimize the error between actual and initial estimation for friction coefficient denoted as (\textit{$\mu_k$}) and (\textit{$\mu_e$}) respectively. Assuming \textit{$\mu_e'$} shows output of friction estimation algorithms after the first step, the accuracy of correcting friction coefficient for each algorithm is calculated by \eqref{eq:correction percentage} and is presented in Fig. \ref{fig:friction_estimate_acc}, demonstrating that the data-driven LSTM approach outperforms the analytical method in the simulation setup. Furthermore, Fig. \ref{fig:sim_estimation_effect} shows a scenario where \textit{$\mu_k = 0.24$} and \textit{$\mu_e = 0.13$} is given as starting state. It demonstrates how the improved friction estimations enhance the accuracy of sliding displacement in subsequent steps compared to the standalone performance of the actor network. The results reveal that friction estimation not only reduces the error in the following step but also enables the task to be completed in less number of steps compared to the standalone actor network. 


\begin{equation}
    \text{Correction of \textit{$\mu_e$}} = (1 - \frac{|\mu_k - \mu_e'|}{|\mu_k - \mu_e|}) 
    \label{eq:correction percentage}
\end{equation}





% Denoting the newly estimated friction coefficient with \textit{$\mu_e'$}\textit{$|\mu_k - \mu_e|$}. The accuracy of each friction estimation algorithm in correcting the friction estimation error is presented in , 








% We conducted simulations to test the accuracy of sliding when the initialized \textit{$\mu$} deviates from the actual friction coefficient. The \textcolor{black}{Fig. \ref{fig:solo_DDPG}} shows the actor model's accuracy for a single desired displacement where actual \textit{$\mu_k$} in the environment is 0.25. It is clear as the \textit{$\mu_k$} given as state to the actor model decreases the error to the actual friction coefficient, the accuracy of the sliding displacement increase. It is worth mentioning, that the model obtain acceptable performance even with error on friction coefficient estimation.




\begin{figure}
\centering
    \includegraphics[trim=0.01cm 0.01cm 0.01cm 0.15cm, clip, width=0.51\textwidth, height=0.32\textheight]{images/plot_main_hard.pdf}
    \caption{\textbf{A and B:} Show the actor network performance on two surfaces with different friction properties respectively \textit{$\mu_k = 0.244$} and \textit{$\mu_k = 0.16$} to slide an object for 8 cm when various \textit{$\mu_e$} are given as state to the network. \textbf{C:} illustrates the commanded and actual velocity of the end-effector (surface) during a single action and \textbf{D:} indicates the platform displacement and relative displacement of the object on the surface within the same action as C.}
    \label{fig:hardware_test}
% \vspace{-10 pt}
\end{figure}


\subsection{ Zero-shot Sim-to-real Transfer Evaluation}



Initially, we test the trained model, referred to as the target actor network, within the DDPG RL framework (see \ref{fig:training_framework}) on a real-world setup designed for sliding an object on a horizontal surface. We conduct evaluations on two different surfaces, each characterized by dynamic friction coefficients of \textit{$\mu_k$ = 0.244} and  \textit{$\mu_k$ = 0.16}. To evaluate the model's adaptability and precision, we supply it with four different estimated friction coefficients (\textit{$\mu_e$}) to determine its capability to effectively perform the sliding task in a single step regardless of the accuracy of provided (\textit{$\mu_e$}). Results of this experiment is illustrated in Fig. \ref{fig:hardware_test}-A and Fig. \ref{fig:hardware_test}-B. With a desired relative displacement of 8 cm, the results show that an accurate estimate of \textit{$\mu_e$} keeps the displacement within the acceptable error range (\textit{$\left | \Delta x^B_A - D_{des} \right | \leq 1 cm$}). Furthermore, even a slight mismatch between \textit{$\mu_k$} and \textit{$\mu_e$} (i.e., \textit{$-0.06 \leq \mu_k - \mu_e \leq 0.06$}) still yields only a minor error—remaining below \textit{2 cm}—for the same \textit{$D_{des}$} in the first step.

To clearly illustrate how the actor network achieves the relative displacement in a single step on the real-world setup, Fig.\ref{fig:hardware_test}-C presents the commanded velocity—derived from the generated action and \eqref{eq:velocity_time}—along with the end-effector’s velocity tracking. Meanwhile, the displacement of the end-effector surface (\textit{$x_A$}) and the resulting relative displacement of the object (\textit{$x^B_A$}) during the one-step maneuver are depicted in Fig.\ref{fig:hardware_test}-D. As shown in Fig.~\ref{fig:hardware_test}-C and Fig. \ref{fig:hardware_test}-D, the predefined conditions for the maneuver, stated in \ref{sec:assumptions}, are satisfied.






% \begin{figure}
% \centering
%     \includegraphics[trim=0.01cm 0.01cm 0.01cm 0.15cm, clip, width=0.36\textwidth, height=0.28\textheight]{images/Hardware_one_step.png}
%     % \vspace{-10pt}
%     \caption{This plot illustrates how each friction estimation algorithm reduces the error in the estimated friction coefficient (\textit{$\mu_e$}), across a range of errors \textit{$|\mu - \mu_e|$} $\in  [0.07, 0.15]$}
%     \label{fig:hardware_one_step}
% % \vspace{-10 pt}
% \end{figure}


Next, we seek to evaluate the accuracy of friction estimation algorithms on real world setup, where the required data stated in \ref{sec:algorithms}, particularly those regarding relative motion of the object are noisier than the simulation provided data. We extend the same experiment mentioned in \ref{sec:Sim_result} and reported for simulation setup in Fig. \ref{fig:friction_estimate_acc} to the real-world scenario. Fig. \ref{fig:LSTM_VS_ANALYTICAL_hard} compare the LSTM and the analytical approach performance in reducing the error between (\textit{$\mu_k$}) and (\textit{$\mu_e$}), when the DDPG trained actor network was supposed to perform an action for \textit{$D_{des} = 8\,cm$} in an environment characterized by \textit{$\mu_k = 0.16$}. Following this action, (\textit{$x_A^B$}), (\textit{$\dot{x}_A^B$}), and (\textit{$\ddot{x}_A$}) were provided to the algorithms. The results indicate, the LSTM approach has a better accuracy for the given conditions.  

\begin{figure}
\centering
    \includegraphics[trim=0.01cm 0.01cm 0.01cm 0.15cm, clip, width=0.48\textwidth, height=0.13\textheight]{images/Real_LSTM_VS_ANALYTICAL.pdf}
    % \vspace{-10pt}
    \caption{This plot illustrates how each friction estimation algorithm reduces the error in the estimated friction coefficient (\textit{$\mu_e$}), across a range of errors \textit{$|\mu_e - \mu_k|$} $\in  [0.06, 0.12] $ in real world experiment. The $y$ axis  shows the percentage improvement in the accuracy of the estimated friction coefficient for each model (LSTM and Analytical), where higher values indicate better performance.}
    \label{fig:LSTM_VS_ANALYTICAL_hard}
% \vspace{-10 pt}
\end{figure}

\begin{figure}
\centering
    \includegraphics[trim=0.01cm 0.01cm 0.01cm 0.15cm, clip, width=0.48\textwidth, height=0.16\textheight]{images/image_hardware_edit.pdf}
    % \vspace{-10pt}
    \caption{Comparison of the impact of friction estimation algorithms (analytical and LSTM) on task performance when the actor network starts with significant friction estimation error in simulation experiment.}
    \label{fig:DDPGvsLSTMvsANALY_Hard}
% \vspace{-10 pt}
\end{figure}

Additionally, in the real-world setup, we examined a scenario where \textit{$\mu_k = 0.244$} and \textit{$\mu_e = 0.13$} was provided as the initial state to the actor network. As shown in the analogous simulation scenario in Fig. \ref{fig:sim_estimation_effect} the hardware tests yielded similar performance improvements. The friction estimation algorithms adjusted \textit{$\mu_e$} from 0.13 to \textit{$\mu_e = 0.2106$} using the LSTM approach and \textit{$\mu_e = 0.255$} using the analytical approach. These results demonstrate that the actor network's ability to compensate for discrepancies between \textit{$\mu_e$} and \textit{$\mu_k$}, thereby reducing the number of steps required to accomplish the sliding task and increasing the sliding accuracy in the subsequent step, extends beyond simulation and remains effective in real-world scenarios.

% --------------------------------------------------------------------------------------

% In this experiment, we compared the performance of the standalone DDPG model with our proposed approach, where the LSTM refines the actor model's actions. The accuracy of the first action, using the friction coefficient estimated by the LSTM, is depicted in \textcolor{red}{Fig. ()}. The results indicate that accuracy decreases as the discrepancy between the friction coefficient given to the actor model and the actual coefficient in the environment increases. This trend remained consistent across different displacements, highlighting that while the model generalizes the friction coefficient well, it appears not to be highly sensitive to variations in this parameter.

% The second part of our experiment is way 




% task was to assess the performance of the model, were accomplishment of the displacement is not possible in a single maneuver, and therefore, we get to observe, how the model try to break the task into smaller steps and to compare that  






% For the experiments, we needed to train the model via simulation with similar friction coefficient as the one between the support and the sliding object. The inclined plane test was performed to determine $\mu_k = \tan \theta$, with the slope of the surface denoted as $\theta$, which for our experiment $\mu_k = 0.18$ is estimated. Therefore, we trained an already trained model for $\mu_k = 0.25 $ to reach our desired episodic reward threshold. The training graph is depicted in Fig. \ref{fig:training}. It is worth mentioning that during the training, each episode had a different desired displacement for sliding, ranging from 0.02 m to 0.12 m.




% After training the model, the actor model was tested with 10 desired displacements. The generated actions were applied to both the simulation model and the real-world experiment. In the simulation, the average absolute error was below 0.005 m, with the absolute error ratio to the desired displacement being around 8\%. A single action generated by the actor model for desired displacement of 4 cm, is given to the platform in simulation and in Fig. \ref{fig:result} the accelerations and the relative velocity are illustrated. 




% Without any fine-tuning, the same trained model was used in the real-world experiment, resulting in an average absolute error of 0.018 m and an absolute error ratio to the desired displacement of 20\%. The result was measured based on the first action generated by the actor model, given the desired sliding displacement. The video linked below shows the training process and the experimental results: \textcolor{blue}{\href{https://youtu.be/bpwviPVbkt4}{https://youtu.be/bpwviPVbkt4}}



% -----------------------------------------------------------------------------------------

% \href{llllllllllllllllIIIIIIIINNNNNNNNNKKKKKKKK}{llllllllllllllllIIIIIIIINNNNNNNNNKKKKKKKK}




 % \textcolor{red}{Apart from challenges related to the simulation environment, another barrier to implement this on a real-world robot is definition of an step for reinforcement learning algorithm.}

% Although, Coulomb friction model, can be effective for static friction, the dynamic friction has been investigated in several studies and it is proven that other than the dynamic friction coefficient, factors such as sliding speed, which is clearly not addressed in coulomb model.


% Achieving this goal involves several challenges, starting with the simulation environment and the physics it relies on, and extending to how actions are defined and sequenced. One key issue is the inaccuracy of the Coulomb friction model commonly used in these simulations.(refc)
% we are trying to perform a controlled sliding manipulation without relying on any surrounding objects. (refc), our method considers the complexities of multi-contact interactions, sliding, and stick-slip behavior. Utilizing a Deep Deterministic Policy Gradient (DDPG) framework, which facilitates learning in continuous action spaces, we automate the planning and execution of dynamic tasks, ensuring the system's adaptability and robustness.

% Although, this is important 

% Various studies have been preformed in order to 

% The system is initially trained in the MuJoCo simulation environment using reinforcement learning (RL) to develop an agent capable of optimizing these movements. Once the agent demonstrates proficiency in the simulation, the learned strategies are implemented in real-world scenarios with a constant friction coefficient. Subsequently, the system adapts to varying friction coefficients, enabling it to understand and adjust to different surface conditions.

% Our approach aims to bridge the gap between simulation and real-world application, addressing the challenges posed by dynamic frictional contacts. Unlike previous work that often focuses on single contact points or quasi-static scenarios, our method considers the complexities of multi-contact interactions, sliding, and stick-slip behavior. By leveraging a reinforcement learning framework, we automate the planning and execution of dynamic tasks, ensuring the system's adaptability and robustness.

% The ultimate goal of this project is to create a versatile and efficient robotic manipulation system that can seamlessly transition from simulated environments to real-world applications, handling varying friction conditions with minimal intervention. This research contributes to the development of more generalizable and scalable robotic solutions for dynamic, contact-rich tasks.

\section{Discussion and Conclusion}

The results of this study highlight the efficacy of the proposed DDPG RL framework in addressing the challenges of non-prehensile manipulation through sliding. By integrating friction estimation algorithms—an analytical approach and a data-driven LSTM model—the framework demonstrated robust performance in both simulation and real-world environments. The zero-shot sim-to-real transfer capability underscores the model's adaptability, and domain randomization effectiveness in bridging the gap between simulation and real world condition despite inaccuracies in the Coulomb friction model used in the simulation. Real-time friction estimation improved the accuracy of sliding displacements and reduced the need for corrective actions, as shown in Fig. \ref{fig:sim_estimation_effect} and Fig. \ref{fig:DDPGvsLSTMvsANALY_Hard}. Moreover, the LSTM friction estimation approach outperformed the analytical approach, emphasizing the value of data-driven techniques in analyzing dynamic and complex physical interactions. 

In conclusion, this study demonstrates the potential of DDPG RL framework, in non-prehensile manipulation through training in physics-accurate simulation environment and deployment in real world scenario. By paving the way for applications beyond sliding, this framework sets a foundation for advancing robotic dexterity in tasks traditionally dominated by human skills.

Future studies could focus on extending this framework to more complex and dynamic non-prehensile manipulation tasks such as flipping, rolling, or throwing. Improving robustness against higher levels of environmental noise and broadening the friction estimation algorithms to include additional factors—such as temperature or surface deformation—will further expand its practical applicability.





% The experimental results show that the DDPG framework effectively generalizes the task of sliding an object over various distances. The accuracy in simulation is higher than the real world experiment, partly due to potential errors in measuring the kinetic friction coefficient and limitations of the coulomb friction model.

% Our ultimate goal is manipulation through sliding without prior knowledge of the friction coefficient. These experiments suggest that designing a neural network model to generalize sliding manipulation and an algorithm to infer the friction coefficient from input actions and displacements makes this goal achievable. Integrating the friction inference algorithm with a DDPG model could enhance the accuracy of non-prehensile manipulation through sliding.

\balance

\begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.

\bibitem{c1} W. Yang and M. Posa, "Dynamic On-Palm manipulation via controlled sliding," \emph{arXiv}, Cornell University, 2024. Available: {https://doi.org/10.48550/arxiv.2405.08731}.
\bibitem{a2} A. Curtis, X. Fang, L. P. Kaelbling, T. Lozano-Pérez and C. R. Garrett, "Long-Horizon Manipulation of Unknown Objects via Task and Motion Planning with Estimated Affordances," 2022 International Conference on Robotics and Automation (ICRA), Philadelphia, PA, USA, 2022, pp. 1940-1946, doi: 10.1109/ICRA46639.2022.9812057.


\bibitem{a1} A. Heins and A. P. Schoellig, "Keep It Upright: Model Predictive Control for Nonprehensile Object Transportation With Obstacle Avoidance on a Mobile Manipulator," in IEEE Robotics and Automation Letters, vol. 8, no. 12, pp. 7986-7993, Dec. 2023, doi: 10.1109/LRA.2023.3324520.


\bibitem{c2} Y. Sang, M. Dubé, and M. Grant, "Dependence of friction on roughness, velocity, and temperature," \emph{Physical Review. E, Statistical, Nonlinear and Soft Matter Physics}, vol. 77, no. 3, 2008. [Online]. Available: https://doi.org/10.1103/physreve.77.036123.

\bibitem{c3} E. Todorov, T. Erez, and Y. Tassa, "MuJoCo: A physics engine for model-based control," 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vilamoura-Algarve, Portugal, 2012, pp. 5026-5033, doi: 10.1109/IROS.2012.6386109.

\bibitem{c4} T. P. Lillicrap et al., "Continuous control with deep reinforcement learning," arXiv:1509.02971 [cs.LG], 2015, doi: 10.48550/arXiv.1509.02971.



\bibitem{c5}
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, Vancouver, BC, Canada, 2017, pp. 23-30, doi: 10.1109/IROS.2017.8202133.

\bibitem{c6}
X. Chen, J. Hu, C. Jin, L. Li, and L. Wang, "Understanding domain randomization for SIM-to-real transfer," \textit{arXiv (Cornell University)}, Jan. 2021, doi: 10.48550/arxiv.2110.03239.

\bibitem{c7} 
W. Zhou, B. Jiang, F. Yang, C. Paxton, and D. Held, "HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation," \textit{arXiv (Cornell University)}, May 2023, doi: 10.48550/arxiv.2305.03942

\bibitem{c8} M. T. Mason, "Progress in Nonprehensile Manipulation," \textit{The International Journal of Robotics Research}, vol. 18, no. 11, pp. 1129-1141, 1999, doi: 10.1177/02783649922067762

\bibitem{c9} M. Higashimori, K. Utsumi, Y. Omoto, and M. Kaneko, "Dynamic Manipulation Inspired by the Handling of a Pizza Peel," \textit{IEEE Transactions on Robotics}, vol. 25, no. 4, pp. 829-838, 2009, doi: 10.1109/TRO.2009.2017085


\bibitem{c10} V. Mnih, K. Kavukcuoglu, D. Silver, et al., "Human-level control through deep reinforcement learning," \textit{Nature}, vol. 518, pp. 529-533, 2015, doi: 10.1038/nature14236.


\bibitem{c11} T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," \textit{arXiv (Cornell University)}, 2015, doi: 10.48550/arXiv.1509.02971

\bibitem{c12} J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal Policy Optimization Algorithms," \textit{arXiv (Cornell University)}, 2017, doi: 10.48550/arXiv.1707.06347.

\bibitem{c13} I. Geles, L. Bauersfeld, A. Romero, J. Xing, and D. Scaramuzza, "Demonstrating Agile Flight from Pixels without State Estimation," \textit{arXiv (Cornell University)}, 2024, doi: 10.48550/arXiv.2406.12505.

\bibitem{c14} C. Zhou, B. Huang, H. Hassan, et al., "Attention-based advantage actor-critic algorithm with prioritized experience replay for complex 2-D robotic motion planning," \textit{Journal of Intelligent Manufacturing}, vol. 34, pp. 151-180, 2023, doi: 10.1007/s10845-022-01988-z.

\bibitem{c15} D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A. Riedmiller, "Deterministic policy gradient algorithms," in \textit{Proc. Int. Conf. Mach. Learn.}, 2014.

\bibitem{c16}
S. J. Pan and Q. Yang, "A Survey on Transfer Learning," IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345-1359, 2010, doi: 10.1109/TKDE.2009.191.

\bibitem{c17} E. Wiewiora, "Potential-Based Shaping and Q-Value Initialization are Equivalent," Journal of Artificial Intelligence Research, vol. 19, pp. 205–208, 2003, doi: 10.1613/jair.1190.

\bibitem{c18} M. Selvaggio, A. Garg, F. Ruggiero, G. Oriolo, and B. Siciliano, "Non-prehensile object transportation via model predictive non-sliding manipulation control," IEEE Transactions on Control Systems Technology, vol. 31, no. 5, pp. 2231–2244, 2023, doi: 10.1109/TCST.2023.3277224.

\bibitem{c19}
D. Serra, F. Ruggiero, A. Donaire, L. Buonocore, V. Lippiello, and B. Siciliano, "Control of nonprehensile planar rolling manipulation: A passivity-based approach," IEEE Transactions on Robotics, vol. 35, no. 2, pp. 317–329, 2019.

\bibitem{c20}
A. Gutierrez-Giles, F. Ruggiero, V. Lippiello, and B. Siciliano, "Closed-loop control of a nonprehensile manipulation system inspired by a pizza-peel mechanism," Proceedings of the European Control Conference (ECC), Naples, Italy, 2019, pp. 1580–1585.

\bibitem{c21}
A. Satici, F. Ruggiero, V. Lippiello, and B. Siciliano, "Coordinate-free framework for robotic pizza tossing and catching," Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2016, pp. 3932–3939.

\bibitem{c22}
Y. Cho, J.-H. Park, J. Choi, and D. E. Chang, "Sim-to-Real Transfer of Image-Based Autonomous Guidewire Navigation Trained by Deep Deterministic Policy Gradient with Behavior Cloning for Fast Learning," Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022, pp. 3468–3475, doi: 10.1109/IROS47612.2022.9982168.

\bibitem{c23}
C. Guo and W. Luk, "FPGA-Accelerated Sim-to-Real Control Policy Learning for Robotic Arms," in IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 71, no. 3, pp. 1690–1694, March 2024, doi: 10.1109/TCSII.2024.3353690.

\bibitem{c24}
G. Tiboni, P. Klink, J. Peters, T. Tommasi, C. D'Eramo, and G. Chalvatzaki, "Domain Randomization via Entropy Maximization," arXiv preprint arXiv:2311.01885, 2023. DOI: 10.48550/arXiv.2311.01885.


\end{thebibliography}

% \subsection{Selecting a Template (Heading 2)}

% First, confirm that you have the correct template for your paper size. This template has been tailored for output on the US-letter paper size. 
% It may be used for A4 paper size if the paper size setting is suitably modified.

% \subsection{Maintaining the Integrity of the Specifications}

% The template is used to format your paper and style the text. All margins, column widths, line spaces, and text fonts are prescribed; please do not alter them. You may note peculiarities. For example, the head margin in this template measures proportionately more than is customary. This measurement and others are deliberate, using specifications that anticipate your paper as one part of the entire proceedings, and not as an independent document. Please do not revise any of the current designations


\end{document}
