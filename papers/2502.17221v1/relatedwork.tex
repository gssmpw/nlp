\section{Related Works}
\subsubsection{\textbf{Non-prehensile manipulation}} Non-prehensile manipulation, or the ability to manipulate objects without grasping, is crucial for tasks involving hard-to-grasp objects or cluttered workspaces \cite{c7, c8}. This types of manipulation includes throwing \cite{c21}, pushing \cite{c20}, rolling \cite{c19}, and sliding \cite{c1} among others. Several studies have explored non-prehensile manipulation of an object on horizontal surfaces \cite{c1, c18, c9}. One of these studies \cite{c18} primarily focused on transporting an object on a horizontal surface while ensuring no sliding or slippage occurs during the task. In contrast, the objective of our work deliberately leverages sliding as the primary method for moving the object across the surface. The other study \cite{c1}, investigates the control approach of sliding an object on the surface and impressive controllability and performance were demonstrated. It is important to note that this study assumes the presence of external barriers to assist in performing the sliding task and incorporates precise knowledge of the surface friction coefficient (\textit{$\mu$}) into its model-based controller. While this setup enables impressive controllability and performance, it inherently limits the approach's ability to generalize to scenarios without external constraints or surfaces with unknown friction properties. Another study in this area \cite{c9}, avoided external barriers but lacked real-time friction adaptation, which our work addresses through on-line friction estimation and robust RL-based control.

% \textbf{Additionally, in this exeriment   In contrast, the approach in \cite{c9} employs linear acceleration and deceleration as their manipulation strategy which does not need external barrier. It uses a supporting platform where the sliding object is placed on it, However, this method relies on assumptions such as known static and dynamic friction coefficients and uniform mass distribution, which limit its adaptability to real-world scenarios where these parameters are often unknown.}



\subsubsection{\textbf{RL for continuous action space}} Reinforcement learning algorithms have long been studied and applied in simplified environments, such as the Atari games discussed in \cite{c10}. In these settings, they have achieved notable success, often surpassing human experts in performance. However, these environments rely on discrete action spaces, and for most control problems in real world applications, continuous action space is necessary. Several methods have been introduced to extend reinforcement learning to continuous action spaces. Among the most prominent are actor-critic models like Deep Deterministic Policy Gradient (DDPG) \cite{c11} and Proximal Policy Optimization (PPO) \cite{c12}, both of which are proven highly effective for handling continuous control tasks. Thus, these method have been utilized more frequently in recent studies in robotics such as training an actor-critic model for non-prehensile manipulation where the end-effector can interact with the object on an external surface, to perform the manipulation, in which they achieved a 50\% success in zero-shot sim-to-real transfer over unseen objects \cite{c7}.
Actor-critic RL models can be applied to other applications as well, such as robotic motion planning in dynamic environments \cite{c14} and drones learning to fly through gates autonomously \cite{c13}.