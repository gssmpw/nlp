\section{Related Work}
\textbf{Personalized Diffusion Models.} Recent advancements in identity-preserving image generation have seen rapid development, with several approaches employing Textual Inversion \cite{gal2022image}, DreamBooth \cite{ruiz2023dreambooth}, and LoRA \cite{hu2021lora} for fine-tuning models on specific IDs, achieving impressive results. However, these methods lack flexibility and the capability for real-time inference. In contrast, training-free methods effectively eliminate the dependency on parameter fine-tuning. For instance, the IP-Adapter \cite{radford2021learning} leverages CLIP features \cite{radford2021learning} to guide pre-trained models, ensuring identity preservation. PuLID \cite{guo2024pulid} adopts EvaClip \cite{sun2023eva} to maintain identity consistency, while InstantID \cite{wang2024instantid} integrates ArcFace \cite{deng2019arcface} with pose networks to achieve flexible ID retention. Within the realm of identity-preserving video generation, maintaining smooth character motion alongside accurate preservation of identity features represents a key challenge. The ID-Animator \cite{he2024id}, built upon the AnimateDiff \cite{guo2023animatediff} base model, has successfully preserved identity characteristics but exhibits noticeable limitations in the fluidity of character movements. The emerging DiT architecture \cite{yang2024cogvideox, opensora} shows promise for enhancing consistency in video output. ConsisID \cite{yuan2024identity}, which builds on CogVideoX\cite{yang2024cogvideox}, employs a frequency-aware control scheme to enhance identity consistency without the need for identity-specific tuning. However, existing methods still face challenges such as low motion amplitude and facial instability during movement, which can result in limited dynamic movements and unnatural changes or distortions in facial features across frames.

\noindent \textbf{Portrait Animation.} Portrait animation techniques have made significant strides in animating static images by mapping motions and expressions from a guiding video while preserving the portrait's identity and background. \cite{khakhulin2022realistic, wiles2018x2face, yao2020mesh, pang2023dpe, wang2021one} Research primarily focuses on 3D morphable models \cite{blanz2023morphable}, such as DECA \cite{DECA:Siggraph2021} and FLAME \cite{li2017learning}, which excel in detailed 3D face modeling but largely concentrate on facial features without extending to full-body animations or scene elements. In rendering, volumetric methods provide high detail but are computationally intense, while CVTHead \cite{ma2024cvthead} offers a more efficient, yet still facially focused, approach. Animation methods like EchoMimic \cite{chen2024echomimic}, which relies on Mediapipe \cite{lugaresi2019mediapipe}, and SadTalker \cite{zhang2023sadtalker}, which uses audio inputs to generate 3D motion coefficients, also emphasize facial regions. Despite their advancements, these methods generally lack the ability to generate or control complete scenes through text-based inputs, highlighting a gap in creating broader narrative or environmental elements through such interactions.