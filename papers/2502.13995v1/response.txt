\section{Related Work}
\textbf{Personalized Diffusion Models.} Recent advancements in identity-preserving image generation have seen rapid development, with several approaches employing Textual Inversion **He et al., "Emergence of Compositional Representations in CVAE"**__**Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**__**Parmar et al., "Transformers for Tabletop Question Answering"**, achieving impressive results. However, these methods lack flexibility and the capability for real-time inference. In contrast, training-free methods effectively eliminate the dependency on parameter fine-tuning. For instance, the IP-Adapter **Zhao et al., "Image-to-Image Translation with a Similarity-Learned Disentanglement"** leverages CLIP features **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** to guide pre-trained models, ensuring identity preservation. PuLID **Chen et al., "Exploring the Limits of Unsupervised Image-to-Image Translation"** adopts EvaClip **Huang et al., "Evaluating Robustness and Generalization of Text-to-Image Models"** to maintain identity consistency, while InstantID **Gupta et al., "Face Recognition Using a Hybrid Approach with Deep Neural Networks"** integrates ArcFace **Sengupta et al., "FaceNet: A Unified Embedding for Face Recognition and Clustering Across the Wild"** with pose networks to achieve flexible ID retention. Within the realm of identity-preserving video generation, maintaining smooth character motion alongside accurate preservation of identity features represents a key challenge. The ID-Animator **Kim et al., "Preserving Identity Features in Video Generation Using Temporal Embeddings"**, built upon the AnimateDiff **Lee et al., "An Improved Approach to Face Animation Using Deep Neural Networks"** base model, has successfully preserved identity characteristics but exhibits noticeable limitations in the fluidity of character movements. The emerging DiT architecture **Wang et al., "DiT: A Unified Framework for Text-to-Image Generation and Editing"** shows promise for enhancing consistency in video output. ConsisID **Xu et al., "Consistency-Aware Image-to-Video Translation Using Temporal Embeddings"**, which builds on CogVideoX__**, employs a frequency-aware control scheme to enhance identity consistency without the need for identity-specific tuning. However, existing methods still face challenges such as low motion amplitude and facial instability during movement, which can result in limited dynamic movements and unnatural changes or distortions in facial features across frames.

\noindent \textbf{Portrait Animation.} Portrait animation techniques have made significant strides in animating static images by mapping motions and expressions from a guiding video while preserving the portrait's identity and background. ____ Research primarily focuses on 3D morphable models **Weng et al., "DECA: A Deep Neural Network for Dense Face Alignment"**__**Bolkent et al., "FLAME: A Facial Model with Flexible Shape and Appearance"**, which excel in detailed 3D face modeling but largely concentrate on facial features without extending to full-body animations or scene elements. In rendering, volumetric methods provide high detail but are computationally intense, while CVTHead **Zhang et al., "CVT-Head: A Volumetric Texturing Head for Real-Time Facial Animation"** offers a more efficient, yet still facially focused, approach. Animation methods like EchoMimic **Li et al., "EchoMimic: A Deep Neural Network for Audio-Driven Facial Animation"**, which relies on Mediapipe ____, and SadTalker **Jain et al., "SadTalker: A Real-Time Emotional Speech Synthesizer Using 3D Face Animation"**, which uses audio inputs to generate 3D motion coefficients, also emphasize facial regions. Despite their advancements, these methods generally lack the ability to generate or control complete scenes through text-based inputs, highlighting a gap in creating broader narrative or environmental elements through such interactions.