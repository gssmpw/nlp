\section{Related Work}
\textbf{Personalized Diffusion Models.} Recent advancements in identity-preserving image generation have seen rapid development, with several approaches employing Textual Inversion ____, DreamBooth ____, and LoRA ____ for fine-tuning models on specific IDs, achieving impressive results. However, these methods lack flexibility and the capability for real-time inference. In contrast, training-free methods effectively eliminate the dependency on parameter fine-tuning. For instance, the IP-Adapter ____ leverages CLIP features ____ to guide pre-trained models, ensuring identity preservation. PuLID ____ adopts EvaClip ____ to maintain identity consistency, while InstantID ____ integrates ArcFace ____ with pose networks to achieve flexible ID retention. Within the realm of identity-preserving video generation, maintaining smooth character motion alongside accurate preservation of identity features represents a key challenge. The ID-Animator ____, built upon the AnimateDiff ____ base model, has successfully preserved identity characteristics but exhibits noticeable limitations in the fluidity of character movements. The emerging DiT architecture ____ shows promise for enhancing consistency in video output. ConsisID ____, which builds on CogVideoX____, employs a frequency-aware control scheme to enhance identity consistency without the need for identity-specific tuning. However, existing methods still face challenges such as low motion amplitude and facial instability during movement, which can result in limited dynamic movements and unnatural changes or distortions in facial features across frames.

\noindent \textbf{Portrait Animation.} Portrait animation techniques have made significant strides in animating static images by mapping motions and expressions from a guiding video while preserving the portrait's identity and background. ____ Research primarily focuses on 3D morphable models ____, such as DECA ____ and FLAME ____, which excel in detailed 3D face modeling but largely concentrate on facial features without extending to full-body animations or scene elements. In rendering, volumetric methods provide high detail but are computationally intense, while CVTHead ____ offers a more efficient, yet still facially focused, approach. Animation methods like EchoMimic ____, which relies on Mediapipe ____, and SadTalker ____, which uses audio inputs to generate 3D motion coefficients, also emphasize facial regions. Despite their advancements, these methods generally lack the ability to generate or control complete scenes through text-based inputs, highlighting a gap in creating broader narrative or environmental elements through such interactions.