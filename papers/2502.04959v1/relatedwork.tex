\section{Related Work}
\minisection{Model merging.}
Pre-trained models serve as a foundation for expert models specialized in specific downstream tasks~\cite{radford2021learning}. Recently, model merging has emerged as a promising technique to combine multiple expert models into a single multi-task model.
One of the pioneering works in the field, Task Arithmetic (TA)~\cite{ilharco2023task}, proposed to compute a \textit{task vector} as a difference between the expert and the pre-trained model and to then aggregate task vectors via scaled addition to create an expert in multiple tasks. The significant performance gap between individual experts and the combined model sparked an abundance of works with the aim of reducing interference when merging models. TIES~\cite{yadav2023tiesmerging} proposed a novel way to reduce sign conflicts between the parameters of expert models, Model Breadcrumbs~\cite{davari2023model} removed outliers from the task vectors, and Consensus Merging~\cite{wang2024localizing} removed catastrophic and selfish weights. These methods focused on per-parameter techniques to mitigate the interference, treating each parameter independently.

\minisection{Singular Value Decomposition of model weights.}
While SVD of weight matrices has been primarily used for model compression~\cite{NIPS2014_2afe4567,KimPYCYS15}, recently its effectiveness was also identified for fine-tuning of large models. LoRA~\cite{hu2021lora} uses SVD to identify the similarities of weight updates between low-rank and full-rank fine-tuning.
MiLORA~\cite{milora} identifies that the bottom singular components correspond to noisy or long-tail information, while the top singular vectors contain important knowledge. Therefore, they propose a fine-tuning approach that updates only the minor singular components of the weight matrix while keeping the top singular components frozen. 
SVFT~\cite{svft} computes outer products of its singular vectors and, during fine-tuning updates, only sparse coefficients of these combinations.

\minisection{SVD for model merging.}
The structure imposed by SVD was used for model merging in KnOTS~\cite{stoica2024knots}, which proposes to concatenate the task-specific low-rank adaptation matrices (LoRA) and average the right-singular vectors before SVD reconstruction to obtain the merged weights.
The most similar work to us is the parallel work Task Singular Vectors (TSV)~\cite{tsv}, which measures task interference based on the interaction of singular vectors from different tasks and uses it to increase merging effectiveness. We share the motivation to improve model merging through SVD decomposition. However, while they focus on the orthogonalization of task-specific subspaces to reduce interference, we show that making singular values uniform in a common subspace is a surprisingly powerful method. Further, we show how to combine shared and task-specific subspaces for improved performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%