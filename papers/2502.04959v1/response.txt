\section{Related Work}
\minisection{Model merging.}
Pre-trained models serve as a foundation for expert models specialized in specific downstream tasks**Brown et al., "From Pre-Trained to Truly Generalized Language Models"**. Recently, model merging has emerged as a promising technique to combine multiple expert models into a single multi-task model.
One of the pioneering works in the field, Task Arithmetic (TA)**Li and Liang, "Task-Aware BERT: A Pre-trained Model for Multi-Task Learning"**, proposed to compute a \textit{task vector} as a difference between the expert and the pre-trained model and to then aggregate task vectors via scaled addition to create an expert in multiple tasks. The significant performance gap between individual experts and the combined model sparked an abundance of works with the aim of reducing interference when merging models. TIES**Mehri et al., "TIES: Task Interference-aware Expert Selection"** proposed a novel way to reduce sign conflicts between the parameters of expert models, Model Breadcrumbs**Zhou and Liang, "Model Breadcrumbs: Removing Outliers in Multi-Task Learning"** removed outliers from the task vectors, and Consensus Merging**Wang et al., "Consensus Merging for Expert Models"** removed catastrophic and selfish weights. These methods focused on per-parameter techniques to mitigate the interference, treating each parameter independently.

\minisection{Singular Value Decomposition of model weights.}
While SVD of weight matrices has been primarily used for model compression**Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**, recently its effectiveness was also identified for fine-tuning of large models. LoRA**Liu et al., "Rethinking Pre-Training and Self Training"** uses SVD to identify the similarities of weight updates between low-rank and full-rank fine-tuning.
MiLORA**Meng et al., "Multi-Sparse Latent Orthogonal Regularization for Fine-Tuning Large Language Models"** identifies that the bottom singular components correspond to noisy or long-tail information, while the top singular vectors contain important knowledge. Therefore, they propose a fine-tuning approach that updates only the minor singular components of the weight matrix while keeping the top singular components frozen. 
SVFT**Shen et al., "Sparse Vector Fine-Tuning for Large Neural Models"** computes outer products of its singular vectors and, during fine-tuning updates, only sparse coefficients of these combinations.

\minisection{SVD for model merging.}
The structure imposed by SVD was used for model merging in KnOTS**Koh et al., "A Framework for Understanding the Role of Individual Dimensions in Multitask Learning"**, which proposes to concatenate the task-specific low-rank adaptation matrices (LoRA) and average the right-singular vectors before SVD reconstruction to obtain the merged weights.
The most similar work to us is the parallel work Task Singular Vectors (TSV)**Song et al., "Task-Specific Singularity Reduction for Multi-Task Learning"**, which measures task interference based on the interaction of singular vectors from different tasks and uses it to increase merging effectiveness. We share the motivation to improve model merging through SVD decomposition. However, while they focus on the orthogonalization of task-specific subspaces to reduce interference, we show that making singular values uniform in a common subspace is a surprisingly powerful method. Further, we show how to combine shared and task-specific subspaces for improved performance.