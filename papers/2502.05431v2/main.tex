\documentclass[]{fairmeta}
% Option "twocolumn" available, but please prioritize single-column
\usepackage{microtype}
\usepackage{lmodern}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algpseudocode}
\usepackage[linesnumbered,lined,boxed,commentsnumbered,ruled,longend]{algorithm2e}
\usepackage[capitalize,noabbrev]{cleveref}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{enumitem}
\title{\raisebox{-3pt}{\includegraphics[width=0.045\linewidth]{figures/APE.pdf}}
APE: Faster and Longer Context-Augmented Generation via \underline{A}daptive \underline{P}arallel \underline{E}ncoding}


\author[\dagger]{Xinyu Yang}
\author[\dagger \ddagger] {Tianqi Chen}
\author[\dagger]{Beidi Chen}

\affiliation[\dagger]{Carnegie Mellon University}
\affiliation[\ddagger]{Nvidia}

\abstract{
 Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly
applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose \underline{A}daptive \underline{P}arallel \underline{E}ncoding (\textbf{APE}), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98\% and 93\% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6\% and 7.9\%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\times$ speedup by reducing 28$\times$ prefilling time for a 128K-length context.
}


%\date{\today}
% You can add additional metadata fields as follows 
\metadata[Github]{\url{https://github.com/Infini-AI-Lab/APE}}
\metadata[Website]{\url{https://infini-ai-lab.github.io/APE-Page}}

\begin{document}

\maketitle
\input{text/introduction}
\input{text/background}
\input{text/observation}
\input{text/methodology}
\input{text/experiments}
\input{text/analysis}
\input{text/conclusion}
\clearpage
\newpage
\bibliographystyle{assets/plainnat}
\bibliography{paper}

\clearpage
\newpage
\beginappendix
\input{text/appendix}

\end{document}