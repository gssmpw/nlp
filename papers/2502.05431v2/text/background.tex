\section{Background and Related Work}
\label{Preliminaries}

\subsection{Context-Augmented Generation}

This work explores CAG problems using LLMs, where user queries are enhanced with additional contexts from external databases. CAG typically involves two scenarios: RAG~\citep{asai2024reliable, gupta2024rag, gao2023retrieval}, which focuses on directly retrieving relevant information, and ICL~\citep{dong2022survey, wei2022emergent, agarwal2024many}, which emphasizes further acquiring
emergent capabilities from in-context examples.

\subsection{Parallel Encoding}
\label{sec:2.2}

Next, we present the formulation of using parallel encoding in LLMs for CAG settings. Let $\mathcal{S}$ represent the input sequence including $N$ contexts $C_1, ..., C_{N}$ and one query $Q$. Formally, this can be denoted as:
\begin{equation}
\mathcal{S} = \{\underbrace{s_{C_1, 1}, ... , s_{C_1, l_1}}_{\text{Context 1}}, \underbrace{s_{C_2, 1}, ..., s_{C_2, l_2}}_{\text{Context 2}}, ..., \underbrace{s_{C_N, 1}, ..., s_{C_N, l_N}}_{\text{Context N}}, \underbrace{s_{Q, 1}, ..., s_{Q, l}}_{\text{Query}}\}.
\end{equation}
For simplicity, we can express this as:
$\mathcal{S} = \{S_{C_1}, S_{C_2}, \ldots, S_{C_N}, S_Q\}$. Given two models $\Theta_{\text{Enc}}$ and $\Theta_{\text{Dec}}$ (which may be the same model), a response $\mathcal{R}$ is generated to the input $\mathcal{S}$ using parallel encoding in two steps:

\textbf{Pre-caching Contexts.} The first step is to encode and cache the KV states for each context independently using $\Theta_{\text{Enc}}$. For a given context $S_{C_i}$, we compute its KV states offline as $(K_{C_i}, V_{C_i}) = \Theta_{\text{Enc}}(S_{C_i})$ and store them for direct loading during inference. Specifically, we denote $K_{C_i} = \{k_{C_i, 1}, \ldots , k_{C_i, l_i}\}$ and $V_{C_i} = \{v_{C_i, 1}, \ldots , v_{C_i, l_i}\}$.

\textbf{Generating Response.} Next, the user query is augmented by all relevant pre-cached KV states to generate the response:
$\mathcal{R} = \Theta_{\text{Dec}}(S_{Q}, K_{C}, V_{C})$, where $K_{C}$, $V_{C}$ are subsets of $\{K_{C_1}, ..., K_{C_N}\}$ and $\{V_{C_1}, ..., V_{C_N}\}$, respectively.

Parallel encoding significantly improves efficiency compared to sequential encoding by reducing the complexity of prefilling from $O((l_1+...+l_N+l_Q)^2)$ (i.e., quadratic) to linear concerning the total context length. With pre-caching, the cost becomes $O((l_1+...+l_N+l_Q)\cdot l_Q)$. In the absence of pre-caching, the complexity is $O(\max(l_1^2, ..., l_N^2)+((l_1+...+l_N+l_Q)\cdot l_Q)$, which remains efficient for multiple contexts of similar length.


Prior parallel encoding approaches vary in their design of $\Theta_{\text{Enc}}$ and $\Theta_{\text{Dnc}}$. Parallel Context Windows (PCW)~\citep{ratner2022parallel} directly employs pre-trained LLMs as both, resulting in significant performance drops. Block-Attention~\citep{Sun2024BlockAttentionFE} further fine-tunes the model, successfully recovering performance in RAG tasks. Alternatively, CEPE~\citep{yen2024long} and FocusLLM~\citep{li2024focusllm} train new Transformer-based encoders using encoder-only and decoder-only architectures, respectively. These methods also differ in $\Theta_{\text{Dec}}$: CEPE trains additional cross-attention layers for processing contexts, whereas other methods directly input the context into original self-attention layers. While these trainable methods show promising results in RAG tasks, challenges remain regarding their training overheads and generalization abilities to more complex ICL scenarios. Moreover, applying parallel encoding in CAG can be viewed as a kind of memory-augmented neural networks~\citep{burtsev2020memory, de2021mention, fevry2020entities}, where external memory is directly stored into KV states.

\subsection{Attention Mechanism}

In a standard $\mathrm{Softmax}$ attention, we attend the query to all past KV states using the following formula:

\begin{equation}
    O = \mathrm{Softmax}(\frac{QK^T}{\sqrt{d}})V \quad Q\in \mathbb{R}^{n \times d} \quad K,V \in \mathbb{R}^{m \times d},
    \label{eq:attention}
\end{equation}

where $Q$ is the query state, and $K$ and $V$ denote the key and value states, respectively. Previous research has revealed several significant insights into the distribution of attention weights (i.e., $\mathrm{Softmax}(\frac{QK^T}{\sqrt{d}})$).

\textbf{Attention Sink.} StreamingLLM~\citep{xiao2023efficient} identifies the presence of an ``attention sink'' in LLMs, a token that receives a significantly higher attention score than other tokens but provides limited semantic information. It observes that the attention sink exists in the initial token and influences the following tokens.

\textbf{Position Embedding.} To effectively process sequential input, LLMs require position embeddings, such as absolute position embeddings~\citep{vaswani2017attention, devlin2018bert} and relative position embeddings~\citep{su2024roformer, press2021train}. However, the introduction of position embedding not only limits the context window to the training length~\citep{chen2023extending} but also results in the ``lost in the middle"~\citep{liu2024lost} issue, where LLMs struggle to produce correct answers when relevant information locates in the middle of the context.

