\section{Experiments}

Empirically, we present the effectiveness and efficiency of APE in CAG scenarios such as RAG and ICL. Since we focus on context encoding problems, we do not include comparisons with long-context LLMs. Specifically,
\begin{itemize}[itemsep=0.0ex,topsep=0pt,leftmargin=*]
	\item In Section~\ref{sec:longbench}, APE can maintain 98\% of the accuracy on ChatRAG-Bench compared to sequential encoding. Furthermore, it improves 3.3\% performance for RAG on LongBench by retrieving more and longer contexts. 
	\item In Section~\ref{icl}, APE outperforms parallel encoding by 7.9\% on average in three ICL tasks. Moreover, APE can maintain 93\% of the accuracy achieved by sequential encoding when using the same number of examples.
        \item In Section~\ref{sec:loft}, APE can scale to many-shot CAG tasks, effectively encoding hundreds of texts in parallel.
	\item In Section~\ref{sec:efficiency}, APE achieves 4.5$\times$ faster inference for 128k context through 28$\times$ reduction in prefilling time.
\end{itemize}

\subsection{Retrieval-Augmented Generation.}
\label{sec:longbench}

In the context of RAG tasks, we validate that APE retains most of the sequential encoding capability while accommodating more and longer contexts, mitigating retrieval errors, and outperforming encoding baselines.

%\bcc{summarize your key results in each subsection too. Be more concrete than your claims above.}

\subsubsection{Retrieval for Multi-turn Question Answering.}

\textbf{Setup.} APE is evaluated on five conversational QA tasks using ChatRAGBench~\citep{liu2024chatqa}. For each query, we prepare about 100 text chunks. Three retrievers of varying quality are employed to retrieve up to the top-5 chunks for evaluation, including Contriever~\citep{izacard2021unsupervised}, GTE-base~\cite{li2023towards}, and Dragon-multiturn~\cite{liu2024chatqa}. We use \textsc{Llama3-ChatQA-1.5-8B} as the base model. To fairly measure performance drop after our modifications, the same retrieved texts are used for APE and sequential encoding.

\textbf{Results.} Table~\ref{tab:chatragbench} shows that switching from sequential encoding to APE results in performance drops of 0.51\%, 0.92\%, and 1.14\% across different retrievers, respectively. While this drop increases with retriever quality, APE still keeps 97\% of the sequential encoding performance for the best retriever. By increasing the text chunk length for $5$ times, APE directly inputs all texts without any retrieval process, achieving superior performance.

\input{tables/chatrag}

\subsubsection{Retrieval for Long-context Understanding.}

\textbf{Setup.} Our evaluation involves eight tasks on LongBench~\citep{bai2023longbench}. Given the long context,  we split it into chunks with a size of $M$ words, employ Contriever~\citep{izacard2021unsupervised} to compute the embeddings of all
chunks and the query and retrieve the top-$N$ chunks according to the cosine similarity of
their embeddings to the query embedding. $M$ and $N$ vary across different methods. We compare APE with sequential encoding with and without RAG, and PCW, using \textsc{Llama-3-8B-Instruct}~\citep{llama3}, \textsc{Mistral-7B-Instruct-v0.3}~\citep{jiang2023mistral}, \textsc{Gemma-2-9b-it}~\citep{gemma2}, and \textsc{Llama-3.1-8B-Instruct} as base models. 

%each requiring multi-document input: HotpotQA~\citep{yang2018hotpotqa}, 2WikiMQA~\citep{ho2020constructing}, MuSiQue~\citep{trivedi2022musique}, and MultiNews~\citep{fabbri2019multi}. We choose three LLMs with limited context window sizes as base models: \textsc{Llama-3-8B-Instruct}~\citep{llama3}, \textsc{Llama-2-7b-chat}~\citep{llama2}, and \textsc{Gemma-2-9b-it}~\citep{gemma2}. 

%\textbf{Baselines.} We compare with various strategies for lengthy input including: (i) \textit{Prompt} \textit{Compression}: Truncation, LLMLingua2~\citep{pan2024llmlingua}, (ii) \textit{KV Cache Eviction}: StreamingLLM~\citep{xiao2023efficient}, (iii) \textit{Long-context FT}: Llama-3-8B-Instruct-262K~\citep{gradllama}, Llama-2-7B-Instruct-32K~\citep{togetherllama}, (iv) \textit{Parallel Encoding}: PCW~\citep{ratner2022parallel}, CEPE~\citep{yen2024long}.

\textbf{Results.} In Table~\ref{tab:rag_longbench}, APE consistently improves performance across all models, achieving a 5.6\% average gain over sequential encoding without RAG. It also outperforms sequential RAG baselines by 3.3\% by retrieving more and longer contexts. The superior performance over PCW further showcases the effectiveness of our modifications in APE. Notably, APE surpasses the 128K-context variant of the \textsc{Llama-3.1-8B-Instruct} model by placing retrieved texts within the 8K context window, mitigating the ``lost in the middle" phenomenon.
\input{tables/longbench}

\subsection{In-context Learning}
\label{icl}

\textbf{Setup.} We evaluate APE on three ICL tasks using the LM Evaluation Harness~\citep{eval-harness} codebase: GSM8K (8-shot)~\citep{cobbe2021training}, TriviaQA (5-shot)~\citep{joshi2017triviaqa}, and MMLU (5-shot)~\citep{hendrycks2020measuring}. Experiments are conducted using the same base models as in our LongBench evaluations. We compare parallel encoding (PCW) to show the improvement of APE. Sequential encoding with varying numbers of shots (i.e., 1-shot, half-shots, and full-shots) is also employed to measure the gap from the ideal scenarios.

\textbf{Results.} In Figure~\ref{fig:icl}, APE surpasses parallel encoding with average improvements of 15.4\% on GSM8K, 4.7\% on TriviaQA, and 3.5\% on MMLU. When compared with the 1-shot sequential baseline with similar context length, our method consistently yields superior results. Moreover, APE performs better than half-shot sequential encoding in 8/12 settings and preserves 93\% accuracy compared to full-shot sequential encoding. Additionally, the \textsc{Llama} family exhibits enhanced compatibility with parallel encoding, potentially due to the stronger directional alignment of initial tokens from different contexts (see Figure~\ref{fig:observation2}a). Across different tasks, the performance gap between APE and full-shot sequential encoding is the largest on GSM8K. This finding suggests that while APE keeps most capabilities, its effectiveness may decrease as task complexity increases.

\input{figures/icl}

\subsection{Many-shot Context-Augmented Generation}
\label{sec:loft}

\input{tables/loft}

\textbf{Setup.} We evaluate the scalability of APE on four RAG and ICL tasks from the LOFT benchmark~\citep{lee2024can}, each involving hundreds of additional texts. We employ \textsc{Llama-3.1-8B-Instruct} as our base model to compare APE with sequential encoding, both applied to the same many-shot inputs. The total context lengths for the RAG and ICL tasks are 128K and 32K, respectively. We also include the zero-shot, few-shot ($\leq$ 5), and half-shot sequential encoding baselines. For metrics, F1 score and EM are used in RAG and ICL tasks.

\textbf{Results.} In Table~\ref{tab:loft}, APE achieves performance comparable to sequential encoding when processing the same many-shot long-context inputs, showing its ability to encode hundreds of texts in parallel efficiently. Notably, it outperforms sequential encoding on ArguAna and FEVER for RAG tasks. While APE is expected to reduce performance, it recovers this drop by positioning all texts close to the query, mitigating the ``lost in the middle" problem in long-context LLMs. For ICL tasks, APE can learn from examples as effective as sequential encoding.

\subsection{Efficiency Evaluation}
\label{sec:efficiency}

\input{figures/latency}

\textbf{Setup.} We measure the latency for sequential encoding, MInference~\citep{jiang2024minference}, and APE usingLlama-3.1-8B-Instruct~\citep{llama3} on an H100 GPU with batch sizes of 1 and 4. The query and generation lengths are fixed at 256 tokens, while the context lengths range from 2K to 128K tokens. We employ VLLM~\citep{kwon2023efficient} as our inference engine and measure both prefilling time and total inference time.

\textbf{Results.} Comparing to sequential encoding and MInference,  APE can accelerate inference up to 4.5$\times$ and 2.2$\times$ respectively for long-context scenarios in Figure~\ref{fig:latency}. For 128K-token contexts, APE reduces prefilling time by 28$\times$ compared to MInference. The prefilling cost of APE exhibits linear scaling and consumes less than 10\% of inference time, whereas baselines require over 50\% as context length increases. APE also shows superior versatility, while MInference slows inference with additional overhead for short contexts and large batches.