\section{Conclusion}
\label{Conclusion}
This work explores the potential of parallel encoding in CAG scenarios, which can pre-cache KV states for fast inference and re-use positions for long context but lead to worse performance. To address this, we propose APE, a training-free method to enable accurate, fast, and long CAG systems. APE achieves this by aligning the attention weight distribution of parallel encoding with sequential encoding via three steps: shared prefix, adaptive temperature, and scaling factor. Empirically, we show that APE improves accuracy and efficiency in various RAG and ICL tasks while successfully scaling to process hundreds of chunks in parallel for both settings.

\section{Limitations}

While APE shows the effectiveness and efficiency of parallel encoding with only inference-time modification in the attention distribution, it remains sensitive to hyperparameter selection, particularly the attention temperature $T$ and scaling factor $S$. 
In real-world applications, where contexts vary in length, quantity, and content, aligning the distribution between sequential and parallel encoding automatically presents a significant challenge. 

\section{Acknowledgement}

This work is supported in part by NSF award CNS-2211882 and a gift from Qualcomm. We thank the authors of ChatQA~\citep{liu2024chatqa}, Longbench~\citep{bai2023longbench}, CRAG~\citep{yang2024crag}, LM Evaluation Harness~\citep{eval-harness}, VLLM~\citep{kwon2023efficient}, and MInference~\citep{jiang2024minference} for their useful codebase, benchmark, and models, and Yixin Dong, Hanshi Sun, Zhuoming Chen for their helpful discussions.

