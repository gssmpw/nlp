\section{Observations}
\label{obs}

\input{figures/observation/observation1}

In Section~\ref{obs2}, we evaluate sequential encoding, parallel encoding, and CEPE-Distilled (CEPED)~\citep{yen2024long} using the \textsc{LLaMA-2-7B-chat} model\footnote{We use the \textsc{LLaMA-2} model for CEPED, as it is the only supported model. For other analyses, we employ \textsc{LLaMA-3}.}. Figure~\ref{fig:obseravtion1} presents our findings on various RAG and ICL tasks, highlighting the limitations of trainable approaches in generalizing to complex reasoning tasks. Next, we explore the alignments and misalignments between parallel encoding and sequential encoding in Section~\ref{obs1}, providing insights into why parallel encoding remains effective and identifying opportunities for further improvement.

\subsection{Trainable Approaches are only Effective for Easy Tasks.}
\label{obs2}
In Figure~\ref{fig:obseravtion1}, we compare the performance of different context encoding methods on RAG and ICL tasks, with detailed setups described in Appendix~\ref{app:obs1}. Our analysis of the long-context RAG capability on LongBench~\citep{bai2023longbench} is showcased in Figure~\ref{fig:obseravtion1rag}. Despite accessing more passages, CEPED only surpasses the sequential baseline in two of the three QA tasks, and it even notably underperforms parallel encoding in the summarization task (MultiNews), which requires synthesizing information from the entire context. We hypothesize that CEPED cannot process complex tasks since the encoder and decoder are only trained on the unlabeled pre-training corpus without instruction-tuning on high-quality QA samples. This conclusion is further supported by the results of ICL tasks (see Figure~\ref{fig:obseravtion1icl}), where CEPED performs on par with the 1-shot sequential encoding baseline on TriviaQA but falls short of it on GSM8K and MMLU, despite using much more examples. The latter involves reasoning steps that are hard for the ill-trained model to understand. In conclusion, fine-tuning models to improve parallel encoding requires (i) more diverse and labeled data and (ii) resource-intensive instruction-tuning (e.g., SFT or RLHF~\citep{ouyang2022training}). Given this unfavorable trade-off between training costs and model \mbox{capabilities, we propose developing a training-free method to improve the performance of parallel encoding.}

\subsection{Comparing Parallel Encoding and Sequential Encoding.}
\label{obs1}
In Figure~\ref{fig:obseravtion1}, we observe that parallel encoding still holds promise, as it can generate reasonable responses without further modifications. This finding is non-trivial as contexts are encoded into KV states separately without guarantee that these states can be compared or combined. However, our analysis reveals that the attention mechanism naturally builds alignments between KV states from different positions in independent contexts similar to sequential encoding. To clarify this, Figure~\ref{fig:observation2} focuses on the impact of the attention sink~\citep{xiao2023efficient}, where we visualize the direction of KV states for different samples and positions. In Figure~\ref{fig:norm}, we further visualize the distribution of various components in the $\mathrm{Softmax}$ attention, resulting in several findings.

\input{figures/observation/observation2}

\input{figures/observation/observation3/observation3}

\input{figures/observation/observation4}


\textbf{Key states from different contexts are similar.}  
In Figure~\ref{fig:observation2}a and \ref{fig:observation2}b, we measure the cosine similarity between the key states of different initial tokens for the \textsc{LLaMA-3-8B-Instruct} and \textsc{Mistral-7B-Instruct-v0.3} models, which consistently yields a value close to 1. This observation indicates that the direction of the initial key state remains invariant mainly across different inputs. Figure \ref{fig:observation2}c and \ref{fig:observation2}d further analyze the similarity between the initial key states and their subsequent states, where we observe comparable negative values from different positions. Therefore, the angles between the initial key states and their subsequent states are similar and significantly larger than the angles between different initial key states, as demonstrated in Figure~\ref{fig:illsuration}. It suggests that the direction of key states remains relatively consistent across contexts, as they are primarily decided by the initial key states, which exhibit similar directions across examples. These findings, combined with the small variance in key state magnitudes across examples in Figure~\ref{fig:norm_k}, indicate that key states from different contexts share similar directions and magnitudes, making them comparable. 
 
\input{figures/observation/observation5}

To further understand this, we experiment on HotPotQA using the \textsc{LLaMA-3-8B-Instruct} model. Our analysis involves applying rotations of varying degrees around random axes to the initial key states. For parallel encoding, we explore two rotation modes: one using the same rotation axis for all contexts and another employing a random rotation axis for each context. Figure~\ref{fig:rotation} reveals that sequential encoding keeps performance across various rotation degrees. In contrast, both modes in parallel encoding deteriorate when rotations exceed 150 degrees. This effect arises from the duplication of initial key states, intensifying our rotations' impact. Notably, using separate axes for each context leads to an earlier breakdown beginning at 90 degrees. This mode disrupts the directional similarity of key states with different initial tokens (i.e., $k_{\text{initial}}$) in Figure~\ref{fig:illsuration} and enlarges the angle between key states from different contexts.

\textbf{Values states from different contexts can be combined.} In Equation \eqref{eq:attention}, all value states are combined through a weighted summation, where the $\mathrm{Softmax}$ operator would normalize the weights of all value states to sum to 1. This normalization indicates that the magnitude of current value states is determined solely by those from previous positions, resulting in a similar $L^2$ norm across positions, as shown in Figure~\ref{fig:norm_v}. Additionally, the small variance shows that the magnitudes are comparable among samples. This finding, coupled with a similar direction across samples and positions in Figure~\ref{fig:observation2} (Bottom), indicates the possibility of combining value states.

\textbf{Opportunities for improvement.} Despite the KV states exhibiting similarity across contexts for most positions, the residual misalignments in Figure~\ref{fig:norm} still severely reduce accuracy. We summarize them as follows:
\begin{itemize}
[itemsep=0.0pt,topsep=0pt,leftmargin=*]
\item In Figure \ref{fig:norm}, we observe a notable discrepancy in direction and magnitude for the initial positions, leading to large QK dot products at these positions in Figure \ref{fig:attn_score}. They are identified as an anomaly in the context.
\item Figure~\ref{fig:attn_score} shows the dot products between the query state and all past key states, revealing a notable increase when the states are positioned close to each other, as reflected in the larger similarity observed in Figure~\ref{fig:qk_sim}.
\end{itemize}