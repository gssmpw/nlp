\section{Detailed Experimental Setups for Section~\ref{obs2}}
\label{app:obs1}

\textbf{RAG.} We select four tasks that require processing multiple input documents from the LongBench dataset~\citep{bai2023longbench}, including HotpotQA~\citep{yang2018hotpotqa}, 2WikiMultihopQA~\citep{ho2020constructing}, MuSiQue~\citep{trivedi2022musique}, and MultiNews~\citep{fabbri2019multi}. The F1 score is used as the evaluation metric for the three QA tasks, while Rouge-L is used for the summarization task. Both parallel encoding and CEPED process each document independently using $\Theta_{\text{Enc}}$. For documents that exceed the length limitation of $\Theta_{\text{Enc}}$, we split them into multiple chunks for encoding. In sequential encoding, we will truncate lengthy inputs from the middle.

\textbf{ICL.} We select three few-shot learning tasks from LM Evaluation Harness~\citep{eval-harness} to evaluate the ICL ability of different encoding methods, involving GSM8K~\citep{gsm8k}, TriviaQA~\cite{joshi2017triviaqa}, and MMLU~\citep{mmlu}. In parallel encoding and CEPED, we will encode each example separately and input all the resulting KV states to $\Theta_{\text{Dec}}$. For sequential encoding, we use variants with different numbers of shots to further measure the effectiveness of other methods, including 0-shot, 1-shot, half-shot, and full-shot.

\section{More Visualization Results for Section 3.2}
\label{app:obs2}

\subsection{Similarity between Tokens from Different Samples in Each Position for Key States.}

In Figure~\ref{fig:app:sim:k}, we showcase that key states in different layers maintain consistently high cosine similarity values for various initial tokens, with only the first layer exhibiting slightly lower similarities. Our analysis reveals that \textsc{LLaMA-3-8B-Instruct} and \textsc{LLaMA-3.1-8B-Instruct} exhibit almost the same direction (approximately 1.0) for different tokens beyond the first layer, while \textsc{Mistral-7B-Instruct-v0.3} and \textsc{Gemma-2-9b-it} show substantial but lower similarities ranging from 0.8 to 0.9. These findings indicate inherent alignments across contexts while highlighting the potential for further improvements through the shared prefix in Section~\ref{sec:shared_prefix}.

\input{figures/appendix/similarity/k_sim}

\subsection{Similarity between Tokens from Different Samples in Each Position for Value States.}

Similarly, Figure~\ref{fig:app:sim:v} shows that value states maintain high cosine similarity across different layers for various initial tokens. There are two notable exceptions: the first layer and the \textsc{Gemma-2-9b-it} model. This distinctive pattern in \textsc{Gemma-2-9b-it} aligns with the model's requirement for a system prompt to function correctly.

\input{figures/appendix/similarity/v_sim}

\subsection{Similarity between the Initial Token and Following Tokens for Key States.}

Figure~\ref{fig:app:sim:kk} illustrates how the cosine similarity between the initial and subsequent key states stabilizes as position increases. This similarity converges to a near-constant value for all base models after 10 tokens. 

\input{figures/appendix/similarity/kk_sim}

\subsection{Similarity between the Initial Token and Following Tokens for Value States.}

Similar to key states, the value states exhibit a stable similarity between the initial token and subsequent tokens in Figure~\ref{fig:app:sim:vv}, with all models convergent to a nearly constant value after approximately 10 tokens.

\input{figures/appendix/similarity/vv_sim}

\subsection{Similarity between the Query State and Past Key States.}

In Figure~\ref{fig:app:sim:qk}, the query states across all layers, and base models exhibit higher cosine similarity with the initial tokens. Additionally, neighboring positions tend to receive higher cosine similarity.

\input{figures/appendix/similarity/qk_sim}

\subsection{Magnitude of Key States from Different Positions.}

Figure~\ref{fig:app:norm:k} illustrates that the magnitude of key states gradually increases with position, except for the first few tokens, which exhibit significantly smaller magnitudes.

\input{figures/appendix/similarity/k_norm}

\subsection{Magnitude of Value States from Different Positions.}

\input{figures/appendix/similarity/v_norm}

In Figure~\ref{fig:app:norm:v}, the value states across all positions exhibit a similar magnitude, except for the first few positions, which show a noticeable deviation. We indicate this region with a red dashed line.

\subsection{Dot Product between the Query State and Past Key States.}

\input{figures/appendix/similarity/qk_score}

In Figure~\ref{fig:app:sim:qk_score}, the query states across all layers, and base models exhibit larger dot product values with the initial tokens. Additionally, neighboring positions also tend to receive larger values.

\section{Formal Derivation of APE}
\label{app:algorithm}

\subsection{Hierarchical Formula for Softmax Attention.}

Here, we begin with the standard $\mathrm{Softmax}$ attention, where $Q$, $K$, and $V$ are the query, key, and value states from the input, respectively. To distinguish different sources, we use the subscript ${C_i}$ for elements originating from the context, while those without a subscript correspond to user queries or generated texts.
\begin{align}
O &= \mathrm{Softmax}\left(\frac{Q[K_{C_1}^\top, \ldots, K_{C_N}^\top, K^\top]}{\sqrt{d}}\right) \times [V_{C_1}, \ldots, V_{C_N}, V] \\
&= \frac{ [A_{C_1}, \ldots, A_{C_N}, A]}{\sum_{i=1}^{N}\sum_{j=1}^{l_{C_i}}a_{C_{i}, j}+ \sum_{j=1}^{l}a_{j}}\times [V_{C_1}, \ldots, V_{C_N}, V], \\
&\text{where }K_{C_i} = [k_{C_i, 1}, ..., k_{C_i, l_{C_i}}], V_{C_i} = [v_{C_i, 1}, ..., v_{C_i, l_{C_i}}], A_{C_i} = [\exp \frac{Qk_{C_i, 1}^\top}{\sqrt{d}}, \ldots, \exp \frac{Qk_{C_i, l_{C_i}}^\top}{\sqrt{d}}],\notag\\
&A = [\exp \frac{Qk_{1}^\top}{\sqrt{d}}, \ldots, \exp \frac{Qk_{l}^\top}{\sqrt{d}}], a_{C_{i}, j} = \exp\frac{Qk_{C_i, j}^\top}{\sqrt{d}},\text{ and } a_{j} = \exp\frac{Qk_{j}^\top}{\sqrt{d}}.\notag
\end{align}
We can restructure the computation hierarchically, first computing $V^h_{C_i}$ and $A^h_{C_i}$ for each context $C_i$:
\begin{align} 
V^h_{C_i} &= \mathrm{Softmax} \left( \frac{Q [k_{C_i, 1}^\top, \ldots, k_{C_i, l_{C_i}}^\top]}{\sqrt{d}} \right) \times [V_{C_i, 1}, \ldots, V_{C_i, l_{C_i}}], A^h_{C_i} = \mathrm{LogSumExp} \left( \frac{Q [k_{C_i, 1}^\top, \ldots, k_{C_i, l_{C_i}}^\top]}{\sqrt{d}} \right) 
\end{align}
Similarly, for the non-context tokens, we compute:
\begin{align} 
V^h &= \mathrm{Softmax} \left( \frac{Q [k_1^\top, \ldots, k_l^\top]}{\sqrt{d}} \right) \times [V_1, \ldots, V_l], A^h = \mathrm{LogSumExp} \left( \frac{Q [k_1^\top, \ldots, k_l^\top]}{\sqrt{d}} \right) 
\end{align}
After we get all these values, we can combine them while renormalizing with $A^h$:
\begin{align} 
O &= \mathrm{Softmax} \left( A^h_{C_1}, ..., A^h_{C_N}, A^h \right) \times [V^h_{C_1}, ..., V^h_{C_N}, V^h]
\end{align}
\subsection{Hierarchical Formula for APE.}

After incorporating all components in APE, we have a new $V^{h'}_{C_i}$ and $A^{h'}_{C_i}$ for each context $C_i$:
\begin{align} 
V^{h'}_{C_i} &= \mathrm{Softmax} \left( \frac{Q [k_{C_i, 1}^\top, \ldots, k_{C_i, l_{C_i}}^\top]}{T \cdot \sqrt{d}} \right) \times [V_{C_i, 1}, \ldots, V_{C_i, l_{C_i}}], A^{h'}_{C_i} = S \cdot \mathrm{LogSumExp} \left( \frac{Q [k_{C_i, 1}^\top, \ldots, k_{C_i, l_{C_i}}^\top]}{T \cdot \sqrt{d}} \right) 
\end{align}
For the non-context tokens, including our shared prefix, the formulas of $V^{h'}$ and $A^{h'}$ remain unchanged. Here, we introduce separate terms $V^{h'}_P$ and $A^{h'}_P$ for the shared prefix. Combining them, we have:
\begin{align} 
O &= \mathrm{Softmax} \left(A^{h'}_{P}, A^{h'}_{C_1}, ..., A^{h'}_{C_N}, A^{h'} \right) \times [V^{h'}_P, V^{h'}_{C_1}, ..., V^{h'}_{C_N}, V^{h'}]
\end{align}
\subsection{Relation with Equation~\ref{algo:ape}.}

Finally, we show that it can be rewritten as Equation~\ref{algo:ape},  with the only difference being that all contexts are treated as a single context. For an token from the position $j$ in context ${C_i}$, the final attention score $a''_{C_i, j}$ is
\begin{align} 
a''_{C_i, j} &= \frac{\exp(Qk_{C_i, j}^\top/T\sqrt{d})}{\sum_{n=1}^{N}\sum_{t=1}^{l_{C_n}}\exp(Qk_{C_i, t}^\top/T\sqrt{d})}\notag \\
&\cdot \frac{\exp\left(S \cdot \mathrm{LogSumExp} \left( \frac{Qt[k_{C_1, 1}^\top, \ldots, Qt[k_{C_1, l_{C_1}}^\top, \ldots, k_{C_n, 1}^\top, \ldots, k_{C_n, l_{C_n}}^\top]}{T \cdot \sqrt{d}} \right)\right)}{\exp\left(S \cdot \mathrm{LogSumExp} \left( \frac{Qt[k_{C_1, 1}^\top, \ldots, Qt[k_{C_1, l_{C_1}}^\top, \ldots, k_{C_n, 1}^\top, \ldots, k_{C_n, l_{C_n}}^\top]}{T \cdot \sqrt{d}} \right)\right) + \exp\left(\mathrm{LogSumExp} \left( \frac{Q [k_{1}^\top, \ldots, k_{l}^\top]}{T \cdot \sqrt{d}} \right)\right)} \\
&= \frac{\exp(Qk_{C_i, j}^\top/T\sqrt{d})}{\sum_{n=1}^{N}\sum_{t=1}^{l_{C_n}}\exp(Qk_{C_n, t}^\top/T\sqrt{d})} \cdot \frac{(\sum_{n=1}^{N}\sum_{t=1}^{l_{C_n}}\exp(Qk_{C_n, t}^\top/T\sqrt{d}))^S}{\sum_{n=1}^N (\sum_{t=1}^{l_{C_n}}\exp(Qk_{C_n, t}^\top/T\sqrt{d}))^S + \sum_{t=1}^{l}\exp(Qk_{t}^\top/\sqrt{d})} \\
&= \frac{\exp(Qk_{C_i, j}^\top/T\sqrt{d}) \cdot (\sum_{t=1}^{l_{C_i}}\exp(Qk_{C_i, t}^\top/T\sqrt{d}))^{(S-1)}}{(\sum_{n=1}^N \sum_{t=1}^{l_{C_n}}\exp(Qk_{C_n, t}^\top/T\sqrt{d}))^S + \sum_{t=1}^{l}\exp(Qk_{t}^\top/\sqrt{d})} = \frac{a'_{C_i, j}}{(\sum_{n=1}^{N}\sum_{t=1}^{l_{C_n}}a'_{C_i, t})^{S}+ \sum_{t=1}^{l}a_{t}}
\end{align}

This formula is equivalent to Equation~\ref{algo:ape}, except it combines the prefix and other non-context tokens for simplicity. Similarly, for the non-context tokens from position $j$, we can derive $a''_{j}$ as

\begin{align} 
a''_{j} = \frac{\exp(Qk_{j}^\top/\sqrt{d})}{\sum_{n=1}^N (\sum_{t=1}^{l_{C_n}}\exp(Qk_{C_n, t}^\top/T\sqrt{d}))^S + \sum_{t=1}^{l}\exp(Qk_{t}^\top/\sqrt{d})} = \frac{a_{j}}{(\sum_{n=1}^{N}\sum_{t=1}^{l_{C_n}}a'_{C_i, t})^{S}+ \sum_{t=1}^{l}a_{t}}
\end{align}

Combining these two components, we obtain the final formula presented in Equation~\ref{algo:ape}.

\subsection{Efficient Implementation.}

To combine the computation for context and non-context tokens, we employ flash attention twice—once for each part—and then merge the results. This only introduces a marginal computational overhead, as shown below.

\begin{lstlisting}[language=Python, numbers=none, basewidth={0.5em,0.5em}]
def ape_attention(query, key, value, temperature, scale):
    # split key and value states into context and non-context parts
    key_context, key_other = key
    value_context, value_other = value
    attn_output_context, lse_context = flash_attn(query, key, value, temperature = temperature)
    attn_output_other, lse_other = flash_attn(query, key, value)
    lse_context = lse_context*(scale)
    attn_weights = [lse_context, lse_other]
    attn_weights = Softmax(attn_weights)
    value_states = [attn_output_context, attn_output_other]
    attn_output = attn_weights @ value_states
\end{lstlisting}


\subsection{Future Directions.}

\input{figures/appendix/structure/cache_structure}


The hierarchical formulation of APE can naturally extend to more complex tree structures, as illustrated in Figure~\ref{fig:app:structure}. This flexibility allows each user query to be enriched with external knowledge organized in such structures, demonstrating APE's capability to handle structured external data effectively.  

\section{Comparing APE with Long-context LLMs.}
\label{sec:app:lclm}

\begin{table}[ht]
\centering
\caption{\textbf{Performance comparison between APE and long-context LLMs on LongBench~\citep{bai2023longbench}.}}
\resizebox{\linewidth}{!}{\setlength{\tabcolsep}{1mm}{
\begin{tabular}{l|cccccc}
\toprule
Method & NarratQA & Qasper & MultiFQA & GovReport & QMSum & LCC \\ \midrule
\textsc{LLaMA-3-8B-Instruct} & 19.32 & 32.83 & 43.38 & 27.89 & 22.40 & 53.22 \\
LLMLingua2 & 21.00 & 25.78 & 48.92 & 27.09 & 22.34 & 16.41 \\
StreamingLLM & 16.99 & 28.94 & 11.99 & 25.65 & 19.91 & 40.02 \\
Long-context FT & 14.88 & 21.70 & 47.79 & \textbf{32.65} & \textbf{24.76} & 55.12 \\
Self-Extend & 24.82 & 37.94 & 50.99 & 30.48 & 23.36 & 58.01 \\
\rowcolor{cyan!10}+APE & \textbf{26.87} & \textbf{39.14} & \textbf{59.12} & 29.10 & 23.08 & \textbf{66.09} \\
\midrule\midrule
Method & RepoBench-P & HotpotQA & 2WikiMQA & MuSiQue & MultiNews & Average \\ \midrule
\textsc{LLaMA-3-8B-Instruct} & 38.15 & 44.24 & 21.01 & 20.47 & 23.63 & 31.50 \\
LLMLingua2 & 20.56 & 40.16 & 24.72 & 20.85 & 21.34 & 26.29 \\
StreamingLLM & 26.16 & 32.76 & 20.12 & 17.32 & 21.49 & 23.76 \\
Long-context FT & 43.05 & 15.89 & 10.49 & 8.74 & 24.28 & 27.21 \\
Self-Extend & 41.83 & \textbf{51.09} & 24.17 & \textbf{28.73} & \textbf{24.11} & 35.96 \\
\rowcolor{cyan!10}+APE & \textbf{49.43} &50.11 & \textbf{28.06} & 25.79 & 22.40 & \textbf{38.11} \\
\bottomrule
\end{tabular}}}
\label{tab:app:single}
\end{table}

In Table~\ref{tab:app:single}, we further compare APE with Long-context LLM, including: (i) \textit{Prompt} \textit{Compression}: Truncation, LLMLingua2~\citep{pan2024llmlingua}, (ii) \textit{KV Cache Eviction}: StreamingLLM~\citep{xiao2023efficient}, (iii) \textit{Long-context FT}: Llama-3-8B-Instruct-262K~\citep{gradllama}, Llama-2-7B-Instruct-32K~\citep{togetherllama}, (iv) \textit{length extrapolation}: Self-Extend~\citep{jin2024llm}. Experimental results show that APE consistently outperforms all existing long-context LLM methods. We hypothesize that this improvement stems from APE enabling queries to access all past contexts, enhancing retrieval ability. However, since APE has limitations in identifying relationships between contexts, we do not emphasize its performance on current long-context tasks.

\section{APE Cache versus Prefix Cache}

Finally, we compare the APE cache with the prefix cache to highlight our advantages in serving multiple queries within the CAG setting. Figure~\ref{fig:app:ape_cache} illustrates an example with four contexts where both caching strategies are allocated the same budget. Each query retrieves three contexts. Under these conditions, the prefix cache can only match a limited number of combinations, achieving an average hit rate of 41.7\%, whereas the APE cache ensures a 100\% hit rate. This gap will become even more pronounced as the number of contexts increases.

\input{figures/appendix/structure/APE_cache}