\section{Adaptive Parallel Encoding}
With all the lessons learned in Section~\ref{obs}, we will design our \textbf{APE} to address the residual misalignments. APE enables a seamless shift to parallel encoding without requiring training while maintaining most of the model's capabilities. Our approach adaptively aligns the distribution of attention weights between sequential and parallel encoding via three steps as illustrated in Figure~\ref{fig:intro}, thereby boosting efficiency and performance. 

\subsection{Prepending Shared Prefix.}
\label{sec:shared_prefix}
Figure~\ref{fig:norm} shows that the distribution of the first few tokens differs significantly from that of subsequent tokens. This discrepancy poses a challenge when encoding contexts in parallel due to duplicating these abnormal KV states. To address this issue, we propose a simple yet effective solution: prepending a shared prefix to all contexts. This approach ensures that these KV states appear only once in each generation step. In practice, the choice of prefix varies with the model and task. We use existing system prompts and instructions as the shared prefix when available. Otherwise, we will insert a few newline characters (i.e., ``\textbackslash n") before all contexts.

Although the later positions are not identified as abnormal, we still observe instability at the start of LLM inputs. To mitigate this issue, we may also consider extending the existing prefix with more newline characters.

\input{figures/methodology/alignment}

\subsection{Adjusting Attention Temperature.}

In Figure \ref{fig:attn_score}, the value of QK dot products increases as the relative distance decreases, with a notably sharper rise when the distance approaches zero. To show its impact on parallel encoding, we set a 50-token prefix and query, encoding the remaining 900 tokens either sequentially or in five parallel chunks, with attention distributions shown in Figure~\ref{fig:alignment}. 
Comparing Figure~\ref{fig:attn_score_p} with \ref{fig:attn_score_s}, duplicating neighboring KV states in parallel encoding will disperse the query's attention to multiple contexts, resulting in a more uniform attention distribution. We adjust the attention temperature $T$ to a value less than 1 to refocus on the most relevant tokens, sharpening the distribution after the $\mathrm{Softmax}$ operation. The comparison between different $T$ is shown in Figure \ref{fig:attn_score_p2} and \ref{fig:attn_score_p3}.

\subsection{Adding Scaling Factor.}

\input{figures/methodology/alignment2}

While adjusting the temperature sharpens the attention distribution among context tokens, it will also alter the overall attention allocated to the whole context, as indicated by the LogSumExp value in Figure~\ref{fig:alignment2}. Specifically, when the sum of the original QK dot product values in a given layer is significantly greater than 0, reducing temperature amplifies these positive values, resulting in an increased, positive LogSumExp value. Conversely, when the sum is closer to 0, lowering temperature has a stronger effect on the negative QK dot products, leading to a decreased, negative LogSumExp value. These effects generally increase the absolute value of LogSumExp(QK). To compensate for these changes, we introduce a scaling factor $S < 1$ to reduce this absolute value.

%In this final step, our aim is to further control the overall attention paid to the context.  

%In Figure \ref{fig:qk_sim}, the cosine similarity between query and key states increases as their distance decreases, with a notably sharper rise when the distance approaches zero. Duplicating these neighboring KV states will increase the overall attention paid to the context. To offset this, we introduce a scaling factor $S$ smaller than one for the overall context. This factor is applied after the $\exp$ operation in $\mathrm{Softmax}$ attention, allowing for a proportionally greater reduction for larger product values between query and key states.




%In the final step, we adjust the attention temperature $T$ to a value less than 1.

%This emphasizes semantically important tokens whose attention weights are above average but not as high as those closest to the query token. A carefully chosen temperature can recover the attention on these tokens while still maintaining a reduced scaling for the query's immediate neighbors. To prevent an overall increase in attention weights across the entire context, we also apply the temperature $T$ as an exponent to the sum of attention weights before normalization. This can be expressed as $(\sum \exp(qk/T))^T$, where $q$ and $k$ represents query and key states, respectively. When there is only one element in the sum, this expression simplifies to $\exp(qk)$, which is equivalent to the standard attention mechanism.

\subsection{Formulation.}

Given these three steps, we can formulate the modified attention in APE. We begin with the standard $\mathrm{Softmax}$ attention, where $Q$, $K$, and $V$ are the query, key, and value states, respectively. We use the subscript ${C_i}$ for elements from the context ${C_i}$, while those without a subscript correspond to user queries or generated texts.

\begin{align}
O &= \mathrm{Softmax}\left(\frac{Q[K_{C_1}^\top, \ldots, K_{C_N}^\top, K^\top]}{\sqrt{d}}\right) \times [V_{C_1}, \ldots, V_{C_N}, V] \\
&= \frac{ [A_{C_1}, \ldots, A_{C_N}, A]}{\sum_{i=1}^{N}\sum_{j=1}^{l_{C_i}}a_{C_{i}, j}+ \sum_{j=1}^{l}a_{j}}\times [V_{C_1}, \ldots, V_{C_N}, V], \\
\text{where }A_{C_i} &= [\exp \frac{Qk_{C_i, 1}^\top}{\sqrt{d}}, \ldots, \exp \frac{Qk_{C_i, l_{C_i}}^\top}{\sqrt{d}}]\text{ and }a_{C_{i}, j} = \exp\frac{Qk_{C_i, j}^\top}{\sqrt{d}}.\text{ Similar for }A\text{ and }a_{j}.\notag
\end{align}

After incorporating the proposed changes, the formula for our refined attention calculation becomes:

\begin{align}
\label{algo:ape}
    O' &= \frac{[A_{P}, A'_{C_1}, \ldots, A'_{C_N}, A]}{\sum_{j=1}^{l_P}a_{P, j} + (\sum_{i=1}^{N}\sum_{j=1}^{l_{C_i}}a'_{C_i, j})^{S}+ \sum_{j=1}^{l}a_{j}}\times [V_P, V_{C_1}, \ldots, V_{C_{N}}, V], \\
    &\text{where }A'_{C_i} = [ \exp \frac{Qk_{C_i, 1}^\top}{T\sqrt{d}}, \ldots,  \exp \frac{Qk_{C_i, l_{C_i}}^\top}{T\sqrt{d}}] \cdot (\sum_{i=1}^{N}\sum_{j=1}^{l_{C_i}}a'_{C_i, j})^{S-1}\text{ and }a'_{C_{i}, j} = \exp\frac{Qk_{C_i, j}^\top}{T\sqrt{d}}.\notag
\end{align}

$A_P$ represents the attention weights for the shared prefix while $A$ denotes that for query and generated tokens. The attention temperature $T$ and the scaling factor $S$ for the context are less than 1. Appendix~\ref{app:algorithm} provides a detailed deduction of this formula for better understanding. All these modifications are compatible with fast attention implementations such as flash attention~\citep{dao2022flashattention} by computing the context and non-context KV states separately and merging them into the attention output. This process only incur a negligible overhead.

For the choice of hyperparameters, we conduct a greedy search over a small validation set. If no prefix is provided, we begin by adding two ``$\backslash$n" and increase the prefix length by 10, 20, and 40. $S$ and $T$ are searched in the ranges [0.1, 1.0] using 0.1 step sizes. We use $S\cdot T$ instead of $S$ as the scaling factor to simplify our search.