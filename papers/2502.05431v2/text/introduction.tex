\section{Introduction}
Recent advances in context-augmented generation (CAG) techniques, particularly retrieval-augmented generation (RAG)~\citep{gupta2024rag, gao2023retrieval} and in-context learning (ICL)~\citep{dong2022survey, wei2022emergent}, have been widely adopted in large language models (LLMs)~\citep{llama3, achiam2023gpt}, improving their ability to generalize to unseen tasks with contextual information, as demonstrated in Figure~\ref{fig:intro} (top).
%These data includes the latest knowledge~\citep{jiang2023active}, illustrative examples~\citep{liu2021makes}, and user-specific information~\citep{shanahan2023role}. 
These techniques employ a \textit{sequential encoding} process to ground LLM inputs with knowledge from external sources: concatenating the retrieved texts into one sequence, and encoding the sequence into key-value (KV) states as the context for subsequent queries. While this new, significantly longer input improves performance, the increased latency in context prefilling becomes a bottleneck in tasks that require long inputs but generate short outputs~\citep{bai2023longbench, agarwal2024many, jiang2024longrag}. For example, prefilling a 128K context takes 17 seconds, whereas generating 256 tokens requires only 6 seconds. This discrepancy leaves significant room to improve the practical efficiency of CAG systems in real-world deployments~\citep{Liu_LlamaIndex_2022, Chase_Longchain_2022}.

Since texts for CAG are typically stored independently in external databases~\citep{Qdrant, douze2024faiss}, pre-caching all these texts for direct loading during inference offers a brute-force approach to accelerate CAG. However, for autoregressive LLMs, the KV states are inherently context-dependent. This dependency makes naive pre-caching impractical, as it would require caching all possible context permutations, leading to factorial growth in memory requirements as the database size increases. For instance, caching all permutations of just ten 256-token text chunks for the \textsc{LLaMA-3-8B} model would demand an impractical 22 PB of memory.

To address this issue, \textit{parallel encoding}~\citep{ratner2022parallel, yen2024long, li2024focusllm, Sun2024BlockAttentionFE} is introduced to encode each context into KV states separately, ensuring that tokens from different contexts cannot attend to each other during encoding. Next, the on-the-fly generation starts by prefilling user queries, which can attend to the cached KV states from all contexts without re-encoding, offering two benefits:

\textbf{Pre-caching Contexts for Fast Inference:} Texts from external sources can be pre-computed and cached into KV states, which serve as contexts for direct loading during inference. Additionally, this approach allows for cost-free manipulation of contexts, including operations like insertion, deletion, replacement, and swapping.

\textbf{Re-using Positions for Long Context:} Contexts can be inserted into the same range of positions in an LLM's context window, allowing for more and longer context chunks. It also mitigates the problem of ``lost in the middle" in context ordering~\citep{liu2024lost}, as each context is equally ``close'' to the generated tokens. 

\input{figures/introduction/overview}

Despite these advantages, parallel encoding leads to significant performance degradation across multiple RAG and ICL scenarios, as shown in Figure \ref{fig:obseravtion1}, with average declines of 4.9\% (despite using 2-10$\times$ more contexts) and 49.0\%, respectively. While prior works~\citep{Sun2024BlockAttentionFE, yen2024long} have attempted to correct this with fine-tuning, these methods continue to exhibit reduced accuracy in reasoning tasks (e.g., GSM8K). This decrease arises from the limited generalization capability of models fine-tuned on simple tasks to complex ones.

However, our results in Figure~\ref{fig:obseravtion1} also reveal that parallel encoding holds promise, as LLMs can still generate reasonable responses due to their inherent alignments with sequential encoding. Based on this observation, we aim to strengthen these alignments while addressing the remaining discrepancies to achieve more accurate parallel encoding. Our insight from Figure~\ref{fig:observation2} and Figure~\ref{fig:norm} is that \textit{KV states from independent contexts can be naturally merged into one sequence due to their similarity in direction and magnitude, attributed to the presence of an attention sink}~\citep{xiao2023efficient}. This observation reduces our challenge to addressing residual misalignments, which manifest as anomalous distributions at the initial and recent positions within each context.

Motivated by this, we propose \underline{A}daptive \underline{P}arallel \underline{E}ncoding (\textbf{APE}) to align the distribution between sequential and parallel encoding, which enables accurate and fast CAG (see Figure~\ref{fig:intro} (Bottom)). Our contributions involve:

\begin{itemize}
[itemsep=0.0pt,topsep=0pt,leftmargin=*]
\item We systematically analyze the distribution properties of attention weights in parallel encoding, focusing on the magnitude and direction of KV states across various samples and positions. Our observations identify major alignments and minor misalignments between parallel and sequential encoding for further improvement.
\item We propose APE to recover the accuracy of parallel encoding with three alignment steps: (i) Prepend a shared prefix to avoid the duplication of abnormal distribution of initial tokens. (ii) Adjust a lower attention temperature to sharpen the distribution, focusing on contextually important tokens. (iii) Apply a scaling factor to offset the increase in the magnitude of the LogSumExp value of attention scores from the context.
\item  We empirically show that (i) APE maintains 98\% and 93\% of the sequential encoding performance in RAG and ICL tasks, respectively. (ii) APE outperforms parallel encoding in RAG and ICL, yielding improvements of 3.6\% and 7.9\%, respectively. (iii) APE scales to handle hundreds of contexts in parallel, matching or exceeding sequential encoding in many-shot scenarios. (iv) APE accelerates long-context generation, achieving up to 4.5$\times$ speedup through a 28$\times$ reduction in prefilling time for a context including 128K tokens.
\end{itemize} 