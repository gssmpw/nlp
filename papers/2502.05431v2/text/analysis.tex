\section{Analysis} 

This section presents analyses to answer the following research questions: \textbf{RQ1}: Can APE improve performance for real-world RAG applications? \textbf{RQ2}: How does each component in APE contribute to the performance? \textbf{RQ3}: Can APE extend LLM context window size in long-context scenarios without RAG?

\subsection{Can APE improve performance for real-world RAG applications?}

In Table~\ref{tab:crag}, we evaluate APE in real-world RAG scenarios using the CRAG benchmark \citep{yang2024crag}. Task 1 augments the model with five webpages, while Task 2 provides an additional knowledge graph as another retrieval source. In our experiments, the sequential encoding baseline is limited to retrieving 4K tokens, whereas APE can process 20 parallel segments of 4K tokens each. By incorporating significantly more external texts, APE consistently outperforms sequential encoding with limited context sizes while reducing latency. Moreover, the improvement in Task 2 shows the effectiveness of APE in merging text from multiple sources.

\input{tables/rag}

\subsection{How does each component in APE contribute to the performance?}

\input{tables/ablation}

In Table~\ref{fig:ablation}, we conduct an ablation study to examine each component in APE, including the shared prefix ($P$), attention temperature ($T$), and scaling factor ($S$). We present results averaged across the four base models evaluated in Figure~\ref{fig:icl}. Our findings indicate that incorporating each of these components can improve performance for all tasks, with average improvements of 5.19\%, 0.59\%, and 2.07\%, respectively. Among them, adding a shared prefix leads to the largest improvement, while adjusting the attention temperature yields minimal accuracy gains without the complementary effect of the scaling factor.

\subsection{Can APE extend context lengths in long-context scenarios without RAG?}

Table~\ref{tab:single} evaluates the effectiveness of APE in handling a single long-context input using the \textsc{Llama-3-8B-Instruct} model on the LongBench dataset~\citep{bai2023longbench}. To accommodate the long context within our APE, we split it into multiple segments of less than 7,500 tokens. Additionally, we append the last 500 tokens to the query for two code completion tasks. Our results indicate that APE enhances performance across 10/11 tasks, yielding an average improvement of 6.6\% compared to the sequential encoding baseline with limited context window size. More baseline results of long-context LLM approaches are provided in Appendix~\ref{sec:app:lclm}.

\begin{table}[h]
\centering
\caption{\textbf{Performance comparison across different long-context tasks on LongBench~\citep{bai2023longbench}.}}
\vspace{-0.7em}
\resizebox{\linewidth}{!}{\setlength{\tabcolsep}{1mm}{
\begin{tabular}{l|cccccc}
\toprule
Method & NarratQA & Qasper & MultiFQA & GovReport & QMSum & LCC \\ \midrule
\textsc{LLaMA-3-8B-Instruct} & 19.32 & 32.83 & 43.38 & 27.89 & 22.40 & 53.22 \\
%LLMLingua2 & 21.00 & 25.78 & 48.92 & 27.09 & 22.34 & 16.41 \\
%StreamingLLM & 16.99 & 28.94 & 11.99 & 25.65 & 19.91 & 40.02 \\
%Long-context FT & 14.88 & 21.70 & 47.79 & 32.65 & 24.76 & 55.12 \\
%Self-Extend & 24.82 & 37.94 & 50.99 & 30.48 & 23.36 & 58.01 \\
\rowcolor{cyan!10}+APE & \textbf{26.87} & \textbf{39.14} & \textbf{59.12} & \textbf{29.10} & \textbf{23.08} & \textbf{66.09} \\
\midrule\midrule
Method & RepoBench-P & HotpotQA & 2WikiMQA & MuSiQue & MultiNews & Average \\ \midrule
\textsc{LLaMA-3-8B-Instruct} & 38.15 & 44.24 & 21.01 & 20.47 & \textbf{23.63} & 31.50 \\
%LLMLingua2 & 20.56 & 40.16 & 24.72 & 20.85 & 21.34 & 26.29 \\
%StreamingLLM & 26.16 & 32.76 & 20.12 & 17.32 & 21.49 & 23.76 \\
%Long-context FT & 43.05 & 15.89 & 10.49 & 8.74 & 24.28 & 27.21 \\
%Self-Extend & 41.83 & 51.09 & 24.17 & 28.73 & 24.11 & 35.96 \\
\rowcolor{cyan!10}+APE & \textbf{49.43} & \textbf{50.11} & \textbf{28.06} & \textbf{25.79} & 22.40 & \textbf{38.11} \\
\bottomrule
\end{tabular}}}
\label{tab:single}
\end{table}

