\begin{figure}
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/methodology/attn_score_s.pdf}
        \caption{Sequential}
        \label{fig:attn_score_s}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/methodology/attn_score_p.pdf}
        \caption{Parallel  (T = 1.0)}
        \label{fig:attn_score_p}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/methodology/attn_score_p2.pdf}
        \caption{Parallel (T = 0.2)}
        \label{fig:attn_score_p2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/methodology/attn_score_comp.pdf}
        \caption{Parallel vs. Sequential}
        \label{fig:attn_score_p3}
    \end{subfigure}
    \caption{\textbf{Comparison of Attention Weight Distribution within Contexts.} \textbf{(a)} 
Sequential encoding allocates high attention scores to neighboring tokens. \textbf{(b)} Parallel encoding distributes attention scores more uniform across neighboring tokens from all contexts. \textbf{(c)} Adjusting the temperature $T$ sparsifies the distribution. \textbf{(d)} After adjustment, the distribution in parallel encoding becomes similar to sequential encoding. The X-axis represents token positions.} %See Appendix~\ref{} for other architectures.}
    \label{fig:alignment}
\end{figure}