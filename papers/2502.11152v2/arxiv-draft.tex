\documentclass[11pt]{article}
%% ===========================================================================

% Packages
\usepackage{adjustbox}
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{latexsym}
\usepackage{mathrsfs}
\usepackage{color}
% \usepackage{refcheck}
% \usepackage{comment}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage{dsfont}
\usepackage[overload]{empheq}
\usepackage{enumitem}

\usepackage{hyperref}       % hyperlinks
\usepackage{cleveref} 
%\hypersetup{
%	colorlinks=true,
%	linkcolor=blue,
%	filecolor=magenta,
%	urlcolor=cyan,
%}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
% \usepackage{romannum}
% Page Formatting
\usepackage[a4paper]{geometry}
\geometry{hmargin=1in,vmargin=1in}
\renewcommand\baselinestretch{1.2}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[numbers]{natbib}
\usepackage{makecell}

%%% 
% \newcommand*{\medcap}{\mathbin{\scalebox{0.75}{\ensuremath{\bigcap}}}}
% \newcommand*{\medcup}{\mathbin{\scalebox{0.75}{\ensuremath{\bigcup}}}}
%%%
\newcommand{\beqa}{\begin{eqnarray}}
\newcommand{\eeqa}{\end{eqnarray}}
\newcommand{\beqas}{\begin{eqnarray*}}
\newcommand{\eeqas}{\end{eqnarray*}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\nn}{\nonumber}
\newcommand{\RN}[1]{%
  \textup{\uppercase\expandafter{\romannumeral#1}}%
}
% Pre-defined macros and hotkeys

% caligraphic letters
\newcommand{\mcK}{{\mathcal K}}
\newcommand{\mcX}{{\mathcal X}}
\newcommand{\mcL}{{\mathcal L}}
\newcommand{\mcA}{{\mathcal A}}
\newcommand{\mcE}{{\mathcal E}}
\newcommand{\mcN}{{\mathcal N}}
\newcommand{\mcJ}{{\mathcal J}}
\newcommand{\mcT}{{\mathcal T}}
\newcommand{\mcI}{{\mathcal I}}
\newcommand{\mcS}{{\mathcal S}}

% math operators
\newcommand{\Rank}{\mathop{Rank}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\ve}{\mathrm{vec}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\dist}{\mathrm{dist}}
\DeclareMathOperator{\argmin}{\arg\min}
\DeclareMathOperator{\argmax}{\arg\max}
\newcommand{\st}{\mbox{s.t.}}

% operators with arguments
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\E}[1]{{\mathbb E}\left[ #1 \right]}
\newcommand{\Inner}[2]{\left\langle #1, #2 \right\rangle}

% italic shorts
\newcommand{\cf}{{\it cf.}}
\newcommand{\eg}{{\it e.g.}}
\newcommand{\ie}{{\it i.e.}}
\newcommand{\etc}{{\it etc.}}
\newcommand{\res}{{\it res.}}


% environment settings
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{coro}{Corollary}
\newtheorem{defi}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{obs}{Observation}
\newtheorem{claim}{Claim}
\newtheorem{assumption}{Assumption}
\newtheorem{fact}{Fact}
\newtheorem{remark}{Remark}
\newtheorem{exam}{Example}
\newtheorem{algo}{Algorithm}
\newtheorem{model}{Model}[section]

\def \openbox{$\sqcup\llap{$\sqcap$}$}

%% define counter spb and rspb
\newcounter{spb}
\setcounter{spb}{1}

\newcommand{\subpb}{(\alph{spb}) \addtocounter{spb}{1}}
\newcommand{\subrpb}{(\roman{spb}) \addtocounter{spb}{1}}
% \newcommand{\subpb}{\arabic{spb}. \addtocounter{spb}{1}}
\newcommand{\resetspb}{\setcounter{spb}{1}}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\blk}{\mathrm{BlkDiag}}
\newcommand{\email}[1]{\protect\href{mailto:#1}{#1}}
%%%%%%%%%%%%%%%%% define some convinient notations %%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\edit}[1]{{\color{red}#1}} 
 
%% bm capital letters
\def\bmw{\bm{W}}
\def\nablag{\nabla G(\bm W)}
\def\bA{\bm{A}}
\def\bB{\bm{B}}
\def\bC{\bm{C}}
\def\bCp{\bC^\prime}
\def\bbC{\bar{\bC}}
\def\bD{\bm{D}}
\def\bE{\bm{E}}
\def\bG{\bm{G}}
\def\bGp{\bG^\prime}
\def\bH{\bm{H}}
\def\bbH{\bar{\bm{H}}}
\def\bHp{\bm{H}^\prime}
\def\bHs{\bm{H}^*}
\def\bHsT{\bH^{*^T}}
\def\bI{\bm{I}}
\def\bLambda{\bm{\Lambda}}
\def\bM{\bm{M}}
\def\bO{\bm{O}}
\def\bP{\bm{P}}
\def\bPp{\bm{P}^\prime}
\def\bQ{\bm{Q}}
\def\bR{\bm{R}}
\def\bS{\bm{S}}
\def\bbU{\bar{\bU}}
\def\bU{\bm{U}}
\def\bV{\bm{V}}
\def\bVp{\bm{V}^\prime}
\def\bW{\bm{W}}
\def\bX{\bm{X}}
\def\bY{\bm{Y}}
\def\bZ{\bm{Z}}
\def\bZp{\bZ^\prime}
% mathcal
 
\def\mO{\mathcal{O}}
\def\mP{\mathcal{P}}
\def\mQ{\mathcal{Q}}
\def\mR{\mathcal{R}}
\def\mS{\mathcal{S}}
\def\mT{\mathcal{T}}
\def\mX{\mathcal{X}}
\def\mZ{\mathcal{Z}}
\def\mW{\mathcal{W}}
% others
\def\bbA{\bar{\bm{A}}}
\def\Diag{\mathbf{Diag}}
\def\C{\mathbb{C}}
\def\E{\mathbb{E}}
\def\M{\mathbb{M}}
\def\O{\mathbb{O}}
\def\P{\mathbb{P}}
\def\R{\mathbb{R}}
\def\S{\mathbb{S}}
\def\bo{\bm{1}}
\def\tK{\tilde{K}}


%% ===========================================================================

% main body

\begin{document}
\title{Error Bound Analysis for the Regularized Loss of \\ Deep Linear Neural Networks}
\author{ 
Po Chen\thanks{School of Data Science, Fudan University, Shanghai (\email{chenp24@m.fudan.edu.cn}).}
\and Rujun Jiang\thanks{School of Data Science, Fudan University, Shanghai (\email{rjjiang@fudan.edu.cn}).}
\and Peng Wang\thanks{Corresponding author. Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor (\email{peng8wang@gmail.com}).}
}
\date{\today}
\maketitle

\vspace{-0.1in} 
\begin{abstract}
The optimization foundations of deep linear networks have received significant attention lately. However, due to the non-convexity and hierarchical structure, analyzing the regularized loss of deep linear networks remains a challenging task. In this work, we study the local geometric landscape of the regularized squared loss of deep linear networks, providing a deeper understanding of its optimization properties. Specifically, we characterize the critical point set and establish an error-bound property for all critical points under mild conditions. Notably, we identify the sufficient and necessary conditions under which the error bound holds. To support our theoretical findings, we conduct numerical experiments demonstrating that gradient descent exhibits linear convergence when optimizing the regularized loss of deep linear networks.    
\end{abstract}

{\bf Key words:} Deep linear networks, critical points, error bounds, linear convergence 
\smallskip 

{\bf MSC numbers:} 90C26, 68T07, 65K10 

\section{Introduction}\label{sec:intro}

Deep learning has been widely used in various fields, including computer vision \cite{he2016deep}, natural language processing \cite{vaswani2017attention}, and healthcare \cite{esteva2019guide}, due to its exceptional empirical performance. Optimization is a key component of deep learning, playing a pivotal role in formulating learning objectives, training neural networks, and improving model generalization. In general, optimization problems arising in deep learning are highly non-convex and difficult to analyze due to the inherent nonlinearity and hierarchical structures of deep neural networks. Even in the context of linear neural networks, which are the most basic form of neural networks, our theoretical understanding remains far from complete and systematic, especially concerning the optimization properties. This motivates us to study the following problem based on deep linear networks: 
\begin{align}\label{eq:P}
\min_{\bm W}\ \left\|\bm W_L\ldots\bm W_1\bm X -  \bm Y \right\|_F^2 + \sum_{l=1}^L \lambda_l \|\bm W_l\|_F^2, 
\end{align}
where $(\bm X, \bm Y) \in \R^{d_0\times N} \times \R^{d_L\times N}$ denotes the data input, $\bm W_l \in \R^{d_l \times d_{l-1}}$ denotes the $l$-th weight matrix for each $l=1,\dots,L$, $\bm W = \{\bm W_l\}_{l=1}^L$ denotes the collection of all weight matrices, and $\lambda_l > 0$ for all $l\in [L]$ are regularization parameters. Notably, such a problem captures a wide range of deep learning problems arising in applications, including deep matrix factorization \cite{arora2019implicit,de2021survey,trigeorgis2016deep}, neural collapse \cite{Han2021,yaras2022neural,zhou2022optimization}, and low-rank adaption (LoRA) of large language models \cite{hu2022lora,yarascompressible}, to name just a few. Moreover, studying linear networks provides a valuable starting point for gaining insights into nonlinear networks, as they exhibit similar learning behaviors and phenomena to their nonlinear counterparts while maintaining a simpler structure \cite{huh2023the,saxe2019mathematical}. 
%\begin{itemize}[leftmargin=*] 
%\item {\bf Deep matrix factorization}: Deep matrix factorization \cite{arora2019implicit,de2021survey,trigeorgis2016deep} is a fundamental tools for extracting the principal components hierarchically from high-dimensional data. In general, it can be formulated as 
%\begin{align*}
%\min_{\bm W} \left\|\bm W_L\ldots\bm W_1  -  \bm Y \right\|_F^2 + \sum_{l=1}^L \lambda_l \|\bm W_l\|_F^2,
%\end{align*}
%where $\lambda_l \ge 0$ for all $l=1,\dots,L$ are regularization parameters. 
%
%\item {\bf Neural collapse}: Neural collapse \cite{papyan2020prevalence} is a prevalent phenomenon observed during the training of over-parameterized neural networks, where the last-layer features and classifiers exhibit a simple yet elegant geometric structure. In the existing literature, analyzing neural collapse often reduces to studying the global optimality and loss landscape of the following problem \cite{yaras2022neural,zhu2021geometric}:  
%\begin{align*}
%\min_{\bm W_1,\bm W_2} \left\|\bm W_2 \bm W_1  -  \bm Y \right\|_F^2 + \lambda_1\|\bm W_1\|_F^2 + \lambda_2\|\bm W_2\|_F^2,
%\end{align*}
%where $\lambda_1, \lambda_2 > 0$ are regularization parameters and $\bm Y$ is a membership matrix.   
%\end{itemize} 
% Moreover, linear neural networks have been recognized as valuable prototypes for studying nonlinear networks, as they exhibit similar learning behaviors and phenomena to their nonlinear counterparts while maintaining a simpler structure; see, e.g., \cite{huh2023the,saxe2019mathematical}. Therefore, studying linear networks is a valuable starting point for gaining mathematical insights into nonlinear networks.

In practice, (stochastic) gradient descent (GD) and its variants are among the most widely used first-order methods for deep learning \cite{kingma2014adam}. Over the past few years, substantial progress has been made in studying the convergence behavior of GD for solving Problem \eqref{eq:P}. In the literature, a large amount of work has been dedicated to studying an unregularized counterpart of Problem \eqref{eq:P}, i.e., $\min \left\|\bm W_L\ldots\bm W_1\bm X -  \bm Y \right\|_F^2$, based on a gradient dynamic analysis. A noteworthy assumption in most of these studies is that the data is whitened, i.e., $\bm X\bm X^T = \bm I_d$, when analyzing the convergence rate of GD. In general, this assumption simplifies the analysis by ensuring that the input samples are uncorrelated and have unit variance. Now, suppose that the data is whitened. \citet{bartlett2018gradient} showed that GD with the identity initialization converges to an $\epsilon$-optimal solution within a polynomial number of iterations. Later, \citet{arora2019convergence} further improved the convergence result, showing that GD converges linearly to a global optimum when $\min\{d_1,\dots,d_{L-1}\} \ge \min\{d_0,d_L\}$, the initial weight matrices are approximately balanced, and the initial loss is smaller than a threshold. Meanwhile, \citet{wu2019global} showed that gradient descent with zero-asymmetric initialization avoids saddle points and converges to an $\epsilon$-optimal solution in $O(L^3\log(1/\epsilon))$ iterations. Other works also proved similar global convergence and convergence rate results of GD under different assumptions; see, e.g., \citep{bah2022learning,chitour2023geometric,Hu2020Provable,nguegnang2024convergence,wu2019global,zhao2024convergence}. Despite these inspiring results, the existing analyses suffer from three notable limitations. First, they are highly specific to a particular problem, relying on the analysis of gradient dynamics under carefully designed weight initialization schemes. This raises questions about the generalizability of these analyses to GD or other first-order optimization methods with different initialization schemes.  Second, regularization is commonly used during the training of neural networks to prevent overfitting, improve generalization, and accelerate convergence \cite{krizhevsky2012imagenet,zhang2021understanding}. However, the existing gradient dynamics analyses mainly focus on unregularized deep neural networks and cannot be directly applied to Problem \eqref{eq:P}, even when the data is whitened. Finally, the existing analyses only apply to analyze the convergence to global optimal solutions. However, Problem \eqref{eq:P} and its regularized counterpart may have local minimizers, to which first-order methods, such as GD, are likely to converge. To the best of our knowledge, the convergence behavior of first-order methods when they approach a critical point—whether a global minimum, a local minimum, or even a saddle point—remains an open question in the literature. To sum up, it remains an unsolved challenge in the literature to develop a unified framework to analyze the convergence behavior of first-order methods to critical points when solving Problem \eqref{eq:P}.   


To address the above challenge, a promising approach is to study the local geometric structure of Problem \eqref{eq:P} associated with its objective function, such as the error bound condition \cite{liao2024error,rebjock2024fast,zhou2017unified}, the Polyak-\L ojasiewicz (P\L) inequality \cite{polyak1963gradient}, and quadratic growth \cite{drusvyatskiy2018error}. When the data is whitened, as is commonly assumed in the literature (see the review above), we let $\bm Y := \bm Y \bm X^T$ with a slight abuse of notation. Under this notation, Problem \eqref{eq:P} reduces to
\begin{align}\label{eq:F}
\min_{\bm W} F(\bm W) :=  \left\|\bm W_L\ldots\bm W_1 -  \bm Y \right\|_F^2 + \sum_{l=1}^L \lambda_l \|\bm W_l\|_F^2. 
\end{align}
Notably, both deep matrix factorization \cite{arora2019implicit,de2021survey,trigeorgis2016deep} and the neural collapse problem \cite{Han2021,zhou2022optimization} are special instances of the above problem. In this work, we mainly focus on the {\em error bound} condition associated with Problem \eqref{eq:F}. Formally, let $\cal W$ denote the set of critical points to Problem \eqref{eq:F}. We say that it possesses an error bound for $\cal W$ if there exist constants $\epsilon,\kappa > 0$ such that  for all $\bm W$ satisfying $\mathrm{dist}(\bm W, \mathcal{W}) \le \epsilon$, 
\begin{align}\label{eq:eb}
\mathrm{dist}(\bm W, \mathcal{W}) \le \kappa\|\nabla F(\bm W)\|_F,
\end{align} 
where $\mathrm{dist}(\bm W, \mathcal{W}) := \min\{\|\bm W - \bm X\|_F: \bm X \in \mathcal{W}\}$ denotes the distance from $\bm W$ to the critical point set $\cal W$. Intuitively, the error bound inequality \eqref{eq:eb} requires the distance from a point to the set of critical points to be bounded by its gradient norm. A unified framework leveraging this condition, along with some algorithm-dependent conditions, has been widely studied to analyze linear convergence of first-order methods \cite{drusvyatskiy2018error,zhou2017unified} or superlinear convergence of second-order methods \cite{yue2019family} in convex optimization. Recently, \citet{liao2024error} showed that even if the objective function is non-convex but smooth and satisfies the error bound, GD converges linearly to the solution set. Moreover, \citet{liao2024error,rebjock2024fast} demonstrated that the error bound is equivalent to other regularity conditions, such as the P\L\ inequality \cite{polyak1963gradient} and quadratic growth \cite{drusvyatskiy2018error} under mild conditions. These regularity conditions are widely used in the literature to prove linear convergence of GD for optimizing non-convex problems \cite{frei2021proxy}. Notably, this type of convergence analysis framework is not limited to studying convergence to global optimal solutions. Instead, it can also be applied to local minima or even saddle points, as long as the error bound holds for the targeted solution sets. This makes such a framework particularly effective for analyzing non-convex optimization problems, where the landscape often contains various types of critical points. 

However, powerful as this approach may seem, a critical challenge is to prove the error bound inequality \eqref{eq:eb} for Problem \eqref{eq:F}. As far as we know, proving the error bound or other regularity conditions for deep neural networks remains relatively underexplored. Recently, \citet{wang2022linear} made progress in this direction by establishing the error bound for the set of global optimal solutions to a special instance of Problem \eqref{eq:F}: $\min  \|\bm W_2\bm W_1  - \bm Y\|_F^2 + \lambda_1\|\bm W_1\|_F^2 +  \lambda_2\|\bm W_2\|_F^2$, where $\lambda_1,\lambda_2 >0$ are regularization parameters and $\bm Y$ is a membership matrix. However, this analysis is limited to global optimal solutions of 2-layer neural networks and a specific data matrix $\bm Y$. Extending these results to deeper networks, more general data, and broader sets of critical points remains an open problem.  

% \edit{PW: highligh we identify the condition under which the error bound hold! This condition is sufficient and necessary.}

In view of the above discussion, our goal in this paper is to establish an error bound for the set of critical points of Problem \eqref{eq:F}. %In addition, \edit{we demonstrate how this result provides deeper insights into the convergence rate analysis of various first-order methods for solving Problem \eqref{eq:F}}. 
Our main contributions are twofold. First, we explicitly characterize the critical point set of Problem \eqref{eq:F}  for general data $\bm Y$ (see \Cref{thm:opti}), despite the non-convexity and hierarchical structure of the problem. This characterization serves as a foundation for establishing the error bound for Problem \eqref{eq:F}. Second, leveraging this explicit characterization, we show that all critical points of Problem \eqref{eq:F} satisfy the error bound (see \Cref{thm:eb}) under mild conditions. Notably, we identify the sufficient and necessary conditions on the relationship between the regularization parameters and the spectrum of the input data that ensure the error bound holds. Such a result is significant, as it expands the currently limited repertoire of non-convex problems for which the local loss geometry is well understood. Moreover, it is important to note that our work develops new techniques in the process of establishing the error bound to handle the repeated singular values in $\bm Y$ and the complicated structure of the critical point set. These techniques could be of independent interest. The established error bound can be used to establish other regularity conditions, such as the P\L\ inequality and quadratic growth. Building on the error bound of Problem \eqref{eq:F},  we demonstrate that first-order methods can achieve linear convergence to a critical point of Problem \eqref{eq:F}, provided that they satisfy certain algorithm-dependent properties. %This result highlights the practical relevance of the error bound by bridging the theoretical understanding of the loss landscape with the convergence behavior of widely used optimization algorithms. 
We conduct numerical experiments in deep linear networks and in more general settings and observe that gradient descent converges linearly to critical points. These results strongly support our theoretical findings. 
\vspace{-0.1in}
\paragraph{Organization.} The rest of this paper is organized as follows. In \Cref{sec:main}, we present the main results of this paper. We introduce key lemmas and propositions without proofs and prove the main results in \Cref{sec:proof}, and provide the detailed proofs for these lemmas and propositions in \Cref{sec:pf tech}. We report experimental results in \Cref{sec:expe} and conclude the paper in \Cref{sec:con}.   

\vspace{-0.1in}
\paragraph{Notation.} Given an integer $n$, we denote by $[n]$ the set $\{1,\dots,n\}$. %Given $K$ integers $n_1,\dots,n_K$, we define $n_{\min} = \min\{n_1,\dots,n_K\}$. 
Given a vector $\bm a$, let $\|\bm a\|$ denote the Euclidean norm of $\bm a$, $a_i$ the $i$-th entry, and $\mathrm{diag}(\bm a)$ the diagonal matrix with $\bm a$ as its diagonal. Unless specified otherwise, all vectors in this paper are column vectors. Given a matrix $\bm A \in \R^{N\times d}$, let $\|\bm A\|$ denote the spectral norm of $\bm A$, $\|\bm A\|_F$ denote the Frobenius norm, $a_{ij}$ denote the $(i,j)$-th element, and $\sigma_i(\bm A)$ denote the $i$-th largest singular value. We use $\bm 0_{m\times n}$ to denote $m\times n$ all-zero matrix, $\bm 0_m$ to denote $m\times m$ all-zero matrix, and simply write $\bm 0$ when its dimension can be inferred from the context. We use $\mathcal{O}^{n\times d}$ to denote the set of all $n\times d$ matrices that have orthonormal columns (in particular, we use $\mathcal{O}^{n}$ to denote the set of all $d\times d$ orthogonal matrices); $\mathcal{P}^{n}$ to denote the set of all $n\times n$ permutation matrices; $\mathrm{BlkDiag}(\bm X_1,\dots,\bm X_n)$ to denote the block diagonal matrix whose diagonal blocks are $\bm X_1,\dots,\bm X_n$. Given a matrix $\bm X\in \R^{m\times n}$ and a non-empty closed set $\mathcal{X} \subseteq \R^{m\times n}$, we use $\mathrm{dist}(\bm X, \mathcal{X})$ to denote the Euclidean distance from $\bm X$ to $\cal X$; the distance between $\mathcal{X}$ and another non-empty closed set $\mathcal{Y}$ is defined as $\mathrm{dist}(\mathcal{X},\mathcal{Y}) = \min_{\bm X \in \mathcal{X}, \bm Y \in \mathcal{Y}}\|\bm X - \bm Y\|_F$.  Given weight matrices $\bm W_1,\dots,\bm W_L$, let $\bm W_{i:1} := \bm W_i\bm W_{i-1}\ldots\bm W_1$ for each $i=2,\dots,L$ and $\bm W_{L:i} := \bm W_L\bm W_{L-1}\ldots\bm W_i$ for each $i=1,\dots,L-1$. In particular, we define $\bm W_{0:1} := \bm I$ and $\bm W_{L:L+1} := \bm I$. For all $i \ge j+1$, let $\bm W_{i:j}=\bm W_{i}\bm W_{i-1}\ldots\bm W_j$ and  $\bm W_{i:j}=\bm W_i$ when $i=j$.  All other notation is standard. 


\section{Main Results}\label{sec:main}

%This is a typical deep matrix factorization problem \cite{arora2019implicit,chou2024gradient,de2021survey}. 
To begin, we compute the gradient of the objective function as follows: 
\begin{align*} 
\nabla_{\bm W_l} F(\bm W) = \bm W_{L:l+1}^T\left(\bm W_{L:1} -  \bm Y\right)\bm W_{l-1:1}^T + \lambda_l \bm W_l,\ \forall l \in [L]. 
\end{align*} 
Then, the set of critical points of Problem \eqref{eq:F} is defined as 
\begin{align}\label{set:crit F}
\mathcal{W} := \left\{ \bm W = (\bm W_1,\dots,\bm W_L): \nabla_{\bm W_l} F(\bm W) = \bm 0,\ \forall l \in [L] \right\}. 
\end{align} 

Before we proceed, it is important to highlight the challenges in characterizing the set of critical points and establishing the error bound inequality \eqref{eq:eb} for Problem \eqref{eq:F}. First, the objective function is non-convex, presenting a complex loss landscape that may include a variety of critical points such as global minimizers, local minimizers, and saddle points \cite{mehta2021loss}. %This intricate structure complicates the task of analyzing the behavior of first-order methods, as the convergence dynamics can vary significantly depending on the type of critical point encountered. 
Second, the objective function admits a hierarchical structure, where the weight matrices are multiplied sequentially and the layers have varying sizes. This hierarchical composition introduces a rotational invariance to the solution set \eqref{set:crit F}, i.e., $\bm W_{l+1}\bm W_l =  (\bm W_{l+1}\bm Q^T) (\bm Q\bm W_l)$ for any $\bm Q \in \mathcal{O}^{d_l}$ while the objective remains unchanged. This invariance leads to equivalence classes of solutions, thereby significantly complicating the analysis. 

\subsection{Characterization of Critical Points}\label{subsec:opti}

In this subsection, we characterize the critical point set of Problem \eqref{eq:F}. To proceed, we first introduce the following assumption on the width of network layers. 
\begin{assumption}\label{AS:1}
It holds that $\min\{d_1,\dots,d_{L-1}\} \ge \min\{d_0,d_L\}$. 
\end{assumption}
\noindent This assumption ensures that the width of each hidden layer is no less than that of the input or output layer. It is important to note that this assumption is not restrictive and aligns with common practices in deep learning, where hidden layers are often designed to have sufficient capacity to capture complex data representations \cite{hanin2019universal}. In addition, this assumption is widely used in the literature to analyze the convergence behavior of GD for optimizing deep networks; see, e.g., \cite{arora2019convergence,du2019width,du2018gradient,laurent2018deep}.  

Throughout the rest of the paper, let $d_{\min} := \min\{d_0,d_1,\dots,d_L\}$. This, together with \Cref{AS:1}, implies $d_{\min} = \min\{d_0,d_L\} $.  Let $r_Y := \mathrm{rank}(\bm Y)$ and 
\begin{align}\label{eq:SVD Y}
\bm Y = \bm U_Y\bm \Sigma_Y \bm V_Y^T 
\end{align}
be a singular value decomposition (SVD) of $\bm Y \in \R^{d_L\times d_0}$, where $\bm U_Y \in \mathcal{O}^{d_L}$, $\bm V_Y \in \mathcal{O}^{d_0}$, and $\bm \Sigma_Y = \mathrm{BlkDiag}(\mathrm{diag}(y_1,y_2,\dots,y_{r_Y}), \bm 0) \in \R^{d_L\times d_0}$ with  $y_1\ge y_2 \ge \dots \ge y_{r_Y} > 0$ being top $r_Y$ positive singular values. In the literature, it is common to assume that the singular values of $\bm Y$ are distinct to simplify the analysis; see, e.g., \cite{achour2024loss,kawaguchi2016deep}. However, this assumption is overly strict and does not hold in many practical scenarios. For example, when $\bm Y$ is a membership matrix in $K$-classification problems, it typically has $K$ repeated singular values. To address this challenge, we introduce the following key setup and notions to improve the analysis.  Let $p_Y$ denote the number of distinct positive singular values of $\bm Y$. In other words, there exist indices $s_0, s_1, \dots, s_{p_Y}$ such that $0= s_{0} < s_{1} < \dots < s_{p_Y} = r_Y$ and
\begin{subequations}\label{eq:SY1}
\begin{align}
    & y_{s_{0}+1} = \dots = y_{s_{1}} > y_{s_{1}+1} = \dots = y_{s_{2}} > \dots > y_{s_{p_Y-1}+1} = \dots = y_{s_{p_Y}} > 0,  \\
    & y_{s_{p_Y}+1} = \cdots = y_{d_{\text{min}}} = 0.
\end{align}
\end{subequations}
Then, let $h_i := s_i - s_{i-1}$ denote the multiplicity of the $i$-th largest positive singular value for each $i \in [p_Y]$. Consequently, we have $\sum_{i=1}^{p_Y} h_i = r_Y$.  
%and write $\tilde{\bm \Sigma}_Y$ in \eqref{eq:Y} as 
Now, we are ready to characterize the set of critical points explicitly as follows: 
\begin{thm}\label{thm:opti}
Suppose that \Cref{AS:1} holds. It holds that $\bm W \in \mathcal{W}$ if and only if
\begin{subequations}\label{eq:W}
\begin{align}
& \bm W_1 = \bm Q_2\bm \Sigma_1 \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\bm O_{p_Y+1} \right)\bm V_Y^T, \\ 
& \bm W_l = \bm Q_{l+1} \bm \Sigma_l \bm Q_l^T,\ l=2,\dots,L-1,  \\
& \bm W_L =  \bm U_Y\mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_Y}^T,\widehat{\bm O}_{p_Y+1}^{T} \right) \bm \Sigma_L \bm Q_L^T,
\end{align}
\end{subequations}
where $\bm Q_l \in \mathcal{O}^{d_{l-1}}$ for all $l=2,\dots,L$, $\bm O_i \in \mathcal{O}^{h_i}$ for each $i \in [p_Y]$, $\bm O_{p_Y+1} \in \mathcal{O}^{d_0 - r_{Y}}$, $\widehat{\bm O}_{p_Y+1} \in \mathcal{O}^{d_L - r_{Y}}$, and $\bm \Sigma_l = \mathrm{BlkDiag}\left(\mathrm{diag}(\bm \sigma)/\sqrt{\lambda_l}, \bm 0 \right) \in \R^{d_l\times d_{l-1}}$ for each $l \in [L]$ with $\bm \sigma \in \R^{r_Y}$ satisfying  
\begin{align}\label{eq:sigma W}
\sigma_i^{2L-1} - \left(\sqrt{\lambda_1\dots\lambda_L}y_i\right) \sigma_i^{L-1} +  \left(\lambda_1\dots\lambda_L\right) \sigma_i = 0,\ \sigma_i \geq 0,\ \forall i \in [r_Y]. 
\end{align}
\end{thm}
 
Before we proceed, let us make some remarks about this theorem. First, despite the non-convexity and hierarchical structure of Problem \eqref{eq:F}, we still characterize the set of critical points of Problem \eqref{eq:F} explicitly, where each weight matrix admits the SVD in \eqref{eq:W}. Note that all weight matrices share the same singular values defined in \eqref{eq:sigma W} up to scaling of the regularization parameters $\{\lambda_l\}_{l=1}^L$, and their left and right singular matrices are determined by the orthogonal matrices $\bm Q_l$ and $\bm O_i$. Here, $\bm Q_l$ for all $l$ are introduced to handle the rotational invariance in the sequential matrix multiplication of the weight matrices, while $\bm O_i$ for all $i$ are used to address the repeated singular values in $\bm Y$.  

Second, to the best of our knowledge, there is no complete characterization of the critical point set for regularized deep linear networks in the existing literature. Recently, \citet{dang2023neural} have studied the geometric properties of the global minimizers of the neural collapse problem with deep linear networks. Notably, this problem is a special instance of Problem \eqref{eq:F}, where $\bm Y$ is a membership matrix. In contrast to their work, which focuses only on global minimizers, our approach provides an explicit characterization of all critical points for arbitrary $\bm Y$. In addition, considerable research has been conducted to study the critical points of unregularized deep linear networks. The most complete and recent result is \cite{achour2024loss}, which provides both necessary and sufficient conditions for identifying first-order critical points. Nevertheless, their analysis is limited to unregularized settings and cannot be applied to our case due to regularization terms. 

Third, the explicit characterization of the critical point set in \eqref{eq:W} serves as a cornerstone for establishing the error bound for Problem \eqref{eq:F}. It is worth noting that each critical point is not an isolated point but a union of connected sets (see \Cref{prop:set W}). This brings significant difficulty to compute the distance $\mathrm{dist}(\bm W, \cal W)$ from a point $\bm W$ to the critical point set. Such an intricacy also underscores the importance of a thorough and precise characterization of the critical point set to facilitate rigorous error-bound analysis.  

\subsection{Error Bound for Deep Linear Networks}

In this subsection, we present our main result, which establishes the error bound for Problem \eqref{eq:F}. Before we proceed, it is important to point out that there are some degenerate cases under which the error bound does not hold. For example, when $L=2$, we consider 
\begin{align*}
\min_{\bm W_1, \bm W_2 \in \R^{d\times d}} F(\bm W) = \frac{1}{2}\|\bm W_2\bm W_1 - \lambda \bm I_d \|_F^2 +   \frac{1}{2}\lambda \left( \|\bm W_1\|_F^2 + \|\bm W_2\|_F^2 \right).
\end{align*}
This, together with \Cref{thm:opti}, yields that the set of critical points is $\mathcal{W} = \{(\bm 0, \bm 0)\}$. Now, consider \(\bm W_1 = \bm W_2 = x \bm I_d	\). We compute  
\begin{align*}
& \nabla_{\bm W_1} F (\bm W) =  \bm W_2^T \left(\bm W_2 \bm W_1 - \lambda \bm I_d\right) +  \lambda \bm W_1 = x^3 \bm I_d,\\
& \nabla_{\bm W_2} F (\bm W) =  \left(\bm W_2 \bm W_1 - \lambda \bm I_d\right)\bm W_1^T +  \lambda \bm W_2 = x^3 \bm I_d. 
\end{align*}
Therefore, we have $\|\nabla F(\bm W)\|_F^2 = 2dx^6$. Moreover, we compute $\mathrm{dist}^2(\bm W,\mathcal{W}) = \|\bm W_1\|_F^2 + \|\bm W_2\|_F^2 = 2d x^2$. Consequently, we have \(\|\nabla F(\bm W)\|_F =  \mathrm{dist}^3(\bm W,\mathcal{W})/(2d)\), and thus the error bound does not hold as $x \to 0$. To avoid such degenerate cases, we impose the following conditions on the relationship between the regularization parameters and the data matrix $\bm Y$. 

\begin{assumption}\label{AS:2}
It holds for $L=2$ that
\begin{align}\label{eq:AS L=2}
\lambda_1\lambda_2 \neq y_i^2,\ \forall i \in [r_Y].    
\end{align}
For all $L \ge 3$, it holds that 
\begin{align}\label{eq:AS y}
\lambda_1\dots\lambda_L \neq y_i^{2(L-1)}\left( \left( \frac{L-2}{L} \right)^{\frac{L}{2(L-1)}} + \left( \frac{L}{L-2} \right)^{\frac{L-2}{2(L-1)}} \right)^{-2(L-1)},\ \forall i \in [r_Y]. 
\end{align}
\end{assumption}

Notably, this assumption provides sufficient and necessary conditions under which the error bound holds for all critical points of Problem \eqref{eq:F}. Indeed, if \Cref{AS:2} holds, the error bound can be rigorously established (see \Cref{thm:eb}). Conversely, if \Cref{AS:2} is violated, we can show that the error bound fails to hold (see \Cref{app sec:counter}). This dual perspective highlights the critical role of \Cref{AS:2} in establishing the error bound for Problem \eqref{eq:F}. Now, under Assumptions \ref{AS:1} and \ref{AS:2}, we are ready to prove the error bound for Problem \eqref{eq:F}. 

\begin{thm}\label{thm:eb}
Suppose that Assumptions \ref{AS:1} and \ref{AS:2} hold. There exist constants $\epsilon_1, \kappa_1 > 0$ such that for all $\bm W$ satisfying $\mathrm{dist}(\bm W, \mathcal{W}) \le \epsilon_1$, 
\begin{align}\label{eq:eb F}
\mathrm{dist}(\bm W, \mathcal{W}) \le \kappa_1\|\nabla F(\bm W)\|_F. 
\end{align}
\end{thm}

This theorem is significant as it establishes the fact that the error bound holds at any critical point of Problem \eqref{eq:F} under \Cref{AS:2}. Importantly, the constants $\kappa_1, \epsilon_1$ can be explicitly derived through our proofs; see \Cref{subsec:ana eb}. Now, we discuss some implications and the related work on this result. First, the established error bound can be used to derive other regularity conditions for analyzing the convergence behavior of first-order methods.  The P\L\ inequality and quadratic growth of Problem \eqref{eq:F} can be obtained in \Cref{coro:PL QG} when the error bound holds, whose proof is deferred to \Cref{subsec:PL QG}.
\begin{coro}\label{coro:PL QG}
Suppose that $\bm W^*$ is a critical point such that \eqref{eq:eb F} holds for all $\bm W$ satisfying $\|\bm W - \bm W^*\|_F \le \epsilon_1$. The following statements hold: \\
(i) There exists a constant $\mu_1 > 0$ such that 
\begin{align}\label{eq:PL}
    \|\nabla F(\bm W)\|_F^2 \ge \mu_1 ( F(\bm W) - F(\bm W^*)). 
\end{align}
(ii) If $\bm W^*$ is a local minimizer, there exists a constant $\mu_2 > 0$ such that
\begin{equation}
%\label{eq:QG}
\notag
    \mathrm{dist}^2(\bm{W}, \mathcal{W}) \leq \mu_2 \left( F(\bm{W}) - F(\bm{W}^*) \right).
\end{equation}
\end{coro}

Second, combining the error bound with other algorithm-dependent conditions, such as {\em sufficient decrease}, {\em cost-to-go estimate}, and {\em safeguard}, we obtain local linear convergence of first-order methods for solving Problem \eqref{eq:F} (see \Cref{prop:threecondition}). In contrast to algorithm-specific convergence rate analyses in \citep{bah2022learning,chitour2023geometric,Hu2020Provable,nguegnang2024convergence,wu2019global,zhao2024convergence}, which are tailored to the dynamics of individual algorithms, the error-bound-based framework provides a unified approach. It applies not only to gradient descent but also to a broad class of first-order methods capable of optimizing Problem \eqref{eq:F}. It is also worth mentioning that our experimental results in \Cref{sec:expe} demonstrate that GD achieves linear convergence to both optimal and non-optimal critical points across various network architectures, which further supports our theoretical findings. 

Third, we emphasize our technical contributions to establishing the error bound. Unlike prior works \cite{Liu2019,jiang2019novel,jiang2022holderian,wang2023linear}, where the critical point set $\cal W$ had a relatively simple structure that allows for explicit computation of the distance $\mathrm{dist}(\bm W, \mathcal{W})$, the presence of orthogonal permutations and hierarchical structures in $\cal W$ (see \Cref{thm:opti}) poses a significant challenge to our analysis. To address this, we carefully construct an intermediate point $\hat{\bm W}$ that leverages the singular vectors of $\bm W$ and singular values of $\bm W^* \in \cal W$. Then, we bound $\mathrm{dist}(\hat{\bm W}, \mathcal{W})$ and $\|\bm W- \hat{\bm W}\|_F$ by the gradient norm at $\bm W$, respectively. Combining these bounds with the triangle inequality yields \eqref{eq:eb F}. Notably, this construction technique provides a new approach to showing the error bound of non-convex problems with a complicated solution set. 

\section{Proofs of the Main Results}\label{sec:proof}

In this section, we prove our main theorems concerning the critical points (i.e., \Cref{thm:opti}) and the error bound (i.e., \Cref{thm:eb}) of Problem \eqref{eq:F}. To avoid interrupting the flow of the main proofs, we introduce key lemmas and propositions, with their detailed proofs deferred to \Cref{sec:pf tech}. Based on these results, we present the complete proofs of \Cref{thm:opti} and \Cref{thm:eb}. 

\subsection{Preliminary Results}\label{subsec:pre}

To characterize the critical points and establish the error bound of Problem \eqref{eq:F}, we claim that it suffices to study the following problem: 
\begin{align}\label{eq:G}
    \min_{\bm W}\ G(\bm W) := \left\|\bm W_L\ldots\bm W_1 -  \sqrt{\lambda}\bm Y \right\|_F^2 + \lambda \sum_{l=1}^L \|\bm W_l\|_F^2,
\end{align}
where $\lambda := \lambda_1\dots\lambda_L$. We compute the gradient of $G(\bm W)$, which will be frequently used, as follows:
\begin{align}\label{eq:grad G}
\frac{1}{2}\nabla_{\bm W_l} G(\bm W) = \bm W_{L:l+1}^T\left(\bm W_{L:1} - \sqrt{\lambda}\bm Y\right)\bm W_{l-1:1}^T + \lambda \bm W_l,\ \forall l \in [L]. 
\end{align} 
The critical point set of this problem is defined as
\begin{align}\label{set:crit G}
\mathcal{W}_G := \left\{ \bm W = (\bm W_1,\dots,\bm W_L): \nabla_{\bm W_l} G(\bm W) = \bm 0,\ \forall l \in [L] \right\}. 
\end{align} 
Now, we present the following lemma to prove our claim.
\begin{lemma}\label{lem:equi FG}
Consider Problems \eqref{eq:F} and \eqref{eq:G}. The following statements hold: \\
(i) $(\bm W_1,\dots,\bm W_L) \in \mathcal{W}$ if and only if $(\sqrt{\lambda_1}\bm W_1,\dots,\sqrt{\lambda_L}\bm W_L) \in \mathcal{W}_G$. \\
(ii) Suppose that there exist constants \(\epsilon, \kappa > 0\) such that for all \(\bm Z\) satisfying \(\mathrm{dist}(\bm Z, \mathcal{W}_G) \leq \epsilon\), it holds that  
\begin{align}\label{eq:ebG}
    \mathrm{dist}(\bm Z,  \mathcal{W}_G) \le \kappa \|\nabla G(\bm Z)\|_F.
\end{align} 
Then, for all \(\bm W\) satisfying \(\mathrm{dist}(\bm W, \mathcal{W}) \le \epsilon/\sqrt{\lambda_{\max}}\), it holds that 
\begin{align}
%\label{eq:ebF}
\notag
    \mathrm{dist}(\bm W, \mathcal{W}) \le \frac{\kappa \lambda}{\lambda_{\min}} \|\nabla F(\bm W)\|_F.
\end{align} 
(iii) Let \eqref{eq:SVD Y} be an SVD of $\bm Y$. It holds that $(\bm W_1,\bm W_2,\dots,\bm W_L)$ is a critical point of Problem \eqref{eq:G} if and only if  $(\bm W_1\bm V_Y,\bm W_2,\dots,\bm U_Y^T\bm W_L)$ is a critical point of the following problem: 
\begin{align*}
    \min_{\bm W}\ \left\|\bm W_L\ldots\bm W_1 -  \sqrt{\lambda}\bm \Sigma_Y \right\|_F^2 + \lambda \sum_{l=1}^L \|\bm W_l\|_F^2.
\end{align*} 
\end{lemma}
\begin{proof}
(i) Let $\bm W := (\bm W_1,\dots,\bm W_L)\in \mathcal{W}$ be arbitrary. Using this and \eqref{set:crit G}, we obtain  
\begin{align}\label{eq:grad F}
\nabla_{\bm W_l} F(\bm W) = 2\bm W_{L:l+1}^T\left(\bm W_{L:1} - \bm Y\right)\bm W_{l-1:1}^T + 2 \lambda_l \bm W_l = \bm 0,\ \forall l \in [L].
\end{align}
Now, let $\hat{\bm W} := (\sqrt{\lambda_1}\bm W_1,\dots,\sqrt{\lambda_L}\bm W_L)$. For each $l\in [L]$, we compute
\begin{align*}
\nabla_{\bm W_l} G(\hat{\bm W}) &\overset{\eqref{eq:grad G}}{=}  2\sqrt{\lambda_L\dots\lambda_{l+1}\lambda_{l-1}\dots\lambda_1}{\bm W_{L:l+1}^T}\left(\sqrt{\lambda} \bm W_{L:1} - \sqrt{\lambda}\bm Y\right)\bm W_{l-1:1}^T + 2\lambda\sqrt{\lambda_l} \bm W_l \\
& = \frac{2\lambda}{\sqrt{\lambda_l}} \left( \bm W_{L:l+1}^T\left(\bm W_{L:1} - \bm Y\right)\bm W_{l-1:1}^T + \lambda_l \bm W_l \right) \overset{\eqref{eq:grad F}}{=} \bm 0.
\end{align*}
Therefore, we have $\hat{\bm W} \in \mathcal{W}_G$. Conversely, let $\hat{\bm W} \in \mathcal{W}_G$ be arbitrary. Using the same argument, we have $\bm W \in \mathcal{W}$. %Then, we complete the proof. 

(ii) Using (i), we express $\mathcal{W}_G$ as
\begin{align*}
 \mathcal{W}_G = \left\{(\sqrt{\lambda_1} \bm W_1,\dots,\sqrt{\lambda_L} \bm W_L): (\bm W_1, \dots, \bm W_L) \in \mathcal{W}\right\}.
\end{align*}
Now, let $\bm W := \left( \bm W_1,\dots,\bm W_L\right)$ be arbitrary and $\bm Z := (\sqrt{\lambda_1}\bm W_1,\dots,\sqrt{\lambda_L}\bm W_L)$. Then, let $(\sqrt{\lambda_1}\hat{\bm W}_1^*,\dots,\sqrt{\lambda_L}\hat{\bm W}_L^*)$ with $\hat{\bm W}^* \in \mathcal{W}$ be such that $\mathrm{dist}^2(\bm Z, \mathcal{W}_G) = \sum_{l=1}^L \lambda_l\|\bm W_l - \hat{\bm W}^*_l\|_F^2$ and $\bm W^* = (\bm W_1^*,\dots,\bm W_L^*) \in \mathcal{W}$ be such that $\mathrm{dist}^2(\bm W,\mathcal{W}) = \sum_{l=1}^L \|\bm W_l - \bm W_l^*\|_F^2$. We obtain 
\begin{align}\label{eq:distconnect}
    \mathrm{dist}^2(\bm W,\mathcal{W}) = \sum_{l=1}^L \|\bm W_l - \bm W_l^*\|_F^2 \ge \frac{\sum_{l=1}^L \|\sqrt{\lambda_l}\bm W_l - \sqrt{\lambda_l}\bm W_l^*\|_F^2}{\lambda_{\max}}  \ge \frac{1}{\lambda_{\max}}\mathrm{dist}^2(\bm Z, \mathcal{W}_G). 
\end{align}
This, together with $\mathrm{dist}(\bm W,\mathcal{W}) \le  \epsilon/\sqrt{\lambda_{\max}}$, implies $\mathrm{dist}(\bm Z, \mathcal{W}_G) \le \epsilon$. Using this and \eqref{eq:ebG}, we obtain 
\begin{align}\label{eq1:lem equi}
    \mathrm{dist}(\bm Z, \mathcal{W}_G) \le \kappa \|\nabla G(\bm Z)\|_F.
\end{align} 
Next, we have
\begin{align}\label{eq2:lem equi}
    \mathrm{dist}^2(\bm Z, \mathcal{W}_G) = \sum_{l=1}^L \lambda_l\|\bm W_l - \hat{\bm W}^*_l\|_F^2 \ge \lambda_{\min} \sum_{l=1}^L \|\bm W_l - \hat{\bm W}^*_l\|_F^2 \ge \lambda_{\min} \mathrm{dist}^2(\bm W, \mathcal{W}).  
\end{align}
Moreover, we have for each $l \in [L]$, 
\begin{align}\label{eq:gradientnorm}
    \nabla_{\bm W_l} G(\bm Z) & \overset{\eqref{eq:grad F}}{=} 2\sqrt{\lambda_L\dots\lambda_{l+1}\lambda_{l-1}\dots\lambda_1}\bm W_{L:l+1}^T\left(\sqrt{\lambda}\bm W_{L:1} - \sqrt{\lambda}\bm Y\right)\bm W_{l-1:1}^T + 2 \lambda \sqrt{\lambda_l} \bm W_l \notag \\
    & = \frac{\lambda}{\sqrt{\lambda_l}}  \nabla_{\bm W_l} F(\bm W). 
\end{align} 
This, together with \eqref{eq1:lem equi} and \eqref{eq2:lem equi}, yields
\begin{align*}
    \mathrm{dist}(\bm W, \mathcal{W}) & \le \frac{1}{\sqrt{\lambda_{\min}}} \mathrm{dist}(\bm Z, \mathcal{W}_G) \le \frac{\kappa}{\sqrt{\lambda_{\min}}} \|\nabla G(\bm W)\|_F  \le \frac{\kappa\lambda}{\lambda_{\min}} \|\nabla F(\bm W)\|_F. 
\end{align*} 
%Then, we complete the proof. 

(iii) Obviously, each critical point of Problem \eqref{eq:G} satisfies $\nabla_{\bm W_l} G(\bm W) = 0$.
This, together with \eqref{eq:SVD Y} and \eqref{eq:grad G}, directly implies the desired result.  
\end{proof}

Using (i) and (ii) of the above lemma, it suffices to characterize the critical points and establish the error bound for Problem \eqref{eq:G} in the rest of this section. Moreover, using (iii) of the above lemma, we assume without loss of generality that 
\begin{align}\label{eq:Y}
\bm Y = \mathrm{BlkDiag}\left(\tilde{\bm \Sigma}_Y,\bm 0_{(d_L-d_{\min}) \times (d_0-d_{\min})}\right) \in \R^{d_L\times d_0},
\end{align}
where $\tilde{\bm \Sigma}_Y = \mathrm{diag}(y_1,\dots,y_{d_{\min}})$ with $y_1\ge y_2 \ge \dots \ge y_{d_{\min}} \ge 0$ being singular values. According to \eqref{eq:SY1}, we write 
\begin{align}\label{eq:SY}
\tilde{\bm \Sigma}_Y = \mathrm{BlkDiag}\left(y_{s_{1}}\bm I_{h_1}, \dots, y_{s_{p_Y}}\bm I_{h_{p_Y}}, \bm 0_{d_{\min}-r_Y}\right) \in \R^{d_{\min}\times d_{\min}}. 
\end{align} 
Moreover, we define
\begin{align}\label{eq:delta y}
\delta_y := \min \left\{|y_{s_i}-y_{s_{i+1}}|: i\in [p_Y] \right\}. 
\end{align} 
Throughout this section, we will consistently use the above notation in all proofs. 

\subsection{Analysis of the Set of Critical Points}\label{subsec:pf opti}

In this subsection, we focus on characterizing all critical points of Problem \eqref{eq:F} explicitly by studying the critical points of Problem \eqref{eq:G}. To begin, we present a lemma to show that the weight matrices $\{\bm W_l\}_{l=1}^L$ at any critical point of Problem \eqref{eq:G} are balanced. 

\begin{lemma}\label{lem:bala}
Let $(\bm W_1,\dots,\bm W_L)$ be a critical point of Problem \eqref{eq:G}. The following statements hold: \\
(i) It holds that 
\begin{align}\label{eq:bala}
\bm W_l\bm W_l^T = \bm W_{l+1}^T\bm W_{l+1},\ \forall l \in [L-1].
\end{align}
(ii) It holds that  
\begin{align}\label{eq:crit}
	(\bm W_l \bm W^T_l)^{L-1} \bm W_l - \sqrt{\lambda}\bm W_{L:l+1}^T \bm Y \bm W_{l-1:1}^T + \lambda \bm W_l=0, \ \forall l \in [L].
\end{align}
\end{lemma}

The proof of this lemma is deferred to \Cref{subsec:pf crit}. Recall the notions in \eqref{eq:SY1}, \eqref{eq:Y}, and \eqref{eq:SY}. Leveraging \Cref{lem:equi FG} and \Cref{lem:bala}, we are ready to characterize the set of critical points \eqref{set:crit G} as follows. 

\begin{prop}\label{prop:opti G}
Suppose that \Cref{AS:1} holds and $\bm Y \in \R^{d_L\times d_0}$ takes the form of \eqref{eq:Y}. The critical point set \eqref{set:crit G} of Problem \eqref{eq:G} can be expressed as 
\begin{align}\label{set:WG}
\mathcal{W}_{G} = \left\{
\bm W :
\begin{array}{l}
\bm \Sigma_l = \mathrm{BlkDiag}\left(\mathrm{diag}( \bm \sigma), \bm 0\right) \in \R^{d_l\times d_{l-1}},\ \forall l \in [L],\ (\bm\sigma,\bm \Pi)\in  \mathcal{B},\\ 
\bm W_1 = \bm Q_2\bm \Sigma_1 \mathrm{BlkDiag}\left(\bm \Pi, \bm I  \right)\mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_{Y}},\bm O_{p_{Y}+1} \right), \\ 
\bm W_l = \bm Q_{l+1} \bm \Sigma_l \bm Q_l^T,\ l=2,\dots,L-1,\ \bm Q_l \in \mathcal{O}^{d_{l-1}},\ l=2,\dots,L,  \\
\bm W_L =  \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\widehat{\bm O}_{p_{Y}+1}^T \right)\mathrm{BlkDiag}\left(\bm \Pi^T, \bm I \right)\bm \Sigma_L \bm Q_L^T, \\
\bm O_i \in \mathcal{O}^{h_i},\ \forall i \in [p_{Y}],\  \bm O_{p_{Y}+1} \in \mathcal{O}^{d_0 - r_{Y}},\ \widehat{\bm O}_{p_{Y}+1}  \in \mathcal{O}^{d_L - r_{Y}} 
\end{array} 
\right\}, 
\end{align}
where 
\begin{align}\label{set:A}
&\mathcal{A} := \left\{ \bm \sigma \in \R^{d_{\min}}: \sigma_i^{2L-1} - \sqrt{\lambda}y_i\sigma_i^{L-1} + \lambda\sigma_i = 0,\ \sigma_i \geq 0,\ \forall i \in [d_{\min}] \right\}, \\
& \mathcal{B} := \left\{ (\bm \sigma, \bm \Pi)  \in \R^{d_{\min}} \times \mathcal{P}^{d_{\min}}: \bm \sigma = \bm \Pi \bm a,\ 
\bm a \in \mathcal{A},\ \sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_{d_{\min}}  \right\}.\label{set:B}
\end{align}
\end{prop}

The proof of this proposition is deferred to \Cref{subsec:pf crit}. Notably, each $\bm W_l$ in \eqref{set:WG} is represented in an SVD form with singular values selected from $\cal A$ up to a permutation determined by $\cal B$. Based on this proposition, we can further simplify the above structure of the critical point set by removing the permutation in \eqref{set:WG}. 

\begin{thm}\label{thm:opti G}
Suppose that \Cref{AS:1} holds and $\bm Y \in \R^{d_L\times d_0}$ takes the form of \eqref{eq:Y}. Then, the critical point set \eqref{set:crit G} of Problem \eqref{eq:G} can be expressed as 
\begin{align}\label{set:WG1}
\mathcal{W}_G = \left\{
\bm W :
\begin{array}{l}
\bm \Sigma_l = \mathrm{BlkDiag}\left(\mathrm{diag}(\bm \sigma), \bm 0\right) \in \R^{d_l\times d_{l-1}},\ \forall l \in [L],\ \bm\sigma\in \mathcal{A},\\
\bm W_1 = \bm Q_2\bm \Sigma_1 \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\bm O_{p_Y+1} \right), \\ 
\bm W_l = \bm Q_{l+1} \bm \Sigma_l \bm Q_l^T,\ l=2,\dots,L-1,\ \bm Q_l \in \mathcal{O}^{d_{l-1}},\ l=2,\dots,L,  \\
\bm W_L =  \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_Y}^T,\widehat{\bm O}_{p_Y+1}^T \right) \bm \Sigma_L \bm Q_L^T, \\
\bm O_i \in \mathcal{O}^{h_i},\ \forall i \in [p_Y],\ \bm O_{p_Y+1} \in \mathcal{O}^{d_0 - r_{Y}},\ \widehat{\bm O}_{p_Y+1} \in \mathcal{O}^{d_L - r_{Y}}
\end{array}\right\}, 
\end{align}
where $\cal A$ is defined in \eqref{set:A}. 
\end{thm}
\begin{proof}
For ease of exposition, we denote the set on the right-hand side of \eqref{set:WG1} by $\cal X$. Let $(\bm W_1,\dots,\bm W_L) \in \mathcal{W}_G$ be arbitrary. It follows from \Cref{prop:opti G} that there exists $(\bm \sigma,\bm \Pi) \in \cal B$ such that $(\bm W_1,\dots,\bm W_L)$ takes the form of \eqref{set:WG}. According to \eqref{set:A} and \eqref{set:B}, there exists $\bm a \in \cal A$ such that $\bm \Pi \mathrm{diag}(\bm a) \bm \Pi^T = \mathrm{diag}(\bm \sigma)$. Then, let
\begin{align*}
    &\bm \Sigma_l^\prime := \blk(\bm \Pi^T , \bm I_{d_l - d_{\min}})\bm \Sigma_l \blk(\bm \Pi , \bm I_{d_{l-1} - d_{\min}}) = \blk\left(\diag(\bm a), \bm 0 \right),\ \forall l \in [L],\\
    &\bm Q_l^\prime := \bm Q_l \blk(\bm \Pi , \bm I_{d_{l-1} - d_{\min}}) \in \mathcal{O}^{d_{l-1}},\ \forall l \in [L].
\end{align*}
Therefore, we have $(\bm W_1,\dots,\bm W_L) \in \mathcal{X}$. 

Conversely, let $(\bm W_1,\dots,\bm W_L) \in \mathcal{X}$ be arbitrary. There exists $\bm \sigma \in \mathcal{A}$ such that $(\bm W_1,\dots,\bm W_L)$ takes the form of \eqref{set:WG1}. Then, we choose a permutation matrix $\bm \Pi \in \mathcal{P}^{d_{\min}}$ such that
$
     \bm\sigma^\prime = \bm \Pi \bm \sigma\ \text{satisfies}\  \sigma^\prime_1 \geq  \sigma^\prime_2 \ge \dots \ge \sigma^\prime_{d_{\min}}. 
$ 
Let 
\begin{align*}
    & \bm \Sigma^\prime_l := \mathrm{BlkDiag}(\bm \Pi , \bm I_{d_l - d_{\min}})\bm \Sigma_l \mathrm{BlkDiag}(\bm \Pi^T , \bm I_{d_{l-1} - d_{\min}}) = \blk\left(\diag(\bm \sigma^\prime), \bm 0 \right),\ \forall l \in [L],\\
    &\bm Q^\prime_l := \bm Q_l \mathrm{BlkDiag}(\bm \Pi^T, \bm I_{d_{l-1} - d_{\min}}) ,\ \forall l \in [L].
\end{align*}
This, together with \eqref{set:WG1}, implies $(\bm W_1,\dots,\bm W_L) \in \mathcal{W}_G$. Then, we complete the proof. 
\end{proof} 

Using \Cref{lem:equi FG} and \Cref{thm:opti G}, we can directly characterize the critical point set \eqref{set:crit F} of Problem \eqref{eq:F}, i.e., \Cref{thm:opti}, as follows.  

\begin{proof}[Proof of \Cref{thm:opti}]
   Note that $y_i=0$ for each $i=r_Y+1,\dots,d_{\min}$. This, together with  \eqref{set:A} and $\lambda > 0$, yields  $\sigma_i=0$ for each $i=r_Y+1,\dots,d_{\min}$. Using this, \eqref{eq:SVD Y}, \Cref{lem:equi FG}, and \Cref{thm:opti G}, we directly obtain \Cref{thm:opti}.  
\end{proof}


Notably, \Cref{prop:opti G} demonstrates that when $\bm Y$ takes the form of \eqref{eq:Y}, the critical point set \eqref{set:crit G} of Problem \eqref{eq:G} can be expressed as 
\begin{align}
%\label{eq:set G}
\notag
\mathcal{W}_G = \bigcup_{(\bm \sigma, \bm \Pi) \in \mathcal{B}} \mathcal{W}_{\bm \sigma, \bm \Pi}, 
\end{align}
where 
\begin{align}\label{set:G2}
\mathcal{W}_{\bm \sigma, \bm \Pi} := \left\{
\bm W :
\begin{array}{l}
\bm \Sigma_l = \mathrm{BlkDiag}\left(\mathrm{diag}( \bm \sigma), \bm 0_{(d_l-d_{\min}\times (d_{l-1}-d_{\min})}\right) \in \R^{d_l\times d_{l-1}},\ \forall l \in [L],\\
\bm W_1 = \bm Q_2\bm \Sigma_1 \mathrm{BlkDiag}\left(\bm \Pi, \bm I  \right)\mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_{Y}},\bm O_{p_{Y}+1} \right), \\ 
\bm W_l = \bm Q_{l+1} \bm \Sigma_l \bm Q_l^T,\ l=2,\dots,L-1,\ \bm Q_l \in \mathcal{O}^{d_{l-1}},\ l=2,\dots,L,  \\
\bm W_L =  \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\widehat{\bm O}_{p_{Y}+1}^T \right)\mathrm{BlkDiag}\left(\bm \Pi^T, \bm I \right)\bm \Sigma_L \bm Q_L^T, \\
\bm O_i \in \mathcal{O}^{h_i},\ \forall i \in [p_{Y}],\ \bm O_{p_{Y}+1} \in \mathcal{O}^{d_0 - r_{Y}},\ \widehat{\bm O}_{p_{Y}+1} \in \mathcal{O}^{d_L - r_{Y}}
\end{array} 
\right\}. 
\end{align} 
Let 
\begin{align}\label{set:Y}
\mathcal{Y} := \bigcup_{ i \in [d_{\min}]}\left\{ \sigma \ge 0:  \sigma^{2L-1} + \lambda\sigma - \sqrt{\lambda} y_i\sigma^{L-1} = 0  \right\}. 
\end{align} 
Now, we define 
\begin{align}\label{eq:delta sigma}
\delta_{\sigma} := \min\left\{|x-y|: x \neq y \in \mathcal{Y}  \right\}. 
\end{align}
The following result elucidates the structure of the collection $\{\mathcal{W}_{\bm \sigma, \bm \Pi}\}_{(\bm \sigma, \bm \Pi) \in \mathcal{B}}$. 
\begin{prop}\label{prop:set W}
Let $(\bm \sigma, \bm \Pi) \in \mathcal{B}$ and $(\bm \sigma^\prime, \bm \Pi^\prime) \in \mathcal{B}$ be arbitrary. The following statements holds: \\
(i) It holds that $\mathcal{W}_{\bm \sigma, \bm \Pi} = \mathcal{W}_{\bm \sigma^\prime, \bm \Pi^\prime}$ if and only if $\bm \sigma = \bm \sigma^\prime$. \\
(ii) If $\bm \sigma \neq \bm \sigma^\prime$, it holds that 
\begin{align}\label{eq:dist W}
\mathrm{dist}\left( \mathcal{W}_{\bm \sigma, \bm \Pi}, \mathcal{W}_{\bm \sigma^\prime, \bm \Pi^\prime} \right) \ge \delta_{\sigma}. 
\end{align}
\end{prop} 
The proof of this proposition is deferred to \Cref{subsec:pf crit}. This proposition demonstrates that for any pair $(\bm \sigma, \bm \Pi) \in \mathcal{B}$ and $(\bm \sigma^\prime, \bm \Pi^\prime) \in \mathcal{B}$, if $\bm \sigma \neq \bm \sigma^\prime$, $\mathcal{W}_{\bm\sigma, \bm\Pi}$ is well separated from $\mathcal{W}_{\bm\sigma^\prime, \bm\Pi^\prime}$, and otherwise they are identical. Therefore, for simplicity we write $\mathcal{W}_{\bm \sigma} := \mathcal{W}_{\bm \sigma, \bm \Pi}$ for any $\bm \Pi$ satisfying $(\bm \sigma, \bm \Pi) \in \mathcal{B}$. Therefore, one can express the set of critical points \eqref{set:crit G} of Problem \eqref{eq:G} as follows:  
\begin{align}\label{set:W sigma}
\mathcal{W}_G := \bigcup_{\bm \sigma \in  \mathcal{A}_{\rm sort}} \mathcal{W}_{\bm \sigma },\ \text{where}\ \mathcal{A}_{\rm sort} := \left\{ \bm \sigma \in \R^{d_{\min}}: (\bm \sigma, \bm \Pi) \in  \mathcal{B} \right\}.  
\end{align} 

\subsection{Analysis of the Error Bound}\label{subsec:ana eb}

According to \Cref{prop:set W} and \eqref{set:W sigma}, for any $\bm W$ satisfying $\mathrm{dist}(\bm W, \mathcal{W}_G) \le \delta_{\sigma}/2$, there exists a $\bm \sigma^* \in \mathcal{A}_{\rm sort}$ such that 
\begin{align*}
\mathrm{dist}(\bm W, \mathcal{W}_G) = \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}). 
\end{align*}
This observation simplifies our analysis, as it suffices to bound $\mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*})$ for each $\bm \sigma^* \in \mathcal{A}_{\rm sort}$. Note that $\bm 0 \in \mathcal{A}_{\rm sort}$. For this case, we directly show the error bound as follows:
\begin{prop}\label{prop:eb zero}
Suppose that \Cref{AS:1} holds. Let $ \mathcal{A}_{\rm sort}$ be defined in \eqref{set:W sigma} and consider $\bm\sigma^* = \bm 0 \in \mathcal{A}_{\rm sort}$. The following statements hold: \\
(i) Suppose that $L=2$ and \Cref{AS:2} holds. For all $\bm W$ satisfying
\begin{align}\label{eq:dist 6}
\mathrm{dist}(\bm W, \mathcal{W}_{\bm 0}) \le \left( \frac{\sqrt{\lambda}}{2(\sqrt{\lambda}+y_1)} \min\left\{ \min_{i \in [s_{p_Y}]}\left| \lambda - y_i^2 \right|, \lambda \right\} \right)^{1/2}, 
\end{align}
it holds that 
\begin{align}\label{eq:eb zero 1}
\mathrm{dist}(\bm W,\mathcal{W}_{\bm 0}) \le \frac{2(\sqrt{\lambda}+y_1)}{\sqrt{\lambda} \min\left\{ \min_{i \in [s_{p_Y}]}\left| \lambda - y_i^2 \right|, \lambda \right\} } \left\| \nabla G(\bm W)\right\|_F.  
\end{align} 
(ii) Suppose that $L \ge 3$.  For all $\bm W$ satisfying
\begin{align}\label{eq:dist 0}
\mathrm{dist}(\bm W, \mathcal{W}_{\bm 0}) \le \min\left\{ \left( \frac{\lambda}{3} \right)^{\frac{1}{2L-2}}, \left( \frac{\sqrt{\lambda}}{3y_1} \right)^{\frac{1}{L-2}}\right\}, 
\end{align}
it holds that 
\begin{align}\label{eq:eb zero}
\mathrm{dist}(\bm W,\mathcal{W}_{\bm 0}) \le \frac{3\sqrt{L}}{2\lambda} \left\| \nabla G(\bm W)\right\|_F.  
\end{align}
\end{prop}
We defer the proof of this proposition to \Cref{app:sec A}. According to this proposition, we assume without loss of generality $\bm \sigma^* \neq \bm 0$ from now on. Let  $\bm 0 \neq \bm \sigma^* \in \mathcal{A}_{\rm sort}$ be arbitrary and we define 
\begin{align}\label{def:r sigma}
\sigma_{\min}^* := \min\left\{\sigma_i^* \neq 0: i \in [d_{\min}] \right\},\ \sigma_{\max}^* := \max\left\{\sigma_i^*: i \in [d_{\min}] \right\},\ r_{\sigma} := \|\bm \sigma^*\|_0, 
\end{align} 
where $\|\bm \sigma^*\|_0$ denotes the number of nonzero entries in $\bm\sigma$. From the definition of \(\delta_\sigma\) in \eqref{eq:delta sigma}, we have \(\delta_\sigma \leq \sigma^*_{\min}\). Now, we present a lemma and a corollary that establish some spectral properties of a point in the neighborhood of the set of critical points. Their proofs are provided in \Cref{subsec:pf error}.
 
\begin{lemma}\label{lem:pre}
    Let $\bm W = (\bm W_1,\dots,\bm W_L)$ and $\bm 0 \neq  \bm \sigma^* \in \mathcal{A}_{\rm sort}$ be arbitrary such that  
    \begin{align}\label{eq:dist 1}
        \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) <  \frac{\sigma^*_{\min}}{2}. 
    \end{align}
    The following statements hold: \\
    (i) It holds that
    \begin{align}
       % & \frac{\|\bm \sigma^*\| }{2} \le \|\bm W_l\|_F \le \frac{3\|\bm \sigma^*\| }{2},\ \forall l \in [L], \label{eq:F norm W} \\
       & \frac{\sigma_{\max}^*}{2}  \le \|\bm W_l\| \le \frac{3\sigma_{\max}^*}{2},\ \forall l \in [L], \label{eq:S norm W} \\
       & \sigma_{i}(\bm W_l) \ge \frac{\sigma_{\min}^*}{2},\ \forall l \in [L],\ i \in [r_{\sigma}]. \label{eq:sigmalowerbound}
    \end{align}
    (ii) It holds that
    \begin{align}\label{eq:balance}
        \left\|\bm W_{l+1}^T \bm W_{l+1} - \bm W_l\bm W_l^T\right\|_F \le \frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda}  \|\nabla G(\bm W)\|_F,\ \forall l \in [L-1]. 
    \end{align}
    (iii) It holds that
    \begin{align}\label{eq:wl+1}
        \left|\sigma_i (\bm W_l) - \sigma_i(\bm W_{l+1})\right| \le \frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda\sigma_{\min}^*} \|\nabla G(\bm W)\|_F,\ \forall l \in [L-1],\ i \in [r_{\sigma}]. 
    \end{align}
    (iv) It holds that 
    \begin{align}
        & \left\|\bm W_{j:i}^T \bm W_{j:i}  -  (\bm W^T_i\bm W_i)^{j-i+1}\right\| \le \frac{(j-i)(j-i+1)}{2\sqrt{2}\lambda}\left(\frac{3\sigma_{\max}^*}{2}\right)^{2j-2i+1}\|\nabla G(\bm W)\|_F,\ \forall i \leq j, \label{eq:transwji}\\
        &  \left\|\bm W_{i:j} \bm W_{i:j}^T - (\bm W_i\bm W_i^T)^{i-j+1}\right\| \le \frac{(i-j)(i-j+1)}{2\sqrt{2}\lambda}\left(\frac{3\sigma_{\max}^*}{2}\right)^{2i-2j+1}\|\nabla G(\bm W)\|_F,\ \forall i \geq j. \label{eq:transwij}
    \end{align} 
\end{lemma}  

According to \Cref{lem:bala}, each critical point satisfies \eqref{eq:crit}. Leveraging this observation and \Cref{lem:pre}, we can further show that any point in the neighborhood of the critical point set approximately satisfies \eqref{eq:crit} with the deviation bounded by its gradient norm.  

\begin{coro}\label{coro1}
Consider the setting in \Cref{lem:pre}. It holds for each $l \in [L]$ that 
\begin{align}\label{eq:coro1}
\left\| (\bm W_l \bm W^T_l)^{L-1} \bm W_l - \sqrt{\lambda}\bm W_{L:l+1}^T \bm Y \bm W_{l-1:1}^T + \lambda \bm W_l\right\|_F \le c_1 \|\nabla G(\bm W)\|_F, 
\end{align}
where 
\begin{align}\label{eq:c1}
c_1 := \max_{l\in [L]} \left\{ \left(\frac{3\sigma^*_{\max}}{2}\right)^{2L-2}\frac{(L-l)(L-l+1)+(l-1)l}{2\sqrt{2}\lambda}+\frac{1}{2} \right\}. 
\end{align} 
\end{coro}

To handle the repeated singular values in $\bm \sigma^* \in \mathcal{A}_{\rm sort}$, let $p \ge 1$ be the number of distinct positive singular values. Recall from \eqref{def:r sigma} that $r_{\sigma}$ denotes the number of positive singular values of $\bm \sigma^*$. Then there exist indices $t_0,t_1,\cdots,t_{p}$ such that 
$0 = t_0 < t_1<\cdots < t_p = r_{\sigma} $ and 
\begin{equation}\label{eq:partsigma}
\sigma_{t_0+1}^* =\dots = \sigma_{t_1}^* >\sigma_{t_1+1}^*=\dots =\sigma_{t_2}^* > \dots>\sigma_{t_{p-1}+1}^*=\dots =\sigma_{t_{p}}^*>0.
\end{equation} 
Let $g_i := t_i - t_{i-1}$ be the multiplicity of the $i$-th largest positive value of $\bm \sigma^*$ for each $i\in [p]$ and $g_{\max} := \max\{g_i:i \in [p]\}$. Obviously, we have $r_{\sigma}= \sum_{i=1}^p g_i$ . Moreover, let 
\begin{align}\label{eq:SVD Wl}
\bW_l = \bU_l\bm{\Sigma}_l\bV_l^T = \begin{bmatrix}
    \bm U_{l}^{(1)} & \dots &  \bm U_{l}^{(p)} & \bm U_{l}^{(p+1)} 
\end{bmatrix}\begin{bmatrix}
\bm{\Sigma}_{l}^{(1)} & \dots &  \bm{0} & \bm 0 \\
\vdots & \ddots & \vdots &  \vdots \\
\bm{0} & \dots & \bm{\Sigma}_{l}^{(p)} &  \bm 0 \\
\bm 0 & \bm 0 & \dots & \bm{\Sigma}_{l}^{(p+1)}
\end{bmatrix}\begin{bmatrix}
        \bm V_{l}^{(1)^T}\\ \vdots \\  \bm V_{l}^{(p)^T} \\ \bm V_{l}^{(p+1)^T} 
\end{bmatrix} 
\end{align}   
be an SVD of $\bm W_l$ for each $l \in [L]$, where $\bm U_l \in \mathcal{O}^{d_l}$ with $\bm U_{l}^{(i)} \in \R^{d_l \times g_i}$ for each $i \in [p]$ and $\bm U_{l}^{(p+1)} \in \R^{d_l \times (d_l-r_{\sigma})}$, $\bm \Sigma_l \in \mathbb{R}^{d_l\times d_{l-1}}$ with decreasing singular values, $\bm \Sigma_{l}^{(i)} \in \R^{g_i \times g_i}$ for each $i \in [p]$, and $\bm \Sigma_{l}^{(p+1)} \in \R^{(d_l-r_{\sigma})  \times (d_{l-1}-r_{\sigma})}$, and $\bm V_l \in \mathcal{O}^{d_{l-1}}$ with $\bm V_{l}^{(i)} \in \R^{d_{l-1} \times g_i}$ for each $i \in [p]$ and $\bm V_{l}^{(p+1)} \in \R^{d_{l-1} \times (d_{l-1} - r_{\sigma})}$. 

With the above setup, we are ready to show that, for any point in the neighborhood of the set of critical points, the product of $\bm U_{l-1}$ and $\bm V_{l}$ is close to a block diagonal matrix, where the diagonal blocks are orthogonal matrices. 
\begin{prop}\label{prop:Hlapproximate}
Let $\bm W = (\bm W_1,\dots,\bm W_L)$ and $\bm 0 \neq  
 \bm \sigma^* \in \mathcal{A}_{\rm sort}$ be arbitrary such that 
\begin{align}\label{eq:dist 2}
    \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) \le   \frac{\delta_{\sigma}}{3}. 
\end{align}  
For each $l \in \{2,\dots,L\}$, there exist matrices $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$ for all $i \in [p]$ and $\bm T_{l}^{(p+1)} \in \mathcal{O}^{d_{l-1} - r_{\sigma}}$ such that
\begin{align}\label{eq:Hlapproximate}
\left\| \bm U_{l-1}^T\bm V_l - \mathrm{BlkDiag}\left(\bm T_{l}^{(1)},\dots,\bm T_{l}^{(p)},\bm T_{l}^{(p+1)} \right)\right\|_F \le \frac{9\sigma_{\max}^{*}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}}\|\nabla G(\bm W)\|_F. 
\end{align}
where $\bm U_l$ and $\bm V_l$ for each $l \in [L]$ are defined in \eqref{eq:SVD Wl}. 
\end{prop} 
The proof of this proposition is deferred to \Cref{subsec:pf error}. Next, using the above proposition, we show that for weight matrices in the neighborhood of the set of critical points, the singular matrices $\bm \Sigma_1$ and $\bm \Sigma_L$ satisfy the following spectral inequalities. 
\begin{lemma}\label{lem:twoinequality}
Consider the setting of \Cref{lem:pre}. Suppose that there exist matrices $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$ for all $i \in [p]$ and $\bm T_{l}^{(p+1)} \in \mathcal{O}^{d_{l-1} - r_{\sigma}}$ such that \eqref{eq:Hlapproximate} holds for all $l=2,\dots,L$. Then, we have 
\begin{align}
& \left\|(\bm \Sigma_1\bm \Sigma_1^T)^{L-1}\bm \Sigma_1 + \lambda \bm \Sigma_1 - \sqrt{\lambda}\mathrm{BlkDiag}\left(\bm A_1, \dots, \bm A_{p}, \bm A_{p+1}\right)\bm U_L^T \bm Y \bm V_1  \right\|_F \leq c_2 \|\nabla G(\bm W)\|_F, \label{eq:twoinequality2} \\
& \left\|(\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L - \sqrt{\lambda}\bm U_L^T \bm Y \bm V_1 \mathrm{BlkDiag}\left(\bm B_1, \dots, \bm B_{p}, \bm B_{p+1}\right) \right\|_F \leq c_2 \|\nabla G(\bm W)\|_F,\label{eq:twoinequality1} 
\end{align} 
where $c_1$ is defined in \eqref{eq:c1} and 
\begin{align} 
& \bm A_i := \left(\prod_{l=1}^{L-1} \bm T_{l+1}^{(i)}\right) \left(\bm \Sigma_{1}^{(i)}\right)^{L-1},\ \forall i \in [p],\ \bm A_{p+1} :=  \prod_{l=1}^{L-1}  \bm T_{l+1}^{(p+1)}\bm \Sigma_{l+1}^{(p+1)^T},\label{eq:A} \\
& \bm B_i := \left( \prod_{l=1}^{L-1} \bm T_{l+1}^{(i)} \right) \left(\bm \Sigma_{L}^{(i)}\right)^{L-1},\ \forall i \in [p],\ \bm B_{p+1} :=  \prod_{l=1}^{L-1}  \bm \Sigma_{l}^{(p+1)^T} \bm T_{l+1}^{(p+1)}, \label{eq:B}\\
& \eta_1 := \frac{\sigma_{\max}^*}{\sigma_{\min}^*}\left(  \frac{3\sqrt{2}\sigma_{\min}^*}{4\lambda} +   \frac{81\sigma_{\max}^{*{2}}}{8\delta_{\sigma}\lambda} +    \frac{9\sqrt{2g_{\max}} L\sigma_{\max}^{*}}{4\lambda} \right), \label{eq:eta1}\\
& c_2 := \left( \frac{3}{2}\sigma_{\max}^* \right)^{L}\frac{3y_1L}{2\sqrt{\lambda}\delta_{\sigma} \sigma^{*}_{\min}} + c_1+ y_1 p \sqrt{\lambda} \left(\frac{3\sigma_{\max}^*}{2}\right)^{L-2}\left(\frac{L^2\eta_1 }{2\sigma^*_{\min}}+  \frac{3\sqrt{2g_{\max}}L^2\sigma_{\max}^*}{2\lambda\sigma_{\min}^*} \right). \label{eq:c2}
\end{align}
\end{lemma} 
The proof of this lemma is deferred to \Cref{subsec:pf error}. Now, we bound the singular values and the associated singular vectors of weight matrices by the gradient norm when they lie in the neighborhood of the critical point set. Notably, according to $\lambda=\lambda_1\cdots\lambda_L$ in \eqref{eq:G}, we obtain that when $L=2$, \eqref{eq:AS L=2} in \Cref{AS:2} is equivalent to  
\begin{align}
\lambda \neq y_i^2,\ \forall  i \in [p_Y]. \label{eq:AS lambda}
\end{align}
When $L \ge 3$, \eqref{eq:AS y} in \Cref{AS:2} is equivalent to  
\begin{align}
\lambda \neq y_i^{2(L-1)}\left( \left( \frac{L-2}{L} \right)^{\frac{L}{2(L-1)}} + \left( \frac{L}{L-2} \right)^{\frac{L-2}{2(L-1)}} \right)^{-2(L-1)},\ \forall  i \in [p_Y].    \label{eq:AS lambda1}
\end{align} 
 
\begin{prop}\label{prop:singular value 2}
Let $\bm 0 \neq \bm \sigma^* \in \mathcal{A}_{\rm sort}$ be arbitrary. The following statements hold: \\
(i) Suppose that $L=2$ and \eqref{eq:AS lambda} holds. Then for all $\bm W$ satisfying 
\begin{subequations}\label{eq:dist grad 1}
\begin{align}
    &\mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) \le \min\left\{ \frac{\delta_{\sigma}}{3},  \frac{\sqrt{\lambda}}{\sqrt{3(\sqrt{\lambda}+y_1)}}\left(
    \min\left\{ \min_{i \in [s_{p_Y}]}\left| \sqrt{\lambda} - y_i \right|, \sqrt{\lambda} \right\} \right)^{\frac{1}{2}}\right\}, \label{eq:dist 7}\\
    &\|\nabla G(\bm W)\|_F \le \frac{ \sqrt{2\lambda} \min\left\{ \min_{i \in [s_{p_Y}]}\left| \sqrt{\lambda} - y_i \right|, \sqrt{\lambda} \right\} \sigma^*_{\min} }{12c_2} , \label{eq:grad 2}
\end{align}
\end{subequations}
it holds for $l=1,2$ that
\begin{align}\label{eq:prop singular}
\sigma_i(\bm W_l) \le c_3   \|\nabla G(\bm W)\|_F,\  \forall i =  r_{\sigma}+1,\dots,\min\{d_l,d_{l-1}\},  
\end{align}
where
\begin{align*}
c_3 := \frac{6c_2(y_1+\sqrt{\lambda})}{\lambda \min\left\{ \min_{i \in [s_{p_Y}]}\left| \sqrt{\lambda} - y_i \right|, \sqrt{\lambda} \right\}}.
\end{align*} 
(ii) Suppose that $L=3$ and \eqref{eq:AS lambda1} holds.Then for all $\bm W$ satisfying
\begin{align}\label{eq:dist 5}
    \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) \le \min\left\{ \frac{\delta_{\sigma}}{3}, \left( \frac{\sqrt{\lambda}}{2y_1} \right)^{1/(L-2)} \right\}, 
\end{align}  
it holds for all $l \in [L]$ that  
\begin{align}\label{eq:singular 2}
\sigma_i(\bm W_l) \le c_3\|\nabla G(\bm W)\|_F,\ \forall i =  r_{\sigma}+1,\dots,\min\{d_l,d_{l-1}\}, 
\end{align}
where  
% \begin{align}\label{eq:c3}
% c_3 := \frac{2}{\lambda}\left( c_1 +  \left( \frac{3}{2}\sigma_{\max}^* \right)^{L}\frac{3(L-1)y_1}{ 2\delta_{\sigma}\sqrt{\lambda} \sigma^{*}_{\min}} \right) .
% \end{align}
\begin{align}\label{eq:c3}
\eta_2 := c_1 +  \left( \frac{3}{2}\sigma_{\max}^* \right)^{L}\frac{3(L-1)y_1}{ 2\delta_{\sigma}\sqrt{\lambda} \sigma^{*}_{\min}}\quad \text{and}\quad c_3 := \frac{2\eta_2}{\lambda}.
\end{align}
\end{prop}
The proof of this proposition is deferred to \Cref{subsec:pf error}. The above proposition bounds the smallest $\min\{d_l,d_{l-1}\} - r_{\sigma}$  singular values by the gradient norm. Next, we proceed to bound the leading $r_{\sigma}$ singular values and all singular vectors by the gradient norm. For ease of exposition, we introduce an auxiliary function as follows: 
\begin{align}\label{eq:phi}
\varphi(x) := \frac{x^{2L-1} + \lambda x}{\sqrt{\lambda} x^{L-1}},\ \forall x \neq 0. 
\end{align} 
\begin{prop}\label{prop:singular control}
Let $\bm 0 \neq \bm \sigma^* \in \mathcal{A}_{\rm sort}$ be arbitrary and $\bm{W}$ be arbitrary such that 
\begin{subequations}\label{eq:dist grad 2}
\begin{align}
    &\mathrm{dist}(\bm{W}, \mathcal{W}_{\bm{\sigma}^*}) \le \min\{\delta_1, \delta_2\}, \label{eq:dist_condition}\\
    &\|\nabla G(\bm{W})\|_F \leq \frac{\delta_y\sqrt{\lambda} (\sigma^{*}_{\min})^{L-1}}{3 \cdot 2^{L-1} \sqrt{\eta_3^2 + \eta_4^2}}, \label{eq:gradient_condition}
\end{align}      
\end{subequations}
where \( \eta_3, \eta_4 \) are respectively defined in \eqref{eq:eta 3} and \eqref{eq0:prop_sing}, and \( \delta_1, \delta_2 \) are respectively described in \eqref{eq13:prop sing} and \eqref{eq15:prop sing}.
Suppose in addition \Cref{AS:1} holds, \eqref{eq:AS lambda} and \eqref{eq:dist grad 1} hold for $L=2$, and \eqref{eq:AS lambda1} and \eqref{eq:dist 5} hold for $L \ge 3$. 
Then the following statements hold: \\
(i) There exist orthogonal matrices $\hat{\bm U}_{L}^{(i)} \in \mathcal{O}^{h_i}$ for each $i \in [p_Y]$, $\hat{\bm U}_{L}^{(p_Y+1)} \in \mathcal{O}^{d_L-r_Y}$, $\hat{\bm V}_{1}^{(p_Y+1)} \in \mathcal{O}^{d_0-r_Y}$, $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$ for each $i \in [p]$, $\bm P \in \mathcal{O}^{d_L - r_{\sigma}}$, $\bm Q \in \mathcal{O}^{d_0 - r_{\sigma}}$, and a permutation matrix $\bm \Pi \in \mathcal{P}^{d_{\min}}$ satisfying $(\bm \sigma^*, \bm \Pi^T) \in \mathcal{B}$ such that 
\begin{align}\label{eq:singular vectors}
  \|\tilde{\bm U}_L - \bm U_L \|_F \le c_4\|\nabla G(\bm W)\|_F,\ \|\tilde{\bm V}_1 - \bm V_1\|_F \le c_4\|\nabla G(\bm W)\|_F,
\end{align}
where $c_4 > 0$ is a positive constant and 
\begin{align} 
& \tilde{\bm U}_L := \mathrm{BlkDiag}\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)},\hat{\bm U}_{L}^{(p_Y+1)} \right)\mathrm{BlkDiag}\left(\bm \Pi, \bm I_{d_L-d_{\min}}\right)\mathrm{BlkDiag}\left(\bm I_{r_{\sigma}}, \bm P \right), \label{eq:UL}\\
& \tilde{\bm V}_1 := \mathrm{BlkDiag}\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)},\hat{\bm V}_{L}^{(p_Y+1)} \right)\mathrm{BlkDiag}\left(\bm \Pi, \bm I_{d_0-d_{\min}}\right)\mathrm{BlkDiag}\left(\bm I_{r_{\sigma}}, \bm Q \right) \hat{\bm T}, \label{eq:V1}\\
& \hat{\bm T} := \blk\left(\prod_{l=2}^L\bm T_{l}^{(1)},\cdots,\prod_{l=2}^L\bm T_{l}^{(p)},\bm I_{d_0-r_{\sigma}}\right).\label{eq:T}
\end{align}
(ii) For each $l \in [L]$, there exists a constant $c_5 > 0$ such that 
\begin{align}\label{eq:singular 1}
| \sigma_i\left( \bm W_l) - \sigma^*_{i} \right| \le c_5\|\nabla G(\bm W)\|_F,\ \forall i \in [r_{\sigma}].  
\end{align}
%In particular, when $L=2$, \eqref{eq:singular 2} holds if $\sqrt{\lambda} \neq y_{s_{\xi+1}}$.   
\end{prop}

Equipped with all the above lemmas and propositions, we are ready to prove the error bound of Problem \eqref{eq:G}. 

\begin{thm}\label{thm:eb G}
Suppose that Assumptions \ref{AS:1}  and \eqref{eq:AS lambda} (resp., \eqref{eq:AS lambda1}) hold for $L=2$ (resp., $L \ge 3$). Let $\bm W$ be arbitrary such that \eqref{eq:dist grad 2} and \eqref{eq:dist grad 1} (resp., \eqref{eq:dist 5}) hold for $L=2$ (resp., $L \ge 3$). It holds that 
\begin{align}
%\label{eq:eb G}
\notag
\mathrm{dist}(\bm W, \mathcal{W}_G) \le \sqrt{L} \left( \frac{9\sigma_{\max}^{*2}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}} + c_3\sqrt{d_{\max} - r_{\sigma}} +  c_4\sigma_{\max}^* + c_5\sqrt{r_{\sigma}} \right) \|\nabla G(\bm W)\|_F.
\end{align}
\end{thm} 
\begin{proof}
According to \eqref{eq:dist 7} or \eqref{eq:dist 5}, we have $\mathrm{dist}(\bm W, \mathcal{W}) \le \delta_{\sigma}/3$. This, together with \Cref{prop:Hlapproximate}, yields that there exist matrices $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$ for each $i \in [p]$ and $\bm T_{l}^{(p+1)} \in \mathcal{O}^{d_{l-1} - r_{\sigma}}$ such that \eqref{eq:Hlapproximate} holds. Moreover, it follows from \Cref{prop:singular value 2}, \Cref{prop:singular control}, we have \eqref{eq:singular 2}, \eqref{eq:singular vectors}, \eqref{eq:singular 1} hold, where $\tilde{\bm U}_L$, $\tilde{\bm V}_1$, and $\hat{\bm T}$ are respectively defined in \eqref{eq:UL}, \eqref{eq:V1}, and \eqref{eq:T}. Since we have $\mathrm{dist}(\bm W, \mathcal{W}_G) \le \delta_{\sigma}/3$, it follows from \Cref{prop:opti G} that there exists some $\bm \sigma^* \in \mathcal{A}_{\rm sort}$ such that 
\begin{align}
%\label{eq1:thm eb}
\notag
\mathrm{dist}(\bm W, \mathcal{W}_G) = \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}). 
\end{align} 
Recall that $\bm U_l \in \mathcal{O}^{d_l}$ and $\bm V_l \in \mathcal{O}^{d_{l-1}}$ for all $l \in [L]$ are introduced in \eqref{eq:SVD Wl}. For ease of exposition, we define $\bm \Sigma_l^* := \blk(\mathrm{diag}(\bm \sigma^*), \bm 0) \in \R^{d_l\times d_{l-1}}$ and 
\begin{align}\label{eq0:thm eb}
\hat{\bm W}_1 := \bm U_1\bm \Sigma_1^* \tilde{\bm V}_1^T,\ \hat{\bm W}_l := \bm U_l\bm \Sigma_l^*\bm V_l^T,\ l =2,\dots,L-1,\ \hat{\bm W}_L := \tilde{\bm U}_L\bm \Sigma_L^*\bm V_L^T. 
\end{align}
Now, we compute
\begin{align}\label{eq2:thm eb}
\|\hat{\bm W}_1 - \bm W_1\|_F & = \left\| \bm U_1\bm \Sigma_1^* \tilde{\bm V}_1^T -  \bm U_1\bm \Sigma_1 \bm V_1^T \right\|_F   \le  \left\| \bm U_1\bm \Sigma_1^* \tilde{\bm V}_1^T -  \bm U_1\bm \Sigma_1^* \bm V_1^T \right\|_F + \quad \notag\\
& \quad\ \left\|   \bm U_1\bm \Sigma_1^* \bm V_1^T -  \bm U_1\bm \Sigma_1 \bm V_1^T  \right\|_F \le \sigma_{\max}^* \|\tilde{\bm V}_1 - \bm V_1\|_F + \|  \bm \Sigma_1^* - \bm \Sigma_1\|_F \notag\\
& \le \left( c_4\sigma_{\max}^* + c_3\sqrt{d_{\max} - r_{\sigma}} + c_5\sqrt{r_{\sigma}} \right) \|\nabla G(\bm W)\|_F, 
\end{align}
where the last inequality uses  \eqref{eq:singular vectors},  \eqref{eq:singular 2}, and \eqref{eq:singular 1}. Using the same argument, we obtain 
\begin{align}\label{eq3:thm eb}
\|\hat{\bm W}_L - \bm W_L\|_F \le \left( c_4\sigma_{\max}^* + c_3\sqrt{d_{\max} - r_{\sigma}} + c_5\sqrt{r_{\sigma}} \right) \|\nabla G(\bm W)\|_F. 
\end{align} 
For all $l=2,\dots,L-1$, we compute
\begin{align*}
\|\hat{\bm W}_l - \bm W_l\|_F &  =  \left\|   \bm \Sigma_l^* - \bm \Sigma_l \right\|_F \le \left( c_3\sqrt{d_{\max} - r_{\sigma}} + c_5\sqrt{r_{\sigma}} \right) \|\nabla G(\bm W)\|_F,
\end{align*}
where the inequality uses  \eqref{eq:singular 2} and \eqref{eq:singular 1}. This, together with \eqref{eq2:thm eb} and \eqref{eq3:thm eb}, yields
\begin{align}\label{eq4:thm eb}
\|\bm W - \hat{\bm W}\|_F \le \sqrt{L} \left( c_4\sigma_{\max}^* + c_3\sqrt{d_{\max} - r_{\sigma}} + c_5\sqrt{r_{\sigma}} \right) \|\nabla G(\bm W)\|_F. 
\end{align}

Next, we further define 
\begin{align}
& \bm T_l := \mathrm{BlkDiag}(\bm T_{l}^{(1)},\dots,\bm T_{l}^{(p)},\bm T_{l}^{(p+1)}),\ l=2,\dots,L, \label{eq6:thm eb}\\
& \bm W_1^* := \bm Q_2 \bm \Sigma_1^* \hat{\bm T}^T \tilde{\bm V}_1^T,\ \bm W_l^* = \bm Q_{l+1} \bm \Sigma_l^*\bm Q_l^T,\ l =2,\dots,L-1,\ \bm W_L^* = \tilde{\bm U}_L\bm \Sigma_L^*\bm Q_L^T, \label{eq7:thm eb}
\end{align}
where 
\begin{align*}
\bm Q_l  := \bm V_l\mathrm{BlkDiag}\left(\left(\prod_{k=l+1}^L\bm T_{k}^{(1)}\right),\dots,\left(\prod_{k=l+1}^L\bm T_{k}^{(p)}\right) , \bm I_{d_{l-1}-r_{\sigma}}\right),\ l = 2,\dots,L-1,\ \bm Q_L := \bm V_L. 
\end{align*}
Using \Cref{prop:opti G}, one can verify that
\begin{align}\label{eq5:thm eb} 
\bm W^* = (\bm W_1^*,\bm W_2^*,\dots,\bm W_L^*) \in \mathcal{W}_{\bm \sigma^*}.
\end{align}
For each $l=2,\dots,L-2$, we compute
\begin{align}\label{eq8:thm eb}
\|\hat{\bm W}_l - \bm W_l^*\|_F & = \left\| \bm U_l\bm \Sigma_l^*\bm V_l^T - \bm Q_{l+1} \bm \Sigma_l^*\bm Q_l^T \right\|_F = \left\| \bm U_l\bm \Sigma_l^* - \bm Q_{l+1} \bm \Sigma_l^*\bm Q_l^T\bm V_l \right\|_F \notag\\
& = \left\| \bm V_{l+1}^T \bm U_l\bm \Sigma_l^* - \blk\left( \sigma_{t_1}^*(\bm T_{l+1}^{(1)})^T,\dots,\sigma_{t_p}^*(\bm T_{l+1}^{(p)})^T, \bm 0 \right) \right\|_F \notag\\
& = \left\| (\bm V_{l+1}^T \bm U_l - \bm T_{l+1}^T) \bm \Sigma_l^* \right\|_F \le \frac{9\sigma_{\max}^{*2}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}} \|\nabla G(\bm W)\|_F,
\end{align}
where the third equality uses $\bm \sigma^* = (\sigma_{t_1}\bm I_{g_1},\dots, \sigma_{t_p}\bm I_{g_p}, \bm 0_{d_{\min}-r_{\sigma}})$ due to \eqref{eq:partsigma}, the last equality follows from the diagonal block forms of $\bm T_{l}$ and $\bm \Sigma_l^*$, and the inequality holds because of \eqref{eq:Hlapproximate}. Using \eqref{eq0:thm eb} and \eqref{eq7:thm eb}, we compute 
\begin{align*}
	\|\hat{\bm W}_1 - \bm W_1^*\|_F 
	&= \|\bm U_1 \bm \Sigma_1^* \tilde{\bm V}_1^T - \bm Q_2 \bm \Sigma_1^*\hat{\bm T}^T \tilde{\bm V}_1^T\|_F 
	= \|\bm V_2^T \bm U_1 \bm \Sigma_1^* - \bm V_2^T \bm Q_2 \bm \Sigma_1^*\hat{\bm T}^T  \|_F \notag \\
	&\leq \|\bm V_2^T\bm U_1 \bm \Sigma_1^* - \bm T_2^T \bm \Sigma_1^*\|_F 
	+ \|\bm T_2^T \bm \Sigma_1^* - \bm V_2^T \bm Q_2 \bm \Sigma_1^* \hat{\bm T}^T \|_F \notag \\
	&= \|(\bm U_1^T \bm V_2 - \bm T_2)^T \bm \Sigma_1^*\|_F 
	+ \|\bm \Sigma_1^* - \bm T_2 \bm V_2^T \bm Q_2 \bm \Sigma_1^* \hat{\bm T}^T \|_F \notag \\
    & = \|(\bm U_1^T \bm V_2 - \bm T_2)^T \bm \Sigma_1^*\|_F\le \frac{9\sigma_{\max}^{*2}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}}\|\nabla G(\bm W)\|_F,
\end{align*} 
where the last equality follows from the forms of $\bm T_l$ in \eqref{eq6:thm eb}, $\bm \Sigma_l^* := \blk(\mathrm{diag}(\bm \sigma^*), \bm 0)$ with  $\bm \sigma^* = (\sigma_{t_1}\bm I_{g_1},\dots, \sigma_{t_p}\bm I_{g_p}, \bm 0_{d_{\min}-r_{\sigma}})$, $
\bm Q_2$, and $\hat{\bm T}$, and the last inequality uses \eqref{eq:Hlapproximate}. This, together with \eqref{eq8:thm eb} and the fact that $\bm W_L^* = \hat{\bm W}_L$ due to $\bm Q_L := \bm V_L$, yields 
\begin{align}\label{eq9:thm eb} 
\|\hat{\bm W} - \bm W^*\|_F \le \frac{9\sigma_{\max}^{*2}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}}\sqrt{L-1}\|\nabla G(\bm W)\|_F.  
\end{align} 
Finally, using \eqref{eq5:thm eb}, we obtain 
 \begin{align*} 
\mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) & \le \|\bm W - \bm W^*\|_F \le \|\bm W - \hat{\bm W}\|_F + \|\hat{\bm W} - \bm W^*\|_F \\
& \le \sqrt{L} \left( \frac{9\sigma_{\max}^{*2}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}}  + c_3\sqrt{d_{\max} - r_{\sigma}} +  c_4\sigma_{\max}^* + c_5\sqrt{r_{\sigma}} \right) \|\nabla G(\bm W)\|_F,
\end{align*} 
where the last inequality uses \eqref{eq4:thm eb} and \eqref{eq9:thm eb}. Then, we complete the proof. 
\end{proof}

\begin{remark}
Now, we highlight our technical contributions for establishing the error bound. Prior works \cite{Liu2019,wang2023linear} derived a closed-form expression of the critical point set, denoted by $\cal X$, of their considered non-convex problems. Leveraging the favorable structure of $\cal X$, they explicitly computed the distance  \( \mathrm{dist}(\bm{X}, \mathcal{X}) \) from a point $\bm X$ to $\cal X$. In contrast, our analysis faces a significant challenge: the distance \( \mathrm{dist}(\bm{W}, \mathcal{W}_G) \) cannot be explicitly computed due to the complicated structure of $\mathcal{W}_G$ in \Cref{thm:opti G}. To address this challenge, we construct an intermediate point $\hat{\bm W}$ in \eqref{eq0:thm eb}, which uses the singular vectors $\bm W$ and singular values of $\bm W^* \in \mathcal{W}_G$. This enables us to bridge the gap between $\bm W$ and $\mathcal{W}_G$. Then, we respectively bound $\mathrm{dist}(\hat{\bm W}, \mathcal{W}_G)$ and $\|\bm W - \hat{\bm W}\|_F$ by the gradient norm at $\bm W$. Finally, we prove the error bound. 
\end{remark}

In \Cref{thm:eb G},  the local neighborhood condition involves both the gradient norm of $\bm W$ (see \eqref{eq:grad 2} when $L = 2$ and \eqref{eq:gradient_condition}) and its distance to the critical point set. However, the local neighborhood condition in \Cref{thm:eb} depends only on the distance to the critical point set. To prove \Cref{thm:eb}, we leverage the continuity of the gradient of $G(\bm W)$ to transform the condition involving the gradient norm into the distance between $\bm W$ and the critical point set.
 
\begin{proof}[Proof of \Cref{thm:eb}]
According to \eqref{eq:grad G} and \Cref{thm:opti G}, we obtain that the gradient norm is a continuous function with respect to $\bm{W}$ and $\mathcal{W}_{G}$ is a compact set. This implies that there exists a positive constant $\delta_g$ such that \eqref{eq:grad 2} for $L = 2$ and \eqref{eq:gradient_condition} hold whenever $\mathrm{dist}(\bm{W}, \mathcal{W}_G) \leq \delta_g$. Since \Cref{AS:2} holds, we have \eqref{eq:AS lambda} (resp., \eqref{eq:AS lambda1}) hold for $L=2$ (resp., $L \ge 3$). These, together with \Cref{thm:eb G}, implies that there exist constants $\epsilon, \kappa > 0$ such that for all $\bm{W}$ satisfying $\mathrm{dist}(\bm{W}, \mathcal{W}_G) \leq \epsilon$, we have
\begin{equation*}
    \mathrm{dist}(\bm{W}, \mathcal{W}_G) \leq \kappa \|\nabla G(\bm{W})\|_F.
\end{equation*}
Using this and (ii) of \Cref{lem:equi FG}, \Cref{thm:eb} holds with $\epsilon_1 =  {\epsilon}/{\sqrt{\lambda_{\max}}}$ and $\kappa_1 =  {\kappa\lambda}/{\lambda_{\min}}$.  
\end{proof}

\section{Proofs of Technical Results}\label{sec:pf tech}

In this section, we present detailed proofs for the lemmas and propositions introduced in \Cref{sec:proof}. Notably, the main tools employed in these proofs are basic and largely self-contained, relying minimally on advanced results from the existing literature. 

\subsection{Proofs of the Set of Critical Points}\label{subsec:pf crit}

In this subsection, we provide detailed proofs of \Cref{lem:bala}, \Cref{prop:opti G}, and \Cref{prop:set W} in \Cref{subsec:pf opti}.  We begin with the proof of \Cref{lem:bala}, which establishes the balanced structure of the weight matrices at each critical point. 

\begin{proof}[Proof of \Cref{lem:bala}]
(i) According to $\nabla_{\bm W_l} G(\bm W) = \bm 0$, we have $\nabla_{\bm W_l} G(\bm W)\bm W_l^T - \bm W_{l+1}^T \nabla_{\bm W_{l+1}} G(\bm W)= \bm 0$ for all $l \in [L-1]$. This, together with \eqref{eq:grad G}, implies \eqref{eq:bala}. 

(ii) Recursively using \eqref{eq:bala}, we have
\begin{align*}
\bm W_{L:l+1}^T \bm W_{L:1}\bm W_{l-1:1}^T & = \bm W_{l+1}^T\ldots\bm W_{L-1}^T\bm W_L^T\bm W_L\bm W_{L-1}\ldots\bm W_2\bm W_1\bm W_1^T\bm W_2^T\ldots\bm W_{l-1}^T \\
& = \bm W_{l+1}^T\ldots\left(\bm W_{L-1}^T\bm W_{L-1}\right)^2\ldots\left(\bm W_2\bm W_2^T\right)^2 \ldots\bm W_{l-1}^T = (\bm W_l \bm W^T_l)^{L-1} \bm W_l.
\end{align*}
Substituting this into \eqref{eq:grad G} yields \eqref{eq:crit}. 
\end{proof}

Now, we present the detailed proof of \Cref{prop:opti G}. Although this proof is complicated and lengthy, the main idea is rather straightforward. It lies in studying the SVD of $\bm W_l$ for each $l$ and characterizing their singular values and singular matrices. 

\begin{proof}[Proof of \Cref{prop:opti G}]  
Suppose that $(\bm W_1,\dots,\bm W_L)$ satisfies \eqref{set:WG}. Using the structures of $\bm W_l$ for each $l \in [L]$ in \eqref{set:WG}, we directly verify that \eqref{eq:bala} holds. 
% \begin{align}\label{eq0:bala}
% \bm W_l\bm W_l^T = \bm W_{l+1}^T\bm W_{l+1},\ \forall l \in [L-1].
% \end{align}
For any $\bm a \in \cal A$, it follows from $\tilde{\bm \Sigma}_Y = \mathrm{diag}(y_1,\dots,y_{d_{\min}})$ that  $\mathrm{diag}^{2L-1}(\bm a) - \sqrt{\lambda} \mathrm{diag}^{L-1}(\bm a) \tilde{\bm \Sigma}_{Y} + \lambda \mathrm{diag}(\bm a)  = \bm 0$. This, together with $\bm \sigma = \bm \Pi\bm a$ due to $(\bm \sigma, \bm \Pi) \in \cal B$, implies $\bm \Pi^T\mathrm{diag}(\bm \sigma)\bm \Pi = \mathrm{diag}(\bm a)$ and 
\begin{align}\label{eq1:prop opti G}
\mathrm{diag}^{2L-1}(\bm \sigma) - \sqrt{\lambda} \mathrm{diag}^{L-1}(\bm \sigma) \bm \Pi \tilde{\bm \Sigma}_{Y}\bm \Pi^T + \lambda \mathrm{diag}(\bm \sigma)  = \bm 0. 
\end{align}
For each $l =2,\dots,L-1$, we compute
\begin{align*}
\nabla_{\bm W_l}G(\bm W) &  \overset{\eqref{eq:grad G}, \eqref{eq:bala}}{=} (\bm W_l^T\bm W_l)^{L-1}\bm W_l + \lambda \bm W_l - \sqrt{\lambda}\bm W_{L:l+1}^T\bm Y \bm W_{l-1:1}^T  \\
&\overset{\eqref{set:WG}}{=} \bm Q_{l+1}\left( \bm \Sigma_l^T\bm \Sigma_l \right)^{L-1}\bm \Sigma_l\bm Q_l^T + \lambda \bm Q_{l+1} \bm \Sigma_l \bm Q_l^T - \sqrt{\lambda}\bm Q_{l+1} \\
&\quad   \left(\prod_{j=l+1}^L \bm \Sigma^T_j\right) \mathrm{BlkDiag}\left(\bm 
 \Pi, \bm I_{d_0-d_{\min}}\right)\bm Y \mathrm{BlkDiag} \left(\bm 
 \Pi^T, \bm I_{d_0-d_{\min}}\right)\left(\prod_{j=1}^{l-1} \bm \Sigma^T_j\right) \bm Q_{l}^T \\
& = \bm Q_{l+1}  \mathrm{BlkDiag}\left( \mathrm{diag}^{2L-1}(\bm \sigma)+\lambda \mathrm{diag}(\bm \sigma) - \sqrt{\lambda} \mathrm{diag}^{L-1}(\bm \sigma)\bm \Pi \tilde{\bm \Sigma}_{Y}\bm \Pi^T, \bm 0\right) \bm Q_l^T  = \bm 0,
  \end{align*} 
%where the first equality uses \eqref{eq:grad G} and \eqref{eq:bala}, the second equality holds because of \eqref{set:WG},
where the third equality is due to $\bm \Sigma_l = \mathrm{BlkDiag}\left(\mathrm{diag}(\bm \sigma), \bm 0\right)$ for each $l\in [L]$ and $\bm \Pi \tilde{\bm \Sigma}_{Y}\bm \Pi^T$ are all diagonal matrices, and the last equality follows from $(\bm \sigma, \bm \Pi) \in \mathcal{B}$ and \eqref{eq1:prop opti G}. For the case $l=1$, we compute
\begin{align*}
    \nabla_{\bm W_1} G(\bm W) &\overset{\eqref{eq:grad G}}{=} \bm W_{L:2}^T\bm W_{L:1} - \sqrt{\lambda}\bm W_{L:2}^T\bm Y + \lambda \bm W_1 \overset{ \eqref{eq:bala}}{=} (\bm W_1\bm W_1^T)^{L-1}\bm W_1 + \lambda \bm W_1 - \sqrt{\lambda}\bm W_{L:2}^T\bm Y   \\
    & \overset{\eqref{set:WG}}{=} \bm Q_2 \left( (\bm \Sigma_1\bm \Sigma_1^T)^{L-1}\bm \Sigma_1 + \lambda \bm \Sigma_1 \right)\mathrm{BlkDiag}\left(\bm \Pi, \bm I_{d_0-d_{\min}} \right)\mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\bm O_{p_Y+1} \right)\\
    &\quad- \sqrt{\lambda} \bm Q_2 \left(\prod_{l=2}^{L}\bm \Sigma_l\right) \mathrm{BlkDiag}\left(\bm \Pi, \bm I_{d_L - d_{\min}} \right)\mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y}, \widehat{\bm O}_{p_Y+1} \right)\bm Y \\
    & = \bm Q_2 \left( (\bm \Sigma_1\bm \Sigma_1^T)^{L-1}\bm \Sigma_1 + \lambda \bm \Sigma_1 - \sqrt{\lambda} \left(\prod_{l=2}^{L}\bm \Sigma_l\right) \mathrm{BlkDiag}\left(\bm \Pi, \bm I \right)  \bm Y \mathrm{BlkDiag}\left(\bm \Pi^T, \bm I  \right)\right) \\
    &\quad \mathrm{BlkDiag}\left(\bm \Pi, \bm I \right) \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\bm O_{p_Y+1} \right) = \bm 0,  
\end{align*} 
where  the fourth inequality uses the block structure of $\bm Y$ and $\mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_{Y}},\widehat{\bm O}_{p_{Y}+1} \right)\bm Y= \bm Y \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_{Y}},\bm O_{p_{Y}+1} \right) $, and the last equality follows from $\bm \Sigma_l = \mathrm{BlkDiag}\left( \mathrm{diag}(\bm \sigma), \bm 0\right)$ for each $l\in [L]$ and \eqref{eq1:prop opti G}. Using a similar argument, we prove $\nabla_{\bm W_L}G(\bm W) = \bm 0$. Consequently, we conclude that $(\bm W_1,\dots,\bm W_L)$ is a critical point. 

Conversely, suppose that $(\bm W_1,\dots,\bm W_L)$ is a critical point. According to \Cref{lem:bala}, we obtain that $\bm W_l$ for all $l \in [L]$ share the same rank denoted by $r$, which satisfies $r \leq d_{\min}$. For each $l \in [L]$, let
\begin{equation}\label{eq:svdwl}
    \bm W_l = \bm U_l \bm \Sigma_l \bm V_l^T
\end{equation}
be an SVD of $\bm W_l$, where $\bm U_l \in \mathcal{O}^{d_l}$, $\bm V_l \in\mathcal{O}^ {d_{l-1}}$, and $\bm \Sigma_l = \mathrm{BlkDiag}(\tilde{\bm \Sigma}_l, \bm 0)\in \mathbb{R}^{d_l \times d_{l-1}}$ with $\tilde{\bm \Sigma}_l \in \R^{r\times r}$ being a diagonal matrix with positive diagonal entries. This, together with \eqref{eq:bala}, yields
\begin{align}\label{eq1:bala}
 \bm U_l\bm{\Sigma}_l\bm{\Sigma}_l^T\bm U_l^T = \bm V_{l+1} \bm{\Sigma}_{l+1}^T \bm{\Sigma}_{l+1}\bm V_{l+1}^T,\ \forall l \in [L-1].
\end{align}  
Since the above both sides are eigenvalue decompositions of the same matrix, with eigenvalues in decreasing order, we have 
\begin{align}
%\label{eq2:bala}
\notag     \bm{\Sigma}_l\bm{\Sigma}_l^T =  \bm{\Sigma}_{l+1}^T \bm{\Sigma}_{l+1},\ \forall l \in [L-1]. 
\end{align} 
This implies that $\bm W_1,\dots,\bm W_L$ have the same positive singular values. Next, let $\{\sigma_i\}_{i=1}^r$ denote the positive singular values of $\bm W_l$ for each $l \in [L]$ and $p$ denote the number of distinct elements of positive singular values. In other words, there exist indices $\hat{s}_0, \hat{s}_1,\dots, \hat{s}_p$ such that $0 = \hat{s}_0 < \hat{s}_1 <\dots < \hat{s}_p=r$ and
\begin{align}
%\label{eq4:bala}
\notag
\sigma_{\hat{s}_0+1} = \dots = \sigma_{\hat{s}_1} > \sigma_{\hat{s}_1+1} = \dots = \sigma_{\hat{s}_2} > \dots > \sigma_{\hat{s}_{p-1}+1} = \dots = \sigma_{\hat{s}_p} > 0. 
\end{align}
Let $\hat{h}_i := \hat{s}_i - \hat{s}_{i-1}$ be the multiplicity of the $i$-th largest positive value  for each $i \in [p]$. Obviously, we have $\sum_{i=1}^p \hat{h}_i = r$ and 
\begin{align}\label{eq3:bala}
    \tilde{\bm \Sigma}_l = \tilde{\bm \Sigma} := \mathrm{BlkDiag}\left( \sigma_{\hat{s}_1}  \bm I_{\hat{h}_1},\dots, \sigma_{\hat{s}_p} \bm I_{\hat{h}_p}\right)  \in \R^{r \times r}. 
\end{align}
Based on the above block form, we write $\bm U_l$ and $\bm V_l$ in \eqref{eq:svdwl} for each $l \in [L]$ as 
\begin{align}\label{eq:Ul Vl}
\bU_{l} =  \left[\bm{U}_{l}^{(1)},\dots,\bm{U}_{l}^{(p)},\bm{U}_{l}^{(p+1)}\right],\ \bV_{l} =  \left[\bm{V}_{l}^{(1)},\dots,\bm{V}_{l}^{(p)},\bm{V}_{l}^{(p+1)}\right],
\end{align}
where $\bm{U}_l^{(i)} \in \mO^{d_l \times \hat{h}_i}$ and $\bm{V}_l^{(i)} \in \mO^{d_{l-1} \times \hat{h}_i}$ for all $i \in [p]$, $\bm{U}_l^{(p+1)} \in \mO^{d_l \times (d_{l}- r)}$, and $\bm{V}_l^{(p+1)} \in \mO^{d_{l-1} \times (d_{l-1}- r)}$. This, together with \eqref{eq1:bala}, \eqref{eq3:bala}, and \citep[Lemma 8(i)]{wang2023understanding}, implies that there exists orthogonal matrix $\bm Q_{l}^{(i)} \in \mO^{\hat{h_i}}$ such that
\begin{equation}\label{eq:u1v1}
\begin{aligned}
\bm{U}_{l}^{(i)} = \bm{V}_{l+1}^{(i)}\bm Q_{l}^{(i)},\ \forall l \in [L-1],\ i \in [p+1]. 
\end{aligned} 
\end{equation}
Using this, along with \eqref{eq:svdwl}, \eqref{eq3:bala}, and the commutativity of orthogonal and identity matrices, we compute
\begin{align}\label{eq1:thm opti G}
\bm W_{L:1} = \bm U_{L} \bm \Sigma_L \ldots \bm \Sigma_1\bm Q \bm V_{1}^T = \bm U_L\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^L, \bm 0_{(d_L-r)\times (d_0-r)} \right)\bm Q\bm V_1^T,
\end{align}
where $\prod_{l=L-1}^1 \bm Q_{l}^{(j)} := \bm Q_{L-1}^{(j)} \ldots \bm Q_{1}^{(j)}$ for each $j \in [p]$ and 
\begin{equation}\label{eq:Qexpression}
    \bm Q = \mathrm{BlkDiag}\left(\tilde{\bm Q}, \bm{I}_{d_0-r} \right) = \mathrm{BlkDiag}\left( \prod_{l=L-1}^1 \bm Q_{l}^{(1)},\dots,\prod_{l=L-1}^1 \bm Q_{l}^{(p)},\bm{I}_{d_0-r} \right)  \in \mathcal{O}^{d_0}.
\end{equation}
Right-multiplying \eqref{eq:crit} by $\bm W_L^T$ when $l=L$ and left-multiplying \eqref{eq:crit} by  $\bm W_1^T$  when $l=1$, we obtain  
\begin{align*}
\left(\bm W_L\bm W_L^T \right)^L - \sqrt{\lambda}\bm Y\bm W_{L:1}^T+\lambda\bm W_L\bm W_L^T = \bm 0,\text{ and }  \left(\bm W_1^T\bm W_1 \right)^L - \sqrt{\lambda}\bm W_{L:1}^T\bm Y + \lambda\bm W_1^T\bm W_1 = \bm 0,
\end{align*}
respectively.
Substituting \eqref{eq:svdwl}, \eqref{eq3:bala}, and \eqref{eq1:thm opti G} into the above equalities, together with $\bm U_L \in \mathcal{O}^{d_L}$ and $\bm V_1 \in \mathcal{O}^{d_0}$, yields
%\begin{align*}
%\bm U_L\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2L}, \bm 0_{d_L-r} \right)\bm U_L^T - \sqrt{\lambda}\bm Y \bm V_1\bm Q^T\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^L, \bm 0_{d_0-r,d_L-r} \right) \bm U_L^T  + &  \\
%\quad  \lambda \bm U_L\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2}, \bm 0_{d_L-r} \right)\bm U_L^T & = \bm 0,\\
%\bm V_1\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2L}, \bm 0_{d_0-r} \right)\bm V_1^T - \sqrt{\lambda}\bm V_1\bm Q^T \mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^L, \bm 0_{d_0-r,d_L-r} \right)\bm U_L^T\bm Y  + & \\
%\lambda \bm V_1\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2}, \bm 0_{d_0-r} \right)\bm V_1^T & = \bm 0,
%\end{align*} 
%which implies 
\begin{align}
& \bm U_L^T \bm Y \bm V_1\bm Q^T\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^L, \bm 0_{(d_0-r)\times (d_L-r)} \right) = \frac{1}{\sqrt{\lambda}}\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2L} + \lambda\tilde{\bm \Sigma}^{2}, \bm 0_{d_L-r} \right), \label{eq2:thm opti G}\\
& \bm Q^T\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^L, \bm 0_{(d_0-r)\times (d_L-r)} \right) \bm U_L^T\bm Y\bm V_1  = \frac{1}{\sqrt{\lambda}}\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2L} + \lambda\tilde{\bm \Sigma}^{2}, \bm 0_{d_0-r} \right)\label{eq3:thm opti G}. 
\end{align}
According to \eqref{eq3:thm opti G} and $\bm Q \in \mathcal{O}^{d_0}$, we obtain 
\begin{align*}
\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^L, \bm 0_{(d_0-r)\times (d_L-r)} \right) \bm U_L^T\bm Y\bm V_1 & = \frac{1}{\sqrt{\lambda}} \bm Q \mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2L} + \lambda\tilde{\bm \Sigma}^{2}, \bm 0_{d_0-r} \right)\\
& = \frac{1}{\sqrt{\lambda}}\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2L} + \lambda\tilde{\bm \Sigma}^{2}, \bm 0_{d_0-r} \right)\bm Q,
\end{align*}
where the second equality follows from the block diagonal structures of $\tilde{\bm \Sigma}$ in \eqref{eq3:bala} and $\bm Q$ in \eqref{eq:Qexpression}. Right-multiplying on both sides of the above equality by $\bm Q^T$ yields 
\begin{align}\label{eq4:thm opti G}
\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^L, \bm 0_{(d_0-r)\times (d_L-r)} \right) \bm U_L^T\bm Y\bm V_1\bm Q^T = \frac{1}{\sqrt{\lambda}}\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}^{2L} + \lambda\tilde{\bm \Sigma}^{2}, \bm 0_{d_0-r} \right). 
\end{align}
%Now, we partition $\bm U_L$ and $\bm V_1$ into
%\begin{align}
%\bm U_L = \begin{bmatrix}
%\bm U_{L,1} & \bm U_{L,2} 
%\end{bmatrix},\ \bm V_1 = \begin{bmatrix}
%\bm V_{1,1} & \bm V_{1,2} 
%\end{bmatrix}
%\end{align}
%where $\bm U_{L,1} \in \R^{d_L\times r}$, $\bm U_{L,2} \in \R^{d_L\times (d_L-r)}$, $\bm V_{1,1} \in \R^{d_0\times r}$, and $\bm V_{1,2} \in \R^{d_0\times (d_0- r)}$. This, together with \eqref{eq:Qexpression}, yields
%\begin{align}
%\bm U_L^T\bm Y\bm V_1\bm Q^T = \begin{bmatrix}
%\bm U_{L,1}^T\bm Y\bm V_{1,1}\tilde{\bm Q}^T & \bm U_{L,1}^T\bm Y\bm V_{1,2}\\
%\bm U_{L,2}^T\bm Y\bm V_{1,1}\tilde{\bm Q}^T & \bm U_{L,2}^T\bm Y \bm V_{1,2}
%\end{bmatrix}
%\end{align}
%\begin{align}
%\bm U_{L,1}^T\bm Y\bm V_{1,1}\tilde{\bm Q}^T  =  \frac{1}{\sqrt{\lambda}} \left( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} \right),\ \bm U_{L,1}^T\bm Y\bm V_{1,2} = \bm 0,\ \bm U_{L,2}^T\bm Y\bm V_{1,1}\tilde{\bm Q}^T = \bm 0. 
%\end{align}
We now partition $\bm C := \bm U_L^T\bm Y\bm V_1\bm Q^T \in \R^{d_L\times d_0}$ into the block form $\bm C = \begin{bmatrix}
\bm C_1 & \bm C_2\\
\bm C_3 & \bm S
\end{bmatrix}$, where $\bm C_1 \in \R^{r\times r}$. 
This, together with \eqref{eq2:thm opti G} and \eqref{eq4:thm opti G}, yields
$
\bm C_1 =   ( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} )/\sqrt{\lambda},\ \bm C_2 = \bm 0,\ \bm C_3 = \bm 0. 
$
Consequently, we obtain 
\begin{align}\label{eq5:thm opti G}
\bm U_L^T\bm Y\bm V_1\bm Q^T = % \mathrm{BlkDiag}\left( \frac{1}{\sqrt{\lambda}} \left( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} \right), \bm S\right),
\begin{bmatrix}
\frac{1}{\sqrt{\lambda}} \left( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} \right) & \bm 0\\
\bm 0 & \bm S 
\end{bmatrix}. 
\end{align}
Now, let $\bm U_S\bm \Sigma_S \bm V_S = \bm S$ be an SVD of $\bm S$, where $\bm U_S \in \mathcal{O}^{d_L-r}$, $\bm V_S \in \mathcal{O}^{d_0-r}$, and $\bm \Sigma_S \in \R^{(d_L-r)\times (d_0-r)}$. Substituting this into \eqref{eq5:thm opti G} and rearranging the terms yields
\begin{align}\label{eq6:thm opti G}
\begin{bmatrix}
\bm I_r & \bm 0 \\
\bm 0 & \bm U_S^T
\end{bmatrix} \bm U_L^T\bm Y\bm V_1\bm Q^T \begin{bmatrix}
\bm I_r & \bm 0 \\
\bm 0 & \bm V_S
\end{bmatrix} = \begin{bmatrix}
\frac{1}{\sqrt{\lambda}} \left( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} \right) & \bm 0\\
\bm 0 & \bm \Sigma_S
\end{bmatrix}.
%\mathrm{BlkDiag}\left(\bm I, \bm U_S^T\right)\bm U_L^T\bm Y\bm V_1\bm Q^T\mathrm{BlkDiag}\left(\bm I, \bm V_S\right) = \mathrm{BlkDiag}\left( \frac{1}{\sqrt{\lambda}} \left( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} \right), \bm \Sigma_S\right).
\end{align}
Since $\bm Y$ is a diagonal matrix, $\mathrm{BlkDiag}\left(\bm I, \bm U_S^T\right)\bm U_L^T \in \mathcal{O}^{d_L}$, and $\bm V_1\bm Q^T\mathrm{BlkDiag}\left(\bm I, \bm V_S\right) \in \mathcal{O}^{d_0}$, the above left-hand side is an SVD of the right-hand diagonal matrix. Therefore, we obtain that the diagonal elements of $ \mathrm{BlkDiag}\left( \left( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} \right)/\sqrt{\lambda}, \bm \Sigma_S\right)$ is a permutation of those of $\bm Y$. This, together with $\bm Y \in \R^{d_L\times d_0}$ in \eqref{eq:Y}, yields that there exists a permutation matrix $\bm \Pi \in \mathcal{P}^{d_{\min}}$ such that 
\begin{align}
%\label{eq7:thm opti G}
\notag
	 \begin{bmatrix}
\frac{1}{\sqrt{\lambda}} \left( \tilde{\bm \Sigma}^{L} + \lambda\tilde{\bm \Sigma}^{2-L} \right) & \bm 0\\
\bm 0 & \bm \Sigma_S
\end{bmatrix}  = \begin{bmatrix}
	\bm \Pi & \bm 0\\ \bm 0 & \bm I_{d_L - d_{\min}}
	\end{bmatrix} \bm Y \begin{bmatrix}
	\bm \Pi^T & \bm 0\\ \bm 0 & \bm I_{d_0 - d_{\min}}
	\end{bmatrix}. 
\end{align} 
Combining this with \eqref{eq6:thm opti G} yields
\begin{align*}
\bm Y = \left(\begin{bmatrix}
	\bm \Pi^T & \bm 0\\ \bm 0 & \bm I_{d_L - d_{\min}}
	\end{bmatrix} \begin{bmatrix}
\bm I_r & \bm 0 \\
\bm 0 & \bm U_S^T
\end{bmatrix} \bm U_L^T\right) \bm Y \left( \bm V_1\bm Q^T \begin{bmatrix}
\bm I_r & \bm 0 \\
\bm 0 & \bm V_S
\end{bmatrix} \begin{bmatrix}
	\bm \Pi & \bm 0\\ \bm 0 & \bm I_{d_0 - d_{\min}}
	\end{bmatrix}\right). 
\end{align*}
Since the right-hand side is an SVD of a diagonal matrix $\bm Y$ in \eqref{eq:Y} and \eqref{eq:SY}, there exist $\bm O_1 \in \mathcal{O}^{h_1},\dots,\bm O_{p_Y} \in \mathcal{O}^{h_{p_Y}},\bm O_{p_Y+1} \in \mathcal{O}^{d_0-r_Y}$ and $\widehat{\bm O}_{p_Y+1} \in \mathcal{O}^{d_L-r_Y}$ such that 
\begin{align*}
& \begin{bmatrix}
	\bm \Pi^T & \bm 0\\ \bm 0 & \bm I_{d_0 - d_{\min}}
	\end{bmatrix} \begin{bmatrix}
\bm I_r & \bm 0 \\
\bm 0 & \bm V_S^T
\end{bmatrix}\bm Q \bm V_1^T = \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\bm O_{p_Y+1} \right), \\
& \begin{bmatrix}
	\bm \Pi^T & \bm 0\\ \bm 0 & \bm I_{d_L - d_{\min}}
	\end{bmatrix} \begin{bmatrix}
\bm I_r & \bm 0 \\
\bm 0 & \bm U_S^T
\end{bmatrix} \bm U_L^T = \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\widehat{\bm O}_{p_Y+1} \right).  
\end{align*}
This implies 
\begin{align}
& \bm V_1 = \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_Y}^T,\bm O_{p_Y+1}^T \right)\blk\left( \bm \Pi^T, \bm I_{d_0 - d_{\min}}\right) \blk\left(\bm I_r, \bm V_S^T \right) \bm Q,\label{eq9:thm opti G} \\
& \bm U_L = \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_Y}^T,\widehat{\bm O}_{p_Y+1}^T \right)\blk\left( \bm \Pi^T, \bm I_{d_L - d_{\min}}\right) \blk\left(\bm I_r, \bm U_S^T \right). \label{eq8:thm opti G}
\end{align}
Substituting \eqref{eq3:bala} and \eqref{eq8:thm opti G} into \eqref{eq:svdwl} yields
\begin{align}\label{eq:defwL}
\bm W_L   = \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_Y}^T,\widehat{\bm O}_{p_Y+1}^T \right)\mathrm{BlkDiag}\left(\bm \Pi^T, \bm I_{d_L - d_{\min}} \right)\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}, \bm 0 \right) \bm V_L^T. 
\end{align}
Next, substituting \eqref{eq:Ul Vl} and \eqref{eq:u1v1} into \eqref{eq:svdwl} yields  
\begin{align*} 
    \bm W_{L-1}&= \bm V_{L} \mathrm{BlkDiag}\left(\bm Q_{L-1}^{(1)},\dots,\bm Q_{L-1}^{(p)}, \bm I \right)
    \mathrm{BlkDiag}\left(\tilde{\bm \Sigma}, \bm 0 \right) \bm V_{L-1}^T = \bm V_L \mathrm{BlkDiag}\left(\tilde{\bm \Sigma}, \bm 0 \right) \bm P_{L-1}^T,
\end{align*}
where $\bm P_{L-1} := \bm V_{L-1}\mathrm{BlkDiag}\left( \bm{Q}_{L-1}^{(1)^T},\dots, \bm{Q}_{L-1}^{(p)^T}, \bm I \right) \in \mathcal{O}^{d_{L-2}}$. Using the same argument, we obtain 
\begin{align}\label{eq:defwl}
\bm W_{l} = \bm P_{l+1}\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}, \bm 0 \right)\bm P_{l}^T,\ l = 2,\dots,L-2,
%& \bm W_1 = \frac{\bm P_2}{\sqrt{\lambda_1}}\mathrm{BlkDiag}\left(\sigma_{\hat{s}_1}^* \bm I_{\hat{h}_1},\dots,\sigma_{\hat{s}_p}^* \bm I_{\hat{h}_p}, \bm 0\right), 
\end{align}
where $\bm P_{l} := \bm V_{l}\mathrm{BlkDiag}\left( \prod_{j=l}^{L-1} \bm Q_{j}^{(1)^T},\dots, \prod_{j=l}^{L-1} \bm Q_{j}^{(p)^T}, \bm I\right) \in \mathcal{O}^{d_{l-1}}$ for all $l=2,\dots,L-2$. Finally, using \eqref{eq:Qexpression}  and \eqref{eq9:thm opti G}, we compute
\begin{align}\label{eq:defw1}
\bm W_1 & = \bm P_{2}\mathrm{BlkDiag}\left(\tilde{\bm \Sigma}, \bm 0 \right)\mathrm{BlkDiag}\left( \prod_{j=1}^{L-1} \bm Q_{j}^{(1)},\dots, \prod_{j=1}^{L-1} \bm Q_{j}^{(p)}, \bm I\right)\bm V_1^T \notag \\
& = \bm P_2 \mathrm{BlkDiag}\left(\tilde{\bm \Sigma}, \bm 0 \right) \mathrm{BlkDiag} \left(\bm \Pi,\bm I_{d_0-d_{\min}}\right) \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\bm O_{p_Y+1} \right). 
\end{align}
By letting $\bm \sigma := \left(\sigma_1, \sigma_2, \dots, \sigma_{d_{\min}} \right) \in \R^{d_{\min}}$, we write the singular value matrix $\bm \Sigma_l$ as
\begin{align*}
\bm \Sigma_l = \mathrm{BlkDiag}\left(\mathrm{diag}(\bm \sigma), \bm 0_{(d_l-d_{\min})\times (d_{l-1}-d_{\min})} \right),\ \forall l \in [L].  
\end{align*}
Next, it remains to show that $(\bm \sigma, \bm \Pi) \in \mathcal{B}$. Substituting \eqref{eq:defwL}, \eqref{eq:defwl}, and \eqref{eq:defw1} into $\nabla_{\bm W_L} G(\bm W) = \bm 0$ yields 
\begin{align*}
  \bm 0 & = \nabla_{\bm W_L}G(\bm W) \overset{\eqref{eq:grad G}}{=} \bm W_{L:1}\bm W_{L-1:1}^T - \sqrt{\lambda}\bm Y\bm W_{L-1:1}^T + \lambda \bm W_L\\
 & = \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\widehat{\bm O}_{p_{Y}+1}^T \right)\mathrm{BlkDiag}\left(\bm \Pi^T \left(\mathrm{diag}^{2L-1}(\bm \sigma)+\lambda \mathrm{diag}(\bm \sigma)\right), \bm 0 \right)   \bm V_L^T - \\
& \quad \sqrt{\lambda}\bm Y \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\bm O_{p_{Y}+1}^T \right)\mathrm{BlkDiag} \left(\bm \Pi^T\mathrm{diag}^{L-1}(\bm \sigma),\bm 0\right)\bm V_L^T.
\end{align*}
This, together with $\bm Y \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\bm O_{p_{Y}+1}^T \right) = \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\widehat{\bm O}_{p_{Y}+1}^T \right)\bm Y $ due to \eqref{eq:SY}, implies  
\begin{align*}
\mathrm{BlkDiag}\left(\bm \Pi^T \left(\mathrm{diag}^{2L-1}(\bm \sigma)+\lambda \mathrm{diag}(\bm \sigma)\right), \bm 0 \right) - 
\sqrt{\lambda}\bm Y \mathrm{BlkDiag}\left(\bm \Pi^T\mathrm{diag}^{L-1}(\bm \sigma), \bm 0 \right) =\bm 0.
\end{align*}  
This directly implies each element of $\bm \Pi^T\bm \sigma \in \mathcal{A}$, and thus there exists $\bm a \in \mathcal{A}$ such that $\bm \Pi\bm a = \bm \sigma$. Consequently, $(\bm a, \bm \Pi) \in \mathcal{B}$. Then, we complete the proof.  
\end{proof}

Next, we proceed to the proof of \Cref{prop:set W}. This proof mainly builds on the structure of the critical point set in \Cref{prop:opti G}. 

\begin{proof}[Proof of \Cref{prop:set W}] 
(i) Obviously, the ``only if'' direction is trivial since $\bm \sigma$ of $\mathcal{W}_{\bm \sigma, \bm \Pi}$ and $\bm \sigma^\prime$ in $\mathcal{W}_{\bm \sigma^\prime, \bm \Pi^\prime}$ share the same non-increasing singular values. Now, we are devoted to proving the ``if'' direction. Note that $(\bm \sigma,\bm \Pi)\in \mathcal{B}$ and  $(\bm \sigma^\prime,\bm \Pi^\prime) \in \mathcal{B}$ satisfy $\bm \sigma = \bm \sigma^\prime$. There exist $\bm a, \bm a^\prime \in \mathcal{A}$ such that
    \begin{equation}\label{eq:defahata}
        \bm a = \bm \Pi^T \bm \sigma,\ \bm a^\prime = \bm \Pi^{\prime T} \bm \sigma^\prime.
    \end{equation}
    This, together with $\bm \sigma = \bm \sigma^\prime$, implies that $\bm a, \bm a^\prime$ have the same positive elements but in a different order. Consider \eqref{set:A} in defition $\mathcal{A}$, 
    \begin{align}
    \label{eq:eqinA}
        x_i^{2L-1} - \sqrt{\lambda}y_i x_i^{L-1} + \lambda x_i = 0,\ x_i \geq 0,\ \forall i \in [d_{\min}].
    \end{align}
    Note that if $x_i = x_j>0$, we have $y_i=y_j$, which implies that if $y_i \neq y_j$, we have $x_i \neq x_j$ when $x_i, x_j>0$. 
    % This, together with the partition of $(y_1,\cdots,y_{d_{\min}})$ in \eqref{eq:SY1}, 
    Since $\bm \sigma = \bm \sigma'$ and each element of $\bm a$ and $\bm a'$ is obtained by solving the equation \eqref{eq:eqinA}, along with the fact that different $y_i$'s correspond to equations with no common positive root and the partition of $(y_1, \cdots, y_{d_{\min}})$ in \eqref{eq:SY1}, yields that the elements of $\bm a$ and $\bm a'$ in each partition of the form $\bm Y$ differ only in their order. Therefore, we obtain the following conclusion: There exist $\bm P_i \in \mathcal{P}^{h_i}$ for all $i \in [p_Y]$ and $\bm P_{p_Y+1} \in \mathcal{P}^{d_{\min} -r_{Y}}$ such that 
    \[
        \mathrm{diag}(\bm a) = \blk(\bm P_1,\cdots,\bm P_{p_Y},\bm P_{p_Y+1})\mathrm{diag}(\bm a^\prime)\blk(\bm P_1^T,\cdots,\bm P_{p_Y}^T,\bm P_{p_Y+1}^T). 
    \]
    Substituting \eqref{eq:defahata} into the above equality, together with $\bm \sigma = \bm \sigma^\prime$, yields 
    \small
        \begin{align}\label{eq1:prop set W}
        \bm \Pi^T\mathrm{diag}(\bm \sigma)\bm \Pi 
         =   \blk(\bm P_1,\cdots,\bm P_{p_Y},\bm P_{p_Y+1})\bm \Pi^{\prime T}\mathrm{diag}(\bm \sigma )\bm \Pi^\prime\blk(\bm P_1^T,\cdots,\bm P_{p_Y}	^T,\bm P_{p_Y+1}^T).
    \end{align}
    \normalsize
   %where the second equality uses $\bm \sigma = \bm \sigma^\prime$. 
    Let $(\bm W_1,\cdots,\bm W_L)\in \mathcal{W}_{\bm \sigma,\bm \Pi}$ be arbitrary. For ease of exposition, let 
    \begin{align*}
        \tilde{\bm Q}_l :=\bm Q_l\blk(\bm \Pi,\bm I)
         \blk\left(\bm P_1,\cdots,\bm P_{p_Y},\bm P_{p_Y+1},\bm I \right)\blk(\bm \Pi^{'T},\bm I) \in \mathcal{O}^{d_{l-1}},\ \forall l \in [2,\ L]. 
    \end{align*}
    Moreover, we have
    \begin{align}\label{eq2:prop set W}
    \bm \Sigma_l \blk(\bm \Pi, \bm I) = \blk(\mathrm{diag}(\bm \sigma)\bm \Pi, \bm 0),\ \forall l \in [L].  
    \end{align}
    Then, we have 
    \begin{align*}
         \bm W_1 &= \bm Q_2\bm \Sigma_1 \mathrm{BlkDiag}\left(\bm \Pi, \bm I  \right)\mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_Y},\bm O_{p_Y+1} \right)\\
        &= \tilde{\bm Q}_2 \bm \Sigma_1 \mathrm{BlkDiag}\left( \bm \Pi^\prime, \bm I  \right)\blk(\bm P_1^T\bm O_1,,\cdots,\bm P_{p_Y}^T\bm O_{p_{Y}},\blk(\bm P_{p_Y+1}^T,\bm I_{d_0-d_{\min}})\bm O_{p_{Y}+1}),  
    \end{align*}
    where the second equality uses \eqref{eq1:prop set W} and \eqref{eq2:prop set W}. Using the same argument, we have
    \begin{align*}
    & \bm W_L = \mathrm{BlkDiag}\left( \bm O_1^T\bm P_1,\dots,\bm O_{p_{Y}}^T\bm P_Y,\widehat{\bm O}_{p_{Y}+1}^T\blk(\bm P_{p_Y+1}, \bm I) \right)\mathrm{BlkDiag}\left(\bm \Pi^{\prime T}, \bm I \right)\bm \Sigma_L \tilde{\bm Q}_{L}^T, \\
    & \bm W_l = \tilde{\bm Q}_{l+1} \bm \Sigma_l \tilde{\bm Q}_l^T,\ l =2,\dots, L-1. 
    \end{align*}
Therefore, we obtain that $(\bm W_1,\cdots,\bm W_L)\in \mathcal{W}_{\bm \sigma^\prime, \bm \Pi^\prime}$ and thus $\mathcal{W}_{\bm\sigma,\bm\Pi} \subseteq  \mathcal{W}_{\bm\sigma^\prime, \bm\Pi^\prime} $. Applying the same argument, we also have $\mathcal{W}_{\bm\sigma^\prime, \bm\Pi^\prime} \subseteq \mathcal{W}_{\bm\sigma,\bm\Pi}$. Therefore, we have $\mathcal{W}_{\bm\sigma,\bm\Pi} = \mathcal{W}_{\bm\sigma^\prime, \bm\Pi^\prime}$.

(ii)  Using Mirsky's inequality (see \Cref{lem:mirsky}) and \eqref{set:G2}, we have for any $\bm W \in \mathcal{W}_{\bm \sigma, \bm \Pi}$ and $\bm W^\prime \in \mathcal{W}_{\bm \sigma^\prime, \bm \Pi^\prime}$, 
    \[
        \|\bm W- \bm W^\prime\|_F \geq \|\bm \sigma -\bm \sigma^\prime\|_2 \geq \delta_{\sigma}, 
    \]
where the last inequality follows from \eqref{eq:delta sigma}. This implies \eqref{eq:dist W}. 
\end{proof}

\subsection{Proofs of the Error Bound}\label{subsec:pf error}

In this subsection, we mainly present the detailed proofs of \Cref{lem:pre}, \Cref{coro1}, \Cref{prop:Hlapproximate}, \Cref{lem:twoinequality},  \Cref{prop:singular value 2}, and \Cref{prop:singular control} in \Cref{subsec:ana eb}. To begin, we prove \Cref{lem:pre} and \Cref{coro1}, which play a key role in the subsequent analysis.  

\begin{proof}[Proof of \Cref{lem:pre}]
    (i) Let $\bm W^* = (\bm W_1^*,\dots,\bm W_L^*) \in \mathcal{W}_{\bm \sigma^*}$ be such that $\mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) = \|\bm W - \bm W^*\|_F$. Using the triangle inequality, we have for each $l \in [L]$, 
    \begin{align*}
        \|\bm W_l\| \ge \|\bm W_l^*\| - \|\bm W_l - \bm W_l^*\| \ge \|\bm W_l^*\| - \|\bm W_l - \bm W_l^*\|_F \ge \sigma^*_{\max} - \frac{\sigma^*_{\min}}{2} \ge  \frac{\sigma^*_{\max}}{2},
    \end{align*}
    where the third inequality follows from \eqref{eq:dist 1} and $\bm W_l^* = \bm Q_{l+1}\bm \Sigma_l^* \bm Q_l^T$ and $\bm \Sigma_l^* = \mathrm{BlkDiag}(\bm \sigma^*, \bm 0)$ for each $\bm Q_l \in \mathcal{O}^{d_l-1}$ with $l=2,\dots,L$ according to \Cref{thm:opti G}. Similarly, we have $\|\bm W_l\| \le 3 \sigma^*_{\max} /2$ for each $l\in [L]$. Therefore, we have  \eqref{eq:S norm W}. Using Weyl's inequality  (see \cite[Corollary 7.3.5]{horn2012matrix}), we have for each $l \in [L]$ and $i \in [r_{\sigma}]$,
        \[
        \left|\sigma_i(\bm W_l) - \sigma_i^*\right| \leq \|\bm W_l - \bm W_l^*\| \leq \frac{\sigma_{\min}^*}{2},
        \]
    which implies
    \begin{equation*}
        \sigma_i(\bm W_l) \geq \sigma_i^* - \frac{\sigma_{\min}^*}{2} \geq \frac{\sigma_{\min}^*}{2}.
    \end{equation*}
   
    (ii) According to \eqref{eq:grad G}, we compute 
    \begin{align*}
        &  \frac{1}{2}\nabla_{\bm W_l} G(\bm W) \bm W_l^T = \bm W_{L:l+1}^T \left(\bm W_{L:1} - \sqrt{\lambda}\bm Y\right) \bm W_{l:1}^T + \lambda \bm W_l \bm W_l^T, \\
        & \frac{1}{2} \bm W_{l+1}^T \nabla_{\bm W_{l+1}} G(\bm W) = \bm W_{L:l+1}^T \left(\bm W_{L:1} - \sqrt{\lambda}\bm Y\right) \bm W_{l:1}^T + \lambda \bm W_{l+1}^T \bm W_{l+1}.
    \end{align*}
    Then, we have for each $l=1,\dots,L-1$, 
    \begin{align*}
        \frac{1}{2}\left(\nabla_{\bm W_l} G(\bm W) \bm W_l^T - \bm W_{l+1}^T \nabla_{\bm W_{l+1}} G(\bm W)\right) =  \lambda \left( \bm W_l \bm W_l^T - \bm W_{l+1}^T \bm W_{l+1} \right). 
    \end{align*}
    This implies
    \begin{align*}
        \left\| \bm W_l \bm W_l^T - \bm W_{l+1}^T \bm W_{l+1} \right\|_F & = \frac{1}{2\lambda}\left\|  \nabla_{\bm W_l} G(\bm W) \bm W_l^T - \bm W_{l+1}^T \nabla_{\bm W_{l+1}} G(\bm W) \right\|_F \\
        & \le  \frac{3\sigma_{\max}^*}{4\lambda}  \left( \|\nabla_{\bm W_l} G(\bm W)\|_F +  \|\nabla_{\bm W_{l+1}} G(\bm W)\|_F \right)  \le  \frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda}  \|\nabla G(\bm W)\|_F,
    \end{align*}
    where the first inequality follows from the triangle inequality, $\|\bm A\bm B\|_F \le \|\bm A\| \|\bm B\|_F$ for all $\bm A, \bm B$ of the same size, and \eqref{eq:S norm W}.  
    
    (iii) 
    Using Weyl's inequality and \eqref{eq:balance}, we obtain for each $l \in [L]$ and $i \in [r_{\sigma}]$, 
        \[
        | \sigma_i^2(\bm W_l) -\sigma_i^2(\bm W_{l+1})| \leq \left\| \bm W_l \bm W_l^T - \bm W_{l+1}^T \bm W_{l+1} \right\|_F \leq \frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda} \|\nabla G(\bm W)\|_F.
        \]
    This implies
        \begin{align*}
        \left|\sigma_i(\bm W_l) - \sigma_i(\bm W_{l+1}) \right| & \leq \frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda\left( \sigma_i(\bm W_l) + \sigma_i(\bm W_{l+1}) \right)} \|\nabla G(\bm W)\|_F  \leq \frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda\sigma_{\min}^*}\|\nabla G(\bm W)\|_F, 
        \end{align*}
    where the last inequality follows from \eqref{eq:sigmalowerbound}.

 (iv)  We first prove \eqref{eq:transwji}. Note that 
    \begin{align}\label{eq1:lem pushl}
    \bm W_{j:i}^T \bm W_{j:i} - (\bm W^T_i\bm W_i)^{j-i+1}  & = \bm W_{j-1:i}^T\left(\bm W_j^T \bm W_j - \bm W_{j-1}\bm W_{j-1}^T\right)\bm W_{j-1:i}  \notag\\
    & \quad + \bm W_{j-2:i}^T\left((\bm W_{j-1}^T\bm W_{j-1})^2- (\bm W_{j-2}\bm W_{j-2}^T)^2\right)\bm W_{j-2:i} + \cdots \notag\\
    &\quad + \bm W_i^T\left((\bm W_{i+1}^T\bm W_{i+1})^{j-i}-\bm W_{i}\bm W_{i}^T)^{j-i}\right)\bm W_i \notag\\
    & = \sum_{k=1}^{j-i} \bm W_{j-k:i}^T\left( (\bm W_{j-k+1}^T\bm W_{j-k+1})^k - (\bm W_{j-k}\bm W_{j-k}^T)^{k} \right) \bm W_{j-k:i}. 
    \end{align}
    For each $k \in [j-i]$, we compute
    \begin{align*}
        & \left\|\bm W_{j-k:i}^T\left( (\bm W_{j+1-k}^T\bm W_{j+1-k})^k - (\bm W_{j-k}\bm W_{j-k}^T)^k \right) \bm W_{j-k:i}\right\|_F \\
       \le\ & \left\| (\bm W_{j+1-k}^T\bm W_{j+1-k})^k - (\bm W_{j-k}\bm W_{j-k}^T)^k \right\|_F \prod_{l=i}^{j-k} \|\bm W_l\|^2\\
       \le\ & k\left(\frac{3\sigma_{\max}^*}{2}\right)^{2(j-i)}\left\|\bm W_{j+1-k}^T\bm W_{j+1-k} - \bm W_{j-k}\bm W_{j-k}^T\right\|_F\\
       \le\ & k\left(\frac{3\sigma_{\max}^*}{2}\right)^{2(j-i)} \frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda}  \|\nabla G(\bm W)\|_F=\frac{\sqrt{2} k}{2\lambda}\left(\frac{3\sigma_{\max}^*}{2}\right)^{2(j-i)+1}\|\nabla G(\bm W)\|_F,
    \end{align*}
    where the last inequality uses \eqref{eq:balance} and the second inequality follows from  
\begin{align*}
&\left\|(\bm W_{j+1-k}^T \bm W_{j+1-k})^k - (\bm W_{j-k}\bm W_{j-k}^T)^k\right\|_F \\
=\ &
 \left\|\sum_{l=1}^k (\bm W_{j+1-k}^T \bm W_{j+1-k})^{k-l} (\bm W_{j+1-k}^T \bm W_{j+1-k} - \bm W_{j-k} \bm W_{j-k}^T) (\bm W_{j-k}\bm W_{j-k}^T )^{l-1}\right\|_F\\
\le\ & \left\| \bm W_{j+1-k}^T\bm W_{j+1-k} - \bm W_{j-k}\bm W_{j-k}^T \right\|_F \sum_{l=1}^k \|\bm W_{j+1-k}\|^{2(k-l)}\|\bm W_{j-k}\|^{2(l-1)}\\
\le\ & k\left( \frac{3\sigma_{\max}^*}{2} \right)^{2(k-1)}\left\| \bm W_{j+1-k}^T\bm W_{j+1-k} - \bm W_{j-k}\bm W_{j-k}^T \right\|_F.
\end{align*}
This, together with \eqref{eq1:lem pushl}, yields
\begin{align*}
   \left\|\bm W_{j:i}^T \bm W_{j:i} - (\bm W^T_i\bm W_i)^{j-i+1}\right\|_F & \leq \left(\sum_{k=1}^{j-i} k\right) \frac{\sqrt{2}}{2\lambda}\left(\frac{3\sigma_{\max}^*}{2}\right)^{2j-2i+1}\|\nabla G(\bm W)\|_F  \\
& \le \frac{(j-i)(j-i+1)}{2\sqrt{2}\lambda}\left(\frac{3\sigma_{\max}^*}{2}\right)^{2j-2i+1}\|\nabla G(\bm W)\|_F.
\end{align*}
Using the same argument, we prove \eqref{eq:transwij}. Then, we complete the proof. 
\end{proof} 

\begin{proof}[Proof of \Cref{coro1}]
 For ease of exposition, let 
     \begin{align*}
         & \bm R_1(l) := \bm W_{l-1:1} \bm W_{l-1:1}^T - (\bm W_l^T\bm W_l)^{l-1},\ l =2,3,\dots,L,\ \bm R_1(1) := \bm 0, \\
         & \bm R_2(l) := \bm W_{L:l+1}^T \bm W_{L:l+1}  -  (\bm W_l\bm W_l^T)^{L-l},\ l=1,2,\dots,L-1,\ \bm R_2(L) := \bm 0.  
     \end{align*}
For each $l \in [L]$, we compute
\begin{align*}
         & \left\|(\bm W_{l-1}\bm W_{l-1}^T)^{l-1}- (\bm W_l^T\bm W_l)^{l-1}\right\|_F \le \left\|(\bm W_{l-1}\bm W_{l-1}^T)^{l-2}(\bm W_{l-1}\bm W_{l-1}^T -\bm W_l^T\bm W_l ) \right\|_F + \cdots \\
         &  +\left\| (\bm W_{l-1}\bm W_{l-1}^T -\bm W_l^T\bm W_l )(\bm W_l^T\bm W_l)^{l-2} \right\|_F   \le \frac{3\sqrt{2}(l-1)\sigma_{\max}^*}{4\lambda} \left(\frac{3\sigma_{\max}^*}{2}\right)^{2l-4} \|\nabla G(\bm W)\|_F,
\end{align*} 
where the last inequality uses \eqref{eq:S norm W} and \eqref{eq:balance} in \Cref{lem:pre}. This, together with the triangular inequality and \eqref{eq:transwij}, yields for $l =2,3,\dots,L$, 
\begin{align}\label{eq1:coro1}
\|\bm R_1(l)\|_F \le &\ \left\|\bm W_{l-1:1} \bm W_{l-1:1}^T -(\bm W_{l-1}\bm W_{l-1}^T)^{l-1} \right\|_F + \left\| (\bm W_{l-1}\bm W_{l-1}^T)^{l-1}- (\bm W_l^T\bm W_l)^{l-1}\right\|_F \notag \\
% \le &\ \left( \frac{(l-1)(l-2)}{2\sqrt{2}\lambda}\left(\frac{3\sigma_{\max}^*}{2}\right)^{2l-3} + \frac{3\sqrt{2}(l-1)\sigma_{\max}^*}{4\lambda} \left(\frac{3\sigma_{\max}^*}{2}\right)^{2l-4} \right) \|\nabla G(\bm W)\|_F \notag\\
\le &\ \frac{l(l-1)}{2\sqrt{2}\lambda} \left(\frac{3\sigma_{\max}^*}{2}\right)^{2l-3} \|\nabla G(\bm W)\|_F.
\end{align}    
Using the same argument, we have for $l = 1,2,\dots,L-1$, 
\begin{align}\label{eq2:coro1}
\|\bm R_2(l)\|_F \le \frac{(L-l+1)(L-l)}{2\sqrt{2}\lambda} \left(\frac{3\sigma_{\max}^*}{2}\right)^{2(L-l)-1} \|\nabla G(\bm W)\|_F.
\end{align}
For each $l \in [L]$, we compute  
     \begin{align*}
     \frac{1}{2}\nabla_{\bm W_l} G(\bm W) &\overset{\eqref{eq:G}}{=} \bm W_{L:l+1}^T(\bm W_L\cdots\bm W_1- \sqrt{\lambda}\bm Y) \bm W_{l-1:1}^T + \lambda\bm W_l  = (\bm W_l\bm W^T_l)^{L-1}\bm W_l  - \\
     & \sqrt{\lambda}\bm W_{L:l+1}^T \bm Y \bm W_{l-1:1}^T +  \lambda\bm W_l + 
     \bm R_2(l)\bm W_l\bm W_{l-1:1}\bm W_{l-1:1}^T + \left(\bm W_l\bm W_l^T\right)^{L-l}\bm W_l\bm R_1(l).
     \end{align*}
     This, together with the triangle inequality, \eqref{eq1:coro1}, and \eqref{eq2:coro1}, yields for each $l \in [L]$, 
        \begin{align}
            &\left\| (\bm W_l \bm W^T_l)^{L-1} \bm W_l - \sqrt{\lambda}\bm W_{L:l+1}^T \bm Y \bm W_{l-1:1}^T + \lambda \bm W_l\right\|_F \notag \\
            \leq\ & \left\|\bm R_2(l)\bm W_l \bm W_{l-1:1}\bm W_{l-1:1}^T\right\|_F+\left\|(\bm W_{l}\bm W_{l}^T)^{L-l}\bm W_l\bm R_1(l)\right\|_F + \frac{1}{2}\|\nabla G(\bm W)\|_F \notag \\
            \leq\ & \left( \left(\frac{3\sigma^*_{\max}}{2}\right)^{2L-2}\frac{(L-l)(L-l+1)+(l-1)l}{2\sqrt{2}\lambda}+\frac{1}{2} \right) \|\nabla G(\bm W)\|_F, 
        \end{align}
   which directly implies \eqref{eq:coro1}. 
\end{proof}

Next, using \Cref{lem:pre} and the SVD of $\bm W_l$ in \eqref{eq:SVD Wl}, we proceed to the proof of \Cref{prop:Hlapproximate}. 

\begin{proof}[Proof of \Cref{prop:Hlapproximate}] 
Let $\bm W^* = (\bm W_1^*,\dots,\bm W_L^*) \in \mathcal{W}_{\bm \sigma^*}$ be such that $\mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) = \|\bm W - \bm W^* \|_F$.  For ease of exposition, let $\bm H_l := \bm U_{l-1}^T\bm V_l$ for each $l \in \{2,\dots,L\}$. According to \eqref{eq:SVD Wl}, the $(i,j)$-th block of $\bm H_l$, denoted by $\bm H_{l}^{(i,j)}$, is  
\begin{align}\label{eq:Hlij}
    \bm H_{l}^{(i,j)} := \bm U_{l-1}^{(i)^T}\bm V_{l}^{(j)},\ \forall (i,j) \in [p+1] \times [p+1]. 
\end{align} 
According to \eqref{eq:dist 2} with $\delta_{\sigma} \le \sigma_{\min}^*$ and \Cref{lem:pre}, we obtain  \eqref{eq:balance}. Substituting \eqref{eq:SVD Wl} and $\bm H_l = \bm U_{l-1}^T\bm V_l$ into \eqref{eq:balance} yields 
\begin{align*}
    \left\| \bm H_l\bm{\Sigma}_l^T\bm{\Sigma}_l -  \bm{\Sigma}_{l-1}\bm{\Sigma}_{l-1}^T\bm H_l  \right\|_F \leq \frac{3\sqrt{2}\sigma^*_{\max}}{4\lambda} \|\nabla G(\bm W)\|_F.
\end{align*} 
Using this and the block structures of $\bm \Sigma_l$ in \eqref{eq:SVD Wl} and $\bm H_l$ in \eqref{eq:Hlij}, we obtain
\begin{align}\label{eq1:prop Hla}
    &\ \sum_{j=1}^{p+1} \sum_{i=1, i \neq j}^{p+1} \| \bm H_{l}^{(i,j)}\bm \Sigma_{l}^{(j)^T}\bm \Sigma_{l}^{(j)} - \bm \Sigma_{l-1}^{(i)}\bm \Sigma_{l-1}^{(i)^T} \bm H_{l}^{(i,j)} \|_F^2 \notag \\
\le &\    \sum_{j=1}^{p+1} \sum_{i=1}^{p+1} \| \bm H_{l}^{(i,j)}\bm \Sigma_{l}^{(j)^T}\bm \Sigma_{l}^{(j)} - \bm \Sigma_{l-1}^{(i)}\bm \Sigma_{l-1}^{(i)^T} \bm H_{l}^{(i,j)} \|_F^2 \le \left( \frac{3\sqrt{2}\sigma^*_{\max}}{4\lambda}\right)^2  \|\nabla G(\bm W)\|_F^2. 
\end{align} 
For each  $l \in [L]$ and $i \in [p+1]$, we have 
$
    \|\bm \Sigma_{l,i} - \sigma^*_{t_i} \bm I\| \le \|\bm W_l - \bm W_l^*\|  \leq \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) \le {\delta_{\sigma}}/{3},$ where the first inequality follows from Weyl's inequality and the last inequality uses \eqref{eq:dist 2}. This implies 
\begin{align} 
    \sigma^*_{t_i} - \frac{\delta_{\sigma}}{3} \le \lambda_{\min}(\bm \Sigma_{l,i}) \le \lambda_{\max}(\bm \Sigma_{l,i}) \le \sigma^*_{t_i} + \frac{\delta_{\sigma}}{3}.
\end{align} 
For each $l \in [L]$ and $i, j \in [p+1]$ with $i > j$, we compute
\begin{align}
    & \|\bm H_{l}^{(i,j)}\bm \Sigma_{l}^{(j)^T}\bm \Sigma_{l}^{(j)}\|_F \ge \lambda_{\min}^2(\bm \Sigma_{l}^{(j)})\|\bm H_{l}^{(i,j)}\|_F \ge  \left(  \sigma^*_{t_j} - \frac{\delta_{\sigma}}{3} \right)^2 \|\bm H_{l}^{(i,j)}\|_F, \label{eq2:prop Hla}\\
    & \|\bm \Sigma_{l-1}^{(i)}\bm \Sigma_{l-1}^{(i)^T}\bm H_{l}^{(i,j)}\|_F^2 \le \lambda_{\max}\left( \bm \Sigma_{l-1}^{(i)}\bm \Sigma_{l-1}^{(i)^T} \right)\|\bm H_{l}^{(i,j)}\|_F \le \left(  \sigma^*_{t_i} + \frac{\delta_{\sigma}}{3} \right)^2\|\bm H_{l}^{(i,j)}\|_F. \label{eq3:prop Hla}
\end{align}
For each $l \in [L]$ and  $i > j \in [p+1]$, we bound
\begin{align*}
&\quad \| \bm H_{l}^{(i,j)}\bm \Sigma_{l}^{(j)^T}\bm \Sigma_{l}^{(j)} - \bm \Sigma_{l-1}^{(i)}\bm \Sigma_{l-1}^{(i)^T} \bm H_{l}^{(i,j)} \|_F \ge \| \bm H_{l}^{(i,j)}\bm \Sigma_{l}^{(j)^T}\bm \Sigma_{l}^{(j)}\|_F - \|\bm \Sigma_{l-1}^{(i)}\bm \Sigma_{l-1}^{(i)^T} \bm H_{l}^{(i,j)}\|_F \\
& \ge  \left(  \left(  \sigma^*_{t_j} - \frac{\delta_{\sigma}}{3} \right)^2 - \left(  \sigma^*_{t_i} + \frac{\delta_{\sigma}}{3} \right)^2 \right) \|\bm H_{l}^{(i,j)}\|_F 
\ge \frac{2\delta_{\sigma} \sigma^*_{\min}}{3}\|\bm H_{l}^{(i,j)}\|_F,
\end{align*}
where the second equality follows from \eqref{eq2:prop Hla} and \eqref{eq3:prop Hla}, and the last inequality is due to \eqref{eq:delta sigma}. Applying the same argument to the case where $i < j$, we obtain the same result. These, together with \eqref{eq1:prop Hla}, yield 
\begin{align}\label{eq4:prop Hla}
\sum_{j=1}^{p+1} \sum_{ i=1, i \neq j}^{p+1} \|\bm H_{l}^{(i,j)}\|_F^2  \le \frac{81\sigma_{\max}^{*2}}{32\delta_{\sigma}^2\lambda^2\sigma^{*2}_{\min}}\|\nabla G(\bm W)\|_F^2. 
\end{align}
For each $l \in [L]$ and $i\in [p+1]$, let $\bm H_{l}^{(i,i)} = \bm P_{l}^{(i)}\bm \Lambda_{l}^{(i)}\bm Q_{l}^{(i)^T}$ be a full SVD of $\bm H_{l}^{(i,i)}$, where $\bm P_{l}^{(i)}, \bm Q_{l}^{(i)}$ are square orthogonal matrix. Then, we compute
\begin{align*}
 \|\bm H_{l}^{(i,i)}- \bm P_{l}^{(i)}\bm Q_{l}^{(i)^T}\|_F^2 & = \sum_{k=1}^{g_i} \left( 1 - \sigma_k(\bm H_{l}^{(i,i)}) \right)^2 \le \sum_{k=1}^{g_i} \left( 1 - \sigma_k^2(\bm H_{l}^{(i,i)}) \right)^2   \\
 & = \left\| \bm I -  \bm H_{l}^{(i,i)}\bm H_{l}^{(i,i)^T} \right\|_F^2 = \sum_{j=1, j\neq i}^{p + 1} \|\bm H_{l}^{(i,j)}\bm H_{l}^{(i,j)^T}\|_F^2 \le  \sum_{j=1,j\neq i}^{p + 1} \|\bm H_{l}^{(i,j)}\|_F^2,
\end{align*}  
where the last equality is due to 
$[\bm H_{l}^{(i,1)},\dots,\bm H_{l}^{(i,p+1)}] = \bm U_{l-1}^{(i)^T}\bm V_l$ with $\bm U_{l-1},\ \bm V_l \in \mathcal{O}^{d_{l-1}}$,
and last inequality follows from $\|\bm H_{l}^{(i,j)}\| \le 1$ for all $l\in [L]$ and $i,j \in [p+1]$. Therefore, we obtain 
\begin{align*}
\sum_{i=1}^{p+1} \|\bm H_{l}^{(i,i)}- \bm P_{l}^{(i)}\bm Q_{l}^{(i)^T}\|_F^2 \le \sum_{i=1}^{p+1} \sum_{j=1, j\neq i}^{p + 1} \|\bm H_{l}^{(i,j)}\|_F^2. 
\end{align*}
This, together with \eqref{eq4:prop Hla} and $\bm T_{l}^{(i)} =\bm P_{l}^{(i)}\bm Q_{l}^{(i)^T}$, yields 
\begin{align*}
\|\bm H_l - \mathrm{BlkDiag}(\bm T_{l}^{(1)},\dots,\bm T_{l}^{(p)},\bm T_{l}^{(p+1)})\|_F^2 \le \frac{81\sigma_{\max}^{*2}}{16\delta_{\sigma}^2\lambda^2\sigma^{*2}_{\min}}\|\nabla G(\bm W)\|_F^2,
\end{align*}
which directly implies \eqref{eq:Hlapproximate}. 
\end{proof}

Now, we prove two key inequalities in \Cref{lem:twoinequality}, which will be frequently used in the subsequent analysis. 

\begin{proof}[Proof of \Cref{lem:twoinequality}]
Let $\bm W^* = (\bm W_1^*,\dots,\bm W_L^*)$ be such that $\mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) = \|\bm W - \bm W^*\|_F$. It follows from \Cref{lem:pre} and \Cref{coro1} that \eqref{eq:S norm W}-\eqref{eq:wl+1} and \eqref{eq:coro1} holds. Recall that \eqref{eq:SVD Wl} denotes an SVD of $\bm W_l$ for each $l \in [L]$.  For ease of exposition, let $\bm H_l:=\bm U_{l-1}^T\bm V_l$  and $\bm T_l := \mathrm{BlkDiag}\left(\bm T_{l}^{(1)},\dots,\bm T_{l}^{(p)},\bm T_{l}^{(p+1)}\right)$ for each $l = 2,\dots,L$. This, together with \eqref{eq:Hlapproximate}, implies 
\begin{align}\label{eq0:lem two}
\left\|\bm H_l - \bm T_l \right\|_F \le \frac{9\sigma_{\max}^{*}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}}\|\nabla G(\bm W)\|_F,\ l=2,\dots,L. 
\end{align}
First, we are devoted to proving \eqref{eq:twoinequality1}. It follows from \eqref{eq:coro1} with $l=L$ that  
\begin{align}\label{eq1:lem two}
\left\| (\bm W_L \bm W^T_L)^{L-1} \bm W_L - \sqrt{\lambda} \bm Y \bm W_{L-1:1}^T + \lambda \bm W_L \right\|_F \le c_1\|\nabla G(\bm W)\|_F,
\end{align}
where $c_1$ is defined in \eqref{eq:c1}. Using the SVD \eqref{eq:SVD Wl}, we have
\begin{align*}
& \left\| (\bm W_L \bm W^T_L)^{L-1} \bm W_L - \sqrt{\lambda} \bm Y \bm W_{L-1:1}^T + \lambda \bm W_L \right\|_F \\ % = \left\| (\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L - \sqrt{\lambda}\bm U_L^T\bm Y \bm W_{L-1:1}^T \bm V_L + \lambda \bm \Sigma_L \right\|_F \\
= & \left\| (\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L - \sqrt{\lambda}\bm U_L^T\bm Y \bm V_1 \prod_{l=1}^{L-1} \bm \Sigma_l^T \bm H_{l+1} + \lambda \bm \Sigma_L \right\|_F \\
\ge & \left\| (\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L  - \sqrt{\lambda}\bm U_L^T\bm Y \bm V_1 \prod_{l=1}^{L-1} \bm \Sigma_l^T \bm T_{l+1} \right\|_F - \sqrt{\lambda}\left\| \bm Y \bm V_1\bm \Sigma_1^T(\bm H_2 - \bm T_2)\prod_{l=2}^{L-1} \bm \Sigma_l^T \bm H_{l+1} \right\|_F \\
& -  \sqrt{\lambda}\left\| \bm Y \bm V_1\bm \Sigma_1^T\bm T_2 \bm \Sigma_2^T(\bm H_3-\bm T_3)\prod_{l=3}^{L-1} \bm \Sigma_l^T \bm H_{l+1} \right\|_F -\dots - \sqrt{\lambda}\left\| \bm Y \bm V_1\left( \prod_{l=1}^{L-2}\bm \Sigma_l^T\bm T_{l+1} \right)\bm \Sigma_{L-1}^T(\bm H_{L} - \bm T_L) \right\|_F \\
% \ge & \left\| (\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L  - \sqrt{\lambda}\bm U_L^T\bm Y \bm V_1 \prod_{l=1}^{L-1} \bm \Sigma_l^T \bm T_{l+1} \right\|_F - \sqrt{\lambda}y_1\left( \frac{3}{2}\sigma_{\max}^* \right)^{L-1}  \sum_{l=2}^{L}\|\bm H_l- \bm T_l\|_F \\
\ge & \left\| (\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L  - \sqrt{\lambda}\bm U_L^T\bm Y \bm V_1 \prod_{l=1}^{L-1} \bm \Sigma_l^T \bm T_{l+1} \right\|_F - \left( \frac{3}{2}\sigma_{\max}^* \right)^{L}\frac{3 y_1 L}{2\sqrt{\lambda} \delta_{\sigma}\sigma^{*}_{\min}}\|\nabla G(\bm W)\|_F, 
\end{align*}
where  the first inequality uses the triangle inequality  and the last inequality follows from \eqref{eq:S norm W} in \Cref{lem:pre}, $\|\bm H_l\| = 1$ and $\|\bm T_l\| = 1$ for all $l \in [L]$, and \eqref{eq0:lem two}. This, together with \eqref{eq1:lem two}, implies 
\small
\begin{align}\label{eq2:lem two}
&\ \left\| (\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L  - \sqrt{\lambda}\bm U_L^T\bm Y \bm V_1 \prod_{l=1}^{L-1} \bm \Sigma_l^T \bm T_{l+1} \right\|_F\notag\\
\le& \ \left( \left( \frac{3}{2}\sigma_{\max}^* \right)^{L}\frac{3 y_1 L}{2 \sqrt{\lambda}\delta_{\sigma}\sigma^{*}_{\min}} + c_1\right) \|\nabla G(\bm W)\|_F.  
\end{align}
\normalsize
According to the block structure of $\bm \Sigma_l$ in \eqref{eq:SVD Wl} and $\bm T_l = \mathrm{BlkDiag}\left(\bm T_{l}^{(1)},\dots,\bm T_{l}^{(p)},\bm T_{l}^{(p+1)} \right)$, we compute 
\begin{align*}
        \prod_{l=1}^{L-1}\bm\Sigma_l^T \bm T_{l+1} = \mathrm{BlkDiag}\left(\prod_{l=1}^{L-1}\bm\Sigma_{l}^{(1)}\bm T_{l+1}^{(1)},\dots,\prod_{l=1}^{L-1}\bm\Sigma_{l}^{(p)}\bm T_{l+1}^{(p)},\prod_{l=1}^{L-1}\bm\Sigma_{l}^{(p+1)^T}\bm T_{l+1}^{(p+1)}\right). 
\end{align*}
Using \eqref{eq:wl+1} in \Cref{lem:pre}, we have for each $l \in [L]$ and $i \in [p]$, 
    \begin{align*}
    \|\bm\Sigma_{l}^{(i)} -\bm \Sigma_{L}^{(i)}\|_F^2 & = \sum_{j=1}^{g_i} \left( \sigma_j(\bm\Sigma_{l}^{(i)}) - \sigma_j(\bm \Sigma_{L}^{(i)})  \right)^2 = \sum_{j=1}^{g_i} \left( \sum_{k=l}^{L-1} \left( \sigma_j(\bm\Sigma_{k}^{(i)}) - \sigma_j(\bm\Sigma_{k+1}^{(i)})\right)  \right)^2\\
    & \le g_i (L-1)^2 \left(\frac{3\sqrt{2}\sigma_{\max}^*}{4\lambda\sigma_{\min}^*} \|\nabla G(\bm W)\|_F\right)^2,
    \end{align*}
    which implies
    \begin{align}\label{eq3:lem two}
    \|\bm\Sigma_{l}^{(i)} -\bm \Sigma_{L}^{(i)}\|_F \le  \frac{3\sqrt{2g	_{\max}} L\sigma_{\max}^*}{4\lambda\sigma_{\min}^*}  \|\nabla G(\bm W)\|_F. 
    \end{align}
    We claim that it holds for each $l \in [L]$ and $i \in [p]$ that 
    \begin{align}\label{eq4:lem two}
        \|\bm T_{l}^{(i)} \bm \Sigma_{L}^{(i)} - \bm \Sigma_{L}^{(i)} \bm T_{l}^{(i)}\|_F \leq \frac{\eta_1}{\sigma_{\min}^*} \|\nabla G(\bm W)\|_F,
    \end{align}
    where $\eta_1$	is defined in \eqref{eq:eta1}. Using the triangular inequality, we obtain    
    \begin{align}\label{eq5:lem two}
    &\ \left\|\prod_{l=1}^{L-1} \bm{\Sigma}_{L}^{(i)} \bm{T}_{l+1}^{(i)} - \left( \prod_{l=1}^{L-1} \bm{T}_{l+1}^{(i)} \right) \left(\bm{\Sigma}_{L}^{(i)}\right)^{L-1} \right\|_F  \notag \\
    \le &\  \sum_{j=2}^L  \left\|\underbrace{\left(\prod_{l=1}^{L-j} \bm{\Sigma}_{L}^{(i)} \bm{T}_{l+1}^{(i)}\right)\left(\bm \Sigma_{L}^{(i)} \prod_{k=L-j+2}^L \bm T_{k}^{(i
    )}  -\left(\prod_{k=L-j+2}^L \bm T_{k}^{(i)} \right)\bm\Sigma_{L}^{(i)}\right)\left(\bm\Sigma_{L}^{(i)}\right)^{j-2}}_{\bm 
R_j} \right\|_F. 
%    \left\|\left(\prod_{l=1}^{L-2} \bm{\Sigma}_{L,i} \bm{T}_{l+1,i}\right)\left(\bm \Sigma_{L,i} \bm T_{L,i} - \bm T_{L,i} \bm \Sigma_{L,i}\right)\right\|_F + \\
%    &\ \left\|\left(\prod_{l=1}^{L-3} \bm{\Sigma}_{L,i} \bm{T}_{l+1,i}\right)\left(\bm \Sigma_{L,i} \bm T_{L-1,i} \bm T_{L,i} \bm \Sigma_{L,i} - \bm T_{L-1,i} \bm T_{L,i} \bm \Sigma_{L,i}^2\right)\right\|_F + \cdots + \\
%    &\  \left\|\underbrace{\left(\prod_{l=1}^{L-j} \bm{\Sigma}_{L,i} \bm{T}_{l+1,i}\right)\left(\bm \Sigma_{L,i} \prod_{k=L-j+2}^L \bm T_{k,i}  -\prod_{k=L-j+2}^L \bm T_{k,i} \bm\Sigma_{L,i}\right)\bm\Sigma^{j-2}_{L,i}}_{\bm 
% R_j} \right\|_F + \dots + \\
%   &\ \left\|\bm \Sigma_{L,i} \left(\prod_{k=2}^L \bm T_{k,i}\right) \bm \Sigma_{L,i}^{L-2} - \left(\prod_{k=2}^L \bm T_{k,i}\right)\bm \Sigma^{L-1}_{L,i} \right\|_F. 
    \end{align}
For each $\|\bm R_j\|_F$, we bound
\begin{align*}
    \|\bm R_j\|_F&\le \left(\frac{3\sigma^*_{\max}}{2}\right)^{L-2}\left\|\bm \Sigma_{L}^{(i)} \prod_{k=L-j+2}^L \bm T_{k}^{(i)} - \left(\prod_{k=L-j+2}^L \bm T_{k}^{(i)}\right) \bm\Sigma_{L}^{(i)}\right\|_F\\
    &\le \left(\frac{3\sigma^*_{\max}}{2}\right)^{L-2}\left\|\left(\bm \Sigma_{L}^{(i)}\bm T_{L-j+2}^{(i)}-\bm T_{L-j+2}^{(i)}\bm \Sigma_{L}^{(i)}\right)\prod_{k=L-j+3}^L \bm T_{k}^{(i)}\right\|_F \\
    &\quad+  \left(\frac{3\sigma^*_{\max}}{2}\right)^{L-2}\left\|\bm T_{L-j+2}^{(i)}\left(\bm \Sigma_{L}^{(i)} \bm T_{L-j+3}^{(i)}- \bm T_{L-j+3}^{(i)}\bm \Sigma_{L}^{(i)}\right)\prod_{k=L-j+4}^L\bm T_{k}^{(i)} \right \|_F + \cdots\\
    &\quad+ \left( \frac{3\sigma^*_{\max}}{2} \right)^{L-2} \left\| \prod_{k=L-j+2}^{L-1} \bm T_{k}^{(i)} (\bm \Sigma_{L}^{(i)}\bm T_{L}^{(i)}-\bm T_{L}^{(i)}\bm \Sigma_{L}^{(i)})\right\|_F\\
    &\le (j-1)\left(\frac{3\sigma^*_{\max}}{2}\right)^{L-2} \frac{\eta_1}{\sigma^*_{\min}}\|\nabla G(\bm W)\|_F,  
\end{align*}
where the first inequality uses (i) of \Cref{lem:pre} and $\|\bm T_{l+1}^{(i)}\| = 1$ for all $l$ and $i$, the second one is due the triangular inequality, and the last one follows from \eqref{eq4:lem two}. This, together with \eqref{eq5:lem two}, implies 
\begin{align}\label{eq6:lem two}
\left\|\prod_{l=1}^{L-1} \bm{\Sigma}_{L}^{(i)} \bm{T}_{l+1}^{(i)} - \left( \prod_{l=1}^{L-1} \bm{T}_{l+1}^{(i)} \right) \left(\bm{\Sigma}_{L}^{(i)}\right)^{L-1} \right\|_F \leq  \left(\frac{3\sigma_{\max}}{2}\right)^{L-2}\frac{L^2\eta_1}{2\sigma^*_{\min}}\|\nabla G(\bm W)\|_F. 
\end{align} 
For each $i \in [p]$, we have 
    \begin{align*}
    &\ \left\|\prod_{l=1}^{L-1} \bm{\Sigma}_{l}^{(i)} \bm{T}_{l+1}^{(i)} - \left( \prod_{l=1}^{L-1} \bm{T}_{l+1}^{(i)} \right) \left(\bm{\Sigma}_{L}^{(i)}\right)^{L-1} \right\|_F \notag \\
    \leq &\ \left\| \prod_{l=1}^{L-1} \bm{\Sigma}_{L}^{(i)}\bm{T}_{l+1}^{(i)} 
    - \left( \prod_{l=1}^{L-1} \bm{T}_{l+1}^{(i)} \right) (\bm{\Sigma}_{L}^{(i)})^{L-1} \right\|_F 
    + \left\|(\bm{\Sigma}_{1}^{(i)} - \bm{\Sigma}_{L}^{(i)}) \bm{T}_{2}^{(i)} \prod_{l=2}^{L-1} \bm{\Sigma}_{l}^{(i)} \bm{T}_{l+1}^{(i)} \right\|_F +  \notag \\
    &\ \left\| \bm{\Sigma}_{L}^{(i)} \bm{T}_{2}^{(i)}(\bm{\Sigma}_{2}^{(i)} - \bm{\Sigma}_{L}^{(i)}) \bm{T}_{3}^{(i)} \prod_{l=3}^{L-1} \bm{\Sigma}_{l}^{(i)} \bm{T}_{l+1}^{(i)} \right\|_F + \cdots + \left\|\prod_{l=1}^{L-2} \bm{\Sigma}_{l}^{(i)} \bm{T}_{l+1}^{(i)} (\bm{\Sigma}_{L-1}^{(i)} - \bm{\Sigma}_{L}^{(i)}) \bm{T}_{L}^{(i)}\right\|_F \\
    \leq &\ \left(\frac{3\sigma_{\max}^*}{2}\right)^{L-2}\left(\frac{L^2\eta_1 }{2\sigma^*_{\min}}+  \frac{3\sqrt{2g_{\max}}L^2\sigma_{\max}^*}{4\lambda\sigma_{\min}^*} \right)\|\nabla G(\bm W)\|_F,
    \end{align*}
    where the first inequality uses the triangular inequality and the last inequality follows from \eqref{eq6:lem two}, \eqref{eq:S norm W} in \Cref{lem:pre}, \( \|\bm T_l\| = 1 \) for all \( l \in [L] \), and \eqref{eq3:lem two}. This, together with \eqref{eq:B}, yields 
\begin{align}\label{eq7:lem two}
 &\ \left\|\prod_{l=1}^{L-1}\bm\Sigma_l^T \bm T_{l+1} - \mathrm{BlkDiag}\left(\bm B_1, \dots, \bm B_{p}, \bm B_{p+1}\right)\right\|_F \notag\\
\le &\ \sum_{i=1}^p \left\|\prod_{l=1}^{L-1} \bm{\Sigma}_{l}^{(i)} \bm{T}_{l+1}^{(i)} - \left( \prod_{l=1}^{L-1} \bm{T}_{l+1}^{(i)} \right) \left(\bm{\Sigma}_{L}^{(i)}\right)^{L-1} \right\|_F \notag \\
\le &\ p \left(\frac{3\sigma_{\max}^*}{2}\right)^{L-2}\left(\frac{L^2\eta_1 }{2\sigma^*_{\min}}+  \frac{3\sqrt{2g_{\max}}L^2\sigma_{\max}^*}{4\lambda\sigma_{\min}^*} \right) \|\nabla G(\bm W)\|_F. 
\end{align}
Now, we are ready to prove \eqref{eq:twoinequality1}. Specifically, we have
	\begin{align*}
	&\ \left\|(\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L - \sqrt{\lambda}\bm U_L^T \bm Y \bm V_1 \mathrm{BlkDiag}\left(\bm B_1, \dots, \bm B_{p}, \bm B_{p+1}\right) \right\|_F \\ 
	\le &\ \left\|(\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L - \sqrt{\lambda}\bm U_L^T \bm Y \bm V_1 \prod_{l=1}^{L-1}\bm\Sigma_l^T \bm T_{l+1} \right\|_F +  \\
&\quad \sqrt{\lambda}y_1\left\|\prod_{l=1}^{L-1}\bm\Sigma_l^T \bm T_{l+1} - \mathrm{BlkDiag}\left(\bm B_1, \dots, \bm B_{p}, \bm B_{p+1}\right)\right\|_F \le c_2 \|\nabla G(\bm W)\|_F, 
\end{align*}	 
where $c_2$ is defined in \eqref{eq:c2} and the last inequality follows from \eqref{eq2:lem two} and \eqref{eq7:lem two}. Applying the same argument to $\nabla_{\bm W_1} G(\bm W)$, we obtain \eqref{eq:twoinequality2}.

The rest of the proof is devoted to proving the claim \eqref{eq4:lem two}. According to \eqref{eq:SVD Wl} and $\bm H_l=\bm U_{l-1}^T\bm V_l$  for each $l=2,\dots,L$, we have for each $i \in [p]$, 
        \begin{align*}
   \quad& \|\bm W_l^T\bm W_{l}- \bm W_{l-1}\bm W_{l-1}^T\|_F = \|\bm H_l \bm \Sigma_l^T \bm \Sigma_l \bm H_l^T - \bm \Sigma_{l-1}\bm \Sigma_{l-1}^T\|_F \\
    \ge & \|\bm T_l \bm \Sigma_l^T \bm \Sigma_l \bm T_l^T - \bm \Sigma_{l-1}\bm \Sigma_{l-1}^T\|_F - 2\|\bm \Sigma_l\|^2 \|\bm H_l - \bm T_l\|_F \\
     \ge & \|\bm T_{l}^{(i)} (\bm \Sigma_{l}^{(i)})^2 - (\bm \Sigma_{l-1}^{(i)})^2\bm T_{l}^{(i)}\|_F - 2\|\bm \Sigma_l\|^2 \|\bm H_l - \bm T_l\|_F \\
      \ge & \|\bm T_{l}^{(i)} (\bm\Sigma_{L}^{(i)})^2 -(\bm\Sigma_{L}^{(i)})^2\bm T_{l}^{(i)}\|_F-2\|\bm \Sigma_l\|^2\|\bm H_l - \bm T_l\|_F -\|(\bm\Sigma_{L}^{(i)})^2-(\bm\Sigma_{l}^{(i)})^2\|_F -\|(\bm\Sigma_{L}^{(i)})^2-(\bm\Sigma_{l-1}^{(i)})^2\|_F\\
      \ge & \|\bm T_{l}^{(i)} (\bm\Sigma_{L}^{(i)})^2 -(\bm\Sigma_{L}^{(i)})^2\bm T_{l}^{(i)}\|_F-2\|\bm \Sigma_l\|^2\|\bm H_l - \bm T_l\|_F -3\sigma^*_{\max}\left(\|\bm\Sigma_{L}^{(i)}-\bm\Sigma_{l}^{(i)}\|_F +\|\bm\Sigma_{L}^{(i)}-\bm\Sigma_{l-1}^{(i)}\|_F\right),
    \end{align*}
    where the second inequality follows from $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$ for each $i \in [p]$ and the last inequality follows from \eqref{eq:S norm W}. This together with  \eqref{eq:balance}, \eqref{eq0:lem two}, and \eqref{eq3:lem two}, yields that for each $l=2,\dots,L$ and $i \in [p]$, we have 
    \begin{align}\label{eq9:lem two}
    \|\bm T_{l}^{(i)} (\bm\Sigma_{L}^{(i)})^2 -(\bm\Sigma_{L}^{(i)})^2\bm T_{l}^{(i)}\|_F \le \eta_1\|\nabla G(\bm W)\|_F.  
    \end{align}
For each $i \in [p]$, using Weyl's inequality yields for each $j \in [g_i]$, 
        \[
        \left|\sigma_j(\bm\Sigma_{L}^{(i)}) - \sigma_{t_i}^*\right| \leq \|\bm W_L - \bm W_L^*\| \overset{\eqref{eq:dist 1}}{\leq} \frac{\sigma_{\min}^*}{2},
        \]
 which implies \(\| \bm\Sigma_{L}^{(i)} -\sigma_{t_i}^* \bm I\| \le \sigma_{\min}^*/2 \). This, together with \Cref{lem:commute} and $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$, yields 
    \begin{align*}
 \|\bm T_{l}^{(i)} (\bm\Sigma_{L}^{(i)})^2 -(\bm\Sigma_{L}^{(i)})^2\bm T_{l}^{(i)}\|_F \ge \sigma_{\min}^* \|\bm T_{l}^{(i)} \bm\Sigma_{L}^{(i)} -\bm\Sigma_{L}^{(i)}\bm T_{l}^{(i)}\|_F. 
     \end{align*}
    Using this and \eqref{eq9:lem two}, we obtain \eqref{eq4:lem two}. 
\end{proof}

Since the assumptions are different for the cases $L=2$ and $L \ge 3$ to bound the singular values by the gradient norm, we prove \Cref{prop:singular value 2} by addressing these two scenarios separately. Each case requires a distinct approach to account for the structural differences due to the varying depth of the networks. 

\begin{proof}[Proof of \Cref{prop:singular value 2}]
Let $\bm W^* = (\bm W_1^*,\dots,\bm W_L^*)$ be such that $\mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) = \|\bm W - \bm W^*\|_F$.
According to \eqref{eq:dist 7} or \eqref{eq:dist 5}, we have $\mathrm{dist}(\bm W, \mathcal{W}) \le \delta_{\sigma}/3 < \sigma_{\min}^*/2$. This, together with \Cref{lem:pre} and \Cref{coro1}, implies that \eqref{eq:S norm W}-\eqref{eq:wl+1} and \eqref{eq:coro1} hold. According to $\mathrm{dist}(\bm W, \mathcal{W}) \le \delta_{\sigma}/3$, \Cref{prop:Hlapproximate}, and \Cref{lem:twoinequality}, there exist matrices $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$ for all $i \in [p]$ and $\bm T_{l}^{(p+1)} \in \mathcal{O}^{d_{l-1} - r_{\sigma}}$  such that \eqref{eq:Hlapproximate}, \eqref{eq:twoinequality2}, and \eqref{eq:twoinequality1} hold for all $l=2,\dots,L$, where $\bm A_i$ and $\bm B_i$ for each $i \in [p+1]$ are respectively defined in \eqref{eq:A} and \eqref{eq:B}. Recall that \eqref{eq:SVD Wl} denotes an SVD of $\bm W_l$ and let $\bm H_l := \bm U_{l-1}^T\bm V_l$ and $\bm T_l := \mathrm{BlkDiag}(\bm T_{l}^{(1)},\dots,\bm T_{l}^{(p)},\bm T_{l}^{(p+1)})$ for each $l=2,\dots,L$. This, together with \eqref{eq:Hlapproximate}, implies 
\begin{align}\label{eq0:prop singular 2}
\left\|\bm H_l - \bm T_l \right\|_F \le \frac{9\sigma_{\max}^{*}}{ 4\delta_{\sigma}\lambda\sigma^{*}_{\min}}\|\nabla G(\bm W)\|_F,\ l=2,\dots,L. 
\end{align}

(i) For ease of exposition, let $\bm \Psi := \bm U_2^T\bm Y\bm V_1 \in \R^{d_2\times d_0}$. Now, we partition $\bm \Psi$ into the following block form  
\begin{align}\label{eq2:prop singular}
\bm \Psi = 
\begin{bmatrix}
\bm \Psi^{(1,1)} & \cdots & \bm \Psi^{(1,p+1)} \\
\vdots & \ddots & \vdots \\
\bm \Psi^{(p+1,1)} & \cdots & \bm \Psi^{(p+1,p+1)}
\end{bmatrix},
\end{align}
where $\bm \Psi^{(i,j)} \in \R^{g_i \times g_j} $ for each $i,j \in [p]$, and $\bm \Psi^{(p+1, j)} \in \R^{(d_2 - r_{\sigma})\times g_j},\ \bm \Psi^{(i, p+1)} \in \R^{g_i \times (d_0 - r_{\sigma})}$ for each $i, j \in [p]$, and $\bm \Psi^{(p+1,p+1)} \in \R^{(d_2-r_\sigma)\times(d_0 - r_\sigma)}$.  According to \eqref{eq:twoinequality2}, we have 
\begin{align}
%\label{eq3:prop singular}
\notag
    c_2^2 \|\nabla G(\bm W)\|_F^2 \ge \left\|(\bm \Sigma_1\bm \Sigma_1^T)\bm \Sigma_1 + \lambda \bm \Sigma_1 - \sqrt{\lambda}\mathrm{BlkDiag}\left(\bm A_1, \dots, \bm A_{p}, \bm A_{p+1}\right)\bm \Psi \right\|_F^2.
\end{align}
    By dropping the diagonal blocks and the $(p+1)$-th row blocks of the matrix on the right-hand side, we have 
\begin{equation}\label{eq4:prop singular}
    c_2^2 \|\nabla G(\bm W)\|_F^2 \ge \lambda\sum_{i=1}^p \sum_{j=1,j\neq i}^{p+1}\|\bm A_i \bm\Psi^{(i,j)}\|_F^2 
    \ge  \frac{\lambda\sigma^{*2}_{\min}}{4}  \sum_{i=1}^p \sum_{j=1,j\neq i}^{p+1}\| \bm\Psi^{(i,j)}\|_F^2,
\end{equation}
where the second inequality follows from \eqref{eq:sigmalowerbound} and \eqref{eq:A}. Applying the same argument to \eqref{eq:twoinequality1} yields
\begin{align*} 
    c_2^2 \|\nabla G(\bm W)\|_F^2 \ge  \frac{\lambda\sigma^{*2}_{\min}}{4}  \sum_{j=1}^p \sum_{i=1,i\neq j}^{p+1}\| \bm\Psi^{(i,j)}\|_F^2. 
\end{align*}
This, together with \eqref{eq4:prop singular}, yields
\begin{equation}\label{eq5:prop singular}
    \frac{8c_2^2}{\lambda\sigma_{\min}^{*2}}\|\nabla G(\bm W)\|_F^2 \ge \sum_{i=1}^{p+1} \sum_{j=1,j\neq i}^{p+1}\| \bm\Psi^{(i,j)}\|_F^2.
\end{equation}
Using \eqref{eq:AS lambda}, we define
$\delta := \min\left\{ \min_{i \in [s_{p_Y}]}\left| \sqrt{\lambda} - y_i \right|, \sqrt{\lambda} \right\} > 0.$ Substituting this into \eqref{eq:grad 2} yields
\begin{align}
\frac{\delta}{3} \ge \frac{2\sqrt{2}c_2}{\sqrt{\lambda} \sigma_{\min}^*} \|\nabla G(\bm W)\|_F \ge \left\| \bm \Psi - \mathrm{BlkDiag}\left(\bm \Psi^{(1,1)},\dots, \bm \Psi^{(p+1,p+1)}\right) \right\|_F, 
\end{align} 
where the second inequality uses \eqref{eq2:prop singular} and \eqref{eq5:prop singular}. This, together with $\bm \Psi = \bm U_2^T\bm Y\bm V_1$ and Weyl's inequality, yields for all $i \in [d_{\min}]$, 
\begin{align*}
\frac{\delta}{3} & \ge\left| \sigma_i\left(\mathrm{BlkDiag} (\bm \Psi^{(1,1)},\dots, \bm \Psi^{(p+1,p+1)})\right) - y_i\right| \\ 
& \ge \left|y_i - \sqrt{\lambda}\right|  - \left| \sigma_i\left(\mathrm{BlkDiag} (\bm \Psi^{(1,1)},\dots, \bm \Psi^{(p+1,p+1)})\right) - \sqrt{\lambda} \right| \\
& \ge \delta - \left| \sigma_i\left(\mathrm{BlkDiag} (\bm \Psi^{(1,1)},\dots, \bm \Psi^{(p+1,p+1)})\right) - \sqrt{\lambda} \right|, 
\end{align*}  
which implies $\left| \sigma_i( \bm \Psi^{(p+1,p+1)} ) - \sqrt{\lambda} \right| \ge {2\delta}/{3}$ for all
$i \in [d_{\min}- r_{\sigma}]$. According to this, we obtain for all $i \in [d_{\min}-r_{\sigma}]$, 
\begin{align}\label{eq6:prop singular}
\left| \sigma_i^2( \bm \Psi^{(p+1,p+1)}) - \lambda \right| & = \left| \sigma_i( \bm \Psi^{(p+1,p+1)}) - \sqrt{\lambda} \right|  \left| \sigma_i( \bm \Psi^{(p+1,p+1)}) + \sqrt{\lambda} \right| \ge \frac{2\delta\sqrt{\lambda}}{3}.  
\end{align}
Using \eqref{eq:twoinequality2} and \eqref{eq:twoinequality1}, we have 
\begin{align}
    &c_2\|\nabla G(\bm W)\|_F \ge \left\|(\bm \Sigma_{1}^{(p+1)}\bm \Sigma_{1}^{(p+1)^T})\bm \Sigma_{1}^{(p+1)} +\lambda \bm
    \Sigma_{1}^{(p+1)} - \sqrt{\lambda} \bm T_{2}^{(p+1)} \bm \Sigma_{2}^{(p+1)^T} \bm \Psi^{(p+1,p+1)}\right\|_F,\label{eq7:prop singular}\\
    &c_2\|\nabla G(\bm W)\|_F \ge \left\|(\bm \Sigma_{2}^{(p+1)}\bm \Sigma_{2}^{(p+1)^T})\bm \Sigma_{2}^{(p+1)} +\lambda \bm\Sigma_{2}^{(p+1)} - \sqrt{\lambda}\bm\Psi^{(p+1,p+1)} \bm \Sigma_2^{(p+1)^T}\bm T_{2}^{(p+1)}\right\|_F.\label{eq8:prop singular}
\end{align}
According to Mirsky's inequality (see \Cref{lem:mirsky}), we have 
\begin{equation}\label{eq:Sigmaledist1}
\| \bm{\Sigma}_1^{(p+1)}\|_F = \| \bm{\Sigma}_1^{(p+1)} - \bm 0\|_F \le \| \bm{\Sigma}_1 - \bm\Sigma_1^*\|_F\le \|\bm W_1-\bm W_1^*\|_F \le  \mathrm{dist}(\bm{W}, \mathcal{W}_{\bm{\sigma}^*}).
\end{equation}
Similarly, we have
\begin{equation}\label{eq:Sigmaledist2}
\| \bm{\Sigma}_2^{(p+1)}\|_F \le  \mathrm{dist}(\bm{W}, \mathcal{W}_{\bm{\sigma}^*}).
\end{equation}
According to $\bm \Psi = \bm U_2^T\bm Y\bm V_1$ and \eqref{eq2:prop singular}, we have  
\begin{align}\label{eq9:prop singular}
    \| \bm{\Psi}^{(p+1,p+1)} \| \leq \| \bm{Y} \| = y_1.
\end{align}
Using this and \eqref{eq7:prop singular}, we obtain 
\small
\begin{align*}
c_2y_1\|\nabla G(\bm W)\|_F &\ge \left\|\left((\bm \Sigma_{1}^{(p+1)}\bm \Sigma_{1}^{(p+1)^T})\bm \Sigma_{1}^{(p+1)} +\lambda \bm
    \Sigma_{1}^{(p+1)} - \sqrt{\lambda} \bm T_{2}^{(p+1)} \bm \Sigma_{2}^{(p+1)^T} \bm \Psi^{(p+1,p+1)}\right) \bm{\Psi}^{(p+1,p+1)^T }\right\|_F \\
& \ge  \left\|\lambda \bm \Sigma_{1}^{(p+1)} \bm\Psi^{(p+1,p+1)^T} - \sqrt{\lambda} \bm T_{2}^{(p+1)} \bm \Sigma_{2}^{(p+1)^T} \bm\Psi^{(p+1,p+1)}\bm\Psi^{(p+1,p+1)^T} \right\|_F \\
    & \quad\ - \left\|\bm \Sigma_{1}^{(p+1)}\bm \Sigma_{1}^{(p+1)^T}\bm \Sigma_{1}^{(p+1)}\bm\Psi^{(p+1,p+1)^T} \right\|_F \\
    & \ge \left\|\lambda\bm T_{2}^{(p+1)^T} \bm \Sigma_{1}^{(p+1)^T} \bm\Psi^{(p+1,p+1)^T} - \sqrt{\lambda}  \bm \Sigma_{2}^{(p+1)^T} \bm\Psi^{(p+1,p+1)}\bm\Psi^{(p+1,p+1)}\right\|_F - y_1 \|\bm \Sigma_{1}^{(p+1)}\|_F^3, 
\end{align*}
\normalsize
where the last inequality follows from $\bm T_{2}^{(p+1)} \in \mathcal{O}^{d_2-r_{\sigma}}$ and \eqref{eq9:prop singular}. Using \eqref{eq8:prop singular}, we have
\small
\begin{align*}
    c_2\sqrt{\lambda}\|\nabla G(\bm W)\|_F &\ge \sqrt{\lambda} \left\|\lambda \bm\Sigma_{2}^{(p+1)} - \sqrt{\lambda}\bm \Psi^{(p+1,p+1)} \bm \Sigma_{1}^{(p+1)^T}\bm T_{2}^{(p+1)} \right\|_F - \sqrt{\lambda} \left\|\bm \Sigma_{2}^{(p+1)}\bm \Sigma_{2}^{(p+1)^T} \bm \Sigma_{2}^{(p+1)} \right\|_F \\
    &\ge \left\| \lambda^{\frac{3}{2}}\bm\Sigma_{2}^{(p+1)^T} - \lambda \bm T_{2}^{(p+1)^T} \bm \Sigma_{1}^{(p+1)}\bm \Psi^{(p+1,p+1)^T} \right\|_F - \sqrt{\lambda} \left\|\bm \Sigma_{2}^{(p+1)} \right\|_F^3. 
\end{align*} 
\normalsize
Summing up the above two inequalities yields
\begin{align}\label{eq10:prop singular}
    c_2(y_1+\sqrt{\lambda})\|\nabla G(\bm W)\|_F & \ge \left\|\lambda^{\frac{3}{2}}\bm\Sigma_{2}^{(p+1)^T} - \sqrt{\lambda}  \bm \Sigma_{2}^{(p+1)^T} \bm\Psi^{(p+1,p+1)}\bm\Psi^{(p+1,p+1)^T}\right\|_F \notag\\
    &\quad\  - \sqrt{\lambda} \left\|\bm \Sigma_{2}^{(p+1)}\right\|_F^3-y_1\left\|\bm \Sigma_{1}^{(p+1)}\right\|_F^3.  
\end{align}
Repeating the argument in \eqref{eq9:prop singular}-\eqref{eq10:prop singular} with multiplying $\sqrt{\lambda}$ on \eqref{eq7:prop singular} and $y_1$ on \eqref{eq8:prop singular}, we obtain 
\begin{align*}
    c_2(y_1+\sqrt{\lambda})\|\nabla G(\bm W)\|_F  \ge & \left\|\lambda^{\frac{3}{2}}\bm\Sigma_{1}^{(p+1)^T} - \sqrt{\lambda}\bm\Psi^{(p+1,p+1)^T}\bm\Psi^{(p+1,p+1)}  \bm \Sigma_{1}^{(p+1)^T} \right\|_F \\
    & - \sqrt{\lambda} \left\|\bm \Sigma_{1}^{(p+1)}\right\|_F^3-y_1 \left\|\bm \Sigma_{2}^{(p+1)}\right\|_F^3. 
\end{align*}
Summing up the above two inequalities yields 
\begin{align*}
   & 2c_2(y_1+\sqrt{\lambda}) \|\nabla G(\bm W)\|_F \\
   % \ge &\ \sqrt{\lambda}  \|(\bm\Psi^{(p+1,p+1)}\bm\Psi^{(p+1,p+1)^T} - \lambda \bm I )\bm \Sigma_{2}^{(p+1)}\|_F +\sqrt{\lambda}\|(\bm\Psi^{(p+1,p+1)^T}\bm\Psi^{(p+1,p+1)} - \lambda \bm I)\bm \Sigma_{1}^{(p+1)}\|_F \\
   % &\quad -(\sqrt{\lambda}+y_1)(\|\bm \Sigma_{2}^{(p+1)}\|_F^3 + \|\bm \Sigma_{1}^{(p+1)}\|_F^3)\\
   \ge &\ \sqrt{\lambda}\sigma_{\min}\left(\bm\Psi^{(p+1,p+1)}\bm\Psi^{(p+1,p+1)^T} - \lambda \bm I\right)\|\bm \Sigma_{2}^{(p+1)}\|_F+\sqrt{\lambda}\sigma_{\min}\left(\bm\Psi^{(p+1,p+1)^T}\bm\Psi^{(p+1,p+1)} - \lambda \bm I\right)\\
   &\quad \|\bm \Sigma_{1}^{(p+1)}\|_F - (\sqrt{\lambda}+y_1)(\|\bm \Sigma_{2}^{(p+1)}\|_F^3 + \|\bm \Sigma_{1}^{(p+1)}\|_F^3)\\
    \ge&\ \left(\frac{2\lambda\delta }{3} -(\sqrt{\lambda}+y_1)\|\bm \Sigma_{1}^{(p+1)}\|_F^2\right)\|\bm \Sigma_{1}^{(p+1)}\|_F +  \left(\frac{2\lambda\delta}{3} -(\sqrt{\lambda}+y_1)\|\bm \Sigma_{2}^{(p+1)}\|_F^2\right)\|\bm \Sigma_{2}^{(p+1)}\|_F \\
    \geq &\ \frac{\lambda\delta}{3}(\|\bm \Sigma_{1}^{(p+1)}\|_F+\|\bm \Sigma_{2}^{(p+1)}\|_F),
\end{align*}
where the second inequality follows from \eqref{eq6:prop singular},
and the last inequality follows from \eqref{eq:dist 7}, \eqref{eq:Sigmaledist1} and \eqref{eq:Sigmaledist2}.
This directly implies \eqref{eq:prop singular}. 

(ii) For ease of exposition, let $\bm \Psi := \bm U_L^T\bm Y\bm V_1 \in \R^{d_L\times d_0}$. Let $l \in [L]$ be such that $l \in \argmax\{\sigma_{r_{\sigma}+1}(\bm W_k):{k \in [L]}\}$. Substituting \eqref{eq:SVD Wl} into \eqref{eq:coro1} yields 
\begin{align*}
c_1\|\nabla G(\bm W)\|_F & \ge \left\|(\bm\Sigma_l\bm\Sigma_l^T)^{L-1}\bm\Sigma_l-\sqrt{\lambda}\left(\prod_{k=l+1}^{L}\bm H_{k}\bm \Sigma_k^T \right)\bm \Psi \left(\prod_{k =1}^{l-1}\bm \Sigma_k^T\bm H_{k+1}\right) + \lambda\bm\Sigma_l \right\|_F \\
& \ge \left\| (\bm\Sigma_l\bm\Sigma_l^T)^{L-1}\bm\Sigma_l-\sqrt{\lambda}\left(\prod_{k=l+1}^{L}\bm T_{k}\bm \Sigma_k^T\right) \bm \Psi \left(\prod_{k =1}^{l-1}\bm \Sigma_k^T \bm T_{k+1}\right) + \lambda\bm\Sigma_l\right\|_F - \sqrt{\lambda}\|\bm \Delta\|_F, 
\end{align*} 
where the second inequality uses the triangular inequality and 
\begin{align*}
\bm \Delta := &\  (\bm H_{l+1} - \bm T_{l+1})\bm \Sigma_{l+1}^T \left(\prod_{k=l+2}^{L}\bm H_{k}\bm \Sigma_k^T \right) \bm \Psi \left(\prod_{k=1}^{l-1}\bm \Sigma_k^T \bm H_{k+1} \right) \\
&\ +\bm T_{l+1}\bm \Sigma_{l+1}^T(\bm H_{l+2} - \bm T_{l+2})\bm \Sigma_{l+2}^T \left(\prod_{k=l+3}^{L}\bm H_{k}\bm \Sigma_k^T \right) \bm \Psi \left(\prod_{k=1}^{l-1}\bm \Sigma_k^T \bm H_{k+1} \right)+ \cdots  \\
&\ +\left( \prod_{k=l+1}^{L}\bm T_{k}\bm \Sigma_k^T \right) \bm \Psi \left(\prod_{k=1}^{l-2}\bm \Sigma_k^T \bm T_{k+1} \right)\bm \Sigma_{l-1}^T(\bm H_l - \bm T_l). 
\end{align*}
Using \eqref{eq:S norm W} and \eqref{eq0:prop singular 2}, we obtain 
\begin{align*}
\|\bm\Delta\| \le \left( \frac{3}{2}\sigma_{\max}^* \right)^{L}\frac{3(L-1)y_1}{ 2\delta_{\sigma}\lambda\sigma^{*}_{\min}}  \|\nabla G(\bm W)\|_F. 
\end{align*}
This, together with the above inequality, yields 
\begin{align*}
\left\| (\bm\Sigma_l\bm\Sigma_l^T)^{L-1}\bm\Sigma_l-\sqrt{\lambda}\left(\prod_{k=l+1}^{L}\bm T_{k}\bm \Sigma_k^T\right) \bm \Psi \left(\prod_{k =1}^{l-1}\bm \Sigma_k^T \bm T_{k+1}\right) + \lambda\bm\Sigma_l\right\|_F \le \eta_2 \|\nabla G(\bm W)\|_F,
\end{align*}
where $\eta_2$ is defined in \eqref{eq:c3}. Using this inequality, the block structures of $\bm \Sigma_l$ and $\bm T_l$, and the fact that
$l \in \argmax\{\sigma_{r_{\sigma}+1}(\bm W_k):{k \in [L]}\}$, we obtain 
\small
\begin{align}\label{eq1:prop singular 2}
\eta_2 \|\nabla G(\bm W)\|_F & \ge \left\| \left(\bm\Sigma_{l}^{(p+1)}\bm \Sigma_{l}^{(p+1)^T}\right)^{L-1}\bm \Sigma_{l}^{(p+1)} -\sqrt{\lambda}\left(\prod_{k=l+1}^{L} \bm T_{k}^{(p+1)} \bm \Sigma_{k}^{(p+1)^T}\right)  \right. \notag \\ 
& \quad\  \left.  \bm \Psi_{r_{\sigma}+1:d_L,r_{\sigma}+1:d_0}\left(\prod_{k=1}^{l-1} \bm \Sigma_{k}^{(p+1)^T} \bm T_{k+1}^{(p+1)}\right) + \lambda\bm \Sigma_{l}^{(p+1)}\right\| \notag \\
& \ge   \left\| \left(\bm\Sigma_{l}^{(p+1)}\bm \Sigma_{l}^{(p+1)^T}\right)^{L-1}\bm \Sigma_{l}^{(p+1)} + \lambda\bm \Sigma_{l}^{(p+1)} \right\| - \sqrt{\lambda}\sigma_{r_{ \sigma}+1}^{L-1}(\bm W_l)\left\|\bm \Psi_{r_{\sigma}+1:d_L,r_{\sigma}+1:d_0}\right\| \notag \\
& \ge \lambda\sigma_{r_{ \sigma}+1}(\bm W_l)-\sqrt{\lambda}y_1\sigma_{r_{ \sigma}+1}^{L-1}(\bm W_l) \ge \sqrt{\lambda}\left(\sqrt{\lambda}-y_{1}\sigma_{r_{\sigma}+1}^{L-2}(\bm W_l)\right) \sigma_{r_{\sigma}+1}(\bm W_l),  
\end{align}
\normalsize
where \(\bm \Psi_{r_{\sigma}+1:d_L, r_{\sigma}+1:d_0}\) denotes the submatrix of \(\bm \Psi\) with the rows being indexed from \(r_{\sigma}+1\) to \(d_L\) and the columns being indexed from \(r_{\sigma}+1\) to \(d_0\), and the third inequality is due to $\|\bm \Psi_{r_{\sigma}+1:d_L, r_{\sigma}+1:d_0}\| \le \|\bm{Y}\| = y_1$. Using Weyl's inequality and \eqref{eq:dist 5} with $L \ge 3$, we have
\begin{align*}
\sigma_{r_{\sigma}+1}(\bm W_l) \le \mathrm{dist}\left(\bm W, \mathcal{W}_{\bm \sigma^*} \right) \le \left( \frac{\sqrt{\lambda}}{2y_1} \right)^{1/(L-2)}.
\end{align*}
This, together with \eqref{eq1:prop singular 2}, directly yields 
$
\sigma_{r_{\sigma}+1}(\bm W_l) \le  {2\eta_2}\|\nabla G(\bm W)\|_F/{\lambda}$, which further implies \eqref{eq:singular 2}.
\end{proof}

Equipped with \Cref{lem:pre}, \Cref{coro1}, \Cref{prop:Hlapproximate}, \Cref{lem:twoinequality}, \Cref{prop:singular value 2}, and Davis-Kahan Theorem (see \Cref{lem:daviskahan}), we are ready to prove \Cref{prop:singular control}. 

\begin{proof}[Proof of \Cref{prop:singular control}]
According to \eqref{eq:dist 7} or \eqref{eq:dist 5}, we have $\mathrm{dist}(\bm W, \mathcal{W}) \le \delta_{\sigma}/3 < \sigma_{\min}^*/2$. This, together with \Cref{lem:pre} and \Cref{coro1}, implies that \eqref{eq:S norm W}-\eqref{eq:wl+1} and \eqref{eq:coro1} hold.  According to \Cref{prop:Hlapproximate} and \Cref{lem:twoinequality} with $\mathrm{dist}(\bm W, \mathcal{W}) \le \delta_{\sigma}/3$, there exist matrices $\bm T_{l}^{(i)} \in \mathcal{O}^{g_i}$ for each $i \in [p]$ and $\bm T_{l}^{(p+1)} \in \mathcal{O}^{d_{l-1} - r_{\sigma}}$ such that \eqref{eq:twoinequality2} and \eqref{eq:twoinequality1} hold, where $\bm A_{i}$ and $\bm B_i$ for each $i \in [p+1]$ are defined in \eqref{eq:A} and \eqref{eq:B}. According to \eqref{eq:dist grad 1} (resp., \eqref{eq:dist 5}) and \Cref{prop:singular value 2}, we have \eqref{eq:prop singular} (resp., \eqref{eq:singular 2}) hold. Recall that \eqref{eq:SVD Wl} denotes an SVD of $\bm W_l$ for each $l \in [L]$. For ease of exposition, let 
\begin{align}\label{eq0:prop sing}
\bm \Psi := \bm U_L^T\bm Y\bm V_1,\ \hat{\bm \Sigma}_l := \mathrm{BlkDiag}\left(\bm \Sigma_{l}^{(1)},\dots,\bm \Sigma_{l}^{(p)}, \bm 0_{(d_l-r_{\sigma})\times(d_{l-1} - r_{\sigma})} \right),\ \forall l \in [L].
\end{align}  
where $\bm \Sigma_{l}^{(1)},\dots,\bm \Sigma_{l}^{(p)}$ are defined in \eqref{eq:SVD Wl} for each $l \in [L]$. Using the form of $\bm \Sigma_l$ in \eqref{eq:SVD Wl} and \eqref{eq:prop singular} (resp., \eqref{eq:singular 2}), we have for each $l \in [L]$, 
\begin{align}
& \left\|\hat{\bm \Sigma}_l - \bm \Sigma_l\right\|_F = \left\| \bm \Sigma_{l}^{(p+1)} \right\|_F \le c_3\sqrt{\min\{d_l,d_{l-1}\}} \|\nabla G(\bm W)\|_F \le c_3\sqrt{d_{\max}}\|\nabla G(\bm W)\|_F, \label{eq1:prop sing}\\
& \left\|(\hat{\bm \Sigma}_l\hat{\bm \Sigma}_l^T)^{L-1}\hat{\bm \Sigma}_l - (\bm \Sigma_l\bm \Sigma_l^T)^{L-1}\bm \Sigma_l \right\|_F   = \left\|\left(\bm \Sigma_{l}^{(p+1)}\bm \Sigma_{l}^{(p+1)^T}\right)^{L-1}\bm \Sigma_{l}^{(p+1)} \right\|_F \notag \\
&\qquad\qquad\qquad\qquad \qquad\qquad\qquad\quad\  \le  c_3\sqrt{d_{\max}}\left( \frac{3\sigma_{\max}^*}{2} \right)^{2(L-1)}\|\nabla G(\bm W)\|_F,\label{eq2:prop sing}
\end{align}
where the inequality uses \eqref{eq:S norm W} and \eqref{eq1:prop sing}. 

(i) We first focus on $l=L$. Now, we bound 
\small
\begin{align}
& \left\|(\hat{\bm \Sigma}_L\hat{\bm \Sigma}_L^T)^{L-1}\hat{\bm \Sigma}_L + \lambda \hat{\bm \Sigma}_L - \sqrt{\lambda}\bm \Psi \mathrm{BlkDiag}\left(\bm B_1, \dots, \bm B_{p}, \bm 0\right)  \right\|_F \notag\\
\le &  \left\|(\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L + \lambda \bm \Sigma_L - \sqrt{\lambda}\bm \Psi \mathrm{BlkDiag}\left(\bm B_1, \dots, \bm B_{p}, \bm B_{p+1}\right) \right\|_F + \lambda\|\hat{\bm \Sigma}_L - \bm \Sigma_L\|_F  \notag\\
\ & + \left\|(\hat{\bm \Sigma}_L\hat{\bm \Sigma}_L^T)^{L-1}\hat{\bm \Sigma}_L - (\bm \Sigma_L\bm \Sigma_L^T)^{L-1}\bm \Sigma_L \right\|_F + \sqrt{\lambda}\|\bm \Psi\| \label{eq3:prop sing}\|\bm B_{p+1}\|_F \notag\\
\le& \left( c_2 + \left(\left( \frac{3\sigma_{\max}^*}{2} \right)^{2(L-1)} + \lambda \right) c_3\sqrt{d_{\max}} \right) \left\| \nabla G(\bm W) \right\|_F + \sqrt{\lambda}y_1\|\bm B_{p+1}\|_F 
\le  \eta_3 \left\| \nabla G(\bm W) \right\|_F, 
\end{align}
\normalsize
where the second inequality follows from  \eqref{eq:twoinequality1}, \eqref{eq:B}, \eqref{eq:T}, \eqref{eq1:prop sing}, and \eqref{eq2:prop sing}, and the last inequality uses \eqref{eq:S norm W}, \eqref{eq:B}, \eqref{eq1:prop sing}, and 
\begin{align}\label{eq:eta 3}
\eta_3 := c_2 +  c_3\sqrt{d_{\max}} \left(\left( \frac{3\sigma_{\max}^*}{2} \right)^{2(L-1)} + \sqrt{\lambda}y_1\left( \frac{3\sigma_{\max}^*}{2} \right)^{L-2} + \lambda \right). 
\end{align}
Substituting $\hat{\bm{\Sigma}}_L$ in \eqref{eq0:prop sing}, \eqref{eq:B}, and \eqref{eq:T} into \eqref{eq3:prop sing} yields 
\small
\begin{align}
&\sum_{i=1}^{r_{\sigma}} \left\| \left( \sigma_i^{2L-1}(\bm W_L) + \lambda \sigma_i(\bm W_L)\right)\bm e_i - \sqrt{\lambda}\sigma_i^{L-1}(\bm W_L) \bm \Psi \hat{\bm T}\bm e_i \right\|^2 \notag\\
\le& \left\|(\hat{\bm \Sigma}_L\hat{\bm \Sigma}_L^T)^{L-1}\hat{\bm \Sigma}_L + \lambda \hat{\bm \Sigma}_L - \sqrt{\lambda}\bm \Psi \hat{\bm T}\mathrm{BlkDiag}\left((\bm \Sigma_L^{(1)})^{L-1}, \dots, (\bm \Sigma_L^{(p)})^{L-1}, \bm 0\right)  \right\|_F 
\le \eta_3^2\|\nabla G(\bm W)\|_F^2. 
\end{align}
\normalsize
Using this and the definition of $\varphi(\cdot)$ in \eqref{eq:phi}, together with dividing the above inequality by $\sqrt{\lambda}\sigma_{r_{\sigma}}^{L-1}(\bm W_L)$ on both sides, yields
\begin{align}\label{eq5:prop sing}
\sum_{i=1}^{r_{\sigma}} \left\| \varphi(\sigma_i(\bm W_L))\bm e_i - \bm \Psi \hat{\bm T} \bm e_i\right\|^2 &  \le   \frac{\eta_3^2 }{\lambda\sigma_{r_{\sigma}}^{2(L-1)}(\bm W_L)}\|\nabla G(\bm W)\|_F^2   \le  \frac{2^{2(L-1)}\eta_3^2}{\lambda(\sigma_{\min}^{*})^ {2L-2}} \|\nabla G(\bm W)\|_F^2,
\end{align}
where the last inequality uses \eqref{eq:sigmalowerbound}. Using the same argument of \eqref{eq3:prop sing} for \eqref{eq:twoinequality2}, we have 
\begin{align}\label{eq4:prop sing}
\left\| ( \hat{\bm \Sigma}_1 \hat{\bm \Sigma}_1^T)^{L-1} \hat{\bm \Sigma}_1+\lambda \hat{\bm \Sigma}_1-\sqrt{\lambda}\mathrm{BlkDiag}\left(\bm A_1,\dots,\bm A_{p},\bm 0\right)\bm \Psi \right\|_F  \le \eta_3 \left\| \nabla G(\bm W) \right\|_F. 
\end{align}
Then we compute
\small  
\begin{align}
    \left\| \left( \hat{\bm \Sigma}_1 \hat{\bm \Sigma}_1^T \right)^{L-1} \hat{\bm \Sigma}_1 
        + \lambda \hat{\bm \Sigma}_1 
        - \sqrt{\lambda}\, \mathrm{BlkDiag}\left( (\bm \Sigma_{1}^{(1)})^{L-1}, \dots, (\bm \Sigma_{1}^{(p)})^{L-1}, \bm 0 \right) \bm \Psi \hat{\bm T} \right\|_F \le \eta_4 \left\| \nabla G(\bm W) \right\|_F,  
    \label{eq0:prop_sing}
\end{align}
\normalsize
where 
\[
    \eta_4 :=  \eta_3 
        + \frac{p \eta_1 (2L - 1)L}{\sigma^*_{\min}} 
            \left( \frac{3 \sigma^*_{\max}}{2} \right)^{2L - 2}  
        + \frac{\lambda p \eta_1 L}{\sigma^*_{\min}}. 
\]
To maintain the flow of the main proof, we defer the proof of \eqref{eq0:prop sing} to Appendix~\ref{sec:comlementary1}.
Applying a similar argument as in \eqref{eq5:prop sing} to rows of the above inequality yields
\begin{align}\label{eq6:prop sing}
\sum_{i=1}^{r_{\sigma}} \left\| \varphi(\sigma_i(\bm W_1))\bm e_i^T - \bm e_i^T\bm \Psi \hat{\bm T} \right\|^2 
& \le  \left( \frac{2^{L-1}\eta_4}{\sqrt{\lambda}\sigma_{\min}^{* {L-1}}} \right)^2 \|\nabla G(\bm W)\|_F^2.
\end{align} 
To proceed, we define $\bm d :=  \left(\varphi(\sigma_1(\bm W_L)),\dots,\varphi(\sigma_{r_{\sigma}}(\bm W_L)) \right) \in \R^{r_{\sigma}}$ and  
\begin{align}
%\label{eq:Z}
\notag
\bm Z := \blk\left(\mathrm{diag}(\bm d), (\bm \Psi \hat{\bm T})_{r_{\sigma}+1:d_L,r_{\sigma}+1:d_0}\right),
\end{align}
where $(\bm \Psi \hat{\bm T})_{r_{\sigma}+1:d_L,r_{\sigma}+1:d_0}$ denotes a submatrix of $\bm \Psi \hat{\bm T}$ with rows indexed by 
$r_{\sigma}+1:d_L$ and columns indexed by $r_{\sigma}+1:d_0$. Now, we compute
\begin{align}\label{eq7:prop sing}
\left\|\bm \Psi \hat{\bm T} - \bm Z \right\|_F^2 & = \sum_{i=1}^{r_{\sigma}} \left\|( \bm \Psi \hat{\bm T}  - \bm Z)\bm e_i \right\|^2 + \sum_{i=r_{\sigma}+1}^{d_0} \|(\bm \Psi \hat{\bm T} - \bm Z )\bm e_i \|^2 \notag \\
& = \sum_{i=1}^{r_{\sigma}} \left\| \bm \Psi \hat{\bm T} \bm e_i - \varphi(\sigma_i(\bm W_L))\bm e_i \right\|^2 +  \left\| \left(\bm \Psi \hat{\bm T}\right)_{1:r_{\sigma},r_{\sigma}+1:d_0,} \right\|_F^2 \notag\\
& \le \sum_{i=1}^{r_{\sigma}} \left\| \bm \Psi \hat{\bm T} \bm e_i - \varphi(\sigma_i(\bm W_L))\bm e_i \right\|^2 + \sum_{i=1}^{r_{\sigma}} \left\|  \bm e_i^T\bm \Psi \hat{\bm T}- \varphi(\sigma_i(\bm W_1))\bm e_i^T \right\|^2 \notag \\
& \le \left( \frac{2^{L-1}}{\sqrt{\lambda}\sigma_{\min}^{* {L-1}}} \right)^2\left( \eta_3^2 + \eta_4^2 \right) \|\nabla G(\bm W)\|_F^2,
\end{align}
where the second equality uses the structure of $\bm Z$, the first inequality is due to
$$
    \left\|  \bm e_i^T\bm \Psi \hat{\bm T}- \varphi(\sigma_i(\bm W_1))\bm e_i^T \right\|^2  \ge \left\| \left(\bm \Psi \hat{\bm T}\right)_{i,r_{\sigma}+1:d_0} \right\|^2,\ \forall i \in [r_{\sigma}],
$$and the second inequality follows from
\eqref{eq5:prop sing} and \eqref{eq6:prop sing}. Let 
$ 
 (\bm \Psi \hat{\bm T})_{r_{\sigma}+1:d_L,r_{\sigma}+1:d_0}  = \bm P \bm \Lambda \bm Q^T
$ 
be an SVD of $(\bm \Psi \hat{\bm T})_{r_{\sigma}+1:d_L,r_{\sigma}+1:d_0} $, where $\bm P \in \mathcal{O}^{d_L-r_{\sigma}}$, $\bm Q \in \mathcal{O}^{d_0-r_{\sigma}}$, and $\bm \Lambda \in \R^{(d_L-r_{\sigma})\times (d_0-r_{\sigma})}$ with diagonal elements $\gamma_1 \ge \dots \ge \gamma_{d_{\min}- r_{\sigma}}$ being top $d_{\min} - r_{\sigma}$ singular values. Now, we define 
\begin{align}\label{eq:c}
\bm c := \left(\varphi(\sigma_1(\bm W_L)),\dots,\varphi(\sigma_{r_{\sigma}}(\bm W_L)),\gamma_1,\dots, \gamma_{d_{\min}- r_{\sigma}}\right) \in \R^{d_{\min}}.
\end{align}
Let $\bm \Pi \in \mathcal{P}^{d_{\min}}$ be a permutation matrix such that the entries $\bm \Pi\bm c$ are in a decreasing order and 
\begin{align}\label{eq:foldef}
& \bar{\bm U}_L := \bm U_L\blk\left(\bm I_{r_{\sigma}}, \bm P^T\right)\blk\left(\bm \Pi^T, \bm I_{d_L-d_{\min}}\right),\notag \\
&\bar{\bm V}_1 := \bm V_1 \hat{\bm T} \blk\left(\bm I_{r_{\sigma}}, \bm Q^T\right)\blk\left(\bm \Pi^T,\bm I_{d_0-d_{\min}}\right),\\
&\bar{\bm Y} := \blk\left(\bm\Pi \mathrm{diag}\left(\bm c\right)\bm \Pi^T,\bm 0_{(d_L-d_{\min})\times (d_0 - d_{\min}) }\right)\notag.
\end{align} 
Since both $\bm{Y}$ and $\bar{\bm{Y}}$ are diagonal matrices with elements in decreasing order, by using Mirsky's inequality (see \Cref{lem:mirsky}), we have
\begin{align}\label{eq8:prop sing}
\|\bm Y - \bar{\bm Y}\|_F \le \|\bar{\bm U}_L^T \bm Y \bar{\bm V}_1 - \bar{\bm Y}\|_F \le    \frac{2^{L-1}}{\sqrt{\lambda}\sigma_{\min}^{* {L-1}}} \sqrt{ \eta_3^2 + \eta_4^2 } \|\nabla G(\bm W)\|_F,
\end{align}
where the last inequality uses \eqref{eq7:prop sing}. This, together with \eqref{eq:gradient_condition}, implies  
\begin{align}\label{eq9:prop sing}
\|\bm Y - \bar{\bm Y}\|_F \le \frac{\delta_y}{3},\ \|\bar{\bm Y}\| \le \|\bar{\bm Y} - \bm Y\|  + \|\bm Y\|  \le y_1 + \frac{\delta_y}{3}. 
\end{align}
According to the triangular inequality, we obtain  
\begin{align}
    \|\bar{\bm U}_L^T \bm Y \bm Y^T \bar{\bm U}_L - \bar{\bm Y} \bar{\bm Y}^T\|_F &\le \|(\bar{\bm U}_L^T \bm Y \bar{\bm V}_1 - \bar{\bm Y}) \bar{\bm V_1}^T \bm Y^T \bar{\bm U}_L\|_F + \|\bar{\bm Y} (\bar{\bm U}_L^T \bm Y \bar{\bm V}_1 - \bar{\bm Y})^T\|_F \notag\\
    & \le \left( 2y_1 + \frac{\delta_y}{3} \right) \frac{2^{L-1}\sqrt{\eta_3^2+\eta_4^2}}{\sqrt{\lambda}\sigma^{* {L-1}}_{\min}} \|\nabla G(\bm W)\|_F,\label{eq:control theorem}
\end{align} 
where the second inequality uses \eqref{eq8:prop sing} and \eqref{eq9:prop sing}. 
%Let $\bar{y}_j$ denote the $j$-th diagonal element of $\bar{\bm Y}$ for each $j \in [d_{\min}]$. Then, we have
%\begin{align}
%\bar{y}_{s_i} - \bar{y}_{s_i+1} > y_{s_i} - y_{s_i+1} - 2\|\bar{\bm Y} - \bm Y\| \ge \frac{\delta_y}{3} > 0,\ \forall i \in [p]. 
%\end{align}
Next, we write \(\bar{\bm{U}}_L\) and \(\bm{I}_{d_L}\) as follows:  
\begin{align*}
&\bar{\bm{U}}_L = \begin{bmatrix}
    \bar{\bm{U}}_{L}^{(1)} \\ 
    \vdots \\ 
    \bar{\bm{U}}_{L}^{(p_Y)} \\
    \bar{\bm{U}}_{L}^{(p_Y+1)}
\end{bmatrix}, \ 
\bm{I}_{d_L}  = \begin{bmatrix}
    \bm{E}^{(1)} \\ 
    \vdots \\ 
    \bm{E}^{(p_Y)} \\
    \bm{E}^{(p_Y+1)}
\end{bmatrix}, \ 
\bm{E}^{(1)} = \begin{bmatrix}
    \ \bm I_{h_1},\ \bm 0_{h_1 \times (d_L - s_1)}
\end{bmatrix},
\\ 
&\bm{E}^{(i)} =\begin{bmatrix}
    \bm 0_{h_i \times s_{i-1}},\ \bm I_{h_i},\ \bm 0_{h_i \times (d_L - s_i)}
\end{bmatrix},\ 
\bm{E}^{(p_Y+1)} =\begin{bmatrix}
    \bm 0_{(d_L -r_Y) \times r_Y},\ \bm I_{d_L - r_Y}
\end{bmatrix},
\end{align*}
where \(\bar{\bm{U}}_{L}^{(i)}, \bm{E}^{(i)} \in \mathbb{R}^{h_i \times d_L }\) for all \(i \in [p_Y]\), and 
\(\bar{\bm{U}}_{L}^{(p_Y+1)}, \bm{E}^{(p_Y+1)} \in \mathbb{R}^{(d_L - r_Y) \times d_L}\).
Applying the Davis-Kahan Theorem (see \Cref{lem:daviskahan}) to the matrices 
$\bar{\bm{U}}_L^T \bm{Y} \bm{Y}^T \bar{\bm{U}}_L$ (considered as $\bm{A}$), 
$\bar{\bm{Y}} \bar{\bm{Y}}^T$ (considered as $\hat{\bm{A}}$), 
$\bar{\bm{U}}_L^{(i)^T}$ (considered as $\bm{V}$) , 
and $\bm{E}^{(i)^T}$ (considered as $\hat{\bm{V}}$) for each $i \in [p_Y+1]$, there exist orthogonal matrices 
\(\hat{\bm{U}}_{L}^{(i)} \in \mathcal{O}^{h_i}\) for each \(i \in [p_Y]\) 
and \(\hat{\bm{U}}_{L}^{(p_Y+1)} \in \mathcal{O}^{d_L - r_Y}\) such that  
\begin{align}\label{eq:bddaviskahan}
    \left\| \bar{\bm{U}}_{L}^{(i)} - \hat{\bm{U}}_{L}^{(i)}\bm{E}^{(i)}  \right\|_F 
    \leq  \frac{4\|\bar{\bm U}_L^T \bm Y \bm Y^T \bar{\bm U}_L - \bar{\bm Y} \bar{\bm Y}^T\|_F}{\min\{\lambda_{i-1}-\lambda_{i},\lambda_i-\lambda_{i+1} \}},\ \forall i\in[p_{Y}+1], 
\end{align}
where $\lambda_i$ is the $i$-th largest non-repeated eigenvalue of $\bar{\bm{U}}_L^T \bm{Y} \bm{Y}^T \bar{\bm{U}}_L$ and $\lambda_0=\infty$ and $\lambda_{p_Y+2}=-\infty$ by convention. Here, according to the SVD of $Y$ in \eqref{eq:Y}, we compute $\lambda_i=y_{s_i}^2$ for $i\in[p_Y]$ and $\lambda_{p_Y+1}=0$ for $i\in[p_Y]$.
Using \eqref{eq:control theorem} and the following inequalities
\begin{align*} 
    &y_{s_j}^2 - y_{s_j+1}^2= (y_{s_j} - y_{s_j+1})(y_{s_j} + y_{s_j+1})\ge \delta_y y_{s_{p_Y}},\ \forall j \in [p_Y],\\
    &\min\left\{y_{s_i}^2 - y_{s_i+1}^2, y_{s_{i+1}}^2 - y_{s_{i+1}+1}^2 \right\}\ge \delta_y y_{s_{p_Y}},\ \forall i \in [p_Y-1], 
\end{align*}
\eqref{eq:bddaviskahan} yields
\begin{align*}
    \left\| \bar{\bm{U}}_{L}^{(i)} - \hat{\bm{U}}_{L}^{(i)}\bm{E}^{(i)}  \right\|_F 
    \leq  \frac{2^{L+1}(6y_1+\delta_y)\sqrt{\eta_3^2+\eta_4^2}}{3\sqrt{\lambda}y_{s_{p_Y}}\delta_y \sigma^{* L-1}_{\min}} \|\nabla G(\bm{W})\|_F.
\end{align*}
Therefore, we have
\begin{align}\label{eq10:prop sing}
 \left\|\bar{\bm U}_L - \blk\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y+1)} \right) \right\|_F & \le \sum_{i=1}^{p_Y+1} \left\| \bar{\bm U}_{L}^{(i)} -\hat{\bm U}_{L}^{(i)}\bm E^{(i)}\right\|_F \le \eta_5  \|\nabla G(\bm W)\|_F, 
\end{align}
where 
\begin{align}
\eta_5 := \frac{2^{L+1}(6y_1+\delta_y)(p_Y+1)\sqrt{\eta_3^2+\eta_4^2}}{3\sqrt{\lambda}y_{s_{p_Y}}\delta_y \sigma^{* L-1}_{\min}}. 
\end{align}
According to definition of $\tilde{\bm U}_L$ in \eqref{eq:UL}, we have
\begin{align*}
 \left\| \bm U_L - \tilde{\bm U}_L \right\|_F 
  = \left\| \bar{\bm U}_L - \mathrm{BlkDiag}\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)},\hat{\bm U}_{L}^{(p_Y+1)} \right) \right\|_F  \le \eta_5  \|\nabla G(\bm W)\|_F,
\end{align*}
where the second equality uses the definition of $\bar{\bm U}_L$ in \eqref{eq:foldef} and the inequality is due to \eqref{eq10:prop sing}. Using the same argument in \eqref{eq:control theorem}-\eqref{eq10:prop sing} to $\bar{\bm V}_1^T \bm Y^T \bm Y \bar{\bm V}_1$ and $\bar{\bm Y}^T \bar{\bm Y}$, we conclude that there exist orthogonal matrices $\hat{\bm V}_{1}^{(i)} \in \mathcal{O}^{h_i}$ for each $i \in [p]$ and $\hat{\bm V}_{1}^{(p+1)} \in \mathcal{O}^{d_0-r_Y}$ such that
\begin{align}\label{eq11:prop sing}
    \left\|\bar{\bm V}_1 - \mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)},\hat{\bm V}_{1}^{(p_Y+1)} \right) \right\|_F \le \eta_5\|\nabla G(\bm W)\|_F.  
\end{align} 
Moreover, we have 
\begin{align*} 
&\ \left\|\mathrm{BlkDiag}\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)},\hat{\bm U}_{L}^{(p_Y+1)} \right)\bm Y\mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)},\hat{\bm V}_{1}^{(p_Y+1)} \right)^T - \bm Y \right\|_F \notag\\
       \le &\  \left\|\left(\mathrm{BlkDiag}\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)},\hat{\bm U}_{L}^{(p_Y+1)} \right) -\bar{\bm U}_L\right)\bm Y \mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)},\hat{\bm V}_{1}^{(p_Y+1)} \right)^T \right\|_F + \notag\\ 
       &\  \left\|\bar{\bm U}_L \bm Y\left(\mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)},\hat{\bm V}_{1}^{(p_Y+1)} \right) - \bar{\bm V}_1 \right)^T \right\|_F  + \|\bar{\bm U}_L \bm Y \bar{\bm V}_1^T - \bar{\bm Y}\|_F +\|\bar{\bm Y} - \bm Y\|_F \\
        \le & \left(\frac{2^{L}\sqrt{\eta_3^2+\eta_4^2}}{\sqrt{\lambda} \sigma^{*L-1}_{\min}}+2y_1\eta_5\right)\|\nabla G(\bm W)\|_F,
\end{align*} 
where the last inequality uses \eqref{eq8:prop sing}, \eqref{eq10:prop sing}, and \eqref{eq11:prop sing}. This, together with the block structure of $\bm Y$ in \eqref{eq:Y} and \eqref{eq:SY}, yields 
\begin{align*}
&\ \left\|\blk\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)} \right)-\mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)} \right) \right\|_F \\
\le &\ \frac{1}{y_{s_{p_Y}}} \left\|\blk\left(y_{s_1}\hat{\bm U}_{L,1}\hat{\bm V}_{1,1}^T,\dots,y_{s_{p_Y}}\hat{\bm U}_{L,p_Y}\hat{\bm V}_{1,p_Y}^T\right)- \blk\left(y_{s_1}\bm I_{h_1}, \dots, y_{s_{p_Y}} \bm I_{h_{p_Y}} \right) \right\|_F \\
=&\ \frac{1}{y_{s_{p_Y}}} \left\|\mathrm{BlkDiag}\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)},\hat{\bm U}_{L}^{(p_Y+1)} \right)\bm Y\mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)},\hat{\bm V}_{1}^{(p_Y+1)} \right)^T - \bm Y\right\|_F \\
\le &\ \frac{1}{y_{s_{p_Y}}}\left(\frac{2^{L}\sqrt{\eta_3^2+\eta_4^2}}{\sqrt{\lambda} \sigma^{*L-1}_{\min}}+2y_1\eta_5\right)\|\nabla G(\bm W)\|_F. 
\end{align*}
Using the definition of $\tilde{\bm V}_1$, we compute
\begin{align*}
\|\bm V_1- \tilde{\bm V}_1\|_F & = \left\| \bar{\bm V}_1 - \blk\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)},\hat{\bm V}_{1}^{(p_Y+1)} \right) \right\|_F \\
& \le \left\| \bar{\bm V}_1 - \mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)},\hat{\bm V}_{1}^{(p_Y+1)} \right) \right\|_F + \\
&\quad\  \left\|\mathrm{BlkDiag}\left(\hat{\bm V}_{1}^{(1)},\dots,\hat{\bm V}_{1}^{(p_Y)} \right) - \blk\left(\hat{\bm U}_{L}^{(1)},\dots,\hat{\bm U}_{L}^{(p_Y)} \right) \right\|_F \\
& \le \left( \eta_5 + \frac{1}{y_{s_{p_Y}}}\left(\frac{2^{L}\sqrt{\eta_3^2+\eta_4^2}}{\sqrt{\lambda} \sigma^{*L-1}_{\min}}+2y_1\eta_5\right) \right) \|\nabla G(\bm W)\|_F,   
\end{align*}
where the last inequality follows from \eqref{eq11:prop sing} and the above inequality. 

The rest of the proof is devoted to showing that $\bm \Pi$ satisfies $(\bm \sigma^*, \bm \Pi^T) \in \cal B$. According to \eqref{eq9:prop sing} and the definition of $\bar{\bm Y}$ in \eqref{eq:foldef}, we have $\left\| \bm c -  \bm \Pi^T\bm y \right\| \le {\delta_y}/{3},$ where $\bm y := (y_1,y_2,\dots,y_{d_{\min}})$. This implies that there exists a permutation $\pi:[d_{\min}] \to [d_{\min}]$ such that\footnote{Let $\pi : [d_{\min}] \rightarrow [d_{\min}]$ denote a permutation of the elements in $[d_{\min}]$. Note that there is an one-to-one correspondence between permutation matrix $\bm\Pi \in \mathbb{R}^{d_{\min} \times d_{\min}}$ and a permutation $\pi$, i.e., $\Pi_{ij} = 1$ if $j = \pi(i)$ and $\Pi_{ij} = 0$ otherwise for each $i \in [d_{\min}]$. Now, suppose that $\bm\Pi$ corresponds to $\pi$ and $\bm \Pi^T$ corresponds to $\pi^{-1}$.} 
\begin{align}\label{eq12:prop sing}
\left|c_{i} - y_{\pi^{-1}(i)}\right| \le \frac{\delta_y}{3},\ \forall i \in [d_{\min}].
\end{align} 
Noting that $\varphi(x)$ is continuous and differentiable at $x\neq 0$ and $\delta^*>0$, there exists a positive constant $\delta_1 > 0$ such that for all $|x - \sigma_i^*| \le \min\{\delta_1,\frac{\delta_\sigma}{3}\}$ and all $i \in [r_{\sigma}]$,    
\begin{align}\label{eq13:prop sing}
\left| \varphi(x) - \varphi(\sigma_i^*) \right| \le \frac{\delta_y}{3}. 
\end{align}
Using Weyl's inequality, \eqref{eq:dist_condition}, and \eqref{eq:dist 7} (resp. \ref{eq:dist 5}), we have
\begin{align*}
\left| \sigma_i(\bm W_L) - \sigma_i^* \right| \le \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) \le  \min\{\delta_1,\frac{\delta_\sigma}{3}\}, 
\end{align*}
This, together with \eqref{eq:c} and \eqref{eq13:prop sing}, implies 
\begin{align}
\left| c_i - \varphi(\sigma_i^*) \right|  =  \left| \varphi(\sigma_i(\bm W_L) ) - \varphi(\sigma_i^*) \right| \le \frac{\delta_y}{3},\ \forall i \in [r_{\sigma}].
\end{align}
According to \eqref{set:A}, there exists $k_i \in [d_{\min}]$ such that $\varphi(\sigma_i^*) = y_{k_i}$ 
for each $i \in [r_{\sigma}]$. For each $i \in [r_{\sigma}]$, we compute
\begin{align*}
\left| y_{k_i} - y_{\pi^{-1}(i)} \right| = \left| \varphi(\sigma_i^*) - y_{\pi^{-1}(i)} \right| \le \left| \varphi(\sigma_i^*) - c_i \right| + \left| c_i - y_{\pi^{-1}(i)} \right| \le \frac{2\delta_y}{3},
\end{align*}
where the last inequality uses \eqref{eq12:prop sing} and \eqref{eq13:prop sing}. This, together with the definition of $\delta_y$ in \eqref{eq:delta y}, yields $\varphi(\sigma_i^*) = y_{k_i} = y_{\pi^{-1}(i)}$ for each $i \in [r_{\sigma}]$. Using this and \eqref{eq:phi}, we have
\begin{align*}
    (\sigma^*_i)^{2L-1} + \lambda \sigma^*_i - \sqrt{\lambda} y_{\pi^{-1}(i)} (\sigma^*_i)^{{L-1}}  = 0,\ \forall i \in [r_{\sigma}]. 
\end{align*}
For each $i \in [r_{\sigma}+1, d_{\min}]$, we note that $\sigma^*_i = 0$. It is trivial to see that the above equation holds. These, together with the definition of $\mathcal{B}$ in \eqref{set:B} and  \Cref{lem:helplem} in the appendix, yields that $(\bm{\sigma}^*, \bm{\Pi}^T) \in \mathcal{B}$. Then, we complete the proof. 

(ii) Using \eqref{eq:c}  and the fact that $y_{\pi^{-1}(i)} = \varphi(\sigma_i^*)$ for each $i \in [r_{\sigma}]$, we have 
\begin{align}\label{eq14:prop sing}
  \left| \varphi(\sigma_i(\bm W_L)) - \varphi(\sigma_i^*)  \right| & = \left| c_i - y_{\pi^{-1}(i)}\right| \le \|\bm c - \bm \Pi^T \bm y\| = \|\bm \Pi\bm c - \bm y\| \notag \\
  & = \|\bar{\bm Y} - \bm Y\|_F \le \frac{2^{L-1}}{\sqrt{\lambda}\sigma_{\min}^{* {L-1}}} \sqrt{ \eta_3^2 + \eta_4^2 } \|\nabla G(\bm W)\|_F,
\end{align}
where the last inequality uses \eqref{eq8:prop sing}. 
By \eqref{eq:AS lambda1} and the continuity of \(\varphi'(x)\), \Cref{lem:phi} holds, and thus we conclude that there exists a positive constant $\delta_2 > 0$ such that for all $|x - \sigma_i^*| \le \delta_2$ and all $i \in [r_{\sigma}]$, we have for all $L \ge 3$,
\begin{align}\label{eq15:prop sing}
\left|\varphi^\prime(x) - \varphi^\prime(\sigma_i^*) \right| \le \frac{|\varphi^\prime(\sigma_i^*)|}{2}. 
\end{align} 
Note that there exists $\delta_2>0$ such that \eqref{eq15:prop sing} holds when $L=2$ {(see \Cref{re:remarkl=2})}. Using \eqref{eq:dist_condition} and Weyl's inequality, it holds that
\begin{align}\label{eq16:prop sing}
\left| \sigma_i(\bm W_L) - \sigma_i^* \right| \le \mathrm{dist}(\bm W, \mathcal{W}_{\bm \sigma^*}) \le \delta_2. 
\end{align}
Applying the mean-value theorem to $\varphi(\cdot)$, there exists $x$ between $\sigma_i(\bm W_L)$ and $\sigma_i^* $  such that 
\begin{align}
  \left|\varphi(\sigma_i(\bm W_L)) - \varphi(\sigma_i^*) \right| = \left|\varphi^\prime(x)\right| \left| \sigma_i(\bm W_L) - \sigma_i^* \right| \ge \frac{\left|\varphi^\prime(\sigma_i^*)\right|}{2} \left| \sigma_i(\bm W_L) - \sigma_i^* \right|, 
\end{align}
where the inequality follows from $|x-\sigma_i^*| \le \left| \sigma_i(\bm W_L) - \sigma_i^* \right| \le \delta_2$ due to \eqref{eq16:prop sing} and $\left|\varphi^\prime(x)\right| \ge  |{\varphi^\prime(\sigma_i^*)}/{2}|$ due to \eqref{eq15:prop sing}. This, together with \eqref{eq14:prop sing}, yields for all $i \in [r_{\sigma}]$, 
\begin{align*}
\left| \sigma_i(\bm W_L) - \sigma_i^* \right| \le \frac{2^{L}\sqrt{ \eta_3^2 + \eta_4^2 }}{\sqrt{\lambda}\sigma_{\min}^{* {L-1}}\min_{i \in [r_{\sigma}] } \left|\varphi^\prime(\sigma_i^*)\right| }  \|\nabla G(\bm W)\|_F. 
\end{align*} 
Using this and \eqref{eq:wl+1} in \Cref{lem:pre}, we obtain for eacl $l \in [L]$,
\begin{align*}
\left| \sigma_i(\bm W_l) - \sigma_i^* \right| \le\left(\frac{2^{L}\sqrt{ \eta_3^2 + \eta_4^2 }}{\sqrt{\lambda}\sigma_{\min}^{* {L-1}}\min_{i \in [r_{\sigma}] } \left|\varphi^\prime(\sigma_i^*)\right| }  + \frac{3\sqrt{2}(L-1)\sigma_{\max}^*}{4\lambda\sigma_{\min}^*} \right) \|\nabla G(\bm W)\|_F. 
\end{align*} 
Then, we complete the proof. 
\end{proof} 


\section{Experimental Results}\label{sec:expe}

In this section, we conduct experiments under different settings to validate our theoretical results. Specifically, we employ GD to solve Problem \eqref{eq:P} using the \texttt{PyTorch} library. Our codes are implemented in Python on a workstation equipped with $24$ GB of RAM and an AMD Ryzen-7 8845H processor with integrated Radeon 780M Graphics operating at $3.80$ GHz. We terminate the algorithm when the squared gradient norm satisfies $\|\nabla f(\bm{W}^k)\|_F^2 \leq 10^{-6}$ and the function value change satisfies $|f(\bm{W}^k) - f(\bm{W}^{k-1})| \leq 10^{-7}$ for all $k=1,2,\dots$, where $\bm W^k$ is the $k$-th iterate.

\subsection{Linear Convergence to Critical Points}\label{subsec:exp 1}

In this subsection, we investigate the convergence behavior of GD to different critical points of Problem \eqref{eq:F}. In our experiments, we set $d_L=20$, $d_0=10$, and $d_l=32$ for each $l=2,\dots,L-1$. Then, we i.i.d. sample each entry of $\bm Y \in \R^{d_L\times d_0}$ from the standard Gaussian distribution, i.e., $y_{ij} \overset{i.i.d.}{\sim} \mathcal{N}(0,1)$. Moreover, we set the regularization parameters $\lambda_l = 10^{-4}$ for all $l \in [L]$ and the learning rate $4.5 \times 10^{-4}$. To ensure convergence to different critical points, we initialize the weight matrices in the neighborhood of two different critical points of Problem \eqref{eq:F}. Now, we specify how to construct these critical points as follows. We apply the SVD to $\bm Y$ and solve the equation \eqref{eq:sigma W} to get its roots. Using these results and \Cref{thm:opti}, we can respectively construct an optimal solution, denoted by $\bm W_{\rm opt}^*$, and a non-global critical point, denoted by $\bm W_{\rm crit}^*$, of Problem \eqref{eq:F}.  
%Specifically, we first compute an optimal solution $\bm W^*$ using \Cref{thm:opti} and 
Then, we set $\bm W^0 = \bm W^* + 0.01 \bm \Delta$, where $\bm W^*$ is $\bm W_{\rm opt}^*$ or $\bm W_{\rm crit}^*$ and each entry of $\bm \Delta$ is i.i.d. sampled from the standard Gaussian distribution. For each initialization, we run GD for solving Problem \eqref{eq:F} with different depths $L\in \{2,4,6\}$.  


% \begin{figure}[t]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%     \centering
%         \includegraphics[width=\textwidth]{Figures/plot_246GD1.eps}\vspace{-0.1in}
%         \subfigure{(a) Training loss }
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.48\textwidth}
%     \centering
%         \includegraphics[width=\textwidth]{Figures/plot_246GD2.eps}\vspace{-0.1in}
%         \subfigure{(b) Square gradient Norm }
%     \end{minipage} \vspace{-0.1in}
%     \caption{Linear convergence of GD to an optimal solution of Problem \eqref{eq:F}: The $x$-axis is the number of iterations; The $y$-axis is the function value gap $F(\bm W^k) - F(\bm W_{\rm opt}^*)$ (resp., $\nabla F(\bm W^k)$) in (a) (resp., (b)).}
%     \label{fig:convergence_rates_1}
% \end{figure}
\begin{figure}[ht]
    %\hspace{-0.2in}
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{Figures/plot_246GD1-eps-converted-to}\vspace{-0.05in}
        \subfigure{(a) Training loss }
\end{minipage}\quad\hspace{-0.12\textwidth}
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{Figures/plot_246GD2-eps-converted-to}\vspace{-0.05in}
        \subfigure{(b) Squared gradient norm }
    \end{minipage}\\
    
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{Figures/plot_246GDtwo1-eps-converted-to}\vspace{-0.05in}
        \subfigure{(c) Training loss }
    \end{minipage}\quad\hspace{-0.12\textwidth}
    \begin{minipage}{0.48\textwidth}
        \centering
        %\resizebox{0.8\textwidth}{!}
        \includegraphics[width=0.7\textwidth]{Figures/plot_246GDtwo2-eps-converted-to}\vspace{-0.05in}
        \subfigure{(d) Squared gradient norm }
    \end{minipage}\vspace{-0.1in} 
    \caption{Linear convergence of GD to a critical point of Problem \eqref{eq:F}.
     In (a) and (b), $\{\bm W^k\}$ converges to an optimal solution $\bm W_{\rm opt}^*$; in (c) and (d), $\{\bm W^k\}$ converges to a non-optimal critical point $\bm W_{\rm crit}^*$. $F^{\rm end}$ denotes the training loss at the final step when the stop condition is triggered.  
    %: The $x$-axis is the number of iterations; The $y$-axis is the function value gap $F(\bm W^k)-F(\bm W_{\rm opt}^*)$ (resp., $\|\nabla F(\bm W^k)\|_F^2$) in (a) (resp., (b)).
  %      {\bf Bottom row}: Linear convergence of GD to a non-optimal critical point: The $y$-axis is the function value gap $F(\bm W^k)-F(\bm W_{\rm crit}^*)$ (resp., $\|\nabla F(\bm W^k)\|_F^2$) in (c) (resp., (d)).
    }
    \label{fig:convergence_rates}
\end{figure}

We respectively plot the function value gap $F(\bm W^k) - F(\bm W^{\rm end})$ and the squared gradient norm $\|\nabla F(\bm W^k)\|_F^2$ against the iteration number in \Cref{fig:convergence_rates}(a) and (b) (resp., \Cref{fig:convergence_rates}(c) and (d)) for $\bm W_{\rm opt}^{\rm end}$ (resp., $\bm W_{\rm crit}^{\rm end}$). We also report the function values of the final iterate, denoted by $\bm W^{\rm end}$, in different settings in \Cref{tab:loss_table}. As observed from \Cref{fig:convergence_rates}, and \Cref{tab:loss_table}, GD converges to an optimal solution at a linear rate for solving Problem \eqref{eq:F} with different network depths, and similarly, it converges to a non-optimal critical point at a linear rate. This aligns with our linear convergence analysis for all critical points of Problem \eqref{eq:F} in \Cref{prop:threecondition}, which leverages the error bound of Problem \eqref{eq:F} and supports \Cref{thm:eb}. 
% \begin{figure}[t]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%     \centering
% \includegraphics[width=\textwidth]{Figures/plot_246GDtwo1.eps}\vspace{-0.1in}
%         \subfigure{(a) Training loss }
%     \end{minipage}
%     \hfill
% \begin{minipage}{0.48\textwidth}
%     \centering
% \includegraphics[width=\textwidth]{Figures/plot_246GDtwo2.eps}\vspace{-0.1in}
%         \subfigure{(b)Square gradient norm }
%     \end{minipage}\vspace{-0.1in}
%         \caption{Linear convergence of GD to a non-optimal critical point for solving Problem \eqref{eq:F}: The $x$-axis is number of iteration; The $y$-axis is the function value gap $F(\bm W^k) - F(\bm W_{\rm crit}^*)$ (resp., $\|\nabla F(\bm W^k)\|_F^2$) in (a) (resp., (b)).}
% \label{fig:convergence_rates_2}
% \end{figure} 
\smallskip 
\begin{table}[!htbp]
\centering
\begin{tabular}{c||c|c|c}
\hline
 & $L = 2$ & $L = 4$ & $L = 6$ \\ \hline 
$F(\bm W_{\rm opt}^*)$& $ 9.2900\times 10^{-3}$&$8.1723 \times 10^{-3}$&$9.5264\times 10^{-3}$\\ \hline
$F(\bm W^{\rm end}_{\rm opt})$ & $9.2985 \times 10^{-3}$ & $8.2020 \times 10^{-3}$ & $9.5741 \times 10^{-3}$ \\ \hline 
$F(\bm W^*_{\rm cri})$ & $6.8038 \times 10^{-1}$ & $6.7906 \times 10^{-1}$ & $6.8022 \times 10^{-1}$ \\  \hline 
$F(\bm W^{\rm end}_{\rm cri})$ & $9.2980 \times 10^{-3}$ & $6.7909 \times 10^{-1}$ & $6.8027 \times 10^{-1}$ \\ \hline
\end{tabular}
\caption{The function values at the final iterate $\bm W^{\rm end}$ and the critical point $\bm W^*$.}
\label{tab:loss_table}
\end{table}

From \Cref{fig:convergence_rates}(c) and (d) and \Cref{tab:loss_table}, we observe that the convergence behavior of GD to a critical point when \( L = 2 \) differs significantly from those when \( L = 4 \) and \( L = 6 \). This difference arises from the benign global loss landscape of Problem \eqref{eq:F} when \( L = 2 \) as shown in \cite{zhou2022optimization}. That is when \( L = 2 \), each critical point of Problem \eqref{eq:F}  is either a global minimizer or a strict saddle point. This, together with the result in \cite{lee2019first}, implies that GD almost surely avoids strict saddle points and converges to an optimal solution. Consequently, even when the starting point is initialized near a non-optimal critical point, GD will escape from it and eventually converge to a global optimum, although it may require more iterations to do so. In contrast, when $L=4$ and $L=6$, we conjecture that the loss landscape of Problem \eqref{eq:F} is not benign, which contains non-optimal local minimizers, to which GD may converge. In the future, we will study the global optimization loss landscape of Problem \eqref{eq:F} for different $L$.  

\subsection{Convergence Behavior in General Settings}

In this subsection, we investigate the convergence behavior of GD in more general setups extending beyond linear networks. Specifically, the network depth is fixed as $L=4$ and network widths at different layers are set as follows: $d_L=16$, $d_0=10$, and $d_l=32$ for each $l=2,\dots,L-1$. The regularization parameter is set as $\lambda_l = 5\times10^{-5}$ for all $l \in [L]$ and the learning rate of GD is set as $10^{-3}$. We use the default initialization scheme in \texttt{PyTorch} to initialize the weights for GD. The data matrix $\bm{X}$ is generated according to a uniform distribution using the \texttt{xavier\_uniform\_} function in \texttt{PyTorch}, while the target matrix $\bm Y$ is generated using the same approach as described in \Cref{subsec:exp 1}.  Below, we outline the different setups used in our experiments. 

\vspace{-0.15in}
\paragraph{General data input.} 
In this experiment, we consider general data inputs $\bm X_1,\bm X_2,\bm X_3$ instead of orthogonal inputs $\bm X$. Then, we apply GD to optimize Problem \eqref{eq:P} for each of these different data matrices. 

\vspace{-0.15in}
\paragraph{Linear networks with bias.} Next, we study the regularized loss of deep linear networks with bias terms as follows: 
\[
\min_{\{\bm W_l,\bm b_l\}}\; \sum_{i=1}^N\left\| \bm{W}_L(\bm W_{L-1} \ldots (\bm {W}_1 \bm x_i+ \bm{b}_1)+\bm b_{L-1}) + \bm b_L - \bm y_i \right\|^2 + \sum_{l=1}^{L} \lambda_l \left(\left\| \bm{W}_l \right\|_F^2 + \left\| \bm{b}_l \right\|^2 \right),
\]
 where $\bm x_i$ and $\bm y_i$ respectively denote the $i$-th column of $\bm X$ and $\bm Y$. We apply GD to solve the above problem when the input is either an identity matrix or general input data.

\vspace{-0.15in}
\paragraph{Deep nonlinear networks.} 
Finally, we study the performance of GD for solving the regularized loss of deep nonlinear networks with different activation functions:
\begin{align*}
   \min_{\{\bm W_l,\bm b_l\}}\ \sum_{i=1}^N \left\|\bm W_L \sigma \left( \bm W_{L-1}\ldots\sigma\left(\bm W_1 \bm x_i + \bm b_1 \right) + \bm b_{L-1} \right) + \bm b_L - \bm y_i \right\|_F^2 + \sum_{l=1}^{L}  \lambda_l \left(\left\| \bm{W}_l \right\|_F^2 +  \left\| \bm{b}_l \right\|_F^2 \right), 
\end{align*}
where $\sigma(\cdot)$ denotes an activation function. We set $\bm{X}$ as the identity matrix and use GD for solving the above problem when the activation functions are chosen as ReLU, Leaky ReLU, and tanh, respectively.

\vspace{-0.1in}
\begin{figure}[h!]
    \centering
\begin{minipage}{0.33\textwidth}
\centering  \includegraphics[width=\textwidth]{Figures/plot_twoplot_x-eps-converted-to}\vspace{-0.05in}
\subfigure{(a) General data input}
\end{minipage}%
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{Figures/plot_twoplot_biasx-eps-converted-to}\vspace{-0.05in}
\subfigure{(b) Linear networks with bias}
    \end{minipage}%
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{Figures/plot_threeplot_activation-eps-converted-to}\vspace{-0.05in}
\subfigure{(c) Deep nonlinear networks}
\end{minipage} \vspace{-0.1in} 
\caption{Linear convergence of GD under different settings: %The $x$-axis is the number of iterations, and the $y$-axis is the function value gap $F(\bm{W}^k) - F(\bm{W}^{\text{end}})$. 
(a) Three different matrices $\bm X_1$, $\bm X_2$, and $\bm X_3$, with condition numbers of 43.43, 36.04, and 16.36, respectively. (b) Identity matrix, $\bm X_1$, and $\bm X_2$, with condition numbers of 1, 4.82, and 14.62, respectively. (c) Identity matrix as data input with ReLU, Leaky ReLU, and tanh as activation functions. $F^{\rm end}$ denotes the training loss at the final step when the stop condition is triggered.}
\label{fig:extent experiments}
\end{figure}

For the above three different settings, we plot the function value gap $F(\bm W^k) - F(\bm W^*)$ against the iteration number in \Cref{fig:extent experiments}. It is observed from \Cref{fig:extent experiments} that GD converges to a solution at a linear rate across these settings. This consistent behavior leads us to conjecture that the error-bound condition may hold for deep networks in more general scenarios. Additionally, we observe that the number of iterations required to meet the stopping criterion generally increases as the condition number of the input data becomes larger. Exploring this phenomenon presents an interesting direction for future research. 


\section{Conclusions}\label{sec:con}

In this paper, we studied the regularized squared loss of deep linear networks (i.e., Problem \eqref{eq:F}) and proved its error bound, a critical regularity condition that characterizes local geometry around the critical point set. This result is not only theoretically significant but also lays the foundation for establishing strong convergence guarantees for various methods for solving Problem \eqref{eq:F}. To establish the error bound, we explicitly characterized the critical point set of \eqref{eq:F} and developed new analytic techniques to show the error bound, which may be of independent interest. Our numerical results across different settings provide strong support for our theoretical findings. One future direction is to extend our analysis to Problem \eqref{eq:P} with more general data input $\bm X$ and loss functions. Another interesting direction is to investigate the regularized loss for deep nonlinear networks. 

\bibliographystyle{abbrvnat}
\bibliography{reference,NC}

\appendix
\begin{appendix}
\begin{center}
{\Large \bf Supplementary Material}
\end{center} 
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}

The appendix is organized as follows. In \Cref{app:sec A}, we provide supplementary results and proofs for \Cref{sec:proof}. In \Cref{app sec:counter}, we show that \Cref{AS:2} is a necessary condition for the error bound to hold for Problem \eqref{eq:F}. In \Cref{app:sec C}, we provide a unified framework to establish linear convergence under the error bound. Finally, we present auxiliary results to prove our main results for completeness. 

\section{Supplementary Results and Proofs for \Cref{sec:proof}}\label{app:sec A}  

\begin{lemma}\label{lem:commute}
    Let \(a > 0\) be a constant, \(\bm \Sigma \in \R^{n\times n} \) be a diagonal matrix, and \(\bm Q \in \mathcal{O}^n\) be an orthogonal matrix. Then, if \(\|\bm \Sigma -  a \bm I\| \leq a/2\), it holds that 
    \[
    \|\bm Q \bm \Sigma^2 - \bm \Sigma^2\bm Q\|_F \geq a \|\bm Q \bm \Sigma - \bm \Sigma\bm Q\|_F.
    \]
\end{lemma}
\begin{proof}
For ease of exposition, let $\bm \Delta := \bm \Sigma - a\bm I$. We compute
\begin{align*}
\|\bm Q \bm \Sigma^2 - \bm \Sigma^2\bm Q\|_F & = \|\bm Q \left(\bm \Delta + a \bm I \right)^2 - \left(\bm \Delta + a \bm I \right)^2\bm Q\|_F = \left\| \bm Q \bm \Delta^2 - \bm \Delta^2 \bm Q + 2a\left(\bm Q\bm \Delta - \bm \Delta\bm Q \right) \right\|_F \\
& \ge 2a \left\|  \bm Q\bm \Delta - \bm \Delta\bm Q  \right\|_F - \left\| \bm Q \bm \Delta^2 - \bm \Delta^2 \bm Q \right\|_F \\
& \ge 2\left( a - \|\bm \Delta\|\right) \left\|  \bm Q\bm \Sigma - \bm \Sigma \bm Q  \right\|_F  \ge a\left\|  \bm Q\bm \Sigma - \bm \Sigma \bm Q  \right\|_F,
\end{align*}
where the second inequality follows from $\| \bm Q \bm \Delta^2 - \bm \Delta^2 \bm Q \|_F = \|\bm \Delta(\bm \Delta\bm Q - \bm Q\bm \Delta) + (\bm \Delta\bm Q - \bm Q\bm \Delta)\bm \Delta \|_F \le 2\|\bm \Delta\| \|\bm Q\bm \Sigma - \bm \Sigma \bm Q\|_F$ and $\bm Q\bm \Delta - \bm \Delta\bm Q = \bm Q\bm \Sigma - \bm \Sigma \bm Q$, and the last inequality uses $\|\bm \Delta\| \le a/2$.   
\end{proof} 


\begin{proof}[Proof of \Cref{prop:eb zero}] 
(i) According to $L=2$, \eqref{set:G2}, and \eqref{set:W sigma}, we have $\mathcal{W}_{\bm 0} = \{(\bm 0, \bm 0)\}$. Then, we have
\begin{align}\label{eq1:lem eb zero}
\mathrm{dist}^2(\bm W,\mathcal{W}_{\bm 0}) = \|\bm W_1\|_F^2 + \|\bm W_2\|_F^2. 
\end{align} 
Without loss of generality, assume that $\| \bm{W}_1 \|_F \ge \| \bm{W}_2 \|_F$. Now, we compute
\begin{align}
 &\frac{1}{2} \nabla_{\bm{W}_1} G(\bm{W}) =  \bm{W}_2^T\left(\bm{W}_2 \bm{W}_1 - \sqrt{\lambda}\bm{Y}\right)  + \lambda  \bm{W}_1,\label{eq2:lem eb zero}\\ 
 & \frac{1}{2}\nabla_{\bm{W}_2} G(\bm{W}) = \left(\bm W_2\bm W_1 - \sqrt{\lambda}\bm Y\right)\bm W_1^T + \lambda \bm W_2\label{eq3:lem eb zero}.
\end{align}
Using \eqref{eq2:lem eb zero}, we have 
\begin{align}\label{eq4:lem eb zero}
\frac{\sqrt{\lambda}}{2}\| \nabla_{\bm{W}_1} G(\bm{W}) \|_F & \ge \left\|\lambda \bm W_2^T\bm Y - \lambda^{\frac{3}{2}}\bm W_1\right\|_F - \sqrt{\lambda}\|\bm W_2^T\bm W_2\bm W_1\|_F \notag\\
& \ge \left\|\lambda \bm W_2^T\bm Y - \lambda^{\frac{3}{2}}\bm W_1\right\|_F - \sqrt{\lambda}\|\bm W_2\|_F^2 \|\bm W_1\|_F.  
\end{align}
Moreover, multiplying $\bm Y^T$ on the both sides of \eqref{eq3:lem eb zero}, together with the triangular inequality, yields 
\begin{align*}
\frac{1}{2}\left\|\bm Y^T\nabla_{\bm{W}_2} G(\bm{W})\right\|_F \ge \left\| \sqrt{\lambda}\bm Y^T\bm Y\bm W_1^T - \lambda\bm Y^T\bm W_2 \right\|_F - \|\bm Y^T\bm W_2\bm W_1\bm W_1^T\|_F,
\end{align*}
which implies
\begin{align*} 
\frac{y_1}{2} \left\|\nabla_{\bm{W}_2} G(\bm{W})\right\|_F \ge \left\| \sqrt{\lambda}\bm Y^T\bm Y\bm W_1^T - \lambda\bm Y^T\bm W_2 \right\|_F - y_1\|\bm W_2\|_F \|\bm W_1\|_F^2. 
\end{align*}
Summing up this inequality with \eqref{eq4:lem eb zero} and using the triangular inequality, we obtain 
\begin{align*}
&\ \frac{\sqrt{\lambda}}{2}\| \nabla_{\bm{W}_1} G(\bm{W}) \|_F + \frac{y_1}{2} \left\|\nabla_{\bm{W}_2} G(\bm{W})\right\|_F \\
\ge &\ \sqrt{\lambda}\left\|\bm W_1\left(\lambda\bm I - \bm Y^T\bm Y \right)  \right\|_F - \|\bm W_1\|_F\|\bm W_2\|_F\left(y_1\|\bm W_1\|_F + \sqrt{\lambda}\|\bm W_2\|_F \right) \\
\ge &\ \left(\sqrt{\lambda} \min\left\{ \min_{i \in [s_{p_Y}]}\left| \lambda - y_i^2 \right|, \lambda \right\}\right) \|\bm W_1\|_F - \left(\sqrt{\lambda} + y_1 \right)\|\bm W_1\|_F^3 \\
\ge &\ \frac{\sqrt{\lambda} }{2} \min\left\{ \min_{i \in [s_{p_Y}]}\left| \lambda - y_i^2 \right|, \lambda \right\}  \|\bm W_1\|_F, 
\end{align*}
where the second inequality follows from \eqref{eq:Y}, $\lambda \neq y_i^2$ for all $i \in [p_Y]$ due to \eqref{eq:AS lambda}, and $\|\bm A\bm B\|_F \ge \sigma_{\min}(\bm A)\|\bm B\|_F$ when $\bm A$ is non-degenerate, and the last inequality uses \eqref{eq:dist 6}. This, together with \eqref{eq1:lem eb zero}, directly implies \eqref{eq:eb zero 1}.


(ii) According to \eqref{set:G2} and \eqref{set:W sigma}, we have $\mathcal{W}_{\bm 0} = \{(\bm 0, \bm 0, \dots, \bm 0)\}$. Then, we have
\begin{align}\label{eq1:prob eb zero}
\mathrm{dist}^2(\bm W,\mathcal{W}_{\bm 0}) = \sum_{l=1}^L \|\bm W_l\|_F^2. 
\end{align}
Let $k \in [L]$ be such that $k \in \argmax_{l\in [L]} \|\bm W_l\|_F$. Then, we compute
\begin{align*}
\frac{1}{2}\left\|\nabla_{\bm W_k} G(\bm W)\right\|_F & \overset{\eqref{eq:grad G}}{=} \left\| \bm W_{L:k+1}^T \left(\bm W_{L:1} - \sqrt{\lambda}\bm Y \right)\bm W_{k-1:1}^T + \lambda \bm W_k \right\|_F \\
& \ge \lambda \|\bm W_k\|_F - \left( \|\bm W_k\|_F^{2L-1} + \sqrt{\lambda}y_1  \|\bm W_k\|_F^{L-1} \right) \\
& = \left( \lambda - \|\bm W_k\|_F^{2L-2} - \sqrt{\lambda}y_1\|\bm W_k\|_F^{L-2} \right) \|\bm W_k\|_F  \ge \frac{\lambda}{3}\|\bm W_k\|_F,  
\end{align*}
where the last inequality uses \eqref{eq:dist 0}. This, together with \eqref{eq1:prob eb zero}, yields
\begin{align*}
\mathrm{dist}^2(\bm W,\mathcal{W}_{\bm 0}) \le L\|\bm W_k\|_F^2 \le \frac{9L}{4\lambda^2}\left\|\nabla_{\bm W_k} G(\bm W)\right\|_F^2,
\end{align*}
which directly implies \eqref{eq:eb zero}. 
\end{proof}

\section{Showing the Necessity of \Cref{AS:2}}\label{app sec:counter} 

Before we proceed, we claim that the error bound of $F$ (see Problem \eqref{eq:F}) holds if and only if the error bound of $G$ (see Problem \eqref{eq:G}) holds. Indeed, it follows from (ii) in \Cref{lem:equi FG} that the ``if'' direction holds. Now, it remains to show the ``only if'' direction. Suppose that the error bound  of Problem \eqref{eq:F} holds (see \eqref{eq:eb F}) for all critical points. Using the proof setup in \Cref{lem:equi FG}, it follows \eqref{eq:distconnect} that 
\[
    \frac{ \lambda  }{ \lambda_{\max} } \, \mathrm{dist}(\bm{Z}, \mathcal{W}_G) \leq \frac{ \lambda }{ \sqrt{ \lambda_{\max} } } \, \mathrm{dist}(\bm{W}, \mathcal{W}) \leq \frac{ \lambda \kappa_1 }{ \sqrt{ \lambda_{\max} } } \, \| \nabla F(\bm{W}) \|_F \leq \kappa_1 \, \| \nabla G(\bm{Z}) \|_F,
\]
where the second inequality follows from \eqref{eq:eb F} and the last inequality uses \eqref{eq:gradientnorm}. Then, we prove the claim. Based on the error-bound equivalence between $F$ and $G$, establishing the necessity of \Cref{AS:2} for $F$ is equivalent to doing so for $G$.

\subsection{The Case $L=2$}

For ease of exposition, we denote $\widehat{\mathcal{W}}_{\bm \sigma^*} := \mathcal{W}_{\rm sort(\bm \sigma^*)}$ for each $\bm \sigma^*\in \mathcal{A}$, where $\mathrm{sort}(\cdot)$ is a sorting function that arranges the elements of a vector in decreasing order. 

\begin{lemma}\label{lem:counter 1}
Suppose that $L=2$ and \eqref{eq:AS lambda} does not hold. For any $\bm \sigma^* \in \cal A$, the error bound for the critical point set $\widehat{\mathcal{W}}_{\bm \sigma^*}$ of Problem \eqref{eq:G} fails to hold, i.e., for any $\kappa, \delta>0$, there exists $\bmw$ satisfying $\mathrm{dist}(\bmw,\widehat{\mathcal{W}}_{\bm \sigma^*}) \le \delta$ such that $\|\nabla G(\bmw)\|_F \le \kappa\cdot\mathrm{dist}(\bmw,\mathcal{W}_G)$. 
\end{lemma}
\begin{proof}
Recall that $\lambda = \lambda_1\lambda_2$ when $L=2$. Since \eqref{eq:AS lambda} does not hold and , there exists $i \in [r_Y]$ such that $y_i = \sqrt{\lambda}$. Now, we define $\bm W(t) := \left(\bm W_1(t),\bm W_2(t) \right)$ as follows: 
\begin{align*}
\begin{cases}
        &\bm W_1(t) = \bm Q_2\bm \Sigma_1(t) \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_{Y}},\bm O_{p_{Y}+1} \right), \\ 
        %\noalign{\smallskip}
        &\bm W_2(t) =  \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\widehat{\bm O}_{p_{Y}+1}^T \right)\bm \Sigma_2(t) \bm Q_2^T, \\
        &\bm \Sigma_l(t) = \mathrm{BlkDiag}\left(\mathrm{diag}(\bm \sigma^* +t \bm e_i), \bm 0\right) \in \R^{d_l\times d_{l-1}}, \forall l = 1,2, \\
        &\bm O_i \in \mathcal{O}^{h_i}, \forall i \in [p_{Y}], \bm O_{p_{Y}+1} \in \mathcal{O}^{d_0 - r_{Y}}, \widehat{\bm O}_{p_{Y}+1} \in \mathcal{O}^{d_2 - r_{Y}},
\end{cases}
\end{align*}
where $\bm e_i \in \R^{d_{\min}}$ is a standard basis vector with the $i$-th entry being 1 and all other entries being $0$. According to \Cref{thm:opti G}, one can verify that $\bm W(0) \in \widehat{\mathcal{W}}_{\bm \sigma^*}$. Using $y_i = \sqrt{\lambda}$, we obtain  
$\sigma^*_i \in \{x: x^3 -\sqrt{\lambda}y_i x+\lambda x=0,\ x\ge 0\} = \{0\}$.
Then, we compute 
\begin{align}\label{eq1:lem counter 1}
\frac{1}{2}\left\|\nabla_{\bmw_l} G(\bmw(t))\right\|_F \overset{\eqref{eq:grad G}}{=} |t^3- \sqrt{\lambda}y_i t+\lambda t|=|t^3|,\  l=1,2,
\end{align}
where the second equality follows from $y_i = \sqrt{\lambda}$. 
Moreover, we have
\begin{align} \mathrm{dist}^2(\bmw(t),\widehat{\mathcal{W}}_{\bm \sigma^*})
    & =\min_{(\bm W_1,\bm W_2) \in \widehat{\mathcal{W}}_{\bm \sigma^*}} \|\bmw_1(t)-\bmw_1 \|_F^2+\|\bmw_L(t)-\bmw_2 \|_F^2 \notag\\
    &   \ge \|\bm \Sigma_1(t) - \bm \Sigma_1(0)\|_F^2 +  \|\bm \Sigma_2(t) - \bm \Sigma_2(0)\|_F^2  = 2t^2, \label{eq:final count1} 
\end{align}
where the first inequality follows from Mirsky's inequality (see \Cref{lem:mirsky}). 
When $t \le  {\delta_\sigma}/{3}$, we have 
\[
\mathrm{dist}(\bm W(t), \widehat{\mathcal{W}}_{\bm \sigma^*})\le \|\bm W(t) - \bm W(0)\|_F  \le \frac{\sqrt{2}\delta_\sigma}{3}.
\]
Using this inequality and \Cref{prop:set W}(ii), we have for each $  \bar{\bm \sigma}^* \in \mathcal{A}$ satisfying $ \widehat{\mathcal{W}}_{\bm \sigma^*}\neq \widehat{\mathcal{W}}_{\bar{\bm \sigma}^*}$, 
\[
    \mathrm{dist}(\bm W(t), \widehat{\mathcal{W}}_{\bar{\bm \sigma}^*}) \ge \mathrm{dist}( \widehat{\mathcal{W}}_{\bm \sigma^*},\widehat{\mathcal{W}}_{\bar{\bm \sigma}^*}) - \mathrm{dist}(\bm W(t), \widehat{\mathcal{W}}_{\bm \sigma^*}) 
    \ge \left(1-\frac{\sqrt{2}}{3}\right)\delta_\sigma > \mathrm{dist}(\bm W(t), \widehat{\mathcal{W}}_{\bm \sigma^*}).
\]
Consequently, we obtain $\mathrm{dist}(\bmw(t),\mathcal{W}_{G}) = \mathrm{dist}(\bmw(t),\widehat{\mathcal{W}}_{\bm \sigma^*})$.
This, together with \eqref{eq1:lem counter 1} and \eqref{eq:final count1}, implies
\begin{align*}
\|\nabla G(\bm W(t))\|_F =	 2\sqrt{2}|t|^{3} \le  2^{3/4} \mathrm{dist}^{\frac{3}{2}}(\bmw(t),\widehat{\mathcal{W}}_{\bm \sigma^*})=  2^{3/4} \mathrm{dist}^{\frac{3}{2}}(\bmw(t),\mathcal{W}_{G}). 
\end{align*}
Obviously, the error bound does not hold when $t \to 0$.  
\end{proof}


\subsection{The Case $L \ge 3$}

To begin, we show the following lemma to derive the equivalent condition of \eqref{eq:AS lambda1}: 

\begin{lemma}\label{lem:phi}
When $L \ge 3$, \eqref{eq:AS lambda1} holds if and only if for all $0 \neq \sigma^* \in \mathcal{Y}$, we have $\varphi'(\sigma^*) \neq 0$, where  $\cal Y$ and $\varphi(\cdot)$ are defined in \eqref{set:Y} and  \eqref{eq:phi}, respectively.
\end{lemma} 
\begin{proof}
Note that $\varphi'(\sigma^*) = 0$ is equivalent to
\begin{equation}\label{eq:gprime=0}
\varphi'(\sigma^*)=\frac{L}{\sqrt{\lambda}} \sigma^{*(L-1)} + \sqrt{\lambda}(2-L)\sigma^{*(1-L)} = 0.
\end{equation}
According to $\sigma^* \in \mathcal{Y}$, there exists $j \in [d_{\min}]$ such that 
\begin{equation}\label{eq:gsigma=y}
        \varphi(\sigma^*) = \frac{\sigma^{*(2L-1)} + \lambda \sigma^*}{\sqrt{\lambda} \sigma^{*(L-1)}} = y_j.
    \end{equation}
    Combining \eqref{eq:gprime=0} and \eqref{eq:gsigma=y}, we obtain
    \[
        y_j= \left[ \left( \frac{L-2}{L} \right)^{\frac{L}{2(L-1)}} + \left( \frac{L}{L-2} \right)^{\frac{L-2}{2(L-1)}} \right]  \lambda^{\frac{1}{2(L-1)}}.
    \]
    This implies that \eqref{eq:AS lambda1} does not hold. 
    
    Conversely, suppose that \eqref{eq:AS lambda1} does not hold. This implies that there exists $j \in [d_{\min}]$ such that $y_j = \left[ \left( \frac{L-2}{L} \right)^{\frac{L}{2(L-1)}} + \left( \frac{L}{L-2} \right)^{\frac{L-2}{2(L-1)}} \right]  \lambda^{\frac{1}{2(L-1)}}$. Using this, one can verify that  
    \begin{equation}
        x^{2L-1}-\sqrt{\lambda}y_j x^{L-1}+\lambda x =0
    \end{equation}
    have a positive root such that
    \[
        x^* = \left(\frac{\lambda(L-1)}{L}\right)^{\frac{1}{2(L-1)}}\ \text{and}\ \varphi'(x^*) = 0.
    \]
    By the definition of $\mathcal{Y}$, we have $x^* \in \mathcal{Y}$. Then, we complete the proof. 
\end{proof}
\begin{remark}
\label{re:remarkl=2}
When $ L = 2 $, we compute
$$\varphi'(\sigma^*) = \frac{2}{\sqrt{\lambda}} \sigma^* > 0,\ \forall\  0 \neq \sigma^* \in \mathcal{Y}.$$
Therefore, $\varphi'(\sigma^*) \neq 0$ holds trivially.  
\end{remark}
\begin{lemma}\label{lem:counter 2}
Suppose that $L \ge 3$ and \eqref{eq:AS lambda1} does not hold. There exists $\bm \sigma^* \in \cal A$ such that the error bound for the critical point set $\widehat{\mathcal{W}}_{\bm \sigma^*}$ of Problem \eqref{eq:G} fails to hold, i.e., for any $\kappa,\ \delta>0$, there exist $\bmw$ satisfying $\mathrm{dist}(\bmw,\widehat{\mathcal{W}}_{\sigma^*}) \le \delta$ such that $\|\nabla G(\bmw)\|_F \le \kappa\cdot \mathrm{dist}(\bmw,\mathcal{W}_G)$. 
\end{lemma}
\begin{proof}
According to \Cref{lem:phi} and the fact that \eqref{eq:AS lambda1} does not hold, there exists $\bm \sigma^* \in \cal A$ such that $\varphi^\prime(\sigma_i^*) = 0$ for some $i \in [d_{\min}]$. This implies 
\begin{align*}
    f(\sigma^*_i) := (\sigma^*_i)^{2L-1} - \sqrt{\lambda} y_i (\sigma^*_i)^{L-1} + \lambda \sigma^*_i = 0,\ 
    \varphi(\sigma^*_i) = y_i.
\end{align*}
Note that $f(x) = \sqrt{\lambda} x^{L-1} \varphi(x) - \sqrt{\lambda} y_i x^{L-1}$ and we compute 
\begin{align*}
    f^\prime(\sigma_i^*) = \sqrt{\lambda} (\sigma_i^*)^{L-1} \varphi'(\sigma_i^*) = 0.
\end{align*}
    Now, we define $\bmw(t)=(\bmw_1(t),\cdots,\bmw_L(t))$  as follow:
    \begin{align*}
    \begin{cases}
        &\bm W_1(t) = \bm Q_2\bm \Sigma_1(t) \mathrm{BlkDiag}\left( \bm O_1,\dots,\bm O_{p_{Y}},\bm O_{p_{Y}+1} \right), \\ 
        %\noalign{\smallskip}
        &\bm W_l(t) = \bm Q_{l+1} \bm \Sigma_l(t) \bm Q_l^T,\ l=2,\dots,L-1, \bm Q_l \in \mathcal{O}^{d_{l-1}},\ l=2,\dots,L, \\
        % \noalign{\smallskip}
        &\bm W_L(t) =  \mathrm{BlkDiag}\left( \bm O_1^T,\dots,\bm O_{p_{Y}}^T,\widehat{\bm O}_{p_{Y}+1}^T \right)\bm \Sigma_L(t) \bm Q_L^T, \\
        &\bm \Sigma_l(t) = \mathrm{BlkDiag}\left(\mathrm{diag}(\bm \sigma^* +t \bm e_i), \bm 0\right) \in \R^{d_l\times d_{l-1}},\ \forall l \in [L], \\
        &\bm O_i \in \mathcal{O}^{h_i}, \forall i \in [p_{Y}],\ \bm O_{p_{Y}+1} \in \mathcal{O}^{d_0 - r_{Y}}, \widehat{\bm O}_{p_{Y}+1} \in \mathcal{O}^{d_L - r_{Y}}. 
    \end{cases}
    \end{align*}
It follows from \Cref{thm:opti G} that $\bmw(0) \in \mathcal{W}_G$. 
% We compute 
% \begin{align*}
%    \frac{1}{2} \nabla_{\bmw_l} G(\bmw(t))
%     = \bmw_{L:l+1}(t)^T\bmw_{L:1}(t)\bmw_{l-1:1}(t)^T - \sqrt{\lambda}\bmw_{L:l+1}(t)^T\bm Y \bmw_{l-1:1}(t)^T+\lambda \bm W_l(t),\ \forall l \in [L].
% \end{align*}
Therefore, we obtain for all $l \in [L]$,  
\[
 \frac{1}{2}   \|\nabla_{\bmw_l} G(\bmw(t))\|_F \overset{\eqref{eq:grad G}}{=} \left|(\sigma^*_i+t)^{2L-1}- \sqrt{\lambda}y_i(\sigma^*_i+t)^{L-1}+\lambda (\sigma^*_i+t)\right|= \left|f(\sigma^*_i+t)\right|.
\]
Applying the Taylor expansion to $f(\sigma^*_i+t)$ at $\sigma^*$, together with $f(\sigma^*_i)=0$ and $ f^\prime(\sigma^*_i)=0$, yields that when $t \to 0$, 
$
    \|\nabla G(\bmw(t))\|_F = O(t^2).
$
We also note that
\[
    \mathrm{dist}^2(\bmw(t),\widehat{\mathcal{W}}_{\bm \sigma^*}) =  \|\bmw_1(t)-\bmw_1^*\|_F^2+\cdots+\|\bmw_L(t)-\bmw_L^*\|_F^2 \ge Lt^2,
\]
where the inequality follows from Weyl's inequality. Using the same argument in \Cref{lem:counter 1}, we conclude that $\mathrm{dist}(\bmw(t),\widehat{\mathcal{W}}_{\bm \sigma^*}) = \mathrm{dist}(\bmw(t),\mathcal{W}_{G})$ when $t$ is sufficient small. Then we have $\|\nablag\|_F =O\left(\mathrm{dist}^2(\bmw(t),\mathcal{W}_{G })\right)$, which implies that the error bound fails to hold.
\end{proof} 

\begin{remark}
    Under \Cref{AS:1}, it follows from \Cref{thm:eb} that \Cref{AS:2} is a sufficient condition for the error bound to hold for the critical point set $\mathcal{W}_G$. According to \Cref{lem:counter 1} and \Cref{lem:counter 2}, we conclude that \Cref{AS:2} is also a necessary condition. Therefore, we establish the sufficient and necessary condition for the error bound of the critical point set $\mathcal{W}_G$. 
\end{remark}


\section{Linear Convergence under the Error Bound}\label{app:sec C}



\begin{prop}[Linear Convergence Analysis \cite{karimi2016linear,schneider2015convergence,zhou2017unified}]\label{prop:threecondition} 
Suppose that \Cref{thm:eb} holds and the sequence $\{\bm{W}^k\}_{k \ge k_1}$ for some index $k_1 \ge 0$ satisfies the following conditions:
\begin{itemize}
    \item[(i)] (\textsl{Sufficient Decrease}) There exists a constant $\kappa_1 > 0$ such that
    \[
        F(\bmw^{k+1}) - F(\bmw^k) \leq -\kappa_1 \|\bmw^{k+1} - \bmw^k\|_F^2.
    \]
    \item[(ii)] (\textsl{Cost-to-Go Estimate}) There exists a constant $\kappa_2 > 0$ such that
    \[
        F(\bmw^{k+1}) - F(\bm{W}^*)\leq \kappa_2\left( \mathrm{dist}^2(\bmw^k, \mathcal{W}) + \|\bmw^{k+1} - \bmw^k\|_F^2 \right).
    \]
    \item[(iii)] (\textsl{Safeguard}) There exists a constant $\kappa_3 > 0$ such that
    \[
        \|\nabla F(\bmw^k)\|_F \leq \kappa_3 \|\bmw^{k+1} - \bmw^k\|_F.
    \]
\end{itemize}
Then, the sequence $\{ F(\bmw^k)\}_{k\geq k_1}$ converges Q-linearly to $F(\bm{W}^*)$ and $\{\bmw^k\}_{k\geq k_1}$ converges R-linearly to some $\bmw^* \in \mathcal{W}$. 
\end{prop}

We should point out that first-order methods, such as GD and proximal alternating minimization method, for solving Problem \eqref{eq:F} satisfy the above conditions provided that an appropriate step size is chosen. 
% \edit{Most gradient-based algorithms generate sequences that satisfy the above supposition.} 

\subsection{Proof of the P\L\ Inequality and Quadratic Growth}\label{subsec:PL QG}

\begin{lemma}[Local Lipschitz Property]\label{lem:locallip}
Let $\bm W$ and $\bar{\bm W}$ be arbitrary such that 
\begin{align}\label{eq:local}
\mathrm{dist}(\bmw, \mathcal{W}) \leq \delta_3\ \text{and}\ \mathrm{dist}(\bar{\bmw}, \mathcal{W}) \leq \delta_3,
\end{align}
where $\delta_3 > 0$ is a constant and $\mathcal{W}$ denotes the critical point set. Then, it holds that 
\[
    \|\nabla F(\bmw) - \nabla F(\bar{\bmw})\|_F \leq L_F \|\bmw - \bar{\bmw}\|_F,
\]
where $L_F > 0$ is a constant. 
\end{lemma}
\begin{proof}
    According to \Cref{thm:opti}, we note that $\mathcal{\bm W}$ is a compact set. It implies that there exists a positve constant $M$ such that $\|\bm W^*_l\| \le M$ for each $l \in [L]$. This, together with \eqref{eq:local}
and the triangular inequality yields that there exists a constant $M$ such that
\[
    \|\bmw_l\| \le M+\delta_3,\ \|\bar{\bmw}_l\| \leq M + \delta_3,\ \forall l \in [L].
\]
Then, we have
\[ 
  \frac{1}{2} \nabla_{\bmw_l} F(\bm W) - \frac{1}{2}\nabla_{\bar{\bmw}_l} F(\bar{\bm W})
   = \lambda_l (\bmw_l - \bar{\bmw}_l) + \bm R_1 + \bm R_2,
\]
where
\begin{align*}
    \bm R_1 &:= (\bmw_{l+1} - \bar{\bmw}_{l+1})^T \bmw_{L:l+2}^T \bmw_{L:1} \bmw_{l-1:1}^T + \cdots + \bar{\bmw}_{L:l+1}^T \bar{\bmw}_{L:1} \bar{\bmw}_{l-2:1}^T (\bmw_{l-1} - \bar{\bmw}_{l-1})^T, \\
    \bm R_2 & := (\bmw_{l+1} - \bar{\bmw}_{l+1})^T \bmw_{L:l+2}^T \bY \bmw_{l-1:1}^T + \cdots + \bar{\bmw}_{L:l+1}^T \bY \bar{\bmw}_{l-2:1}^T (\bmw_{l-1} - \bar{\bmw}_{l-1})^T.
\end{align*}
Then we compute 
\[
    \|\bm R_1\|_F + \|\bm R_2\|_F \leq \left(\|\bY\| (L-1) (M + \delta_3)^{L-1} + (2L-1) (M + \delta_3)^{2L-1} \right) \|\bmw_l - \bar{\bmw}_l\|_F.
\]
It implies that
\begin{align*}
   \|\nabla F(\bmw) - \nabla F(\bar{\bmw})\|_F \leq 2L(\lambda_{\max} + L (M + \delta_3)^{L-1}\|\bY\|  +  2L(M + \delta_3)^{2L-1}) \|\bmw - \bar{\bmw}\|_F. 
\end{align*}
\end{proof}

\begin{proof}[Proof of \Cref{coro:PL QG}] 
(i) Let \( \mathcal{U} \) be a neighborhood  in which both \Cref{thm:eb} and \Cref{lem:locallip} hold and $F(\bm W) \le F(\bm W^*) + \nu$, where $\nu > 0$ is a constant. For any \( \bm{W} \in \mathcal{U} \), let $\bm W^* \in \mathcal{W}$ be such that 
\begin{align}\label{eq1:prop PL}
\mathrm{dist}(\bm{W}, \mathcal{W}) = \| \bm{W} - \bm{W}^* \|_F.
\end{align}
According to \Cref{lem:locallip}, it follows that
\begin{align*}
F(\bm{W}) & \le F(\bm{W}^*) + \langle \nabla F(\bm W^*), \bm W - \bm W^*\rangle + \frac{L_F}{2}\|\bm W - \bm W^*\|_F^2  \\
& =  F(\bm{W}^*) + \frac{L_F}{2}\|\bm W - \bm W^*\|_F^2 \le F(\bm{W}^*) + \frac{\kappa_1L_F}{2}\|\nabla F(\bm W)\|_F^2,
\end{align*}
where the equality is because $\bm W^*$ is a critical point, and the last inequality uses \eqref{eq:eb F} and \eqref{eq1:prop PL}. 

(ii) This result has been established in \cite[Proposition~2.2]{rebjock2024fast}.
\end{proof}



% \edit{
% \begin{prop}[Linear Convergence Analysis]
% Suppose that the assumptions \Cref{AS:1} and \Cref{AS:2} hold, and the sequence $\{\bm{W}^k\}_{k \ge k_1}$ satisfies the following conditions:
% \begin{itemize}
%     \item[(i)] \textit{Sufficient Decrease:} There exists a constant $\omega > 0$ such that
%     \[
%         F(\bm{W}^{k+1}) - F(\bm{W}^k) \leq -\omega  \|\nabla F(\bm W^k)\|_F^2.
%     \]
%     \item[(ii)] The sequence converges to a critical point $\bm{W}^*$.
% \end{itemize}
% Then, the sequence $\{F(\bm{W}^k)\}_{k \ge k_1}$ converges Q-linearly to $F(\bm{W}^*)$. Furthermore, both $\{\text{dist}(\bm{W}^k, \mathcal{W})\}$ and $\{\|\nabla F(\bm{W}^k)\|_F\}$ converge linearly to zero.
% \end{prop}
% \begin{proof}
%     According to \Cref{coro:PL QG}, \Cref{thm:eb}, item (i) and item (ii) of this proposition, without loss generality, we can assume that ${\bm W^k}_{k \ge k_1}$ satisfies the $\eqref{eq:eb F}$ and $\eqref{eq:PL}$.
%     Then we have
%     \[
%      F(\bmw^{k+1}) - F(\bmw^k) \le -\omega \|\nabla F(\bm W^k)\|_F^2\le -\mu_1 \omega(F(\bm W^k) - F(\bmw^*)).
%     \]
%     Since $\{F(\bmw^k)\}$ is a decreasing sequence, we have
%     \[
%         F(\bmw^{k+1}) - F(\bm W^*)\le (1 - \omega \mu_1)(F(\bmw^k)- F(\bm W^*)).
%     \]
%     This show that the sequence $\{F(\bm{W}^k)\}_{k \ge k_1}$ converges Q-linearly to $F(\bm{W}^*)$.
%     Using this, we have
%     \[
%         \mathrm{dist}(\bmw^k, \mathcal{W}) \le \kappa_1 \|\nabla F(\bmw^k)\|_F^2 \le \frac{\kappa_1}{\omega}( F(\bm{W}^{k+1}) - F(\bm{W}^k))\le \frac{\kappa_1}{\omega}( F(\bm{W}^{k+1}) - F(\bm{W}^*)).
%     \]
%     This show that $\{\text{dist}(\bm{W}^k, \mathcal{W})\}$ and $\{\|\nabla F(\bm{W}^k)\|_F\}$ converge linearly to zero.
% \end{proof}
% }


% \begin{proof}
%     We say that the error bound holds if there exists a constant \(\kappa > 0\) such that, in some neighborhood of \(\mathcal{W}\),
% \[
%     \mathrm{dist}(\bmw, \mathcal{W}) \leq \kappa \|\nabla F(\bmw)\|_F.
% \]
% \edit{Using suppose (i)}, we derive the following inequality:
% \[
%     1 + \kappa_1 \frac{\|\bmw^{k+1} - \bmw^k\|_F^2}{F(\bmw^{k+1}) - F^*} \leq \frac{F(\bmw^k) - F^*}{F(\bmw^{k+1}) - F^*}.
% \]
% In the following analysis, we show that:
% \[
%     F(\bmw^{k+1}) - F^* \leq \kappa_2 \left( \mathrm{dist}^2(\bmw^k, \mathcal{W}) + \|\bmw^{k+1} - \bmw^k\|_F^2 \right) \leq \kappa_2 (\kappa^2 \kappa_3^2 + 1) \|\bmw^{k+1} - \bmw^k\|_F^2.
% \]
% \edit{
% The first inequality follows from item (ii) of Proposition~\ref{prop:threecondition}(ii), while the second inequality follows from the error bound and item (iii) of Proposition~\ref{prop:threecondition}}.
% It implies that 
% \[
%     F(\bmw^k)-F^*\geq (F(\bmw^{k+1})-F^*)(1+\frac{\kappa_1}{\kappa_2 (\kappa^2 \kappa_3^2 + 1)})=\frac{F(\bmw^{k+1})-F^*}{\omega^2},
% \]
% where 
% \[
%     \omega = \left(1+\frac{\kappa_1}{\kappa_2 (\kappa^2 \kappa_3^2 + 1)}\right)^{-1/2}.
% \]
% Thus $\{ F(\bmw^k)\}_{k\geq 0}$ converges Q-linearly to $F^*$. Then we have
% \[
%     \kappa_1 \|\bmw^{k+1} - \bmw^k\|_F^2 \leq F(\bmw^{k+1}) - F(\bmw^k) \leq F(\bmw^{k+1}) - F^*
%     \leq \omega^{2k-2 k_1}(F(\bmw^{k_1}) - F^*).
% \]
% Thus, we obtain:
% \[
%     \|\bmw^{k+1} - \bmw^k\|_F \leq \left( \frac{F(\bmw^{k_1}) - F^*}{\kappa_1} \right)^{\frac{1}{2}} \omega^{k-k_1}.
% \]
% This shows that the sequence \(\{\bmw^k\}\) is a Cauchy sequence, and we will now demonstrate that it converges to an optimal point \(\bmw^*\). We have:
% \[
%     \|\bmw^k - \bmw^m\|_F \leq \left( \frac{F(\bmw^{k_1}) - F^*}{\kappa_1} \right)^{\frac{1}{2}} \sum_{i=k}^{m-1} \omega^{i-k_1}.
% \]
% Letting \(m \to \infty\), we obtain:
% \[
%     \|\bmw^k - \bmw^*\|_F \leq \left( \frac{F(\bmw^{k_1}) - F^*}{\kappa_1} \right)^{\frac{1}{2}} \frac{\omega^{k-k_1}}{1 - \omega}.
% \]
% Thus, we conclude that the sequence \(\{\bmw^k\}_{k \geq 0}\) converges R-linearly to some \(\bmw^* \in \mathcal{W}\).
% \end{proof}
%\begin{proof}
%Let $\bar{\alpha} = \sup_k \alpha^k$ and $\underline{\alpha} = \inf_k \alpha^k$. We now prove each part.
%
%\begin{itemize}
%    \item[(i)] Since $\nabla F$ is locally Lipschitz continuous (by Lemma~\ref{lem:locallip}), we have
%    \[
%        |F(\bmw^{k+1}) - F(\bmw^k) - \langle \nabla F(\bmw^k), \bmw^{k+1} - \bmw^k \rangle| \leq \frac{L_1}{2} \|\bmw^{k+1} - \bmw^k\|_F^2.
%    \]
%    This implies that
%    \[
%        F(\bmw^k) - F(\bmw^{k+1}) \geq \left(\frac{1}{\alpha^k} - \frac{L_1}{2}\right) \|\bmw^{k+1} - \bmw^k\|_F^2 \geq \left(\frac{1}{\bar{\alpha}} - \frac{L_1}{2}\right) \|\bmw^{k+1} - \bmw^k\|_F^2.
%    \]
%    
%    \item[(ii)] $\mathrm{dist}(\bmw^k, \mathcal{W}) = \|\bmw^k - \bmw^*\|_F$. Then, we have
%    \[
%        F(\bmw^{k+1}) - F^* \leq \frac{L_1}{2} \|\bmw^{k+1} - \bmw^*\|_F^2 \leq L_1\left( \|\bmw^k - \bmw^*\|_F^2 + \|\bmw^{k+1} - \bmw^k\|_F^2 \right).
%    \]
%
%    \item[(iii)]  This follows directly from the gradient descent rule:
%    \[
%        \|\bmw^{k+1} - \bmw^k\|_F \geq \alpha_k \|\nabla F(\bmw^k)\|_F \geq \underline{\alpha} \|\nabla F(\bmw^k)\|_F.
%    \]
%\end{itemize}
%\end{proof}

%\begin{thm}[Linear Convergence]
%Under the condition mentioned holds, Then, the sequence $\{ F(\bmw^k)\}_{k\geq 0}$ converges Q-linearly to $F^*$ and $\{\bmw^k\}_{k\geq 0}$ converges R-linearly to some $\bmw^* \in \mathcal{W}$.
%\end{thm}

% \begin{thm}[Local Polyak-Łojasiewicz Inequality]
% \label{thm:local_PL}
% Suppose that \( \bm{W}^* =(\bm W_1,\dots, \bm W_L) \) is a critical point of \( G \) (i.e., \( \nabla F(\bm{W}^*) = \bm{0} \)). Then, there exists a neighborhood \( \mathcal{U} \) around \( \bm{W}^* \) and a constant \( \mu > 0 \) such that for all \( \bm{W} \in \mathcal{U} \), the following inequality holds:
% \[
%  \| \nabla F(\bm{W}) \|_2^2 \geq \mu \left( F(\bm{W}) - F(\bm{W}^*) \right).
% \]
% \end{thm}
% \begin{proof}
% We select a neighborhood \( \mathcal{U} \) in which both \Cref{thm:eb} and \Cref{lem:locallip} are satisfied. For any \( \bm{W} \in \mathcal{U} \), we assume that the distance to the set \( \mathcal{W} \) is given by
% \[
%     \mathrm{dist}(\bm{W}, \mathcal{W}) = \| \bm{W} - \bm{W}^* \|_F.
% \]
% Since \Cref{lem:locallip} and equation~\eqref{eq:eb F} hold, it follows that
% \[
%     F(\bm{W}) - F(\bm{W}^*) \leq \frac{L_1}{2} \| \bm{W} - \bm{W}^* \|_F^2 \leq \frac{\kappa_1 L_1}{2} \| \nabla F(\bm{W}) \|_F^2.
% \]
% Thus, the desired inequality is established. 
% \end{proof}

\section{Auxiliary Results}\label{app:sec D}

To bound the spectral gap between eigenvectors associated with repeated eigenvalues between two symmetric matrices, we introduce the Davis-Kahan theorem \cite{yu2015useful}. 

\begin{lemma}\label{lem:daviskahan}
 Let $\bm A, \hat{\bm A} \in \mathbb{R}^{p \times p}$ be symmetric with eigenvalues $\lambda_1 \geq \cdots \geq \lambda_p$ and $\hat{\lambda}_1 \geq \cdots \geq \hat{\lambda}_p$ respectively. Fix $1 \leq r \leq s \leq p$ and assume that $\min\{\lambda_{r-1} - \lambda_r, \lambda_s - \lambda_{s+1}\} > 0$, where $\lambda_0 := \infty$ and $\lambda_{p+1} := -\infty$. Let $d := s - r + 1$ and $\bm V = (\bm v_r, \bm v_{r+1}, \dots, \bm v_s) \in \mathbb{R}^{p \times d}$ and $\hat{\bm V} = (\hat{\bm v}_r, \hat{\bm v}_{r+1}, \dots, \hat{\bm v}_s) \in \mathbb{R}^{p \times d}$ have orthonormal columns satisfying $\bm A \bm v_j = \lambda_j \bm v_j$ and $\hat{\bm A} \hat{\bm v}_j = \hat{\lambda}_j \hat{\bm v}_j$ for all $j = r, r+1, \ldots, s$. Then, there exists an orthogonal matrix $\bm O \in \mathcal{O}^{d}$ such that
\[
\|\hat{\bm V} \bm O - \bm V\|_F \leq \frac{4 \|\hat{\bm A} - \bm A\|_F}{\min\{\lambda_{r-1} - \lambda_r, \lambda_s - \lambda_{s+1}\}}.
\]
\end{lemma} 
\begin{lemma}[Mirsky Inequality \cite{stewart1990matrix}]\label{lem:mirsky}
For any matrices $\bm X, \tilde{\bm X} \in \mathbb{R}^{m \times n}$ with singular values 
$$
\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{l} \quad \text{and} \quad \tilde{\sigma}_1 \geq \tilde{\sigma}_2 \geq \cdots \geq \tilde{\sigma}_{l},
$$
where $l=\min\{m,n\}$,
then for any unitarily invariant norm (e.g., $\|\cdot\|_F$), we have
\begin{align*}
\|\mathrm{diag}(\tilde{\sigma}_1 - \sigma_1,\dots,\tilde{\sigma}_{l} - \sigma_{l})\| \leq \|\tilde{\bm X} - \bm X\|.
\end{align*}
\end{lemma} 

 \begin{lemma}\label{lem:helplem}
    Suppose that $(\bm{\sigma}, \bm{\Pi}) \in \mathbb{R}^{d_{\min}} \times \mathcal{P}^{d_{\min}}$ satisfies the following conditions:
    \begin{align}
    \label{eq:decreasinglem12}
        \begin{cases}
            \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{d_{\min}} \geq 0, \\
            \sigma_i^{2L - 1} - \sqrt{\lambda}\, y_{\pi(i)}\, \sigma_i^{L - 1} + \lambda\, \sigma_i = 0,\ \forall i \in [d_{\min}],
        \end{cases}
    \end{align}
    where $\pi: [d_{\min}] \to [d_{\min}]$ is the permutation corresponding to $\bm{\Pi}$. Then, the pair $(\bm{\sigma}, \bm{\Pi})$ belongs to the set $\mathcal{B}$.
\end{lemma}
\begin{proof}
    According to \eqref{eq:decreasinglem12}, we conclude
    \begin{align*} 
        \sigma_{\pi^{-1}(i)}^{2L - 1} - \sqrt{\lambda}\, y_{i}\, \sigma_{\pi^{-1}(i)}^{L - 1} + \lambda\, \sigma_{\pi^{-1}(i)} = 0,\ \sigma_{\pi^{-1}(i)} \geq 0,\ \forall i \in [d_{\min}]. 
    \end{align*}
    Combining this with the definition of $\mathcal{A}$ in \eqref{set:A}, we have $  \bm{a} := \bm{\Pi}^T  \bm{\sigma} = \left(\sigma_{\pi^{-1}(1)},\ \cdots,\ \sigma_{\pi^{-1}(d_{\min})}\right)^T \in \mathcal{A}$. Since $\bm{\Pi} \bm{a} = \bm{\sigma}$ and by the definition of $\mathcal{B}$ in \eqref{set:B}, it follows that the pair $(\bm{\sigma}, \bm{\Pi}) \in \mathcal{B}$.
\end{proof}

 
\subsection{Proof of \eqref{eq0:prop_sing}}
\label{sec:comlementary1}
To simplify the notation, for each $ i \in [p] $, we introduce the following definition:
$ \hat{\bm T}_i := \prod_{l=2}^L \bm{T}_l^{(i)} $. Then we have
\begin{align}
   & \quad\left\| \left( \hat{\bm \Sigma}_1 \hat{\bm \Sigma}_1^T \right)^{L-1} \hat{\bm \Sigma}_1 
        + \lambda \hat{\bm \Sigma}_1 
        - \sqrt{\lambda}\, \mathrm{BlkDiag}\left( (\bm \Sigma_{1}^{(1)})^{L-1}, \dots, (\bm \Sigma_{1}^{(p)})^{L-1}, \bm 0 \right) \bm \Psi \hat{\bm T} \right\|_F \notag \\
    &= \left\| 
        \mathrm{BlkDiag}\left( \hat{\bm T}_1, \dots, \hat{\bm T}_p, \bm I_{d_1 - r_{\sigma}} \right) 
        \left( \left( \hat{\bm \Sigma}_1 \hat{\bm \Sigma}_1^T \right)^{L-1} \hat{\bm \Sigma}_1 
            + \lambda \hat{\bm \Sigma}_1 \right) \hat{\bm T}^T 
        \right. \notag \\
    &\quad \left. 
        - \sqrt{\lambda}\, \mathrm{BlkDiag}\left( \bm A_1, \dots, \bm A_p, \bm 0 \right) \bm \Psi 
    \right\|_F \notag \\
    &\leq \left\| 
        \left( \hat{\bm \Sigma}_1 \hat{\bm \Sigma}_1^T \right)^{L-1} \hat{\bm \Sigma}_1 
        + \lambda \hat{\bm \Sigma}_1 
        - \sqrt{\lambda}\, \mathrm{BlkDiag}\left( \bm A_1, \dots, \bm A_p, \bm 0 \right) \bm \Psi 
    \right\|_F \notag \\
    &\quad + \sum_{i=1}^p \left\| \hat{\bm T}_i (\bm \Sigma_{1}^{(i)})^{2L-1} \hat{\bm T}_i^T 
        - (\bm \Sigma_{1}^{(i)})^{2L-1} \right\|_F 
     +\lambda \sum_{i=1}^p \left\| \hat{\bm T}_i \bm \Sigma_{1}^{(i)} \hat{\bm T}_i^T 
        - \bm \Sigma_{1}^{(i)} \right\|_F \notag \\
    &\leq \eta_3 \left\| \nabla G(\bm W) \right\|_F 
        + \sum_{i=1}^p \left\| \hat{\bm T}_i (\bm \Sigma_{1}^{(i)})^{2L-1} 
            - (\bm \Sigma_{1}^{(i)})^{2L-1} \hat{\bm T}_i \right\|_F + \lambda\sum_{i=1}^p \left\| \hat{\bm T}_i \bm \Sigma_{1}^{(i)} 
        - \bm \Sigma_{1}^{(i)}\hat{\bm T}_i \right\|_F \notag \\
    &\leq \left( \eta_3 
        + \frac{p \eta_1 (2L - 1)L}{\sigma^*_{\min}} 
            \left( \frac{3 \sigma^*_{\max}}{2} \right)^{2L - 2}  
        + \frac{\lambda p \eta_1 L}{\sigma^*_{\min}} 
    \right) \left\| \nabla G(\bm W) \right\|_F, 
    \notag
\end{align}
where the first equality follows from the definition of $\bm  A_i$ for each $i \in [p]$ in \eqref{eq:A}; the second inequality follows from \eqref{eq4:prop sing}; and the last inequality follows from \eqref{eq4:lem two} and the following fact(see  \eqref{eq:complementary1} and \eqref{eq:complementary2}). 
For each i in $[p]$, we have:
\begin{equation}
\begin{aligned}
& \quad \left\| \hat{\bm T}_i (\bm \Sigma_{1}^{(i)})^{2L-1} 
- (\bm \Sigma_{1}^{(i)})^{2L-1} \hat{\bm T}_i \right\|_F  = \left\|\left(\prod_{l=2}^L\bm T_{l}^{(i)}\right)(\bm \Sigma_{1}^{(i)})^{2L-1} 
- (\bm \Sigma_{1}^{(i)})^{2L-1}\prod_{l=2}^L\bm T_{l}^{(i)} \right\|_F\\
&\le \left\|\left(\prod_{l=2}^{L-1}\bm T_{l}^{(i)}\right) \left(\bm T_{L}^{(i)}(\bm\Sigma_{1}^{(i)})^{2L-1}-(\bm \Sigma_{1}^{(i)})^{2L-1}\bm T_{L}^{(i)}\right)\right\|_F + \\
& \quad \left\|\left(\prod_{l=2}^{L-2}\bm T_{l}^{(i)}\right) \left(\bm T_{L-1}^{(i)}(\bm\Sigma_{1}^{(i)})^{2L-1}-(\bm \Sigma_{1}^{(i)})^{2L-1}\bm T_{L-1}^{(i)} \right) \bm T_{L}^{(i)}\right\|_F + \cdots  \\
& \quad +\left\| \left(\bm T_{2}^{(i)}(\bm\Sigma_{1}^{(i)})^{2L-1}-(\bm \Sigma_{1}^{(i)})^{2L-1}\bm T_{2}^{(i)}\right)\left(\prod_{l=3}^{L}\bm T_{l}^{(i)}\right)\right\|_F\\
&\le \sum_{l=2}^L\sum_{j=1}^{2L-1} \left\|(\bm\Sigma_{1}^{(i)})^{j-1}\left(\bm T_{l}^{(i)}\bm\Sigma_{1}^{(i)}-\bm \Sigma_{1}^{(i)}\bm T_{l}^{(i)}\right)(\bm\Sigma_{1}^{(i)})^{2L-j-1} \right\|_F \\
& \le  \frac{ \eta_1 (2L - 1)L}{\sigma^*_{\min}} 
            \left( \frac{3 \sigma^*_{\max}}{2} \right)^{2L - 2}\|\nabla G(\bm W)\|_F,
\end{aligned}\label{eq:complementary1}
\end{equation}
where the last inequality follows from \eqref{eq4:lem two} and \eqref{eq:S norm W}. Similarly, we have
\begin{equation}
\left\| \hat{\bm T}_i \bm \Sigma_{1}^{(i)} 
- \bm \Sigma_{1}^{(i)}\hat{\bm T}_i \right\|_F \le \frac{ \eta_1 L}{\sigma^*_{\min}} 
            \left( \frac{3 \sigma^*_{\max}}{2} \right)^{2L - 2}\|\nabla G(\bm W)\|_F.
            \label{eq:complementary2}
\end{equation}
\end{appendix}

\end{document}
