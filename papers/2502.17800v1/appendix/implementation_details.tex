In this section, we provide additional implementation details about the experiments.

\subsection{Simplified Attention}
\label{subsection: appendix-simplified_attention}
% \subsubsection{Simplified Attention}
The Simplified Attention~(\ref{equ:simplified_attention}) was proposed in \texttt{MechanisticProbe}~\citep{hou2023towards}. To make this paper self-contained, we introduce some necessary details here.

Given a causal language model (LM) with $L$ layers and $H$ attention heads, the attention matrix \( A \) is represented as \( A = \{A(l, h) \mid 1 \leq l \leq L, 1 \leq h \leq H\} \), where \( A(l, h) \in \mathbb{R}^{|T| \times |T|} \). To reduce the size and complexity of \( A \) while retaining relevant information for reasoning analysis, we simplify it into \( A_{\text{simp}} \) using the following steps:

\textbf{Focus on the Last Token:} For causal LMs, attention values directed at the last input token are retained, reducing the size of \( A \) to \( A_{\text{simp}} \in \mathbb{R}^{L \times H \times |T|} \). This reduction focuses on information most relevant to the final prediction.

\textbf{Attention Head Pooling:} We apply mean pooling across all attention heads to reduce dimensionality further, resulting in \( A_{\text{simp}} \in \mathbb{R}^{L \times |T|} \).

\textbf{Hypernode Simplification (Cross-Statement Attention):} For tasks involving multi-token statements, each statement is treated as a hypernode. We apply mean pooling across tokens within each statement and max pooling across question tokens, yielding \( A_{\text{cross\_simp}} \in \mathbb{R}^{L \times (|S| + 1)} \), where \( |S| \) represents the number of statements in the query.

These simplification steps ensure that \( A_{\text{simp}} \) preserves the key information needed for probing reasoning behavior while significantly reducing computational overhead and noise. Full details are in their paper~\citep{hou2023towards}.

% \subsection{Probing Experiment Details}


\subsection{Experiment Setup Details}
\label{subsection: appendix-experiment-setup}
The experiments are mainly conducted on the~\texttt{PromptBench} benchmark~\citep{zhu2023dyval}. To make the paper self-contained, we explain the data generation process for queries and responses in the experiments. Full details are provided in the documentation of \texttt{PromptBench}.

% \ZC{we don't need to write too many details of data generation; actually we need a specific example of a concrete QA pair. you need to attach an example including 1) DAG and 2) Query+answer generated for this DAG for both logic and arithmetic problem. So people will understand what exact type of reasoning problem you are solving}

\subsubsection{Query data generation}
The \texttt{promptbench} generates the data in two stages: (1) DAG construction; and (2) Natural Language Description of the DAG. An illustration of the DAG is shown in FIgure~\ref{fig: DAG illustration}.

\begin{figure}[h]
    \centering
    % \vspace{-8pt} 
    \includegraphics[width=1\linewidth]{figure/appendix_fig/Illustration_DAG.pdf}
    % \vspace{-1mm}
    \caption{Illustration of the DAG structure: An example DAG with depth=$3$ and the number of redundant dependency to be $2$.}
    \label{fig: DAG illustration}
    % \vspace{-2mm}
\end{figure}

\textbf{DAG construction:} We generate the DAG with a specified depth and number of redundancy. Based on these, we generate the DAG from top to bottom: Firstly generate the root node, and then sample the dependency between itself and its parent node(s). If the dependency contains a $2$-variable operator (e.g. \{$+$, $-$, $\land$ (\texttt{AND})\}), then it has two parent nodes to generate, otherwise, if it contains a $1$-variable operator (e.g. \{$\square^2$, $\neg$ (\texttt{NOT})\}), it only has one parent node. After the generation of the root node, we then go to the parent generation of its parent node(s), and do this recursively until reach the expected depth. Along with the DAG generation, we also obtain the names of all the generated nodes from a random string generator.
After obtaining the DAG, we sample the values for all the leaf nodes from the pre-defined set, and calculate the value of their child nodes from bottom to top.


\textbf{Natural Language Description}
After obtaining the DAG and all the node information, we use natural language to describe the question with pre-defined templates. If it is the leaf node, we describe its name and value as: 
\begin{tcolorbox}[colback=blue!2!white, colframe=blue!80!black, boxrule=0.5pt]
\small
\textit{"The value of} \texttt{node.name} \textit{is} \texttt{node.value}\textit{."}
\end{tcolorbox}

If it is an intermediate node or root node, we describe the dependency with its parent nodes as:

\begin{tcolorbox}[colback=blue!2!white, colframe=blue!80!black, boxrule=0.5pt]
\small
\textit{"}\texttt{node.name} \textit{gets its value by}  \texttt{template}-\texttt{func}(\texttt{node.operator}, \texttt{node.parent})\textit{."}
\end{tcolorbox}
where \texttt{template}-\texttt{func} is a template function to generate strings based on the operator. For example, if the operator is ``$-$'', then the generated string from \texttt{template}-\texttt{func} is:

\begin{tcolorbox}[colback=blue!2!white, colframe=blue!80!black, boxrule=0.5pt]
\small
\textit{"subtracting the value of} \texttt{node.parent1.name} from the value of \texttt{node.parent2.name}\textit{."}
\end{tcolorbox}

If all the information is described in the \texttt{topological} sorting order, then it forms the \texttt{topological} order dataset. If the sentences are shuffled, it forms the \texttt{random} dataset. If the sentences are reversed permutated, then it forms the \texttt{reversed} dataset. From the templates above, we can observe that permutating the sentence order in the question prompts does not change the overall semantic meaning of this query.

For the redundancy generation, we describe the redundant nodes and their corresponding dependency based on the process and templates describe above. Since the redundant information is not related to the root node, and their names are not overlapped, describing the redundant information also does not change the semantic meaning of the useful information.

At the end of the problem description, we add one sentence to conclude the question:
\begin{tcolorbox}[colback=blue!2!white, colframe=blue!80!black, boxrule=0.5pt]
\small
\textit{"What is the value of} \texttt{root.name}\textit{?"}
\end{tcolorbox}

\label{subsection: example-qa-pairs}
We provide example QA pairs for the arithmetic reasoning and logic reasoning tasks and their corresponding DAG structure in this part.
\begin{figure}[t]
    \centering
    % \vspace{-8pt} 
    \includegraphics[width=1\linewidth]{figure/appendix_fig/DAG_example_2.pdf}
    % \vspace{-1mm}
    \caption{DAG Example in the arithmetic reasoning task and the logical reasoning task.}
    \label{fig: DAG_math_example}
    % \vspace{-2mm}
\end{figure}

\subsubsection{Response generation}
In addition to the query mentioned above, SFT data also contains the description of the reasoning chain. As mentioned in section~\ref{section: preliminary}, the ground-truth reasoning chain is a topological sorting of this DAG. Similar to the process in query generation, the reasoning chain generation also utilizes some templates. The difference is that, it contains the answer after every intermediate and final reasoning step. For example:

\begin{tcolorbox}[colback=blue!2!white, colframe=blue!80!black, boxrule=0.5pt]
\small
\textit{"}\texttt{node.name} \textit{gets its value by}  \texttt{template}-\texttt{func}(\texttt{node.operator}, \texttt{node.parent})\textit{, so the value of} \texttt{node.name} \textit{is} \texttt{node.value} \textit{."}
\end{tcolorbox}

\subsection{Example QA Pairs}
\label{subsection:appendix-example-QA-pairs}

For the DAG example shown in Figure~\ref{fig: DAG_math_example}, the corresponding QA pairs are shown in the following text blocks:



% \newpage
% \clearpage
\textbf{Arithmetic reasoning:}
% \TODO{make the font in all colorbox smaller}

\begin{tcolorbox}[colback=yellow!2!white, colframe=black, boxrule=0.5pt]
\small
\textit{Question: \\ "The value of aag is 8.\\ The value of aah is 2.\\ aai gets its value by subtracting the value of aah from the value of aag.\\ aav gets its value by multiplying together the value of aat and aau.\\ The value of aaj is 6.\\ The value of aat is 9.\\ The value of aak is 8.\\ aan gets its value by multiplying together the value of aaj and aak.\\ aao gets its value by subtracting the value of aan from the value of aai.\\ The value of aau is 1.\\ The value of aab is 7.\\ The value of aad is 7.\\ The value of aaa is 1.\\ aac gets its value by subtracting the value of aaa from the value of aab.\\ aae gets its value by squaring the value that aad has.\\ aaf gets its value by multiplying together the value of aac and aae.\\ aap gets its value by multiplying together the value of aaf and aao.\\ What is the value of aap?"}
\end{tcolorbox}

\begin{tcolorbox}[colback=yellow!2!white, colframe=black, boxrule=0.5pt]
\small
\textit{Response: \\ "aag is 8.0\\ aah is 2.0\\ aai = aag - aah = 8.0 - 2.0 = 6.0\\ aaj is 6.0\\ aak is 8.0\\ aan = aaj * aak = 6.0 * 8.0 = 48.0\\ aao = aai - aan = 6.0 - 48.0 = -42.0\\ aab is 7.0\\ aad is 7.0\\ aaa is 1.0\\ aac = aab - aaa = 7.0 - 1.0 = -6.0\\ aae = $aad^2$ = $(7.0)^2$ = 49.0\\ aaf = aac $*$ aae = -6.0 $*$ 49.0 = -294.0\\ aap = aao $*$ aaf = -42.0 $*$ -294.0 = 12348.0\\ Thus, the answer is 12348.0"}
\end{tcolorbox}
% \clearpage
% \newpage

\textbf{Logical reasoning:}

\begin{tcolorbox}[colback=yellow!2!white, colframe=black, boxrule=0.5pt]
\small
\textit{Question: \\ "The value 1 means True, and the value 0 means False.\\ aak is 1.\\ aai is 1.\\ aah is 1.\\ The value of aaj equals to (aai AND aah).\\ aan is 0.\\ aax is 1.\\ The value of aao equals to (aak OR aan).\\ The value of aap equals to (aaj OR aao).\\ aab is 1.\\ aaa is 0.\\ The value of aac equals to (aab OR aaa).\\ aad is 1.\\ aae is 1.\\ The value of aaz equals to (aax AND aay).\\ The value of aaf equals to (aad AND aae).\\ The value of aag equals to (aaf AND aac).\\ The value of aaq equals to (aap AND aag).\\ aay is 1.\\ What is the value of aaq?"}
\end{tcolorbox}

\begin{tcolorbox}[colback=yellow!2!white, colframe=black, boxrule=0.5pt]
\small
\textit{Response: \\ "aak is 1.\\ aan is 0.\\ aao = (aak OR aan) = (1 OR 0) = 1.\\ aah is 1.\\ aai is 1.\\ aaj = (aah AND aai) = (1 AND 1) = 1.\\ aap = (aao OR aaj) = (1 OR 1) = 1.\\ aab is 1.\\ aaa is 0.\\ aac = (aab OR aaa) = (1 OR 0) = 1.\\ aad is 1.\\ aae is 1.\\ aaf = (aad AND aae) = (1 AND 1) = 1.\\ aag = (aaf AND aac) = (1 AND 1) = 1.\\ aaq = (aag AND aap) = (1 AND 1) = 1.\\ Thus, the answer is 1"}
\end{tcolorbox}

% \newpage

\subsection{Baseline Implementation Details}
\begin{itemize}
    \item \texttt{SCoP}-$k$. First, it paraphrases the question prompts \( k \) times, then performs reasoning based on these paraphrases, and obtains the final answer through majority voting. Unlike the closed-source LLMs used in~\cite{zhou2024paraphrase}, our LLMs after SFT lose some of their paraphrasing capability. Additionally, in our experiment, sentence order permutation does not alter the overall semantic meaning, which is a good way for paraphrase. Therefore, we permute the sentences to achieve paraphrasing, resulting in better final performance.

    \item \texttt{RC-Aug} utilizes the reasoning chain (RC) augmented dataset for SFT, as used in previous work~\citep{yu2023metamath}. Specifically, we augment the answers with different topological orderings while keeping the query unchanged. For a DAG with more than two levels, the topological ordering is not unique. For example, in the DAG shown in Figure~\ref{fig: DAG illustration}, the topological ordering can start by solving either node B first or node C first. When solving node B, it can begin with either node D or node E. Consequently, a fixed DAG can have multiple valid topological orderings, resulting in different reasoning chains that lead to the same correct answer. In \texttt{RC-Aug}, we incorporate these additional reasoning chains from different topological orderings to augment the SFT dataset.

    \item \texttt{MEND-RC} is an ablation variant that applies \method\ to transform the original question queries without introducing new queries into the dataset, while augmenting the dataset with additional reasoning paths.
\end{itemize}

All the methods include the baselines and \method\ utilize full-parameter fine-tuning. We report the accuracy as Pass@1 if not indicated otherwise.

\subsection{Computing Resources.}
% \TODO{the total computational budget (e.g., GPU hours), and computing infrastructure used}
The experiments are run on a server with 2$\times$AMD EPYC 7542 32-Core Processor CPU, 2$\times$NVIDIA RTX A6000 graphics, and $252$ GB memory. For the arithmetic reasoning tasks with 3B model, it takes about $20$ GPU hours for SFT and evaluation. For Logical reasoning tasks with 1B model, it takes about $4$ GPU hours for SFT and evaluation.

% For one single experiment, \method\ takes about $4$ hours with $200,000$ steps to train the data generator. It takes about $1.5$ hours to train a \texttt{BCQ-Lag} agent on this generated dataset for $200,000$ steps.
