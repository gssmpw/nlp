In this work, we addressed the problem of reasoning equivalence in LLMs with respect to diverse surface form variations corresponding to the same semantic meaning. We proposed \methodlong, enhancing reasoning equivalence by applying structured query-level transformations. Our method improves reasoning robustness at the knowledge extraction stage by enforcing the query symmetry in the SFT dataset.
Experiments demonstrate that \method\ achieves superior sampling efficiency and generalizability in reasoning tasks. Although one potential risk is that the misuse of this work in real-world scenarios can cause unexpected damage, we hope our findings can provide a foundation for future research on improving LLM reasoning consistency through structured dataset curation. 
% through data-centric approaches.
