

\label{section: method}
% \TODO{adjust the statement, "surface" is never well explained before. either define it above, or avoid using it until definition} 
% As shown in Figure~\ref{fig: motivation}, LLMs face the challenges of reasoning consistency with different surface form variations.
In this section, we first analyze the surface form and its transformations and reveal the symmetry in description queries. We then discuss LLMs' capability for reasoning consistency. Lastly, we introduce the data augmentation method to improve reasoning consistency and a probing tool for capability verification.



\subsection{Surface Form and Description Symmetry}
A reasoning problem consists of both a semantic meaning and a surface form~\citep{zhou2024paraphrase}. The semantic meaning is determined by the structure of the DAG and the values of its leaf nodes, capturing the core reasoning chain of the problem~\citep{pearl2009causality, velickovic2018geometric}. 
The surface form is the natural language description of the semantic meaning~\citep{zhou2024paraphrase}.
There exists a mapping \( f: \mathcal{Q} \to \mathcal{S} \) from the query space \( \mathcal{Q} \) to the semantic space \( \mathcal{S} \), which translates a natural language description into its underlying structured representation. This abstracts away linguistic or surface-level variations to focus on the task's core logical or mathematical structure~\cite{russell2021ai}. 
% \TODO{so what is surface, explanation missing}

\begin{figure}[t]
    \centering
    % \vspace{-8pt} 
    \includegraphics[width=\linewidth]{figure/illustration/Overview_MEND.pdf}
    % \vspace{-1mm}
    \caption{Overview of Symmetry-Enhanced Data Augmentation and its Comparison with Reasoning Chain Data Augmentation.}
    \label{fig: illustration}
    \vspace{-4mm}
\end{figure}

% \subsubsection{Description Symmetry}
The symmetry relationship \( \sim \) on \( \mathcal{Q} \) is defined as:
\begin{equation}
    q_1 \sim q_2 \iff f(q_1) = f(q_2),
\end{equation}
where \( q_1, q_2 \in \mathcal{Q} \). That is, two query descriptions are symmetric if they correspond to the same semantics in \( \mathcal{S} \). 
We categorize symmetry relations in \( \mathcal{Q} \) into three types: \textbf{(1) Permutation}: reordering the provided information, \textbf{(2) Redundancy addition}: introducing irrelevant information as a distraction, and \textbf{(3) Surface-level variations}: paraphrasing questions at the linguistic level. Given the complexity of linguistic paraphrasing, this work primarily focuses on the first two types, leaving the third for future research.

\subsection{LLM Reasoning Consistency}
\label{subsection: LLM Reasoning Equivalence}
Given a query $q$, the reasoning process to find its ground-truth final answer $y$ of an auto-regressive LLM can be formulated as a conditioned generation process~\citep{xiang2025towards}:
\begin{equation}
\label{eq:math_format reasoning}
    \mathcal{P}(y | q) = \int_z \prod_{t=1}^T \mathcal{P}(y_t | y_{<t}, z, q) \mathcal{P}(z | q) dz,
\end{equation}
where \( \mathcal{P}(y | q) \) means the probability of generating right final answer and \( \mathcal{P}(z | q) \) represents \textbf{in-context knowledge extraction}, which extracts relevant knowledge \( z \) from \( q \). \( \mathcal{P}(y_t | y_{<t}, z, q) \) represents the conditioned generation to the final results step by step. 

We define the \textbf{reasoning consistency} as the ability of an LLM to maintain similar reasoning performance despite variations in the phrasing or description of a query $q$, as long as its underlying semantics remains unchanged, i.e.,
\begin{equation}
\label{eq: reasoning equivalence}
    \mathcal{P}(y \mid q_1) \approx \mathcal{P}(y \mid q_2), \quad q_1 \sim q_2
\end{equation}
% where \( \approx \) denotes equivalence up to minor variations. This equivalence requires that reasoning outcomes remain consistent despite transformations. 
Equation~\ref{eq:math_format reasoning} demonstrates that extracting high-quality representations $z$ from queries is crucial for ensuring reasoning consistency. In this work, we focus on \textit{invariant knowledge extraction}: improving the LLMs' capability to consistently extract useful $z$ despite the surface form changes.
% \begin{equation}
%     \mathcal{P}(z \mid q) \approx \mathcal{P}(z \mid q') \label{eq:Extraction-equivalence}
% \end{equation}
% Alternatively, it can also be achieved by \textit{invariant generation}: based on different retrieved information embedding, the LLM has the ability to reach the same conclusion through reasoning chains:
% \begin{equation}
%     \prod_{t=1}^T \mathcal{P}(y_t | y_{<t}, z, q) \approx \prod_{t=1}^T \mathcal{P}(y_t | y_{<t}, z, q') \label{eq:Manipulation-equivalence}
% \end{equation}
\subsection{Symmetry-Enhanced Data Augmentation}
To improve representation extraction capability, we can adopt either \textit{model-centric} approaches, which focus on modifying model architectures, or \textit{data-centric} approaches, which aim to improve data quality. Given the complexity of modifying the architecture of pre-trained LLMs without compromising overall performance, we focus on a data-centric approach that encodes symmetry information in the post-training dataset and is compatible with most general LLM architectures.
% As analyzed above, question queries with different descriptions exhibit symmetry if they share the same underlying semantic meaning. To improve the invariant knowledge extraction ability and reasoning consistency, 
We propose \methodlong as shown in Figure~\ref{fig: illustration}. Compared to previous methods that enhance the training chains in the training dataset~\citep{yu2023metamath}, we focus on query augmentation to impose a better understanding of symmetric structured information. Our method contains two parts:
% The proposed method is shown in Figure~\ref{fig: illustration}. The core idea is to increase the diversity of question queries. 

% Considering the high space dimension of the reasoning chains

% To improve the extraction-equivalence performance~(\ref{eq:Extraction-equivalence}), we augment the diversity of query with respect to one underlying instruction, as shown in Figure~\ref{fig: illustration}. Following the above analysis, and motivated by the DAG reasoning structure~\citep{zhu2023dyval}, the data augmentation is performed with the following two steps: 

\textbf{Step 1: Order Permutation.} We begin by splitting the original query $q$ into a list of partitions, using the newline character ("$\backslash$n") as the delimiter. Next, we shuffle the order of these partitions while preserving the semantic meaning, resulting in a reordered list of sentences. 

\textbf{Step 2: Redundancy Addition.} To add redundant information to the original question, we first randomly sample some new nodes and edges, which are not connected to the original DAG so that they will not contribute to the computation of the target node, and then generate the new premises accordingly following the previous template.
After this, we add the partition to a random position of the list, and then combine them together forming a query $q^\prime$ in the augmented dataset \( \mathcal{D}_\text{aug} \).

More details are presented in Alg.~\ref{algo:data_augmentation}. After the data augmentation, we finetune the language model using supervised fine-tuning (SFT) on the augmented dataset \( \mathcal{D}_\text{aug} \). 


\begin{algorithm}[t]
\caption{\method\ }
{\bfseries Input:} \raggedright QA pair $(q, a)$ from the original dataset, augmentation times $K$ redundant information number $R$, separation delimiter SEP. \par
{\bfseries Output:} \raggedright Augmented dataset \( \mathcal{D}_\text{aug} \) for this QA pair \par
\begin{algorithmic}[1]
\STATE Initialize \( \mathcal{D}_\text{aug} \gets \{(q, a)\}\)
\FOR{\( i = 1, \dots, K \)}
\STATE \textcolor{blue}{\# Step 1: Order Permutation}
\STATE Divide the query into a segmentation list $L$ by SEP;
\STATE $\text{random.shuffle}(L)$
\STATE \textcolor{blue}{\# Step 2: Redundancy Addition}
    \FOR{\( j = 1, \dots, R \)}
        % \STATE \# Sample template from list
        % \STATE $t \gets \text{random.choice}(l_q^\prime)$
        
        \STATE Randomly sample redundant nodes with random values and edges;
        \STATE Construct a redundant partition $l_r$ by applying template to the new nodes;
        \STATE $L$.append($l_r$);
    \ENDFOR
\STATE random.shuffle($L$);
\STATE $q^{\prime} \gets$ SEP.join($L$)
\STATE    \( \mathcal{D}_\text{aug} \gets \mathcal{D}_\text{aug} \cup \{(q^\prime, a)\} \)
\ENDFOR
\STATE {\bfseries Return:} \( \mathcal{D}_\text{aug} \)
\end{algorithmic} \label{algo:data_augmentation}
\end{algorithm}

% \paragraph{Manipulation-equivalence data augmentation}
% To improve the manipulation-equivalence performance~(\ref{eq:Manipulation-equivalence}), we augment the diversity of reasoning chain for one fixed question, as shown in Figure~\ref{fig: illustration}. In practice, we utilize different topological sorting to traverse the intermediate nodes before reaching the root node.

\subsection{Knowledge Extraction Verification Tool}
\label{subsection: probing}
In the previous subsections, we proposed \method\ to enhance knowledge extraction capability and address reasoning consistency requirements.  
However, it remains unclear whether \method\ fundamentally improves reasoning capability or merely relies on memorization.  
In this section, we introduce the probing tool we use to evaluate the representation extraction capability, providing an explanation for the performance enhancement brought by \method.

% Researchers discovered that a LLM can assess its own reasoning capability while processing a question query, even before generating a response ~\citep{YXLA2024-gsm2}. Additionally, 

Recent literature has revealed that in transformer-based LLMs, attention patterns are good indicators of whether LLMs can effectively retrieve useful information from queries~\citep{wang2024towards, hou2023towards}. Based on these findings, we utilize the attention of LLMs to analyze the knowledge extraction capability.

The core component of the attention-based probing is a binary classification task to determine whether a statement $q[i]$ in question query $q$ contains any useful information~\citep{hou2023towards}:
\begin{equation}
\label{eq:classification_probe}
    \mathcal{P}(q[i] | \boldsymbol{A}) \rightarrow \{0, 1\},
\end{equation}
where $\boldsymbol{A}=\{\boldsymbol{A}(l, h) \mid 1 \leq l \leq L ; 1 \leq h \leq H\}$ is the combination of attention weights across $L$ layers and $H$ heads after processing the question query $q$. The prediction result $1$ indicates $q[i]$ is useful in reasoning, and $0$ indicates $q[i]$ only contains irrelevant information. Following the previous work~\citep{hou2023towards}, we segment all tokens in a query into multiple groups and construct simplified attention $\boldsymbol{A}_\text{simp}$ by pooling in each group as the conditions to reduce the dimension of the attention weights:
\begin{equation}
\label{equ:simplified_attention}
    \mathcal{P}(q[i] | q, \boldsymbol{A}) \approx \mathcal{P}(q[i] | q, \boldsymbol{A}_\text{simp}),
\end{equation}
% where the simplified attention  is defined as the last token attention with pooling among .
% \begin{equation}
% \boldsymbol{A}_{\mathrm{simp}}(l, h)[i]=\max _{t_{j^{\prime}} \in Q}\left(\operatorname{mean}_{t_{i^{\prime}} \in S_i}\left(\boldsymbol{A}(l, h)\left[i^{\prime}, j^{\prime}\right]\right)\right)
% \end{equation}
The previous attention-based probing method~\citep{hou2023towards} employs non-parametric algorithms, such as KNN, for query information retrieval. However, it struggles to accurately identify relevant premises, particularly in complex tasks or larger networks. This limitation arises because KNN treats all attention entries equally, overlooking the fact that the information at each position is inherently influenced by preceding content. As input length and network depth increase, this aggregation effect becomes more pronounced, further undermining the probing method's effectiveness. To address this issue, we adopt a linear probing approach based on logistic regression:
\begin{equation}
    \mathcal{P}(q[i] | \boldsymbol{A}_{\mathrm{simp}}) = \sigma(\boldsymbol{w}^\top \boldsymbol{A}_{\mathrm{simp}} + b),
\end{equation}
where $\boldsymbol{w}$ and $b$ are trainable variables. See more discussions on comparison between KNN-based and our probing methods in Appendix~\ref{subsection: appendix-model-probing}.
% Using KNN classification implicitly assumes that all entries in the simplified attention vector are equally important for information retrieval. However, we argue that this assumption may not hold in complex tasks with more intricate graph structures and deeper networks, as the information will aggregate on the beginning tokens. By applying linear probing, we can address this limitation. 

% Since the attention weights would aggregate on the first tokens in question queries, it is better to assign different weights to different attention units rather than treating them equally by using KNN.