Large Language Models (LLMs) have demonstrated superior performance across various reasoning tasks, including mathematical reasoning~\citep{YXLA2024-gsm1, qu2024recursive, lin2024rho, gou2023tora, shen2025satori}, code generation~\citep{chen2021evaluating, zhang2024diversity, zhang2023planning, dainese2024generating}, and autonomous system decision-making~\citep{yang2024agentoccam, sima2024drivelm}. Despite their ability to handle complex reasoning tasks, LLMs exhibit naive failure modes. For instance, they suffer from premise order sensitivity~\citep{chen2024premise, zhu2024dynamic}, the reverse-curse phenomenon~\citep{berglund2023reversal, golovneva2024reverse}, and distractibility~\citep{pmlr-v202-shi23a, zhu2024dyval}, making them vulnerable to variations in the natural language description of a query, even when the underlying semantic meaning remains unchanged.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/illustration/motivation_example_0215.pdf}
    \caption{Failure examples of LLMs under surface form variations. Queries are modified from R-GSM~\citep{chen2024premise}. Table: The correctness for $10$ evaluations across different LLMs. \includegraphics[height=0.8em]{figure/icon/check.png}: all correct; \includegraphics[height=0.8em]{figure/icon/no.png}: all wrong; \includegraphics[height=0.8em]{figure/icon/warning-sign.png}: error occurs. Full incorrect answers are provided in Appendix~\ref{subsection: close-source model evaluation}.}
    \label{fig: motivation}
    % GSM8K~\citep{cobbe2021training} and 
    \vspace{-4mm}
\end{figure*}

% Upper: order permutation of provided information leads to DeepSeek's incorrect response. Bottom: redundancy addition in the query leads to GPT-4's incorrect response. Both models can correctly generate the answer before surface form transformations. 

A reasoning problem can be structurally decomposed into two layers~\citep{zhu2023dyval, zhou2024paraphrase}: the underlying layer represents the \textit{semantic meaning}, which includes the structure of reasoning chains and the knowledge required for logical deduction, while the upper layer consists of the \textit{surface form}, referring to the natural language description of the problem~\citep{zhou2024paraphrase}. The inability of LLMs to maintain consistent performance across different surface forms highlights the need for \textit{reasoning consistency}, which we define as the model's ability to generate consistent and correct answers from different query transformations of a problem with the same semantic meaning. In this work, we explore the question: \textit{how to improve LLMs' reasoning consistency across varying surface forms}.


Our key insight is that LLM reasoning problems maintain certain underlying structures, such as \textit{description symmetry}, which describes transformations of the query description that preserve its semantic meaning. Existing LLM post-training methods typically focus on improving the quality of training data for reasoning chains, such as using bootstrapping to enhance reasoning diversity~\citep{yu2023metamath}. However, these methods overlook the description symmetry in reasoning tasks, which naturally exists in surface form variations. As a result, they often suffer from overfitting and inconsistent reasoning with respect to different surface forms as shown in Figure~\ref{fig: motivation}.  
To address this issue, we propose \methodlong\ for LLM post-training. Our method augments the dataset with the symmetry information, enforcing the model's capability to better capture semantic meanings in different query variations, thereby improving post-training data efficiency and enhancing OOD generalization.
The main contributions of this work are:


\textbf{1. Formal analysis of the reasoning consistency problem.} We associate reasoning consistency in terms of an LLMâ€™s ability to extract invariant knowledge, providing a formal framework for improving reasoning consistency.

\textbf{2. Introduction of \method\ to address the consistency of reasoning through post-training.} To our knowledge, this is the first systematic work that investigates post-training techniques to mitigate issues of consistency of reasoning.

\textbf{3. Extensive evaluation of \method\ in LLM reasoning tasks.} Our experimental results show that \method\ achieves superior sampling efficiency and generalizability in reasoning consistency with a significantly enhanced in-context knowledge extraction capability.


