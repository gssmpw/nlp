% \input{table/permutation_math_table}

In the experiments, we are going to answer the following research questions. (\textbf{RQ-1}) How do current LLMs perform in terms of reasoning consistency?
(\textbf{RQ-2}) How does \method\ benefit reasoning consistency?
(\textbf{RQ-3}) Why does \method\ improve reasoning consistency?
To answer these questions, we make the following experiment setup.


\subsection{Experiment setup}
\textbf{Tasks:} We evaluate our approach using two categories of tasks, logical reasoning and arithmetic reasoning, both selected from \texttt{PromptBench}~\citep{zhu2023dyval}. For simplicity, we focus on questions that can be represented as tree structures, where each non-leaf node has up to two parent nodes. The objective is to compute the value of the root node given the values of the leaf nodes and the computational rules from parent nodes to child nodes. Two example QA pairs are provided in Appendix~\ref{subsection:appendix-example-QA-pairs}.

In logical reasoning tasks, each node takes a boolean value, and the computational rules are sampled from $\{ \land (\texttt{AND}), \lor (\texttt{OR}), \neg(\texttt{NOT}) \}$. In arithmetic reasoning tasks, each node takes an integer value, with leaf node values sampled from $0\sim10$. The computational rules are drawn from $\{ +, -, \times, \square^2\}$. Given an underlying tree representation, \texttt{PromptBench} first assigns a random name to each node and then generates a natural language query using a fixed template, as shown in Appendix~\ref{subsection: example-qa-pairs}.

For the evaluation dataset, we create variations of surface forms with both order permutation and redundancy addition. For order permutation, we choose \texttt{Topological}: the topological sorting of the underlying DAG, which aligns with the reasoning chain~\citep{zhu2023dyval}, \texttt{Random}: random permutation of sentences in the question queries, and \texttt{Reversed}: the inversed order of \texttt{Topological}. We utilize the templates in the benchmark~\citep{zhu2023dyval} to ensure that the overall semantical meaning does not change with the order permutation. For redundancy addition, we add irrelevant descriptions in the query and set the number of redundant dependencies ranging from $0$ to $40$.

\begin{figure}[t ]
    \centering
    % \vspace{-8pt} 
    \includegraphics[width=1\linewidth]{figure/exp/DeepSeek_eval.pdf}
    % \vspace{-1mm}
    \caption{Accuracy evaluation of the DeepSeek-math-7B-base model on the arithmetic reasoning task with different surface forms. Left: results with different DAG depth; Right: results with different redundant information addition.}
    \label{fig: mathstral_evaluation}
    \vspace{-4mm}
\end{figure}

\textbf{Models:} For all experiments, we utilize the Llama-3.2 1B and 3B models~\citep{dubey2024llama3} as our base models unless specified otherwise.

\textbf{Baselines:} We compare our method with three types of baselines: \textbf{(1) Post-training:} \texttt{Vanilla}: trains base models on the vanilla dataset using SFT; and \texttt{RC-Aug}: utilizes the reasoning chain (RC) augmented dataset for SFT~\citep{yu2023metamath}. Specifically, we augment the answers with different topological orderings while keeping the query unchanged; \textbf{(2) Inference scaling:} \texttt{SCoP}-$k$~\citep{zhou2024paraphrase}, first paraphrases the question prompts $k$ times, then performs reasoning based on these paraphrases, and obtains the final answer via majority voting. It is combined with \texttt{vanilla} models unless specified otherwise. \textbf{(3) Ablation:} In addition to these baselines, we also create a variant of our method called \texttt{MEND-RC} that uses \texttt{\method} to transform the queries, while augmenting the dataset with more reasoning chains. We evaluate the models with greedy generation (temperature=$0$) and report the averaged accuracy on $200$ testing samples on every evaluation dataset unless specified otherwise.
% Implementation details are available in the Appendix.


% \TODO{make it more clear, e.g., how VoV is computed, why we need such a metric except for 
% the typical acc}

% \input{table/main_table}
% \input{table/permutation_logic_table}

\subsection{Reasoning-Consistency Evaluation}
\label{subsection: Reasoning-Equivalence Evaluation}
To answer \textbf{RQ1}, we evaluate several LLMs with query variations and present the reasoning accuracy in Figure~\ref{fig: mathstral_evaluation}. Due to space limitations, we defer the additional results to Appendix~\ref{subsection:appendix-open-source models}. We observe two key findings: (1) The topological order improves the accuracy of generation, while adversarial permutations, including random and reversed orders, degrade performance. Furthermore, the harder the reasoning task (i.e., greater DAG depth), the more pronounced the performance drop, consistent with previous findings for closed-source LLMs~\citep{chen2024premise}.
(2) {Redundant information in the question query negatively affects reasoning performance.} As the amount of irrelevant information increases, the model's reasoning accuracy decreases.


Both observations indicate that open-source models still suffer from \textbf{overfitting to topological order and redundancy-free queries}, leading to poor performance in reasoning consistency tasks. This issue further motivates the development of techniques to enhance reasoning-consistency robustness in LLMs.




% \begin{figure*}[ht]
%     \centering
%     % \vspace{-8pt} 
%     \includegraphics[width=1\linewidth]{figure/exp/Math_llama_1B_redundant_info.pdf}
%     % \vspace{-1mm}
%     \caption{1B Logical reasoning}
%     \label{fig: Logic reasoning}
%     \vspace{-2mm}
% \end{figure*}

\begin{figure*}[ht]
    \centering
    % \vspace{-8pt} 
    \includegraphics[width=\linewidth]{figure/exp/Redundancy_exp_0215.pdf}
    % \vspace{-1mm}
    \caption{Evaluations with respect to different query variations. Each figure refers to one permutation order type, the x-axis represents the number of redundancies of the test set, and the y-axis represents the accuracy of final answers. For each dataset, we report the accuracy value over a dataset with a size of $200$. 
    % \textbf{In-distribution data:} training dataset including augmented data only covers the redundancy ranging from 0 to 4.
    }
    \label{fig: Math reasoning}
    \vspace{-2mm}
\end{figure*}

\subsection{Reasoning-Consistency Enhancement}
\label{subsection:exp reasoning-equivalence enhancement}
To answer \textbf{RQ2}, we present the main experiments with two main parts: reasoning consistency on order permutation and redundancy addition.

\textbf{Order permutation}: We train the LLM on arithmetic questions with difficulty level $2$ and $3$ while testing its performance on difficulty level $1\sim4$ with three different permutation orders: \texttt{Topological}, \texttt{Random} and \texttt{Reversed}. The evaluation results on Llama-3.2-3B are listed in Table~\ref{tab: permutation-topological}. More results with different base models are available in the Appendix~\ref{subsection:appendix-permutation-order-experiments}.

\input{table/permutation}

Table~\ref{tab: permutation-topological} shows that training solely on the topological order corpus results in overfitting - it achieves superior performance on the \texttt{Topological} test set but suffers significant degradation on the \texttt{Random} and \texttt{Reversed} sets. \texttt{RC-Aug} provides only minor improvements in overall performance. \texttt{SCoP-k}, the inference-time scaling baseline, improves performances on the \texttt{Reversed} and \texttt{Random} sets but degrades on the \texttt{Topological} set. In contrast, our method demonstrates consistently strong performances on all evaluation datasets, effectively mitigating overfitting issues.


\textbf{Redundancy Addition}: To evaluate the robustness of LLMs against redundancy, we add $0$ to $4$ redundant premises to each query in the SFT dataset and test the LLMs' performance on questions with up to 40 redundant premises, with useful information provided in three permutation orders. The evaluation results are presented in Figure~\ref{fig: Math reasoning}. Additional results with different model sizes are provided in Appendix~\ref{subsection:supplementary-exp-redundancy}.


Figure~\ref{fig: Math reasoning} shows the \texttt{Vanilla} method performs well only on in-distribution samples with \texttt{Topological} order. As redundancy increases in the OOD setting, prediction accuracy declines sharply, indicating overfitting. When SFT with the \texttt{RC-Aug} dataset, the model shows noticeable performance improvements under in-distribution conditions, likely due to an enhanced capability for reasoning chain generation, as it learns from more reasoning chain data during post-training. Despite this improvement, \texttt{RC-Aug} still struggles to generalize well as the level of redundancy increases.  
The inference-time scaling method, \texttt{SCoP}-$k$, results in more consistent performance across surface variations. We observe its significant improvements in OOD settings, including \texttt{Random} and \texttt{Reversed} permutation orders and increased redundancy. However, the improvement is still limited, especially for more challenging arithmetic reasoning task. Additionally, it even shows performance degradation on the \texttt{Topological} dataset, where information is originally provided in order and disturbed in paraphrases.

Results from Figure~\ref{fig: Math reasoning} also shows \method, consistently outperforms all baselines across different permutation orders and levels of redundancy. These results demonstrate that diversifying question queries for SFT sufficiently enhances the ability of models to capture symmetry information across various surface forms. Consequently, our method not only performs well on in-distribution evaluation samples but also shows strong generalizability to OOD cases with unseen redundancy.  
We also conduct an ablation study with \texttt{MEND-RC}, where we observe a performance degradation when replacing the data augmentation direction from queries to reasoning chains, highlighting the necessity of enhancing query symmetry information for SFT.

% \TODO{rewrite it following "order permutation": 1. what is the exp setup for this part; 2. what's the result, 3. analysis} The evaluation results on reasoning consistency with redundancy addition are presented in Figure~\ref{fig: Math reasoning}. The post-training datasets for SFT contain a number of redundant information samples ranging from $0$ to $4$. We observe that for the \texttt{Vanilla} method, when fine-tuned on the original dataset, the model performs well only on in-distribution samples with \texttt{Topological} order. However, as the level of redundancy increases in the OOD setting, prediction accuracy drops significantly. When fine-tuned on the \texttt{RC-Aug} dataset, the model shows noticeable performance improvements under in-distribution conditions, likely due to an enhanced capability for reasoning chain generation, as it learns from more reasoning chain data during post-training. Despite this improvement, \texttt{RC-Aug} still struggles to generalize well as the level of redundancy increases.  
% The inference-time scaling method, \texttt{SCoP}-$k$, diversifies question query forms during inference, resulting in more consistent performance across surface variations. We observe significant improvements in OOD settings, including adversarial permutation orders and increased redundancy. However, the improvement is still limited, especially for the more challenging arithmetic reasoning task, and even shows performance degradation on the \texttt{Topological} dataset, where information is provided in order.

\begin{figure}[t]
    \centering
    % \vspace{-8pt} 
    \includegraphics[width=\linewidth]{figure/exp/data_efficiency_variance_0215.pdf}
    % \vspace{-1mm}
    \caption{Data efficiency evaluation. R in figure titles indicates the number of redundancy in the query. The size of dataset $=1$ indicates using the original dataset for SFT. All plots are averaged among $3$
random seeds with temperature$=1$. The solid line is the mean value, and the light shade represents the first standard deviation.}
    % \TODO{add explanation of R} 
    \label{fig: data efficiency}
    \vspace{-2mm}
\end{figure}


\input{table/VoV_exp}

\textbf{Reasoning consistency:} We adopt the Variance of Variations (VoV)~\citep{zhou2024paraphrase} to quantitatively evaluate the reasoning consistency:  
% \TODO{}
\begin{equation}
\mathrm{VoV}_{\mathbf{f}}=\operatorname{Var}_{\mathbf{f}}(\mathrm{acc}(p)),\ {\mathbf{f}} \in \{\mathbf{o}, \mathbf{r}\},
\end{equation}
where $\mathrm{VoV}_\mathbf{o}$ and $\mathrm{VoV}_\mathbf{r}$ are the variance of prediction accuracy with respect to different permutation orders and different levels of redundancy. Smaller $\mathrm{VoV}$ values indicate better reasoning consistency. 

Table~\ref{tab:VoV} shows the VoV values for the arithmetic reasoning task in Figure~\ref{fig: Math reasoning}. The \texttt{Vanilla} and \texttt{RC-Aug} method show very large VoV values, indicating poor reasoning consistency and poor capability to capture the symmetry information. The \texttt{SCoP} method successfully reduces the variance since the final results are obtained through majority voting from diverse query paraphrases. Our method also demonstrates strong performance in reducing variance, indicating that we effectively mitigate reasoning consistency issues during post-training without sacrificing inference-time efficiency. 

\textbf{Data Efficiency:} We use different amounts of data for SFT and present the results of the arithmetic reasoning task with {Llama-3.2-1B} model in Figure~\ref{fig: data efficiency}. For the \texttt{RC-Aug} baseline, we observe that increasing the size of the augmented dataset leads to a rapid accuracy improvement in in-distribution conditions but a much slower improvement in OOD settings. This indicates that while \texttt{RC-Aug} can improve performance, it is not efficient in OOD scenarios. 
In contrast, \method\ consistently outperforms the baselines across different amounts of SFT data in OOD settings, demonstrating superior data efficiency and generalizability.




\begin{figure}[t]
    \centering
    % \vspace{-8pt} 
    \includegraphics[width=0.85\linewidth]{figure/exp/Linear_probing.pdf}
    \vspace{-3mm}
    \caption{F1-macro score of linear probing on the logical reasoning task with the base model as \texttt{Llama-3.2-1B}. \texttt{Method}-$n$ indicates the dataset size is $n$ times the original one after augmentation.}
    \label{fig: Linear probing}
    \vspace{-4mm}
\end{figure}

\subsection{Reasoning-Consistency Verification} 
In this subsection, we aim to answer \textbf{RQ3} by verifying whether \method\ captures the structured information in question queries. We utilize the probing method described in Section~\ref{subsection: probing} and report the F1-Macro scores of prediction (\ref{eq:classification_probe}) as the evaluation metric~\citep{hou2023towards}. A higher score means a better ability to retrieve relevant information from the input queries.



The results are presented in Figure~\ref{fig: Linear probing}. We observe that the baseline methods \texttt{RC-Aug} and \texttt{MEND-RC} do not show significant improvement in detecting useful information compared to the \texttt{Vanilla} model.  
In contrast, the proposed method \method\ achieves a much higher score than the baselines. This confirms that by enhancing query symmetry, \method\ captures more structured information and significantly improves LLMs' \textit{in-context knowledge extraction} capability for reasoning tasks. The probing results also explain the OOD generalizability improvement of \method, as it enhances the LLMs' ability to understand OOD queries. A comparison between our linear probing and KNN probing is provided in Appendix~\ref{subsection: appendix-model-probing}.



% This observation also indicates that the ability to extract knowledge in context plays an important role in LLM reasoning. The initial state and embedding information are critical for LLM to generate a correct reasoning path. This extend the previous research finding that the reasoning ability of LLM is determined before its generation from pre-trained models to post-trained models.








% Inspired by~\cite{wang2024towards,hou2023towards}, we perform attention analysis on the model when fed with different question queries. We adopt \texttt{MechanisticProbe}~\citep{hou2023towards} to exam whether the LLM has the ability to extract useful information from question prompts.

% The goal is to identify the set of useful input statements required for reasoning. This is achieved by analyzing the language model's attention patterns to determine whether each statement contributes to the reasoning process. The task is formulated as a binary classification problem, where the model predicts whether a given statement is useful or not based on the attention of the last token in the input with respect to the whole input query.

% The probing results used for logic reasoning are presented in Figure~\ref{fig: Linear probing}, where we report the F-1 Macro score as consistent in~\citep{hou2023towards}. A higher score means a better ability to identify and extract useful information from the question prompt. We can observe that we more irrelevant information (the number of redundant dependency), it is harder for LLM to recognize the useful tokens in the question prompt. In addition, performing \method\ can significantly improves the in-context knowledge extraction ability of LLM, as with the number of augmented question increase, the score increases under all the testing cases. More details about the probing experiment are provided in the Appendix.

% \subsection{Scaling Discussion}
% TODO
% \begin{itemize}
%     \item How does the proposed method compared to the CoT augmentation?
%     \item How does the proposed method compared to the inference-time scaling?
% \end{itemize}