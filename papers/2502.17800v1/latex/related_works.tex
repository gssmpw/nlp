\textbf{LLM Reasoning}: Recent studies highlight the remarkable reasoning capabilities of LLMs, exploring in-context learning~\citep{wei2022chain, yao2022react}, pre-training~\citep{YXLA2024-gsm1, YXLA2024-gsm2, shao2024deepseekmath, lightman2023let, lin2024not}, and post-training~\citep{ni2024exploring}. A key focus in training LLMs is the curation of high-quality datasets for instruction tuning~\citep{yue2023mammoth, liu2024augmenting, ni2024exploring}. Another approach involves leveraging LLM-generated datasets~\citep{wang2024math, cen2024bridging}, often with reinforcement learning~\citep{ouyang2022training, kumar2024training}. Both strategies highlight the essential role of high-quality data in enhancing LLM reasoning performance.
\\
\textbf{LLM Failure Modes:} Despite their advanced reasoning capabilities, LLMs exhibit surprising brittleness to variations in question descriptions with the same semantical meaning~\citep{chen2024premise}. For example, the \textit{Reversal Curse}~\citep{berglund2023reversal} refers to LLMs failing to generalize from statements like ``A is B'' to ``B is A.'' \textit{Premise Ordering} describes the performance degradation when the order of premises in a query differs from the order in their reasoning chains~\citep{chen2024premise}. \textit{Distractibility} is another failure mode in which LLMs' reasoning performance declines when irrelevant context is included in the query~\citep{shi2023large}. To mitigate these issues, researchers have explored inference-time scaling methods that paraphrase queries~\citep{zhou2024paraphrase} and post-training techniques~\citep{golovneva2024reverse} that reverses the order of tokens in training. However, a systematic analysis and solution to address these robustness challenges still remain an open problem.
\\
\textbf{Symmetry and Equivariant Learning:} Symmetry has been widely used to indicate rotationally
symmetric problems, such as image-input machine learning tasks~\citep{weiler2019general}. The definition of symmetry can also be extended to groups that preserve structured information while performing transformations~\citep{muglich2022equivariant}. Encoding data symmetries in the model training pipeline can improve both generalization and sample efficiency~\citep{wang2022mathrm}, an idea first proposed in G-Convolution~\citep{cohen2016group}. Recent works also incorporate equivariant learning in reinforcement learning~\citep{liu2023continual, wang2022mathrm} and robotics~\citep{yang2024equibot}. The symmetry in the training corpus of LLMs remains a widely unexplored area.
% \\
% \textbf{Large Language Model Reasoning:} Permutation issues (surface form issues): previous work also found that the permutation order affect the reasoning capability (the language modelâ€™s lack of robustness and sensitivity to the surface form in reasoning through complex problems.)~\citep{chen2024premise, zhou2024paraphrase}. SCoP diversifies reasoning paths from specific surface forms of the problem, which requires the additional paraphrasing ability to generate surface forms with identical semantics~\citep{zhou2024paraphrase}. This paper is on example of the inference-time vertical-horizontal thinking trade-off~\citep{wang2024towards}. Graph-based reasoning in Autonomous Driving foundation models~\citep{sima2024drivelm}. Self-consistency setting~\citep{wang2022self}. Masked Thought~\citep{chen2024masked}.
% \\
% \textbf{Tree structure in LLM Reasoning:} The reasoning process for answering a multi-step reasoning question can be represented as a reasoning tree~\citep{hou2023towards}. Many reasoning graphs can be formalized as the chain-like tree structure~\citep{wei2022chain}.
% \\
% \textbf{In-Context Learning:} Investigated and compared in~\citep{li2024language}. (1) ICL serves as a Task Selection module. LLM identifies the task given provided demonstrations.~\citep{min2022rethinking}. (2) Meta-Learning: previous works~\citep{von2023transformers, akyurek2022learning} suggest that the pre-training stage prepares the parameters in an LLM to have the ability to further perform optimization such as utilizing gradient descent. (3) Structured Task Selection: the LLM uses the in-context demonstration to compose a sequence of learned tasks and uses this for further generation~\citep{hahn2023theory}.\\
% \textbf{Data Augmentation for LLM Reasoning:} Reverse the order of input~\citep{golovneva2024reverse, guo2024mitigating}
% \\
% \textbf{Equivariant Learning (Symmetry):} Group Equivariance~\citep{liu2023continual}. More related works are discussed in the Appendix.