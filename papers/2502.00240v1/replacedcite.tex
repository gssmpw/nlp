\section{Related work}
\paragraph{Data-driven regularization:} There have been several lines of work that aim to learn regularizers in a data-driven fashion. As mentioned previously, early works include the field of dictionary learning or sparse coding ____. Such approaches can be described as learning a polyhedral regularizer from data. Regularization by Denoising ____ constructs an explicit regularization functional using an off-the-shelf deep neural network-based denoiser. The works ____ learn a regularizer via the critic-based adversarial loss inspired by the Wasserstein distance. The difference in these works lie in the way the neural network is parametrized to enforce certain properties, such as convexity or weak convexity. The latter ____ also establishes convergence guarantees of the Input Weakly Convex Neural Network (IWCNN) regularizer. Similarly, ____ developed convex and weakly convex regularizers given by convolutional ridge functions with provable convergence guarantees. There have also been other works ____ that have aimed to approximate the proximal operator of a potentially nonconvex data-driven regularizer.

\paragraph{DC programming and regularization:} Our work is inspired by the rich literature on DC programming and its use in regularization. For a general survey, please see ____. In terms of DC structure in sparse regularization, several works have analyzed the use of the DC function $x \mapsto \|x\|_1 - \|x\|_2$ as a sparsity inducing regularizer ____ as its zeros correspond to $1$-sparse vectors. Many popular nonconvex regularizers have also been shown to have a DC decomposition ____, such as SCAD ____, MCP ____, or the Logarithmic penalty ____.