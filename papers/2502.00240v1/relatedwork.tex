\section{Related work}
\paragraph{Data-driven regularization:} There have been several lines of work that aim to learn regularizers in a data-driven fashion. As mentioned previously, early works include the field of dictionary learning or sparse coding \cite{MairaletalSurvey14, EladSurvey10}. Such approaches can be described as learning a polyhedral regularizer from data. Regularization by Denoising \cite{romano2017RED} constructs an explicit regularization functional using an off-the-shelf deep neural network-based denoiser. The works \cite{lunz2018adversarial, acr, shumaylov2024weakly} learn a regularizer via the critic-based adversarial loss inspired by the Wasserstein distance. The difference in these works lie in the way the neural network is parametrized to enforce certain properties, such as convexity or weak convexity. The latter \cite{shumaylov2024weakly} also establishes convergence guarantees of the Input Weakly Convex Neural Network (IWCNN) regularizer. Similarly, \cite{goujon2023neural, goujon2024learning} developed convex and weakly convex regularizers given by convolutional ridge functions with provable convergence guarantees. There have also been other works \cite{Fanetal24} that have aimed to approximate the proximal operator of a potentially nonconvex data-driven regularizer.

\paragraph{DC programming and regularization:} Our work is inspired by the rich literature on DC programming and its use in regularization. For a general survey, please see \cite{le2018dc}. In terms of DC structure in sparse regularization, several works have analyzed the use of the DC function $x \mapsto \|x\|_1 - \|x\|_2$ as a sparsity inducing regularizer \cite{ahn2017difference, yin2015minimization} as its zeros correspond to $1$-sparse vectors. Many popular nonconvex regularizers have also been shown to have a DC decomposition \cite{cao2022unifying}, such as SCAD \cite{fan2001variable}, MCP \cite{zhang2010nearly}, or the Logarithmic penalty \cite{mazumder2011sparsenet}.