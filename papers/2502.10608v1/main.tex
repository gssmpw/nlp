\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage[square,numbers]{natbib}
% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{authblk}
\usepackage{float}
\usepackage{subcaption}

\title{\textbf{Universal Lesion Segmentation Challenge 2023: \\ A Comparative Research of Different Algorithms}}
\author[1]{Kaiwen Shi}
\author[2]{Yifei Li}
\author[1]{Binh Ho}
\author[2]{Jovian Wang}
\author[2]{Kobe Guo}
\affil[1]{Mathematics Department, Vanderbilt University, Nashville, TN, USA}
\affil[2]{Department of Computer Science, Vanderbilt University, Nashville, TN, USA}
\date{March 30, 2024}

\begin{document}
\maketitle

\begin{abstract}
    In recent years, machine learning algorithms have achieved much success in segmenting lesions across various tissues. There is, however, not one satisfying model that works well on all tissue types universally. In response to this need, we attempt to train a model that 1) works well on all tissue types, and 2) is capable of still performing fast inferences. To this end, we design our architectures, test multiple existing architectures, compare their results, and settle upon SwinUnet. We document our rationales, successes, and failures. Finally, we propose some further directions that we think are worth exploring. codes: https://github.com/KWFredShi/ULS2023NGKD.git
    \\\\ \textbf{Keywords:} Universal Lesion Segmentation, UNet
\end{abstract}

\section{Introduction}
\hspace{0.5cm} Medical image segmentation is a crucial task in medical image processing. Thanks to the advent of CNN\cite{NIPS1989_53c3bce6}, U-Net \cite{RonnebergerFB15}, and their variants such as V-Net\cite{7785132}, 3D U-Net\cite{DBLP:journals/corr/CicekALBR16}, Res-UNet\cite{Xiao2018WeightedRF}, Dense-UNet\cite{8379359}, we are able to perform segmentation task with precision.  More recently, with implementations of transformer-based models, the medical imaging community enjoyed satisfying success in segmentation tasks. Networks like Medical Transformers\cite{DBLP:journals/corr/abs-2102-10662medtrans} and SwinUnet\cite{cao2021swinunet} push the front-line boundary to another degree. Others have implemented learning methodologies from other fields, such as dictionary learning, to work on medical images. KEN\cite{10.1007/978-3-031-20074-8_28KEN} - knowledge embedding network - for example, takes advantage of the fruitfulness of information embedding in each layer via dictionary learning to provide a more semantically meaningful network.

While most of the networks have achieved very promising results on datasets composed of one tissue type, none of them works well universally across various tissue types. Meanwhile, facing a growing need for CT exams and their lesion segmentation, radiologists have suffered much from intensive human labor. In response to these two situations, Max de Grauw, Bram van Ginneken, and Alessa Hering of the Diagnostic Image Analysis Group, partnering with Mathai Tejas, Pritam Mukherjee, and Ronald Summers of the National Institutes of Health, launched the Universal Lesion Challenge 23 (https://uls23.grand-challenge.org/). 

To address the dire situation, we attempt to adopt/devise an algorithm that is 1) precise enough based on the Dice score, 2) adequately robust to respond to variation in tissue types, and 3) light-weight in model complexity so that the inference runs around 5 seconds per input. To this end, we test the effectiveness of various algorithms, including nnUNetv2\cite{Isensee2020nnUNetAS}, DeepLabV3+\cite{DBLP:journals/corr/ChenPSA17DLV3}, Medical Transformer\cite{DBLP:journals/corr/abs-2102-10662medtrans}, SwinUnet\cite{cao2021swinunet}, and TransUNet\cite{DBLP:journals/corr/abs-2102-04306TransUNet}. We compare their results and fine-tune our final model on TransUNet, the one that has worked best in our testing.

\textbf{Contributions.} 1. We tested the effectiveness of nnUNetv2, DeepLabV3+, Medical Transformer, SwinUnet, and TransUNet on the Bone Lesion dataset. 2. We fine-tuned the TransUNet model, with well-written data augmentations and transformations. 3. We included a discussion on potential improvements that can be made to our project.

\section{Task Description}

\hspace{0.5cm} The task at hand is to design, implement, and train a model that can segment lesions universally. To be more specific, the input of the model will be a 256x256x128 data point, with only one channel. The input is named VOI, or volume of interest. The output of the model should be a segmentation of the lesion, and the performance will be measured with inference speed, segmentation accuracy, and robustness. 
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.7\textwidth]{img/pipeline.png}
    \caption{The Proposed Pipeline from ULS Challenge Host}
\end{figure}

\hspace{0.5cm} Once the model is trained and deployed, radiologists can locate the VOI manually, pass the VOI into the model pipeline, and get the returned lesion segmentation almost immediately.



\section{Experiments}
\hspace{0.5cm} In this section, we describe the rationale behind the use and test of these models, the methodologies and highlights of each model, and their corresponding results.
\subsection{Rationale}
\hspace{0.5cm} We started the project by testing the effectiveness of each model on the Bone dataset, Part 1. We split the data into a 0.7-0.2-0.1 train-test-validation compartmentalization. We trained the models on the train data and compared the validation scores. We understood that the model that worked well on the Bone dataset did not necessarily tell its effectiveness on the entire data population, so after having initial results that did not deviate too much, we further trained the working models on the whole Part 1 data. We again compared the results and went on with the most promising model to complete the project.

\subsection{Models and Results}
\subsubsection{nnUNetv2 - Baseline}

\hspace{0.5cm} nnUNetv2\cite{Isensee2020nnUNetAS} is an advanced neural network architecture designed for medical image segmentation tasks. Built upon the success of its predecessor, nnUNet, nnUNetv2 incorporates several enhancements and optimizations, making it a state-of-the-art solution in the field of medical image analysis.

\hspace{0.5cm} nnUNetv2 adopts a cascaded architecture, consisting of multiple processing stages, each refining the segmentation output progressively. The network architecture comprises deep convolutional neural networks (CNNs), augmented with advanced modules such as residual connections, dilated convolutions, and attention mechanisms. These components enable nnUNetv2 to capture complex spatial dependencies and contextual information crucial for accurate segmentation. Some of its highlights include performance, versatility, and robustness. 

\hspace{0.5cm} nnUNetv2 served as the baseline model in the ULS 23 challenge. It has produced very promising results on data points from one tissue type. However, when training on multi-typed data, the model exhibited a large variation across tissue types and even within each tissue type.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth, center]{img/BaselineR.png}
    \caption{ Baseline Result}
\end{figure}

\hspace{0.5cm} Since the nnUNetv2 pipeline had written optimizers to tune its hyperparameters, we did not continue fine-tuning it. We instead use it as a benchmark against which we test our experiments.

\subsubsection{Medical Transformer}
\hspace{0.5cm} Medical Transformer\cite{DBLP:journals/corr/abs-2102-10662medtrans} utilized the LOGO learning strategy and Gated Axial Attention. The former strategy helps to discern the local features while also caring for the global context\cite{DBLP:journals/corr/abs-2010-11929Vit}, and the latter Attention mechanism saves time complexity while giving more attention to the axial components.\cite{DBLP:journals/corr/abs-1912-12180AA}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth, center]{img/MedTrans.png}
    \caption{ LOGO Learning and Gated Axial Attention}
\end{figure}
The structure of Medical Transformer gave it prime effectiveness in segmentation tasks on MoNuSeG, GLAS, and Brain Anatomy US datasets. The problem of adopting this model, however, lay with its fixed input dimension and type. The input of the original Medical Transformer was a 128x128 image, while the VOI that was put into the model pipeline was a 256x256x128 volume. The disagreement with regard to the inputs called for modification of the existing model.

The team primarily experimented with three different potential fixes. The first one involved simple flattening of the VOI into 2D images and adjusting the model parameters according to the size of the image; the second one utilized an FCNN to extract the features from the input and fit the extracted feature into the model; the last one, being the most straightforward solution, was to implement a 3D version of Medical Transformer.

None of the fixes, unfortunately, worked well to address the current challenge at hand. The first two fixes, no matter where the feature extraction filters were put or simple flattening, took more than 100 GB of RAM to initialize the model. The last solution, even though it seemed more promising, suffered from the same problem of exploding model size. Therefore, the Medical Transformer failed to solve our problem.

\subsubsection{SwinUnet}

\hspace{0.5cm} The SwinUNet model\cite{cao2021swinunet} differs from UNet due to its use of the Swin transformer blocks. It uses a hierarchical Swin transformer with shifted windows as the encoder and a symmetric Swin transformer as the decoder. This allows the model to adeptly capture complex patterns in medical images. The hierarchical design lets the model efficiently process different scales of spatial context.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/SwinUNet.png}
    \caption{SwinUNet Architecture}
\end{figure}

\hspace{0.5cm}Our experimental process began with training the model on the ULS23 bone dataset. Through the training, we were able to track the model's training dice loss and validation dice loss metrics. The training dice loss had a consistent downward trend, which indicated an increase in dice score, but the validation dice loss had upward spikes throughout the iterations. This showed us that the model could have been overfitting on the dataset.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/SwinUNetR1.jpg}
    \caption{SwinUNet Result}
\end{figure}

\hspace{0.5cm}To mitigate overfitting, we employed data augmentation techniques such as random rotations, horizontal and vertical flips, and Gaussian blurs onto the medical images in the dataset. Even through this, however, the validation training loss did not improve much, which led to this model not being our best one.

\subsubsection{DeepLabV3+}

\hspace{0.5cm} The DeepLabV3+\cite{DBLP:journals/corr/abs-1802-02611DLV3+} is designed with a four-layer architecture featuring downsampling and upsampling blocks, integrated with skip connections to facilitate information flow. In the downsampling path, convolutional layers extract and learn features using convolutional filters followed by max pooling. The upsampling path then reconstructs the spatial dimensions through transposed convolution. Skip connections bridge these paths, allowing information to be directly passed from downsampling blocks to the corresponding upsampling blocks to mitigate the loss of finer details during the simplification of the image.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=0.3\textheight]{img/atrous.png}
    \caption{Atrous/Dilated Convolution}
\end{figure}

\hspace{0.5cm} We began the experiment with the 2D DeepLabV3\cite{DBLP:journals/corr/ChenPSA17DLV3} model. We flattened the input directly and sent it into the inference pipeline. Initially promising, this model displayed tendencies of overfitting, as evidenced by its attainment of a 0.82 Dice coefficient following 2500 epochs of trainingâ€”surpassing the baseline model's 0.72 Dice score significantly on the bone dataset.

\hspace{0.5cm}In response, we ventured into a 3D U-Net model, referred to as DeepLabV3+, owing to its atrous convolution (or dilated convolution) features similar to the DeepLabV3 architecture. However, the complex structure of this 3D model introduced significant challenges, notably extending the training duration. To overcome the challenges, we tried various local pooling techniques before inferencing, but the result turned out bleak: After 25 epochs of training, the dice score was only around 0.1, which made this model nearly useless for our purpose.

\subsubsection{TransUNet}

\hspace{0.5cm} TransUNet\cite{DBLP:journals/corr/abs-2102-04306TransUNet} is a model that aims to improve the performance of the UNet architecture by leveraging Transformer blocks as the encoders for medical image segmentation tasks. In particular, while the UNet architecture is better than pure Transformers-based models in extracting low-level details, the UNet architecture alone is not sufficient in modeling long-range dependencies, a drawback that is directly addressed by the introduction of Transformer blocks. Therefore, by using Transformer blocks as the encoders in a UNet-like architecture, TransUNet is able to efficiently tackle both long-range dependencies as well as low-level details. In their original paper, the authors of TransUNet show that TransUNet greatly outperforms other contemporary models in segmentation tasks related to the aorta, left kidney, pancreas, and spleen. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.3\textheight]{img/TransUNetModel.png}
    \caption{TransUNet Model}
\end{figure}

\hspace{0.5cm} In our experiments, we observed that TransUNet achieved a high dice score on the bone dataset, with a training and validation dice score of 0.87. 
\begin{figure}[H]
    \centering
    \includegraphics[width=1.03\textwidth, height=0.4\textheight]{img/TransUNetR1.png}
    \caption{TransUNet Result 1}
\end{figure}

\hspace{0.5cm} However, when we expanded the dataset to also include data from the pancreas dataset, the training dice score dropped to 0.80 and the validation dice score dropped to 0.79. We speculated that this is because the TransUNet architecture is able to perform well when it is tasked with a single type of tissue and that its performance deteriorates when it has to work with a dataset that contains different types of tissues.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, height=0.4\textheight]{img/TransUNetR2.png}
    \caption{TransUNet Result 2}
\end{figure}

\subsection{Results}

\hspace{0.5cm} The validation results of the working models on the Bone dataset are shown below. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/Compare.png}
    \caption{Model Result Comparison}
\end{figure}

\hspace{0.5cm} Based on the comparison, we decided to further train and fine-tune the TransUNet model. The last time we looked at its training curve showed its potential to generate a comparable result against the baseline. Below are some quantitative and qualitative results of TransUNet.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, height=0.4\textheight]{img/TransUNetR3.png}
    \caption{TransUNet Result 3}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth, height=0.2\textheight]{img/GT1.png}
        \caption{Ground Truth 1}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth, height=0.2\textheight]{img/Inf1.png}
        \caption{Inference 1}
    \end{subfigure}
    \\
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth, height=0.2\textheight]{img/GT2.png}
        \caption{Ground Truth 2}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth, height=0.2\textheight]{img/Inf2.png}
        \caption{Inference 2}
    \end{subfigure}
    \caption{TransUNet Qualitative Results}
\end{figure}

\section{Discussion}
\hspace{0.5cm} Most of our experimented models did not work as intended and failed due to out-of-memory error. Some of them were able to run but suffered very badly when we trained our models across various parts of the dataset. In this section, we will attempt to find the reasons behind these phenomena and propose potential remedies.

\subsection{Model Size}
\hspace{0.5cm} Model sizes have been always an issue during our experimentation. Many models failed to run because the RAM requirements for their initialization were too ambitious, exceeding our current 50 GB capacity.

\hspace{0.5cm}  The reason behind this gigantic memory consumption is straightforward: the original models typically work on small data points, such as a 128x128 image; however, once we adopted it to work on the data from the ULS challenges, the immense increase in the size of the data jumped the magnitude of the parameters up, thus creating a barrier for initializing the models. 

\hspace{0.5cm} Now, there are some potential remedies. One is to combine data parallelism, pipeline partition, and model parallelism.\cite{DBLP:journals/corr/abs-1712-04432modelpara} This way the model can be split across many devices, and potentially, we can train a more complex model that can provide better results. 

\hspace{0.5cm} After getting a well-performing giant model, we can distill its knowledge to small-scale, easy-to-deploy specialist models.\cite{hinton2015distilling} After all the specialist models are trained, we can train a gated model on the giant model, distributing the data points to its corresponding specialists. This essentially composes a "mixture of experts" model\cite{MoE}, which could deviate from the original intent of the challenge host. However, it sounds to us a very effective solution to boost up the segmentation accuracy without sacrificing much of the inference speed.
\subsection{Contextual Information}
\hspace{0.5cm} Another compelling reason behind the inability of the models to perform well is, perhaps, innate to the problem at hand. One single model, with its predefined architecture, may fail to capture the variation of all the tissue types. That is to say, with the limited computation resources and strict requirements on the inference speed, it is impossible to have a universal model working well on all tissue types.

\hspace{0.5cm} To address this issue, we found Knowledge Embedding Network\cite{10.1007/978-3-031-20074-8_28KEN}, which follows dictionary learning principles to carefully select a collection of vocabularies and incorporate the context information into the inference layers using that collection. The model, trained for several epochs, was not satisfying to us concerning the training and validation loss. We encourage further investigation.

\section{Conclusion}

\hspace{0.5cm} In response to the growing demand for robust and efficient lesion segmentation models across various tissue types, a comparative research study was conducted as part of the Universal Lesion Segmentation Challenge 2023. We aimed to develop a model capable of universal lesion segmentation while maintaining fast inference times. Several state-of-the-art architectures were evaluated, including nnUNetv2, DeepLabV3+, Medical Transformer, SwinUnet, and TransUNet. Most of them did not perform well, and none of the experiments beat the baseline. Problems involved a lack of RAM in initializing models, insufficient segmentation accuracies, and etc. Based on our results, we continued training TransUNet and included a qualitative demonstration of its inferences on some slices. Finally, we discussed what could be done to potentially have a better model for the universal segmentation task.

\newpage

\bibliography{main}
\bibliographystyle{plainnat}

\end{document}