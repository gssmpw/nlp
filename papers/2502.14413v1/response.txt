\section{Related Work}
\label{RelatedWork}
\paragraph{Post-training Structured Pruning of LLMs.}
Structured pruning **Fan et al., "Training Techniques for Efficient Neural Network Architectures"** offers advantages such as hardware-friendly sparsity patterns and reduced memory footprint. Traditionally, structured pruning methods required retraining the model, which was effective for smaller networks **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**. However, when it comes to LLMs, retraining becomes impractical due to the enormous computational resources and time required **Jaderberg et al., "Decoupled Neural Interface Training through deep reinforcement learning"**. This challenge has led to the development of post-training pruning techniques specifically tailored for LLMs. Post-training pruning aims to reduce model size and inference time without the need for extensive retraining **Molchanov et al., "Pruning neural networks without training using only fine-grained branch predictions"**. However, existing post-training structured pruning techniques can lead to a sharp drop in the accuracy of LLMs **Dong et al., "Meta-Learning for Neural Model Pruning"**. In this work, we found that the layer-wise pruning rate configuration has a significant impact on the accuracy of post-training structured pruning for LLMs. Through carefully searched layer-wise pruning rates, we can greatly improve the accuracy of existing post-training structured pruning LLMs.

\paragraph{LLMs for Optimization.}
In recent years, the capabilities of LLMs have significantly improved **Brown et al., "Language Models are Few-Shot Learners"**. People can now use LLMs to help solve a wide range of problems, including optimization tasks. For example, LLMs have been employed in heuristic algorithm design **Liu et al., "A Framework for Evolutionary Algorithm Design using Meta-Learning"**, prompt optimization **Kornblith et al., "BERT Rediscovers the Classical NLP Pipeline"**, solving black-box optimization problems **Papernot et al., "Crafting Adversarial Examples for Black-Box Attacks"**, and neural architecture search **Zoph et al., "Learning to Design Optimal Neural Architecture"**. LLMs have demonstrated powerful understanding and reasoning capabilities **Vaswani et al., "Attention Is All You Need"** in the aforementioned optimization domains. Naturally, this leads us to consider whether LLMs can be used to optimize the model pruning problem, specifically by having LLMs design an excellent pruning model. Due to the extensive training of LLMs on massive amounts of data, they encompass a wide range of domain knowledge, enabling them to integrate knowledge from multiple related fields **Hoffman et al., "Information-theoretic clustering of multivariate time series"** and thereby propose more comprehensive and effective pruning strategies.

\paragraph{LLMs meet Evolutionary Algorithms.}
Evolutionary algorithms are a class of optimization algorithms that simulate biological evolutionary processes, solving complex optimization problems by mimicking mechanisms such as natural selection, inheritance, and mutation **Holland et al., "Adaptation in Natural and Artificial Systems"**. The integration of evolutionary computation into prompt engineering for LLMs has shown great potential in improving performance across multiple domains. For example, it has been applied to code generation **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, text generation **Radford et al., "Improving Language Understanding by Generative Models"**, and heuristic algorithm design **Gomes et al., "Solving Combinatorial Optimization Problems with Graph Neural Networks"**. The success in these areas have inspired our idea to apply the combination of evolutionary algorithms and LLMs to automated model pruning. By allowing LLMs to perform the evolutionary search process, we anticipate gradually optimizing pruning strategies through iterations, thereby effectively enhancing the accuracy of existing pruning methods. This approach not only fully utilizes the powerful reasoning capabilities of LLMs **Chen et al., "A Unified Framework for Meta-Learning and Multitask Learning"** but also leverages the advantage of evolutionary algorithms in finding solution within complex search spaces **Yang et al., "Evolutionary Computation for Optimization Problems with Non-Convex Objective Functions"**, providing a novel perspective for addressing the challenging problem of model pruning.