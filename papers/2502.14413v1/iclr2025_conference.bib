@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@misc{llama3,
  author = {Meta},
  title = {LLaMA3},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/meta-llama/llama3}}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  pages={7432--7439},
  year={2020}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{gao2021framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  year={2021}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{zhang2023dynamic,
  title={Dynamic sparse no training: Training-free fine-tuning for sparse llms},
  author={Zhang, Yuxin and Zhao, Lirui and Lin, Mingbao and Sun, Yunyun and Yao, Yiwu and Han, Xingjia and Tanner, Jared and Liu, Shiwei and Ji, Rongrong},
  journal={arXiv preprint arXiv:2310.08915},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{deepsparse,
  author = {NeuralMagic},
  title = {NeuralMagic DeepSparse Inference Engine},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/neuralmagic/deepsparse}}
}

@misc{gpt4o,
  author = {OpenAI},
  title = {GPT4-o},
  year = {2024},
  howpublished = {\url{https://openai.com/index/hello-gpt-4o}}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  journal={arXiv preprint arXiv:2310.05175},
  year={2023}
}

@article{guo2024ebft,
  title={EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs},
  author={Guo, Song and Wu, Fan and Zhang, Lei and Zheng, Xiawu and Zhang, Shengchuan and Chao, Fei and Shi, Yiyu and Ji, Rongrong},
  journal={arXiv preprint arXiv:2402.12419},
  year={2024}
}

@article{zheng2021information,
  title={An information theory-inspired strategy for automatic network pruning},
  author={Zheng, Xiawu and Ma, Yuexiao and Xi, Teng and Zhang, Gang and Ding, Errui and Li, Yuchao and Chen, Jie and Tian, Yonghong and Ji, Rongrong},
  journal={arXiv preprint arXiv:2108.08532},
  year={2021}
}

@article{virtanen2020scipy,
  title={SciPy 1.0: fundamental algorithms for scientific computing in Python},
  author={Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and others},
  journal={Nature methods},
  volume={17},
  number={3},
  pages={261--272},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={URL https://lmsys. org/blog/2023-03-30-vicuna},
  volume={3},
  number={5},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{zhu2023survey,
  title={A survey on model compression for large language models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2308.07633},
  year={2023}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@inproceedings{agarwal2024policy,
  title={On-policy distillation of language models: Learning from self-generated mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Garea, Sabela Ramos and Geist, Matthieu and Bachem, Olivier},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{
huang2024billm,
title={Bi{LLM}: Pushing the Limit of Post-Training Quantization for {LLM}s},
author={Wei Huang and Yangdong Liu and Haotong Qin and Ying Li and Shiming Zhang and Xianglong Liu and Michele Magno and XIAOJUAN QI},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=qOl2WWOqFg}
}

@inproceedings{
egiazarian2024extreme,
title={Extreme Compression of Large Language Models via Additive Quantization},
author={Vage Egiazarian and Andrei Panferov and Denis Kuznedelev and Elias Frantar and Artem Babenko and Dan Alistarh},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=5mCaITRTmO}
}

@inproceedings{
zhang2024lqer,
title={{LQER}: Low-Rank Quantization Error Reconstruction for {LLM}s},
author={Cheng Zhang and Jianyi Cheng and George Anthony Constantinides and Yiren Zhao},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=dh8k41g775}
}

@inproceedings{
wan2024knowledge,
title={Knowledge Fusion of Large Language Models},
author={Fanqi Wan and Xinting Huang and Deng Cai and Xiaojun Quan and Wei Bi and Shuming Shi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jiDsk12qcz}
}

@inproceedings{
ko2024distillm,
title={Disti{LLM}: Towards Streamlined Distillation for Large Language Models},
author={Jongwoo Ko and Sungnyun Kim and Tianyi Chen and Se-Young Yun},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=lsHZNNoC7r}
}

@article{li2024nuteprune,
  title={NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models},
  author={Li, Shengrui and Han, Xueting and Bai, Jing},
  journal={arXiv preprint arXiv:2402.09773},
  year={2024}
}

@article{li2024lorap,
  title={LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models},
  author={Li, Guangyan and Tang, Yongqiang and Zhang, Wensheng},
  journal={arXiv preprint arXiv:2404.09695},
  year={2024}
}

@article{song2024sleb,
  title={SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks},
  author={Song, Jiwon and Oh, Kyungseok and Kim, Taesu and Kim, Hyungjun and Kim, Yulhwa and Kim, Jae-Joon},
  journal={arXiv preprint arXiv:2402.09025},
  year={2024}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{ma2024affinequant,
  title={AffineQuant: Affine Transformation Quantization for Large Language Models},
  author={Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Ling, Feng and Xiao, Xuefeng and Wang, Rui and Wen, Shilei and Chao, Fei and Ji, Rongrong},
  journal={arXiv preprint arXiv:2403.12544},
  year={2024}
}

@article{agarwal2023gkd,
  title={Gkd: Generalized knowledge distillation for auto-regressive sequence models},
  author={Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  journal={arXiv preprint arXiv:2306.13649},
  year={2023}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}

@inproceedings{gu2023minillm,
  title={MiniLLM: Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{zhang2023pruning,
  title={Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
  journal={arXiv preprint arXiv:2305.18403},
  year={2023}
}

@article{guo2023compresso,
  title={Compresso: Structured pruning with collaborative prompting learns compact large language models},
  author={Guo, Song and Xu, Jiahang and Zhang, Li Lyna and Yang, Mao},
  journal={arXiv preprint arXiv:2310.05015},
  year={2023}
}

@article{chen2024compressing,
  title={Compressing large language models by streamlining the unimportant layer},
  author={Chen, Xiaodong and Hu, Yuxuan and Zhang, Jing},
  journal={arXiv preprint arXiv:2403.19135},
  year={2024}
}

@article{zhao2024apt,
  title={Apt: Adaptive pruning and tuning pretrained language models for efficient training and inference},
  author={Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing},
  journal={arXiv preprint arXiv:2401.12200},
  year={2024}
}

@article{xu2024besa,
  title={BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation},
  author={Xu, Peng and Shao, Wenqi and Chen, Mengzhao and Tang, Shitao and Zhang, Kaipeng and Gao, Peng and An, Fengwei and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2402.16880},
  year={2024}
}

@article{chen2023lorashear,
  title={Lorashear: Efficient large language model structured pruning and knowledge recovery},
  author={Chen, Tianyi and Ding, Tianyu and Yadav, Badal and Zharkov, Ilya and Liang, Luming},
  journal={arXiv preprint arXiv:2310.18356},
  year={2023}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{xu2023compress,
  title={Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt},
  author={Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali},
  journal={arXiv preprint arXiv:2305.11186},
  year={2023}
}

@inproceedings{zhang2023adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{pfeiffer2020adapterfusion,
  title={Adapterfusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}

@inproceedings{gretton2005measuring,
  title={Measuring statistical dependence with Hilbert-Schmidt norms},
  author={Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  booktitle={International conference on algorithmic learning theory},
  pages={63--77},
  year={2005},
  organization={Springer}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

@article{robert1976unifying,
  title={A unifying tool for linear multivariate statistical methods: the RV-coefficient},
  author={Robert, Paul and Escoufier, Yves},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={25},
  number={3},
  pages={257--265},
  year={1976},
  publisher={Oxford University Press}
}

@article{lorenzo2006tucker,
  title={Tucker's congruence coefficient as a meaningful index of factor similarity},
  author={Lorenzo-Seva, Urbano and Ten Berge, Jos MF},
  journal={Methodology},
  volume={2},
  number={2},
  pages={57--64},
  year={2006},
  publisher={Hogrefe \& Huber Publishers}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{zheng2022neural,
  title={Neural architecture search with representation mutual information},
  author={Zheng, Xiawu and Fei, Xiang and Zhang, Lei and Wu, Chenglin and Chao, Fei and Liu, Jianzhuang and Zeng, Wei and Tian, Yonghong and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11912--11921},
  year={2022}
}

@article{hubara2021accelerated,
  title={Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks},
  author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Joseph and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={21099--21111},
  year={2021}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{dong2024pruner,
  title={Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models},
  author={Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2406.02924},
  year={2024}
}

@article{hong2023metagpt,
  title={Metagpt: Meta programming for multi-agent collaborative framework},
  author={Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and others},
  journal={arXiv preprint arXiv:2308.00352},
  year={2023}
}

@article{holland1992genetic,
  title={Genetic algorithms},
  author={Holland, John H},
  journal={Scientific american},
  volume={267},
  number={1},
  pages={66--73},
  year={1992},
  publisher={JSTOR}
}

@article{liu2024evolutionvit,
  title={EvolutionViT: Multi-objective evolutionary vision transformer pruning under resource constraints},
  author={Liu, Lei and Yen, Gary G and He, Zhenan},
  journal={Information Sciences},
  pages={121406},
  year={2024},
  publisher={Elsevier}
}

@article{li2022eapruning,
  title={Eapruning: evolutionary pruning for vision transformers and cnns},
  author={Li, Qingyuan and Zhang, Bo and Chu, Xiangxiang},
  journal={arXiv preprint arXiv:2210.00181},
  year={2022}
}

@article{salehinejad2021edropout,
  title={Edropout: Energy-based dropout and pruning of deep neural networks},
  author={Salehinejad, Hojjat and Valaee, Shahrokh},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={33},
  number={10},
  pages={5279--5292},
  year={2021},
  publisher={IEEE}
}

@inproceedings{yang2023global,
  title={Global vision transformer pruning with hessian-aware saliency},
  author={Yang, Huanrui and Yin, Hongxu and Shen, Maying and Molchanov, Pavlo and Li, Hai and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18547--18557},
  year={2023}
}

@article{cheng2024survey,
  title={A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations},
  author={Cheng, Hongrong and Zhang, Miao and Shi, Javen Qinfeng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{back1997handbook,
  title={Handbook of evolutionary computation},
  author={B{\"a}ck, Thomas and Fogel, David B and Michalewicz, Zbigniew},
  journal={Release},
  volume={97},
  number={1},
  pages={B1},
  year={1997}
}

@article{madani2023large,
  title={Large language models generate functional protein sequences across diverse families},
  author={Madani, Ali and Krause, Ben and Greene, Eric R and Subramanian, Subu and Mohr, Benjamin P and Holton, James M and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z and Socher, Richard and others},
  journal={Nature Biotechnology},
  volume={41},
  number={8},
  pages={1099--1106},
  year={2023},
  publisher={Nature Publishing Group}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11264--11272},
  year={2019}
}

@article{minitron2024,
  title={Compact language models via pruning and knowledge distillation},
  author={Muralidharan, Saurav and Sreenivas, Sharath Turuvekere and Joshi, Raviraj and Chochowski, Marcin and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan and Kautz, Jan and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2407.14679},
  year={2024}
}

@article{hou2020dynabert,
  title={Dynabert: Dynamic bert with adaptive width and depth},
  author={Hou, Lu and Huang, Zhiqi and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9782--9793},
  year={2020}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@inproceedings{liu2024evolution,
  title={Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model},
  author={Liu, Fei and Xialiang, Tong and Yuan, Mingxuan and Lin, Xi and Luo, Fu and Wang, Zhenkun and Lu, Zhichao and Zhang, Qingfu},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{
yang2024large,
title={Large Language Models as Optimizers},
author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V Le and Denny Zhou and Xinyun Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Bb4VGOWELI}
}

@inproceedings{liu2024evolutionary,
  title={Large language models as evolutionary optimizers},
  author={Liu, Shengcai and Chen, Caishun and Qu, Xinghua and Tang, Ke and Ong, Yew-Soon},
  booktitle={2024 IEEE Congress on Evolutionary Computation (CEC)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@inproceedings{wang2020pruning,
  title={Pruning from scratch},
  author={Wang, Yulong and Zhang, Xiaolu and Xie, Lingxi and Zhou, Jun and Su, Hang and Zhang, Bo and Hu, Xiaolin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  pages={12273--12280},
  year={2020}
}


@article{chen2021chasing,
  title={Chasing sparsity in vision transformers: An end-to-end exploration},
  author={Chen, Tianlong and Cheng, Yu and Gan, Zhe and Yuan, Lu and Zhang, Lei and Wang, Zhangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19974--19988},
  year={2021}
}

@inproceedings{lange2024large,
  title={Large language models as evolution strategies},
  author={Lange, Robert and Tian, Yingtao and Tang, Yujin},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  pages={579--582},
  year={2024}
}

@inproceedings{
song2024position,
title={Position: Leverage Foundational Models for Black-Box Optimization},
author={Xingyou Song and Yingtao Tian and Robert Tjarko Lange and Chansoo Lee and Yujin Tang and Yutian Chen},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=ea2MgKn3sV}
}

@article{romera2024mathematical,
  title={Mathematical discoveries from program search with large language models},
  author={Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M Pawan and Dupont, Emilien and Ruiz, Francisco JR and Ellenberg, Jordan S and Wang, Pengming and Fawzi, Omar and others},
  journal={Nature},
  volume={625},
  number={7995},
  pages={468--475},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{eiben2015evolutionary,
  title={What is an evolutionary algorithm?},
  author={Eiben, Agoston E and Smith, James E and Eiben, AE and Smith, JE},
  journal={Introduction to evolutionary computing},
  pages={25--48},
  year={2015},
  publisher={Springer}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{whitley2001overview,
  title={An overview of evolutionary algorithms: practical issues and common pitfalls},
  author={Whitley, Darrell},
  journal={Information and software technology},
  volume={43},
  number={14},
  pages={817--831},
  year={2001},
  publisher={Elsevier}
}

@article{bartz2014evolutionary,
  title={Evolutionary algorithms},
  author={Bartz-Beielstein, Thomas and Branke, J{\"u}rgen and Mehnen, J{\"o}rn and Mersmann, Olaf},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={4},
  number={3},
  pages={178--195},
  year={2014},
  publisher={Wiley Online Library}
}

@article{zheng2023can,
  title={Can gpt-4 perform neural architecture search?},
  author={Zheng, Mingkai and Su, Xiu and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang and Albanie, Samuel},
  journal={arXiv preprint arXiv:2304.10970},
  year={2023}
}

@article{chen2024evoprompting,
  title={EvoPrompting: language models for code-level neural architecture search},
  author={Chen, Angelica and Dohan, David and So, David},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{
liu2024large,
title={Large Language Models to Enhance Bayesian Optimization},
author={Tennison Liu and Nicol{\'a}s Astorga and Nabeel Seedat and Mihaela van der Schaar},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=OOxotBmGol}
}

@inproceedings{liu2024llmsasevolutionaryoptimizers,
  title={Large language models as evolutionary optimizers},
  author={Liu, Shengcai and Chen, Caishun and Qu, Xinghua and Tang, Ke and Ong, Yew-Soon},
  booktitle={2024 IEEE Congress on Evolutionary Computation (CEC)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@inproceedings{guo2020dmcp,
  title={Dmcp: Differentiable markov channel pruning for neural networks},
  author={Guo, Shaopeng and Wang, Yujie and Li, Quanquan and Yan, Junjie},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1539--1547},
  year={2020}
}

@inproceedings{liventsev2023fully,
  title={Fully autonomous programming with large language models},
  author={Liventsev, Vadim and Grishina, Anastasiia and H{\"a}rm{\"a}, Aki and Moonen, Leon},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={1146--1155},
  year={2023}
}

@article{guo2023connecting,
  title={Connecting large language models with evolutionary algorithms yields powerful prompt optimizers},
  author={Guo, Qingyan and Wang, Rui and Guo, Junliang and Li, Bei and Song, Kaitao and Tan, Xu and Liu, Guoqing and Bian, Jiang and Yang, Yujiu},
  journal={arXiv preprint arXiv:2309.08532},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@incollection{lehman2023evolution,
  title={Evolution through large models},
  author={Lehman, Joel and Gordon, Jonathan and Jain, Shawn and Ndousse, Kamal and Yeh, Cathy and Stanley, Kenneth O},
  booktitle={Handbook of Evolutionary Machine Learning},
  pages={331--366},
  year={2023},
  publisher={Springer}
}

@inproceedings{chin2020towards,
  title={Towards efficient model compression via learned global ranking},
  author={Chin, Ting-Wu and Ding, Ruizhou and Zhang, Cha and Marculescu, Diana},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1518--1528},
  year={2020}
}

@article{zhang2023automl,
  title={Automl-gpt: Automatic machine learning with gpt},
  author={Zhang, Shujian and Gong, Chengyue and Wu, Lemeng and Liu, Xingchao and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:2305.02499},
  year={2023}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@inproceedings{
ashkboos2024slicegpt,
title={Slice{GPT}: Compress Large Language Models by Deleting Rows and Columns},
author={Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=vXxardq6db}
}

@inproceedings{
shao2024omniquant,
title={OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models},
author={Wenqi Shao and Mengzhao Chen and Zhaoyang Zhang and Peng Xu and Lirui Zhao and Zhiqian Li and Kaipeng Zhang and Peng Gao and Yu Qiao and Ping Luo},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=8Wuvhh0LYW}
}

@inproceedings{
gu2024minillm,
title={Mini{LLM}: Knowledge Distillation of Large Language Models},
author={Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=5h0qf7IBZZ}
}

@inproceedings{
ma2024affinequant,
title={AffineQuant: Affine Transformation Quantization for Large Language Models},
author={Yuexiao Ma and Huixia Li and Xiawu Zheng and Feng Ling and Xuefeng Xiao and Rui Wang and Shilei Wen and Fei Chao and Rongrong Ji},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=of2rhALq8l}
}

@inproceedings{
frantar2023optq,
title={{OPTQ}: Accurate Quantization for Generative Pre-trained Transformers},
author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=tcbBPnfwxS}
}

@inproceedings{
agarwal2024onpolicy,
title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
author={Rishabh Agarwal and Nino Vieillard and Yongchao Zhou and Piotr Stanczyk and Sabela Ramos Garea and Matthieu Geist and Olivier Bachem},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3zKtaqxLhW}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{kim2024shortened,
  title={Shortened llama: A simple depth pruning for large language models},
  author={Kim, Bo-Kyeong and Kim, Geonmin and Kim, Tae-Ho and Castells, Thibault and Choi, Shinkook and Shin, Junho and Song, Hyoung-Kyu},
  journal={arXiv preprint arXiv:2402.02834},
  year={2024}
}



@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}

@inproceedings{an2024fluctuation,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={10865--10873},
  year={2024}
}

@article{muralidharan2024compact,
  title={Compact Language Models via Pruning and Knowledge Distillation},
  author={Muralidharan, Saurav and Sreenivas, Sharath Turuvekere and Joshi, Raviraj and Chochowski, Marcin and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan and Kautz, Jan and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2407.14679},
  year={2024}
}

@inproceedings{liu2019metapruning,
  title={Metapruning: Meta learning for automatic neural network channel pruning},
  author={Liu, Zechun and Mu, Haoyuan and Zhang, Xiangyu and Guo, Zichao and Yang, Xin and Cheng, Kwang-Ting and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3296--3305},
  year={2019}
}

@article{lin2020channel,
  title={Channel pruning via automatic structure search},
  author={Lin, Mingbao and Ji, Rongrong and Zhang, Yuxin and Zhang, Baochang and Wu, Yongjian and Tian, Yonghong},
  journal={arXiv preprint arXiv:2001.08565},
  year={2020}
}

@misc{llama31,
  author = {Meta},
  title = {LLaMA3},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/meta-llama/llama3}}
}

@article{shang2022neural,
  title={Neural network pruning by cooperative coevolution},
  author={Shang, Haopu and Wu, Jia-Liang and Hong, Wenjing and Qian, Chao},
  journal={arXiv preprint arXiv:2204.05639},
  year={2022}
}

@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={784--800},
  year={2018}
}