\section{Related Work}
\label{RelatedWork}
\paragraph{Post-training Structured Pruning of LLMs.}
Structured pruning \citep{ma2023llm} offers advantages such as hardware-friendly sparsity patterns and reduced memory footprint. Traditionally, structured pruning methods required retraining the model, which was effective for smaller networks \citep{molchanov2016pruning, hou2020dynabert}. However, when it comes to LLMs, retraining becomes impractical due to the enormous computational resources and time required \citep{xia2023sheared, minitron2024}. This challenge has led to the development of post-training pruning techniques specifically tailored for LLMs. Post-training pruning aims to reduce model size and inference time without the need for extensive retraining \citep{ma2023llm, sun2023simple}. However, existing post-training structured pruning techniques can lead to a sharp drop in the accuracy of LLMs \citep{ma2023llm, an2024fluctuation}. In this work, we found that the layer-wise pruning rate configuration has a significant impact on the accuracy of post-training structured pruning for LLMs. Through carefully searched layer-wise pruning rates, we can greatly improve the accuracy of existing post-training structured pruning LLMs.

\paragraph{LLMs for Optimization.}
In recent years, the capabilities of LLMs have significantly improved \citep{naveed2023comprehensive}. People can now use LLMs to help solve a wide range of problems, including optimization tasks. For example, LLMs have been employed in heuristic algorithm design \citep{liu2024evolution, romera2024mathematical}, prompt optimization \citep{yang2024large}, solving black-box optimization problems \citep{liu2024large, song2024position}, and neural architecture search \citep{zheng2023can, chen2024evoprompting}. LLMs have demonstrated powerful understanding and reasoning capabilities \citep{brown2020language, wei2022chain} in the aforementioned optimization domains. Naturally, this leads us to consider whether LLMs can be used to optimize the model pruning problem, specifically by having LLMs design an excellent pruning model. Due to the extensive training of LLMs on massive amounts of data, they encompass a wide range of domain knowledge, enabling them to integrate knowledge from multiple related fields \citep{madani2023large, hong2023metagpt} and thereby propose more comprehensive and effective pruning strategies.

\paragraph{LLMs meet Evolutionary Algorithms.}
Evolutionary algorithms are a class of optimization algorithms that simulate biological evolutionary processes, solving complex optimization problems by mimicking mechanisms such as natural selection, inheritance, and mutation \citep{eiben2015evolutionary, bartz2014evolutionary}. The integration of evolutionary computation into prompt engineering for LLMs has shown great potential in improving performance across multiple domains. For example, it has been applied to code generation \citep{liventsev2023fully, lehman2023evolution}, text generation \citep{guo2023connecting, xu2023wizardlm}, and heuristic algorithm design \citep{liu2024evolution, romera2024mathematical}. The success in these areas have inspired our idea to apply the combination of evolutionary algorithms and LLMs to automated model pruning. By allowing LLMs to perform the evolutionary search process, we anticipate gradually optimizing pruning strategies through iterations, thereby effectively enhancing the accuracy of existing pruning methods. This approach not only fully utilizes the powerful reasoning capabilities of LLMs \citep{huang2022towards} but also leverages the advantage of evolutionary algorithms in finding solution within complex search spaces \citep{whitley2001overview}, providing a novel perspective for addressing the challenging problem of model pruning.