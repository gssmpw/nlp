
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage{tablefootnote}
\usepackage{enumitem}
\usepackage{url}
\usepackage{pifont}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tcolorbox}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\yellow}[1]{\textcolor{yellow!70!brown}{#1}}


\newcommand{\gr}{\rowcolor[gray]{.95}}
\newcommand{\xmark}{\ding{55}}
\usepackage{colortbl} 
\definecolor{mygray}{gray}{0.95} 


\title{Towards Efficient Automatic Self-Pruning of Large Language Models}

\author{%
  Weizhong Huang$^{1}$ \quad Yuxin Zhang$^{1}$ \quad Xiawu Zheng$^{1,2}$ \quad Fei Chao$^{1}$ \quad \textbf{Rongrong Ji}$^{1,2}$\thanks{Corresponding author: rrji@xmu.edu.cn}
  \\[0.2cm] 
  $^1$Key Laboratory of Multimedia Trusted Perception and Efficient Computing, \\ \; Ministry of Education of China, Xiamen University, 361005, P.R. China. \; \\ $^2$ Institute of Artificial Intelligence, Xiamen University\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}


\maketitle

\begin{abstract}
Despite exceptional capabilities, Large Language Models (LLMs) still face deployment challenges due to their enormous size. 
Post-training structured pruning is a promising solution that prunes LLMs without the need for retraining, reducing computational overhead, and it is hardware-deployment friendly.
However, the training-free nature of post-training structured pruning leads to significant performance degradation. 
We argue that the key to mitigating this issue lies in accurately determining the pruning rate for each layer. 
Meanwhile, we find that LLMs may have prior knowledge about their own redundancy. 
Based on this insight, we introduce \textbf{Self-Pruner} an end-to-end automatic self-pruning framework for LLMs, which efficiently search layer-wise pruning rates.
Specifically, \textbf{Self-Pruner} leverages LLMs to autonomously execute the entire evolutionary search process to search for pruning rate configurations. 
In this process, LLMs are used to generate populations, select parent solutions from the current population, and perform crossover and mutation operations to produce offspring solutions. 
In this way, LLMs automatically generate and evaluate a large number of candidate solutions, effectively converging to find the pruning rate configurations with minimal human intervention.
Extensive experiments demonstrate \textbf{Self-Pruner}'s better performance compared to existing state-of-the-art methods. 
Notably, \textbf{Self-Pruner} prunes LLaMA-2-70B to 49B level with only 0.80$\%$ drop in accuracy across seven commonsense reasoning tasks, achieving a 1.39$\times$ speedup on NVIDIA A100 80GB GPU. Further pruning to 35B level resulted in only a 3.80$\%$ decrease in accuracy while obtaining a 1.70$\times$ speedup.
\end{abstract}

\section{Introduction}
In recent years, with the rapid advancement of Large Language Models (LLMs) \citep{zhang2022opt, touvron2023llama, touvron2023llama2}, these models have achieved remarkable performance in language understanding and generation \citep{brown2020language, wei2022chain, lewis2020retrieval}. However, the dramatic increase in the number of parameters has led to significant rises in computational resource consumption and deployment costs \citep{zhu2023survey}. To maintain model performance while mitigating computational complexity, various model compression techniques such as pruning \citep{frantar2023sparsegpt, sun2023simple, ashkboos2024slicegpt, dong2024pruner}, quantization \citep{egiazarian2024extreme, xiao2023smoothquant, huang2024billm, shao2024omniquant}, and knowledge distillation \citep{agarwal2024onpolicy, gu2024minillm, ko2024distillm, wan2024knowledge} have emerged. Among these, structured pruning \citep{ma2023llm, li2024lorap, an2024fluctuation} stands out as it not only significantly reduces computational overhead and memory usage but also enhances inference speed on various hardware platforms. This dual benefit of efficiency and accelerated inference has paved the way for more widespread practical applications of LLMs \citep{ma2023llm, muralidharan2024compact}.

Traditional structured pruning methods often involve retraining the model, including but not limited to training from random initialization \citep{wang2020pruning}, fine-tuning the pruned model \citep{hou2020dynabert}, or performing iterative pruning \citep{zhu2017prune, molchanov2019importance}. However, the inherent complexity of LLMs and their substantial demands for computational resources and data make these traditional retraining-required structured pruning strategies difficult to implement in practice \citep{xia2023sheared, minitron2024}. 
As a result, post-training pruning has emerged as an increasingly important alternative. This approach is particularly advantageous when pruning LLMs due to its minimal resource requirements \citep{zhang2023dynamic, dong2024pruner}.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=\linewidth]{figures/fig1.pdf}
\end{center}
\caption{\label{fig:framework} An overview of Self-Pruner. Self-Pruner first instructs LLMs with prompts to generate layer-wise pruning rates as the initial population. Next, Self-Pruner uses these layer-wise pruning rates to prune the model, evaluating the pruned model to obtain each individual's fitness. Then, Self-Pruner instructs LLMs to select parent individuals for crossover and mutation, generating offspring. This process is repeated for N evolutionary iterations to obtain the final pruned model.}
\end{figure}

However, it is precisely due to the training-free nature of post-training pruning that leads to a severe decrease in the accuracy, especially in structured pruning. We observe that this is largely caused by the inaccurate setting of layer-wise pruning rate. A straightforward way to set the layer-wise pruning rate is to apply a uniform pruning rate to each layer \citep{sun2023simple, frantar2023sparsegpt}. For instance, LLM-Pruner \citep{ma2023llm} applies the same pruning rate to all the pruned layers. However, this approach is suboptimal, as the contribution of each layer to the final accuracy varies significantly, applying a uniform pruning rate across all layers risks removing important weights \citep{cheng2024survey, yang2023global}. OWL \citep{yin2023outlier} has already recognized this issue and adopted a heuristic metric to set the pruning rate of LLMs inversely proportional to the observed ratio of abnormal activations within each layer, thereby achieving non-uniform pruning of LLMs. However, the OWL method relies on manually designed importance metrics, requiring human expert involvement and tedious iterative experimentation. Additionally, it demands meticulous tuning of hyperparameters to achieve best performance, making it inefficient.

In this paper, we propose an end-to-end automatic LLMs pruning framework named Self-Pruner, which efficiently searches for layer-wise pruning rates, significantly enhancing the quality of post-training pruning for LLMs. Self-Pruner is a framework that uses evolutionary algorithm \citep{holland1992genetic, back1997handbook} to search for layer-wise pruning rates. Evolutionary algorithms have been used to automate pruning of CNNs \citep{liu2019metapruning, salehinejad2021edropout} and Transformers \citep{li2022eapruning, liu2024evolutionvit}. However, the above algorithm requires numerous iterations to converge to the final solution, which is impractical for LLMs with billions of parameters, as evaluating the performance of pruned LLMs is highly time-consuming \citep{chang2024survey}. Meanwhile, the success of evolutionary algorithms for automate pruning depends largely on the design of the algorithm. For different pruning networks and compression constraints, specialized genetic operators (such as crossover and mutation) need to be customized \citep{liu2019metapruning, shang2022neural}. Designing these evolutionary algorithms manually is often time-consuming and requires extensive experience and knowledge \citep{liu2024llmsasevolutionaryoptimizers, lange2024large}.

To accelerate the convergence of the evolutionary search and achieve automation in algorithm design, we chose to have the LLM execute the entire evolutionary algorithm process itself. The insight behind this approach is that we found LLMs may possess prior knowledge about their own redundancy \citep{dong2022survey, zhang2023automl, zheng2023can}. We can take advantage of this by having LLMs generate and evaluate feasible solutions. Therefore, Self-Pruner uses LLMs to generate the initial population, constructs a prompt to guide LLMs to select parent solutions from the current population and perform crossover and mutation to generate offspring solutions. These new solutions are then evaluated and added to the population for the next round by LLMs. In this way, LLMs can automatically generate and evaluate a large number of candidate solutions, quickly converging to find the superior pruning rate configurations with minimal human intervention. Notably, we achieved self-pruning of LLMs through evolutionary search process, marking an important step towards the fully automated compression of LLMs.

Extensive experiments on language modeling and zero-shot tasks demonstrate that Self-Pruner achieves better performance compared to existing post-training pruning methods. Using the Self-Pruner method to prune the LLaMA-2-70B \citep{touvron2023llama2} model, we obtained a 49B model that achieved only 0.80$\%$ drop in average zero-shot accuracy across seven commonsense reasoning tasks as LLaMA-2-70B with a 1.39$\times$ increase in inference speed on GPU. Further pruning resulted in a 35B model shows only a 3.80$\%$ decrease in accuracy compared to LLaMA-2-70B while achieving a 1.70$\times$ increase in inference speed. Both results represent state-of-the-art performance in existing post-training structured LLMs pruning.
The main contributions of this paper are summarized as follows:
\begin{itemize}[leftmargin=*,itemsep=0.4ex,parsep=-0.1ex]
    \item We propose a novel end-to-end automatic pruning framework that utilizes LLMs to efficiently search for layer-wise pruning rate without human intervention, marking significant step toward fully automated LLMs compression.

    \item We propose a novel method which leverages LLMs to execute the entire evolutionary search process, including population generation, selection, crossover, and mutation, enabling the self-pruning of LLMs.
    
    \item Extensive experiments show that Self-Pruner outperforms existing post-training pruning methods, achieving competitive accuracy, while significantly reducing model size.
\end{itemize}


\section{Related Work}
\label{RelatedWork}
\paragraph{Post-training Structured Pruning of LLMs.}
Structured pruning \citep{ma2023llm} offers advantages such as hardware-friendly sparsity patterns and reduced memory footprint. Traditionally, structured pruning methods required retraining the model, which was effective for smaller networks \citep{molchanov2016pruning, hou2020dynabert}. However, when it comes to LLMs, retraining becomes impractical due to the enormous computational resources and time required \citep{xia2023sheared, minitron2024}. This challenge has led to the development of post-training pruning techniques specifically tailored for LLMs. Post-training pruning aims to reduce model size and inference time without the need for extensive retraining \citep{ma2023llm, sun2023simple}. However, existing post-training structured pruning techniques can lead to a sharp drop in the accuracy of LLMs \citep{ma2023llm, an2024fluctuation}. In this work, we found that the layer-wise pruning rate configuration has a significant impact on the accuracy of post-training structured pruning for LLMs. Through carefully searched layer-wise pruning rates, we can greatly improve the accuracy of existing post-training structured pruning LLMs.

\paragraph{LLMs for Optimization.}
In recent years, the capabilities of LLMs have significantly improved \citep{naveed2023comprehensive}. People can now use LLMs to help solve a wide range of problems, including optimization tasks. For example, LLMs have been employed in heuristic algorithm design \citep{liu2024evolution, romera2024mathematical}, prompt optimization \citep{yang2024large}, solving black-box optimization problems \citep{liu2024large, song2024position}, and neural architecture search \citep{zheng2023can, chen2024evoprompting}. LLMs have demonstrated powerful understanding and reasoning capabilities \citep{brown2020language, wei2022chain} in the aforementioned optimization domains. Naturally, this leads us to consider whether LLMs can be used to optimize the model pruning problem, specifically by having LLMs design an excellent pruning model. Due to the extensive training of LLMs on massive amounts of data, they encompass a wide range of domain knowledge, enabling them to integrate knowledge from multiple related fields \citep{madani2023large, hong2023metagpt} and thereby propose more comprehensive and effective pruning strategies.

\paragraph{LLMs meet Evolutionary Algorithms.}
Evolutionary algorithms are a class of optimization algorithms that simulate biological evolutionary processes, solving complex optimization problems by mimicking mechanisms such as natural selection, inheritance, and mutation \citep{eiben2015evolutionary, bartz2014evolutionary}. The integration of evolutionary computation into prompt engineering for LLMs has shown great potential in improving performance across multiple domains. For example, it has been applied to code generation \citep{liventsev2023fully, lehman2023evolution}, text generation \citep{guo2023connecting, xu2023wizardlm}, and heuristic algorithm design \citep{liu2024evolution, romera2024mathematical}. The success in these areas have inspired our idea to apply the combination of evolutionary algorithms and LLMs to automated model pruning. By allowing LLMs to perform the evolutionary search process, we anticipate gradually optimizing pruning strategies through iterations, thereby effectively enhancing the accuracy of existing pruning methods. This approach not only fully utilizes the powerful reasoning capabilities of LLMs \citep{huang2022towards} but also leverages the advantage of evolutionary algorithms in finding solution within complex search spaces \citep{whitley2001overview}, providing a novel perspective for addressing the challenging problem of model pruning.



\section{Methodology}
\subsection{Preliminaries}\label{sec:Preliminaries}
In post-training pruning of LLMs, a certain proportion of pre-trained weights needs to be removed to obtain the pruned LLMs. Since the redundancy varies across different layers of the LLMs, the number of parameters removed from each layer has a significant impact on the accuracy of the pruned LLMs \citep{yin2023outlier, xu2024besa}. Given a pre-trained LLM with $n$ layers, we define the layer-wise pruning rates as $\boldsymbol{p} = (p_1, p_2, ..., p_n)$, where $0\le p_i\le 1$ represents the pruning rate of the $i$-th layer, i.e., the ratio of the remaining parameters after pruning to the original number of parameters in that layer. Our goal is to find the layer-wise pruning rates $\boldsymbol{p}^*$ such that the pruned LLM achieves the best accuracy on the test set, while ensuring that satisfies the constraint on the average pruning rate, i.e., $\frac{1}{n} \sum_{i=1}^{n}p_i= \beta$, where $\beta$ is the given average pruning rate. This problem can be formulated as an optimization problem as bellow:
\begin{equation}
\begin{aligned}
& (p_1, p_2, ... p_n)^* = \argmax_{p_1, p_2, ... p_n} \text{acc}(\text{LLM}(p_1, p_2, ... p_n)) \\
& s.t. ~~~\frac{1}{n} \sum_{i=1}^{n}p_i = \beta,
\label{pruning_problem}
\end{aligned}
\end{equation}
where $\text{LLM}(\cdot)$ denotes the pruned LLM with layer-wise
pruning rate $(p_1, p_2, ... p_L)$, and $\text{acc}(\cdot)$ refers to the accuracy of the pruned LLM on the test set. Since this optimization problem is generally intractable, we employ evolutionary algorithm to search for $\boldsymbol{p}^*$. Although evolutionary algorithm have been successfully applied to optimize pruning rates in CNNs \citep{liu2019metapruning, lin2020channel, shang2022neural} or Transformers \citep{li2022eapruning, liu2024evolutionvit}, we improve the search process of the evolutionary algorithm by leveraging LLMs. 

\subsection{Self-Pruner Algorithm}\label{sec:Self-PrunerAlgorithm}
The Self-Pruner algorithm framework is illustrated in Figure \ref{fig:framework} and detailed in Algorithm \ref{alg:1}. Overall, Self-Pruner uses LLMs to generate the initial population and employs model perplexity as the fitness metric for individuals. LLMs then select parents for mutation and crossover to generate offspring. This process is repeated for N iterations to obtain the final pruning rate configuration and model fitness.
The Self-Pruner algorithm consists of several stages, which we describe below:

\paragraph{Population initialization.}
Self-Pruner utilizes LLMs to generate the initial population for evolutionary search, a process assisted by carefully constructed prompts. The prompt is shown in Figure \ref{fig:initialization}. Specifically, the prompt consists of two parts:

\begin{itemize}[leftmargin=*,itemsep=0.4ex,parsep=-0.1ex]
\item \textcolor{orange}{\textbf{Problem description and task instruction:}} this part describes the problem that LLMs are instructed to solve, namely to assist in model pruning by outputting layer-wise pruning rate configurations.

\item \textcolor{green}{\textbf{Solution attributes:}} this section specifies some fundamental attributes that the new solutions generated by LLMs should adhere to.
\end{itemize}

\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=gray!8, colframe=gray!70, width=1.\textwidth, size=small]
\textcolor{orange}{\#\#\# \textbf{Problem description and task instruction} \#\#\#}

Let's think step by step! You are helping me prune the \{model\}, aiming to minimize perplexity on the WikiText-2 dataset. The model has \{number of model layers\} transformer layers. Layer-wise pruning rate measures how many parameters are pruned from each layer of the model. Different layers may have different pruning rates based on their importance and contribution to the performance of model. You need to generate \{population size\} valid layer-wise pruning rate configurations. Each configuration should:

\textcolor{green}{\#\#\# \textbf{Solution attributes} \#\#\#}

· Contain \{number of model layers\} decimals between 0 and 1, accurate to 5 decimal places.

· Ensure the average of these numbers equals \{pruning ratio\}.

· Be distinct, starting with "[" and ending with "]".

Your response should only contain the \{population size\} configurations without any additional text.

\textcolor{red}{Note: \{\} is used to indicate placeholders.}
\end{tcolorbox}
\vspace{-1em}
\caption{Prompt for population initialization.}
\label{fig:initialization}
\end{figure}

Leveraging the prior knowledge that LLMs inherently possess about model architecture can generate a high-quality initial population. Compared to random initialization, this method accelerates the search by providing high-quality initial solutions. 

\paragraph{Select Individuals based on Fitness.} 
After generating the layer-wise pruning rate configuration, we determine the number of parameters to prune for each layer based on the layer-wise pruning rate and use the Wanda-sp \citep{an2024fluctuation} metric to identify which neurons within the layer should be pruned.
Additionally, we evaluate the pruned LLM on the WikiText-2 \citep{merity2016pointer} dataset to obtain the perplexity of the pruned LLM, which serves as the fitness metric. In this context, a lower perplexity score indicates higher fitness. We select individuals based on their fitness.

\paragraph{Mutation and Crossover.}

\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=gray!8, colframe=gray!70, width=1.\textwidth, size=small]
\textcolor{orange}{\#\#\# \textbf{Problem description and task instruction} \#\#\#}

Let's think step by step! You will receive \{population size\} lists representing the layer-wise pruning rates of the \{model\} and a fitness value for each list. The lower the fitness value, the better. Your task is to perform the mutation/crossover operation in the evolutionary algorithm to generate new configurations. Each new pruning rate configuration list should:

\textcolor{green}{\#\#\# \textbf{Solution attributes} \#\#\#}

· Contain \{number of model layers\} decimals between 0 and 1, accurate to 5 decimal places.

· Ensure the average of these numbers equals \{pruning ratio\}.

· Be distinct, starting with "[" and ending with "]".

Please provide exactly \{number of mutation/crossover\} new configurations based on the existing data provided below without any additional text.

\textcolor{blue}{\#\#\# \textbf{Current Population and Fitnesses} \#\#\#} 

Here are the existing layer-wise pruning rate configurations and their fitness values:

Configuration1: \{layer-wise pruning rate\}, Fitness1: \{fitness\}

Configuration2: \{layer-wise pruning rate\}, Fitness2: \{fitness\}

...

\textcolor{red}{Note: \{\} is used to indicate placeholders.}
\end{tcolorbox}
\vspace{-1em}
\caption{Prompt for crossover and mutation.}
\label{fig:crossoverandmutation}
\end{figure}

Self-Pruner leverages LLMs to execute key steps of the evolutionary search: parent selection, crossover, and mutation. This process is guided by a carefully crafted prompt, as illustrated in Figure \ref{fig:crossoverandmutation}. The prompt consists of three critical components:

\begin{itemize}[leftmargin=*,itemsep=0.4ex,parsep=-0.1ex]
\item \textcolor{orange}{\textbf{Problem Description and Task Instructions:}}
this part instructs the LLMs to perform parent selection, crossover, and mutation operations to generate new offerspring.
\item \textcolor{green}{\textbf{Solution attributes:}} provides detailed guidelines on the attributes and format requirements that LLMs must adhere to when generating new solutions. This part is consistent with the prompt used in population initialization.
\item \textcolor{blue}{\textbf{Current Population and Fitnesses:}} provide information about the individuals in the current population and their corresponding fitness values, allowing LLMs to select individuals for crossover and mutation.
\end{itemize}

The uniqueness of Self-Pruner lies in its approach of not guiding the LLM through detailed algorithmic steps for precise mutation and crossover operation, but instead using high-level natural language instructions. This approach significantly reduces the human effort and tedious trial-and-error required for designing mutation and crossover operators, enhancing the method's generality and flexibility. Based on its understanding of the problem and the provided context, LLMs autonomously perform selection, crossover, and mutation operations to generate new, potentially  pruning configuration schemes.

We present the specific algorithmic process of Self-Pruner in Algorithm \ref{alg:1}. Self-Pruner begins by initializing a population $\mathcal{G}_0$ of $\mathcal{K}$ layer-wise pruning rate configurations using LLMs and the individuals in the population all satisfy the constraint which the average pruning rate is equal to $\beta$ (Line 1). It then proceeds through $\mathcal{N}$ evolutionary iterations (Line 2-8). In each iteration, the algorithm evaluates the fitness of the individuals (Line 3) and selects the top $\mathcal{K}$ individuals (Line 4). Self-Pruner utilizes LLMs to select individuals for mutation (Line 5) and crossover (Line 6) to generate offspring. These offspring are added to the population for the next selection (Line 7). This iterative process continues until the predefined number of iterations is reached. Ultimately, Self-Pruner outputs the final layer-wise pruning rate configuration (Line 9-10).

\begin{algorithm}[ht]
\caption{Self-Pruner}
\label{alg:1}
\textbf{Hyper Parameters}: Population Size: $\mathcal{K}$, Number of Mutation: $\mathcal{M}$, Number of Crossover: $\mathcal{S}$, Max Number of Iterations: $\mathcal{N}$ . \\
\textbf{Input}: Pre-trained LLM: LLM, Average pruning rate: $\beta$ .\\
\textbf{Output}: The best found pruning rates: $\boldsymbol{p}^*$ with fitness $\text{Fitness}^*$.\\
\begin{algorithmic}[1]
\STATE $\mathcal{G}_0$ = Initialization($\mathcal{K}$), s.t. $\beta$; 
\FOR{$i = 0:\mathcal{N}$}
\STATE \{$\mathcal{G}_i$, $\text{Fitness}$\} = Inference($\text{LLM}(\mathcal{G}_i$));\
\STATE $\mathcal{G}_i$ = Top $\mathcal{K}$(\{$\mathcal{G}_i$, $\text{Fitness}$\});\
\STATE $\mathcal{G}_{mutation}$ = Mutation($\mathcal{G}_{i}, \mathcal{M}$), s.t. $\beta$;\
\STATE $\mathcal{G}_{crossover}$ = Crossover($\mathcal{G}_{i},  \mathcal{S}$), s.t. $\beta$;\
\STATE $\mathcal{G}_{i+1}$ = $\mathcal{G}_{i}$+$\mathcal{G}_{mutation}$ + $\mathcal{G}_{crossover}$;\
\ENDFOR
\STATE $\boldsymbol{p}^*$, $\text{Fitness}^*$= Top1(\{$\mathcal{G}_{\mathcal{N}+1}$, $\text{Fitness}$\});\
\STATE \textbf{return} $\boldsymbol{p}^*$, $\text{Fitness}^*$.\
\end{algorithmic}
\end{algorithm}


\section{Experiments}\label{sec:experiments}
\subsection{Experimental Settings}\label{sec:ExperimentalSettings}
\paragraph{Models.} We implemented our method on LLaMA-1 \citep{touvron2023llama}, LLaMA-2 \citep{touvron2023llama2}, LLaMA-3 \citep{llama3}, LLaMA-3.1 \citep{llama31}, and Vicuna \citep{chiang2023vicuna}, with parameter counts ranging from 7 billion to 70 billion.

\paragraph{Baselines.} We compared our method with two prior state-of-the-art pruning methods: LLM-Pruner \citep{ma2023llm} and Wanda-sp \citep{an2024fluctuation}, where Wanda-sp is the structured pruning extension of the unstructured pruning method Wanda \citep{sun2023simple}. All these methods applied post-training pruning to LLMs without updating the pruned model weights.

\paragraph{Evaluation.} We assessed the perplexity of the pruned LLMs on the WikiText-2 \citep{merity2016pointer} dataset. Additionally, we evaluated the zero-shot commonsense reasoning capability on tasks such as Winogrande \citep{sakaguchi2021winogrande}, HellaSwag \citep{zellers2019hellaswag}, BoolQ \citep{clark2019boolq}, ARC-Easy, ARC-Challenge \citep{clark2018think}, OpenBookQA \citep{mihaylov2018can}, and PIQA \citep{bisk2020piqa}. We utilized lm-eval-harness \citep{gao2021framework} to generate commonsense question-answering results.

\paragraph{Implementation Details.} All implementations were carried out on NVIDIA A100 80GB GPUs. Models with up to 30 billion parameters used a single GPU, while the 70 billion parameter model used two GPUs. The settings of all hyperparameters in evolutionary search are: population size $\mathcal{K}=30$, number of mutation $\mathcal{M}=10$, number of crossover $\mathcal{S}=10$ and max number of iterations $\mathcal{N}=20$. We employed the OpenAI GPT4-o model \citep{gpt4o} to generate solutions for the evolutionary search.

\subsection{Language Modeling}
We report the perplexity of pruned LLMs with pruning rates ranging from 20$\%$ to 50$\%$ on the WikiText-2 \citep{merity2016pointer} dataset in Table \ref{tab:ppl_results}. Self-Pruner significantly outperforms existing post-training pruning techniques, further narrowing the accuracy gap between post-training structured pruned LLMs and the original models, especially under high pruning rate settings. Notably, Self-Pruner is particularly beneficial for larger models, especially LLaMA-2-70B. Using the Self-Pruner method, under the 30$\%$ pruning rate setting, the model's perplexity only increased by 1.88 compared to the original model, and under the 50$\%$ pruning rate setting, the perplexity increased by only 5.96. This comprehensive superiority over existing techniques once again demonstrates that LLMs can leverage their inherent knowledge to design compressed LLMs architectures with good accuracy, proving the feasibility of LLMs automatically performing model compression tasks.

\begin{table*}[t!]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Perplexity of pruned LLMs on WikiText-2 dataset at 20-50$\%$ pruning ratio.}
\vspace{1em}
\small
\label{tab:ppl_results}
\setlength{\tabcolsep}{8pt}
\resizebox{1.0\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|cc|ccc|c|c|c}
\toprule 
 & \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{LLaMA-1} & \multicolumn{3}{c|}{LLaMA-2} & LLaMA-3 & LLaMA-3.1 &Vicuna  \\
\midrule
   Sparsity & Method  & 7B & 13B &7B& 13B & 70B &8B & 8B & 13B \\
\midrule
    0$\%$ & Dense  & 5.68 & 5.09  & 5.12 & 4.57 & 3.12 & 6.05 & 6.18 & 5.94 \\
\midrule
    \multirow{3}{*}{20$\%$} & LLM-Pruner &9.87 & 7.72 & 10.48& 8.00&/ & 12.02& 12.77& 9.95\\
    & Wanda-sp & 13.46& 7.74 & 12.01& 7.45&4.27 &12.61 &11.35 & 9.45\\
     \gr &  \bf\texttt{Self-Pruner} & \bf 9.07 & \bf 7.32 & \bf 9.92 &  \bf 6.46 & \bf 4.24 & \bf 10.35 & \bf10.37  & \bf 7.94\\
\midrule
    \multirow{3}{*}{30$\%$} & LLM-Pruner  & 18.42& 11.47 &17.90 &11.64 & /& 20.94&21.17 & 13.97\\
    & Wanda-sp & 21.02& 10.74 &24.53 & 14.01& 5.10&37.46 & 27.00& 16.10 \\
    \gr & \bf\texttt{Self-Pruner} & \bf 16.64 & \bf 9.92 & \bf16.25  &  \bf  10.26 & \bf5.00 & \bf 16.80 & \bf 16.79 & \bf11.08 \\
\midrule
    \multirow{3}{*}{40$\%$} & LLM-Pruner  &35.82 & 21.73 & 46.32&21.68 &/ & 39.32&44.27 &26.95 \\
    & Wanda-sp & 40.79& 27.53 & 38.65& 69.86&6.74 &71.97 &58.49 &144.68 \\
     \gr & \bf\texttt{Self-Pruner} & \bf 30.17 & \bf14.44  & \bf 28.52 &  \bf16.92 & \bf 5.95 & \bf 33.57 & \bf 41.46 & \bf18.07 \\
\midrule
    \multirow{3}{*}{50$\%$} & LLM-Pruner  & 111.00& 51.14 &253.13 & 55.81&/ &111.90 & 121.54&71.18 \\
    & Wanda-sp &411.06 & 82.87 & 249.18& 90.90&16.78 &205.21 &160.49 &183.44 \\
     \gr & \bf\texttt{Self-Pruner} & \bf59.11  & \bf23.24  & \bf 53.63 &  \bf41.95 & \bf9.08  & \bf 68.40 & \bf 64.56 & \bf46.47 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\normalsize
\item * LLM-Pruner \citep{ma2023llm} employs the Taylor pruning \citep{molchanov2016pruning} metric, which requires expensive gradient computations. For the LLaMA-2-70B model, we did not find relevant experimental data. Furthermore, due to the limited number of our GPU devices, we did not report its experimental results.
\end{tablenotes}
\end{threeparttable}
}
\end{table*}


\subsection{Zero-shot Tasks}
To evaluate the generalization capability of Self-Pruner, we assessed the performance of pruned LLMs in zero-shot settings across seven commonsense tasks. Table \ref{tab:zero_shot_main_results} shows the average performance of pruned LLMs across all seven tasks, with detailed results for each task provided in Appendix \ref{app:DetailedZero-shotTaskResults}. The experimental results demonstrate that Self-Pruner significantly outperforms existing post-training structured pruning techniques. For instance, for the LLaMA-2-7B model, Self-Pruner outperforms the Wanda-sp \citep{sun2023simple} method by 3.13$\%$ at a 30$\%$ pruning rate and outperforms the LLM-Pruner \citep{ma2023llm} method by 14.59$\%$ at a 50$\%$ pruning rate, further narrowing the performance gap with the original model. Additionally, we observed that as the model size increases, the gap in zero-shot task accuracy between the pruned models and the original model further reduces. Larger models, such as LLaMA-2-70B, there is only a 0.80$\%$ drop in accuracy at a 30$\%$ pruning rate and experience only a 3.80$\%$ drop under an extreme high pruning rate of 50$\%$. These experimental results further validate the effectiveness of the Self-Pruner method.

\begin{table*}[t!]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Mean zero-shot accuracy results of pruned LLMs on the Winogrande, HellaSwag, BoolQ, ARC-Easy, ARC-Challenge, OpenBookQA and PIQA datasets at 20-50$\%$ pruning ratio.}
\vspace{1em}
\small
\label{tab:zero_shot_main_results}
\setlength{\tabcolsep}{8pt}
\resizebox{1.0\textwidth}{!}{%
\begin{threeparttable}
\begin{tabular}{l|c| cc | cc c|c| c |c}
\toprule 
 & \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{LLaMA-1} & \multicolumn{3}{c|}{LLaMA-2} & LLaMA-3 & LLaMA-3.1 &Vicuna \\
 \midrule
   Sparsity & Method  & 7B & 13B  &7B& 13B & 70B &8B &8B & 13B\\
 \midrule
    0$\%$ & Dense &66.30 &68.40  & 66.82&69.28 &73.81 &70.21 &70.64 &69.73 \\
 \midrule
    \multirow{3}{*}{20$\%$} & LLM-Pruner  &60.14 & 64.34 &60.19 &64.24 &/ & 56.50 & 57.45 &65.06 \\
    & Wanda-sp &63.12 & 65.41 &62.33 & 66.28& 72.84& 59.90 &62.16 &66.91 \\
     \gr &  \bf\texttt{Self-Pruner} & \bf64.80  & \bf 66.23  & \bf 63.44  &  \bf 67.15& \bf 73.11 & \bf 61.71  & \bf  63.61 & \bf68.18 \\
 \midrule
    \multirow{3}{*}{30$\%$} & LLM-Pruner  & 53.76&59.31  &51.89 &58.95 & /& 51.80& 52.45 & 57.92\\
    & Wanda-sp & 59.02& 61.71 &57.66 & 60.94& 72.66 &40.91 & 47.19&62.35 \\
     \gr &  \bf\texttt{Self-Pruner} & \bf  61.79 & \bf 64.22 & \bf60.79  &  \bf64.34 & \bf73.01 & \bf56.74  & \bf  58.09 & \bf 65.90  \\
 \midrule
    \multirow{3}{*}{40$\%$} & LLM-Pruner  &46.56 & 54.02 & 46.63& 50.59& /& 41.93& 42.75&51.74 \\
    & Wanda-sp &43.98 &56.76  &52.86 & 38.81& 69.71&38.43 & 40.06 &39.08 \\
    \gr &  \bf\texttt{Self-Pruner} & \bf58.26  & \bf 63.15  & \bf57.59  &  \bf63.22 & \bf 71.97 & \bf53.30  & \bf 53.88 & \bf 63.35\\
 \midrule
    \multirow{3}{*}{50$\%$} & LLM-Pruner  & 41.96& 47.25 & 40.45&42.12 &/ &39.15 & 40.61 &43.77 \\
    & Wanda-sp &37.88 & 43.09 &36.76 & 39.50& 60.63&37.80 &38.43 &39.50\\
    \gr  & \bf\texttt{Self-Pruner} & \bf 52.96 & \bf 58.38 & \bf  51.35 &  \bf58.52 & \bf 70.01 & \bf 46.13  & \bf 46.99  & \bf  54.64\\
 \bottomrule
\end{tabular}
\begin{tablenotes}
\normalsize
\item * LLM-Pruner \citep{ma2023llm} employs the Taylor pruning \citep{molchanov2016pruning} metric, which requires expensive gradient computations. For the LLaMA-2-70B model, we did not find relevant experimental data. Furthermore, due to the limited number of our GPU devices, we did not report its experimental results.
\end{tablenotes}
\end{threeparttable}
}
\end{table*}

\subsection{Ablation Study}\label{sec:AblationStudy}
\paragraph{Ablation of each component in Self-Pruner.}
\begin{wraptable}{r}{5.5cm}
\small
\centering
\setlength\tabcolsep{0.53em}
\caption{Ablation of effectiveness of each component in Self-Pruner.}\label{tab:Ablationofeachcomponent}
\vspace{1em}
\begin{tabular}{@{}l ccc}
\toprule 
   Method  &Perplexity & Accuracy  \\
   \midrule
  w/o initialization &17.43 & 59.30\\
   w/o mutation & 17.65 &  58.98 \\
  w/o crossover & 17.59 & 59.15 \\
  \gr  \bf \texttt{Self-Pruner}  & \bf 16.25 &\bf 60.79  \\
    \bottomrule
\end{tabular}
\end{wraptable}
To demonstrate the effectiveness of each component in Self-Pruner, we present the final accuracy of the algorithm when each component is removed in Table \ref{tab:Ablationofeachcomponent}. Specifically, we are targeting a 30$\%$ pruning rate for LLaMA-2-7B. First, instead of using LLMs to generate the initial population, we replace it with a random initial population (w/o initialization). Additionally, we remove LLMs responsible for performing mutation and crossover operations, meaning no mutation or crossover is performed during evolutionary search (w/o mutation/crossover). The results show that each component of Self-Pruner contributes positively to the final performance of the algorithm, and removing any component results in a drop in final accuracy.


\paragraph{Different LLMs.}
To analyze the impact of different capabilities of LLMs on the final accuracy, we compared four commonly used LLMs: GPT-3.5, GPT 4 and GPT 4o. From the experimental results in Table \ref{tab:differentllms}, we can observe that Self-Pruner can generate high-accuracy pruning rate using these different LLMs. However, due to the strong reasoning ability of GPT 4o, it is able to search and obtain the best pruning rate than other LLM.
\begin{table}[ht]
  \begin{minipage}[b]{0.56\linewidth}
    \centering
    \caption{Impact of different LLMs on final accuracy.}
    \vspace{1em}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{@{}l ccc}
\toprule 
   Model & Pruning Ratio &Perplexity & Accuracy  \\
   \midrule
  LLaMA-2-7B & 0$\%$&5.12 &66.82 \\
  GPT-3.5 &30$\%$  & 17.44 & 58.94 \\
  GPT 4 &30$\%$ &17.13 & 59.87 \\
  \gr  \bf GPT 4o &\bf 30$\%$ &\bf 16.25 &\bf 60.79  \\
    \bottomrule
\end{tabular}}\label{tab:differentllms}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.44\linewidth}
    \centering
    \caption{Results of Self-Pruner vs. OWL.}
    \vspace{1em}
    \resizebox{1.0\textwidth}{!}{
\begin{tabular}{@{}l ccc}
\toprule 
   Model & Pruning Ratio &Perplexity & Accuracy  \\
  \midrule
  LLaMA-2-7B & 0$\%$&  5.12&  66.82\\
  \midrule
  OWL& 30$\%$&21.40 &59.08 \\
  \gr  \bf \texttt{Self-Pruner} &\bf 30$\%$ & \bf 16.25 & \bf 60.79 \\
   \midrule
  OWL& 50$\%$&81.56 & 39.06\\
  \gr  \bf  \texttt{Self-Pruner} & \bf 50$\%$ & \bf53.63& \bf 51.35 \\
    \bottomrule
\end{tabular}
}\label{tab:vsowl}
  \end{minipage}
\end{table}


\subsection{Analysis}\label{sec:analysis}

\paragraph{Self-Pruner vs. OWL.} 
We further compared another method for determining the layer-wise pruning rates of LLMs: OWL \citep{yin2023outlier}. We applied OWL to determine the layer-wise pruning rates of pruned LLMs and use the Wanda-sp \citep{sun2023simple} metric to decide which components within the layers should be pruned like Self-Pruner. Two important hyperparameters in OWL follow the settings in the paper, which are $\lambda=0.08, M=5$. The results in Table \ref{tab:vsowl} indicate that Self-Pruner outperforms OWL across different pruning rates. This demonstrates that the manual determination of layer importance metrics is suboptimal, whereas Self-Pruner, by utilizing LLMs to assist in evolutionary search, minimizes the impact of manual intervention on the final accuracy and automatically finds the good layer-wise pruning rate.


\paragraph{LoRA Fine-tuning.} 
\begin{wraptable}{r}{7cm}
\small
\centering
\setlength\tabcolsep{0.53em}
\caption{Experimental results of restoring the accuracy of pruned LLaMA-2-7B using LoRA fine-tuning.}\label{tab:finetune_results}
\vspace{1em}
\begin{tabular}{@{}l ccc}
\toprule 
   Method & Pruning Rate &Perplexity & Accuracy  \\
   \midrule
  Self-Pruner & 30$\%$&16.25 &60.79 \\
  \gr  \bf w. LoRA &\bf 30$\%$ &\bf 11.87 &\bf 63.01  \\
   \midrule
  Self-Pruner & 50$\%$&53.63 &51.35 \\
  \gr  \bf w. LoRA &\bf 50$\%$ & \bf 25.32 &\bf 59.78  \\
    \bottomrule
\end{tabular}
\end{wraptable}

Due to the severe accuracy degradation caused by structured pruning at high pruning rates, we further demonstrate the potential of fine-tuning to mitigate performance loss in pruned LLMs. Specifically, we apply the LoRA \citep{hu2021lora}  (r = 8) method for to fine-tune the pruned LLaMA-2-7B. We randomly sampled a 10K subset from the Alpaca-GPT4 \citep{peng2023instruction} dataset as our fine-tuning dataset. The experimental results in Table \ref{tab:finetune_results} show that LoRA fine-tuning can recover the performance of pruned LLMs, further narrowing the performance gap with the original model.


\paragraph{Inference Speedup and GPU Memory Usage.}
We report the parameter count, GPU memory usage, and inference speedup of structural pruning LLMs in Table \ref{tab:speedup}. The results were obtained using the vLLM inference engine \citep{kwon2023efficient} on NVIDIA A100 80GB GPUs. Thanks to the hardware-friendly nature of structured pruning, it effectively reduces the number of model parameters, lowers GPU memory usage, and achieves up to 1.82$\times$ inference speedup.

\begin{table}[h]
\centering
\caption{Inference speedup and GPU memory usage statistics of structured pruning LLMs.} 
\vspace{1em}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{l|c|cccc}
    \toprule
    Model & Pruning Ratio & Params (B) & Memory (GB) & Tokens/s & Speedup\\
    \midrule
     \multirow{5}{*}{LLaMA-2-7B}  & 0$\%$ & 6.74 & 12.55 & 83.77 &\bf 1.00$\times$ \\ 
     & 20$\%$ & 5.47 &  10.20 & 115.39 &\bf 1.23$\times$ \\
    & 30$\%$ & 4.77 &  8.88  &123.66& \bf 1.48$\times$ \\
    & 40$\%$ & 4.13 &  7.80  & 137.11& \bf 1.64$\times$ \\
    & 50$\%$ & 3.50 &   6.55 & 152.24 & \bf 1.82$\times$ \\
    \midrule
      \multirow{5}{*}{LLaMA-2-70B}  & 0$\%$ & 68.98 & 128.48 & 18.44 &\bf1.00$\times$ \\ 
     & 20$\%$ &  55.53 & 103.43  &22.13 &\bf1.20$\times$ \\
    & 30$\%$ & 48.56 &  90.40  &25.63 &\bf1.39$\times$ \\
    & 40$\%$ &  41.59 &  77.50  & 28.58&\bf1.55$\times$ \\
    & 50$\%$ &34.75  &  64.64  & 31.31 &\bf1.70$\times$ \\
    \bottomrule
\end{tabular}
}
\label{tab:speedup}
\end{table}


\section{Limitations and Future Work}
The limitations of existing methods mainly include two points: 1) The complete automation of LLMs compression has not yet been achieved. In this paper, we take the first step towards LLMs self-compression by using LLMs to generate solutions for evolutionary algorithm to search for layer-wise pruning rate configurations. However, layer-wise pruning rates alone are not sufficient to achieve fully compressed LLMs, indicating that our study is still some distance away from enabling LLMs to perform compression tasks on their own. In the future, we aim to further automate the LLMs pruning process. 2) Post-training structured pruning of LLMs still falls short of the original model's accuracy. Post-training pruning significantly reduces LLM accuracy. Although this study has narrowed this gap, the accuracy of pruned LLMs remains low at high pruning rates. In the future, we will explore efficient fine-tuning methods for LLMs to restore the accuracy of pruned models.

\section{Conclusion}
In this paper, we propose Self-Pruner, an novel framework that automatically finds layer-wise pruning rates for LLMs using an evolutionary search driven by LLMs. Self-Pruner enables LLMs to execute the evolutionary search process themselves, capitalizing on their prior knowledge of model redundancy to generate, evaluate, and optimize pruning rates. By integrating LLMs into the search process, Self-Pruner accelerates the convergence of the evolutionary search, reducing the need for extensive human intervention in evolutionary algorithm design. Extensive  experimental results demonstrate that Self-Pruner significantly enhances the accuracy of post-training structural pruning for LLMs.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix



\section{Detailed Zero-shot Task Results} \label{app:DetailedZero-shotTaskResults}
To further enhance our comprehensive understanding of the performance of pruned LLMs, we present experimental data of pruned LLMs across all seven commonsense tasks in this section. These datasets include Winogrande \citep{sakaguchi2021winogrande}, HellaSwag \citep{zellers2019hellaswag}, BoolQ \citep{clark2019boolq}, ARC-Easy, ARC-Challenge \citep{clark2018think}, OpenBookQA \citep{mihaylov2018can}, and PIQA \citep{bisk2020piqa}. We show the experimental data for pruning rates ranging from 20$\%$ to 50$\%$ in Tables \ref{tab:zero_shot_20}, \ref{tab:zero_shot_30}, \ref{tab:zero_shot_40}, and \ref{tab:zero_shot_50}, respectively. From the experimental data, we observe that Self-Pruner consistently enhances the capabilities of pruned LLMs, achieving higher accuracy compared to existing techniques.

\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot performance of pruned LLMs with 20$\%$ pruning ratio.
  }\label{tab:zero_shot_20}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{4}{*}{LLaMA-1-7B}   & Dense  &76.19 &69.85 &75.11 &44.40 &78.62 &75.25 &44.71 &66.30 \\
  \cmidrule{2-10}
  & LLM-Pruner &69.85 &62.51& 64.56& 41.60 & 75.57& 68.01 & 38.91
 & 60.14 \\ 
  &  Wanda-sp&72.70&66.93&71.47&40.20&76.33&71.80&42.41&63.12\\
  \gr & \bf \texttt{Self-Pruner} & \bf 73.67 & \bf68.58 & \bf75.57  & \bf 43.60 & \bf 77.31 & \bf 72.39 &   \bf42.49  &\bf 64.80 \\ 
  \midrule
   \multirow{4}{*}{LLaMA-1-13B} &Dense &79.06&72.69&77.86&44.80&79.16&77.36&47.87&68.40 \\
  \cmidrule{2-10}
  & LLM-Pruner & 75.72&68.27&71.13&45.00&77.69&71.13&41.47&64.34\\ 
  &  Wanda-sp & 77.28& 70.56& 71.47& 43.40& 78.18& 72.22& 44.80& 65.41\\
  \gr & \bf \texttt{Self-Pruner} & \bf 77.93 & \bf 72.06& \bf 72.14 & \bf45.00  & \bf 78.56 & \bf  72.85&   \bf45.05  &\bf 66.23 \\ 
  \midrule 
   \multirow{4}{*}{LLaMA-2-7B}
   & Dense&75.97&69.06&77.74&44.20&78.07&76.35&46.33&66.82\\
  \cmidrule{2-10}
  & LLM-Pruner&68.72&63.54&65.14&\bf 39.80&75.90&68.81&39.42&60.19\\ 
  &  Wanda-sp &72.86&65.98&67.61&38.80&76.61&72.18&42.24&62.33 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 73.51 & \bf68.98 & \bf 69.63 & 39.20  & \bf76.93  & \bf 72.19 &   \bf 43.60 &\bf 63.44 \\ 
\midrule
     \multirow{4}{*}{LLaMA-2-13B}   & Dense & 79.38& 72.22& 80.55& 45.20& 79.11& 79.42& 49.06& 69.28 \\
  \cmidrule{2-10}
  & LLM-Pruner  &74.65&65.74&67.77&\bf 43.80&78.29&73.06&46.33&64.24\\ 
  &  Wanda-sp&77.60&69.69&73.58&43.40&78.40&75.84&45.48&66.28 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 78.42 & \bf70.40 & \bf76.76  &   42.40& \bf 78.56 & \bf 76.85 &   \bf46.67  &\bf 67.15 \\ 
  \midrule
\multirow{4}{*}{LLaMA-2-70B}   & Dense &83.81&77.90&83.73&48.80&82.21&82.74&57.51&73.81 \\
  \cmidrule{2-10}
  &  Wanda-sp &83.15 &77.51 &83.15 &47.40 &\bf 82.05 &81.00 &55.63 &72.84 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 83.85 & \bf 77.59& \bf83.70  & \bf 47.60 & 81.92  & \bf81.45 &   \bf 55.66 &\bf 73.11 \\ 
  \midrule
\multirow{4}{*}{LLaMA-3-8B}   & Dense &79.16&72.77&81.35&45.00&79.71&80.09&53.41&70.21\\
  \cmidrule{2-10}
  & LLM-Pruner  & 62.15 & 58.41&59.54 &37.80 & 75.08& 66.62& 35.92&56.50 \\ 
  &  Wanda-sp  &65.45& 66.93 &\bf 64.46 &39.00 &76.28& 68.60& 38.57 & 59.90\\
  \gr & \bf \texttt{Self-Pruner} & \bf 71.24 & \bf68.27 & 61.10  & \bf41.60  & \bf  78.13& \bf 71.09 &   \bf 40.53 &\bf 61.71\\ 
   \midrule   
      \multirow{4}{*}{LLaMA-3.1-8B}   & Dense& 79.01& 73.32& 82.05& 45.00& 79.98& 81.57& 53.58& 70.64 \\
  \cmidrule{2-10}
  & LLM-Pruner & 61.02 & 59.27&62.45 & 38.40& 74.81&68.90 &37.29 &57.45 \\ 
  &  Wanda-sp & 68.91 &67.64 &66.91 &41.00 & 77.31&71.72 &41.64 &62.16 \\
  \gr & \bf \texttt{Self-Pruner} & \bf71.04  & \bf69.22 & \bf67.46  & \bf43.00  & \bf78.13 & \bf73.06  &   \bf 43.34 &\bf 63.61 \\ 
  \midrule
\multirow{4}{*}{Vicuna-13B}   & Dense&77.49&71.59&85.26&45.40&79.00&78.66&50.68&69.73  \\
  \cmidrule{2-10}
  & LLM-Pruner&72.54&66.30&71.99&\bf 45.40&77.75&75.21&46.25&65.06\\ 
  &  Wanda-sp &75.10&68.75&79.51&44.00&77.37&76.18&47.44&66.91 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 76.80 & \bf69.69 & \bf82.87  &  44.00 & \bf 78.40 & \bf 77.69 &   \bf47.78  &\bf 68.18 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h]
\vspace{-1cm}
  \centering
  \small
  \caption{Zero-shot performance of pruned LLMs with 30$\%$ pruning ratio.
  }\label{tab:zero_shot_30}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{4}{*}{LLaMA-1-7B}   & Dense  &76.19 &69.85 &75.11 &44.40 &78.62 &75.25 &44.71 &66.30 \\
  \cmidrule{2-10}
  & LLM-Pruner &61.27&58.64&53.33&38.60&72.03&58.04&34.39&53.76\\ 
  &  Wanda-sp &67.91&62.12&66.97&38.60&74.16&65.74&37.63&59.02 \\
  \gr &  \bf \texttt{Self-Pruner} & \bf 70.96 & \bf66.30 & \bf72.51  & \bf 39.80 & \bf75.57  & \bf67.17 &   \bf 40.19 &\bf61.79  \\ 
  \midrule
   \multirow{4}{*}{LLaMA-1-13B} &Dense &79.06&72.69&77.86&44.80&79.16&77.36&47.87&68.40\\
  \cmidrule{2-10}
  & LLM-Pruner &70.70&63.22&62.39&42.20&75.73&62.12&38.82&59.31\\ 
  &  Wanda-sp&74.17&66.06&65.20&42.00&76.82&67.72&40.02&61.71 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 76.79 & \bf71.51 & \bf 67.25 & \bf43.00  & \bf 77.64 & \bf 69.95 &   \bf 43.43 &\bf 64.22 \\ 
  \midrule 
   \multirow{4}{*}{LLaMA-2-7B}   & Dense&75.97&69.06&77.74&44.20&78.07&76.35&46.33&66.82\\
  \cmidrule{2-10}
  & LLM-Pruner& 57.77&55.49&50.12&36.80&71.87&58.84&32.34&51.89\\ 
  &  Wanda-sp&65.94&58.33&63.12&38.00&74.76&65.99&37.46&57.66 \\
  \gr & \bf \texttt{Self-Pruner} & \bf70.63  & \bf67.40 & \bf 64.71 & \bf40.20 & \bf75.30  & \bf 67.85 &   \bf 39.42 &\bf 60.79 \\ 
\midrule
     \multirow{4}{*}{LLaMA-2-13B}   & Dense & 79.38& 72.22& 80.55& 45.20& 79.11& 79.42& 49.06& 69.28 \\
  \cmidrule{2-10}
  & LLM-Pruner &68.31&60.38&57.09&43.40&76.12&66.66&40.70&58.95\\ 
  &  Wanda-sp &71.59&63.54&66.94&39.40&76.33&68.64&40.10&60.94\\
  \gr & \bf \texttt{Self-Pruner}& \bf 75.81 & \bf70.09 & \bf  69.94& \bf 41.00 & \bf  77.53& \bf72.73  &   \bf 43.26 &\bf 64.34  \\ 
  \midrule
\multirow{4}{*}{LLaMA-2-70B}   & Dense &83.81&77.90&83.73&48.80&82.21&82.74&57.51&73.81 \\
  \cmidrule{2-10}
  &  Wanda-sp&81.45&\bf78.27&84.20&47.40&81.66&80.95&54.66& 72.66\\
  \gr & \bf \texttt{Self-Pruner} & \bf 82.75 & 78.06 & \bf84.22  & \bf48.20 & \bf81.73  & \bf 81.35 &   \bf54.76  &\bf 73.01 \\ 
  \midrule
\multirow{4}{*}{LLaMA-3-8B}   & Dense &79.16&72.77&81.35&45.00&79.71&80.09&53.41&70.21 \\
  \cmidrule{2-10}
  & LLM-Pruner & 54.16 &56.12 & \bf 52.87& 36.40&71.93 &58.63 &32.51 & 51.80\\ 
  &  Wanda-sp &36.96  & 51.22& 40.89&26.80 &62.46 &43.01 &25.00 &40.91 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 65.21 & \bf64.64 &49.94 & \bf39.00  & \bf 76.12 & \bf 65.87 &   \bf36.43  &\bf 56.74 \\ 
   \midrule   
      \multirow{4}{*}{LLaMA-3.1-8B}   & Dense & 79.01& 73.32& 82.05& 45.00& 79.98& 81.57& 53.58& 70.64 \\
  \cmidrule{2-10}
  & LLM-Pruner  & 53.22 & 57.93& \bf 54.89 & 33.20&72.14 &61.95 &33.87 & 52.45\\ 
  &  Wanda-sp  &  48.52&53.75 & 52.11 &30.20 & 66.38&51.85 &27.56 &47.19 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 66.07 & \bf65.59  & 53.61 & \bf  39.60 & \bf 76.99 & \bf 67.26 &   \bf 37.54 &\bf 58.09\\ 
  \midrule
\multirow{4}{*}{Vicuna-13B}   & Dense &77.49&71.59&85.26&45.40&79.00&78.66&50.68&69.73  \\
  \cmidrule{2-10}
  & LLM-Pruner&65.62&59.27&53.15&42.20&75.57&68.43&41.21&57.92\\ 
  &  Wanda-sp &69.19&62.67&72.94&39.40&75.95&71.76&44.52&62.35 \\
  \gr & \bf \texttt{Self-Pruner} & \bf74.35  & \bf68.98 & \bf 81.01 & \bf 42.30 & \bf76.39  & \bf73.78  &   \bf44.54  &\bf 65.90 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h]
\vspace{-1cm}
  \centering
  \small
  \caption{Zero-shot performance of pruned LLMs with 40$\%$ pruning ratio.
  }\label{tab:zero_shot_40}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{4}{*}{LLaMA-1-7B}    & Dense  &76.19 &69.85 &75.11 &44.40 &78.62 &75.25 &44.71 &66.30 \\
  \cmidrule{2-10}
  & LLM-Pruner &49.34&54.62&45.08&33.20&67.36&46.55&29.78&46.56\\ 
  &  Wanda-sp &39.45&53.51&59.39&28.60&62.40&38.47&26.02&43.98 \\
  \gr & \bf \texttt{Self-Pruner} & \bf66.49  & \bf 64.09& \bf68.44 & \bf37.20 & \bf 72.52 & \bf61.20 &   \bf37.88 &\bf58.26 \\ 
  \midrule
   \multirow{4}{*}{LLaMA-1-13B} &Dense &79.06&72.69&77.86&44.80&79.16&77.36&47.87&68.40 \\
  \cmidrule{2-10}
  & LLM-Pruner&61.60&57.77&60.92&37.60&72.85&53.37&34.04&54.02\\ 
  &  Wanda-sp&67.24&59.75&62.75&34.20&73.83&62.12&37.46&56.76\\
  \gr & \bf \texttt{Self-Pruner}& \bf 74.29 & \bf 70.01& \bf 70.34 & \bf41.80  & \bf 75.46 & \bf 67.17 &   \bf 43.00 &\bf  63.15  \\ 
  \midrule 
   \multirow{4}{*}{LLaMA-2-7B}   & Dense &75.97&69.06&77.74&44.20&78.07&76.35&46.33&66.82 \\
  \cmidrule{2-10}
  & LLM-Pruner &43.48&52.80&59.82&32.20&65.56&42.59&29.95&46.63\\ 
  &  Wanda-sp &57.31&54.70&61.56&35.40&70.62&56.99&33.45&52.86 \\
  \gr & \bf \texttt{Self-Pruner}  & \bf65.13 & \bf64.25 & \bf 62.72 & \bf 39.00& \bf73.72 & \bf60.52 & \bf37.80& \bf57.59\\ 
\midrule
     \multirow{4}{*}{LLaMA-2-13B}   & Dense & 79.38& 72.22& 80.55& 45.20& 79.11& 79.42& 49.06& 69.28 \\
  \cmidrule{2-10}
  & LLM-Pruner& 57.59&55.64&42.45&39.40&71.16&54.76&33.11&50.59\\ 
  &  Wanda-sp &28.50&50.04&62.14&24.00&54.90&28.45&23.63&38.81 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 73.39 & \bf68.59 & \bf 75.17 & \bf 40.20 & \bf75.14 & \bf66.96  &   \bf 43.09&\bf63.22 \\ 
  \midrule
\multirow{4}{*}{LLaMA-2-70B}   & Dense &83.81&77.90&83.73&48.80&82.21&82.74&57.51&73.81 \\
  \cmidrule{2-10}
  &  Wanda-sp &80.60&74.03&80.45&45.20&79.98&77.30&50.43&69.71 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 81.00 & \bf78.53 & \bf 84.80 & \bf48.20  & \bf80.85  & \bf78.90  &   \bf 51.53 &\bf 71.97 \\ 
  \midrule
\multirow{4}{*}{LLaMA-3-8B}   & Dense&79.16&72.77&81.35&45.00&79.71&80.09&53.41&70.21 \\
  \cmidrule{2-10}
  & LLM-Pruner & 37.76 &52.17 &42.45 &28.80 & 64.36&42.09 &25.85 & 41.93 \\ 
  &  Wanda-sp  & 30.16 & 49.88& \bf 52.66&25.00 & 56.53& 33.42& 21.33&38.43  \\
  \gr & \bf \texttt{Self-Pruner} & \bf59.24  & \bf62.83 &  51.16 & \bf 37.80 & \bf73.45  & \bf 56.57 &   \bf 32.08 &\bf53.30 \\ 
   \midrule   
      \multirow{4}{*}{LLaMA-3.1-8B}   & Dense & 79.01& 73.32& 82.05& 45.00& 79.98& 81.57& 53.58& 70.64 \\
  \cmidrule{2-10}
  & LLM-Pruner & 38.12 &52.96 &43.09 &29.80 &65.56 & 45.24& 24.49&42.75 \\ 
  &  Wanda-sp  & 31.86 & 52.57&\bf 57.09 & 25.20& 57.78&33.84 &22.10 &40.06 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 60.32 & \bf63.22 &  46.73 & \bf 39.40 & \bf 73.78 & \bf 59.09 &   \bf 34.64 &\bf 53.88 \\ 
  \midrule
\multirow{4}{*}{Vicuna-13B}   & Dense &77.49&71.59&85.26&45.40&79.00&78.66&50.68&69.73  \\
  \cmidrule{2-10}
  & LLM-Pruner&56.99&54.78&49.54&39.00&72.09&55.18&34.64&51.74\\ 
  &  Wanda-sp&28.49&51.14&61.93&26.40&54.13&29.17&22.27&39.08 \\
  \gr & \bf \texttt{Self-Pruner} & \bf71.75  & \bf66.85 & \bf 80.40 & \bf 39.10 & \bf74.37  & \bf 69.49 & \bf 41.55 &\bf 63.35 \\ 
\bottomrule
\end{tabular}
}
% \vspace{-0.5cm}
\end{table*}

\begin{table*}[h]
\vspace{-1cm}
  \centering
  \small
  \caption{Zero-shot performance of pruned LLMs with 50$\%$ pruning ratio. 
  }\label{tab:zero_shot_50}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{4}{*}{LLaMA-1-7B}  & Dense  &76.19 &69.85 &75.11 &44.40 &78.62 &75.25 &44.71 &66.30 \\
  \cmidrule{2-10}
  & LLM-Pruner  &36.64&52.96&51.07 &30.20 &61.81 &35.19 &25.85 &41.96\\ 
  &  Wanda-sp&29.39&50.83&54.13&23.40&54.79&29.84&22.78&37.88 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 59.35 & \bf 59.67 & \bf63.06  & \bf 33.20  & \bf69.75  & \bf52.95  &   \bf  32.76 &\bf 52.96 \\ 
  \midrule
   \multirow{4}{*}{LLaMA-1-13B} &Dense &79.06&72.69&77.86&44.80&79.16&77.36&47.87&68.40\\
  \cmidrule{2-10}
  & LLM-Pruner &47.16&53.43&61.01&35.20&67.14&37.79&29.01&47.25\\ 
  &  Wanda-sp&38.86&51.54&57.92&28.00&61.43&38.47&25.43&43.09 \\
  \gr & \bf \texttt{Self-Pruner} & \bf69.68& \bf66.85 & \bf 65.66 & \bf37.20  & \bf 72.25& \bf58.96  &   \bf 38.05 &\bf 58.38 \\ 
  \midrule 
   \multirow{4}{*}{LLaMA-2-7B}   & Dense &75.97&69.06&77.74&44.20&78.07&76.35&46.33&66.82 \\
  \cmidrule{2-10}
  & LLM-Pruner &31.41&50.43&57.92&28.00&57.73&31.65&26.02&40.45\\ 
  &  Wanda-sp&27.88&48.93&48.53&25.40&54.73&29.50&22.35&36.76 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 55.64 & \bf53.12 & \bf60.73 & \bf36.40  & \bf 69.26 & \bf51.68 &   \bf32.59 &\bf 51.35 \\ 
\midrule
     \multirow{4}{*}{LLaMA-2-13B}   & Dense & 79.38& 72.22& 80.55& 45.20& 79.11& 79.42& 49.06& 69.28\\
  \cmidrule{2-10}
  & LLM-Pruner  &43.45&52.88&38.26&33.20&64.31&35.90&26.88&42.12\\ 
  &  Wanda-sp & 30.23&50.20&62.02&26.00&54.57&27.78&25.68&39.50 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 66.26& \bf66.38  & \bf64.77  & \bf 39.00& \bf 71.55  & \bf59.97 &   \bf 41.72 &\bf 58.52 \\ 
  \midrule
\multirow{4}{*}{LLaMA-2-70B}   & Dense &83.81&77.90&83.73&48.80&82.21&82.74&57.51&73.81 \\
  \cmidrule{2-10}
  &  Wanda-sp& 68.30&58.96&67.85&40.20&76.39&71.25&41.47&60.63 \\
  \gr &  \bf \texttt{Self-Pruner} & \bf79.15  & \bf78.85 & \bf82.85  & \bf45.00  & \bf79.11  & \bf  75.65&   \bf49.48  &\bf 70.01 \\ 
  \midrule
\multirow{4}{*}{LLaMA-3-8B}  
& Dense&79.16&72.77&81.35&45.00&79.71&80.09&53.41&70.21\\
  \cmidrule{2-10}
  & LLM-Pruner  &33.22  &49.81 &41.53 & 29.00& 60.45&36.32 &23.72 & 39.15\\ 
  &  Wanda-sp  & 29.03 & 50.91& \bf 50.09& 25.00& 55.88& 31.57& 22.10& 37.80 \\
  \gr & \bf \texttt{Self-Pruner} & \bf47.33& \bf 55.88& 41.50  & \bf 32.80 & \bf 67.79 & \bf48.11  &   \bf 29.52 &\bf46.13 \\ 
   \midrule   
      \multirow{4}{*}{LLaMA-3.1-8B}   & Dense & 79.01& 73.32& 82.05& 45.00& 79.98& 81.57& 53.58& 70.64 \\
  \cmidrule{2-10}
  & LLM-Pruner  & 32.45 &52.64 &  49.60 & 28.00 &60.34 &38.05 &23.21&40.61 \\ 
  &  Wanda-sp  & 28.29 & 49.33&\bf 57.68 & 25.00& 56.20&30.47 &22.01 &38.43  \\
  \gr & \bf \texttt{Self-Pruner} & \bf 48.95 & \bf 56.75& 45.57  & \bf 30.80 & \bf 67.08 & \bf 49.83 &   \bf 29.95 &\bf46.99  \\ 
  \midrule
\multirow{4}{*}{Vicuna-13B}   & Dense&77.49&71.59&85.26&45.40&79.00&78.66&50.68&69.73 \\
  \cmidrule{2-10}
  & LLM-Pruner &44.59&52.01&43.15&31.80&65.29&40.74&28.84&43.77\\ 
  &  Wanda-sp&29.92&51.14&62.08&24.80&53.97&28.45&26.11&39.50 \\
  \gr & \bf \texttt{Self-Pruner} & \bf 59.55 & \bf59.67 & \bf63.46  & \bf33.60  & \bf70.35   & \bf 58.96 &   \bf36.86  &\bf 54.64 \\ 
\bottomrule
\end{tabular}
}
\end{table*}


\end{document}
