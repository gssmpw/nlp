[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2024mathpilebilliontokenscalepretrainingcorpus",
        "author": "Zengzhi Wang and Xuefeng Li and Rui Xia and Pengfei Liu",
        "title": "MathPile: A Billion-Token-Scale Pretraining Corpus for Math"
      },
      {
        "key": "azerbayev2024llemmaopenlanguagemodel",
        "author": "Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck",
        "title": "Llemma: An Open Language Model For Mathematics"
      },
      {
        "key": "paster2023openwebmathopendatasethighquality",
        "author": "Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba",
        "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text"
      },
      {
        "key": "shao2024deepseekmath",
        "author": "Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Yu and Guo, Daya",
        "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yue2023mammothbuildingmathgeneralist",
        "author": "Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen",
        "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"
      },
      {
        "key": "yue2024mammoth2scalinginstructionsweb",
        "author": "Xiang Yue and Tuney Zheng and Ge Zhang and Wenhu Chen",
        "title": "MAmmoTH2: Scaling Instructions from the Web"
      },
      {
        "key": "li2024common7blanguagemodels",
        "author": "Chen Li and Weiqi Wang and Jingcheng Hu and Yixuan Wei and Nanning Zheng and Han Hu and Zheng Zhang and Houwen Peng",
        "title": "Common 7B Language Models Already Possess Strong Math Capabilities"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical",
        "author": "Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"
      },
      {
        "key": "zhang2024carefulexaminationlargelanguage",
        "author": "Hugh Zhang and Jeff Da and Dean Lee and Vaughn Robinson and Catherine Wu and Will Song and Tiffany Zhao and Pranav Raja and Charlotte Zhuang and Dylan Slack and Qin Lyu and Sean Hendryx and Russell Kaplan and Michele Lunati and Summer Yue",
        "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical",
        "author": "Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chu2025sftmemorizesrlgeneralizes",
        "author": "Tianzhe Chu and Yuexiang Zhai and Jihan Yang and Shengbang Tong and Saining Xie and Dale Schuurmans and Quoc V. Le and Sergey Levine and Yi Ma",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Kambhampati_2024",
        "author": "Kambhampati, Subbarao",
        "title": "Can large language models reason and plan?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kaplan2020scalinglawsneurallanguage",
        "author": "Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei",
        "title": "Scaling Laws for Neural Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "openai-o1",
        "author": "OpenAI",
        "title": "Learning to reason with llms, September 2024"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "brown2024largelanguagemonkeysscaling",
        "author": "Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher R\u00e9 and Azalia Mirhoseini",
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"
      },
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "Li_2022",
        "author": "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R\u00e9mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and Hubert, Thomas and Choy, Peter and de Masson d\u2019Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Sutherland Robson, Esme and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol",
        "title": "Competition-level code generation with AlphaCode"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hao2023reasoninglanguagemodelplanning",
        "author": "Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu",
        "title": "Reasoning with Language Model is Planning with World Model"
      },
      {
        "key": "chen2024alphamath",
        "author": "Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai",
        "title": "AlphaMath Almost Zero: process Supervision without process"
      },
      {
        "key": "yao2023treethoughtsdeliberateproblem",
        "author": "Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "openai-o1",
        "author": "OpenAI",
        "title": "Learning to reason with llms, September 2024"
      },
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhou2024lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "Lima: Less is more for alignment"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "merrill2024expressivepowertransformerschain",
        "author": "William Merrill and Ashish Sabharwal",
        "title": "The Expressive Power of Transformers with Chain of Thought"
      },
      {
        "key": "xiang20252reasoningllmslearning",
        "author": "Violet Xiang and Charlie Snell and Kanishk Gandhi and Alon Albalak and Anikait Singh and Chase Blagden and Duy Phung and Rafael Rafailov and Nathan Lile and Dakota Mahan and Louis Castricato and Jan-Philipp Franken and Nick Haber and Chelsea Finn",
        "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhou2024programmingexampleliftingpretraining",
        "author": "Fan Zhou and Zengzhi Wang and Qian Liu and Junlong Li and Pengfei Liu",
        "title": "Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale"
      }
    ]
  }
]