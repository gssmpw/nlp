\documentclass{article}
\usepackage{authblk}
\usepackage[utf8]{inputenc}
\usepackage{main}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{float}
\usepackage{footnote}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{color}
\usepackage{xcolor}     
\usepackage{colortbl}
\usepackage{bbding}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{imakeidx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\makeindex
\usepackage{arydshln}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage[toc]{multitoc}
\usepackage[edges]{forest}
\usepackage[normalem]{ulem}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage[colorlinks=true,linkcolor=mydarkblue,citecolor=mydarkblue,filecolor=mydarkblue,urlcolor=mydarkblue]{hyperref}
% For timeline
\usepackage{CJKutf8}
\usepackage{awesomebox} % for infobox
\usepackage{bbding}
\usepackage[most]{tcolorbox}
\usepackage{booktabs}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\definecolor{wkblue}{rgb}{0.2, 0.3, 0.6}
\definecolor{meta-color}{rgb}{0.5, 0.5, 0.5}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{lscape} 
\usepackage{booktabs}
\usepackage{algorithm}      % For the algorithm environment
% \usepackage{algorithmic}    % For the algorithmic environment
% 如果你使用的是更现代的版本，也可以用：
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{tabularx,booktabs}
\usepackage{makecell}

\usepackage{amssymb}
\usepackage{amsfonts}

% \usepackage[table]{xcolor}

% for tableofcontent
% \usepackage{tocloft}
% \renewcommand{\contentsname}{\hfill\bfseries\Large Contents\hfill}   
% \renewcommand{\cftaftertoctitle}{\hfill}
% \renewcommand{\listtablename}{\hfill\bfseries\Large List of Tables} % no \hfill after "List of Tables"...
%%% using the command "\renewcommand{\cftlottitlefont}{\hfill\bfseries\Large}" works too...
% \renewcommand{\cftafterlottitle}{\hfill}


% infobox 
\usepackage[tikz]{bclogo}
\usepackage[framemethod=tikz]{mdframed}
\definecolor{bgblue}{RGB}{245,243,253}
\definecolor{ttblue}{RGB}{91,194,224}

\mdfdefinestyle{mystyle}{%
  rightline=true,
  innerleftmargin=10,
  innerrightmargin=10,
  outerlinewidth=3pt,
  topline=false,
  rightline=true,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep
}

\newtcolorbox{myboxi}[1][]{
  breakable,
  title=#1,
%   colback=white,
  colback=red!5,
  colbacktitle=red!5,
  coltitle=black,
  fonttitle=\bfseries,
  bottomrule=0pt,
  toprule=0pt,
  leftrule=2pt,
  rightrule=2pt,
  titlerule=0pt,
  arc=0pt,
  outer arc=0pt,
  colframe=red,
}

\newtcolorbox{myboxnote}[1][]{
  breakable,
  title=#1,
%   colback=white,
  colback=orange!0,
  colbacktitle=orange!0,
  coltitle=black,
  fonttitle=\bfseries,
  bottomrule=0pt,
  toprule=0pt,
  leftrule=2pt,
  rightrule=2pt,
  titlerule=0pt,
  arc=0pt,
  outer arc=0pt,
  colframe=orange,
}




\newtcolorbox{myboxii}[1][]{
  breakable,
  freelance,
  title=#1,
  colback=white,
  colbacktitle=white,
  coltitle=black,
  fonttitle=\bfseries,
  bottomrule=0pt,
  boxrule=0pt,
  colframe=white,
  overlay unbroken and first={
  \draw[red!75!black,line width=3pt]
    ([xshift=5pt]frame.north west) -- 
    (frame.north west) -- 
    (frame.south west);
  \draw[red!75!black,line width=3pt]
    ([xshift=-5pt]frame.north east) -- 
    (frame.north east) -- 
    (frame.south east);
  },
  overlay unbroken app={
  \draw[red!75!black,line width=3pt,line cap=rect]
    (frame.south west) -- 
    ([xshift=5pt]frame.south west);
  \draw[red!75!black,line width=3pt,line cap=rect]
    (frame.south east) -- 
    ([xshift=-5pt]frame.south east);
  },
  overlay middle and last={
  \draw[red!75!black,line width=3pt]
    (frame.north west) -- 
    (frame.south west);
  \draw[red!75!black,line width=3pt]
    (frame.north east) -- 
    (frame.south east);
  },
  overlay last app={
  \draw[red!75!black,line width=3pt,line cap=rect]
    (frame.south west) --
    ([xshift=5pt]frame.south west);
  \draw[red!75!black,line width=3pt,line cap=rect]
    (frame.south east) --
    ([xshift=-5pt]frame.south east);
  },
}



% --------page number start
\usepackage{fancyhdr} % to change header and footers
\usepackage{blindtext} % to quickly get a full document
\usepackage{makecell}

\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}} 

\pagestyle{fancy}
\fancyhf{}
% \fancyhfoffset[L]{1cm} % left extra length
% \fancyhfoffset[R]{1cm} % right extra length
% \rhead{\today}
\lhead{\rightmark}
% \lhead{\bfseries My name}
\cfoot{\thepage}






\DeclareCaptionFont{black}{\color{black}}
\newcommand{\foo}{\color{cyan}\makebox[0pt]{\textbullet}\hskip-0.5pt\vrule width 1pt\hspace{\labelsep}}
\newcommand{\term}[1]{\index{\lowercase{#1}}\emph{#1}}
\newcommand{\tcterm}[1]{\index{#1}\emph{#1}}

\definecolor{myblue}{rgb}{0.9, 0.1, 0.94}
\definecolor{mygreen}{rgb}{0.64, 0.56, 0.88}
\definecolor{myyellow}{rgb}{0.68, 0.6, 0.1}
\definecolor{fancygreen}{rgb}{0.33, 0.68, 0.20}
\definecolor{salmon}{rgb}{0.94, 0.52, 0.49}
\definecolor{tablegreen}{rgb}{0.82, 0.94, 0.75}
\definecolor{tableblue}{rgb}{0.81, 0.90, 0.94}
\definecolor{tablered}{rgb}{0.97, 0.85, 0.85}
\definecolor{tableorange}{rgb}{0.96, 0.85, 0.81}
\newcommand{\pfliu}[1]{\textcolor{myblue}{\bf\small [#1 --pfliu]}}
\newcommand{\yxye}[1]{\textcolor{cyan}{\bf\small [#1 --yxye]}}
\newcommand{\huangz}[1]{\textcolor{blue}{(huangz: #1)}}
\newcommand{\yangxiao}[1]{\textcolor{pink}{\bf\small [#1 --yangxiao]}}
\newcommand{\ethan}[1]{\textcolor{red}{[#1 -ethan]}}

\newenvironment{MyColorPar}[1]{%
    \leavevmode\color{#1}\ignorespaces%
}{%
}%


\newenvironment{itemize*}%
 {\leftmargini=10pt\begin{itemize}%
  \setlength{\itemsep}{0pt}%
  \setlength{\parskip}{0pt}%
  }%
 {\end{itemize}}
\newenvironment{enumerate*}%
 {\begin{enumerate}%
  \setlength{\itemsep}{0pt}%
  \setlength{\parskip}{0pt}}%
 {\end{enumerate}}

\usepackage{xcolor}
\usepackage{listings}

\newcommand\JSONnumbervaluestyle{\color{blue}}
\newcommand\JSONstringvaluestyle{\color{red}}

% switch used as state variable
\newif\ifcolonfoundonthisline

\makeatletter

\lstdefinestyle{json}
{
  showstringspaces    = false,
  keywords            = {false,true},
  alsoletter          = 0123456789.,
  morestring          = [s]{"}{"},
  stringstyle         = \ifcolonfoundonthisline\JSONstringvaluestyle\fi,
  MoreSelectCharTable =%
    \lst@DefSaveDef{`:}\colon@json{\processColon@json},
  basicstyle          = \ttfamily,
  keywordstyle        = \ttfamily\bfseries,
}

% flip the switch if a colon is found in Pmode
\newcommand\processColon@json{%
  \colon@json%
  \ifnum\lst@mode=\lst@Pmode%
    \global\colonfoundonthislinetrue%
  \fi
}

\lst@AddToHook{Output}{%
  \ifcolonfoundonthisline%
    \ifnum\lst@mode=\lst@Pmode%
      \def\lst@thestyle{\JSONnumbervaluestyle}%
    \fi
  \fi
  %override by keyword style if a keyword is detected!
  \lsthk@DetectKeywords% 
}

% reset the switch at the end of line
\lst@AddToHook{EOL}%
  {\global\colonfoundonthislinefalse}

\makeatother


\usepackage{etoolbox}
\usepackage{natbib}
\usepackage{url}
\newcounter{bibcount}
\makeatletter
\patchcmd{\@lbibitem}{\item[}{\item[\hfil\stepcounter{bibcount}{[\thebibcount]}}{}{}
\setlength{\bibhang}{2\parindent}
\renewcommand\NAT@bibsetup%
  [1]{\setlength{\leftmargin}{\bibhang}\setlength{\itemindent}{-\parindent}%
      \setlength{\itemsep}{\bibsep}\setlength{\parsep}{\z@}}
\makeatother



\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}


\definecolor{mybrown}{RGB}{128,64,0}


% \author{%
% \textbf{Yixin Ye$^{1, 3}$\thanks{~~Co-first authors}\space\space\space 
% Zhen Huang$^{1, 2, 3}$\samethanks\hspace{0.5em}
% Yang Xiao$^{3}$\space\space 
% Ethan Chern$^{1, 2, 3}$\space\space 
% Shijie Xia$^{1, 2, 3}$\space\space\space 
% Pengfei Liu$^{1, 2, 3}$}\thanks{~~Corresponding author}\\
% $^1$SJTU, $^2$SII, $^3$GAIR}



\author{%
\textbf{Yixin Ye\thanks{~~Co-first authors}\space\space\space 
Zhen Huang\samethanks\hspace{0.5em}
Yang Xiao\space\space 
Ethan Chern\space\space 
Shijie Xia\space\space\space 
Pengfei Liu}\thanks{~~Corresponding author}\\
SJTU, SII, GAIR}


\begin{document}






% \title{O1 Replication Journey -- Part 2: Demystifying the Shortcuts and Advocating for Transparent Innovation} 


\title{LIMO: Less is More for Reasoning} 


  
\maketitle
\thispagestyle{fancy}
\fancyhead{}
\lhead{\includegraphics[height=0.67cm]{assets/GAIR_Logo3.pdf}}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headsep}{0mm}



\begin{abstract}
% Complex reasoning, particularly in domains like mathematics, has traditionally been considered one of the most data-hungry capabilities to develop in large language models, typically requiring tens or hundreds of thousands of training examples. We challenge this conventional wisdom by demonstrating that high-quality reasoning capabilities can emerge from fine-tuning on just hundreds of carefully curated examples. This finding is enabled by two recent developments: the massive increase in reasoning content in pretraining data (up to 1TB tokens in recent models) and advances in test-time scaling through long-chain reasoning. We conduct extensive experiments on challenging benchmarks including AIME and MATH, showing that our approach achieves performance competitive with or superior to models trained on 100x more data. Through careful ablation studies, we demonstrate that the key to this success lies in the quality of reasoning chains in training examples rather than quantity. Our findings fundamentally challenge current assumptions about the data requirements for developing reasoning capabilities and suggest new directions for efficient model training. This work extends the ``Less is More" principle from simple alignment to complex reasoning tasks, offering insights into how modern language models acquire and deploy reasoning capabilities.
% \pfliu{we also need to claim ``the difficulty of question is rather important''}


We present a fundamental \textit{discovery} that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (often $> 100,000$  examples), we demonstrate a striking phenomenon: complex mathematical reasoning abilities can be effectively elicited with \emph{surprisingly} few examples. This finding challenges not only the assumption of massive data requirements but also the common belief that supervised fine-tuning primarily leads to memorization rather than generalization.
% Through comprehensive experiments, LIMO demonstrates unprecedented performance and efficiency in mathematical reasoning. With merely \textbf{817} curated training samples, LIMO achieves \textbf{57.1\%} accuracy on the highly challenging AIME benchmark and \textbf{94.8\%} on MATH, demolishing previous strong SFT-based model from {6.5\%} $\rightarrow$ {57.1\%} on AIME and from {59.2\%} $\rightarrow$ {94.8\%} on MATH - all while using just 1\% of the training data required by previous approaches.
Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance and efficiency in mathematical reasoning. With merely \textbf{817} curated training samples, LIMO achieves \textbf{57.1\%}  accuracy on the highly challenging AIME benchmark and \textbf{94.8\%} on MATH, improving the performance of previous strong SFT-based models from {6.5\%}  to {57.1\%} on AIME and from {59.2\%} to {94.8\%} on MATH, while only using 1\% of the training data required by previous approaches.
Most remarkably, LIMO demonstrates exceptional out-of-distribution generalization, achieving \textbf{40.5\%} absolute improvement across \textbf{10} diverse benchmarks, outperforming models trained on 100x more data, directly challenging the prevailing notion that SFT inherently leads to memorization rather than generalization.
Synthesizing these pioneering results, we propose the \textbf{Less-Is-More Reasoning Hypothesis (LIMO Hypothesis)}: \emph{In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes}.
This hypothesis posits that the \emph{elicitation threshold} for complex reasoning is not inherently bounded by the complexity of the target reasoning task, but fundamentally determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples, which serve as ``cognitive templates'' that show the model how to effectively utilize its existing knowledge base to solve complex reasoning tasks.
To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at \url{https://github.com/GAIR-NLP/LIMO}.



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/limo.pdf}
    \caption{
    LIMO achieves substantial improvement over NuminaMath with fewer samples while excelling across diverse mathematical and multi-discipline benchmarks.
    }
    \label{fig:enter-label}
\end{figure}


% Through comprehensive experiments on challenging mathematical benchmarks including AIME and MATH, our approach - LIMO with simply \textbf{817} training samples - demonstrates unprecedented efficiency by dramatically outperforming models trained on 100 times more data.  Significantly, our SFT-based approach exhibits strong generalization across dozens of out-of-distribution datasets, directly challenging the prevailing notion that SFT inherently leads to memorization rather than generalization.


% Critically, this suggests that the sample efficiency of eliciting advanced reasoning is not inherently bounded by the complexity of the target reasoning task, but rather by two factors: the completeness of the model's encoded knowledge foundation and the quality of training examples that demonstrate effective reasoning strategies.

\end{abstract}

% \begin{figure}[ht]
%     \centering
%     % \includegraphics[width=0.5\linewidth]{figure/performance.png}
%     \includegraphics[width=\linewidth]{figure/1figure1.pdf}
%     \caption{
%     Performance comparison on MATH and AIME benchmarks. Using only 500 high-quality examples with long-chain reasoning outperforms standard approaches with 100-1000x more training data. \pfliu{please re-design this figure. too ugly and non-informative now.}
%     }
%     \label{fig:enter-label}
% \end{figure}



\newpage

\pagestyle{fancy}
\lhead{\rightmark}
\renewcommand{\headrulewidth}{0.7pt}
\setlength{\headsep}{5mm}



% \tableofcontents

\clearpage



\newpage

\section{Introduction}

% Complex reasoning has long been considered one of the most challenging capabilities to instill in large language models (LLMs). While recent work has shown that LLMs can be effectively aligned with user preferences through relatively small amounts of instruction data~\cite{xx}, the prevailing wisdom suggests that teaching models to reason—particularly in domains like mathematics and programming—requires vastly more training examples~\cite{xx}. This belief stems from two fundamental assumptions: first, that reasoning capabilities demand extensive supervised examples to develop given their inherent complexity involving multiple steps of logical deduction, and second, that supervised fine-tuning primarily leads to memorization rather than true generalization~\cite{xx}. We present evidence that fundamentally challenges both of these assumptions.

% The conventional approach to developing reasoning capabilities in LLMs typically involves training on tens or hundreds of thousands of examples. This paradigm is rooted in the intuition that complex reasoning tasks—which require multi-step logical deduction, domain knowledge application, and the ability to structure coherent solution paths—necessitate correspondingly large datasets to master. While this approach has shown success, it comes with substantial computational costs and data collection challenges.


Complex reasoning has long been considered one of the most challenging capabilities to instill in large language models (LLMs). While recent work has shown that LLMs can be effectively aligned with user preferences through relatively small amounts of instruction data~\cite{zhou2024lima}, teaching models to reason—particularly in mathematics and programming—is widely believed to require vastly more training examples~\cite{paster2023openwebmathopendatasethighquality,yue2024mammoth2scalinginstructionsweb}. This conventional wisdom stems from the inherent complexity of reasoning tasks, which demand multi-step logical deduction, domain knowledge application, and structured solution paths.
The resulting paradigm typically involves training on tens or hundreds of thousands of examples~\cite{yu2024metamathbootstrapmathematicalquestions,li2024numinamath}, based on two fundamental assumptions: first, that mastering such complex cognitive processes requires extensive supervised demonstrations, and second, that supervised fine-tuning leads primarily to memorization rather than true generalization~\cite{zhang2024carefulexaminationlargelanguage,xu2024benchmarkingbenchmarkleakagelarge,chu2025sftmemorizesrlgeneralizes}. 

While this approach has shown success, it imposes substantial computational costs and data collection burdens. More importantly, we argue this data-intensive paradigm may no longer be necessary. Recent advances have fundamentally transformed how LLMs acquire, organize, and utilize reasoning knowledge, suggesting the possibility of a more efficient approach. Two key developments in particular have created the conditions for a fundamental reimagining of how we approach reasoning in LLMs:

\begin{enumerate*}
   \item \textbf{Knowledge Foundation Revolution}: 
    Modern foundation models now incorporate unprecedented amounts of mathematical content during pre-training~\cite{qwen2025qwen25technicalreport, yang2024qwen2,wang2024mathpilebilliontokenscalepretrainingcorpus}.
    For example: Llama 2's total training data across all domains was 1.8T tokens~\cite{touvron2023llama2openfoundation}, while Llama 3 used 3.7T tokens just for mathematical reasoning~\cite{grattafiori2024llama3herdmodels}. 
    This suggests that contemporary LLMs may already possess rich mathematical knowledge in their parameter space, transforming the challenge from knowledge acquisition to knowledge elicitation.    
        \item \textbf{Inference-time Computation Scaling Revolution}: 
    The emergence of techniques scaling longer reasoning chains has revealed that effective reasoning requires substantial computational space during inference. Recent works~\cite{openai2024openaio1card, qin2024o1,huang2024o1} have shown that allowing models to generate extended reasoning chains significantly improves their reasoning ability. 
    In essence, inference-time computation provides the crucial \emph{cognitive workspace} where models can systematically unpack and apply their pre-trained knowledge.
\end{enumerate*}


We hypothesize that successful reasoning emerges from the synergy of these two factors: rich pre-trained knowledge and sufficient computational resources at inference time.
These developments collectively suggest a striking possibility: if models possess rich reasoning knowledge and are given adequate computational space, then activating their reasoning capabilities may require only a small number of high-quality training samples that encourage extended deliberation, rather than massive fine-tuning datasets.
Building on this insight, we propose the \textbf{Less-Is-More Reasoning (LIMO) Hypothesis}. This hypothesis identifies two critical factors that determine the \emph{elicitation threshold} for complex reasoning: (1) the latent presence of prerequisite knowledge within the model's parameter space, and (2) the effectiveness of minimal exemplars in demonstrating systematic problem-solving processes that encourage extended deliberation. Critically, this suggests that the sample efficiency of eliciting advanced reasoning is not inherently bounded by the complexity of the target reasoning task, but rather by the completeness of the model's encoded knowledge foundation and its exposure to training samples that effectively utilize the inference-time computation space.



Through comprehensive experiments, we demonstrate that LIMO achieves 57.1\% accuracy on the highly challenging AIME benchmark and 94.8\% on MATH with merely 817 training samples, demolishing previous strong SFT-based models while using just 1\% of their training data. Most remarkably, these benefits generalize across a diverse spectrum of previously unseen scenarios, with LIMO consistently outperforming models trained on 100x more data by 40.5\% absolute improvement.
This discovery has profound implications for artificial intelligence research: it suggests that even competition-level complex reasoning abilities can be effectively elicited through minimal but curated training samples. More fundamentally, it points to a promising technical pathway toward AGI - any sophisticated reasoning capability, no matter how complex, could potentially be activated with minimal samples given two key conditions: (1) sufficient domain knowledge embedded during pre-training, and (2) optimal cognitive reasoning chains for activation. This represents not merely an argument for data efficiency, but a fundamental insight into how complex reasoning capabilities emerge in large language models.



The main contributions of this work are:
(1) We establish the LIMO hypothesis, demonstrating that complex reasoning capabilities can be elicited through surprisingly small datasets (hundreds of examples) by leveraging rich mathematical knowledge in pre-trained models and detailed reasoning chains.
(2) We provide systematic empirical evidence challenging current assumptions about scaling laws in reasoning tasks, showing that benefits generalize robustly to out-of-distribution problems and suggesting the acquisition of genuine reasoning capabilities rather than superficial pattern matching.
(3) We identify critical factors for effective reasoning elicitation, particularly the synergy between pre-trained knowledge foundations and test-time computation scaling, providing insights into how these advances can be combined to achieve superior reasoning performance with minimal fine-tuning data.
(4) We release a comprehensive open-source suite including our fine-tuned models, evaluation pipelines, training code, and carefully curated datasets with varying quality levels. This release enables systematic investigation of data efficiency in complex reasoning and facilitates reproducibility of our findings, while providing valuable resources for future research in this direction.

% 1. We demonstrate that complex reasoning capabilities can be acquired through fine-tuning on surprisingly small datasets (hundreds of examples) when those examples contain high-quality, detailed reasoning chains, leveraging the rich mathematical knowledge already present in pre-trained models.
% 2. We provide empirical evidence that the amount of training data required for teaching reasoning skills is orders of magnitude smaller than commonly believed, challenging current assumptions about scaling laws in this domain.
% 3. We identify key characteristics of effective training examples for reasoning tasks, particularly focusing on the importance of long, detailed reasoning chains that maximize the benefits of test-time computation.
% 4. We show that these benefits generalize to out-of-distribution problems, suggesting the acquisition of genuine reasoning capabilities rather than superficial pattern matching.
% 5. We provide insights into how recent advances in mathematical pretraining and test-time scaling can be effectively combined to achieve superior reasoning performance with minimal fine-tuning data.




% \section{Rethinking Less-is-More: From Alignment to Reasoning}

\section{Phenomena Rethinking: Less-is-More and RL Scaling}


The emergence of LIMO represents a paradigm shift in how we conceptualize and activate complex reasoning capabilities in large language models. This section examines two key comparisons that illuminate the fundamental nature of this advance: first, contrasting {LIMO} with LIMA to understand how Less-is-More principles extend from \emph{general alignment} to \emph{complex reasoning}; and second, comparing LIMO with reinforcement learning (RL) scaling approaches to highlight distinct philosophical perspectives on developing reasoning capabilities. Through these analyses, we aim to establish a deeper understanding of how complex cognitive abilities emerge in language models and the conditions that enable their efficient activation.








\subsection{LIMO vs LIMA}


\begin{table*}[t]
\caption{Comparative Analysis: Less-is-More Phenomena in Language Models}
\label{table:comparison}
\centering
\small  % 可以根据需要调整字体大小
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{2.2cm}|X|X}
\toprule
\textbf{Aspect} & \textbf{General Alignment (LIMA)} & \textbf{Complex Reasoning (LIMO)} \\
\midrule
\textbf{Core Capability} & 
Response format and style adaptation for general-purpose interaction & 
Multi-step logical inference and complex cognitive reasoning \\
\midrule
\textbf{Knowledge Foundation} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item General text corpus sufficient
\item Social interaction patterns
\item Basic world knowledge
\end{itemize} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Diverse reasoning paradigms and problem-solving approaches
\item Rich context for exploring alternative solutions
\item Deep conceptual connections across domains
\end{itemize} \\
\midrule
\textbf{Computation Requirements} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Fixed-length generation sufficient
\item Single-pass processing adequate
\item Limited context window acceptable
\end{itemize} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Scalable inference-time computation essential
\item Extended reasoning chain support required
\item Large cognitive workspace necessary
\end{itemize} \\
\midrule
\textbf{Historical Prerequisites} & 
Emerged in 2023, requiring only:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Base models with general knowledge
\item Basic prompt engineering techniques
\end{itemize} & 
Emerged in 2025, requiring convergence of:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Advanced reasoning architectures
\item Inference-time scaling revolution
\end{itemize} \\
\midrule
\textbf{Training Data Quality} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{Question Design:}
  \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
  \item Common interaction scenarios
  \item Standard task diversity
  \item Basic instruction following
  \end{itemize}
\item \textbf{Solution Quality:}
  \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
  \item Clear communication style
  \item Format consistency
  \item Appropriate tone
  \end{itemize}
\end{itemize} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{Question Design:}
  \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
  \item High-difficulty problems fostering complex reasoning
  \item Problems deviating from training distribution
  \item Cross-domain knowledge integration challenges
  \end{itemize}
\item \textbf{Solution Quality:}
  \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
  \item Optimal structure with adaptive step granularity
  \item Strategic cognitive scaffolding for reasoning
  \item Rigorous verification throughout solution
  \end{itemize}
\end{itemize} \\
\bottomrule
\end{tabularx}
\end{table*}


The emergence of \emph{Less-is-More phenomena} in LLMs represents a fundamental shift in our understanding of how complex capabilities can be elicited with minimal data. While LIMA~\cite{zhou2024lima} first demonstrated this phenomenon in the context of general alignment, extending this principle to complex mathematical reasoning presents unique challenges and requirements. This section examines the critical developments that enable Less-is-More for reasoning, analyzing the essential differences between alignment and reasoning scenarios, and providing insights into the conditions necessary for efficient capability activation in large language models.


% The emergence of LIMO represents a significant milestone in understanding how complex capabilities can be activated in large language models with minimal data. 
% % While LIMA \cite{zhou2023lima} demonstrated that general alignment could be achieved with just 1,000 examples, LIMO extends this finding to the more challenging domain of mathematical reasoning. 
% This evolution, however, was not merely a straightforward extension but required fundamental advances in both model capabilities and computational frameworks.

\paragraph{Knowledge Foundation Revolution} 
The past two years have witnessed a transformation in how language models acquire and organize mathematical knowledge. While LIMA could rely on general text corpora for alignment, LIMO's success builds upon the rich mathematical content now embedded in modern foundation models through specialized pre-training \cite{wang2024mathpilebilliontokenscalepretrainingcorpus}. This specialized knowledge foundation serves as a prerequisite for efficient reasoning capability activation.

\paragraph{Computation Capability Revolution} 
A crucial distinction between LIMA and LIMO lies in their computational requirements. While LIMA's alignment tasks could be accomplished with fixed-length generation and single-pass processing, LIMO's reasoning tasks demand extensive computation space for multi-step deliberation. The emergence of inference-time scaling techniques \cite{openai2024openaio1card, qin2024o1} provided the necessary ``cognitive workspace'' where models can systematically unpack and apply their pre-trained knowledge.

\paragraph{Synergistic Convergence} 
The timing of LIMO's discovery reflects the necessary convergence of these two revolutions. The two-year gap between LIMA and LIMO represents not just the time needed for better pre-trained models, but the essential wait for inference-time computation breakthroughs. This convergence enables a phenomenon we call the \textit{Reasoning Elicitation Threshold}: when models possess both rich domain knowledge and sufficient computation space, complex reasoning capabilities can be activated with minimal but precise demonstrations.

\paragraph{Implications for Future Research} 
This comparative analysis reveals Less-is-More not merely as an advocacy for using fewer data, but as a fundamental principle governing the efficient elicitation of model capabilities. The success of LIMO demonstrates that when essential prerequisites (knowledge foundation and computation framework) are met, complex capabilities can be elicited with remarkable data efficiency. This insight suggests a new research direction: systematically identifying the prerequisites and optimal activation conditions for different capabilities. Future work should explore whether other advanced capabilities (e.g., planning, creative problem-solving) can achieve similar efficiency once their corresponding knowledge and computation foundations are established. The Less-is-More principle thus serves as both a theoretical framework for understanding capability emergence and a practical guide for pursuing data-efficient capability development across various domains.


\subsection{LIMO vs RL Scaling}

% \begin{table*}[t]
% \caption{Comparative Analysis of LIMO and RL Scaling Approaches}
% \label{tab:limo_vs_rl}
% \begin{tabular}{p{0.15\textwidth}|p{0.4\textwidth}|p{0.4\textwidth}}
% \toprule
% \textbf{Aspect} & \textbf{RL Scaling (e.g., o1, R1)} & \textbf{LIMO} \\
% \midrule
% \textbf{First Principle} & An implementation of the general principle: searching for optimal reasoning trajectories through RL & The fundamental principle: reasoning capabilities exist and need to be activated by high-quality reasoning trajectories \\
% \midrule
% \textbf{Solution Nature} & Discovers reasoning trajectories through extensive RL-based exploration & Directly constructs high-quality reasoning trajectories based on cognitive understanding \\
% \midrule
% \textbf{Core Challenge} & How to efficiently search for effective reasoning trajectories in a large solution space & How to identify and construct optimal reasoning trajectories that activate existing capabilities \\
% \midrule
% \textbf{Methodology} & Implicit trajectory discovery through large-scale RL optimization & Explicit trajectory design through cognitive templates \\
% \midrule
% \textbf{Search Strategy} & Broad exploration of solution space using computational resources & Targeted exploration guided by cognitive principles \\
% \midrule
% \textbf{Resource\hspace{2em} Efficiency} & Resource-intensive search process & Resource-efficient direct construction \\
% \midrule
% \textbf{Generalization} & Through extensive sampling of trajectory space & Through understanding of fundamental reasoning patterns \\
% \bottomrule
% \end{tabular}
% \end{table*}

\begin{table*}[t]
\caption{Comparative Analysis of LIMO and RL Scaling Approaches}
\label{tab:limo_vs_rl}
\begin{tabular}{>{\centering\arraybackslash}m{0.15\textwidth}|>{\centering\arraybackslash}m{0.4\textwidth}|>{\centering\arraybackslash}m{0.4\textwidth}}
\toprule
\textbf{Aspect} & \textbf{RL Scaling (e.g., o1, R1)} & \textbf{LIMO} \\
\midrule
\textbf{First Principle} & An implementation of the general principle: searching for optimal reasoning trajectories through RL & The fundamental principle: reasoning capabilities exist and need to be activated by high-quality reasoning trajectories \\
\midrule
\textbf{Solution Nature} & Discovers reasoning trajectories through extensive RL-based exploration & Directly constructs high-quality reasoning trajectories based on cognitive understanding \\
\midrule
\textbf{Core Challenge} & How to efficiently search for effective reasoning trajectories in a large solution space & How to identify and construct optimal reasoning trajectories that activate existing capabilities \\
\midrule
\textbf{Methodology} & Implicit trajectory discovery through large-scale RL optimization & Explicit trajectory design through cognitive templates \\
\midrule
\textbf{Search Strategy} & Broad exploration of solution space using computational resources & Targeted exploration guided by cognitive principles \\
\midrule
\textbf{Resource\hspace{2em} Efficiency} & Resource-intensive search process & Resource-efficient direct construction \\
\midrule
\textbf{Generalization} & Through extensive sampling of trajectory space & Through understanding of fundamental reasoning patterns \\
\bottomrule
\end{tabular}
\end{table*}

The emergence of two distinct approaches to developing reasoning capabilities in large language models - RL Scaling and LIMO - represents a fundamental divergence in how we understand and enhance model intelligence.
RL Scaling, exemplified by works like o1~\cite{openai-o1}, DeepSeek-R1~\cite{guo2025deepseek}, approaches the challenge from an engineering optimization perspective. It assumes that reasoning capabilities need to be extensively trained into models through large-scale reinforcement learning. While effective, this approach essentially treats RL as a broad search mechanism to discover effective reasoning patterns through massive computational resources. In contrast, LIMO introduces a more foundational perspective: reasoning capabilities are already latent within pre-trained models, embedded during the pre-training phase. The key challenge shifts from ``training'' to ``elicitation'' - finding precise cognitive templates that can elicit these innate abilities. 

From this perspective, RL Scaling approaches like DeepSeek-R1 can be viewed as specific implementations of this principle, using reinforcement learning as a mechanism to search for such trajectories. While both approaches ultimately seek high-quality reasoning solutions, LIMO offers a more principled and direct path through explicit trajectory design, while RL Scaling discovers these trajectories through extensive computational exploration. This reframing suggests that various methods, including RL, expert design, or hybrid approaches, could all be understood and evaluated within LIMO's framework as different strategies for discovering optimal reasoning trajectories.




% Our results not only challenge conventional wisdom about data requirements for teaching reasoning but also reveal a fundamental property of large language models: their ability to manifest sophisticated reasoning capabilities through minimal but precise demonstration. This work builds upon the ``Less is More" principle previously demonstrated for alignment [Zhou et al., 2023], extending it to the more complex domain of mathematical and logical reasoning, and offering a new paradigm for understanding and developing advanced cognitive capabilities in artificial intelligence systems.




% Complex reasoning has long been considered one of the most challenging capabilities to instill in large language models (LLMs). While recent work has shown that LLMs can be effectively aligned with user preferences through relatively small amounts of instruction data [Zhou et al., 2023], the prevailing wisdom suggests that teaching models to reason—particularly in domains like mathematics and programming—requires vastly more training examples. This belief stems from the inherent complexity of reasoning tasks, which often involve multiple steps of logical deduction, the application of domain knowledge, and the ability to structure coherent solution paths. However, we present evidence that challenges this conventional wisdom. Through careful experimentation, we demonstrate that high-quality reasoning capabilities can emerge from fine-tuning on remarkably small datasets—as few as hundreds of examples—provided these examples are carefully curated and contain sufficiently detailed reasoning chains. This finding stands in stark contrast to current approaches that typically employ tens or hundreds of thousands of examples to train models for complex reasoning tasks.

% Our work comes at a particularly opportune moment, driven by two significant developments in the field. First, there has been a dramatic expansion in mathematical pretraining data for large language models, with recent models like Qwen 2.5 incorporating up to 1T tokens of mathematics-related content. This massive infusion of domain knowledge into model weights suggests that modern LLMs already possess much of the necessary mathematical understanding, but may lack the ability to structure and apply this knowledge effectively. The revolution in pre-training has essentially transformed the challenge from one of teaching mathematical knowledge to one of teaching how to organize and deploy existing knowledge. Second, the emergence of test-time scaling techniques, particularly those leveraging longer reasoning chains (long thought), has provided new insights into how models can enhance their reasoning capabilities through additional computation at inference time. This development has not only improved model performance, but also offered a new standard for defining high-quality responses - those that demonstrate detailed, step-by-step reasoning processes. These advances in test-time scaling suggest that models can significantly improve their reasoning capabilities when given the opportunity to think more thoroughly and systematically.

% We validate this hypothesis through extensive experiments on challenging mathematical reasoning benchmarks, including AIME and MATH datasets, demonstrating that models fine-tuned on carefully selected examples can achieve performance competitive with or superior to those trained on much larger datasets. Importantly, these benefits generalize beyond the training distribution, suggesting that the models are learning fundamental reasoning skills rather than merely memorizing patterns.
% Our findings have significant implications for understanding the scaling laws of post-training alignment, particularly in the context of complex reasoning tasks. They suggest that the relationship between training data quantity and model performance is more nuanced than previously thought, with data quality and the presence of detailed reasoning chains playing a crucial role. This insight could lead to more efficient training protocols and a deeper understanding of how language models acquire reasoning capabilities.

\section{LIMO Dataset}

\subsection{The LIMO Hypothesis}
We formalize the Less-Is-More Reasoning (LIMO) Hypothesis as follows: In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis rests on two fundamental premises:
(I) The latent presence of prerequisite knowledge within the model's parameter space
(II) The quality of reasoning chains that precisely decompose complex problems into detailed, logical steps, making the cognitive process explicit and traceable.
To validate this hypothesis, we propose a systematic approach to construct a high-quality, minimal dataset that can effectively elicit the model's inherent reasoning capabilities.

\subsection{Problem Definition}

In this paper, we focus on the reasoning tasks with verifiable answer. Given a question $q \in \mathcal{Q}$, where $\mathcal{Q}$ represents the space of reasoning problems, the goal is to generate an answer $a \in \mathcal{A}$ and a reasoning chain $r \in \mathcal{R}$. We define a reasoning chain $r$ as a sequence of intermediate steps $\{s_1, s_2, ..., s_n\}$, where each step $s_i$ represents a logical deduction that bridges the gap between the question and the final answer.

Formally, we can represent the reasoning process as a function $f$:

\begin{equation}
    f: \mathcal{Q} \rightarrow \mathcal{R} \times \mathcal{A}
\end{equation}

Therefore, the quality of the resulting dataset $\mathcal{D}$ is determined by two fundamental yet multifaceted components: (1) the quality of questions $q \in \mathcal{Q}$, which encompasses factors such as diversity in problem-solving approaches, appropriate difficulty levels to challenge model capabilities, and the breadth of knowledge domains covered, and (2) the quality of solutions $(r, a) \in \mathcal{R} \times \mathcal{A}$, which encompasses aspects including pedagogical value, logical coherence, and methodological rigor. Questions should be designed to encourage sophisticated reasoning patterns and knowledge integration, while solutions should demonstrate clear logical progression and serve as effective learning examples. These interrelated quality dimensions, among others, guide our systematic data curation process detailed in the following sections.


\subsection{High-Quality Data Curation}
Our data curation process focuses on constructing a high-quality dataset $\mathcal{D} = \{(q_i, r_i, a_i)\}_{i=1}^N$ where $N$ is intentionally kept small to validate our LIMO hypothesis.

\subsubsection{Question Selection}
\label{Question Selection}

We hypothesize that high-quality questions $q \in \mathcal{Q}$ should naturally elicit extended reasoning processes. Our selection criteria include the following:

\begin{itemize*}
    \item \textbf{Level of difficulty}
    % We prioritize problems that require multiple logical deduction steps $|r| > k$ (typically $k \geq 5$) while remaining within the scope of the model's pre-trained knowledge.
    We prioritize challenging problems that foster complex reasoning chains, diverse thought processes, and knowledge integration, enabling LLMs to effectively leverage pre-trained knowledge for high-quality inference.
    
    \item \textbf{Generality}
    Problems that deviate more from the model's training distribution can better challenge its fixed thinking patterns, encourage exploration of new reasoning approaches, thus expanding its inference search space.

    \item \textbf{Knowledge Diversity} The selected problems should cover various mathematical domains and concepts, requiring the model to integrate and connect distant knowledge during problem-solving.
\end{itemize*}

% To implement these criteria, we:
% \begin{enumerate}
%     \item 
%     \item Sample candidate questions from established mathematical competitions (AIME, IMO)
%     \item Apply automatic filtering based on solution length $|r|$ and concept coverage
%     \item Conduct expert review to ensure question quality
% \end{enumerate}

To implement these criteria effectively, we first assembled a comprehensive pool of candidate problems from various established datasets: NuminaMath-CoT, featuring meticulously annotated problems from high school to advanced competition levels; AIME historical examination problems, known for its extremely challenging and integrative problems spanning multiple mathematical domains; MATH~\cite{hendrycks2021measuring}, encompassing various competitive mathematics problems from prestigious contests; and several other sources of mathematical problems.

From this rich initial collection, we employed a systematic multi-stage filtration process. Beginning with an initial pool of tens of millions of problems, we first applied a baseline difficulty filter using Qwen2.5-Math-7B-Instruct~\cite{yang2024qwen2}, eliminating problems that this model could solve correctly in a few attempts. This process helped establish a preliminary difficulty threshold. Subsequently, we subjected the remaining problems to a more rigorous evaluation using state-of-the-art reasoning models including R1, DeepSeek-R1-Distill-Qwen-32B~\cite{guo2025deepseek}, and models from \citet{huang2024o1}, retaining only problems where even these most capable models achieved success rates below certain threshold through multiple sampling iterations. Finally, to maintain corpus diversity, we employed strategic sampling techniques that balanced representation across mathematical domains and complexity levels while avoiding conceptual redundancy. This meticulous selection process ultimately yielded \textbf{817} carefully curated problems from an initial pool of tens of millions of candidates, with the selected problems collectively satisfying our stringent quality criteria while spanning a rich spectrum of mathematical reasoning challenges.


% \begin{enumerate}
%     \item Identified requisite problem characteristics:
%         \begin{itemize}
%             \item Multi-step logical reasoning
%             \item Non-trivial pattern abstraction
%             \item Cross-domain knowledge synthesis
%         \end{itemize}
%     \item Evaluated mathematical benchmarks against:
%         \begin{itemize}
%             \item Reasoning complexity
%             \item Solution verifiability
%             \item Domain coverage
%         \end{itemize}
%     \item Analyzed LLM performance gaps across benchmarks
% \end{enumerate}

% Thus, we selected the American Invitational Mathematics Examination (AIME) as our benchmark, given its alignment with our criteria and the observed sub-20\% accuracy rates across most of LLMs. AIME problems necessitate complex reasoning while remaining within standard mathematical knowledge bounds, providing an effective testbed for LLM reasoning capabilities.



\subsubsection{Reasoning Chain Construction}
\label{Reasoning Chain Construction}

Beyond high-quality questions, the quality of solutions plays a pivotal role in the training phase of large language models. To curate high-quality solutions, we adopted a comprehensive selection strategy. We began by gathering official solutions for problems where available, complemented by solutions authored by both human experts and AI specialists. Additionally, we leveraged state-of-the-art reasoning models, including DeepSeek R1, DeepSeek-R1-Distill-Qwen-32B~\cite{guo2025deepseek}, and Qwen2.5-32b-Instruct, to generate diverse solution approaches. Furthermore, following the methodology proposed in O1-Journey-Part2~\cite{huang2024o1}, we utilized self-distillation techniques based on Qwen2.5-32b-Instruct to create additional model variants, which were then used to generate supplementary problem responses. These responses were then filtered according to the correctness of the answers to establish a baseline collection of valid solutions. Subsequently, all the authors conducted a comprehensive analysis of these filtered solutions through collaborative examination. Through careful observation and systematic review, we identified several key characteristics that distinguish high-quality reasoning chains:

\begin{itemize*}
    \item \textbf{Optimal Structural Organization}: The solution exhibits clear and well-organized structural formatting, with adaptive granularity in step decomposition. Particularly, it allocates more tokens and detailed elaboration at crucial reasoning junctures while maintaining concise expressions for straightforward steps. This self-adaptive approach to step granularity ensures that complex transitions receive appropriate attention while avoiding unnecessary verbosity in simpler deductions.

    \item \textbf{Effective Cognitive Scaffolding}: High-quality solutions provide strategic educational support by gradually building understanding through carefully structured explanations. This includes progressive concept introduction, clear articulation of key insights at critical points, and thoughtful bridging of conceptual gaps, making complex reasoning processes more accessible and learnable.

    \item \textbf{Rigorous Verification}: High-quality solutions incorporate extremely frequent verification steps throughout the reasoning process. This includes validating intermediate results, cross-checking assumptions, and confirming the logical consistency of each deduction, thereby ensuring the reliability of the final answer.
    
\end{itemize*}

Based on these identified characteristics, we developed a hybrid approach combining rule-based filtering and LLM-assisted curation to select high-quality solutions for each question identified in the previous section. This systematic process ensures that each selected solution adheres to our established quality criteria while maintaining consistency across the dataset. By focusing on a minimal yet meticulously curated set of reasoning chains, we embody the core principle of \textbf{Less-Is-More}: high-quality demonstrations, rather than sheer data volume, are key to unlocking complex reasoning capabilities. The resulting dataset $\mathcal{D}$ consists of carefully curated triples $(q, r, a)$, where each reasoning chain $r$ satisfies our quality criteria. By maintaining these stringent standards while limiting the dataset size $|\mathcal{D}|$, we aim to demonstrate that high-quality demonstrations, rather than large quantities of training data, are crucial for unlocking complex reasoning capabilities.



% \paragraph{MATH}

% The MATH dataset~\cite{hendrycks2021measuring}represents a comprehensive collection of mathematical competition problems, including content from prestigious contests such as AMC 10, AMC 12, and others, designed to evaluate advanced mathematical problem-solving abilities for students. These problems require sophisticated problem-solving techniques and heuristics rather than straightforward application of standard K-12 mathematics. The dataset encompasses seven distinct mathematical subjects ranging from Prealgebra to Precalculus, with problems categorized into five difficulty levels across both subject areas and complexity. 


% \paragraph{AIME}

% The American Invitational Mathematics Examination (AIME) serves as a prestigious second-tier competition within the American Mathematics Competition (AMC) series and plays a crucial role in selecting candidates for the International Mathematical Olympiad (IMO) team. Distinguished by its heightened complexity compared to AMC 10 and AMC 12. AIME problems are characterized by their exceptional flexibility and integrative nature. These problems demand deep mathematical understanding across multiple domains including algebra, geometry, number theory, and combinatorics, often requiring students to synthesize concepts from various mathematical areas within a single problem. The examination's distinctive feature lies in its emphasis on creative problem-solving approaches, where questions typically offer multiple potential solution paths but require careful analysis to identify the most efficient approach, moving beyond mere formula application to test students' mathematical insight and analytical capabilities.







\section{Methodology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. Data
%     - Source
%         - shortcut basemodel cot(from different models)
%     - Meta:
%         - Length?
%         - Size?
%         - Quality?
% 2. Training
%     - Model Architecture
%         - Why Qwen2.5-32B-Insturct?
%         - Other models
%     - Hyper-parameters(maybe in appendix)
%     - Prompt
% 3. Evaluation
%     - AIME2024
%     - Other Dataset

Based on the \textbf{Less-Is-More} principle, a model with substantial reasoning knowledge from pre-training and the ability to perform long-chain reasoning at test time can develop robust reasoning abilities. After training on only a few hundred instances of SFT data, the model learns to integrate meta-reasoning tasks into a cohesive reasoning chain.

% \subsection{Data Collection and Curation}
% % \paragraph{Sources of Training Data}

% \paragraph{Rejection Sampling and Selection Criteria for High-quality Examples}

% For the source dataset, we performed rejection sampling using the models DeepSeek-R1 and DeepSeek-R1-Distill-Qwen-32B, Qwen2.5-32b-Instruct, and Qwen2.5-32b-Instruct-Manual(Qwen2.5-32b-Instruct fintuned on manual collected data). The signal used for rejection sampling was the standard answer to the question: if the sampled candidate answer matches the standard answer, we consider the sampled response to be of high quality and include its reasoning process in the dataset. During the sampling process, we record the number of samples needed to obtain the correct reasoning process. For instance, some questions are more challenging and may require multiple samples to arrive at the correct answer. Ultimately, we categorize the difficulty of the questions based on the number of samples needed to achieve the correct answer: Simple (1-2 samples), Complex (2-5 samples), and Advanced (5 or more samples). Throughout the sampling process, we used the parameters specified in the official DeepSeek paper: zero-shot setting, a temperature of 0.6, and a top-p value of 0.95.


% Selection criteria for high-quality examples

% Sources of training data

% Quality Assessment Methods


\subsection{Training Protocol}

We fine-tune Qwen2.5-32B-Instruct using supervised fine-tuning on our LIMO dataset. The training process employs full-parameter fine-tuning with DeepSpeed ZeRO-3 optimization \citep{rajbhandari2020zero} and FlashAttention-2 \citep{dao2023flashattention}, with a sequence length limit of 16,384 tokens.


% \yxye{Training Hyperparameters?}

% Model architecture and configuration

% Fine-tuning approach

\subsection{Evaluation Framework}

\paragraph{In-domain Evaluation}

To comprehensively assess the models' performance across various reasoning capabilities, we have established a diverse evaluation framework encompassing both traditional and novel benchmarks. Our primary evaluation suite includes several well-established mathematical competitions and benchmarks: the American Invitational Mathematics Examination \textbf{(AIME24)}, \textbf{MATH500}~\cite{hendrycks2021measuring}, and the American Mathematics Competitions \textbf{(AMC23)}.

\paragraph{Out-of-distribution Evaluation}

% \pfliu{justifying ood performance is very important in this paper. so, let's make this part better organized.  highlighting how different datasets are different from in-domain ones. e.g., different languages, subject domains, and knowledge distributions etc...}

To rigorously evaluate the models' performance on out-of-distribution (OOD) tasks, we carefully selected benchmarks that differ from our training data in various aspects. These benchmarks can be categorized into three distinct groups:

\begin{itemize*}
    \item \textbf{Diverse Mathematical Competitions}: We furthur selected \textbf{OlympiadBench}~\cite{he2024olympiadbench}, which represents a distinct distribution of mathematical challenges to test models' OOD performance.

    \item \textbf{Novel Multilingual Benchmarks}: To minimize data contamination, we constructed several benchmarks using the most recent examination problems: \textbf{CHMath} from the 2024 Chinese High School Mathematics League Competition, \textbf{Gaokao} from China's 2024 National College Entrance Examination, \textbf{Kaoyan} from Chinese Graduate School Entrance Examinations, and \textbf{GradeSchool}, our newly developed benchmark for elementary mathematical reasoning. Notably, all problems in these benchmarks are written in \textbf{Chinese, while our training data contains no Chinese problems}. This introduces an additional OOD dimension, assessing not only the model’s ability to generalize across problem distributions but also its cross-lingual reasoning capabilities when confronted with unseen languages.

    \item \textbf{Multi-disciplinary Benchmarks}: To assess broader generalization capabilities beyond mathematics (our training domain), we incorporated \textbf{Miverva}~\cite{lewkowycz2022solving} (which includes undergraduate-level STEM problems) and \textbf{GPQA}~\cite{rein2023gpqa}. These benchmarks evaluate reasoning abilities across multiple disciplines and cognitive levels, providing insights into the model's capacity to transfer mathematical reasoning skills to broader contexts.
    
\end{itemize*}

\paragraph{Performance metrics}

We evaluate performance using the pass@1 metric across our suite of benchmarks. All evaluations are conducted in a Zero-shot Chain-of-Thought (CoT) setting to better assess the model’s reasoning capabilities. For benchmarks including MATH500, OlympiadBench, Gaokao, Kaoyan, GradeSchool, MinervaMath, and GPQA, we employ a straightforward approach using greedy decoding with a single sample to assess correctness. However, for the smaller benchmarks containing fewer than 50 problems each (specifically AIME24, AMC23, and CHMATH), we implement a more thorough evaluation protocol, generating 16 samples with a temperature setting of 0.7 and calculating the unbiased pass@1 metric as introduced in~\citet{chen2021evaluating}. For problems where answers are well-structured numerical values, we directly apply rule-based evaluations to check for mathematical equivalence. For more complex answer formats—such as expressions, equations, or structured solutions—we leverage an LLM-based evaluator, which we have validated for high reliability. Throughout all evaluations, we maintain a maximum output length of 32,768 tokens to minimize the potential for output truncation, ensuring our assessment captures complete problem-solving attempts. Additionally, when evaluating LIMO, we observed that inference-time scaling occasionally results in repetitive patterns at the end of lengthy outputs. In such cases, we extract the most likely final answer from the model's response for evaluation to ensure accurate assessment of its problem-solving capabilities.


\section{Experiment}

% \input{table/data-quality}


\subsection{Baselines}

We compare LIMO against a comprehensive set of baselines with the following prominent models:
\begin{itemize*}
\item \textbf{OpenAI-o1-preview~\cite{openai-o1}}, a large language model that has demonstrated advanced mathematical reasoning abilities across various complex tasks.
\item \textbf{QwQ-32B-Preview~\cite{qwq}}, a model specifically designed for mathematical problem-solving with strong reasoning capabilities.
\item \textbf{Qwen2.5-32B-Instruct}, which serves as our base model for comparative analysis.
\end{itemize*}

For evaluation, we use the OpenAI API to access OpenAI-o1-preview, while using VLLM \citep{kwon2023efficient} to deploy other open-weight models (e.g. QwQ-32B-Preview). To ensure fair comparison, all models follow the same evaluation protocol with identical inference hyper-parameters. 

To investigate the impact of training data efficiency, we conduct comparative experiments using mainstream open-source reasoning datasets for supervised fine-tuning on our base model.
For a fair comparison, all experiments use the same LLM backbone as LIMO, ensuring that performance differences are solely attributable to the training data characteristics.

\begin{itemize*}
\item \textbf{OpenThoughts-114k}:\footnote{\url{https://github.com/open-thoughts/open-thoughts}} A synthetic reasoning dataset containing 114k examples covering mathematics, science, coding, and puzzles. The solutions follow a structured reasoning format generated by DeepSeek-R1.
\item \textbf{NuminaMath-100k}: A randomly selected 100k subset of NuminaMath-CoT,  featuring mathematical problems ranging from Chinese high school exercises to international mathematics olympiad competitions. Each solution follows a Chain of Thought (CoT) format \citep{wei2022chain}.
\end{itemize*}
These datasets contain substantially more samples than LIMO's training set (817 examples), allowing us to examine the relationship between data quantity and model performance. 

\subsection{Main Results}

Our experimental results demonstrate LIMO's superior performance across both in-domain and out-of-domain tasks, as shown in Table~\ref{tab:main_results}.
\paragraph{In-domain Performance}
On in-domain tasks, LIMO achieves the best results across all benchmarks. For AIME24, LIMO achieves 57.1\% accuracy, outperforming QwQ-32B-Preview (50.0\%) and OpenAI-o1-preview (44.6\%) by significant margins. Most notably, on MATH500, LIMO achieves 94.8\% accuracy, surpassing QwQ-32B-Preview (89.8\%) and OpenAI-o1-preview (85.5\%). The performance gap is even more pronounced on AMC23, where LIMO reaches 92.0\% accuracy compared to QwQ-32B-Preview's 83.6\%.
\paragraph{Out-of-domain Generalization}
LIMO demonstrates strong generalization capabilities across diverse out-of-domain tasks. On OlympiadBench, LIMO achieves 66.8\% accuracy, significantly outperforming QwQ-32B-Preview (58.5\%) and the base model (45.3\%). Similar improvements are observed on other challenging benchmarks such as CHMath (75.4\% vs 68.5\%) and GradeSchool (76.2\% vs 63.8\%). Notably, LIMO maintains competitive performance even on GPQA, where it achieves 66.7\% accuracy, close to OpenAI-o1-preview's leading score of 73.3\%.
\paragraph{Comparison with Larger Datasets}
% Despite using significantly fewer training examples, LIMO outperforms models trained on much larger datasets. OpenThoughts-114k, despite having 114k examples, achieves lower performance across most benchmarks, with particularly notable gaps in MATH500 (80.6\% vs 94.8\%) and AMC23 (80.5\% vs 92.0\%). NuminaMath-100k, with its 100k examples, shows significantly degraded performance across all benchmarks, highlighting that the quality and curation of training data is more crucial than quantity.
Our experiments reveal that despite larger scale, both baseline datasets underperform compared to LIMO. NuminaMath-100k shows significant degradation (32.3\% vs. base model's 49.9\%) due to uncurated reasoning chains, while OpenThoughts-114k achieves suboptimal results (58.3\%) probably due to unfocused problem selection. In contrast, LIMO's carefully curated 817 problems yield superior performance (72.8\%), demonstrating that targeted selection and high-quality annotations are more crucial than data quantity for developing robust reasoning capabilities.

\paragraph{Overall Performance}
LIMO achieves the highest average performance of 72.1\% across all benchmarks, substantially outperforming OpenAI-o1-preview (67.8\%), QwQ-32B-Preview (66.4\%), and other baselines. This comprehensive evaluation demonstrates that LIMO's carefully curated training approach with just 817 examples can outperform models trained on datasets that are orders of magnitude larger.


\input{table/main-results}



% Performance comparison with baselines

% Scaling behavior analysis

% [√] Out-of-distribution generalization


% \begin{table}[h]
%     \centering
%     \begin{tabular}{|c|l|c|c|c|c|c|}
%         \hline
%         \textbf{} & \textbf{Model} & \textbf{AIME24} & \textbf{MATH} & \textbf{AMC23} & \textbf{Minerva Math} & \textbf{Olympiad Bench} \\
%         \hline
%         \multirow{3}{*}{Ours} 
%         & AIME494\_Deepseek-R1 & 0.5292 & 0.924 & 0.95 & \textit{0.4412} & \textit{0.6311} \\
%         & AIME533-official\_R1-distill & 0.4875 & 0.93 & 0.925 & \textit{0.4412} &  \\
%         & AIME\_MATH\_4k\_OurModel & 0.4688 & 0.904 & 0.9 & \textit{0.4301} & \textit{0.6148} \\
%         \hline
%         \multirow{2}{*}{Qwen} 
%         & Qwen2.5-Math-72B-Instruct & 0.3 & 0.7 & 0.75 & 0.441 & 0.490 \\
%         & QwQ-32B-Preview & 0.50 & 0.906 &  &  &  \\
%         \hline
%     \end{tabular}
%     \caption{\textbf{Pass@1 Accuracy for Our Best Models and Qwen Models} }
% \end{table}


% \begin{table}[h]
%     \centering
%     \renewcommand{\arraystretch}{1.2} % 增加行距，提高可读性
%     \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
%         \hline
%         \multirow{2}{*}{} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{AIME24}} & \multicolumn{2}{c|}{\textbf{MATH}} \\
%         \cline{3-6}
%         &  & Accuracy & Rank & Accuracy & Rank \\
%         \hline
%         \multirow{3}{*}{Ours} 
%         & AIME494\_Deepseek-R1 & 0.5292 & 8 & 0.924 & 8 \\
%         & AIME533-official\_R1-distill & 0.4875 & 10 & 0.93 & 7 \\
%         & AIME\_MATH\_4k\_OurModel & 0.4688 & 11 & 0.904 & 10 \\
%         \hline
%         \multirow{2}{*}{Qwen} 
%         & Qwen2.5-Math-72B-Instruct & 0.3 & 13 & 0.7 & 13 \\
%         & QwQ-32B-Preview & 0.50 & 9 & 0.906 & 9 \\
%         \hline
%         \multirow{6}{*}{OpenAI} 
%         & OpenAI-o1-preview & 0.446 & 12 & 0.855 & 12 \\
%         & OpenAI-o1-1217 & 0.792 & 4 & 0.964 & 4 \\
%         & OpenAI-o1-mini & 0.636 & 6 & 0.9 & 11 \\
%         & OpenAI-o3-mini(low) & 0.6 & 7 & 0.958 & 5 \\
%         & OpenAI-o3-mini(medium) & 0.796 & 3 & 0.973 & 2 \\
%         & OpenAI-o3-mini(high) & 0.873 & 1 & 0.979 & 1 \\
%         \hline
%         \multirow{2}{*}{DeepSeek} 
%         & DeepSeek-R1-Distill-Qwen-32B & 0.726 & 5 & 0.943 & 6 \\
%         & DeepSeek-R1 & 0.798 & 2 & 0.973 & 2 \\
%         \hline
%     \end{tabular}
%     \caption{Pass@1 Accuracy for AIME24 and MATH (Landscape)}
% \end{table}


\subsection{Analysis}

\subsubsection{RQ1: Impact of Reasoning Chain Quality}

To gain a deeper understanding of why Less-Is-More achieves such remarkable results, we investigate the quality of reasoning chains (CoT). A fundamental question naturally arises: what characteristics define a high-quality reasoning chain that leads to superior model performance? To address this, we conducted a controlled comparative study examining how solutions of varying quality for the same problem statements affect the performance of models trained on them.

\paragraph{Setup}

To conduct this analysis, we selected 500 problems from the LIMO dataset. The selection was based on the intersection of problems for which the models used in rejection sampling exhibited performance differences and those with corresponding human-annotated solutions, ensuring consistency across comparisons. For these 500 problems, we collected and categorized solutions into five distinct quality levels based on our comprehensive evaluation framework. These solutions were sourced from various origins, including human experts, AI specialists, and model-generated responses, then classified strictly based on their reasoning quality rather than their source.

\paragraph{Quality Measure}

Following the principles outlined in Section~\ref{Reasoning Chain Construction}, we took a holistic approach to categorize the reasoning chains into five quality levels (L1-L5, with L5 being the highest). Our assessment focused on several key aspects: how well the steps were organized and connected, whether important logical transitions were properly explained, and if the solution included self-verification steps to check the work. Using these general guidelines, we classified L5 solutions as those showing excellent organization with clear, well-explained steps and thorough self-verification. L4 solutions were also well-structured but perhaps with slightly less rigorous checking. L3 solutions showed decent organization but sometimes skipped over explaining crucial logical leaps. L2 solutions often provided abbreviated reasoning without much explanation, while L1 solutions typically just listed basic steps with minimal elaboration and rarely included any verification.

\paragraph{Results} 
\input{table/cot_quality} 
Our training results (Figure \ref{fig:data_quality}) strongly correlate with the reasoning chain quality levels. Models trained on L5 quality reasoning chains achieved the highest performance on both AIME24 and MATH500, demonstrating the effectiveness of well-structured, detailed, and self-verified reasoning. Performance consistently decreased with each quality level, with L4 and L3 showing moderate success, while L2 and L1 resulted in notably lower performance. These results empirically validate our quality assessment framework and highlight the crucial role of high-quality reasoning chains in model performance. Specifically, we observed that the performance gap between L5 and L1 solutions was substantial - approximately 15 percentage points on AIME24 and 12 percentage points on MATH500. This significant difference suggests that the quality of reasoning chains plays a far more important role in model performance than previously assumed, reinforcing the importance of carefully curating training data to include well-structured, thorough solutions. 

\subsubsection{RQ2: Impact of Question Quality}
% \pfliu{The difficulty of a question also matters}

We hypothesize that more challenging problems foster complex reasoning chains, diverse thought processes, and enhanced knowledge integration, enabling LLMs to better leverage pre-trained knowledge for high-quality inference. To validate this hypothesis, we investigate how question quality affects the reasoning capabilities of models fine-tuned on these questions and their corresponding solutions.

\paragraph{Setup}

We selected three sets of problems of similar size but increasing difficulty, constructing solutions in a consistent manner to form three training datasets. Our findings indicate that models trained on more challenging datasets exhibit superior reasoning performance. Specifically, we sampled three sets of problems, each containing 500 samples, from MATH and AIME:

\begin{itemize*}
    \item \textbf{Simple-500}: 500 simple problems randomly selected problems from MATH levels 1 and 2.
    \item \textbf{Complex-500}: 500 complex problems randomly selected problems from MATH levels 3, 4, and 5.
    \item \textbf{Advanced-500}: 500 advanced problems randomly selected problems from past AIME tests.
\end{itemize*}



% \yxye{Why 500 samples?}

To rigorously establish the increasing difficulty of these sets, we evaluated various LLMs on them, observing a decline in accuracy and an increase in the average length of correctly generated reasoning chains.

We then used DeepSeek-R1 to generate solutions, which represent the highest-quality solutions available, for each problem set, forming the training data for fine-tuning Qwen2.5-32B-Instruct.

\paragraph{Results} 
We evaluate all three fine-tuned models on the AIME2024 and MATH500 benchmarks to assess their reasoning performance. The results (Figure \ref{fig:question_quality}) indicate that modifying the selection of problems alone leads to a 16\% improvement in accuracy on the challenging AIME2024 benchmark, reaching 51.5\%. Furthermore, despite the absence of in-domain training data, the model fine-tuned on Advanced-500 outperforms the other two models, achieving an accuracy of 91.2\% on the MATH500 benchmark. This result suggests that the improvement in reasoning ability due to increased problem difficulty generalizes across datasets.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/quality-1.png}
    \caption{
    Performance comparison on MATH and AIME benchmarks between models trained on different question quality: Simple-500, Complex-500, and Advanced-500. 
    }
    \label{fig:question_quality}
\end{figure}

\subsubsection{RQ3: LLM Backbone}
Building on our LIMO hypothesis, which emphasizes the importance of latent prerequisite knowledge within the model's parameter space, we examine how different pre-training data affect a model's capacity to leverage minimal exemplars for mathematical reasoning. This investigation allows us to assess the first key factor of our hypothesis: the role of pre-trained knowledge in enabling complex reasoning capabilities.
\paragraph{Setup}
To isolate the impact of pre-training while controlling for model architecture and fine-tuning procedures, we conduct experiments using two 32B-parameter variants of the Qwen model family: Qwen1.5-32B-Chat \cite{qwen1.5} and Qwen2.5-32B-Instruct (the base model of LIMO). Both models share the same architecture and parameter count, while Qwen2.5 demonstrates significant improvements in pre-training data quality, particularly in mathematical and code-related data, compared to its predecessor. We SFT both models using identical LIMO datasets and evaluation protocols, assessing their performance on the AIME2024 and MATH500 benchmarks.

\paragraph{Results}
Our experiments reveal that the choice of pre-trained model dramatically impacts reasoning performance, as demonstrated in \ref{fig:llm_backbone}. LIMO, built on Qwen2.5-32B-Instruct, significantly outperforms its predecessor across both benchmarks. On the challenging AIME2024 test, LIMO achieves 57.1\% accuracy, a remarkable 47.1 percentage point improvement over Qwen1.5-32B-Instruct's 10.0\%. Similarly, on MATH500, LIMO demonstrates exceptional performance with 94.8\% accuracy, surpassing Qwen1.5-32B-Instruct by 34.4 percentage points.
These substantial improvements suggest that the enhanced pre-training in Qwen2.5 creates a stronger foundation for mathematical reasoning. The results align with our LIMO hypothesis, indicating that richer pre-trained knowledge within the model's parameter space enables more effective utilization of minimal exemplars during fine-tuning. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figure/llm-backbone.png}
    \caption{Impact of Pre-trained Model Choice on Mathematical Reasoning Performance}
    \label{fig:llm_backbone}
\end{figure}

\subsubsection{Case study}
\paragraph{Qualitative Analysis} Fig.~\ref{fig:limo_case_study} compares responses generated by Qwen2.5-32B-Instruct, DeepSeek-R1, and LIMO. \textbf{LIMO achieves capabilities and behaviors comparable to DeepSeek-R1, despite using minimal data and compute resources (only 817 training samples).} Notably, LIMO demonstrates strong self-reflection and long chain-of-thought generation capabilities. LIMO verifies its own statements (``Wait, 24 minutes is 0.4 hours? Wait, no. Wait, 60 minutes is 1 hour, so 24 minutes is 24/60, which
is 0.4 hours") and validates its calculations (``But let me check again. Maybe I made a mistake in calculations."). Furthermore, it learns to allocate additional tokens (compute) for detailed complex equation-solving (``Now let's compute the left side, $\cdots$, multiply both sides by 2) to prevent errors. In contrast, the base model Qwen2.5-32B-Instruct exhibits limitations in its reasoning process, being unable to correct inaccurate statements and failing to cross verify equation 2 in its solution. 
\textbf{These results strongly support the LIMO hypothesis: With minimal but high-quality post-training examples, models can be empowered into strong reasoners.}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure/limo_case_study.pdf}
    \caption{Comparison between the responses generated by Qwen2.5-32B-Instruct, DeepSeek-R1, and LIMO}
    \label{fig:limo_case_study}
\end{figure}

\paragraph{Quantitative Analysis}
Table~\ref{tab:limo_statistics} demonstrates the differences between models trained with varying sample quality. We observe a general trend where increasing post-training example quality leads to models that generate longer responses with more lines. Additionally, these higher-quality models employ more self-reflecting transitions (e.g., wait, perhaps, maybe, therefore) to allocate additional inference tokens (compute) for deeper thinking.

\input{table/example-statistics}

\section{Background and Related Work}


\subsection{Evolution of Mathematical Reasoning in LLMs}

% \pfliu{we can cover the following points:}


% Traditional approaches requiring large datasets

% Recent developments in mathematical pretraining

% The current state of reasoning capabilities


Large-scale training data has been the driving force behind the development of reasoning abilities in LLMs. In the pretraining phase, the reasoning ability of LLMs can be enhanced by relevant corpora~\cite{wang2024mathpilebilliontokenscalepretrainingcorpus,azerbayev2024llemmaopenlanguagemodel,paster2023openwebmathopendatasethighquality,shao2024deepseekmath}. These curated corpora can be composed of multiple sources, such as textbooks, scientific papers, and mathematical code, which capture diverse human cognitive patterns used to solve problems. In the post-training phase, a line of research focuses on curating large-scale instruction data to teach LLMs to reason~\cite{yue2023mammothbuildingmathgeneralist,yue2024mammoth2scalinginstructionsweb,li2024common7blanguagemodels}. This includes scaling the number of questions and their corresponding solutions. The scaling approach is promising and has achieved significant performance gains. However, the reasoning ability gained through this method has been criticized for relying on the memorization of fixed patterns rather than achieving true generalization~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,zhang2024carefulexaminationlargelanguage}. For example,~\citet{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical} finds that LLMs exhibit noticeable variance when responding to different instantiations of the same question, and their performance declines when only the numerical values in the question are altered. This raises doubts about the generalization capability of SFT methods~\cite{chu2025sftmemorizesrlgeneralizes} and whether LLMs can be true reasoners rather than mere knowledge retrievers~\cite{Kambhampati_2024}.  



\subsection{Test-time Scaling and Long Chain Reasoning}

% \pfliu{we can cover the following points:}

% Recent advances in inference-time techniques

% Role of reasoning chain length Impact on model performance

Instead of focusing on scaling model parameters and training data~\cite{kaplan2020scalinglawsneurallanguage}, recent work has shifted to exploring test-time scaling~\cite{openai-o1,snell2024scaling}, i.e., increasing the number of tokens to improve performance. This can be achieved by augmenting LLMs with methods such as parallel sampling~\cite{brown2024largelanguagemonkeysscaling, wang2022self, Li_2022} or symbolic tree search~\cite{hao2023reasoninglanguagemodelplanning, chen2024alphamath, yao2023treethoughtsdeliberateproblem} to enhance reasoning ability. Furthermore,~\citet{openai-o1, guo2025deepseek} explore training LLMs using reinforcement learning to generate long CoT, which often include self-reflection, verification, and backtracking—processes commonly employed by humans when solving complex problems. This approach not only innovates the training paradigm for LLMs but also provides a new form of training data to augment their reasoning ability. Our work demonstrates that this long CoT exhibits high-quality characteristics in eliciting the inherent reasoning abilities of LLMs.



\subsection{Data Efficiency in Language Models}

% \pfliu{we can cover the following points:}

% LIMA and lessons from alignment

% Current understanding of data requirements

% Quality vs. quantity trade-offs

\citet{zhou2024lima} demonstrates that with just 1,000 carefully curated prompts and responses, models can learn to follow specific formats and generalize well to unseen tasks. The findings emphasize the importance of quality over quantity in the alignment process. However, whether this lesson can be applied to reasoning tasks remains uncertain, given the potential high computational complexity of such tasks~\cite{merrill2024expressivepowertransformerschain,xiang20252reasoningllmslearning}. While some work on reasoning highlights the importance of quality during the curation of training data~\cite{zhou2024programmingexampleliftingpretraining}, the quantity of such data is still much larger compared to that in LIMA. Our work extends the ideology of LIMA to reasoning tasks by investigating what constitutes high-quality questions and solutions, and demonstrates that the reasoning ability of LLMs can be enhanced in a highly data-efficient manner.


\section{Future Work}

While LIMO demonstrates remarkable success in mathematical reasoning with minimal data, several promising directions remain for future exploration.

\paragraph{Domain Generalization:}
First, extending the LIMO hypothesis to broader reasoning domains represents a critical next step. While our work focuses on mathematical reasoning, the principles of high-quality reasoning chains could potentially generalize to scientific reasoning, logical deduction, and causal inference. Understanding how these principles transfer across domains could reveal universal patterns in effective reasoning. This exploration would require adapting our quality metrics and developing domain-specific evaluation frameworks, ultimately contributing to a more comprehensive theory of machine reasoning.

\paragraph{Theoretical Foundations:}
A deeper theoretical understanding of LIMO's success is also essential. Future research should focus on formalizing the relationship between pre-training knowledge, inference-time computation, and reasoning capabilities. This includes investigating the minimum threshold of pre-trained knowledge required for effective reasoning and developing mathematical models to predict the optimal balance between reasoning chain quality and quantity. Such theoretical foundations could guide the development of more efficient training strategies and provide insights into the fundamental nature of machine reasoning.

\paragraph{Automated Assessment:}
The development of automated quality assessment tools represents another crucial direction. Current manual evaluation of reasoning chain quality, while effective, is time-consuming and difficult to scale. Future work should focus on creating automated systems that can evaluate and improve reasoning chain quality based on our proposed metrics. This could include developing algorithms that automatically enhance existing reasoning chains and generate high-quality ones with minimal human intervention, making the LIMO approach more accessible and scalable.

\paragraph{Multi-modal Integration:}
Cross-modal reasoning presents an exciting frontier for extending LIMO's principles. As real-world reasoning often involves multiple modalities, investigating how visual information and structured data can enhance mathematical reasoning capabilities is crucial. This research direction would require developing new quality metrics for multi-modal reasoning chains and understanding how different types of information can be effectively integrated into the reasoning process.


\paragraph{Real-world Impact:}
The application of LIMO principles to real-world scenarios deserves significant attention. Future work should focus on adapting these approaches to practical problems in education, scientific research, and industrial applications. This includes developing specialized versions of LIMO for specific domains and creating tools that help human experts generate high-quality reasoning chains for complex real-world problems. Such applications could significantly impact how we approach problem-solving in various fields.

\paragraph{Cognitive Science Bridge:}
Finally, integrating insights from cognitive science could provide valuable directions for improvement. Understanding the parallels between LIMO's reasoning patterns and human cognitive processes could inform the development of more effective reasoning strategies. This includes studying how different reasoning approaches affect model performance and generalization, and incorporating cognitive science principles into the design of reasoning chains. Such research could not only improve AI systems but also provide insights into human reasoning processes.

These future directions collectively aim to deepen our understanding of efficient reasoning in large language models while expanding their practical applications. By pursuing these paths, we can work toward more sophisticated, efficient, and widely applicable reasoning systems that better serve human needs across various domains.


\section{Acknowledgement}
We would like to express our sincere gratitude to Yixiu Liu and Yiwei Qin for their valuable contributions to this research work. Their expertise, dedication, and collaborative spirit have significantly enhanced the quality of our study. Their insightful suggestions and technical assistance were instrumental in achieving our research objectives.
We also wish to extend our appreciation to Haoyang Zou and Xuefeng Li for their valuable discussions during the early stages of this work. Their perspectives and insights helped shape the foundation of our research.

\bibliographystyle{acl_natbib}
\bibliography{related}

\end{document}