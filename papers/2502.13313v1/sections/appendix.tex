\section*{Appendix}


\section{Hyperparameters}
\label{appendix:hp-models}

The following hyperparameters were used for fine-tuning our models:
\begin{table}[H]
\centering
\scriptsize
\caption{Hyperparameters used for fine-tuning methods}
\begin{tabular}{l|c|c|c}
\textbf{Hyperparameter} & \textbf{Full Fine-Tuning (FFT)} & \textbf{LoRA} & \textbf{DP} \\
\toprule
Learning Rate           & 0.00025                         & 0.00025       & 0.00005         \\
Scheduler               & Linear                          & Linear        & Linear          \\
Warmup Steps            & 10                              & 10            & 10              \\
Clipping gradient          & -                               & -             &  (\(1 \times 10^{-2}\)) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\scriptsize
\caption{Batch sizes for CustomerSim and SynBio}
\begin{tabular}{l|l|c|c|c}
\textbf{Dataset} & \textbf{Model} & \textbf{Full Fine-Tuning (FFT)} & \textbf{LoRA} & \textbf{DP} \\
\toprule
\multirow{3}{*}{CustomerSim} & Pythia & 16 & 32 & 8 \\
                             & Gemma  & 8  & 16 & 4 \\
                             & Llama2 & 4  & 8  & 2 \\
\midrule
\multirow{3}{*}{SynBio}         & Pythia & 8  & 16 & 4 \\
                             & Gemma  & 4  & 8  & 2 \\
                             & Llama2 & 2  & 4  & 2 \\
\bottomrule
\end{tabular}
\end{table}


\section{Validating GPT-4 Predictions}
\label{appendix:validating-gpt4-preds}

We analyzed the predictions shown in the heatmap in Figure~\ref{fig:pile-heatmap} and observed that misclassified instances were frequently assigned to sections of a similar nature (e.g., \textit{NIH Explorer misclassified as PubMed Central}).
This overlap suggests that the nature of the misclassifications may not always indicate clear inaccuracies, making it difficult to definitively assess the accuracy of certain misclassified instances.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.2]{figures-paper/section2/gpt4_accuracy_heatmap.pdf}
    \caption{
    Misclassified GPT-4 instances were often placed in similar sections (e.g., \textit{NIH Explorer misclassified as PubMed Central}), suggesting these misclassifications may not reflect clear inaccuracies, complicating definitive accuracy assessment.
    }
    \label{fig:pile-heatmap}
\end{figure}


\section{Prompt for Annotating Privacy-Sensitive Information}
\label{appendix:example-priv-annotatation}

We used the following prompt to obtain the privacy-sensitive information from GPT-4o for CustomerSim:
% \vspace{-3pt}
\begin{mdframed}[backgroundcolor=lavender, linewidth=0pt]
\small
\textit{``You are a private data identifying bot. You will be provided a conversation between a user and a customer service bot. You need to identify ALL instances of private data in the conversation. Private data includes any information that can be used to identify an individual, such as names, phone numbers, email addresses, and locations. It can also include any other sensitive information, such as credit card numbers, social security numbers, tracking data, health information etc.. Identify all of these use your discretion to identify any other information that can be used to identify an individual.''} 
\end{mdframed}
% \vspace{-5pt}
\normalsize

\section{Human Survey: Assessing Privacy-Sensitivity in Memorized Segments}
\label{appendix:human-survey-priv-mem-txt}

We conducted a human annotation survey to assess if instances memorized by an LLM contain privacy-sensitive information, sampling 100 sequences divided into 5 questionnaires. Each was given to 40 Prolific workers. After an introductory explanation, participants evaluated each instance by (1) flagging privacy-sensitive information (ignoring routine commands unless uniquely identifiable) and (2) listing detected items if flagged. Figure~\ref{fig:human-survey-mem-txt} shows that most questions were labeled as non-sensitive.


\section{Human Survey: Assessing the Quality of Privacy-Sensitive Annotations from Presidio and GPT-4}
\label{appendix:human-survey-priv-annotatations}


We evaluated privacy-sensitive annotations from Presidio and GPT-4 through two Prolific surveys, each with 10 samples for 40 participants. \textbf{Binary Survey:} For each sample, participants viewed side-by-side annotations from both tools (order randomized) and chose the more accurate one. Figure~\ref{fig:binary-pii} shows GPT-4 was consistently preferred over Presidio for identifying privacy-sensitive information. \textbf{Multiple-Choice Survey:} Each of the 10 instances included 5 samples from each tool, displayed randomly. Participants had four options: (1) \emph{Accurate} (all sensitive info correctly annotated), (2) \emph{Under-Annotated} (some sensitive info missed), (3) \emph{Over-Annotated} (non-sensitive info included), and (4) \emph{Mixed-Annotated} (both missed and non-sensitive info annotated). Figure~\ref{fig:mult-pii} shows GPT-4 rated as ``Accurate'' across datasets.


\begin{figure}[!h]
    \centering
    % \begin{subfigure}{.48\linewidth}
    %     \includegraphics[scale=0.5,width=\linewidth]{figures/plot1_binary_customersim.pdf}
    %     \caption{CustomerSim}
    % \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \includegraphics[scale=0.22]{figures-paper/section3/plot1_binary_pii.pdf}
        \caption{Binary survey}
        \label{fig:binary-pii}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \includegraphics[scale=0.22]{figures-paper/section3/plot1_multiplechoice_pii.pdf}
        \caption{Multiple-choice survey}
        \label{fig:mult-pii}
    \end{subfigure}
    \caption{
    (a) GPT-4 was also preferred in SynBio by human annotators for identifying privacy-sensitive information.
    (b) GPT-4 also demonstrates in SynBio higher accuracy than Presidio for identifying privacy-sensitive information.
    }
    \label{fig:human-survey-quest-tools}
\end{figure}


\section{Why distinguish between sensitive and non-sensitive entities?}
\label{appendix:sens_non_sens_distinction}

Figure \ref{fig:app_FFT_All_log} shows the plots observed in Figure \ref{fig:FFT_All} in log-scale for better distinction.

\begin{figure}[t!]
    \centering
        \begin{subfigure}{.48\linewidth}
       \includegraphics[scale=0.25]{figures-paper/section2/train-privacy-customersim--pythia-gpt4_log.pdf}
        \caption{Privacy measure in Pythia}
        \label{fig:pyt_priv_log}
    \end{subfigure}
    \hfil
    \begin{subfigure}{.48\linewidth}
        \includegraphics[scale=0.25]{figures-paper/section2/test-utility-customersim--pythia-gpt4_log.pdf}
        \caption{Utility measure in Pythia}
        \label{fig:pyt_util_log}
    \end{subfigure}
    
    \begin{subfigure}{.48\linewidth}
       \includegraphics[scale=0.25]{figures-paper/section2/train-privacy-customersim--gemma-gpt4_log.pdf}
        \caption{Privacy measure in Gemma}
        \label{fig:gemma_priv_log}
    \end{subfigure}
    \hfil
    \begin{subfigure}{.48\linewidth}
        \includegraphics[scale=0.25]{figures-paper/section2/test-utility-customersim--gemma-gpt4_log.pdf}
        \caption{Utility measure in Gemma}
        \label{fig:gemma_util_log}
    \end{subfigure}


    \begin{subfigure}{.48\linewidth}
       \includegraphics[scale=0.25]{figures-paper/section2/train-privacy-customersim--llama2-gpt4_log.pdf}
        \caption{Privacy measure in Llama2}
        \label{fig:llama_priv_log}
    \end{subfigure}
    \hfil
    \begin{subfigure}{.48\linewidth}
        \includegraphics[scale=0.25]{figures-paper/section2/test-utility-customersim--llama2-gpt4_log.pdf}
        \caption{Utility measure in Llama2}
        \label{fig:llama_util_log}
    \end{subfigure}
    
    \caption{
    Our measures offer a more precise assessment of privacy and utility when fine-tuning LLMs by distinguishing between sensitive and non-sensitive tokens, revealing higher privacy (higher loss) for sensitive tokens and better utility (lower loss) for non-sensitive tokens compared to traditional measures that overlook this sensitivity-based distinction.
    }
    \label{fig:app_FFT_All_log}
\end{figure}


\section{Exploring the Tradeoffs between Privacy and Utility}
\label{appendix:exploring-tradeoffs}

We see a similar trade-off between utility and privacy when using Presidio annotations in Figure ~\ref{fig:appendix-fft}, as observed in Figure~\ref{fig:fft}. For the CustomerSim dataset, we notice that privacy decreases at the beginning of training, while utility improves. However, as training progresses, both metrics start to worsen. In contrast, the SynBio dataset shows a decline in both utility and privacy, except for the Pythia model, which initially experiences an improvement in utility during the first few epochs.
\vspace{-10pt}
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[scale=0.22]{figures-paper/section3/tradeoff_pu_customersim-200-presidio.pdf}
    \caption{\textbf{CustomerSim}}
    \label{fig:appendix-ffta}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[scale=0.22]{figures-paper/section3/tradeoff_pu_pii-8000-presidio.pdf}
    \caption{\textbf{SynBio}}
    \label{fig:appendifftb}    
    \end{subfigure}
    \caption{\textbf{Full fine-tuning} on (a) CustomerSim and  (b) SynBio using Presidio to annotate the privacy-sensitive information.}
    \label{fig:appendix-fft}
\end{figure}

A similar pattern as in Figure~\ref{fig:combined_dp} using GPT-4 annotations, has also emerged in figure \ref{fig:appendix-dp-csim} using Presidio annotations. We observe that in the CustomerSim dataset \ref{fig:appendix-dp-csim}, DP maintains privacy effectively with minimal degradation. However, when it comes to utility, we notice that lower noise values lead to better utility, but also result in a decrease in privacy. 
The same holds true for the SynBio dataset \ref{fig:appendix-dp-pii}, with the only exception being Gemma experiencing a decline in utility after a few iterations.
\begin{figure}[!h]
    \centering
    \begin{subfigure}{.48\linewidth}
        \includegraphics[scale=0.2,height=3cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_customersim-200_pythia-presidio.pdf}
        \caption{\textbf{Pythia}}
        \label{fig:appendix-dp_csima}
    \end{subfigure}
    \begin{subfigure}{.48\linewidth}
        \includegraphics[scale=0.2,height=3cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_customersim-200_gemma-presidio.pdf}
        \caption{\textbf{Gemma}}
        \label{fig:appendix-dp_csimb}
    \end{subfigure}
    \begin{subfigure}{.48\linewidth}
        \includegraphics[scale=0.2,height=3cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_customersim-200_llama2-presidio.pdf}
        \caption{\textbf{Llama2}}
        \label{fig:appendix-dp_csimc}
    \end{subfigure}
    \caption{\textbf{DP} on \emph{CustomerSim} dataset using Presidio tool for annotating privacy-sensitive information.}
    \label{fig:appendix-dp-csim}
\end{figure}


\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.2,height=3cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_pii-8000_pythia-presidio.pdf}
    \caption{\textbf{Pythia}}
    \label{fig:appendix-dp_piia}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.2,height=3cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_pii-8000_gemma-presidio.pdf}
    \caption{\textbf{Gemma}}
    \label{fig:appendix-dp_piib}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.2,height=3cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_pii-8000_llama2-presidio.pdf}
    \caption{\textbf{Llama2}}
    \label{fig:appendix-dp_piic}
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{\textbf{DP} on \emph{SynBio} dataset using Presidio tool for annotating privacy-sensitive information.}
    \label{fig:appendix-dp-pii}
\end{figure}

Comparing the results from Figures \ref{fig:appendix-lora16-csim} and \ref{fig:appendix-lora32-csim} using Presidio annotations with Figure~\ref{fig:combined_lora}, which used GPT-4 annotations, we can observe the same trend. This suggests that the findings are consistent across different annotation methods. Similarly, when we analyze the trade-off for the SynBio dataset in Figures ~\ref{fig:appendix-lora16-pii} and \ref{fig:appendix-lora32-pii}, we see similar observations to those discussed in Figure~\ref{fig:combined_lora_synbio}, using GPT-4 annotations.


\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_customersim-200_pythia-presidio.pdf}
    \caption{\textbf{Pythia}}
    \label{fig:appendix-lora16_csima}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_customersim-200_gemma-presidio.pdf}
    \caption{\textbf{Gemma}}
    \label{fig:appendix-lora16_csimb}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_customersim-200_llama2-presidio.pdf}
    \caption{\textbf{Llama2}}
    \label{fig:appendix-lora16_csimc}
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{\textbf{Lora with rank 16} on \emph{CustomerSim} dataset using Presidio tool for annotating privacy-sensitive information.}
    \label{fig:appendix-lora16-csim}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_customersim-200_pythia-presidio.pdf}
    \caption{\textbf{Pythia}}
    \label{fig:appendix-lora32_csima}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_customersim-200_gemma-presidio.pdf}
    \caption{\textbf{Gemma}}
    \label{fig:appendix-lora32_csimb}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_customersim-200_llama2-presidio.pdf}
    \caption{\textbf{Llama2}}
    \label{fig:appendix-lora32_csimc}
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{\textbf{Lora with rank 32} on \emph{CustomerSim} dataset using Presidio tool for annotating privacy-sensitive information.}
    \label{fig:appendix-lora32-csim}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_pii-8000_pythia-presidio.pdf}
    \caption{\textbf{Pythia}}
    \label{fig:appendix-lora16_piia}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_pii-8000_gemma-presidio.pdf}
    \caption{\textbf{Gemma}}
    \label{fig:appendix-lora16_piib}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_pii-8000_llama2-presidio.pdf}
    \caption{\textbf{Llama2}}
    \label{fig:appendix-lora16_piic}
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{\textbf{Lora with rank 16} on \emph{SynBio} dataset using Presidio tool for annotating privacy-sensitive information.}
    \label{fig:appendix-lora16-pii}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_pii-8000_pythia-presidio.pdf}
    \caption{\textbf{Pythia}}
    \label{fig:appendix-lora32_piia}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_pii-8000_gemma-presidio.pdf}
    \caption{\textbf{Gemma}}
    \label{fig:appendix-lora32_piib}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_pii-8000_llama2-presidio.pdf}
    \caption{\textbf{Llama2}}
    \label{fig:appendix-lora32_piic}
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{\textbf{Lora with rank 32} on \emph{SynBio} dataset using Presidio tool for annotating privacy-sensitive information.}
    \label{fig:appendix-lora32-pii}
\end{figure}

\vspace{4mm}
Analyzing the trade-offs between privacy, utility, and efficiency in Figures~\ref{fig:appendix-fdl_csim} and \ref{fig:appendix-fdl_pii} for CustomerSim and SynBio using Presidio annotations reveals the same efficiency ranking observed in Figure~\ref{fig:combined_fdl} with GPT-4 annotations. LoRA is the most efficient method, while DP is the least efficient, requireing the highest number of FLOPs.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section4/tradeoff_pu_all_fix_customersim_pythia-presidio.pdf}
    \caption{\textbf{Pythia}}
    \label{fig:appendix-fdl_csim1}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section4/tradeoff_pu_all_fix_customersim_gemma-presidio.pdf}
    \caption{\textbf{Gemma}}
    \label{fig:appendix-fdl_csim2}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section4/tradeoff_pu_all_fix_customersim_llama2-presidio.pdf}
    \caption{\textbf{Llama2}}
    \label{fig:appendix-fdl_csim3}
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{\textbf{Full fine-tuning, DP and LoRA} on \emph{CustomerSim} with Presidio annotations for privacy-sensitive information.}
    \label{fig:appendix-fdl_csim}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section4/tradeoff_pu_all_fix_pii_pythia-presidio.pdf}
    \caption{\textbf{Pythia}}
    \label{fig:appendix-fdl_pii1}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section4/tradeoff_pu_all_fix_pii_gemma-presidio.pdf}
    \caption{\textbf{Gemma}}
    \label{fig:appendix-fdl_pii2}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5,width=\linewidth]{figures-paper/section4/tradeoff_pu_all_fix_pii_llama2-presidio.pdf}
    \caption{\textbf{Llama2}}
    \label{fig:appendix-fdl_pii3}
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{\textbf{Full fine-tuning, DP and LoRA} on \emph{SynBio} with Presidio annotations for privacy-sensitive information.}
    \label{fig:appendix-fdl_pii}
\end{figure}



\section{Comparison of fine-tuning methods}
\label{appendix:comparison}


\begin{table}[H]
\centering
\scriptsize
\caption{Comparison of all fine-tuning methods across privacy, utility, efficiency, and benchmark performance. LoRA is seen to outperform FFT and DP over all dimensions.}
\label{tab:overall}
\begin{tabular}{|c|c|c|c|c|}
% \toprule
\hline
 & \textbf{Utility} & \textbf{Privacy} & \textbf{Efficiency} & \textbf{Benchmark} \\
% \toprule
\hline
\textbf{FFT} & \textcolor{gemmagreen}{Good} & \textcolor{red}{Poor} & \textcolor{yellow}{Moderate} & \textcolor{red}{Poor} \\
% \midrule
\hline
\textbf{DP} & \textcolor{yellow}{Moderate} & \textcolor{gemmagreen}{Good} & \textcolor{red}{Poor} & \textcolor{red}{Poor} \\
% \midrule
\hline
\rowcolor{green!20}
\textbf{LoRA} & \textcolor{gemmagreen}{Good} & \textcolor{gemmagreen}{Good} & \textcolor{gemmagreen}{Good} & \textcolor{gemmagreen}{Good} \\
% \bottomrule
\hline
\end{tabular}
\end{table}