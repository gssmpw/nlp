\section{Comparison of Fine-tuning Methods}
\label{sec:comparison}

In this section, we compare all fine-tuning methods across three key aspects: privacy, utility, and efficiency.
Recall that privacy is measured as training loss on sensitive tokens and utility is measured as test loss on non-sensitive tokens.
For efficiency, we measure floating point operations (FLOPs) based on the number of operations incurred (e.g., matrix multiplication, addition, etc.) during training.
% We report this metric for one instance as it scales linearly.
In this section, for DP we use a noise ratio of $\sigma = 0.1$ and for LoRA we use $r=16,\alpha=16 $. Figures~\ref{fig:fdl_csim1}-\ref{fig:fdl_csim3} present the Pareto plots for CustomerSim, comparing the three fine-tuning strategies across three dimensions: privacy, utility, and efficiency. 

\noindent
\textbf{Privacy}: Regarding \textit{privacy}, as previously noted, full fine-tuning shows poor privacy performance over extended training, while DP achieves the highest privacy levels.
LoRA provides a similar degree of privacy compared to DP throughout most training epochs for small-scale models (Pythia and Gemma), but declines with the large-scale model (Llama2).

\noindent
\textbf{Utility:} In terms of \textit{utility}, full fine-tuning maintains relatively strong performance throughout most of its training (\textit{right on the x-axis}).
DP while yielding good utility in smaller models (Pythia and Gemma), performs poorly in the larger model (Llama2) and overall needs more epochs to reach higher utility levels.
In contrast, LoRA consistently preserves higher utility across the entire training period.


\noindent
\textbf{Efficiency}: The color bar in Figure~\ref{fig:combined_fdl} highlights the FLOPs intensity associated with each model across all fine-tuning strategies.
% For CustomerSim (Figures~\ref{fig:fdl_csim1}, \ref{fig:fdl_csim2} and \ref{fig:fdl_csim3}), in terms of \textit{efficiency},
DP requires the \textit{highest number of FLOPs} (in \textcolor{red}{red}) due to the need for per-sample gradient computation, where each sample corresponds to a token within each training sequence.
FFT demands a \textit{moderate number of FLOPs} (in \textcolor{blue}{blue}), proportional to the total number of parameters.
Finally, LoRA requires the \textit{fewest FLOPs} (in \textcolor{green}{green}), as especially during backpropagation, most operations only operate on the low-rank matrices.
% Similar patterns are observed for the SynBio dataset in Figures~\ref{fig:fdl_pii1}, \ref{fig:fdl_pii2} and \ref{fig:fdl_pii3}). 

\begin{figure*}[!t]
    \centering
        \begin{subfigure}{.32\linewidth}
       \includegraphics[scale=0.25]{figures-paper/section4/SCIQ.pdf}
        \caption{Benchmark SCIQ}
        \label{fig:sciq_all}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
       \includegraphics[scale=0.25]{figures-paper/section4/MMLU.pdf}
        \caption{Benchmark MMLU}
        \label{fig:mmlu_all}
    \end{subfigure}
     \begin{subfigure}{.32\linewidth}
       \includegraphics[scale=0.25]{figures-paper/section4/Hellaswag.pdf}
        \caption{Benchmark HellaSwag}
        \label{fig:hs_all}
    \end{subfigure}
    \caption{
    % Comparison of all methods on the benchmark datasets.   
    LoRA demonstrates superior knowledge retention on benchmark datasets throughout the training regime on the CustomerSim dataset, while FFT and DP show fragility with sharp and gradual performance declines, respectively.
    }
    \label{fig:bench-all}
\end{figure*}

\noindent
\textbf{Benchmark performance:} Figure~\ref{fig:bench-all} shows strong knowledge retention capabilities of LoRA on the three benchmarks after being fine-tuned on the CustomerSim dataset for 50 epochs.
FFT and DP, on the other hand, decline sharply and gradually, respectively from the pretrained base model performance.
% The sharp and gradual declines in the performance of FFT and DP-SGD respectively hint at fragility.
%\sdcomment{add here}

\noindent
Assessing all the above aspects, we can observe the following: 

1. \textit{Full fine-tuning} achieves \textit{high utility initially, but starts diminishing} after a few epochs, and also witnesses a significant \textit{drop in privacy}.
It has a relatively \textit{high computational cost}.
% due to the large number of FLOPs associated proportional to the number of model parameters.
Additionally, the fully fine-tuned model's performance \textit{diminishes significantly on benchmark datasets}.

2. \textit{DP} offers the \textit{strongest privacy protection} and achieves a \textit{reasonable privacy-utility tradeoff in smaller models}.
However, this tradeoff deteriorates in larger models.
% due to the extensive number of noisy gradient updates.
DP incurs the \textit{highest computational cost} as its per-sample noisy gradient updates significantly increase FLOPs and also memory requirements.
Additionally, models fine-tuned with DP exhibit a \textit{gradual decline in their benchmark performance} over the course of training.

3. \textit{LoRA}, a parameter-efficient fine-tuning method, maintains \textit{high utility} and achieves \textit{privacy levels comparable to DP} in smaller models, though this advantage reduces in larger ones. 
Figure~\ref{fig:fdl_csim3} shows that while LoRA preserves less privacy as training progresses, it is possible to select checkpoints that balance strong privacy with utility.
This finding challenges the prevailing notion that privacy must come at the cost of high efficiency, demonstrating that \textit{\textbf{LoRA can offer privacy benefits}}.
Moreover, LoRA-tuned models \textit{retain performance on benchmark datasets} close to that of pre-trained models throughout training. Figure \ref{tab:overall} in Appendix \ref{appendix:comparison} shows a comprehensive comparison across all the fine-tuning methods. 