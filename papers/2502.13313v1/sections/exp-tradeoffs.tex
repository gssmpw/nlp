\section{Privacy-Utility-Efficiency tradeoffs in different fine-tuning methods}
\label{sec:trade_off}
%Mention about the datasets and models used in the experiments.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[scale=0.22]{figures-paper/section3/tradeoff_pu_customersim-200-gpt4.pdf}
    \caption{CustomerSim}
    \label{fig:ffta}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[scale=0.22]{figures-paper/section3/tradeoff_pu_pii-8000-gpt4.pdf}
    \caption{SynBio}
    \label{fig:fftb}    
    \end{subfigure}
    %\captionsetup{font=scriptsize}
    \caption{
    Privacy-utility trade-off shows that privacy increases with higher training loss on sensitive tokens, while utility improves with lower test loss on non-sensitive tokens, enabling desired checkpoint selection to balance both objectives.
    }
    \label{fig:fft}
\end{figure}

\begin{figure*}[h!]
    \centering
        \begin{subfigure}{.32\linewidth}
       \includegraphics[scale=0.25,height=3.5cm]{figures-paper/section3/SCIQ_FFT.pdf}
        \caption{SCIQ Benchmark}
        \label{fig:sciq_fft}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
       \includegraphics[scale=0.25,height=3.5cm]{figures-paper/section3/MMLU_FFT.pdf}
        \caption{MMLU Benchmark}
        \label{fig:mmlu_fft}
    \end{subfigure}
     \begin{subfigure}{.32\linewidth}
       \includegraphics[scale=0.25,height=3.5cm]{figures-paper/section3/Hellaswag_FFT.pdf}
        \caption{HellaSwag Benchmark}
        \label{fig:hs_fft}
    \end{subfigure}
    \caption{
    % \textbf{Benchmark performance of fully fine-tuned Gemma model}
    Full fine-tuning of the Gemma model leads to a significant drop in accuracy on benchmark datasets, with declines of approximately 75\%, 9\%, and 30\% for SCIQ, MMLU, and HellaSwag, respectively.
    }
    \label{fig:bench-fft}
\end{figure*}

We use the distinction between sensitive and non-sensitive tokens to study the privacy and utility impact of training models with three different fine-tuning methods: full fine-tuning (FFT), Differentially Privacy (DP), and Low-Rank Adaptation (LoRA).
We also investigate the computational efficiency of each method.
Our goal is to answer the following questions: ``\textit{How prone is each method to recollecting the sensitive parts of the training data? (Privacy)'' , ``How effective is each method at predicting non-sensitive parts of test data? (Utility)'' , ``What is the computational cost associated with each method? (Efficiency})''.

To answer these questions, we use each fine-tuning method to train three models from different families, Pythia-1B~\cite{biderman2023pythia}, Gemma-2B~\cite{team2024gemma} and Llama2-7B~\cite{touvron2023llama}, on two datasets, CustomerSim and SynBio.
More information about the datasets and our methodology for distinguishing between sensitive and non-sensitive tokens can be found in Section~\ref{sec:sens_non_sens_distinction}.
We train the models for 50 epochs on each dataset.
Details on hyperparameters can be found in Appendix~\ref{appendix:hp-models}.

For each fine-tuning method, model and dataset we report three metrics: 1) privacy as the loss on sensitive tokens (annotated by GPT-4) in the training data, 2) utility as the loss on non-sensitive tokens (the remaining tokens) on a held-out test set, and 3) efficiency based on the relative amount of computation and memory usage of each method.
Additionally, to assess how fine-tuning affects the underlying abilities and knowledge of the base model, we measure the performance of the fine-tuned Gemma model (trained with CustomerSim data) on general language understanding benchmarks: SCIQ \cite{SciQ}, a dataset of over 13,000 crowdsourced questions on Physics, Chemistry, and Biology; MMLU \cite{hendryckstest2021, hendrycks2021ethics}, a large multi-task dataset covering various domains of knowledge; and Hellaswag \cite{zellers-etal-2019-hellaswag}, a dataset for commonsense inference.

\noindent
\textbf{Update rules:}
For each fine-tuning method, we describe how updated weights $W_{t + 1}$ are computed from the previous weights $W_t$ in each step, where $W_0$ are the weights of the pre-trained base model before fine-tuning.
We use $X$ to refer to a batch of $|X|$ datapoints and $x_i$ to refer to individual datapoint, $\mathcal{M}_{W}$ to refer to the model parameterized by weights $W$, and $\mathcal{L}(\mathcal{M}_{W}(X), X)$ to refer to the autoregressive cross-entropy loss of the model on data $X$.
We denote by $\nabla_{W} \mathcal{L}(...)$ as the gradient of the loss wrt. weights $W$ and $\eta$ is the learning rate.

\noindent
\textbf{Efficiency:}
The efficiency of each method is determined by the amount of computation it requires, and also other factors such as memory requirements, which can affect the usable batch-size and thus the overall training throughput.
Following~\cite{kaplan2020scaling}, we estimate the amount of training compute ($C$) in floating point operations (FLOPs) for full fine-tuning as $C_{\text{FFT}} = 6 D N$, where $D$ is the number of training tokens and $N$, the number of model parameters.
For each method, we report its compute requirements relative to the FFT-baseline based on measurements using the PyTorch profiler\footnote{\url{https://pytorch.org/docs/stable/profiler}}.
We also comment on other factors that affect training throughput.


\subsection{Method 1 : Full fine-tuning}

\textbf{Update rules:}
Full fine-tuning (FFT) updates all model parameters at each step:
\begin{equation}
    W_{t + 1} = W_t - \eta \nabla_{W_t} \mathcal{L}(\mathcal{M}_{W_{t}}(X), X)
\end{equation}



\noindent
\textbf{Privacy-Utility trade-off:}
Figures~\ref{fig:ffta} and~\ref{fig:fftb} show the \textit{privacy-utility trade-off} for the CustomerSim and SynBio datasets, respectively.
In these figures, privacy increases with the training loss on sensitive tokens (\textit{up $\Uparrow$ on the y-axis}), while utility increases when the test loss on non-sensitive tokens decreases (\textit{right $\Longrightarrow$ on the x-axis}).
Each curve starts with the baseline performance of the pre-trained model.
For CustomerSim (Figure~\ref{fig:ffta}), as training advances (\textit{denoted by an arrow $\rightarrow$} on the lines), privacy progressively decreases (\textit{lower on the y-axis}), while utility improves (\textit{rightward on the x-axis}) for approximately the first 5 epochs across all models before stabilizing and eventually declining (\textit{leftward on the x-axis}).
However, for SynBio (Figure \ref{fig:fftb}), the privacy-utility trade-off primarily worsens for Gemma and Llama models. On examining these curves, one can select a desired checkpoint that aligns with specified privacy and utility thresholds.

\noindent
\textbf{Impact on benchmark datasets:}
Figures \ref{fig:sciq_fft}, \ref{fig:mmlu_fft}, and \ref{fig:hs_fft} show the fully fine-tuned Gemma model's accuracy at each epoch for the three benchmarks: SCIQ, MMLU, and Hellaswag, respectively. Note that the accuracy corresponding to the first point represents the performance of the pre-trained model. We observe that full fine-tuning shows a substantial decline in accuracy (around $0.75$, $0.09$, and $0.3$ decrease in accuracy in SCIQ, MMLU, and HellaSwag,  respectively).

\noindent
\textbf{Efficiency:}
FFT serves as our efficiency baseline.
It has moderate compute requirements (discussed above), and relatively high memory requirements, since in addition to the input-dependent activations, we need to keep four numbers per model parameter in GPU memory: the parameter value, its gradient, and two optimizer states (first and second moments of the gradient for Adam \cite{kingma2014adam}).

\noindent
\textbf{Takeaway:}
FFT offers poor privacy-utility trade-offs, since gains in utility in most cases come at the cost of a significant loss in privacy.
During FFT, models learn to both predict the training distribution better, but also quickly learn to recollect sensitive tokens.
In addition, FFT deteriorates the base performance of the model, as can be seen by the rapid decline of the benchmark scores.
FFT is moderately efficient and has relatively high memory requirements. The degree of measures along the \textit{trade-off, knowledge retention, and efficiency} are: 

\indent Utility-privacy trade-offs: \textit{poor} \\
\indent Retention of base performance: \textit{poor} \\
\indent Efficiency: \textit{moderate}


\begin{figure*}[h!]
    \centering
    % First row of subfigures (CustomerSim dataset)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_customersim-200_pythia-gpt4.pdf}
        \caption{Pythia (CustomerSim)}
        \label{fig:dp_csima}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_customersim-200_gemma-gpt4.pdf}
        \caption{Gemma (CustomerSim)}
        \label{fig:dp_csimb}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_customersim-200_llama2-gpt4.pdf}
        \caption{Llama2 (CustomerSim)}
        \label{fig:dp_csimc}
    \end{subfigure}

    % Second row of subfigures (SynBio dataset)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_pii-8000_pythia-gpt4.pdf}
        \caption{Pythia (SynBio)}
        \label{fig:dp_piia}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_pii-8000_gemma-gpt4.pdf}
        \caption{Gemma (SynBio)}
        \label{fig:dp_piib}
    \end{subfigure}    
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_dpsgd_pu_sp_pii-8000_llama2-gpt4.pdf}
        \caption{Llama2 (SynBio)}
        \label{fig:dp_piic}
    \end{subfigure}

    \caption{
        Privacy-utility trade-offs for DP across models and datasets. \emph{Top row:} For the \emph{CustomerSim} dataset, DP enhances privacy across all models, though higher noise scales reduce utility, especially for larger models. \emph{Bottom row:} For the \emph{SynBio} dataset, DP similarly improves privacy, with utility declines at higher noise levels.
        Gemma shows a unique utility drop after two epochs, highlighting the complexity of privacy-utility trade-off.
    }
    \label{fig:combined_dp}
\end{figure*}


\subsection{Method 2 : Differential Privacy}

Differential Privacy (DP) algorithms ~\cite{10.1145/2976749.2978318} aim to safeguard the privacy of individual training data points by limiting their influence on the gradient updates during training.

\noindent
\textbf{Update rules:}
DP clips the \textit{$l_2$} norm of each data point’s gradient at a threshold $T$, followed by adding noise with magnitude $\sigma$ to each clipped gradient.
The purpose of clipping is to reduce data sensitivity by ensuring that the impact of data points with high gradient magnitudes on the model parameters is limited.
% The added noise serves to smooth the probability distribution \bgcomment{probability distribution of what?}, preventing any data point’s outcome from being disproportionately likely.
Adding noise further obscures the contribution of individual data points, making it difficult to infer specific data points from the model.

\vspace{-10pt}
\small{
\begin{equation}
%\begin{align}
\begin{split}
    W_{t + 1} &= W_t - \eta \, \text{Noise} \left( \frac{1}{B} \sum_i \text{Clip}\left( \nabla_{W_t} \mathcal{L}(\mathcal{M}_{W_t}(x_i), x_i) \right) \right) \\
    \text{Clip}(y) &= y / \max \left( 1, \frac{\lVert y \rVert_2}{T} \right) \\
    \text{Noise}(y) &= y + \mathcal{N}(0, \sigma^2 T^2 \mathds{1})
\end{split}
%\end{align}
\end{equation}
}
\normalsize
\vspace{-10pt}

We vary the noise hyperparameter $\sigma$ for the experiments.
The clipping gradient norm $T$ is fixed at $10^{-2}$, as in \cite{shi-etal-2022-selective}.

\begin{figure*}[h!]
    \centering
        \begin{subfigure}{.3\linewidth}
       \includegraphics[scale=0.25,height=3.5cm]{figures-paper/section3/SCIQ_DPSGD.pdf}
        \caption{SCIQ Benchmark}
        \label{fig:sciq_dpsgd}
    \end{subfigure}
    \begin{subfigure}{.3\linewidth}
       \includegraphics[scale=0.25,height=3.5cm]{figures-paper/section3/MMLU_DPSGD.pdf}
        \caption{MMLU Benchmark}
        \label{fig:mmlu_dpsgd}
    \end{subfigure}
     \begin{subfigure}{.3\linewidth}
       \includegraphics[scale=0.25,height=3.5cm]{figures-paper/section3/Hellaswag_DPSGD.pdf}
        \caption{HellaSwag Benchmark}
        \label{fig:hs_dpsgd}
    \end{subfigure}
    \caption{
    % Benchmark performance of DP-SGD fine-tuned Gemma model
    Fine-tuning Gemma with DP results in a substantial accuracy declines of 70\%, 10\%, and 30\% across the benchmarks.
    }
    \label{fig:bench-dpsgd}
\end{figure*}

\noindent
\textbf{Privacy-Utility trade-off:}
Figures \ref{fig:dp_csima}, \ref{fig:dp_csimb}, and \ref{fig:dp_csimc} illustrate the privacy-utility trade-off when varying the noise $\sigma$ on the CustomerSim dataset across the Pythia, Gemma, and Llama2 models.
As in previous plots, each curve begins with the performance of the pre-trained model.
It is evident that DP maintains privacy effectively with minimal degradation.
But there is a trade-off between privacy and utility.
For all models, lower noise values such as $\sigma = 0.1$ are able to achieve better utility than higher ones such as $\sigma = 0.5$ and $\sigma = 0.9$, but they also decrease privacy more.
While the total amount of utility achievable with DP is limited, especially for the larger Llama2-7B model, overall, it provides good privacy-utility trade-offs.
Similar trends can be observed for the SynBio dataset in Figures~\ref{fig:dp_piia} and \ref{fig:dp_piic}, but for Figure~\ref{fig:dp_piib} (with Gemma) utility declines after approximately two epochs.  

\noindent
\textbf{Impact on benchmark datasets:} Figures~\ref{fig:sciq_dpsgd}, \ref{fig:mmlu_dpsgd} and \ref{fig:hs_dpsgd} show the benchmark accuracy over epochs for the DP fine-tuned Gemma model with $\sigma=0.1$ on SCIQ, MMLU, and HellaSwag datasets, respectively. Accuracy drops gradually with increasing epochs for all benchmarks, stabilizing at lower levels, indicating that fine-tuning with DP significantly reduces the knowledge retention capacity.

\noindent
\textbf{Efficiency:}
Differential privacy comes with a high computational cost, since the gradients of each datapoint need norm and clipping computations, and additional noise values are added.
Empirically, we observe a relative FLOPs requirement of $C_{\text{DP}} / C_{\text{FFT}} = 1.33$.
Additionally, the per-sample operations required for clipping mean that we need to keep copies of the gradient for each datapoint in memory, which requires substantially more GPU memory, which decreases the feasible batch size and the overall training throughput even further.

\begin{figure*}[t]
    \centering
    % First row of subfigures (LoRA with rank 16 on CustomerSim)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_customersim-200_pythia-gpt4.pdf}
        \caption{Pythia (Rank 16)}
        \label{fig:lora_16_csima}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_customersim-200_gemma-gpt4.pdf}
        \caption{Gemma (Rank 16)}
        \label{fig:lora_16_csimb}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_customersim-200_llama2-gpt4.pdf}
        \caption{Llama2 (Rank 16)}
        \label{fig:lora_16_csimc}
    \end{subfigure}

    % Second row of subfigures (LoRA with rank 32 on CustomerSim)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_customersim-200_pythia-gpt4.pdf}
        \caption{Pythia (Rank 32)}
        \label{fig:lora_32_csima}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_customersim-200_gemma-gpt4.pdf}
        \caption{Gemma (Rank 32)}
        \label{fig:lora_32_csimb}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_customersim-200_llama2-gpt4.pdf}
        \caption{Llama2 (Rank 32)}
        \label{fig:lora_32_csimc}
    \end{subfigure}

    \caption{
        Privacy-utility trade-offs for LoRA fine-tuning on the \emph{CustomerSim} dataset. \emph{Top row:} LoRA with rank 16 shows that smaller models achieve a better privacy-utility trade-off, while larger models retain utility but experience reduced privacy with more epochs. \emph{Bottom row:} LoRA with rank 32 yields similar results, with smaller models performing better in privacy-utility trade-off and larger models maintaining utility but with privacy reduction as epochs increase.
    }
    \label{fig:combined_lora}
\end{figure*}

\noindent
\textbf{Takeaway:} DP offers a reasonable privacy-utility trade-off and is less susceptible to learning sensitive data. Increasing noise rates lead to poor utility across all models.
Convergence with DP
%\bgcomment{convergence of what?} 
is not guaranteed, particularly in larger models such as Llama2.
% This issue accounts for the lower utility observed in the Llama2 7B models in Figures \ref{fig:dp_csimc} and \ref{fig:dp_piic}.
The privacy gains come at the cost of efficiency.
Additionally, fine-tuning with DP leads to quick decline in the benchmark performance. The degree of measures along the \textit{tradeoff, knowledge retention, and efficiency} are: 

\indent Utility-privacy trade-offs: \textit{moderate} \\
\indent Retention of base performance: \textit{poor} \\
\indent Efficiency: \textit{poor}

\begin{figure*}[ht!]
    \centering
    % First row of subfigures (LoRA with rank 16 on SynBio)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_pii-8000_pythia-gpt4.pdf}
        \caption{Pythia (Rank 16)}
        \label{fig:lora_16_piia}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_pii-8000_gemma-gpt4.pdf}
        \caption{Gemma (Rank 16)}
        \label{fig:lora_16_piib}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_16_pii-8000_llama2-gpt4.pdf}
        \caption{Llama2 (Rank 16)}
        \label{fig:lora_16_piic}
    \end{subfigure}

    % Second row of subfigures (LoRA with rank 32 on SynBio)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_pii-8000_pythia-gpt4.pdf}
        \caption{Pythia (Rank 32)}
        \label{fig:lora_32_piia}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_pii-8000_gemma-gpt4.pdf}
        \caption{Gemma (Rank 32)}
        \label{fig:lora_32_piib}
    \end{subfigure}    
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.28,height=3.5cm]{figures-paper/section3/tradeoff_lora_pu_sp_rank_32_pii-8000_llama2-gpt4.pdf}
        \caption{Llama2 (Rank 32)}
        \label{fig:lora_32_piic}
    \end{subfigure}

    \caption{
        Privacy-utility trade-offs for LoRA fine-tuning on the \emph{SynBio} dataset. \emph{Top row:} LoRA with rank 16 shows that smaller models achieve a better privacy-utility trade-off, while larger models retain utility but experience reduced privacy with more epochs. \emph{Bottom row:} LoRA with rank 32 yields similar results, with smaller models performing better in privacy-utility trade-off and larger models maintaining utility but with privacy reduction as epochs increase.
    }
    \label{fig:combined_lora_synbio}
\end{figure*}


\begin{figure*}[ht!]
    \centering
        \begin{subfigure}{.3\linewidth}
       \includegraphics[scale=0.3,height=3.5cm]{figures-paper/section3/SCIQ_LoRA.pdf}
        \caption{SCIQ Benchmark}
        \label{fig:sciq_lora}
    \end{subfigure}
    \begin{subfigure}{.3\linewidth}
       \includegraphics[scale=0.3,height=3.5cm]{figures-paper/section3/MMLU_LoRA.pdf}
        \caption{MMLU Benchmark}
        \label{fig:mmlu_lora}
    \end{subfigure}
     \begin{subfigure}{.3\linewidth}
       \includegraphics[scale=0.3,height=3.5cm]{figures-paper/section3/Hellaswag_LoRA.pdf}
        \caption{HellaSwag Benchmark}
        \label{fig:hs_lora}
    \end{subfigure}
    \caption{
    LoRA fine-tuned model maintains accuracy levels close to the pre-trained model with declines of 5\%, 3\% and 3\% across SCIQ, MMLU, and Hellaswag benchmarks, highlighting its effectiveness in knowledge retention.
    }
    \label{fig:bench-lora}
\end{figure*}


\subsection{Method 3 : Low-Rank Adaptation (LoRA)}

LoRA~\cite{hu2022lora} is a parameter-efficient fine-tuning method developed to reduce the compute and memory requirements of fine-tuning LLMs, and to reduce the size of storing fine-tuned checkpoints.
It enjoys large popularity for LLM fine-tuning.

\textbf{Update rules:}

\begin{equation}
\begin{split}
    W_{t + 1} &= W_0 + \frac{\alpha}{r} \Delta W_{t + 1}
 \\
    \Delta W_{t + 1} &= \Delta W_t - \eta \nabla_{\Delta W_t} \mathcal{L}(\mathcal{M}_{W_t}(X), X)
\end{split}
\label{eq:compute_fft}
\end{equation}



LoRA freezes the weights of the pretrained base model $W_0$ and only fine-tunes an adapter matrix $\Delta W_t$.
During training, $\Delta W_t$ is stored separately from $W_0$ as two low-rank matrices with rank $r$: $\Delta W_t = B_t A_t$ with $B_t \in \mathbb{R}^{d \times r}, A_t \in \mathbb{R}^{r \times k}, W_0 \in \mathbb{R}^{d \times k}$.
$r$ is typically very small, often between $4$ and $32$.
$\alpha$ is a constant that controls how much the LoRA adapters affect the behavior of the base weights $W_0$.

We hypothesize that LoRA's low-rank updates restrict the model's capacity to memorize precise details, which could have an effect similar to the noisy updates in DP.
Therefore, LoRA has the potential to provide better privacy-utility trade-offs than FFT, similar to DP, while also being computationally more efficient.


\noindent
\textbf{Privacy-Utility trade-off:}
We investigate the effects of varying both the rank $r$ and scaling parameter $\alpha$ of LoRA.
We use common rank values of $16$ and $32$ and $\alpha \in \{16,32,64,128\}$.
While LoRA has been explored for privacy in conjunction with DP~\cite{yu2022differentiallyprivatefinetuninglanguage}, there has been no prior work that specifically examines the privacy benefits of LoRA alone. 

Figures~\ref{fig:lora_16_csima}-~\ref{fig:lora_16_csimc} show the trade-off with rank $16$ and varying parameters of $\alpha$ for the CustomerSim dataset across Pythia, Gemma and Llama2 models. As seen in Figures \ref{fig:lora_16_csima} and \ref{fig:lora_16_csimb}, smaller-scale models (Pythia and Gemma) exhibit a better privacy-utility trade-off when the rank and $\alpha$ values are equal, compared to the other configurations. For the larger model, the privacy declines at later epochs, while utility is mostly retained.
This trend is similar to the one observed for rank $32$ in Figures~\ref{fig:lora_32_csima}-~\ref{fig:lora_32_csimc}. 
We also analyze the trade-off for the SynBio dataset in Figures~\ref{fig:lora_16_piia}-~\ref{fig:lora_16_piic} for rank 16, and Figures~\ref{fig:lora_32_piia}-~\ref{fig:lora_32_piic} for rank 32, which make similar observations. 
However, due to the more unstructured nature of the SynBio dataset, there is a larger reduction in utility after certain epochs compared to the CustomerSim dataset.

\noindent
\textbf{Impact on benchmark datasets:} We use the LoRA model with configuration $r=16, \alpha=16$ for this experiment. Figures \ref{fig:sciq_lora}-~\ref{fig:hs_lora} illustrate the LoRA fine-tuned model's accuracy at each epoch for the three benchmarks. 
%Note that the accuracy corresponding to the first point on x-axis represents the performance of the pre-trained model.
The LoRA-based fine-tuned Gemma model retains performance levels close to those of the pre-trained model.

\noindent
\textbf{Efficiency:}
During training, LoRA has slightly larger compute requirements for the forward pass than full fine-tuning, since additional FLOPs are required for the adapter matrices $\Delta W$, though they are much smaller than the full base weight $W_0$.
However, during the backward pass, LoRA requires less compute, since no gradients need to be computed for the base weights $W_0$.
We observe a relative FLOPs requirement of $C_{\text{LoRA}} / C_{\text{FFT}} \approx 0.65$.
The original paper~\cite{hu2022lora} reports a 25\% speedup during training.
LoRA has needs of less GPU memory than FFT, since no optimizer states and gradients need to be stored for the base weights $W_0$, which makes it possible to run it with larger batch-sizes and thus an overall increased training throughput.

\begin{figure*}[t]
    \centering
    % First row of subfigures (CustomerSim dataset)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures-paper/section4/tradeoff_pu_all_fix_customersim_pythia-gpt4.pdf}
        \caption{Pythia (CustomerSim)}
        \label{fig:fdl_csim1}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures-paper/section4/tradeoff_pu_all_fix_customersim_gemma-gpt4.pdf}
        \caption{Gemma (CustomerSim)}
        \label{fig:fdl_csim2}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures-paper/section4/tradeoff_pu_all_fix_customersim_llama2-gpt4.pdf}
        \caption{Llama2 (CustomerSim)}
        \label{fig:fdl_csim3}
    \end{subfigure}

    % Second row of subfigures (SynBio dataset)
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures-paper/section4/tradeoff_pu_all_fix_pii_pythia-gpt4.pdf}
        \caption{Pythia (SynBio)}
        \label{fig:fdl_pii1}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures-paper/section4/tradeoff_pu_all_fix_pii_gemma-gpt4.pdf}
        \caption{Gemma (SynBio)}
        \label{fig:fdl_pii2}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures-paper/section4/tradeoff_pu_all_fix_pii_llama2-gpt4.pdf}
        \caption{Llama2 (SynBio)}
        \label{fig:fdl_pii3}
    \end{subfigure}

    \caption{
        Privacy-utility tradeoffs for different fine-tuning methods on the \emph{CustomerSim} and \emph{SynBio} datasets. \emph{Top row:} On the CustomerSim dataset, DP achieves high privacy with a significant computational cost, full fine-tuning maximizes utility but offers less privacy, and LoRA provides a balanced trade-off with the lowest computational overhead. \emph{Bottom row:} Similar trends are observed for the SynBio dataset.
        % , with DP-SGD excelling in privacy at high computational cost, full fine-tuning performing best in utility, and LoRA balancing privacy, utility, and efficiency.
    }
    \label{fig:combined_fdl}
\end{figure*}

\noindent
\textbf{Takeaway:}
We are one of the first works to explore the privacy benefits of parameter-efficient fine-tuning methods, particularly LoRA.
We vary LoRA's $r$ and $\alpha$ hyperparameters and observe that all configurations are able to achieve high utility, while especially lower $r$ and $\alpha$ values also preserve a high degree of privacy.
For each model, the optimal privacy-utility trade-off value is achieved with $r = \alpha$.
% Higher $\alpha$ values lead to marginally better utilities than the lower ones. However, for each model, we observe that the optimal privacy-utility trade-off value is achievable when $r=\alpha$. It is also worth noting that LoRA is the most cost-effective approach among all the other fine-tuning methods. 
In addition, we observe that LoRA, after being fine-tuned on the CustomerSim dataset, did not lose much of its abilities on the benchmark datasets, maintaining results comparable to those of pre-trained model.
Finally, LoRA is much more computationally and memory efficient than FFT and especially DP.
Overall it provides the best trade-offs in terms of utility, privacy and efficiency and shows that privacy can be achieved without additional computational costs. The degree of measures along the \textit{trade-off, knowledge retention, and efficiency} are: 

\indent Utility-privacy trade-offs: \textit{good} \\
\indent Retention of base performance: \textit{moderate} \\
\indent Efficiency: \textit{good}