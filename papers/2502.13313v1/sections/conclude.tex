\section{Concluding Discussion}
\label{sec:conclusion}
In this paper, we examine the  trade-offs among privacy, utility, and efficiency while fine-tuning an LLM. The traditional wisdom of achieving privacy comes at the cost of computational inefficiency using dedicated methods like DP. In contrast, we demonstrate that parameter efficient fine-tuning methods like LoRA, initially designed for efficiency, achieves privacy of sensitive data without any computational overhead. Simultaneously, LoRA retains the utility of general language understanding compared to DP, or even full-fine-tuning, realizing the superiority of LoRA in optimizing all three aspects. Towards our investigation, we  establish the significance of redefining privacy and utility using a careful distinction between sensitive and non-sensitive counterparts of the fine-tuned data. Through case studies, we demonstrate how existing measures exaggerate privacy threats and undermine the utility of an LLM. Our paper calls for a joint venture of   privacy and systems communities in achieving privacy-aware efficient fine-tuning of LLMs while retaining utility.