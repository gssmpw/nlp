\section{Related Work}
\label{sec:related_work}
\textbf{Privacy Attacks and Quantification.}
Privacy concerns in LLMs have gained attention in recent years \cite{KIBRIYA2024109698, YAO2024100211}, particularly the possibility of exposing data via membership inference attacks (MIA) \cite{mireshghallah-etal-2022-quantifying, mattern2023membership, fu2023practical, kaneko2024sampling} and training data extraction attacks \cite{carlini2021extractingtrainingdatalarge, mireshghallah-etal-2022-empirical, panda2024teach}. While membership inference attacks have the goal of identifying if a certain datapoint was in the training dataset using model confidence scores, training data extraction attacks try to extract specific parts of data from the training dataset using different prompting strategies. A recent study \cite{zhang2024membershipinferenceattacksprove} underscores that membership inference attacks may not be reliable due to their practical limitations of requiring knowledge of the entire training data, thus proving data extraction attacks to be more viable.

Existing works have quantified privacy attacks in large language models especially through the lens of memorization. For example, the authors in \cite{carlini2023quantifying} assess memorization by quantifying how closely LLM generation matches the exact training data phrases when prompted with tailored prefixes. The metric of `exposure' is introduced in \cite{10.5555/3361338.3361358} to evaluate a model’s vulnerability when exposed to data that was artificially introduced into the dataset (also known as "canaries") several times during training. 
This metric has been widely adopted in subsequent research \cite{shi-etal-2022-selective, zhao-etal-2022-provably} as a measure of privacy. However, the metric’s formulation is limited by its reliance on assumptions about the surrounding knowledge of other canaries, which do not hold in practical scenarios. 

Intuitively, privacy evaluation should be more related to sensitive data seen by the model during its training. Some of the existing studies ~\cite{10.5555/3666122.3667033, 10179300, zhao-etal-2022-provably, shi-etal-2022-selective} focusing on privacy attacks in LLMs may consider sensitive and non-sensitive counterparts in the data explicitly during training, but they do not retain this distinction during privacy quantification. We believe (and later show) that this lack of distinction can lead to an inaccurate (sometimes, overestimated) assessment of privacy threats. In our paper, we carefully make the distinction between sensitive and non-sensitive data and propose a revised quantification.

\noindent
\textbf{Privacy-Utility Tradeoffs.}
To mitigate privacy leakage, differential privacy (DP) measures have been proposed, which add theoretical privacy guarantees to the training process \cite{10.1145/2976749.2978318}. The key idea of DP is to clip the gradients of each datapoint and add noise to the clipped gradients in every iteration.  Its primary goal is to reduce the influence of individual datapoints in the training procedure thus preventing its leakage during inference. 

Existing work utilising DP~\cite{yu2022differentiallyprivatefinetuninglanguage} has shown a clear trade-off between privacy and utility where privacy is measured in terms of theoretical guarantee and utility in terms of overall performance on the end task. The authors in \cite{10.1145/3531146.3534642} also question the notion of privacy considered in these privacy preserving techniques. Another line of work from \cite{ shi-etal-2022-selective} and \cite{zhao-etal-2022-provably} distinguishes between sensitive and non-sensitive data using techniques like regex and redaction and proposes a customised training strategy using DP. However, their evaluation measure also relies on theoretical guarantees for privacy and overall performance for utility. The question that arises here is whether this measure of quantification of privacy and utility is sufficient in the context of large language models, or whether it needs to be more nuanced.

Additionally, to ensure privacy guarantees, differential privacy requires higher computational resources (time and memory) during training. For example, compared to vanilla SGD, DP-SGD may incur up to $20$x training time, which is often a bottleneck in resource-intensive tasks \cite{dupuy2022efficient}. Therefore, the conventional wisdom is that privacy comes at the cost of efficiency. This leads us to probe ways in which the above limitation can be prevented.

\noindent
\textbf{Utility-Efficiency Tradeoffs.} Fine-tuning a pre-trained LLM on a task specific context has been used in several applications like medical chatbots, AI assistants, etc. However, full fine-tuning of large language models is expensive as it involves updating all the parameters. Recent work has focused on reducing the training cost while maintaining utility, with the introduction of parameter efficient fine-tuning (PEFT) techniques~\cite{han2024parameterefficientfinetuninglargemodels}. Well-known PEFT methods include adapter-based fine-tuning \cite{houlsby2019parameter, lei2023conditional, zhu2021counter, he2021towards}, soft prompt-based fine-tuning \cite{li2021prefix, li2023prefix, liu2021p, liu2024gpt, lester2021power}, and parameterized fine-tuning \cite{hu2022lora,liu2024dora}. Among different PEFT methods, the Low-Rank Adaptation method, called LoRA \cite{hu2022lora} has emerged as one of the most widely used methods. LoRA updates fewer parameters in the model via low-rank approximation, providing computational efficiency with a relatively low cost to utility.

In recent work on private fine-tuning \cite{liu2023differentially, yu2021differentially,ma2024efficient}, the authors combine LoRA with DP to reduce the additional computational overhead induced by differential privacy. In such a context, we ask whether DP is the only method towards ensuring privacy. LoRA's training procedure of updating fewer parameters can be thought of as analogous to the noisy gradient update in DP. This leads us to the question of whether \textit{LoRA has any privacy benefits} besides having control over utility-efficiency tradeoffs. 


\noindent
\textbf{Privacy-Utility-Efficiency Tradeoffs.}
To the best of our knowledge, our work is the first to \textit{investigate the privacy benefits of LoRA and systematically examine the privacy, utility, and efficiency tradeoffs among different fine-tuning methods}. Besides, instead of relying on the existing measures of privacy and utility, we distinguish between sensitive and non-sensitive data to \textit{propose a nuanced quantification of privacy and utility}.  
