\section{Introduction}


Large language models (LLMs) have demonstrated remarkable abilities in solving varied natural language tasks~\cite{naveed2023comprehensive}, with a host of applications in multiple domains such as education~\cite{wang2024large}, medical chatbot~\cite{thirunavukarasu2023large}, AI assistant~\cite{dong2023towards}, etc. 
%
Their ability is acquired by first pre-training models on a large Web corpus\cite{kaplan2020scaling}, which results in general language understanding~\cite{brown2020language} and then fine-tuning the pre-trained models on domain-specifc data, which customizes their performance to specific tasks~\cite{zhang2023instruction}. 
%
Two foundational challenges with pre-training/fine-tuning these models at scale are centered around enhancing \emph{privacy} and \emph{efficiency}, while retaining the models' \emph{utility}.  
%
The former is related to reducing the risk of LLMs leaking sensitive user information contained in the training data, while the latter is related to reducing the computational cost of training. 
%

A long line of recent research in the privacy community has focussed on methods to mitigate privacy risks when training LLMs~\cite{shi-etal-2022-selective, zhao-etal-2022-provably, 10.1145/2976749.2978318,yu2022differentiallyprivatefinetuninglanguage}. A notable example of such methods is differentially privacy (DP)\cite{10.1145/2976749.2978318}.
%
Similarly, a flurry of recent research in the systems community has explored parameter-efficient fine-tuning (PEFT) methods in  LLMs~\cite{han2024parameterefficientfinetuninglargemodels}. A notable example of this class of methods is low-rank adaptation (LoRA)~\cite{hu2022lora}.
%
However, no prior works, to the best of our knowledge, have investigated the privacy risks associated with the efficient training methods.
%
So the central question driving our research here is: \emph{do efficient fine-tuning methods enhance or mitigate privacy risks during training?}

Our question above is motivated by the following high-level observations about DP and LoRA: methodologically, both DP and LoRA
%SD
%PEFT 
restrict the impact training examples can have on model parameters -- DP deliberately through its noisy gradient update, and LoRA through low-rank adaptation.  
%
However, DP incurs significant additional computational overhead~\cite{dupuy2022efficient} compared to traditional fine-tuning, while LoRA significantly reduces the computational costs.
%
So the answer to the above question can have significant consequences for achieving good privacy-efficiency-utility tradeoffs when fine-tuning. 
%
For instance, if LoRA mitigates privacy risks of training, then it suggests that this method can simultaneously achieve both privacy and efficiency objectives, contradicting the conventional wisdom drawn from DP literature that privacy comes at a computational cost.
%
One of the key (surprising) findings of our work below lies in establishing that LoRA does indeed mitigate privacy risks.
%On the other hand, if LoRA enhances privacy risks, it highlights an additional concern that literature on efficient fine-tuning methods are currently ignoring.

When attempting to answer the above question, we encountered a more foundational question that has been not been well-addressed by the existing literature namely, \emph{how should one quantify the privacy risks associated with a fine-tuning method, so that it allows for a performance comparison across different methods?} 
%
For instance, the claimed privacy benefits of the DP method are based on theoretical guarantees and not empirical evaluation, making it hard to compare it against privacy benefits of other fine-tuning methods.


%
Numerous recent works explored privacy concerns with LLMs that arise from their ability to memorize and recollect training data sequences containing sensitive personally identifiable information (PII), such as names, email addresses, phone numbers, credentials, etc. 
%
This sensitive information can leak from an LLM, either by accidental generation in response to benign requests to the model, or by deliberate extraction using adversarially crafted requests to the model\cite{mireshghallah-etal-2022-quantifying, mattern2023membership, fu2023practical, kaneko2024sampling, carlini2021extractingtrainingdatalarge, mireshghallah-etal-2022-empirical, panda2024teach}.

However, surprisingly, existing privacy as well as utility metrics for LLMs ignore the distinction between sensitive and non-sensitive counterparts (tokens) in training data sequences~\cite{carlini2023quantifying,mireshghallah-etal-2022-quantifying,biderman2024emergent}.\footnote{A few works used this distinction to train models \cite{shi-etal-2022-selective,zhao-etal-2022-provably}, but it has not been applied when quantifying privacy risks or utility metrics.}
%
Sensitive data is often inherently random and less predictable, such as social security number or phone number. 
%
Non-sensitive data, on the other hand, is more predictable, e.g. there is only a small set of valid completions for the sentence ``The dog chases the \_". 
%
Consequently, the ability of LLMs to recollect or predict sensitive vs. non-sensitive data is very different (see Figure~\ref{fig:illustrative_example} for details).
%
As we show in Section~\ref{sec:sens_non_sens_distinction}, ignoring this distinction leads to inaccurate measures of privacy and utility, and in particular, may have resulted in some prior works unintentionally exaggerating the threat of LLM's memorizing training data\cite{biderman2024emergent,carlini2021extractingtrainingdatalarge,carlini2023quantifying}.
%

Here, we propose and justify new measures for privacy and utility explicitly accounting for the differences in model's performance for sensitive and non-sensitive data.
%
For sensitive data, we are primarily interested in \emph{privacy}, i.e. we want the loss on sensitive tokens to be \emph{as high as possible}.
%
Conversely, for non-sensitive data, we are interested in \emph{utility}, i.e. we want the loss on non-sensitive tokens to be \emph{as low as possible}.
%

\begin{figure}[]
    \centering
        \begin{subfigure}{.48\linewidth}
       \includegraphics[scale=0.25]{figures-paper/section1/intro-pretrained-trtscustomersim--gpt4.pdf}
        \caption{Pretrained}
        \label{fig:a}
    \end{subfigure}
    %\hfil
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.25]{figures-paper/section1/intro-ft-trtscustomersim--gpt4.pdf}
        \caption{Fine-tuned}
        \label{fig:b}
    \end{subfigure}
    \caption{
    % (a): Performance of pretrained models on sensitive and non-sensitive data; (b-d): Performance of full fine-tuned models.}
    Sensitive and non-sensitive tokens have different predictability, measured as the recollection loss by pre-trained  models in Figure~\ref{fig:a} and fine-tuned models (recorded at epoch $5$) in Figure~\ref{fig:b}. The distinction motivates us to quantify privacy using sensitive token loss on training data (higher is better) and utility as non-sensitive token loss on test data (lower is better).
    }
    \label{fig:illustrative_example}
\end{figure}


\noindent \textbf{Contributions.} We summarize our main findings below:

\noindent \emph{1. Quantifying privacy and utility, when training LLMs:} We conceptually argue and empirically demonstrate that LLM's ability to recollect (predict) sensitive and non-sensitive data in training (test) datasets are so starkly different that we need to account for them when quantifying privacy and utility. Privacy is best captured by a model's ability to recollect sensitive tokens in training data, while utility is best captured by the model's ability to predict non-sensitive tokens in test data.

\noindent \emph{2. Exaggerated estimates of privacy threat from LLM memorisation:} We show that compared to our measure, existing measures significantly over-estimate privacy risks from LLM training. LLMs recollect non-sensitive tokens at a significantly higher level than sensitive tokens. So by not distinguishing between them, existing studies are over-estimating privacy threats.

\noindent \emph{3. Comparing privacy-utility-efficiency tradeoffs for three different fine-tuning methods:} Our measures allow us to conduct a systematic and extensive empirical study of three different fine-tuning methods: full fine-tuning, DP, and LoRA, using models from three different LLM families, Pythia~\cite{biderman2023pythia}, Gemma~\cite{team2024gemma} and Llama~\cite{touvron2023llama} over both real-world and synthetically-generated datasets. For each method and model, we evaluate privacy, utility (in terms of train and test loss respectively) and its impact on benchmark performance, and efficiency. Our comparative study yields several interesting insights:  we find that -- full fine-tuning results in poor utility-privacy tradeoffs; DP offers the best utility-privacy tradeoffs, but is computationally very expensive; LoRA is almost on par with DP in terms of utility and privacy, but is much more computationally efficient. 

\noindent \emph{4. Feasibility of achieving all the three privacy, utility and efficiency objectives simultaneously:} Our findings about LoRA performance suggest that it may be possible for methods to achieve all three objectives simultaneously. Our results challenge prevailing wisdom that enhancing privacy during training is necessarily more computationally expensive and call for investigating privacy benefits of existing and new parameter efficient fine-tuning methods.  