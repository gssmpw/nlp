\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}


\title{Deep Fake Detection\thanks{Funded by Shiv Nadar University Chennai}}

\author{
\IEEEauthorblockN{
Akhshan P\textsuperscript{1}, Taneti Sanjay\textsuperscript{2}, Chandrakala S\textsuperscript{3}
}
\IEEEauthorblockA{
\textsuperscript{1,2}UG Students, Dept of CSE, Shiv Nadar University Chennai, India \\
Email: \{akhshan21110171, taneti22110220\}@snuchennai.edu.in \\
\textsuperscript{3}Professor, Dept of CSE, Shiv Nadar University Chennai, India \\
Email: chandrakalas@snuchennai.edu.in
}
}


\maketitle

\begin{abstract}
The proliferation of deep fake technology poses significant challenges to digital media authenticity, necessitating robust detection mechanisms. This project evaluates deep fake detection using the SP Cup's 2025 deep fake detection challenge dataset. We focused on exploring various deep learning models for detecting deep fake content, utilizing traditional deep learning techniques alongside newer architectures. Our approach involved training a series of models and rigorously assessing their performance using metrics such as accuracy.
\end{abstract}

\section{\textbf{Introduction}}

\subsection{Background}
The rapid advancement of artificial intelligence has led to the development of sophisticated techniques for generating realistic synthetic media, commonly known as deep fakes. Deep fakes leverage deep learning algorithms, particularly generative adversarial networks (GANs), to create highly convincing fake images, audio, and videos. While these technologies have promising applications in entertainment and education, they also pose serious threats to privacy, security, and the integrity of information. The potential misuse of deep fakes for malicious purposes, such as misinformation, fraud, and political manipulation, underscores the critical need for effective detection methods.

\subsection{Problem Statement}
The detection of deep fakes is a complex and pressing challenge due to the high degree of realism that these synthetic media can achieve. Traditional detection techniques often fall short in identifying subtle manipulations. Therefore, there is an urgent need for advanced machine learning models capable of discerning genuine media from deep fakes with high accuracy.

\subsection{Significance}
Detecting deep fakes is crucial for maintaining the integrity of digital media and ensuring the trustworthiness of information disseminated through various platforms. Effective deep fake detection can help mitigate the risks associated with the malicious use of synthetic media, protecting individuals and organizations from potential harm. By exploring and comparing different model architectures, this project aims to contribute to the growing body of research dedicated to combating the deep fake threat and enhancing media security.

The findings from this study are expected to provide valuable insights into the capabilities and limitations of current deep fake detection techniques, guiding future research and development efforts in this vital field.\\


\section{\textbf{Dataset Description}}

\subsection{Dataset}
In this study, we utilized the \textbf{DeepfakeBench benchmark dataset}, consisting of genuine and manipulated images, provided as part of the SP Cup 2025 Deep Fake Detection Challenge \cite{r1}. The dataset, as provided, exhibited a significant imbalance between the number of real and fake images, which could have introduced bias in the model's performance. To address this issue, we applied an undersampling strategy, reducing the dataset to 44,000 samples for each class to ensure an equal representation of real and fake images. By balancing the dataset, we aimed to provide a fair and unbiased evaluation of our models. This approach allowed us to accurately assess the models' ability to differentiate between real and fake images under balanced conditions, thereby enhancing the robustness and reliability of our results.

\subsection{Preprocessing}
The following preprocessing steps were applied to the images before training the models:

\begin{itemize}
    \item {Normalization:} Pixel values were normalized by dividing them by 255. This scales the pixel values to the range [0, 1], which helps improve the efficiency and stability of the training process.\\
\end{itemize}
\section{\textbf{Architecture of the CMVit Repeat Model\\(Model 1)}}

The \textbf{Cross multi scale vision transformer} model is a novel deep learning architecture designed for image classification tasks. It combines the strengths of multiscale Vision Transformers (MViT) and  Cross Model Fusion (CMF) blocks, allowing the model to process both spatial and frequency-domain features. This hybrid design enhances the model's ability to learn complex representations from input images.

\subsubsection{\textbf{Multi scale Vision Transformer (MViT) Block}}

The Vision Transformer (MViT) block is the backbone of the CMVit\_repeat model. It is composed of several key components that allow the model to learn spatial relationships between image patches.

\subsubsection{\textbf{Patch Embedding}}
The first step in the MViT block is the \textit{patch embedding}, where the input image is divided into smaller patches. These patches are flattened and then linearly embedded into a fixed-size vector (referred to as the embedding dimension). This embedding is achieved using a convolutional layer with a patch size equal to the kernel size. This transformation results in a set of patch embeddings representing the image.

\subsubsection{\textbf{Multi-Head Self-Attention}}
The next step in the MViT block is the \textit{multi-head self-attention} mechanism. This mechanism allows the model to focus on different regions of the image by attending to multiple "heads" in parallel. The attention mechanism learns relationships between different parts of the image, enabling the model to capture long-range dependencies across the entire image.

\subsubsection{\textbf{Feed-Forward Network (FFN)}}
After the attention block, the output is passed through a \textit{feed-forward network} (FFN). The FFN consists of two fully connected layers with a ReLU activation function in between. The FFN helps the model learn more complex representations of the data, further refining the feature map generated by the attention block.

\subsubsection{\textbf{Positional Encoding}}
Since transformers do not have a built-in mechanism for capturing spatial information, \textit{positional encodings} are added to the patch embeddings. This provides the model with information about the relative positions of the patches within the image, enabling it to understand the spatial arrangement of image content.

\subsubsection{\textbf{Residual Connections and Layer Normalization}}
Each of the attention and feed-forward layers in the MViT block is followed by \textit{residual connections} and \textit{layer normalization}. The residual connections help mitigate issues like the vanishing gradient problem by allowing gradients to flow more easily through the network. Layer normalization ensures that the model's training is stable by normalizing the outputs at each layer.

\subsection{\textbf{Cross Model Fusion (CMF) Block}}

The CMF block is designed to process both spatial and frequency-domain features of the input image. It is composed of several stages, including the Fourier transform, magnitude spectrum computation, convolutional processing, and feature fusion.

\subsubsection{\textbf{Fast Fourier Transform (FFT)}}
The first operation in the CMF block is the computation of the \textit{Fast Fourier Transform (FFT)}. The FFT converts the image from the spatial domain to the frequency domain, allowing the model to capture frequency-based features that may not be obvious in the spatial domain. The FFT operation transforms the image into a frequency spectrum.

\subsubsection{\textbf{Magnitude Spectrum Calculation}}
Once the FFT is computed, the \textit{magnitude spectrum} is calculated. The magnitude spectrum represents the strength of various frequency components in the image. By focusing on the magnitude, the model ignores phase information, which is often less important for image classification tasks.

\subsubsection{\textbf{Convolutional Layers}}
The magnitude spectrum is passed through a series of \textit{convolutional layers}. These layers extract high-level frequency features from the spectrum, similar to how traditional convolutional layers extract spatial features from images.

\subsubsection{\textbf{Concatenation with RGB Features}}
In the next step, the features from the magnitude spectrum are concatenated with the original RGB features from the input image. This fusion of spatial and frequency-domain features enhances the model's ability to learn complex patterns from both domains.

\subsubsection{\textbf{Residual Connections and Layer Normalization}}
The CMF block also employs \textit{residual connections} and \textit{layer normalization}. The residual connections help preserve important information throughout the layers, while layer normalization stabilizes the model's training and learning process.

\subsection{\textbf{Combined Architecture: Stacking MViT and CMF Blocks}}

The overall architecture of the CMVit\_repeat model consists of multiple stacked \textit{MViT\_combined\_cmf} blocks. These blocks are repeated several times to allow the model to learn progressively more complex features. Each block combines the strengths of the Vision Transformer and CMF block, enabling the model to capture both spatial and frequency-domain information.

\subsection{\textbf{Output Layer}}

At the final stage of the architecture, the output of the stacked MViT and CMF blocks is passed through an adaptive average pooling layer to reduce the spatial dimensions. The resulting feature map is then flattened and passed through a series of fully connected layers. These layers process the features further, enabling the model to classify the input image. The final output is produced using a softmax activation, generating probabilities for each class.

The model is illustrated in fig1.


\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{model_1.jpg}
    \caption{Cmvit-repeat}
    \label{fig:enter-label}
\end{figure*}


\section{\textbf{Architecture of the CMVit Repeat + Local Binary Pattern Model\\(Model 2)}}

The \textbf{CMVit+LBP} model extends the previously discussed architecture by integrating the strengths of the CMVit model with extracted Local Binary Patterns (LBPs). This combination enhances the model's ability to capture texture information within the image, improving overall performance.

\subsection{\textbf{Multi scale Vision Transformer (MViT) Block}}

The \textbf{Multi scale Vision Transformer (MViT)} block used in the \textbf{CMVit Repeat} model is \textit{the same as the MViT block in Model 1}. It serves as the backbone for learning spatial relationships between image patches, with the following components:

\begin{enumerate}
    \item \textbf{Patch Embedding}: Divides the image into smaller patches and embeds them into fixed-size vectors.
    \item \textbf{Multi-Head Self-Attention}: Focuses on different regions of the image to capture long-range dependencies.
    \item \textbf{Feed-Forward Network (FFN)}: Refines the feature map with two fully connected layers and ReLU activation.
    \item \textbf{Positional Encoding}: Adds spatial information to the patch embeddings.
    \item \textbf{Residual Connections and Layer Normalization}: Ensures stable training and mitigates vanishing gradient issues.
\end{enumerate}

\subsection{\textbf{Cross model fusion (CMF) Block}}

The \textbf{CMF block} in the \textbf{CMVit Repeat} model is \textit{the same as in the previous model}. It processes both spatial and frequency-domain features of the input image and includes:

\begin{enumerate}
    \item \textbf{Fast Fourier Transform (FFT)}: Converts the image from the spatial domain to the frequency domain.
    \item \textbf{Magnitude Spectrum Calculation}: Represents the strength of various frequency components.
    \item \textbf{Convolutional Layers}: Extracts high-level frequency features from the magnitude spectrum.
    \item \textbf{Concatenation with RGB Features}: Fuses spatial and frequency-domain features for enhanced pattern learning.
    \item \textbf{Residual Connections and Layer Normalization}: Stabilizes training and preserves important information throughout the layers.
\end{enumerate}

\subsection{\textbf{Local Binary Pattern (LBP) Feature Extraction}}

In addition to the MViT and CMF blocks, the model incorporates \textbf{Local Binary Pattern (LBP)} feature extraction as part of the feature preprocessing. LBP is a texture descriptor that captures local texture patterns by comparing pixel intensities in a local neighborhood. These patterns are encoded into binary values and can provide additional discriminative features for classification tasks. The LBP features are extracted and incorporated into the feature fusion process, further enhancing the model's ability to capture subtle texture variations in the image.

\subsection{\textbf{Combined Architecture: Stacking MViT, CMF, and LBP Features}}

The overall architecture of the \textbf{CMVit Repeat} model consists of multiple stacked \textbf{MViT} and \textbf{CMF} blocks, along with the integration of \textbf{LBP} features. These blocks are repeated several times to allow the model to learn progressively more complex features. The MViT block captures spatial features, the CMF block processes frequency-domain information, and the LBP features contribute additional texture-based features for a richer representation.

\subsection{\textbf{Output Layer}}

At the final stage of the architecture, the output of the stacked MViT, CMF blocks, and LBP features is passed through an \textbf{adaptive average pooling} layer to reduce the spatial dimensions. The resulting feature map is then \textbf{flattened} and passed through a series of \textbf{fully connected layers}. These layers process the features further, enabling the model to classify the input image. The final output is produced using a \textbf{softmax activation}, generating probabilities for each class.
The model is illustrated in fig2.\\\\

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{model_2.jpg}
    \caption{Cmvit-repeat-lbp}
    \label{fig:enter-label}
\end{figure*}



\section{\textbf{Architecture of the XceptionNet-based Model\\(Model 3)}}
The \textbf{XceptionNet-based model} is a deep learning architecture designed for deepfake detection tasks. It utilizes the \textbf{Xception} architecture, which is built upon depthwise separable convolutions, to efficiently capture both low-level and high-level features from input images. The model is fine-tuned for binary classification, distinguishing between real and deepfake images.




\subsection{\textbf{XceptionNet Backbone}}
The \textbf{XceptionNet} backbone used in this model is \textit{the same as the original Xception model}. The architecture is composed of a series of depthwise separable convolutions, where the convolution operation is split into two stages: a depthwise convolution and a pointwise convolution. This reduces the number of parameters and computations while maintaining high performance. The backbone consists of the following components:

\begin{enumerate}
    \item \textbf{Initial Convolutional Layer}: The input image is processed by an initial convolutional layer, extracting basic visual features.
    \item \textbf{Depthwise Separable Convolutions}: The core of the Xception architecture, consisting of depthwise separable convolution blocks that capture spatial patterns by applying convolutions to each channel independently.
    \item \textbf{Residual Connections}: In each block, residual connections are used to ensure that important features are preserved and allow for smoother gradient flow, improving the training process.
    \item \textbf{Batch Normalization}: Applied after each convolution to normalize the activations, improving convergence and model stability.
    \item \textbf{ReLU Activation}: The ReLU activation function is applied to introduce non-linearity after each convolutional operation.
\end{enumerate}

\subsection{\textbf{Fully Connected Layer}}
After the feature extraction by the Xception backbone, the output is passed through a global average pooling layer to reduce the spatial dimensions. The pooled features are then passed through a fully connected (FC) layer to perform the final classification task:

\begin{enumerate}
    \item \textbf{Global Average Pooling}: This layer aggregates the spatial information by computing the average across all spatial locations in the feature map, effectively reducing the feature map's dimensions to a single vector per feature map.
    \item \textbf{Fully Connected Layer}: The output of the global average pooling is passed through a fully connected layer, which is modified to produce outputs corresponding to the number of target classes.
    \item \textbf{Output Layer Modification}: The final fully connected layer is adjusted to output \texttt{num\_classes} logits, with a softmax activation function applied to convert these logits into class probabilities.\\
\end{enumerate}

The model is illustrated in fig3.\\\\

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{model_3.png}
    \caption{XCEPTION-NET}
    \label{fig:enter-label}
\end{figure}

\section{\textbf{Training and Environment Setup}}

The following details outline the training procedures and environment configurations utilized in this study:

\begin{enumerate}
    \item \textbf{Framework:} 
    \begin{itemize}
        \item PyTorch was the primary framework used for model training. 
        \item This framework was selected due to its flexibility and strong support for deep learning research.
    \end{itemize}

    \item \textbf{Optimizer:} 
    \begin{itemize}
        \item Adam optimizer used with default parameters for its efficiency and adaptive learning rate.
    \end{itemize}
    
    \item \textbf{Loss Function:} 
    \begin{itemize}
        \item Cross Entropy Loss for measuring the performance of classification models.
    \end{itemize}
    
    \item \textbf{Hardware:}
    \begin{itemize}
        \item Training on a high-performance cluster with One NVIDIA A5000 GPUs (24gb VRAM) for faster training and large batch handling.
    \end{itemize}
    
    \item \textbf{Batch Size:} 
    \begin{itemize}
        \item Uniform batch size of 64 for consistency and balance between memory usage and speed.
    \end{itemize}
    
    \item \textbf{Stopping Criteria:} 
    \begin{itemize}
        \item Training stopped when validation loss plateaued to prevent overfitting and ensure good generalization.
    \end{itemize}
\end{enumerate}

\onecolumn
\section{\textbf{Results}}
In this section, we present the evaluation results of the three deep fake detection models: CMVit, CMVit+LBP, and XceptionNet. Each model was assessed using validation accuracy only for the SP CUP 25 challenge dataset. \\\\

\centering{
    \renewcommand{\arraystretch}{1.3} % Adjust this value to increase/decrease the row spacing
    \begin{tabular}{|c|c|c|c|}
        \hline Type of result & \textbf{Model 1} & \textbf{Model 2} & \textbf{Model 3} \\ \hline
        \textbf{Trainable Parameters} & 71,605,646 & 125,631,088 & 20,811,050 \\
        \hline \textbf{Non Trainable Parameters} & 0 & 0 & 0 \\ 
        \hline \textbf{Optimizer} & Adam & Adam & Adam \\
        \hline \textbf{Loss Function} & Cross Entropy Loss & Cross Entropy Loss & Cross Entropy Loss \\
        \hline \textbf{No. of Epochs} & 70 & 23 & 100 \\
        \hline \textbf{Batch Size} & 64 & 64 & 64 \\
        \hline \textbf{Training Loss} & 0.0984 & 0.0165 & 0.0012 \\
        \hline \textbf{Stopping Criteria} & Validation loss plateau & Validation loss plateau & Validation loss plateau \\
        \hline \textbf{Training Accuracy} & 95.65\% & 98.35\% & 99.98\%\\
        \hline \textbf{Validation Accuracy} & 91.99\% & 87.15\% & 89\%\\
        \hline \textbf{Validation F1 Score (Class 0)} & 0.93 & 0.86 & 0.88\\
        \hline \textbf{Validation F1 Score (Class 1)} & 0.91 & 0.88 & 0.90\\
        \hline \textbf{Time per test file} & 0.0692 sec & 0.0564 sec & 0.0082 sec \\
        \hline
    \end{tabular}
}
\newline 
\newline
\newline

\bibliographystyle{plain}
\bibliography{spcup_reference}

\end{document}

