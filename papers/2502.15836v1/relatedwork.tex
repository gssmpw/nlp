\section{Related work}
While advances have been made in developing machine unlearning algorithms for LLMs, rigorous methodologies for auditing the efficacy of unlearning remain understudied.
Adversarial soft token attacks (\sta{s})~\cite{schwinn2024soft} and 5-shot in-context prompting~\cite{doshi2024doesunlearningtrulyunlearn} have been shown to recover unlearned knowledge in models. 
When model weights can be modified, techniques such as model quantization~\cite{zhang2024does} and retraining on a partially unlearned dataset ~\cite{lucki2024adversarial,hu2024jogging} have also proven effective in recalling forgotten information. 
~\cite{lynch2024eight} examined eight methods for evaluating LLM unlearning techniques and found that their latent representations remained similar.
News and book datasets are used to analyze unlearning algorithms from six different perspectives~\cite{shi2024muse}.
It was shown that fine-tuning on \emph{unrelated} data can restore information unlearned from the LLM~\cite{qi2024unrelateddata}, indicating the existing unlearning methods do not actually remove the information but learn a \emph{refusal filter} instead.
Several benchmarks have been developed to evaluate the existing unlearning algorithms.
Besides, an unlearning benchmark was introduced based on fictitious author information~\cite{maini2024tofu}.
For real-world knowledge unlearning, \emph{Real-World Knowledge Unlearning} (RWKU) used 200 famous people as unlearning targets~\cite{jin2024rwku}, while WDMP focused on unlearning hazardous knowledge in biosecurity, cybersecurity~\cite{li2024wmdp}.