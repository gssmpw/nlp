\section{Evaluation}\label{sec:eval}
% HP and TOFU baselines
% Complete failure of said framework
% Evaluating with soft prompts isn't even possible -> random string experiments
% We can do it for the trained, unlearned, and even base.
% Btw, also random strings.

\subsection{Experiment setup}

\fauxsection{Datasets.}
To attack unlearning and evaluate its effectiveness, we use two popular benchmark datasets:
\begin{enumerate*}[label=\arabic*)]
    \item \textit{Who Is Harry Potter?}~\cite{eldan2023harrypotter}, a benchmark that intends to remove information about the world of Harry Potter and the associated characters; \whp hereafter.
    \item \tofu~\cite{maini2024tofu}, a dataset of fictional writers that are guaranteed to be absent in the LLM's training data\footnote{This cannot be guaranteed for models published after \tofu.}.
\end{enumerate*}

\whp does not publish a complete dataset.
For that reason we use the passages included in the associated Hugging Face page~\cite{eldan2023harrypotter-hf}.
Additionally, we augment it with $20$ $(x_p\rightarrow c)$ pairs generated with Llama2-7b-chat-hf.
These contain general trivia about the Harry Potter universe.

For \tofu, we use the $10\%$ forget to $90\%$ retain split provided by the authors~\cite{tofuhf}.

\fauxsection{Models \& environment.}
For all experiments, we use Llama-2-7b-chat-hf~\cite{touvron2023llama} (Llama2), and Llama-3-8b-instruct~\cite{meta2024llama} (Llama3) downloaded from Hugging Face.
We get the unlearned \whp model from it's Hugging Face repository~\cite{eldan2023harrypotter-hf} (Llama2-\whp).

We implement \sta using LLMart~\cite{llmart2025github} -- a PyTorch and Hugging Face-based library for crafting adversarial prompts. 
We use implementations of the unlearning methods from the \tofu~\cite{tofugithub}, and NPO~\cite{npogithub} repositories.
We benchmark the attack against seven different unlearning algorithms: gradient ascent (GA), gradient difference (GDF)~\cite{liu2022continual}, refusal (IDK)~\cite{rafailov2024direct}, knowledge distillation (KL)~\cite{hinton2015distilling}, negative preference optimization (NPO)~\cite{zhang2024negative}, NPO-GDF, NPO-KL.

We run our experiments on a machine equipped with Intel Xeon Gold 5218 CPU, eight NVIDIA A6000, and 256 GB of RAM.

\begin{table}[t]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \sta          & soft token attack\\
        $A_o$         & oracle auditor\\
        $A_\sta$      & \sta auditor\\
        $x_p$         & base prompt (benign)\\
        $x_s$         & adversarial suffix\\
        $x_a$         & adversarial prompt ($x_p \oplus x_s$)\\
        $c$           & target completion\\
        $f_\emptyset$ & base model\\
        $f_{ft}$      & fine-tuned model\\
        $f_{u}$       & unlearned model\\
        $f_{u-*}$     & model unlearned using *\\
        $D_{train}$   & training data\\
        $D_{forget}$  & forget data\\
        $D_{retain}$  & retain data\\
        \hline
    \end{tabular}
    \caption{Summary of the notation. '*' is replaced with the specific unlearning method.}
    \label{tab:notation}
\end{table}

\subsection{Auditing with attacks}\label{sec:eval:attacks}

\fauxsection{Who Is Harry Potter?.}
To elicit information about the Harry Potter universe, we initialize the soft tokens using randomly selected hard tokens, and append them to the prompt ($embed(x_a = x_p \oplus x_s)$).
We then train the soft prompt using AdamW~\cite{loshchilov2018decoupled} for up to $3000$ iterations; using learning $0.005$, and default $\beta{s}$ -- we reiterate that the $x_p$ does not change, only the embedded suffix does.
If the optimization fails, we double the the number of soft tokens up to the maximum of $16$.
We rerun the experiment five times, and report the mean and standard deviation across all prompt and reruns.
In Table~\ref{tab:attack-unlearning-hp} we report the average number of soft tokens needed to elicit a completion. 
\whp* denotes the unlearned model with different prompt templates.

Our results show that all information can be generated with $\approx 4-6$ added soft tokens.
For all pairs of models, we conduct a \emph{t-test} under the null hypothesis $\mathcal{H}_0$ of \emph{equivalent population distributions} with $\alpha=0.05$.
We use an unpaired Welch's t-test since sample variances are not equal~\cite{welch1947ttest}.
We cannot reject the hypothesis for any of the pairs i.e. $p>0.05$.
In other words, for all models, there is not enough evidence to say that eliciting completions is more difficult.

Additionally, we observe that the ease of eliciting the completions changes depending on the prompt template.
We conducted our initial exploratory experiments in the chat setting with a chat prompt template.
We notice that the model (\whp+chat) reveals all unlearned information with manually paraphrased prompts.
Furthermore, when using the example prompts in the corresponding Hugging Face repository~\cite{eldan2023harrypotter-hf}, the model (\whp) would often begin the response with a double new line (\textbackslash n\textbackslash n).
We suspect that the provided unlearned model is overfit to ``prompt\textbackslash n\textbackslash n completion''.
To run our evaluation in the most favorable setting, we report all three.
Our results show that attacking is the easiest for \whp+chat, and the most difficult for \whp+\textbackslash n\textbackslash n.
Given these discrepancies, and the lack of a standard \whp dataset, we believe it is not a good unlearning benchmark, despite its popularity.

Also, in our dataset there are three challenging outlier prompts that require $16$ soft tokens, unlike other prompts.
Filtering these out results in $4.05, 4.60, 1.61$ average required tokens for \whp, \whp+\textbackslash n\textbackslash n, and \whp+chat respectively.

\begin{table}[t]
    \centering
    \begin{tabularx}{\linewidth}{|Y|Yc|}
        \hline
        Prompt                                & \multicolumn{2}{c|}{Model}\\
        template                              & Llama2-\whp    & Llama3 \\
        \hline
        N/A                                   & N/A            & $5.61\pm6.32$ \\
        \hline
        \whp                                  & $4.63\pm3.69$ & N/A \\
        \whp+\textbackslash n\textbackslash n & $6.50\pm5.13$ & N/A \\
        \whp+chat                             & $4.12\pm5.53$ & N/A \\
        \hline
    \end{tabularx}
    \caption{Number of soft tokens required to elicit a completion for a fixed number of iterations.
    Soft tokens are appended to the prompt.
    Results are averaged over all prompts in the \whp set and over five runs for each prompt.
    When we increase the maximum iterations to $10,000$ we can elicit \textbf{all} completions with $1-2$ soft tokens. We do not report the results for Llama2 hf because it was used to generated the data. For comparison we also report the results for Llama3 without any prompt template (N/A).}
    \label{tab:attack-unlearning-hp}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabularx}{\linewidth}{|c|YY|}
        \hline
        \multirow{2}{*}{Unlearning method} & \multicolumn{2}{c|}{Model}\\
                                           & Llama2        & Llama3 \\
        \hline
        $f_\emptyset$ (none)               & $3.07\pm3.25$ & $3.11\pm3.15$ \\
        $f_{ft}$ (none)                    & $2.95\pm3.35$ & $3.21\pm3.19$ \\
        \hline
        $f_{u-IDK}$                        & $3.40\pm3.20$ & $3.33\pm3.09$ \\
        $f_{u-GA}$                         & $3.34\pm3.97$ & $3.21\pm3.87$ \\
        $f_{u-GDF}$                        & $3.06\pm3.34$ & $3.11\pm3.40$ \\
        $f_{u-KL}$                         & $3.08\pm3.31$ & $3.12\pm3.17$ \\
        $f_{u-NPO}$                        & $3.11\pm3.27$ & $3.12\pm3.27$ \\
        $f_{u-NPO-GDF}$                    & $3.15\pm3.24$ & $3.16\pm3.16$ \\
        $f_{u-NPO-KL}$                     & $3.23\pm3.62$ & $3.24\pm3.57$ \\
        \hline
    \end{tabularx}
    \caption{Number of soft tokens required to elicit a completion for a fixed number of iterations.
    Soft tokens are appended to the prompt.
    Results are averaged over all prompts in the \tofu set and over five runs for each prompt.
    When we increase the maximum iterations to $10,000$ we can elicit \textbf{all} completions with $1-2$ soft tokens.}
    \label{tab:attack-unlearning-tofu}
\end{table}

\fauxsection{TOFU.}
For \tofu, we follow the same setup as for \whp --
we initialize soft tokens using random hard tokens, and append them; 
we then train the soft prompt using AdamW for up to $3000$ iterations; using learning $0.005$, and $\beta{s}=(0.9, 0.999)$;
we double the soft tokens if the optimization fails;
we rerun the experiments five times and report the averages across all prompts.
In Table~\ref{tab:attack-unlearning-tofu}, we report the number of soft tokens required to elicit the completions.
$f_\emptyset$ refers to the unmodified baseline model, $f_{ft}$ corresponds to the models fine-tuned on \tofu, followed by the unlearned models.

For all methods, we can elicit the completions with $\approx 3$ appended soft tokens.
Similarly to \whp, for all possible pairs of models (within the same architecture), we conduct a \emph{t-test} under the null hypothesis $\mathcal{H}_0$ of \emph{equivalent population distributions} with $\alpha=0.05$.
We use an unpaired Welch's t-test since sample variances are not equal.

We cannot reject the hypothesis for any of the pairs i.e. $p>0.05$; $f_{u-IDK}$ vs $f_{ft}$ (for Llama2) gives the lowest p-value of $0.509$.
In other words, for all models (regardless if trained on \tofu, or used unlearning method), there is not enough evidence to say that eliciting completions is more difficult.

One could argue that used unlearning methods are not effective (when comparing $f_{ft}$ vs $f_{u-*}$), hence they require similar numbers of soft tokens.\footnote{Most of these approaches have been in fact been shown ineffective, and susceptible to simple paraphrasing.}
However, the same holds when compared to $f_\emptyset$.
In the next section, we demonstrate that the result cannot be attributed to the (in-)effectiveness of the unlearning methods but rather the power of \sta.

\subsection{Eliciting random strings}\label{sec:eval:random-strings}

To further show the power of \sta{s}, we use them to elicit \emph{random strings}.
Unlike natural text, the chance of a random string appearing in the training set is negligible.
Also, preceding tokens do not inform the selection of the next token. 
In the following experiments, we pick characters uniformly at random in the range 33-126 of the ASCII table~\cite{asciitable}.

To elicit a random string, we initialize the soft prompt using randomly selected tokens.
Unlike in the \whp and \tofu experiments, there is only the soft prompt.
We then train the soft prompt using AdamW for up to $3000$ iterations per soft token; using learning rate $0.005$, and $\beta{s}=(0.9, 0.999)$.

In Figure~\ref{fig:random_strings}, we report the longest elicited string for a given number of soft tokens.
We repeat the experiment five times -- e.g., the first marker implies that for each of the five tested random strings of length $150$, we found an effective soft prompt.
We observe that not all initializations and seed configurations succeed, in which case a run needs to be restarted with a different seed.
If the loss plateaus around $25\%$ of the iterations, we restart the run.
However, no single string was restarted more than ten times.
We experimented with learning rate schedulers but they did not improve the search.

Our results show that \sta{s} can be used to elicit completely random strings, thus undermining their application for auditing unlearning.
Due to limited computational resources and long run-times, our evaluation is limited to $10$ soft tokens, and $400$-character long random strings.
This does not provide a bound on the longest string that can be elicited.

Next, we aim to answer why eliciting strings is possible.
Prompt-tuning~\cite{lester2021prompttuning} is a \emph{performance efficient fine-tuning technique} in which instead of training all weights, one trains only a soft prompt added to the input.
\sta{s} can be viewed as an extreme case of prompt-tuning, where instead of training over many prompts, one trains an attack per each prompt.
Hence, an LLM that outputs a completion that it was trained on is an expected behavior. 
We urge against misinterpreting the results and declaring techniques ineffective.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/random_strings.pdf}
    \caption{Numbers of random characters generated for the given soft prompt length. A single soft token can force over $150$ random characters -- more than any text in the \tofu benchmark. With $10$ soft tokens, it is possible to generate over $400$ random characters.}
    \label{fig:random_strings}
\end{figure}