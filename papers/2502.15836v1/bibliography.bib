@article{eldan2023harrypotter,
  title={Who's Harry Potter? Approximate Unlearning in LLMs},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@article{xi2025rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal={Science China Information Sciences},
  volume={68},
  number={2},
  pages={121101},
  year={2025},
  publisher={Springer}
} 

@inproceedings{acharya2023llm,
  title={Llm based generation of item-description for recommendation system},
  author={Acharya, Arkadeep and Singh, Brijraj and Onoe, Naoyuki},
  booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={1204--1207},
  year={2023}
}

@article{zhu2023multilingual,
  title={Multilingual machine translation with large language models: Empirical results and analysis},
  author={Zhu, Wenhao and Liu, Hongyi and Dong, Qingxiu and Xu, Jingjing and Huang, Shujian and Kong, Lingpeng and Chen, Jiajun and Li, Lei},
  journal={arXiv preprint arXiv:2304.04675},
  year={2023}
}

@article{liu2024towards,
  title={Towards safer large language models through machine unlearning},
  author={Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  journal={arXiv preprint arXiv:2402.10058},
  year={2024}
}


@article{zhang2024safe,
  title={Safe unlearning: A surprisingly effective and generalizable solution to defend against jailbreak attacks},
  author={Zhang, Zhexin and Yang, Junxiao and Ke, Pei and Cui, Shiyao and Zheng, Chujie and Wang, Hongning and Huang, Minlie},
  journal={arXiv preprint arXiv:2407.02855},
  year={2024}
}
 
@article{liu2024large,
  title={Large Language Model Unlearning via Embedding-Corrupted Prompts},
  author={Liu, Chris Yuhao and Wang, Yaxuan and Flanigan, Jeffrey and Liu, Yang},
  journal={arXiv preprint arXiv:2406.07933},
  year={2024}
}
 

@article{liang2024unlearning,
  title={Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning},
  author={Liang, Siyuan and Liu, Kuanrong and Gong, Jiajun and Liang, Jiawei and Xun, Yuan and Chang, Ee-Chien and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2403.16257},
  year={2024}
}
 
@article{liu2024efficient,
  title={Efficient backdoor defense in multimodal contrastive learning: A token-level unlearning method for mitigating threats},
  author={Liu, Kuanrong and Liang, Siyuan and Liang, Jiawei and Dai, Pengwen and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2409.19526},
  year={2024}
}
 

@article{ashuach2024revs,
  title={REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space},
  author={Ashuach, Tomer and Tutek, Martin and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2406.09325},
  year={2024}
}
 

@inproceedings{feng2024fine,
  title={Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models},
  author={Feng, XiaoHua and Chen, Chaochao and Li, Yuyuan and Lin, Zibin},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={10141--10155},
  year={2024}
}
 

@article{liu2024rethinking,
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Yao, Yuguang and Liu, Chris Yuhao and Xu, Xiaojun and Li, Hang and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}
 

@article{yao2023large,
  title={Large language model unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  journal={arXiv preprint arXiv:2310.10683},
  year={2023}
}
 

@article{wang2024rkld,
  title={RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models},
  author={Wang, Bichen and Zi, Yuzhe and Sun, Yixin and Zhao, Yanyan and Qin, Bing},
  journal={arXiv preprint arXiv:2406.01983},
  year={2024}
}
 

@article{li2024wmdp,
  title={The wmdp benchmark: Measuring and reducing malicious use with unlearning},
  author={Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and others},
  journal={arXiv preprint arXiv:2403.03218},
  year={2024}
}

 
@misc{jin2024rwku,
    title={RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models},
    author={Zhuoran Jin and Pengfei Cao and Chenhao Wang and Zhitao He and Hongbang Yuan and Jiachun Li and Yubo Chen and Kang Liu and Jun Zhao},
    year={2024},
    eprint={2406.10890},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{meta2024llama,
  title={Llama 3 model card},
  author={Meta, AI},
  journal={GitHub https://github. com/meta-llama/llama-models/blob/main/models/llama3\_1/MODEL\_CARD. md. Accessed},
  volume={21},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
 

@article{zhang2024negative,
  title={Negative preference optimization: From catastrophic collapse to effective unlearning},
  author={Zhang, Ruiqi and Lin, Licong and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2404.05868},
  year={2024}
}



@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
 

@article{shi2024muse,
  title={Muse: Machine unlearning six-way evaluation for language models},
  author={Shi, Weijia and Lee, Jaechan and Huang, Yangsibo and Malladi, Sadhika and Zhao, Jieyu and Holtzman, Ari and Liu, Daogao and Zettlemoyer, Luke and Smith, Noah A and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2407.06460},
  year={2024}
}
 

@article{huu2024effects,
  title={On effects of steering latent representation for large language model unlearning},
  author={Huu-Tien, Dang and Pham, Trung-Tin and Thanh-Tung, Hoang and Inoue, Naoya},
  journal={arXiv preprint arXiv:2408.06223},
  year={2024}
}
 

@inproceedings{tamirisa2024toward,
  title={Toward robust unlearning for LLMs},
  author={Tamirisa, Rishub and Bharathi, Bhrugu and Zhou, Andy and Mazeika, Bo Li4 Mantas},
  booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
  year={2024}
}
 

@article{pawelczyk2023context,
  title={In-context unlearning: Language models as few shot unlearners},
  author={Pawelczyk, Martin and Neel, Seth and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2310.07579},
  year={2023}
}
 

@article{bhaila2024soft,
  title={Soft Prompting for Unlearning in Large Language Models},
  author={Bhaila, Karuna and Van, Minh-Hao and Wu, Xintao},
  journal={arXiv preprint arXiv:2406.12038},
  year={2024}
}

@article{schwinn2024soft,
  title={Soft prompt threats: Attacking safety alignment and unlearning in open-source llms through the embedding space},
  author={Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel, Gauthier and Gunnemann, Stephan},
  journal={arXiv preprint arXiv:2402.09063},
  year={2024}
}
 

@article{zhang2024does,
  title={Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge},
  author={Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang},
  journal={arXiv preprint arXiv:2410.16454},
  year={2024}
}
 

@article{maini2024tofu,
  title={Tofu: A task of fictitious unlearning for llms},
  author={Maini, Pratyush and Feng, Zhili and Schwarzschild, Avi and Lipton, Zachary C and Kolter, J Zico},
  journal={arXiv preprint arXiv:2401.06121},
  year={2024}
}

 
@misc{doshi2024doesunlearningtrulyunlearn,
      title={Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning Methods}, 
      author={Jai Doshi and Asa Cooper Stickland},
      year={2024},
      eprint={2411.12103},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.12103}, 
}

@inproceedings{hu2024jogging,
  title={Jogging the Memory of Unlearned Models Through Targeted Relearning Attacks},
  author={Hu, Shengyuan and Fu, Yiwei and Wu, Steven and Smith, Virginia},
  booktitle={ICML 2024 Workshop on Foundation Models in the Wild},
  year={2024}
}


@inproceedings{bourtoule2021machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={141--159},
  year={2021},
  organization={IEEE}
}

@misc{zou2023gcg,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.15043}, 
}

@inproceedings {thudi2022auditunlearning,
    author = {Anvith Thudi and Hengrui Jia and Ilia Shumailov and Nicolas Papernot},
    title = {On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning},
    booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
    year = {2022},
    isbn = {978-1-939133-31-1},
    address = {Boston, MA},
    pages = {4007--4022},
    url = {https://www.usenix.org/conference/usenixsecurity22/presentation/thudi},
    publisher = {USENIX Association},
    month = aug
}

@misc{hyland2022empiricalsgdprivacy,
      title={An Empirical Study on the Intrinsic Privacy of SGD}, 
      author={Stephanie L. Hyland and Shruti Tople},
      year={2022},
      eprint={1912.02919},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.02919}, 
}

@misc{eisenhofer2023verifiedunlearning,
      title={Verifiable and Provably Secure Machine Unlearning}, 
      author={Thorsten Eisenhofer and Doreen Riepel and Varun Chandrasekaran and Esha Ghosh and Olga Ohrimenko and Nicolas Papernot},
      year={2023},
      eprint={2210.09126},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.09126}, 
}

@misc{kaplan2020scalinglaws,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@article{yang2024muso,
  title={MUSO: Achieving Exact Machine Unlearning in Over-Parameterized Regimes},
  author={Yang, Ruikai and He, Mingzhen and He, Zhengbao and Qiu, Youmei and Huang, Xiaolin},
  journal={arXiv preprint arXiv:2410.08557},
  year={2024}
}
 

@inproceedings{izzo2021approximate,
  title={Approximate data deletion from machine learning models},
  author={Izzo, Zachary and Smart, Mary Anne and Chaudhuri, Kamalika and Zou, James},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2008--2016},
  year={2021},
  organization={PMLR}
}
 

@article{zhang2024right,
  title={Right to be forgotten in the era of large language models: Implications, challenges, and solutions},
  author={Zhang, Dawen and Finckenberg-Broman, Pamela and Hoang, Thong and Pan, Shidong and Xing, Zhenchang and Staples, Mark and Xu, Xiwei},
  journal={AI and Ethics},
  pages={1--10},
  year={2024},
  publisher={Springer}
}
 

@inproceedings{bourtoule2021unlearning,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={141--159},
  year={2021},
  organization={IEEE}
} 

@article{lucki2024adversarial,
  title={An adversarial perspective on machine unlearning for ai safety},
  author={{\L}ucki, Jakub and Wei, Boyi and Huang, Yangsibo and Henderson, Peter and Tram{\`e}r, Florian and Rando, Javier},
  journal={arXiv preprint arXiv:2409.18025},
  year={2024}
}
 

@article{lynch2024eight,
  title={Eight methods to evaluate robust unlearning in llms},
  author={Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2402.16835},
  year={2024}
}

@inproceedings{matternmembership,
    title = "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
    author = "Mattern, Justus  and
      Mireshghallah, Fatemehsadat  and
      Jin, Zhijing  and
      Schoelkopf, Bernhard  and
      Sachan, Mrinmaya  and
      Berg-Kirkpatrick, Taylor",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.719",
    pages = "11330--11343",
    
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}
 

@inproceedings{huang2024eco,
  title={ECO: Efficient Computational Optimization for Exact Machine Unlearning in Deep Neural Networks},
  author={Huang, Yu-Ting and Wu, Pei-Yuan and Wang, Chuan-Ju},
  booktitle={2nd Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ ICML 2024)}
}

@inproceedings{fumembership,
  title={Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration},
  author={Fu, Wenjie and Wang, Huandong and Gao, Chen and Liu, Guanghua and Li, Yong and Jiang, Tao},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems}
}

@inproceedings{liu2022continual,
  title={Continual learning and private unlearning},
  author={Liu, Bo and Liu, Qiang and Stone, Peter},
  booktitle={Conference on Lifelong Learning Agents},
  pages={243--254},
  year={2022},
  organization={PMLR}
}

@software{tofuhf,
  author = {Pratyush Maini and Zhili Feng and Avi Schwarzschild and Zachary C. Lipton and J. Zico Kolter},
  title = {TOFU: Task of Fictitious Unlearning},
  year = 2024,
  url = {https://huggingface.co/datasets/locuslab/TOFU},
}

@software{tofugithub,
  author = {Pratyush Maini and Zhili Feng and Avi Schwarzschild and Zachary C. Lipton and J. Zico Kolter},
  title = {TOFU: Task of Fictitious Unlearning},
  year = 2024,
  url = {https://github.com/locuslab/tofu},
}

@software{npogithub,
  title={Negative preference optimization: From catastrophic collapse to effective unlearning},
  author={Zhang, Ruiqi and Lin, Licong and Bai, Yu and Mei, Song},
  year={2024},
  url = {https://github.com/licong-lin/negative-preference-optimization}
}
@software{llmart2025github,
  author = {Cory Cornelius and Marius Arvinte and Sebastian Szyller and Weilin Xu and Nageen Himayat},
  title = {{LLMart}: {L}arge {L}anguage {M}odel adversarial robutness toolbox},
  url = {http://github.com/IntelLabs/LLMart},
  version = {2025.01},
  year = {2025},
}

@inproceedings{lester2021prompttuning,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243/",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
}


@misc{qi2024unrelateddata,
      title={On Evaluating the Durability of Safeguards for Open-Weight LLMs}, 
      author={Xiangyu Qi and Boyi Wei and Nicholas Carlini and Yangsibo Huang and Tinghao Xie and Luxi He and Matthew Jagielski and Milad Nasr and Prateek Mittal and Peter Henderson},
      year={2024},
      eprint={2412.07097},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2412.07097}, 
}

@inproceedings{loshchilov2018decoupled,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@inproceedings{maini2021di,
  title={Dataset Inference: Ownership Resolution in Machine Learning},
  author={Pratyush Maini and Mohammad Yaghini and Nicolas Papernot},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=hvdKKV2yt7T}
}

@misc{maini2024dillms,
      title={LLM Dataset Inference: Did you train on my dataset?}, 
      author={Pratyush Maini and Hengrui Jia and Nicolas Papernot and Adam Dziedzic},
      year={2024},
      eprint={2406.06443},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.06443}, 
}

@misc{zou2024circuitbreakers,
      title={Improving Alignment and Robustness with Circuit Breakers}, 
      author={Andy Zou and Long Phan and Justin Wang and Derek Duenas and Maxwell Lin and Maksym Andriushchenko and Rowan Wang and Zico Kolter and Matt Fredrikson and Dan Hendrycks},
      year={2024},
      eprint={2406.04313},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04313}, 
}

@inproceedings{cao2015towards,
  title={Towards making systems forget with machine unlearning},
  author={Cao, Yinzhi and Yang, Junfeng},
  booktitle={2015 IEEE symposium on security and privacy},
  pages={463--480},
  year={2015},
  organization={IEEE}
}

@software{eldan2023harrypotter-hf,
  title={Who's Harry Potter? Approximate Unlearning in LLMs},
  author={Eldan, Ronen and Russinovich, Mark},
  year={2023},
  url = { https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter}
}

@article{welch1947ttest,
    author = {WELCH, B. L.},
    title = {THE GENERALIZATION OF ‘STUDENT'S’ PROBLEM WHEN SEVERAL DIFFERENT POPULATION VARLANCES ARE INVOLVED},
    journal = {Biometrika},
    volume = {34},
    number = {1-2},
    pages = {28-35},
    year = {1947},
    month = {01},
    issn = {0006-3444},
    doi = {10.1093/biomet/34.1-2.28},
    url = {https://doi.org/10.1093/biomet/34.1-2.28},
    eprint = {https://academic.oup.com/biomet/article-pdf/34/1-2/28/553093/34-1-2-28.pdf},
}

@misc{asciitable,
    title = {ASCII Table},
    author = {asciitable.com},
    url = {https://www.asciitable.com },
}
