\begin{abstract}
% LLMs are popular
% Privacy, IP, and harmful output considerations
% Unlearning is one of the key reserach angles to address both
% Is approximate unlearning succesful
% You can attack/audit with soft-prompts
% Soft-prompts are too strong and prior eval cannot be taken for granted
Large language models (LLMs) have become increasingly popular.
Their emergent capabilities can be attributed to their massive training datasets.
However, these datasets often contain undesirable or inappropriate content, e.g., harmful texts, personal information, and copyrighted material.
This has promoted research into \emph{machine unlearning} that aims to remove information from trained models.
In particular, \emph{approximate unlearning} seeks to achieve information removal by strategically editing the model rather than complete model retraining.

Recent work has shown that soft token attacks (\sta) can successfully extract purportedly unlearned information from LLMs, thereby exposing limitations in current unlearning methodologies.
In this work, we reveal that \sta{s} are an inadequate tool for auditing unlearning.
Through systematic evaluation on common unlearning benchmarks (\textit{Who Is Harry Potter?} and \textit{TOFU}), we demonstrate that such attacks can elicit any information from the LLM, regardless of (1) the deployed unlearning algorithm, and (2) whether the queried content was originally present in the training corpus.
Furthermore, we show that \sta with just a few soft tokens ($1-10$) can elicit random strings over 400-characters long.
Thus showing that \sta{s} are too powerful, and misrepresent the effectiveness of the unlearning methods.

Our work highlights the need for better evaluation baselines, and more appropriate auditing tools for assessing the effectiveness of unlearning in LLMs.
\end{abstract}