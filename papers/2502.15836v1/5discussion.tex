\section{Discussion \& Conclusion}\label{sec:discussion}

\fauxsection{Auditing with hard prompts.}
Attacks such as greedy coordinate gradient~\cite{zou2023gcg} optimize the attack prompt in the \emph{hard} token space instead of the soft token space.
Hence, they are weaker at eliciting completions.
On one hand, this might make them more suitable for auditing unlearning.
On the other hand, due to their computational requirements, they are often used to force only the beginning of a harmful completion (e.g. \textit{Sure, here's how to build a bomb...}) with the hope that the LLM follows.
It is unclear whether this would be sufficient to produce specific unlearned passages.
We see it as an interesting direction for future work.

\fauxsection{Unlearning vs jail-breaking.}
Our findings are applicable to the jail-breaking community as well.
Prior work~\cite{zhang2024safe} hinted that unlearning and preventing harmful outputs can be viewed as the same task -- removing or suppressing particular information.
\sta{s} and fine-tuning attacks~\cite{hu2024jogging} are useful tools for evaluating LLMs in powerful threat models.
It was shown that fine-tuning on benign data, or data unrelated to the unlearning records (for jail-breaking and unlearning respectively) can restore undesirable behavior~\cite{lucki2024adversarial}.

\fauxsection{Variation in gradient-based learning.}
Prior work showed that removing training records from the training set,
and repeating the training can result in the same final model~\cite{thudi2022auditunlearning} depending on the random seed.
Even though a record was part of the training run, its influence might be minimal, making unlearning unnecessary.
Similarly, it was shown that SGD has intrinsic privacy guarantees, assuming there exists a group of similar records~\cite{hyland2022empiricalsgdprivacy}.
Thus, algorithmic auditing of unlearning might not be possible, and one would have to rely on verified or attested procedures instead~\cite{eisenhofer2023verifiedunlearning}, regardless of their impact on the model.

\fauxsection{Distinguishing learned soft tokens.}
Even though, in most our results, the number of soft tokens required to elicit a completion is the same,
we attempted to distinguish between them.
To this end, we take all single-token \sta{s} optimized for \tofu (Table~\ref{tab:attack-unlearning-tofu}) and assign a label $y=\{0, 1\}$: $y=0$ for $f_\emptyset$, and $y=1$ for $f_{ft}$ and the unlearned models.
We then train a binary classifier using $f_\emptyset$ and $f_{ft}$.
While we are able to overfit it and distinguish between $f_\emptyset$ and $f_{ft}$,
we were not able to train a model that would generalize to the unlearned models, and decisively assign a class. 
Our approach is similar to Dataset Inference~\cite{maini2021di,maini2024dillms} which showed there can be distributional differences between the models, depending on the data they were trained on.
Further investigation into \emph{what} soft tokens are learned during the audit is an interesting direction for future work.

\section{Conclusion}\label{sec:conclusion}

In this work, we show that soft token attacks (\sta{s}) cannot reliably distinguish between base, fine-tuned, and unlearned models.
In all cases, the auditor can elicit all unlearned information by appending optimized soft prompts to the base prompt.
Additionally, we show that \sta with a single soft token can elicit $150$ random characters, and over $400$ with soft tokens.

Our work demonstrates that machine unlearning in LLMs needs better evaluation frameworks.
While many unlearning methods can be broken by simple paraphrasing of original prompts, or by fine-tuning on partial unlearned data or even \emph{unrelated data}, 
\sta misrepresents their efficacy.

\section{Limitations \& ethical considerations}\label{sec:limitations}

\fauxsection{Limitations.}
Due to computational constraints our work is limited to 7-8 billion parameter models.
Nevertheless, given that LLMs' expressive power increases with size~\cite{kaplan2020scalinglaws}, our results should hold for larger LLMs.
Our evaluation with random strings could be extended to verify if there is a clear and generalizable dependency between the number of soft tokens and the maximum number of generated characters.

\fauxsection{Ethical considerations.}
In this work, we show that an auditor (a user) with white-box access to the model, and sufficient compute can elicit any text from the LLM.
While it does require knowing the target completion for a given prompt, it is likely that partial completions might be enough, thus allowing the user to elicit harmful information.
This may be particularly dangerous in settings where the user has approximate knowledge of the information that had been scrubbed off the LLM.