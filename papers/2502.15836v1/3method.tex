\section{Auditing with Soft Token Attacks}\label{sec:method}
% -- Outline
% --- Hard and soft token attacks
% --- A framework for auditing approximate unlearning -- some algo?
% --- Instantiation of the framework using soft token attacks

\subsection{Adversarial prompts}

An \emph{adversarial prompt} $x_a$, is an input prompt to the LLM, obtained by applying the transform $T(\cdot)$ to the base prompt $x_p$, $x_a = T(x_p, \textsl{aux})$ in order to elicit a desired completion $c$.
$T$ can be any function that swaps, removes or adds tokens; \textsl{aux} denotes any additional needed information.
However, such arbitrary attacks are expensive to optimize\footnote{Suffix-only attacks allow efficient use of the KV-cache.}, and difficult to reason about.
In practice, $T$ optimizes an \emph{adversarial suffix} $x_s$ that is appended to $x_p$ to elicit $c$~\cite{zou2023gcg}.
Specifically, we optimize the probability:
\begin{equation}
    Prob = P(c | x_p \oplus x_s).
\end{equation}

An adversary with white-box access to the LLM, can instead mount the attack in the \emph{embedding space} i.e. modify the \emph{soft tokens}:
\begin{equation}
    Prob = P(c | embed(x_p) \oplus embed(x_s)).
\end{equation}
In this case, $T$ uses the gradient from the LLM to update $x_s$.

\subsection{Unlearning auditor}\label{sec:method:framework}

An \emph{oracle} auditor $A_o$ takes an unlearned model $f_u$ and the candidate sentences $x_c \in X_c$ and outputs a \emph{ground} truth, binary decision $a=\{0, 1\}$ indicating whether the given records was part of $D_{train}$ of:
\begin{equation}
    a = A_o(f_u, X_c = D_{forget}, \textsl{aux})
\end{equation}
$A_o$ is unrealistic in many scenarios; however, it can be easily instantiated for exact unlearning where $A_o$ knows the training data associated with $f$: $aux = \{  D_{retain} \}$.

On the other hand, a realistic unlearning $A_u$ takes an $f_u$, and $D_{forget}$ and outputs a score $s=(0,1)$ indicating whether the records were in $D_{train}$:
\begin{equation}
    s = A_u(f_u, \{x_c\} = D_{forget}, \textsl{aux}=\emptyset)
\end{equation}
$A_u$ represents cases where users remove information from models that they did not create, e.g. to prevent harmful outputs.

In this work, we instantiate the soft token attack auditor $A_\sta$ based on the soft token attacks (\sta{s}) against unlearning~\cite{schwinn2024soft,zou2024circuitbreakers}.
Our auditor compares the relative difficulty of eliciting $c$ for $f_{ft}$ and $f_u$.
The unlearning procedure is effective if eliciting completions using $f_u$ is more difficult than $f_{ft}$.
\begin{equation}
    s = A_\sta(f_u, \{x_c\}=D_{forget}, \textsl{aux}=\{f_{ft}\}).
\end{equation}

Figure~\ref{fig:auditor} gives a complete overview of the auditing procedure, and the difference between $A_0$ and $A_\sta$.
In Table~\ref{tab:notation}, we summarize the notation.

In the next Section, we show that $A_\sta$ cannot reliably audit LLMs.
