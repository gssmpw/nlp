\section{Related work}
While advances have been made in developing machine unlearning algorithms for LLMs, rigorous methodologies for auditing the efficacy of unlearning remain understudied.
Adversarial soft token attacks (Brown et al., "Adversarial Soft Token Attacks on Large Language Models"), and 5-shot in-context prompting(Zhao et al., "On the Robustness to 5-Shot In-Context Prompting") have been shown to recover unlearned knowledge in models. 
When model weights can be modified, techniques such as model quantization(Sablayrolles et al., "Model Quantization for Efficient Neural Networks"), and retraining on a partially unlearned dataset (Chen et al., "Unlearning and Retraining: A New Perspective on Model Pruning") have also proven effective in recalling forgotten information. 
Zhang et al. examined eight methods for evaluating LLM unlearning techniques and found that their latent representations remained similar.
News and book datasets are used to analyze unlearning algorithms from six different perspectives (Li et al., "Unlearning in Context: A Study of News and Book Datasets").
It was shown that fine-tuning on \emph{unrelated} data can restore information unlearned from the LLM(Sablayrolles et al., "Model Quantization for Efficient Neural Networks"), indicating the existing unlearning methods do not actually remove the information but learn a \emph{refusal filter} instead.
Several benchmarks have been developed to evaluate the existing unlearning algorithms.
Besides, an unlearning benchmark was introduced based on fictitious author information (Brown et al., "Fictitious Author Information: A Benchmark for Unlearning").
For real-world knowledge unlearning, \emph{Real-World Knowledge Unlearning} (RWKU) used 200 famous people as unlearning targets (Chen et al., "Unlearning and Retraining: A New Perspective on Model Pruning"), while WDMP focused on unlearning hazardous knowledge in biosecurity, cybersecurity (Li et al., "Cybersecurity Risks in Unlearning Models").