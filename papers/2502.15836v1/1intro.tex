\section{Introduction}\label{sec:introduction}
% LLMs are popular
% XYZ considerations
% Unlearning is one of key reserach angles
% Is approximate unlearning succesful
% You can audit with soft-prompts
% We show that auditing unlearning with soft prompts is too strong
% Contribution:
% benchmarks with datasets and on llms
% --- point out that harry potter is broken in general because chat and no \n\n
% eliciting randoms strings is possible and invalidates regular unlearning benchmarks
% moving forward, researchers need to calibrate they solutions better and get better baselines
In recent years, large language models (LLMs) have undergone substantial advancements, leading to enhanced performance and widespread adoption.
LLMs have demonstrated exceptional performance in various downstream tasks, such as machine translation~\cite{zhu2023multilingual}, content generation~\cite{acharya2023llm}, and complex problem-solving~\cite{xi2025rise}.
Their performance is attributed to their large-scale architectures that require datasets consisting of up to billions of tokens to train effectively~\cite{kaplan2020scalinglaws}.
These datasets are typically derived from large-scale corpora sourced from public internet text.
However, such datasets inadvertently contain harmful or inappropriate content, including instructions for hazardous activities (e.g., bomb-making), violent or explicit material, private information, or copyrighted content unsuitable for applications.
Given the sensitive nature of such data, it may be necessary to remove it from the LLM to comply with the local regulations, or internal company policies.

\emph{Machine unlearning} is a tool for removing information from models~\cite{cao2015towards, bourtoule2021unlearning}.
\emph{Approximate unlearning} usually refers to removing information from models without resorting to retraining them from scratch~\cite{zhang2024right, eldan2023harrypotter, izzo2021approximate}, ensuring that the resulting model deviates from a fully retrained version within a bounded error.
While numerous studies have proposed various unlearning algorithms, most lack formal guarantees of effectiveness.
In fact, prior research has demonstrated that many unlearning techniques can be circumvented through simple rephrasings of the original data~\cite{shi2024muse}.
Recent work has shown that a \emph{soft token attack} (\sta) can be used to elicit harmful completions and extract supposedly unlearned information from models~\cite{schwinn2024soft,zou2024circuitbreakers}.

In this work, we introduce a simple framework for \emph{auditing unlearning} and demonstrate that \sta{s} are overly powerful, and hence, inappropriate for verifying the effectiveness of approximate unlearning.
We show that the auditor can elicit any information from the model, regardless of its training data. 
Our work highlights the need for better unlearning auditing baselines and methodologies.

We claim the following contributions:
\begin{enumerate}[label=\arabic*.]
    \item We introduce a simple auditing framework for unlearning in LLMs (Section~\ref{sec:method:framework}).
    \item We show that \sta{s} effectively elicit unlearned information in all tested unlearning methods and benchmark datasets (\emph{Who Is Harry Potter?}, and \emph{TOFU}). Additionally, we show that \sta{s} also elicit information in the base models that \textbf{were not fine-tuned on the benchmark datasets} (Section~\ref{sec:eval:attacks}).
    \item We further demonstrate that the \sta{s} are inappropriate for evaluating unlearning -- we show that a single soft token can elicit $150$ random tokens, and ten soft tokens can elicit over $400$ random tokens (Section~\ref{sec:eval:random-strings}).
\end{enumerate}

The remainder of this paper is organized as follows:
In Section~\ref{sec:background} we provide an overview of the necessary background, and the related work.
Section~\ref{sec:method} introduces a general auditing framework, and instantiates it using \sta.
In Section~\ref{sec:eval}, we demonstrate the efficacy of \sta, and subsequently its failure, as a tool for auditing unlearning in LLMs. 
In Section~\ref{sec:discussion} we discuss additional considerations for auditing unlearning in LLMs.
We conclude the paper in Section~\ref{sec:conclusion}, and highlight some limitations in Section~\ref{sec:limitations}.


