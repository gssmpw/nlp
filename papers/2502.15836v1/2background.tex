\section{Background \& Related work}\label{sec:background}
% Background
% LLMs
% Unlearning -- briefly exact. and approx.; key papers.
% Auditing

% Related
% Papers about analysing effectiveness of unlearning, questioning efficacy of robustness eval
% Only here we introduce related works, not the previous works in unlearning/auditing..
\subsection{Background}

\begin{figure*}[ht!]
    \includegraphics[width=\textwidth]{figures/Fig_STA.pdf}
    \caption{Overview of the auditing process using $A_\sta$. For a perfect unlearning method, $A_o$ always correctly audits the model. On the other hand, $A_\sta$ can elicit the completion regardless of the information in the model -- the audit is ineffective.}
    \label{fig:auditor}
\end{figure*}
\fauxsection{Large language models} (LLMs) process input text through an auto-regressive framework.
Given an input sequence of tokens $x_{1:t}$, the model computes the conditional probability distribution $p(x_{t+1}|x_{1:t})$ over the vocabulary at each time-step.
The likelihood of the sequence is given by:
\begin{equation}
    \log p(x_{t+1}|x_{1:t}) = \sum_{t=1}^T \log p(x_{t}|x_{1:t-1})
\end{equation}
At inference time, the tokens is generated iteratively by sampling the next token $x_{t+1}$ from this distribution (e.g., via greedy decoding or nucleus sampling~\cite{holtzman2019curious}), then appending it to the context $x_{1:t}$ for the subsequent step.

\fauxsection{Machine unlearning} is a tool for removing information from models.
Consider a machine learning model $f$ optimized over a training dataset $D_{train}$. 
When a data owner submits an unlearning request to remove a specified subset $D_{forget} \in D_{train}$, 
the objective of machine unlearning is to produce an unlearned model $f_{u}$ that eliminates the influence of $D_{forget}$.
Machine unlearning methodologies are categorized into two paradigms -- exact, and approximate unlearning.

\fauxsection{Exact unlearning} ensures the output distribution of $f_u$ is statistically indistinguishable from that of a model retrained exclusively on the retained dataset $D_{retain} = D_{train} / D_{forget}$.
This guarantees provable data removal, satisfying:
\begin{equation}
\centering
\begin{aligned}
    p(f_u(x) = y) & = p(f_{ret}(x) = y) \quad \\ 
    & s.t. \quad \forall(x,y) \in D_{train},
\end{aligned}
\end{equation}
where $f_{ret}$ denotes a model retrained from scratch on $D_{retain}$ -- which is the most straightforward way of achieving exact unlearning.

The process can be made more efficient by splitting the $D_{train}$ into overlapping chunks, and training an ensemble of models~\cite{bourtoule2021machine}.
During an unlearning request, only the models containing the requested records need to be retrained.
For certain classes of models, it is possible to achieve exact unlearning without retraining, e.g. ECO adapts the Cauwenberghs and Poggio (CP) algorithm for exact unlearning within LeNet~\cite{huang2024eco}, and MUSE relabels the target data to achieve unlearning for over-parameterized linear models~\cite{yang2024muso}.

\fauxsection{Approximate unlearning}, sometimes called \emph{in-exact unlearning}, relaxes the strict equivalence requirement, instead only requiring that $f_{u}$ approximates $f_{ret}$ within some bounded error.
This paradigm relies on empirical metrics or probabilistic frameworks.
In LLMs, approximate unlearning is typically accomplished by overwriting the information in the model~\cite{eldan2023harrypotter,wang2024rkld}, guiding the model away from it~\cite{feng2024fine}, or editing the weights and/or activations~\cite{liu2024large,bhaila2024soft,li2024wmdp,tamirisa2024toward,huu2024effects, ashuach2024revs, meng2022locating, meng2022mass}.

\subsection{Related work}
While advances have been made in developing machine unlearning algorithms for LLMs, rigorous methodologies for auditing the efficacy of unlearning remain understudied.
Adversarial soft token attacks (\sta{s})~\cite{schwinn2024soft} and 5-shot in-context prompting~\cite{doshi2024doesunlearningtrulyunlearn} have been shown to recover unlearned knowledge in models. 
When model weights can be modified, techniques such as model quantization~\cite{zhang2024does} and retraining on a partially unlearned dataset ~\cite{lucki2024adversarial,hu2024jogging} have also proven effective in recalling forgotten information. 
~\cite{lynch2024eight} examined eight methods for evaluating LLM unlearning techniques and found that their latent representations remained similar.
News and book datasets are used to analyze unlearning algorithms from six different perspectives~\cite{shi2024muse}.
It was shown that fine-tuning on \emph{unrelated} data can restore information unlearned from the LLM~\cite{qi2024unrelateddata}, indicating the existing unlearning methods do not actually remove the information but learn a \emph{refusal filter} instead.
Several benchmarks have been developed to evaluate the existing unlearning algorithms.
Besides, an unlearning benchmark was introduced based on fictitious author information~\cite{maini2024tofu}.
For real-world knowledge unlearning, \emph{Real-World Knowledge Unlearning} (RWKU) used 200 famous people as unlearning targets~\cite{jin2024rwku}, while WDMP focused on unlearning hazardous knowledge in biosecurity, cybersecurity~\cite{li2024wmdp}.
