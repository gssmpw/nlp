%\documentclass[anon,12pt]{colt2025} % 
 \documentclass[journal,onecolumn]{IEEEtran}
%\documentclass{jmlr}

%Anonymized submission
%\documentclass[final,12pt]{colt2025} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{eufrak}

\usepackage{natbib}

\usepackage{comment}
\usepackage{paralist}

\usepackage{amsmath}%
\usepackage{MnSymbol}%
\usepackage{wasysym}%

%\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{algorithm}
%\usepackage[ruled,linesnumbered]{algorithm2e}

\usepackage{enumitem}

\begin{comment}
\usepackage{titlesec}

\titlespacing\section{0pt}{8pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{8pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{8pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\end{comment}

\newenvironment{myassumptions}{%
   \begin{description}[style=multiline, leftmargin = 18pt, align=left,font=\normalfont]%
}{%
   \end{description}%
}
 \makeatletter
  \def\nl#1#2{\begingroup
     \scalebox{0.85}[1]{\textbf{#2}}%
    \def\@currentlabel{\textnormal{\scalebox{0.85}[1]{\textbf{#2}}}}% Apply the scaling to the cross-reference
     \phantomsection\label{#1}\endgroup
}
\makeatother

%\title{Malliavin Cubature for Efficient NSDE Training}
%\author{Luke Snow}
%\date{October 2024}

\newcommand{\E}{\mathcal{E}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\ep}{\epsilon}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\del}{\partial}
\newcommand{\CL}{\mathcal{L}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\CE}{\mathbb{E}}
\newcommand{\bo}{\boldsymbol{1}}
\newcommand{\om}{\omega}
\newcommand{\CD}{\mathcal{D}}
\newcommand{\CF}{\mathcal{F}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\CA}{\mathcal{A}}

\newcommand{\BA}{\mathbb{A}}
\newcommand{\bg}{\mathfrak{g}}
\newcommand{\hn}{\hat{n}}
\newcommand{\hx}{\hat{x}}
\newcommand{\Q}{Q}
\newcommand{\tQ}{\tilde{Q}}

\newcommand{\KLV}{\text{KLV}}
\newcommand{\RMP}{\text{RMP}}

\newcommand{\ps}{p^*}
\newcommand{\ydim}{d_y}
\newcommand{\nndim}{d}
\newcommand{\bdim}{d_b}
\newcommand{\adim}{d_{\vvar}}
\newcommand{\xdim}{d_x}

\newcommand{\tdim}{d_{\theta}}

\newcommand{\CO}{\mathcal{O}}

\newcommand{\CN}{\mathcal{N}}

\newcommand{\PR}{\mathbb{P}}

\newcommand{\CX}{\mathcal{X}}

\newcommand{\Xd}{X_{\text{data}}}

\newcommand{\law}{\text{Law}}

\newcommand{\mc}{n}

\newcommand{\CLd}{\CL_{\text{data}}}
\newcommand{\fdata}{f_{\text{data},t}}

\newenvironment{proof}{\noindent \textbf{Proof:}}{$\hfill\blacksquare$}

\newcommand{\lf}{f}

%\newcommand{\infb}{I_1}
\newcommand{\infbd}{I}

\newcommand{\clfint}{C_{\lf}}

\newcommand{\tds}{T_{\Delta^2}}

\newcommand{\ut}{\underline{t}}

\newcommand{\ui}{\underline{i}}

\newcommand{\us}{\underline{s}}

\newcommand{\lfgsup}{A}

\newcommand{\vfbd}{M'}
\newcommand{\pmlen}{L}

\newcommand{\tPhi}{\tilde{\Phi}}
\newcommand{\tphi}{\tilde{\phi}}
\newcommand{\tlam}{\tilde{\lambda}}

\newcommand{\CP}{\mathcal{P}}


\newcommand{\cfac}{c}
\newcommand{\Xt}{X^{\theta}}
\newcommand{\A}{A}
\newcommand{\vvar}{v}

\newcommand{\cn}{q} % cubature-num

\newcommand{\phisol}{\phi_z^{\theta}(t)}

\newtheorem{mtheorem}{Meta-Theorem}
\newtheorem{mcorollary}{Meta-Corollary}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}

\usepackage{xr}
\usepackage{titlesec}
\usepackage{times}
\usepackage{hyperref}
%\externaldocument{colt2025_supp}


%\title{Efficient Neural SDE Training using Wiener-Space Cubature}
%\usepackage{times}

% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:

%\allowdisplaybreaks
\title{Efficient Neural SDE Training using Wiener-Space Cubature \thanks{This research was supported by NSF grants CCF-2312198 and CCF-2112457 and U. S. Army Research Office under grant W911NF-24-1-0083}}

\author{Luke Snow,  Vikram Krishnamurthy \thanks{Department of Electrical \& Computer Engineering, Cornell University, Ithaca, NY 14853, USA.  emails: las474@cornell.edu and vikramk@cornell.edu}}

\begin{document}

%\setlength{\abovedisplayskip}{2pt}
%\setlength{\belowdisplayskip}{2pt}

\maketitle

\begin{abstract}%
%As such, neural SDEs are highly flexible continuous-time generative models for time-series data.
%This objective functional statistic measures the distance between the law of the neural SDE sample paths and an empirical data distribution on path-space.
 A neural stochastic differential equation (SDE) is an SDE with drift and diffusion terms parametrized by neural networks. The training procedure for neural SDEs consists of optimizing the SDE vector field (neural network) parameters to minimize the expected value of an \textit{objective functional} on infinite-dimensional path-space. Existing training techniques focus on methods to efficiently compute path-wise gradients of the objective functional with respect to these parameters, then pair this with Monte-Carlo simulation to estimate the expectation, and stochastic gradient descent to optimize. In this work we introduce a novel training technique which bypasses and improves upon Monte-Carlo simulation; we extend results in the theory of Wiener space cubature to approximate the expected objective functional by a weighted sum of \textit{deterministic ODE solutions}. This allows us to compute gradients by \textit{efficient ODE adjoint methods}. Furthermore, we exploit a high-order recombination scheme to drastically reduce the number of ODE solutions necessary to achieve a reasonable approximation. We show that this Wiener-space cubature approach can surpass the $\CO(n^{-1/2})$ rate of Monte-Carlo simulation, or the $\CO(\log(n)/n)$ rate of quasi-Monte-Carlo, to achieve a  $\CO(n^{-1})$ rate under reasonable assumptions. 
\end{abstract}


\section{Introduction}

%Stochastic differential equations (SDEs) are ubiquitous models in physics, engineering, biology and finance which are used to describe continuous-time processes subjected to random fluctuations and perturbations. The stochasticity is a fundamental component which allows for probabilistic and uncertainty modeling. 

A neural stochastic differential equation (SDE) \citep{tzen2019neural,liu2019neural,kidger2021neural,issa2024non,tzen2019theoretical,li2020scalable}  is a SDE with drift and diffusion terms parametrized by neural networks. Neural SDEs are highly flexible \textit{continuous-time generative models} for time-series data, capable of approximating arbitrary dynamics.  They are useful for modeling continuous-time systems with noise, as  they learn  the underlying stochastic processes directly from data, using the function approximation capabilities of neural networks. Applications include generative time-series tasks such as in quantitative finance \citep{gierjatowicz2020robust,arribas2020sig,choudhary2024funvol}.

The training procedure for neural SDEs involves  tuning the vector field (neural network) parameters to minimize a functional distance measure between the distributional law of the neural SDE and an empirical data distribution. Existing works innovate along two main directions: the development  of novel functional distance measures, and construction of more efficient computation of gradients with respect to parameters. The former  includes approaches such  as adversarial GAN objectives \citep{kidger2021neural}, variational free energy minimization \citep{tzen2019neural}, finite-dimensional distribution matching \citep{zhang2024efficient}, and signature kernal scores \citep{issa2024non}. The latter  includes methods such as automatic differentiation (AD) \citep{tzen2019neural} and stochastic adjoint methods \citep{kidger2021efficient}. 

Existing training techniques assume  that path-wise parameter gradient computations of neural SDE sample paths can be combined with Monte-Carlo simulation to obtain gradients of the \textit{distributional} performance, quantified by the  expected value of an appropriate loss functional. This Monte-Carlo simulation produces an estimate with error $\CO(n^{-1/2})$, where $n$ is the number of SDE sample path gradient evaluations.

In this work, we move beyond this Monte-Carlo training regime to construct a more efficient training scheme. Specifically, we extend results from  the theory of Wiener space cubature to express the expected loss functional as a linear functional of a \textit{weighted sum of deterministic ordinary differential equation (ODE) solutions}. This reformulation allows us to express the optimization objective as a minimization problem with respect to this functional of ODE solutions. Optimization of this derived expression offers two important advantages:
\begin{compactenum}%[label=\roman*)]
    \item \textit{Exploiting efficient ODE adjoint methods}. We improve upon existing neural SDE path-wise gradient computation by leveraging  \textit{efficient ODE gradient computation methods}. 
    \item \textit{Improve  Monte-Carlo complexity}. Under reasonable assumptions, we improve upon the $\CO(n^{-1/2})$ Monte-Carlo simulation complexity, achieving $\CO(n^{-1})$ approximation bounds. 
    %Furthermore, under our framework $n$ is the number of \textit{ODE} adjoint equations to be solved, rather than the more expensive SDE adjoint or AD computations of existing works.  
\end{compactenum}



%Instead of summing function values at discrete points in a finite-dimensional space, function space cubature sums a convex combination of representative functions (called "cubature paths") with corresponding weights to approximate an integral over a function space. 
\textit{Context -- Cubature on Function Space}: As mentioned above, our key mathematical construction is an extension of cubature on Wiener space to the path functional domain. Cubature on function space extends the idea of cubature for numerical integration, as represented by Tchakaloffs theorem \citep{bayer2006proof}, to infinite-dimensional path-space. A key example arises in stochastic processes, where cubature methods approximate expectations of functions of diffusions by replacing the process with a weighted sum of deterministic paths. This approach, introduced in the context of cubature on Wiener space by Lyons and Victoir \citep{lyons2004cubature}, is particularly useful for solving partial differential equations and stochastic differential equations.



Next we introduce the neural SDE model in Section~\ref{sec:nsde}, discuss the learning framework in Section~\ref{sec:learnfram}, and outline our contributions in more detail in Section~\ref{sec:contrib}. 

%Neural SDEs are extensions of neural ODEs \cite{chen2018neural}, where the injected stochasticity effectively stabilizes and regularizes the output \cite{liu2019neural} while also serving as a natural construction for deep \textit{probabilistic} generative models \cite{tzen2019neural}.  In this sense, they are theoretically interesting models for studying the properties of their discrete counterparts. They are also practically

\subsection{Neural SDE}
\label{sec:nsde}

In this section we introduce the neural SDE as an It\^o stochastic differential equation since that is the prevailing form in the neural SDE literature. However, in Section~\ref{sec:nsdecub} we reformulate it as a Stratonovich SDE for analytical convenience and consistency with the cubature methodology.

Fix a finite time horizon $T>0$, and let $B: [0,T] \to \reals^{\bdim}$ be a $\bdim$-dimensional standard Brownian motion. Let $\vvar \sim \CN(0,I_{\adim})$ be a $\adim$-dimensional normal random variable.
We define the neural SDE model analagously to \citet{kidger2021neural,issa2024non,zhang2024efficient}\footnote{Some works treat the SDE \eqref{eq:nsde} as a latent process which gives rise to a different observable process through an affine transformation; our exposition is without loss of generality, since this affine transformation can be absorbed into the drift and diffusion terms by It\^o's Lemma.}, as
\begin{align}
\begin{split}
\label{eq:nsde}
 \Xt_0 = \zeta^{\theta}(\vvar), \quad dX_t^{\theta} = \mu^{\theta}(t,X_t^{\theta})dt + \sigma^{\theta}(t,X_t^{\theta})dB_t
\end{split}
\end{align}
with $t\in[0,T]$, and
\begin{align}
    \zeta^{\theta} :\reals^{\adim} \to \reals^{\xdim}, \quad \mu^{\theta}: [0,T] \times \reals^{\xdim}\to \reals^{\xdim}, \quad \sigma^{\theta}: [0,T] \times \reals^{\xdim} \to \reals^{\xdim\times \bdim}
\end{align} 
Here $\zeta^{\theta}, \mu^{\theta}, \sigma^{\theta}$ are \textit{neural networks} collectively parametrized by weights $\theta \in \Theta$, where $\Theta$ is a compact subset of $\reals^{\tdim}$. We specify assumptions on the structure of the neural networks in Section~\ref{sec:prelim}, such that \eqref{eq:nsde} has a unique strong solution for all $t\in[0,T]$, denoted by $X_t^{\theta}\in\reals^{\xdim}$.

\subsection{Learning Framework}
\label{sec:learnfram}
In this section we outline the framework for training neural SDEs, and provide an outline of our main results. Neural SDEs are trained such that their path-space distributional statistics approximate some empirical (training) data in path-space.

\textit{Neural SDE Path-Space Distribution}: Let $\CX$ be the space of continuous paths on $[0,T]$, and denote $\Xt \in \CX$ a sample path of the neural SDE \eqref{eq:nsde} with parameter $\theta$. We index path $X^{\theta}$ in time as $(X^{\theta}_t)_{t\in[0,T]}$. Let $\CP(\theta) = \law((X_t^{\theta})_{t\in[0,T]})$ be the distributional law of the process $\Xt$. 
%Denote $\Delta(\CX)$ the space of infinite-dimensional distributions over $\CX$. \eqref{eq:nsde}. 

\textit{Empirical Training Data}: We assume access to an empirical data distribution $\Xd$ on path-space $\CX$, i.e., a collection of paths on $\CX$. We aim to train \eqref{eq:nsde} such that $\CP(\theta) \approx\Xd$; such that $\CP(\theta)$ approximates $\Xd$ in a distributional sense. This approximation can be quantified rigorously by utilization of a loss functional, as we now discuss.


%\subsubsection{Loss Functional}
%The general procedure for training a neural SDE exploits the construction of a loss functional $\CL : \Delta(\CX) \times \Delta(\CX) \to \reals$, which compares two infinite-dimensional path-space distributions (denoted by the simplex notation $\Delta(\CX)$). This is utilized by allowing one path-space distribution to take the empirical distribution of the observational data which we aim to approximate, and the other to be the path-spae distribution produde by our neural SDE \eqref{eq:nsde}. Then, the neural SDE may be optimized w.r.t. $\theta$ to minimize this loss functional, in order to approximate the data statistics well. 

\subsubsection{Loss Functional}
The procedure for training a neural SDE exploits the construction of a loss functional $\CL : \CX \times \CX \to \reals$, which compares two paths on $\CX$. Then, $\CP(\theta) \approx \Xd$ can be identified with minimizing $\CE_{\Xt\sim \CP(\theta), X' \sim \Xd}[\CL(X,X')]$. 
In this work we assume a fixed empirical data distribution $\Xd$, without loss of generality. Then for ease of notation we represent the training objective through a \textit{data-dependent} loss functional \[\CLd = \CE_{X\sim \Xd}[\CL(\cdot,X)]: \CX \to \reals\] which quantifies the \textit{distance} between neural SDE paths $\Xt\in \CX$ and the specified data distribution $\Xd$, in some well-defined sense. In terms of the learning framework, we may then identify the relation $\CP(\theta) \approx \Xd$ as occurring when $\CE[\CLd(\Xt)]$ is small. Thus, the training objective for a neural SDE, given empirical data distribution $\Xd$, is to compute the optimization
\begin{equation}
\label{eq:objective} \arg\min_{\theta\in\reals^{\tdim}}\CE[\CLd(\Xt)] = \CE\left[\int_0^T f(t,X^{\theta}_t)dt \right]
\end{equation}
where the equivalence to an integral form (for some appropriately defined data-dependent function $f$) is consistent with standard loss functionals. Indeed, in this work we will assume the loss functional can be represented in this way. \citet{kidger2021neural,issa2024non,zhang2024efficient,briol2019statistical,gierjatowicz2022robust,cuchiero2020generative,li2020scalable} explore the performance of neural SDE models under different constructions of (what is equivalently) $\CLd$. In contrast, in this work we investigate an efficient method for \textit{training} with respect to the general objective \eqref{eq:objective}. A representative loss functional \citep{li2020scalable} can be found in Appendix~\ref{ap:lossfun}.


\textit{Existing Training Methodology}: State-of-the-art techniques to optimizing \eqref{eq:objective} operate along three main principles. First, path-wise simulation and gradient evaluation $\frac{\del}{\del\theta}\CLd(\Xt)$. Then, Monte-Carlo simulation to approximate $\CE\left[\frac{\del}{\del \theta}\CLd(\Xt)\right] = \frac{\del}{\del \theta}\CE[\CLd(\Xt)]$ with $\CO(n^{-1/2})$ error. Third, optimization using gradient methods. Most innovation in training occurs in the first step, with the construction of loss functionals and efficient methods for SDE gradient evaluation. 
%It is typically taken for granted that the $\CO(n^{-1/2})$ Monte-Carlo approximation complexity must be incurred.

\subsubsection{Main Results: Efficient Learning using Wiener Space Cubature}
\label{sec:contrib}

%Here we present an overview of our contributions. We first provide our abstract theoretical guarantees, then embed these within a practical learning framework. 
%\subsubsection{Theoretical Guarantees}
In this paper we present an alternative approach to gradient backpropagation which circumvents the stochastic (Monte-Carlo) simulation paradigm. We extend Wiener space cubature techniques \citep{lyons2004cubature} to approximate the expected loss functional $\CE[\CLd(\Xt)]$ by a linear functional of a \textit{weighted sum of deterministic ODE solutions}. 
Specifically,

\begin{mtheorem}[Wiener Space Cubature Approximation]
\label{thm:meta}
The expected loss functional in \eqref{eq:objective} can be approximated by a convex combination of ordinary differential equation trajectories:
\begin{align}
\begin{split}
\label{eq:mto}
\CE[\CLd(\Xt)] &= \CE\left[\int_0^T f(t,X_t^{\theta})dt\right] = \int_0^T \sum_{z=1}^n \lambda_z f\left(t,\phisol\right)dt + \text{error}
\end{split}
\end{align}
Here $n$ is the number of ODEs which need to be solved and the weights satisfy $\lambda_z \geq 0, \,\,\sum_{z=1}^n\lambda_z = 1$. $\phisol$ is the solution at time $t$ of an ODE 
w.r.t. vector fields in \eqref{eq:nsde} and "cubature" path $\om_z(t), z= 1,\dots,n$. Also, under reasonable assumptions\footnote{We derive this bound for dimension $\xdim \leq 6$, see Corollary~\ref{cor:noneap}. We observe that in higher dimensions the rate decreases. In future work we would like to develop schemes to reduce the dependence on dimensionality.} we achieve the following approximation bound:
\[\text{error} = \CO\left(n^{-1}\right)\]
%where $C$ is a finite constant, $k$ is a
\end{mtheorem}

%[*edit phi dependent on same vector fields, deterministic ode driven by cubature path, right hand side has nice properties, [lambda, omega \textit{independent} of theta]]
%[*edit right hand side ... allows ODE gradient methods, remarks: beats MC, etc., remove MC2]\
Observe that Meta-Theorem 1 implies the following 
\begin{equation}
\label{eq:mineq}
\min_{\theta\in\Theta}\int_0^T \sum_{z=1}^n f(t,\phisol)dt = \min_{\theta \in\Theta}\CE[\CLd(\Xt)] + \CO\left(n^{-1}\right)
\end{equation}
Thus, instead of optimizing the objective \eqref{eq:objective}, we  optimize the left hand side of \eqref{eq:mineq}. Let us make several remarks on the advantages of this approach.

\textbf{Remarks}.  \begin{compactenum}
\item {\em Gradient Computation.} Solving the optimization in the left-hand side of \eqref{eq:mineq} may be accomplished by gradient descent on $\int_0^T \sum_{z=1}^n f(t,\phisol)dt$, enabling the use of \textit{efficient ODE gradient computation methods}. We outline this advantage further in Meta-Corollary 1 below.

\item{ \em Improved Approximation Complexity}. Oberve that \eqref{eq:mineq} has $\CO(n^{-1})$ approximation complexity, where $n$ is the number of ODEs to be solved. This is an improvement from the $\CO(n^{-1/2})$ complexity of Monte-Carlo simulation. 

\item{\em Cubature Path and Weight Construction.} The cubature paths $\om_z$ and weights $\lambda_z$, $z\in[n]$, will be constructed such they are \textit{independent of $\theta$}. Thus, we need only compute them once in a pre-processing operation, as discussed in Section~\ref{sec:hor}.

\item{ \em Sparse Path and Weight Recombination}. The key operation that allows our $\CO(n^{-1})$ complexity bound is a high-order recombination procedure introduced in \cite{litterer2012high}. This procedure  \textit{sparsifies} the cubature weights thereby reducing the number of ODEs to be solved, for a given approximation error, from exponential to polynomial. 
\end{compactenum}

%This formulation is advantageous for several distinct reasons. First, our $\CO(n^{-1})$ approximation convergence rate improves upon the Monte-Carlo approximation rate of $\CO(n^{-1/2})$. Furthermore, in our case $n$ is the number of \textit{deterministic ODEs} which need to be solved, rather than SDE sample paths in the Monte-Carlo simulation regime. Thus, we may compute gradients more efficiently, as outlined in the following.


\begin{mcorollary}[Efficient ODE Gradient Evaluations]
Given the representation \eqref{eq:mto}, we can optimize the approximate loss functional \eqref{eq:mineq} by computing gradients as
\begin{equation}
\label{eq:gradeval}
    \frac{\del}{\del\theta}\int_0^T\sum_{z=1}^n \lambda_z f(t,\phisol)dt = \sum_{z=1}^n\lambda_z \frac{\del}{\del\theta} \int_0^T f(t,\phisol)dt
\end{equation}
where $\frac{\del}{\del\theta}\int_0^Tf(t,\phisol)dt$ can be computed using \textit{ODE gradient computation}; these ODE gradient methods  \citep{chen2018neural,kidger2021hey,matsubara2021symplectic,mccallum2024efficient} are more efficient than analagous methods for SDE gradient computation, such as automatic differentiation through SDE solvers \citep{tzen2019neural} or stochastic adjoint methods \citep{kidger2021efficient}.
\end{mcorollary}

\textbf{Remark.} An important  consequence of Meta-Corollary 1 is that a  gradient descent algorithm can  be used to compute a local stationary point of the approximate loss functional $\CE[\CLd(\Xt)]$:
\[\theta_{k+1}= \theta_{k} - \eta_k\sum_{z=1}^n\lambda_z \frac{\del}{\del\theta} \int_0^T f(t,\phisol)dt, \,\quad  k=1,2,\dots\]
where the step size $\eta_k$ is chosen  as $\eta_k = 1/k$. Obtaining a local stationary point is consistent with the neural SDE literature. The key advantage of our cubature approach is that we can estimate this local stationary point \textit{more efficiently}: we solve $n$ \textit{ODE} gradient evaluations to produce an error $\CO(n^{-1})$, rather than $n$ \textit{SDE} sample path gradient evaluations to produce an error $\CO(n^{-1/2})$. 

%A key to efficient training in this regime is that we construct the cubature paths $\om_z$ and weights $\lambda_z$ to be \textit{independent of $\theta$}. This means they must only be computed once in a pre-processing phase, and can then be used in \eqref{eq:mto} to improve efficiency in every gradient estimation \eqref{eq:gradeval}. We discuss this pre-processing construction, and detail its computational complexity, in Section~\ref{sec:hor}
%\begin{mcorollary}[Improved Approximation Complexity]
%Under reasonable conditions we achieve the approximation complexity \[\CO(n^{-1})\] where $n$ is the number of ODE gradients which need to be solved. We contrast this to the Monte-Carlo guarantee of $\CO(n^{-1/2})$ or the quasi-Monte-Carlo guarantee of $\CO(\log(n)/n)$, where furthermore $n$ is the number of (more expensive) SDE pathwise gradient evaluations. 
%\end{mcorollary}


%\textit{Pre-Processing Necessity}: These computational gains arise from carefully constructing ODE paths which approximate the statistical structure of the underlying SDE Brownian motion, and applying a high-order recombination technique \citep{litterer2012high} to reduce the number of necessary paths. Thus, the tradeoff of achieving these efficiencies is the necessity of a \textit{pre-processing} Cubature path construction. Details of the complexity of this pre-processing procedure are given in Section~\ref{sec:hor}.



%In the following section we introduce the framework of Wiener space cubature, and develop a generaized cubature scheme which efficiently approximated the loss functional statistic $\CE_{\CP(\theta)}[\CLd(X)]$,


%\textit{Existing Training Methodology}: State-of-the-art techniques to optimizing \eqref{eq:objective} operate along three main principles. First, path-wise simulation and gradient evaluation $\frac{\del}{\del\theta}\CLd(\Xt)$. Then, Monte-Carlo simulation to approximate $\CE\left[\frac{\del}{\del \theta}\CLd(\Xt)\right] = \frac{\del}{\del \theta}\CE[\CLd(\Xt)]$. Third, optimization using gradient methods. Most of the innovation in neural SDE training occurs in the first step, with the development of clever loss functionals and efficient methods for SDE gradient evaluation. It is typically taken for granted that the $\CO(n^{-1/2})$ Monte-Carlo approximation complexity must be incurred.

%Our proposed methodology will circumvent the stochastic Monte-Carlo evaluation of the above methodology, by replacing the expected functional \eqref{eq:objective} by a weighted sum of deterministic ODE solutions. As demonstrated in the following section, this will allow us to both improve upon the $\CO(n^{-1/2})$ Monte-Carlo approximation rate, and exploit efficient methods for ODE gradient compputation,



%Existing approaches to neural SDE training \cite{tzen2019neural}, \cite{kidger2021efficient} compute path-wise gradients using automatic differentiation or stochastic adjoint methods, then exploit Monte-Carlo simulation to approximate 
%\[\frac{\del}{\del\theta}\CE_{X\sim \CP(\theta)}[\CLd(X)]\]

%Theorem~\ref{thm:meta} allows us to improve upon this methodology in two distinct directions:


\textit{Organization}: Section~\ref{sec:nsdecub} outlines the Wiener space cubature methodology and provides our first result, Theorem~\ref{thm:cubap}, which extends the cubature techniques to approximate \textit{functionals} on infinite-dimensional pathspace. Section~\ref{sec:hor} introduces a high-order recombination procedure to drastically reduce the number of necessary computations to achieve a desirable cubature approximation. It contains our second main result, Theorem~\ref{thm:bd2}, which provides the precise guarantees that were presented in Meta-Theorem~\ref{thm:meta}. Section~\ref{sec:train} places the previous contributions in the framework of the overall neural SDE training procedure. \textit{We note that all proofs can be found in the appendix}. 


\section{Neural SDE Wiener Space Cubature}
\label{sec:nsdecub}
In this section we first introduce the existing methodology of Wiener space cubature approximations. We then extend these techniques to handle loss \textit{functionals}, enabling application to neural SDE training. The main result of this section is Theorem~\ref{thm:cubap}, which provides explicit error bounds between the expected loss functional and our constructed Wiener space cubature approximation.
%which will be the key mathematical technique in our investigation. Existing Wiener space cubature methods \citep{lyons2004cubature} are algorithmic techniques to approximate the statistics of an SDE at a fixed time, by a weighted combination of solutions of a finite set of \textit{deterministic ODEs}. 

%We will first show that, under reasonable conditions, we can extend the Wiener space cubature method to approximate the \textit{expected loss functional} $\CE[\CLd(\Xt)]$ by a linear functional depending on a weighted combination of solutions of \textit{deterministic ODEs}. Thus, we may compute gradients with respect to the individual ODE solutions, which can be done \textit{efficiently}. 


\subsection{Wiener Space Cubature Formulation}
\label{sec:cubform}

In this section we first introduce the Stratonovich SDE reformulation of the 
 neural SDE \eqref{eq:nsde}. We then introduce the fixed-time Wiener space cubature formulation of \citet{lyons2004cubature}, which utilizes this Stratonovich form. 

\subsubsection{Stratonovich Reformulation}
For notational convenience and consistency with the cubature literature, we absorb the time-dependency in \eqref{eq:nsde} into the state space. This allows us to express vector fields in terms of only the augmented state vector $\hat{X}_t$. First, recall neural SDE \eqref{eq:nsde} in It\^o form.

The time-dependence of vector fields $\mu^{\theta}(t,\cdot), \sigma^{\theta}(t,\cdot)$ can be absorbed into the state-space by constructing the following equivalent augmented system:  Let $\hat{X}_{t,[i:j]}$ denote the $i$th to $j$th elements of augmented vector $\hat{X}_t$, and $\hat{X}_{t,[i]}$ denote the $i$'th element. For variable $\hat{X}_t\in \reals^{\xdim+1}$, with $\hat{X}_{t,[0]} = t$, let
\begin{align}
\begin{split}
\label{eq:tindsde}
    \hat{X}_{0,[1:\xdim]} &= \zeta^{\theta}(\vvar), \quad d\hat{X}_t = \hat{\mu}^{\theta}(\hat{X}_t)dt + \hat{\sigma}^{\theta}(\hat{X}_t)dB_t, \quad
    \hat{X}_{0,[0]} = 0,\quad  d\hat{X}_{t,[0]} = dt \\
    &\zeta^{\theta} :\reals^{\adim} \to \reals^{\xdim}, \quad \hat{\mu}^{\theta}: \reals^{\xdim+1}\to \reals^{\xdim+1}, \quad \hat{\sigma}^{\theta}: \reals^{\xdim+1} \to \reals^{\xdim+1\times \bdim}
\end{split}
\end{align}
%It is straightforward to construct the vector fields $\hat{\mu}^{\theta}$ and $\hat{\sigma}^{\theta}$ such that the process \eqref{eq:tindsde} is equivalent to \eqref{eq:nsde}. 

Next, let $\{V_i^{\theta}\}_{i=0}^{\bdim}$ be the vector fields in the Stratonovich formulation of the augmented neural SDE \eqref{eq:nsde}, with $X_t^{\theta} \in \reals^{\xdim+1}$. Specifically,
\[\,V_0^{\theta}(x) = \hat{\mu}^{\theta}(x) - \frac{1}{2}\sum_{i=1}^{\bdim} dV_i^{\theta}(x)\cdot V_i^{\theta}(x),\, \quad \,V_i^{\theta}(x) = \hat{\sigma}_i^{\theta}(x), \,\,i=1,\dots,\bdim\]
Then, 
\begin{align}
\begin{split}
\label{eq:strat}
    X_t^{\theta} &= \int_0^t V_0^{\theta}(X_s^{\theta})ds + \int_0^t\sum_{i=1}^{\bdim} V_i^{\theta}(X_s^{\theta}) \circ dB_s^i = \int_0^t\sum_{i=0}^{\bdim} V_i^{\theta}(X_s^{\theta}) \circ dB_s^i ,\quad 
    X_{t,[0]}^{\theta} = t
\end{split}
\end{align}
with $\circ\,dB_s^i$ denoting Stratonovich stochastic integration, and  by convention $\circ\, dB_t^0 = dt$. 
The initial value in the  Stratonovich formulation is chosen identical to  \eqref{eq:nsde}; therefore, the process described by 
 \eqref{eq:strat} is equivalent to the original process \eqref{eq:nsde}.

\subsubsection{Wiener Space Cubature}
Given the Stratonovich SDE formulation \eqref{eq:strat}, the following is the main Wiener-space cubature result of \cite{lyons2004cubature}

\begin{theorem}[\cite{lyons2004cubature}]
\label{thm:lyons}
Denote by $C_{bv}^0([0,T],\reals^{d})$ the space of $\reals^{d}$-valued continuous functions of bounded variation defined on $[0,T]$. Let $(X_t^{\theta})_{t\in[0,T]}$ be the solution of a SDE in Stratonovich form \eqref{eq:strat}, with vector fields $\{V_i^{\theta}\}_{i=0}^{\bdim}$. There exist, for any sufficiently smooth $f: \reals^{\xdim+1} \to \reals$, paths and weights 
\begin{equation}
\label{eq:pathwt}
\om_1,\dots,\om_{\cn} \in C_{bv}^0([0,T],\reals^{\bdim+1}), \quad \lambda_1,\dots,\lambda_{\cn} > 0: \,\sum_{i=1}^{\cn}\lambda_i = 1
\end{equation} such that for $t > 0$ , 
\begin{equation}
\label{eq:w_cube}
\CE[f(X_t^{\theta})] = \sum_{j=1}^{\cn}\lambda_j f(\phi_j^{\theta}(t)) + \CO(t^{\frac{m+1}{2}})
\end{equation}
Here $m$ is the cubature "order", and $\phi_j^{\theta}(t)$ is the solution at time $t$ of the following \textit{ordinary} differential equation with respect to $\om_j$:
\begin{align}
\begin{split}
\label{eq:ODE}
&d\phi_j^{\theta}(t)= \sum_{i=0}^{\bdim}
V_i^{\theta}(\phi_j^{\theta}(t))d\om_j^i(t), \quad \phi_0^{\theta} = X_0 = \zeta^{\theta}(\vvar)
\end{split}
\end{align}

\end{theorem}
%\textit{Contributions}: We extend the guarantees of such techniques from the approximation of a \textit{fixed-time} approximation \eqref{eq:w_cube} to an approximation of the loss \textit{functional} $\CE[\CLd(\Xt)]$. This allows us to train our neural \textit{SDE} by differentiating through these \textit{ODE}s using efficient ODE adjoint techniques. Furthermore, we exploit a careful time-discretization and high-order recombination in order to improve the approximation bound such that it can become more efficient than standard Monte-Carlo techniques. 
%$\{\om_j\}_{j\in[r]}$ and $\{\lambda_j\}_{j\in[r]}$ are chosen carefully to approximate the statistics of the underlying Brownian motion in $X_t$. $m$ is the cubature \textit{order} which depends on the construction of paths $\{\om_j\}_{j\in[r]}$, and $r$ depends on the regularity of $f$.




\subsubsection{Cubature Path and Weight Construction}
\label{sec:cpwc}
Theorem~\ref{thm:lyons} provides the existence of cubature paths and weights defining an approximation of the form \eqref{eq:w_cube}. Furthermore,  \citet{lyons2004cubature} augment Theorem~\ref{thm:lyons} with quantitative conditions on the paths and weights which form the approximation \eqref{eq:w_cube}.
In particular, explicit paths and weights \eqref{eq:pathwt} are constructed to approximate the statistics of the driving Brownian motion in \eqref{eq:nsde}. The following definition, although abstract and not explained here, can be viewed as a stochastic Taylor expansion based on 
 rough path theory, see for example \citet{lyons2004cubature}.
\begin{definition}[Cubature Path and Weights]
\label{def:cpw}
Denote
\begin{equation}
\label{eq:CAm}
\A = \bigcup_{k=0}^{\infty} \{0,\dots,\bdim\}^k,\,\, \CA_m = \{(i_1,\dots,i_k) \in \A\backslash\{\emptyset,0\}: k + \text{card}\{j: i_j=0\} \leq m\}
\end{equation}
where $\text{card}\{S\}$ returns the cardinality of the set $S$. The paths $\om_1,\dots,\om_{\cn} \in C_{bv}^0([0,T],\reals^{\bdim})$ and weights $\lambda_1,\dots,\lambda_{\cn} > 0$
define a cubature formula on Wiener space of degree $m$ at time $T$ if for all $(i_1,\dots,i_k) \in \CA_m$,
\[\CE\left(\int_{0 < t_1 < \dots < t_k = T}\circ dB_{t_1}^{i_1}\dots\circ dB_{t_k}^{i_k} \right) = \sum_{j=1}^{\cn} \lambda_j \int_{0 < t_1 < \dots < t_k = T} d\om_j^{i_1}(t_1)\dots d\om_j^{i_k}(t_k)\]
\end{definition}


The paths and weights necessary to achieve the approximation \eqref{eq:w_cube} are precisely those that define a cubature formula on Wiener space of degree $m$ at time $t$. These will be used in our cubature path construction in Section~\ref{sec:cpc}.

The contribution of \citet{lyons2004cubature} is to provide not only existence of such cubature paths and weights, but explicit constructions of them for certain orders $m$. Lemma~\ref{lem:lv4}, together with Definition~\ref{def:cpw} provide the sufficient condition for Theorem~\ref{thm:lyons}.
\begin{lemma}[\citep{lyons2004cubature} Theorem 2.4]
\label{lem:lv4}
    Let $m$ be a natural number. Then there exist $\cn$ paths $\om_1,\dots,\om_{\cn} \in C_{bv}^0([0,T],\reals^{\bdim+1})$ and $\cn$ positive weights $\lambda_1,\dots,\lambda_{\cn}$ with $\cn \leq \text{card}\{\CA_m\}$, defining a cubature formula on Wiener space of degree $m$ at time $T$.
\end{lemma}

Lemma~\ref{lem:lv4} can be viewed as an extension of Tchakaloff's theorem \cite{bayer2006proof}. Furthermore, explicit constructions of these paths and weights for orders $m=3$ and $m=5$ are given in Section 5 of \citet{lyons2004cubature}; we omit these for brevity. 


\subsection{Wiener Space Cubature for Loss Functional}

In Section~\ref{sec:cubform} we introduced the structure of cubature formulations on Wiener space, to approximate the expected value of functions of solutions of SDEs at \textit{single time-instances}. Recall that the training procedure for neural SDEs involved minimization of a data-dependent loss \textit{functional}, which acts on entire paths rather than solutions at a single time instance. In this section we introduce an algorithmic procedure for extending the approximation capabilities of cubature methods to the path functional domain. Put simply, we extend Wiener space cubature to a function space. 

\subsubsection{Preliminaries: Assumptions and Structural Definitions}
\label{sec:prelim} 
Recall we denote $(X_t^{\theta})_{t\in[0,T]}$ the unique strong solution of the neural SDE \eqref{eq:nsde}. We are interested in forming cubature approximations of the expected path loss $\CE[\CLd(\Xt)]$ with respect to loss functional $\CLd$.
We now introduce several assumptions  which will be used in the cubature path construction of the subsequent section.


\begin{myassumptions}
\item[\nl{as:vecbd}{A1}]
The vector fields $\zeta^{\theta}, \mu^{\theta}, \sigma^{\theta}$ in the It\^o neural SDE formulation \eqref{eq:nsde} are Lipschitz-continuous, and $\CE_{\vvar}[\zeta^{\theta}(\vvar)^2] < \infty$. Furthermore, assume there exists a constant $M < \infty$ such that  
\[\sup_{t\in[0,T],x\in\reals^{\xdim},\theta\in\Theta}\left\{\|\mu^{\theta}(t,x)\| + \|\frac{\del}{\del x}\mu^{\theta}(t,x)\| + \|\sigma^{\theta}(t,x)\| + \|\frac{\del}{\del x}\sigma^{\theta}(t,x)\|\right\} \leq M\]
i.e., the  vector fields are uniformly bounded in magnitude and in first-order spatial differentiation. 

\item[\nl{as:intfun}{A2}] 
The loss functional $\CLd(X)$ can be expressed in integral form. Specifically, there exists a time-varying function $\lf: \reals^{\xdim} \times [0,T] \to \reals$ such that
\begin{equation}
\label{eq:intexp}
    \CLd(X) = \int_0^T \lf(t,X_t)dt
\end{equation} 
Furthermore, assume $\lf$ is differentiable with bounded derivatives w.r.t. $t$, twice differentiable w.r.t. $X_t$, and \\$\sup_{t\in[0,T]}\CE_{X\sim \CP(\theta)}[f(t,X_t)] < \infty,
\,\,\sup_{t\in[0,T]}\|\nabla \lf(t,\cdot)\|_{\infty} := \lfgsup < \infty$. 

\item[\nl{as:uh}{A3}] 
 Recall $\CA_m$ in \eqref{eq:CAm}, and inductively define a family of Lie-bracketed vector fields as 
 \[V_{[\emptyset]}^{\theta} = 0, \quad V_{[i]}^{\theta} = V_i^{\theta}, \quad V_{[\alpha*i]}^{\theta} = [V_{[\alpha]}^{\theta},V_i^{\theta}], \quad 0\leq i\leq \bdim, \,\,\,\alpha\in\A \]
where $[V_i,V_j] = J_{V_j}V_i - J_{V_i}V_j$ is the Lie bracket expansion and $J_V$ is the Jacobian matrix of vector field $V$.
Then, we assume the vector fields $\{V_i^{\theta}\}_{i\in[\bdim+1]}$ satisfy the uniform H\"ormander's condition: there exists an integer $\ps$ such that 
\begin{equation}
\inf\left\{\sum_{\alpha\in A_1(\ps)}\langle V_{[\alpha]}^{\theta}(x),\zeta \rangle^2; \, x,\zeta \in \reals^{\xdim}, |\zeta| = 1, \, \theta\in\Theta \right\} := M > 0
\end{equation}

%\item[\nl{as:vfbd}{A4}] The vector fields $\{V_i^{\theta}\}_{i\in[r]}$ are uniformly bounded by some $\vfbd > 0$: \[\sup_{i\in[r],x\in\reals^{\xdim},\theta\in\Theta}\|V_i^{\theta}(x)\| \leq \vfbd\]
\end{myassumptions}

\textit{Discussion of Assumptions}: The continuity and growth assumptions in \ref{as:vecbd} are the standard conditions guaranteeing unique strong solutions to \eqref{eq:nsde} \citep{kloeden1992stochastic}. The uniform boundedness in \ref{as:vecbd} is assumed in \cite{litterer2012high}, and can easily be satisfied in practice; with arbitrarily high probability the process $X_t^{\theta}$ will be confined to some compact subset of the domain $\reals^{\xdim}$ on the finite time interval $[0,T]$. Thus, in computation we may effectively consider the behavior of vector fields $\mu^{\theta}$ and $\sigma^{\theta}$ within compact sets, in which case the boundedness immediately follows. \ref{as:intfun} admits many standard neural SDE loss functionals, for instance the latent SDE formulation \citep{li2020scalable}. \ref{as:uh} is known as the uniform H\"ormander condition, and was introduced in \citet{kusuoka1987applications}. It is widely assumed in formal analyses of SDEs. \ref{as:uh} ensures the smoothness of the solution of SDEs; see Section 5 of \citet{elst2009uniform} for a detailed treatment. 



\subsubsection{Cubature Path Construction and Error}
\label{sec:cpc}

The purpose of this subsection is twofold: we first provide an explicit construction of ODEs with respect to paths in $\CX$, which exploits cubature paths and weights in Definition~\ref{def:cpw}. Second, we analyze this construction to show that it approximates the loss functional statistic $\CE[\CLd(\Xt)]$ sufficiently well. Theorem~\ref{thm:cubap} is our first main result, and provides a guarantee on this approximation.

The procedure for approximating a loss functional of the form \eqref{eq:intexp} is as follows:
\begin{compactenum}%[label=\roman*)]
\item For $\cn,m > 0$, take cubature paths $\{\om_j\}_{j\in[\cn]}$ and weights $\{\lambda_j\}_{j\in[\cn]}$ which define a cubature formula on Wiener space of degree $m$ at time $1$, as in Definition~\ref{def:cpw}. Explicit constructions for such paths and weights are given in \citet{lyons2004cubature} for orders $m=3$ and $m=5$.

\item Divide the interval $[0,T]$ as 
\begin{equation}
\label{eq:discint}
0 = t_0 < t_1 < \dots < t_k = T,
\end{equation}
and let $s_i = t_i - t_{i-1}, \, i= 1,\dots, k$. 

\item Construct the exhaustive set of vectors $\{I_z \in \nat^k\}_{z\in[\cn^k]}$ with each element taking a natural number $1,\dots,\cn$ and index beginning at 0. 
Construct scaled paths $\{\om_{s,j}\}_{j\in[\cn]}$ as 
\begin{equation}
\om_{s,j}: [0,T] \to \reals^{\bdim+1},\,\, \om_{s,j}^0(t) = t,\,\, \om_{s,j}^i(t) = \sqrt{s}\om_{j}^i(t/s), \text{ for } i=1,\dots, \bdim
\end{equation}
Then, construct paths $\om_z: [0,T] \to \reals^{\bdim+1}, z\in[\cn^k]$ as 
\begin{equation}
\label{eq:pathcon}
    \om_z(t) = \sum_{j=1}^{\cn} \sum_{i=0}^k\bo(t\in[t_{i},t_{i+1})\bo(I_z[i] = j)\om_{s_{i+1},j}(t)
\end{equation}
%[*edit See Fig below Ex 3.4 in \cite{lyons2004cubature}: \eqref{eq:pathcon} constructs the exhaustive combinatorial set of $r^k$ paths.]

\item Let $\phisol$ be the solution of the ODE 
\begin{equation}
\label{eq:ode}
d\phisol = \sum_{i=0}^{\bdim} V_i^{\theta}(\phisol)d\om_z^i(t),\quad \phi_0^{\theta} = X_0 = \zeta^{\theta}(\vvar)
\end{equation}
%\[\frac{d}{d t}\phi_z^{\theta} = \langle V
%^{\theta}(\phi_z^{\theta}), \,\frac{d}{dt}{\om}_z \rangle,\quad \phi_0^{\theta} = X_0 = \zeta^{\theta}(\vvar)\]
at time $t$, and where the initial point $\zeta^{\theta}(\vvar)$ is specified in \eqref{eq:nsde}. 

\end{compactenum}

%\subsubsection{Cubature Path Approximation Error}
Then, letting $\lambda_z' := \lambda_{I_z[0]}\dots\lambda_{I_z[k]}$, the integral
\begin{align}
\label{eq:sum}
    \int_0^T \Phi_{\lf}^{\theta}(t)dt, \text{ with } \Phi_{\lf}^{\theta}(t) := \sum_{z\in [\cn^k]}\lambda_z' \lf(t,\phisol)
\end{align}
forms an approximation of the loss functional statistic $\CE[\CLd(\Xt)]$, with approximation error given by the following theorem. This is the first main result of the paper.

\begin{theorem}[Cubature Path Approximation Error] 
\label{thm:cubap}
Let $t_i = T\left(1 - \left(1-\frac{i}{k}\right)^{\gamma}\right)$ and $0 < \gamma < m-1$. Recall $m$ is the cubature order in Definition~\ref{def:cpw}. Under \ref{as:vecbd}, \ref{as:intfun} and \ref{as:uh} we have: 
\begin{align}
\begin{split}
\label{eq:cpae}
&\left|\CE[\CLd(\Xt)] - \int_0^T\Phi_{\lf}^{\theta}(t)dt\right|\leq 3\,T^{3/2}\,K \,\lfgsup\, C(m,\gamma) k^{-\gamma/2}
\end{split}
\end{align}
where $K$ is a constant independent of $T$, $C(m,\gamma)$ is a constant depending only on $m$ and $\gamma$, and $\lfgsup := \sup_{t\in[0,T]}\|\nabla \lf(t,\cdot)\|_{\infty} < \infty$.
\end{theorem}
%%%
%\begin{proof}
%See Appendix~\ref{sec:pf3}.
%\end{proof}

Thus, under suitable  time discretization, we can  approximate the loss functional statistic $\CE[\CLd(\Xt)]$ \textit{arbitrarily well}, taking $k$ large enough, by solving the \textit{deterministic} system of ODEs \eqref{eq:sum}. \footnote{Recall that we require the inequality
$0 < \gamma < m-1$
to hold. In practice, we need an explicit construction of cubature paths and weights of order $m$, as in Definition~\ref{def:cpw}. Thus, currently we are limited to several specific choices for $m$ (for instance, \citet{lyons2004cubature} provide explicit constructions for $m=3,5$). However, there is active research in providing explicit constructions of Wiener space cubature paths and weights of higher order; for instance \citet{ferrucci2024high} constructs order $m=7$ Wiener space cubature paths using Hopf algebras. \textit{Such advancements to higher-order $m$ cubature approximations will allow our bound \eqref{eq:cpae} to become tighter by allowing $\gamma$ to increase.}}

\textit{Exponential Complexity}: However, the price of achieving the approximation error \eqref{eq:cpae} is that the number of ODEs \eqref{eq:sum} to be solved grows \textit{exponentially} ($\cn^k$) with the discretization \eqref{eq:discint} resolution $k$. In the following section we employ the high-order recombination method of \citet{litterer2012high} to \textit{drastically reduce the number of ODEs to be solved}, while \textit{maintaining desirable approximation error}. We do so in a constructive way that results in an overall improvement in computational efficiency as compared to Monte-Carlo stochastic simulation. 


\section{High-Order Recombination for Efficient Neural SDE Cubature Approximations}
\label{sec:hor}

%Previously we presented a method for constructing deterministic Cubature paths which can provide effective approximations of the loss functional statistic $\CE_{\CP(\theta)}[\CLd(X)]$ of interest. However, in this construction the number of ODEs which must be evaluated grows \textit{exponentially} as we decrease the approximation error. 
This section establishes the second main result of the paper, namely Algorithm~\ref{alg:wscr} and Theorem~\ref{thm:bd2}. 
In particular, we show how to exploit the high-order recombination technique of \citet{litterer2012high} in order to drastically reduce the number of ODE evaluations required for a given approximation error. The idea is to partition the space of paths and re-weight within these partitions, such that the center of mass is preserved and the number of paths in the partition is reduced. First we introduce some preliminary definitions which will be used in the main recombination algorithm.

\begin{definition}[Localization \citep{litterer2012high}] Consider a discrete probability measure $\mu$ on $\reals^N$ and a collection $(U_j)_{j=1}^l$ of balls of radius $u$ on $\reals^N$ which covers the support of $\mu$. Then, construct a collection of positive measures $\mu_j, 1\leq j \leq l$ such that $\mu_i \perp \mu_j\,\, \forall i\neq j$ (disjoint supports), $\mu = \sum_{i=1}^l \mu_i$, and $\text{supp}(\mu_j) \subseteq U_j\cap\text{supp}(\mu)$. Such a collection $(U_j,\mu_j)_{j=1}^l$ is called a localization of $\mu$ to the cover $(U_j)_{j=1}^l$, with radius $u$. 
\end{definition}

\begin{definition}[Measure Reduction \citep{litterer2012high}]
\label{def:redmeas}
Let $P_r$ denote a finite set of test functions $\{p_1,\dots,p_r\}$ on a measure space $(\Omega, \mu)$ with $\mu$ a finite discrete measure:
\begin{equation}
\label{eq:mrd}
    \mu = \sum_{i=1}^{\hn}\lambda_i\delta_{z_i},\,\, \lambda_i > 0, z_i \in\Omega
\end{equation}
A probability measure $\tilde{\mu}$ is a reduced measure w.r.t. $\mu$ and $P_r$ if $\text{supp}(\tilde{\mu}) \subseteq \text{supp}(\mu)$, $\int p(x) \tilde{\mu}(dx) = \int p(x) \mu(dx)\,\, \forall p\in P_r$, $\text{card}(\text{supp}(\tilde{\mu})) \leq r+1$.

\end{definition}



\begin{definition}[Reduced Measure w.r.t. Localization \citep{litterer2012high}]
A measure $\tilde{\mu}$ is a reduced measure with respect to the localization $(U_j,\mu_j)_{j=1}^l$ and a finite set of integrable test functions $P$ if there exists a localization $(U_j,\tilde{\mu}_j)_{j=1}^l$ of $\tilde{\mu}$ such that for $1 \leq j \leq l$ the measures $\tilde{\mu}_j$ are reduced measures (Def.~\ref{def:redmeas}) w.r.t. $\mu_j$ and $P$. 
\end{definition}

Computing a reduced measure with respect to a localization is a key component of our pre-processing procedure (Algorithm~\ref{alg:wscr}). It allows one to transform a finite measure into another with \textit{drastically reduced support} which approximates the statistics of the original. \citet{litterer2012high} presents an algorithmic procedure for computing such reduced measures. We refer to this operation which takes a measure $\mu$ \eqref{eq:mrd} and outputs a reduced measure with respect to a localization $(U_j,\mu_j)_{j=1}^l$, with $\text{card}\{\text{supp}(\tilde{\mu})\} = l(r+1)$ as RM Procedure (RMP). For our purposes we may consider only this abstract procedure; for brevity we provide all details of RMP in Appendix~\ref{ap:mrd}. 

\begin{definition}[Reduced Measure Procedure (RMP)]
A Reduced Measure Procedure (RMP) takes as input a discrete measure $\mu$ \eqref{eq:mrd} and a localization $(U_j,\mu_j)_{j=1}^l$ of $\mu$ with radius $u$ and w.r.t. test functions $P_r$. It outputs a reduced measure $\tilde{\mu}$ w.r.t. these specifications. We write
$\tilde{\mu} = \RMP(\mu,(U_j,\mu_j)_{j=1}^l,P_r)$
\end{definition}


Now we introduce a recombination algorithm which exploits RMP to drastically reduce the number of ODEs which need to be solved within the cubature approximation.

\begin{algorithm}[h]
\begin{algorithmic}
%\SetAlgoLine
\State Initialize time partition $\CD = 0 = t_0 < t_1 < \dots < t_k = T$ of $[0,T]$, and let $s_i = t_i - t_{i-1}$.
\State Take $r>0$; Initialize $P_r$ as a basis for the
\textit{space of polynomials} on $\reals^{\xdim}$ with degree at most $r$. 
\State Initialize paths $\om_1,\dots,\om_{\cn} \in C_{bv}^0([0,T],\reals^{\bdim})$ and weights $\lambda_1,\dots,\lambda_{\cn} > 0$ defining a cubature formula on Wiener space of degree $m > 0$, as in Section~\ref{sec:cpwc}. 
\State For discrete measure $\mu = \sum_{j=1}^l \mu_j \delta_{x_j}$ on $\reals^{\xdim}$, let $\KLV(\mu,s) := \sum_{j=1}^l \sum_{i=1}^
{\cn}\mu_j \lambda_i \delta_{x_j + w_{s,i}(s)}$
\State Initialize point mass $x$ at the origin of $\reals^{\xdim}$
 $\tQ_{\CD,u}^{(1)}(x) = \KLV(\delta_x, s_1)$ 
%Construct $r$ linear paths $\om_{i}^{1}$, $i\in[r]$ with left endpoint $x$ and right endpoints the points in $\tQ_{\CD,u}^{(1)}(x)$. \;\\
\For{$i=2:k-1$}
\State $\Q_{\CD,u}^{(i)}(x) = \KLV(\tQ_{\CD,u}^{(i-1)}(x), s_i)$; take any localization $(U_j,\mu_j)_{j=1}^{l_i}$ of $\Q_{\CD,u}^{(i)}(x)$ with radius $u_i$.
 \State  $\tQ_{\CD,u}^{(i)}(x) = \RMP(\Q_{\CD,u}^{(i)}(x), (U_j,\mu_j)_{j=1}^{l_i},P_r)$. \;
\EndFor
   \State $\tQ_{\CD,u}^{(k)}(x) = \KLV(\tQ_{\CD,u}^{(k-1)}(x), s_k)$\; \\
\For{$i=1:k$}
\State Express $\,\tQ_{\CD,u}^{(i)}(x) = \sum_{j=1}^{l_i(r+1)}\alpha_{\tilde{x}_j}\delta_{\tilde{x}_j}$
 \For{$j=1:l_i(r+1)$}
       \State Take $z\in[\cn^k]$ s.t. $\om_z(t_i) = \tilde{x}_j$ (exists by construction). Denote $\tilde{\lambda}_{I_z[i]} := \alpha_{\tilde{x}_j}$.
    \EndFor
    \State For the remaining $z \in[\cn^k]$ s.t. there is no $\tilde{x}_j \in \text{supp}(\tQ_{\CD,u}^{(i)}(x))$ with $\tilde{x}_j = \om_z(t_i)$, set $\tilde{\lambda}_{I_z[i]}=0$
\EndFor
\State Return weights $\{\tilde{\lambda}_{I_z[i]}, z\in[r^k],i\in[k]\}$
\caption{Pre-Processing for Wiener Space Cubature Recombination}\label{alg:wscr}
\end{algorithmic}
\end{algorithm}
%\textit{Construction of Recombined Cubature Paths}: Recall the set of paths $\om_z(t), \, z\in[r^k]$ \eqref{eq:pathcon}, and observe that for each $i\in[k]$ and each point $\tilde{x} \in \tilde{Q}_{\CD,u}^{(i)}(x)$ there exists a unique $z\in[r^k]$ such that $\om_z(t_i) = \tilde{x}$. Also this $z$ corresponds to a unique vector $I_z$. Thus, denote $\tlam_{I_z[i]}$ the weight associated to each point $\tilde{x} \in  \tilde{Q}_{\CD,u}^{(i)}(x)$, for the associated unique $z\in[r^k]$ and discrete-index $i$. For the remaining pairs $z,i$ with no point $\tilde{x} \in \tilde{Q}_{\CD,u}^{(i)}(x)$ s.t. $\om_z(t_i) = \tilde{x}$, set $\tlam_{I_z[i]} = 0$.



%In this section we presented a high-order recombination algorithm which will be used to drastically reduce the number of necessary ODE computations necessary to obtain a desirable cubature approximation. The resultant approximation error bounds and computational efficiency improvement are detailed in the following section. 

%It is important to realize that much of our computational gain results from this high-order recombination procedure, which must only be computed \textit{once as a pre-processing step}. In particular, after this pre-processing step has been completed, the training procedure of iterative gradient estimation and optimization may be implemented without interruption. We provide a bound on the complexity of Algorithm~\ref{alg:recom} operation in Corollary~\ref{cor:preproc}.



%\subsection{Wiener Space Recombination for Efficient Loss Functional Estimator}


Algorithm~\ref{alg:wscr} is a \textit{pre-processing} procedure which must only be completed once before training the neural SDE \eqref{eq:nsde}. Specifically, once the weights $\tilde{\lambda}_{I_z[i]}$ are recovered, we may construct for \textit{any $\theta \in \Theta$} the "recombined" cubature approximation
\begin{equation}
\label{eq:rcps}
    \int_0^T \tPhi_{\lf}^{\theta}(t)dt := \int_0^T \sum_{z\in [\cn^k]}\lf(t,\phisol)\tlam_{I_z[0]}\dots\tlam_{I_z[k]}dt
\end{equation} where, recall $\phisol$ is the solution of the ODE \eqref{eq:ode}. That is, we solve ODEs along weighted cubature paths \eqref{eq:pathcon} with weights constructed as above from the recombination procedure. \textit{By the recombination procedure, \textit{most} of the sequences of weights $\tlam_{I_z[0]}\dots\tlam_{I_z[k]}$ will equal zero, and thus ODEs will only need to be solved along a small fraction of the original paths $\om_z$.} Thus, \eqref{eq:rcps} forms a \textit{computationally efficient} approximation of $\CE[\CLd(\Xt)]$. 

The approximation error of the recombined cubature sum \eqref{eq:rcps} in terms of the number of paths is quantified in the following result, which is the second main result of this paper.

\begin{theorem}[Recombined Cubature Path Approximation Error]
\label{thm:bd2}
Let $t_i = T\left( 1-\left( 1-\frac{i}{k}\right)^{\gamma}\right)$ for $0\leq i\leq k$. Take $m,\gamma,r$ such that $r\ps \geq m \geq \gamma + 1$. Construct weights $\{\tilde{\lambda}_{I_z[i]}\}_{z\in[\cn^k]}$ from Algorithm~\ref{alg:wscr} using any localization with radius $u_i = s_i^{\ps/{2\gamma}}$. Then, under \ref{as:vecbd}, \ref{as:intfun}, \ref{as:uh},
\begin{align}
\begin{split}
\label{eq:cbound}
 \left|\CE[\CLd(\Xt)] - \int_0^T  \tPhi_{\lf}^{\theta}(t)dt \right| = \mathcal{O}\left(n^{\frac{-\gamma\left(
    r\ps/\cfac - r\ps  + 1\right)}{\xdim(\ps-2)}} \right)
\end{split}
\end{align}
where $n$ is the number of ODE computations, and $\cfac$ is such that $\gamma \leq \frac{\ps(r+1)}{(\ps r)/\cfac + 1}$. The explicit constant factor in \eqref{eq:cbound} is given in \eqref{eq:interbd}.
\end{theorem}
%%%
%\begin{proof}
%See Appendix~\ref{sec:pf4}.
%\end{proof}

%\subsubsection{Recombined Cubature Effectiveness} 

%Theorem~\ref{thm:bd2} effectively tells us that the recombined cubature integral $\int_0^T \tPhi_{\lf}^{\theta}(t)dt$ approximates the expected loss functional $\CE[\CLd(\Xt)]$ slightly worse than the original cubature integral $\int_0^T\Phi_{\lf}^{\theta}(t)dt$ (Theorem~\ref{thm:cubap}), but does so \textit{much more efficiently}. In particular, the bound is still manageable and can be controlled as \eqref{eq:cbound} in terms of the number of necessary ODE computations, which is \textit{no longer exponential in $k$}.
As a result of Theorem~\ref{thm:bd2} we may achieve an improvement from traditional Monte-Carlo methods. The specific rate will depend on the set of parameters in the exponent \eqref{eq:cbound}, but the following Corollary illustrates a choice of reasonable parameters which produces a $\CO(n^{-1})$ rate.



\begin{corollary}[Improved Estimate Efficiency]
\label{cor:noneap}
Suppose $\ps \leq 100$, $\xdim \leq 6$. Taking $\gamma = 0.6$, $m = 5, r=15$, $c=0.6$ gives us 
\begin{equation}
\label{eq:noneap}
\left|\CE[\CLd(X)] - \int_0^T  \tPhi_{\lf}^{\theta}(t)dt \right| = \CO\left(n^{-1}\right)
\end{equation}
where $n$ is the number of necessary ODE solution computations. This is contrasted to the Monte-Carlo rate of $\mathcal{O}(n^{-1/2})$ and quasi-Monte-Carlo rate of $\mathcal{O}\left(\frac{\log(n)}{n}\right)$. \citet{lyons2004cubature} provides an explicit construction of Wiener space cubature paths and weights for this order $m=5$.
\end{corollary}

Corollary~\ref{cor:noneap} demonstrates that under certain parametrizations, and in low dimensions, we may achieve a convergence rate exceeding traditional Monte-Carlo or quasi-Monte-Carlo simulation. We propose as the object of future study to investigate the dependency of this rate on the respective parameters. Related to these considerations will be a tradeoff between approximation efficiency \eqref{eq:noneap} and the complexity of the pre-processing operation (Algorithm~\ref{alg:wscr}). The following Corollary quantifies the complexity bound for this \textit{pre-processing} operation Algorithm~\ref{alg:wscr}\footnote{this is a consequence of Corollary~\ref{cor:ll12} and the bound \eqref{eq:nbound}}:

\begin{corollary}[Pre-Processing Complexity]
\label{cor:preproc}
    Algorithm~\ref{alg:wscr} operates in 
    \begin{equation}
    \label{eq:precomplex}
        \CO\left(r k^{\xdim(\ps - 2)} + rk^{\xdim(\ps/2 - 1)}\log(k^{\xdim(\ps/2 - 1)}/r)C(r+2,r+1)\right)
    \end{equation}
    time. Here $C(r+2,r+1)$ is the time to solve a system of linear equations in $r+2$ variables and $r+1$ constraints.
\end{corollary}

In the context of neural SDE training, we must incur the additional complexity \eqref{eq:precomplex} to construct recombined cubature weights, but then benefit from improved efficiency, e.g. \eqref{eq:noneap}, for every iterative gradient evaluation in the optimization scheme. The following section puts our ideas together within the overall training procedure, and discusses the advantages of the cubature approximation \eqref{eq:rcps} for this \textit{gradient computation}. 
%In this section we implemented a high-order recombination technique to drastically reduce the number of necessary ODE computations to obtain a desirable approximation of $\CE[\CLd(\Xt)]$. We showed that this may allow us to achieve improved efficiency as compared to Monte-Carlo simulation. In the following section we embed this cubature approximation within the overarching neural SDE training procedure. In particular, we show how this \textit{also} gives us an advantage in training by exploiting efficient ODE adjoint methods to compute gradients. 

\section{Efficient Gradient Back-Propagation using Recombined Neural SDE Cubature}
\label{sec:train}

This section provides the logic for using the efficient cubature estimate $\int_0^T\tPhi_{\lf}^{\theta}(t)dt$ to efficiently \textit{learn} by solving the broader optimization goal: to find some vector field parametrization $\theta^*$ such that 
\begin{equation}
\label{eq:opt}
    \theta^* \in \arg\min_{\theta \in\Theta} \CE[\CLd(\Xt)]
\end{equation}

Thus far, we have shown that we may efficiently form an approximation of $\CE[\CLd(\Xt)]$ for \textit{any given} $\theta\in\Theta$ using the recombined cubature path sum \eqref{eq:rcps}. This is achieved by solving a set of \textit{ODEs} with respect to vector fields $\{V_i^{\theta}\}_{i\in[\bdim+1]}$ and well-chosen cubature paths $\om_z, z\in[\cn^k]$.  

\subsection{Computing Gradient Estimate from Recombined Cubature ODEs}

Theorem~\ref{thm:bd2} tells us that \eqref{eq:rcps} approximates $\CE[\CLd(\Xt)]$ well \textit{for any $\theta \in\Theta$}. In particular, the bound in Theorem~\ref{thm:bd2} does not depend on the vector field parametrization $\theta$ directly, but does so through the uniform H\"ormander regularity parameter $\ps$ given in \ref{as:uh}. 

Thus, we have that 
\begin{equation}
\label{eq:maprox}
\min_{\theta\in\Theta}\int_{0}^T \tPhi_{\lf}^{\theta}(t)dt = \min_{\theta\in\Theta}\CE[\CLd(\Xt)]  + \CO\left(n^{\frac{-\gamma\left(
    r\ps/\cfac - r\ps  + 1\right)}{\xdim(\ps-2)}} \right)
\end{equation}
where the error can be made arbitrarily small by decreasing the bound in \eqref{eq:cbound}, as in e.g., \eqref{eq:noneap}. Thus, we may instead train with respect to the objective: find $\theta^*$ such that 
$\theta^* \in \arg\min_{\theta\in\Theta}\int_{0}^T \tPhi_{\lf}^{\theta}(t)dt$
\subsection{Efficient Back-Propagation using ODE Adjoint Methods}

Now we aim to minimize \eqref{eq:rcps}, and thus approximately minimize $\CE[\CLd(X)]$ with approximation error \eqref{eq:maprox}. A key advantage of our framework is that this minimization can be done \textit{efficiently} using ODE gradient methods. Specifically, observe
\begin{align*}
\begin{split}
    \frac{\del}{\del\theta}\int_0^T\tPhi_{\lf}^{\theta}(t)dt &= \frac{\del}{\del\theta}\int_0^T\sum_{z\in [\cn^k]}\lf(t,\phi_z^{\theta}(t))\tilde{\lambda}_{I_z[0]}\dots\tilde{\lambda}_{I_z[k]}dt = \sum_{z\in [r^k]}\frac{\del}{\del\theta} F(T,\om_z,\tilde{\lambda}_{I_z})
\end{split}
\end{align*}
where we introduce the shorthand notation $F(T,\om_z,\tilde{\lambda}_{I_z}) := \int_0^T\lf(t,\phi_t^{\theta}(\om_z))\tilde{\lambda}_{I_z[0]}\dots\tilde{\lambda}_{I_z[k]}dt$. 
Now, the gradient 
\begin{equation}
\label{eq:Fsens}
    \frac{\del}{\del\theta}F(T,\om_z,\tilde{\lambda}_{I_z})
\end{equation}
may be computed efficiently using ODE adjoint methods. Specifically, $F(T,\om_z,\tilde{\lambda}_{I_z})$ may be expressed as the solution at time $T$ of the ODE 
\[dF(t,\om_z,\tilde{\lambda}_{I_z}) = \lf(t,\phi_z^{\theta}(t))\tilde{\lambda}_{I_z[0]}\dots\tilde{\lambda}_{I_z[k]}dt\]
where recall $\phi_z^{\theta}(t)$ is itself the solution of the ODE \eqref{eq:ode}.

There are very efficient methods to compute sensitivities of ODE solutions with respect to parameters. One class of these exploits adjoint equations for ODEs, in order to express sensitivities of the form \eqref{eq:Fsens} as the solution of a different, augmented ODE. Thus, after reformulating this new "adjoint" ODE, one need only call a black-box ODE solver to the obtain gradient \eqref{eq:Fsens}. For brevity, we refer to works \citet{chen2018neural,kidger2021hey,matsubara2021symplectic,mccallum2024efficient} for specifics of ODE adjoint methods. 

%A key property of our framework is that these ODE adjoint computations are \textit{more efficient} than analagous computations for SDEs. Thus, we reduce the computational complexity of achieving accurate gradient estimates by solving a set of well-chosen ODEs rather than the original SDE \eqref{eq:nsde}.

\section{Conclusions}
We have provided a new framework for training neural SDEs by exploiting and extending the theory of Wiener space cubature. In particular, we have provided a constructive approach to approximating an expected loss functional of a neural SDE by a (linear functional of) a weighted combination of deterministic ODE solutions. This proves advantageous for two reasons. First, we may compute gradients more efficiently by exploiting ODE gradient evaluation techniques such as the adjoint method. Second, we demonstrate that under reasonable assumptions we improve upon the Monte-Carlo complexity of $\CO(n^{-1/2})$ to achieve a $\CO(n^{-1})$ approximation guarantee. This rate is obtained by carefully choosing algorithmic parameters; in future work we propose to explore the dependence of this rate on the underlying parameters. This will lead to an adaptable framework where the computational load can be optimized for specific generative modeling settings.
% Acknowledgments---Will not appear in anonymized version
%\acks{We thank a bunch of people and funding agency.}

\bibliographystyle{abbrvnat}
\bibliography{Bibliography.bib}

\appendix


\subsection{Loss Functional Example}
\label{ap:lossfun}

Here we present an example of a representative loss functional.

 \textit{Latent SDE Loss Functional}. \citet{li2020scalable} construct an auxiliary SDE $\hat{X}$ with drift $\nu_{\phi}$ parametrized by $\phi$ and diffusion $\sigma^{\theta}$, and optimize a KL divergence of the form:
    \begin{equation}
    \label{eq:latentsde}
        \min_{\theta,\phi} \CE_{X_t \sim \Xd}\left[\int_0^T(X_{t} - l^{\theta}(\hat{X}_t))^2 + \frac{1}{2}\left\| (\sigma^{\theta}(t,\hat{X}_t))^{-1}(\mu^{\theta}(t,\hat{X}_t) - \nu_{\phi}(t,\hat{X}_t,\Xd))\right\|_2^2 dt\right]
    \end{equation}
    where $l^{\theta}$ may be constructed from vector fields $\mu^{\theta},\sigma^{\theta}$. The full construction may be found in \citet{li2020scalable} or \citet{kidger2021efficient}. Observe that \eqref{eq:latentsde} may be written in the abstract form $\min_{\theta}\CE[\CLd(\Xt)]$. Indeed, the expectation w.r.t. empirical distribution $\Xd$ may be treated as a weighted sum. Furthermore, It\'o's Lemma can be applied to write the auxiliary process $\hat{X}_t$ as a function of $X_t^{\theta}$; thus, the integrand may be expressed as a function of $X_t^{\theta}$.


\subsection{Measure Reduction Details}
\label{ap:mrd}
This appendix provides a detailed explanation of the recombination framework of Section~\ref{sec:hor}.

Consider the finite set of test functions $P_r = \{p_1,\dots,p_r\}$ on $(\Omega,\mu)$, a measure space with $\mu$ a finite discrete measure 
\[\mu = \sum_{i=1}^{\hn}\lambda_i \delta_{z_i}, \,\, \lambda_i > 0, z_i \in \Omega\]

Let $P$ be a $\reals^r$-valued random variable $P := (p_r,\dots,p_r)$ defined on $(\Omega,\mu)$. Then the law $\mu_P$ of $P$ is the discrete measure on $\reals^r$:
\[\mu_P =\sum_{i=1}^{\hn} \lambda_i\delta_{x_i},\,\,\, x_i = (p_i(z_i),\dots,p_r(z_i))^T \in \reals^r\]
and the center of mass (CoM) for measure $\mu_P$ is given as 
$CoM(\mu_P) = \sum_{i=1}^{\hn}\lambda_i x_i$.

The key insight is that to construct a \textit{reduced measure} $\tilde{\mu}_P$
w.r.t. $\mu_P$ and $P_r$ it is sufficient to find a subset $x_{i_k}$ of the points $x_i$ and positive weights $\tilde{\lambda}_{i_k}$ to produce a new probability measure $\tilde{\mu}_P = \sum \tilde{\lambda}_{i_k} \delta_{x_{i_k}}$ s.t. $CoM(\tilde{\mu}_P) = CoM(\mu_P)$. Then a reduced measure can be constructed as 
\[\tilde{\mu} = \sum \tilde{\lambda}_{i_k}\delta_{z_{i_k}}\]
with $z_{i_k} \in \text{supp}(\mu)$ satisfying $P(z_{i_k}) = x_{i_k}$. 

We now provide a concrete algorithmic procedure for computing a reduced measure with respect to a localization. We first introduce a single-step reduction iteration, as follows. 

\begin{algorithm}[H]
\begin{algorithmic}
    %\SetAlgoLined
   \State Input: $n$ points $\{x_{i}\}_{i=1}^n$ and weights $\{\lambda_i\}_{i=1}^n$
   \State Solve the system of equations: $ \sum_{i=1}^{n} u_i x_i = 0,\quad \sum_{i=1}^n u_i = 0$
\State Compute $\alpha = \min_{ u_i>0} \frac{\lambda_i}{u_i},\, \lambda_i' = \lambda_i - \alpha u_i$ \algorithmiccomment{$\lambda_i' = 0$ for some $i$}
   \State Set $i' = \{i : \lambda_i' = 0\}$ 
\While {$i \leq n-1$}
\If {$i < i'$}
 \State $\tilde{\lambda}_i = \lambda_i'$
 \Else
 \State $\tilde{\lambda}_i = \lambda_{i+1}'$, \,
 $\tilde{x}_i = x_{i+1}$\;
\EndIf
\EndWhile
\State Output weights $(\tilde{\lambda}_i)_{i=1}^{n-1}$, points $(\tilde{x}_i)_{i=1}^{n-1}$ %\algorithmiccomment{Observe $\sum_{i=1}^{n-1}\tilde{\lambda}_i\tilde{x}_i = \sum_{i=1}^{n}\lambda_i x_i$}
\caption{Reduction Iteration \citep{litterer2012high}}\label{alg:redone}
\end{algorithmic}
\end{algorithm}

Then, applying Algorithm~\ref{alg:redone} iteratively in the following Algorithm~\ref{alg:recom} allows us to obtain a reduced measure 
\begin{algorithm}[H]
\begin{algorithmic}
%\SetAlgoLined
\caption{Recombination \citep{litterer2012high}}\label{alg:recom}
\State Input: $\mu_P = \sum_{i=1}^{\hn} \lambda_i \delta_{x_i}$
\State Partition the support of $\mu_P$ into $2(r+1)$ sets $I_j, 1\leq j \leq 2(n+1)$ of equal size.
\State Compute $\nu = \sum_{i=1}^{2(r+1)}\nu_i \delta_{\hx_i}$, where 
$\hx_j = \CE_{\mu_P}(x | x\in I_j) = \sum_{x_i\in I_j} \frac{\lambda_i x_i}{\nu_j}$
and $\nu_j = \mu_P(I_j) = \sum_{i: x_i \in I_j} \lambda_i$
\State Apply Algorithm~\ref{alg:redone} $r+1$ times beginning with weights $\{\nu_i\}_{i=1}^{2(r+1)}$ and $\{\hat{x}_i\}_{i=1}^{2(r+1)}$, to produce a measure $\tilde{\nu} = \sum_{j=1}^{r+1}\tilde{\nu}_{i_j}\delta_{\tilde{x}_{i_j}}$ with CoM($\tilde{\nu}$) = CoM($\nu$). 
\State Repeat 1 - 4 with $\mu_P = \sum_{j=1}^{r+1}\sum_{x_k \in I_{i_j}} \tilde{\nu}_{i_j} \frac{\lambda_k}{\nu_{i_j}}\delta_{x_k}$ until $r+1$ particles remain.\;
\end{algorithmic}
\end{algorithm}


\begin{lemma}[\cite{litterer2012high}]
Algorithm~\ref{alg:recom} requires $\lceil \log (\hat{n}/r)\rceil$ iterations of Algorithm~\ref{alg:redone}.
\end{lemma}

\begin{corollary}[\cite{litterer2012high}]
\label{cor:ll12}
 Algorithm~\ref{alg:recom} operates in 
 \[O(r\hn + r\log(\hn/r)C(r+2,r+1))\]
 time, where $C(r+2,r+1)$ is the time to compute the solution of a system of linear equations in $r+2$ variables and $r+1$ constraints. 
\end{corollary}


\subsection{Proofs}

Let us first provide a technical definition that will be used in the proofs. Associated to the structural dynamics of the neural SDE \eqref{eq:nsde} is a Markov semigroup. 

\begin{definition}[Markov Semigroup]
    The Markov semigroup $P_t^{\theta}$ associated to the dynamics of the SDE \eqref{eq:nsde} is given by 
    \[P_t^{\theta}f(x) := \CE(f(X_t^{\theta}) | X_0^{\theta} =x)\]
\end{definition}
\begin{comment}
\subsection{Lemma~\ref{lem:clfint}}
\label{sec:pf1}
\begin{proof}
For any $x\in\reals^{\xdim}$,
\begin{align*}
    \left|P_s \lf(s,x) - P_s \lf(t,x)\right| &= \left|\CE[\lf(s,X_s) - \lf(t,X_s) \right| X_0 = x]|\\
    &\leq |t-s| \sup_{q\in[0,T],\,x\in\reals^{\xdim}}\left|\frac{\del}{\del t}|_{t=q}\lf(t,x)\right| = \frac{\clfint}{2}|\Delta|
\end{align*}
\end{proof}

\subsection{Lemma~\ref{lem:infgenbd}}
\label{sec:pf2}
\begin{proof}
By \ref{as:intfun} there exists a constant $\infbd$ such that
\begin{equation}
\label{eq:infb}
\sup_{t\in[0,T]}\|\nabla \lf(t,\cdot)\|_{\infty} \leq \infbd< \infty
\end{equation}
Thus, \eqref{eq:infgenbd} follows.
\end{proof}
\end{comment}

\subsubsection{Theorem~\ref{thm:cubap}}
\label{sec:pf3}
\begin{proof}
By \ref{as:intfun}, $\lf$ is measurable with finite expected value for all $t\in[0,T]$. Thus, we may interchange expectation and integration by Fubini's Theorem:
\begin{align*}
\CE[\CLd(\Xt)] &= \CE\left[\int_0^T \lf(t,X_t^{\theta})dt\right] = \int_0^T\CE[\lf(t,X_t^{\theta})]dt
\end{align*}
Let $\ut= \arg\min_{t_i\leq t}|t-t_i|$ and $s_i = t_{i+1} - t_i$. Then we aim to control
\begin{align*}
&\left|\int_0^T\CE[\lf(t,X_t^{\theta})]dt - \int_0^T \Phi_{\lf}(t)dt\right|  \leq \int_0^T\left|\CE[\lf(t,X_t^{\theta})] - \Phi_{\lf}(t)\right|dt 
\end{align*}
We may handle this by bounding the integrand for each time $t$. This is accomplished as follows. The term $f(t,X_t^{\theta})$ can be expressed as the following stochastic Taylor expansion (see \citet{lyons2004cubature} Proposition 2.1). Denoting $X_{t,x}^{\theta}$ the stochastic process $X_t^{\theta}$ initialized at point $x$, we have
\begin{align*}
   & f(t,X_{t,x}^{\theta}) = \sum_{(i_1,\dots,i_k)\in\CA_m} V_{i_1}^{\theta}\dots V_{i_k}^{\theta} f(t,x)\int_{0 < t_1 < \dots < t_k < t} \circ dB_{t_1}^{i_1}\dots \circ dB_{t_k}^{i_k} + R_m(t,x,f) \\
    & \text{where }
    \sup_{x\in\reals^{\xdim}}\sqrt{\CE(R_m(t,x,f))^2)} \leq C t^{(m+1)/2}\sup_{(i_1,\dots,i_k)\in\CA_{m+2}\backslash \CA_m}\|V_{i_1}^{\theta}\dots V_{i_k}^{\theta}f(t,\cdot)\|_{\infty}
\end{align*}
and $C$ is a constant depending only on $\bdim$ and $m$. Take cubature weights and paths as defined in Section~\ref{sec:cpc}, and define at time $T>0$ the Wiener space measure 
\[Q_T = \sum_{j=1}^{\cn} \lambda_j \delta_{\om_T,j}\] 
such that 
\[\CE_{Q_T}(f(T,X_{T,x}^{\theta})) = \sum_{j=1}^{\cn}\lambda_j f(T,\phi_{T,x}^{\theta}(\om_{T,j}))\]
where $\phi_{T,x}^{\theta}(\om_{T,j})$ denotes the time $T$ solution of ODE \eqref{eq:ode} with initial condition $x$ and path $\om_{T,j}$
Then, for $t \leq T$ we have 
\begin{align}
\begin{split}
\label{eq:tTbound}
    &|(\CE - \CE_{Q_T})(f(t,X_{t,x}^{\theta}))|\\
    &\quad \leq \CE(|R_m(t,x,f)|) + \CE_{Q_T}(|R_m(t,x,f)|) \\
    &\quad \quad + \left|(\CE - \CE_{Q_T})\left(\sum_{(i_1,\dots,i_k) \in\CA_m}V_{i_1}^{\theta}\dots V_{i_k}^{\theta} f(t,x)\int_{0<t_1 <\dots < t_k < t} \circ dB_{t_1}^{i_1} \dots \circ dB_{t_k}^{i_k}\right) \right|
\end{split}
\end{align}
By Lemma 3.1 of \citet{lyons2004cubature} we see that
\[\sup_{x\in\reals^{\xdim}}\CE_{Q_T}[|R_m(t,x,f)|] \leq C_{\bdim,m} t^{(m+1)/2}\sup_{(i_1,\dots,i_k)\in\CA_{m+2}\backslash\CA_m}\|V_{i_1}^{\theta}\dots V_{i_k}^{\theta}f(t,\cdot)\|_{\infty}\]
and the third term in \eqref{eq:tTbound} can be upper bounded by the expansion:
\begin{align}
\begin{split}
\label{eq:expan}
    &\biggl|(\CE - \CE_{Q_t})\left(\sum_{(i_1,\dots,i_k) \in\CA_m}V_{i_1}^{\theta}\dots V_{i_k}^{\theta} f(t,x)\int_{0<t_1 <\dots < t_k < t} \circ dB_{t_1}^{i_1} \dots \circ dB_{t_k}^{i_k}\right)  \\
    &\quad + (\CE_{Q_t} - \CE_{Q_T})\left(\sum_{(i_1,\dots,i_k) \in\CA_m}V_{i_1}^{\theta}\dots V_{i_k}^{\theta}f(t,x)\int_{0<t_1 <\dots < t_k < t} \circ dB_{t_1}^{i_1} \dots \circ dB_{t_k}^{i_k}\right) \biggr| \\
    & = \sum_{(i_1,\dots,i_k) \in\CA_m}V_{i_1}^{\theta}\dots V_{i_k}^{\theta} f(t,x) \int_{0<t_1 <\dots < t_k < t} (d\om_{t,j}^{i_1}(t_1)\dots d\om_{t,j}^{i_k}(t_k) - d\om_{T,j}^{i_1}(t_1)\dots d\om_{T,j}^{i_k}(t_k)) \\
    &\leq 2\tilde{C}_{\bdim,m} t^{(m+1)/2}\sup_{(i_1,\dots,i_k)\in\CA_m}\|V_{i_1}^{\theta}\dots V_{i_k}^{\theta} f(t,\cdot)\|_{\infty}
\end{split}
\end{align}
where $\tilde{C}_{\bdim,m}$ is a constant depending only on $\bdim$ and $m$.
In \eqref{eq:expan} the first equality follows since
the first term vanishes by definition of the Wiener space cubature paths, and the bound follows as in the proof of Lemma 3.1 in \citet{lyons2004cubature}. 

Thus, putting these together into \eqref{eq:tTbound}, we obtain the bound
\begin{align}
\begin{split}
\label{eq:supbd}
\
&\sup_{x\in\reals^{\xdim}}\biggl| \CE\left(f(t,X_{t,x}^{\theta}) - \sum_{j=1}^{\cn}\lambda_j f(t,\phi_{t,x}^{\theta}(\om_{T,j})) \right) \biggr| \\
& \quad \leq C t^{(m+1)/2}\left( \sup_{(i_1,\dots,i_k)\in\CA_{m+2}\backslash \CA_m}\|V_{i_1}^{\theta}\dots V_{i_k}^{\theta} f(t,\cdot)\|_{\infty} + 2\sup_{(i_1,\dots,i_k)\in\CA_m}\|V_{i_1}^{\theta},\dots,V_{i_k}^{\theta} f(t,\cdot)\|_{\infty}\right) 
\end{split}
\end{align}
for some $C$ dependent only on $\bdim$ and $m$.

Consider the construction $ \Phi_{\lf}(t) = \sum_{z\in [{\cn}^k]}\lf(t,\phi_{t,x}^{\theta}(\om_z))\lambda_{I_z[0]}\dots\lambda_{I_z[k]}$ \eqref{eq:sum}. Let $\ut = \arg\min_{t_i < t, i\in[k]}|t-t_i|$ and $\ui$ be the index of $\ut$. Identify $t$ with $t_{\ui+1}$. Let $\us_i = t_i - t_{i-1}$ for all $i \leq \ui$, $
\us_{\ui + 1} = (t-\ut)$. Then, defining the Markov random variable $(\Psi_{i}^{\theta})_{1\leq i\leq \ui+1}$ by $\Psi_0^{\theta} = y$,
\[\PR(\Psi_{i}^{\theta} = \phi_{\us_i,x}^{\theta}(\om_{s_i,j})|\Psi_{i-1}  =x) = \lambda_j,\quad i\in 1,\dots,\ui+1\]
observe that $\CE_{\Psi^{\theta}}\lf(t,\Psi_{\ui+1}^{\theta}) = \Phi_f^{\theta}(t)$.
So, by Theorem 3.3 of \citet{lyons2004cubature} we may extend \eqref{eq:supbd} to $\Phi_f(t)$ to conclude that
\begin{align*}
\begin{split}
    &|\Phi_f^{\theta}(t) - \CE[f(t,X_t^{\theta})]|\\
    &\leq C\sum_{j=1}^{\ui + 1} \us_{j}^{(m+1)/2}\biggl\{ \sup_{(i_1,\dots,i_k)\in\CA_{m+2}\backslash \CA_m}\|V_{i_1}^{\theta}\dots V_{i_k}^{\theta} P_{t - t_j}^{\theta}f(t_j,\cdot)\|_{\infty} \\
    &\qquad \qquad \qquad \qquad \qquad + 2\sup_{(i_1,\dots,i_k)\in\CA_m}\|V_{i_1}^{\theta},\dots,V_{i_k}^{\theta} P_{t - t_j}^{\theta}f(t_j,\cdot)\|_{\infty}\biggr\} \\
    & \leq C\sum_{j=1}^{k} s_j^{(m+1)/2}\biggl\{ \sup_{\substack{(i_1,\dots,i_k)\in\CA_{m+2}\backslash \CA_m \\ t\in[0,T]}}\|V_{i_1}^{\theta}\dots V_{i_k}^{\theta} P_{T - t_j}^{\theta}f(t,\cdot)\|_{\infty} \\
    &\qquad \qquad \qquad \qquad \qquad + 2\sup_{\substack{(i_1,\dots,i_k)\in\CA_m \\ t\in[0,T]}}\|V_{i_1}^{\theta},\dots,V_{i_k}^{\theta} P_{T - t_j}^{\theta}f(t,\cdot)\|_{\infty}\biggr\}
\end{split}
\end{align*}
where the second inequality follows simply by monotonicity of the sum in $j$.
Now, under the H\"ormander condition \ref{as:uh} \citep{kusuoka1985applications}, 
\[\|V_{i_1}^{\theta}\dots V_{i_k}^{\theta} P_s^{\theta} f\|_{\infty} \leq \frac{Ks^{1/2}}{s^{(k+\text{card}\{j,i_j=0\})/2}}\|\nabla f\|_{\infty}\]
for some constant $K$; this can be applied to both $\infty$-norm terms above. 
By the same derivation as in the proof of Proposition 3.6 of \citet{lyons2004cubature} we obtain for every $t\in[0,T]$  
\begin{align*}
    |\Phi_f^{\theta}(t) - \CE[f(t,X_t^{\theta})]| \leq 3\,K \sup_{t\in[0,T]}\|\nabla f(t,\cdot)\|_{\infty}\left(s_k^{1/2} + \sum_{i=1}^{k-1}\frac{s_i^{(m+1)/2}}{(T-t_i)^{m/2}}\right)
\end{align*}
Thus, since  $\lf$ is measurable with finite expected value for all $t\in[0,T]$ by \ref{as:intfun}, 
\begin{align*}
  &\left|\CE[\CLd(\Xt)] - \int_0^T\Phi_{\lf}^{\theta}(t)dt\right| = \left| \CE\int_0^T \lf(t,\Xt_t)dt -\int_0^T\Phi_{\lf}^{\theta}(t)dt\right| \\
    & \quad \leq \int_0^T \left|\CE[f(t,\Xt_t)] - \Phi_f^{\theta}(t)\right|dt \\
    &\quad \leq 3\,T\,K \sup_{t\in[0,T]}\|\nabla f(t,\cdot)\|_{\infty}\left(s_k^{1/2} + \sum_{i=1}^{k-1}\frac{s_i^{(m+1)/2}}{(T-t_i)^{m/2}}\right)
\end{align*}

Then, by \citet{lyons2004cubature} we have, for $t_j = T\left(1 - \left(1-\frac{j}{k}\right)^{\gamma}\right)$ and $0 < \gamma < m-1$,
 \[\left(s_k^{1/2} + \sum_{i=1}^{k-1}\frac{s_i^{(m+1)/2}}{(T-t_i)^{m/2}}\right) \leq C(m,\gamma)T^{1/2} k^{-\gamma/2}\] which completes the bound.
\end{proof}

\begin{comment}
\begin{proof}
By \ref{as:intfun}, $\lf$ is measurable with finite expected value for all $t\in[0,T]$. Thus, we may interchange expectation and integration by Fubini's Theorem:
\begin{align*}
\CE_{X\sim \CP(\theta)}[\CLd(X)] &= \CE_{X\sim \CP(\theta)}\left[ \lf(t,X_t)\right]dt = \int_0^T\left[\CE_{X\sim \CP(\theta)}\lf(t,X_t)dt\right]
\end{align*}
Let $\ut= \arg\min_{t_i\leq t}|t-t_i|$ and $s_i = t_{i+1} - t_i$. Then we control
\begin{align}
\begin{split}
\label{eq:split}
    &\left|\int_0^T\left[\CE_{X\sim \CP(\theta)}\lf(t,X_t)\right]dt - \sum_{i=1}^k s_i \,\Phi_{\lf}(t_i)\right| \\ 
     &\leq \int_0^T\left|\CE_{X\sim \CP(\theta)}\lf(t,X_t) - \Phi_{\lf}(\ut)\right|dt \\
    & = \sum_{i=0}^{k-1} \int_{t_i}^{t_{i+1}} \left|\CE_{X\sim \CP(\theta)}\lf(t,X_t) - \Phi_{\lf}(t_i)\right|dt \\
    & \leq \sum_{i=0}^{k-1} \int_{t_i}^{t_{i+1}}  \left\{\left|\CE_{X\sim \CP(\theta)}\lf(t,X_t) - \CE_{X\sim \CP(\theta)}\lf(\ut,X_{\ut})\right| + \left|\CE_{X\sim \CP(\theta)}\lf(\ut,X_{\ut}) - \Phi_{\lf}(t_i)\right|\right\}dt
\end{split}
\end{align}
Let $P_x$ denote the projection of the law $\CP(\theta)$ onto the initial value $X_0$. According to the neural SDE model \eqref{eq:nsde}  $P_x$ admits the density $\CN(0,I_{\adim})$. The term $\left|\CE_{X\sim \CP(\theta)}\lf(t,X_t) - \CE_{X\sim \CP(\theta)}\lf(\ut,X_{\ut})\right|$ can be re-expressed as 
\begin{equation}
\label{eq:pfeq4}
    \left|\int_{\reals^{\xdim}}P_t \lf(t,x)dP_x - \int_{\reals^{\xdim}}P_{\ut}\lf(\ut,x)dP_x \right| \leq \int_{\reals^{\xdim}}\left|P_t \lf(t,x) - P_{\ut}\lf(\ut,x) \right|dP_x
\end{equation} which can in turn be decomposed within the integrand as 
\begin{align*}
&\left|P_t \lf(t,x) - P_{\ut}\lf(\ut,x) \right| \leq \left| P_t\lf(t,x) - P_t\lf(\ut,x) \right| + \left|P_t\lf(\ut,x) - P_{\ut}\lf(\ut,x) \right|
\end{align*}
By Lemma~\ref{lem:clfint} the first term $\left| P_t\lf(t,x) - P_t\lf(\ut,x) \right|$ is bounded by $\frac{\clfint}{2} s_i$ for all $x$, and by Lemma~\ref{lem:infgenbd} the second term is bounded by $\infbd s_i$ for all $x$. Thus \eqref{eq:pfeq4} is bounded by $\left(\frac{\clfint}{2} + \infbd\right)s_i$.

Now, defining the Markov random variable $(\Psi_i)_{0 \leq i \leq k}$ by 
\[\PR(\Psi_{l+1} = \phi_{s_l}^x(\om_{s_l,i}) | \Psi_l  =x) = \lambda_i\]
we have $\CE_{\Psi}\lf(t_i,\Psi_{t_i}) = \Phi_{\lf}(t_i)$. Here $\CE_{\Psi}$ is the expectation w.r.t. the law of random variable $(\Psi_i)_{0\leq i\leq k}$. Invoking Theorem 3.3 of \cite{lyons2004cubature}, we may then bound the remaining term in \eqref{eq:split} as 
\begin{align*}
    &\left|\CE_{X\sim \CP(\theta)}\lf(\ut,X_{\ut}) - \CE_{\Psi}\lf(\ut,\Psi_{\ut})\right| \\
    \quad\quad & \leq C\sum_{j=0}^{i-1}s_j^{(m+1)/2}\sup_{(l_0,\dots,l_{i-1})\in\CA_{m+2}\backslash\CA_{m}}\|V_{l_0},\dots V_{l_{i-1}} P_{t_{i-1}-t_{t_j}}\lf(t_j,\cdot)\|_{\infty} \\
    & \leq  C\sum_{j=0}^{k-1}s_j^{(m+1)/2}\sup_{(l_0,\dots,l_{k-1})\in\CA_{m+2}\backslash\CA_{m}}\|V_{l_0},\dots V_{l_{k-1}} P_{t_{k-1}-t_{t_j}}\lf(t_j,\cdot)\|_{\infty}
\end{align*}
Thus, putting everything together in \eqref{eq:split}, we obtain
\begin{align*}
    &\left|\CE_{X\sim \CP(\theta)}[\CLd(X)] - \sum_{i=1}^k s_i \,\Phi_{\lf}(t_i)\right| \\
      & \leq \sum_{i=0}^{k-1} s_i \left[\left(\frac{\clfint}{2} + \infbd\right)s_i + C\sum_{j=0}^{k-1}s_j^{(m+1)/2}\sup_{(l_0,\dots,l_{k-1})\in\CA_{m+2}\backslash\CA_{m}}\|V_{l_0},\dots V_{l_{k-1}} P_{t_{k-1}-t_{t_j}}\lf(t_j,\cdot)\|_{\infty}\right] \\
      & = \left(\frac{\clfint}{2} + \infbd \right)\sum_{i=1}^{k-1}s_i^2 + C \,T\, \sum_{j=0}^{k-1}s_j^{(m+1)/2}\sup_{(l_0,\dots,l_{k-1})\in\CA_{m+2}\backslash\CA_{m}}\|V_{l_0},\dots V_{l_{k-1}} P_{t_{k-1}-t_{t_j}}\lf(t_j,\cdot)\|_{\infty}
\end{align*}

\end{proof}
\end{comment}


\subsubsection{Theorem~\ref{thm:bd2}}
\label{sec:pf4}
\begin{proof}
By Theorem~\ref{thm:cubap} and Theorem 19 of \citet{litterer2012high} we derive
%[*edit more explicit derivation? e.g. T7 guarantees for \textit{all} t, inputting into LT19 proof and bounding..., KLV op. now only vector field independent...] 
\begin{align*}
    &\sup_{x,t} |P_t f(t,x) - \tPhi_{\lf}^{\theta}(t)| \\
    &\quad \leq \biggl(C_1\left( s_k^{1/2} + \sum_{i=1}^{k-1}\sum_{j=m}^{m+1} \frac{s_i^{(j+1)/2}}{(T-t_i)^{j/2}}\right)  + C_5\sum_{i=1}^{k-1} \frac{u_i^{r+1}}{(T-t_i)^{r\ps/2}}\biggr)\sup_{t\in[0,T]}\|\nabla f(t,\cdot))\|_{\infty}
\end{align*}
where $C_1:= 3\,T\,K \,\lfgsup$. By the results in \citet{lyons2004cubature} (also \citet{litterer2012high} page 20) we have, since $m-1 \geq \gamma$,
\[s_k^{1/2} + \sum_{i=1}^{k-1}\sum_{j=m}^{m+1} \frac{s_i^{(j+1)/2}}{(T-t_i)^{j/2}} \leq C_7(m,\gamma)T^{1/2} k^{-\gamma/2}\]
Let $\cfac$ be such that $\gamma \leq \frac{p(r+1)}{\frac{\ps r}{\cfac} + 1}$. Substituting $u_j = s_j^{\ps/2\gamma}$, we have 
\[\sum_{i=1}^{k-1} \frac{s_i^{\ps(r+1)/2\gamma}}{(T-t_i)^{r\ps/2}} \leq \sum_{i=1}^{k-1} c_i \frac{s_i^{(\frac{r\ps}{\cfac}+1)/2}}{(T-t_i)^{r\ps/2\cfac}} \leq C_6\sum_{i=1}^{k-1} c_i \frac{s_i^{(\frac{r\ps}{\cfac}+1)/2}}{(T-t_i)^{r\ps/2\cfac}}\]
where 
\begin{align*}
    c_i = \frac{s_i^{(\frac{\ps(r+1)}{2\gamma} - \frac{\frac{\ps r}{\cfac}+1)}{2})}}{(T-t_i)^{r\ps/2c - r\ps/2}} &\leq T^{\ps(r+1) - \gamma(\frac{\ps r}{\cfac}+1)} k^{-\gamma(r\ps/2\cfac - r\ps/2)} \\
    & = T^{\ps(r+1) - \gamma(\frac{\ps r}{\cfac}+1)} k^{\gamma r\ps/2 - \gamma r\ps/2\cfac}
\end{align*}
%\[c_i = \frac{s_i^{(\frac{\ps(r+1)}{2\gamma} - \frac{\frac{\ps r}{\cfac}+1)}{2})}}{(T-t_i)^{r\ps/2c - r\ps/2}} \leq T^{\ps(r+1) - \gamma(\frac{\ps r}{\cfac}+1)} k^{-\gamma(r\ps/2\cfac - r\ps/2)} = T^{\ps(r+1) - \gamma(\frac{\ps r}{\cfac}+1)} k^{\gamma r\ps/2 - \gamma r\ps/2\cfac}\] 

\begin{comment}and since $\gamma\geq \frac{\cfac\ps(r+1)}{\xdim(\ps r +1)}$ [*edit assumption/justify], \[C_6 = k^{\frac{\xdim\gamma(\ps r+1) - \cfac\ps(r+1)}{2\xdim\gamma}} \geq \max_i c_i\]
\end{comment}

Then, since $r\ps-1\geq\gamma$,
\[ C_6\,\sum_{i=1}^{k-1} \frac{s_i^{(r\ps+1)/2}}{(T-t_i)^{r\ps/2}} \leq \,C_7(r\ps,\gamma) T^{1/2+\ps(r+1) - \gamma(\frac{\ps r}{\cfac}+1)}k^{\gamma r\ps/2 - \gamma r\ps/2\cfac-\gamma/2}\]
which gives us 
\begin{align*}
    &\sup_{x,t} |P_t f(t,x) - \tilde{\Phi}_f^{\theta}(t)| \\
    & \quad \leq \left(C_1(m,\gamma,T) + C_2(r,\ps,\gamma,T) \right) \lfgsup T^{1/2}k^{\gamma r\ps/2 - \gamma r\ps/2\cfac-\gamma/2}
\end{align*}
Then, we may simply bound
\begin{align}
\begin{split}
\label{eq:kbd}
    &\left|\CE [\CLd(\Xt)] - \int_0^T \tilde{\Phi}_f^{\theta}(t)dt\right| = \left|\int_0^T \CE[ \lf(t,\Xt_t)]dt - \int_0^T \tilde{\Phi}_f^{\theta}(t)dt\right| \\
    &\quad \leq \int_0^T \left| P_t\lf(t,x) - \tilde{\Phi}_f^{\theta}(t)\right|dt \\
    & \quad \leq T\,\left(C_1(m,\gamma,T) + C_2(r,\ps,\gamma,T) \right) \lfgsup T^{1/2}k^{\gamma r\ps/2 - \gamma r\ps/2\cfac-\gamma/2}
\end{split}
\end{align}
with $\lfgsup = \sup_{t\in[0,T]}\|\nabla f(t,\cdot)\|_{\infty}$ and $C_1$ and $C_2$ are constants depending only on their arguments. 

Now, to obtain \eqref{eq:cbound}, we first define 
\[\pmlen = \max_{i\in[r]}\text{length}(\om_i)\] 
as the maximum of the lengths of paths in the degree $m$ cubature formula on Wiener space over the unit time interval. Then, observe that given \ref{as:intfun} there exists $M'$ such that vector fields $\{V_i^{\theta}(\cdot)\}_{i=1}^{\bdim+1}$ are uniformly bounded by $M'$. Then (see \citet{litterer2012high} for more details), 
\[\text{supp}(Q_{\CD,u}^{(k)})(x) \subseteq B(x,\vfbd \pmlen \sum_{i=1}^{k} s_j^{1/2}) \subseteq B(x,\vfbd \pmlen kT^{1/2})\]
Thus, the total number of ODE computations is exactly the total number of localizing balls of radius $u_j = s_j^{\ps/2\gamma}$ needed to cover the ball of radius $\vfbd \pmlen kT^{1/2}$.
The volume $V_{\xdim}(r)$ of an $\xdim$-dimensional ball of radius $r$ is given by
$V_{\xdim}(r) = \frac{\pi^{\xdim/2}}{\Gamma(\frac{\xdim}{2}+1)}r^{\xdim}$
where $\Gamma$ is the gamma function. Thus, the number of balls $n$ of radius $u_j = s_j^{\ps/2\gamma}$ necessary to cover $B(x,\vfbd \pmlen kT^{1/2})$ is given by 
\begin{equation}
\label{eq:nbound}
n = \frac{(\vfbd \pmlen kT^{1/2})^{\xdim}}{(s_j^{\ps/2\gamma})^{\xdim}} \leq \frac{(\vfbd\pmlen  kT^{1/2})^{\xdim}}{(T(\frac{1}{k})^{\gamma})^{\ps \xdim/2\gamma}} = (\vfbd \pmlen )^{\xdim} k^{\xdim(\ps/2 - 1)}T^{\xdim(\ps-1)}
\end{equation}
Inverting, we have $k = \left(n (\vfbd \pmlen )^{-\xdim}T^{-\xdim(\ps-1)} \right)^{\frac{1}{\xdim(\ps/2-1)}} = (\vfbd \pmlen )^{-\frac{1}{(\ps/2-1)}}T^{-\frac{\ps-1}{\ps/2-1}}n^{\frac{1}{\xdim(\ps/2-1)}}$. Thus, substituting into \eqref{eq:kbd}, we obtain 
\begin{align}
\begin{split}
\label{eq:interbd}
    &\left|\CE [\CLd(\Xt)] - \int_0^T \tilde{\Phi}_f^{\theta}(t)dt\right| \\
    &\quad \leq  (\vfbd \pmlen )^{\frac{\gamma}{\ps/2}}T^{\frac{\gamma(\ps-1)+1}{\ps-2}}\lfgsup\left(C_1(m,\gamma,T) + C_2(r,\ps,\gamma,T)\right)n^{\frac{-\gamma\left(
    r\ps/\cfac - r\ps  + 1\right)}{\xdim(\ps-2)}}
\end{split}
\end{align}
\begin{comment}
and then integrating yields
\begin{align}
\begin{split}
\label{eq:interbd}
&\sup_{x} \left|\CE[\CLd(\Xt)] - \int_0^T \CE_{Q_{\CD,u}^t(x)}\lf(t,\cdot)dt \right|\\
& \quad \leq \int_0^T \left| P_t\lf(t,x) - \CE_{Q_{\CD,u}^t(x)}\lf(t,\cdot)\right|dt \\
&\quad \leq  (\vfbd \pmlen )^{\frac{\gamma}{\ps/2}}T^{\frac{\gamma(\ps-1)+1}{\ps-2}}\lfgsup\left(C_1(m,\gamma,T) + C_2(r,\ps,\gamma,T)\right)n^{\frac{-\gamma\left(
    r\ps/\cfac - r\ps  + 1\right)}{\xdim(\ps-2)}}
\end{split}
\end{align}
\end{comment}
\end{proof}


\end{document}
