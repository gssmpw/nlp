@inproceedings{Park2020Investigating,
author = {Park, Sunjeong and Lim, Youn-kyung},
title = {Investigating User Expectations on the Roles of Family-shared AI Speakers},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376450},
doi = {10.1145/3313831.3376450},
abstract = {AI assistants that use a voice user interface (VUI), such as AI speakers, have become popular in family homes. However, it is still unclear what roles the AI speaker can support within the family unit. We investigated the roles of an AI speaker as a family-shared technology. By conducting a one-week participatory user study, we discovered that family members' co-ownership toward the AI speaker was the key in the development of its family-oriented roles. Our findings showed seven domains of user expectations on these roles, and we realized that all the expectations can be represented as family cohesion. In addition, privacy awareness was emphasized regarding personal supports. Finally, we discuss a new perspective for AI speaker design and offer two suggestions: 1) leveraging human-likeness to develop its potential roles of supporting the unit of a family and 2) interpreting the home context to seamlessly connect family and personal supporting roles.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {AI speaker, family, participatory design study},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{Anjum2020VertextAE,
  title={Vertext: An End-to-end AI Powered Conversation Management System for Multi-party Chat Platforms},
  author={Omer Anjum and Chak Ho Chan and Tanitpong Lawphongpanich and Yucheng Liang and Tianyi Tang and Shuchen Zhang and Wen-mei W. Hwu and Jinjun Xiong and Sanjay J. Patel},
  journal={Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:222805393}
}
@inproceedings{lowe-etal-2015-ubuntu,
    title = "The {U}buntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
    author = "Lowe, Ryan  and
      Pow, Nissan  and
      Serban, Iulian  and
      Pineau, Joelle",
    editor = "Koller, Alexander  and
      Skantze, Gabriel  and
      Jurcicek, Filip  and
      Araki, Masahiro  and
      Rose, Carolyn Penstein",
    booktitle = "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2015",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-4640",
    doi = "10.18653/v1/W15-4640",
    pages = "285--294",
}
@inproceedings{gu-etal-2022-hetermpc,
    title = "{H}eter{MPC}: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
    author = "Gu, Jia-Chen  and
      Tan, Chao-Hong  and
      Tao, Chongyang  and
      Ling, Zhen-Hua  and
      Hu, Huang  and
      Geng, Xiubo  and
      Jiang, Daxin",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.349",
    doi = "10.18653/v1/2022.acl-long.349",
    pages = "5086--5097",
    abstract = "Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-type-dependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
}
@inproceedings{gu-etal-2023-madnet,
    title = "{MADN}et: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation",
    author = "Gu, Jia-Chen  and
      Tan, Chao-Hong  and
      Chu, Caiyuan  and
      Ling, Zhen-Hua  and
      Tao, Chongyang  and
      Liu, Quan  and
      Liu, Cong",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.476",
    doi = "10.18653/v1/2023.emnlp-main.476",
    pages = "7681--7692",
    abstract = "Modeling multi-party conversations (MPCs) with graph neural networks has been proven effective at capturing complicated and graphical information flows. However, existing methods rely heavily on the necessary addressee labels and can only be applied to an ideal setting where each utterance must be tagged with an {``}@{''} or other equivalent addressee label. To study the scarcity of addressee labels which is a common issue in MPCs, we propose MADNet that maximizes addressee deduction expectation in heterogeneous graph neural networks for MPC generation. Given an MPC with a few addressee labels missing, existing methods fail to build a consecutively connected conversation graph, but only a few separate conversation fragments instead. To ensure message passing between these conversation fragments, four additional types of latent edges are designed to complete a fully-connected graph. Besides, to optimize the edge-type-dependent message passing for those utterances without addressee labels, an Expectation-Maximization-based method that iteratively generates silver addressee labels (E step), and optimizes the quality of generated responses (M step), is designed. Experimental results on two Ubuntu IRC channel benchmarks show that MADNet outperforms various baseline models on the task of MPC generation, especially under the more common and challenging setting where part of addressee labels are missing.",
}
@inproceedings{Zhang2018Addressee,
author = {Zhang, Rui and Lee, Honglak and Polymenakos, Lazaros and Radev, Dragomir},
title = {Addressee and response selection in multi-party conversations with speaker interaction RNNs},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {In this paper, we study the problem of addressee and response selection in multi-party conversations. Understanding multi-party conversations is challenging because of complex speaker interactions: multiple speakers exchange messages with each other, playing different roles (sender, addressee, observer), and these roles vary across turns. To tackle this challenge, we propose the Speaker Interaction Recurrent Neural Network (SI-RNN). Whereas the previous state-of-the-art system updated speaker embeddings only for the sender, SI-RNN uses a novel dialog encoder to update speaker embeddings in a role-sensitive way. Additionally, unlike the previous work that selected the addressee and response separately, SI-RNN selects them jointly by viewing the task as a sequence prediction problem. Experimental results show that SI-RNN significantly improves the accuracy of addressee and response selection, particularly in complex conversations with many speakers and responses to distant messages many turns in the past.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {698},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{Gu2021MPCBERTAP,
  title={MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding},
  author={Jia-Chen Gu and Chongyang Tao and Zhenhua Ling and Can Xu and Xiubo Geng and Daxin Jiang},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.01541},
  url={https://api.semanticscholar.org/CorpusID:235313361}
}

@inproceedings{Gu2023GIFTGF,
  title={GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding},
  author={Jia-Chen Gu and Zhen-Hua Ling and QUAN LIU and Cong Liu and Guoping Hu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258715296}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
@inproceedings{Hu2019GSNAG,
  title={GSN: A Graph-Structured Network for Multi-Party Dialogues},
  author={Wenpeng Hu and Zhangming Chan and Bing Liu and Dongyan Zhao and Jinwen Ma and Rui Yan},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:173188574}
}

@inproceedings{zhu-etal-2022-multi,
    title = "Multi-Party Empathetic Dialogue Generation: A New Task for Dialog Systems",
    author = "Zhu, Ling.Yu  and
      Zhang, Zhengkun  and
      Wang, Jun  and
      Wang, Hongbin  and
      Wu, Haiying  and
      Yang, Zhenglu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.24",
    doi = "10.18653/v1/2022.acl-long.24",
    pages = "298--307",
    abstract = "Empathetic dialogue assembles emotion understanding, feeling projection, and appropriate response generation. Existing work for empathetic dialogue generation concentrates on the two-party conversation scenario. Multi-party dialogues, however, are pervasive in reality. Furthermore, emotion and sensibility are typically confused; a refined empathy analysis is needed for comprehending fragile and nuanced human feelings. We address these issues by proposing a novel task called Multi-Party Empathetic Dialogue Generation in this study. Additionally, a Static-Dynamic model for Multi-Party Empathetic Dialogue Generation, SDMPED, is introduced as a baseline by exploring the static sensibility and dynamic emotion for the multi-party empathetic dialogue learning, the aspects that help SDMPED achieve the state-of-the-art performance.",
}
@article{velikovi2017graph,
  added-at = {2020-01-17T13:34:34.000+0100},
  author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/2139f6728c62157ee1bd6543d9de013c5/denklu},
  interhash = {d2f5ba17510bf494b75ce245fa72ccdd},
  intrahash = {139f6728c62157ee1bd6543d9de013c5},
  journal = {6th International Conference on Learning Representations},
  keywords = {},
  timestamp = {2020-01-17T13:34:34.000+0100},
  title = {Graph Attention Networks},
  year = 2017
}
@article{Wang2019HeterogeneousGA,
  title={Heterogeneous Graph Attention Network},
  author={Xiao Wang and Houye Ji and Chuan Shi and Bai Wang and Peng Cui and Philip S. Yu and Yanfang Ye},
  journal={The World Wide Web Conference},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:81978964}
}
@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}
@inproceedings{afantenos-etal-2015-discourse,
    title = "Discourse parsing for multi-party chat dialogues",
    author = "Afantenos, Stergos  and
      Kow, Eric  and
      Asher, Nicholas  and
      Perret, J{\'e}r{\'e}my",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1109",
    doi = "10.18653/v1/D15-1109",
    pages = "928--937",
}
@inproceedings{asher-etal-2016-discourse,
    title = "Discourse Structure and Dialogue Acts in Multiparty Dialogue: the {STAC} Corpus",
    author = "Asher, Nicholas  and
      Hunter, Julie  and
      Morey, Mathieu  and
      Farah, Benamara  and
      Afantenos, Stergos",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1432",
    pages = "2721--2727",
    abstract = "This paper describes the STAC resource, a corpus of multi-party chats annotated for discourse structure in the style of SDRT (Asher and Lascarides, 2003; Lascarides and Asher, 2009). The main goal of the STAC project is to study the discourse structure of multi-party dialogues in order to understand the linguistic strategies adopted by interlocutors to achieve their conversational goals, especially when these goals are opposed. The STAC corpus is not only a rich source of data on strategic conversation, but also the first corpus that we are aware of that provides full discourse structures for multi-party dialogues. It has other remarkable features that make it an interesting resource for other topics: interleaved threads, creative language, and interactions between linguistic and extra-linguistic contexts.",
}
@inproceedings{perret-etal-2016-integer,
    title = "Integer Linear Programming for Discourse Parsing",
    author = "Perret, J{\'e}r{\'e}my  and
      Afantenos, Stergos  and
      Asher, Nicholas  and
      Morey, Mathieu",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1013",
    doi = "10.18653/v1/N16-1013",
    pages = "99--109",
}
@inproceedings{Shi2019DeepSequential,
author = {Shi, Zhouxing and Huang, Minlie},
title = {A deep sequential model for discourse parsing on multi-party dialogues},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33017007},
doi = {10.1609/aaai.v33i01.33017007},
abstract = {Discourse structures are beneficial for various NLP tasks such as dialogue understanding, question answering, sentiment analysis, and so on. This paper presents a deep sequential model for parsing discourse dependency structures of multi-party dialogues. The proposed model aims to construct a discourse dependency tree by predicting dependency relations and constructing the discourse structure jointly and alternately. It makes a sequential scan of the Elementary Discourse Units (EDUs)1 in a dialogue. For each EDU, the model decides to which previous EDU the current one should link and what the corresponding relation type is. The predicted link and relation type are then used to build the discourse structure incrementally with a structured encoder. During link prediction and relation classification, the model utilizes not only local information that represents the concerned EDUs, but also global information that encodes the EDU sequence and the discourse structure that is already built at the current step. Experiments show that the proposed model outperforms all the state-of-the-art baselines.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {860},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{li-etal-2020-molweni,
    title = "Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure",
    author = "Li, Jiaqi  and
      Liu, Ming  and
      Kan, Min-Yen  and
      Zheng, Zihao  and
      Wang, Zekun  and
      Lei, Wenqiang  and
      Liu, Ting  and
      Qin, Bing",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.238",
    doi = "10.18653/v1/2020.coling-main.238",
    pages = "2642--2652",
    abstract = "Research into the area of multiparty dialog has grown considerably over recent years. We present the Molweni dataset, a machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni{'}s source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances. We annotate 30,066 questions on this corpus, including both answerable and unanswerable questions. Molweni also uniquely contributes discourse dependency annotations in a modified Segmented Discourse Representation Theory (SDRT; Asher et al., 2016) style for all of its multiparty dialogs, contributing large-scale (78,245 annotated discourse relations) data to bear on the task of multiparty dialog discourse parsing. Our experiments show that Molweni is a challenging dataset for current MRC models: BERT-wwm, a current, strong SQuAD 2.0 performer, achieves only 67.7{\%} F1 on Molweni{'}s questions, a 20+{\%} significant drop as compared against its SQuAD 2.0 performance.",
}
@inproceedings{ijcai2021p543,
  title     = {A Structure Self-Aware Model for Discourse Parsing on Multi-Party Dialogues},
  author    = {Wang, Ante and Song, Linfeng and Jiang, Hui and Lai, Shaopeng and Yao, Junfeng and Zhang, Min and Su, Jinsong},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {3943--3949},
  year      = {2021},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2021/543},
  url       = {https://doi.org/10.24963/ijcai.2021/543},
}
@inproceedings{liu-chen-2021-improving,
    title = "Improving Multi-Party Dialogue Discourse Parsing via Domain Integration",
    author = "Liu, Zhengyuan  and
      Chen, Nancy",
    editor = "Braud, Chlo{\'e}  and
      Hardmeier, Christian  and
      Li, Junyi Jessy  and
      Louis, Annie  and
      Strube, Michael  and
      Zeldes, Amir",
    booktitle = "Proceedings of the 2nd Workshop on Computational Approaches to Discourse",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.codi-main.11",
    doi = "10.18653/v1/2021.codi-main.11",
    pages = "122--127",
    abstract = "While multi-party conversations are often less structured than monologues and documents, they are implicitly organized by semantic level correlations across the interactive turns, and dialogue discourse analysis can be applied to predict the dependency structure and relations between the elementary discourse units, and provide feature-rich structural information for downstream tasks. However, the existing corpora with dialogue discourse annotation are collected from specific domains with limited sample sizes, rendering the performance of data-driven approaches poor on incoming dialogues without any domain adaptation. In this paper, we first introduce a Transformer-based parser, and assess its cross-domain performance. We next adopt three methods to gain domain integration from both data and language modeling perspectives to improve the generalization capability. Empirical results show that the neural parser can benefit from our proposed methods, and performs better on cross-domain dialogue samples.",
}
@inproceedings{chi-rudnicky-2022-structured,
    title = "Structured Dialogue Discourse Parsing",
    author = "Chi, Ta-Chung  and
      Rudnicky, Alexander",
    editor = "Lemon, Oliver  and
      Hakkani-Tur, Dilek  and
      Li, Junyi Jessy  and
      Ashrafzadeh, Arash  and
      Garcia, Daniel Hern{\'a}ndez  and
      Alikhani, Malihe  and
      Vandyke, David  and
      Du{\v{s}}ek, Ond{\v{r}}ej",
    booktitle = "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2022",
    address = "Edinburgh, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigdial-1.32",
    doi = "10.18653/v1/2022.sigdial-1.32",
    pages = "325--335",
    abstract = "Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse \textit{links} and corresponding \textit{relations}. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model{'}s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).",
}
@inproceedings{ouchi-tsuboi-2016-addressee,
    title = "Addressee and Response Selection for Multi-Party Conversation",
    author = "Ouchi, Hiroki  and
      Tsuboi, Yuta",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1231",
    doi = "10.18653/v1/D16-1231",
    pages = "2133--2143",
}
@inproceedings{Meng2017TowardsNS,
  title={Towards Neural Speaker Modeling in Multi-Party Conversation: The Task, Dataset, and Models},
  author={Zhao Meng and Lili Mou and Zhi Jin},
  booktitle={International Conference on Language Resources and Evaluation},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:9464771}
}
@inproceedings{le-etal-2019-speaking,
    title = "Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations",
    author = "Le, Ran  and
      Hu, Wenpeng  and
      Shang, Mingyue  and
      You, Zhenjun  and
      Bing, Lidong  and
      Zhao, Dongyan  and
      Yan, Rui",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1199",
    doi = "10.18653/v1/D19-1199",
    pages = "1909--1919",
    abstract = "Previous research on dialogue systems generally focuses on the conversation between two participants, yet multi-party conversations which involve more than two participants within one session bring up a more complicated but realistic scenario. In real multi- party conversations, we can observe who is speaking, but the addressee information is not always explicit. In this paper, we aim to tackle the challenge of identifying all the miss- ing addressees in a conversation session. To this end, we introduce a novel who-to-whom (W2W) model which models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements.",
}
@inproceedings{wang-etal-2020-response,
    title = "Response Selection for Multi-Party Conversations with Dynamic Topic Tracking",
    author = "Wang, Weishi  and
      Hoi, Steven C.H.  and
      Joty, Shafiq",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.533",
    doi = "10.18653/v1/2020.emnlp-main.533",
    pages = "6581--6591",
    abstract = "While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.",
}
@inproceedings{li-zhao-2023-em,
    title = "{EM} Pre-training for Multi-party Dialogue Response Generation",
    author = "Li, Yiyang  and
      Zhao, Hai",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.7",
    doi = "10.18653/v1/2023.acl-long.7",
    pages = "92--103",
    abstract = "Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which two-party dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Thanks to the huge amount of two-party conversational data, various pre-trained language models for two-party dialogue response generation have been proposed. However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues. To tackle this obstacle, we propose an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Theoretical analyses and extensive experiments have justified the feasibility and effectiveness of our proposed method. The official implementation of this paper is available at \url{https://github.com/EricLee8/MPDRG}.",
}
@inproceedings{mehri-etal-2019-pretraining,
    title = "Pretraining Methods for Dialog Context Representation Learning",
    author = "Mehri, Shikib  and
      Razumovskaia, Evgeniia  and
      Zhao, Tiancheng  and
      Eskenazi, Maxine",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1373",
    doi = "10.18653/v1/P19-1373",
    pages = "3836--3845",
    abstract = "This paper examines various unsupervised pretraining objectives for learning dialog context representations. Two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed. Further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability.",
}
@inproceedings{Shen2020DialogXLAX,
  title={DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition},
  author={Weizhou Shen and Junqing Chen and Xiaojun Quan and Zhixiang Xie},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:229211839}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{sun-etal-2019-utilizing,
    title = "Utilizing {BERT} for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
    author = "Sun, Chi  and
      Huang, Luyao  and
      Qiu, Xipeng",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1035",
    doi = "10.18653/v1/N19-1035",
    pages = "380--385",
    abstract = "Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets. The source codes are available at \url{https://github.com/HSLCY/ABSA-BERT-pair}.",
}

@inproceedings{Loshchilov2017DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:53592270}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{fleisskappa1971,
author = {Fleiss, Joseph},
year = {1971},
month = {11},
pages = {378-},
title = {Measuring Nominal Scale Agreement Among Many Raters},
volume = {76},
journal = {Psychological Bulletin},
doi = {10.1037/h0031619}
}