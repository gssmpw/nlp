\section{Related Work}
We discuss two areas of research relevant to our work: adversarial correctness of probabilistic data structures, and LSM store benchmarking and optimization.

\subsubsection*{Adversarial Data Structures.} There have been many recent efforts to rigorously define a game-based~\cite{naor_eylon,naor_oved,hayder_2024,clayton_2019} and simulator-based~\cite{filic_2022,filic_2024,hayder_2025} adversarial model for probabilistic data structures such as the Bloom Filter and the Learned Bloom Filter. In particular, the work of Naor et. al.~\cite{naor_eylon,naor_oved} was the first to show provably secure constructions for the Bloom Filter. There is also prior work showing feasible attacks on probabilistic data structures including the Bloom Filter~\cite{gerbet_2015} and the Learned Bloom Filter~\cite{reviriego_2021}. 
Our work builds upon these insights by applying them to LSM stores and designing countermeasures tailored to real-world storage systems. 

\subsubsection*{LSM Store Benchmarking / Optimization.} KVBench~\cite{zhu_et_al_2024} is one of many works that focus on benchmarking workloads for LSM stores. Prior work on LSM store optimizations includes ~\cite{monkey,thakkar_2024,spooky,huynh_et_al_2022}. Monkey~\cite{monkey} focuses on optimizing the memory budget allocation of the Bloom Filters used in LSM stores for better query performance. Huynch et. al.~\cite{huynh_et_al_2022} use Large Language Models (LLMs) to tune the design knobs of LSM stores. All prior work discussed primarily focuses on benchmarking and improving performance under non-adversarial workloads. To the best of our knowledge, this is the first paper to rigorously define an adversarial model for LSM stores and propose concrete, provably secure LSM store constructions.