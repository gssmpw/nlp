@inproceedings{
shazeer2017,
title={ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=B1ckMDqlg}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@inproceedings{oldfield2024mumoe,
    title={Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization},
    author={James Oldfield and Markos Georgopoulos and Grigorios G. Chrysos and Christos Tzelepis and Yannis Panagakis and Mihalis A. Nicolaou and Jiankang Deng and Ioannis Patras},
    booktitle={Thirty-eighth Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=bIa03mAtxQ}
}

@ARTICLE{10335740,
  author={Du, Zhenbang and Peng, Ruimin and Liu, Wenzhong and Li, Wei and Wu, Dongrui},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Mixture of Experts for EEG-Based Seizure Subtype Classification}, 
  year={2023},
  volume={31},
  number={},
  pages={4781-4789},
  keywords={Feature extraction;Electroencephalography;Brain modeling;Routing;Training;Manuals;Adaptation models;EEG;mixture of experts;seizure subtype classification;class imbalance},
  doi={10.1109/TNSRE.2023.3337802}}

@article{fedus2022review,
  title={A review of sparse expert models in deep learning},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  year={2022}
}

@article{zoph2022st,
  title={St-moe: Designing stable and transferable sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022}
}

@INPROCEEDINGS{7178876,
  author={Mitra, Vikramjit and Shriberg, Elizabeth and Vergyri, Dimitra and Knoth, Bruce and Salomon, Ronald M.},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Cross-corpus depression prediction from speech}, 
  year={2015},
  volume={},
  number={},
  pages={4769-4773},
  keywords={Speech;Interviews;Feature extraction;Cepstral analysis;Correlation;Robustness;depression detection;cross-corpus modeling;mental health;acoustic features;prosodic features;articulatory features;phonetic features;AVEC Challenge},
  doi={10.1109/ICASSP.2015.7178876}}


@inproceedings{ilias24_interspeech,
  title     = {A Cross-Attention Layer coupled with Multimodal Fusion Methods for Recognizing Depression from Spontaneous Speech},
  author    = {Loukas Ilias and Dimitris Askounis},
  year      = {2024},
  booktitle = {Interspeech 2024},
  pages     = {912--916},
  doi       = {10.21437/Interspeech.2024-188},
  issn      = {2958-1796},
}

@inproceedings{tao23_interspeech,
  title     = {The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection},
  author    = {Fuxiang Tao and Anna Esposito and Alessandro Vinciarelli},
  year      = {2023},
  booktitle = {INTERSPEECH 2023},
  pages     = {4149--4153},
  doi       = {10.21437/Interspeech.2023-894},
  issn      = {2958-1796},
}

@ARTICLE{6797059,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}}

@inproceedings{escobargrisales23_interspeech,
  title     = {An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients},
  author    = {Daniel Escobar-Grisales and Tomás Arias-Vergara and Cristian David Ríos-Urrego and Elmar Nöth and Adolfo M. García and Juan Rafael Orozco-Arroyave},
  year      = {2023},
  booktitle = {INTERSPEECH 2023},
  pages     = {1703--1707},
  doi       = {10.21437/Interspeech.2023-2287},
  issn      = {2958-1796},
}

@inproceedings{laquatra24_interspeech,
  title     = {Exploiting Foundation Models and Speech Enhancement for Parkinson's Disease Detection from Speech in Real-World Operative Conditions},
  author    = {Moreno {La Quatra} and Maria Francesca Turco and Torbjørn Svendsen and Giampiero Salvi and Juan Rafael Orozco-Arroyave and Sabato Marco Siniscalchi},
  year      = {2024},
  booktitle = {Interspeech 2024},
  pages     = {1405--1409},
  doi       = {10.21437/Interspeech.2024-522},
  issn      = {2958-1796},
}

@article{zhao2016tensor,
  title={Tensor ring decomposition},
  author={Zhao, Qibin and Zhou, Guoxu and Xie, Shengli and Zhang, Liqing and Cichocki, Andrzej},
  journal={arXiv preprint arXiv:1606.05535},
  year={2016}
}

@INPROCEEDINGS{6639130,
  author={Alghowinem, Sharifa and Goecke, Roland and Wagner, Michael and Epps, Julien and Breakspear, Michael and Parker, Gordon},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Detecting depression: A comparison between spontaneous and read speech}, 
  year={2013},
  volume={},
  number={},
  pages={7547-7551},
  keywords={Speech;Speech recognition;Feature extraction;Mel frequency cepstral coefficient;Jitter;Support vector machines;Mood detection;clinical depression;voice feature classification;affective sensing},
  doi={10.1109/ICASSP.2013.6639130}}


@inproceedings{tao20_interspeech,
  title     = {Spotting the Traces of Depression in Read Speech: An Approach Based on Computational Paralinguistics and Social Signal Processing},
  author    = {Fuxiang Tao and Anna Esposito and Alessandro Vinciarelli},
  year      = {2020},
  booktitle = {Interspeech 2020},
  pages     = {1828--1832},
  doi       = {10.21437/Interspeech.2020-2888},
  issn      = {2958-1796},
}

@article{hitchcock1927expression,
  title={The expression of a tensor or a polyadic as a sum of products},
  author={Hitchcock, Frank L},
  journal={Journal of Mathematics and Physics},
  volume={6},
  number={1-4},
  pages={164--189},
  year={1927},
  publisher={Wiley Online Library}
}

@INPROCEEDINGS{9508549,
  author={Huo, Zepeng and Zhang, Lida and Khera, Rohan and Huang, Shuai and Qian, Xiaoning and Wang, Zhangyang and Mortazavi, Bobak J.},
  booktitle={2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)}, 
  title={Sparse Gated Mixture-of-Experts to Separate and Interpret Patient Heterogeneity in EHR data}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  keywords={Training;Precision medicine;Conferences;Biological system modeling;Machine learning;Predictive models;Logic gates;Deep learning;electronic health record (EHR);mixture-of-experts},
  doi={10.1109/BHI50953.2021.9508549}}


@INPROCEEDINGS{716791,
  author={Jordan, M.I. and Jacobs, R.A.},
  booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)}, 
  title={Hierarchical mixtures of experts and the EM algorithm}, 
  year={1993},
  volume={2},
  number={},
  pages={1339-1344 vol.2},
  keywords={Machine learning algorithms;Surface fitting;Vectors;Supervised learning;Mars;Orbital robotics;Biological neural networks;Jacobian matrices;Psychology;Partitioning algorithms},
  doi={10.1109/IJCNN.1993.716791}}

@article{huang2024depression,
  title={Depression recognition using voice-based pre-training model},
  author={Huang, Xiangsheng and Wang, Fang and Gao, Yuan and Liao, Yilong and Zhang, Wenjing and Zhang, Li and Xu, Zhenrong},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={12734},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{GUPTA2025101710,
title = {Deep multi-task learning based detection of correlated mental disorders using audio modality},
journal = {Computer Speech \& Language},
volume = {89},
pages = {101710},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101710},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000937},
author = {Rohan Kumar Gupta and Rohit Sinha},
keywords = {Human–computer interaction, Correlated mental disorders, Hybrid deep learning model, Representation learning, Cross-corpus generalization},
abstract = {The existence of correlation among mental disorders is a well-known phenomenon. Multi-task learning (MTL) has been reported to yield enhanced detection performance of a targeted mental disorder by leveraging its correlation with other related mental disorders, mainly in textual and visual modalities. The validation of the same on audio modality is yet to be explored. In this study, we explore homogeneous and heterogeneous MTL paradigms for detecting two correlated mental disorders, namely major depressive disorder (MDD) and post-traumatic stress disorder (PTSD), on a publicly available audio dataset. The detection of both disorders is interchangeably employed as an auxiliary task when the other is the main task. In addition, a few other tasks are employed as auxiliary tasks. The results show that both MTL paradigms, implemented using two considered deep-learning models, outperformed the corresponding single-task learning (STL). The best relative improvement in the detection performance of MDD and PTSD is found to be 29.9% and 28.8%, respectively. Furthermore, we analyzed the cross-corpus generalization of MTL using two distinct datasets that involve MDD/PTSD instances. The results indicate that the generalizability of MTL is significantly superior to that of STL. The best relative increment in the cross-corpus generalization performance of MDD and PTSD detection is found to be 25.0% and 56.5%, respectively.}
}

@INPROCEEDINGS{10448231,
  author={Zuo, Lishi and Mak, Man-Wai and Tu, Youzhi},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Promoting Independence of Depression and Speaker Features for Speaker Disentanglement in Speech-Based Depression Detection}, 
  year={2024},
  volume={},
  number={},
  pages={10191-10195},
  keywords={Detectors;Interference;Signal processing;Depression;Feature extraction;Acoustics;Speech processing;Speaker disentanglement;depression detection;mutual information;speaker embedding},
  doi={10.1109/ICASSP48485.2024.10448231}}

@article{CUMMINS201510,
title = {A review of depression and suicide risk assessment using speech analysis},
journal = {Speech Communication},
volume = {71},
pages = {10-49},
year = {2015},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2015.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167639315000369},
author = {Nicholas Cummins and Stefan Scherer and Jarek Krajewski and Sebastian Schnieder and Julien Epps and Thomas F. Quatieri},
keywords = {Depression, Suicide, Automatic assessment, Behavioural markers, Paralinguistics, Classification},
abstract = {This paper is the first review into the automatic analysis of speech for use as an objective predictor of depression and suicidality. Both conditions are major public health concerns; depression has long been recognised as a prominent cause of disability and burden worldwide, whilst suicide is a misunderstood and complex course of death that strongly impacts the quality of life and mental health of the families and communities left behind. Despite this prevalence the diagnosis of depression and assessment of suicide risk, due to their complex clinical characterisations, are difficult tasks, nominally achieved by the categorical assessment of a set of specific symptoms. However many of the key symptoms of either condition, such as altered mood and motivation, are not physical in nature; therefore assigning a categorical score to them introduces a range of subjective biases to the diagnostic procedure. Due to these difficulties, research into finding a set of biological, physiological and behavioural markers to aid clinical assessment is gaining in popularity. This review starts by building the case for speech to be considered a key objective marker for both conditions; reviewing current diagnostic and assessment methods for depression and suicidality including key non-speech biological, physiological and behavioural markers and highlighting the expected cognitive and physiological changes associated with both conditions which affect speech production. We then review the key characteristics; size, associated clinical scores and collection paradigm, of active depressed and suicidal speech databases. The main focus of this paper is on how common paralinguistic speech characteristics are affected by depression and suicidality and the application of this information in classification and prediction systems. The paper concludes with an in-depth discussion on the key challenges – improving the generalisability through greater research collaboration and increased standardisation of data collection, and the mitigating unwanted sources of variability – that will shape the future research directions of this rapidly growing field of speech processing research.}
}

@article{DAS2024105898,
title = {A deep learning model for depression detection based on MFCC and CNN generated spectrogram features},
journal = {Biomedical Signal Processing and Control},
volume = {90},
pages = {105898},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105898},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423013319},
author = {Arnab Kumar Das and Ruchira Naskar},
keywords = {Audio signals, Convolutional neural network, Depression detection, Deep learning, Spectrogram, Glorot uniform},
abstract = {Depression is one of the leading forms of mental health issues encountered by individuals of diverse age groups today worldwide. Like any other mental health concerns, depression too poses diagnostic challenges for medical practitioners and clinical experts, given obvious social reservations and lack of awareness and acceptance in the society. Since long researchers have been looking for methods to identify symptoms of depression among individuals from their speech and responses, by utilizing automation systems and computers. In this paper, we propose an audio based depression detection method, which relies on neural networks for audio spectrogram based feature extraction as well as classification between speech/response patterns of depressed vs. non-depressed persons. We adopt a multi-modal approach in our work, by combining Mel-Frequency Cepstral Coefficients (MFCC) features, as well as Spectrogram features extracted from an audio file, by a novel CNN network. Our CNN model demonstrates optimized residual blocks and the “glorot uniform” kernel initializer. The proposed method’s performance is assessed in both multi-modal and multi-feature trials. We show our results on standard benchmark datasets DAIC-WOZ and MODMA, which provide repositories of questionnaire and patient responses, relevant in identification of depressive symptoms. We have also tested our model on standard emotion recognition audio dataset, RAVDESS. The proposed model achieves detection accuracy of over 90% in DAIC-WOZ and MODMA, and over 85% in RAVDESS, which is proven to surpass the present state-of-the-art.}
}

@article{DU2023299,
title = {Depression recognition using a proposed speech chain model fusing speech production and perception features},
journal = {Journal of Affective Disorders},
volume = {323},
pages = {299-308},
year = {2023},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2022.11.060},
url = {https://www.sciencedirect.com/science/article/pii/S0165032722013209},
author = {Minghao Du and Shuang Liu and Tao Wang and Wenquan Zhang and Yufeng Ke and Long Chen and Dong Ming},
keywords = {Depression, Deep learning, Audio, Feature fusion, Auxiliary diagnosis},
abstract = {Background
Increasing depression patients puts great pressure on clinical diagnosis. Audio-based diagnosis is a helpful auxiliary tool for early mass screening. However, current methods consider only speech perception features, ignoring patients' vocal tract changes, which may partly result in the poor recognition.
Methods
This work proposes a novel machine speech chain model for depression recognition (MSCDR) that can capture text-independent depressive speech representation from the speaker's mouth to the listener's ear to improve recognition performance. In the proposed MSCDR, linear predictive coding (LPC) and Mel-frequency cepstral coefficients (MFCC) features are extracted to describe the processes of speech generation and of speech perception, respectively. Then, a one-dimensional convolutional neural network and a long short-term memory network sequentially capture intra- and inter-segment dynamic depressive features for classification.
Results
We tested the MSCDR on two public datasets with different languages and paradigms, namely, the Distress Analysis Interview Corpus-Wizard of Oz and the Multi-modal Open Dataset for Mental-disorder Analysis. The accuracy of the MSCDR on the two datasets was 0.77 and 0.86, and the average F1 score was 0.75 and 0.86, which were better than the other existing methods. This improvement reveals the complementarity of speech production and perception features in carrying depressive information.
Limitations
The sample size was relatively small, which may limit the application in clinical translation to some extent.
Conclusion
This experiment proves the good generalization ability and superiority of the proposed MSCDR and suggests that the vocal tract changes in patients with depression deserve attention for audio-based depression diagnosis.}
}

@article{MUZAMMEL2020100005,
title = {AudVowelConsNet: A phoneme-level based deep CNN architecture for clinical depression diagnosis},
journal = {Machine Learning with Applications},
volume = {2},
pages = {100005},
year = {2020},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2020.100005},
url = {https://www.sciencedirect.com/science/article/pii/S2666827020300050},
author = {Muhammad Muzammel and Hanan Salam and Yann Hoffmann and Mohamed Chetouani and Alice Othmani},
keywords = {Major Depressive Disorder, Clinical depression detection, AI-based application, HCI-based healthcare, Speech depression recognition, Deep phoneme-level learning},
abstract = {Depression is a common and serious mood disorder that negatively affects the patient’s capacity of functioning normally in daily tasks. Speech is proven to be a vigorous tool in depression diagnosis. Research in psychiatry concentrated on performing fine-grained analysis on word-level speech components contributing to the manifestation of depression in speech and revealed significant variations at the phoneme-level in depressed speech. On the other hand, research in Machine Learning-based automatic recognition of depression from speech focused on the exploration of various acoustic features for the detection of depression and its severity level. Few have focused on incorporating phoneme-level speech components in automatic assessment systems. In this paper, we propose an Artificial Intelligence (AI) based application for clinical depression recognition and assessment from speech. We investigate the acoustic characteristics of phoneme units, specifically vowels and consonants for depression recognition via Deep Learning. We present and compare three spectrogram-based Deep Neural Network architectures, trained on phoneme consonant and vowel units and their fusion respectively. Our experiments show that the deep learned consonant-based acoustic characteristics lead to better recognition results than vowel-based ones. The fusion of vowel and consonant speech characteristics through a deep network significantly outperforms the single space networks as well as the state-of-art deep learning approaches on the DAIC-WOZ database.}
}

@article{kanter2008nature,
  title={The nature of clinical depression: Symptoms, syndromes, and behavior analysis},
  author={Kanter, Jonathan W and Busch, Andrew M and Weeks, Cristal E and Landes, Sara J},
  journal={The Behavior Analyst},
  volume={31},
  pages={1--21},
  year={2008},
  publisher={Springer}
}

@ARTICLE{10.3389/fnbot.2021.684037,

AUTHOR={Zhao, Yan  and Liang, Zhenlin  and Du, Jing  and Zhang, Li  and Liu, Chengyu  and Zhao, Li },

TITLE={Multi-Head Attention-Based Long Short-Term Memory for Depression Detection From Speech},

JOURNAL={Frontiers in Neurorobotics},

VOLUME={15},

YEAR={2021},

URL={https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2021.684037},

DOI={10.3389/fnbot.2021.684037},

ISSN={1662-5218},

ABSTRACT={<p>Depression is a mental disorder that threatens the health and normal life of people. Hence, it is essential to provide an effective way to detect depression. However, research on depression detection mainly focuses on utilizing different parallel features from audio, video, and text for performance enhancement regardless of making full usage of the inherent information from speech. To focus on more emotionally salient regions of depression speech, in this research, we propose a multi-head time-dimension attention-based long short-term memory (LSTM) model. We first extract frame-level features to store the original temporal relationship of a speech sequence and then analyze their difference between speeches of depression and those of health status. Then, we study the performance of various features and use a modified feature set as the input of the LSTM layer. Instead of using the output of the traditional LSTM, multi-head time-dimension attention is employed to obtain more key time information related to depression detection by projecting the output into different subspaces. The experimental results show the proposed model leads to improvements of 2.3 and 10.3% over the LSTM model on the Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ) and the Multi-modal Open Dataset for Mental-disorder Analysis (MODMA) corpus, respectively.</p>}}

@article{carroll1970analysis,
  title={Analysis of individual differences in multidimensional scaling via an N-way generalization of “Eckart-Young” decomposition},
  author={Carroll, J Douglas and Chang, Jih-Jie},
  journal={Psychometrika},
  volume={35},
  number={3},
  pages={283--319},
  year={1970},
  publisher={Springer}
}

@article{doi:10.1137/070690729,
author = {De Lathauwer, Lieven},
title = {Decompositions of a Higher-Order Tensor in Block Terms—Part II: Definitions and Uniqueness},
journal = {SIMAX},
volume = {30},
number = {3},
pages = {1033-1066},
year = {2008},
doi = {10.1137/070690729},

URL = { 
        https://doi.org/10.1137/070690729
    
},
eprint = { 
        https://doi.org/10.1137/070690729
    
}
,
    abstract = { In this paper we introduce a new class of tensor decompositions. Intuitively, we decompose a given tensor block into blocks of smaller size, where the size is characterized by a set of mode-n ranks. We study different types of such decompositions. For each type we derive conditions under which essential uniqueness is guaranteed. The parallel factor decomposition and Tucker's decomposition can be considered as special cases in the new framework. The paper sheds new light on fundamental aspects of tensor algebra. }
}

@article{Ben-younes_Cadene_Thome_Cord_2019, title={BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4818}, DOI={10.1609/aaai.v33i01.33018102}, abstractNote={&lt;p&gt;Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.&lt;/p&gt;}, number={01}, journal={AAAI}, author={Ben-younes, Hedi and Cadene, Remi and Thome, Nicolas and Cord, Matthieu}, year={2019}, month={Jul.}}

@inproceedings{mcfee2015librosa,
  title={librosa: Audio and music signal analysis in python},
  author={McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
  booktitle={Proceedings of the 14th python in science conference},
  volume={8},
  pages={18--25},
  year={2015}
}

@article{cai2024survey,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2407.06204},
  year={2024}
}

@inproceedings{
puigcerver2024from,
title={From Sparse to Soft Mixtures of Experts},
author={Joan Puigcerver and Carlos Riquelme Ruiz and Basil Mustafa and Neil Houlsby},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jxpsAj7ltE}
}

@inproceedings{peters-etal-2019-sparse,
    title = "Sparse Sequence-to-Sequence Models",
    author = "Peters, Ben  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1146/",
    doi = "10.18653/v1/P19-1146",
    pages = "1504--1519",
    abstract = "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of $\alpha$-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any $\alpha > 1$. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models."
}

@inproceedings{correia-etal-2019-adaptively,
    title = "Adaptively Sparse Transformers",
    author = "Correia, Gon{\c{c}}alo M.  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1223/",
    doi = "10.18653/v1/D19-1223",
    pages = "2174--2184",
    abstract = "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter {--} which controls the shape and sparsity of alpha-entmax {--} allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."
}

@inproceedings{gratch-etal-2014-distress,
    title = "The Distress Analysis Interview Corpus of human and computer interviews",
    author = "Gratch, Jonathan  and
      Artstein, Ron  and
      Lucas, Gale  and
      Stratou, Giota  and
      Scherer, Stefan  and
      Nazarian, Angela  and
      Wood, Rachel  and
      Boberg, Jill  and
      DeVault, David  and
      Marsella, Stacy  and
      Traum, David  and
      Rizzo, Skip  and
      Morency, Louis-Philippe",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}`14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L14-1421/",
    pages = "3123--3128",
    abstract = "The Distress Analysis Interview Corpus (DAIC) contains clinical interviews designed to support the diagnosis of psychological distress conditions such as anxiety, depression, and post traumatic stress disorder. The interviews are conducted by humans, human controlled agents and autonomous agents, and the participants include both distressed and non-distressed individuals. Data collected include audio and video recordings and extensive questionnaire responses; parts of the corpus have been transcribed and annotated for a variety of verbal and non-verbal features. The corpus has been used to support the creation of an automated interviewer agent, and for research on the automatic identification of psychological distress."
}
