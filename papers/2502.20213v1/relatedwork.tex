\section{Related Works}
\label{related_works}

In this section, we present a brief overview of existing depression recognition algorithms from speech and MoE settings. 

\subsection{Depression Recognition from Speech}

\subsubsection{Traditional Machine Learning Algorithms}

In \cite{tao20_interspeech}, the authors extracted a set of features from read speech, including energy (loudness), Mel-Frequency Cepstral Coefficients (MFCC), F0, and more, and trained a Support Vector Machine (SVM) classifier.

The study in \cite{6639130} utilized both read and spontaneous speech samples to recognize depression. A feature extraction approach was adopted by the authors followed by the train of an SVM classifier. Findings showed that spontaneous speech gave better results than read speech. Jitter, shimmer, loudness, and energy were proved robust features. 

\subsubsection{Deep Neural Networks}

The authors in \cite{ilias24_interspeech} introduced a DNN consisting of a cross-attention layer and multimodal fusion methods. They used both speech and transcripts as inputs to the proposed DNN. The authors investigated via a multi-task learning (MTL) setting if gender, age, and education level improve depression recognition performance. Findings showed that the single-task learning framework achieved better results than the multi-task learning one.

The authors in \cite{huang2024depression} utilized the DAIC-WOZ dataset \cite{gratch-etal-2014-distress} and fine-tuned a wav2vec2 pretrained model for recognizing depression. 

The authors in \cite{GUPTA2025101710} presented a multitask learning framework to recognize major depressive disorder and post-traumatic stress disorder (PTSD). In terms of the architecture, the authors used Mel-spectrograms and passed them through CNN layers followed by LSTMs. Findings suggested that MTL performed better than the single-task learning framework. 

The authors in \cite{10448231} presented a mutual information based approach to recognize depression. The aim of the study was to maximize depression information, while minimizing at the same time speaker information. Results demonstrated the effectiveness of the proposed approach. 

The study in \cite{DAS2024105898} extracted both MFCCs and spectrogram from audio files and trained a deep neural network based on CNNs for recognizing depression. 

Linear predictive coding and MFCC features were extracted in \cite{DU2023299}. Next, the authors trained a deep neural network consisting of CNNs and LSTMs. Results demonstrated the effectiveness of both production and perception features in depression recognition task.  

Phoneme-based features were used in \cite{MUZAMMEL2020100005}. Specifically, the authors used spectrograms of vowels and consonants as inputs to CNN models. Finally, a deep neural network based on the fusion of these models was trained. Results showed that the fusion of both networks yielded the highest evaluation results. 

A set of features, including F0, jitter, shimmer, loudness, MFCC, voicing probability, and more, were extracted by \cite{10.3389/fnbot.2021.684037}. The authors trained a deep learning model consisting of LSTMs and Multihead Attention layer. 

\subsection{Mixture of Experts}

The idea of Mixture of Experts was originally proposed in \cite{6797059} and is based on the divide-and-conquer approach. Instead of using a same single layer for the inputs, MoE models consist of expert layers and a routing (or gating) network. The expert layers are usually simply dense layers, while the routing network is responsible for determining which experts can be used for the input. Then, the outputs of each expert are aggregated through a weighted average. Multiple levels of hierarchy are also employed \cite{716791}. Many variations of MoE models have been proposed throughout the years. In \cite{shazeer2017}, the authors introduced the sparsely-gated MoE layer, which computes a weighted sum of the outputs from only the top-\textit{k} experts, rather than aggregating the outputs from all the experts. However, sparse MoEs have the limitations of training instability, parameter-inefficiencies, and non-differentiable nature \cite{puigcerver2024from}. To tackle these limitations, a recent study \cite{oldfield2024mumoe} introduces Multilinear MoE layers, namely $\mu MoE$.

MoE models have found extensive applications in several domains, including natural language processing, speech processing, computer vision, and so on \cite{fedus2022review, cai2024survey}. MoE models have been used in seizure subtype classification \cite{10335740}. Specifically, the authors in \cite{10335740} present two methods based on MoE models. In terms of the first method, the authors employ a DNN and extract a representation vector from electroencephalography (EEG) signals. Next, the method introduced in \cite{shazeer2017} is adopted. Regarding the second method, the authors extract manually a set of features from EEG signals apart from training only the DNN. In \cite{9508549}, the authors utilize electronic health records (EHR) in intensive care units (UCU). To deal with patient heterogeneity, the authors use MoE models based on the method described in \cite{shazeer2017}.