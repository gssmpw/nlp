\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage[dvipsnames,table]{xcolor}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Mixture of Experts for Recognizing Depression from Interview and Reading
Tasks}
\author{Loukas Ilias, Dimitris Askounis
        % <-this % stops a space
\thanks{L. Ilias and D. Askounis are with the Decision Support Systems Laboratory, School of Electrical and Computer Engineering, National Technical University of Athens, 15780 Athens, Greece (e-mail: lilias@epu.ntua.gr; askous@epu.ntua.gr). 
}}

\maketitle

\begin{abstract}
Depression is a mental disorder and can cause a variety of symptoms, including psychological, physical, and social. Speech has been proved an objective marker for the early recognition of depression. For this reason, many studies have been developed aiming to recognize depression through speech. However, existing methods rely on the usage of only the spontaneous speech neglecting information obtained via read speech, use transcripts which are often difficult to obtain (manual) or come with high word-error rates (automatic), and do not focus on input-conditional computation methods. To resolve these limitations, this is the first study in depression recognition task obtaining representations of both spontaneous and read speech, utilizing multimodal fusion methods, and employing Mixture of Experts (MoE) models in a single deep neural network. Specifically, we use audio files corresponding to both interview and reading tasks and convert each audio file into log-Mel spectrogram, delta, and delta-delta. Next, the image representations of the two tasks pass through shared AlexNet models. The outputs of the AlexNet models are given as input to a multimodal fusion method. The resulting vector is passed through a MoE module. In this study, we employ three variants of MoE, namely sparsely-gated MoE and multilinear MoE based on factorization. Findings suggest that our proposed approach yields an Accuracy and F1-score of 87.00\% and 86.66\% respectively on the Androids corpus.
\end{abstract}

\begin{IEEEkeywords}
depression, spontaneous speech, read speech, fusion, mixture of experts
\end{IEEEkeywords}

\section{Introduction}

Depression comes with a series of emotional and physical problems hindering the person's ability to carry out daily activities \cite{kanter2008nature}. According to the World Health Organization\footnote{https://www.who.int/news-room/fact-sheets/detail/depression}, approximately 3.8\% of the population suffers from depression, with suicide constituting the fourth leading cause of death in ages between 15 and 29 years old. The study in \cite{CUMMINS201510} shows that depression entails both cognitive and physiological changes affecting speech production processes. Therefore, speech constitutes a reliable biomarker for recognizing depression. Early recognition of depression is crucial for ensuring a better quality of life.

Existing studies \cite{ilias24_interspeech} use multimodal methods by combining transcripts and audio files. However, manual transcripts are not always available and thus automatic speech recognition systems are used. These systems often have high word-error rate in languages other than english and thus insert errors in machine learning models. Additionally, literature suggests that both read and spontaneous speech should be considered by clinical depression collections in their protocols \cite{7178876}. However, the majority of the existing studies utilize only spontaneous speech tasks, i.e., description of picture, interview. Traditional approaches obtain representation vectors of the input modalities and then use the same single layer for the batch of the representation vectors. On the other hand, Mixture of Experts (MoEs) have emerged as a powerful and effective approach for increasing evaluation performance by employing some experts.

In order to tackle these limitations, we present the first study, which uses both spontaneous and read speech, multimodal fusion methods, and MoE models into a single end-to-end trainable deep neural network (DNN). Specifically, we convert audio files of read and spontaneous speech into images of three channels, namely log-Mel spectrogram, velocity, acceleration. We pass these images through two shared AlexNet pretrained models (one model for each input). Next, the outputs of the AlexNet model are used as input to a multimodal fusion approach, namely BLOCK \cite{Ben-younes_Cadene_Thome_Cord_2019}, which generalizes both concepts of rank and mode ranks for tensors. Finally, we use three variants of MoE models, namely sparse MoEs \cite{shazeer2017} and two variants of multilinear MoEs based on factorization \cite{oldfield2024mumoe}. Experiments conducted on Androids corpus \cite{tao23_interspeech} show that our proposed approach obtains valuable advantages over state-of-the-art approaches.

Our main contributions can be summarized as follows:
\begin{itemize}
    \item We present a new method to recognize depression from read and spontaneous speech.
    \item We employ three variants of MoE layers and compare their performances.
    \item We perform a series of ablation experiments to verify the effectiveness of the introduced approach.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{related_works} presents an overview of the existing works for recognizing depression through speech and describes the concept of the MoE layers. Section~\ref{dataset} describes the dataset used for conducting our experiments. Section~\ref{methodology_section} presents the methodology adopted in this study for recognizing depression. Section~\ref{experiments_results} describes the experimental setup, the baselines used as a comparison with our study, the results, and a series of ablation experiments. Finally, Section~\ref{conclusion} describes the main results and limitations of our study, while it also presents some ideas for future work.

\section{Related Works} \label{related_works}

In this section, we present a brief overview of existing depression recognition algorithms from speech and MoE settings. 

\subsection{Depression Recognition from Speech}

\subsubsection{Traditional Machine Learning Algorithms}

In \cite{tao20_interspeech}, the authors extracted a set of features from read speech, including energy (loudness), Mel-Frequency Cepstral Coefficients (MFCC), F0, and more, and trained a Support Vector Machine (SVM) classifier.

The study in \cite{6639130} utilized both read and spontaneous speech samples to recognize depression. A feature extraction approach was adopted by the authors followed by the train of an SVM classifier. Findings showed that spontaneous speech gave better results than read speech. Jitter, shimmer, loudness, and energy were proved robust features. 

\subsubsection{Deep Neural Networks}

The authors in \cite{ilias24_interspeech} introduced a DNN consisting of a cross-attention layer and multimodal fusion methods. They used both speech and transcripts as inputs to the proposed DNN. The authors investigated via a multi-task learning (MTL) setting if gender, age, and education level improve depression recognition performance. Findings showed that the single-task learning framework achieved better results than the multi-task learning one.

The authors in \cite{huang2024depression} utilized the DAIC-WOZ dataset \cite{gratch-etal-2014-distress} and fine-tuned a wav2vec2 pretrained model for recognizing depression. 

The authors in \cite{GUPTA2025101710} presented a multitask learning framework to recognize major depressive disorder and post-traumatic stress disorder (PTSD). In terms of the architecture, the authors used Mel-spectrograms and passed them through CNN layers followed by LSTMs. Findings suggested that MTL performed better than the single-task learning framework. 

The authors in \cite{10448231} presented a mutual information based approach to recognize depression. The aim of the study was to maximize depression information, while minimizing at the same time speaker information. Results demonstrated the effectiveness of the proposed approach. 

The study in \cite{DAS2024105898} extracted both MFCCs and spectrogram from audio files and trained a deep neural network based on CNNs for recognizing depression. 

Linear predictive coding and MFCC features were extracted in \cite{DU2023299}. Next, the authors trained a deep neural network consisting of CNNs and LSTMs. Results demonstrated the effectiveness of both production and perception features in depression recognition task.  

Phoneme-based features were used in \cite{MUZAMMEL2020100005}. Specifically, the authors used spectrograms of vowels and consonants as inputs to CNN models. Finally, a deep neural network based on the fusion of these models was trained. Results showed that the fusion of both networks yielded the highest evaluation results. 

A set of features, including F0, jitter, shimmer, loudness, MFCC, voicing probability, and more, were extracted by \cite{10.3389/fnbot.2021.684037}. The authors trained a deep learning model consisting of LSTMs and Multihead Attention layer. 

\subsection{Mixture of Experts}

The idea of Mixture of Experts was originally proposed in \cite{6797059} and is based on the divide-and-conquer approach. Instead of using a same single layer for the inputs, MoE models consist of expert layers and a routing (or gating) network. The expert layers are usually simply dense layers, while the routing network is responsible for determining which experts can be used for the input. Then, the outputs of each expert are aggregated through a weighted average. Multiple levels of hierarchy are also employed \cite{716791}. Many variations of MoE models have been proposed throughout the years. In \cite{shazeer2017}, the authors introduced the sparsely-gated MoE layer, which computes a weighted sum of the outputs from only the top-\textit{k} experts, rather than aggregating the outputs from all the experts. However, sparse MoEs have the limitations of training instability, parameter-inefficiencies, and non-differentiable nature \cite{puigcerver2024from}. To tackle these limitations, a recent study \cite{oldfield2024mumoe} introduces Multilinear MoE layers, namely $\mu MoE$.

MoE models have found extensive applications in several domains, including natural language processing, speech processing, computer vision, and so on \cite{fedus2022review, cai2024survey}. MoE models have been used in seizure subtype classification \cite{10335740}. Specifically, the authors in \cite{10335740} present two methods based on MoE models. In terms of the first method, the authors employ a DNN and extract a representation vector from electroencephalography (EEG) signals. Next, the method introduced in \cite{shazeer2017} is adopted. Regarding the second method, the authors extract manually a set of features from EEG signals apart from training only the DNN. In \cite{9508549}, the authors utilize electronic health records (EHR) in intensive care units (UCU). To deal with patient heterogeneity, the authors use MoE models based on the method described in \cite{shazeer2017}.


\section{Dataset} \label{dataset}

We use the Androids Corpus \cite{tao23_interspeech} to conduct our experiments. The Androids corpus consists of a reading and an interview task. We use both tasks in this study. In terms of the interview task, the person is asked to answer to some questions about everyday life. Interview task corresponds to a spontaneous speech task. Regarding the reading task, persons are asked to read a short fairy tale written by Aesop, namely "The Wind of the North and the Sun". Androids corpus includes 52 participants in the control group and 58 people in the depression group. The authors of \cite{tao23_interspeech} have conducted a chi-squared test, which reveals that there is no difference between the control group and the depression one in terms of gender and education level. Similarly, results of a two-tailed t-test demonstrated that there is no difference in terms of age distribution. Therefore, speech differences are attributable only to the speech pathologies.

\section{Methodology} \label{methodology_section}

In this section, we describe our proposed methodology for recognizing depression through read and spontaneous speech. In Fig.~\ref{methodology}, our proposed methodology is illustrated.

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{Methodology_1.png}
    \caption{Our proposed Methodology. Speech signals corresponding to interview and reading tasks are inputs to the deep neural network. These signals are transformed into log-Mel spectrogram, delta, and delta-delta. Next, they are passed through two pretrained shared AlexNet models. The outputs are then given as input to BLOCK (multimodal fusion method). The output vector is then passed through the Mixture of Experts module. Finally, an output layer with two units is used to differentiate healthy control from depression.}
    \label{methodology}
\end{figure*}

\subsection{Speech Processing}

        \subsubsection{Reading Task}
        
        Each audio file is converted into an image of three channels, namely log-Mel spectrogram, its velocity ($\Delta$), and its acceleration ($\Delta\Delta$). Specifically, we use \textit{librosa} \cite{mcfee2015librosa}. For obtaining the log-Mel spectrogram, we use 224 Mel bands, hop length accounting for 512, and a Hanning window. Each image is resized to $(224 \times 224)$ pixels. Let $f^{read}$ be the image corresponding to the reading task.
        \subsubsection{Interview Task}
        
        We adopt the same methodology to obtain an image per audio file. Let $f^{interview}$ be the image corresponding to the interview task.
        \subsubsection{Deep Neural Network}
        Both $f^{read}$ and $f^{interview}$ are passed through two pretrained AlexNet models sharing the same weights \cite{krizhevsky2014one}. We modify the last layer of the AlexNet model, so as to ensure that the output of the AlexNet model has a dimensionality of 768d. 

        In terms of the interview task, let $f^{interview} _{AlexNet} \in \mathbb{R}^{768}$ denote the output of the AlexNet pretrained model.

        With regards to the reading task, let $f^{read} _{AlexNet} \in \mathbb{R}^{768}$ denote the output of the pretrained AlexNet model. 

\subsection{Fusion}

We pass both $f^{interview} _{AlexNet}$ and $f^{read} _{AlexNet}$ through the BLOCK fusion introduced in \cite{Ben-younes_Cadene_Thome_Cord_2019}. Specifically, this multimodal fusion method is based on the block-term tensor decomposition \cite{doi:10.1137/070690729, carroll1970analysis}. Let the output of this component be $f_{fusion} \in \mathbb{R}^d$, where $d=768$.

\subsection{Mixture of Experts -- Output}

We pass $f_{fusion}$ through MoE models, which are described in detail below:
    
\subsubsection{Sparse MoE}

The output of Sparse MoE is denoted as $y$. Let $G(x)_i$ denote the gating (router) network, while $E_i (x)$ indicates the output of the $ith$ expert. Specifically, $G(x)_i$ indicates the weight assigned to each expert by the gating network. $n$ denotes the number of experts.

\begin{equation}
            y = \sum_{i=1}^n G(x)_i E_i (x)
        \end{equation}

        In our experiments, we use two-layer MLPs as expert networks.

        Next, we describe the method for obtaining the gating coefficients. Before applying the softmax activation function to get the coefficients (see Eq.~\ref{equation_coefficients}), we add noise to the input (see Eq.~\ref{equation_noise}) and keep only the top-\textit{k} values (see Eq.~\ref{keep_topk}). Adding noise facilitates load balancing among the experts, while keeping only the top-\textit{k} most relevant experts saves computation, since only a few of the experts are activated via a gating network.

        \begin{equation}
            G(x) = Softmax\left(KeepTopK\left(H\left(x\right),k\right)\right)
            \label{equation_coefficients}
        \end{equation}

        \begin{multline}
            H(x)_i = (x \cdot W_g)_i + \\ StandardNormal() \cdot Softplus \left(\left(x \cdot W_{noise} \right)_i \right)
            \label{equation_noise}
        \end{multline}
        , where $StandardNormal()$ indicates standard normal distribution.


        \begin{equation} 
        \resizebox{\linewidth}{!}{$
        \text{KeepTopK}(v, k)_i = \begin{cases} v_i & \text{if } v_i \text{ is in the top } k \text{ elements of } v, \\ 
            -\infty & \text{otherwise.}
            \end{cases}$}
            \label{keep_topk}
        \end{equation}

        Next, we describe the loss function, which is minimized in this study.

        \begin{itemize}
            \item $L_{imp}$: This loss is used for ensuring uniform gating weights for all experts. In this way, the phenomenon of producing large weights to specific experts is avoided.

        This loss is equal to the square of the coefficient of variation of the set of importance values and is defined via Equations~\ref{first_equation_imp}-\ref{last_equation_imp}.
        
        \begin{equation}
            L_{\text{Imp}}(\mathcal{B}) = \text{CV} \left( \left\{ \text{Imp}_i(\mathcal{X}) \right\}_{i=1}^n \right)^2
            \label{first_equation_imp}
        \end{equation}, 
        where 
        \begin{equation}
            \text{CV}(\cdot) = \frac{\text{Std}(\cdot)}{\text{Mean}(\cdot)},
        \end{equation}

        \begin{equation}
            \text{Imp}_i(\mathcal{X}) = \sum_{x \in \mathcal{X}} G(x)
            \label{last_equation_imp}
        \end{equation}
        \item $L_{Load}$: This loss function ensures balanced loads across all experts, i.e., each expert receives equal number of training samples. 
        \begin{equation}
            L_{\text{Load}}(\mathcal{B}) = \text{CV} \left( \left\{ \text{Load}_i(\mathcal{X}) \right\}_{i=1}^n \right)^2
        \end{equation}

        \begin{equation}
            \text{Load}_i(\mathcal{X}) = \sum_{x \in \mathcal{X}} P_i(x)
        \end{equation}
        , where $Load_i$ denotes the number of training examples per expert and $P_i(x)$ denotes the probability that $G(x)_i$ is non-zero.
        \begin{equation}
            P_i(x) = \Phi \left ( \frac{\left(xW_G \right)_i - kth\_excluding \left(H (x),k, i \right)}{Softplus \left(\left (xW_{noise}\right)_i \right)} \right)
        \end{equation},
        where $kth\_excluding \left(H (x),k, i \right)$ is the $k$th highest component of $H$ excluding the $i$th component. $\Phi$ is the cumulative distribution function of the standard normal distribution.

        Overall, we minimize the following loss function:
        \begin{equation}
            L = L_{cross\_entropy} + \alpha \cdot (L_{imp} + L_{load})
            \label{overall_loss_function_sparse_moes}
        \end{equation}
        , where $\alpha$ is a hyperparameter.

        \end{itemize}

\subsubsection{The $\mu MoE$ layer}

The $\mu MoE$ layer consists of $N$ experts and is parameterized by a weight tensor $W \in \mathbb{R}^{N \times I \times O}$ and an expert gating parameter $\mathbf{G} \in \mathbb{R}^{I \times N}$, where $I$ and $O$ indicate the input and output dimensions respectively. Let $z \in \mathbb{R}^{I}$ denote the input vector. Its forward pass can be obtained via the following equations:

\begin{equation}
    \mathbf{a} = \phi(\mathbf{G}^\top \mathbf{z}) \in \mathbb{R}^N
\end{equation}
, where $\mathbf{a}$ is the vector of expert coefficients and $\phi$ denotes the entmax activation \cite{peters-etal-2019-sparse, correia-etal-2019-adaptively}. 
\begin{equation}
    \mathbf{y} = \mathcal{W} \times_1 \mathbf{a} \times_2 \mathbf{z} = \sum_{n=1}^N \sum_{i=1}^I \mathbf{w}_{ni} z_i a_n \in \mathbb{R}^O
\end{equation}
, where $y$ represents the output vector. 

The $\mu MoE$ layer can be interpreted as computing a sparse, convex combination of $N$ affine transformations of the input vector $\mathbf{z}$, with the coefficients provided by $\mathbf{a}$. In the forward pass, the first tensor contraction, denoted as $\sum_i \mathbf{W}_{:i:} z_i \in \mathbb{R}^{N \times O}$, performs a matrix multiplication between the input vector and the weight matrices of all experts. The subsequent tensor contraction with the expert coefficients $\mathbf{a}$ combines these results linearly, producing the output vector.

Below, we mention two tensor factorization methods employed in this study.

        \paragraph{\textbf{CP$\mu$MoE.}}
        
        This method is based on the multilinear MoE layer ($\mu MoE$) \cite{oldfield2024mumoe}. 
        
        CP$\mu$MoE relies on a CP decomposition \cite{carroll1970analysis, hitchcock1927expression} of the weight tensor with a rank of $R$. Specifically, the weight matrix $W$ can be rewritten as a sum of $R$ outer products with factor matrices $\mathbf{U}^{(1)} \in \mathbb{R}^{R \times N}, \mathbf{U}^{(2)} \in \mathbb{R}^{R \times I}, \mathbf{U}^{(3)} \in \mathbb{R}^{R \times O}$.
        
        \begin{equation}
        W=\sum_{r=1}^R \mathbf{u}_r^{(1)} \circ \mathbf{u}_r^{(2)} \circ \mathbf{u}_r^{(3)} \in \mathbb{R}^{N \times I \times O}
        \end{equation}
         
        Therefore, CP$\mu$MoE reduces the parameters from $NIO$ to $R(N+I+O)$, where $R$ denotes the rank.
        
        We set $O=128$. Finally, we use a dense layer of two units to get the final prediction. We minimize the cross-entropy loss function.
        
        \paragraph{\textbf{TR$\mu$MoE.}}
        
        Similar to CP$\mu$MoE, this method is based on the $\mu MoE$ layer. 
        
        TR$\mu$MoE relies on the Tensor Ring (TR) Decomposition \cite{zhao2016tensor} and reduces the parameters to $ (R1NR2 + R2IR3 + R3OR1)$. Specifically, in TR format, the weight matrix $W \in \mathbb{R}^{N \times I \times O}$ has three factor tensors: $\mathbf{U}^{(1)} \in \mathbb{R}^{R_1 \times N \times R_2}, \mathbf{U}^{(2)} \in \mathbb{R}^{R_2 \times I \times R_3}, \mathbf{U}^{(3)} \in \mathbb{R}^{R_3 \times O \times R_1} $.

        We set $O=128$. Finally, we use a dense layer of two units to get the final prediction. We minimize the cross-entropy loss function.


\section{Experiments and Results} \label{experiments_results}

\subsection{Baselines}
We compare our approach with the following baselines, since these works have performed their experiments on the Androids corpus.

\begin{itemize}
    \item \textit{Silences \cite{tao20_interspeech}}: This method extracts a set of features, including the number and total length of silences, and trains an SVM classifier.
    \item \textit{Only speech \cite{ilias24_interspeech}}: This method converts audio into log-Mel spectrogram, delta, and delta-delta and finetunes a pretrained AlexNet model.
    \item \textit{BS1 \cite{tao23_interspeech}}: Features per window frame are extracted and used to train an SVM classifier.
    \item \textit{BS2 \cite{tao23_interspeech}}: Features per window frame are extracted and used to train an LSTM layer, while the final prediction is obtained via majority vote.
\end{itemize}

\subsection{Experimental Setup}

In terms of the MoE layer, we use 4 experts and keep the 3 most relevant ones. We set $\alpha$ of Eq.~\ref{overall_loss_function_sparse_moes} equal to 0.1. Regarding $CP\mu MoE$ and $TR\mu MoE$, we set: $I=768$, $O=128$, and $N=3$. In terms of $CP\mu MoE$, we set $R=4$. With regards to $TR\mu MoE$, we set $R1=R2=R3=4$. We use a learning rate of \texttt{1e-4}. We use the Adam optimizer. We train our proposed models for 30 epochs with a batch size of 8. Experiments are conducted on a 5-fold cross-validation setting. Experiments are ran four times. Experiments are conducted on a NVIDIA A100 80GB PCIe GPU.

\subsection{Evaluation Metrics}

Accuracy, Precision, Recall, and F1-score have been used to evaluate the results of our proposed approach. We report the mean and standard deviation of these metrics over four runs in a 5-fold cross-validation setting.

\subsection{Results}

Results of our proposed methodology are reported in Table~\ref{performance_comparison}. As one can observe, $TR\mu MoE$ is our best performing model outperforming the rest of our introduced approaches in Accuracy by 1.75-3.13\%, in Recall by 2.19-3.47\%, in Precision by 0.99-2.75\%, and in F1-score by 2.43-2.74\%. Differences in performance between $TR\mu MoE$ and $CP\mu MoE$ are attributable to the factorization method used. Specifically, it is shown that Tensor Ring Factorization is a more powerful method than CP decomposition in our task. We also observe that Sparsely-Gated MoE layer presents lower evaluation results than both $TR\mu MoE$ and $CP\mu MoE$. We speculate that this difference is attributable to the inherent limitations of sparse MoE layers, including training instability, non-differentiable nature, and parameter-inefficiency. Values of standard deviations are in alignment with existing literature \cite{escobargrisales23_interspeech,laquatra24_interspeech} and are attributable to the limited datasets used. In comparison with baselines, we observe that our best performing model surpasses these approaches in terms of Accuracy by 2.50-13.70\%, Recall by 1.00-12.60\%, F1-score by 1.96-13.06\%, and Precision by 1.00-13.30\%. These differences demonstrate the advantages of combining multimodal fusion methods and MoE layers into a single DNN.


\begin{table*}[htbp]

\centering
\caption{Performance comparison among proposed models and baselines. Best results per evaluation metric are in bold.}
\begin{tabular}{lccccc}
\toprule
\multicolumn{1}{l}{}&\multicolumn{5}{c}{\textbf{Evaluation metrics}}\\
\cline{2-6} 
\multicolumn{1}{l}{\textbf{Architecture}}&\textbf{Precision}&\textbf{Recall}&\textbf{F1-score}&\textbf{Accuracy}&\textbf{Specificity}\\
\midrule
\multicolumn{6}{>{\columncolor[gray]{.8}}l}{\textbf{Comparison with state-of-the-art}} \\
\textit{Silences \cite{tao20_interspeech}} & 84.50 & 84.60 & 84.55 & 84.50 & - \\ \hline
\textit{Only speech \cite{ilias24_interspeech}} & 80.73 & 85.70 & 82.49 & 80.52 & 74.21 \\ \hline
\textit{BS1 \cite{tao23_interspeech}} & 73.50 & 74.50 & 73.60 & 73.30 & - \\ \hline
\textit{BS2 \cite{tao23_interspeech}} & 85.80 & 86.10 & 84.70 & 83.90 & - \\ 
\midrule
\multicolumn{6}{>{\columncolor[gray]{.8}}l}{\textbf{Introduced Approaches}} \\
\textit{Sparsely-Gated MoE Layer} & 84.05 & 84.91 & 83.92 & 83.87 & 81.10 \\ & $\pm$11.81 & $\pm$8.37 & $\pm$7.82 & $\pm$7.76 & $\pm$13.22 \\ \hline
\textit{CP$\mu$MoE} & 85.81 & 83.63 & 84.23 & 85.25 & \textbf{84.81} \\ 
& $\pm$10.79 & $\pm$12.47 & $\pm$9.82 & $\pm$8.80 & $\pm$10.51 \\ \hline
\textit{TR$\mu$MoE} & \textbf{86.80} & \textbf{87.10} & \textbf{86.66} & \textbf{87.00} & \textbf{84.81} \\
& $\pm$9.02 & $\pm$8.99 & $\pm$7.44 & $\pm$6.64 & $\pm$10.51 \\

\bottomrule
\end{tabular}
\label{performance_comparison}
\end{table*}

\subsection{Ablation Study}

In this section, we conduct a series of ablation experiments to prove the effectiveness of the proposed approach. Results are reported in Table~\ref{ablation_study}. 

Firstly, we use as input only the read speech and thus remove the multimodal fusion component. Results state that \textit{Only Read Speech} yields an Accuracy of 79.41\%, which corresponds to a decline of 7.59\% in comparison with our proposed framework. 

Secondly, we use only spontaneous speech. Findings show that an Accuracy and F1-score of 81.73\% and 82.14\% respectively are obtained. 

Thirdly, we use two AlexNet models without shared weights. Results show that an Accuracy and F1-score of 84.71\% and 84.93\% respectively are obtained. Thus, fine-tuning two pretrained AlexNet models is a complex task for our limited dataset. 

Next, we concatenate the representations obtained through read and spontaneous speech instead of using BLOCK fusion. A decline in Accuracy and F1-score by 1.92\% and 2.28\% respectively is observed compared to our introduced methodology. 

Next, we remove the MoE component and use a dense layer of 128 units. Results show that Accuracy and F1-score drop to 83.69\% and 84.03\% respectively.

\begin{table*}[htbp]
\centering
\caption{Ablation Study. Best results per evaluation metric are in bold.}
\begin{tabular}{lccccc}
\toprule
\multicolumn{1}{l}{}&\multicolumn{5}{c}{\textbf{Evaluation metrics}}\\
\cline{2-6} 
\multicolumn{1}{l}{\textbf{Architecture}}&\textbf{Precision}&\textbf{Recall}&\textbf{F1-score}&\textbf{Accuracy}&\textbf{Specificity}\\
\midrule
\multicolumn{6}{>{\columncolor[gray]{.8}}l}{\textbf{Ablation Experiments}} \\
\textit{Only read speech} & 79.06 & 79.98 & 78.87 & 79.41 & 78.82 \\ 
& $\pm$13.85 & $\pm$13.91 & $\pm$12.15 & $\pm$10.97 & $\pm$14.97 \\ \hline
\textit{Only spontaneous speech} & 81.25 & 84.72 & 82.14 & 81.73 & 77.68 \\ & $\pm$13.04 & $\pm$8.61 & $\pm$8.16 & $\pm$8.99 & $\pm$17.11 \\ \hline
\textit{Non-shared weights in AlexNet} & 84.50 & 86.81 & 84.93 & 84.71 & 80.29 \\ & $\pm$11.99 & $\pm$11.85 & $\pm$9.98 & $\pm$9.90 & $\pm$17.67 \\ \hline
\textit{Concatenation} & 86.51 & 83.84 & 84.38 & 85.08 & 86.12 \\ 
& $\pm$12.40 & $\pm$11.78 & $\pm$9.53 & $\pm$8.73 & $\pm$12.98 \\ \hline
\textit{Removal of MoE layer} & 84.81 & 84.80 & 84.03 & 83.69 & 81.98 \\ 
& $\pm$11.84 & $\pm$9.24 & $\pm$7.55 & $\pm$7.89 & $\pm$16.12 \\ \midrule
\multicolumn{6}{>{\columncolor[gray]{.8}}l}{\textbf{Proposed Methodology}} \\

\textit{\textbf{Our proposed methodology}} & \textbf{86.80} & \textbf{87.10} & \textbf{86.66} & \textbf{87.00} & \textbf{84.81} \\
& $\pm$9.02 & $\pm$8.99 & $\pm$7.44 & $\pm$6.64 & $\pm$10.51 \\
\bottomrule
\end{tabular}
\label{ablation_study}
\end{table*}

Finally, we modify the number of experts in terms of the MoE layer. Results are presented in Fig.~\ref{ablation_study_number_of_experts}. We observe that as the number of experts increases, accuracy decreases. We speculate that this difference in performance is attributable to the limited dataset used.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{test_accuracy_vs_experts_1.png}
    \caption{Test accuracy with respect to the number of experts}
    \label{ablation_study_number_of_experts}
\end{figure}


\section{Conclusion} \label{conclusion}

In this paper, we present the first study utilizing both spontaneous and read speech in the Italian language, multimodal fusion methods based on the block-term tensor decomposition, and MoE models (sparse, multilinear based on factorization) in a single neural network. Results show that multilinear MoE layers based on Tensor Ring Decomposition yielded the highest performance reaching Accuracy and F1-score up to 87\% and 86.66\% respectively. Results of an ablation study verified the effectiveness of the proposed approach. 

\textbf{Limitations:} We used one limited dataset consisting of 110 samples. Additionally, our approach depends on labelled datasets. However, obtaining large labelled datasets in healthcare domain is a challenging task due to privacy issues. 

\textbf{Future Work:} In the future, we aim to use self-supervised learning approaches and parameter-efficient fine-tuning strategies in conjunction with MoE variants.


\bibliographystyle{unsrt}  
\bibliography{references} 


\end{document}


