@ARTICLE{10.3389/fnbot.2021.684037,

AUTHOR={Zhao, Yan  and Liang, Zhenlin  and Du, Jing  and Zhang, Li  and Liu, Chengyu  and Zhao, Li },

TITLE={Multi-Head Attention-Based Long Short-Term Memory for Depression Detection From Speech},

JOURNAL={Frontiers in Neurorobotics},

VOLUME={15},

YEAR={2021},

URL={https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2021.684037},

DOI={10.3389/fnbot.2021.684037},

ISSN={1662-5218},

ABSTRACT={<p>Depression is a mental disorder that threatens the health and normal life of people. Hence, it is essential to provide an effective way to detect depression. However, research on depression detection mainly focuses on utilizing different parallel features from audio, video, and text for performance enhancement regardless of making full usage of the inherent information from speech. To focus on more emotionally salient regions of depression speech, in this research, we propose a multi-head time-dimension attention-based long short-term memory (LSTM) model. We first extract frame-level features to store the original temporal relationship of a speech sequence and then analyze their difference between speeches of depression and those of health status. Then, we study the performance of various features and use a modified feature set as the input of the LSTM layer. Instead of using the output of the traditional LSTM, multi-head time-dimension attention is employed to obtain more key time information related to depression detection by projecting the output into different subspaces. The experimental results show the proposed model leads to improvements of 2.3 and 10.3% over the LSTM model on the Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ) and the Multi-modal Open Dataset for Mental-disorder Analysis (MODMA) corpus, respectively.</p>}}

@ARTICLE{10335740,
  author={Du, Zhenbang and Peng, Ruimin and Liu, Wenzhong and Li, Wei and Wu, Dongrui},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Mixture of Experts for EEG-Based Seizure Subtype Classification}, 
  year={2023},
  volume={31},
  number={},
  pages={4781-4789},
  keywords={Feature extraction;Electroencephalography;Brain modeling;Routing;Training;Manuals;Adaptation models;EEG;mixture of experts;seizure subtype classification;class imbalance},
  doi={10.1109/TNSRE.2023.3337802}}

@INPROCEEDINGS{10448231,
  author={Zuo, Lishi and Mak, Man-Wai and Tu, Youzhi},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Promoting Independence of Depression and Speaker Features for Speaker Disentanglement in Speech-Based Depression Detection}, 
  year={2024},
  volume={},
  number={},
  pages={10191-10195},
  keywords={Detectors;Interference;Signal processing;Depression;Feature extraction;Acoustics;Speech processing;Speaker disentanglement;depression detection;mutual information;speaker embedding},
  doi={10.1109/ICASSP48485.2024.10448231}}

@INPROCEEDINGS{6639130,
  author={Alghowinem, Sharifa and Goecke, Roland and Wagner, Michael and Epps, Julien and Breakspear, Michael and Parker, Gordon},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Detecting depression: A comparison between spontaneous and read speech}, 
  year={2013},
  volume={},
  number={},
  pages={7547-7551},
  keywords={Speech;Speech recognition;Feature extraction;Mel frequency cepstral coefficient;Jitter;Support vector machines;Mood detection;clinical depression;voice feature classification;affective sensing},
  doi={10.1109/ICASSP.2013.6639130}}

@ARTICLE{6797059,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}}

@INPROCEEDINGS{716791,
  author={Jordan, M.I. and Jacobs, R.A.},
  booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)}, 
  title={Hierarchical mixtures of experts and the EM algorithm}, 
  year={1993},
  volume={2},
  number={},
  pages={1339-1344 vol.2},
  keywords={Machine learning algorithms;Surface fitting;Vectors;Supervised learning;Mars;Orbital robotics;Biological neural networks;Jacobian matrices;Psychology;Partitioning algorithms},
  doi={10.1109/IJCNN.1993.716791}}

@INPROCEEDINGS{9508549,
  author={Huo, Zepeng and Zhang, Lida and Khera, Rohan and Huang, Shuai and Qian, Xiaoning and Wang, Zhangyang and Mortazavi, Bobak J.},
  booktitle={2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)}, 
  title={Sparse Gated Mixture-of-Experts to Separate and Interpret Patient Heterogeneity in EHR data}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  keywords={Training;Precision medicine;Conferences;Biological system modeling;Machine learning;Predictive models;Logic gates;Deep learning;electronic health record (EHR);mixture-of-experts},
  doi={10.1109/BHI50953.2021.9508549}}

@article{DAS2024105898,
title = {A deep learning model for depression detection based on MFCC and CNN generated spectrogram features},
journal = {Biomedical Signal Processing and Control},
volume = {90},
pages = {105898},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105898},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423013319},
author = {Arnab Kumar Das and Ruchira Naskar},
keywords = {Audio signals, Convolutional neural network, Depression detection, Deep learning, Spectrogram, Glorot uniform},
abstract = {Depression is one of the leading forms of mental health issues encountered by individuals of diverse age groups today worldwide. Like any other mental health concerns, depression too poses diagnostic challenges for medical practitioners and clinical experts, given obvious social reservations and lack of awareness and acceptance in the society. Since long researchers have been looking for methods to identify symptoms of depression among individuals from their speech and responses, by utilizing automation systems and computers. In this paper, we propose an audio based depression detection method, which relies on neural networks for audio spectrogram based feature extraction as well as classification between speech/response patterns of depressed vs. non-depressed persons. We adopt a multi-modal approach in our work, by combining Mel-Frequency Cepstral Coefficients (MFCC) features, as well as Spectrogram features extracted from an audio file, by a novel CNN network. Our CNN model demonstrates optimized residual blocks and the “glorot uniform” kernel initializer. The proposed method’s performance is assessed in both multi-modal and multi-feature trials. We show our results on standard benchmark datasets DAIC-WOZ and MODMA, which provide repositories of questionnaire and patient responses, relevant in identification of depressive symptoms. We have also tested our model on standard emotion recognition audio dataset, RAVDESS. The proposed model achieves detection accuracy of over 90% in DAIC-WOZ and MODMA, and over 85% in RAVDESS, which is proven to surpass the present state-of-the-art.}
}

@article{DU2023299,
title = {Depression recognition using a proposed speech chain model fusing speech production and perception features},
journal = {Journal of Affective Disorders},
volume = {323},
pages = {299-308},
year = {2023},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2022.11.060},
url = {https://www.sciencedirect.com/science/article/pii/S0165032722013209},
author = {Minghao Du and Shuang Liu and Tao Wang and Wenquan Zhang and Yufeng Ke and Long Chen and Dong Ming},
keywords = {Depression, Deep learning, Audio, Feature fusion, Auxiliary diagnosis},
abstract = {Background
Increasing depression patients puts great pressure on clinical diagnosis. Audio-based diagnosis is a helpful auxiliary tool for early mass screening. However, current methods consider only speech perception features, ignoring patients' vocal tract changes, which may partly result in the poor recognition.
Methods
This work proposes a novel machine speech chain model for depression recognition (MSCDR) that can capture text-independent depressive speech representation from the speaker's mouth to the listener's ear to improve recognition performance. In the proposed MSCDR, linear predictive coding (LPC) and Mel-frequency cepstral coefficients (MFCC) features are extracted to describe the processes of speech generation and of speech perception, respectively. Then, a one-dimensional convolutional neural network and a long short-term memory network sequentially capture intra- and inter-segment dynamic depressive features for classification.
Results
We tested the MSCDR on two public datasets with different languages and paradigms, namely, the Distress Analysis Interview Corpus-Wizard of Oz and the Multi-modal Open Dataset for Mental-disorder Analysis. The accuracy of the MSCDR on the two datasets was 0.77 and 0.86, and the average F1 score was 0.75 and 0.86, which were better than the other existing methods. This improvement reveals the complementarity of speech production and perception features in carrying depressive information.
Limitations
The sample size was relatively small, which may limit the application in clinical translation to some extent.
Conclusion
This experiment proves the good generalization ability and superiority of the proposed MSCDR and suggests that the vocal tract changes in patients with depression deserve attention for audio-based depression diagnosis.}
}

@article{GUPTA2025101710,
title = {Deep multi-task learning based detection of correlated mental disorders using audio modality},
journal = {Computer Speech \& Language},
volume = {89},
pages = {101710},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101710},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000937},
author = {Rohan Kumar Gupta and Rohit Sinha},
keywords = {Human–computer interaction, Correlated mental disorders, Hybrid deep learning model, Representation learning, Cross-corpus generalization},
abstract = {The existence of correlation among mental disorders is a well-known phenomenon. Multi-task learning (MTL) has been reported to yield enhanced detection performance of a targeted mental disorder by leveraging its correlation with other related mental disorders, mainly in textual and visual modalities. The validation of the same on audio modality is yet to be explored. In this study, we explore homogeneous and heterogeneous MTL paradigms for detecting two correlated mental disorders, namely major depressive disorder (MDD) and post-traumatic stress disorder (PTSD), on a publicly available audio dataset. The detection of both disorders is interchangeably employed as an auxiliary task when the other is the main task. In addition, a few other tasks are employed as auxiliary tasks. The results show that both MTL paradigms, implemented using two considered deep-learning models, outperformed the corresponding single-task learning (STL). The best relative improvement in the detection performance of MDD and PTSD is found to be 29.9% and 28.8%, respectively. Furthermore, we analyzed the cross-corpus generalization of MTL using two distinct datasets that involve MDD/PTSD instances. The results indicate that the generalizability of MTL is significantly superior to that of STL. The best relative increment in the cross-corpus generalization performance of MDD and PTSD detection is found to be 25.0% and 56.5%, respectively.}
}

@article{MUZAMMEL2020100005,
title = {AudVowelConsNet: A phoneme-level based deep CNN architecture for clinical depression diagnosis},
journal = {Machine Learning with Applications},
volume = {2},
pages = {100005},
year = {2020},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2020.100005},
url = {https://www.sciencedirect.com/science/article/pii/S2666827020300050},
author = {Muhammad Muzammel and Hanan Salam and Yann Hoffmann and Mohamed Chetouani and Alice Othmani},
keywords = {Major Depressive Disorder, Clinical depression detection, AI-based application, HCI-based healthcare, Speech depression recognition, Deep phoneme-level learning},
abstract = {Depression is a common and serious mood disorder that negatively affects the patient’s capacity of functioning normally in daily tasks. Speech is proven to be a vigorous tool in depression diagnosis. Research in psychiatry concentrated on performing fine-grained analysis on word-level speech components contributing to the manifestation of depression in speech and revealed significant variations at the phoneme-level in depressed speech. On the other hand, research in Machine Learning-based automatic recognition of depression from speech focused on the exploration of various acoustic features for the detection of depression and its severity level. Few have focused on incorporating phoneme-level speech components in automatic assessment systems. In this paper, we propose an Artificial Intelligence (AI) based application for clinical depression recognition and assessment from speech. We investigate the acoustic characteristics of phoneme units, specifically vowels and consonants for depression recognition via Deep Learning. We present and compare three spectrogram-based Deep Neural Network architectures, trained on phoneme consonant and vowel units and their fusion respectively. Our experiments show that the deep learned consonant-based acoustic characteristics lead to better recognition results than vowel-based ones. The fusion of vowel and consonant speech characteristics through a deep network significantly outperforms the single space networks as well as the state-of-art deep learning approaches on the DAIC-WOZ database.}
}

@article{cai2024survey,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2407.06204},
  year={2024}
}

@article{fedus2022review,
  title={A review of sparse expert models in deep learning},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  year={2022}
}

@inproceedings{gratch-etal-2014-distress,
    title = "The Distress Analysis Interview Corpus of human and computer interviews",
    author = "Gratch, Jonathan  and
      Artstein, Ron  and
      Lucas, Gale  and
      Stratou, Giota  and
      Scherer, Stefan  and
      Nazarian, Angela  and
      Wood, Rachel  and
      Boberg, Jill  and
      DeVault, David  and
      Marsella, Stacy  and
      Traum, David  and
      Rizzo, Skip  and
      Morency, Louis-Philippe",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}`14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L14-1421/",
    pages = "3123--3128",
    abstract = "The Distress Analysis Interview Corpus (DAIC) contains clinical interviews designed to support the diagnosis of psychological distress conditions such as anxiety, depression, and post traumatic stress disorder. The interviews are conducted by humans, human controlled agents and autonomous agents, and the participants include both distressed and non-distressed individuals. Data collected include audio and video recordings and extensive questionnaire responses; parts of the corpus have been transcribed and annotated for a variety of verbal and non-verbal features. The corpus has been used to support the creation of an automated interviewer agent, and for research on the automatic identification of psychological distress."
}

@article{huang2024depression,
  title={Depression recognition using voice-based pre-training model},
  author={Huang, Xiangsheng and Wang, Fang and Gao, Yuan and Liao, Yilong and Zhang, Wenjing and Zhang, Li and Xu, Zhenrong},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={12734},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{ilias24_interspeech,
  title     = {A Cross-Attention Layer coupled with Multimodal Fusion Methods for Recognizing Depression from Spontaneous Speech},
  author    = {Loukas Ilias and Dimitris Askounis},
  year      = {2024},
  booktitle = {Interspeech 2024},
  pages     = {912--916},
  doi       = {10.21437/Interspeech.2024-188},
  issn      = {2958-1796},
}

@inproceedings{oldfield2024mumoe,
    title={Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization},
    author={James Oldfield and Markos Georgopoulos and Grigorios G. Chrysos and Christos Tzelepis and Yannis Panagakis and Mihalis A. Nicolaou and Jiankang Deng and Ioannis Patras},
    booktitle={Thirty-eighth Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=bIa03mAtxQ}
}

@inproceedings{tao20_interspeech,
  title     = {Spotting the Traces of Depression in Read Speech: An Approach Based on Computational Paralinguistics and Social Signal Processing},
  author    = {Fuxiang Tao and Anna Esposito and Alessandro Vinciarelli},
  year      = {2020},
  booktitle = {Interspeech 2020},
  pages     = {1828--1832},
  doi       = {10.21437/Interspeech.2020-2888},
  issn      = {2958-1796},
}

