PPI \citep{Angelopoulos2023} was introduced to obtain shorter CIs for the parameters of interest by leveraging machine learning predictions in semi-supervised settings. PPI has since been extended in multiple directions. PPI\texttt{++} \citep{Angelopoulos2023a} proposes a different, loss-based formulation of PPI, leading to a more computationally efficient procedure, along with an additional power-tuning parameter to enhance PPI's performance. Stratified PPI \citep{Fisch2024} improves upon PPI by employing a data stratification strategy. Cross PPI \citep{Zrnic2024a} demonstrates how the training of $f$ can be included in the PPI pipeline. Active statistical inference \citep{Zrnic2024} applies an active learning approach to select which inputs from the unlabelled set should be labelled. Closer to our work, Bayesian PPI \citep{Hofer2024} considers an alternative PPI estimator motivated by Bayesian ideas. However, their approach provides Bayesian credible intervals, which do not offer frequentist guarantees. Additionally, their approach achieves similar experimental performance to PPI, while we demonstrate that FAB-PPI may significantly improve upon PPI. \looseness=-1

As discussed in \citet{Angelopoulos2023,Angelopoulos2023a}, PPI has close ties with control variates for variance reduction \citep[\S4.1]{Glasserman2003}. In the case of mean estimation, the form of the PPI estimator is similar to the one proposed by \citet{Zhang2019}. PPI is also related to work in semiparametric inference with missing data \citep{Robins1995}.

The concept of Bayes-optimal confidence regions originates from the work of \citet{Pratt1961,Pratt1963}. Pratt's approach, which has been given the name FAB by \citet{Yu2018}, has since been extended in multiple directions \citep{Brown1995,Farchione2008,Kabaila2013,Kabaila2022,Yu2018,Hoff2019,Hoff2023}. In particular, \citet{Cortinovis2024} show that, when combined with priors with power-law tails, FAB provides robust confidence regions that revert to classical ones in the presence of outliers. \citet{Hoff2023} applied FAB in a predictive supervised context, showing that it can lead to more accurate predictions than standard methods.
