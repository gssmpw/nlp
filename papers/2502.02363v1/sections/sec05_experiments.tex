We compare FAB-PPI and power-tuned FAB-PPI (FAB-PPI\texttt{++}) to classical inference, PPI and power-tuned PPI (PPI\texttt{++}) on both synthetic and real estimation problems.
For FAB-PPI, we use $\text{(HS)}$ and $\text{(N)}$ to indicate the use of the horseshoe and Gaussian priors defined in \cref{sec:fabppi_priors}.
As already mentioned, PPI is motivated by settings in which labelled data are scarce, while unlabelled data are abundant. Moreover, the application of FAB to PPI specifically targets the estimation of the rectifier $\Delta_\theta$.
For these reasons, we choose to focus on cases where $N \gg n$ is large enough to rule out any uncertainty on the measure of fit $m_\theta$, which we estimate using the sample mean $\widehat{m}_\theta$ \eqref{eq:msamplemean}. As a result of this, given a $1 - \delta$ confidence interval (FAB or not) $\mathcal{R}_\delta$ for $\Delta_\theta$, the corresponding $1 - \alpha$ CI for $\theta^\star$ is obtained simply by setting $\delta = \alpha$ and shifting $\mathcal{R}_\delta$ by $\widehat{m}_\theta$. This simplification allows us to evaluate the direct effect of FAB on the procedure, eliminating concerns about the loss of tightness in the CI on $\theta^*$ due to the Minkowski sum in \cref{eq:fabppiconfidenceinterval}. In all experiments, we check empirically that $N$ is large enough to make this assumption by monitoring the coverage of the resulting intervals against both the nominal level $1 - \alpha$ and the coverage of PPI intervals that also consider the uncertainty on $m_\theta$ (denoted with PPI (full) and PPI\texttt{++} (full) in \cref{app:additional_results}). \looseness=-1

\subsection{Synthetic data}\label{sec:synthetic_data}
The simulated experiments below have a common structure. We sample two datasets, $n$ labelled observations $\{(X_i,Y_i)\}_{i=1}^n$ iid from $\mathbb{P}$ and $N$ unlabelled observations $\{\widetilde{X}_i\}_{i=1}^N$ iid from $\mathbb{P}_X$. We use a prediction rule $f$ to obtain predictions $\{f(X_i)\}_{i=1}^n$ and $\{f(\widetilde{X}_i)\}_{i=1}^N$. We apply the different procedures to obtain estimates and $1 - \alpha$ confidence regions for the mean $\theta^* = \mathbb{E}[Y]$. For all experiments, we set $\alpha = 0.1$ and report the average mean squared error (MSE), interval volume, and coverage over $1000$ repetitions.

\paragraph{Biased predictions.}
We sample $X_i \overset{iid}{\sim} \mathcal{N}(0,1)$ and $Y_i = X_i + \epsilon_i$ with $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, 1)$, so that $\theta^* = \mathbb{E}[Y] = 0$.
The prediction rule is defined as $f(X_i) = X_i + \gamma$, where $\gamma \in \mathbb{R}$.
For this choice, the bias of $f$ is controlled by $\gamma$, since $\text{MSE}(f) = \gamma^2 + 1$.
For this experiment, we assume that $N$ is infinite, set $n = 200$, and vary $\gamma$ between $-1.5$ and $1.5$.
\cref{fig:simul_bias_width_main} shows the average interval volume as a function of $\gamma$ for classical inference, PPI\texttt{++}, and FAB-PPI\texttt{++} with both a horseshoe and a Gaussian prior.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\textwidth]{./figures/simul_bias_width_main.pdf}
    \caption{Biased predictions study. The panel shows the average CI volume as the bias level $\gamma$ varies.}
    \label{fig:simul_bias_width_main}
\end{figure}
Results for the non power-tuned methods, as well as MSE and coverage plots, are reported in \cref{fig:simul_bias_supplementary}.
Except for the version with the Gaussian prior, all the PPI procedures outperform classical inference for every bias level $\gamma$, but the behaviour exhibited by PPI\texttt{++} deserves attention, as its CI volume is approximately constant across values of $\gamma$.
This is due to the fact that, since $N$ is taken to be infinite and $n$ is fairly large, $\widehat\lambda \approx \cov(Y, f(X)) = 1$ and the rectifier is accurately estimated with similar variance across all values of $\gamma$.
On the other hand, the CI volume for the FAB-PPI methods varies greatly with $\gamma$. When the bias is small ($\gamma \simeq 0$), the observed rectifier has a value close to $0$, leading to smaller CIs.
As the bias increases, the volume of the confidence intervals grows, until it surpasses that of the PPI intervals.
At this point, the two FAB-PPI procedures behave differently: the volume of the Gaussian intervals grows unbounded, whereas the horseshoe intervals eventually revert to the PPI ones.
This example clearly shows that FAB-PPI with a horseshoe prior allows to obtain smaller CIs when the predictions are good, while ensuring robustness as the quality of the predictions decreases (\cref{prop:fabppi_robustness}). \looseness=-1

\paragraph{Noisy predictions.}
We consider the mean estimation example of \citet[\S7.1.1]{Angelopoulos2023a}, which does not involve any covariate $X$.
We sample $Y_i \overset{iid}{\sim} \mathcal{N}(0, 1)$, so that $\theta^* = \mathbb{E}[Y] = 0$.
The prediction rule is defined as $f(X_i) = Y_i + \sigma_Y \epsilon_i$, where $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0,1)$ and $\sigma_Y$ is successively set to $0.1$, $1$, and $2$.
For this experiment, we set $N = 10^6$ and vary $n$ from $100$ to $1000$.
\cref{fig:simul_noisy_width_main} shows the average interval volume as a function of $n$ for the different methods as the noise level $\sigma_Y$ varies, while similar plots for the MSE and coverage are reported in \cref{fig:simul_noisy_supplementary}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.45\textwidth]{./figures/simul_noisy_width_main.pdf}
    \caption{Noisy predictions study. The left, middle and right panels show the average CI volume for noise levels $\sigma_Y = 0.1, 1, 2$.}
    \label{fig:simul_noisy_width_main}
\end{figure}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/real_main.pdf}
    \caption{Real data mean estimation study. The left, middle, and right panels correspond to the \textsc{alphafold}, \textsc{galaxies}, and \textsc{forest} datasets. The top, middle, and bottom rows show average MSE, CI volume, and CI coverage over $1000$ repetitions for $\alpha = 0.1$.}
    \label{fig:real_main}
\end{figure*}
In this case, the effect of power tuning matches the observations of \citet{Angelopoulos2023a}: as the noise level increases, $\hat{\lambda}$ decreases and less weight is given to the predicted labels.
When the noise is small, all PPI procedures perform similarly, and much better than classical inference.
When the noise is large, the power-tuned procedures perform similar to or better than classical inference, whereas the non-tuned alternatives lose ground.
At the intermediate noise level, the power-tuned methods clearly outperform the other baselines.
Crucially, FAB-PPI outperforms the PPI counterpart at all noise levels, with FAB-PPI\texttt{++} being the best performer overall. This is because, in this setting, while predictions exhibit increasing variance with $\sigma_Y$, they remain unbiased. As a result of this, regardless of the value of $\lambda$ used, any additional shrinkage performed on the rectifier by FAB-PPI is beneficial. This example shows that FAB-PPI\texttt{++} retains the benefits of power tuning, while also taking advantage of the adaptive shrinkage provided by the FAB procedure.

\subsection{Real data}\label{sec:real_data}
We consider several estimation experiments using the datasets presented in \citet{Angelopoulos2023} and briefly described in \cref{app:datasets}. Each dataset comes with covariate/label/prediction triples $\{X_i, Y_i, f(X_i)\}_{i=1}^N$, which we randomly split into two subsets with $n$ labelled and $N - n$ unlabelled observations, for varying values of $n$. For all experiments and methods, we report the average estimation MSE, CI volume and coverage across multiple repetitions.

We begin with three mean estimation experiments, involving the \textsc{alphafold}, \textsc{forest}, and \textsc{galaxies} datasets, and one logistic regression experiment on the \textsc{healthcare} dataset. \Cref{fig:real_main} shows the results for classical inference, PPI\texttt{++}, and FAB-PPI\texttt{++} applied to the mean estimation datasets. The mean estimation results for the non power-tuned methods are reported in \cref{fig:real_supplementary}, whereas the ones for the logistic regression experiment are reported in \cref{fig:real_logistic_supplementary}. In all cases, FAB-PPI/FAB-PPI\texttt{++} outperform classical inference and the corresponding PPI methods, both in terms of MSE and CI volume, while achieving comparable coverage.
These examples suggest that the quality of the predictions of existing machine learning models on several real datasets may fall into the regime where the adaptive shrinkage provided by the FAB framework leads to a further improvement over standard PPI. In these settings, as the predictions are good, FAB-PPI under the horseshoe and Gaussian priors exhibit similar gains, as already seen in \cref{fig:simul_bias_width_main}.

However, the same is not true in the presence of bad predictions. For instance, \cref{fig:real_quantile_supplementary} shows the results of a quantile estimation experiment on the \textsc{genes} dataset, where predictions are heavily biased.
In this case, the behaviour of the FAB-PPI methods under the horseshoe and Gaussian priors differs significantly: the former matches the performance of the PPI methods, which outperform classical inference, whereas the latter leads to much larger MSE and CIs.
As previously discussed, such desirable behaviour of FAB-PPI under the horseshoe prior is due to its robustness against large bias levels (\cref{prop:fabppi_robustness}). 
Similarly, \cref{fig:real_ols_supplementary} reports the results of a linear regression experiment on the \textsc{census} dataset.
For one of the two parameters considered (panel~(a)), FAB-PPI underperforms the alternatives under both priors for small $n$.
However, as $n$ increases, the performance under the horseshoe prior improves and eventually matches that of the PPI methods, while the Gaussian prior does not.
This example shows another facet of the horseshoe's robustness: even for moderate bias levels, as the available labelled sample size grows, disagreements between the prior and the data become apparent (i.e.~$\var(\widehat{\Delta}_\theta)$ decreases), eventually leading \cref{prop:fabppi_robustness} to kick in.
