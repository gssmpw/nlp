Statistical inference crucially relies on the availability of high-quality labelled data to draw actionable conclusions. As the scale of machine learning models keeps growing, their increasingly accurate predictions become a tempting alternative to labelled data in fields where the latter are traditionally scarce, such as proteomics \citep{Jumper2021}. However, blindly using potentially biased predictions as a surrogate for labelled data voids the statistical validity of the conclusions drawn. To address this, prediction-powered inference~\citep{Angelopoulos2023} provides a general framework for statistical inference in the presence of a large number of black-box predictions by combining them with a smaller number of labelled observations, which are used to \textit{correct} for the discrepancy between the predictions and the true labels. The estimators and confidence intervals (CIs) resulting from PPI are statistically valid regardless of the machine learning model used. Moreover, when the predictions are good, PPI results in more accurate estimates and shorter CIs than traditional methods that rely solely on labelled data. \looseness=-1

More formally, for an input/output pair $(X,Y)\sim \bbP=\bbP_X\times\bbP_{Y|X}$ and a convex loss function $\mathcal{L}_\theta(x,y)$, where $\theta\in\bbR^d$, we wish to estimate
\begin{align}
    \theta^\star=\underset{\theta\in\bbR^d}{\arg\min}~~\E[\mathcal{L}_\theta(X,Y)]. \label{eq:defthetastar1}
\end{align}
For instance, if $\mathcal{L}_\theta(x,y)=(\theta - y)^2/2$ is the squared loss, then $\theta^\star=\E[Y]$.
We assume that we have $n$ labelled observations $\{(X_i,Y_i)\}_{i=1}^n$ iid from $\bbP$ and $N$ unlabelled observations $\{\widetilde X_{i}\}_{i=1}^N$ iid from $\bbP_X$, which are also independent of the labelled data. The number of unlabelled observations is typically much larger than the number of labelled ones, $N\gg n$. Additionally, we are provided with a machine learning prediction rule $f$, that can be used to predict an output $f(x)$ at any input $x$. PPI aims to obtain an estimator $\widehat \theta$ and a $1-\alpha$ CI $\calC_\alpha^\pp$ for $\theta^\star$, which take advantage of $f$. Under mild assumption, $\theta^\star$ can be expressed as the solution to \looseness=-1
\begin{align}
    g_{\theta^\star}:=\E[\mathcal{L}_{\theta^\star}'(X,Y)]=0, \label{eq:defthetastar2}
\end{align}
where $\mathcal{L}_\theta'$ is a subgradient of $\mathcal{L}_\theta$ with respect to $\theta$. It is easy to see that the quantity above can be decomposed as $g_{\theta} = m_\theta + \Delta_\theta$, where 
\begin{align}
    m_\theta&:=\E[\mathcal{L}_{\theta}'(X,f(X))], \label{eq:m_theta} \\
    \Delta_\theta&:=\E[\mathcal{L}_{\theta}'(X,Y)-\mathcal{L}_{\theta}'(X,f(X))] \label{eq:Delta_theta}.
\end{align}
In this setting, $m_\theta$ represents a measure of fit of the predictor, whereas $\Delta_\theta$, called the \textit{rectifier}, accounts for the discrepancy between the predicted outputs $f(X)$ and the true outputs $Y$.
In the case of the squared loss, $\Delta_\theta=\E[f(X)-Y]$ does not depend on $\theta$, but this is not true in general.
By estimating the two quantities $m_\theta$ and $\Delta_\theta$, \citet{Angelopoulos2023a} derive an estimator and a CI for $\theta^\star$, which use both labelled and unlabelled data. The resulting CI is shorter than the classical confidence interval based solely on the labelled data when $N \gg n$ and $f$ is accurate because, in this case, $m_\theta$ can be estimated with low variance using the unlabelled data, while $\Delta_\theta$ is close to zero.

Standard PPI employs off-the-shelf estimation and CI procedures for $\Delta_\theta$, which do not take advantage of any prior knowledge on the quality of the machine learning model $f$. 
However, in many applications, we expect the latter's predictions to be
\begin{enuminline}
    \item usually very good, but
    \item sometimes prone to large errors and hallucinations.
\end{enuminline}
We propose to encode such an inductive bias with a horseshoe prior $\pi_\theta$ on $\Delta_\theta$~\citep{Carvalho2010}, which accommodates the aforementioned properties by exhibiting
\begin{enuminline}
    \item an infinitely tall spike at the origin, and
    \item Cauchy-like tails at infinity.
\end{enuminline}
In order to construct valid confidence regions for $\Delta_\theta$ using the horseshoe prior, we resort to the frequentist-assisted by Bayes (FAB) framework \citep{Pratt1961,Pratt1963,Yu2018}. 
This approach provides confidence regions such that their expected length is lower for rectifiers $\Delta_\theta$ that have high probability under $\pi_\theta$, and larger otherwise.
While the resulting confidence regions have exact coverage for any prior $\pi_\theta$, the horseshoe prior is particularly well-suited for PPI. Being concentrated around the origin, it produces shorter confidence regions when the predictions are good, i.e.~$||\Delta_\theta|| \simeq 0$.
At the same time, its heavy tails ensure robustness when the predictions are poor. Indeed, as shown by \citet{Cortinovis2024}, if $||\Delta_\theta||\gg0$, the FAB procedure with the horseshoe prior reverts to the traditional CI based on the sample mean.

In this work, we introduce FAB-PPI, a Bayes-assisted approach for PPI that encodes prior information on the quality of the machine learning predictions by specifying a prior for the rectifier $\Delta_\theta$. FAB-PPI is:
\begin{itemize}\setlength\itemsep{0em}
    \item Statistically valid, as its confidence regions have correct coverage for any choice of prior;
    \item Efficient, as its confidence regions have smaller expected length when the predictions are good;
    \item Robust, as it reverts to standard PPI when the predictions are poor, if the horseshoe prior is used;
    \item Modular, as it can be used in conjunction with power tuning \citep{Angelopoulos2023a}.
\end{itemize}

The remainder of the paper is organised as follows. \cref{sec:relatedwork} reviews related work. \cref{sec:background} provides background on control variates, PPI, and FAB confidence regions. \cref{sec:fab-ppi} describes our novel approach for PPI, called FAB-PPI. \cref{sec:experiments} demonstrates the benefits of FAB-PPI on synthetic and real data. Finally, \cref{sec:discussion} discusses limitations and further extensions of our approach.
