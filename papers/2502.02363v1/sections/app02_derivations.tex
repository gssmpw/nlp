
\subsection{FAB-PPI for mean estimation}\label{app:fabppi_squared_loss}
Here, we outline the steps to derive the FAB-PPI mean estimator presented in \cref{eq:mean_estimator_fabppi}, as well as the corresponding FAB-PPI confidence region.
The convex loss function that corresponds to estimating $\theta^* = \mathbb{E}[Y]$ is the squared loss $\mathcal{L}_\theta(x, y) = \frac{1}{2}\left(\theta - y\right)^2$. In this case, the subgradient of $\mathcal{L}_\theta$ with respect to $\theta$ is given by $\mathcal{L}_\theta'(x, y) = \theta - y$. As a result of this, the measure of fit $m_\theta$ and the rectifier $\Delta_\theta$ take the form
\begin{align*}
    m_\theta &= \mathbb{E}[\mathcal{L}_\theta'(X, f(X))] = \theta - \mathbb{E}[f(X)], \\
    \Delta_\theta &= \mathbb{E}[\mathcal{L}_\theta(X, Y) - \mathcal{L}_\theta'(X, f(X))] = \mathbb{E}[f(X) - Y].
\end{align*}
In particular, under the squared loss, the rectifier $\Delta_\theta$ does not depend on $\theta$, and we indicate this by dropping the subscript $\theta$ and writing $\Delta := \Delta_\theta$.

In order to apply FAB-PPI to this setting, we follow the steps outlined in \cref{sec:fab-ppi}. In particular, we use the sample mean of the unlabelled data \eqref{eq:msamplemean} as the estimator $\widehat{m}_\theta$ of $m_\theta$,
\begin{equation*}
    \widehat{m}_\theta = \frac{1}{N} \sum_{i=1}^N \mathcal{L}_\theta'(X_i, f(X_i)) = \theta - \frac{1}{N}\sum_{i=1}^N f(\tX_i),
\end{equation*}
and either the sample mean \eqref{eq:deltasamplemean} or the control variate estimator \eqref{eq:deltahatpowertuning} as the estimator $\widehat{\Delta}$ of $\Delta$, as in PPI and PPI\texttt{++}, respectively. To avoid repetitions, in this section we write $\widehat{\Delta}$ as the following general control variate estimator with tuning parameter $\lambda \in \mathbb{R}$,
\begin{align*}
    \widehat \Delta&=\frac{1}{n}\sum_{i=1}^n \left(\mathcal{L}_{\theta}'(X_i,Y_i)-\mathcal{L}_{\theta}'(X_i,f(X_i))\right) - (\lambda - 1) \left( \frac{1}{n}\left [\sum_{i=1}^n \mathcal{L}_{\theta}'(X_i,f(X_i))\right ]  -\widehat m_\theta\right) \\
    &= \frac{1}{n}\sum_{i=1}^n \left(\mathcal{L}_{\theta}'(X_i,Y_i)- \lambda \mathcal{L}_{\theta}'(X_i,f(X_i))\right) + (\lambda - 1) \widehat m_\theta \\
    &= -\overline Y + \lambda \left(\frac{1}{n}\sum_{i=1}^n f(X_i) - \frac{1}{N}\sum_{j=1}^N f(\tX_j)\right) + \frac{1}{N}\sum_{j=1}^N f(\tX_j).
\end{align*}
The sample mean estimator \eqref{eq:deltasamplemean} and the control variate estimator \eqref{eq:deltahatpowertuning} under the squared loss are recovered by setting $\lambda$ to $1$ and $\widehat\lambda$ \eqref{eq:lambdahat}, respectively. From this, the standard PPI mean estimators \eqref{eq:thetahatppi} and \eqref{eq:thetahatppipp} are obtained by solving the equation $\widehat m_\theta + \widehat \Delta = 0$ for $\theta$.

Instead, we first define the Bayes assisted estimator \eqref{eq:FABPPIestimatorDelta} under the chosen prior $\pi_0(\Delta; \tau_n)$,
\begin{equation*}
    \widehat\Delta^{\FABPPI} = \widehat\Delta + \widehat\sigma^2 \ell'\left(\widehat\Delta; \widehat\sigma, \tau_n\right),
\end{equation*}
where $\widehat\sigma^2$ is an estimator of $\var(\widehat\Delta)$ and $\ell_\theta'(z; \sigma, \tau)$ is the derivative of the log-marginal likelihood of a Gaussian likelihood model with mean $\Delta$ and variance $\sigma^2$ under the prior $\pi_0(\Delta, \tau)$. Then, the FAB-PPI mean estimator $\widehat\theta^{\FABPPI}$ under $\pi_0$ is given by the solution to the equation
\begin{equation*}
    \widehat m_\theta + \widehat\Delta^{\FABPPI} = 0
\end{equation*}
in $\theta$, that takes the form
\begin{align*}
    \widehat\theta^{\FABPPI} &= \overline Y - \lambda \left(\frac{1}{n}\sum_{i=1}^n f(X_i) - \frac{1}{N}\sum_{j=1}^N f(\tX_j)\right) - \widehat\sigma^2 \ell'\left(\widehat\Delta; \widehat\sigma, \tau_n\right),
\end{align*}
which matches the expression in \cref{eq:mean_estimator_fabppi}. Furthermore, by recognising that the first two terms in the above expression match \eqref{eq:thetahatppipp}, we can alternatively write the FAB-PPI mean estimator as
\begin{align*}
    \widehat\theta^{\FABPPI} = \widehat{\theta} - \widehat\sigma^2 \ell'\left(\widehat\Delta; \widehat\sigma, \tau_n\right),
\end{align*}
where $\widehat\theta$ is the corresponding standard PPI mean estimator.

Given $\alpha \in (0, 1)$, we construct the FAB-PPI confidence region $\calC_\alpha^{\FABPPI}$ for the mean as described in \cref{sec:fabppi_cr}. In particular, let $\mathcal{T}_{\alpha - \delta}(\widehat m_\theta)$ denote a standard $1 - (\alpha - \delta)$ confidence interval for $m_\theta$,
\begin{align*}
    \mathcal{T}_{\alpha - \delta}(\widehat m_\theta) = (\widehat m_\theta \pm \widehat\sigma^f z_{1 - (\alpha - \delta) / 2}) = \left(\theta - \widehat{\theta}^f \pm \widehat\sigma^f z_{1 - (\alpha - \delta) / 2}\right),
\end{align*}
where $\widehat\theta^f := \frac{1}{N}\sum_{i=1}^N f(\tX_i)$ for conciseness, and $(\widehat\sigma^f)^2$ is an estimator of $\var(\widehat m_\theta)$. Then, we apply the FAB framework under the prior $\pi_0(\Delta; \tau_n)$ to obtain a $1 - \delta$ confidence region for $\Delta$,
\begin{align*}
    \calR^{\FABPPI}_\delta(\widehat\Delta) = \text{FAB-CR}(\widehat\Delta; \pi_0(\cdot~;\tau_n), \widehat\sigma, \delta),
\end{align*}
where, again, $\widehat\sigma^2$ is an estimator of $\var(\widehat\Delta)$. Finally, to avoid making assumptions on the specific form of $\calR^{\FABPPI}_\delta(\widehat\Delta)$, we use $(\inf\calR^{\FABPPI}_\delta), \sup(\calR^{\FABPPI}_\delta)) \supseteq \calR^{\FABPPI}_\delta(\widehat\Delta)$ in the definition of $\calC_\alpha^{\FABPPI}$ \eqref{eq:fabppiconfidenceinterval} to obtain the FAB-PPI interval
\begin{align*}
    \calC_\alpha^{\FABPPI} &= \left\{\theta \mid 0\in \left(\theta - \widehat\theta^f \pm \widehat\sigma^f z_{1 - (\alpha - \delta) / 2}\right) + (\inf(\calR^{\FABPPI}_\delta), \sup(\calR^{\FABPPI}_\delta))\right\} \\
    &= \left\{\theta \mid 0\in \left(\theta - \widehat\theta^f - \widehat\sigma^f z_{1 - (\alpha - \delta) / 2} + \inf(\calR^{\FABPPI}_\delta), \theta - \widehat\theta^f + \widehat\sigma^f z_{1 - (\alpha - \delta) / 2} + \sup(\calR^{\FABPPI}_\delta)\right)\right\} \\
    &= \left(\widehat\theta^f - \widehat\sigma^f z_{1 - (\alpha - \delta) / 2} - \sup(\calR^{\FABPPI}_\delta), \widehat\theta^f + \widehat\sigma^f z_{1 - (\alpha - \delta) / 2} - \inf(\calR^{\FABPPI}_\delta)\right).
\end{align*}
\cref{algo:FABPPIMean} summarises the FAB-PPI approach under the squared loss, where $\widehat\xi$ is defined for notational convenience and the corresponding sample variances are used as $(\widehat\sigma^f)^2$ and $\widehat\sigma^2$.
\begin{algorithm}
    \caption{FAB-PPI for mean estimation}
    \label{algo:FABPPIMean}
    {\bfseries Input:} labelled $\{(X_i,Y_i)\}_{i=1}^n$, unlabelled $\{\tX_j\}_{j=1}^N$, predictor $f$, prior $\pi_0(\cdot~; \tau_n)$, error levels $\alpha$, $\delta$
    \begin{algorithmic}
        \STATE Set $\widehat\lambda=1$ (FAB-PPI) or estimate $\widehat\lambda$ from data (FAB-PPI\texttt{++}) using \cref{eq:lambdahat}
        \STATE $\widehat\theta^f \gets \frac{1}{N}\sum_{j=1}^N f(\tX_j)$
        \STATE $\widehat\xi \gets \frac{1}{n}\sum_{i=1}^n (\widehat\lambda f(X_i) - Y_i)$
        \STATE $\widehat \Delta \gets \widehat\xi  - (\widehat\lambda-1)\widehat\theta^f$
        \STATE $(\widehat\sigma^f)^2 \gets \frac{1}{N-1}\sum_{j=1}^N (f(\tX_j)-\widehat\theta^f)^2$
        \STATE $\widehat\sigma_\xi^2 = \frac{1}{n-1}\sum_{i=1}^n (\widehat\lambda f(X_i) - Y_i - \widehat\xi)^2$
        \STATE $\widehat\sigma^2 \gets \frac{1}{n}\widehat\sigma_\xi^2 + \frac{(\widehat\lambda-1)^2}{N}(\widehat\sigma^f)^2$
        \STATE $\calR^{\FABPPI}_\delta \gets \text{FAB-CR}(\widehat \Delta;\pi_0(\cdot~;\tau_n), \widehat\sigma,\delta)$
    \end{algorithmic}
    {\bfseries Outputs:} estimator $\widehat \theta^{\FABPPI} =  \widehat\theta^f - \widehat \Delta - \widehat\sigma^2 \ell'(\widehat\Delta; \widehat\sigma, \tau_n)$ and CR $\calC_\alpha^\FABPPI=(\widehat\theta^f-\sigma^f z_{1-(\alpha-\delta)/2}-\sup(\calR^{\FABPPI}_\delta),\widehat\theta^f+\sigma^f z_{1-(\alpha-\delta)/2}-\inf(\calR^{\FABPPI}_\delta))$
\end{algorithm}
