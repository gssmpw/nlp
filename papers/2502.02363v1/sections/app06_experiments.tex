\subsection{Experiments with synthetic data}
The complete results for the experiments discussed in \cref{sec:experiments} are presented here. The legend names for the figures are as in \cref{sec:experiments}.

\subsubsection{Biased predictions simulation study}\label{app:simul_bias_supplementary}
\cref{fig:simul_bias_supplementary} shows the average MSE, CI volume, and CI coverage as a function of the bias level $\gamma$ for the biased predictions study in \cref{sec:synthetic_data}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/simul_bias_supplementary.pdf}
    \caption{Full results for the biased predictions study. The left, middle, and right panels show the average MSE, CI volume, and CI coverage as the bias level $\gamma$ varies.}
    \label{fig:simul_bias_supplementary}
\end{figure}
Compared to \cref{fig:simul_bias_width_main}, we include results for the non power-tuned methods, as well as for the ones that take into account the uncertainty on the measure of fit $m_\theta$ (i.e.~PPI (full) and PPI\texttt{++} (full)). In this example, power-tuning does not play a significant role and the same conclusions as in \cref{sec:synthetic_data} hold. In particular, standard PPI induces shorter CIs than classical inference with constant volume across bias levels. On the other hand, FAB methods induce shorter CIs when the predictions are good. As the prediction bias increases, the volume of the FAB CIs with Gaussian prior grows unbounded, while the horseshoe prior eventually reverts to the PPI intervals. Furthermore, the coverage plot shows that the methods tested achieve similar coverage to the nominal level and to PPI (full) and PPI\texttt{++} (full).

\subsubsection{Noisy predictions simulation study}\label{app:simul_noisy_supplementary}
\cref{fig:simul_noisy_supplementary} shows the average MSE, CI volume, and CI coverage as a function of $n$ for the values of $\sigma_Y$ considered in the noisy predictions study of \cref{sec:synthetic_data}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/simul_noisy_supplementary.pdf}
    \caption{Full results for the noisy predictions study. The left, middle, and right panels correspond to noise levels $\sigma_Y = 0.1, 1, 2$, respectively. The top, middle, and bottom rows show average MSE, CI volume, and CI coverage, respectively.}
    \label{fig:simul_noisy_supplementary}
\end{figure}
Compared to \cref{fig:simul_noisy_width_main}, we include results for the methods that use the Gaussian prior (FAB-PPI (N) and FAB-PPI\texttt{++} (N)) and those that take into account the uncertainty on the measure of fit $m_\theta$ (i.e.~PPI (full) and PPI\texttt{++} (full)). Like the CI volume plots in the main text, the MSE plots clearly show the benefits of both power-tuning and adaptive shrinkage through the horseshoe prior: as $\sigma_Y$ increases, the power-tuned methods clearly outperform the standard alternatives, while shrinkage always helps compared to standard PPI because the predictions remain unbiased. In this case, the Gaussian prior performs similarly to the horseshoe as the prediction rule $f$ is unbiased. The coverage plots confirm that all methods achieve comparable coverage across noise levels.

\subsection{Experiments with real data}\label{app:real_supplementary}
\subsubsection{Mean estimation}\label{app:mean_estimation}
\paragraph{Full comparison.}
\cref{fig:real_supplementary} shows the average MSE, CI volume, and CI coverage as a function of $n$ for the three datasets considered in \cref{sec:real_data}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/real_supplementary.pdf}
    \caption{Full results for  mean estimation experiment on real data. The left, middle, and right panels correspond to the \textsc{alphafold}, \textsc{galaxies}, and \textsc{forest} datasets, respectively. The top, middle, and bottom rows show average MSE, CI volume, and CI coverage, respectively over $1000$ repetitions for $\alpha = 0.1$.}
    \label{fig:real_supplementary}
\end{figure}
Compared to \cref{fig:real_main}, we include results for the non power-tuned methods, as well as for the ones that take into account the uncertainty on the measure of fit $m_\theta$ (i.e.~PPI (full) and PPI\texttt{++} (full)). The results are consistent with those presented in \cref{sec:real_data}. In particular, FAB methods outperform the standard PPI alternatives and classical inference, while achieving comparable coverage. For the datasets and the values of $n$ considered, power-tuned methods perform similarly to the non-tuned ones. Among the FAB methods, the horseshoe and Gaussian priors achieve similar performance.

\paragraph{Example intervals.}
\cref{fig:real_intervals_supplementary} shows 10 randomly chosen intervals for the classical, PPI\texttt{++}, and FAB-PPI\texttt{++} methods for the three datasets considered in \cref{sec:real_data} and different choices of the number of labelled observations $n$.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/real_intervals_supplementary.pdf}
    \caption{Each subfigure includes 10 randomly chosen intervals for the classical, PPI\texttt{++} and FAB-PPI\texttt{++} methods. The left, middle, and right panels refer to the \textsc{alphafold}, \textsc{galaxies}, and \textsc{forest} datasets, respectively. The top, middle, and bottom rows refer correspond to different values of $n$.}
    \label{fig:real_intervals_supplementary}
\end{figure}

\paragraph{Varying the prior scale.}
We repeat the mean estimation experiment on the \textsc{forest} dataset while varying the scale of the horseshoe prior used for FAB-PPI\texttt{++} in \cref{app:mean_estimation}. In addition to the scale $\widehat{\sigma}$ used in the main text, we consider the sample independent scale $1 / \sqrt{n}$ and the data independent scale $1$. As already mentioned, the computation of the FAB-PPI confidence regions under a horseshoe prior with scale other than $\widehat{\sigma}$ involves numerical integration to compute the corresponding marginal likelihood.
\Cref{fig:real_forest_scaled_supplementary} shows the average MSE, CI volume, and CI coverage for each of these choices, as well as for classical inference and PPI\texttt{++}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/real_forest_scaled_supplementary.pdf}
    \caption{Mean estimation experiment on the \textsc{forest} dataset with varying horseshoe prior scale. The left, middle, and right panels show average MSE, CI volume, and CI coverage over $100$ repetitions for $\alpha = 0.1$.}
    \label{fig:real_forest_scaled_supplementary}
\end{figure}
While the scale $\widehat{\sigma}$ achieves the best performance, the other scales also provide shorter CIs than classical inference and PPI\texttt{++}. In particular, the sample independent scale $1 / \sqrt{n}$ results in good performance across all metrics without requiring the estimation of $\widehat{\sigma}$.

\subsubsection{Logistic regression}\label{app:logistic}
\cref{fig:real_logistic_supplementary} shows the average MSE, CI volume, and CI coverage as a function of $n$ for the logistic regression experiment on the \textsc{healthcare} dataset mentioned in \cref{sec:real_data}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/real_logistic_supplementary.pdf}
    \caption{Logistic regression experiment on the \textsc{healthcare} dataset. The left, middle, and right panels show average MSE, CI volume, and CI coverage over $1000$ repetitions for $\alpha = 0.1$.}
    \label{fig:real_logistic_supplementary}
\end{figure}
As mentioned in the main text, FAB methods outperform the standard PPI alternatives and classical inference, while achieving comparable coverage. Among the FAB methods, the horseshoe and Gaussian priors achieve similar performance.

\subsubsection{Quantile estimation}\label{app:quantile}
\cref{fig:real_quantile_supplementary} shows the average MSE, CI volume, and CI coverage as a function of $n$ for the quantile estimation experiment on the \textsc{genes} dataset mentioned in \cref{sec:real_data}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/real_quantile_supplementary.pdf}
    \caption{Quantile estimation experiment on the \textsc{genes} dataset. The left, middle, and right panels show average MSE, CI volume, and CI coverage over $100$ repetitions for $\alpha = 0.1$.}
    \label{fig:real_quantile_supplementary}
\end{figure}
The predictions contained in this dataset are highly biased and this is reflected in the performance of the FAB-PPI methods. In particular, the Gaussian prior underperforms both classical inference and standard PPI, while the horseshoe prior achieves similar performance to standard PPI thanks to its robustness against large bias levels.

\subsubsection{Linear regression}\label{app:ols}
\cref{fig:real_quantile_supplementary} shows the average MSE, CI volume, and CI coverage as a function of $n$ for the linear regression experiment on the \textsc{census} dataset mentioned in \cref{sec:real_data}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./figures/real_ols_supplementary.pdf}
    \caption{Linear regression experiment on the \textsc{census} dataset. The (a) and (b) panels correspond to the two covariates in the dataset. The left, middle, and right panels show average MSE, CI volume, and CI coverage over $1000$ repetitions for $\alpha = 0.1$.}
    \label{fig:real_ols_supplementary}
\end{figure}
More specifically, panel (a) and (b) corresponds to the OLS parameters associated with the \textit{age} and \textit{sex} covariates, respectively. On the one hand, FAB-PPI seems to perform well for the \textit{sex} covariate, with similar performance between the Gaussian and horseshoe priors, and slightly improved MSE and CI volume compared to classical inference and standard PPI. On the other hand, the performance of FAB-PPI for the \textit{age} covariate seems to be affected by bias in the dataset predictions. In particular, FAB-PPI under the Gaussian prior underperforms the alternatives for all $n$. On the other hand, while the horseshoe prior achieves worse performance than the other methods for small $n$, its performance improve as $n$ grows, and it eventually matches standard PPI. This suggests that, as $n$ increases and $\var(\widehat{\Delta}_\theta)$ decreases, the observed value of the rectifier is increasingly considered as extreme, causing the influence from the horseshoe prior to eventually vanish thanks to its robustness to extreme bias levels.
