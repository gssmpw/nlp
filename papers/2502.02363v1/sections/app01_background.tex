\subsection{Horseshoe prior}
\label{sec:app:backgroundhorseshoe}
Consider the Gaussian likelihood model
$$
    Y\mid \beta\sim \Normal(\beta,\sigma^2)
$$
with standard deviation $\sigma>0$ and mean parameter $\beta\in\bbR$. The horseshoe prior~\citep{Carvalho2010} with density $\pi_\HS$ can be represented as a scale mixture of normals \citep{Andrews1974}
\begin{align}
    \beta\mid \nu^2 &\sim \Normal(0,\eta^2\sigma^2\nu^2)\\
    \nu &\sim C^+(0,1),
\end{align}
where $\eta>0$ and $C^+(0,1)$ is the half-Cauchy distribution with location parameter $0$ and scale parameter $1$. Throughout this section and the main text, we assume $\eta=1$. The rationale for this choice, along with a discussion of the general case $\eta\neq 1$, is provided at the end of this section.

The marginal likelihood is given by
\begin{align*}
    \pi(y)&=\int_{-\infty}^{\infty} \Normal(y\mid \beta,\sigma^2)\pi_\HS(\beta)d \beta\\
    &=\frac{1}{\sqrt{2\pi\sigma^2}} \int_0^\infty \frac{1}{\sqrt{1+\nu^2}}e^{-\frac{y^2}{2\sigma^2(1+\nu^2)}}p(\nu)d\nu\\
    &=\frac{2}{\pi\sqrt{2\pi\sigma^2}} \int_0^\infty e^{-\frac{y^2}{2\sigma^2(1+\nu^2)}}\frac{1}{(1+\nu^2)^{3/2}}d\nu.
\end{align*}
Using the change of variable $u=\frac{1}{1+\nu^2}$, we obtain
\begin{align*}
    \pi(y)&=\frac{1}{\pi\sqrt{2\pi\sigma^2}} \int_0^1 e^{-\frac{uy^2}{2\sigma^2}}(1-u)^{-1/2}du\\
    & = \frac{2}{\pi\sqrt{2\pi\sigma^2}} ~\OneFOne\left(1,\frac{3}{2},-\frac{y^2}{2\sigma^2}\right),
\end{align*}
where $\OneFOne$ is (Kummer's) confluent hypergeometric function of the first kind, with integral representation
\begin{align*}
    \OneFOne(a,b,z)=\frac{\Gamma(b)}{\Gamma(a)\Gamma(b-a)}\int_0^1 e^{zt}t^{a-1}(1-t)^{b-a-1}dt.
\end{align*}
Alternatively, the marginal can be expressed in function of the imaginary error function (erfi) or Dawson function (aka Dawson integral) as
\begin{align*}
    \pi(y)&=\frac{1}{\pi\sqrt{2\sigma^2}} e^{-y^2/(2\sigma^2)}\frac{\erfi(|y|/\sqrt{2\sigma^2})}{|y|/(\sqrt{2\sigma^2})}\\
    &=\frac{2}{\pi^{3/2}}\frac{1}{|y|}D\left( \frac{|y|}{\sqrt{2\sigma^2}}\right)
\end{align*}
where Dawson's function is defined as
$$
    D(z)=e^{-z^2}\int_0^z e^{t^2}dt.
$$ 
The marginal likelihood exhibits power-law tails
$$
    \pi(y)\sim C\frac{1}{|y|^2}\quad \text{as } |y|\to\infty
$$
for some constant $C>0$. Let $\ell(y)=\log\pi(y)$ denote the log-marginal likelihood. Kummer's function has the derivative
\begin{align*}
    \frac{d}{dz}\OneFOne(a,b,z)=\frac{a}{b}\OneFOne(a+1,b+1,z).
\end{align*}
It follows that
\begin{align*}
    \ell'(y) &= \frac{\pi'(y)}{\pi(y)}\\
    &=-\frac{2}{3}\frac{y}{\sigma^2}\frac{\OneFOne\left(2,\frac{5}{2},-\frac{y^2}{2\sigma^2}\right)}{\OneFOne\left(1,\frac{3}{2},-\frac{y^2}{2\sigma^2}\right)}.
\end{align*}
Applying Tweedie's formula \citep{}, we obtain the posterior mean
\begin{align}
    \E[\beta \mid y] &= y + \sigma^2 \ell'(y;\sigma)= (1-\kappa(y)) y,
\end{align}
where the shrinkage function $\kappa(y)\in(0,1)$ is given by
$$
    \kappa(y)=\frac{2}{3}\frac{\OneFOne\left(2,\frac{5}{2},-\frac{y^2}{2\sigma^2}\right)}{\OneFOne\left(1,\frac{3}{2},-\frac{y^2}{2\sigma^2}\right)}.
$$

Using the asymptotic expansion \citep[Chapter 4, Eq. (4.I.3)]{Slater1960}
$$
    \OneFOne(a,b,-z)\sim z^{-a}\frac{\Gamma(b)}{\Gamma(b-a)}
$$
as $z\to\infty$, we find
\begin{align*}
    \kappa(y)&\sim \frac{2\sigma^2}{y^2}\\
    |\bbE[\beta\mid y]-y|=\sigma^2|\ell'(y)|&\sim \frac{2\sigma^2}{|y|}
\end{align*}
as $|y|\to\infty$.

The horseshoe prior $\pi_\HS$ has two key properties: an infinite spike at zero, inducing strong shrinkage near $y=0$, and Cauchy-like tails, ensuring that strong signals remain largely unshrunk ($\kappa(y)\to 0$ and $|\bbE[\beta\mid y]-y|\to 0$  as $|y|\to\infty$). This is illustrated in \cref{fig:shrinkage_horseshoe}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=.35\textwidth]{./figures/shrinkage_horseshoe.pdf}
    \caption{Shrinkage function $\kappa(y)$ for the horseshoe prior when $\sigma^2=0.1$.}
    \label{fig:shrinkage_horseshoe}
\end{figure}

\begin{remark}[Parameterisation]
In this section and in the main text, we focused on the specific parametrisation $\eta=1$. For a general $\eta$, similar expressions can be derived for the marginal likelihood and posterior mean, replacing Kummer's $\OneFOne$ function with the more general degenerate hypergeometric function of two variables, $\Phi_1$ (see \citep[Equations (4) in the main text and (A1) in the appendix]{Carvalho2010}). While Kummer's $\OneFOne$ function is implemented in many standard scientific libraries, such as SciPy, $\Phi_1$ is not. Consequently, computing the marginal likelihood when $\eta\neq 1$ requires numerical integration. Since the evaluation of the marginal likelihood is crucial to our approach, it is therefore reasonable to set $\eta=1$ here.
\end{remark}

\subsection{FAB framework}
\label{sec:app:backgroundFAB}
In this section, we provide additional background on the FAB framework~\citep{Pratt1961,Pratt1963,Yu2018}.
Let $Y\mid \beta\sim\Normal(\beta,\sigma^2)$ with some prior $\pi_0(\beta)$. Denote by $\pi(y)=\int_\bbR p(y\mid \beta)\pi_0(\beta)d\beta$ the corresponding marginal likelihood.
For $\alpha\in(0,1)$, let $\calC_\alpha$ be the confidence procedure that solves the constrained optimisation problem
\begin{align*}
    &\calC_\alpha=\arg\min_{\widetilde \calC_\alpha} \E[\vol(\widetilde\calC_\alpha(Y))]\\
    \text{under the constraints }&\Pr(\beta\in \calC_\alpha(Y)\mid \beta=\beta')=1-\alpha \text{ for all fixed }\beta',
\end{align*}
where $\vol(\calC_\alpha(y))=\int_{\beta'\in\calC_\alpha(y)}d\beta'$ is the volume of $\calC_\alpha(y)$ and
\begin{equation}
    \E[\vol(\calC_\alpha(Y))] = \int_{\bbR} \vol(\calC_\alpha(y))\pi(y)dy
\end{equation}
is the expected volume under the marginal distribution $\pi(y)$. By the Ghosh-Pratt identity \citep{Ghosh1961,Pratt1961},
\begin{align*}
    \E[\vol(\calC_\alpha(Y))] &= \int_\bbR \vol(\calC_\alpha(y))\pi(y)dy\\
    &=\int_\bbR \int_\bbR 1_{\beta'\in \calC_\alpha(y)} d\beta'\pi(y)dy\\
    &=\int_\bbR \Pr(\beta'\in \calC_\alpha(Y))d\beta'.
\end{align*}
That is, minimising $\E[\vol(\calC_\alpha(Y))]$ is equivalent to minimising $\Pr(\beta'\in \calC_\alpha(Y))$ for each $\beta'\in\bbR$. Define the acceptance region 
$$
    A_\alpha(\beta')=\{y\mid \beta'\in \calC_\alpha(y)\}.
$$
The constrained optimisation problem above then reduces to solving, for each $\beta'$,
\begin{align*}
    &A_\alpha(\beta')=\arg\max_{\widetilde A_\alpha} \Pr(Y \notin \widetilde A_\alpha(\beta')) \\
    \text{such that }&\Pr(Y\notin A_\alpha(\beta)\mid \beta=\beta')=\alpha.
\end{align*}
The term $\Pr(Y \notin  A_\alpha(\beta'))$ may be interpreted as the power of a size-$\alpha$ test 
$$
    H_0:\beta=\beta'\text{ vs }H_1:\beta\sim\pi_0
$$
where $Y\mid \beta\sim\Normal(\beta,\sigma^2)$. By the Neyman-Pearson lemma, the most powerful test is of the form
$$
    A_\alpha(\beta')=\left \{y\mid \frac{\pi(y)}{p(y\mid \beta')}\leq k_\alpha(\beta')\right \}
$$
where $k_\alpha(\beta')$ is such that $\Pr(Y\in A_\alpha(\beta)\mid \beta=\beta')=1-\alpha$. The acceptance region is an interval $[\loA_\alpha(\beta'),\upA_\alpha(\beta')]$ \citep[Theorem 2.1]{Cortinovis2024}. Defining
$$
    w_\alpha(\beta')=\frac{1}{\alpha}\Phi\left( \frac{\loA_\alpha(\beta')-\beta'}{\sigma}\right),
$$
the confidence region is given by
$$
    \calC_\alpha(y)=\{\beta' \mid \loA_\alpha(\beta')=\beta' -\sigma z_{1-\alpha w_\alpha(\beta')}\leq y \leq \beta'+\sigma z_{1-\alpha(1-w_\alpha(\beta')) }=\upA_\alpha(\beta')\}.
$$
The function $w_\alpha(\beta')\in[0,1]$ is called the spending function or tail function \citep{Puza2006,Yu2018}, and represents the proportion of the $\alpha$ rejection budget allocated to the left tail of the acceptance interval $[\loA_\alpha(\beta'),\upA_\alpha(\beta')]$.

The spending function $w_\alpha$ satisfies several key properties, which will be useful for our asymptotic analysis. Most of these originate from \citet{Cortinovis2024}. Under mild assumptions on the prior, satisfied for the models considered in this paper, $w_\alpha(\beta)$ is continuous in $\beta$. If the prior $\pi_0$ is symmetric around zero, we have
\begin{align}
    w_\alpha(-\beta')=1-w_\alpha(\beta').
    \label{eq:propw1}
\end{align}

Additionally, if the prior $\pi_0(\beta):=\pi_0(\beta;\sigma)$ admits $\sigma$ as a scale parameter, writing $w_\alpha(\beta;\sigma)$ for the corresponding tail function, we have
\begin{align}
    w_\alpha(\beta;\sigma)=w_\alpha\left(\frac{\beta}{\sigma};1\right).
    \label{eq:propw2}
\end{align}

We now describe other properties of the spending function in the case of a Gaussian prior and of a prior with power-law tails, such as the horseshoe.
\begin{proposition}[FAB with a Gaussian prior \citep{Pratt1963,Yu2018}]
    \label{prop:fabgaussianprop}
    If the prior $\pi_0(\beta)=\mathcal N(\beta;0,\tau^2\sigma^2)$ is Gaussian, the spending function is given by $w_\alpha(\beta)=g_\alpha^{-1}\left(\frac{2\beta}{\sigma\tau^2}\right)$ where $g_\alpha:(0,1)\to\bbR$ is the one-to-one function
    \begin{equation}
        g_\alpha(\omega)=\Phi^{-1}(\alpha \omega)-\Phi^{-1}(\alpha(1-\omega)).
        \label{eq:galpha}
    \end{equation}
    $w_\alpha$ is strictly increasing and
    $$
        \lim_{\beta\to\infty} w_\alpha(\beta)=1.
    $$%
\end{proposition}
\begin{proposition}[FAB with a prior with power-law tails \citep{Cortinovis2024}]\label{prop:horseshoefab}
    Let $\pi_0(\beta;\sigma)$ be a symmetric prior such that the marginal density $\pi(y)$ has power-law tails:
    $$
        \pi(y)\sim \frac{C_\sigma}{|y|^{2\beta+1}} \text{ as }|y|\to\infty
    $$
    for some constant $C_\sigma$ and some $\beta>0$. Then,
    $$
        \lim_{\beta\to\infty} w_\alpha(\beta)=\lim_{\beta\to -\infty} w_\alpha(\beta)=\frac{1}{2}.
    $$
\end{proposition}
The difference between the spending functions of the two priors greatly affects the resulting FAB confidence regions. In particular, while both priors lead to confidence regions that are shorter than the classical one when the observed $y$ is close to zero, their behaviour differs as the disagreement between the prior and the data increases. In particular, the FAB confidence regions under the Gaussian prior become unbounded as $|y|$ grows, while the horseshoe prior leads to confidence regions that eventually revert to the classical confidence interval. This is illustrated in \cref{fig:fab_comparison_supplementary}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./figures/fab_comparison_supplementary.pdf}
    \caption{Comparison of the FAB procedures under a Gaussian ($\tau^2 = 1$) and a horseshoe ($\eta = 1$) priors when $\sigma^2 = 1$ and $\alpha = 0.1$.}
    \label{fig:fab_comparison_supplementary}
\end{figure}
