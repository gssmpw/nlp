
\subsection{Control variates}
The method of control variates is a standard variance reduction technique in Monte Carlo approximation \citep[\S4.1]{Glasserman2003}. For simplicity, we present the method in the scalar case, but extensions to the multivariate setting are available.
Let $(Z,Y)$ be a pair of real-valued random variables, and assume we are interested in estimating $\E [Y]$ based on an iid sample $\{(Z_i,Y_i)\}_{i=1}^n$. Assuming $\mu=\E [Z]$ is known, one defines the control variate estimator (CVE)
\begin{align}
    \widehat Y^{\cv}_\lambda=\overline Y - \lambda (\overline Z - \mu)=\frac{1}{n}\sum_{i=1}^n \left( Y_i - \lambda(Z_i-\mu)\right), \label{eq:cvestimator}
\end{align}
where $\lambda\in\bbR$ is a tuning coefficient and $\overline Z$ and $\overline Y$ are the sample means of $(Z_i)$ and $(Y_i)$, respectively. The centered random variable $Z_i-\mu$ serves as a control variate to estimate $\E[Y]$. The CVE is a consistent and unbiased estimator of $\E[Y]$ with $\var(\widehat Y^{\cv}_\lambda)=(\var(Y)-2\lambda \cov(Z,Y)+\lambda^2 \var(Z))/n$, while $\var(\overline Y)=\var(Y)/n$. Therefore, CVE achieves smaller variance whenever $\lambda<\frac{2\cov(Z,Y)}{\var(Z)}$, and the optimal coefficient is given by $\lambda^* = \cov(Z,Y) / \var(Z)$.
In this case, $\var(\widehat Y^{\cv}_{\lambda^*}) =(1-\rho_{Z,Y}^2)\var(\overline Y)$, where $\rho_{Z,Y}$ is the correlation between $Z$ and $Y$. The more correlated $Z$ and $Y$, the larger the variance reduction.
By plugging the estimator \looseness=-1
\begin{align}
    \widehat \lambda = \frac{\sum_{i=1}^n (Z_i-\overline Z)(Y_i-\overline Y) }{\sum_{i=1}^n (Z_i-\overline Z)^2 } \label{eq:estimatorlambda}
\end{align}
for $\lambda$ in \cref{eq:cvestimator}, one has
$$
    \frac{\widehat Y^\cv_{\widehat \lambda} - \E[Y]}{s/\sqrt{n}}\to \Normal(0,1)
$$
as $n\to\infty$, where $s$ is the sample standard deviation of $\{(Y_i - \widehat\lambda Z_i)\}_{i=1,\ldots,n}$. Hence, $\widehat Y^\cv_{\widehat \lambda} \pm z_{1-\alpha/2}s/\sqrt{n}$ is an asymptotically valid $1-\alpha$ CI for $\E[Y]$, whose asymptotic width is $2z_{1-\alpha/2}\sqrt{1-\rho_{ZY}^2}\sqrt{\var(Y)}/\sqrt{n}$.

\subsection{Prediction-powered inference}
PPI~\citep{Angelopoulos2023} defines an estimator $\widehat \theta$ and a CI $\calC_\alpha^\pp$ for a parameter of interest $\theta^\star$ satisfying \cref{eq:defthetastar2}.
In particular, let $\widehat m_\theta$ and $\widehat \Delta_\theta$ be some estimators of $m_\theta$ and $\Delta_\theta$. Using \cref{eq:defthetastar2}, the estimator $\widehat \theta$ is defined as the solution, in $\theta$, to the equation
\begin{align}
    \widehat m_\theta + \widehat \Delta_\theta=0.
\end{align}
Similarly, let $\calR_\delta$ and $\calT_{\alpha-\delta}$ be $1 - \delta$ and $1 - (\alpha - \delta)$ CIs for $\Delta_\theta$ and $m_\theta$, respectively. Then, the PPI confidence interval $\calC_\alpha^\pp$ is defined as
\begin{align}
    \calC_\alpha^\pp=\left\{\theta \mid 0\in \calR_\delta + \calT_{\alpha-\delta}\right\}, \label{eq:ppconfidenceinterval}
\end{align}
where $+$ denotes the Minkowski sum.
Typical choices for $\widehat m_\theta$ and $\calT_{\alpha-\delta}$ are the sample mean of the unlabelled data,
\begin{align}
    \widehat m_\theta&=\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}_{\theta}'(\widetilde X_i,f(\widetilde X_i)), \label{eq:msamplemean}
\end{align}
and classical CIs for sample means, respectively.
Different choices for $\widehat \Delta_\theta$ have been proposed in the literature, leading to different PPI estimators.

\paragraph{Standard PPI.}
\citet{Angelopoulos2023} propose to use the sample mean
\begin{align}
    \widehat \Delta^\PP_\theta&=\frac{1}{n}\sum_{i=1}^n \left(\mathcal{L}_{\theta}'(X_i,Y_i)-\mathcal{L}_{\theta}'(X_i,f(X_i))\right)\label{eq:deltasamplemean}
\end{align}
as an estimator for $\Delta_\theta$ and the associated classical CIs to construct $\calR_\delta$.
For the squared loss, the estimator $\widehat\theta^\PP$ solving $\widehat m_\theta + \widehat \Delta_\theta=0$ takes the control variate form
\begin{align}
    \widehat\theta^\PP= \overline Y - \left( \frac{1}{n}\sum_{i=1}^n f(X_i) - \frac{1}{N}\sum_{j=1}^{N} f(\tX_j)     \right) \label{eq:thetahatppi}
\end{align}
with control variate $f(X_i)-\frac{1}{N}\sum_{j=1}^{N} f(\tX_j)$ and $\lambda=1$.

\paragraph{PPI\texttt{++}.}
\citet{Angelopoulos2023a} extend standard PPI by introducing an additional control variate parameter $\lambda$, which they call power tuning parameter.
The chosen $\widehat m_\theta$ is still the sample mean \eqref{eq:msamplemean},
while $\widehat \Delta^\PPpp_\theta$ now takes the control variate form
\begingroup
    \allowdisplaybreaks
    \begin{align}
        \widehat \Delta^\PPpp_\theta&=\frac{1}{n}\sum_{i=1}^n \left(\mathcal{L}_{\theta}'(X_i,Y_i)-\mathcal{L}_{\theta}'(X_i,f(X_i))\right) \label{eq:deltahatpowertuning} \\
        & ~ - (\widehat \lambda -1 )\left( \frac{1}{n}\left [\sum_{i=1}^n \mathcal{L}_{\theta}'(X_i,f(X_i))\right ]  -\widehat m_\theta\right), \nonumber
    \end{align}
\endgroup
where $\widehat\lambda$ is estimated from the data.
In this case, the centered control variate is $\mathcal{L}_{\theta}'(X_i,f(X_i))-\widehat m_\theta$, which depends only on the machine learning predictions. For the squared loss, we obtain
\begin{align}
    \widehat\theta^\PPpp = \overline Y - \widehat\lambda\left( \frac{1}{n}\sum_{i=1}^n f(X_i) - \frac{1}{N}\sum_{j=1}^{N} f(\tX_j)     \right)
    \label{eq:thetahatppipp}
    \end{align}
with plug-in estimator
\begin{align}
    \widehat\lambda = \frac{c_n}{(1+\frac{n}{N})v_{n+N}}, \label{eq:lambdahat}
\end{align}
where $c_n$ is the sample covariance of $(Y_i,f(X_i))_{i=1}^n$ and $v_{n+N}$ is the sample variance of $((f(X_i))_{i=1}^n,(f(\tX_j))_{j=1}^N)$. The estimator \eqref{eq:thetahatppipp} is closely related (though slightly different) to the one introduced by \citet{Zhang2019} for mean estimation in semi-supervised inference.

\paragraph{CLT-based CIs.}
While the definition of the PPI confidence interval \eqref{eq:ppconfidenceinterval} allows for merging any CIs $\mathcal{R}_\delta$ and $\mathcal{T}_{\alpha-\delta}$ for $\Delta_\theta$ and $m_\theta$, in practice these are often chosen to be CLT-based CIs that, once combined into $\calC_\alpha$ give exact asymptotic coverage,
\begin{equation*}
    \liminf_{n,N \to \infty} \mathrm{Pr}(\theta^* \in \calC_\alpha) \geq 1 - \alpha.
\end{equation*}
Such CLT-based CIs rely on the following standard assumption on the estimators $\widehat m_\theta$ and $\widehat \Delta_\theta$.
\begin{assumption}[CLT assumption for PPI and PPI++]
\label{assump:clt}
    Let  $\widehat m_\theta$ be the sample mean $\eqref{eq:msamplemean}$ and consider some estimator $(\widehat \sigma^f_\theta)^2$ of $\var(\widehat m_\theta)$.
    Let $\widehat \Delta_\theta$ be either the PPI estimator \eqref{eq:deltasamplemean} or the PPI++ estimator \eqref{eq:deltahatpowertuning} and consider some estimator $\widehat \sigma_\theta$ of $\var(\widehat \Delta_\theta)$ with $\widehat\sigma_\theta/\var(\widehat \Delta_\theta) \to 1$ almost surely.  Assume that, as $\min(n,N)\to\infty$,
    \begin{align}
        (\widehat m_\theta - m_\theta) / \widehat \sigma^f_\theta & \to \Normal(0,1)\\
        (\widehat\Delta_\theta -\Delta_\theta) / \widehat\sigma_\theta &\to \Normal(0,1).
    \end{align}
\end{assumption}

\subsection{Bayes-optimal confidence regions}\label{sec:background_fab}
The FAB framework \citep{Pratt1961,Pratt1963,Yu2018} aims to construct valid confidence regions with smaller expected volume.
Let $W\mid \beta\sim\Normal(\beta,\sigma^2)$ with some prior $\pi_0(\beta)$ and denote by $\pi(w)=\int p(w\mid \beta) \pi_0(\beta)d\beta$ the corresponding marginal likelihood.
For $\alpha\in(0,1)$, let $\calC_\alpha(w)$ be an exact $1-\alpha$ confidence region for $\beta$ based on the data $w$. That is,
for any fixed $\beta_0$,
\begin{align}
    \Pr(\beta\in \calC_\alpha(W)\mid \beta=\beta_0)=1-\alpha. \label{eq:confidencelevel}
\end{align}
Let $\vol(\calC_\alpha(w))=\int_{\beta'\in\calC_\alpha(w)}d\beta'$ be the volume of $\calC_\alpha(w)$, and consider its expected value under the prior $\pi_0$,
\begin{equation}
    \E[\vol(\calC_\alpha(W))] = \int \vol(\calC_\alpha(w)) \pi(w) dw.
\end{equation}
\begin{definition}
    For $\alpha\in(0,1)$, $\sigma>0$ and a prior $\pi_0(\beta)$, the FAB confidence region $\calC_\alpha$ for the mean parameter $\beta$ of the normal model $Y\mid\beta\sim\Normal(\beta,\sigma^2)$, is the minimiser of the (Bayesian) expected volume
    \begin{align}
        \calC_\alpha = \arg\min_{\widetilde\calC_\alpha} \E[\vol(\widetilde\calC_\alpha(W))] \label{eq:bayes_optimal_C}
    \end{align}
    subject to the (frequentist) coverage constraint \eqref{eq:confidencelevel}. We write $\calC_\alpha(w)=\text{FAB-CR}(w;\pi_0,\sigma^2,\alpha)$.
\end{definition}
The solution to \cref{eq:bayes_optimal_C}, which exists and is unique if $\pi_0(\beta)$ is not degenerate \citep[Theorem~2.1]{Cortinovis2024}, may be found numerically as long as the marginal likelihood $\pi(w)$ can be evaluated pointwise.
Additional details are provided in \cref{sec:app:backgroundFAB}.
Intuitively, the FAB confidence region $\calC_\alpha(w)$ constructed through \cref{eq:bayes_optimal_C} will be smaller for values of $w$ that are likely under the marginal likelihood, and larger otherwise.
As a result of this, while FAB guarantees the right coverage for any prior, one that assigns high probability to the value of $\beta$ that generated the data is required to achieve smaller expected volume compared to the standard CI $(w\pm \sigma z_{1-\alpha/2})$, whose width does not depend on $w$.

\paragraph{Bayes-assisted estimator.} A natural estimator to use alongside the FAB confidence region $\calC_\alpha(w)$ is the posterior mean $\widehat{\beta}(W) = \mathbb{E}[\beta\mid W]$. As shown by \citep[Theorem~2.1]{Cortinovis2024}, it is always contained within the confidence region: $\widehat{\beta}(w)\in  \calC_\alpha(w)$ for any $w\in\bbR$ and any $\alpha\in(0,1)$. We refer to $\widehat{\beta}(W)$ as the Bayes-assisted estimator.
