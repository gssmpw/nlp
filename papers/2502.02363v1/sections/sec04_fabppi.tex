Our approach, which we call FAB-PPI, combines the PPI framework with the FAB construction of confidence regions by specifying a prior on the rectifier $\Delta_\theta$. To ease the presentation, here we describe the method for $Y, \theta \in\bbR$. The general multivariate case is discussed in \cref{app:multivariate}.

As in PPI, we use the sample mean \eqref{eq:msamplemean} as the estimator of $m_\theta$.
For $\Delta_\theta$, we start by considering a consistent estimator $\widehat \Delta_\theta$, such as the sample mean \eqref{eq:deltasamplemean} used in PPI, or the control variate estimator \eqref{eq:deltahatpowertuning} used in PPI\texttt{++}.
Throughout this section, we assume that \cref{assump:clt} is satisfied. That is, a CLT holds for $\widehat{m}_\theta$ and $\widehat{\Delta}_\theta$ with respect to some estimators $(\widehat{\sigma}^f_\theta)^2$ and $\widehat{\sigma}^2_\theta$ of $\var(\widehat{m}_\theta)$ and $\var(\widehat{\Delta}_\theta)$, respectively.
In this setting, let $\pi_0(\Delta_\theta; \tau_n)$ be a prior on $\Delta_\theta$ with scale parameter $\tau_n$, which may depend on the labelled data through $\widehat \sigma_\theta$.
Denote by $\ell(w; \sigma, \tau)$ the log-marginal likelihood, evaluated at $w$, of a Gaussian likelihood model with mean $\Delta$ and variance $\sigma^2$ under the prior $\pi_0(\Delta; \tau)$,
\begin{align}
    \ell(w; \sigma, \tau) = \log \int_\bbR \Normal(w; \Delta,\sigma^2) \pi_0(\Delta; \tau) d\Delta. \nonumber
\end{align}

\subsection{Bayes-assisted PPI Estimators}
 Consider the Bayes-assisted estimator
\begin{align}
    \widehat\Delta_\theta^{\FABPPI}= \widehat\Delta_\theta + \widehat\sigma_\theta^2 \ell'\left(\widehat\Delta_\theta; \widehat\sigma_\theta, \tau_n\right)
\label{eq:FABPPIestimatorDelta}
\end{align}
for the rectifier $\Delta_\theta$.
By Tweedie's formula \citep{}, the above estimator is the posterior mean of the mean parameter of a Gaussian likelihood model under the prior $\pi_0$.
Note however that we do not assume here that $\widehat\Delta_\theta$ is normally distributed for a fixed $n$.

The FAB-PPI estimator of $\theta^\star$, denoted by $\widehat \theta^{\FABPPI}$, is then obtained as the solution, in $\theta$, to the equation
$$
    \widehat m_\theta + \widehat\Delta_\theta^{\FABPPI} = 0.
$$

\subsection{FAB-PPI Confidence Regions}\label{sec:fabppi_cr}
As in PPI, let $\calT_{\alpha-\delta}(\widehat m_\theta)$ denote a standard $1-(\alpha-\delta)$ confidence interval for $m_\theta$.
For $\Delta_\theta$, we apply the FAB framework with the prior $\pi_0$ to obtain a $1-\delta$ confidence region $\calR^{\FABPPI}_\delta(\widehat\Delta_\theta)=\text{FAB-CR}(\widehat\Delta_\theta;\pi_0(\cdot~;\tau_n),\widehat\sigma_\theta,\delta)$.
Then, the FAB-PPI confidence region $\calC_\alpha^\FABPPI$ is obtained as
\begin{align}
    \calC_\alpha^\FABPPI=\left\{\theta \mid 0\in \calR^{\FABPPI}_\delta(\widehat\Delta_\theta) + \calT_{\alpha-\delta}(\widehat m_\theta)\right\}.
    \label{eq:fabppiconfidenceinterval}
\end{align}
\cref{algo:FABPPIConvex} summarises the steps of the FAB-PPI approach in a general convex estimation problem.
\begin{algorithm}
    \caption{FAB-PPI for convex estimation}
    \label{algo:FABPPIConvex}
    {\bfseries Input:} labelled $\{(X_i,Y_i)\}_{i=1}^n$, unlabelled $\{\tX_j\}_{j=1}^N$, predictor $f$, prior $\pi_0(\cdot~; \tau_n)$, error levels $\alpha,\delta$
    \begin{algorithmic}
        \STATE Set $\widehat\lambda=1$ (FAB-PPI) or estimate $\widehat\lambda$ from data (FAB-PPI\texttt{++}) as in \citet{Angelopoulos2023a}.
        \FOR{$\theta \in \Theta_\text{grid}$}
            \STATE $\widehat m_\theta \gets \frac{1}{N}\sum_{i=1}^{N} \mathcal{L}_{\theta}'(\widetilde X_i,f(\widetilde X_i))$
            \STATE $\widehat{\xi} \gets \frac{1}{n}\sum_{i=1}^n \left(\mathcal{L}_{\theta}'(X_i,Y_i)- \widehat{\lambda} \mathcal{L}_{\theta}'(X_i,f(X_i))\right)$
            \STATE $\widehat \Delta_\theta \gets \widehat{\xi} + (\widehat \lambda -1 ) \widehat m_\theta$
            \STATE $(\widehat{\sigma}^f_\theta)^2 \gets \frac{1}{N - 1} \sum_{i=1}^{N} \left(\mathcal{L}_{\theta}'(\widetilde X_i,f(\widetilde X_i)) - \widehat m_\theta\right)^2$
            \STATE $\widehat \sigma_\xi^2 \gets \frac{1}{n-1} \sum_{i=1}^n \left(\mathcal{L}_{\theta}'(X_i,Y_i)- \widehat{\lambda} \mathcal{L}_{\theta}'(X_i,f(X_i)) - \widehat{\xi}\right)^2$
            \STATE $\widehat{\sigma}^2_\theta \gets \frac{1}{n} \widehat \sigma_\xi^2 + \frac{(\widehat \lambda -1 )^2}{N} \widehat{\sigma}^2_m$
            \STATE $\mathcal{T}_{\alpha - \delta}(\widehat m_\theta) \gets \left(\widehat{m}_\theta \pm \widehat{\sigma}^f_\theta z_{1-(\alpha - \delta)/2}\right)$
            \STATE $\calR^{\FABPPI}_\delta(\widehat\Delta_\theta)\gets \text{FAB-CR}(\widehat\Delta_\theta;\pi_0(\cdot~;\tau_n),\widehat\sigma_\theta,\delta)$
            \STATE $\widehat\Delta_\theta^{\FABPPI} \gets \widehat\Delta_\theta + \widehat\sigma_\theta^2 \ell'\left(\widehat\Delta_\theta;\widehat\sigma_\theta, \tau_n\right)$
        \ENDFOR
    \end{algorithmic}
    {\bfseries Outputs:} estimator $\widehat \theta^{\FABPPI} = \arg\min_{\Theta_\text{grid}} \left|\widehat{m}_\theta + \widehat\Delta_\theta^{\FABPPI}\right|$ and CR $\calC_\alpha^\FABPPI=\left\{\theta \mid 0\in \calR^{\FABPPI}_\delta(\widehat\Delta_\theta) + \calT_{\alpha-\delta}(\widehat m_\theta)\right\}$
\end{algorithm}

\subsection{Choosing the prior}\label{sec:fabppi_priors}
FAB-PPI is motivated by applications in which the PPI predictor $f$ is expected to be generally accurate, as measured by the rectifier $\Delta_\theta$.
Such a property may be encoded in $\pi_0(\Delta_\theta; \tau_n)$ by choosing a prior that concentrates around zero.
As mentioned in \cref{sec:background_fab}, the FAB construction of $\calR^{\FABPPI}_\delta(\widehat\Delta_\theta)$ will exhibit smaller volume compared to the classical CI, and hence result in downstream efficiency gains over standard PPI, if the true rectifier $\Delta_\theta$ is likely under $\pi_0$.
In particular, the prior scale $\tau_n$ controls the size of the potential efficiency gains and losses of FAB-PPI over PPI: the smaller $\tau_n$, the more the resulting CR will shrink (resp. grow) when $\Delta_\theta \approx 0$ (resp. $|\Delta_\theta| \gg 0$). Experimentally, we find that the choice $\tau_n = \widehat\sigma_\theta$ results in a parameter-free approach that strikes a good compromise. More general choices of $\tau_n$ are briefly mentioned in \cref{sec:discussion}.

A seemingly natural proposal for $\pi_0$ that meets the requirements above is the Gaussian prior
\begin{align}
    \pi_\text{N}(\Delta_\theta; \widehat\sigma_\theta) = \Normal(\Delta_\theta; 0, \widehat\sigma_\theta).
    \label{eq:Gaussianprior}
\end{align}
However, as we will discuss in \cref{sec:fabppi_theory}, $\pi_\text{N}$ exhibits undesirable properties for FAB-PPI. Instead, we propose to use the horseshoe prior~\citep{Carvalho2010}
\begin{align}
    \pi_\text{HS}(\Delta_\theta;\widehat\sigma_\theta)=\int_0^\infty \Normal(\Delta_\theta;0,\nu^2\widehat\sigma^2_\theta) C^+(\nu; 0,1)d\nu,
    \label{eq:horseshoeprior}
\end{align}
where $C^+(\nu; 0,1)$ denotes the pdf of the half-Cauchy distribution with location parameter 0 and scale parameter 1. In the case of $\pi_\text{HS}$, the choice of scaling $\tau_n = \widehat\sigma_\theta$ is further motivated by \citet[\S3.3]{Piironen2017}. Furthermore,  the horseshoe prior has power-law tails, making it a particularly robust choice for FAB-PPI, as discussed in \cref{sec:fabppi_theory}. Crucially, for both priors $\pi_\text{N}$ and $\pi_\text{HS}$, the marginal likelihood under a Gaussian model with standard deviation $\widehat\sigma_\theta$ can be expressed in terms of standard functions (see \cref{sec:app:backgroundhorseshoe} for the horseshoe), enabling us to compute $\widehat\Delta_\theta^{\FABPPI}$ and $\calR^{\FABPPI}_\delta(\widehat\Delta_\theta)$ in \cref{algo:FABPPIConvex}.

\subsection{Theoretical properties}\label{sec:fabppi_theory}
As shown by the following result, proved in \cref{app:thm:fabppi_coverage}, the FAB-PPI CR has exact asymptotic coverage.
\begin{theorem}[Asymptotic coverage]\label{thm:fabppi_coverage}
    For $\alpha \in (0, 1)$, let $\calC_\alpha^\FABPPI$ be the FAB-PPI confidence region \eqref{eq:fabppiconfidenceinterval} under the Gaussian prior \eqref{eq:Gaussianprior} or the horseshoe prior \eqref{eq:horseshoeprior}. Then, under \cref{assump:clt},
    \begin{equation*}
        \liminf_{\min(n,N) \to \infty} \Pr(\theta^* \in \calC_\alpha^\FABPPI) \geq 1 - \alpha.
    \end{equation*}
\end{theorem}
The proof of \cref{thm:fabppi_coverage} crucially relies on showing exact asymptotic coverage of the FAB CR $\calR^{\FABPPI}_\delta(\widehat\Delta_\theta)$. While the latter holds for both priors introduced in the previous sections, the two limits are very different. In particular, as discussed in \cref{rem:fabppi_asymptotic}, the volume of $\calR^{\FABPPI}_\delta(\widehat\Delta_\theta)$ vanishes asymptotically under $\pi_\text{HS}$, while it does not under $\pi_\text{N}$. \looseness=-1

The behaviour of $\calR^{\FABPPI}_\delta(\widehat\Delta_\theta)$ under the two priors also differs for large values of observed $\widehat\Delta_\theta$.
In case of increasing disagreement between the prior and the data, Gaussian FAB confidence regions are known to become arbitrarily large \citep{Yu2018}. 
On the other hand, thanks to its power-law tails, the horseshoe results in confidence regions that revert to the corresponding standard CI \citep{Cortinovis2024}.
Here, we state the implication of this property on FAB-PPI informally, and provide a formal proof in \cref{app:thm:fabppi_robustness}.
\begin{proposition}[Robustness under the horseshoe, informal]\label{prop:fabppi_robustness}
    For $\alpha\in(0,1)$, let $\calC_\alpha^\FABPPI$ and $\calC_\alpha^\PP$ denote, respectively, the FAB-PPI confidence region \eqref{eq:fabppiconfidenceinterval} under the horseshoe prior \eqref{eq:horseshoeprior} and the standard PPI confidence region \eqref{eq:ppconfidenceinterval}, both viewed as functions of $\widehat\Delta_\theta$. If $|\widehat\Delta_\theta| \gg 0$, then
    \begin{align*}
        \calC_\alpha^\FABPPI \simeq \calC_\alpha^\PP.
    \end{align*}
\end{proposition}
In practice, this means that, in the presence of heavily biased predictors, FAB-PPI with the horseshoe prior reverts to standard PPI. In a sense, this represents a form of robustness to prior misspecification of FAB-PPI under the horseshoe. \looseness=-1

Overall, \cref{rem:fabppi_asymptotic} and \cref{prop:fabppi_robustness} provide strong support for preferring $\pi_\text{HS}$ over $\pi_\text{N}$ within the FAB-PPI framework.

\subsection{FAB-PPI for mean estimation}\label{sec:fabppi_mean_estimation}
To provide a concrete example, a specialised version of \cref{algo:FABPPIConvex} under the squared loss is derived in \cref{app:fabppi_squared_loss}.
Here, we briefly discuss the differences between the FAB-PPI mean estimator and its standard PPI counterpart, as well as the asymptotic behaviour of the former.
Under the squared loss, the rectifier $\Delta := \Delta_\theta$ does not depend on $\theta$ and the FAB-PPI estimator $\widehat \theta^{\FABPPI}$ corresponding to the chosen estimator $\widehat\Delta$ (PPI or PPI++) is given by \looseness=-1
\begin{align}
    \widehat \theta^{\FABPPI} &=  \widehat \theta - \widehat\sigma^2 \ell'(\widehat\Delta; \widehat\sigma, \tau_n) \label{eq:mean_estimator_fabppi}\\
    &=\overline Y - \widehat\lambda\left( \frac{1}{n}\sum_{i=1}^n f(X_i) - \frac{1}{N}\sum_{j=1}^{N} f(\tX_j) \right) \nonumber \\
    &~~~~- \widehat\sigma^2 \ell'\left(\widehat\Delta; \widehat\sigma, \tau_n\right),\nonumber
\end{align}
where $\widehat\sigma^2$ is an estimator of $\var(\widehat\Delta)$, $\widehat \theta$ is the PPI estimator corresponding to $\widehat\Delta$, and $\widehat\lambda$ is set either to one (PPI) or \eqref{eq:lambdahat} (PPI++).
In both cases, the estimator $\widehat \theta^{\FABPPI}$ takes the form
$$
    \text{\normalsize Classic Estimator + PPI correction + \textbf{Bayes correction}},
$$
where the last component depends on the chosen prior. The following proposition, proved in \cref{app:prop:fabppi_consistency}, further differentiates between the priors presented in \cref{sec:fabppi_priors} in favour of the horseshoe.
\begin{proposition}[Consistency of FAB-PPI mean estimators]
    \label{prop:fabppi_consistency}
    Let $\widehat \theta^{\FABPPI}_\text{HS}$ and $\widehat \theta^{\FABPPI}_\text{N}$ be the FAB-PPI estimators \eqref{eq:mean_estimator_fabppi} under the horseshoe \eqref{eq:horseshoeprior} and Gaussian \eqref{eq:Gaussianprior} priors, respectively. If the PPI estimator $\widehat \theta$ is a consistent estimator of $\theta^\star$, then $\widehat \theta^{\FABPPI}_\text{HS}$ is a consistent estimator of $\theta^\star$, while $\widehat \theta^{\FABPPI}_\text{N}$ is not.
\end{proposition}
Intuitively, this is due to the fact that the influence of $\pi_\text{HS}$ vanishes asymptotically, while for $\pi_\text{N}$ it does not.
