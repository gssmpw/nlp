\section{Experiments}

\input{tab/tab1}

\subsection{Datasets}
\noindent{\textbf{Generation of Simulated Datasets.}} 
Our experiments are conducted on CIFAR-10, CIFAR-100, and Tiny ImageNet with two types of synthetic noise (\ie, symmetric and asymmetric).
\textit{Symmetric noise} is generated by uniformly flipping labels in each class to incorrect labels from other classes.
\textit{Asymmetric noise} flips the labels within a specific set of classes.
For CIFAR-10, labels are flipped as follows: TRUCK $\rightarrow$ AUTOMOBILE, BIRD $\rightarrow$ AIRPLANE, DEER $\rightarrow$ HORSE, CAT $\rightarrow$ DOG. 
For CIFAR-100, the 100 classes are grouped into 20 super-classes, and each has 5 sub-classes. 
Each class is then flipped to the next within the same super-class.
We use noise ratios of $20\%$ and $40\%$ in our experiments to evaluate the effectiveness of our proposed method.


\noindent{\textbf{Real-World Datasets.}}
We validate our method on the real-world datasets CIFAR-10N and CIFAR-100N, which contain human-annotated noisy labels obtained through Amazon Mechanical Turk~\cite{wei2022learning}.
Our evaluation primarily considers the CIFAR-10N (Worst) and CIFAR-100N (Fine) label sets, with noise ratios of 40.21$\%$ and 40.20$\%$, respectively.


\subsection{Experimental Setup}

Our method builds upon trajectory matching, and we conduct comprehensive comparisons with several baseline approaches, including MTT~\cite{cazenavette2022distillation}, FTD~\cite{du2023minimizing}, DATM~\cite{guo2024lossless}, and ATT~\cite{liu2024dataset}.
The detailed configurations of hyperparameters in the buffer phase and distillation phase align with the settings described in the corresponding works.
Experimental results for additional distillation methods (\eg, gradient matching, distribution matching, \etc) can be found in the appendix.
Without explicit mention, we use ConvNet by default to conduct experiments.
Following previous methods~\cite{cazenavette2022distillation, du2023minimizing, guo2024lossless, liu2024dataset}, we use a 3-layer ConvNet for CIFAR-10 and CIFAR-100, and a 4-layer ConvNet for Tiny ImageNet.



\subsection{Results}

\input{tab/tab2}
\input{tab/tab3}


\noindent{\textbf{Dataset Distillation on Simulated Datasets.}} 
We conduct comprehensive experiments to compare the performance of previous dataset distillation methods with noisy labels.
In these experiments, we consider the effects of different noise types and noise ratios on various trajectory matching-based methods at different Images Per Class (IPC) configurations. 
This evaluation is performed on multiple datasets to ensure a thorough and reliable comparison.
As shown in Table~\ref{tab1}, noisy labels lead to a maximum accuracy drop of over 20\% in dataset distillation (\eg, for CIFAR-10 with 40\% asymmetric noise, FTD and DATM show significant accuracy drops when the IPC is set to 50).
In general, dataset distillation methods exhibit poorer performance with asymmetric noisy labels compared to symmetric ones. 
This is because asymmetric noise flips labels to closely related classes, making it more challenging for the model to distinguish between correct and noisy labels. 
Additionally, as IPC increases, the influence of noisy labels on dataset distillation grows increasingly significant.
Higher IPC requires the student model to align with the later stages of the training trajectory, where noisy labels exert a greater influence.
Our method iteratively refines samples to minimize the impact of noise on training trajectories. 
As a result, our approach achieves significant performance improvements.


In our evaluation of CIFAR-10, CIFAR-100, and Tiny ImageNet, we observe that our method achieves the most noticeable improvement on CIFAR-10.
This is because CIFAR-10 has fewer classes (10) and is relatively easy. 
In contrast, CIFAR-100 (100 classes) and Tiny ImageNet (200 classes) have more classes, which increases the difficulty and complexity. 
Our method demonstrates significant improvements on CIFAR-100 and Tiny ImageNet.
Overall, our method adapts effectively to various levels of dataset complexity and noise, demonstrating its robustness and versatility.
More results can be found in the appendix.


\input{tab/tab4}
\noindent{\textbf{Dataset Distillation on Real-world Datasets.}} 
To further verify the effectiveness of our method in real-world scenarios with noisy labels, we conduct comparative experiments on CIFAR-10N (Worst) and CIFAR-100N (Fine).
The experiments compare different dataset distillation methods, including MTT~\cite{cazenavette2022distillation}, FTD~\cite{du2023minimizing}, and ATT~\cite{liu2024dataset}, with and without our proposed method.
As shown in Table~\ref{tab2}, our proposed method (\ie, + Ours) generally enhances performance across different distillation methods on both CIFAR-10N (Worst) and CIFAR-100N (Fine) datasets.
On CIFAR-10N (Worst), our method significantly improves the accuracy of all baseline methods, especially as IPC increases (\eg, IPC=10 and 50). 
For instance, the accuracy of FTD improves from 60.3\% to 69.2\% at IPC=50, demonstrating notable enhancement.
On CIFAR-100N (Fine), the improvement is consistent, especially at higher IPC. 
Overall, our method demonstrates robustness and effectiveness in addressing realistic noisy labels.


\subsection{Ablation Study}
We conduct ablation studies to validate the effectiveness of our components.
The baseline for comparison is DATM~\cite{guo2024lossless}. 
IPC is set to 50, and evaluations are conducted on CIFAR-10 with asymmetric noise ratios of 20\% and 40\%.
As can be seen in Table~\ref{tab3}, the consistent regularization term effectively mitigates overfitting to noisy samples, improving performance (\eg, Config B, D, and F), particularly at higher noise ratios.
The outer loop (\ie, Config C) effectively filters out noisy samples. 
This keeps the expert model focused on accurate representations, minimizing the influence of noise and enabling high-quality distillation.
For instance, at a noise ratio of 40\%, the outer loop improves the performance of dataset distillation from  55.0\% to 65.1\%.
Additionally, the inner loop effectively corrects noisy labels by leveraging reliable anchors, allowing more data to contribute to the training process.
The collaboration of these modules proves effective in addressing dataset distillation with noisy labels, achieving superior performance.




\subsection{Comparison with Learning from Noisy Labels}

To further validate our method, we benchmark it against state-of-the-art LNL (Learning with Noisy Labels) methods, C2D~\cite{zheltonozhskii2022contrast} and L2B-DivideMix~\cite{zhou2024l2b}.
However, we find that using LNL methods to train the expert model creates a challenge. 
The student model struggles to align with the expert trajectory.
We speculate that LNL methods rely on semi-supervised learning, while the student model depends solely on CE. 
This difference causes a divergence in their optimization.
To address this, we use a straightforward approach.
We apply LNL methods to generate pseudo labels for the training set. 
Then, we perform dataset distillation directly using these pseudo-labeled data.


For the C2D, the pre-training model uses ResNet50 optimized with SimCLR~\cite{chen2020simple}.
Experiments are conducted on CIFAR-100 with symmetric noise.
As shown in Table~\ref{tab4}, our proposed method outperforms LNL methods.
Notably, at a noise ratio of 20\%, LNL methods provide minimal improvement to dataset distillation performance and may even degrade it (\eg, C2D).
The reason is that LNL methods cannot assign correct labels to all samples, and noisy samples greatly affect the expert trajectory.
Our proposed method, in contrast, is conservative. 
We model the per-sample loss to distinguish clean and noisy samples.
This approach helps us keep a training set with a high proportion of clean samples.
Training the expert models exclusively on this clean subset significantly reduces the impact of noisy labels on their training.
