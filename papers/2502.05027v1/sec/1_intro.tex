\section{Introduction}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95 \linewidth]{fig/fig1.pdf}
    \vspace{-1em}
    \caption{
    Illustration of our proposed Trust-Aware Diversion (TAD) dataset distillation method.
    The outer loop separates data into trusted and untrusted spaces, rerouting distillation toward trusted samples.
    The inner loop recalibrates untrusted samples and transposes them to the trusted space.
    Through iterative interactions, the trusted space expands while the untrusted space shrinks, improving dataset distillation under noise conditions.
    }
    \vspace{-1em}
    \label{fig1}
\end{figure}


In an era of massive datasets, data-efficient learning is pivotal for achieving high performance with a limited computation budget. 
Dataset Distillation (DD) presents a promising solution by synthesizing a small amount of highly informative data that summarizes large volumes of real data, allowing models to maintain high performance with less data~\cite{zhao2021datasetcondensation, cazenavette2022distillation, deng2024exploiting}. 


The current success of DD relies on the assumption that all labels are completely correct. 
However, erroneously labeled data is ubiquitous in real-world scenarios. 
State-of-the-art DD methods~\cite{du2023minimizing, guo2024lossless, liu2024dataset} usually adopt a trajectory matching strategy, which aligns the parameters trained on distilled synthetic data (\ie, student model) with the parameters trained on real data (\ie, expert model). 
With the existence of noisy labels, an expert model tends to produce a misrepresented trajectory caused by the incorrectly labeled data. 
Consequently, if a student model attempts to match this expert trajectory, it would produce inferior synthetic data and suffer overall performance degradation. 


In this paper, we explore a more realistic scenario, \ie, Dataset Distillation with Noisy Labels (DDNL).
Notably, noisy labels introduce a unique challenge to DD: how to maintain a trustworthy distillation process without being misled by noisy samples.
We observed that existing methods struggle to identify mislabeled or noisy data perfectly~\cite{li2022neighborhood, zhao2022centrality, karim2022unicon}.
Thus, when these methods are applied, noisy data inevitably propagates into the dataset distillation, amplifying errors and compromising the performance of the distillation.

\begin{figure}[t]
    \centering
    \includegraphics[width=.99 \linewidth]{fig/fig3.pdf}
    \vspace{-1em}
    \caption{
    Training loss and test accuracy curves of an expert model on CIFAR-10 with clean and 40\% symmetric noisy labels. 
    Noise impedes convergence and significantly degrades performance.
   }
   \vspace{-1em}
    \label{fig2}
\end{figure}

To tackle this issue, we propose a \textbf{Trust-Aware Diversion (TAD)} dataset distillation method, which introduces an iterative dual-loop optimization framework (\ie, an outer loop and an inner loop) for data-effective distillation. 
The outer loop is in charge of diverting data into trusted and untrusted spaces to ensure trustworthy distillation, while the inner loop recalibrates untrusted samples, refining them into valuable data for distillation.
Through this iterative interaction, TAD can reliably refine the distinction between trusted and untrusted samples, progressively mitigating the impact of mislabeled data on data distillation.


To be specific, the outer loop separates data into trusted and untrusted spaces, and redirects distillation towards trusted samples, thus ensuring a trustworthy distillation process.
The memorization effect~\cite{arpit2017closer,yao2020searching} of deep neural networks (DNNs) shows that DNNs first learn clean samples and then noisy samples. 
In addition, we observed a clear gap in loss values between clean and noisy samples during training. 
Clean samples usually have smaller losses, while noisy samples typically exhibit higher losses, as shown in Fig.~\ref{fig2}.
Motivated by this, data can be interpreted as a mixture of two distinct sample categories (\ie, trusted and untrusted).
Thus, we employ the class-wise mean posterior probability for the division criterion.
In addition, we introduce a consistent regularization term that avoids overfitting to the wrongly divided samples, ensuring trusted samples contribute to distillation effectively.


Although data has been partitioned into trusted and untrusted spaces in the outer loop, some noisy samples may still be mistakenly classified as trusted ones.
To address this issue, we introduce a reliability score in the inner loop to quantify the trustworthiness of each sample in the trusted space, providing a fine-grained evaluation of reliability.
Since the distilled synthetic data captures a representative pattern of real data~\cite{li2024prioritize, guo2024lossless}, we measure the reliability of samples in the trusted space by computing their Mahalanobis distance~\cite{de2000mahalanobis} to the class distributions.
Based on the reliability score, we rank trusted samples and select the most reliable ones from each class.
Then, these highly-reliabile samples can be used to sieve less reliable trusted samples by measuring their similarities. 
Moreover, they can be employed to recall samples from the untrusted space, thus fully exploiting all the samples.
The inner loop compensates for the outer loop, thus further preventing the propagation of unreliable information into dataset distillation.
Therefore, this iterative interaction between the inner and outer loops continuously refines the data partitioning, progressively expanding the trusted space while shrinking the untrusted space.


To the best of our knowledge, we are the first to investigate the Dataset Distillation with Noisy Labels (DDNL). 
Extensive experiments validate the superior performance of our proposed \textbf{Trust-Aware Diversion (TAD)} dataset distillation method.
For instance, on CIFAR-100 with 40\% symmetric mislabeled data, our proposed TAD consistently outperforms ATT~\cite{liu2024dataset}.
When Images Per Class (IPC) is 10, TAD achieves 41.5\% accuracy, significantly outperforming ATT (32.6\%) and the two-stage baseline (36.5\%), which first denoises the data followed by applying dataset distillation.
Similarly, when IPC is 50, TAD attains 44.2\% accuracy, outperforming ATT (37.1\%) and the two-stage baseline (41.0\%).

