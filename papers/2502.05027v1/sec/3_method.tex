\section{Preliminary and Problem Setup}
\label{sec_3-1}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.81\linewidth]{fig/fig2.pdf}
    \vspace{-1em}
    \caption{
    Overview of the proposed Trust-Aware Diversion (TAD) dataset distillation method. 
    TAD introduces a dual-loop optimization framework for trustworthy dataset distillation. 
    The outer loop divides data into trusted and untrusted spaces, rerouting distillation toward reliable samples, while the inner loop refines untrusted samples for potential reuse. 
    Through iterative interaction, the two loops progressively expand the trusted space and mitigate the impact of noisy labels to achieve data-effective distillation.
    }
    \vspace{-1em}
    \label{fig3}
\end{figure*}


\noindent{\textbf{Dataset Distillation (DD).}} 
Given a large training set $\mathcal{T}=\left\{\left(\bm{x}_{i}, \bm{y}_{i}\right)\right\}_{i=1}^{|\mathcal{T}|}$ containing $|\mathcal{T}|$ images and their labels, dataset distillation aims to synthesize a smaller set $\mathcal{S}=\left\{\left(\bm{\widetilde x}_{i}, \bm{\widetilde y}_{i}\right)\right\}_{i=1}^{|\mathcal{S}|}$. 
The objective of dataset distillation is to optimize the synthetic set $\mathcal{S}$, making sure when a \emph{\textbf{student}} model $f_{\boldsymbol{\theta}^\mathcal{S}}$ is trained on the synthetic dataset $\mathcal{S}$, it can achieve comparable performance 
with an \emph{\textbf{expert}} model $f_{\boldsymbol{\theta}^\mathcal{T}}$ trained on the original dataset $\mathcal{T}$. 


\noindent{\textbf{Trajectory Matching (TM).}} 
The expert model $f_{\boldsymbol{\theta}^\mathcal{T}}$ generates a sequence of parameters to make up an expert trajectory $\{\boldsymbol{\theta}^\mathcal{T}_i\}_{i=1}^{m}$. 
Similarly, the student model generates a trajectory $\{\boldsymbol{\theta}^\mathcal{S}_i\}_{i=1}^{n}$. 
TM-based methods perform distillation by matching the student trajectory with the expert trajectory. 
Starting from step $t$, the matching objective is defined as: 
\begin{equation}
    \label{eq1}
    \mathcal{L}_{TM} = \frac{\|\boldsymbol{\theta}^\mathcal{S}_{t+N} - \boldsymbol{\theta}^\mathcal{T}_{t+M}\|_2^2}
    {\|\boldsymbol{\theta}^\mathcal{T}_{t} - \boldsymbol{\theta}^\mathcal{T}_{t+M}\|_2^2}\,,
\end{equation}
where $N$ and $M$ are skipping steps ($N\ll M$). $\mathcal{L}_{TM}$ is adopted to optimize the synthetic set $\mathcal{S}$. 

\noindent{\textbf{Dataset Distillation with Noisy Labels (DDNL).}} 
Traditional dataset distillation assumes that labels in $\mathcal{T}$ are completely correct.
However, noisy labels are ubiquitous in real-world scenarios and impair model performance.
This work extends the dataset distillation to handle situations where the original training set $\mathcal{T}$ contains noisy data. 
Given $\mathcal{T}' = \left\{ \left( \bm{x}_i, \bm{y}'_i \right) \right\}_{i=1}^{|\mathcal{T}|}$, the label $\bm{y}'_i $ may be noisy. 


To analyze the impact of noisy labels on dataset distillation, we investigate the behavior of cross-entropy (CE) loss and its gradient under noisy labels. 
CE is the most commonly used loss for trajectory matching in dataset distillation. Formally, CE loss and gradient are defined as: 
\begin{align}
\label{eq2}
& \mathcal{L}_\text{CE}\left(f_{\boldsymbol{\theta}}(\boldsymbol{x}), \boldsymbol{y}\right) =  -\sum_{k=1}^{K} \boldsymbol{y}_{(k)} \log (\sigma(\boldsymbol{z})_{k})\,, \\
\label{eq3}
& \frac{\partial \mathcal{L}_{\mathrm{CE}}(f_{\boldsymbol{\theta}}(\boldsymbol{x}), \boldsymbol{y})}{\partial \boldsymbol{\theta}} = \sum_{k=1}^{K} (\sigma(\boldsymbol{z})_k - \boldsymbol{y}_{(k)}) \frac{\partial \boldsymbol{z}_k}{\partial \boldsymbol{\theta}} \,,
\end{align}
where $\boldsymbol{z} = f_{\boldsymbol{\theta}}(\boldsymbol{x})$ denotes the predicted logit value for a sample $\boldsymbol{x}$ 
, $\sigma(\boldsymbol{z})_k$ denotes the $k$-th output of softmax function $\sigma$. 
$\mathcal{L}_{\mathrm{CE}}$ is unbounded and particularly vulnerable to noisy labels~\cite{ghosh2017robust, wei2023mitigating}.
If $\boldsymbol{y}$ is mis-labeled for a true class $k$, then $\boldsymbol{y}_{(k)} = 0$. But for a decent model, $\sigma(\boldsymbol{z})_k \rightarrow 1$. 
Consequently, the gradient updates in Eq.~\ref{eq3} will be misled by noisy labels, causing the parameters to gradually deviate from the ideal trajectory derived from clean data.

In dataset distillation, noisy labels will drive the expert trajectory $\{\boldsymbol{\theta}^\mathcal{T}_i\}_{i=1}^{m}$, away from the ideal path derived from clean data. 
For trajectory matching (Eq.~\ref{eq1}), the student model will align with this distorted expert trajectory and inherit the noise distortions, thus producing suboptimal synthetic data that ultimately degrade the overall performance of dataset distillation. 


\section{Method}

To tackle the challenging DDNL problem, we propose a \textbf{Trust-Aware Diversion} (TAD) dataset distillation method, as shown in Fig.~\ref{fig3}.
Specifically, our proposed TAD introduces an end-to-end dual-loop (\ie, outer loop and inner loop) optimization framework for data-effective distillation. 
The outer loop (Sec.~\ref{sec:outer}) divides data into trusted and untrusted spaces, redirecting distillation toward trusted samples to minimize the impact of mislabeled samples on dataset distillation.
The inner loop (Sec.~\ref{sec:inner}) maximizes the distillation objective by recalibrating untrusted samples, thus transforming them into valuable ones for distillation.
This dual-loop iteratively refines and compensates for each other, gradually expanding the trusted space and shrinking the untrusted space.


\subsection{Outer Loop: Diverting Distillation for Trustworthy Learning}
 \label{sec:outer}
 
Effectively distinguishing clean samples from noisy ones is essential to fully leverage high-quality information while mitigating the adverse effects of noisy labels.
This distinction allows dataset distillation to prioritize learning from more reliable samples, ensuring a trustworthy distillation process.
During model training, the loss associated with each sample can help indicate whether it is a noisy sample~\cite{li2020dividemix, wei2020combating, yao2020searching, zhou2020robust}, as shown in Fig.~\ref{fig2}.
Clean samples are mutually consistent, allowing the model to produce gradient updates more efficiently and to train faster. 
In contrast, noisy samples often contain conflicting information, causing persistent inconsistencies in training progress.
Intuitively, samples with smaller loss values are highly likely to be clean samples during the training process.


Motivated by this, we interpret data as a mixture of two distinct sample categories: trusted and untrusted. 
To model this distinction, we represent loss distribution using a two-mode Gaussian Mixture Model (GMM), where one mode denotes trusted samples with lower losses, and the other represents untrusted samples with higher losses. 
This formulation is expressed as follows:
\begin{equation}
\begin{aligned}
p(\ell_i) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(\ell_i \mid \mu_k, \sigma_k^2),
\end{aligned}
\label{eq4}
\end{equation}
where $\ell_i= \mathcal{L}_{CE}(f_{\boldsymbol{\theta}}(\bm{x}_i), \bm{y}'_i)$ is the cross-entropy loss of the $i$-th sample and $\pi_k$ is the mixing coefficient.
For each sample, its trusted sample confidence $w_i$ is the posterior probability $p(k=1|\ell_i)$, where $k=1$ denotes the component of GMM with a smaller mean $\mu_k$ (\ie, smaller loss).
The GMM parameters are estimated with the Expectation-Maximization algorithm.
The trusted sample confidences $\{ w_i \}$ of the training dataset are then used to divide samples into trusted and untrusted spaces based on a threshold $\bm{\tau}$. 


A straightforward way is to adopt a constant threshold (\eg, 0.5) for all samples, as in~\cite{arazo2019unsupervised, li2020dividemix, yang2022learning, zheltonozhskii2022contrast}. 
However, there are some issues: 
\begin{itemize} 
    \item \emph{
            The learning of the network is progressive. 
            In the early stages, confidence in identifying trusted samples is low.
            A fixed threshold cannot adapt to the estimation of trusted and untrusted samples throughout training.
    }
    \item \emph{
            Different classes have varying learning difficulty levels.
            Employing a unified threshold for all classes inadvertently causes class imbalance, particularly in datasets with numerous classes.
            }
\end{itemize}
To address the above issues, we propose a class-wise dynamic threshold to divide trusted samples and untrusted samples.
For a class $c$, we define the threshold $\bm{\tau}_c$ based on the posterior probabilities of GMM, \emph{i.e.}, the confidence $w_i$ of $\boldsymbol{x}_i$ being a trusted sample. 
Per-class mean confidence score is calculated as the threshold:
\begin{equation}
\begin{aligned}
\bm{\tau}_c = \frac{1}{N_c} \sum_{i=1}^{N_c} w_i, \; w_i = p(k=1|\ell_i) \; \text{ and } \; \bm{y}'_i = c,
\end{aligned}
\label{eq5}
\end{equation}
where $N_c$ is the number of samples in class $c$. 
The mean eliminates the effect of outliers, while the class-wise dynamic thresholds evolve as the training progresses to reflect the training dynamics and imbalanced class difficulties. 


Building on existing dataset distillation approaches~\cite{cazenavette2022distillation, du2023minimizing, guo2024lossless, liu2024dataset}, we use CE loss to train expert networks solely on trusted space, preventing the model from learning misrepresented patterns in mislabeled data.
Additionally, to avoid overfitting to outliers and improve model consistency, we introduce a consistent regularization term $\mathcal{L}_{C}$ by reversing the roles of predicted probabilities and given labels.  
The training objective is formulated as follows:
\begin{equation}
\begin{aligned}
\mathcal{L} &= \mathcal{L}_{CE} + \lambda \mathcal{L}_{C} \\
&= - \sum_{i=1}^N \{\bm{y}'_i \log(f_{\boldsymbol{\theta}}(\bm{x}_i) + \lambda f_{\boldsymbol{\theta}}(\bm{x}_i) \log(\bm{y}'_i) \},
\end{aligned}
\label{eq6}
\end{equation}
where $\lambda$ is a coefficient to balance the two loss items.
For $\bm{y}'_i = 1$, the regularization term $\mathcal{L}_{C}=0$ does not contribute to the loss $\mathcal{L}$.
For $\bm{y}'_i = 0$, $\log(0)$ is defined as a negative constant to ensure numerical stability.
For the mislabeled samples, $\mathcal{L}_{CE}$ imposes a large loss, magnifying the effect of incorrect labels. 
This is counteracted by the $\mathcal{L}_{C}$ term, which reduces excessive loss and stabilizes the training process. 


\subsection{Inner Loop: Trust-Aware Recalibration}
\label{sec:inner}

Although data has been partitioned into trusted and untrusted spaces in the outer loop, some noisy samples may still be mistakenly classified as trusted ones.
To address this issue, we introduce a reliability score in the inner loop to quantify the trustworthiness of each sample in the trusted space, providing a fine-grained evaluation of reliability.
Synthetic data aligning only the initial training phases of expert networks (\eg, the first four epochs) effectively capture representative patterns in real data~\cite{li2024prioritize, guo2024lossless}.
Thus, we use distilled data as stable anchors for assessing data quality.
A reliability score of each trusted sample is then calculated by computing its Mahalanobis distance~\cite{de2000mahalanobis} to the class distributions. 
For accurate label calibration of untrusted samples, we reference only the top-$k$ reliable trusted samples based on reliability scores.


First, anchors are defined as the synthetic images $\{\left(\hat{\bm{x}}_{i}, \hat{\bm{y}}_{i}\right)\}_{i=1}^{N_A}$ synthesized by only matching the early trajectory, which is feasible due to the memorization effects of deep networks~\cite{xia2020robust, arpit2017closer, liu2020early}.
Here $N_A = n \times C$, where $n$ represents the number of synthetic images per class, and $C$ is the total number of classes.
Second, the anchor images and trusted samples are embedded into the feature space by a feature extractor $f$ (\ie, ResNet~\cite{he2016deep}), which is pre-trained in a self-supervision manner (\eg, SimCLR~\cite{chen2020simple}, MoCo~\cite{he2020momentum}).
Third, we calculate the Mahalanobis distance~\cite{de2000mahalanobis} between trusted samples and the per-class anchor distribution. 
Mahalanobis distance is an effective multivariate distance metric used to calculate the distance between a point and a cluster, and it is widely used in multivariate anomaly detection, classification, and clustering analysis~\cite{colombo2022beyond, goswami2024fecam}. 
For each class $c$, the Mahalanobis distance is defined as:
\begin{equation}
\begin{aligned}
D_M(\bm{x}_{i}, \bm{\mu}_c) = \sqrt{(\bm{x}_{i} - \bm{\mu}_c)^T \mathbf{\Sigma}_c^{-1} (\bm{x}_{i} - \bm{\mu}_c)}, 
\end{aligned}
\label{eq7}
\end{equation}
where $\bm{x}_i$ denotes the $i$-th trusted sample, $\bm{\mu}_c$ and $\mathbf{\Sigma}_c^{-1}$ are the estimated class mean and covariance from anchors $\{\left(\hat{\bm{x}}_{i}, \hat{\bm{y}}_{i}\right)\}_{i=1}^{N_A}$ belonging to class $c$.
We then apply min-max normalization to re-scale each $D_M$ to the range $ \left [ 0,1 \right ]$. 
Then, we use the Mahalanobis distance to estimate the reliability of a trusted sample $\bm{x}_i$ and define $\bm{M}_i^c = D_M(\bm{x}_{i}, \bm{\mu}_c)$. 
$\bm{M}_i^c$ reflects the overall proximity of the sample to the anchor distribution of the class $c$. 


To skip the unreliable trusted samples, we only refer to the top-$k$ reliable samples in the trusted space to calibrate the samples in the untrusted space. 
As Mahalanobis distance requires an accurate covariance structure, it is not suitable for comparing distances between untrusted and trusted samples. 
Instead, we use cosine similarity to compare trusted samples $\{\bm{x}_i\}$ with an untrusted sample $\bm{x}'_j$.
The calibrated pseudo label of $\bm{x}'_j$ for each class $c$ is computed as: 
\begin{equation}
p^c(\bm{x}'_j) = \sum_{i=1}^{k} \bm{M}_i^c \cdot \mathit{sim}(\bm{x}'_j, \bm{x}_i) \cdot \delta(y_i = c),
\label{eq10}
\end{equation}
where $\delta(y_i = c)$ is an indicator function equal to $1$ if the label $y_i$ of $\bm{x}_i$ is class $c$, and $0$ otherwise. 
The pseudo label for $\bm{x}'_j$ is then assigned based on the class $c$ with the highest score $p^c(\bm{x}'_j)$.



