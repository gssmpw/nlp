\section{Related Work}

\noindent{\textbf{Dataset Distillation.}} 
Dataset distillation aims to create a compact synthetic dataset that preserves the essential information in the large-scale original dataset, making it more efficient for training while achieving performance comparable to the original dataset.
Dataset distillation was initially introduced by Wang~\etal ~\cite{wang2018dataset}, drawing inspiration from Knowledge Distillation~\cite{hinton2015distilling}.
Subsequent work has explored various methods to enhance the effectiveness of dataset distillation, focusing on different strategies to match the information in the original and synthetic datasets. 
\textit{Gradient matching} aims to align gradients by performing a one-step distance-matching process between the network trained on the original dataset and the same network trained on the synthetic data~\cite{zhao2021datasetcondensation, lee2022dataset, zhao2021dataset}.
\textit{Distribution matching} directly aligns the data distributions of original and synthetic datasets through a single-level optimization process, offering an efficient yet effective strategy for dataset distillation~\cite{sajedi2023datadam, liu2023wasserstein, zhao2023improved, deng2024exploiting}.
\textit{Training trajectory matching} aligns the training trajectories of models trained on the original versus synthetic data, allowing multi-step optimization to capture nuanced information about how model parameters evolve during training~\cite{cazenavette2022distillation, yang2024nsd, du2023minimizing, guo2024lossless, liu2024dataset}. 
Other concurrent works have improved dataset distillation baselines with various approaches, including soft labels~\cite{xiao2024soft, sucholutsky2021soft, qin2024label}, decoupled distillation~\cite{yin2023sre2l, shao2024gvbsm, sun2024rded}, data parameterization~\cite{wei2023sparse, son2024fyi, kim2022dataset},~\etc.
Dataset distillation has enabled various applications including neural architecture search~\cite{medvedev2021tabular, cui2022dc, zhao2021dataset}, continual learning~\cite{sangermano2022sample, yang2023efficient, gu2024ssd}, privacy protection~\cite{chung2024backdoor, li2023sharing, li2024infodist},~\etc. 


\noindent{\textbf{Practical Dataset Distillation.}}  
With the rapid advancement of deep learning techniques and optimization methods, dataset distillation has achieved remarkable progress.
However, these methods assume that all labels are completely correct.
This assumption may not hold in real-world scenarios, where mislabeled or noisy data is ubiquitous. 


Imperfect data can significantly impact the effectiveness of dataset distillation.
The synthetic dataset generated from such data often inherits the inconsistencies and errors of the original dataset. 
As a result, it struggles to accurately capture essential information from the original dataset.
Specifically, surrogate-based methods~\cite{zhao2021datasetcondensation, zhao2021dataset, du2023minimizing, guo2024lossless, liu2024dataset} rely on accurate gradients derived from the original dataset.
Noisy or mislabeled data can distort these gradients, leading to incorrect optimization.
In addition, distribution-based methods~\cite{sajedi2023datadam, zhao2023improved, yin2023sre2l, sun2024rded} aim to match distributions between the original and synthetic datasets in a randomly initialized or pretrained network.
However, noisy data makes it difficult to capture the true data distribution.


A straightforward approach to addressing this issue involves first denoising the data and then applying dataset distillation.
This includes identifying mislabeled data~\cite{han2018co, li2020dividemix, albert2022addressing, northcutt2021confident} and then either removing it or assigning a pseudo-label~\cite{tanaka2018joint, yi2019probabilistic}.
However, the independent denoising step has side effects in dataset distillation.
The denoising step, despite its effectiveness, may introduce biases by misidentifying noisy data, leading to the removal of useful samples or the assignment of incorrect labels. 
As a result, some erroneous labels or pseudo-labels inevitably propagate to the dataset distillation stage, amplifying errors and compromising its effectiveness.
This motivates us to design a trustworthy data-effective dataset distillation method through an iterative dual-loop optimization framework.

