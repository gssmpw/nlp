\section{Related Work}
% Jodie, EdgeBank, TGB, TGB2.0, DTGB(Dynamic Text-attributed Graph Benchmark), A Robust Comparative Analysis of Graph Neural Networks on Dynamic Link Prediction,
\header{\bf Temporal Graph Datasets and Benchmarks.}
Several studies~\citep{edgebank,TGB} have highlighted that existing benchmarks for dynamic graph learning often lead to overly optimistic assessments of current approaches. Specifically, widely used datasets such as Reddit, Wikipedia, MOOC, and LastFM~\citep{Jodie} suffer from inconsistent preprocessing and simplistic negative sampling strategies, which result in inflated performance metrics and unreliable comparisons. To address these issues, BenchTeMP~\citep{Benchtemp} provides a unified evaluation framework with consistent datasets and comprehensive performance metrics. \citet{edgebank} construct six dynamic graph datasets across diverse domains, such as politics, economics, and transportation, and introduce two negative sampling strategies to create more challenging evaluations. 
TGB~\citep{TGB} further advances the field by introducing several large datasets for future link prediction, establishing a comprehensive benchmark based on the unified evaluation metric of MRR.
~\citet{gastinger2024tgb2} extend TGB with multi-relational datasets for Temporal Knowledge Graphs (TKG) and Temporal Heterogeneous Graphs (THG). These extensions focuses on future link prediction on large-scale TKG and THG datasets.
% Different from previous work that emphasizes expanding dataset diversity, scale, and evaluation protocols, we construct a collection of challenging benchmark datasets that originate from typical application scenarios of future link prediction, challenging the existing temporal GNNs with complex sequential dynamics inherently in these applications.
%we focus on analyzing the characteristics of existing datasets and constructing more complex ones that originate from typical application scenarios of future link prediction.

\header{\bf Temporal Graph GNNs for future link prediction.} % dtdg, ctdg
Future link prediction is a critical task in various dynamic systems, which aim to predict future interactions or relationships between entities based on historical data. 
% Early research treated temporal graphs as a series of snapshots, applying static GNNs to encode them and using sequence analysis to capture temporal changes~\citep{GRNN, xu2019generative, Evolvegcn, Dysat, ROLAND}. 
% However, recent studies take continuous-time models as a paradigm, which view the input as a stream of interactions to preserve the evolution of temporal graphs. 
% In this paper, we also focus on the continuous-time dynamic graphs, as it better reflects how dynamic graphs form incrementally in real-world scenarios. 
To capture the evolution pattern, memory-based methods such as TGN~\citep{TGN}, Jodie~\citep{Jodie}, DyRep~\citep{DyRep}, and APAN~\citep{APAN}, use dynamic memory modules to store and update node information during interactions, allowing for more effective modeling. 
On the other hand, approaches like TGAT~\citep{TGAT}, CAWN~\citep{CAWN}, TCL~\citep{TCL}, GraphMixer~\citep{GraphMixer}, and DyGFormer~\citep{DyGFormer}, aggregate historical neighbor information directly during prediction without memory modules. These methods employ contrastive learning and Transformer-based techniques to capture evolving node interactions and temporal dependencies. The Hawkes process~\citep{Hawkes1,Hawkes2} is another widely used technique for capturing the impacts of historical events on current events and is employed by methods such as TREND~\citep{TREND} and LDG~\citep{LDG}, etc.
% utilizes the Hawkes process to model the exciting effects between sequential interactions and captures both individual and collective characteristics of events by integrating event and node dynamics.
Drawing inspiration from natural language processing (NLP) studies, SimpleDyG~\citep{simpledyg} models dynamic graphs as a sequence modeling problem, using a simple Transformer architecture without complex modifications.
\citet{edgebank} observe that edges reoccur over time in the existing datasets and propose a simple memory-based heuristic approach, EdgeBank, without any learnable components. 
This method predicts edges based solely on past observations, yet it demonstrated remarkable performance in current evaluations. 
This further highlights the need for more comprehensive benchmarks that assess models' ability to generalize to unseen edges, thereby ensuring robust performance in real-world scenarios.