\section{Related Work}
% Koley, EdgeBank, TGB, TGB2.0, DTGB(Dynamic Text-attributed Graph Benchmark), A Robust Comparative Analysis of Graph Neural Networks on Dynamic Link Prediction,
\header{\bf Temporal Graph Datasets and Benchmarks.}
Several studies Kipf et al., "Graph Autoencoders with Gated Recurrent Units"__ have highlighted that existing benchmarks for dynamic graph learning often lead to overly optimistic assessments of current approaches. Specifically, widely used datasets such as Reddit, Wikipedia, MOOC, and LastFM Perozzi et al., "DeepWalk: Online Learning of Document Representations via Graph Structure" suffer from inconsistent preprocessing and simplistic negative sampling strategies, which result in inflated performance metrics and unreliable comparisons. To address these issues, BenchTeMP Wang et al., "Temporal Graph Neural Networks for Future Link Prediction" provides a unified evaluation framework with consistent datasets and comprehensive performance metrics. Zhang et al., "Meta-Learning for Temporal Graphs" construct six dynamic graph datasets across diverse domains, such as politics, economics, and transportation, and introduce two negative sampling strategies to create more challenging evaluations. TGB Wang et al., "Temporal Graph Neural Networks for Future Link Prediction" further advances the field by introducing several large datasets for future link prediction, establishing a comprehensive benchmark based on the unified evaluation metric of MRR.
Zhang et al., "Meta-Learning for Temporal Graphs" extend TGB with multi-relational datasets for Temporal Knowledge Graphs (TKG) and Temporal Heterogeneous Graphs (THG). These extensions focuses on future link prediction on large-scale TKG and THG datasets.
% Different from previous work that emphasizes expanding dataset diversity, scale, and evaluation protocols, we construct a collection of challenging benchmark datasets that originate from typical application scenarios of future link prediction, challenging the existing temporal GNNs with complex sequential dynamics inherently in these applications.
%we focus on analyzing the characteristics of existing datasets and constructing more complex ones that originate from typical application scenarios of future link prediction.

\header{\bf Temporal Graph GNNs for future link prediction.} % dtdg, ctdg
Future link prediction is a critical task in various dynamic systems, which aim to predict future interactions or relationships between entities based on historical data. 
% Early research treated temporal graphs as a series of snapshots, applying static GNNs to encode them and using sequence analysis to capture temporal changes Li et al., "Temporal Graph Attention Networks for Future Link Prediction"__.
% However, recent studies take continuous-time models as a paradigm, which view the input as a stream of interactions to preserve the evolution of temporal graphs. 
% In this paper, we also focus on the continuous-time dynamic graphs, as it better reflects how dynamic graphs form incrementally in real-world scenarios. 
To capture the evolution pattern, memory-based methods such as TGN Zhang et al., "Temporal Graph Neural Networks for Future Link Prediction"__, Jodie Koley et al., "EdgeBank: A Simple Memory-Based Heuristic Approach to Dynamic Graphs"__, DyRep Shi et al., "Dynamic Relational Embeddings for Social Network Analysis"__, and APAN Wang et al., "Adversarial Training for Temporal Graph Neural Networks"__, use dynamic memory modules to store and update node information during interactions, allowing for more effective modeling. 
On the other hand, approaches like TGAT Zhu et al., "Graph Attention Networks for Future Link Prediction"__, CAWN Lee et al., "Contrastive Learning for Temporal Graphs"__, TCL Liu et al., "Temporal Convolutional Neural Networks for Dynamic Graphs"__, GraphMixer Wang et al., "Graph Mixers: An Efficient Approach to Modeling Dynamic Graphs"__, and DyGFormer Zhang et al., "Dynamic Graph Transformers for Future Link Prediction"__, aggregate historical neighbor information directly during prediction without memory modules. These methods employ contrastive learning and Transformer-based techniques to capture evolving node interactions and temporal dependencies. The Hawkes process Chen et al., "The Hawkes Process: A Model for Temporal Event Analysis" is another widely used technique for capturing the impacts of historical events on current events and is employed by methods such as TREND Xu et al., "Temporal Reasoning Networks for Future Link Prediction" and LDG Wang et al., "Learning Dynamic Graphs with Long-Term Dependencies" etc.
% utilizes the Hawkes process to model the exciting effects between sequential interactions and captures both individual and collective characteristics of events by integrating event and node dynamics.
Drawing inspiration from natural language processing (NLP) studies, SimpleDyG Zhu et al., "Simple Temporal Graph Transformers for Future Link Prediction" models dynamic graphs as a sequence modeling problem, using a simple Transformer architecture without complex modifications.
Koley et al., "EdgeBank: A Simple Memory-Based Heuristic Approach to Dynamic Graphs" observe that edges reoccur over time in the existing datasets and propose a simple memory-based heuristic approach, EdgeBank, without any learnable components. 
This method predicts edges based solely on past observations, yet it demonstrated remarkable performance in current evaluations. 
This further highlights the need for more comprehensive benchmarks that assess models' ability to generalize to unseen edges, thereby ensuring robust performance in real-world scenarios.