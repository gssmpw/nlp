\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
% \usepackage{xcolor}  % 导入颜色宏包
\usepackage{hyperref}  % 导入超链接宏包
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl} % 导入包
\usepackage[table,xcdraw]{xcolor}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}
\title{Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for UAV-View Geo-Localization} %标题
\author{Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong\\
\thanks{This paper is submitted for review on \today. This work was supported in part by the Key Research and Development Program of Shaanxi, PR China (No. 2023-YGBY-235), the National Natural Science Foundation of China (No. 61976172 and No. 12002254), Major Scientific and Technological Innovation Project of Xianyang, PR China (No. L2023-ZDKJ-JSGG-GY-018). (Corresponding author: Zhao-Xu Yang and Hai-Jun Rong)}

\thanks{Zhongwei Chen, Zhao-Xu Yang and Hai-Jun Rong  are with the State Key Laboratory for Strength and Vibration of Mechanical Structures, Shaanxi Key Laboratory of Environment and Control for Flight Vehicle, School of Aerospace Engineering, Xi’an Jiaotong University, Xi’an 710049, PR China (e-mail:ISChenawei@stu.xjtu.edu.cn; yangzhx@xjtu.edu.cn; hjrong@mail.xjtu.edu.cn).}}

% \markboth{IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY}%
% {How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
UAV-View Geo-Localization (UVGL) aims to achieve accurate localization of unmanned aerial vehicles (UAVs) by retrieving the most relevant GPS-tagged satellite images. However, existing methods heavily rely on pre-paired UAV-satellite images for supervised learning. Such dependency not only incurs high annotation costs but also severely limits scalability and practical deployment in open-world UVGL scenarios. To address these limitations, we propose an end-to-end self-supervised UVGL method. Our method leverages a shallow backbone network to extract initial features, employs clustering to generate pseudo labels, and adopts a dual-path contrastive learning architecture to learn discriminative intra-view representations. Furthermore, our method incorporates two core modules, the dynamic hierarchical memory learning module and the information consistency evolution learning module. The dynamic hierarchical memory learning module combines short-term and long-term memory to enhance intra-view feature consistency and discriminability. Meanwhile, the information consistency evolution learning module leverages a neighborhood-driven dynamic constraint mechanism to systematically capture implicit cross-view semantic correlations, thereby improving cross-view feature alignment. To further stabilize and strengthen the self-supervised training process, a pseudo-label enhancement strategy is introduced, which refines the quality of pseudo supervision. Our method ultimately constructs a unified cross-view feature representation space under self-supervised settings. Extensive experiments on three public benchmark datasets demonstrate that the proposed method consistently outperforms existing self-supervised methods and even surpasses several state-of-the-art supervised methods. These results validate the effectiveness and generalizability of our method, highlighting its potential for real-world deployment in UVGL. {Our code is available at {\href{https://github.com/ISChenawei/DMNIL}{https://github.com/ISChenawei/DMNIL}}}.
\end{abstract}

\begin{IEEEkeywords}
UAV-View Geo-Localization, Self-Supervised Learning, Contrastive Learning.
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{U}{AV-View} Geo-Localization (UVGL) aims to achieve high-precision localization of UAV in GPS-denied scenarios by retrieving GPS-tagged satellite images that correspond to UAV-captured near-ground view images \cite{shen2023mccg}. This capability is critical for applications such as autonomous navigation, disaster rescue, and emergency response. In recent years, existing UVGL methods have primarily relied on supervised learning using strictly paired UAV-satellite images \cite{chen2024multi, chen2024sdpl, xia2024enhancing}. Although these supervised methods have achieved encouraging success, they also reveal several inherent limitations: (1) acquiring instance-level paired UAV-satellite images is highly costly \cite{li2024learning}, leading to increased training overhead; (2) large amounts of unpaired cross-view image resources remain underutilized; and (3) when applied to new scenarios, these methods often require recollection and re-pairing UAV-satellite images for training to adapt to domain shifts. These limitations significantly hinder the scalability and practical deployment of UVGL in real-world scenarios. Thereby, it is necessary and of great significance to explore a self-supervised UVGL method that can effectively mine latent cross-view correlations from large-scale unpaired UAV-satellite images.
\begin{figure}[t]
  \centering
  \includegraphics[width=3.2in]{1.pdf}
  \caption{(A) ReID tasks are characterized by relatively small intra-class variations and pronounced inter-class differences, which facilitates effective feature discrimination. (B) In contrast, UVGL tasks suffer from large intra-class variations and small inter-class differences, leading to significant cross-view gaps and intra-view ambiguities that hinder feature discrimination.}
  \label{fig1}
  \end{figure}
  
In recent years, self-supervised person re-identification (ReID) methods \cite{yin2023real,9840394} have achieved remarkable progress, offering crucial theoretical foundations for UVGL. Typically, These methods employ clustering algorithms \cite{ester1996density, macqueen1967some} on features extracted by a backbone network to generate pseudo-labels, which serve as the initial supervision signals for self-supervised learning. However, the quality of these pseudo-labels is heavily dependent on the discriminative capability of backbone network. As illustrated in Fig. \ref{fig1}, compared with conventional ReID, UVGL faces additional challenges. These factors make it more difficult for the backbone to extract discriminative features, as illustrated in Fig. \ref{fig2}. When pseudo-labels contain a high error rate and the model lacks the capacity to capture intra-view and cross-view consistency and discriminative features, the self-supervised learning process becomes prone to catastrophic negative optimization.
\begin{figure}[t]
  \centering
  \includegraphics[width=3.1in]{2.pdf}
  \caption{Recall@k performance of ConvNeXt-Tiny\cite{liu2022convnet}, using pre-trained weights from ImageNet-22k\cite{deng2009imagenet}, for direct retrieval tests on the University-1652\cite{zheng2020university} UVGL benchmark and the mainstream ReID benchmarks, including MSMT17\cite{Wei_2018_CVPR}, DukeMTMC-reID\cite{zheng2017discriminatively}, CUHK03\cite{sun2014deep}, and Market1501\cite{zheng2015scalable}.}
  \label{fig2}
  \end{figure}
In this paper, we propose a novel end-to-end self-supervised UVGL method. A lightweight, shallow ConvNeXt-Tiny backbone network \cite{liu2022convnet} is employed to extract features, which are subsequently clustered to generate pseudo-labels. Combined with a dual-path contrastive learning strategy \cite{dai2022cluster,adca}, these components form the baseline network of our self-supervised method for UVGL to learn intra-view feature representations. Building upon this baseline, we proposed the dynamic hierarchical memory learning (DHML) module and the information consistency evolution learning (ICEL) module as two key components of our method. The DHML module captures the relationship between short-term and long-term memory to learn the discriminability of intra-view feature representations. The ICEL module constructs a neighborhood-driven dynamic constraint mechanism based on feature distributions. By integrating neighborhood consistency constraints with mutual information optimization, it effectively aligns cross-view feature representations within a shared embedding space, promoting both consistency and discriminative learning. Additionally, to improve the reliability of pseudo-labels, we introduce a pseudo-label enhancement (PLE) strategy, which contributes to a more stable and effective self-supervised feature learning process. The main contributions can be summarized as follows:

\begin{itemize}
  \item We propose an end-to-end self-supervised method for UVGL. Built upon a shallow backbone and a dual-path contrastive learning strategy, our method integrates dynamic hierarchical memory learning and information consistency evolution learning to effectively capture and enhance feature consistency and discriminability for both intra-view and cross-view representation learning.
  \item We propose a dynamic memory hierarchical learning module that captures the relationships among different levels of memory to adaptively model intra-class and inter-class feature dynamics within the intra-view, thereby enhancing the discriminability of feature representations.
  \item we propose a novel information consistency evolution learning module which combines neighborhood consistency and mutual information optimization to learn consistent and discriminative cross-view representations.
  \item Extensive experiments demonstrate that our method, utilizing only a lightweight backbone, achieves state-of-the-art performance on three UVGL benchmarks, outperforming even certain supervised methods.
\end{itemize}
The remainder of this paper is organized as follows. Section \ref{related works} systematically reviews prior research. In Section \ref{method}, the proposed self-supervised UVGL method is presented in detail. The experimental results are reported and analyzed in Section \ref{results}. Finally, conclusions are outlined in Section \ref{conclusions}.

\section{RELATED WORKS}\label{related works}
This section establishes a systematic and comprehensive theoretical framework to support the development of the proposed self-supervised UVGL method. Although this work focuses on UVGL as a sub-task of cross-view geo-localization (CVGL), we first review the overall progress in supervised CVGL. We then discuss recent advances in self-supervised CVGL methods. Finally, we summarize main methodes in self-supervised ReID.
\subsection{Supervised Cross-View Geo-localization}
CVGL aims to achieve high-precision localization of query images by retrieving GPS-tagged satellite images. Early research in CVGL primarily focused on the ground-view CVGL \cite{Shi_Yu_Liu_Zhang_Li_2020,zhang2023cross,10601183}, leading to the development of several benchmark datasets, such as CVUSA \cite{zhai2017predicting}, CVACT \cite{2019Lending}, and VIGOR \cite{zhu2021vigor}. To address the challenge of ground-view CVGL, researchers have proposed various methods. CVM-Net 
\cite{Hu_2018_CVPR} employed NetVLAD \cite{arandjelovic2016netvlad} to encode images as global descriptors, thereby reducing visual discrepancies caused by viewpoint variations. TransGeo \cite{zhu2022transgeo} utilized attention-based zooming and sharpness-aware optimization to enhance detail learning and improve model generalization. GeoDTR \cite{zhang2023cross} leveraged a Transformer-based \cite{liu2022swin} feature extractor to disentangle geometric features, effectively addressing perspective shifts. Sample4Geo \cite{deuser2023sample4geo} introduced a hard negative mining strategy, focusing on challenging negative samples to further improve discriminative capability.

With the continuous development and application of drone technology, UVGL has gradually become an important research direction and achieves significant progress \cite{wu2024camp,lv2024direction,du2024ccr}. Several benchmark datasets for UVGL have been proposed, such as University-1652\cite{zheng2020university}, SUES-200, \cite{zhu2023sues} and DenseUAV \cite{DenseUAV}, which have driven further research in this field. In this scenario, a variety of methods have been proposed by researchers. LPN \cite{wang2021each} mined fine-grained features through local pattern partitioning, improving the ability to capture local details. IFS \cite{ge2024multibranch} utilized a multi-branch strategy to effectively fuse global and local features, enhancing the diversity and robustness of feature representations. CAMP \cite{wu2024camp} improved cross-view matching accuracy by contrastive attribute mining and position-aware partitioning, while DAC \cite{xia2024enhancing} enhanced feature consistency and matching accuracy by introducing domain alignment and scene consistency constraints. MEAN \cite{chen2024multi} adopted progressive embedding diversification, global-to-local associations, and cross-domain enhanced alignment to further improve feature representation and alignment.

Nevertheless, the remarkable performances exhibited by these UVGL methods require extensive pre-processed UAV-satellite image pairs. In this work, we pivot our focus to the realm of self-supervised UVGL, where prior pairing relationships are absent, presenting important applications for real-world UVGL deployments.

\subsection{Self-Supervised Cross-View Geo-localization}
Due to the dependence of CVGL on extensive pre-processed image pairs, which significantly increases costs and limits applications for real-world, researchers have begun to explore self-supervised CVGL methods. The work in \cite{Li_2024_CVPR} proposed a self-supervised method that leveraged correspondence-free projection to transform ground panorama images into bird’s-eye view images. It also applied CycleGAN \cite{Zhu_2017_ICCV} to generate fake satellite images, coupled with an additional SAFA module \cite{shi2019spatial} to reduce discrepancies. However, this explicit alignment method has inherent limitations in capturing the deep semantic relationships of cross-view features and may introduce noise. Furthermore, the non-end-to-end training framework employed significantly increases training time and computational costs, reducing the efficiency and practical applicability. Additionally, existing work \cite{li2024learning} utilized frozen foundation models (FMs) \cite{oquab2023dinov2,radford2021learning} to extract features, and a self-supervised adapter was introduced to mitigate visual discrepancies between different point of view. However, this method heavily relied on the ability of FMs for feature extraction. When FMs fail to effectively handle viewpoint discrepancies, the performance of the adapter is also affected. To reduce reliance on the feature extractor, CDIKTNet \cite{chen2025limited} trained with a small amount of paired UAV-satellite images to optimize the initial feature distribution, and then performed clustering to generate pseudo-labels, thereby improving the quality of pseudo-labels generation. However, this method inherently depends on the feature extractor, as it still requires limited paired UAV-satellite images for effective learning. 

The above methods have made meaningful exploratory attempts. However, these methods still face challenges, such as the additional computation time and overhead introduced by non-end-to-end frameworks, as well as dependence on the capability of feature extractor. In contrast, our method is fully end-to-end, eliminating the need for additional training processes and completely removing the reliance on feature extractor initialization.

\subsection{Self-Supervised Person Re-Identification}
ReID \cite{10703072,9585547} aims at matching the same person image captured by non-overlapping cameras. Due to its critical role in video surveillance, ReID has attracted extensive research attention and achieved remarkable progress. However, these achievements are facilitated by extensive human-labeled data. To address this limitation, self-supervised learning methods \cite{yin2023real,9840394} have been proposed. Among them, clustering algorithms are widely employed to generate pseudo-labels, which serve as supervision signals for model training. In addition, benefiting from the introduction of instance-level contrastive learning \cite{Wu_2018_CVPR} with memory banks and momentum update strategies \cite{He_2020_CVPR}, many methods \cite{chen2021ice,wang2022optimal} consider each unlabeled sample as an individual class to learn discriminative instance-level feature representations. However, these instance-level features was modified largely during training, resulting in unstable representations \cite{10882953}. To address this issue, recent studies \cite{Yang_2023_ICCV,cheng2023efficient} have begun exploring cluster-level relationships to discover more stable and robust associations among pedestrian representations. Cluster Contrast \cite{dai2022cluster} addresses the problem of cluster inconsistency by performing clustering and introducing a distinct representation for each cluster. Nevertheless, in cross-modal ReID scenarios, where images are captured by heterogeneous sensors, modality discrepancies hinder the effectiveness of these methods in learning modality-invariant features. ADCA\cite{adca} introduced a dual-path contrastive framework to bridge modality gaps and enhance cross-modal feature alignment. Building on this, SDCL \cite{yang2024shallow} employs a dual-path Transformer for shallow-to-deep contrastive learning, and refines pseudo-labels via consistency constraints across feature hierarchies, leading to more stable and discriminative cross-modal representations.

The above methods focus on alleviating the dependency on human-labeled data in the ReID and provide valuable insights for UVGL. However, UVGL suffers from more severe viewpoint and scale variations. Furthermore, the spatial continuity of geographic environments inevitably induces feature ambiguity, severely degrading clustering effectiveness. Thereby, we incorporate instance-level and cluster-level features into a self-supervised UVGL method to learn more discriminative and robust cross-view representations.
\begin{figure*}[t]
  \centering
  \includegraphics[width=7.2in]{3.pdf}
  \caption{The overall pipeline of our method consists of a lightweight backbone, a dual-path contrastive learning strategy, a dynamic hierarchical memory learning module, and an information consistency evolution module. Specifically, the dual-path contrastive learning is designed to learn discriminative and consistent intra-view feature representations. The dynamic hierarchical memory module further captures intra-view feature variations under different perspectives and scales, thereby enhancing the robustness and discriminability of the learned representations. The information consistency evolution module focuses on modeling cross-view feature consistency through a neighborhood-driven learning strategy, and further improves the training process by integrating a pseudo-label enhancement strategy.}
  \label{fig3}
  \end{figure*}
\section{METHOD}\label{method}
As illustrated in Fig.~\ref{fig3}, the proposed framework is composed of two key modules, DHML and ICEL. Additionally, a PLE strategy is employed to optimize the self-supervised learning process. Specifically, we adopt a weight-shared ConvNeXt-Tiny network as the backbone, equipped with cluster memory and a dual-path contrastive learning strategy to learn intra-view feature representations. To capture the dynamic variations of intra-class features caused by changes in viewpoint and scale, the DHML module establishes a collaborative relationship between short-term and long-term memory, thereby enhancing the discriminability of intra-view feature representations. Furthermore, the ICEL module leverages instance memory to construct neighborhood-driven dynamic constraints. By integrating neighborhood consistency with mutual information optimization, it guides cross-view features to form unified and consistent representations.
%问题定义 

\textit{Problem Formulation}: In the UVGL, we are given a set of paired UAV-satellite images, denoted by \(\{x^d, x^s\}\). Here, \(x^d\) represents UAV-view images and \(x^s\) represents satellite-view images. Unlike the supervised setting, our self-supervised method is developed without using paired UAV-satellite images, where the correspondences between UAV-view and satellite-view images are entirely unavailable. Thereby, we randomly sample UAV and satellite images independently during training. The objective is to discover reliable cross-view correspondences under such conditions and accomplish the UVGL task.

\subsection{Dual-Path Contrastive Learning Baseline}\label{baseline}
Cluster-based contrastive learning methods have achieved remarkable success in various fields. However, in UVGL, due to the unique challenges of cross-view scenarios, single-path cluster-based contrastive learning struggles to effectively capture view-specific features. To address this issue, we design the baseline network with a dual-path architecture, where UAV-view and satellite-view features are extracted using a shared backbone and clustered to assign pseudo-labels. Subsequently, a dual-path contrastive learning strategy \cite{yang2023dual} is employed to enhance intra-view representation learning.

We first introduce the notations used in this paper for better clarity. Let \( X_d = \{x^d_i\}_{i=1}^N \) denote a set of drone images with \( N \) instances, and \( X_s = \{x^s_i\}_{i=1}^M \) denote a set of satellite images with \( M \) instances. \( F_d = \{\mathbf{f}^d_i\}_{i=1}^N \) and \(F_s = \{\mathbf{f}^s_i\}_{i=1}^M \) represent the feature representations extracted by the shared feature extractor \( \mathcal{F}_{backbone} \) for the drone and satellite views, respectively. \( q^d\) and \( q^s\) denote the drone query instance features and satellite query instance features extracted by the feature extractor \( \mathcal{F}_{backbone} \).
The processing procedure of the backbone can be described as
\begin{equation}
\mathbf{f}^j_i = \mathcal{F}_{\text{backbone}}(x^j_i), \quad
\begin{cases}
i \in \{1, 2, \dots, N\}, & \text{if } j = d  \\
i \in \{1, 2, \dots, M\}, & \text{if } j = s \ 
\end{cases}
\end{equation}

\textit{Cross-View Feature Memory Initialization}. At the beginning of each training epoch, the feature memories for both views are initialized. Specifically, the cluster representations of the drone view \(\{ \phi_1^d, \phi_2^d, \cdots, \phi_K^d \}\) and the satellite view \(\{ \phi_1^s, \phi_2^s, \cdots, \phi_L^s \}\) are stored in the drone memory dictionary \(\mathcal{M}_d\) and satellite memory dictionary \(\mathcal{M}_s\), respectively, for view-specific memory initialization. This process can be formalized as
\begin{equation}
\mathcal{M}_d\ \leftarrow \{\phi_1^d, \phi_2^d, \cdots, \phi_K^d \}, \mathcal{M}_s\ \leftarrow \{ \phi_1^s, \phi_2^s, \cdots, \phi_L^s \}
\label{eq2}
\end{equation}

\begin{equation}
\phi_k^d = \frac{1}{|\mathcal{H}_k^d|} \sum_{\mathbf{f}_n^d \in \mathcal{H}_k^d} \mathbf{f}_n^d, \phi_l^s = \frac{1}{|\mathcal{H}_l^s|} \sum_{\mathbf{f}_m^s \in \mathcal{H}_l^s} \mathbf{f}_m^s
\end{equation}
where \(n=1,\cdots,N\), \(m=1,\cdots,M\), \(k=1,\cdots,K\), \(l=1,\cdots,L\), \(\mathcal{H}_{k(l)}^{d(s)}\) denotes the $k$-th cluster set in drone or satellite view, \(| \cdot |\) indicates the number of instances per cluster.

\textit{Cross-View Feature Memory Updating}.
During the training phase, we randomly sample \( P \) distinct localization points from the training set, with each point comprising a fixed number \( Z \) of instances. This sampling strategy results in a minibatch containing a total of \( P \times Z \) query images. Subsequently, the query features extracted from the minibatch are integrated into the cluster representations via a momentum-based update mechanism. Through this method, we iteratively update the features for both perspectives. This process can be formalized as
\begin{equation}
\quad \phi_k^{d(\omega)} \leftarrow \alpha \phi_k^{d(\omega-1)} + (1 - \alpha) q^d
\label{eq4}
\end{equation}
\begin{equation}
\quad \phi_k^{s(\omega)} \leftarrow \alpha \phi_k^{s(\omega-1)} + (1 - \alpha) q^s
\label{eq5}
\end{equation}
where \(\alpha\) is the momentum updating factor. \(\omega\) is the iteration number.

\textit{Cross-View Contrastive Loss}.
Given drone and satellite query \( q^d\) and \( q^s\), we compute the contrastive loss for drone view and satellite view by the following equations:
\begin{equation}
\mathcal{L}^{d}_{cv} = - \log \frac{\exp(q^{d} \cdot \phi^{d}_{+} / \tau)}{\sum_{k=0}^{K} \exp(q^{d} \cdot \phi^{d}_k / \tau)} 
\end{equation}
\begin{equation}
\mathcal{L}^{s}_{cv} = - \log \frac{\exp(q^{s} \cdot \phi^{s}_{+} / \tau)}{\sum_{k=0}^{K} \exp(q^{s} \cdot \phi^{s}_l / \tau)} 
\end{equation}
where \(\tau\) denotes the temperature hyper-parameter. The cluster centroids \(\phi^{d(s)}_{k(l)}\) are utilized as feature vectors at the cluster level to compute the distances between the query instance \(q_{d(s)}\) and all clusters. Here,  \(\phi^{d(s)}_+\) corresponds to the positive cluster feature associated with \(q_{d(s)}\).

The final optimization for baseline is denoted by the following combination:
\begin{equation}
\mathcal{L}_{cv} = \mathcal{L}^{d}_{cv} + \mathcal{L}^{s}_{cv}
\end{equation}
% \textcolor{red}{\textit{Discussion}}

\subsection{Dynamic Hierarchical Memory Learning }
Geographic environments inherently exhibit spatial continuity, where neighboring regions usually present similar visual patterns, while distant regions display significant differences. However, such spatial characteristics often introduce ambiguity in intra-view feature representations. To address this, we propose a DHML module that jointly exploits long-term and short-term memory to capture spatial dynamics and enhance the discriminability of intra-view features.

Consistent with the baseline network, we construct the dynamic hierarchical memory learning in a dual-path manner, using drone feature extraction as an illustrative example. To build the long-term memory, we first construct two novel clustering memory dictionaries according to Eq.\ref{eq2}, which are used to store the short-term \(\mathcal{M}_d^s\) and long-term \(\mathcal{M}_d^l\) memories, respectively. 
\begin{equation}
\mathcal{M}_d^s\ \leftarrow \{\varphi_1^{ds}, \varphi_2^{ds}, \cdots, \varphi_K^{ds} \}
\end{equation}
\begin{equation}
\mathcal{M}_d^l\ \leftarrow \{\varphi_1^{dl}, \varphi_2^{dl}, \cdots, \varphi_K^{dl} \}
\end{equation}
where, \( \varphi_1^{ds}, \varphi_2^{ds}, \dots, \varphi_K^{ds} \) represent the short-term memory embeddings that encode the cluster representations of the drone view, and \( \varphi_1^{dl}, \varphi_2^{dl}, \dots, \varphi_K^{dl} \) represent the long-term memory embeddings that store historically accumulated stable representations of the drone view.

Unlike the weight updating scheme in Eq.\ref{eq4} and Eq.\ref{eq5}, we assign different weights to the input features and the historical memory during the long-term memory update. Specifically, since the momentum parameter is set to 0.2 throughout this study, we enhance the influence of historical memory during the initialization of long-term memory updates. The update procedure can defined as
\begin{equation}
\quad \varphi_k^{dl(\omega)} \leftarrow (0.5 - \alpha)\phi_k^{dl(\omega-1)} +  \alpha  q_{dl}
\end{equation}
where, \( q_{dl} \) represents the drone query instance features. This obtain a new dictionary of long-term memory .

Additional, we further introduce an adaptive weight updating mechanism to update the long-term memory. In detail, we compute the Euclidean distance between the current input and its corresponding long-term memory, then take the average over all samples in the batch. This average distance is then mapped to the range \([0,1]\) via the sigmoid function, yielding an adaptive update coefficient \(\beta\):
\begin{equation}
\beta = \sigma\!\left(\frac{1}{B}\sum_{i=1}^{B} \| q_{dl} - \phi_k^{dl(\omega-1)} \| \right)
\end{equation}
where, \(B\) denotes the batch size and \(\sigma(\cdot)\) represents the sigmoid function. The short-term memory is subsequently updated as follows:
\begin{equation}
\quad \varphi_k^{ds(\omega)} \leftarrow \beta \cdot \varphi_k^{dl(\omega-1)} + (1 - \beta) \cdot \varphi_k^{ds(\omega-1)}
\end{equation}

Then, the long-term and short-term memories are fused in a weighted manner to yield the combined feature \(\varphi_k^{db}\) representation:
\begin{equation}
\varphi_k^{db(\omega)} = w_l \cdot \varphi_k^{dl(\omega)} + w_s \cdot \varphi_k^{ds(\omega)}
\end{equation}
where \(w_l\) and \(w_s\) denote the respective fusion weights.
As a result, we obtain a comprehensive memory dictionary \(\mathcal{M}_d^{cb}\) that effectively integrates long-term global consistency with localized short-term adaptability.

Given the drone query \( q^d \), we compute the contrastive loss by measuring their similarity to the cluster representations maintained in the memory dictionary by the following equations:
\begin{equation}
\mathcal{L}^{dd}_{id} = - \log \frac{\exp(q^{d} \cdot \varphi^{db}_{+} / \tau)}{\sum_{k=0}^{K} \exp(q^{d} \cdot \varphi^{db}_k / \tau)} 
\end{equation}
where \(\tau\) denotes the temperature hyper-parameter. The combined cluster centroids \(\phi^{db}_{k}\) are utilized as feature vectors at the cluster level to compute the distances between the query instance \(q^{d}\) and all clusters. Here,  \(\phi^{db}_+\) corresponds to the positive cluster feature associated with \(q^{d}\).

Finally, we introduce a fusion factor \( \lambda_{cv} \) to perform weighted fusion of \(\mathcal{L}^{dd}_{id}\) and \(\mathcal{L}^{d}_{cv}\), aiming to enhance the discriminability of intra-view feature representations and obtain the optimized loss $\mathcal{L}_{dhml}^d$. Similarly, $\mathcal{L}_{dhml}^s$ can be obtained in the same way. Therefore, the dynamic hierarchical memory learning process can be summarized as
\begin{equation}
\mathcal{L}_{dhml} = \mathcal{L}^{d}_{dhml} + \mathcal{L}^{s}_{dhml}
\end{equation}
% \textcolor{red}{\textit{Discussion}}

\subsection{Information Consistency Evolution Learning}
The baseline network and the DHML module primarily focus on intra-view feature learning, lacking the ability to model and associate cross-view features effectively. Therefore, the proposed ICEL module employs a threshold-based and ranking-based neighborhood selection strategy to achieve effective cross-view feature alignment. By promoting the consistency evolution of cross-view feature representations, ICEL enhances both the discriminability and representation capability of cross-view features.

In the ICEL module, unlike the baseline and DHML module, we employ instance-level features instead of cluster-level representations. To achieve this, we define two instance memory dictionaries, \(\mathcal{I}_{d}\) and \(\mathcal{I}_{s}\), which store the instance features extracted by the shared feature encoder, as described in Section \ref{baseline}. This process can be described as

\begin{equation}
\mathcal{I}_{d} \leftarrow \{ \mathbf{f}_1^d, \mathbf{f}_2^d, \dots, \mathbf{f}_N^d \}
\end{equation}
\begin{equation}
\mathcal{I}_{s} \leftarrow \{ \mathbf{f}_1^s, \mathbf{f}_2^s, \dots, \mathbf{f}_M^s \}
\end{equation}
where \( \mathbf{f}_n^d \) and \( \mathbf{f}_m^s \) denote the drone and satellite instance features stored in memory dictionaries  \(\mathcal{I}_{d}\) and \(\mathcal{I}_{s}\), respectively.

In addition, we define the similarity between a given drone (or satellite) query \(q_z^{d{(s)}}\) and each drone (or satellite) instance \(\mathbf{f}_v^{d{(s)}}\) in the training set as
\begin{equation}
\mathcal{S}(q_z^{d{(s)}},\mathbf{f}_v^{d{(s)}}) = \frac{q_z^{d{(s)}} \cdot \mathbf{f}_v^{d{(s)}}}{\| q_z^{d{(s)}} \|_2 \| \mathbf{f}_v^{d{(s)}} \|_2},
\end{equation}
where \(z\) and \(v\) are the indices of queries and instances. Thus, we obtain intra-view similarity \(\mathcal{S}(q_z^{d{}},\mathbf{f}_v^{d})\), \(\mathcal{S}(q_z^{s{}},\mathbf{f}_v^{s})\) and cross-view similarity \(\mathcal{S}(q_z^{d{}},\mathbf{f}_v^{s})\), \(\mathcal{S}(q_z^{s{}},\mathbf{f}_v^{d})\).

Then, we adopt a threshold-based neighborhood selection strategy to select reliable neighbors, which can be defined as
\begin{equation}
\Omega^{dd} = \Big\{ \mathbf{f}_{v}^{d} \,\Big|\, S \big(q_{z}^{d}, \mathbf{f}_{v}^{d} \big) > \gamma  \max_{v \in \{1, \dots, N\}} S \big(q_{z}^{d}, \mathbf{f}_{v}^{d}) \Big\}
\end{equation}
\begin{equation}
\Omega^{ds} = \Big\{ \mathbf{f}_{v}^{s} \,\Big|\, S \big(q_{z}^{d}, \mathbf{f}_{v}^{s} \big) > \gamma  \max_{v \in \{1, \dots, M\}} S \big(q_{z}^{d}, \mathbf{f}_{v}^{s} ) \Big\}
\end{equation}
where \(\Omega^{dd}\) represents the set of drone feature instances selected as valid intra-view neighbors. \(\Omega^{ds}\) denotes the set of drone feature instances that exhibit strong cross-view associations with satellite features. \(\gamma\in (0,1)\) is a predefined threshold. The \(\Omega^{ss}\) and \(\Omega^{sd}\) can be obtained by the same way.

Additionally, we introduce a ranking-based neighborhood selection strategy. Specifically, we independently select the top-\( k_1 \) most relevant samples to construct a strictly filtered neighborhood, ensuring high-confidence associations, while also selecting the top-\( k_2 \) samples define an expanded neighborhood, capturing additional informative relationships. The process can be defined as
\begin{equation}
\mathcal{N}_{k1}^{dd}= \Big\{ \mathbf{f}_v^{d} \mid v \in \arg\max_{V \subset \{1, \dots, N\}, |V| = k_1} \sum_{v \in V} S(q_z^{d}, \mathbf{f}_v^{d}) \Big\}
\end{equation}

\begin{equation}
\mathcal{N}_{k1}^{ds} = \Big\{ \mathbf{f}_v^{s} \mid v \in \arg\max_{V \subset \{1, \dots, M\}, |V| = k_1} \sum_{v \in V} S(q_z^{d}, \mathbf{f}_v^{s}) \Big\}
\end{equation}

\begin{equation}
\mathcal{N}_{k2}^{dd}= \Big\{ \mathbf{f}_v^{d} \mid v \in \arg\max_{V \subset \{1, \dots, N\}, |V| = k_2} \sum_{v \in V} S(q_z^{d}, \mathbf{f}_v^{d}) \Big\}
\end{equation}

\begin{equation}
\mathcal{N}_{k2}^{ds} = \Big\{ \mathbf{f}_v^{s} \mid v \in \arg\max_{V \subset \{1, \dots, M\}, |V| = k_2} \sum_{v \in V} S(q_z^{d}, \mathbf{f}_v^{s}) \Big\}
\end{equation}
where \( \mathcal{N}_{k1}^{dd} \), \( \mathcal{N}_{k2}^{dd} \) and \( \mathcal{N}_{k1}^{ds} \), \( \mathcal{N}_{k2}^{ds} \) denote the selected intra-view and cross-view neighborhood sets, respectively, with \( \mathcal{N}_{k1}^{dd} \) and \( \mathcal{N}_{k1}^{ds} \) containing exactly \( k_1 \) elements, and \( \mathcal{N}_{k2}^{dd} \) and \( \mathcal{N}_{k2}^{ds} \) containing exactly \( k_2 \) elements. Similarly, the neighborhood sets \( \mathcal{N}_{k1}^{ss} \), \( \mathcal{N}_{k1}^{sd} \), \( \mathcal{N}_{k2}^{ss} \), and \( \mathcal{N}_{k2}^{sd} \) can be obtained following the same selection strategy.

Based on the constructed neighborhoods, the normalized similarity distribution is computed within the larger neighborhood to promote the aggregation of similar samples in both intra-view and cross-view scenarios. This distribution is then optimized by a cross-entropy loss to enhance alignment between query samples and their neighbors. The intra-view \(\mathcal{L}_{\Omega}^{dd}\) and cross-view \(\mathcal{L}_{\Omega}^{ds}\) losses can be formulated as
\begin{equation}
\mathcal{L}_{\Omega}^{dd} = -\frac{1}{N^b} \sum_{z=1}^{N^b} \sum_{v \in \Omega^{dd}} \log \frac{\exp \left( \mathcal{S}(q_z^{d{}},\mathbf{f}_v^{d}) / \tau \right)}
{\sum_{v=1}^{N^{{\Omega}^{dd}}} \exp \left( \mathcal{S}(q_z^{d{}},\mathbf{f}_v^{d}) / \tau \right)}
\end{equation}

\begin{equation}
\mathcal{L}_{\Omega}^{ds} = -\frac{1}{N^b} \sum_{z=1}^{N^b} \sum_{v \in \Omega^{ds}} \log \frac{\exp \left( \mathcal{S}(q_z^{d{}},\mathbf{f}_v^{s}) / \tau \right)}
{\sum_{v=1}^{N^{{\Omega}^{ds}}} \exp \left( \mathcal{S}(q_z^{d{}},\mathbf{f}_v^{s}) / \tau \right)}
\end{equation}
where \( N^b \) represents the batch size of the query \( q_z^d \), \( N^{{\Omega}^{dd}} \) and \( N^{{\Omega}^{ds}} \) denote the total number of neighborhood samples within the set \({\Omega}^{dd}\) and \({\Omega}^{ds}\).

Incorporating neighborhoods \( \mathcal{N}_{k1}^{dd} \), \( \mathcal{N}_{k2}^{dd} \) and \( \mathcal{N}_{k1}^{ds} \), \( \mathcal{N}_{k2}^{ds} \), we introduce consistency loss to enhance alignment of relative similarity distributions within the large-scale neighborhood \( k_2 \) across both intra-view and cross-view representations. The intra-view loss \(\mathcal{L}_{k2}^{dd}\) and cross-view loss \(\mathcal{L}_{k2}^{ds}\) under this consistency loss can be formulated as
\begin{equation}
p_{z,v}^{dd} = \frac{\exp(S(q_z^{d}, \mathbf{f}_{v}^{d}))}{\sum_{v' \in \mathcal{N}_{k2}^{dd}} \exp(S(q_z^{d}, \mathbf{f}_{v'}^{d}))}
\end{equation}
\begin{equation}
p_{z,v}^{ds} = \frac{\exp(S(q_z^{d}, \mathbf{f}_{v}^{s}))}{\sum_{v' \in \mathcal{N}_{k2}^{ds}} \exp(S(q_z^{d}, \mathbf{f}_{v'}^{s}))}
\end{equation}
\begin{equation}
\mathcal{L}_{k2}^{dd} = \frac{1}{B} \sum_{z=1}^{B} 
\sum_{v \in \mathcal{N}_{k2}^{dd}} 
p_{z,v}^{dd}
\log \left( k_2 \cdot p_{z,v}^{dd} \right)
\end{equation}
\begin{equation}
\mathcal{L}_{k2}^{ds} = \frac{1}{B} \sum_{z=1}^{B} 
\sum_{v \in \mathcal{N}_{k2}^{ds}} 
p_{z,v}^{ds}
\log \left( k_2 \cdot p_{z,v}^{ds} \right)
\end{equation}
where \( p_{z,v}^{d(s)}\) is the softmax-normalized similarity distribution, \( v' \) is used for summation to normalize the similarity scores over all neighborhood samples. \(\mathcal{L}_{k2}^{ss}\) and \(\mathcal{L}_{k2}^{sd}\) can be obtained in the same way.

Within a more restrictive and smaller set of \( k_1 \) neighbors, a mutual information constraint is proposed to learn intra-view and cross-view features representations. This complements the consistency constraint described above, achieving a balance between global structural alignment and local discriminative information. Within the selected \( k_1 \) neighborhood, we first compute, for each neighbor sample \(\mathbf{f}_{v_i}^{d(s)}\) (where \(i = 1, \ldots, k_1\)), the similarity between the deep query feature \(q_z^{d}\) and the neighbor feature \(\mathbf{f}_{v_i}^{d(s)}\) using the similarity function \(S(q_z^{d(s)}, \mathbf{f}_{v_i}^{d(s)})\). Based on these similarity scores, the posterior probability for each neighbor is defined via a softmax normalization as:
\begin{equation}
p_i^{d(s)d(s)}(v_i|q_z^{d(s)d(s)}) = \frac{\exp\big(S(q_z^{d(s)}, \mathbf{f}_{v_i}^{d(s)})\big)}{\sum_{j=1}^{R}\exp\big(S(q_z^{d(s)}, \mathbf{f}_{v_j}^{d(s)})\big)}
\end{equation}
where \(R\) denote the total number of neighborhood samples
within the set \(\mathcal{N}_{k1}^{dd}\) and \(\mathcal{N}_{k1}^{ds}\). 

Therefore, the intra-view and cross-view mutual information losses \(\mathcal{L}_{k1}^{dd}\) and \(\mathcal{L}_{k1}^{ds}\) can be derived and are defined as follows:
\begin{equation}
\mathcal{L}_{k1}^{dd} = - \frac{1}{B} \sum_{z=1}^{B} \sum_{i=1}^{R} p_i^{dd}(v_i | q_z^{d}) \log \frac{p_i^{dd}(v_i | q_z^{d})}{1/R}
\end{equation}

\begin{equation}
\mathcal{L}_{k1}^{ds} = - \frac{1}{B} \sum_{z=1}^{B} \sum_{i=1}^{R} p_i^{ds}(v_i | q_z^{d}) \log \frac{p_i^{ds}(v_i | q_z^{d})}{1/R}
\end{equation}
where \( p_i^{dd}(v_i | q_z^{d}) \) and \( p_i^{ds}(v_i | q_z^{d}) \) represent the normalized similarity distributions over the selected \( R \) intra-view and cross-view neighbors, respectively. \(\mathcal{L}_{k1}^{ss}\) and \(\mathcal{L}_{k1}^{ss}\) can be obtain in the same way. 

% For each viewpoint, the corresponding loss can be formulated as follows:
% \begin{equation}
% \mathcal{L}_{icel}^{dd} = \mathcal{L}_{\Omega}^{dd} + \lambda_{k1} \mathcal{L}_{k1}^{dd} + \lambda_{k2}\mathcal{L}_{k2}^{dd}
% \end{equation}
% \begin{equation}
% \mathcal{L}_{icel}^{ds} = \mathcal{L}_{\Omega}^{ds} + \lambda_{k1}\mathcal{L}_{k1}^{ds} + \lambda_{k2}\mathcal{L}_{k2}^{ds}
% \end{equation}
% \begin{equation}
% \mathcal{L}_{icel}^{ss} = \mathcal{L}_{\Omega}^{ss} + \lambda_{k1}\mathcal{L}_{k1}^{ss} + \lambda_{k2}\mathcal{L}_{k2}^{ss}
% \end{equation}
% \begin{equation}
% \mathcal{L}_{icel}^{sd} = \mathcal{L}_{\Omega}^{sd} + \lambda_{k1}\mathcal{L}_{k1}^{sd} + \lambda_{k2}\mathcal{L}_{k2}^{sd}
% \end{equation}
% where \( \lambda_{k1} \) and \( \lambda_{k2} \) are importance adjustment factors.
% The final optimization for neighbor learning is denoted by the following combination:
% \begin{equation}
% \mathcal{L}_{icel} = \mathcal{L}_{icel}^{d} + \mathcal{L}_{icel}^{s} 
% \end{equation}
% where \( \mathcal{L}_{icel}^{d} = \mathcal{L}_{icel}^{dd} + \mathcal{L}_{icel}^{ds} \) and \( \mathcal{L}_{icel}^{s} = \mathcal{L}_{icel}^{ss} + \mathcal{L}_{icel}^{sd} \).
%   The overall loss for DMNIL is denoted as:
% \begin{equation}
% \mathcal{L}_{total} = \mathcal{L}_{cv} + \mathcal{L}_{dhml}+\mathcal{L}_{icel}
% \end{equation}
For each view combination, the loss is computed as the weighted sum of three components: 
the original neighborhood loss $\mathcal{L}_{\Omega}$, the strict neighborhood loss $\mathcal{L}_{k1}$, and the extended neighborhood loss $\mathcal{L}_{k2}$:
\begin{equation}
\mathcal{L}_{icel}^{VQ} = \mathcal{L}_{\Omega}^{VQ} + \lambda_{k1} \mathcal{L}_{k1}^{VQ} + \lambda_{k2} \mathcal{L}_{k2}^{VQ}, \quad V,Q \in \{d,s\}
\end{equation}
where $V$ and $Q$ denote the drone or satellite view.

The total neighbor learning loss is then formulated as

\begin{equation}
\mathcal{L}_{icel} = \sum_{V \in \{d,s\}} \sum_{Q \in \{d,s\}} \mathcal{L}_{icel}^{VQ}
\end{equation}

Finally, the overall loss of our method is defined as

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{cv} + \mathcal{L}_{dhml} + \mathcal{L}_{icel}
\end{equation}

\subsection{Pseudo-Label Enhancement }
In self-supervised learning, the quality of pseudo-labels exerts a decisive influence on model performance. Inspired by \cite{yang2024shallow}, our proposes a pseudo-label optimization strategy based on dynamic feature robustness. Specifically, we introduce small perturbations to the original feature representations and exploit the top-K neighbor consistency between the original and perturbed features to identify high-confidence samples while filtering out noisy instances. Then, a spatial smoothing operation based on the similarity matrix is applied to refine the pseudo-labels, enforcing local label consistency.

We first employe small Gaussian noise \(\boldsymbol{\epsilon}_d, \boldsymbol{\epsilon}_s \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})\) into the original satellite instances feature and the original drone instances features to obtain perturbed features, which are used for more robust similarity estimation and subsequent cluster consistency evaluation. It can be formulated as
\begin{equation} 
\mathbf{f}_n^{d{\boldsymbol{\epsilon}}} = \mathbf{f}_n^{d} + \boldsymbol{\epsilon}_d, \quad 
\mathbf{f}_m^{s{\boldsymbol{\epsilon}}} = \mathbf{f}_m^{s} + \boldsymbol{\epsilon}_s, \quad 
\end{equation}
where, with the similarities \(\mathcal{S}(\mathbf{f}_m^{s},\mathbf{f}_n^{d})\)and \(\mathcal{S}(\mathbf{f}_m^{s{\boldsymbol{\epsilon}}},\mathbf{f}_n^{d{\boldsymbol{\epsilon}}})\), we can get two cross-view ranking lists $\{S(\mathbf{f}_m^{s},\mathbf{f}_n^{d})\}$and $\{S(\mathbf{f}_m^{s{\boldsymbol{\epsilon}}}, \mathbf{f}_n^{d{\boldsymbol{\epsilon}}})\}$ for $\mathbf{f}_m^s$ and $\mathbf{f}_m^{s{\boldsymbol{\epsilon}}}$, which are denoted as $\mathcal{K}_s (\mathbf{f}_m^s)$ and $\mathcal{K}_s^{\boldsymbol{\epsilon}} (\mathbf{f}_m^{s{\boldsymbol{\epsilon}}})$. 

The label of the $k$-th similar infrared instance in two ranking lists can be represented as
\begin{equation}
    \tilde{y}_{\mathbf{f}_m^s}^{sd}[k] = y_{\mathbf{f}_n^d}, \quad \mathbf{f}_n^d = \mathcal{K}_s (\mathbf{f}_m^s)[k]
\end{equation}
\begin{equation}
    \tilde{y}_{\mathbf{f}_m^{s{\boldsymbol{\epsilon}}}}^{sd}[k] = y_{\mathbf{f}_n^{d{\boldsymbol{\epsilon}}}}, \quad \mathbf{f}_n^{d{\boldsymbol{\epsilon}}} = \mathcal{K}_s^d (\mathbf{f}_m^{s{\boldsymbol{\epsilon}}})[k]
\end{equation}
where $\tilde{y}_{\mathbf{f}_m^s}^{sd}[k]$ and $\tilde{y}_{\mathbf{f}_m^{s{\boldsymbol{\epsilon}}}}^{sd}[k]$ are refined $k$-th cross-view labels of $\mathbf{f}_m^s$ and $\mathbf{f}_n^{d{\boldsymbol{\epsilon}}}$ through the ranking on cross-view initial features and perturbed features similarities. 

So, we associate the cross-modality labels with the intersection of two label sets to investigate collaborative ranking consistency. This process can be described as
\begin{equation}
    I_{\mathbf{f}_m^{s}} (k) = \{\tilde{y}_{\mathbf{f}_m^s}^{sd}[k] \cap \tilde{y}_{\mathbf{f}_m^{s{\boldsymbol{\epsilon}}}}^{sd}[k]\}_{p=1}^{P},
\end{equation}
where $I_{\mathbf{f}_m^{s}} (k)$ records the samples with the same identity of top-$k$ identity in instances from top-1 to top-$X$. 

The reliable refined cross-view label of $\mathbf{f}_m^{s}$ can be expressed by the label of maximum count in $I_{\mathbf{f}_m^{s}} (k)$.
\begin{equation}
    \tilde{y}_{\mathbf{f}_m^{s}}^{cm} = \tilde{y}_{\mathbf{f}_m^{s{\boldsymbol{\epsilon}}}}^{sd}[k], \quad k = \arg\max_k (|I_{\mathbf{f}_m^{s}} (k)|, k \in [1,K])
\end{equation}
where $|\cdot|$ denotes the counting function. The $\arg\max$ operation traverses $I_{\mathbf{f}_m^{s}} (k)$ and finds the labels with maximum number as cross-view refined labels. 

Then, we convert the label list $\{\tilde{y}_{\mathbf{f}_m^{s}}^{cm}, m \in [1, M]\}$ to the form of one-hot code matrix $\tilde{Y}^{cm} \in \mathbb{R}^{M \times N}$ by setting the column according to the refined labels to 1 and the rest to 0.Then,two initial and perturbed homogeneous initial $P^{ss}$ and deep $P_{\boldsymbol{\epsilon}}^{ss}$ similarity matrices are constructed to investigate intra-view ranking consistency, enhancing the precision of refined cross-view pseudo labels by:
\begin{equation}
    P^{ss} (m,m') = S(\mathbf{f}_m^s, \mathbf{f}_{m'}^s)
\end{equation}
\begin{equation}
    P_{\boldsymbol{\epsilon}}^{ss} (m,m') = S(\mathbf{f}_m^{s{\boldsymbol{\epsilon}}}, \mathbf{f}_{m'}^{s{\boldsymbol{\epsilon}}}),
\end{equation}
where $P_*^{ss} \in \mathbb{R}^{M \times M}$ represents the intra-view similarity structure. 

In order to explore the intra-view initial and perturbed collaboration, we calculate the sum of $ P^{ss} (m,m')$ and $P_{\boldsymbol{\epsilon}}^{ss} (m,m')$ by:
\begin{equation}
    P (m,m') =  P^{ss} (m,m') + P_{\boldsymbol{\epsilon}}^{ss} (m,m'),
\end{equation}
where $P (m,m')$ indicates the consistency of initial and perturbed similarity matrix. 

We keep the 5-max values of $P (m,m')$ in each row to 1 and set the rest to 0, acquiring the ranking relations. The process of intra-modality ranking smoothing is formulated as follows:
\begin{equation}
    y ^{cm} = P\tilde{y}^{cm},
\end{equation}
where $y^{cm} \in \mathbb{R}^{M \times N}$ is the final refined cross-view label matrix of satellite instance. In $y^{cm}$, the column number of the maximum value in each row is the refined label of samples. 

\begin{table*}[t]
\begin{center}
  \centering
  \caption{Comparisons between the proposed method and some state-of-the-art methods on the University-1652 datasets.}
  \label{tab:comparison}
  \footnotesize
  \setlength{\tabcolsep}{12pt}
     \resizebox{\textwidth}{!}{
  \begin{tabular}{cccccccc}
  \hline \hline
   &&  \multicolumn{3}{c}{University-1652} \\ \cline{1-8}
   &  &  && \multicolumn{2}{c}{Drone$\rightarrow$Satellite} & \multicolumn{2}{c}{Satellite$\rightarrow$Drone} \\ \cline{5-8}
\multirow{-2}{*}{Model} & \multirow{-2}{*}{Venue} & \multirow{-2}{*}{Supervision Ratio{}} & \multirow{-2}{*}{Paramars(M)} & {R@1} & {AP} & {R@1} & {AP} \\ \hline
\rowcolor{gray!20}
  MuSe-Net\cite{wang2024multiple}      & PR’2024      &100(\%)& 50.47       & 74.48       & 77.83      & 88.02     & 75.10 \\\rowcolor{gray!20}
  % LPN\cite{wang2021each}               & TCSVT’2022   &100(\%)& 62.39       & 75.93       & 79.14      & 86.45     & 74.49 \\\rowcolor{gray!20}
  % Dai\cite{dai2023vision}              & TIP’2023     &100(\%)& 43.91       & 82.22       & 84.78      & 87.59     & 81.49 \\\rowcolor{gray!20}
  % IFSs\cite{ge2024multibranch}         & TGRS’2024    &100(\%)& -           & 86.06       & 88.08      & 91.44     & 85.73 \\\rowcolor{gray!20}
  MCCG\cite{shen2023mccg}              & TCSVT’2023   &100(\%)& 56.65       & 89.40       & 91.07      & 95.01     & 89.93 \\\rowcolor{gray!20}
  SDPL\cite{chen2024sdpl}              & TCSVT’2024   &100(\%)& 42.56       & 90.16       & 91.64      & 93.58     & 89.45  \\\rowcolor{gray!20}
  CCR\cite{du2024ccr}                  & TCSVT'2024   &100(\%)& 156.57      & 92.54       & 93.78      & 95.15     & 91.80 \\\rowcolor{gray!20}
  Sample4Geo\cite{deuser2023sample4geo}& ICCV’2023    &100(\%)& 87.57       & 92.65       & 93.81      & 95.14     & 91.39 \\\rowcolor{gray!20}
  SRLN\cite{lv2024direction}           & TGRS’2024    &100(\%)& 193.03      & 92.70       & 93.77      & 95.14     & 91.97 \\
  MEAN\cite{chen2024multi}      & arXiv'2024   &100(\%) &36.50 &93.55  &94.53 &{96.01}  & 92.08\\
  DAC\cite{xia2024enhancing}           & TCSVT'2024   &100(\%)& 96.50      & 94.67       & 95.50      & 96.43     & 93.79 \\
      Ours                                 & -     &100(\%)& 28.59      & 92.02       & 93.36      & 95.57     & 91.27 \\ \hline
EM-CVGL\cite{li2024learning}  & TGRS'2024 &0(\%)  & 2.25  & 70.29      & 74.93       & 79.03      & 61.03    \\
CDIKTNet\cite{chen2025limited} & arXiv'2025 &0(\%) &91.52 & 83.30  &85.73 &{87.73} & 76.53\\    
  Ours & -     &0(\%)& 28.59      & 90.17       & 91.67      & 95.01     & 88.95\\\hline

  \hline
\end{tabular}}
\end{center}
\end{table*}
% \begin{table*}[t]
% \begin{center}
%   \centering
%   \caption{Comparisons between the proposed method and some state-of-the-art methods on the University-1652 datasets.}
%   \label{tab:comparison}
%   \normalsize
%   \setlength{\tabcolsep}{12pt}
%      \resizebox{\textwidth}{!}{
%   \begin{tabular}{cccccccc}
%   \hline \hline
%    &&  \multicolumn{3}{c}{University-1652} \\ \cline{1-8}
%    &  &  && \multicolumn{2}{c}{Drone$\rightarrow$Satellite} & \multicolumn{2}{c}{Satellite$\rightarrow$Drone} \\ \cline{5-8}
% \multirow{-2}{*}{Model} & \multirow{-2}{*}{Venue} & \multirow{-2}{*}{Supervision Ratio{}} & \multirow{-2}{*}{Paramars(M)} & {R@1} & {AP} & {R@1} & {AP} \\ \hline
% \rowcolor{gray!20}
%   MuSe-Net\cite{wang2024multiple}      & PR’2024      &100(\%)& 50.47       & 74.48       & 77.83      & 88.02     & 75.10 \\\rowcolor{gray!20}
%   LPN\cite{wang2021each}               & TCSVT’2022   &100(\%)& 62.39       & 75.93       & 79.14      & 86.45     & 74.49 \\\rowcolor{gray!20}
%   Dai\cite{dai2023vision}              & TIP’2023     &100(\%)& 43.91       & 82.22       & 84.78      & 87.59     & 81.49 \\\rowcolor{gray!20}
%   IFSs\cite{ge2024multibranch}         & TGRS’2024    &100(\%)& -           & 86.06       & 88.08      & 91.44     & 85.73 \\\rowcolor{gray!20}
%   MCCG\cite{shen2023mccg}              & TCSVT’2023   &100(\%)& 56.65       & 89.40       & 91.07      & 95.01     & 89.93 \\\rowcolor{gray!20}
%   SDPL\cite{chen2024sdpl}              & TCSVT’2024   &100(\%)& 42.56       & 90.16       & 91.64      & 93.58     & 89.45  \\\rowcolor{gray!20}
%   CCR\cite{du2024ccr}                  & TCSVT'2024   &100(\%)& 156.57      & 92.54       & 93.78      & 95.15     & 91.80 \\\rowcolor{gray!20}
%   Sample4Geo\cite{deuser2023sample4geo}& ICCV’2023    &100(\%)& 87.57       & 92.65       & 93.81      & 95.14     & 91.39 \\\rowcolor{gray!20}
%   SRLN\cite{lv2024direction}           & TGRS’2024    &100(\%)& 193.03      & 92.70       & 93.77      & 95.14     & 91.97 \\
%    MEAN    & -          &100(\%)  &\textcolor{red}{36.50}  &{93.55}       &{94.53} &{96.01}     & {92.08}\\
%   DAC\cite{xia2024enhancing}           & TCSVT'2024   &100(\%)& 96.50  & \textcolor{blue}{94.67}   &\textcolor{blue}{95.50}     & \textcolor{red}{96.43}     & \textcolor{blue}{93.79} \\ 
%       Ours                                 & -     &100(\%)& 91.51    & \textcolor{red}{95.02}  &\textcolor{red}{95.79}      & \textcolor{blue}{96.15}     & \textcolor{red}{93.93} \\ \hline\hline
%   Ours                                 & -     &0(\%)& 28.59      & 90.17       & 91.67      & 95.01     & 88.95\\\hline

%   \hline
% \end{tabular}}
% \end{center}
% \end{table*}

\section{Experimental results}\label{results}
\subsection{Experimental Datasets and Evaluation Metrics}
To evaluate our self-supervised method, we conduct experiments on three benchmark datasets including University-1652 \cite{zheng2020university}, SUES-200 \cite{zhu2023sues} and DenseUAV \cite{wang2024multiple}.

\textit{University-1652} contains images from 1,652 buildings across 72 universities worldwide. It includes drone, satellite, and ground-level views. The training set consists of 701 buildings from 33 universities, and the test set covers 951 buildings from 39 geographically disjoint universities.

\textit{SUES-200} focuses on altitude diversity in aerial imagery, containing 200 locations with 120 for training and 80 for testing. Each location provides one satellite image and aerial images captured at four altitudes (150m, 200m, 250m, and 300m), covering urban areas, natural landscapes, and water bodies. SUES-200 evaluates cross-view retrieval under varying flight heights, reflecting real-world UAV scenarios.

\textit{DenseUAV} focuses on UAV self-positioning in low-altitude urban environments, containing over 27,000 UAV and satellite images collected from 14 university campuses. It features dense sampling, multi-scale satellite images, and varying altitudes (80m, 90m, and 100m) under multiple temporal conditions. The training set includes 6,768 UAV images and 13,536 satellite images, while the test set comprises 2,331 UAV images and 4,662 satellite images from non-overlapping areas. DenseUAV poses challenges such as cross-view matching, neighbor confusion, and spatial-temporal variation.

We evaluate the models using Recall@K (R@K) and Average Precision (AP), where R@K measures the proportion of correct matches among the Top-\(K\)results and AP balances precision and recall. Model efficiency is assessed by the number of parameters, reflecting portability under resource constraints. All comparisons are conducted using the best configurations for each method.

\subsection{Implementation Details}
A \textit{ConvNeXt-Tiny} model, pre-trained on \textit{ImageNet}, serves as the backbone for feature extraction, with a newly added classifier module initialized via the Kaiming method. During both training and testing, all input images are resized to \(3\times 384\times 384\). We employ several data augmentation techniques, including random cropping, random horizontal flipping, and random rotation. The batch size is set to 64, and DBSCAN \cite{ester1996density} is used to generate pseudo labels. For optimization, we use the SGD optimizer with an initial learning rate of 0.001, training the model for 30 epochs in total. All experiments are conducted under the PyTorch framework on an Ubuntu 22.04 system equipped with four NVIDIA RTX 4090 GPUs. Additionally, in the University-1652 and SUES-200 datasets,we replicate each satellite feature 50 times for clustering purposes, leaving UAV features unchanged. In DenseUAV, we replicate UAV features four times at each altitude, and replicate the satellite features three times for each time point and altitude.
\subsection{Comparison with State-of-the-art Methods}
To evaluate the effectiveness of our method, we compare it with the state-of-the-art supervised methods MuSe-Net \cite{wang2024multiple}, MCCG \cite{shen2023mccg}, SDPL \cite{chen2024sdpl}, CCR \cite{du2024ccr}, Sample4Geo \cite{deuser2023sample4geo}, SRLN\cite{lv2024direction}, MEAN \cite{chen2024multi} and DAC \cite{xia2024enhancing}, and the recent self-supervised methods EM-CVGL \cite{li2024learning}, CDIKTNet \cite{chen2025limited}.

\textit{Results on University-1652}. As shown in Table \ref{tab:comparison}, our method exhibits superior performance compared with the current advanced self-supervised methods. Specifically, our method achieves R@1/AP scores of 90.17\%/91.67\% and 95.01\%/88.95\% under the Drone$\rightarrow$Satellite and Satellite$\rightarrow$Drone settings, respectively. Compared with EM-CVGL, our method achieves significant improvements, outperforming it by approximately 20\% in R@1 and 17\% in AP on the Drone$\rightarrow$Satellite setting, and by 16\% in R@1 and 28\% in AP on the Satellite$\rightarrow$Drone setting. Furthermore, even compared with CDIKTNet, which utilizes paired data from other domains for self-supervised fine-tuning, our method still consistently achieves superior performance without relying on any cross-domain paired data. 

We further benchmark against eight representative supervised methods, and the results show that our method clearly surpasses some of them, including MuSe-Net, MCCG, and SDPL. Furthermore, with self-supervised pre-training and subsequent supervised fine-tuning, our method achieves competitive performance under the supervised setting, comparable to CCR, Sample4Geo, and SRLN.
\begin{table*}[t]
  \centering
  \footnotesize
  \caption{Comparisons between the proposed method and several state-of-the-art methods on the SUES-200 dataset. ${\dagger}$ indicates results from self-supervised training on University-1652 and testing on SUES-200. ${\ddagger}$ denotes results from self-supervised training and evaluation on SUES-200. ${\ast}$ corresponds to results after one epoch self-supervised fine-tuning on SUES-200 based on the ${\dagger}$ pre-trained model.
}
  \label{tab:comparison SUES-200-1}
  \setlength{\tabcolsep}{5.2pt}
   \resizebox{\textwidth}{!}{
  \begin{tabular}{cccccccccccc}
  \hline\hline
   &  \multicolumn{9}{c}{SUES-200} \\ \cline{1-12}
   &  &  & &\multicolumn{8}{c}{Drone$\rightarrow$Satellite} \\ \cline{5-12} 
   &  &  & & \multicolumn{2}{c}{150m} & \multicolumn{2}{c}{200m} & \multicolumn{2}{c}{250m} & \multicolumn{2}{c}{300m} \\ \cline{5-12} 
  \multirow{-3}{*}{Model} & \multirow{-3}{*}{Venue} & \multirow{-3}{*}{Supervision Ratio{}} & \multirow{-3}{*}{Paramars(M)}  & R@1 & AP & R@1 & AP & R@1 & AP & R@1 & AP \\ \hline\rowcolor{gray!20}
  % LPN\cite{wang2021each}      & TCSVT’2022  &100(\%)& 62.39  & 61.58 & 67.23 & 70.85 & 75.96 & 80.38 & 83.80 & 81.47 & 84.53 \\\rowcolor{gray!20}
  % Vit\cite{zhu2023sues}       & TCSVT’2023  &100(\%)& 172.20 & 59.32 & 64.93 & 62.30 & 67.24 & 71.35 & 75.49 & 77.17 & 80.67 \\\rowcolor{gray!20}
  MCCG\cite{shen2023mccg}     & TCSVT’2023  &100(\%)& 56.65  & 82.22 & 85.47 & 89.38 & 91.41 & 93.82 & 95.04 & 95.07 & 96.20 \\\rowcolor{gray!20}
  SDPL\cite{chen2024sdpl}     & TCSVT’2024  &100(\%)& 42.56  & 82.95 & 85.82 & 92.73 & 94.07 & 96.05 & 96.69 & 97.83 & 98.05 \\\rowcolor{gray!20}
  CCR\cite{du2024ccr}         & TCSVT’2024  &100(\%)& 156.57 & 87.08 & 89.55 & 93.57 & 94.90 & 95.42 & 96.28 & 96.82 & 97.39 \\\rowcolor{gray!20}
  SRLN\cite{lv2024direction}  & TGRS’2024   &100(\%)& 193.03 & 89.90 & 91.90 & 94.32 & 95.65 & 95.92 & 96.79 & 96.37 & 97.21 \\\rowcolor{gray!20}
  Sample4Geo\cite{deuser2023sample4geo} & ICCV’2023   &100(\%)& 87.57  & 92.60 & 94.00 & 97.38 & 97.81 & 98.28 & 98.64 & 99.18 & 99.36 \\
  DAC\cite{xia2024enhancing}  & TCSVT’2024  &100(\%)& 96.50 & 96.80  & 97.54 & 97.48 & 97.97 & 98.20 & 98.62 & 97.58 & 98.14 \\
  MEAN\cite{chen2024multi}    &arXiv'2024     &100(\%)& 36.50  & 95.50 & 96.46 & 98.38 & 98.72 & 98.95 & 99.17  & 99.52  & 99.63 \\
    Ours & -          &100(\%)& 28.59   & 94.00   & 95.21   & 97.83   & 98.88  &99.14  & 95.44  &99.25   &99.39    \\\hline
  EM-CVGL\cite{li2024learning}  & TGRS'2024 &0(\%)  & 2.25  & 55.23      & 60.80       & 60.95      & 61.03 & 68.10      & 72.62       & 74.42      & 78.20    \\
  CDIKTNet\cite{chen2025limited} &arXiv'2025 &{0(\%)} &91.52 &{82.75} & {85.25} &{89.35} &{91.13} & {93.15} &{94.39} &{95.18} &{96.12}\\
  Ours$^{\dagger}$ & -          &0(\%)& 28.59   & 82.60   & 85.60   & 90.48   & 92.28  &94.33  & 95.44  &96.78   &97.42    \\
  Ours$^{\ddagger}$ & -          &0(\%)& 28.59   & 87.00  & 89.20   & 93.98   & 95.11  &96.50  & 97.23  &97.23   &97.86    \\
  Ours$^{\ast}$     & -               &0(\%) & 28.59  & 90.98 &92.66  & 95.03 & 96.14 &97.45  & 98.03  & 98.23  & 98.64\\ \hline
   &  &  && \multicolumn{8}{c}{Satellite$\rightarrow$Drone} \\ \cline{5-12} 
   &  &  && \multicolumn{2}{c}{150m} & \multicolumn{2}{c}{200m} & \multicolumn{2}{c}{250m} & \multicolumn{2}{c}{300m} \\ \cline{5-12} 
  \multirow{-3}{*}{Model} & \multirow{-3}{*}{Venue}  & \multirow{-3}{*}{Supervision Ratio} & \multirow{-3}{*}{Paramars(M)} & R@1 & AP & R@1 & AP & R@1 & AP & R@1 & AP \\ \hline\rowcolor{gray!20}
  % LPN\cite{wang2021each}       & TCSVT’2022       &100(\%)& 62.39  & 83.75 & 83.75 & 83.75 & 83.75 & 83.75 & 83.75 & 83.75 & 83.75 \\\rowcolor{gray!20}
  % Vit\cite{zhu2023sues}        & TCSVT’2023       &100(\%)& 172.20 & 82.50 & 58.95 & 85.00 & 62.56 & 88.75 & 69.96 & 96.25 & 84.16 \\\rowcolor{gray!20}
  CCR\cite{du2024ccr}          & TCSVT’2024       &100(\%)& 156.57 & 92.50 & 88.54 & 97.50 & 95.22 & 97.50 & 97.10 & 97.50 & 97.49 \\\rowcolor{gray!20}
  Sample4Geo\cite{deuser2023sample4geo} & ICCV’2023&100(\%)& 87.57  & 92.60 & 94.00 & 97.38 & 97.81 & 98.28 & 98.64 & 99.18 & 99.36 \\\rowcolor{gray!20}
  MCCG\cite{shen2023mccg}      & TCSVT’2023       &100(\%)& 56.65  & 93.75 & 89.72 & 93.75 & 92.21 & 96.25 & 96.14 & 98.75 & 96.64 \\\rowcolor{gray!20}
  SDPL\cite{chen2024sdpl}      & TCSVT’2024       &100(\%)& 42.56  & 93.75 & 83.75 & 96.25 & 92.42 & 97.50 & 95.65 & 96.25 & 96.17 \\\rowcolor{gray!20}
  SRLN\cite{lv2024direction}   & TGRS’2024        &100(\%)& 193.03 & 93.75 & 93.01 & 97.50 & 95.08 & 97.50 & 96.52 & 97.50 & 96.71 \\
  DAC\cite{xia2024enhancing}   & TCSVT’2024       &100(\%)& 96.50 & 97.50 & 94.06  & 98.75 & 96.66 & 98.75 & 98.09 & 98.75 & 97.87 \\
  MEAN\cite{chen2024multi} & arXiv'2024             &100(\%)& 36.50  & 97.50  & 94.75 & 100.00  & 97.09 & 100.00 & 98.28  & 100.00 & 99.21\\
  Ours & -          &100(\%)& 28.59   & 97.50   & 92.71   & 97.50   &  97.43  &100.00  & 98.98  &98.75   &98.25    \\\hline
  EM-CVGL\cite{li2024learning}  & TGRS'2024 &0(\%)  & 2.25  & 73.75      &54.00       & 91.25      & 65.65  & 96.25      & 72.02       & 97.50      & 74.74    \\
  CDIKTNet\cite{chen2025limited} &arXiv'2025 &{0(\%)} &91.52 &{88.75}   &{80.33}   &{93.75} &{88.17}   &{95.00}   &{92.15}   &{98.75}    &{94.37}\\
  Ours$^{\dagger}$ & -          &0(\%)& 28.59   & 91.25   & 81.28   & 97.50   & 90.62  &98.75  & 94.74  &97.50   &96.17    \\
  Ours$^{\ddagger}$ & -         &0(\%)& 28.59  & 95.00   & 85.46   & 97.50   & 92.72  &100.00  & 95.43  &100.00   &96.39    \\
  Ours$^{\ast}$     & -         &0(\%)& 28.59  & 96.25 & 90.05  & 97.50 & 94.80 & 98.75  &  96.82  & 98.75  & 97.33\\ \hline\hline
\end{tabular}}
\end{table*}


\textit{Results on SUES-200}.
As shown in Table \ref{tab:comparison SUES-200-1},
% ${\dagger}$ denotes the results obtained by training the model in an unsupervised manner on the university-1652 dataset and testing it on the SUES-200 dataset. ${\ddagger}$ represents the results of training the model in an unsupervised manner using the university-1652 dataset. ${\ast}$ refers to the results obtained by fine-tuning the model, which was pre-trained using ${\dagger}$, on the SUES-200 dataset for 1 epoch in an unsupervised manner.
As shown in Table \ref{tab:comparison SUES-200-1}, our method, when directly transferring the pre-trained model from the University-1652 dataset to the SUES-200 dataset without any re-training, consistently outperforms all existing self-supervised methods and achieves competitive performance compared to supervised methods such as MCCG and SDPL. Moreover, by performing self-supervised re-training on the SUES-200 dataset, our method brings a significant performance boost, achieving results comparable to the supervised CCR. Notably, when further fine-tuning the transferred model for only one epoch, we observe additional improvements, surpassing the supervised SRLN. These results clearly demonstrate the effectiveness of our method in self-supervised learning, transfer learning, and efficient fine-tuning.

\textit{Results on Cross-Domain Generalization Performance}:
Our method consistently outperforms existing self-supervised method under cross-domain settings, showing both significant improvements and stable generalization. In the Drone to Satellite setting, our model achieves better performance than EM-CVGL at all altitudes. At 150 meters, R@1 and AP increase by approximately 22\% and 20\%; at 200 meters, the gains are around 19\% and 16\%; at 250 meters, nearly 14\% and 12\%; and even at 300 meters, our method still outperforms by about 11\% and 9\%. In the Satellite to Drone scenario, similar improvements are observed. At 150 meters, R@1 and AP improve by approximately 18\% and 25\%; at 200 meters, by 10\% and 20\%; at 250 meters, by 6\% and 13\%; and at 300 meters, by 3\% and 10\%. These results confirm the robustness and strong cross-domain generalization capability of our method across varying altitudes.Remarkably, our method even surpasses all existing state-of-the-art supervised methods in terms of cross-domain generalization. These results confirm the robustness and strong cross-domain generalization capability of our method across varying altitudes.
\begin{table*}[t]
\centering
  \footnotesize
\caption{Comparisons between the proposed method and state-of-the-art methods in cross-domain evaluation.}
\label{tab:comparison University-SUES}
\setlength{\tabcolsep}{5.6pt}
   \resizebox{\textwidth}{!}{
\begin{tabular}{cccccccccccc}
\hline\hline
   &  \multicolumn{9}{c}{University-1652$\rightarrow$SUES-200} \\ \cline{1-12}
   &  &  & &\multicolumn{8}{c}{Drone$\rightarrow$Satellite} \\ \cline{5-12} 
   &  &  & & \multicolumn{2}{c}{150m} & \multicolumn{2}{c}{200m} & \multicolumn{2}{c}{250m} & \multicolumn{2}{c}{300m} \\ \cline{5-12} 
  \multirow{-3}{*}{Model} & \multirow{-3}{*}{Venue}  & \multirow{-3}{*}{Supervision Ratio{}} & \multirow{-3}{*}{Paramars(M)}  & R@1 & AP & R@1 & AP & R@1 & AP & R@1 & AP \\ \hline\rowcolor{gray!20}
MCCG\cite{shen2023mccg}              & TCSVT’2023 &100(\%)& 56.65  & 57.62 & 62.80 & 66.83 & 71.60 & 74.25 & 78.35 & 82.55 & 85.27  \\\rowcolor{gray!20}
Sample4Geo\cite{deuser2023sample4geo}& ICCV’2023  &100(\%)& 87.57  & 70.05 & 74.93 & 80.68 & 83.90 & 87.35 & 89.72 & 90.03 & 91.91   \\\rowcolor{gray!20}
DAC\cite{xia2024enhancing}           & TCSVT’2024 &100(\%)& 96.50 & 76.65 & 80.56 & 86.45 & 89.00 & 92.95  & 94.18 & 94.53 & 95.45   \\\rowcolor{gray!20}
CAMP\cite{wu2024camp}                & TGRS’2024  &100(\%)& 91.40  & 78.90 & 82.38 & 86.83  & 89.28 & 91.95 & 93.63 & 95.68  & 96.65   \\\rowcolor{gray!20}
MEAN\cite{chen2024multi}             & arXiv'2024    &100(\%)& 36.50  & 81.73 & 87.72  & 89.05  & 91.00  & 92.13  & 93.60  & 94.63  & 95.76\\
\hline
  EM-CVGL\cite{li2024learning}  & TGRS'2024 &0(\%)  & 2.25  & 60.03  &65.69       & 71.50      & 76.17  & 80.35  & 83.85       & 85.93      & 88.34    \\
Ours                                 & -  &0(\%)& 28.59   & 82.60   & 85.60   & 90.48   & 92.28  &94.33  & 95.44  &96.78   &97.42    \\\hline
   &  &  && \multicolumn{8}{c}{Satellite$\rightarrow$Drone} \\ \cline{5-12} 
   &  &  && \multicolumn{2}{c}{150m} & \multicolumn{2}{c}{200m} & \multicolumn{2}{c}{250m} & \multicolumn{2}{c}{300m} \\ \cline{5-12} 
  \multirow{-3}{*}{Model} & \multirow{-3}{*}{Venue}   & \multirow{-3}{*}{Supervision Ratio{}} & \multirow{-3}{*}{Paramars(M)} & R@1 & AP & R@1 & AP & R@1 & AP & R@1 & AP \\ \hline\rowcolor{gray!20}
MCCG\cite{shen2023mccg}               & TCSVT’2023  &100(\%)& 56.65  & 61.25 & 53.51 & 82.50 & 67.06 & 81.25 & 74.99 & 87.50 & 80.20   \\\rowcolor{gray!20}
Sample4Geo\cite{deuser2023sample4geo} & ICCV’2023  &100(\%)& 87.57  & 83.75 & 73.83 & 91.25 & 83.42 & 93.75 & 89.07 & 93.75 & 90.66   \\\rowcolor{gray!20}
DAC\cite{xia2024enhancing}            & TCSVT’2024 &100(\%)& 96.50 & 87.50 & 79.87 & 96.25 & 88.98  & 95.00 & 92.81 & 96.25 & 94.00   \\\rowcolor{gray!20}
CAMP\cite{wu2024camp}                 & TGRS’2024  &100(\%)& 91.40  & 87.50 & 78.98 & 95.00 & 87.05  & 95.00 & 91.05 & 96.25 & 93.44   \\\rowcolor{gray!20}
MEAN\cite{chen2024multi}              &arXiv'2024    &100(\%)& 36.50   & 91.25   & 81.50   & 96.25   &  89.55  & 95.00 & 92.36 & 96.25  & 94.32   \\\hline
  EM-CVGL\cite{li2024learning}  & TGRS'2024 &0(\%)  & 2.25  & 73.75  &56.99       & 87.50      & 70.62  & 92.50  & 81.18       & 95.00      & 86.04    \\
Ours & -                              &0(\%)& 28.59   & 91.25  & 81.28   & 97.50   & 90.62  &98.75  & 94.74  &97.50   &96.17    \\\hline\hline
\end{tabular}}
\end{table*}
\begin{table*}[t]
\begin{center}
  \centering
  \caption{Comparisons between the proposed method and some state-of-the-art methods on the DenseUAV datasets.}
  \label{tab:DenseUAV}
  \footnotesize
  \setlength{\tabcolsep}{4pt}
     \resizebox{\textwidth}{!}{
  \begin{tabular}{cccccccccccc}
  \hline \hline
   &&  \multicolumn{6}{c}{DenseUAV} \\ \cline{1-12}
   &  &  && \multicolumn{2}{c}{All height}& \multicolumn{2}{c}{80m} &\multicolumn{2}{c}{90m}& \multicolumn{2}{c}{100m}  \\ \cline{5-12}
\multirow{-2}{*}{Model} & \multirow{-2}{*}{Venue} & \multirow{-2}{*}{Supervision Ratio{}} & \multirow{-2}{*}{Paramars(M)} & {R@1} & {R@5} & {R@1} & {R@5} & {R@1} & {R@5} & {R@1} & {R@5}\\ \hline\rowcolor{gray!20}
  ConvNext-Tiny\cite{DenseUAV}       & TIP’2024  &100(\%)& 30.10  & 60.23 & 81.94 & -  & - & -  & -& -  & -\\\rowcolor{gray!20}
  Sample4Geo\cite{deuser2023sample4geo}& ICCV’2023    &100(\%)& 87.57& 80.57 &96.53  &80.57&96.53 &86.87&98.71 &91.38& 99.74 \\\rowcolor{gray!20}
  CAMP\cite{wu2024camp}                & TGRS’2024  &100(\%)& 91.40  & 88.72 & 97.76 &76.45  &96.01 &84.81  &99.23 & 89.19  &99.23 \\\rowcolor{gray!20}
  DAC\cite{xia2024enhancing}           & TCSVT'2024   &100(\%)&96.50 & 84.47 & 96.53 &80.92 &97.04  &85.32 &98.71 &85.46 &99.10\\
  MEAN\cite{chen2024multi}             & arXiv'2024  &100(\%)& 36.50   &90.18 &97.86        &78.76 &97.04 &85.33 &99.10 &89.32 &98.97 \\
  Ours                                 & -     &100(\%)& 28.59       & 86.31  & 97.81      &86.10 &98.46   &88.93 &99.61  &90.35 &99.49\\ \hline
  EM-CVGL\cite{li2024learning}  & TGRS'2024 &0(\%)  & 2.25  & 18.15  & 50.97       & 18.02      &50.36   & 17.76  & 50.19       & 17.50      & 49.98    \\
  CDIKTNet\cite{chen2025limited} &arXiv'2025 &{0(\%)} &91.52 &{22.57}   &{40.24}   &{17.25} &{40.15}   &{23.04}   &{48.39}   &{25.48}    &{52.38}\\
   Ours   & -     &0(\%)& 28.59        & 73.79 & 90.69      &73.10 &92.92    &74.65 &95.37   &75.03 &95.50\\ 
  \hline\hline
\end{tabular}}
\end{center}
\end{table*}

\textit{Results on DenseUAV}:
The DenseUAV dataset presents a highly challenging scenario due to its densely sampled nature, where neighboring images exhibit high visual similarity, often leading to severe confusion in UVGL. In addition, the dataset includes satellite images captured at different times and UAV images from multiple altitudes, which further increases intra-domain variability and task complexity.
As shown in Table \ref{tab:DenseUAV}, under the fully unsupervised setting, our method achieves 73.79\% R@1 and 90.69\% R@5 across all altitudes, significantly outperforming existing self-supervised methods. It is noteworthy that the two representative self-supervised baselines consistently yield poor performance on DenseUAV, highlighting their limitations in handling densely sampled and spatiotemporal diverse scenarios. 
Furthermore, after supervised fine-tuning, our method surpasses several supervised baselines, including ConvNeXt-Tiny, Sample4Geo, CAMP, and DAC. In some cases, it even outperforms the current best-performing supervised method, MEAN. These results clearly demonstrate the effectiveness of our method in addressing extreme feature confusion caused by spatial continuity and appearance shifts induced by temporal variation, confirming its strong generalization and robustness.

\subsection{Ablation Studies}
To thoroughly evaluate the effectiveness of our proposed method, we conduct an ablation study on the University-1652 dataset. We systematically assess the impact of each individual module and investigate their contributions under different configurations. The results are presented in Table \ref{Ablation Studies}.

\textit{ConvNeXt-Tiny Backbone}.
The Backbone refers to the untrained ConvNeXt-Tiny network used as a direct feature extractor. As shown in Table \ref{Ablation Studies}, its performance is significantly limited, achieving an R@1 of only {9.55\%} and {35.24\%}, and an AP of {11.88\%} and {13.20\%} for the {Drone} $\rightarrow$ {Satellite} and {Satellite} $\rightarrow$ {Drone}, respectively. These results indicate that the backbone alone lacks the ability to extract discriminative cross-view features.

\textit{Effectiveness of Dual-Path Contrastive Learning Baseline}.
After incorporating the dual-path contrastive learning baseline, the R@1 improves to 71.06\% and 81.31\%, while AP reaches 74.91\% and 65.26\%, respectively. Although the baseline primarily learns intra-view feature discrimination, the training process also facilitates a certain degree of alignment between UAV and satellite representations. This leads to the formation of a more consistent and unified feature representation space, which serves as a solid foundation for subsequent cross-view learning.

\textit{Effectiveness of Dynamic Hierarchical Memory Learning}.
Compared with the contrastive learning baseline, incorporating DHML leads to significant performance improvements under both settings. Specifically, R@1 increases by 12.97\% and 9.13\%, and AP improves by 11.5\% and 17.48\% , reaching 84.03\% and 90.44\% in R@1, and 86.41\% and 82.74\% in AP, respectively. These gains can be attributed to the joint modeling of long-term and short-term feature dependencies introduced by DHML, which enhances the capacity to capture spatial structures in UVGL scenarios and enhances intra-view feature representation and discrimination.
\begin{table}[t]
  \setlength{\tabcolsep}{2pt}
  \caption{The influence of each component on the performance of proposed method.}
  \begin{tabular}{ccccccccc}
  \hline\hline
  \multicolumn{5}{c}{\multirow{2}{*}{Setting}} & \multicolumn{4}{c}{University-1652}                                       \\ \cline{6-9} 
  \multicolumn{5}{c}{}                         & \multicolumn{2}{c}{Drone$\rightarrow$Satellite} & \multicolumn{2}{c}{Satellite$\rightarrow$Drone} \\ \hline
   Backbone          &Baseline         & DHML      &ICEL       & PLE  & R@1    & AP       & R@1    & AP               \\ \hline\rowcolor{gray!20}
\checkmark &         &                 &       &             & 9.55  & 11.88    & 35.24  & 13.20 \\\rowcolor{gray!20}
 \checkmark& \checkmark        &      &       &             & 71.06  & 74.91    & 81.31  & 65.26 \\\rowcolor{gray!20}
 \checkmark& \checkmark  &\checkmark    &      &            & 84.03  & 86.41    & 90.44  & 82.74  \\\rowcolor{gray!20}
  \checkmark&\checkmark  &    &\checkmark      &         & 74.48  & 77.96    & 89.87  & 74.37     \\\rowcolor{gray!20}
\checkmark &  \checkmark & \checkmark      & \checkmark     & & 86.13    & 88.31  & 94.01 & 82.97 \\
\checkmark & \checkmark  & \checkmark      & \checkmark     & \checkmark &{90.17}    & {91.67} & {95.01}  & {88.95}            \\ \hline\hline
\label{Ablation Studies}
\end{tabular}
% \vspace{-1.8em}
\end{table}

\textit{Effectiveness of Information Consistency Evolution Learning}.
Although DHML enhances intra-view feature representation and discrimination, its optimization remains limited to within-view information. To address this limitation, ICEL is proposed to learn cross-view dependencies. By enabling bidirectional information interaction between different views, ICEL facilitates cross-view feature refinement. As a result, R@1 improves to 86.13\% and 51.04\%, while AP reaches 88.31\% and 82.97\%. These results further validate the effectiveness of ICEL in enhancing cross-view consistency for UVGL, thereby contributing to overall performance improvement.

\textit{Pseudo-Label Enhancement (PLE): Refining Feature Learning with Robust Labels}.
To mitigate pseudo-label noise caused by clustering, we apply PLE to improve label reliability. After incorporating PLE, our final model achieves state-of-the-art performance, with R@1 increasing to 90.17\% and 95.01\%, and AP reaching 91.67\% and 88.95\%. This confirms that refining pseudo-label quality significantly enhances the final model's effectiveness in cross-geographic view matching.

The ablation study systematically evaluates the contribution of each component in the proposed self-supervised framework. Starting with a backbone that lacks sufficient discriminative power, we first establish a dual-branch contrastive learning baseline. On top of this, DHML is introduced to strengthen intra-view feature representation and discriminability, ICEL is employed to facilitate cross-view feature association, and PLE is leveraged for pseudo-label refinement. Experimental results show that our method significantly enhances the learning of discriminative and view-consistent representations under self-supervised learning.
\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in]{4.pdf}
  \caption{The first row presents the t-SNE visualization, while the second row illustrates the similarity distribution for randomly selected geographic locations. In the t-SNE visualization, each color represents a distinct location, where '×' markers correspond to the drone viewpoint, and the triangles indicate the satellite perspective of the same location.}
  \label{fig4}
  \end{figure}

\subsection{Visualization}
\textit{Feature space and similarity distribution}. To further illustrate the effectiveness of the our method, We perform feature space (t-SNE\cite{van2008visualizing} map)
and similarity distribution visualization for our method, as presented in Fig~\ref{fig4}.Comparing (a) Baseline and (b) Ours, it is evident that our method enhances the clustering of cross-view positive samples, effectively bringing the drone and satellite representations of the same location closer together. Additionally, the separation between different locations is more distinct, indicating better feature discriminability. The second row shows the similarity distribution between drone-satellite positive and negative pairs. Our method demonstrates a clearer distinction between positive and negative pairs, with a reduced overlap between distributions, which suggests improved cross-view matching and reduced feature ambiguity. This highlights the effectiveness of our method in UVGL.

\textit{Cluster analysis}.
To validate the efficacy of our method, as shown in Fig.~\ref{fig5}, we illustrate the evolution of cluster counts across epochs for three cross-view datasets. In the DenseUAV dataset (2,255 locations), satellite images exhibit significant clustering fluctuations during the early training stages, due to low intra-class discriminability and weak cross-view correlations. In contrast, the University-1652 dataset (701 locations) demonstrates greater UAV image clustering instability, while satellite image clusters stabilize more rapidly in the initial epochs. The SUES-200 dataset (1,200 locations), with its smaller data size and simpler distribution, exhibits minimal early-stage variation and stabilizes faster than larger datasets. As training progresses, the cluster counts across all datasets gradually converge toward the ground-truth location counts, despite initial inter-class confusion. This process validates the ability of our method to progressively learn discriminative representations.
\begin{figure}[t]
  \centering
  \includegraphics[width=3.2in]{5.pdf}
  \caption{Cluster analysis across different epochs for various datasets. The plot shows the number of clusters for DenseUAV, University-1652, and SUES-200 datasets.}
  \label{fig5}
  \end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in]{6.pdf}
  \caption{Top-6 retrieval results of the proposed method on the University-1652 dataset.}
  \label{fig6}
  \end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in]{7.pdf}
  \caption{Top-6 retrieval results of the proposed method on the DenseUAV dataset.}
  \label{fig7}
  \end{figure}
\textit{Retrieval results}. To demonstrate the effectiveness of our method, we present retrieval results on the University-1652 and DenseUAV dataset in Fig.~\ref{fig6} and Fig.~\ref{fig7}. Green borders indicate correct matches, while red borders denote errors, showing that our method significantly improves retrieval accuracy.

In the University-1652 dataset, the drone$\rightarrow$satellite retrieval task presents drone-view images as queries and retrieves corresponding satellite-view images. Since each location is associated with a single satellite image, our method consistently achieves Top-1 accuracy, demonstrating its high effectiveness in cross-view matching. In the satellite$\rightarrow$drone setting, our method accurately retrieves the corresponding drone-view images given satellite queries, further showcasing its strong cross-view localization capability and reliable feature alignment across perspectives.

In the DenseUAV dataset, the retrieval task becomes significantly more challenging due to dense spatial sampling, multi-scale satellite imagery, and temporal dynamics, all of which exacerbate the difficulty of cross-view representation learning. In the Drone $\rightarrow$ Satellite retrieval setting, despite variations in UAV altitudes and satellite image resolutions, our method effectively captures spatial-geographic cues, achieving high retrieval accuracy while substantially reducing mismatches. Compared to University-1652, DenseUAV provides a more realistic and complex benchmark for UAV-based geo-localization. Under this setting, our method exhibits strong cross-view retrieval performance, demonstrating robust feature discrimination in densely distributed urban regions.

% \section{DISCUSSION}\label{discussion}
\section{CONCLUSION}\label{conclusions}
This paper investigates a highly valuable and challenging task namely self-supervised UAV-view cross-view geo-localization, aiming to reduce the reliance on paired UAV–satellite images. To this end, we propose a self-supervised UVGL method, which employs a shallow backbone with only 9 layers for feature extraction. Pseudo labels are generated via clustering, and a dual-branch contrastive learning mechanism is introduced to enhance the capability in representing intra-view visual patterns. To further improve intra-view discriminability and consistency, we design a dynamic hierarchical memory learning module. Additionally, to mitigate the structural and semantic domain gap between views, we propose an information consistency evolution learning module, which facilitates semantic alignment by learning high-dimensional feature consistency across views. To improve the quality and robustness of pseudo labels, we introduce a bidirectional feature consistency-based Pseudo Label Enhancement strategy, which refines pseudo-supervision signals and boosts cross-view representation learning. As a result, the proposed method effectively unifies both intra-view and cross-view feature representations in the embedding space. Extensive experiments conducted on three public benchmarks University-1652, SUES-200, and DenseUAV, demonstrate that our method significantly outperforms existing self-supervised methods, even surpassing several fully supervised methods in certain scenarios, without relying on paired UAV–satellite images. These results validate the effectiveness and generalization capability of our method, highlighting its potential for real-world UVGL applications.
\bibliographystyle{IEEEtran}
\bibliography{reference}
% \begin{IEEEbiography}[{\includegraphics
% [width=1in,height=1.25in,clip,
% keepaspectratio]{2.jpeg}}]
% {Zhongwei Chen} received the B.S. degree from Civil Aviation University of China, Tianjin, China, in 2024. He is currently pursuing the M.S. degree at the Xi’an Jiaotong University, Xi’an, China. His research focuses on computer vision and deep learning, with primary interests in cross-modality person re-identification, cross-view geo-localization, and related intelligent vision technologies.
% \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics
% [width=1in,height=1.25in,clip,
% keepaspectratio]{2.jpeg}}]
% {Zhongwei Chen} received the B.S. degree from Civil Aviation University of China, Tianjin, China, in 2024. He is currently pursuing the M.S. degree at the Xi’an Jiaotong University, Xi’an, China. His research focuses on computer vision and deep learning, with primary interests in cross-modality person re-identification, cross-view geo-localization, and related intelligent vision technologies.
% \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics
% [width=1in,height=1.25in,clip,
% keepaspectratio]{2.jpeg}}]
% {Zhongwei Chen} received the B.S. degree from Civil Aviation University of China, Tianjin, China, in 2024. He is currently pursuing the M.S. degree at the Xi’an Jiaotong University, Xi’an, China. His research focuses on computer vision and deep learning, with primary interests in cross-modality person re-identification, cross-view geo-localization, and related intelligent vision technologies.
% \end{IEEEbiography}

\end{document}


