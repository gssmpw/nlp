\subsection{Hypergraph neural networks}

Hypergraph neural networks (HGNNs) extend graph neural networks(GNNs) by enabling complex interactions through hyperedges. Early models like HGNN\cite{feng2019hypergraph} and HyperGCN\cite{yadati2019hypergcn} convert hypergraphs into graphs for processing, leading to the loss of higher-order relationships. HNHN\cite{dong2020hnhn} improves this problem by using a two-step message passing that preserves these relationships. Attention-based models such as HCHA\cite{bai2021hypergraph} further refine node and hyperedge aggregations. Advanced frameworks such as  UniGNN\cite{huangunignn} and AllSet\cite{chien2021you} have generalized these concepts to hypergraphs, with UniGCNII focusing on preventing over-smoothing in deep networks. AllSet and its variant AllSetTransformer use sophisticated functions for improved aggregation. However, most models still struggle with fully capturing the nuanced interactions within hyperedges. Furthermore, a majority of these models are still primarily limited to the settings of supervised or semi-supervised learning.

\subsection{Graph contrastive learning}
The realm of graph contrastive learning has been largely inspired by its success in computer vision and natural language processing. Techniques like DGI\cite{velivckovic2018deep}, InfoGraph\cite{sun2019infograph}, GRACE\cite{zhu2020deep}, and MVGRL\cite{hassani2020contrastive} have shown promising results in obtaining expressive representations for graphs and nodes by maximizing mutual information between various levels of graph structure. More recent approaches like GCA\cite{zhu2021graph} and DGCL\cite{zhao2021graph} have introduced novel methods for data augmentation, which is crucial for contrastive learning. However, these methods often require manual trial-and-error, cumbersome search processes, or expensive domain knowledge for augmentation selection, limiting their scalability and general applicability. 


\subsection{Hypergraph contrastive learning}
Contrastive learning on hypergraphs, though still in its nascent stages, is beginning to receive more attention. Initial studies such as S\textsuperscript{2}-HHGR\cite{zhang2021double} have focused on applying contrastive learning for specific applications like group recommendation, employing unique hypergraph augmentation schemes. However, they often overlook the broader potential of hypergraph structures in general representation learning. Recent efforts in this area are starting to explore more comprehensive approaches that fully leverage the complex structure of hypergraphs. For instance, TriCL\cite{lee2023m} uses a tri-directional contrast mechanism to learn node embeddings by contrasting node, group, and membership labels. However, many of these methods still focus on specific applications and do not fully exploit the rich, multi-relational nature of hypergraphs.

In this paper, we propose a hypergraph-specific contrastive learning method that exploits the high-order correlations of hypergraphs by presenting a fine-grained approach to learning node pairs. We also present a method that does not exploit the structure of hypergraphs, enabling more efficient learning than existing hypergraph models.