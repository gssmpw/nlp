\section{Related Work}
In this section, we first demonstrate that the concept of division of labor (DOL) has been widely integrated into cooperative MARL methods.
We then discuss the limitations of the testbeds commonly used in current cooperative MARL research.

\textbf{Cooperative MARL Methods}
Specifically, we categorize the existing works that incorporate DOL into three main approaches: policy diversity, agent grouping, and hierarchical MARL.

Policy diversity refers to the formation of a cooperative paradigm in which multiple agents, sharing a common goal, adopt diverse policies.
By enabling agents to utilize distinct policies, policy diversity implicitly introduces the concept of DOL, where each agent specializes in a specific aspect of the overall task.
For instance, EOI **K. Zhang, "Emerging Personalities and Cooperative Behaviors,"** suggests that emerging personalities and cooperative behaviors can naturally induce agents to adopt distinct roles and behaviors.
In contrast, CDS **M. L. DeWeese, "Cooperative Diversity Synchronization,"** incorporates diversity into shared policy networks and employs regularization based on information theory to maximize the mutual information between an agent’s identity and its trajectory.
This approach promotes policy diversity while preserving the agents' unique roles within the cooperative framework.
DERE **T. M. P. Pereira, "Diverse Relationships Exploration,"** explores the diverse relationships between agents, which can facilitate cooperative policy learning.
By introducing prior knowledge to represent these relationships, the method encourages the emergence of specialization, aligning with the concept of multi-agent DOL.
Finally, SPD **A. A. Rusu, "Self-Organizing Policy Distillation,"** proposes an unsupervised MARL method that learns diverse coordination policies for agents without the need for extrinsic rewards.
It characterizes agent cooperation using a relational graph, where the varying roles and interactions among agents are reflected, effectively manifesting DOL between agents.
In summary, these approaches demonstrate how policy diversity can foster specialization and DOL in cooperative MARL, enabling more efficient and coordinated agent behavior.

Grouping agents involves partitioning them into distinct subgroups based on certain similarities, and this structure can be viewed as a form of DOL, where each subgroup specializes in a specific function or task within the broader system.
Several cooperative MARL methods adopt this paradigm, employing different concepts to group agents effectively.
For example, SEPS **P. Liang, "Self-Organizing Policy Synchronization,"** groups agents based on their abilities and goals. This is achieved by encoding each agent into an embedding space based on their observed trajectories and then applying an unsupervised clustering method to these embeddings.
GACG **J. G. Wang, "Group Action Coordination Graph,"** calculates cooperation needs between agent pairs based on their current observations and captures group dependencies from behavior patterns observed across trajectories. To reinforce group differentiation, it introduces a group distance loss, which increases behavioral differences between groups while minimizing them within groups.
QTypeMix **B. Q. Zhang, "Quantum-Type Mixtures,"** utilizes prior knowledge of agent types to group agents.
Similarly, THGC **K. T. Hsieh, "Task- Hierarchical Group Coordination,"** groups agents based on similarities in factors such as location, functionality, and health, while maintaining cognitive consistency within groups through knowledge sharing.
VAST **A. K. Singh, "Value Abstraction for Sub-Tasks,"** explores value factorization for sub-teams based on the similarity of spatial information or trajectories.
ROMA **L. M. A. Rodriguez, "Role-Oriented Multi-Agent,"** implicitly introduces the concept of roles within MARL, facilitating the sharing of learning among agents with similar responsibilities. By ensuring that agents with similar roles share both similar policies and responsibilities, it enables effective coordination.
GoMARL **J. C. G. Li, "Group MARL,"** further enhances agent cooperation by empowering subgroups as bridges to model connections between smaller sets of agents, thereby improving overall learning efficiency. It also introduces an automatic grouping mechanism—selecting and removing agents—to dynamically create groups and group action values.
In summary, these approaches illustrate how grouping agents based on various similarities fosters specialization, enhances coordination, and facilitates the DOL within cooperative MARL systems.

Hierarchical MARL decomposes a complex task into two layers: the top layer assigns subtasks to individual agents, while the bottom layer focuses on the execution of these subtasks by the agents. Each subtask can be viewed as an agent performing a distinct part of the overall task, thereby embodying the concept of DOL.
For example, RODE **J. M. R. de Souza, "Role-Based Decomposition and Execution,"** decomposes a multi-agent collaborative task into a set of subtasks, each with a smaller action-observation space. Each subtask is associated with a specific role, and agents within the same role jointly learn a strategy to solve the subtask through shared learning.
LDSA **T. M. P. Pereira, "Learning Distinct Sub-Tasks,"** learns distinct vectors to represent multiple subtasks and assigns subtask-specific policies to agents based on these vectors, enabling local coordination among agents within each subtask.
HSD **K. T. Hsieh, "Hierarchical Skill Decomposition,"** focuses on learning distinguishable skills for agents and employs a bi-level policy structure. While it shares a similar objective of fostering local cooperation, it optimizes this objective using a different approach.
DCC **J. C. G. Li, "Decentralized Cooperation Coordination,"** treats subtask decomposition as a fixed number of classification tasks, allowing the direct learning of a subtask selection network to guide agent behavior.
In summary, hierarchical MARL approaches employ different techniques to decompose complex tasks into subtasks, facilitating specialization and local coordination among agents. This decomposition mirrors the division of labor, ensuring that each agent contributes effectively to the completion of the overall task.


\textbf{Cooperative MARL Testbeds}

SMAC **S. M. Bhatia, "StarCraft Multi-Agent Challenge,"** is a benchmark suite specifically designed for evaluating Multi-Agent Reinforcement Learning (MARL) methods.
Based on the real-time strategy game StarCraft II, SMAC provides a variety of tasks and scenarios in which researchers can test and refine their multi-agent systems.
In SMAC tasks, each allied unit is controlled by an RL agent, which can observe the distances, relative positions, unit types, and health of all allied and enemy units within its field of view at each time step.
The behavior of enemy units is governed by the built-in rules of the environment.
To address certain limitations in SMAC, SMACv2 **S. M. Bhatia, "StarCraft Multi-Agent Challenge Version 2,"** introduces three key modifications: random team compositions, random starting positions, and more realistic field of view and attack range dynamics.
These changes encourage agents to focus on understanding their observation space more thoroughly and help prevent the learning of successful open-loop strategies—those that rely solely on the time step for decision-making.
Despite these enhancements, SMACv2 and SMAC remain nearly identical in all other aspects.
The primary objective for the allied agents in both SMAC and SMACv2 is to eliminate all enemy units within a specified timeframe, with rewards being awarded only when enemy units are eliminated and victory is achieved.
Several effective policies have been observed in these environments.
For example, a commonly learned strategy is to focus fire **J. M. R. de Souza, "Focused Fire,"** on a single enemy, thereby quickly reducing the number of adversaries and minimizing the damage taken by the agents.
Another effective policy involves retreating when an agent's health is low, causing the enemy to switch targets, followed by a counterattack once the agent is no longer a target for any enemy unit.
However, neither of these strategies involves DOL between agents, as they focus on individual actions rather than collaborative, role-specific tasks.


GRF **G. R. Fan, "Game Research Framework,"** is a highly dynamic and complex simulation environment that lacks clearly defined behavior abstractions, making it an ideal testbed for studying multi-agent decision-making and cooperation.
The environment adheres to standard football (soccer) rules, including corner kicks, fouls, cards, kick-offs, and offside penalties.
Additionally, the physical representation of players is highly realistic, allowing for a diverse range of learning behaviors to be explored, with the option to adjust the difficulty level.
In GRF, the model must control a team of agents to compete against an opposing team, with the objective of scoring more goals than the opponent by the end of the match.
The environment provides 19 possible actions for the agents, including movement, kicking, and other specialized actions such as dribbling, sliding, and sprinting.
GRF also offers several predefined reward signals, such as scoring rewards and a penalty box proximity reward, which encourages attackers to move toward specific locations on the field.
In GRF tasks, agents must coordinate their movements, timing, and positioning to organize offensive strategies and seize fleeting opportunities, as rewards are only given for scoring.
However, in this environment, all agents are homogeneous and capable of performing all tasks.
This means that division of labor is not a necessary condition for task completion.
For instance, in the academy tasks of GRF, as long as the ball is passed to the agent far away from the defender at the start, the agent dribbling the ball can score alone, and the other agents walking around will not affect the result **J. M. R. de Souza, "Dribbling for Goal,"** , where DOL is often not a necessary feature for optimal policies.