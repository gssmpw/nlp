%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\icmltitlerunning{Under Review in ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yurui Li}{yyy}
\icmlauthor{Yuxuan Chen}{yyy}
\icmlauthor{Li Zhang}{yyy}
\icmlauthor{Shijian Li}{yyy}
\icmlauthor{Gang Pan}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{College of Computer Science and Technology, Zhejiang University, Hangzhou, China}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Shijian Li}{shijianli@zju.edu.cn}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Cooperation, Division of Labor}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    The significant role of division of labor (DOL) in promoting cooperation is widely recognized in real-world applications.
    Many cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among agents.
    However, the tasks used in existing testbeds typically correspond to tasks where DOL is often not a necessary feature for achieving optimal policies.
    Additionally, the full utilize of DOL concept in MARL methods remains unrealized due to the absence of appropriate tasks.
    To enhance the generality and applicability of MARL methods in real-world scenarios, there is a necessary to develop tasks that demand multi-agent DOL and cooperation.
    In this paper, we propose a series of tasks designed to meet these requirements, drawing on real-world rules as the guidance for their design.
    We guarantee that DOL and cooperation are necessary condition for completing tasks and introduce three factors to expand the diversity of proposed tasks to cover more realistic situations.
    We evaluate 10 cooperative MARL methods on the proposed tasks.
    The results indicate that all baselines perform poorly on these tasks.
    To further validate the solvability of these tasks, we also propose simplified variants of proposed tasks.
    Experimental results show that baselines are able to handle these simplified variants, providing evidence of the solvability of the proposed tasks.
    The source files is available at https://github.com/Yurui-Li/CTC.
    % Many cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among agents.
    % However, their testbeds typically correspond to low-level, relatively simple tasks that are far away from real-world and DOL is often not a necessary feature for achieving optimal policies.
    % To enhance the applicability of MARL methods in real-world scenarios, there is a need for more complex tasks that demand multi-agent cooperation and the application of DOL.
    % In this paper, we propose a series of tasks (CTC) designed to meet these requirements, drawing on real-world rules as the guidance for their design.
    % We guide the development of these tasks from three key perspectives to ensure that they reflect a broad range of real-world scenarios.
    % We report the performances of 10 cooperative MARL methods on the proposed tasks and validate the solvability of CTC tasks.
\end{abstract}

\section{Introduction}
\label{intro}
Specialization has long been recognized as a cornerstone of efficiency enhancement, a foundational concept dating back to Adam Smith’s seminal analysis in The Wealth of Nations~\cite{smith2002inquiry}.
In practical applications, this principle manifests through division of labor (DOL) and cooperative mechanisms, with DOL serving as a critical catalyst for improving both the efficiency and effectiveness of cooperative systems.
Within the domain of Multi-Agent Reinforcement Learning (MARL), DOL presents a natural framework for advancing cooperative learning, as task specialization among agents can significantly improve collective performance.
However, prevailing MARL testebds—including the StarCraft Multi-Agent Challenge (SMAC)~\cite{samvelyan2019starcraft}, SMACv2~\cite{ellis2022smacv2}, and Google Research Football (GRF)~\cite{kurach2020google}—prioritize evaluations of low-level task execution while neglecting systematic assessment of agents’ capacity to establish meaningful DOL structures.
This limitation undermines their utility in guiding research on sophisticated cooperative policies.  
For instance, in SMAC tasks, optimal performance is frequently achieved through simplistic tactics such as concentrated fire on shared targets~\cite{li2023ace,liu2024interaction,liu2023contrastive,mahajan2019maven,yang2022ldsa,yu2023ghq,hu2023attention,shao2022self,zang2024automatic}, a policy that does not inherently necessitate role differentiation or specialization.
Similarly, in GRF’s academy scenarios, the most readily learnable policies involves passing the ball to agents positioned far from defenders, after which a single agent can independently score while others engage in non-coordinated movement~\cite{fu2024iteratively,li2021celebrating,xu2023group}.
Although some MARL methods have incidentally developed DOL policies~\cite{zang2024automatic,fu2022revisiting,fu2024iteratively,li2021celebrating,xu2023group}, such specialization remains non-essential for task success in these tasks.
Consequently, existing testbeds fail to adequately assess or incentivize the development of advanced DOL capabilities, thereby limiting their ability to drive progress in cooperative MARL research.
This gap highlights the need for testbeds that explicitly require DOL and cooperation to advance the study of cooperative multi-agent systems.  
% Specialization has long been recognized as a key driver of efficiency gains, a concept that can be traced back at least to Adam Smith’s Wealth of Nations~\cite{smith2002inquiry}.
% In practice, specialization manifests itself as the division of labor (DOL) and cooperation.
% The DOL plays a crucial role in enhancing the efficiency and effectiveness of cooperation. 
% Given that, the concept of DOL is a natural framework for enhancing cooperation in Multi-Agent Reinforcement Learning (MARL), where it can substantially improve system performance by facilitating task specialization and cooperative learning.
% However, the testbeds commonly used (e.g. SMAC~\cite{samvelyan2019starcraft}, SMACv2~\cite{ellis2022smacv2}), and GRF~\cite{kurach2020google}) for cooperative MARL methods only focuses on evaluating the performance on low-level tasks, but ignores the evaluation of the ability of cooperative MARL methods to achieve division of labor and cooperation among multiple agents.
% For instance, in the SMAC environment, focusing fire on the same enemy is a highly effective strategy~\cite{li2023ace,liu2024interaction,liu2023contrastive,mahajan2019maven,yang2022ldsa,yu2023ghq,hu2023attention,shao2022self,zang2024automatic}, where DOL is not explicitly required.
% In the academy tasks of GRF, the easiest strategy to learn for success is that passing the ball to the agent far away from the defender at the start.
% Then the agent dribbling the ball can score alone even with the situation that other agents walking around~\cite{fu2024iteratively,li2021celebrating,xu2023group}.
% Even though there are many policies for achieving success through DOL that have been learned by many MARLs~\cite{zang2024automatic,fu2022revisiting,fu2024iteratively,li2021celebrating,xu2023group}, DOL is not a necessary condition for a successful policy for the GRF task.
% Therefore, these testbeds cannot guide the research on effective DOL and cooperation on multi-agent domain.

Lots of cooperative MARL researches have increasingly integrated the principle of DOL into algorithmic design, with methodologies broadly categorized into three paradigms: policy diversity, agent grouping, and hierarchical MARL.  
Policy diversity~\cite{jiang2021emergence,mahajan2019maven,li2021celebrating} addresses the policy similarity inherent in parameter-sharing architectures by encouraging agents with shared objectives to adopt distinct behavioral policies.
This diversity implicitly induces role specialization, where agents develop complementary policies aligned with specific subtasks, thereby implementing DOL through emergent heterogeneity.  
Agent grouping formalizes DOL by partitioning agents into functionally differentiated subgroups.
Group composition may be determined by predefined roles~\cite{wang2020roma}, sub-goals~\cite{christianos2021scaling}, environmental tasks~\cite{yang2022ldsa}, or intrinsic capabilities~\cite{christianos2021scaling}.
Agents within a group share policies, while inter-group differentiation ensures specialized task execution, creating an explicit DOL structure where subgroups focus on distinct components of the collective objective.  
Hierarchical MARL architectures decompose cooperative tasks into two interdependent layers: a top layer for task decomposition and subtask allocation, and an bottom layer for subtask execution.
This stratification enables systematic DOL implementation, with agents specializing in subtask categories defined through joint action space partitioning~\cite{wang2020rode}, learned sub-task representations~\cite{yang2022ldsa}, or classification-based sub-task selection networks~\cite{li2024coordinating}.
Such hierarchical frameworks inherently align with DOL principles by task decomposition and subtask allocation.  
While these methodologies demonstrate the integration of DOL into MARL frameworks, their full potential remains unrealized due to the absence of appropriate tasks explicitly designed to evaluate and incentivize DOL mechanisms.
Existing testbeds prioritize low-level task completion over assessing DOL.
Consequently, current testbeds fail to quantify the efficacy of DOL policies or provide optimization signals for their refinement.
% This limitation underscores the critical need for dedicated benchmarks that rigorously measure how effectively MARL systems allocate, coordinate, and adapt specialized roles—a prerequisite for advancing cooperative intelligence in complex multi-agent domains.  
% Actually, lots of cooperative MARL methods have incorporated the concept of DOL.
% These methods can be broadly classified into three categories: policy diversity, agent grouping, and hierarchical MARL.
% Policy diversity~\cite{jiang2021emergence,mahajan2019maven,li2021celebrating} refers to the formation of a cooperative paradigm where multiple agents, sharing a common goal, adopt diverse policies.
% This diversity addresses the issue of policy similarity that arises from parameter sharing among agents.
% By enabling agents to use distinct policies, policy diversity implicitly introduces the concept of DOL, where each agent can be seen as specializing in a particular aspect of the overall task.
% Grouping agents involves partitioning the agents into distinct groups based on certain similarities.
% In this approach, agents within a group share the same policy, while agents in different groups employ different policies.
% The groups can be formed using various criteria, such as roles~\cite{wang2020roma}, goals~\cite{christianos2021scaling}, agent capabilities~\cite{christianos2021scaling}, or tasks~\cite{yang2022ldsa}.
% This grouping structure can be viewed as a form of DOL, where each subgroup of agents specializes in a specific function or task within the broader system.
% Hierarchical MARL decomposes a task into two layers: the top layer decomposes the task into subtasks and assigns subtasks to individual agents, while the bottom layer is concerned with the execution of these subtasks by the agents.
% Each subtask can be considered as an agent performing a distinct part of the overall task, thus reflecting the principle of DOL.
% Hierarchical methods can decompose tasks in various ways, such as by partitioning joint action spaces~\cite{wang2020rode}, learning sub-task representation vectors ~\cite{yang2022ldsa}, or developing sub-task selection networks through classification tasks~\cite{li2024coordinating}.
% In summary, the concept of DOL is effectively integrated into cooperative MARL through policy diversity, agent grouping, and hierarchical task decomposition.
% However, due to the lack of a testbed for DOL, each method does not fully utilize the DOL.

To advance research on DOL in cooperative MARL and fully exploit the potential of DOL mechanisms, we propose a suite of benchmark tasks addressing critical gaps in existing testbeds.
Our design philosophy stems from a empirical observation: real-world complex tasks inherently emerge through the hierarchical composition of atomic subtasks.
For instance, cooking preparation involves sequential subtasks (e.g., ingredient washing, chopping, and cooking) that combine to form higher-order objectives (e.g., meal preparation), which further integrate with service and procurement tasks to constitute restaurant management systems.
This compositional hierarchy mirrors the organizational structure of real-world cooperative activities, thereby providing an valid foundation for DOL-oriented MARL testbeds.  
we design tasks by composing atomic subtasks, ensuring that DOL and cooperation are not merely beneficial but necessary for task completion.
This design principle enforces explicit role specialization and inter-agent coordination, as no single agent can independently execute all subtasks within the given constraints.
In addition, to systematically implement these principles, we introduce three key design factors: 
1) Information Interference: Mirroring real-world scenarios where agents encounter extraneous or conflicting observational signals, we implement controlled interference through cross-agent observational noise.
Task variants with graduated interference levels enable quantitative assessment of agents’ robustness to decision-distorting information.  
2) Subtask Dissimilarity: Recognizing the functional heterogeneity inherent in complex systems, our tasks incorporate dissimilar subtask configurations.
This setting challenges MARL methods to develop specialized policies for each subtask while maintaining inter-subtask coordination.  
3) Subtask Quantity: While complex systems theoretically permit unlimited subtask decomposition, we constrain maximum subtasks to four based on cognitive science principles of human working memory limits. This intentional bottleneck makes task diffculities solvable at present while preserving scalability—hierarchical nesting allows infinite atom subtasks through recursive composition of four-task modules.
% Through these tasks, we provide both an evaluation platform for current DOL mechanisms and a design template for future task creation, ultimately bridging the gap between artificial cooperative systems and real world.
By adjusting these factors, we create a diverse suite of tasks that challenge cooperative MARL methods to learn and operationalize DOL and cooperation mechanisms under varying levels of complexity.
This design not only mirrors the organization of real-world collaborative systems but also provides a rigorous framework for evaluating and advancing the state of the art in cooperative MARL.

% To guide the research on DOL for cooperative MARL methods and make full use of the mechanism of DOL and cooperation, we propose a series of tasks to fill the gaps in the testbed.
% In this paper, we propose a series of tasks to meet the requirements.
% The core idea behind these tasks is based on a real-world observation: complex tasks are often composed of simpler, atomic tasks through combination and layering.
% For example, in the context of cooking, a multi-agent task may involve several simple actions such as washing vegetables, cutting vegetables, and cooking.
% These atomic tasks can be combined to form the broader task of preparing a dish.
% Similarly, tasks such as cooking, restaurant service, and procurement can be combined to create the more complex task of running a restaurant.
% This hierarchical structure—where low-level tasks are combined to form higher-level, more complex tasks—reflects the way real-world activities are organized and provides a natural way to incorporate the need for DOL in cooperative MARL testbed.
% By structuring tasks in this way, we aim to create more realistic and challenging testbeds that better simulate the complexities of multi-agent cooperation in real-world applications.
% In addition, there are three factors to consider when designing these tasks.
% The first factor is information interference between subtasks.
% In real-life settings, people often encounter information that is either irrelevant or even disruptive.
% A similar phenomenon occurs in multi-agent environments, where the observation of other agents’ actions or subtasks can interfere with an agent’s decision-making process on its own subtask.
% To assess the impact of such interference, we designed tasks with varying levels of information interference.
% The second factor is the subtasks similarity.
% In practice, subtasks within a complex task are often not homogeneous.
% While some subtasks may be similar, dissimilar subtasks are more common.
% Our designed tasks incorporate both similar and dissimilar subtasks to evaluate how MARL methods handle different levels of task dissimilarity.
% The third factor is the number of subtasks.
% Complex real-world tasks frequently involve numerous subtasks.
% In order to provide a promising goal for the cooperative MARL method rather than a distant mirage, we limit the maximum number of subtasks to 4.
% At the same time, this setting also leaves an interface for future expansion, because the number of bottom-level subtasks can be expanded by hierarchically stacking while keeping the number of top-level subtasks at 4.
% This allows the design ideas and training logic used when the maximum number of subtasks is 4 to be extended to more subtasks without change.
% Combining the core ideas mentioned above and these three factors, we can construct tasks that are suitable for guiding the research on DOL and cooperation in cooperative MARL methods and making deeper use of the DOL and cooperation mechanism.

We propose the \textbf{C}omposite \textbf{T}asks \textbf{C}hallenge (\textbf{CTC}), a testbed designed to evaluate the ability of cooperative MARL methods to effectively implement DOL and cooperation.
The CTC consists of eight tasks, categorized into three dimensions:  
1) Information Interference: 3 tasks with varying levels of observational noise and cross-agent interference.  
2) Subtask Dissimilarity: 2 tasks with heterogeneous subtask structures.  
3) Subtasks Quantity: 3 tasks with compositions of 2 to 4 subtasks.  
To evaluate the effectiveness of existing MARL methods, we select 10 representative baselines spanning three major categories: policy diversity, agent grouping, and hierarchical MARL.
Experimental results reveal that all baseline methods struggle to achieve satisfactory performance on the CTC tasks, with success rates consistently equal to zero.
This underscores a critical limitation: despite the theoretical incorporation of DOL principles in these methods, they fail to fully operationalize DOL and cooperation mechanisms in practice.  
To validate the solvability of the CTC tasks, we derive nine simplified variants by introducing structural symmetry (via similar subtasks) and agent homogeneity (using identical agent types).
Baseline methods demonstrate significantly improved performance on these simplified tasks, confirming that the CTC tasks are computationally tractable while retaining sufficient complexity to challenge current MARL approaches.  
Furthermore, we observe that most baselines exhibit high performance fluctuations at different seeds across both the CTC tasks and their simplified variants.
To quantify this instability, we introduced a stability coefficient and compute the value of baseline methods on 8 CTC tasks and 9 simplified variants.
The results show that most methods indicate low stability in performance and highlight the need for more robust methods.   
To sum up, our contributions is summary as follow:  
\begin{enumerate}
    \item Testbed Design: We propose the CTC, a novel testbed suite that presents solvable, practical tasks with explicit requirements for DOL, offering significant potential to advance the development of cooperative MARL methods.
    \item Comprehensive Evaluation: We evaluate 10 cooperative MARL methods on the CTC tasks, demonstrating their inability to effectively handle tasks requiring sophisticated DOL and cooperation. 
    \item Solvability Validation: We validate the solvability of the CTC tasks through simplified variants, providing evidence that the tasks are tractable while remaining challenging for current methods.
    \item Stability Analysis: We introduce a stability coefficient to quantitatively assess the stability of MARL methods across multiple seeds, revealing their instability across both CTC and simplified tasks.
\end{enumerate} 
% The CTC framework fills a critical gap in MARL research by providing a testbed that explicitly evaluates DOL and cooperation mechanisms.
% By exposing the limitations of current methods, CTC establishes a foundation for future advancements in cooperative MARL.  

% The tasks we propose are named \textbf{C}omposite \textbf{T}asks \textbf{C}hallenge (\textbf{CTC}).
% CTC contains 8 tasks, of which 3 are related to information interference, 2 are related to subtask similarity, and 3 are related to the number of subtasks.
% We select representative works from three categories of MARL methods as baselines, totaling 10, and construct experiments on the CTC task.
% The results reveal that all these methods struggle to handle the CTC tasks.
% This shows that even though the three types of MARL methods introduce the idea of DOL, they still cannot fully utilize the mechanism of DOL and cooperation.
% Furthermore, to validate the solvability of the CTC, we reduce the difficulty of the tasks by introducing symmetry (through similar subtasks) and homogeneity (by using agents of the same type) to form 9 simplified tasks.
% Experimental results show that the methods are able to handle these simplified tasks, providing evidence of the solvability of the CTC.
% In addition, we also observe that most of baselines exhibit low stability on CTC tasks and simplified tasks.
% To quantitatively assess the stability of each method, we introduced a stability coefficient to evaluate their stability across the CTC tasks.
% To sum up, our contributions is summary as follow:
% \begin{enumerate}
%     \item We propose CTC tasks which presents solvable, practical tasks with a clear requirement for DOL and has significant potential to advance the development of cooperative MARL methods.
%     \item We evaluate the performance 10 cooperative MARL methods on CTC tasks and demonstrate their powerlessness.
%     \item We evaluate the performance of the baselines on the simplified tasks and validate the sovability of CTC tasks.
%     \item we introduce a stability coefficient to quantitatively assess the stability of each method and demonstrate their powerlessness.
% \end{enumerate}


% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centering
% 	\begin{minipage}[content]{0.3\textwidth}
% 	\centering
% 	\includegraphics[scale=0.3]{./Figs/smac_P2G.jpg}
%          \numberedblock (a) Pursuit\_2\_Subtask
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.3\textwidth}
% 	\centering
% 	\includegraphics[scale=0.3]{./Figs/smac_M2G.jpg}
%          \numberedblock (b) Mixed\_2\_Subtask
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.3\textwidth}
% 	\centering
% 	\includegraphics[scale=0.3]{./Figs/smac_M3G.jpg}
%          \numberedblock (c) Mixed\_3\_Subtask
% 	\end{minipage}
% \\
% \centering
% 	\begin{minipage}[content]{0.3\textwidth}
% 	\centering
% 	\includegraphics[scale=0.3]{./Figs/smac_D2G.jpg}
%         \numberedblock (d) Denfense\_2\_Subtask
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.3\textwidth}
% 	\centering
% 	\includegraphics[scale=0.3]{./Figs/smac_D3G.jpg}
%          \numberedblock (e) Denfense\_3\_Subtask
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.3\textwidth}
% 	\centering
% 	\includegraphics[scale=0.3]{./Figs/smac_D4G.jpg}
%          \numberedblock (f) Denfense\_4\_Subtask
% 	\end{minipage}
% \caption{Basic tasks.}
% \label{fig:env}
% \end{center}
% \vskip -0.2in
% \end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\begin{tabular}{@{\extracolsep{\fill}}c@{}c@{}c@{\extracolsep{\fill}}}
            \includegraphics[scale=0.3]{./Figs/smac_P2G.jpg} &
            \includegraphics[scale=0.3]{./Figs/smac_M2G.jpg} &
            \includegraphics[scale=0.3]{./Figs/smac_M3G.jpg}\\
            (a) Pursuit\_2\_Subtask & (b)  Mixed\_2\_Subtask & (c) Mixed\_3\_Subtask \\
            \includegraphics[scale=0.3]{./Figs/smac_D2G.jpg} &
            \includegraphics[scale=0.3]{./Figs/smac_D3G.jpg} &
            \includegraphics[scale=0.3]{./Figs/smac_D4G.jpg} \\
            (d) Denfense\_2\_Subtask & (e) Denfense\_3\_Subtask & (f) Denfense\_4\_Subtask\\
\end{tabular}
\caption{Performance of baselines on HeA tasks.}
\label{fig:env}
\end{center}
\vskip -0.2in
\end{figure*}




\section{CTC Tasks}
\label{proposed env}
In this section, we first introduce how CTC ensures that DOL and cooperation are necessary condition for completing tasks.
Second, we introduce three factors to expand the diversity of CTC tasks to cover more realistic situations.
Finally, we introduce the SMAC-based CTC tasks implementation.

\subsection{DOL and Cooperation Guarantee}
We design the CTC tasks by combining atomic subtasks in a way that ensures DOL and cooperation are essential for task completion.

We first introduce two atomic subtasks: the defense subtask and the pursuit subtask.
In the defense subtask, a base building is set, and a group of enemies aims to attack this building.
The defense subtask fails if any enemy occupies the base building.
The success condition for this subtask is the complete elimination of all enemies.
In the pursuit subtask, an enemy base building is established, and a group of enemies attempts to flee towards the base building.
The task fails if any enemy reaches the base building, and the success condition is the same as the defense subtask.
While the pursuit subtask shares some design similarities with the defense subtask, the primary difference lies in the behavior of the enemies.
In the pursuit subtask, enemies do not retaliate when attacked but persistently move toward their target. 
In contrast, enemies in the defense subtask remain stationary and actively engage in combat when attacked.

Next, we combine these two atomic subtasks to form the CTC task.
The composition of these subtasks is designed such that DOL and cooperation are necessary condition for task completion.
Specifically, we combine $N$ defense or pursuit subtasks into a single composite task, ensuring that the subtasks remain independent of one another.
The success condition for the composite task is the completion of all subtasks, while the failure condition occurs if any subtask fails.
To prevent a single group of agents from addressing multiple subtasks simultaneously, we ensure that the enemy groups are sufficiently spaced apart.
This arrangement guarantees that DOL and cooperation are fundamental requirements for successfully completing the task. 

% We design CTC tasks by composing atomic subtasks, ensuring that DOL and cooperation are necessary condition for completing tasks.

% We first introduce our designed two atomic subtasks: the defense subtask and the pursuit subtask.
% In the defense subtask, we set a base building and a group of enemies which aim to attack the base building.
% The defense subtask fails if the base building is occupied by any enemy.
% The success condition for the defense subtask is the complete elimination of all enemies.
% In the pursuit subtask, we set a enemy base building and a group of enemies which aim to flee towards their base building.
% The task fails if any enemy reaches the base building.
% The success condition is same as the defense subtask.
% While the pursuit subtask shares some design similarities with the defense subtask, the essential difference lies in the behavior of the enemies.
% In the pursuit subtask, enemies do not fight back under attacked but instead persistently move toward their target.
% In contrast, enemies in the defense subtask stay in place under attacked and actively engage in combat.

% Then, we introduce the composition of these two atomic subtasks to form the CTC task.
% The composition of these two atomic subtasks need to guarantee that DOL and cooperation are necessary conditions for completing tasks.
% Our implementation method is to combine $N$ defense subtasks or pursuit subtasks into one task and ensure that these subtasks are independent of each other.
% At the same time, we set the success condition of this task to be the completion of all subtasks, and the failure condition is the failure of any subtask.
% To prevent a single group of agents from addressing multiple subtasks simultaneously, we ensure that the enemy groups are sufficiently spaced apart.
% This fundamentally guarantees the necessity of DOL and cooperation.


\subsection{Diversity of CTC tasks}
Real-world applications are inherently complex and diverse, and merely combining two atomic subtasks does not adequately capture the full scope of these scenarios.
By incorporating three key factors (information interference, subtasks dissimilarity, subtasks quantity) we can design a wider range of tasks with varying levels of difficulty.
This design offers a step-by-step progression for advancing cooperative MARL methods and brings CTC tasks closer to real-world applications, providing a more robust evaluation framework for multi-agent cooperation and decision-making.
% Real-world applications are complex and diverse, and simply combining two atomic subtasks is not enough to fully reflect them.
% With these three easy-to-control and important factors, we can construct a variety of tasks with different difficulty levels.
% This provides a step-by-step route for the advancement of cooperative MARL methods and can also make CTC tasks closer to real-world applications.

\textbf{Information Interference}

\begin{table}[t]
\caption{Different content of information interference. In the column "CONTENT", SUBTASK means the information of other subtask is visible, and AGENT means the information of agents devoted to other subtask is visible.}
\label{tab:sep}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Tasks  & $L_1$ & $L_2$ & Content \\
\midrule
        HeA\_P2G-D1  &7 &7 &\makecell{subtask \\ agent}  \\
        HeA\_P2G-D2  &10 &10 &agent  \\
        HeA\_P2G-D3  &14 &14 &none \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

The composition of CTC tasks inherently involves multiple atomic subtasks, where the information from other subtasks is irrelevant to the completion of the current subtask.
If such information is observed, it can interfere with the agent's ability to effectively complete the task at hand.
To evaluate the impact of this information interference on MARL methods, we introduce three composite tasks with varying interference conditions.
As shown in Fig.~\ref{fig:env}(a), these tasks share the same subtask configuration, but differ in the distances between the subtasks.
By adjusting the distance between the subtasks, we create three distinct interference conditions, which are summarized in Table~\ref{tab:sep}.


% The composition paradigm of CTC tasks determines that they often involve multiple atomic subtasks.
% The information of other atomic subtasks is useless for completing the current atomic subtask.
% If the information of other subtasks is observed, it will interfere with the agent's completion of the current subtask.
% To evaluate the impact of information interference on MARL methods, we introduce three composite tasks with varying interference conditions.
% As illustrated in Fig.~\ref{fig:env}(a), the three tasks share same subtasks setting, but the distances between subtasks are not the same.
% By adjusting the distance between the two subtasks, we obtain interference information for three different situations, as shown in Table~\ref{tab:sep}.


\textbf{Subtasks Dissimilarity}

In practical applications, subtasks are often not homogeneous in nature, and the similarity between subtasks significantly influences the difficulty of completing the overall task.
Tasks composed of similar subtasks tend to be less challenging than those involving dissimilar subtasks.
Furthermore, the proportion of dissimilar subtasks within a task also affects its overall difficulty.
By incorporating these two factors, we ensure that the tasks we design better reflect the complexities encountered in real-world scenarios. 
Figures~\ref{fig:env}(b) and (c) illustrate two complex tasks we developed.
Task Mixed\_2\_Subtask combines a pursuit subtask with a defense subtask, where the proportion of dissimilar subtasks are balanced;
Task Mixed\_3\_Subtask integrates a pursuit subtask with two defense subtasks, where the proportion are not balanced.


% In practical applications, subtasks are not necessarily similar in nature.
% Whether the subtasks are similar also affects the difficulty of completing the task.
% The difficulty of a task composed of similar subtasks is significantly lower than that of a task composed of dissimilar subtasks.
% In addition, the proportion of dissimilar subtasks that make up a task will also affect the difficulty of completing the task.
% Taking these two points into consideration can make the tasks we design better align with real-world scenarios.
% Fig.~\ref{fig:env}(b) and (c) show two mixed tasks we designed. 
% \begin{enumerate}
%     \item \textbf{Task Mixed\_2\_Subtask} consists of a pursuit subtask and a defense subtask. 
%     \item  \textbf{Task Mixed\_3\_Subtask} combines a pursuit subtask with two defense subtasks. The imbalance in the number of subtasks increases the difficulty, while also making the task more representative of real-world situations.
% \end{enumerate}


\textbf{Subtasks Quantity}

The number of subtasks within a task is directly influence its complexity and difficulty.
In general, tasks involving a greater number of subtasks are more challenging to solve.
A method capable of managing a wider type and bigger number of subtasks demonstrates greater adaptability across diverse tasks.
Additionally, the ability to handle a larger number of subtasks often indicates that the method can control more agents, thereby showcasing better scalability in terms of agent count.
Given these considerations, designing tasks with varying numbers of subtasks is crucial for evaluating the performance of cooperative MARL methods.
We propose three tasks, each containing 2, 3, and 4 defense subtasks, as shown in Figures~\ref{fig:env}(d), (e), and (f).
To adjust the difficulty and complexity, we vary the number and type of enemies in each subtask.
The imbalance in enemy distribution, combined with the similarity (or dissimilarity) of subtask types, introduces an additional layer of complexity and aligns these tasks more closely with real-world scenarios.
This design not only tests the method’s capacity to handle different numbers of subtasks but also evaluates the capacity to identify the importance of subtasks.

% The number of subtasks is directly correlated with the complexity and difficulty of a task.
% In general, tasks involving more subtasks are more challenging.
% The ability to process a greater number of subtasks also reflects the capacity of an method.
% A method capable of handling more subtasks, especially of different types, demonstrates greater cross-task applicability.
% Furthermore, the ability to manage a larger number of subtasks implies that the method can control a greater number of agents, indicating better scalability with respect to the number of agents.
% Therefore, designing tasks with varying numbers of subtasks is essential for evaluating the performance of cooperative MARL methods.
% We propose three tasks, each containing 2, 3, and 4 defense subtasks, as shown in Fig.~\ref{fig:env}(d), (e), and (f).
% Given the similarity of subtasks in type, we adjust the similarity of subtasks by adjusting the number and type of enemies contained in each subtask.
% The imbalance in enemy distribution and the similarity in subtask types mean that these three tasks also add additional examination of the importance of subtasks.
% This additional feature is also consistent with the actual situation.

\begin{table*}[t]
\caption{The maximum test winning rate of baseline methods on all CTC tasks across 3 seeds.}
\label{tab:result}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccccccc}
\toprule
Tasks&QMIX&EOI&MAVEN&CDS&GoMARL&RODE&ROMA&LDSA&HSD&DCC \\
\midrule
         HeA\_D2G&\textbf{100}&0&0&0&97.12&93.75&62.5&0&37.5&18.75\\
         HeA\_D3G&0&0&0&0&0&0&0&0&0&0 \\
         HeA\_D4G&0&0&0&0&0&0&0&0&0&0 \\
         HeA\_P2G-D1 &0&0&0&0&\textbf{18.75}&0&0&0&0&0 \\
         HeA\_P2G-D2 &0&0&0&0&0&0&0&0&0&0 \\
         HeA\_P2G-D3 &0&0&0&0&\textbf{100}&0&50&0&0&0\\
         HeA\_M2G &0&0&0&0&0&0&0&0&0&0\\
         HeA\_M3G &0&0&0&0&0&0&0&0&0&0\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{The maximum test winning rate of baseline methods on all simplified variants across 3 seeds.}
\label{tab:appendix_result}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccccccc}
\toprule
Tasks&QMIX&EOI&MAVEN&CDS&GoMARL&RODE&ROMA&LDSA&HSD&DCC \\
\midrule
         HeS\_D2G &100&0&0&0&100&6.25&50&0&0&0 \\
         HeS\_D3G &93.75&0&0&0&100&0&4.17&0&0&0 \\
         HeS\_D4G &0&0&0&0&0&0&0&0&0&0 \\
         HoA\_D2G &100&100&93.75&0&100&100&100&100&100&100 \\
         HoA\_D3G &100&100&0&0&100&100&100&81.25&96.875&100 \\
         HoA\_D4G &100&75&0&0&100&100&100&0&50&96.875 \\
         HoS\_D2G &100&100&100&0&100&100&100&100&100&100 \\
         HoS\_D3G &100&100&0&0&100&100&100&0&90.625&100 \\
         HoS\_D4G &100&100&0&0&100&100&100&0&75&93.75 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{SMAC-Based Implementation}
We implement the CTC tasks within the SMAC environment, a widely adopted testbed for cooperative MARL.
Detailed descriptions of each task are provided in Table~\ref{tab:hea}.
To maintain consistency across tasks, both the enemy and agent forces are composed of identical types and numbers of units.
This design ensures that the difficulty of the tasks is not influenced by imbalanced military strength, allowing MARL methods to focus on addressing the unique challenges posed by the CTC tasks.
For the force composition, we select three types of Terran units from StarCraft II: Marine, Marauder, and Medivac.
The Medivac unit, in particular, possesses two distinctive features, which we leverage to facilitate the pursuit subtask and priority within subtask. 
Feature 1: When no other units are present in its group, the Medivac will continue moving toward the target point, even when under attack.
Feature 2: When other units are present in its group, the Medivac will halt to heal its allies upon being attacked;
If all allied units are defeated, the Medivac will resume its movement toward the target point, ignoring the attacker.
Feature 1 is crucial for the pursuit subtask, as it ensures that the Medivac can continue toward its target even under attack.
This requires agents to defeat the Medivac within a specified time frame to prevent it from reaching the target. 
Feature 2, on the other hand, introduces a prioritization mechanism within the task as it elevates the importance of defeating the Medivac.
% We implement the proposed tasks based on SMAC environment, a widely adopted testbed for cooperative MARL.
% The details of each task are provided in Table~\ref{tab:hea}.
% For consistency across tasks, the types and numbers of troops for both the enemy and the agents are identical.
% In SMAC tasks, most methods can perform well under equal military strength conditions.
% This design ensures that the difficulty of the task is not influenced by imbalanced forces, allowing the MARL methods to focus on addressing the unique challenges posed by the CTC tasks.
% For the force composition, we select three types of Terran soldiers from StarCraft II: Marine, Marauder, and Medivac.
% As Medivac possesses two unique features, we use it to facilitate the pursuit subtask.
% \textbf{Feature 1}: When there are no other units in its group, Medivac can continue moving towards the target point even when it is under attack.
% \textbf{Feature 2}: When other units are present in its group, Medivac will stop to heal its allies upon being attacked. If all allied units are defeated, Medivac will resume its movement towards the target point, ignoring the attacker.
% Feature 1 is the basis for achieving the pursuit subtask.
% And it necessitates defeating Medivac within a certain time frame to prevent it from reaching the target point.
% Feature 2 provides priorities for subtasks and within subtasks.
% As it makes the subtask with Medivac a higher priority, and it also makes defeating Medivac within the subtask a higher priority.


\subsection{Simplified Variants}
We validate the solvability of the CTC tasks through simplified variants, providing evidence that the tasks are tractable while still remaining challenging for current methods.
The simplification is achieved by adjusting two key factors: (1) the heterogeneity of the agents and (2) the symmetry of the enemies across subtasks.
For factor (1), we classify tasks based on agent types as either homogeneous or heterogeneous.
A task is considered homogeneous when all agents are of the same type, whereas it is heterogeneous when agents differ in type. 
For factor (2), we classify tasks as symmetrical or asymmetrical based on the number and type of enemies assigned to each subtask.
A task is symmetrical when each subtask has identical numbers and types of enemies, while it is asymmetrical when these characteristics differ across subtasks.
Using this setting, we categorize the CTC tasks as heterogeneous asymmetrical (HeA) tasks, which are the most challenging configurations compared to homogeneous or symmetrical tasks.
Formally, we introduce simplified variants of CTC tasks: heterogeneous symmetrical (HeS) tasks, homogeneous asymmetrical (HoA) tasks, and homogeneous symmetrical (HoS) tasks to demonstrate the solvability of the CTC tasks.
It is important to note that when agents are homogeneous, the pursuit task effectively degenerates into a defense task.
Therefore, we provide homogeneous or symmetrical versions only for the tasks illustrated in Fig.~\ref{fig:env}(d), (e), and (f). 
The details of each task configuration are provided in Table~\ref{tab:hes}, Table~\ref{tab:hoa}, and Table~\ref{tab:hos}.

% We validate the solvability of the CTC tasks through simplified variants, providing evidence that the tasks are tractable while remaining challenging for current methods.
% We simplify the task by two factors: 1) the heterogeneity of the agents and 2) the symmetry of enemies on each subtask.
% For factor 1, We classify tasks according to the agent types as either homogeneous or heterogeneous.
% A task is homogeneous when all agents are of the same type, while it is heterogeneous when the agents differ in type.
% For factor 2, we classify tasks into symmetrical and asymmetrical tasks based on the number and type of enemies on each subtask.
% A task is considered symmetrical when the number and type of enemies in each subtask are identical.
% In contrast, it is asymmetrical when these characteristics differ across subtasks.

% Based on this setting, we can classify the CTC task as heterogeneous asymmetrical (HeA) tasks, which are the most challenging compared to homogeneous or symmetrical tasks.
% In contrast, homogeneous and symmetrical tasks are generally the easiest to handle.
% To provide a step-by-step evaluation pathway and demonstrate the solvability of the tasks, we also introduce additional task configurations: heterogeneous symmetrical (\textbf{HeS}) tasks, homogeneous asymmetrical (\textbf{HoA}) tasks, and homogeneous symmetrical (\textbf{HoS}) tasks.
% It is also worth noting that when the agents are homogeneous, the pursuit task effectively degenerates into a defense task.
% Therefore, we only provide homogeneous or symmetrical versions for the tasks shown in Fig.~\ref{fig:env}(d)(e)(f).
% The details of each task configuration are presented in Table~\ref{tab:hes}, Table~\ref{tab:hoa}, and Table~\ref{tab:hos}.




\section{Experiments}
In this section, we carry out our experiments to evaluate the performance of three types cooperative MARL methods on our proposed tasks.

\subsection{Experimental Setup}
\textbf{Baselines} To demonstrate the performance of the current state-of-the-art methods in our CTC tasks, we select 10 MARL methods from three categories (policy diversity, agent grouping, and hierarchical MARL) as baselines.
The selected methods are as follows: QMIX\footnote{https://github.com/hijkzzz/pymarl2}, EOI\footnote{https://github.com/jiechuanjiang/eoi\_pymarl}, MAVEN\footnote{https://github.com/chaobiubiu/DCC}, CDS\footnote{https://github.com/lich14/CDS}, RODE\footnote{https://github.com/TonghanWang/RODE}, ROMA\footnote{https://github.com/TonghanWang/ROMA}, GoMARL\footnote{https://github.com/zyfsjycc/GoMARL}, QTypeMix\footnote{https://github.com/linkjoker1006/pymarl3}, LDSA\footnotemark[3], HSD\footnotemark[3] and DCC\footnotemark[3].

\textbf{Runners} For the methods whose source code adopt the episode runner, we uniformly set the maximum number of training steps to 2M;
for the method whose source code adopt the parallel runner, we uniformly set the number of parallel runs to 8 and the maximum number of training steps to 10M.

\textbf{Evaluation} The official evaluation metric used is the test winning rate.
First, we conduct 32 test episodes without exploration and report the maximum of the test winning rate across 3 distinct seeds.
In addition, for tasks with a test win rate greater than 0, we plot the mean and variance of the test winning rate.
In order to evaluate the stability of each method on CTC, we design an coefficient to evaluate the stability of methods (denoted as $\mathbf{V}$).
We define stability coefficient as the average of variance of the $M$ recorded test winning rate across 3 distinct seeds.
\begin{equation}
   \mathbf{V} =\frac{1}{M} \sum_{i=1}^{M} var([w_{1}^i,\cdots,w_{N}^i])
   \label{eq:sc}
\end{equation}
The minimum value of the stability coefficient is 0.
A larger value means a greater difference in the test winning rate of multiple runs, that is, a worse stability.

\subsection{Main Results}
% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_D2G1.jpg}
%         \numberedblock (a)
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_D2G2.jpg}
%         \numberedblock (b)
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_P2GD1.jpg}
%         \numberedblock (c)
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_P2GD3.jpg}
%         \numberedblock (d)
% 	\end{minipage}
% \caption{Performance of baselines on HeA tasks.}
% \label{fig:hea}
% \end{center}
% \vskip -0.2in
% \end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_D2G1.jpg}
%         \numberedblock (a)
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_D2G2.jpg}
%         \numberedblock (b)
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_P2GD1.jpg}
%         \numberedblock (c)
% 	\end{minipage}
% \centering
% 	\begin{minipage}[content]{0.23\textwidth}
% 	\centering
% 	\includegraphics[scale=0.25]{./Figs/HeA_P2GD3.jpg}
%         \numberedblock (d)
% 	\end{minipage}
\begin{tabular}{@{\extracolsep{\fill}}c@{}c@{}c@{}c@{\extracolsep{\fill}}}
            \includegraphics[scale=0.25]{./Figs/HeA_D2G1.jpg} &
            \includegraphics[scale=0.25]{./Figs/HeA_D2G2.jpg} &
            \includegraphics[scale=0.25]{./Figs/HeA_P2GD1.jpg} &
            \includegraphics[scale=0.25]{./Figs/HeA_P2GD3.jpg}\\
            (a) & (b) & (c) & (d)\\
\end{tabular}
\caption{Performance of baselines on HeA tasks.}
\label{fig:hea2}
\end{center}
\vskip -0.2in
\end{figure*}



We present the maximum test winning rates of baseline methods on all CTC tasks across three seeds in Table~\ref{tab:result}.
Given the prevalence of zero values, we focus on displaying only the non-zero results in Fig.~\ref{fig:hea2}. 

First, we examine the three tasks related to information interference.
For the HeA\_P2G-D2 task, the maximum test winning rate of all baseline methods is zero.
In the HeA\_P2G-D1 task, only one method achieves a non-zero maximum test winning rate, and in the HeA\_P2G-D3 task, two methods show non-zero maximum test winning rates.
Notably, GoMARL achieves a maximum test winning rate of 100\% on the HeA\_P2G-D3 task, but this drops to 18.75\% on the HeA\_P2G-D1 task and to zero on the HeA\_P2G-D2 task.
As illustrated in Fig.~\ref{fig:hea2}(c), the mean test winning rate of GoMARL remains non-zero only in the early stages of training, but it becomes zero for the remainder of the training.
In contrast, Fig.~\ref{fig:hea2}(d) shows that the mean test winning rate of GoMARL steadily increases throughout the training process and eventually converges.
This indicates that GoMARL performs well in the absence of information interference and is capable of utilizing information from other subtasks when available.
For ROMA, the maximum test winning rate on the HeA\_P2G-D3 task reaches 50\%, but drops to zero on both the HeA\_P2G-D1 and HeA\_P2G-D2 tasks.
From Fig.~\ref{fig:hea2}(d), we observe that the mean test winning rate of ROMA increases steadily throughout training, but it fails to converge by the end of the training process.
ROMA's slower convergence and lower final performance compared to GoMARL highlight its reduced robustness to information interference.
In summary, the HeA\_P2G tasks present a significant challenge for all baseline methods, with information interference exacerbating the task difficulty.

Next, we turn to tasks involving subtask dissimilarity, specifically HeA\_M2G and HeA\_M3G.
In these tasks, the maximum test winning rate for all baselines is zero.
The difficulty of these tasks, which involve dissimilar subtasks, exceeds the capabilities of the listed baseline methods.

Finally, we analyze the performance of each baseline on tasks related to subtasks quantity.
In the HeA\_D2G task, the maximum test winning rate of six baseline methods is non-zero.
QMIX, RODE, and GoMARL achieve high maximum test winning rates, with QMIX even reaching a 100\% win rate.
In contrast, ROMA, HSA, and DCC show relatively low maximum test winning rates, with DCC's performance reaching a minimum of 18.75\%.
As shown in Fig.~\ref{fig:hea2}(a) and (b), QMIX, GoMARL, and RODE maintain a continuous upward trend in performance throughout training and converge by the end of the process.
In contrast, ROMA, HSA, and DCC exhibit increasing performance but fail to converge.
It is worth noting that QMIX and GoMARL perform well on the HeA\_D2G task, while ROMA, RODE, HSA, and DCC struggle either with convergence speed or final performance.
On the more complex tasks, HeA\_D3G and HeA\_D4G, the maximum test winning rate of all baselines is zero, indicating that as the number of subtasks increases, the task difficulty surpasses the capabilities of the baseline methods.

In conclusion, all baseline methods perform poorly across the eight CTC tasks, highlighting the need for improvements in the ability of existing algorithms to solve composite tasks effectively.


% We show the maximum test winning rate of baseline methods on all HeA tasks across 3 seeds in Table~\ref{tab:result}.
% There are a lot of zeros in it.
% So we only draw non-zero results shown in Fig.~\ref{fig:hea}.

% We first observe that in the three tasks related to information interference: on the HeA\_P2G-D2 task, the maximum test winning rate of all baseline methods is zero; on the HeA\_P2G-D1 task, only one method has a non-zero maximum test winning rate; on the HeA\_P2G-D3 task, two methods have non-zero maximum test winning rate.
% Specifically, the maximum test winning rate of GoMARL on the HeA\_P2G-D3 task is 100\%, and degrade to 18.75\% on the HeA\_P2G-D1 task and to 0 on the HeA\_P2G-D1 task.
% As can be seen from Fig.~\ref{fig:hea}(c), the mean test winning rate of GoMARL is not 0 only in the early stage of training, and is 0 throughout the later stage.
% And as can be seen from Fig.~\ref{fig:hea}(d), the mean test winning rate of GoMARL keeps increasing throughout the training process and eventually converges.
% This phenomenon shows that GoMARL can work well without information interference and has the potential to utilize information from other subtasks.
% For ROMA, its maximum test winning rate on the HeA\_P2G-D3 task reaches 50\%, and degrade to zero on the HeA\_P2G-D1 task and HeA\_P2G-D2 task.
% As can be seen from Fig.~\ref{fig:hea}(d), the mean test winning rate of ROMA keeps increasing throughout the training process and it does not converge at the end of training.
% ROMA’s convergence speed and final performance on this task are far inferior to GoMARL.
% This shows that ROMA is less resistant to information interference.
% In summary, HeA\_P2G is a difficult challenge for all baselines, and the influence of information interference makes the task even more difficult.

% Then, it is easy to find that in the subtask similarity related tasks HeA\_M2G and HeA\_M3G, the maximum test winning rate of all baseline is 0.
% The difficulty of these tasks contained dissimilar subtasks exceeds the capabilities of all the listed baseline methods.

% Finally, we observe the performance of each baseline on three tasks related to the number of subtasks.
% In the task HeA\_D2G, the maximum test winning rate of six baseline methods is not 0.
% QMIX, RODE and GoMARL achieved high maximum test winning rates, and even QMIX reach 100\% win rate.
% ROMA, HSA, and DCC achieve relatively low maximum test winning rates, the lowest of which was 18.75\% by DCC.
% Furthermore, as can be seen from Fig.~\ref{fig:hea}(a)(b), we can see that the performance of QMIX, GoMARL and RODE maintains a continuous upward trend during the training process and converges at the end of the training.
% The performance of ROMA,HSD and DCC keeps increasing during the training process, but eventually fails to converge.
% It is worth noting that QMIX and GoMARL work well on this task, but ROMA, RODE, HSD, and DCC struggle with either convergence speed or final performance.
% In addition, it is easy to find that in the task HeA\_D3G, HeA\_D4G, the maximum test winning rate of all baseline is 0.
% When the number of subtasks increases, the task difficulty exceeds the capabilities of all baseline.

% In summary, all baseline methods performed poorly on all 8 CTC tasks.
% This makes it necessary to improve the ability of each baseline to solve composite tasks.


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HeS_D2G1.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HeS_D2G2.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HeS_D3G1.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoA_D2G1.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoA_D2G2.jpg}
	\end{minipage}
\\
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoA_D3G1.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoA_D3G2.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoA_D4G1.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoA_D4G2.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoS_D2G1.jpg}
	\end{minipage}
\\
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoS_D2G2.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoS_D3G1.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoS_D3G2.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoS_D4G1.jpg}
	\end{minipage}
\centering
	\begin{minipage}[content]{0.18\textwidth}
	\centering
	\includegraphics[scale=0.2]{./Figs/HoS_D4G2.jpg}
	\end{minipage}
\caption{Performance of baselines on HeS, HoA and HoS tasks.}
\label{fig:other}
\end{center}
\vskip -0.2in
\end{figure*}




\begin{table*}[t]
\caption{The stability coefficient of baseline methods on all tasks across 3 seeds.}
\label{tab:sta}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccccccc}
\toprule
Tasks&QMIX&EOI&MAVEN&CDS&GoMARL&RODE&ROMA&LDSA&HSD&DCC \\
\midrule
         HeA\_D2G &0.063&/&/&/&0.082&\textbf{0.162}&0.043&/&0.026&0.009\\
         HeA\_P2G-D1  &/&/&/&/&0.008&/&/&/&/&/\\
         HeA\_P2G-D3  &/&/&/&/&\textbf{0.376}&/&0.030&/&/&/\\
         HeS\_D2G  &\textbf{0.140}&/&/&/&0.099&6e-4&0.044&/&/&/\\
         HeS\_D3G  &\textbf{0.200}&/&/&/&\textbf{0.224}&/&8e-5&/&/&/\\     
         HoA\_D2G  &0.022&0.053&0.073&/&0.019&0.044&\textbf{0.430}&0.067&0.054&0.057\\
         HoA\_D3G  &0.028&0.072&/&/&0.031&0.060&0.057&0.080&0.056&0.056\\
         HoA\_D4G  &\textbf{0.124}&0.081&/&/&0.099&0.294&0.106&/&0.023&\textbf{0.193}\\
         HoS\_D2G  &0.024&0.057&0.050&/&0.025&0.044&0.034&0.060&0.048&0.060\\
         HoS\_D3G  &0.029&0.056&/&/&0.030&0.090&0.048&/&0.061&0.052\\
         HoS\_D4G  &0.034&0.059&/&/&0.036&/0.047&0.056&/&0.054&0.089\\   
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Solvability}
The maximum test winning rates of baseline methods on the HeS, HoA, and HoS tasks across three seeds are presented in Table~\ref{tab:appendix_result}.
On the six tasks with lower difficulty (HoS and HoA), most baselines performed well, with QMIX, GoMARL, RODE, and ROMA achieving a 100\% winning rate on all six tasks.
However, for the three HeS tasks, which are more difficult than HoA and HoS, the number of subtasks plays a significant role in performance.
As the number of subtasks increases, the number of baselines that can successfully solve the task decreases.
Specifically, on the HeS\_D4G task, all baseline methods reached a maximum test winning rate of zero.
The test winning rate curves for the baselines on these eight tasks are shown in Fig.~\ref{fig:other}.
For the HoS and HoA tasks, QMIX, GoMARL, and ROMA exhibit similar convergence patterns, with these methods quickly converging to a near 100\% win rate in the early stages of training.
QMIX and GoMARL maintain consistent performance on the HeS\_D2G task, matching their performance on the HoA and HoS tasks.
However, on the more challenging HeS\_D3G task, their convergence speed and final performance are significantly reduced.
ROMA also experiences slower convergence, ultimately achieving a lower win rate on the HeS\_D2G task, and only shows a non-zero test win rate for a few test steps on the HeS\_D3G task.
RODE and EOI show performance similar to GoMARL on the HoA and HoS tasks, except for the HeA\_D4G task.
However, their performance drops substantially on the HeS tasks.
DCC performs well on the HoA and HoS tasks, but its performance significantly deteriorates when the number of subtasks increases, particularly on tasks with four subtasks.
HSD performs well on HoA and HoS tasks, but its success is limited to tasks with two subtasks.
LDSA shows a non-zero maximum test win rate on only three tasks, where its performance is relatively high.
MAVEN displays a similar trend, but only performs well on two tasks.
Finally, the maximum test winning rate for CDS is zero on all tasks, as its design is not suitable for composite tasks.
In summary, as task difficulty increases, the performance of all baseline methods declines, which aligns with our expectations and intuition.
This phenomenon further confirms the solvability of the CTC tasks and highlights the need for enhanced methods capable of tackling more complex, composite tasks.

% The maximum test winning rate of baseline on HeS, HoA and HeS tasks across 3 seeds are shown in Table~\ref{tab:appendix_result}.
% On the six tasks of HoS and HoA with lower difficulty, most baselines performed well.
% Among them, QMIX, GoMARL, RODE and ROMA even achieved a 100\% win rate on all six tasks.
% For the three HeS tasks, which are relatively more difficult than HoA and HoS, as the number of subtasks increases, the baseline that can work decreases.
% On the HeS\_D4G task, even all baselines have a maximum test winning rate of 0.
% We draw the test winning rate curve of the baseline on these eight tasks in Fig.~\ref{fig:other}.
% On the HoS and HoA tasks, QMIX, GoMARL, and ROMA show similar convergence speed and final convergence performance, that is, they quickly converge to a test win rate close to 100\% in the early stages of training.
% QMIX and GoMARL also maintain the same performance on the HeS\_D2G task as on the HoA and HoS tasks. 
% However, on the HeS\_D3G task, their convergence speed and final convergence performance are greatly reduced.
% ROMA converges to a lower win rate on the D2G task, and has a non-zero test win rate only at a few test steps on the D3G task.
% RODE and EOI perform close to GoMARL on the HoA and HoS tasks, except for the HoA\_D4G task.
% But on the HeS tasks, they perform poorly.
% DCC performs well on HoA and HoS tasks, but its performance drops significantly when there are 4 subtasks.
% HSD works on HoA and HoS tasks, but performs well only on tasks with 2 subtasks.
% LDSA has a maximum test win rate greater than 0 only on 3 tasks, and its value is high. 
% MAVEN also shows a similar situation, but only on 2 tasks.
% The maximum test success rate of CDS on all tasks is 0 because its design is not suitable for combination tasks.
% In summary, as the difficulty of the task increases, the performance of each baseline decreases, which is consistent with intuition and our expectations.
% This phenomenon also proves the solvability of the CTC task.




\subsection{Stability}
The stability coefficients of various baselines across each task are presented in Table~\ref{tab:sta}, calculated using Eq.~\ref{eq:sc}.
The largest stability coefficients are highlighted in bold.
From Fig.~\ref{fig:other}, we observe the corresponding test winning rate curves.
Notably, the difference between the upper and lower bounds of these curves is substantial, indicating significant variation in performance across different training seeds and revealing poor stability in the methods.
Additionally, we observed that higher stability coefficients were more commonly associated with more difficult tasks than with less difficult ones.
Notably, there were no instances of large stability coefficients in the HoS tasks. 
This suggests that the CTC tasks impose higher stability requirements on the methods, and as task difficulty increases, these stability demands also rise.

% We list the stability coefficients of various baseline on each task in Table~\ref{tab:sta} (calculated by Eq.~\ref{eq:sc}).
% We highlight the largest stability coefficients by bold.
% From Fig.~\ref{fig:other}, we can find the corresponding test winning rate curves, and we can see that the difference between upper and lower bounds of these curves are huge.
% This reveals that the performance of the corresponding method in training with different seeds varies greatly, showing poor stability.
% We also observed that higher stability coefficients were more common on difficult tasks than on less difficult tasks.
% There was no case of a large stability coefficients in the HoS task.
% This shows that the CTC task has requirements for the stability of the method, and as the difficulty of the task increases, the requirements also increase.

\section{Conclusion and Discussion}    
In this study, we propose the CTC tasks, designed to enhance the generality and applicability of MARL methods in real-world scenarios, there is a need for more complex tasks that demand multi-agent cooperation and the application of DOL.
The CTC tasks guarantee that DOL and cooperation are necessary conditions for completing the tasks and incorporate three factors to expand the diversity to cover more realistic situations.
We evaluate 10 cooperative MARL methods on CTC tasks.
The results indicate that most of these baselines struggle to handle the challenges posed by CTC tasks.
We also conduct experiments on simplified variants to validate the solvability of the CTC tasks.
The results show that these simplified tasks are more tractable for the baseline methods, confirming the solvability of the CTC tasks.
Additionally, we demonstrate the poor stability of these baselines through stability coefficients and fluctuations in test winning rates curves.
In summary, the CTC provides a set of solvable, practical tasks that require the DOL and cooperation, with significant potential to advance the development of cooperative MARL methods.

However, the design of CTC tasks has some limitations.
Specifically, CTC tasks involve static DOL requirements and do not involve dynamic DOL conditions, which are generally more complex and have a wide range of real-world applications.
While CTC tasks do not directly capture dynamic DOL, they can serve as a foundation for extending to dynamic DOL scenarios.
Furthermore, the design principles of CTC tasks can offer valuable insights for constructing tasks that incorporate dynamic DOL requirements.

% %总结
% In this study, we create realistic and challenging testbeds CTC, which simulate the complexities of multi-agent cooperation in real-world applications.
% we guide CTC tasks design from three key perspectives to ensure that the tasks reflect a broad range of real-world scenarios.
% We conducted experiments on the CTC using 10 basline methods from three categories of MARL methods.
% The results reveal that most of these baslines struggle to handle the CTC tasks.
% At the same time, we demonstrate the low stability of baselines on same tasks by stability coefficient and test Winning rate fluctuations.
% Furthermore, we validate the solvability of CTC tasks on diffculity reduced tasks.
% Experimental results show that the baselines are able to handle these simplified tasks.
% In summary, the CTC presents solvable, practical tasks with a clear requirement for DOL, and it has significant potential to advance the development of cooperative MARL methods.

% The design of CTC tasks also has some limits.
% CTC tasks only involves static DOL requirements, and does not include dynamic DOL requirements.
% Dynamic DOL requirements are more difficult than static DOL, and there are also a large number of application scenarios in practice.
% However, CTC tasks can be used as a prerequisite for dynamic DOL requirements.
% In addition, CTC tasks can provide ideas for task design for dynamic DOL requirements.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


\newpage
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Appendix}
\subsection{Related Work}
In this section, we first demonstrate that the concept of division of labor (DOL) has been widely integrated into cooperative MARL methods.
We then discuss the limitations of the testbeds commonly used in current cooperative MARL research.

\textbf{Cooperative MARL Methods}
Specifically, we categorize the existing works that incorporate DOL into three main approaches: policy diversity, agent grouping, and hierarchical MARL.

Policy diversity refers to the formation of a cooperative paradigm in which multiple agents, sharing a common goal, adopt diverse policies.
By enabling agents to utilize distinct policies, policy diversity implicitly introduces the concept of DOL, where each agent specializes in a specific aspect of the overall task.
For instance, EOI~\cite{jiang2021emergence} suggests that emerging personalities and cooperative behaviors can naturally induce agents to adopt distinct roles and behaviors.
In contrast, CDS~\cite{li2021celebrating} incorporates diversity into shared policy networks and employs regularization based on information theory to maximize the mutual information between an agent’s identity and its trajectory.
This approach promotes policy diversity while preserving the agents' unique roles within the cooperative framework.
DERE~\cite{jiang2022diverse} explores the diverse relationships between agents, which can facilitate cooperative policy learning.
By introducing prior knowledge to represent these relationships, the method encourages the emergence of specialization, aligning with the concept of multi-agent DOL.
Finally, SPD~\cite{jiang2022spd} proposes an unsupervised MARL method that learns diverse coordination policies for agents without the need for extrinsic rewards.
It characterizes agent cooperation using a relational graph, where the varying roles and interactions among agents are reflected, effectively manifesting DOL between agents.
In summary, these approaches demonstrate how policy diversity can foster specialization and DOL in cooperative MARL, enabling more efficient and coordinated agent behavior.

Grouping agents involves partitioning them into distinct subgroups based on certain similarities, and this structure can be viewed as a form of DOL, where each subgroup specializes in a specific function or task within the broader system.
Several cooperative MARL methods adopt this paradigm, employing different concepts to group agents effectively.
For example, SEPS~\cite{christianos2021scaling} groups agents based on their abilities and goals. This is achieved by encoding each agent into an embedding space based on their observed trajectories and then applying an unsupervised clustering method to these embeddings.
GACG~\cite{duan2024group} calculates cooperation needs between agent pairs based on their current observations and captures group dependencies from behavior patterns observed across trajectories. To reinforce group differentiation, it introduces a group distance loss, which increases behavioral differences between groups while minimizing them within groups.
QTypeMix~\cite{fu2024qtypemix} utilizes prior knowledge of agent types to group agents.
Similarly, THGC~\cite{jiang2021multi} groups agents based on similarities in factors such as location, functionality, and health, while maintaining cognitive consistency within groups through knowledge sharing.
VAST~\cite{phan2021vast} explores value factorization for sub-teams based on the similarity of spatial information or trajectories.
ROMA~\cite{wang2020roma} implicitly introduces the concept of roles within MARL, facilitating the sharing of learning among agents with similar responsibilities. By ensuring that agents with similar roles share both similar policies and responsibilities, it enables effective coordination.
GoMARL~\cite{zang2024automatic} further enhances agent cooperation by empowering subgroups as bridges to model connections between smaller sets of agents, thereby improving overall learning efficiency. It also introduces an automatic grouping mechanism—selecting and removing agents—to dynamically create groups and group action values.
In summary, these approaches illustrate how grouping agents based on various similarities fosters specialization, enhances coordination, and facilitates the DOL within cooperative MARL systems.

Hierarchical MARL decomposes a complex task into two layers: the top layer assigns subtasks to individual agents, while the bottom layer focuses on the execution of these subtasks by the agents. Each subtask can be viewed as an agent performing a distinct part of the overall task, thereby embodying the concept of DOL.
For example, RODE~\cite{wang2020rode} decomposes a multi-agent collaborative task into a set of subtasks, each with a smaller action-observation space. Each subtask is associated with a specific role, and agents within the same role jointly learn a strategy to solve the subtask through shared learning.
LDSA~\cite{yang2022ldsa} learns distinct vectors to represent multiple subtasks and assigns subtask-specific policies to agents based on these vectors, enabling local coordination among agents within each subtask.
HSD~\cite{yang2019hierarchical} focuses on learning distinguishable skills for agents and employs a bi-level policy structure. While it shares a similar objective of fostering local cooperation, it optimizes this objective using a different approach.
DCC~\cite{li2024coordinating} treats subtask decomposition as a fixed number of classification tasks, allowing the direct learning of a subtask selection network to guide agent behavior.
In summary, hierarchical MARL approaches employ different techniques to decompose complex tasks into subtasks, facilitating specialization and local coordination among agents. This decomposition mirrors the division of labor, ensuring that each agent contributes effectively to the completion of the overall task.


\textbf{Cooperative MARL Testbeds}

SMAC~\cite{samvelyan19smac} is a benchmark suite specifically designed for evaluating Multi-Agent Reinforcement Learning (MARL) methods.
Based on the real-time strategy game StarCraft II, SMAC provides a variety of tasks and scenarios in which researchers can test and refine their multi-agent systems.
In SMAC tasks, each allied unit is controlled by an RL agent, which can observe the distances, relative positions, unit types, and health of all allied and enemy units within its field of view at each time step.
The behavior of enemy units is governed by the built-in rules of the environment.
To address certain limitations in SMAC, SMACv2~\cite{ellis2022smacv2} introduces three key modifications: random team compositions, random starting positions, and more realistic field of view and attack range dynamics.
These changes encourage agents to focus on understanding their observation space more thoroughly and help prevent the learning of successful open-loop strategies—those that rely solely on the time step for decision-making.
Despite these enhancements, SMACv2 and SMAC remain nearly identical in all other aspects.
The primary objective for the allied agents in both SMAC and SMACv2 is to eliminate all enemy units within a specified timeframe, with rewards being awarded only when enemy units are eliminated and victory is achieved.
Several effective policies have been observed in these environments.
For example, a commonly learned strategy is to focus fire~\cite{li2023ace,liu2024interaction,liu2023contrastive,mahajan2019maven,yang2022ldsa,yu2023ghq,hu2023attention,shao2022self,zang2024automatic} on a single enemy, thereby quickly reducing the number of adversaries and minimizing the damage taken by the agents.
Another effective policy involves retreating when an agent's health is low, causing the enemy to switch targets, followed by a counterattack once the agent is no longer a target for any enemy unit.
However, neither of these strategies involves DOL between agents, as they focus on individual actions rather than collaborative, role-specific tasks.


GRF~\cite{kurach2020google} is a highly dynamic and complex simulation environment that lacks clearly defined behavior abstractions, making it an ideal testbed for studying multi-agent decision-making and cooperation.
The environment adheres to standard football (soccer) rules, including corner kicks, fouls, cards, kick-offs, and offside penalties.
Additionally, the physical representation of players is highly realistic, allowing for a diverse range of learning behaviors to be explored, with the option to adjust the difficulty level.
In GRF, the model must control a team of agents to compete against an opposing team, with the objective of scoring more goals than the opponent by the end of the match.
The environment provides 19 possible actions for the agents, including movement, kicking, and other specialized actions such as dribbling, sliding, and sprinting.
GRF also offers several predefined reward signals, such as scoring rewards and a penalty box proximity reward, which encourages attackers to move toward specific locations on the field.
In GRF tasks, agents must coordinate their movements, timing, and positioning to organize offensive strategies and seize fleeting opportunities, as rewards are only given for scoring.
However, in this environment, all agents are homogeneous and capable of performing all tasks.
This means that division of labor is not a necessary condition for task completion.
For instance, in the academy tasks of GRF, as long as the ball is passed to the agent far away from the defender at the start, the agent dribbling the ball can score alone, and the other agents walking around will not affect the result~\cite{fu2024iteratively,li2021celebrating,xu2023group}, where DOL is often not a necessary feature for optimal policies.


\subsection{Details of CTC tasks}
It is worth noting that, for the three tasks for information interference, we augment Medivac with additional units, making this task functionally equivalent to a defense task.
This modification is necessary because the pursuit task is inherently challenging to train, with early training often resulting in quick failures.
These repeated failures lead to the environment instances being restarted frequently, which significantly reduces the overall training efficiency.

\begin{table*}[t]
\caption{Details of each HeA task.}
\label{tab:hea}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Tasks  & Agents & Subtask 1 & Subtask 2 & Subtask 3 & Subtask 4 \\
\midrule
        HeA\_Denfense\_2\_Subtask  & \makecell{2 Marine \\ 2 Marauder \\ 2 Medivac}  & \makecell{1 Marauder \\ 1 Medivac} & \makecell{2 Marine \\ 1 Marauder \\ 1 Medivac} &/ &/ \\
        \hline
        HeA\_Denfense\_3\_Subtask  & \makecell{3 Marine \\ 3 Marauder \\ 3 Medivac}  & \makecell{1 Marauder \\ 1 Medivac} & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &\makecell{2 Marine \\ 1 Marauder \\ 1 Medivac} &/ \\
        \hline
        HeA\_Denfense\_4\_Subtask  & \makecell{4 Marine \\ 4 Marauder \\ 4 Medivac}  & \makecell{ 1 Marine \\ 1 Medivac} & \makecell{ 1 Marauder \\ 1 Medivac} &\makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &\makecell{2 Marine \\ 2 Marauder \\ 1 Medivac} \\
        \hline
        HeA\_Pursuit\_2\_Subtask  & \makecell{2 Marine \\ 2 Marauder \\ 2 Medivac}  & \makecell{1 Marauder \\ 1 Medivac} & \makecell{2 Marine \\ 1 Marauder \\ 1 Medivac} &/ &/ \\
        \hline
        HeA\_Mixed\_2\_Subtask  & \makecell{2 Marine \\ 2 Marauder \\ 2 Medivac}  & \makecell{1 Medivac} & \makecell{2 Marine \\ 2 Marauder \\ 1 Medivac} &/ &/ \\
        \hline
        HeA\_Mixed\_3\_Subtask  & \makecell{3 Marine \\ 3 Marauder \\ 3 Medivac}  & \makecell{1 Medivac} & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &\makecell{2 Marine \\ 2 Marauder \\ 1 Medivac} &/ \\   
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Details of each HeS task.}
\label{tab:hes}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Tasks  & Agents & Subtask 1 & Subtask 2 & Subtask 3 & Subtask 4 \\
\midrule
        HeS\_Denfense\_2\_Subtask & \makecell{2 Marine \\ 2 Marauder \\ 2 Medivac}  & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &/ &/ \\
        \hline
        HeS\_Denfense\_3\_Subtask  & \makecell{3 Marine \\ 3 Marauder \\ 3 Medivac}  & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &\makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &/ \\
        \hline
        HeS\_Denfense\_4\_Subtask  & \makecell{4 Marine \\ 4 Marauder \\ 4 Medivac}  & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} & \makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &\makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} &\makecell{1 Marine \\ 1 Marauder \\ 1 Medivac} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Details of each HoA task.}
\label{tab:hoa}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Tasks  & Agents & Subtask 1 & Subtask 2 & Subtask 3 & Subtask 4 \\
\midrule
        HoA\_Denfense\_2\_Subtask & 6 Marine  & 2 Marine & 4 Marine &/ &/ \\
        \hline
        HoA\_Denfense\_3\_Subtask  & 9 Marine  & 1 Marine & 3 Marine & 5 Marine &/ \\
        \hline
        HoA\_Denfense\_4\_Subtask  & 12 Marine  & 1 Marine & 2 Marine & 4 Marine &5 Marine \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Details of each HoS task.}
\label{tab:hos}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Tasks  & Agents & Subtask 1 & Subtask 2 & Subtask 3 & Subtask 4 \\
\midrule
        HoS\_Denfense\_2\_Subtask & 6 Marine  & 3 Marine & 3 Marine &/ &/ \\
        HoS\_Denfense\_3\_Subtask  & 9 Marine  & 3 Marine & 3 Marine & 3 Marine &/ \\
        HoS\_Denfense\_4\_Subtask  & 12 Marine  & 3 Marine & 3 Marine & 3 Marine &3 Marine \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
