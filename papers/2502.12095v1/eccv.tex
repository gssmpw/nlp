\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version
%\usepackage[review,year=2024,ID=11480]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{eccv}

% OPTIONAL: Un-comment the following line for a version which is easier to read
% on small portrait-orientation screens (e.g., mobile phones, or beside other windows)
%\usepackage[mobile]{eccv}


% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{multirow}
% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% ---------------------------------------------------------------
% Hyperref package

% It is strongly recommended to use hyperref, especially for the review version.
% Please disable hyperref *only* if you encounter grave issues.
% hyperref with option pagebackref eases the reviewers' job, but should be disabled for the final version.
%
% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

% TODO FINAL: Comment out the following line for the camera-ready version
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
%\usepackage{hyperref}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% Support for ORCID icon
\usepackage{orcidlink}
\renewcommand{\thesubfigure}{\roman{subfigure}}

\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.\ }}
\def\ie{\emph{i.e.}} \def\Eg{\emph{I.e.\ }}
\def\iid{\emph{i.i.d.\ }} \def\Iid{\emph{I.i.d.\ }}





\newcommand{\x}[1]{x_{#1}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\norm}[1]{\|#1\|}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\tenc}{g} % \newcommand{\tenc}{E_T}
\newcommand{\ienc}{f} % \newcommand{\tenc}{E_I}
\newcommand{\D}{d}
\newcommand{\ctok}{*}

\def\pseudowords{custom tokens}

\begin{document}

% ---------------------------------------------------------------
% TODO REVIEW: Replace with your title


% TODO REVIEW: If the paper title is too long for the running head, you can set
% an abbreviated paper title here. If not, comment out.
\titlerunning{Abbreviated paper title}

% TODO FINAL: Replace with your author list. 
% Include the authors' OCRID for the camera-ready version, if at all possible.
\title{Descriminative-Generative Custom Tokens\\ for Vision-Language Models}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Pramuditha Perera\\
AWS AI Labs\\
{\tt\small pramudi@amazon.com}
\and
Matthew Trager\\
AWS AI Labs\\
{\tt\small mttrager@amazon.com}
\and
Luca Zancato\\
AWS AI Labs\\
{\tt\small zancato@amazon.it}
\and
Alessandro Achille\\
AWS AI Labs\\
{\tt\small aachille@amazon.com}
\and
Stefano Soatto\\
AWS AI Labs\\
{\tt\small soattos@amazon.com}
}

\author{Pramuditha Perera \and
Matthew Trager \and
Luca Zancato \and
Alessandro Achille \and
Stefano Soatto 
}

% TODO FINAL: Replace with an abbreviated list of authors.
\authorrunning{P.~Perera et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list.
\institute{AWS AI Labs}

% TODO FINAL: Replace with an abbreviated list of authors.
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list.


\maketitle


\begin{abstract}
  This paper explores the possibility of learning \emph{\pseudowords} for representing new concepts in Vision-Language Models (VLMs). Our aim  is to learn tokens that can be effective for both discriminative and generative tasks while composing well with words to form new input queries. The targeted concept is specified in terms of a small set of images and a parent concept described using text. We operate on CLIP text features and propose to use a combination of a textual inversion loss and  a classification loss to ensure that text features of the learned token are aligned with image features of the concept in the CLIP embedding space. We restrict the learned token to a low-dimensional subspace spanned by tokens for attributes that are appropriate for the given super-class. These modifications improve the quality of compositions of the learned token with natural language for generating new scenes. Further, we show that learned \pseudowords{} can be used to form queries for text-to-image retrieval task, and also have the important benefit that composite queries can be \emph{visualized} to ensure that the desired concept is faithfully encoded. Based on this, we introduce the method of Generation Aided Image Retrieval, where the query is modified at inference time to better suit the search intent. On the DeepFashion2 dataset, our method improves Mean Reciprocal Retrieval (MRR) over relevant baselines by 7\%.
  \keywords{Vision-language Models \and Textual-inversion \and Text-to-image retrieval}
\end{abstract}

\section{Introduction}


Humans use language to represent and communicate concepts. They have the ability to easily associate new meanings to words or phrases, for example to describe refinements of known concepts---\emph{the neighbor's dog} is a particular \emph{dog} that one can quickly become familiar with. A phrase with a new meaning can immediately be used together with other words to compose new concepts---\emph{the neighbor's dog, covered in mud, running in the living room} has a clear meaning, even if this scene has never happened. Finally, language is flexible since a phrase can be used to imagine or visualize an associated concept (``generation''), and also to determine whether a particular scene belongs to that concept (``discrimination''). %All of these aspects of language are essential for humans to interact with and reason about the world.

\begin{figure}[!h]
\centering
	\resizebox{0.7\linewidth}{!}{
\includegraphics[width=1\linewidth]{images/splash2.pdf}}
    \caption{ Learning custom tokens to represent a given set of images. Prompt tuning and textual inversion learn representations that are optimized for image recognition and generation, respectively. In this paper we examine the possibility of learning {\pseudowords} that can be used for both generation and recognition and also compose with natural language. \vspace{-.3cm}}% 
    \label{fig:overview}
\end{figure}

In this work, we focus on the task of learning new tokens to represent custom concepts, for pre-trained VLM~\cite{radfordLearningTransferableVisual2021}.  We assume that each concept is defined in terms of its parent concept and a small number of example images. We refer to these tokens as \emph{\pseudowords} for the remainder of the paper. Learned \emph{\pseudowords}  should behave similarly to words in all of the aspects discussed above: i) exhibit correspondence with images of the same concept in a joint text-image feature space, ii) generate visual samples from the distribution of the underlying concept, and iii) compose well with natural language as shown in Figure~\ref{fig:overview}. 




Large-scale Vision Language Models (VLMs) can express complex associations between language and images, but they currently lack this level of flexibility. In particular, while many recent works propose methods for learning concepts from small sets of images~\cite{zhou2022coop,saitoPic2WordMappingPictures2023,cohenThisMyUnicorn2022a,textinversion,ruizDreamBoothFineTuning2022}, each one of these approaches has only focused on a particular aspect of the broader problem. Prompt tuning methods learn a token for discriminative tasks, such as classification~\cite{zhou2022coop} or retrieval~\cite{saitoPic2WordMappingPictures2023,cohenThisMyUnicorn2022a}; on the other hand, text inversion methods learn tokens specialized for image generation~\cite{textinversion,ruizDreamBoothFineTuning2022}. While most of these works also aim to learn tokens that compose well with natural language, the more general question of whether analysis and synthesis tasks can be balanced together simultaneously has not been addressed.

In this work, we aim to train {custom tokens} that compose with natural language and yield prompts that are effective for both image generation and for retrieval/discriminative tasks. Since any prompt constructed compositionally for retrieval is also applicable to image generation, our approach opens the possibility of \emph{visualizing} queries to better understand the retrieval process. In particular, if the visualization of the query does not represent the search intent, a user could apply prompt engineering to change the query until the visualization is consistent with their intent. In this paper, we also introduce an algorithm for \emph{Generation Augmented Image Retrieval (GAIR)}, which can be used to automate this process and improve retrieval results based on a CLIP similarity measure.

In order to learn {\pseudowords} we consider an architecture that can handle discriminative and generative tasks jointly. In particular, we leverage a generative model (Stable Diffusion \cite{stable_diffusion_Rombach_21}) conditioned on CLIP text features \cite{radfordLearningTransferableVisual2021}  and learn {\pseudowords} with a new loss function which combines both discriminative and generative objectives. We show that restricting {\pseudowords} to lie in a lower dimensional subspace spanned by relevant English tokens improves the compositionality of the learned token, \ie, the token can be composed with English language to represent new scenes. Similarly, we show that composing new search queries with the learned token improves text-to-image retrieval performance. Since the composed search query can be visualized, it provides a qualitative measure for the user to determine whether the search query is consistent with the intended search. In summary, our main contributions are as follows:

\begin{itemize} 
    %\item We analyze the limitations of learning custom tokens with Stable Diffusion and propose regularization mechanisms to promote compositionality and discriminativeness. 
    \item We explore a unified framework for learning custom tokens for both generative and discriminative tasks while preserving compositionality with natural language.
    \item We demonstrate that learned custom tokens can generate images of the target concept and produce classifiers that generalize better compared to prompt tuning.
    \item We introduce a Generation Augmented Image Retrieval algorithm that can be used to automatically modify a test query to improve text-to-image retrieval performance.
\end{itemize}
%We use textual inversion \cite{textinversion} as the foundation of our method. However, instead of using a BERT text encoder as in~\cite{textinversion}, we use a CLIP text encoder as it allows us to use the learned token for classification tasks by considering the joint image-text embedding space. We introduce a classification loss in addition to the stable diffusion loss to align the embedding of the pseudo-word with the image embedding in the CLIP space. Further, we empirically show that restricting the {pseudo-words} to lie in a lower dimensional subspace of text attributes relevant to the parent-concept improves compositionalty of the learned pseudo-word. We also observe out a general trade-off between image fidelity and the compositionality of the learned pseudo-word(s). We examine how different choices of hyperparameters allow us to operate in different regions in this spectrum.  











\begin{figure}[!t] \centering
\includegraphics[width=.8\linewidth]{images/network_pdf.pdf}
    \caption{ Overview of the proposed method for learning a custom token for the teapot class. We learn custom tokens that can be used to generate images of the targeted concept and produce discrimination between other concepts. 
     \vspace{-.3cm}}% 
    \label{fig:method}
\end{figure}


\section{Related Work}


Diffusion Probabilistic Models (DPMs)~\cite{ddmps-sohl-dickstein15,ddpm} have recently been shown to be remarkably effective for image generation. In particular, models such as DALLE-2~\cite{rameshHierarchicalTextConditionalImage2022}, Imagen~\cite{sahariaPhotorealisticTexttoImageDiffusion2022}, Stable diffusion~\cite{stable_diffusion_Rombach_21} are able to generate high-quality images conditioned on arbitrary textual prompts (text-to-image synthesis). A key component for this success is the availability of vision-language models such as CLIP~\cite{radfordLearningTransferableVisual2021}, that were trained on massive amounts of data and provide strong text-image priors. However, generating peculiar objects that were not observed during training requires \emph{inversion}, i.e., associating example images to the conditioning variables that are able to generate those same images. This problem has been studied extensively in the GAN literature~\cite{creswellInvertingGeneratorGenerative2016,liptonPreciseRecoveryLatent2017,abdalImage2StyleGANHowEmbed2019}. Using diffusion models, \emph{textual inversion}~\cite{textinversion} has been proposed as an approach for inverting 3-5 images into a token (or ``pseudo-word'') for the frozen language model. Although the learned tokens can be used in generic sentences to generate images in new contexts and styles, the compositional abilities are still limited (see Section~\ref{sec:results}). \emph{Dreambooth}~\cite{ruizDreamBoothFineTuning2022} is an alternative inversion method for diffusion models, however it requires fine-tuning the parameters of the diffusion model for each new concept. We also mention \emph{Null-Text Inversion}~\cite{mokadyNulltextInversionEditing2022} that modifies the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This approach, however, does not easily allow to generate compositions of learned concepts. Finally, several recent methods use inversion-style techniques for flexible image editing~\cite{mokadyNulltextInversionEditing2022, dongPromptTuningInversion2023}.

Prompt tuning is a parameter-efficient fine-tuning method originally proposed in NLP literature~\cite{liuPretrainPromptPredict2021} that optimizes a restricted set of token embeddings in a transformer-based model. The adoption of transformers in Computer Vision has allowed for similar techniques to be used for vision tasks as well \cite{jiaVisualPromptTuning2022, zhou2022coop, wang2022learning}. In particular, in the vision domain, prompt tuning can be performed by optimizing image tokens (VPT~\cite{jiaVisualPromptTuning2022}) or, in the case of vision-language models, text tokens (CoOP~\cite{zhou2022coop}). Learning language tokens opens the door to do compositional manipulations, for example compositions of multiple learned tokens~\cite{nayakLearningComposeSoft2022} or compositions with natural language~\cite{saitoPic2WordMappingPictures2023, cohenThisMyUnicorn2022a}. These methods, however, train tokens to perform classification or retrieval tasks. To our knowledge, the problem of learning tokens that are effective for both generation and classification has not been investigated.


% \vspace{.5cm}

% \noindent \textbf{{Generative Diffusion Models.}} Diffusion models have emerged as a powerful approach for image generation, offering a flexible and tractable framework for modeling complex data distributions. In this section, we review some of the most relevant works on diffusion models for image generation. Generative diffusion models (GDMs) are a class of diffusion models that use an iterative procedure to generate high-quality images. GDMs start with a noise image and iteratively apply a diffusion process to generate samples from the data distribution. The first GDM model was introduced by \cite{ho2020denoising}, which used a denoising diffusion probabilistic model (DDPM) to generate images. Later, \cite{grathwohl2021your} proposed the YourGAN model, which uses a GAN-style objective function to learn the generator. \cite{song2020diffeq} proposed a novel approach called Diffusion Equilibrium Generative Models (DEGMs), which use a diffusion process to reach a stationary distribution. \cite{xie2021guided} introduced Guided Diffusion (GD), a diffusion-based model that can synthesize high-quality images from unconditioned or partially conditioned inputs.


% \noindent \textbf{Prompt tuning.} Prompt tuning is a recent approach to improving the performance of language models on specific tasks, by fine-tuning the model on a set of prompts that are tailored to the task. This technique has gained significant attention in the natural language processing community, and here we review some of the most relevant works on prompt tuning.
% The GPT-3 language model is known for its remarkable performance on a wide range of natural language processing tasks. One of the key factors behind this success is the use of prompt engineering, where the model is fine-tuned on a set of prompts that are carefully designed to elicit the desired behavior. For example, \cite{brown2020language} showed that prompt engineering can significantly improve the performance of GPT-3 on tasks such as question answering and summarization. Learning to prompt is a technique for automatically generating prompts that are tailored to the task, using a combination of machine learning and natural language processing. One approach to learning to prompt was proposed by \cite{petroni2021how}, who introduced a method for generating prompts using a neural network that is trained to predict the output of the language model on a given task. Another approach was proposed by \cite{radford2021learning}, who used a combination of reinforcement learning and human feedback to learn effective prompts for a question answering task.
  
% \noindent \textbf{Textual inversion.} Prompt textual inversion is a relatively new research area that aims to generate natural language inputs that would result in a given output when processed by a pre-trained language model. Dreambooth is a recently proposed framework that uses prompt textual inversion to generate diverse and controllable dream-like text. Dreambooth leverages the power of pre-trained language models to generate text that corresponds to a given set of dream attributes, such as vividness, bizarreness, and coherence. The main idea behind Dreambooth is to invert the representations learned by the language model to obtain a prompt that would generate text with the desired dream attributes.

\section{Preliminaries}
\noindent \textbf{Prompt Tuning for Classification}. Prompt tuning \cite{lester-etal-2021-power, wang2022learning} allows the output of a Transformer-based model to be influenced by a learnable token embedding. \cite{zhou2022coop} adopted prompt tuning for image recognition tasks by forming input prompts of the form $\tau_i = [* , CLASS_i ]$, where `$\ast$' is a nominal token corresponding to the learnable token embedding $e_*$. However, this style of prompting can only be used with known-concepts, since concepts are specified in terms of natural-language. \cite{nayakLearningComposeSoft2022} introduced an alternative form of prompting that can be used to learn tokens for new concepts. In their version, the text prompt is formed as $\tau_i = [t, *_i]$ for each concept $i$, where $t$ was a nominal text of the form `image of a'.  During training, embedding vectors $e_{*_i}$ are optimized with a CLIP-style contrastive loss by considering the similarity between the text feature $\tenc(\tau_i)$ and the corresponding image feature,  where $\tenc$ is the text encoder. 


\noindent \textbf{Diffusion Based Generation}.
Latent Diffusion model operates on principles of Denoising Diffusion Probabilistic Models (DDPMs)~\cite{ddpm} where images are first projected to a latent space through an autoencoder. The autoencoder consists of an encoder $\epsilon$ and decoder $\delta$. A diffusion model is trained to produce learned latent codes and can be conditioned on an external variable $\tau$. During the reverse diffusion process, at time step $t$, a noise vector $\eta$ is  sampled from the standard normal distribution. Parameters $\theta$ of the denoising network $\nu_\theta$ are optimized by minimizing the loss $l_{\text{DM}} = E_{\eta \sim N(0,1), z \sim \epsilon(x) ,t} \big[|| \eta - \nu_\theta (z, t, \tau) ||_2^2\big].$ With this approach, the network learns to remove the noise added to the image during the forward diffusion process. Then, at inference time, a sample from the trained model is produced drawing a noise vector from $N(0,1)$ which is sequentially denoised for $T$ steps using the denoising network $ \nu_\theta $ conditioned with the input $\tau$ to produce a latent vector $z_0$. The output image $\hat{x}$ is produced  by passing the latent vector $z_0$ through the decoder network $\delta$. We conceptualize the entire image generation pipeline by the process $d(\tau)$, where    $\hat{x} = d(\tau) = \delta(z_0)$. The latent diffusion was initially proposed for text-to-image generation where the generator was conditioned on BERT features. More recently, Stable Diffusion has been introduced where the text conditioning uses a CLIP text encoder \cite{stable_diffusion_Rombach_21}.

\noindent \textbf{Textual Inversion}.
% Textual inversion was originally built on a Latent Diffusion backbone~\cite{stable_diffusion_Rombach_21} that used a BERT text encoder~\cite{devlin-etal-2019-bert} to condition image generation. 
Textual inversion \cite{textinversion} is a method based on diffusion models that learns a new token with the objective of generating images of a novel concept. Textual inversion originally used latent diffusion in their architecture and thus conditioned the generation process using a BERT text encoding.  The model is trained by feeding training images $\mathbf{x}$ to the network while using $\tau = g([t, *])$ as conditioning, where $\ast$ is a new token added to the vocabulary. To avoid overfitting, a set of different paraphrasing of $\tau = \text{`image of a *'}$ is used. During training, the parameters of the diffusion model $\theta$ and text encoder $\tenc$ are kept frozen while the diffusion loss $l_{DM}$ is minimized by optimizing the embedding $e_*$, where $e_*$ is the embedding of the token $\ast$.

\section{Proposed Method}

With multi-modal models, it is possible to use natural language to specify concepts for visual tasks (\eg, in classification and generation). Despite the flexibility and convenience of these models, the quality of the results is limited by 1) the ability of the user to express their exact intent in a textual prompt 2) the capability of the model to interpret such prompt correctly. Both of these issues are especially significant in the case of fine-grained image classification or generation and/or in the case of customized concepts, since textual prompts may be unsuited for conveying visual details and the model may have never seen a personalized concept during training (\eg, “my dog”). To address both issues, we propose to define new custom tokens that can be composed with plain text tokens to query VLMs both for image generation and classification. Our learned tokens are multi-modal in that they are defined through the use of positive images (different images of the desired concept) and coarse textual description. 

In particular, we assume we have access to a set of $n$ positive images $ \mathbf{x} = [x_1, x_2, \dots ,x_n]$ (where $n$ is generally 3-5) of the desired concept drawn from an image distribution $ \mathbf{\mathcal{X}}$ and an associated textual description of it given by a super-concept label $c$. 
Given a text encoder $\tenc$, image encoder $\ienc$ and a diffusion model $\D$, our task is to learn a customized word-token (that we denote with $\ctok$) expressed as the token embedding $e_{\ctok}$ that (i) can be used both for image analysis and synthesis tasks and (ii) that is composable with other English tokens in the pre-trained vocabulary of a given VLM. 
In particular, our customized word-tokens are designed so that they can be used for:


\begin{itemize}
    \item \textbf{Generation}: Given a context text $t$, our custom token $\ctok$ can be composed with the textual query and can be passed as input to a text to image generative model: 
    $\D \big(\tenc([t, c, \ctok])\big) \longrightarrow x_i \in \mathbf{\mathcal{X}} $, where $t$ is the text context and we use the square brackets to denote the tokens concatenation. For example, it is common to use different paraphrasing of ``an image of a'' for $t$.
    \item \textbf{Recognition}: Given a context text $t$, negative class labels $y_i$, our custom token $\ctok$ can be used to discriminate the custom concept from negative classes: $\tenc([t, c, \ctok])^T \ienc (x_j) >  \tenc([t, c, y_i])^T \ienc(x_j) \quad \forall i,j$, where we parametrize the scoring function for classification as the product of text/image embedding vectors (following CLIP dual encoder parametrization \cite{radfordLearningTransferableVisual2021}).
    % \item \textbf{Recognition}:  $P_T( E_T(t) \circ e_*) \cdot P_I E_I(x_j) >  P_T(E_T(t) \circ E_T(c_i)) \cdot P_I E_I(x_j) \forall i,j $  where $c$ is the vector containing  all relevant classes, and $P_T$ and $P_I$ are projections that projects text and image features to a common feature space respectively.
\end{itemize}



\begin{figure} 
  %\setlength{\fboxsep}{-\fboxrule}
  %\fbox{%
      \centering
	\resizebox{0.8\linewidth}{!}{

      \begin{tabular}{@{}c|ccc|ccc|ccc@{}}
      \toprule
      &
    \multicolumn{3}{c}{Image of a $\ast$ teapot on a table} &
    \multicolumn{3}{c}{ Image of a $\ast$ teapot on a sink} &
    \multicolumn{3}{c}{ Image of a $\ast$ teapot on a beach} \\
    \hline
    
            TI(BERT) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/ct_ld_8.jpg} \\
        
      TI(CLIP) &
      \includegraphics[width=0.11\linewidth]{images/ti_samples/redtable0.jpg} & \includegraphics[width=0.11\linewidth]{images/ti_samples/redtable1.jpg} & \includegraphics[width=0.11\linewidth]{images/ti_samples/redtable2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/ti_samples/beach0.jpg} & \includegraphics[width=0.11\linewidth]{images/ti_samples/beach1.jpg} & \includegraphics[width=0.11\linewidth]{images/ti_samples/beach2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/ti_samples/broken0.jpg} & \includegraphics[width=0.11\linewidth]{images/ti_samples/broken1.jpg} & \includegraphics[width=0.11\linewidth]{images/ti_samples/broken2.jpg} \\

        Ours(CLIP) &
        \includegraphics[width=0.11\linewidth]{images/t00.jpg} & \includegraphics[width=0.11\linewidth]{images/t01.jpg} & \includegraphics[width=0.11\linewidth]{images/t02.jpg} &
        \includegraphics[width=0.11\linewidth]{images/t03.jpg} & \includegraphics[width=0.11\linewidth]{images/t04.jpg} & \includegraphics[width=0.11\linewidth]{images/t05.jpg} &
        \includegraphics[width=0.11\linewidth]{images/t06.jpg} & \includegraphics[width=0.11\linewidth]{images/t07.jpg} & \includegraphics[width=0.11\linewidth]{images/t08.jpg} \\
        \hline
        
      \end{tabular}} \caption{Textual Inversion conditioned on CLIP does not compose well with natural language. Our \pseudowords{} compose better with other text after subspace projection is performed.} \label{fig:ti_comp}
\end{figure} 

\begin{figure}[ht] 
\centering
\resizebox{.9\linewidth}{!}{ 
    \qquad \hspace*{-0.7cm}
    \subfloat[\centering]{{\includegraphics[width=.24\linewidth ]{images/sd_mtx_4.png} } \label{fig:np_mtx}} 
    \subfloat[\centering]{{\includegraphics[width=.24\linewidth ]{images/ce_mtx_4.png}} \label{fig:ce_mtx}}
    \subfloat[\centering]{{\includegraphics[width=.24\linewidth ]{images/pw_mtx_2.png} }\label{fig:pw_mtx}}%
    \subfloat[\centering]{{\includegraphics[width=.20\linewidth ]{images/diff_plot_3.png} }\label{fig:normplot}}%
     }
    \caption{ Cosine similarity between normalized text embeddings and the learned token embedding (last column) for  (i) textual inversion: TI (ii) textual inversion + cross-entropy loss: TI+CE (iii) subspace projection + cross-entropy loss: P+TI+CE (iv) norm of learned word embedding with different methods. Image intensity clipped at 0.4 for visualization. }%

   
\end{figure}



\begin{table} 
  %\fbox{%
        \centering
\caption{Classification performance on Caltech256-teapot class and generated images when the weights of the loss is varied. \vspace{-0cm}}
	\resizebox{0.7\linewidth}{!}{ 
  %\setlength{\fboxsep}{-\fboxrule}
      \begin{tabular}{@{}lccccc@{}}
        %  \centering
      \toprule
      Weights $(\lambda_{SD}, \lambda_{CE})$ & (0,1) & (1, 0.1) & (1, 0.001) & (1, 0) \\ \midrule
      Accuracy (teapot) & 98.5\% &  99.2\% & 93.3\% & 100\% \\ 
      Accuracy ($\ast$ teapot) & 100\% &  100\% & 100\% & 0.0\% \\[.1cm] 
        \raisebox{.5cm}{\multirow{1}{*}{Generated Images}}
         &          
        \includegraphics[width=0.2\linewidth]{images/ld0.jpeg} & \includegraphics[width=0.2\linewidth]{images/ld1.jpeg} & \includegraphics[width=0.2\linewidth]{images/ld2.jpeg} & \includegraphics[width=0.2\linewidth]{images/ld4.jpeg}  \\
        \bottomrule
      \end{tabular}} 
      \label{fig:weight_ablation}
\end{table} 



%%%%%%


The outline of the proposed solution is illustrated in Figure~\ref{fig:method}. Our method has two main components: a textual inversion network and a classification network. The token embedding $e_*$ is trained using textual inversion network (shaded in blue) such that suitable prompts containing the token generate images of the targeted concept. When CLIP's text encoder is used as $\tenc$, one would expect that its latent feature may be useful for classification. However, its recognition performance tends to be poor as we later show in experimental results (Section~\ref{sec:results}). We therefore use losses of classification network (yellow) jointly with the textual inversion network to learn token embeddings that provides stronger joint classification/generation performance. Furthermore, we show that our joint training helps regularize discriminative tokens learned with prompt tuning and leads to a classifier that generalizes better than the prompt-tuning paragon \cite{zhou2022coop}.
% We show that joint training produces a classifier that generalizes  better than a classifier trained with prompt tuning. 

\subsection{Textual Inversion Network} \label{sec:clip}

We consider the network proposed in \cite{textinversion} as the blueprint to our textual-inversion network. Although this setup works well for learning image-tokens for generation, since text features are encoded with BERT, there is no direct way of integrating the model with the image-to-text classifier as shown in Figure~\ref{fig:method}. To facilitate this operation, we modify the textual inversion model by replacing its backbone with a Stable Diffusion Model \cite{stable_diffusion_Rombach_21} that uses a frozen CLIP ViT-L/14 text encoder for conditioning. By making this choice, we are able to obtain text-to-image scores by comparing CLIP text features with image feature of a given image. In this setup, $\tenc$ and $f$ are CLIP text and image encoders, $d$ is a stable-diffusion model.


With this change, the generated \pseudowords{}  are able to faithfully generate images similar to the training set. However, the learned model has several issues. In Figure~\ref{fig:ti_comp} we visualize images sampled from a  model trained on the concept shown in Figure~\ref{fig:overview} with 5 tokens. Although the model appears to have learned structure and textures of the target concept,  Figure~\ref{fig:ti_comp} suggests that the learned token (TI-CLIP) doesn't compose well with natural language. For example, the image generated for the caption `image of a $\ast$ teapot on a beach' looks very similar to the image generated for `image of a $\ast$ teapot on sink'. On the other hand, we observed that the learned token cannot be used to differentiate images of the target concept from images of its parent-concept (see Section~\ref{sec:align} for more details). 

In order to understand the reason behind these shortcomings, we investigated the embedding of the learned token. First, we considered a set of attributes that could be used to describe the parent concept. For example, black, blue, ceramic \emph{etc.} can be regarded as valid attributes of the parent-concept teapot. We considered average token embeddings for each attribute and compared it with the average learned embedding in terms of cosine similarity. In Figure~\ref{fig:np_mtx} we visualize an extraction of the obtained affinity matrix. According to this figure (final row and column), it seems that the learned embedding is significantly different from other attribute embeddings. In Figure~\ref{fig:normplot} we compare the distribution of norms of each attribute embedding considered  against the average norm of the learned embedding. This figure suggests that the the learned embedding has a norm $> 50 \times$ than the average attribute embedding norm. 

These findings suggest two potential issues with learned embedding: i) the learned embedding has a significantly larger norm compared to other word embeddings. Since word embeddings are not normalized before passing to the encoder, the learned embeddings could negate the contribution from the context. 
% This could be why, learned embedding doesn't compose well with other word embeddings. 
ii) Even when embeddings are normalized, the learned token has low similarity to other adjectives/attributes that are known to compose well with the parent concept.

\subsection{Classification Network} \label{sec:align}

%When embeddings for pseudo-words are learned, it is reasonable to expect that the text feature of the learned token will have higher correlation with image embeddings of images of the same concept. However, experimentally, we find this is not the case. To illustrate this point, we carry out a simple experiment on \textit{pink prime rose} class from Oxford Flowers102 [] dataset. This concept is a concept known to the diffusion model as it can sample images from this concept when the model is prompted with the prompt `image of a pink prime rose` as shown in Figure[]. 

%We train a textual inversion model on this concept using the training images from Oxford Flowers102 dataset. In Figure~[] we illustrate the similarity between the learned pseudo-word with the the text  \textit{pink prime rose} and few other randomly sampled flower names from the Oxford Flowers dataset. From Figure~[] it is clear that the learned embedding is far from it's natural language counterpart with respect to names of other flowers. This suggests that the solution ovtained through textual inversion doesn't align well with existing CLIP embeddings.

To improve the classification performance of the learned token, we propose to add a regularization term to the loss function through a classification loss. First, we sample $k$ images $\mathbf{x'} = [x'_1, x'_2, \dots , x'_k]$ from the diffusion model with the prompt `image of a $c$', where $c$ is is the parent concept provided. We treat these images as negative images during training. We form a secondary training set by considering images $\mathbf{x''} = [\mathbf{x}, \mathbf{x'} ]$ with corresponding ground truth labels $\mathbf{y} = [\mathbf(1)\in \mathcal(R)^n, \mathbf(0)\mathcal(R)^k]$. During training we consider two prompts of the form `image of a * c' and `image of a $c$' and obtain their text features $\tau_*$ and $\tau_{parent}$ respectively. We then compute a class scores  $\ienc(\mathbf{x''}) \cdot \tau_*$ and  $\ienc(\mathbf{x''}) \cdot \tau_{parent}$ and evaluate a classification loss with balanced binary cross entropy $l_{CE} = \text{BCE}(\mathbf{x''} ,\mathbf{y} )$. Our final training loss becomes a linear combination of stable-diffusion loss $l_{SD}$ and classification loss $l_{CE}$ in the form $\lambda_{SD} l_{SD}+ \lambda_{CE} l_{CE}$. 

In Figure~\ref{fig:weight_ablation}, we illustrate samples generated with the prompt `image of a $\ast$ teapot' when weights $\lambda_{SD}$ and $\lambda_{CE}$ are varied. Here, we report how well the learned embedding is able to recognize the target concept from its parent-concept (teapot from the Caltech256 dataset). Note that the cases at either ends with weight (0,1) and (1,0) are equivalent to prompt tuning and textual inversion respectively. For pure textual inversion case, it appears that balanced-classification accuracy is merely 50\%. According to Figure~\ref{fig:weight_ablation}, increasing weight $\lambda_{CE}$ increases overall classification accuracy. In addition to improving recognition performance, adding regularization adds more structure to the learned embedding. In Figure~\ref{fig:normplot}, we plot the norm of the embedding (TI+CE). According to the plot, adding the regularization has  caused norm of the embedding to drop significantly. However, the correlation between the learned embedding with other attribute embeddings remains poor as shown in Figure~\ref{fig:ce_mtx}.


\subsection{Improving Compositionality}

In Section~\ref{sec:clip}, we considered possible adjectives/attributes of the parent-concept. It should be noted that the parent-concept composes well with these adjectives to form more complex concepts (\eg, ceramic teapot, red teapot, red ceramic teapot). We hypothesize that the embeddings of these adjectives lie in a lower-dimensional sub-space where each embedding vector meaningfully composes well with the parent concept. If this is true, we argue that by enforcing the learned embedding to lie in the same subspace we may increase the possibility of it composing well with natural language. In order to enforce this constraint, we first form a matrix $\mathbf{e}$ with average embeddings of all the selected adjectives. We calculate the mean vector $\mu_\mathbf{e}$ of the matrix $\mathbf{e}$  and perform Principal Component Analysis to find a projection matrix $P$. During training we transform the token $e_*$ using the linear transformation $P(e_*-\mu_\mathbf{e})+\mu_\mathbf{e}$. %In our experiments we chose $m$ such that 50\% of spectral energy is preserved. 

In Figure~\ref{fig:normplot}, we plot the norm of the learned embedding under TI+CE+P. We see that the linear projection has further reduced the norm of the embedding from before - and now lies withing one standard deviation of the word embeddings. In Figure~\ref{fig:pw_mtx}, we illustrate the correlation between the learned embedding and other attribute embeddings. Compared to other alternatives, it can be seen that learned embedding is now much more similar to other word embeddings. In Figure~\ref{fig:ti_comp} (Ours-CLIP), we visualize some of the images generated when subspace projection is employed. According to the Figure, backgrounds of generated images have changed with respect to the prompt. However, it can be seen that preserving fidelity to original concept is challenging in some cases, particularly when composing with other words. We study this phenomenon in detail in the experimental results section.



\subsection{Generation Aided Image Retrieval(GAIR)}
Our \pseudowords{} can be composed with natural language and be used for both discriminative and generative tasks. These properties can be particularly useful for image retrieval. Indeed, a user can construct a query $[t,*,c]$ in natural language which includes the learned token, and also then \emph{visualize} the query and verify whether it is consistent with the intended search. This kind of verification is useful to monitor two kinds of potential issues: 1) the token may have not encoded the target concept correctly (for example, spurious common features of the training images may have been incorporated), 2) the token might not compose well with the textual query. Observing sample images generated from the query allows the user to easily identify these issues and correct for them. 

Once an issue has been identified, a simple approach to improve the query would be to try paraphrasing the text via prompt engineering. An alternative method that is more easily controllable is to manually amplify contribution from certain words to alter the generated image. \cite{Trager2023} showed that CLIP textual features of a sentence can be expressed as linear feature combination of features associated with its constituent words. Drawing inspiration from this observation, we express a query $q$ as a linear combination of a query with and without the learned token, that is, $q = w g([t, * , c]) + (1-w) \sum_i \frac{1}{|A|}g([t, a_i, c ]) $, where $a_i \in A$ is a set of related attributes. Note that when $w=1$, the query $q$ is equal to the original search query $g([t,*,c])$. 


This formulation allows the user to give more importance to some part of the context by reducing the weight $w$ as needed. An optimal value for $w$ for a given query can be chosen by visualizing queries for different choices of $w$. When human supervision is not feasible, this process can be automated based on CLIP score using the Algorithm~\ref{alg:cap}.  Here, for different values of $w$, we formulate the conditioning vector $q$ and use it to generate images. In each case we check whether the generated image contains i) the object under consideration,  by comparing it to original training images $\mathbf{x}$ in CLIP space and ii) the context described by the caption, by comparing the image to context images $\mathbf{x_c}$ in the CLIP space. Context images are generated with the same query where  custom tokens are replaced with a related attribute as in $d(\sum_{a_i \in A} \frac{1}{|A|} g([t, a_i, c ]) $. We select the value of $w$ that had produced the largest value of the minimum CLIP score for context and object. Note that taking the minimum of $f(I)^T f(\mathbf{x}) $ and $f(I)^T f(\mathbf{x_c}) $ ensures that the chosen parameter produces images that has reasonably good CLIP score for both objects and contexts.
% In Figure~\ref{fig:gen} we demonstrate the effect of varying the value of $w$ on a test case from the DeepFashion2 dataset. In this task, the user wishes to search for the image that matches to a given query. When  $w=1$ the the query has captured the concept of `shelves' but has not paid attention to `white counter'. By reducing $w=0.7$, this issue is rectified. With this modification, the model is able to improve the retrieval rank as well.



% \begin{algorithm}
% \caption{Generation Aided Image Retrieval }\label{alg:cap}
% \begin{algorithmic}
% \Require training images $\mathbf{x}$, images of the parent concept $\mathbf{x_c}$, caption $t$
% \State $S \gets []$
% \State $W\gets [w_0 \dots w_N]$
% \For{$ w \in W$}
%          \State $q \gets w g[t,*,c])+ (1-w) \sum_{i} \frac{1}{|A|} g([t,a_i,c]) $
% \State $I \gets d(g[q])$ \Comment{Generate a image}

% \State $S.append(\min (f(I)^T\cdot f(\mathbf{x_c}), f(I)^T\cdot f(\mathbf{x}))$  \Comment{Min of Context score, Object score}
% \EndFor

% \State $ w^* \gets  W[\argmax(S)] $
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}
\centering
\caption{Generation Aided Image Retrieval }\label{alg:cap}
\begin{algorithmic}
\Require training images $\mathbf{x}$, parent concept images $\mathbf{x_c}$, caption $t$
\State $S \gets []$ \Comment{Scores list}
\State $W\gets [w_0, \dots, w_N]$ \Comment{Weights list}
\For{$ w \in W$}
    \State $q \gets w g[t,*,c]+ (1-w) \frac{1}{|A|} \sum_{i} g[t,a_i,c]$
    \State Generate image $I \gets d(g[q])$
    \State $score \gets \min(f(I)^T f(\mathbf{x_c}), f(I)^T f(\mathbf{x}))$
    \State $S.\text{append}(score)$ \Comment{Update scores list}
\EndFor
\State $w^* \gets W[\argmax(S)]$ \Comment{Optimal weight}
\end{algorithmic}
\end{algorithm}





%\begin{figure}[ht] 
%\centering
%\resizebox{1\linewidth}{!}{ 
   
%    \centering{{\includegraphics[width=.55\linewidth] {images/gen.png} } } 
%     }
%    \caption{Generation aided image retrieval. User submits a text query to find the closest matching image from a image database. In this case, the retrieval rank is 2 initially. Our framework provides the opportunity for the user to visualize the search query and make changes to make the query better represent the search requirement. By increasing context-weight (to 0.3) user is able to create a query more faithful to the search objective. The new query results in rank 1 image retrieval.  }%

%\label{fig:gen}
   
%\end{figure}


%\begin{figure}[ht] 
%\centering
%\resizebox{1\linewidth}{!}{ 
   
%    \centering{{\includegraphics[width=1\linewidth] {images/fashion_samples.png} }} 
%     }
%    \caption{Retrieval results for DeepFashion2 dataset. For each query (row), few samples of the concept is visualized. We show a sample image generated from the query along with the GT image and top-3 ranked image retrievals. In top 3 rows, the generated image is in-line with the intended query. The GT image is amongst the top-3 image retrievals. Last row is an example of a failed image retrieval. Once visualized, the generated image shows that the search is different from the intended criteria.  }%

%    \label{fig:qualDeepFashion}
%\end{figure}


\section{Experimental Results} \label{sec:results}
In this section we present  qualitative results in terms of generation quality, quantitative results with respect to recognition performance on the Textual Inversion dataset \cite{textinversion} and text-to-image retrival performance on DeepFashion2\cite{ge2019deepfashion2} dataset. As baseline comparisons we consider i) textual inversion with a BERT encoder \cite{textinversion}, ii) textual inversion with a CLIP text encoder,  and iii) CLIP based prompt tuning \cite{nayakLearningComposeSoft2022}.
To make our comparison fair we add to the prompt tuning objective negative images sampled with the Stable Diffusion model using the superclass textual information as input conditioning.\\

% Similar to how the alignment loss is applied in our proposed method, we first sample images of the parent concept from the stable diffusion model. We train CoOp  by treating these images as negatives (while given images constitute positive class samples).

\noindent \textbf{Model details.} For all the experiments we used 10 trainable tokens. For main results section, we considered  100 attributes from CQGA dataset \cite{Naeem_2021_CVPR} that correlated most with the given concept in the CLIP space to define the lower dimensional sub-space and set $\lambda_{CE}=10^{-5}$, $\lambda_{SD}=1$. We used an effective batch size of 4 with a learning rate of $5 \cdot 10^{-4}$. We trained each model for 20000 iterations. We used the publicly available Stable Diffusion 1.4 trained on LAION dataset as the backbone model.\\

\noindent \textbf{Datasets.} The Textual Inversion dataset contain multiple classes with 5-6 images per class. In our experiments we trained models using 3 images for training and used remaining images for inference. We carried out quantitative experiments on classes that had parent classes in common with Caltech256 dataset. We used the Caltech256 dataset to simulate negative classes since it contains all superclass concepts for all the classes present in the Textual Inversion dataset. In addition, we experimented on DeepFashion2 dataset\cite{ge2019deepfashion2} - which consists of clothing products where product identity is available. We used the PerVL benchmark\cite{cohenThisMyUnicorn2022a} to obtain train-test splits and image captions for the dataset. We used all available training images per class when training on this dataset.\\

%The Oxford Flowers dataset is a widely-used benchmark for evaluating image classification models. It contains 102 flower categories, with each category consisting of between 40 and 258 images. In our experiments we choose the three classes Cardoon, Pink primrose and Siam tulip that exhibits with worst zero-shot performance on CLIP baseline.The Stanford Dogs dataset is a widely-used benchmark for evaluating fine-grained image classification models. It consists of 20,580 images of 120 dog breeds, with each breed containing between 150 and 300 images. For evaluations, we focus on the three classes with worst performance on the CLIP baseline: Irish water spaniel, Japanese spaniel and Soft-coated wheaten terrier.


\noindent \textbf{Image Generation: Textual Inversion Dataset.}
In Figure~\ref{fig:ti_comp} and Figure~\ref{fig:results} we illustrate images sampled from the model for different textual prompts. These figures suggest that all three methods considered have captured the underlying concept reasonably well. This is evident as all the methods are able to produce high quality image samples when the model is prompted with `image of a $\ast$ c'. However, when textual inversion is conditioned on CLIP text features, it appears that the learned token does not compose well with natural language. This observation is consistent with Figure~\ref{fig:ti_comp}. On the other hand, when textual inversion is conditioned on BERT encoder, it better composes with natural language in half of the cases. In the remaining cases, the results tend to be very poor. For example, when the model attempts to place the colorful teapot in a sink, the model fails to generate even a trace of a teapot in the scene. In certain situations (such as the ``physics mug'' and ``red teapot'' on the beach), object fidelity is seen to be heavily affected. Even when the learned token fails to compose well with natural language, the generated image maintains details of the targeted concept. For example, in the case of the mug skull on a sink, textual inversion (BERT) does not retain the general structure of the object. On the other hand our method, while failing at the composition, preserves the structure of the object. 





\begin{figure*} 
  %\setlength{\fboxsep}{-\fboxrule}
  %\fbox{%
  
    \centering
	\resizebox{0.9\linewidth}{!}{
      \begin{tabular}{@{}c|ccc|ccc|ccc||c@{}}
      \toprule 
     Method &
    \multicolumn{3}{c}{Image of a $\ast$} &
    \multicolumn{3}{c}{ Image of a $\ast$ on a sink} &
    \multicolumn{3}{c}{ Image of a $\ast$ on a beach} & Class Samples \\
    \hline
        TI (BERT) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_5.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_6.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_9.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_7.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_ld_8.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c1.jpeg} \\
        TI (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_sd_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/c2.jpeg} \\
        Ours (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_pw_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_pw_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_pw_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_cw_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_cw_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_cw_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/c_cw_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_cw_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c_cw_8.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/c3.jpeg} \\
        \hline
        
        TI (BERT) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_ld_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/s1.jpeg} \\
        TI (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_sd_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/s2.jpeg} \\
        Ours (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/s_cw_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/s3.jpeg} \\
        \hline
        
        TI (BERT) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_ld_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/r1.jpg} \\
        TI (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_sd_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/r2.jpg} \\
        Ours (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/b_cw_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/r3.jpg} \\
        \hline
        TI (BERT) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_0.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_ld_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/p1.jpeg} \\
        TI (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_sd_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/p2.jpeg} \\
        Ours (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/p_cw_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/p3.jpeg} \\
        \hline
        TI (BERT) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_0.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_ld_8.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r1.jpeg}\\
        TI (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_sd_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/r2.jpeg} \\
        Ours (CLIP) &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_0.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_1.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_2.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_3.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_4.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_5.jpg} &
        \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_6.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_7.jpg} & \includegraphics[width=0.11\linewidth]{images/qual_samples/r_cw_8.jpg}& \includegraphics[width=0.11\linewidth]{images/qual_samples/r3.jpeg} \\
      \end{tabular}} \caption{Images generated with different captions for classes from the Textual Inversion dataset.} \label{fig:results}
\end{figure*} 


We quantified image generation capabilities of each method based on object/context relevance score with respect to CLIP and ALIGN features. For each concept, we considered 10 contexts  and generated 8 images per context. For context relevance, we verified whether image features of the generated image correlates most with text features of the context (out of the 10 contexts). For object relevance, we verified whether image features of the generated image correlates most with image features of the corresponding training class (compared to other classes in textual inversion dataset). In Table~\ref{tbl:clipscore}, we show accuracy for object and context classifiers when classifiers are based on CLIP and ALIGN models. On average, our proposed method leads to improved performance  of TI(CLIP) by over 15\% in both CLIP and ALIGN metrics. We note that when a better text encoder BERT is used with TI, context accuracy improves but object accuracy degrades. On average our method performs marginally better than TI(CLIP) when on CLIP metric but lags behind by nearly 6\% on ALIGN metric.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\caption{CLIP/ALIGN-score accuracy for Textual-Inversion dataset classes considering object and context classes. Our oroposed method produces images where the concept composes better compared to the Textual Inversion baseline(CLIP).}
	\resizebox{0.7\linewidth}{!}{
\begin{tabular}{@{}lcccccc@{}}
\toprule
           & \multicolumn{3}{c}{CLIP}                                                                         & \multicolumn{3}{c}{ALIGN}                                                                        \\  \midrule
           & \multicolumn{1}{l}{Object Acc} & \multicolumn{1}{l}{Context Acc} & \multicolumn{1}{l}{Average} & \multicolumn{1}{l}{Object Acc} & \multicolumn{1}{l}{Context Acc} & \multicolumn{1}{l}{Average}  \\ \midrule
Random     & 0.166                           & 0.100                            & 0.133                       & 0.166                           & 0.100                            & 0.133                       \\
TI (BERT)  & 0.621                           & 0.743                            & 0.682                       & 0.764                           & 0.754                            & \textbf{0.759}              \\
TI(CLIP)   & 0.975                           & 0.118                            & 0.546                       & 0.983                           & 0.098                            & 0.541                       \\
Ours(CLIP) & 0.942                           & 0.456                            & \textbf{0.699}              & 0.967                           & 0.438                            & 0.702 \\ \bottomrule                      
\end{tabular}}
\label{tbl:clipscore}
\end{table}


\vspace{-.3cm}



In order to evaluate the image recognition capabilities of the learned token, we carried out benchmarking with on the Caltech256 dataset. We coupled each concept with a parent concept from Caltech256 dataset. In our evaluation we focused on three questions: 
i) Can the token distinguish the specialized concept from the parent concept?
ii) Can it distinguish specialized concept from other concepts 
iii) Can it be used to distinguish the parent concept from other concepts?
In Table~\ref{tbl:qual} we tabulate average recognition performance in terms of Area Under the Curve of ROC curve (with standard deviation in brackets). Since the prompt tuning model is trained to differentiate between target concept from its parent concept, it is not surprising that it performs well whenever the given concept is involved. However, despite the target concept being structurally similar the parent concept, the token learned by prompt tuning only obtains a AUC-ROC of 60\% when separating the parent class from other classes.  Table~\ref{tbl:qual} further suggests that textual inversion (CLIP) performs reasonably well. The proposed method, on the other hand, improves performance of textual inversion by 15\%, 19\% and 40\%. This suggests that prompt tuning overfits to local features of  objects. Since it is closely bound to the target concept, it cannot be later used to perform inference on behalf of the parent class. On  the other hand, due to the additional constraints used during training our model learns a very general representation.  Therefore, the representation of the specialized concept can use in lieu of parent concept to obtain very high performance.\\

\noindent \textbf{Text-to-Image Retrieval: DeepFashion2 Dataset.}
We trained concept tokens for each class from the train-set of the DeepFashion2 dataset from the PerVL benchmark\cite{cohenThisMyUnicorn2022a}. We used captions provided by the benchmark during training. Prior to training, we pre-processed images by manually redacting faces of all human subjects appearing in the dataset. We use this dataset to assess text-to-image retrieval performance.  We use Mean Reciprocal Rank(MRR) as the metric to compare performance between different methods. 

%In Figure~\ref{fig:qualDeepFashion}, we visualize few examples of retrieval from the DeepFashion2 dataset. In Figure~\ref{fig:qualDeepFashion}, few samples of the underlying concept is visualized along the query. For each case, we have illustrated a image sample generated from the model. In the top three rows, the model has obtained a top-3 retrieval. Here we note that the generated image is inline with what is described by the text query. In the bottom row, we illustrate a failure case - where GT image is not obtained as a part of the top-3 retrieval. In this case, we show that the generated image is very different from the description of the query. It appears the model has paid more attention to the phrase `brick wall' in the query. This is an example of how our model can be used to interpret results.



In Table~\ref{tbl:ret_deepfashion} we show the MRR obtained for classes in PerVL benchmark. In all methods, we compare the text feature with the image feature in the CLIP space to obtain a ordering of images. We note that, extracting text features with \pseudowords~have improved retrieval performance by 5\% compared to prompt tuning. Performance further increases by 2\% when the search query is modified using Algorithm~\ref{alg:cap}. For this dataset we considered `dress' to be the super-class name and used  ``red'', ``old'', ``worn'', ``bright'', ``dark'', ``pink'' as the list of attributes $A$. Here, we note that using Algorithm~\ref{alg:cap} with prompt tuning has deteriorated its original performance. This deterioration is expected since prompt tuning does not produce semantically meaningfully images when visualized. When this is the case, the algorithm is unable to locate the optimum weight by considering the CLIP similarity score. 



In Figure~\ref{fig:qualDeepFashion}, we visualize few examples of retrieval from the DeepFashion2 dataset. In Figure~\ref{fig:qualDeepFashion}, few samples of the underlying concept is visualized along the query. For each case, we have illustrated a image sample generated from the model. In the top three rows, the model has obtained a top-3 retrieval. Here we note that the generated image is inline with what is described by the text query. In the bottom row, we illustrate a failure case - where GT image is not obtained as a part of the top-3 retrieval. In this case, we show that the generated image is very different from the description of the query. It appears the model has paid more attention to the phrase `brick wall' in the query. This is an example of how our model can be used to interpret results.

\begin{figure}[ht] 
\centering
\resizebox{1\linewidth}{!}{ 
   
    \centering{{\includegraphics[width=1\linewidth] {images/fashion_samples.png} }} 
     }
    \caption{Retrieval results for DeepFashion2 dataset. For each query (row), few samples of the concept is visualized. We show a sample image generated from the query along with the GT image and top-3 ranked image retrievals. In top 3 rows, the generated image is in-line with the intended query. The GT image is amongst the top-3 image retrievals. Last row is an example of a failed image retrieval. Once visualized, the generated image shows that the search is different from the intended criteria.  }%

    \label{fig:qualDeepFashion}
\end{figure}

\vspace{-.2cm}

\begin{table}[t]
\caption{Aggregated recognition performance measured by AUC-ROC for five classes from the Textual-Inversion dataset with standard deviation. }
%To test fine-grained capabilities of {\pseudowords} we compare discrimination performance on recognizing the custom concept (specified by images in the Textual Inversion dataset) vs. the superclass/other concepts (extracted from the Caltech256 dataset \cite{caltech256_dataset}). Furthermore, to test whether {\pseudowords} overfit the specific instance used to learn the token or capture higher level semantic information, we compare the discrimination performance on recognizing the superclass of the learned custom concept vs. other classes from Caltech256.}

\centering
	\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lccc}

\toprule
                                                      & Prompt Tuning (Paragon)%\cite{nayakLearningComposeSoft2022}    
                                                      & Textual Inversion%\cite{textinversion} 
                                                      & Ours                   \\ %\hline
\hline
% Target concept vs parent concept        & \textbf{100.0 (0.0)} & 58.85 (42.12)     & 98.88 (1.98)           \\ %\hline
% Target concept vs other Caltech classes & \textbf{100.0 (0.0)} & 81.29 (30.81)     & \textbf{100.0 (0.0)}   \\ %\hline
% Parent concept vs other Caltech classes & 59.06 (26.55)        & 70.51 (16.70)     & \textbf{85.86 (11.27)} \\ %\hline
Target concept vs parent concept        & \textbf{100.0 (0.0)} & 58.89 (42.12)     & 98.88 (2.21)           \\ %\hline
Target concept vs other Caltech classes & \textbf{100.0 (0.0)} & 80.53 (30.44)     & \textbf{100.0 (0.0)}   \\ %\hline
Parent concept vs other Caltech classes & 58.93 (25.24)        & 70.54 (15.96)     & \textbf{85.49 (11.42)} \\ %\hline
\bottomrule
\end{tabular}} 
\label{tbl:qual}
\vspace{-.2cm}
\end{table}


% \begin{table}[h]
% \centering
% \caption{MRR for retrieval on the DeepFashion2 Dataset (50 classes). 
% }
% \label{tab:my_label}
% 	\resizebox{0.6\linewidth}{!}{
% \begin{tabular}{lc lc}
% \toprule
% Method & MRR & Method & MRR \\
% \midrule
% Text Only & 17.6 (0.0) & Prompt Tuning & 51.4 (2.1) \\
% Text+Avg Image & 21.7 (2.4) & Prompt Tuning (Adaptive Weights) & 45.2 (0.8) \\
% PALAVRA \cite{cohenThisMyUnicorn2022a} & 28.4 (0.7) & \textbf{Ours} & \textbf{56.6 (2.3)} \\
% Ideal Words \cite{Trager2023} & 37.0 (1.1) & \textbf{Ours (Adaptive Weights)} & \textbf{58.8 (2.3)} \\
% \bottomrule
% \end{tabular}}
% \label{tbl:ret_deepfashion}
% \end{table}

\begin{table}[h]
\centering
\caption{MRR for retrieval on the DeepFashion2 Dataset (50 classes).}
\label{tab:single_col}
\resizebox{.4\linewidth}{!}{
\begin{tabular}{l c}
\toprule
Method & MRR \\
\midrule
Text Only & 17.6 (0.0) \\
Text+Avg Image & 21.7 (2.4) \\
PALAVRA \cite{cohenThisMyUnicorn2022a} & 28.4 (0.7) \\
Ideal Words \cite{Trager2023} & 37.0 (1.1) \\
Prompt Tuning & 51.4 (2.1) \\
Prompt Tuning + GAIR & 45.2 (0.8) \\
\textbf{Ours} & \textbf{56.6 (2.3)} \\
\textbf{Ours + GAIR} & \textbf{58.8 (2.3)} \\
\bottomrule
\end{tabular}}
\label{tbl:ret_deepfashion}
% \vspace{-.3cm}
\end{table}


\vspace{-.5cm}




%\noindent\textbf{Ablation Study.}
%In order to study the effect of different parameters on model performance, we carry out an ablation analysis by varying the number of tokens, amount of energy preserved during projection, and the amount of regularization introduced by varying the weight $\lambda_{CE}$. For this experiment we chose 200 attributes related to the concept `teapot' from the GPT model. In Figure~\ref{fig:ablation}, we visualize images generated using models trained on the \textit{colourful teapot} class with the prompts ``image of a * teapot'',``image of a * teapot on a sink'' and ``image of a * teapot on a beach''. Overall, we find that there is a tension between image fidelity and compositionalty. The visual representation of the concept can be forced to closely follow the distribution of training images by either increasing number of tokens or by increasing the amount of spectral energy preserved. However, in both these cases, the ability of the learned pseudo-word to compose with natural-language deteriorates as illustrated in Figure~\ref{fig:ablation}. On the other hand, when lower number of tokens are used and only the main principal components are chosen to define the projection space, the learned token becomes more compositional at the expense of image fidelity. Interestingly, figure~\ref{fig:ablation} further suggests that adding the regularization term with $\lambda_{CE}>0$ further improves the compositionality of the tokens.

%\begin{figure}[ht] 
%\centering
%\resizebox{0.9\linewidth}{!}{ 
%   \includegraphics[width=1\linewidth ]{images/ablation.png} } 
%    \caption{Increasing number of tokens and preserving more spectral energy leads to better image fidelity in the expense of compositionaliy. Introducing regularization also improves compositionality. }\label{fig:ablation}
    
%\end{figure}

%\noindent \textbf{Fidelity vs Compositionalty.} We find that there exists a dual relationship between Fidelity vs Compositionalty. Visual representation of the concept can be forced to closely follow the distribution of training images by either increasing number of tokens or by increasing the amount of Eigen energy preserved. However, in both these cases, ability of the learned pseudo-word to compose with natural-language deteriorates as illustrated in Figure~[]. On the other hand, when lower number of tokens are used and only the largest principle components are chosen to  define the Eigen space, the learned token becomes more compositional in the expense of image fidelity. 

%\noindent \textbf{Fidelity vs Recognition ability.} We find a similar relationship between image fidelity and recognition ability. To independently study this phenomena, we train models without Eigen projection by varying the value of $\lambda_{CE}$ and $\lambda_{SD}$. When $\lambda_{CE}$ is set to 0 (with non zero $\lambda_{SD}$), image fidelity is very well preserved. In this setting, the learned pseudo-word does not have the ability to differentiate the concept from the parent concept. This in fact is identical to textual inversion []. On the other hand when $\lambda_{SD}$ is set to 0 (with non zero $\lambda_{CE}$), the learned pseudo-word can perfectly distinguish learned concept from it's parent concept - how ever with very poor visual representation. This setting is identical to prompt tuning[]. In Figure~[] we visualize the learned representation of the pseudo-token by decoding it through a CLIP based diffusion model. Interestingly it appears that the pseudo-word here has learned only to differentiate the difference between the two classes presented during training - the image color. There is no incentive for the model to learn object structure in this case as learning color information is sufficient for the class separation. When non-zero $\lambda_{CE}$ and $\lambda_{SD}$ are used, model learns a pseudo-word that captures both textural and structural information of the concept in interest as shown in Figure~[].


\vspace{-0.5cm}
\section{Conclusion}
% In this paper we investigated strategies to learn new concepts that exhibit same qualities natural language words posses. We showed that generative and recognition performance is balanced depending on the choice of hyper-parameters. In the future work, we hope to investigate techniques that can flatten the trade-off between fidelity and compositionality.
We investigated the problem of learning flexible custom tokens that can be used for both generative and discriminative visual tasks. The proposed method generates custom tokens 
% that were more compositional and effective in classification and generation tasks. Learned tokens can
that can be used with natural language in search queries and lead to strong text-to-image retrieval performance, particularly when coupled with the Generation Augmented Image Retrieval algorithm. We have observed how our modifications and their parameters contribute to the tradeoff between visual fidelity and compositionality. In the future, we hope to investigate solutions that can find a good operating point that balances visual fidelity and compositionality.
%We found that tokens learned via textual inversion are generally too specialized for visual appearance and do not encode enough semantic information, since they often fail at compositional generation and are also not well-aligned with the corresponding parent class. On the other hand, prompt tuning learns tokens that are effective at discriminating a fine-grained concept, but are unable to generate images of the concept, and are also inconsistent with the parent class. We have proposed two solutions---an additional alignment loss and a projection step of the learned token onto a semantically meaningful linear subspace---that allow the learned token to be more compositional and be effective in classification and generation tasks. We have also observed how our modifications and their parameters contribute to the tradeoff between visual fidelity and compositionality. In the future, we hope to investigate solutions that do not require these parameters to fixed before training, so that the the learned token can be adapted for any downstream task directly at inference time.

{
    \small
    \bibliographystyle{splncs04}
    \bibliography{eccv}
}
% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
