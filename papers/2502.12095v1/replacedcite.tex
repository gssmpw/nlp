\section{Related Work}
Diffusion Probabilistic Models (DPMs)____ have recently been shown to be remarkably effective for image generation. In particular, models such as DALLE-2____, Imagen____, Stable diffusion____ are able to generate high-quality images conditioned on arbitrary textual prompts (text-to-image synthesis). A key component for this success is the availability of vision-language models such as CLIP____, that were trained on massive amounts of data and provide strong text-image priors. However, generating peculiar objects that were not observed during training requires \emph{inversion}, i.e., associating example images to the conditioning variables that are able to generate those same images. This problem has been studied extensively in the GAN literature____. Using diffusion models, \emph{textual inversion}____ has been proposed as an approach for inverting 3-5 images into a token (or ``pseudo-word'') for the frozen language model. Although the learned tokens can be used in generic sentences to generate images in new contexts and styles, the compositional abilities are still limited (see Section~\ref{sec:results}). \emph{Dreambooth}____ is an alternative inversion method for diffusion models, however it requires fine-tuning the parameters of the diffusion model for each new concept. We also mention \emph{Null-Text Inversion}____ that modifies the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This approach, however, does not easily allow to generate compositions of learned concepts. Finally, several recent methods use inversion-style techniques for flexible image editing____.

Prompt tuning is a parameter-efficient fine-tuning method originally proposed in NLP literature____ that optimizes a restricted set of token embeddings in a transformer-based model. The adoption of transformers in Computer Vision has allowed for similar techniques to be used for vision tasks as well ____. In particular, in the vision domain, prompt tuning can be performed by optimizing image tokens (VPT____) or, in the case of vision-language models, text tokens (CoOP____). Learning language tokens opens the door to do compositional manipulations, for example compositions of multiple learned tokens____ or compositions with natural language____. These methods, however, train tokens to perform classification or retrieval tasks. To our knowledge, the problem of learning tokens that are effective for both generation and classification has not been investigated.


% \vspace{.5cm}

% \noindent \textbf{{Generative Diffusion Models.}} Diffusion models have emerged as a powerful approach for image generation, offering a flexible and tractable framework for modeling complex data distributions. In this section, we review some of the most relevant works on diffusion models for image generation. Generative diffusion models (GDMs) are a class of diffusion models that use an iterative procedure to generate high-quality images. GDMs start with a noise image and iteratively apply a diffusion process to generate samples from the data distribution. The first GDM model was introduced by ____, which used a denoising diffusion probabilistic model (DDPM) to generate images. Later, ____ proposed the YourGAN model, which uses a GAN-style objective function to learn the generator. ____ proposed a novel approach called Diffusion Equilibrium Generative Models (DEGMs), which use a diffusion process to reach a stationary distribution. ____ introduced Guided Diffusion (GD), a diffusion-based model that can synthesize high-quality images from unconditioned or partially conditioned inputs.


% \noindent \textbf{Prompt tuning.} Prompt tuning is a recent approach to improving the performance of language models on specific tasks, by fine-tuning the model on a set of prompts that are tailored to the task. This technique has gained significant attention in the natural language processing community, and here we review some of the most relevant works on prompt tuning.
% The GPT-3 language model is known for its remarkable performance on a wide range of natural language processing tasks. One of the key factors behind this success is the use of prompt engineering, where the model is fine-tuned on a set of prompts that are carefully designed to elicit the desired behavior. For example, ____ showed that prompt engineering can significantly improve the performance of GPT-3 on tasks such as question answering and summarization. Learning to prompt is a technique for automatically generating prompts that are tailored to the task, using a combination of machine learning and natural language processing. One approach to learning to prompt was proposed by ____, who introduced a method for generating prompts using a neural network that is trained to predict the output of the language model on a given task. Another approach was proposed by ____, who used a combination of reinforcement learning and human feedback to learn effective prompts for a question answering task.
  
% \noindent \textbf{Textual inversion.} Prompt textual inversion is a relatively new research area that aims to generate natural language inputs that would result in a given output when processed by a pre-trained language model. Dreambooth is a recently proposed framework that uses prompt textual inversion to generate diverse and controllable dream-like text. Dreambooth leverages the power of pre-trained language models to generate text that corresponds to a given set of dream attributes, such as vividness, bizarreness, and coherence. The main idea behind Dreambooth is to invert the representations learned by the language model to obtain a prompt that would generate text with the desired dream attributes.