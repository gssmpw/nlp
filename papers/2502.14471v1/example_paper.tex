%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables 
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs} 
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[table,xcdraw,dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{balance}
% \usepackage{subcaption}
 

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother



%color definition
%\definecolor{rouse}{rgb}{0.981,0.961,0.941}
\definecolor{c2}{HTML}{FBD9BD}
\definecolor{c3}{HTML}{fe793d}
\definecolor{c4}{HTML}{eedeb0}
\definecolor{rouse}{rgb}{0.981,0.961,0.941}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
% \usepackage{hyperref}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{caption}

% if you use cleveref..
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
% \crefname{table}{Table}{Tabs.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{Integrating Extra Modality Helps Segmentor Find Camouflaged Objects Well}
% 可以，很fancy


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chengyu Fang}{thu,equal}
\icmlauthor{Chunming He}{dku,equal}
\icmlauthor{Longxiang Tang}{thu}
\icmlauthor{Yuelin Zhang}{cuhk}
\icmlauthor{Chenyang Zhu}{thu} \\
\icmlauthor{Yuqi Shen}{thu}
\icmlauthor{Chubin Chen}{thu}
\icmlauthor{Guoxia Xu}{nupt}
\icmlauthor{Xiu Li}{thu}
\end{icmlauthorlist}


\icmlaffiliation{thu}{SIGS, Tsinghua University, Shenzhen, China.}
\icmlaffiliation{dku}{BME, Duke University, Durham, US.}
\icmlaffiliation{cuhk}{MAE, The Chinese University of Hong Kong, Hongkong, China.}
\icmlaffiliation{nupt}{SCIE, Nanjing University of Posts and Telecommunications, Nanjing, China}

\icmlcorrespondingauthor{Xiu Li}{li.xiu@sz.tsinghua.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}



% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} 
% otherwise use the standard text.


\begin{abstract}
Camouflaged Object Segmentation (COS) remains a challenging problem due to the subtle visual differences between camouflaged objects and backgrounds. Owing to the exceedingly limited visual cues available from visible spectrum, previous RGB single-modality approaches often struggle to achieve satisfactory results, prompting the exploration of multimodal data to enhance detection accuracy. In this work, we present UniCOS, a novel framework that effectively leverages diverse data modalities to improve segmentation performance. UniCOS comprises two key components: a multimodal segmentor, UniSEG, and a cross-modal knowledge learning module, UniLearner. UniSEG employs a state space fusion mechanism to integrate cross-modal features within a unified state space, enhancing contextual understanding and improving robustness to integration of heterogeneous data. Additionally, it includes a fusion-feedback mechanism that facilitate feature extraction. UniLearner exploits multimodal data unrelated to the COS task to improve the segmentation ability of the COS models by generating pseudo-modal content and cross-modal semantic associations. Extensive experiments demonstrate that UniSEG outperforms existing Multimodal COS (MCOS) segmentors, regardless of whether real or pseudo-multimodal COS data is available. Moreover, in scenarios where multimodal COS data is unavailable but multimodal non-COS data is accessible, UniLearner effectively exploits these data to enhance segmentation performance. Our code will be made publicly available on \href{https://github.com/cnyvfang/UniCOS}{GitHub}.
\end{abstract}

\begin{figure}[h]
\setlength{\abovecaptionskip}{0cm}
	\centering
	\includegraphics[width=\linewidth]{figure/IR-SMALL-COLOR.pdf}\vspace{-4mm}
	\caption{RGB images with segmentation ground truth, corresponding estimated depth maps provided by PopNet \cite{wu2023source}, estimated infrared images generated by an individually trained ResUNet and our UniLearner, which employs the same network architecture as ResUNet. Our approach enhances the performance of RGB-Infrared conversion, delivering outstanding results in representing the structure and location of camouflaged objects.}
	\label{fig:IR}
	\vspace{-5mm}
\end{figure}

% \setlength{\abovedisplayskip}{4pt}
% \setlength{\belowdisplayskip}{4pt}
\section{Introduction}



Camouflaged Object Segmentation (COS) aims to detect hard-to-identify targets within a scene. This task is particularly challenging due to the limited visual information and minimal differences between the camouflaged objects and their surrounding background. The lack of clear visual cues complicates the identification of these targets, making COS a challenging task for both machines and humans.


A recognized strategy to address the limitations of single-image COS is to incorporate auxiliary cues from other modalities. For instance, IPNet \cite{wang2024ipnet} and PolarNet \cite{wang2023polarization} employ polarization-based datasets comprising 1,200 RGB-polarization object camouflage scenes pairs to improve segmentation accuracy through polarization cues. Nevertheless, these datasets remain limited in scale, and models trained on such sparse data often yield only marginal improvements in performance.


Developments in source-free depth estimation have made the use of depth information increasingly prevalent in COS task. For instance, PopNet \cite{wu2023source} enhances COS by incorporating depth maps through a specialized network architecture and loss function. Similarly, DSAM \cite{yu2024exploring} explores the interplay between depth and RGB information within the COS domain, facilitated by the SAM framework \cite{kirillov2023segment}, to achieve more effective integration of these modalities. However, monocular depth estimation sometimes fails when objects and background on same focal plane, as seen in \cref{fig:IR}, or when visual confusion is significant. This results in minimal depth discrimination, significantly reducing the effectiveness of these methods.


Infrared data is another modality recognized for its potential in object-centered segmentation tasks, as it 
captures the thermal radiation differences of objects, providing effective cues for distinguishing camouflaged objects from their surroundings. However, incorporating infrared data into COS presents significant challenges. Constructing paired datasets of infrared and camouflaged object images is notably difficult, and there are currently no reliable methods for generating pseudo-infrared data for camouflaged object images. These challenges hinder the effective integration of infrared and similar modalities into COS tasks.


Advances in state space models, such as Mamba, have enabled vision tasks to leverage longer contextual dependencies, demonstrating significant potential for cross-modal feature fusion. To maximize effective feature, we propose the State Space Fusion Mechanism (SSFM) with Cross State Space Model (CSSM), which unify multimodal features into a shared state space for efficient fusion. Building on upon this design, we introduce \textbf{UniSEG}, an MCOS network.

To avoid guidance issues from pseudo-modal uncertainty, UniSEG employs the Latent Space Fusion Module (LSFM) to perform preliminary feature fusion within the latent space and incorporates the Feature Feedback Module (FFM) to reintroduce the results of latent space fusion into the additional modality encoder to provide targeted guidance for subsequent feature extraction by the encoder, and facilitating further fusion within the state space through SSFM. By adopting a fusion-feedback-fusion strategy, UniSEG effectively extracts and integrates critical information across modalities, leading to improved MCOS performance.

To better leverage additional modalities in the COS task, we propose \textbf{UniLearner}, a framework to acquire cross-modal knowledge from an auxiliary RGB-X dataset which is not related to COS task. UniLearner generates pseudo-modal results and a semantically rich latent vector mapping an RGB image to the auxiliary modality, guiding the segmentation network. By jointly optimizing UniLearner with the segmentation network, the framework improves the generation of features that enhance segmentation performance, and obtains a better results in cross-domain image translation.


The modular design of UniSEG allows it to function as a plug-and-play enhancement for existing segmentation networks. Its components can seamlessly transform a single-modal segmentor into a multimodal one. Furthermore, UniLearner can collaborate with dual-branch multimodal segmentors, boosting their performance through effective cross-modal knowledge integration.

\textbf{Our contributions can be summarized as follows:}


(1) We propose \textbf{UniCOS}, a unified MCOS framework that integrates a multimodal segmentor, \textbf{UniSEG}, and a cross-modal knowledge learning plugin, \textbf{UniLearner}.



(2) \textbf{UniSEG} fuses encoded multimodal and image features within both latent and state spaces, subsequently feeding the fused features back into the extra-modal encoder to guide further feature extraction. This iterative fusion-feedback mechanism enhances contextual understanding and noise robustness, thereby improving segmentation performance.


(3) \textbf{UniLearner} acquires cross-modal knowledge from task-unrelated multimodal data. It maps an image into the target modal space, generating pseudo-modal content and a mapping vector. By embedding this vector into UniSEG, UniLearner establishes cross-modal semantic associations that enhance segmentation performance.


(4) Extensive experiments across various COS tasks demonstrate that our approach achieves state-of-the-art performance while offering plug-and-play versatility.



\section{Related Works}


\noindent \textbf{Camouflaged Object Segmentation}. Recent studies on COS have progressed using techniques such as multi-scale \cite{pang2024zoomnext}, multi-space \cite{zhong2022detecting, sun2025frequency}, multi-stage \cite{jia2022segment}, and biomimetic strategies \cite{he2023strategic}, which focus on enhancing information extraction from camouflaged images. Despite these advancements, most methods still rely on single-modal inputs, which limits the potential of multimodal data due to challenges in acquiring paired multimodal data with camouflaged samples. Advances in depth estimation have encouraged the integration of depth data, underscoring the benefits of multimodal approaches \cite{xiang2022exploringdepthcontributioncamouflaged, wu2023source, yu2024exploring, wang2024depth, wang2023depth}. However, research into RGB-to-X modal translation for other modalities is still limited, which restricts the advancement of additional modality-assisted COD tasks.

To address this issue, we propose UniLearner to learns and utilizes cross-modal information between images and various modalities to enhance MCOS performance. By embedding a cross-modal semantic vector into the segmentor and leveraging existing non-camouflaged multimodal data% or pseudo-modal data generated by current methods
, this framework improves COS performance when real multimodal datasets with camouflaged objects are unavailable.

\noindent \textbf{State Space Models}. Rooted in classical control theory \cite{10.1115/1.3662552}, State Space Models (SSMs) are essential for analyzing continuous long-sequence data. The Structured State Space Sequence Model (S4) \cite{gu2022efficientlymodelinglongsequences} initially modeled long-range dependencies, recently, Mamba \cite{gu2024mambalineartimesequencemodeling, xiao2025mambatree} introduced a selection mechanism  that enables the model to extract relevant information from the inputs. Mamba has been applied effectively in image restoration \cite{guo2024mambairsimplebaselineimage, li2024fouriermambafourierlearningintegration, yang2024learning, zheng2024fd, zheng2024u}, segmentation \cite{wang2024mamba, xing2024segmamba}, and other domains \cite{zhang2024motion, zubic2024state}, achieving competitive results. 
In the context of image fusion, approaches like MambaDFuse \cite{li2024mambadfusemambabaseddualphasemodel} and FusionMamba \cite{xie2024fusionmambadynamicfeatureenhancement} have leveraged Mamba to improve performance. However, these methods utilize SSMs only for feature extraction, neglecting the cross-modal state space features and Mamba’s selection capabilities across different modal features in a unified state space.
To address this, we propose a universal State Space Fusion Mechanism that integrates and selectively extracts features across modalities within a unified state space, enhancing MCOS performance.

\begin{figure*}[h]
\setlength{\abovecaptionskip}{0cm}
	\centering
	\includegraphics[width=\linewidth]{figure/Framework.pdf}\vspace{-4mm}
	\caption{Framework of our UniCOS, and the details of FFM, LSFM, \(g_w\), and SSFM. The modules outlined by dashed lines mean the modules introduced by UniLearner, which can be omitted when using paired RGB-X data.
    }
	\label{fig:Framework}
	\vspace{-5mm}
\end{figure*}

\section{Methodology} 

\subsection{Preliminaries}
\textbf{Structured State Space Sequence Models (S4)}. S4 transforms a one-dimensional input \(x(t) \in \mathbb{R}\) into an output \(y(t) \in \mathbb{R}\) through an implicit state representation \(h(t)\in \mathbb{R}^{N}\). The system dynamics are governed by the following linear ordinary differential equation:
\begin{equation}
\label{eq:ssm}
    h'(t) = Ah(t) + Bx(t),\quad
    y(t)  = Ch(t),
\end{equation}
where \(N\) denotes the dimensionality of the hidden state. The matrices \(A \in \mathbb{R}^{N\times N}\), \(B \in \mathbb{R}^{N \times 1}\), and \(C \in \mathbb{R}^{1\times N}\) define the dynamics of the system and control how the hidden state evolves and how the output is derived.

To integrate \cref{eq:ssm} into deep learning pipelines, the continuous formulation is typically discretized. Let \(\Delta\) denote a timescale step size that discretizes \(A\) and \(B\) into discretized \(\overline{A}\) and \(\overline{B}\). A common discretization approach is the zero-order hold, defined as:
\begin{equation}
\label{eq:ZOH}
    \overline{A} = \exp (\Delta A),\,
    \overline{B} = (\Delta A)^{-1} (\exp(\Delta A) - I) \Delta B.
\end{equation}
By discretizing \cref{eq:ssm} with the timestep 
\(\Delta\), the system is transformed into the following RNN-like representation: 
\begin{equation}
\label{eq:discret-ssm}
    h_k = \overline{A}h^{k-1} + \overline{B}x^k,\quad
    y_k = Ch^k.
\end{equation}
where $h_k$ and $y_k$ represent the discretized hidden state and output, respectively, at timestep $k$.

In Mamba \cite{gu2022efficientlymodelinglongsequences}, the matrix \(\overline{B}\) can be approximated using the first-order Taylor series as follows: 
\begin{equation}
    \overline{B}\approx(\Delta A)(\Delta A)^{-1}\Delta B=\Delta B
\end{equation}

\noindent\textbf{Selective Scan Mechanism.} State Space Models (SSMs) are effective for modeling discrete sequences but are inherently constrained by their Linear Time-Invariant (LTI) nature, resulting in static parameters that remain unchanged regardless of input variations. The Selective State Space Model (S6, also known as Mamba) addresses this limitation by introducing input-dependent dynamics. In the design of Mamba, the matrices \( B \in \mathbb{R}^{L \times N} \), \( C \in \mathbb{R}^{L \times N} \), and \( \Delta \in \mathbb{R}^{L \times D} \) are directly derived from the input data \( x \in \mathbb{R}^{L \times D} \). This dependency allows the model to adapt dynamically to the input context, enabling it to capture complex interactions within long sequences more effectively. 


\subsection{UniSEG: Unified Multimodal Segmentor}

UniSEG integrates features from RGB images and additional modalities within both the state space and the latent space.
The framework employs a Latent Space Fusion Module (LSFM) and a State Space Fusion Mechanism (SSFM) to selectively combine features from RGB images and auxiliary modalities, enhancing the performance of camouflaged object segmentation. 
Furthermore, a Feature Feedback Module (FFM) is introduced to leverage the outputs of LSFM at specific network layers, guiding subsequent encoder layers toward more effective feature extraction.


\subsubsection{MultiModal Segmentation-Oriented Encoder} 

UniSEG conducts a two-branch encoder architecture to extract and utilize the features beneficial from different modalities. Give inputs \(\mathbf{x}_i\) and \(\mathbf{x}_u\), we first interpolate them to a uniform size of \(W \times H\). 
We begin by using a basic encoder \(\mathcal{E}_i\) to extract a set of deep features \(\{f_i^k\}_{k=0}^4\) from \(\mathbf{x}_i\), where each \(f_i^k\) has a resolution of \(\frac{W}{2^{k+1}} \times \frac{H}{2^{k+1}}\). 
To handle features from the additional modality, a secondary encoder \(\mathcal{E}_u\) with a similar architecture is employed. This encoder includes a customized embedding layer to adapt to the specific characteristics of \(\mathbf{x}_u\). 
The output of layer \(k\) of \(\mathcal{E}_u\) is denoted as \(f_u^k\), with the same resolution as \(f_i^k\).

To fuse features from different modalities in the latent space, we implement LSFM to fuse features \(f_i^k\) and \(f_u^k \in \mathbb{R}^{B \times C \times H \times W}\), generating a fused latent feature \(f_x^k\) of the same size at $k=\{1,2,3,4\}$:
\begin{equation}
\label{eq:LSFM}
f_x^k = f_i^k \odot \mathrm{Sigmoid}(W_c^1 \mathcal{C}(f_u^k)) + W_c^2 \mathcal{C}(f_u^k),
\end{equation}
where \(W_c\) is a convolution, $\mathcal{C}$ means a $\text{Conv+LReLU+BN}$ block, and \(\odot\) denotes elementwise multiplication.

The last fused latent feature map \(f_x^4\), which is rich in semantic content, is processed by an atrous spatial pyramid pooling (ASPP) module \(A_s\) \cite{yang2018denseaspp} to produce a coarse prediction \(p_s^5=W_c(A_s(f_x^4)\), with the spatial resolution of \(f_x^4\) and serving as the initial point for the decoder.

Different from \(f_x^4\), the purpose of \(\{f_x^k\}_{k=1}^3\) is to guide $\mathcal{E}_u$ to extract targeted features from extra-modal by existing feature. % and share fused latent features to state-space fusion process. 
To achieve this, UniSEG introduces FFM to inject \(f_x^k\) into \(f_u^k\) in a gated way, generating \(f_u^{k'}\).
This updated feature serves as an input for both the $(k+1)^{th}$ layer of $\mathcal{E}_u$ and the SSFM following layer $k$:
\begin{equation}
\begin{aligned}
        \alpha &= \mathrm{Sigmoid}(W_c^1 \mathrm{conca}\Big[f_u^k,\, \mathcal{C}_1(f_x^k)\Big]), \\
    f_u^{k'} &= \mathcal{C}_2((f_u^k \odot \alpha \odot W_c^2\mathcal{C}_1(f_x^k)) + \mathrm{f_u^k}), \\
\end{aligned}
\end{equation}
For a robust feature fusion, we propose SSFM, which selectively integrates features from different modalities within a unified state space representation:
\begin{equation}
\label{eq:SSFM_input}
    F^k = \mathrm{SSFM}(W_c^1f_i^k, W_c^2f_u^{k'}),
\end{equation}
where \(W_c^1f_i^k\), \(W_c^2f_u^{k'} \in \mathbb{R}^{B \times d_{m} \times H \times W} \), and \(\{F^k\}_{k=1}^4\) providing more complete context, reducing redundancy, filtering out noise, and capturing relationships between modalities. In the decoding stage, each layer of the decoder takes \(F^k\) as a conditional input. Combined with \(p_s^5\) reconstructed using \(A_s\) and features fused through the latent space, these inputs collectively enrich the reconstruction process by providing detailed and modality-aware information.

\begin{figure*}[h]
\setlength{\abovecaptionskip}{0cm}
	\centering
	\includegraphics[width=\linewidth]{figure/CSSM.pdf}\vspace{-4mm}
	\caption{Details of our proposed CSSM.}
	\label{fig:CSSM}
	\vspace{-5mm}
\end{figure*}

\subsubsection{Details of SSFM}
\textbf{State Space Fusion Mechanism}
% Describe Vision SSM
In the vision state space model with a two-dimensional selective scan module, the feature is flattened into a sequence and scanned in four directions 
(top-left to bottom-right, bottom-right to top-left, top-right to bottom-left, and bottom-left to top-right) 
to capture the long-range dependencies of each sequence using the discrete state space equation. 
We propose the Cross State Space Model to facilitate information interaction between different sequences within the state space.


After reshape \(f_i^k, f_u^{k'}\)% \in \mathbb{R}^{B \times d_{m} \times H \times W}\) 
in \cref{eq:SSFM_input} to \(\mathbb{R}^{B \times H \times W \times d_{m}}\).We implement the vision state space module (SSM) as a residual state space block, as demonstrated by \cite{guo2024mambairsimplebaselineimage}, and utilize it as a form of long-range self-attention to process \(f_i^k\) and \(f_u^{k'}\), calculating the intra-modal correlation:
\begin{equation}
    \tilde{f}_i^k = SSM(f_i^k),\quad \tilde{f}_u^k = SSM(f_u^{k'}),\\
\end{equation}
then we process the self-modal correlation and cross-modal correlation with cross state space model (\(CSSM\)) we proposed to further fuse the bi-modals features in state space:
\begin{equation}
\label{eq:SSM-CSSM}
    \begin{aligned}
    \tilde{f}_x^k &= SSM(\mathcal{C}(\mathrm{conca}(f_i^k, f_u^{k'}))),  \\
    F_i^k &= CSSM(\tilde{f}_i^k, \tilde{f}_x^k), F_u^k = CSSM(\tilde{f}_u^k, \tilde{f}_x^k)
    \end{aligned}
\end{equation}
% Describe weighted gate mechanism gw
We utilize a weighted gate mechanism \(g_w\) to merge the transformed features as follows:
\begin{equation}
    \begin{aligned}
        F^k &= \mathcal{C}(\mathcal{C}(g_wF_i^k + \tilde{f}_x^k) + \mathcal{C}((1-g_w)F_u^{k} + \tilde{f}_x^k)), \\
        g_w &= \mathrm{Sigmoid}(\lambda_g\mathrm{conca}\Big[\delta_1,\delta_2\Big] + \mu_g), \\
        \delta_1 &= \mathcal{F}(\tilde{f}_x^k, \theta),\, \delta_2= \mathcal{F}(\tilde{f}_x^k + \delta_1, \theta).
    \end{aligned}
\end{equation}
This gate mechanism balances the contributions of \(F_i^k\) and \(F_u^{k}\) based on the guidance from \(\tilde{f}_x^k\). The function \(\delta_1=\mathcal{F}(\tilde{f}_x^k, \theta)\) and \(\delta_2=\mathcal{F}(\tilde{f}_x^k + \delta_1, \theta)\) generate intermediate signals that influence the final fused feature. The \(\mathrm{Sigmoid}\) ensures that \(g_w\) remains between 0 and 1, thus regulating the relative contributions of each path to the output \(F^k\).

\textbf{Cross State Space Model.} Let the input be \(\tilde{f}_n^k, \tilde{f}_x^k \in \mathbb{R}^{B \times H \times W \times d_m}\), where $\tilde{f}_n^k$ can be $\tilde{f}_i^k$ or $\tilde{f}_u^k$ in \cref{eq:SSM-CSSM}.
 We first apply a linear projection to extend the channel dimension of \(\tilde{f}_n^k\) and \(\tilde{f}_x^k\) to $d \times 2$ and split them along the last dimension into two parts:$\tilde{f}_n^{k'}, \tilde{f}_x^{k'} $,and $ z_n^k, z_x^k  \in \mathbb{R}^{B \times H \times W \times d}.$ 

Next, we regard \(\tilde{f}_n^{k'}, \tilde{f}_x^{k'}\) as having the shape \(\mathbb{R}^{B \times d \times H \times W}\) and apply a depthwise convolution with a kernel size of \(d_{\text{conv}}\), followed by a nonlinear activation:
\begin{equation}
        \hat{f}_n^k = \mathrm{SiLU}\!\bigl(W_c^1(\tilde{f}_n^{k'})\bigr), \,\hat{f}_x^k = \mathrm{SiLU}\!\bigl(W_c^2(\tilde{f}_x^{k'})\bigr).
\end{equation}
Here, the number of convolution groups equals the channel dimension \(d\), \(\mathrm{SiLU}\) is the activation function, and \(W_c\) means the convolutional layer. To fuse the two modalities in state space, we rewrite the \cref{eq:ZOH} and \cref{eq:discret-ssm} with:
\begin{equation}
\begin{aligned}
    \overline{A} &= \exp \bigl(\Delta_n A\bigr), \quad
    \overline{B}_n = \Delta_n B_n \\
    h^k_n &= \overline{A}h^{k-1}_n \,+\, \overline{B}_n\hat{f}_x^k, \quad
    y^k = C_nh^k_n ,
\end{aligned}
\end{equation}
where the $B_n$, $C_n$, and $\Delta_n$ mean matrices $B$, $C$, and $\Delta$ with the selective mechanism parameters $sB(\hat{f}_n^k)=\mathrm{Linear_N}(\hat{f}_n^k)$, and $sC(\hat{f}_n^k)=\mathrm{Linear_N}(\hat{f}_n^k)$.


After combining the four directional sequences, we apply a layer normalization to \(y^k\) and then multiply it elementwisely by the activation of \(z_n^k\) and \(z_x^k\):
\begin{equation}
y'^k \;=\; \mathrm{LayerNorm}(y^k) \;\odot\; \mathrm{SiLU}(z_n^k) \;\odot\; \mathrm{SiLU}(z_x^k),
\end{equation}
we map \(y'^k\) back to the desired output dimension:
\begin{equation}
Y^k \;=\; y'^k \, W_{l} \;+\; b_{l},
\end{equation}
where \(W_{l} \in \mathbb{R}^{d \times d_{m}}\), \(b_{l} \in \mathbb{R}^{d_{m}}\), and \(Y^k \in \mathbb{R}^{B \times H \times W \times d_{m}}\).


Finally, to enhance the expressive capacity of different channels, we incorporate a Channel Attention mechanism (\(CA\)) within the CSSM to reduce channel redundancy. Additionally, we employ two weighted residual connections with \(s\) and \(s' \in \mathbb{R}^{C}\) to improve the network’s robustness:
\begin{equation}
    F^k = CA(W_c(\mathrm{LayerNorm}(Y^k+s\tilde{f}_n^k)))+ s'\tilde{f}_n^k
\end{equation}

\begin{figure*}[htb]
\setlength{\abovecaptionskip}{0cm}
	\centering
	\includegraphics[width=\linewidth]{figure/IR-COMP-ROW.pdf}\vspace{-4mm}
	\caption{Qualitative results of UniCOS-I and other cutting-edge methods.}
	\label{fig:I-COMP}
	\vspace{-3mm}
\end{figure*}

\begin{table*}[htb]
		\setlength{\abovecaptionskip}{0cm} 
		\setlength{\belowcaptionskip}{-0.2cm}
		\centering
            \caption{Quantitative comparisons of UniCOS-I and other 12 SOTAs with two different type of backbones. {\color[HTML]{FF0000} \textbf{Red}} means the best results.} \label{table:CODQuanti}
            \vspace{1mm}
		\resizebox{2.08\columnwidth}{!}{
			\setlength{\tabcolsep}{1.4mm}
			\begin{tabular}{l|cccc|cccc|cccc|cccc} 
				\toprule
				\multicolumn{1}{c|}{}      & \multicolumn{4}{c|}{\textit{CHAMELEON} }                                                                                                                                         & \multicolumn{4}{c|}{\textit{CAMO} }                                                                                                                                             & \multicolumn{4}{c|}{\textit{COD10K} }                                                                                                                                          & \multicolumn{4}{c}{\textit{NC4K} }                                                                                                                        \\ \cline{2-17} 
				\multicolumn{1}{l|}{\multirow{-2}{*}{Methods}} & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   \\ \midrule %\midrule
				\multicolumn{17}{c}{CNNs-Based Methods (ResNet50 Backbone)}                                   \\ \midrule

				\multicolumn{1}{l|}{SINet~\cite{fan2020camouflaged}}                   & 0.034                                 & 0.823                                 & 0.936                                 & \multicolumn{1}{c|}{0.872}                                 & 0.092                                 & 0.712                                 & 0.804                                 & \multicolumn{1}{c|}{0.745}                                 & 0.043                                 & 0.667                                 & 0.864                                 & \multicolumn{1}{c|}{0.776}                                 & 0.058                                 & 0.768                                 & 0.871                                 & 0.808                                 \\
				\multicolumn{1}{l|}{LSR~\cite{lv2021simultaneously}}                                          & 0.030                                 & 0.835                                 & 0.935                                 & \multicolumn{1}{c|}{0.890}                                 & 0.080                                 & 0.756                                 & 0.838                                 & \multicolumn{1}{c|}{0.787}                                 & 0.037                                 & 0.699                                 & 0.880                                 & \multicolumn{1}{c|}{0.804}                                 & 0.048 & 0.802 & 0.890                                 & 0.834                                 \\
				\multicolumn{1}{l|}{SLT-Net~\cite{cheng2022implicit}}                   & 0.030                                 & 0.835                                 & 0.940                                 & \multicolumn{1}{c|}{0.887}                                 & 0.082                                 & 0.763                                 & 0.848                                 & \multicolumn{1}{c|}{0.792}                                 & 0.036                                 & 0.681                                 & 0.875                                 & \multicolumn{1}{c|}{0.804}                                 & 0.049                                 & 0.787                                 & 0.886                                 & 0.830                                 \\
				\multicolumn{1}{l|}{SegMaR-1~\cite{jia2022segment}}                                    & 0.028& 0.828                                 &  0.944 & \multicolumn{1}{c|}{{0.892}} & 0.072 &0.772 & 0.861 & 0.805 & 0.035                                 & 0.699                                 & 0.890 & \multicolumn{1}{c|}{0.813}                                 & 0.052                                 & 0.767                                 & 0.885                                 & 0.835                                 \\
				\multicolumn{1}{l|}{OSFormer~\cite{pei2022osformer}}                                     & 0.028 & 0.836 & 0.939                                 & \multicolumn{1}{c|}{0.891}                                 & 0.073                                 & 0.767                                 & 0.858                                 & \multicolumn{1}{c|}{0.799}                                 & 0.034 & 0.701 & 0.881                                 & \multicolumn{1}{c|}{0.811}                                 & 0.049                                 & 0.790                                 & 0.891 & 0.832                                 \\
	
                \multicolumn{1}{l|}{FEDER~\cite{He2023Camouflaged}}   & 0.028 & 0.850 & 0.944 & \multicolumn{1}{c|}{0.892} &  0.070 & {0.775} & 0.870 & \multicolumn{1}{c|}{0.802} & 0.032 & 0.715 & 0.892 & \multicolumn{1}{c|}{0.810} & {{0.046}} & {{0.808}} & {{0.900}} & {{0.842}} \\
                \multicolumn{1}{l|}{FGANet~\cite{zhaiexploring}}                                        & 0.030                                 & 0.838                                 & 0.945                                 & 0.891                                 & 0.070  & 0.769  & 0.865  & \multicolumn{1}{c|}{0.800}  & 0.032   & 0.708 & 0.894  & \multicolumn{1}{c|}{0.803}                                 & 0.047                                & 0.800                                 & 0.891                                 & 0.837                                 \\
                \multicolumn{1}{l|}{FocusDiff~\cite{zhao2025focusdiffuser}}  & 0.028 & 0.843 & 0.938 & 0.890  & {\color[HTML]{FF0000} \textbf{0.069}} & 0.772 & {\color[HTML]{FF0000} \textbf{0.883}} &0.812 &0.031 &0.730 &0.897 & 0.820 &0.044 &0.810 &0.902 &0.850    \\
                
                \multicolumn{1}{l|}{FSEL~\cite{sun2025frequency}}  & 0.029 & 0.847 & 0.941 &0.893  & {\color[HTML]{FF0000} \textbf{0.069}} & 0.779 &0.881 & {\color[HTML]{FF0000} \textbf{0.816}} & 0.032 & 0.722 & 0.891 &0.822 & 0.045 & 0.807 & 0.901 & 0.847    \\ 
                
                \rowcolor{c2!20} \multicolumn{1}{l|}{UniCOS-I (Ours) }
                &{\color[HTML]{FF0000} \textbf{0.024}} &{\color[HTML]{FF0000} \textbf{0.866}} &{\color[HTML]{FF0000} \textbf{0.951}} &{\color[HTML]{FF0000} \textbf{0.902}} 
                
                &{\color[HTML]{FF0000} \textbf{0.069}} &{\color[HTML]{FF0000} \textbf{0.787}} &0.878 &{\color[HTML]{FF0000} \textbf{0.816}} 
                
                &{\color[HTML]{FF0000} \textbf{0.029}} &{\color[HTML]{FF0000} \textbf{0.757}} &{\color[HTML]{FF0000} \textbf{0.905}} &{\color[HTML]{FF0000} \textbf{0.839}} 
                
                &{\color[HTML]{FF0000} \textbf{0.042}} &{\color[HTML]{FF0000} \textbf{0.820}} &{\color[HTML]{FF0000} \textbf{0.910}} &{\color[HTML]{FF0000} \textbf{0.857}} \\
                \midrule
                \multicolumn{17}{c}{Transformer-Based Methods (PVTv2 Backbone)}\\
                \midrule
				\multicolumn{1}{l|}{HitNet~\cite{hu2022high}}                                  & 0.024                                 & 0.861                                 & 0.944                                 & \multicolumn{1}{c|}{0.907}                                 & 0.060                                 & 0.791                                 & 0.892                                 & \multicolumn{1}{c|}{0.834}                                 & 0.027                                 & 0.790                                 & 0.922                                 & \multicolumn{1}{c|}{0.847}                                 & 0.042                                 & 0.825                                 & 0.911                                 & 0.858                                 \\
                \multicolumn{1}{l|}{DaCOD~\cite{wang2023depth} }                &0.026 &0.829 &0.939 &0.893 &0.051 &0.831 &0.905 &0.855 &0.028 &0.740 &0.907 &0.840 &0.035 &0.833 &0.924 &0.874\\
                \multicolumn{1}{l|}{RISNet~\cite{wang2024depth} }                &--- &--- &--- &---  &0.050 &0.844 &0.922 &{\color[HTML]{FF0000} \textbf{0.870}} &0.025 &0.804  &0.931 &0.873 &0.037  &0.851 &0.925 &0.882 \\

                \rowcolor{c2!20} \multicolumn{1}{l|}{UniCOS-I (Ours)} &{\color[HTML]{FF0000} \textbf{0.019}} &{\color[HTML]{FF0000} \textbf{0.884}} &{\color[HTML]{FF0000} \textbf{0.962}} &{\color[HTML]{FF0000} \textbf{0.920}} &{\color[HTML]{FF0000} \textbf{0.048}} &{\color[HTML]{FF0000} \textbf{0.845}} &{\color[HTML]{FF0000} \textbf{0.923}} &{\color[HTML]{FF0000} \textbf{0.870}} &{\color[HTML]{FF0000} \textbf{0.021}} &{\color[HTML]{FF0000} \textbf{0.809}} &{\color[HTML]{FF0000} \textbf{0.933}} &{\color[HTML]{FF0000} \textbf{0.874}} &{\color[HTML]{FF0000} \textbf{0.032}} &{\color[HTML]{FF0000} \textbf{0.859}} &{\color[HTML]{FF0000} \textbf{0.932}} &{\color[HTML]{FF0000} \textbf{0.887}} \\ 
    \bottomrule            
		\end{tabular}}
		\vspace{-4mm} 
\end{table*}


\begin{figure*}[htb]
	\centering
    \begin{minipage}[c]{0.497\textwidth}
    	\centering
        \includegraphics[width=\linewidth]{figure/P-COMP.pdf}\vspace{-4mm}
	\caption{Visual comparison on RGB-P COS task.}
	\label{fig:P-COMP}
    \end{minipage}
    \begin{minipage}[c]{0.497\textwidth}
    	\centering
        \includegraphics[width=\linewidth]{figure/D-COMP.pdf}\vspace{-4mm}
	\caption{Visual comparison on RGB-D COS task.}
	\label{fig:IR-COMP}
    \end{minipage}
    \vspace{-5mm}
\end{figure*}

\subsubsection{Replaceable Segmentation Decoder}

As our MultiModal Segmentation-Oriented Encoder employs a plug-and-play design, the decoder in UniSEG can be substituted with any decoder that utilizes a coarse result or latent map and skip connections as inputs. 

In our implementation, we default to using a multi-task segmentation decoder, such as ICEG \cite{he2023strategic}. 
This decoder features separate task heads for segmentation and edge reconstruction at each layer, with edge reconstruction providing additional supervision. 
The decoding process can be formulated as follows:
\begin{equation}
    \{p_s^k\}_{k=1}^4,\{p_e^k\}_{k=1}^4=\mathcal{D}(p_s^5, \{F^k\}_{k=1}^4),
\end{equation}
where \(\mathcal{D}\) represents the decoder, \(\{p_s^k\}_{k=1}^4\) and \(\{p_e^k\}_{k=1}^4\) denote the segmentation results and reconstructed edges.

\subsubsection{Optimization}

As a unified plug-and-play method, our MultiModal Segmentation-Oriented Encoder with multi-space fusion can easily integrate with most non-specialized input design decoders.
Here, we use the multi-task segmentation decoder, which we employ as the default, as an example.

Our UniSEG employs the weighted intersection-over-union loss \(L_{I}\), the weighted binary cross-entropy loss \(L_{B}\) to constrain the segmentation results \(\{p_s^k\}_{k=1}^5\), and the dice loss \(L_{D}\) to supervise the edge reconstruction results \(\{p_e^k\}_{k=1}^4\). Let the segmentation \(\mathbf{y_s}\) and edge \(\mathbf{y_e}\) as ground-truth, the total loss of UniSEG can be presented as:
\begin{equation}
\begin{aligned}
L_{\mathcal{S}}&=\sum\nolimits_{k=1}^5\frac{1}{2^{k-1}}\left(L_{B}\left(p_s^k,\mathbf{y_s}\right)+L_{I}\left(p_s^k,\mathbf{y_s}\right)\right)\\&+\sum\nolimits_{k=1}^4\frac{1}{2^{k-1}}L_{D}\left(p_e^k,\mathbf{y_e}\right).
\end{aligned}
\end{equation}


\subsection{UniLearner: Cross-Modal Knowledge Learning}
UniLearner \(\mathcal{L}\) is a plug-in encoder-decoder-like network. When the COS dataset lacks corresponding multimodal data, UniLearner enables learning the mapping between images and modalities by introducing additional non-COS multimodal datasets, thereby aiding the COS task.

Specifically, we denote the images of the introduced additional dataset as \(\mathbf{e_i}\) and the corresponding additional modal data as \(\mathbf{e_u}\). We expect \(\mathcal{L}\) to learn the mapping relationship between them and obtain:
\begin{equation}
    \mathbf{\dot{e_u}} = \mathcal{L}(\mathbf{e_i}), \quad \mathbf{\dot{e_u}} \rightarrow \mathbf{e_u}.
\end{equation}
When working in collaboration with UniSEG, UniLearner inputs the image \(\mathbf{x_i}\) and, through the encoding and decoding process, obtains the corresponding pseudo-modality \(\mathbf{x_u}\) as well as the latent vector \(z_{\mathbf{i}\rightarrow\mathbf{u}}\) that embodies the knowledge of mapping between image and modality:
\begin{equation}
        \mathbf{x_u} = \mathcal{L}_\mathcal{D}(z_{\mathbf{i}\rightarrow\mathbf{u}})
,\quad z_{\mathbf{i}\rightarrow\mathbf{u}} = \mathcal{L}_\mathcal{E}(\mathbf{x_i}),
\end{equation}
where \(\mathcal{L}_\mathcal{E}\) and \(\mathcal{L}_\mathcal{D}\) are the encoder and decoder of \(\mathcal{L}\), \(z_{\mathbf{i}\rightarrow\mathbf{u}}\) means the latent vector which contains the knowledge of the map from \(\mathbf{x_i}\) to \(\mathbf{x_u}\). 

To integrate the \(z_{\mathbf{i}\rightarrow\mathbf{u}}\) to guide the segment process, we inject it to UniSEG at \(k=4\) by replacing the LSFM(\cref{eq:LSFM}) with a new formula:
\begin{equation}
\begin{aligned}
f_x^4 = &f_i^4 \odot \mathrm{Sigmoid}(W_c^1 \mathcal{C}(FFM(f_u^4, z_{\mathbf{i}\rightarrow\mathbf{u}}))) \\ 
&+ W_c^2 \mathcal{C}(FFM(f_u^4, z_{\mathbf{i}\rightarrow\mathbf{u}})),
\end{aligned}
\end{equation}
This operation integrates the mapping information between image and pseudo-modality, along with the semantic information extracted from both modalities, into the latent space. This unified representation strengthens the segmentation by leveraging complementary cross-modal knowledge.

\subsubsection{Optimization}
When employing UniLearner, we perform joint training of UniLearner and UniSEG, optimizing the parameters of both networks using a shared optimizer. To enable UniLearner to learn the mapping between \(\mathbf{e_i}\) and \(\mathbf{e_u}\), we utilize an L1 norm loss, formulated as:
\begin{equation}
    L_{\mathcal{L}} = ||\dot{e_u} - e_u||_1
\end{equation}
The total loss \(L_t\) for this joint training setup is expressed as: 
\begin{equation}
    L_t = L_{\mathcal{S}} + L_{\mathcal{L}}
\end{equation}

\begin{table*}[htb]
\centering
\setlength{\abovecaptionskip}{0cm}
\caption{Results on RGB-Depth COS. All the methods trained with source-free depth provided by \cite{wu2023source}}\label{Table:DCOD-Quanti}
\resizebox{2.08\columnwidth}{!}{ 
\setlength{\tabcolsep}{1.4mm}
\begin{tabular}{l|cccc|cccc|cccc|cccc} 
            \toprule
            \multicolumn{1}{c|}{} &\multicolumn{4}{c|}{\textit{CHAMELEON} }   & \multicolumn{4}{c|}{\textit{CAMO} }      & \multicolumn{4}{c|}{\textit{COD10K} }     & \multicolumn{4}{c}{\textit{NC4K} }         \\ 
            \cline{2-17} 
            \multicolumn{1}{l|}{\multirow{-2}{*}{Methods}}  & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F^{x}_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E^{x}_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                  & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F^{x}_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E^{x}_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F^{x}_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E^{x}_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F^{x}_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E^{x}_\phi$~$\uparrow$}                               & \multicolumn{1}{c}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   \\ 
            \midrule 
            CDINet \cite{zhang2021cross}  &0.036 &0.787 &0.903 &0.879    &0.100 &0.638 &0.766 &0.732    &0.044 &0.610 &0.821 &0.778     &0.067 &0.697 &0.830 &0.793 \\
            DCMF \cite{wang2022learning}  &0.059 &0.807 &0.853 &0.830      &0.115 &0.737 &0.757 &0.728    &0.063 &0.679 &0.776 &0.748     &0.077 &0.782 &0.820 &0.794 \\ 
            SPSN \cite{lee2022spsn}  &0.032 &0.866 &0.932 &0.887      &0.084 &0.782 &0.829 &0.773    &0.042 &0.727 &0.854 &0.789     &0.059 &0.803 &0.867 &0.813 \\ 
            DCF \cite{ji2021calibrated}   &0.037 &0.821 &0.923 &0.850    &0.089 &0.724 &0.834 &0.749    &0.040 &0.685 &0.864 &0.766     &0.061 &0.765 &0.878 &0.791 \\ 
            CMINet \cite{zhang2021rgb} &0.032 &0.881 &0.930 &0.891    &0.087 &0.798 &0.827 &0.782    &0.039 &0.768 &0.868 &0.811     &0.053 &0.832 &0.888 &0.839 \\ 
            SPNet \cite{zhou2021specificity} &0.033 &0.872 &0.930 &0.888 &0.083 &0.807 &0.831 &0.783    &0.037 &0.776 &0.869 &0.808     &0.054 &0.828 &0.874 &0.825 \\ 
            PopNet \cite{wu2023source} &0.022 &0.893 &0.962 &0.910  &0.073 &0.821 &0.869 &0.806    &0.031 &0.789 &0.897 &0.827     &0.043 &0.852 &0.908 &0.852 \\ 
   
            DSAM \cite{yu2024exploring} & 0.028 & 0.877 & 0.957 & 0.883  &0.061 &0.834 &0.920 &0.832    &0.033 &{\color[HTML]{FF0000} \textbf{0.807}} &0.931 &0.846     &0.040 &0.862 &0.940 &0.871 \\              
            \rowcolor{c2!20}UniCOS-D &{\color[HTML]{FF0000} \textbf{0.020}} &{\color[HTML]{FF0000} \textbf{0.901}} &{\color[HTML]{FF0000} \textbf{0.965}} &{\color[HTML]{FF0000} \textbf{0.918}}   & {\color[HTML]{FF0000} \textbf{0.049}} &{\color[HTML]{FF0000} \textbf{0.853}} &{\color[HTML]{FF0000} \textbf{0.923}} &{\color[HTML]{FF0000} \textbf{0.866}}    &{\color[HTML]{FF0000} \textbf{0.022}} &{\color[HTML]{FF0000} \textbf{0.807}} &{\color[HTML]{FF0000} \textbf{0.932}} &{\color[HTML]{FF0000} \textbf{0.871}}     &{\color[HTML]{FF0000} \textbf{0.033}} &{\color[HTML]{FF0000} \textbf{0.872}} &{\color[HTML]{FF0000} \textbf{0.943}} &{\color[HTML]{FF0000} \textbf{0.882}}\\ 
            
            \bottomrule
    \end{tabular}}\vspace{-5mm}
\end{table*}


\begin{table}[htb]
    \setlength{\abovecaptionskip}{0cm} 
	\setlength{\belowcaptionskip}{-0.2cm}
	\centering
        \caption{Results on RGB-Polarization COS.}
        \label{table:PCODQuanti}
	\resizebox{1\columnwidth}{!}{ 
\setlength{\tabcolsep}{2.7mm}
\begin{tabular}{l|cccc}\toprule 
        Methods   & \cellcolor{gray!40}$M \downarrow$ & \cellcolor{gray!40}$F_{\beta}^m \uparrow$ &\cellcolor{gray!40} $E_{\phi} \uparrow$& \cellcolor{gray!40}$S_{\alpha} \uparrow$\\
        \midrule
        SINet-V2 \cite{fan2021concealed}     & 0.013 & 0.819 & 0.941 & 0.882\\
        OCENet \cite{Liu_2022_WACV}       & 0.013 & 0.827  & 0.945 & 0.883\\
        ZoomNet \cite{pang2022zoom}      & 0.010 & 0.842 & 0.922 & 0.897\\
        BSANet \cite{zhu2022can}        & 0.011 & 0.861 & 0.945 & 0.903\\
        ERRNet \cite{ji2022fast}         & 0.023 & 0.704 & 0.901 & 0.833\\
        C2FNet-V2 \cite{chen2022camouflaged}  & 0.012 & 0.845 & 0.945 & 0.895\\
        PGSNet \cite{mei2022glass}        & 0.010 & 0.868 & 0.965 & 0.916\\
        CMX \cite{zhang2023cmx}         & 0.009 & 0.876 & 0.965 & 0.922\\
        DaCOD \cite{wang2023depth} &0.011 &0.846 &0.959 &0.899 \\
        IPNet \cite{wang2024ipnet}      & 0.008 & 0.882 & 0.970 & 0.922\\
        RISNet \cite{wang2024depth} &0.007 &0.904 &0.971 &0.933 \\
        \rowcolor{c2!20}UniCOS-P          & {\color[HTML]{FF0000} \textbf{0.006}}  & {\color[HTML]{FF0000} \textbf{0.910}} & {\color[HTML]{FF0000} \textbf{0.975}}  & {\color[HTML]{FF0000} \textbf{0.937}} \\
		 \bottomrule  \end{tabular}
    }
    \vspace{-5mm}
\end{table}


\section{Experiments}
We evaluated the performance of our method across three multimodal COS tasks: RGB-Infrared (RGB-I), RGB-Depth (RGB-D), and RGB-Polarization (RGB-P). 
For the RGB-Infrared task (UniCOS-I), we utilized datasets unrelated to the COS task to showcase UniLearner's ability to leverage non-relevant data for improving COS task performance. 
In the RGB-Depth task (UniCOS-D), pseudo-depth data was employed, while in the RGB-Polarization task (UniCOS-P), real degree of linear polarization (DoLP) data was used. 
This experimental setup allowed us to comprehensively evaluate UniSEG's performance and robustness when applied to both pseudo and real multimodal data.

For UniLearner, we utilize a simple ResUNet with 9 residual blocks as the backbone. For UniSEG, we adopt PVTv2 \cite{wang2022pvt} pre-trained on ImageNet \cite{deng2009imagenet} as our default backbone. We also report results on ResNet50 \cite{he2016deep} for fair comparison. Details on implementation, datasets and metrics are in \cref{sec:impl}, \cref{sec:datasets} and \cref{sec:metrics}. All results are evaluated with consistent task-specific evaluation tools.


\begin{table*}[htb]
\centering
	\setlength{\abovecaptionskip}{0.1cm}
	\begin{minipage}{.586\textwidth}
		\centering
		\setlength{\abovecaptionskip}{0cm}
		\caption{Effect of our UniSEG: $\mathcal{E}_u$ and $\mathcal{E}_i$ represent the extra modality and RGB image decoders, respectively, each equipped with corresponding fusion modules.
		}
        \label{table:abl_uniseg}
		\resizebox{\columnwidth}{!}{
			\setlength{\tabcolsep}{1mm}
			\begin{tabular}{c|c|ccccc|c}
\toprule
\multirow{2}{*}{Metrics} &w/o & \multicolumn{5}{c|}{Effect of UniSEG} &\cellcolor{c2!20}{UniCOS-D} \\ 
\cline{3-7}

&$\mathcal{E}_u$ 
&w/o $\mathcal{E}_i$  
&w/o SSFM 
&w/o CSSM 
&w/o LSFM
& \multicolumn{1}{c|}{w/o FFM }    
&\cellcolor{c2!20}{(Ours)} 
 \\ \midrule
$M$~$\downarrow$  &0.025  &0.059  &0.024  &0.023  &0.021 & 0.021 &\cellcolor{c2!20}\textbf{0.022} \\

$F_\beta$~$\uparrow$ &0.770  &0.579  &0.792  &0.798 &0.802 & 0.812  &\cellcolor{c2!20}\textbf{0.807} \\

$E_\phi$~$\uparrow$ &0.923  &0.785  &0.927  &0.931  &0.934 & 0.937&\cellcolor{c2!20}\textbf{0.932} \\

$S_\alpha$~$\uparrow$ &0.867  &0.713  &0.873  &0.876 &0.877 & 0.880 &\cellcolor{c2!20}\textbf{0.871} \\
\bottomrule
		\end{tabular}}\label{table:small-object}
		% \vspace{-0.1cm}
	\end{minipage} 
    \begin{minipage}{.409\textwidth}
		    \centering
		\setlength{\abovecaptionskip}{0cm}
		\caption{Effect of our UniLearner. Know-Inject means the process of integrate \(z_{i\rightarrow u}\) to guide the segmentation. 
		}
        \label{table:abl_unilearner}
		\resizebox{\columnwidth}{!}{
			\setlength{\tabcolsep}{1mm}
\begin{tabular}{c|cc|c}
\toprule
\multirow{2}{*}{Metrics}  & \multicolumn{2}{c|}{Effect of UniLearner}       &\cellcolor{c2!20}{UniCOS-I} \\ 
\cline{2-3}
& w/o Know-Inject  
& \multicolumn{1}{c|}{only Know-Inject}  
&  \cellcolor{c2!20} (Ours)                  \\  
\midrule
$M$~$\downarrow$    &0.024  &0.023  &\cellcolor{c2!20}\textbf{0.021} \\

$F_\beta$~$\uparrow$   &0.792  &0.795  &\cellcolor{c2!20}\textbf{0.809} \\

$E_\phi$~$\uparrow$   &0.927  &0.929  &\cellcolor{c2!20}\textbf{0.933} \\

$S_\alpha$~$\uparrow$   &0.869  &0.873  &\cellcolor{c2!20}\textbf{0.874} \\
 \bottomrule
		\end{tabular}}\label{table:Multi-object} 
		\end{minipage}
        \vspace{-5mm}
\end{table*}

\begin{table*}[htb]
		\setlength{\abovecaptionskip}{0cm} 
		\setlength{\belowcaptionskip}{-0.2cm}
		\centering
            \caption{
            Ablation study on applying our modules to other COS methods. The modules proposed in UniSEG can easily transform a single-modal COS method into a multimodal approach, enhancing performance using UniLearner and multimodal data unrelated to COS.} \label{table:Abl-Plugin}
            \vspace{1mm}
		\resizebox{2.08\columnwidth}{!}{
			\setlength{\tabcolsep}{1.4mm}
			\begin{tabular}{l|cccc|cccc|cccc|cccc} 
				\toprule
				\multicolumn{1}{c|}{}      & \multicolumn{4}{c|}{\textit{CHAMELEON} }                                                                                                                                         & \multicolumn{4}{c|}{\textit{CAMO} }                                                                                                                                             & \multicolumn{4}{c|}{\textit{COD10K} }                                                                                                                                          & \multicolumn{4}{c}{\textit{NC4K} }                                                                                                                        \\ \cline{2-17} 
				\multicolumn{1}{l|}{\multirow{-2}{*}{Methods}} & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   & {\cellcolor{gray!40}$M$~$\downarrow$}                                  & {\cellcolor{gray!40}$F_\beta$~$\uparrow$}                               & {\cellcolor{gray!40}$E_\phi$~$\uparrow$}                               & \multicolumn{1}{c}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}                                   \\ \midrule 
				\multicolumn{17}{c}{Modify Native Single-Modal Method with Our Work}                                   \\ \midrule
				\multicolumn{1}{l|}{FEDER~\cite{He2023Camouflaged}}   & 0.028 & 0.850 & 0.944 & \multicolumn{1}{c|}{0.892} &  0.070 & {0.775} & 0.870 & \multicolumn{1}{c|}{0.802} & 0.032 & 0.715 & 0.892 & \multicolumn{1}{c|}{0.810} & {{0.046}} & {{0.808}} & {{0.900}} & {{0.842}} \\

                \multicolumn{1}{l|}{FEDER in UniSEG-D} &0.026 &0.852 &0.950 &0.902 &0.070 &0.779 &0.871 &0.810  &0.031 &0.739 &0.902 &0.838 &0.043 &0.806 &0.907 &0.855
                \\ 
                \multicolumn{1}{l|}{FEDER in UniCOS-I} &\textbf{0.026} &\textbf{0.858} &\textbf{0.959} &\textbf{0.904} &\textbf{0.069} &\textbf{0.783} &\textbf{0.873} &\textbf{0.816}  &\textbf{0.030} &\textbf{0.743} &\textbf{0.903} &\textbf{0.839} &\textbf{0.042} &\textbf{0.813} &\textbf{0.909} &\textbf{0.856}\\
                
                \midrule  
				\multicolumn{17}{c}{Modify Native Multimodal Method with Our Work}                                   \\ \midrule
                DaCOD~\cite{wang2023depth} &0.026 &0.829 &0.939 &0.893 &0.051 &0.831 &0.905 &0.855 &0.028 &0.740 &0.907 &0.840 &0.035 &0.833 &0.924 &0.874\\
                DaCOD in UniCOS-D &0.024 &0.857 &0.945 &0.904 &\textbf{0.050} &0.836 &0.910 &0.861  &0.026 &0.771 &0.925 &0.849 &\textbf{0.034} &0.840 &0.927 &0.878\\
                DaCOD in UniCOS-I &\textbf{0.023} &\textbf{0.865} &\textbf{0.951} &\textbf{0.908} &\textbf{0.050} &\textbf{0.839} &\textbf{0.917} &\textbf{0.863}  &\textbf{0.025} &\textbf{0.783} &\textbf{0.929} &\textbf{0.856} &\textbf{0.034} &\textbf{0.847} &\textbf{0.930} &\textbf{0.882}\\ 
    \bottomrule            
		\end{tabular}}
		\vspace{-5mm} 
	\end{table*}


\subsection{Quantitative and Qualitative Results}

\noindent \textbf{RGB and Task-Unrelated Infrared Data.} 
As shown in \cref{table:CODQuanti}, our UniCOS-I method outperforms all 12 state-of-the-art approaches across various datasets. 
The superior visual performance is further illustrated in \cref{fig:I-COMP}, where UniCOS-I generates more complete and coherent segmentation maps compared to other leading methods, underscoring the effectiveness of our approach in integrating multimodal data. 
Furthermore, as depicted in \cref{fig:IR}, the joint training of UniSEG and UniLearner significantly enhances RGB-to-Infrared reconstruction performance. 
This demonstrates UniLearner's ability to effectively address the semantic complexities inherent in RGB-Infrared data, which often challenge traditional end-to-end image translation methods.



\textbf{Paired RGB and Pseudo-Depth Data.} 
In the RGB-D task, our UniCOS-D model leverages pseudo-depth data paired with RGB images to effectively address the challenges of camouflaged object segmentation. 
Quantitative results presented in \cref{Table:DCOD-Quanti} demonstrate that UniCOS-D outperforms competing methods, achieving the highest scores across all evaluated metrics. 
Additionally, visual comparisons in \cref{fig:IR-COMP} highlight UniCOS-D's capability to clearly distinguish foreground objects from their surroundings. 
Even in scenarios with minimal depth cues, as shown in the first row of \cref{fig:IR-COMP}, UniCOS-D consistently delivers superior segmentation performance. 
These results show the robustness of our approach and its effectiveness under challenging conditions.

\textbf{Paired RGB and Real Polarization Data.}
For the RGB-P task, our UniCOS-P model demonstrates exceptional performance by integrating real DoLP data with RGB imagery to improve the detection of camouflaged objects. 
As detailed in \cref{table:PCODQuanti}, UniCOS-P achieves superior results on the PCOD1200 dataset. 
By leveraging polarization cues, the model uncovers details that are otherwise imperceptible to traditional RGB sensors. 
These cues are critical for precisely delineating object boundaries, as visually illustrated in \cref{fig:P-COMP}, where UniCOS-P excels in segmenting subtle features and defining edges with remarkable precision. 
The success of UniCOS-P in these complex scenarios highlights the significant advantages of incorporating real polarization data, enabling the detection of objects that would otherwise remain concealed in traditional imaging systems.


\subsection{Ablation Study}
We conduct ablation studies on \textit{COD10K} of the COD task.

\noindent \textbf{Effect of UniSEG}. 
As illustrated in \cref{table:abl_uniseg}, UniSEG significantly improves segmentation by integrating multimodal data. 
The absence of the extra modality encoder $\mathcal{E}_u$ or the image encoder $\mathcal{E}_i$ significantly reduces segmentation accuracy, underlining their essential roles. 
Moreover, removing the state space based fusion mechanisms such as SSFM or CSSM, or the LSFM, detrimentally affects performance metrics. 
This confirms the critical nature of these components in enhancing robustness and accuracy. The omission of the FFM also leads to performance decreases, showcasing its role in optimizing feature integration across stages.

 

\noindent \textbf{Effect of UniLearner}. Referencing \cref{table:abl_unilearner}, UniLearner enhances camouflaged object segmentation through the utilization of cross-modal knowledge. 
Disabling the 'Knowledge Injection' process, which involves integrating the latent vector $z_{i\rightarrow u}$, results in a noticeable decline in all metrics. 
This validates UniLearner's efficacy in using extra multimodal data to improve the segmentation of camouflaged objects, enhancing both the accuracy and consistency of segmentation results across various datasets.


\noindent \textbf{Generalization of UniCOS}. 
As demonstrated in \cref{table:Abl-Plugin}, when we modify the single-modal method, FEDER, to a multimodal method using our UniCOS-D approach, it leads to improved performance. 
Further improvements are observed when we enhance both the modified FEDER and the original multimodal method DaCOD with our UniCOS-I scheme that incorporates UniLearner. 
This progression underscores the effectiveness of our approach in utilizing multimodal data and demonstrates the robust and generalization of our methods to serve as a plug-and-play framework in significantly boosting the performance of COS tasks.


\section{Conclusions}

This work introduces UniCOS for MCOS task. 
UniCOS comprises UniSEG, a multimodal segmentor, and UniLearner, a cross-modal knowledge learning plugin, which cooperatively enhances segmentation accuracy. 
UniSEG utilizes an SSFM and an LSFM to integrate cross-modal features each layer, along with an FFM to guide the encoding of subsequent layers, improving contextual understanding and reducing susceptibility to noise.
Simultaneously, UniLearner leverages multimodal data unrelated to the COS task to refine model segmentation capabilities by generating pseudo-modal content and learning cross-modal semantic knowledge.
Our evaluations demonstrate that UniCOS outperforms existing MCOS approaches.


\newpage

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
\section{Methodology}


\section{Experiments}
\subsection{Experimental Settings}

\subsubsection{Implementation Details.} \label{sec:impl}  We implement our method in PyTorch and train our model on four RTX 4090 GPUs. We use the Adam optimizer with a learning rate of $1e-4$ and a batch size of 16. The input image size is $448 \times 448$, following \cite{wang2023depth}. We train the model for 160 epochs, with the learning rate gradually decaying to 5e-6. $d_m$ is set as 96, $d$ is set as 192, and \(d_{\text{conv}}\) is set as 3.

\subsubsection{Datasets} 
\label{sec:datasets}
Except for the RGB-P task, we employ the CHAMELEON \cite{skurowski2018animal}, CAMO \cite{le2019anabranch}, COD10K \cite{fan2021concealed}, and NC4K \cite{lv2021simultaneously} datasets for our evaluation. We follow the common setting of previous work, combining 3,040 pairs from COD10K with 1,000 pairs from CAMO to the training set. 
\begin{itemize}
    \item In the RGB-D task, to evaluate the performance of our methods under paired RGB with pseudo-modal data. we adopt the pseudo-depth map used in PopNet \cite{wu2023source} and DSAM \cite{yu2024exploring}, which paired with above four dataset, to fair comparison.
    \item In the RGB-I task, to evaluate our UniCOS in the scenario where an extra modality is missing, unlike the RGB-D task that uses a pseudo-depth map, we utilize the M3FD-Fusion dataset \cite{liu2022target} to allows our UniLearner to learn and leverage cross-modal knowledge from the task unrelated RGB-Infrared data.
\end{itemize}
For the RGB-P task, we use the PCOD1200 dataset \cite{wang2024ipnet} to evaluate our methods in the scenario with real multimodal data. This dataset contains 1,200 manually annotated pairs of RGB and DoLP (Degree of Linear Polarization) images. It is divided into 970 pairs for training and 230 pairs for testing. 


\subsubsection{Metrics} 
\label{sec:metrics}
We use the different metrics on different tasks to fairly compare with previous works with the tasks common settings. 
The metrics we used include Mean Absolute Error (M), max F-measure ($F^x_\beta$), mean F-measure ($F^m_\beta$), adaptive F-measure ($F_\beta$), mean E-measure ($E_\phi$), max E-measure ($E^x_\phi$) and Structure Similarity ($S_\alpha$).


\section{Limitations and Future Works} 
While UniCOS has achieved outstanding results in various RGB-X COS tasks, two limitations remain. 

\textit{1) The Bias Between UniLearner and Modal Translation:} 
UniLearner is designed to capture associative knowledge and mapping relationships between RGB and additional modalities, primarily to guide the UniSEG segmentation network. 
Its focus is not on generating highly precise pseudo-modal information, which may result in outputs that deviate from traditional modality translation expectations. 
Further research is needed to improve the interpretability of these generative mechanisms and understand their contribution to segmentation performance.
 

\textit{2) Restricted Segmentation in Dual-Modal Scenarios:} 
At present, the application of SSMs and UniSEG in MCOS is confined to dual-modality setups employing a dual-encoder architecture. 
However, leveraging the robust capabilities of SSMs in capturing long-range contextual dependencies, the framework holds promise for extension to support additional modalities, such as triple modalities or beyond, which could significantly enhance segmentation performance.


To further enhance segmentation performance, future efforts could focus on jointly fine-tuning existing pre-trained pre-processing models \cite{fang2024real,he2023reti,zhang2024unified}, translation networks \cite{fang2023joint}, refinement models \cite{ahn2021refining}, even the generative model \cite{zhu2024multibooth,zhu2024instantswap,wang2024taming,
  wang2024cove,he2024diffusion} alongside segmentation models \cite{xiao2024survey}, aiming to simultaneously improve the performance of both components. 
Additionally, leveraging multitask guidance to enhance RGB-X image translation, particularly for tasks that are challenging for conventional image-to-image translation methods, which emerges as a promising avenue for future research.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
 

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
