\section{Related Works}
\noindent \textbf{Camouflaged Object Segmentation}. Recent studies on COS have progressed using techniques such as multi-scale \cite{pang2024zoomnext}, multi-space \cite{zhong2022detecting, sun2025frequency}, multi-stage \cite{jia2022segment}, and biomimetic strategies \cite{he2023strategic}, which focus on enhancing information extraction from camouflaged images. Despite these advancements, most methods still rely on single-modal inputs, which limits the potential of multimodal data due to challenges in acquiring paired multimodal data with camouflaged samples. Advances in depth estimation have encouraged the integration of depth data, underscoring the benefits of multimodal approaches \cite{xiang2022exploringdepthcontributioncamouflaged, wu2023source, yu2024exploring, wang2024depth, wang2023depth}. However, research into RGB-to-X modal translation for other modalities is still limited, which restricts the advancement of additional modality-assisted COD tasks.

To address this issue, we propose UniLearner to learns and utilizes cross-modal information between images and various modalities to enhance MCOS performance. By embedding a cross-modal semantic vector into the segmentor and leveraging existing non-camouflaged multimodal data% or pseudo-modal data generated by current methods
, this framework improves COS performance when real multimodal datasets with camouflaged objects are unavailable.

\noindent \textbf{State Space Models}. Rooted in classical control theory \cite{10.1115/1.3662552}, State Space Models (SSMs) are essential for analyzing continuous long-sequence data. The Structured State Space Sequence Model (S4) \cite{gu2022efficientlymodelinglongsequences} initially modeled long-range dependencies, recently, Mamba \cite{gu2024mambalineartimesequencemodeling, xiao2025mambatree} introduced a selection mechanism  that enables the model to extract relevant information from the inputs. Mamba has been applied effectively in image restoration \cite{guo2024mambairsimplebaselineimage, li2024fouriermambafourierlearningintegration, yang2024learning, zheng2024fd, zheng2024u}, segmentation \cite{wang2024mamba, xing2024segmamba}, and other domains \cite{zhang2024motion, zubic2024state}, achieving competitive results. 
In the context of image fusion, approaches like MambaDFuse \cite{li2024mambadfusemambabaseddualphasemodel} and FusionMamba \cite{xie2024fusionmambadynamicfeatureenhancement} have leveraged Mamba to improve performance. However, these methods utilize SSMs only for feature extraction, neglecting the cross-modal state space features and Mambaâ€™s selection capabilities across different modal features in a unified state space.
To address this, we propose a universal State Space Fusion Mechanism that integrates and selectively extracts features across modalities within a unified state space, enhancing MCOS performance.

\begin{figure*}[h]
\setlength{\abovecaptionskip}{0cm}
	\centering
	\includegraphics[width=\linewidth]{figure/Framework.pdf}\vspace{-4mm}
	\caption{Framework of our UniCOS, and the details of FFM, LSFM, \(g_w\), and SSFM. The modules outlined by dashed lines mean the modules introduced by UniLearner, which can be omitted when using paired RGB-X data.
    }
	\label{fig:Framework}
	\vspace{-5mm}
\end{figure*}