\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins 

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{latexsym}

\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{times}
\usepackage{soul}
\usepackage{cite}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{bm}
% \usepackage{bm}

% \usepackage[font=small]{caption}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage[switch]{lineno}
\usepackage{color}

% \usepackage[small]{caption}


\usepackage{algpseudocode}
\algnewcommand{\Initialize}[1]{%
  \State \textbf{Initialize:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\algnewcommand{\Inputs}[1]{%
  \State \textbf{Inputs:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

\input{macros.tex}



\algnewcommand{\Try}[1]{%
  \State \textbf{Try:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

% New "environments"
\algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}%
\algtext*{EndSwitch}%
\algtext*{EndCase}%
\newcommand{\bluetext}[1]{{\color{blue}#1}}


\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newtheorem{problem}{Problem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{objective}{Objective}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\newcommand{\marta}[1]{\rm {\raggedright\color{magenta}\textsf{MK: #1}\marginpar{$\star$}}}   

\newcommand{\pian}[1]{\rm {\raggedright\color{red}\textsf{PY: #1}\marginpar{$\star$}}} 

\newcommand{\mkrevise}[1]{\color{blue}{#1}} 

\title{\LARGE \bf Planning with Linear Temporal Logic Specifications:\\ Handling Quantifiable and Unquantifiable Uncertainty}

\author{Pian Yu$^*$, Yong Li, David Parker, and Marta Kwiatkowska% <-this % stops a space
\thanks{This work was supported by the ERC AdG FUN2MODEL (Grant agreement ID: 834115), ISCAS Basic Research (Grant Nos. ISCAS-JCZD-202406, ISCAS-JCZD-202302), CAS Project for Young Scientists in Basic Research (Grant No. YSBR-040), and ISCAS New Cultivation Project ISCAS-PYFX-202201. }% <-this % stops a space
\thanks{Pian Yu is currently with the Department of Computer Science, University College London (UCL), United Kingdom
        {\tt\small {pian.yu}@ucl.ac.uk} $^*$Work was done when she was with the Department of Computer Science, University of Oxford.}
\thanks{David Parker and Marta Kwiatkowska are with the Department of Computer Science, University of Oxford, United Kingdom
        {\tt\small {david.parker, marta.kwiatkowska}@cs.ox.ac.uk}}%
\thanks{Yong Li is with Key Laboratory of System Software (Chinese Academy of Sciences) and State Key Laboratory of Computer Science, Institute of Software, Chinese
Academy of Sciences, China
        {\tt\small {liyong}@ios.ac.cn}}
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
This work studies the planning problem for robotic systems under both quantifiable and unquantifiable uncertainty. The objective is to enable the robotic systems to optimally fulfill high-level tasks specified by Linear Temporal Logic (LTL) formulas. To capture both types of uncertainty in a unified modelling framework, we utilise Markov Decision Processes with Set-valued Transitions (MDPSTs).
We introduce a novel solution technique for the optimal robust strategy synthesis of MDPSTs with LTL specifications. To improve efficiency, our work leverages limit-deterministic B\"uchi automata (LDBAs) as the automaton representation for LTL to take advantage of their efficient constructions. To tackle the inherent nondeterminism in MDPSTs, which presents a significant challenge for reducing the LTL planning problem to a reachability problem, we introduce the concept of a Winning Region (WR) for MDPSTs. Additionally, we propose an algorithm for computing the WR over the product of the MDPST and the LDBA. Finally, a robust value iteration algorithm is invoked  to solve the reachability problem. We validate the effectiveness of our approach through a case study involving a mobile robot operating in the hexagonal world, demonstrating promising efficiency gains.  
\end{abstract}

% In addition to requiring the agent's capability to perform complex tasks, there is also a desirability for minimising the cumulative cost. 

\section{Introduction}

Uncertainty in planning can be categorised into two types based on the effects of actions: probabilistic and nondeterministic. In probabilistic planning, uncertainty is quantified using probabilities, with Markov Decision Processes (MDPs) and their generalisations serving as the standard modelling frameworks \cite{puterman2014markov,natarajan2022planning}. Nondeterministic planning, on the other hand, addresses unquantifiable uncertainty (such as ambiguity and adversarial environments), typically exploiting the fully observable nondeterministic domain (FOND) as a modelling framework \cite{CimattiPRT03,FOND,GeffnerBonet2013}. Both probabilistic and nondeterministic planning have been extensively studied, leading to significant advances in the field.

Robotic systems are susceptible to many different types of uncertainty, such as sensing and actuation noise, unpredictability in a robot's perception, and dynamic environments \cite{thrun2005probabilistic,du2011robot}. Some sources of uncertainty, such as sensing and actuation noise, can be quantified probabilistically using statistical methods. However, ambiguous uncertainties, such as unpredictable perception and dynamic environments, are often more challenging to quantify. Existing works in robot planning mainly focus on addressing either quantifiable or unquantifiable uncertainty. However, in scenarios such as human-robot collaboration \cite{ajoudani2018progress}, both quantifiable and unquantifiable uncertainties are present. Quantifiable uncertainties may arise from robotic actuation errors, while unquantifiable uncertainties often stem from the unpredictable nature of human behavior.
To the best of our knowledge, approaches that can effectively handle both types of uncertainty simultaneously have, however, been less explored.
In light of this, we propose to utilise
MDPs with set-valued transitions (MDPSTs)~\cite{trevizan2007planning,trevizan2008mixed} as our unified modelling framework. 
They are attractive because they admit a simplified Bellman equation compared to (more general) MDPs with imprecise probabilities~(MDPIPs) \cite{white1994markov,satia1973markovian,givan2000bounded} and Uncertain MDPs~(UMDPs)~\cite{nilim2005robust,buffet2005robust,hahn2019interval}, and thus stochastic games~\cite{Con93}. %~\cite{DBLP:journals/ejcon/SvorenovaK16}.

Recently, MDPSTs have been employed to formalise the \emph{trembling-hand problem in nondeterministic domains} ~\cite{yu2024trembling}, where the term ``trembling-hand" refers to the phenomenon in which an agent, due to faults or imprecision in its action
selection mechanism, may mistakenly perform unintended actions with a certain probability, potentially leading to goal failures. Specifically, this approach demonstrates that the human-robot co-assembly problem can be modelled using MDPSTs, yielding more efficient solution techniques compared to the stochastic game formulation. In this work, Linear Temporal Logic on finite traces~(\LTLf)~\cite{de2013linear} was used as the task specification language. \LTLf\ shares the same syntax as Linear Temporal Logic (\LTL)~\cite{Pnu77} but is interpreted over \emph{finite} rather than infinite traces~\cite{de2013linear}.
However, in many robotic applications, such as persistent surveillance and repetitive supply delivery, it is necessary to define the robot's tasks over \emph{infinite} trajectories.
Indeed, \LTL has been widely applied in robotics research for specifying complex temporal objectives over infinite traces, such as~\cite{ding2014ltl,schillinger2019hierarchical,guo2018probabilistic,sadigh2014learning,wen2016probably,hasanbeig2019reinforcement}, including 
MDPs with LTL objectives, 
e.g., \cite{kwiatkowska2013automated, kwiatkowska2022probabilistic,BrafmanGP18}. A typical approach involves converting the LTL specification into a Deterministic Rabin Automaton (DRA) and then taking the product of the MDP and the DRA \cite{baier2008principles}. This reduces the problem to a planning problem with a reachability goal over the product space. 

In this paper, we propose to formalise the planning problem for robotic systems under both quantifiable and unquantifiable uncertainty with temporal objectives as the strategy synthesis problem for MDPSTs with (full)  \LTL objectives.
We highlight that MDPSTs with LTL objectives are 
studied for the first time in this paper. Due to the presence of unquantifiable uncertainty in MDPSTs, computing the end components \cite{de1998formal} of an MDPST becomes nontrivial. As a result, the conventional procedure for MDPs with LTL objectives  based on conversion to DRAs does not apply, necessitating new solution techniques developed in this work. 

 
The main contributions are summarised as follows. (i) We propose using MDPSTs as the modelling framework for robot planning under both quantifiable and unquantifiable uncertainty. Although the model has been relatively little studied since it was initially proposed in 2007 \cite{trevizan2007planning}, recent findings highlight its advantages, particularly in terms of computational efficiency \cite{yu2024trembling}. (ii) A novel solution technique is proposed for the optimal robust strategy synthesis for MDPSTs with LTL specifications. This technique addresses the inherent nondeterminism in MDPSTs, which complicates the reduction of the LTL planning problem to a reachability problem, by introducing the concept of a Winning Region (WR). 
To further improve efficiency, we leverage limit-deterministic B\"uchi automata (LDBAs)~\cite{courcoubetis1995complexity,hahn2013lazy,sickert2016limit}, which are typically smaller than conventional DRAs thanks to their efficient construction from LTL. 
We devise an algorithm for computing
WR over the product of the MDPST and the LDBA, and its correctness is demonstrated formally.

% The remainder of the paper is organised as follows. In Section II, we introduce MDPSTs and motivate its usage with two motion planning examples. In Section III, we provide preliminaries for LTL and formulate the optimal strategy synthesis problems under consideration. Section IV presents efficient algorithms for solving the optimal strategy synthesis problems. The results are
% validated by case studies in Sections V. Conclusions are given in Section VI.



% Nondeterministic B\"uchi automata (NBAs) are standard automata representation for LTL formulas in verification.
% However, NBAs cannot generally be used in the analysis of probabilistic systems such as Markov decision processes (MDPs)~\cite{courcoubetis1995complexity}.
% Nonetheless, a subclass of NBAs called limit deterministic B\"uchi automata (LDBAs) that become deterministic after seeing accepting states, can be used for the qualitative analysis of MDPs~~\cite{courcoubetis1995complexity}. 
% For quantitative analysis, however, deterministic Rabin automata were the standard automata being used~\cite{baier2008principles}.
% In 2010s, it has been shown in \cite{hahn2013lazy,sickert2016limit} that LDBAs under mild conditions can also be applied for quantitative analysis.
% In particular, direct and efficient constructions of such LDBAs from LTL formulas areavailable in tools like Owl~\cite{sickert2016limit} and Rabinizer~\cite{kvretinsky2018rabinizer}.
% Another class of NBAs, known as Good-for-MDP automata~\cite{hahn2020good}, can also be applied to quantitative analysis.
% However, these automata are constructed from NBAs in~\cite{hahn2020good} rather than directly from LTL.
% As a result, converting LTL to Good-for-MDP automata involves an intermediate NBA construction, which can be less efficient than the direct construction of LDBAs in~\cite{sickert2016limit}.
% Hence, our work leverages LDBAs to take advantage of their efficient constructions.

\section{Preliminaries}

This section provides preliminaries for \LTL \cite{Pnu77} and its equivalent LDBA  \cite{sickert2016limit} representation. 

% \subsection{Linear Temporal Logic~(\LTL)}

%\LTL is a specification language expressing temporal properties on infinite, nonempty traces ~\cite{Pnu77}. 
%\LTL~\cite{Pnu77} is built from a set of atomic propositions ${\prop}$, the logic operators of negation ($\neg$), conjunction ($\wedge$), and the temporal operators next ($\bigcirc$), until ($\mathsf{U}$). An \LTL formula is defined inductively
%according to the following syntax \cite{baier2008principles}:
\LTL~\cite{Pnu77} extends propositional logic with temporal operators.
The syntax of an \LTL formula over a finite set of propositions $\prop$ is defined inductively as:
\begin{equation}\label{LTL}  \varphi::= \ltltrue | p \in \prop |\neg\varphi|\varphi \wedge\varphi |\varphi \vee\varphi |\bigcirc \varphi|\varphi \mathsf{U}\varphi,
\end{equation}
where $\bigcirc$ (Next) and $\ltlU$ (Until) are temporal operators.
As usual, additional Boolean 
and temporal operators are derived as follows: $\varphi_1 \Rightarrow \varphi_2 \equiv\neg \varphi_1 \vee \varphi_2$ (Implies), $\lozenge \varphi \equiv \ltltrue \ltlU\varphi$ (Eventually), and $\ltlG \varphi = \neg (\ltlF \neg\varphi)$ (Always). The detailed semantics of \LTL can be found in \cite{Pnu77,baier2008principles}.

A \textit{trace} $\trace = \trace_0\trace_1\ldots$ is a finite or infinite sequence of propositional interpretations~(sets), where for every $i \geq 0$, $\trace_i \in 2^{\prop}$ is the $i$-th interpretation in $\trace$. Intuitively, $\trace_i$ is interpreted as the set of propositions that are $\ltltrue$ at instant~$i$.
For a finite trace $\trace \in (2^{\prop})^{*}$, we denote the interpretation at the last instant~(i.e., index) by $\ls(\trace)$,
%For technical reasons, we define $\ls(\trace) = \infty$ for an infinite trace $\trace \in (2^{\prop})^{\omega}$.
and we write $\trace \models \varphi$ when an infinite trace $\trace \in (2^{\prop})^{\omega}$ satisfies \LTL formula~$\varphi$.
The language of $\varphi$, denoted
by $\lang(\varphi)$, is the set of infinite traces over $2^{\prop}$ that satisfy $\varphi$.
%, which is formally denoted as $\trace\in (2^{\pro})^{\omega}$; otherwise $\trace$ is a \textit{finite} trace, denoted as $\trace\in (2^{\pro})^{*}$.

Every \LTL formula $\varphi$ over $\prop$ can be translated into a \emph{nondeterministic B\"{u}chi automaton} (NBA) $\aut$ \cite{DBLP:journals/iandc/VardiW94} over the alphabet $\alphabet = 2^{\prop}$ that recognises the language $ \lang(\varphi)$.
\begin{definition}[]\label{def:nba}
    An NBA $\aut$ is defined as a tuple $\aut = (Q, \alphabet, q_0, \trans, \acc)$, where $Q$ is the set of states, $q_0$ is the initial state, $\acc \subseteq Q$ is the set of accepting states, and $\trans : Q \times \alphabet \mapsto 2^Q$ is the nondeterministic transition function.
\end{definition}
A \emph{run} $\rho $ of $\aut$ over an infinite trace $w_0 w_1\cdots \in \alphabet^{\omega}$ is an infinite sequence $ r_0 r_1 \cdots \in Q^{\omega}$ of states such that $r_0 = q_0$ and, for all $i\geq 0$, we have $r_{i+1} \in \trans(r_i, w_i)$.
We denote by $\infset(\rho)$ the set of states that appear infinitely often in the run $\rho$.
A run $\rho$ of $\aut$ is called \emph{accepting} if $\infset(r)\cap \acc \neq \emptyset$.
The language of $\aut$, denoted $\lang(\aut)$, is the set of all traces that have an accepting run in $\aut$.
%An automaton $\A$ is called \emph{deterministic} if $|\trans(q, w)| \leq $ for all $q \in Q$ and $w \in \alphabet$.

NBAs, in general, cannot be used for quantitative analysis of probabilistic systems.
Recently, a class of NBAs called limit-deterministic B\"uchi automata (LDBAs), under mild constraints, have been applied for the quantitative analysis of MDPs~\cite{courcoubetis1995complexity,hahn2013lazy,sickert2016limit}.
We will also use the LDBAs constructed by \cite{sickert2016limit} for our planning problem.

\begin{definition}[LDBA \cite{sickert2016limit}]\label{def:ldba}
    An LDBA $\aut$ is defined as a tuple $\aut = (Q, \alphabet, q_0, \trans, \acc)$ where 
\begin{itemize}
  \item $Q=Q_i\cup Q_{acc}$ is the set of states partitioned into two disjoint sets $Q_i$ and $Q_{acc}$,
  \item $q_0 \in Q_i$ is the initial state,
  \item $\acc \subseteq Q_{acc}$ is the set of accepting states, and
  \item $\trans = \trans_i \cup \trans_j \cup \trans_{\acc}$ where $\trans_i : Q_i \times \alphabet \mapsto Q_i, \trans_{acc} : Q_{acc} \times \alphabet \mapsto Q_{acc}$ and $\trans_j : Q_i \times \{\epsilon\} \mapsto 2^{Q_{acc}}$. 
\end{itemize}
% An infinite \emph{run} $r$ of an LDBA $\A$ is called \emph{accepting} if $\inf(r)\cap {\rm Acc}\neq \emptyset$.
\end{definition}

By Definition~\ref{def:ldba}, the LDBAs considered here are deterministic within $Q_i$ and $Q_{acc}$ components; the only nondeterminism lies in the $\epsilon$-transition jumps from $Q_i$-states to $Q_{acc}$-states via $\trans_j$ function.
Note that the $\epsilon$-transitions do not consume a letter from $\alphabet$: they are just explicit representations of the nondeterministic jumps in the runs of $\aut$.
To be accepting in $\A$, a run has to eventually make a nondeterministic jump through $\trans_j$ since all accepting states reside only in $Q_{acc}$.
It is easy to translate an \LTL formula $\varphi$ to an LDBA $\aut$ such that $\lang(\aut) = \lang(\varphi)$ using state-of-the-art tools such as Owl \cite{sickert2016limit} and Rabinizer 4 \cite{kvretinsky2018rabinizer}.

% , have been proposed to transform \LTL formulas into LDBAs.
% which is defined as a tuple  $\A = (Q, q_{0}, \Sigma, \Delta, {\rm Acc})$, where
% \begin{itemize}
%   \item $Q$ is a finite set of states,
%   \item $q_{0}\in Q$ is the initial state,
%   \item $\Sigma = 2^{\prop}\cup \{\epsilon\}$ is the finite set of alphabet;
%   \item $\Delta\subseteq Q\times \Sigma \times Q$ are transitions, and
%   \item ${\rm Acc}\subseteq Q$ is the set of accepting states.
% \end{itemize}


% The \emph{language}, $L_\A$, of $\A$ is the subset of words that have accepting runs in $\A$.

% Unlike \LTL, Linear Temporal Logic on finite traces~(\LTLf) expresses temporal properties on finite traces. In particular, \LTLf shares syntax with \LTL.

% Linear Temporal Logic on finite traces~(\LTLf) has identical syntax as \LTL but is interpreted over \emph{finite} rather than infinite traces/words~\cite{de2013linear}.
% The detailed semantics of \LTLf can be found in \cite{de2013linear}.
% It is also shown that, every \LTLf formula $\varphi$ can be converted into a deterministic finite automaton (DFA) accepting all the words that satisfy $\varphi$.
% \ly{Not sure whether above paragraph belongs here or other places}


% , one can construct a Deterministic Finite Automaton~(DFA) $\D$ such that for every trace $\pi$, we have $\pi$ satisfies $\varphi$, denoted by $\pi\models \varphi$, iff $\pi$ is accepted by $\D$.


% \subsection{Linear Temporal Logic on finite traces~(\LTLf)}

% \LTLf is a specification language expressing temporal properties on finite, nonempty traces. In particular, \LTLf shares syntax with \LTL. Given $\trace$, we define when an \LTLf formula $\varphi$ \emph{holds} at instant $i$, $0 \leq i \leq \ls(\trace)$, written as $\trace, i \models \varphi$, inductively on the structure of $\varphi$, as:
% % 
% \begin{itemize}
% 	\item 
% 	$\trace, i \models a \tiff a \in \trace_i\nonumber$ (for $a\in{Prop}$);
% 	\item 
% 	$\trace, i \models \lnot \varphi \tiff \trace, i \not\models \varphi\nonumber$;
% 	\item 
% 	$\trace, i \models \varphi_1 \wedge \varphi_2 \tiff \trace, i \models \varphi_1 \text{ and } \trace, i \models \varphi_2\nonumber$;
% 	\item 
% 	$\trace, i \models \Next\varphi \tiff  i< \ls(\trace)$ and $\trace,i+1 \models \varphi$;
% 	\item 
% 	$\trace, i \models \varphi_1 \mathop{\U} \varphi_2$ iff $\exists j$ such that $i \leq j \leq \ls(\trace)$ and $\trace,j \models\varphi_2$, and $\forall k, i\le k < j$ we have that $\trace, k \models \varphi_1$.
% \end{itemize}
% %%
% We say $\trace$ \emph{satisfies} $\varphi$, written as $\trace \models \varphi$, if $\trace, 0 \models \varphi$. 

% % , which is instead interpreted over infinite traces~\cite{Pnu77}. Given a set of atomic propositions $Prop$, \LTLf formulas are generated as follows: 
% % {\centerline{$\varphi ::= a \mid \varphi \wedge \varphi \mid \neg \varphi \mid  
% %  \Next \varphi \mid \varphi \mathop{\U} \varphi,$}}
% % where $a \in Prop$ is an \textit{atom}, $\Next$~(\emph{Next}) and $\mathop{\U}$~ (\emph{Until}) are temporal operators. 
% % We make use of standard Boolean abbreviations, e.g., $\vee$~(or) and $\rightarrow$~(implies), $\true$ and $\false$. In addition, we define the following abbreviations: \emph{Weak Next} $\Wnext \varphi \equiv \neg \Next \neg \varphi$, \emph{Eventually} $\Diamond \varphi \equiv \true \Until \varphi$ and \emph{Always} $\Box \varphi \equiv \false \Release \varphi$, where $\Release$ is for \emph{Release}.

% A \textit{trace} $\trace = \trace_0\trace_1\ldots$ is a sequence of propositional interpretations~(sets), where for every $i \geq 0$, $\trace_i \in 2^{Prop}$ is the $i$-th interpretation of $\trace$. Intuitively, $\trace_i$ is interpreted as the set of propositions that are $true$ at instant $i$. We denote the last instant~(i.e., index) in a trace $\trace$ by $\ls(\trace)$.  
% % A trace $\trace$ is an \textit{infinite} trace if $\ls(\trace) = \infty$, which is formally denoted as $\trace\in (2^{Prop})^{\omega}$; otherwise $\trace$ is a \textit{finite} trace, denoted as $\trace\in (2^{Prop})^{*}$. Moreover, 
% By $\trace^k = \trace_0 \cdots \trace_k$ we denote the \emph{prefix} of $\trace$ up to the $k$-th iteration, and $\trace^k = \epsilon$ denotes an empty trace if $k < 0$. 
% % \LTLf formulas are interpreted over finite, nonempty traces. 
% We denote $\trace$ \emph{satisfies} $\varphi$ by $\trace \models \varphi$. The detailed semantics of \LTLf can be found in~\cite{DegVa13}.
% It is also shown there that, for every \LTLf formula $\varphi$, one can construct a Deterministic Finite Automaton~(DFA) $Aut_\varphi = (2^{Prop}, \Q, q_0, \delta, acc)$, where $2^{Prop}$ is a finite alphabet, $\Q$ is a finite set of states, $q_0 \in \Q$ is the initial state, $\delta : \Q \times 2^{Prop} \rightarrow \Q$ is the transition function, and $acc$ is the set of accepting states, such that for every trace $\pi$ we have $\pi\models \varphi$ iff $\pi$ is accepted by $Aut_\varphi$.

% \subsection{Markov decision processes (MDPs)}

% Following \cite{puterman2014markov}, a (finite, state-labeled) MDP is a tuple $\M = (\S, s_0, A, \T,  \L)$, where $\S$ is a finite set of states, $s_0\in \S$ is the initial state, $A$ is a finite set of actions, $\T: \S \times A \times \S \rightarrow [0, 1]$ is the probabilistic transition function. We use $A(s)\subseteq A$ to denote the set of actions \emph{applicable} at state $s$. $\L: \S \to 2^{Prop}$ is the proposition labelling function, where $Prop$ is a finite set of propositions. 

% A path $\xi$ of $\M$ is a finite or infinite sequence of alternating states and actions $\xi=s_0 a_0 s_1 a_1\cdots$, ending with a state if finite, such that, $a_i\in A(s_i)$ and $\T(s_i, a_i, s_{i+1})>0$ for all $i\ge 0$. Denote by $\fpaths$ ($\fpaths_s$) and ${\rm IPaths}$ (${\rm IPaths}_s$) the set of all finite and infinite paths of $\M$ (starting from state $s$), respectively. For a path $\xi=s_0 a_0 s_1 a_1\cdots$, we define the corresponding labelled run as $L(\xi)=L(s_0)L(s_1), \cdots$. 

% A strategy $\sigma$ of $\M$ is a function $\sigma: \fpaths \to \distr(A)$ such that, for each $\xi\in \fpaths$, $\sigma(\xi)\in \distr(A({\ls}(\xi)))$, where ${\ls}(\xi)$ is the last state of the finite path $\xi$ and $\distr(A)$ denotes a distribution over $A$. Let $\Omega_\sigma^{\M}(s)$ denote the subset of (in)finite paths of $\M$ that corresond to strategy $\sigma$ and initial state $s$. Let $\Pi_{\M}$ be the set of all strategies. A strategy for an MDP is called \emph{deterministic} (or pure) if the action choice at each step is a Dirac distribution, and \emph{randomized} otherwise. We say a strategy is Markovian (or positional) if $\ls(\xi)=\ls(\hat{\Xi})$ implies $\sigma(\xi)=\sigma(\hat{\Xi})$ for all paths $\xi, \hat{\Xi}$.

% % A strategy for an MDP is called \emph{deterministic} if the action choice at each step is a Dirac distribution, and \emph{randomized} otherwise. For the purpose of this work, we focus on deterministic agent strategies for MDPs and MDPSTs. 
% % In this work, we focus on \emph{deterministic} agent strategies for MDPs, instead of \emph{randomized}. A strategy $\sigma_m$ of $\M$ is a function $\sigma_m: \fpaths \to A$ such that, for each $\xi\in \fpaths$, $\sigma_m(\xi)\in A({\ls}(\xi))$, where ${\ls}(\xi)$ is the last state of $\xi$. We denote by $\Xi^{\sigma_m}$ the set of all probably infinite paths of $\M$ generated by $\sigma_m$.

% Given an MDP $\M = (\S, s_0, A, \T,  \L)$ and a set of goal states $G \subseteq \S$, the probability of an agent strategy $\sigma$ enforcing $G$ in $\M$ is defined as ${\rm Pr}_{\M}^{\sigma(G)} := {\rm Pr}_{\M}(\{\xi\in \Xi^{\sigma} \mid \ls(\xi^k) \in G \text{ for some } k \geq 0\})$.
% % \begin{equation*}
% %     {\rm Pr}_{\M}^{\sigma_{m}}(G) := {\rm Pr}_{\M}(\{\xi\in \Xi^{\sigma_m} \mid \ls(\xi) \in G\}).
% % \end{equation*}
% Computing an optimal strategy $\sigma^*_m$ that maximizes the probability of enforcing $G$ is the \emph{reachability problem} over $\M$.  Analogously, an agent strategy can also enforce a temporal objective on an MDP. Given an MDP $\M$ and an \LTL (\LTLf) formula $\varphi$, the probability of an agent strategy $\sigma$ enforcing $\varphi$ in $\M$ is defined as 
% \begin{equation*}
%     {\rm Pr}_{\M}^{\sigma(\varphi)} := {\rm Pr}_{\M}(\{\xi\in \Xi^{\sigma} \mid \pi(\xi, \M) \models \varphi\}).
% \end{equation*}
% The problem of MDP with \LTL (\LTLf) objective is to compute an optimal strategy $\sigma^*$, which maximizes the probability of enforcing $\varphi$ in $\M$~\cite{baier2008principles}.

% An \emph{end-component (EC)} \cite{de1998formal,baier2008principles} of an MDP $\M$ is a sub-MDP $\M'$ of $\M$ such that its underlying graph is strongly connected. A \emph{maximal end-component (MEC)} is maximal under set-inclusion.

% \begin{lemma}[EC properties]\cite{de1998formal}
%     Once an end-component $E$ of an MDP $\M$ is entered, there is a strategy that visits every state-action combination in $E$ infinitely often with probability 1 and stays in $E$ forever.
% \end{lemma}

% % \begin{definition}
% %     An end component (EC) of an MDPST $\mathcal{M}_N$ is a pair $(X', A')$, where $X'\subseteq X$ and $A': X' \to 2^{A}$ is a map that associates each state $x\in X'$ with a subset of available actions $A'(x')$ such that, after taking an action $a\in A'(x')$, all possible target states, i.e., $\post_{\M}(x, a)\subseteq X'$.
% % \end{definition}

% \subsection{Limit-Deterministic B\"{u}chi Automata~(LDBA)}

% Broadly speaking, an LDBA behaves deterministically once it has
% seen an accepting state \cite{courcoubetis1995complexity,hahn2013lazy,sickert2016limit}. In this study, we use the LDBA construction proposed in \cite{sickert2016limit}, which is defined as follows.

% \begin{definition}[LDBA]\label{def:ldba}\cite{sickert2016limit}
%     A BA is called \emph{limit-deterministic} if the set of states $Q$ can be partitioned into a deterministic set $Q_D$ and a non-deterministic set $Q_N$ such that
% \begin{itemize}
%   \item $Q=Q_D\cup Q_N$ and $Q_D\cap Q_N=\emptyset$;
%   \item ${\rm Acc}\subseteq Q_D$ and $q_0\in Q_N$;
%   \item $|\delta(q, \nu)|\le 1$ for all $q\in Q_N$ and $\nu\neq \epsilon\in \Sigma$;
%   \item $\delta(q, \nu)\subseteq Q_D$ and $|\delta(q, \nu)|\le 1$ for all $q\in Q_D$ and $\nu\in \Sigma$.
% \end{itemize}
% % An infinite \emph{run} $r$ of an LDBA $\A$ is called \emph{accepting} if $\inf(r)\cap {\rm Acc}\neq \emptyset$.
% \end{definition}

% The constructions in Definition \ref{def:ldba} produce an LDBA that behaves deterministically except over the $\epsilon$-transitions. That is, transitions within $Q_N$ and $Q_D$ are deterministic, and the nondeterminism only occurs in the transitions from $Q_N$ to $Q_D$, through the $\epsilon$-transitions. 

% The satisfaction of any \LTL formula  $\varphi$ can be captured through an LDBA $\A$. Previous works \cite{hahn2011synthesis,kamil2019control,hahn2020good} have demonstrated that LDBAs can be utilised for quantitative analysis of MDPs.




% \ly{Owl \cite{sickert2016limit} is current state of the art tool for obtaining LDBAs from LTL. But yes, Rabinizer can also produce LDBAs}

% An LDBA is a BA where the nondeterminism
% is limited in the initial component $Q_N$ of the automaton. An LDBA starts in a nondeterministic initial
% component and then transitions into a deterministic accepting
% component $Q_D$ through $\epsilon$-transitions after reaching an
% accepting state, where all transitions after this point are deterministic.

 





\section{Markov
decision processes with set-valued transitions}

This section introduces Markov
Decision Processes with Set-valued Transitions (MDPSTs) \cite{trevizan2007planning, trevizan2008mixed} as the modelling framework for robot planning under quantifiable and unquantifiable uncertainty.  Compared to the definition in  \cite{trevizan2007planning}, we further introduce a labelling function that associates the states of the MDPST with the propositions of an LTL formula.

\begin{definition}[MDPSTs]\label{def:MDPST}
A \emph{MDPST} $\mathcal{M}$ is a tuple $(S, s_0, A, \mdpsttr, \probtr, \L)$, where 
\begin{itemize}
    \item $S$ is a finite set of states;
    \item $s_0\in S$ is the initial state;
    \item $A$ is a finite set of actions;
    \item $\mdpsttr: S \times A \Mapsto 2^{2^S}$ is the set-valued nondeterministic state transition (partial) function;
    % \ly{$\mdpsttr: S \times A  \times 2^S \Mapsto \{\ltltrue, \ltlfalse\}$}
    \item $\probtr: S \times A \times 2^S \mapsto (0, 1]$ is the transition probability (or mass assignment) function, i.e., given a set $\Theta\in \mdpsttr(s, a)$, $\probtr(s, a, \Theta)=\prob(\Theta|s, a)$,
    \item $\L: S \to 2^{\prop}$ is the proposition labelling function, where $\prop$ is a finite set of propositions.
\end{itemize}
\end{definition}
Traditional MDPs are, in fact, a special type of MDPSTs, where $\probtr$ only maps a state and an action to a probabilistic distribution over $S$ instead of the powerset $2^{S}$.
As usual, we use $A(s) \subseteq A$ to denote the set of actions \emph{applicable} at state $s$. Note that, in MDPSTs, the transition function $\mdpsttr(s, a)$ returns a set of state sets, i.e., $\F(s,a) \subseteq 2^\S$, and the transition probability function $\probtr$ expresses the probability of transitioning to such sets via a given action. 

A path $\xi$ of $\M$ is a finite or infinite sequence of alternating states and actions $\xi=s_0 a_0 s_1 a_1\cdots$, ending with a state if finite, such that for all $i 
\geq 0$, $a_i \in A(s_i)$ and $s_{i+1}\in \Theta_i$ for some set $\Theta_i \in \F(s_i, a_i)$.
We denote by $\fpaths$ ($\fpaths_s$) and ${\rm IPaths}$ (${\rm IPaths}_s$) the set of all finite and infinite paths of $\M$ (starting from state $s$), respectively. 
For a path $\xi=s_0 a_0 s_1 a_1\cdots$ of $\M$, the sequence $\L(\xi)=\L(s_0)\L(s_1), \cdots$ over $\prop$ is called the \emph{trace} induced by $\xi$ over $\M$.

A strategy $\sigma$ of $\M$ is a function $\sigma: \fpaths \to \distr(A)$ such that, for each $\xi\in \fpaths$, $\sigma(\xi)\in \distr(A({\ls}(\xi)))$, where ${\ls}(\xi)$ is the last state of the finite path $\xi$ and $\distr(A)$ denotes the set of all possible distributions over $A$.
Let $\Omega_\sigma^{\M}(s)$ denote the subset of (in)finite paths of $\M$ that correspond to strategy $\sigma$ and initial state $s$.
Let $\Pi_{\M}$ be the set of all strategies.
%We will omit the MDPST $\M$ when the context is clear.
% A strategy for an MDPST is called \emph{deterministic} (or pure) if the action choice at each step is a Dirac distribution, and \emph{randomized} otherwise. We say a strategy is Markovian (or positional) if $\ls(\xi)=\ls(\hat{\xi'})$ implies $\sigma(\xi)=\sigma(\xi')$ for all paths $\xi, \hat{\xi'}$.

Let us now motivate MDPSTs for robot planning under uncertainty with a running example.

\vspace{-0.2cm}
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{environment.png}
	\caption{Hexagonal world. }
	\label{Fig:hexagonal}
\end{figure}
\vspace{-0.2cm}

\begin{example}[Hexagonal world]\label{example2}
We consider a hexagonal grid map, as shown in Fig.~\ref{Fig:hexagonal}. Hexagonal grid map representations offer several advantages over quadrangular grid maps, including lower quantisation error \cite{jeevan2018image} and enhanced performance in cooperative robot exploration \cite{quijano2007improving}.
The yellow regions, labelled as $\emph{b1}, \emph{b2}, \cdots, \emph{b5}$, are
base stations, and the grey regions, labelled as $\emph{obs}$, are obstacle regions. The state of the robot is defined as $(q_i, w)$, where $q_i$ represents the specific region where the robot is located and $w\in \{N, S, E, W\}$ represents the orientation of the robot. There are 4 action primitives $\{\textsc{FR}, \textsc{BK}, \textsc{TR}, \textsc{TL}\}$, which stand for move forward, move backward, turn right, and turn left, respectively. The robot’s motion is subject to uncertainty due to actuation noise and drifting. It is known that the probabilities of the 4 actions being executed correctly are $0.8, 0.7, 0.9,$ and $0.9$, respectively. 
Moreover, it should be noted that, depending on the precise location (which is not available due to imprecise sensing/perception) within each region and the orientation of the robot, there may be several potential target states when the action is correctly executed. For instance, as depicted in Fig.~\ref{Fig:hexagonal}, when the
robot is at state $(q_{17}, E)$ and wants to take an action $\textsc{FR}$, with probability 0.8 it ends up in state $(q_{12}, E)$ or $(q_{22}, E)$. 

In this example, it is convenient to abstract the robot dynamics
as an MDPST since one can combine the moves of all potential target regions as a set-valued transition for each correctly executed motion. For instance, when the robot's state is $(q_{17}, E)$ and it takes an action $\textsc{FR}$, then there are three possible transitions: i) with probability 0.8 it moves to the set $\{(q_{12}, E), (q_{22}, E)\}$, ii) with probability 0.1 it moves to the singleton set $\{(q_{27}, E)\}$, and iii) with probability 0.1 it moves to the singleton set $\{(q_7, E)\}$. 
\end{example}

Previously, MDPSTs were used to 
formulate the trembling-hand problem in nondeterministic domains~\cite{yu2024trembling}, where \LTLf was utilised as the specification language.
Note that, for \LTLf objectives, one can translate the formula into a deterministic finite automaton.
For \LTL goals, however, this is more involved because DFAs are not sufficient, and one has to resort to automata
over infinite traces.
This complicates the strategy synthesis procedure and requires us to develop new solution techniques 
based on the 
Winning Region (see Def. \ref{def:WR})
to capture acceptance conditions. 

Let us first recap the definitions of a \emph{feasible distribution} and \emph{nature} for MDPSTs originally introduced in \cite{yu2024trembling}.

Given a MDPST $\M$, define the set of reachable states of $(s, a)$ as $\post_{\M}(s, a) = \{s' \mid \exists \Theta\in \mdpsttr(s, a) \text{ s.t. } \probtr(s, a, \Theta)>0 \wedge  s'\in \Theta \}.$
% \begin{equation*}
% \begin{aligned}
%     \post_{\M}&(s, a) = \\
%     &\{s' \mid \exists \Theta\in \mdpsttr(s, a) \text{ s.t. } \probtr(s, a, \Theta)>0 \wedge  s'\in \Theta \}.
%     \end{aligned}
% \end{equation*}
A \emph{feasible distribution} of $\M$ guarantees that, given a state-action pair $(s,a)$, $\emph{(i)}$ the sum of probabilities of selecting a state from $\post_{\M}(s,a)$ equals 1; $\emph{(ii)}$ the sum of probabilities of selecting a state from a set $\Theta \in \F(s, a)$ equals $\T(s, a, \Theta)$;
and $\emph{(iii)}$  the probability of selecting a state outside $\post_{\M}(s,a)$ is 0. 
In the following definition, 
$\iota^{\Theta}_{s'}$ indicates whether $s'$ is \emph{in} ${\Theta}$. Hence $\iota^{\Theta}_{s'}=1$ if $s'\in \Theta$ and $\iota^{\Theta}_{s'}=0$ otherwise. Furthermore, $\alpha_{s'}^{\Theta}$ represents the probability of $s'$ being \emph{selected} from ${\Theta}$ if $s'\in \Theta$. Note that Definition 5 in \cite{yu2024trembling} only allows deterministic choice within $\Theta$ (i.e., $\alpha_{s'}^{\Theta} = 1$ if $s'$ is selected, and 0 otherwise). In this work, we also permit probabilistic choice within $\Theta$. 

 
\begin{definition}[Feasible distribution in MDPSTs]\label{Def:feasibledistribution}
    Let $\mathcal{M}=(S, s_0, A, \mdpsttr, \probtr, \L)$ be an MDPST and $(s, a)$ a state-action pair, where $a \in A(s)$. $\mathfrak{h}_s^a\in \distr(S)$ is a \emph{feasible distribution} of $(s, a)$, denoted by $s \xrightarrow[]{a} \mathfrak{h}_s^a$, if 
    % there exist variables $\alpha_{s'}^{\Theta}\in \{0, 1\}$ for each set $\Theta\in \mdpsttr(s, a)$ and each state $s'\in \Theta$ such that
    \begin{itemize}
        \item[(i)] 
        $\sum_{s'\in \Theta} \alpha_{s'}^{\Theta} =1, \mbox{ for } \Theta\in \mdpsttr(s, a)$;
        \item[(ii)] $\mathfrak{h}_x^a(s'){=}\sum\limits_{\Theta\in \mdpsttr(s, a)}\iota^{\Theta}_{s'}\alpha_{s'}^{\Theta} \probtr(s, a, \Theta), 
        \mbox{ for } s'{\in} \post_{\M}(s, a)$; 
        \item[(iii)] $\mathfrak{h}_s^a(s')=0, \mbox{ for } s'\in S\setminus \post_{\M}(s, a)$.
    \end{itemize}  
\end{definition}

A \emph{nature} is defined to characterize the unquantifiable uncertainty in MDPSTs, motivated by the definition of \emph{nature} in robust MDPs~\cite{nilim2005robust}.
One can think of nature as the strategy controlled by the adversarial environment.

\begin{definition}[Nature for MDPSTs \cite{yu2024trembling}] 
A \emph{nature} of an MDPST is a function
$\gamma: \fpaths \times A \to \distr(S)$ such that  $\gamma(\xi, a)\in \mathcal{H}_s^a$ for $\xi\in \fpaths$ and $a\in A({\ls}(\xi))$, where $\H_s^a$ is the set of feasible distributions of $(s,a)$.
\end{definition}

Suppose we fix a nature $\gamma$. The probability of an agent strategy $\sigma$ satisfying an \LTL specification $\varphi$ is denoted by 
\begin{equation*}
    \prob_{\M}^{\sigma, \gamma}(\varphi) := \prob_{\M}(\{\xi\in \Omega_{\sigma, \gamma}^\M(s_0) \mid \L(\xi) \models \varphi\}),
\end{equation*}
where $\Omega_{\sigma, \gamma}^\M(s_0)$ is the set of all probable paths generated by the agent strategy $\sigma$ and nature $\gamma$ from initial state $s_0$. 

Similarly to Definitions 7-8 in \cite{yu2024trembling}, we now define (optimal) robust strategies for MDPSTs with \LTL specifications, rather than \LTLf, which account for all possible natures.  
Of course, given a fixed strategy $\sigma$ and a nature $\gamma$, one can deduce a Markov chain $\M_{\sigma, \gamma} $ from $\M$.
 
\begin{definition}[Robust strategy]\label{RobustSat}
    Let $\M$ be an MDPST, $\varphi$ an \LTL formula, and $\beta \in [0,1]$ a threshold. An agent strategy $\sigma$ \emph{robustly enforces} $\varphi$ in ${\mathcal{M}}$ wrt $\beta$ if, for every nature $\gamma$, the probability of generating paths satisfying $\varphi$ in $\M$ is no less than $\beta$, that is, $P_{{\mathcal{M}}}^{\sigma} (\varphi)\ge \beta$, where $P_{{\mathcal{M}}}^{\sigma} (\varphi):={\min}_{\gamma}\{{\rm Pr}_{{\mathcal{M}}}^{\sigma, \gamma}(\varphi)\}.$
    Such a strategy $\sigma$ is referred to as a robust strategy for $\M$~(with respect to $\beta$).
\end{definition}

\begin{definition}[Optimal robust strategy]
    An optimal strategy $\sigma^*$ that robustly enforces an \LTL formula $\varphi$ in an MDPST $\M$ is given by  $\sigma^*=\arg{\max}_{\sigma}\{{\rm Pr}_{{\mathcal{M}}}^{\sigma} (\varphi)\}.$
     In this case, $\sigma^*$ is referred to as an \emph{optimal robust strategy} for $\M$.
\end{definition}

The optimal robust strategy synthesis problem considered in this paper is formulated as follows.

\begin{problem}[Optimal Robust Strategy Synthesis]\label{Prom1}
Given an MDPST ${\mathcal{M}}$ 
and an \LTL formula $\varphi$, synthesise an optimal robust strategy $\sigma^*$.
\end{problem}

\section{Solution technique}

We now introduce our solution to Problem~\ref{Prom1} for a given MDPST $\M$ and \LTL formula $\varphi$. Our approach is based on a reduction to the reachability problem, but is more challenging than for MDPs because of unquantifiable uncertainty, and than for \LTLf because of the need to consider infinite paths. It contains 3 steps.
\begin{itemize}
    \item[1)] Construct the product $\M^{\times}$ of the MDPST $\M$ and the LDBA $\aut$ derived from the \LTL specification $\varphi$.
    \item[2)] Reduce Problem \ref{Prom1} to a reachability problem over the product MDPST $\M^{\times}$. This step presents a significant challenge; we address it by introducing the concept of a Winning Region for MDPSTs, along with a novel algorithm for computing it.
    \item[3)] Synthesise the strategy of the reachability problem over the product $\M^{\times}$. 
\end{itemize}
We will introduce each step in detail below.

\subsection{Product MDPST}

% In our previous work \cite{yu2024trembling}, we propose an effective robust value iteration algorithm for solving the problem of MDPST with \LTLf objectives. The idea is by the reduction to the reachability problem of a product MDPST. 
% More specifically, given an MDPST $\M$ with \LTLf objective $\varphi$, we first construct the corresponding DFA $Aut_\varphi$ of the \LTLf formula $\varphi$, then construct the product MDPST $\mathcal{M}^{\times}$ of $\M$ and $Aut_\varphi$. In this case, computing an optimal strategy for $\M$ with $\varphi$ reduces to the reachability problem over $\mathcal{M}^{\times}$, where the goal states $G$ are those in $\M^\times$ that consist of the accepting states of $Aut_\varphi$. 

%As previously noted, \LTL does not have an equivalent translation to DFAs. For MDPs, the commonly used automaton translation for \LTL is a Deterministic Rabin Automaton (DRA). More recently, LDBAs have been proposed for quantitative model checking and reinforcement learning in MDPs, demonstrating computational advantages over DRAs. Therefore, we adopt LDBA as the automaton translation for \LTL in this work.

% For MDPs with \LTL objectives, the common approach for optimal strategy synthesis involves reducing the problem to a reachability problem within a product MDP~\cite{baier2008principles}. 
% More specifically, given an MDP $\M$ with \LTL objective $\varphi$, we first construct the automaton translation $\A$ (which can be DRA or LDBA) of the \LTL formula $\varphi$ , then construct the product MDP $\mathcal{M}^{\times}$ of $\M$ and $\A$. After that, the \emph{accepting maximal end component (AMEC)} for the product MDP $\mathcal{M}^{\times}$ is computed.
% In this case, computing an optimal strategy for an MDP $\M$ with $\varphi$ reduces to the reachability problem over $\mathcal{M}^{\times}$, where the goal states $G$ are those states that contained in the underlying graph of the AMECs. 

As mentioned before, we first obtain an LDBA $\A= (Q, q_{0}, \Sigma, \trans = \trans_i\cup\trans_j\cup \trans_{acc}, \acc)$ from the LTL formula $\varphi$ using  state-of-the-art tools such as Rabinizer 4~\cite{kvretinsky2018rabinizer}.
We prefer LDBAs over DRAs because nondeterministic LDBAs are usually smaller than DRAs~\cite{sickert2016limit}.
This thus yields smaller product MDPSTs and smaller strategies.
Then, we construct the product MDPST $\M^\times$ of the MDPST $\M=(S, s_0, A,  \mdpsttr, \probtr, \L)$ and the LDBA $\A$ as follows.


% \begin{definition}[Product MDPST]\label{productautomaton}
% A product MDPST is a tuple $\mathcal{M}^{\times} =(S^\times, s^\times_{0}, A^{\times}, \mdpsttr^{\times}, \probtr^{\times}, \L^{\times}, {\rm Acc}^\times)$, where $S^\times=S\times Q$ is the set of states, $s^\times_{0}=(s_0, q_{0})$, and
% \begin{itemize}
% \item $A^{\times}=A\cup A^\epsilon, A^\epsilon:=\{\epsilon_q\mid q\in Q\}$;
% \item $\mdpsttr^{\times}: S^\times \times A^{\times} \Mapsto 2^{S^\times}$ is the set-valued nondeterministic transition function, where 
% \begin{itemize}
%     \item[i)] $\forall a\in A$, $(\Theta, q')\in \mdpsttr^{\times}((s, q), a)$ iff $\Theta\in \mdpsttr(s, a)$ and $q'= \delta(q, \L(s))$;
%     \item[ii)] $\forall a\in A^\epsilon, \mdpsttr^{\times}((s, q), a)=(s, q')$ iff $q'\in \delta(q, \epsilon)$;
% \end{itemize}
%     \item $\probtr^{\times}: S^\times \times A^{\times} \times 2^{S^\times} \mapsto [0, 1]$, where
%     \begin{equation*}
%     \begin{aligned}
%         &\probtr^{\times}((s, q), a, (\Theta, q'))\\
%         &=\begin{cases}
%             \probtr(s, a, \Theta), &\text{if $a\in A(s), (\Theta, q')\in \mdpsttr^{\times}((s, q), a)$,}\\
%             1, & \text{if $a\in A^\epsilon, (\Theta, q')\in \mdpsttr^{\times}((s, q), a)$,}\\
            
            
%             0, & \text{Otherwise;}
%         \end{cases}
%     \end{aligned}
%     \end{equation*}
%   \item $\L^{\times}: S^\times \to 2^{Prop}$, where $\L^{\times}((s, q))=\L(s)$;
%   \item ${\rm Acc}^\times=\{(s, q)\in S|q\in {\rm Acc}\}$.
% \end{itemize}

    % \begin{equation*}
    % \begin{aligned}
    %     &\probtr^{\times}((s, q), a, \Theta^{\times})\\
    %     &=\begin{cases}
    %         \probtr(s, a, \Theta), &\text{if $a\in A(s), \Theta^{\times} = \Theta \times \{q'\} $  for $\Theta \in \mdpsttr(s, a)$ where $q' = \delta(q, \L(s))$}\\
    %         1, & \text{if $a\in A^\epsilon, \Theta^{\times} = \{(s, q')\}$ for some $q' = \delta(q, \epsilon)$ with $a = \epsilon_{q'}$,}\\            
    %         0, & \text{Otherwise;}
    %     \end{cases}
    % \end{aligned}
    % \end{equation*}


% An infinite path $\xi^\times$ of $\M^\times$ satisfies the B\"{u}chi condition if $\inf(\xi^\times)\cap {\rm Acc}^\times \neq \emptyset$.
% \end{definition}



\begin{definition}[Product MDPST]\label{productautomaton2}
A product MDPST is a tuple $\mathcal{M}^{\times} =(S^\times, s^\times_{0}, A^{\times}, \mdpsttr^{\times}, \probtr^{\times}, \L^{\times}, \acc^\times)$, where $S^\times=S\times Q$ is the set of states, $s^\times_{0}=(s_0, q_{0})$ is the initial state, and
\begin{itemize}
\item $A^{\times}=A\cup A^\epsilon$ where $ A^\epsilon:=\{\epsilon_q\mid q\in Q\}$;
\item $\mdpsttr^{\times}: S^\times \times A^{\times} \Mapsto 2^{2^{S^\times}}$ is the set-valued nondeterministic transition function. For every $a \in A^{\times} $, $ (s, q) \in S^{\times}$, we define $\Theta^{\times} \in \mdpsttr^{\times}((s, q), a)$ as follows:
\begin{itemize}
    \item[i)] if $a \in A$, let $q'= \delta(q, \L(s))$, and define $\Theta^{\times} = \{(s', q') \mid s'\in \Theta\}$, for every $\Theta \in \mdpsttr(s, a)$;
    \item[ii)] otherwise $ a = \epsilon_{q'}\in A^\epsilon$, then for every $q' \in \trans_j(q, \epsilon)$, define $\Theta^{\times} = \{ (s, q')\}$.
\end{itemize}
    \item $\probtr^{\times}: S^\times \times A^{\times} \times 2^{S^\times} \mapsto [0, 1]$, where
\begin{itemize}
    \item $\probtr^{\times}((s, q), a, \Theta^{\times}) = \probtr(s, a, \Theta)$, if $a\in A(s), \Theta^{\times} = \Theta \times \{q'\} $ for $\Theta \in \mdpsttr(s, a)$ where $q' = \delta(q, \L(s))$;
    \item $\probtr^{\times}((s, q), a, \Theta^{\times}) = 1$ if $a\in A^\epsilon, \Theta^{\times} = \{(s, q')\}$ for some $q' \in \trans_j(q, \epsilon)$ with $a = \epsilon_{q'}$,
    \item $\probtr^{\times}((s, q), a, \Theta^{\times}) = 0$, otherwise.
\end{itemize}
  \item $\L^{\times}: S^\times \to 2^{Prop}$, where $\L^{\times}((s, q))=\L(s)$;
  \item $\acc^\times=\{(s, q)\in S^{\times} \mid q\in \acc\}$.
\end{itemize}

An infinite path $\xi^\times$ of $\M^\times$ satisfies the B\"{u}chi condition if $\infset(\xi^\times)\cap {\rm Acc}^\times \neq \emptyset$. Such a path is said to be accepting.
\end{definition}

When an MDPST action $a\in A$ is taken in the product MDPST $\M^\times$, the alphabet used to transition the LDBA is deduced
by applying the proposition labelling function to the current
MDP state: $\L(s)\in 2^{\prop}$. In this case, the LDBA transition
$\delta(q, \L(s))$ is deterministic. Otherwise, if an $\epsilon$-transition $\epsilon_{\hat q}\in \{\epsilon_q\mid q\in Q\}$ is taken, the LDBA selects an $\epsilon$-transition, and the nondeterminism of $\trans_j(q, \epsilon)$ is resolved by transitioning the automaton state to $\hat q$. 

% Denote by $\Sigma^\times$ and $\Pi^\times$ the sets of strategies and natures of $\mathcal{M}^\times$, respectively. The construction of
% the product MDPST $\mathcal{M}^\times$ can be interpreted as a principled way
% to augment the state space of the MDPST $\mathcal{M}_N$ in order to account for the temporal
% objective. In this way, the LTL strategy synthesis problem for the MDPST $\mathcal{M}_N$ can be reduced to a reachability problem for the product MDPST $\mathcal{M}^\times$. 
% Conventional planning approaches for MDPs against \LTL specification require  computing \emph{maximal end components} (MECs) in the product and determining which MECs are accepting.
% These MECs are then regarded as the goal states in the reachability planning problems.
% For MDPSTs, we  
% follow the same methodology and define an analogous concept of accepting maximal set-valued end components below.

\subsection{Reduction to a reachability problem}

Conventional planning approaches for MDPs against \LTL specification require  computing \emph{maximal end components} (MECs) in the product and determining which MECs are accepting.
These MECs are then regarded as the goal states in the reachability planning problems. 
For MDPSTs, however, this approach is not  applicable. To better understand this, let's first review the definition of (maximal) end-components ((M)EC) for MDPs.

\begin{definition}[EC for MDPs \cite{de1998formal,baier2008principles}]
An \emph{end component (EC)} of an MDP $\M$ is a sub-MDP $\M'$ of $\M$ such that its underlying graph is strongly connected. A \emph{maximal EC (MEC)} is maximal under set inclusion.
\end{definition}

\begin{lemma}[EC properties for MDPs. Theorems 3.1 and 4.2 of \cite{de1998formal}]\label{lem:mdp-mec}
    Once an end component $E$ of an MDP $\M$ is entered, there is a strategy that i) visits every state-action pair %combination 
    in $E$ infinitely often with probability 1, and ii) stays in $E$ forever.
\end{lemma}

Lemma~\ref{lem:mdp-mec} makes it possible to reduce a planning problem over MDPs with \LTL objectives to a reachability problem over the product MDP. This is due to the fact that, whenever a state 
$s^\times$ of an accepting MEC 
$E$ of the product MDP $\M^\times$ is reached, there exists a strategy of $\M^\times$ starting from $s^\times$ that ensures every state in 
$E$ (including the accepting states) will be visited infinitely often (according to Lemma \ref{lem:mdp-mec}), thereby satisfying the LTL objective.

For EC decomposition, MDPs can be seen as directed graphs where each state corresponds to a node and each action-labelled (probabilistic) transition corresponds to an edge.
However, this approach cannot be directly applied
to MDPSTs where transitions lead to set-valued successors rather than individual states.
For instance, consider a set-valued transition $\Theta^\times = \{s', s'', s'''\}\in \F^\times(s^\times, a)$ in a product MDPST $\M^\times$.
Here it is not sufficient to add edges for all pairs $(s^\times, s'), (s^\times, s''), (s^\times, s''')$, as the adversarial nature may prevent reaching some states in $\Theta^\times$ from $s^\times$. This poses a significant challenge in identifying the set of states in the product MDPST $\M^\times$ that are guaranteed to visit the set of accepting states $\acc^\times$ infinitely often with probability 1, an essential step in reducing the LTL planning problem to a reachability problem.
To address this challenge, we propose a procedure for identifying a set of states, called the Winning Region, in an MDPST that are guaranteed to visit a set of accepting states infinitely often with probability 1, defined formally below.

\begin{definition}[Winning Region for MDPSTs]\label{def:WR}
Given an MDPST $\M = (S, s_0, A, \mdpsttr, \probtr, \L)$ with a set of accepting states $\acc \subseteq S$, we say a set of states $W \subseteq S$ is a Winning Region (WR) for the MDPST $\M$ if, for every state $s\in W$, there exists a strategy $\sigma(s)$ starting from $s$ such that $\prob_{{\mathcal{M}}}^{\sigma(s)}(\square\diamondsuit \acc) = 1$.
\end{definition}



Next, we propose an algorithm for computing the WR $W^\times$ of the product MDPST $\M^\times$, which is outlined in Algorithm 1. The algorithm consists of the following steps. 

\input{Algorithm_computeWR}

First, we introduce an optimisation that computes a sub-MDPST $\M^\times_{sub}$ of $\M^\times$, which includes only states that are~(forward) reachable from the initial state $s^\times_0$ and~(backward) reachable from the set of accepting states ${\acc}^\times$ (line 1). Denote by i) $S_p\subseteq S^\times$ the set of states that can be reached from both the initial and accepting states and ii) $\M^\times_{sub} = (S_p, s^\times_0, A^\times, \F_p, \mathcal{T}_p, \L^\times)$ the sub-MDPST constructed from $\M^\times$ with respect to $S_p$ (an algorithm for computing $S_p$ and $\M^\times_{sub}$ can be found in \cite{yu2024trembling}). 

Second, we iteratively remove states in $S_p$ that cannot visit $\acc^\times$ infinitely often with probability 1 (lines 2-20). Before starting the iteration, a flag is set to 1 (line 2), indicating that the iteration should proceed. Each iteration begins by splitting the set of accepting states ${\acc}^\times$ into two virtual copies: i) $I_{in}$, which only has incoming transitions into
${\acc}^\times$, and ii) $I_{out}$, which only has outgoing transitions from ${\acc}^\times$ (line 4). 
Then a new state space can be defined as $\hat{S}:=(S_p \setminus {\acc}^\times)\cup I_{in}\cup I_{out}$ (line 5).

Over $\hat S$, we can construct a new product MDPST 
\begin{equation}\label{def:hat_mdpst}
    \hat{\mathcal{M}}^\times_{sub}=(\hat{S}, s^\times_{0}, \hat{A}^{\times}, \hat{\mathcal{F}}^{\times},  \hat{\mathcal{T}}^{\times}, \hat{\mathcal{L}}^{\times})
\end{equation}
which is equivalent to ${\M}^\times_{sub}$ (line 6). For each copy $s^{\rm in}\in I_{in}$ of an accepting state $s\in \acc^\times$, we assign only a self-loop transition. As a result, each time $s^{\rm in}$ is visited, it will be visited infinitely often. 
The detailed construction of $\hat{\mathcal{M}}^\times_{sub}$ can be found in Appendix A.

% \begin{itemize}
%     % \item $\hat{A}^{\times}=A^\times\cup \{\tau_0\}$ and $\tau_0$ is a self-loop action. The set of available actions $\hat{A}^{\times}(s)$ is defined as $\hat{A}^{\times}(s)=A^\times(s), \forall s\in S_p \setminus \acc^\times$, $\hat{A}^{\times}(s^{\rm out})=A^\times(s), \forall s \in \acc^\times$, and $\hat{A}^{\times}(s^{\rm in})=\tau_0, \forall s\in \acc^\times$, where $s^{\rm out}$ and $s^{\rm in}$ are respectively the virtual copies of $s\in \acc^\times$ in $I_{in}$ and $ I_{out}$.
%     \item $\hat{\F}^{\times}: \hat{S} \times \hat{A}^{\times} \Mapsto 2^{2^{\hat{S}^\times}}$ is the set-valued nondeterministic transition function. To define $\hat{\F}^{\times}$, we let $\Phi = \cup_{s\in S_p}\cup_{a\in {A}^{\times}} \cup_{\Theta\in \F_p(s, a)}\{\Theta\}$ be the set of all possible target (single- or set-valued) states originated from ${S}_p$. For each set $\Theta \in \Phi$, we define a \emph{copy} $\hat{\Theta}$ of $\Theta$ as i) $\hat{\Theta} = \Theta$ if $\Theta \subseteq S_p \setminus \acc^\times$, ii) $\hat{\Theta} = \{s^{\rm in}: s\in \Theta\}$ if $\Theta \subseteq \acc^\times$, and iii) $\hat{\Theta} = \{s: s\in \Theta \wedge s\in S_p \setminus \acc^\times\} \cup \{s^{\rm in}: s\in \Theta \wedge s \in \acc^\times\}$, otherwise. Then, one has that
%     \begin{itemize}
%         \item if $s\in S_p\setminus \acc^\times$, $\hat{\Theta} \in \hat{\F}^{\times}(s, a) \ \text{iff} \ \Theta \in \F_p(s, a)$;
%         \item if $s^{\rm out}\in I_{out}$, $\hat{\Theta} \in \hat{\F}^{\times}(s^{\rm out}, a) \ \text{iff} \ \Theta \in \F_p(s, a)$;
%         \item if $s^{\rm in}\in I_{in}$, $\hat{\F}^{\times}(s^{\rm in}, \tau_0) = s^{\rm in}$.
%     \end{itemize}
%    \item  $\hat{\mathcal{T}}^{\times}: \hat{S}\times \hat{A}^{\times} \times {2^{\hat{S}^\times}}\mapsto (0, 1]$ is the transition probability function, given by
%    \begin{itemize}
%        \item $\hat{\mathcal{T}}^{\times}(s, a, \hat{\Theta})= \mathcal{T}_p(s, a, {\Theta})$ for $s\in S_p\setminus \acc^\times$,
%        \item $\hat{\mathcal{T}}^{\times}(s^{\rm out}, a, \hat{\Theta})= \mathcal{T}_p(s, a, {\Theta})$ for $s^{\rm out}\in I_{out}$,
%        \item $\hat{\mathcal{T}}^{\times}(s^{\rm in}, \tau_0, s^{\rm in})= 1$ for $s^{\rm in}\in I_{in}$.       
%    \end{itemize}
%    \item $\hat{\L}^{\times}: \hat{S} \to 2^{Prop}$, where $\hat{\L}^{\times}(s)=\L^\times(s)$ if $s\in S_p\setminus \acc^\times$ and $\hat{\L}^{\times}(s^{\rm out})=\hat{\L}^{\times}(s^{\rm in})=\L^\times(s)$ otherwise.
% \end{itemize}
% Notice that each copy $s^{\rm in}\in I_{in}$ of an accepting state $s\in \acc^\times$ has only one self-loop transition. Thus, whenever $s^{\rm in}$ is visited, it will be visited infinitely many times. 

We now give a robust value iteration algorithm \cite{nilim2005robust} for computing the robust maximal probability of reaching $I_{in}$ from each state $s^\times \in \hat{S}$ (line 7). Define a value function $V_{sat}: \hat{S} \to \mathbb{R}_{\ge 0}$, where
\begin{equation*}\label{valuefunction_MDPST}
\begin{aligned}
    V_{sat}(s^\times)=&\max_{\sigma^\times(s^\times)}\min_{\gamma^\times}\big\{  \\
    &\hspace{-8mm}{\rm Pr}_{\hat{\mathcal{M}}^\times_{sub}}(\{\xi^\times\in \Omega_{\sigma^\times(s^\times), \gamma^\times}^{\hat{\mathcal{M}}^\times_{sub}}(s^\times) \mid \hat{\L}^{\times}(\xi^\times) \models \diamondsuit I_{in}\})\big\},
\end{aligned}    
\end{equation*}
which represents the robust maximal probability of reaching $I_{in}$ from $s^\times$. Then one can get that $V_{sat}(s^\times)=1, \forall s^\times \in I_{in}$.

It was shown in \cite{trevizan2007planning,yu2024trembling} that a simplified Bellman equation exists for MDPSTs. Therefore, for $s^\times\in \hat{S}\setminus I_{in}$,  
the robust dynamic programming operator $T$ can be designed as
\begin{equation}\label{VF_deterministic}
\begin{aligned}
    T(V_{sat})(s^\times)
    =\max_{a\in \hat{A}^\times(s^\times)}\Big\{&\sum_{\Theta\in \hat{\mdpsttr}^{\times}(s^\times, a)}\hat{\probtr}^\times(s^\times, a, \Theta) \\
    & \min_{s'\in \Theta}\{V_{sat}(s')\}\Big\}.
    \end{aligned}
\end{equation}

Once the robust value iteration converges and thus the optimal value function $V_{sat}$ is obtained, we first check whether there exists a state $s^{out} \in I_{out}$ such that $V_{sat}(s^{out}) \neq 1$. If such a state exists, we set flag to 1, and then remove the corresponding state $s$ from both $S_p$ and $\acc^\times$ (lines 8-10). Otherwise, the flag is set to 0 (11-12). The iteration continues if the flag is 1 and terminates once flag becomes 0 or $\acc^\times = \emptyset$. Once the iteration terminates, the algorithm returns the WR $W^\times$ (line 15). Our main result then follows.

\begin{theorem}\label{thm1}
    Given an MDPST ${\mathcal{M}}$ and an \LTL formula $\varphi$, the maximal probability of satisfying $\varphi$ is given by
    \begin{equation}\label{thm:correctness}
    \begin{aligned}
        \max_{\sigma\in \Pi_{\M}}\{\prob_{{\M}}^\sigma(\varphi)\}
        = \max_{{{\sigma^\times}}\in {\Pi_{{\M}^\times}}} \{\prob_{{{\M}}^\times}^{{{\sigma^\times}}}( \diamondsuit W^\times)\},
    \end{aligned}       
    \end{equation}
    where $W^\times$ is the WR computed by Algorithm 1.
\end{theorem}

\begin{proof}[Proof Sketch]
    We prove Theorem \ref{thm1} in two steps. First, we show the correctness of Algorithm 1, i.e., that the output $W^\times$ of Algorithm 1 is indeed the WR of the product MDPST $\M^\times$. Then, we show that (\ref{thm:correctness}) holds by verifying both sides of the inequality and subsequently constructing the induced policy on $\M$. 
\end{proof}

An example illustrating the steps of Algorithm 1 are provided in Appendix B. The full proof of Theorem \ref{thm1} can be found in Appendix C.

% \begin{remark}
%     For MDPs with LTL specifications, it is well known that the maximum probability of satisfying an LTL specification is equivalent to the maximum probability of reaching the accepting MECs in the product of the MDP and the DRA translation of the LTL formula \cite{baier2008principles}. However, we emphasize that Theorem \ref{thm1} is not a straightforward extension of this result due to: (1) the inherent nondeterminism in MDPSTs, characterized by set-valued rather than single-valued transitions, and (2) the presence of nondeterminism in LDBA, specifically in the form of $\epsilon$-transitions.
% \end{remark}


% The proof for direction $\geq$ is trivial since once we reach the winning region, we have a strategy against any nature to visit accepting states infinitely often from every state in the winning region with probability one.
% So, every induced run there is accepting and thus the computed probability is not greater than the semantic one. 
% For the other direction $\leq$, we just use the optimal strategy of $\M$ that obtains the optimal probability against all natures and resolves nondeterminism in $\aut$ using the strategy described in~\cite{sickert2016limit}.
% The full proof can be found in the arXiv version\cite{pian25}.

\subsection{Optimal robust strategy synthesis}\label{Sec:ors}

We have thus reduced Problem \ref{Prom1} to the reachability problem over $\M^\times_{sub}$, where the goal set is given by the WR $W^\times$. 
For states $s^\times \in W^\times,$ it holds that $V_{sat}(s^\times)=1$. For states $s^\times \in S_p\setminus W^\times$, the optimal value function $V_{sat}$ can be determined by conducting another run of the robust value iteration algorithm (\ref{VF_deterministic}).
The optimal robust strategy $\sigma^\times$ can be derived from $V_{sat}$ using standard methods.

\section{Experiments}

In this section, a case study is provided to demonstrate the effectiveness of our method. 
We implemented the solution technique proposed in Section IV in Python, and use Rabinizer 4 \cite{kvretinsky2018rabinizer} for the LTL-to-LDBA construction. For the robust value iteration, we set the convergence threshold to $10^{-3}$, i.e., the value iteration stops when $\max_{s\in S_l}\{|V^{k+1}_{sat}(s)-V^{k}_{sat}(s)|\}< 10^{-3}$. 
All simulations are carried out on a Macbook Pro (2.6 GHz 6-Core Intel Core i7 and 16 GB of RAM) and the implementation
code can be found at: \href{https://github.com/piany/MDPST-full-LTL}{https://github.com/piany/MDPST-full-LTL}.

We consider a mobile robot moving in the hexagonal world described in Example \ref{example2}, where the size of the workspace is denoted by $(N_x, N_y)$.
As explained, the robot dynamics can be abstracted as an MDPST.  The robot is required to persistently survey three goal regions while avoiding obstacles at all times. This task is expressed as the \LTL formula 
%$\varphi_{persistavoid}=(\square\lozenge \texttt{b1} \vee \texttt{b2}) \wedge (\square\lozenge \texttt{b3}) \wedge (\square\lozenge \texttt{b4} \vee \texttt{b5}) \wedge (\square\neg \texttt{obs}).$
\begin{equation*}
\begin{aligned}
   & \varphi_{persistavoid}=\\
    & \hspace{3mm}(\square\lozenge \texttt{b1} \vee \texttt{b2}) \wedge (\square\lozenge \texttt{b3}) \wedge (\square\lozenge \texttt{b4} \vee \texttt{b5}) \wedge (\square\neg \texttt{obs}).
    \end{aligned}
\end{equation*}

The corresponding LDBA derived using Rabinizer 4 has 4 states. For the scenario 
$(N_x, N_y)= (10, 5)$, the constructed product MDPST  
$\mathcal{M}^\times$ has 800 states and 5440 (single and set-valued) edges. The WR $W^\times$ is computed using Algorithm 1, which has 509 states. The initial state of the robot is $(q_1, N)$ and one can compute that $\max\{{\rm Pr}_{\mathcal{M}^\times} (\varphi_{persistavoid})\}=0.85$. The (MDPST, LDBA, and product MDPST) model construction took in total 0.467s and the strategy synthesis took 9.144s.

To highlight the computational advantage of LDBA over DRA, we also consider DRA as representations for the LTL task $\varphi_{persistavoid}$. The resulting DRA has 8 states, whereas the LDBA has only 4 states. 
We compare the performance of both representations in three scenarios: $(N_x, N_y) = (10, 5), (N_x, N_y) = (16, 8)$, and $(N_x, N_y) = (20, 10)$.
TABLE I shows the number of states and transitions  $(|S^\times|, |\mathcal{T}^\times|)$ of the product MDPST $\mathcal{M}^\times$, along with the model construction time $T_{mdl}$, and the strategy synthesis time $T_{sys}$ for each scenario $(N_x, N_y)$. The results clearly show that strategy synthesis with LDBA is significantly faster than with DRA, particularly as the workspace size increases.

\begin{table}[]
\caption{The number of states and transitions  $(|S^\times|, |\mathcal{T}^\times|)$ of the product MDPST $\mathcal{M}^\times$, the model construction time $T_{mdl}$, and the strategy synthesis time $T_{sys}$ for different automation choices $\mathcal{A}$ and different scenarios $(N_x, N_y)$.}
\begin{tabular}{c|c|c|c|c}
\hline
$(N_x, N_y)$              &  $\mathcal{A}$    & $(|S^\times|, |\mathcal{T}^\times|)$  & $T_{mdl} (s)$ & $T_{sys} (s)$ \\ \hline
\multirow{2}{*}{(10, 5)}  & LDBA & (800, 5440)                                                         & 0.888     & 10.745      \\ \cline{2-5} 
                          & DRA  & (1600, 10880)                                                       & 0.986      & 17.363     \\ \hline
\multirow{2}{*}{(16, 8)}  & LDBA & (2048, 14344)                                                       & 0.975       & 88.529     \\ \cline{2-5} 
                          & DRA  & (4098, 28688)                                                       & 1.696       & 231.343    \\ \hline
\multirow{2}{*}{(20, 10)} & LDBA & (3200, 22728)                                                       & 2.058       & 288.662    \\ \cline{2-5} 
                          & DRA  & (6400, 45456)                                                       & 1.962       & 748.126   \\ \hline
\end{tabular}
\end{table}


% In our implementation, we vary the size of the hexagonal world and examine several other LTL tasks. In all cases, strategy synthesis with LDBA consistently outperforms DRA in terms of speed. Due to space constraints, we have omitted the detailed results.

% We point out that, when calculating the AMSECs of the product MDPST $\M^\times$, it is not necessary to build the underlying graph over the entire state space of $\M^\times$. 
% We employed two methods to improve efficiency. First, we construct a sub-MDPST $\M_{sub}^\times$, which only consists of states that are i) (forward) reachable from the initial state $s_0^\times$ and ii) (backward) reachable from the set of accepting states $\acc^\times$. Second, given the sub-MDPST $\M_{sub}^\times$, one can always build a corresponding MDP $\M'$, where for each set-valued transition $\Theta \in \F^\times(s, a)$ in $\M_{sub}^\times$, the (single-valued) transition function $\T'$ in MDP $\M'$ is given by $\T'(s, a, s') = \T^\times(s, a, \Theta)/|\Theta|, \forall s'\in \Theta$. Over the MDP $\M'$, we then compute the complete set of AMECs $\E_{\M'}$. One can show that the AMSECs $\E$ of $\M_{sub}^\times$ constitute a subset of the AMECs of the corresponding MDP $\M'$. Therefore, in the end, one only needs to consider states that are contained in the AMECs of the MDP $\M'$. (The detailed algorithm
% for computing AMSECs of the product MDPST $\M^\times$ can be found in the implementation code).

\begin{figure}
\centering
\includegraphics[width=0.42\textwidth]{GFabc_traj.png}
	\caption{\footnotesize  Simulated trajectory of 2000 time steps for the LTL task $\varphi_{persistavoid}$, where the color bar denotes the time steps. }
	\label{Fig:gridworld_obs}
\end{figure}

To verify robustness, we perform
1000 Monte Carlo simulations of 2000 time steps for scenario $(N_x, N_y)= (10, 5)$. For each simulation, we randomly choose the set of parameters $\{\alpha_{s'}^{\theta}: \theta\in \mdpsttr^{\times}(s, a), s'\in \theta\}$ for each state-action pair $(s, a)$, to resolve the uncertainty for the set-valued transitions. We adopt
the optimal robust strategy for the strategy prefix and the Round-Robin strategy once the system enters the WR. The task $\varphi_{persistavoid}$ is satisfied 868 times out of the 1000 simulations, which verifies the probabilistic satisfaction guarantee.  Fig. \ref{Fig:gridworld_obs} depicts one of the simulated trajectories. One can see that the LTL specification $\varphi_{persistavoid}$ is satisfied.



\section{Conclusion}

This work studied the robot planning problem under both quantifiable and unquantifiable uncertainty, and subject to high-level \LTL task specifications. MDPSTs were proposed as a unified modelling framework for handling both types of uncertainties.  Additionally, a sound solution technique was introduced for synthesising the optimal robust strategy for MDPSTs with \LTL specifications.
For future work, we plan to explore the plan synthesis problem for MDPSTs, subject to both temporal logic specifications
and cost constraints.

% We consider a mobile robot working in the workspace described in Example 1. As explained, the robot dynamics can be abstracted as a MDPST, which contains 100 states. The
% costs for the 5 action primitives $\{\textsc{FR}, \textsc{BK}, \textsc{TR}, \textsc{TL}, \textsc{ST}\}$ are given by $[2, 4, 3, 3, 1]$, respectively. The robot is required to persistently survey the three base stations $\text{b1, b2, b3}$ while avoiding obstacles for all time. In LTL this task is expressed as the formula $\varphi_{gridavoid}$:
% \begin{equation}\label{task2}
%     \varphi_{gridavoid}=(\square\lozenge \texttt{b1}) \wedge (\square\lozenge \texttt{b2}) \wedge (\square\lozenge \texttt{b3}) \wedge (\square\neg \texttt{Obs}).
% \end{equation} 
% The associated LDBA derived using \cite{kvretinsky2018rabinizer} has 4 states. 
% The product MDPST $\mathcal{M}^\times$ is constructed and the set of MECs is computed. It took 0.189s to construct $\mathcal{M}^\times$, which has 400 states, 184 edges, and 4 accepting states. There are 2 MECs, of which 1 is accepting and it contains 264 states. The initial state of the robot is $(1, 1, N)$ and one can verify that $\max\{{\rm Pr}_{\mathcal{M}^\times} (\varphi_{persist})\}=0.8$. Firstly, the value function $V_{sat}$ is computed for states 
% $s\in S_l$ using the value iteration of Eqn (\ref{VF_deterministic}), which took 0.258s. Then we synthesise the constrained minimal cost optimal robust strategy for both states $s\in S_l$ and $s\in \Xi_{acc}$ via solving Algorithm 1 with $\beta=0.5, \gamma =0.75$, which took 0.279s. The resulting total expected cost ${\rm Exp}_{{\mathcal{M}}_N}^{\bar\sigma}[C]= $ and the probability ${\rm Pr}_{{\mathcal{M}}_N}^{\bar\sigma} (\varphi) = $.

% We consider two cases for comparison. The first one adopts the optimal robust strategy for the prefix and the Round-Robin strategy once the system enters the AMEC and the second one applies the obtained minimal cost optimal robust strategy. We perform 100 Monte Carlo simulations of 500 time steps each, where we evaluate the total cost for one execution of the accepting cyclic path (i.e., the suffix). On average, the costs for one execution of the accepting cyclic path for the two cases are 1032.3 and 57.6, respectively. One can see that the minimal cost optimal robust strategy results in much cheaper strategy suffix. In Figures \ref{Fig:gridworld_noobs_round_rabin} and  \ref{Fig:gridworld_noobs}, we plot simulated trajectories for the former and latter cases, respectively, where the colour bar indicates the time evolution. For better visualization, we plot the prefix (12 steps for both cases) and one execution of the accepting cyclic path (400 steps for the former case and 27 steps for the latter case). Using the minimal cost optimal robust strategy, the average number of times each base station is visited by the robot is much higher than using the Round-Robin strategy (1 vs 15).

% \begin{figure}
% \centering
% \includegraphics[width=0.4\textwidth]{figure_noobs_round_rabin.pdf}
% 	\caption{\footnotesize  Simulated trajectory for the LTL task $\varphi_{persist}$. The optimal robust strategy is applied for the prefix and the Round-Robin strategy is adopted once the system enters the AMEC.}
% 	\label{Fig:gridworld_noobs_round_rabin}
% \end{figure}

% \begin{figure}
% \centering
% \includegraphics[width=0.4\textwidth]{figure_noobs.pdf}
% 	\caption{\footnotesize  Simulated trajectory for the LTL task $\varphi_{persist}$. The minimal cost optimal robust strategy obtained via solving the LP (\ref{overall}) with $\beta=0.5$ is applied.}
% 	\label{Fig:gridworld_noobs}
% \end{figure}


\iffalse
\begin{definition}[EC for MDPs]\cite{de1998formal,baier2008principles}
Given an MDP $\M =(S, s_0, A, \T)$, where $S, s_0, A$ are the same as defined in MDPSTs and $\T$ is a probabilistic (single-valued) transition function. An \emph{EC} of $\M$ is pair $(S', A')$, where $S'\subseteq S$ and $A': S' \to 2^{A}$ is a map that associates each state $s\in S'$ with a subset of available actions $A'(s')$ such that, its underlying graph is strongly connected. A \emph{maximal EC (MEC)} is maximal under set-inclusion.
\end{definition}

\begin{lemma}[EC properties]\cite{de1998formal}
    Once an end-component $E$ of an MDP $\M$ is entered, there is a strategy that i) visits every state-action combination in $E$ infinitely often with probability 1 and ii) stays in $E$ forever.
\end{lemma}

Analogous to MDP, a direct adaptation of the definition of EC for MDPSTs is given as follows.

\begin{definition}[EC for MDPSTs]\label{def:EC_mdpst}
    An end component (EC) of an MDPST $\M = (S, s_0, A,  \mdpsttr, \probtr)$ is a pair $(S', A')$, where $S'\subseteq S$ and $A': S' \to 2^{A}$ such that, after taking an action $a\in A'(s')$, all possible target states remain in $S'$, i.e., $\post_{\M}(s, a)\subseteq S'$.
\end{definition}

We note, however, that for MDPSTs, an EC $E=(S', A')$ defined according to Definition \ref{def:EC_mdpst} does not satisfy the EC properties. To be more specific, once an EC $E$ of the MDPST $\M$
is entered, it guarantees that there is a strategy $\sigma$ that stays in $E$ forever. Nevertheless, the strategy $\sigma$ does not guarantee that every state-action combination in $E$ is visited infinitely often with probability 1. For instance, consider that $S'=\{s_1, s_2, s_3, s_4\}$ and $A'=\{a\}$ and $F(s_1, a)=\{(s_2, s_3), s_4\}$, $F(s_2, a)=\{s_4\}$, $F(s_3, a)=\{(s_1, s_2)\}$, and $F(s_4, a)=\{s_2\}$. Notice that in MDPSTs, the unquantifiable uncertainty, i.e., the nondeterminisim in the transition set, is resolved by nature. Assume that at state-action $(s_3, a)$, the nature persistently choose to transition to state $s_1$ rather than $s_2$, and then at state-action $(s_1, a)$, the nature persistently choose to transition back to state $s_3$ rather than $s_2$. In this case, the robot may never visit the state $s_2$ in the EC.  

To resolve this problem, we define a set-valued end component (SEC) for MDPSTs.

Let $\Xi:={\cup_{s\in S}\cup_{a\in A(s)}\cup_{\Theta\in \F(s, a)} \Theta}$ be the set of all possible target sets for an MDPST $\M$. Define 
\begin{equation}\label{eqn:state}
    \hat{\Xi}=S\cup \Xi.
\end{equation}
The transition system over $\hat{\Xi}$ is then defined as $\G_{\M} =(\hat{\Xi}, A, F_T)$, where $F_T: \hat{\Xi}\times A \Mapsto 2^{\hat{\Xi}}$ is given by
\begin{equation}\label{eqn:transition}
    F_T(\Theta, a) = \{\Theta': \exists s\in \Theta, \Theta'\in \F(s, a)\}.
\end{equation}

Notice that $\hat{\Xi}$ includes states that are set-valued. We can then define EC for $\G_{\M}$, which is a pair $(\Xi', A')$ such that its underlying graph is strongly
connected. Moreover, the EC properties are held for $\G_{\M}$, i.e., once an EC of $\G_{\M}$ is entered, there is a strategy that visits every state-action combination in EC infinitely often with probability 1 and stays in EC forever. 

Next, we show that the EC computed for the transition system $\G$ (which includes set-valued states) can be used for MDPSTs.

\begin{definition}[Set-valued end-component for MDPSTs]\label{def:SEC_mdpst}
    The set-valued end component (SEC) of an MDPST $\mathcal{M}=(S, s_0, A, \F, \T, \C)$ is defined as the EC of the corresponding transition system $\G_{\M}=(\hat{\Xi}, A, F_T)$, where $\hat{\Xi}$ and $F_T$ are given by (\ref{eqn:state}) and (\ref{eqn:transition}), respectively.
\end{definition}
\fi


\bibliographystyle{IEEEtran}
\bibliography{references,ref}

% \newpage
% \section*{Appendix}

% \subsection{Proof of Theorem 1}

\newpage

\input{appendix}

\end{document}
