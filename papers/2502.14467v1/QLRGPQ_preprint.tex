\documentclass[10pt]{article} 
\usepackage[preprint]{tmlr}

\input{math_commands.tex}
\usepackage{mathtools}
\usepackage{braket}
\usepackage{graphicx}%
\usepackage{dcolumn}%
\usepackage{bm}%
\usepackage{comment}
\usepackage{color}
\usepackage{xcolor}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{url}
\usepackage{tabularx}
\usepackage[normalem]{ulem}
\usepackage[hidelinks,colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usetikzlibrary{quantikz2}

\definecolor{myblue}{rgb}{0.2, 0.0, 0.6}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{myblue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}

\definecolor{Color1}{HTML}{8f0606}
\definecolor{Color2}{HTML}{008080}
\definecolor{Color3}{HTML}{9C19E0}

\title{Provable Quantum Algorithm Advantage for Gaussian Process Quadrature}



\author{\name Cristian A. Galvis-Florez \email cristian.galvis@aalto.fi \\
	\addr Department of Electrical Engineering and Automation\\
	Aalto University, Finland
	\AND
	\name Ahmad Farooq \email ahmad.farooq@aalto.fi \\
	\addr Department of Electrical Engineering and Automation\\
	Aalto University, Finland
	\AND
	\name Simo S\"arkk\"a \email simo.sarkka@aalto.fi\\
	\addr Department of Electrical Engineering and Automation\\
	Aalto University, Finland}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{02}  %
\def\year{2025} %
\def\openreview{\url{https://openreview.net/forum?id=K6CvWPtF62}} %


\begin{document}
	
	
	\maketitle
	\begin{abstract}
		The aim of this paper is to develop novel quantum algorithms for Gaussian process quadrature methods. Gaussian process quadratures are numerical integration methods where Gaussian processes are used as functional priors for the integrands to capture the uncertainty arising from the sparse function evaluations. Quantum computers have emerged as potential replacements for classical computers, offering exponential reductions in the computational complexity of machine learning tasks. In this paper, we combine Gaussian process quadratures and quantum computing by proposing a quantum low-rank Gaussian process quadrature method based on a Hilbert space approximation of the Gaussian process kernel and enhancing the quadrature using a quantum circuit.
		The method combines the quantum phase estimation algorithm with the quantum principal component analysis technique to extract information up to a desired rank. Then, Hadamard and SWAP tests are implemented to find the expected value and variance that determines the quadrature.
		We use numerical simulations of a quantum computer to demonstrate the effectiveness of the method. Furthermore, we provide a theoretical complexity analysis that shows a polynomial advantage over classical Gaussian process quadrature methods.  The code is available at \url{https://github.com/cagalvisf/Quantum_HSGPQ}.
	\end{abstract}
	\newpage
	\section{Notation}
	\centerline{\bf Classical notation}
	\bgroup
	\def\arraystretch{1.5}
	\begin{tabular}{p{1.25in}p{4in}}
		$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\
		$\mathcal{D}$ & Data set $\mathcal{D} = \{(\mathbf{x}_i,\mathbf{y}_i) \mid i = 1,\dots, N\}$\\
		$\mathrm{E}[{f}(\mathbf{x}_*)\mid \mathcal{D}]$  & Expectation of $f(\mathbf{x})$ at $\mathbf{x} = \mathbf{x}_*$ given the data set $\mathcal{D}$ \\
		$k(\mathbf{x}, \mathbf{x}')$ & Kernel function evaluated at $\mathbf{x}$ and $\mathbf{x}'$\\
		$K$ or $K(X,X)$ & $N \times N$ covariance (or Gram) matrix\\
		$\displaystyle \mathcal{N} ( \vx \mid \vmu , \mSigma)$ & Gaussian distribution %
		over $\vx$ with mean $\vmu$ and covariance $\mSigma$ 
		\\
		$\mathrm{V}[{f}(\mathbf{x}_*)\mid \mathcal{D}]$  & Variance of $f(\mathbf{x})$ at $\mathbf{x} = \mathbf{x}_*$ given the data set $\mathcal{D}$ \\
		$\mathbf{x}^\top$ & Transpose of the vector $\mathbf{x}$\\
		
		$\delta_{ij}$ & Kronecker delta, $\delta_{ij} = 1$ iff $i=j$ and $0$ otherwise\\
		$\lambda$ & Eigenvalue of an eigenfunction or singular value of a matrix\\
		
		$\sigma$ & Standard deviation of noise or a scale parameter\\
		$\phi_i(\mathbf{x})$ & The $i$th eigenfunction of the Laplace operator\\
	\end{tabular}
	\egroup\\
	
	\centerline{\bf Quantum computing notation}
	\bgroup
	\def\arraystretch{1.5}
	\begin{tabular}{p{1.25in}p{4in}}
		$\otimes$ & Tensor product \\
		$\ket{\cdot}$ & Ket notation of a column vector \\
		$\bra{\cdot}$ & Bra notation for the conjugate transpose of $\ket{\cdot}$\\
		$\braket{\cdot|\cdot}$ & Braket or inner product between two vectors \\
		$\ket{\cdot}\ket{\cdot}$ & Tensor product of the two separate ket vectors \\
		$\ket{\cdot}\bra{\cdot}$ & Outer product between two vectors\\
		$\ket{j}$ & Orthonormal basis state of integer $j$\\
		$\ket{j_1}\otimes\ket{j_2} =$  & Tensor product of quantum systems\\
		$ \ket{j_1}\ket{j_2} = \ket{j_1 j_2}$ &\\
		$\ket{\xi}$ & Used for intermediate states in the quantum algorithms\\
		$\bra{\psi}U \ket{\psi}$ or $\braket{U}_\psi$ & Expected value of U over the quantum state $\bra{\psi}$\\
		$\ket{\psi_\mathbf{X}}$ & Quantum state encoding the data matrix $\mathbf{X} $\\
		$ \mathrm{Im}\left(\cdot\right)$ & Imaginary part of the value \\
		$p_i$ & Probability to measure the quantum system in the state $\ket{i}$\\
		$ \mathrm{Re}\left(\cdot\right)$ & Real part of the value \\
		
		$\mathrm{tr}_{n}\left(\cdot\right)$ & Partial trace over $n$th subsystem \\
		$U$ & Unitary quantum operator. Matrix with components $U_{ij}=\bra{i}U\ket{j}$ \\
		$\tilde{\sigma}$ & Density matrix \\
		$\rho$ & Density matrix \\
		$\tilde{\rho}^{(n)}$ & Density matrix of $n$th subsystem 
	\end{tabular}
	\egroup
	\vspace{0.25cm}
	
	\color{black}
	
	\section{\label{sec:Introduction} Introduction}
	
	The integration of analytically intractable functions is usually addressed by numerical methods such as classical quadrature rules \citep{Philip1986NumericalIntegration}. However, alternatives to the classical quadrature rules are Bayesian quadratures which formulate the numerical integration problem as a Bayesian inference problem over the integral value, hence allowing for quantification of uncertainty arising from the finite number of function evaluations \citep{Hagan1991BHQuadrature, Minka2000QuadraturesGPR, Hennig2022ProbabilisticNumerics}. In particular, a common way is to consider a Gaussian prior distribution for the integrand, which leads to so-called Gaussian process quadratures (GPQs) \citep{Minka2000QuadraturesGPR, Hennig2022ProbabilisticNumerics}. However, the GPQ methods struggle with computational efficiency, especially when handling large datasets since the algorithm complexity scales as $O(N^3)$, with $N$ being the number of evaluation points, which is a problem inherited from the similar complexity challenge in Gaussian process regression.
	
	Quantum computers have emerged as a novel approach for general computational purposes, showing even exponential speedups in computational complexity over their classical counterparts.
	Quantum computers exploit superposition, entanglement, and interference to perform a task, which substantially reduces the computational complexity of an algorithm \citep{Nielsen2011QuantumComputing}.
	The Harrow-Hassidim-Lloyd (HHL) algorithm \citep{Harrow2009HHLalgorithm}, quantum kernel methods \citep{schuld2019quantum}, quantum phase estimation \citep[QPE,][]{kitaev1995QPE}, and quantum amplitude amplification \citep[QAA,][]{Brassard2002QAA} are quantum algorithms that offer a computational advantage in different numerical tasks \citep{Schuld2016QLinearRegresion, Shor1994ShorAlgorithm, Grover1996GroverAlgorithm, wiedemann2023quantum,canatar2023bandwidth}. Quantum algorithms for numerical integration in the classical setting have been proposed by \citet{Yu2020PracticalIntegrationNISQ} and \citet{ Shu2024GeneralQAforIntegration}. Furthermore, \citet{Vazquez2021EfficientQAE} and \citet{Martinez2023QFIterativeAmplitude} implement quantum algorithms that have a quadratic speedup with respect to Monte Carlo integration methods. These integration methods for classical numerical integration on a quantum computer have been proposed as an application of efficient state preparation for quantum amplitude estimation \citep{Martinez2023QFIterativeAmplitude}.
	
	The contribution of this paper is to propose a quantum Bayesian quadrature algorithm for numerical integration.  A naive numerical integration algorithm using a Gaussian quadrature on a non-quantum classical computer would have a complexity of $O(N^3)$, which is inherited from the complexity of Gaussian process regression \citep{Rasmussen2006GPforML}. In this paper, we develop a low-rank quantum Gaussian process quadrature method for numerical integration employing a low-rank Gaussian process prior, using the GPR method in \citet{Farooq2024QAHSGPR} as the starting point.
	Our algorithm harnesses the quantum Fourier transform, quantum phase estimation, and quantum principal component analysis to exploit the superposition of quantum states for computing the matrix inverse in GPQ, thereby providing an advantage over the classical approach. The proposed method begins by implementing a Hilbert space kernel approximation \citep{Solin2020HilbertSpaceGPR} using a classical computer to evaluate the basis function integrals at the evaluation points.
	Then, the resulting data matrix needs to be loaded into a quantum computer efficiently, and for this purpose, the approximate quantum amplitude encoding scheme is implemented \citep{Nakaji2022ApproximateAmplitudeEncoding}.
	Using quantum principal component analysis (qPCA) \citep{Lloyd2014QPCA} and QPE we extract information about the dominant eigenvalues of the previously encoded data matrix. 
	This information is used to implement conditional rotations that allow the estimation of the expected value and the variance of the quadrature using a Hadamard and SWAP test respectively \citep{Schuld2021QML, Buhrman2001QFingerprinting}.
	We provide the numerical simulation of our proposed solution for quantum low-rank Gaussian process quadrature in a classical processor that simulates a quantum computer, allowing us to compare it with classical Gaussian process quadrature methods.
	
	Our method also provides a polynomial computational advantage compared to classical methods for Gaussian process quadrature. The quantum algorithm implemented in this paper delivers a complexity $O(\log(M)\kappa^2 s^2/\epsilon)$, where $\kappa$ is the condition number, $s$ is the sparsity of the matrix, $\epsilon$ is the desired level of accuracy, and $M$ is the number of basis functions used in the Hilbert space approximation (usually $M \ll N)$, with given quantum data, and $O(\operatorname{poly}(\log (NM))\log(M) \kappa^2 \epsilon^{-3})$ when data is given classically. The computational complexity of a GPQ method implemented on a classical computer with the Hilbert space method would be $O(M^3)$ and hence, the advantage is polynomial.
	
	The paper is organized as follows. In Section \ref{sec: Classical method} we briefly review the Gaussian process quadrature methods and show how to approximate them with the Hilbert space method. In Section \ref{sec: Quantum algorithms} we summarize the generic quantum algorithms used in this paper. Section \ref{sec: Quantum GPQ} encompasses the core of this work, presenting the quantum low-rank Gaussian process quadrature method and discussing its computational complexity. In Section \ref{sec: Simulations} numerical simulations using a classical computer are shown. Section \ref{sec: Conclusions} concludes the work.
	
	\section{\label{sec: Classical method}Hilbert Space Approximation for Gaussian Process Quadrature}
	
	Gaussian processes $f \sim \mathcal{GP}\left(0, k\left(\mathbf{x},\mathbf{x}^\prime\right)\right)$ can be used as functional prior distributions to model unknown functions $f$ over a $d$-dimensional inputs $\mathbf{x}$ on to a space $\Omega \subseteq \mathbb{R}^d$. The covariance function $k\left(\mathbf{x},\mathbf{x}^\prime\right)$ defines the properties of the unknown function $f$ and the mean can often be assumed to be zero, as we do here. Regression of a function $f(\mathbf{x})$ can be performed using the Gaussian process as the prior distribution in a process known as Gaussian process regression (GPR) \citep{Rasmussen2006GPforML}. 
	Then, a set of $N$ noisy measurements $\mathcal{D} = \{\mathcal{X} = \{\mathbf{x}_1,\cdots,\mathbf{x}_N\}, \mathbf{y} = (y_1,\cdots,y_N)\}$ with $y_i = f(\mathbf{x}_i) + \varepsilon_i$, where  $\varepsilon_i \sim \mathcal{N}\left(0,\sigma^2\right)$, induce a posterior Gaussian distribution $ p\left(f(\mathbf{x}_*) \mid \mathbf{x}_{*},\mathcal{D}\right) =  \mathcal{N}\left(f(\mathbf{x}_*)\mid\mathrm{E}[{f}(\mathbf{x}_*)], \mathrm{V}\left[{f}(\mathbf{x}_*)\right]\right)$ on a new point $\mathbf{x}_*$ given by 
	\begin{eqnarray}
		\mathrm{E}[{f}(\mathbf{x}_*)\mid \mathcal{D}] &=& \mathbf{k}_*^{\top}\left(\mathbf{K}+\sigma^2 \mathbf{I}\right)^{-1} \mathbf{y},\label{eq: Classic GPR} \\
		\mathrm{V}\left[{f}(\mathbf{x}_*)\mid \mathcal{D} \right] &=& k\left(\mathbf{x}_{*}, \mathbf{x}_{*}\right)-\mathbf{k}_*^{\top}\left(\mathbf{K}+\sigma^2 \mathbf{I}\right)^{-1} \mathbf{k}_* .\label{eq: Classic GPR var}
	\end{eqnarray}
	Here, $\mathbf{K}$ is a $N\times N$ matrix with entries $K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$ and $\mathbf{k}_*$ is a vector with the $i$th entry being $k(\mathbf{x}_*, \mathbf{x}_i)$. 
	
	The choice of the covariance or kernel function defines the properties of the Gaussian process regression solution. A common kernel choice for GPR is the squared exponential covariance function \citep{Rasmussen2006GPforML}
	\begin{equation}
		k\left(\mathbf{x},\mathbf{x}^\prime\right)=\sigma_{f}^{2}\exp\left(-\frac{1}{2l^{2}}\|\mathbf{x} - \mathbf{x}^\prime\|^2\right),
		\label{eq: squared exp kernel}
	\end{equation}
	where $\sigma_{f}$ and $l$ are the signal scale and length scale hyperparameters respectively. 
	Gaussian processes can also be used to construct Bayesian quadrature rules leading to so-called Gaussian process quadratures \citep{Minka2000QuadraturesGPR, Hennig2022ProbabilisticNumerics} which we discuss next.
	
	
	
	
	
	
	
	
	
	The objective of a Gaussian process quadrature is to estimate the integral of a given function $f(\mathbf{x})$ over a domain $\mathbf{x} \in \Omega$ against a weight function $\mu(\mathbf{x})$, that is,
	\begin{equation}
		\mathcal{I} = \int_\Omega f(\mathbf{x})\mu(\mathbf{x}) d\mathbf{x}.
	\end{equation}
	
	To perform this estimation, we consider a Gaussian process approximation of the function $f$ conditional on a finite set of evaluation points $\mathbf{y}$ with $y_i = f(\mathbf{x}_i) + \epsilon_i$ at some given evaluation points $\mathbf{x}_i$. Then the conditional expected value of the integral given this data $\mathcal{D}$ is
	\begin{equation}
		\begin{aligned}
			\hat{\mathcal{I}}  &= \int_\Omega \mathrm{E}[{f}(\mathbf{x})\mid \mathcal{D}]\mu(\mathbf{x}) d\mathbf{x}\\
			&= \left[\int_\Omega k(\mathcal{X},\mathbf{x})\mu(\mathbf{x})d\mathbf{x}\right]^{\top}(\mathbf{K} + \sigma^2 I)^{-1} \mathbf{y},
		\end{aligned}
	\end{equation}
	which can be used as an estimator for the integral $\hat{\mathcal{I}} \approx \mathcal{I}$. Similarly, we can evaluate the variance of the estimator. The resulting conditional expected value and variance are given by \citep{Hennig2022ProbabilisticNumerics, Karvonen2017ClassicalquadraturefromGP}
	\begin{eqnarray}
		Q_{\mathrm{BQ}} &:=& k_\mu(\mathcal{X})^{\top}\left(\mathbf{K}+\sigma^2 \mathbf{I}\right)^{-1} \mathbf{y},\label{eq: Classic GPQ mean} \\
		V_{\mathrm{BQ}} &:=&\mu\left(k_\mu\right)-k_\mu(\mathcal{X})^{\top} (\mathbf{K} + \sigma^2 I)^{-1} k_\mu(\mathcal{X}), \label{eq: Classic GPQ var}
	\end{eqnarray}
	where the $i$-th component of the vector  $k_\mu(\mathcal{X})$ is $k_\mu(\mathbf{x}_i) = \int_\Omega k(\mathbf{x}_i,\mathbf{x})\mu(\mathbf{x})d\mathbf{x}$ and the scalar $\mu\left(k_\mu\right) = \int_\Omega\int_\Omega k(\mathbf{x},\mathbf{x}')\mu(\mathbf{x}) d\mathbf{x} \mu(\mathbf{x}') d\mathbf{x}'$. The resulting quadrature $Q_{\mathrm{BQ}}$ approximates the integral $\mathcal{I}$ by performing integration over the kernel rather than the function $f$ itself, and by using these values it forms weights for the function values. There exist a number of methods that can be used to reduce the complexity of Gaussian process regression such as inducing point methods \citep{Quinonero2005UnifyingGPR} and Hilbert space kernel approximations \citep{Solin2020HilbertSpaceGPR}, which can be used to speed up the GPQ rule.
	
	In this paper, we implement the Hilbert space method of \citet{Solin2020HilbertSpaceGPR} on the quadrature rule to reduce the complexity of the operation. We start by considering the eigenvalue problem of the Laplace operator in a well-behaved domain $\zeta$ such that the eigenfunctions and eigenvalues of the operator exist. The solution would give a set of eigenfunctions $\phi_j(\mathbf{x}) \in \zeta$ with correspondent real and positive eigenvalues $\lambda_j$ \citep{Arfken2013MathPhysics}. The eigenfunction are orthonormal respect to the inner product $\int \phi_j \phi_i d\mathbf{x} = \delta_{ij}$, defining a Hilbert space over $\zeta$.
	
	If the kernel function is isotropic, that is,  $k(\mathbf{x}, \mathbf{x}') = k(||\mathbf{x}-\mathbf{x}'||)$, then the covariance function can be approximated in the domain $\zeta$ as \citep{Solin2020HilbertSpaceGPR}
	\begin{equation}
		k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \approx \sum_{j = 1}^{M} S\left(\sqrt{\lambda_j}\right) \phi_j(\mathbf{x}) \phi_j\left(\mathbf{x}^{\prime}\right),
		\label{eq: Kernel approximation}
	\end{equation}
	where the spectral density $S(\omega)$ is the Fourier transform $(\mathcal{F})$ of the scalar covariance function $k(\tau) \xrightarrow[]{\mathcal{F}} S(\omega)$ where $\tau = ||\mathbf{x}-\mathbf{x}'||$. 
	
	Equations~(\ref{eq: Classic GPQ mean}) and~(\ref{eq: Classic GPQ var}) can be written in terms of the approximated kernel, giving
	\begin{eqnarray} 
		Q_{\mathrm{BQ}}&\approx \boldsymbol{\Phi}_\mu^{\top}(\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} + \sigma^2 \boldsymbol{\Lambda}^{-1})^{-1} \boldsymbol{\Phi}^{\top} \mathbf{y}\label{eq: HSGPQ mean}  , \\
		V_{\mathrm{BQ}}&\approx \sigma^2 \boldsymbol{\Phi}_\mu^{\top}(\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} + \sigma^2 \boldsymbol{\Lambda}^{-1})^{-1}\boldsymbol{\Phi}_{\mu}\label{eq: HSGPQ var},
	\end{eqnarray}
	where the vector $\boldsymbol{\Phi}_\mu$ has the components ${\Phi_{\mu}}_i = \int_{\Omega} \phi_i(\mathbf{x})\mu(\mathbf{x})d\mathbf{x}$, $\boldsymbol{\Lambda}$ is a diagonal matrix with components $\boldsymbol{\Lambda}_{jj} = S(\sqrt{\lambda_j})$ and the matrix $\boldsymbol{\Phi}$ has components $\Phi_{ij} = \phi_j(\mathbf{x}_i)$. 
	The approximation of the kernel now depends on the domain $\zeta$, which is not necessarily the same as the integration domain $\Omega$. The matrix in Eqs.~(\ref{eq: HSGPQ mean}) and~(\ref{eq: HSGPQ var}) that needs to be inverted now has rank $M$ instead of $N$.
	This kernel approximation reduces the computational complexity involved in matrix inversion provided that $M \ll N$. 
	
	The expected value and variance expressions of the quadrature can be decomposed using singular value decomposition (SVD) to embed them into quantum states \citep{Schuld2016QLinearRegresion}. Some modifications of Eqs.~(\ref{eq: HSGPQ mean}) and~(\ref{eq: HSGPQ var}) must be done before the implementation of the algorithm \citep{Farooq2024QAHSGPR}.  For the GPQ, we need to obtain the eigenvalues and eigenvectors of $\left( \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} + \sigma^2 \boldsymbol{\Lambda}^{-1} \right)$, which we aim to express in terms of $\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi}$. By writing the expected value and variance of the quadrature in terms of this common set of eigenvectors, the expected value and variance can be written in terms of quantum states.
	
	Similarly to \citet{Farooq2024QAHSGPR} we start by considering $\mathbf{X} = \boldsymbol{\Phi}\sqrt{\boldsymbol{\Lambda}}\in \mathbb{R}^{N\times M}$, where $\sqrt{\boldsymbol{\Lambda}}$ is a diagonal matrix with elements $\sqrt{\boldsymbol{\Lambda}_{ii}} = \sqrt{S(\sqrt{\lambda_i})}$, the quadrature equations  take the form
	\begin{eqnarray} \label{mean:1}
		Q_{\mathrm{BQ}}&=&\mathbf{X}_\mu^{\top}\left(\mathbf{X}^{\top}\mathbf{X}+
		\sigma^{2} \mathbf{I}\right)^{-1}\mathbf{X}^{\top}  \mathbf{y},\\ \label{varianve:1}
		V_{\mathrm{BQ}}&=&\sigma^{2} \mathbf{X}_\mu^{\top}\left(\mathbf{X}^{\top}\mathbf{X}+\sigma^{2} \mathbf{I} \right)^{-1} \mathbf{X}_\mu,
	\end{eqnarray}
	where $\mathbf{X_\mu^{\top}} = \boldsymbol{\Phi}_\mu^{\top} \sqrt{\boldsymbol{\Lambda}}$. Now the eigenvectors of $\mathbf{X}^{\top}\mathbf{X}+\sigma^{2} \mathbf{I}$ are the same as those of $\mathbf{X}^{\top}\mathbf{X}$. Previous equations are referred to as the Hilbert space quadrature (HSQ). In this paper, we use a quantum computer to calculate these quantities, which we call the quantum Hilbert space quadrature method (QHSQ).  The variables $\mathbf{X}$ and $\mathbf{X_\mu}$ are computed on a classical computer before we feed them into the quantum computer.
	
	
	Using singular value decomposition (SVD) to the data matrix $\mathbf{X}$ it is possible to write the expected value and variance of the GPQ in terms of quantum states. 
	Let $\mathbf{X}=\mathbf{U}\boldsymbol{\Sigma} \mathbf{V}^{\top}$ be the SVD of the data matrix $\mathbf{X}$. 
	The matrix $\boldsymbol{\Sigma}\in \mathbb{R}^{R\times R}$ is diagonal, containing the real singular values $\lambda_1,\lambda_2, \ldots,\lambda_R$, being $R$ the rank of the matrix $\mathbf{X}$. 
	The orthogonal matrices $\mathbf{U}\in \mathbb{R}^{N \times R}$ and $\mathbf{V}\in \mathbb{R}^{R \times M}$ correspond to the left and right singular vectors, respectively.
	Thereof, the SVD of $\mathbf{X}^{\top}\mathbf{X}+\sigma^{2}\mathbf{I} = \mathbf{V}\boldsymbol{\Sigma}^{\prime} \mathbf{V}^{\top}$ leads to the diagonal matrix $\boldsymbol{\Sigma}^{\prime}$ with elements $\boldsymbol{\Sigma}^{\prime}_{ii} = \lambda_{i}^{2} + \sigma^2$. 
	Then, the eigendecomposition of  $\left(\mathbf{X}^{\top}\mathbf{X}+\sigma^{2} \mathbf{I}\right)^{-1}\mathbf{X}^{\top}$ is given by
	\begin{equation} 
		\left(\mathbf{X}^{\top}\mathbf{X}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{X}^{\top} = \mathbf{V}\boldsymbol{\Sigma}^{\prime\prime}\mathbf{U}^{\top} =\sum_{r=1}^{R} \frac{\lambda_{r}}{\lambda_{r}^{2}+\sigma^{2}}
		\mathbf{v}_{r} \mathbf{u}_{r}^{\top},
		\label{eq: eigendecomposition}
	\end{equation}
	where $\boldsymbol{\Sigma}^{\prime\prime}$ is diagonal with components $\boldsymbol{\Sigma}^{\prime\prime}_{ii} = \frac{\lambda_{i}}{\lambda_{i}^{2}+\sigma^{2}}$ and the vectors $ \mathbf{u}_{r}$ and $\mathbf{v}_{r}$ correspond to the $r$-th column vectors of the matrices $\mathbf{U}$ and $\mathbf{V}$ respectively. 
	Then, using the SVD, the expected value and variance of the GPQ can be written as
	\begin{eqnarray}
		Q_{\mathrm{BQ}}&=& \sum_{r=1}^{R} \frac{\lambda_{r}}{\lambda_{r}^{2} + \sigma^{2}} \mathbf{X}_\mu^{\top} \mathbf{v}_{r} \mathbf{u}_{r}^{\top}\mathbf{y}, \label{eq: SVD mean GPQ}\\
		V_{\mathrm{BQ}}&=&\sigma^{2}\sum_{r=1}^{R} \frac{1}{\lambda_{r}^{2}+\sigma^{2}} \mathbf{X}_\mu^{\top}  \mathbf{v}_{r}\mathbf{v}_{r}^{\top}\mathbf{X}_\mu.\label{eq: SVD variance GPQ}
	\end{eqnarray}
	
	The SVD in the expected value and variance of the GPQ allows us to write them as expected values of quantum operators, extending the complexity reduction provided by the Hilbert space approximation of the kernel into the quantum algorithm.
	
	
	\section{\label{sec: Quantum algorithms}Quantum Algorithm Background}
	
	In this section, we will review the quantum algorithms needed to implement the quantum low-rank Gaussian process quadrature. Readers who are familiar with fundamental quantum algorithms can skip this section and directly move to the next Section \ref{sec: Quantum GPQ}. We have provided more details on the representation of quantum states, unitary transformation operations, and quantum measurements in  Appendices \ref{app: qubits}, \ref{app: Quantum gates}, and \ref{app: Measurement}. 
	\subsection{\label{sec: QFT}Quantum Fourier transform}
	
	The quantum Fourier transform (QFT) performs the same transformation as the discrete Fourier transform. It was first used in Shor's integer factoring algorithm \citep{Shor1994ShorAlgorithm}. Nowadays, QFT is an essential part of various quantum linear algebra algorithms such as quantum phase estimation, matrix inversion problems, and qPCA \citep{Nielsen2011QuantumComputing, Harrow2009HHLalgorithm, Lloyd2014QPCA}. The discrete Fourier transform takes an input vector $\mathbf{x}$ and transforms it into another vector in the frequency domain $\mathbf{y}$ through 
	\begin{equation}
		y_k = \frac{1}{\sqrt{2^n}} \sum_{j=0}^{2^n-1} e^{\frac{2\pi i j k}{2^n}} x_j, \hspace{0.5cm} k = 0, \cdots, 2^n-1.
	\end{equation}
	Similarly, QFT is defined as a linear operator that acts on an orthonormal basis $\ket{0}, \ldots, \ket{N-1}$ where $N = 2^n$. The QFT acts over the basis element $\ket{j}$ as
	\begin{equation} \label{DFT}
		\ket{j} \rightarrow \frac{1}{\sqrt{2^n}} \sum_{k=0}^{2^n-1} e^{\frac{2\pi i j k}{2^n}} \ket{k}.
	\end{equation}
	We can express this as the unitary transformation
	\begin{eqnarray}
		U=\frac{1}{\sqrt{2^n}} \sum_{k=0}^{2^n-1}  \sum_{j=0}^{2^n-1} e^{\frac{2\pi i j k}{2^n}} \ket{k}\bra{j},
	\end{eqnarray}
	acting on an arbitrary state $\ket{\psi}$, which can be expressed as
	\begin{equation}
		\ket{\psi}= \sum_{j=0}^{2^{n}-1}x_{j}\ket{j}\rightarrow \sum_{k=0}^{2^{n}-1}y_{k}\ket{k}.
	\end{equation}
	To understand how the QFT has been implemented on quantum computers, it is helpful to express the state $\ket{j}$ in a binary representation as $j=j_{1}j_{2}\cdots j_{n}$, where $j_{1},j_{2},\cdots,j_{n}$ are either $0$ or $1$. For example, we can represent the state $\ket{j}$ when $j=2$ as a unit vector using binary representation: $\ket{2}=\ket{10}=\ket{1}\otimes \ket{0}=\begin{pmatrix}
		0 & 0&1&0 
	\end{pmatrix}^\top$, where single qubit states are $\ket{0}=\begin{pmatrix}
		1\\0 
	\end{pmatrix}$ and $\ket{1}=\begin{pmatrix}
		0\\1 
	\end{pmatrix}$. The binary representation of an entire number can be written as $j = j_{1}2^{n-1} + j_{2}2^{n-2} + \cdots + j_{n}2^{0}$. Similarly, a decimal number $0.j_{1}j_{2}\cdots j_{l}$ can be expressed in a binary representation as $j_{1}/2 + j_{2}/4 + \cdots + j_{l}/2^{l}$. The QFT in Eq. (\ref{DFT}) can be written in product form following \citet{Nielsen2011QuantumComputing} as
	\begin{equation}
		\ket{j_1,\cdots, j_n}\rightarrow\frac{1}{2^{n/2}} \left[\left(\ket{0}+e^{2\pi\dot{\iota} 0.j_n}\ket{1}\right)\left(\ket{0}+e^{2\pi\dot{\iota} 0.j_{n-1}j_n}\ket{1}\right)\cdots\left(\ket{0}+e^{2\pi\dot{\iota} 0.j_1 j_2\cdots j_n}\ket{1}\right) \right].
	\end{equation}
	This expression can be implemented using a quantum circuit composed only of the Hadamard gate $H = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$, and the two-qubit controlled rotation $CR_k=\begin{bmatrix}
		I & 0\\
		0& R_{k}
	\end{bmatrix}$ given in a block-diagonal form,
	where
	$
	R_{k}=\begin{bmatrix}
		1 & 0\\
		0& e^{2\pi \dot{\iota}/2^{k}}
	\end{bmatrix}.
	$
	The action of the Hadamard gate on the target quantum state $\ket{j_t}$ is
	\begin{equation} \label{Hadamard gate}
		H\ket{j_t}=\frac{1}{\sqrt{2}}\left(\ket{0}+e^{\frac{2\pi \dot{\iota}}{2}j_{t}}\ket{1}\right)=\frac{1}{\sqrt{2}}\left(\ket{0}+e^{2\pi \dot{\iota} 0.j_{t}}\ket{1}\right),
	\end{equation}
	where we represented the $j_t/2$ as a decimal number $0.j_t$. The Eq. (\ref{Hadamard gate}) can be seen as follows: when $j_t=0$, we have $H\ket{0}=\frac{1}{\sqrt{2}}\left(\ket{0}+\ket{1}\right)$,  and when $j_t=1$ we have $H\ket{1}=\frac{1}{\sqrt{2}}\left(\ket{0}-\ket{1}\right)$.
	On the other hand, the operation of $CR_k$ on a two-qubit state $\ket{j_c j_t}$, where the first qubit is the control and the second is the target, does not affect the second qubit if the first is in the zero state
	$
	CR_{k}\ket{0j_{t}}=\ket{0j_{t}}
	$,
	and when the control qubit in one state is
	$
	CR_{k}\ket{1j_{t}}=e^{\frac{2\pi \dot{\iota}}{2^{k}}j_{t}}\ket{1j_{t}}
	$.
	
	We start with an $n$-qubit input quantum state represented in  $\ket{j_1\cdots j_n}$. Applying the Hadamard gate to the first qubit gives the following quantum state
	\begin{equation}
		H\ket{j_{1}}\ket{j_2 \cdots j_n}={\frac{1}{2^{1/2}}\left(\ket{0}+e^{\frac{2\pi \dot{\iota}}{2}j_{1}} \ket{1}\right)\ket{j_2 \cdots j_n}}=\frac{1}{2^{1/2}}\left(\ket{0}+e^{2\pi \dot{\iota}0.j_{1}} \ket{1}\right)\ket{j_2 \cdots j_n}.
	\end{equation}
	Then, the operation of the controlled-$R_{2}$ gate to the first two qubits results in
	\begin{equation}
		CR_{2}\ket{j_{1} j_2 }\ket{j_3 \cdots j_n}={\frac{1}{2^{1/2}}\left(\ket{0}+e^{\frac{2\pi \dot{\iota}}{2}j_{1}}e^{\frac{2\pi \dot{\iota}}{4}j_{2}} \ket{1}\right)\ket{j_2 \cdots j_n}}=\frac{1}{2^{1/2}}\left(\ket{0}+e^{2\pi \dot{\iota}0.j_{1}j_{2}} \ket{1}\right)\ket{j_2 \cdots j_n}.
	\end{equation}
	Similarly, applying the controlled gates $R_{k}$ for $k=3, \ldots, n$ on the first qubit yields the following expression:
	\begin{equation}
		\ket{\xi_1}={\frac{1}{2^{1/2}}\left(\ket{0}+e^{2\pi \dot{\iota}\left(j_1/2 +j_2/4 +\cdots+j_n/2^n\right)} \ket{1}\right)\ket{j_2 \cdots j_n}} =\frac{1}{2^{1/2}}\left(\ket{0}+e^{2\pi \dot{\iota}0.j_{1}j_{2}\cdots j_{n}} \ket{1}\right)\ket{j_2 \cdots j_n},
	\end{equation}
	where $\ket{\xi_1}$ is an intermediate quantum state. We then apply the Hadamard gate to the second qubit and repeat the controlled operation of $R_{k}$ for $k=3, \ldots, n$ on the second qubit giving
	\begin{equation}
		\ket{\xi_2} =   \frac{1}{2^{2/2}}\left(\ket{0}+e^{2\pi \dot{\iota}0.j_{1}j_{2}\cdots j_{n}} \ket{1}\right)\left(\ket{0}+e^{2\pi \dot{\iota}0.j_{2}j_{3}\cdots j_{n}} \ket{1}\right)\ket{j_3 \cdots j_n}.
	\end{equation}
	We continue in this fashion for each qubit until $k=n$ and perform the SWAP operation to change the location of the least significant bit, which leads us to the final quantum state
	\begin{equation}
		\ket{\xi_3} =\frac{\left(\ket{0}+e^{2\pi\dot{\iota} 0.j_n}\ket{1}\right)\left(\ket{0}+e^{2\pi\dot{\iota} 0.j_{n-1}j_n}\ket{1}\right)\cdots\left(\ket{0}+e^{2\pi\dot{\iota} 0.j_1 j_2\cdots j_n}\ket{1}\right) }{2^{n/2}}.
	\end{equation}
	
	Fig.~\ref{fig:1} shows the circuit demonstration of the QFT.  In the first step, one Hadamard gate and $n-1$ conditional rotation gates make a total of $n$ gates. Similarly, in the second step, we require $n-1$ gates. Continuing in this manner, the total number of gates is $n + (n-1) + \cdots + 1 = n(n+1)/2$. Therefore, the computational complexity of the QFT is $O(n^2)$. In contrast, representing the same amount of information on a classical computer requires $N=2^{n}$ classical bits. Therefore, implementing the discrete Fourier transform using the classical fast Fourier transform (FFT) would require a computational complexity of $O(n 2^n)$. The QFT is often used as a subroutine of several quantum algorithms such as quantum phase estimation, which we discuss in the next section.
	
	\begin{figure}[tb!]
		\centering
		\includegraphics[width = 1\linewidth]{QFT.pdf}
		\caption{ Circuit diagram to implement the quantum Fourier transform on a quantum computer.}
		\label{fig:1}
	\end{figure}
	
	\subsection{\label{sec: QPE}Quantum phase estimation}
	Given a unitary operator $U$ acting on an $n$ qubit quantum state $\ket{\phi}$ with eigenvalue equation $U\ket{\phi}=e^{2\pi \dot{\iota}\varphi}\ket{\phi}$, the objective of the quantum phase estimation (QPE) algorithm is to estimate the value of $\varphi$ \citep{kitaev1995QPE}. The QPE starts with a unitary transformation that can implement powers of $U$ conditioned on ancillary qubits
	\begin{equation} 
		\frac{1}{2^{n/2}}  \sum_{k=0}^{2^{n}-1}\ket{k}\ket{\phi}\rightarrow  \frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1}\ket{k}U^{k}\ket{\phi}.
	\end{equation}
	The action of the unitary operator $U^{k}$ on the eigenvector $\ket{\phi}$ yields the eigenvalue $e^{2\pi \dot{\iota} k \varphi}$ given as
	\begin{equation}
		\begin{aligned} \label{eq: QPE}
			\frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1}\ket{k}U^{k}\ket{\phi} &=\frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1}\ket{k}\left(e^{2\pi \dot{\iota}k\varphi}\ket{\phi}\right)\nonumber\\&=\frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1} e^{2\pi \dot{\iota}k\varphi}\ket{k}\ket{\phi}.
		\end{aligned}
	\end{equation}
	Applying the inverse QFT to the equation above results in the eigenvalue estimate being stored in the quantum register as follows
	\begin{equation}
		\frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1} e^{2\pi \dot{\iota}k\varphi}\ket{k}\ket{\phi}\xrightarrow[]{QFT^{\dagger}} \ket{\hat{\varphi}} \ket{\phi}.
	\end{equation}
	Quantum registers consist of multiple qubits that are used in quantum computers, similar to classical registers in classical computers \citep{Nielsen2011QuantumComputing}. We then perform a measurement in the computational basis to obtain the estimate $\hat{\varphi}$ of the eigenvalue.
	
	The binary string $l$ with the highest success probability corresponds to the exact estimate $\hat{\varphi}$ only if $\hat{\varphi} = \frac{l}{2^{n}}$.
	Generally, $\varphi$ cannot be represented exactly by $\frac{l}{2^{n}}$, however, the QPE algorithm still gives the best possible approximation, according to  \citet{kitaev1995QPE}, given by
	
	\begin{equation}
		\frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1} e^{2\pi \dot{\iota}k\varphi}\ket{k}\ket{\phi}\xrightarrow[]{QFT^{\dagger}}\frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1} e^{2\pi \dot{\iota}k\left(\varphi-j/2^{n}\right)}\ket{i}\ket{\phi}. 
	\end{equation}
	The highest success probability of measuring the closest estimate using the QPE algorithm becomes
	\begin{equation}
		p(j)= \left|\frac{1}{2^{n/2}} \sum_{k=0}^{2^{n}-1} e^{2\pi \dot{\iota}k\left(\varphi-j/2^{n}\right)}\right|^{2}.
	\end{equation}
	This depends on the difference between $\varphi$ and the binary fractional integer $j/2^{n}$. Therefore, to increase the precision of the estimate $\hat{\varphi}$, we need to add more ancilla qubits to the QFT. Fig.~\ref{fig:2}
	presents the circuit of the quantum phase estimation algorithm.
	
	\begin{figure}[tb!]
		\centering
		\includegraphics[width = 0.65\linewidth]{QPE.pdf}
		\caption{ Circuit diagram to implement the quantum phase estimation on a quantum computer.}
		\label{fig:2}
	\end{figure}
	QPE requires the implementation of powers of the unitary operator $U=e^{\dot{\iota}\Tilde{H} t}$ on a quantum computer using Hamiltonian simulation of the $\Tilde{H}$ operator \citep{Lloyd1996UniversalQS}.
	Standard methods to implement Hamiltonian simulation involve decomposing the unitary matrix into all possible combinations of Pauli operators which has a huge computational complexity \citep{Nielsen2011QuantumComputing}. 
	To obtain practical advantages over classical systems, we have to find solutions in which we can efficiently simulate the unitary system. 
	Several algorithms have been proposed in this area, for example, \citet{berry2007efficient} consider a sparse Hamiltonian operator, another approach approximates it using Taylor series \citep{Berry2015TruncatedTaylor}, and the commonly used Suzuki-Trotter method \citep{Suzuki1976Suzuki-trotter}.
	Hamiltonian simulation can be efficiently done when the input state and the Hamiltonian operator are the same \citep{Lloyd2014QPCA}, with a method called density matrix exponentiation.
	It can be used in the quantum principal component analysis (qPCA)  if the quantum density operator is non-sparse but low-rank.
	In this paper, we utilize this concept of qPCA for the Gaussian process quadrature rule.
	
	\subsection{\label{sec: QPCA} Quantum principal component analysis}
	Instead of explicitly calculating the unitary matrix $U=e^{\dot{\iota} \Tilde{H} t}$, we can simulate the unitary operation using SWAP operators. For this, we add the extra condition that the Hermitian operator should be represented as the density matrix of some quantum state, that is, $\Tilde{H}=\Tilde{\rho}$. We apply the SWAP operation to the state $\Tilde{\rho}\otimes \tilde{\sigma}$ and perform a partial trace operation.  The expression for this simulation turns out to be \citep{Lloyd2014QPCA, Schuld2021QML}
	\begin{equation}
		\begin{split}    
			\mathrm{tr}_{2}\{e^{-\dot{\iota}S\Delta t}\left(\tilde{\sigma}\otimes \Tilde{\rho}\right)e^{\dot{\iota}S\Delta t}\}&={\tilde{\sigma}}-\dot{\iota}\Delta t[\Tilde{\rho},{\tilde{\sigma}}]+O(\Delta t^{2})\nonumber \\ &\approx e^{-\dot{\iota}\Tilde{\rho}\Delta t}{\tilde{\sigma}} e^{\dot{\iota}\Tilde{\rho}\Delta t},
		\end{split}
	\end{equation}
	where ${\tilde{\sigma}}-\dot{\iota}\Delta t[\Tilde{\rho},{\tilde{\sigma}}]$ with  $[\Tilde{\rho},{\tilde{\sigma}}]=\Tilde{\rho}{\tilde{\sigma}}-{\tilde{\sigma}} \Tilde{\rho}$ are the first terms of Taylor expansion of $e^{-\dot{\iota}\Tilde{\rho}\Delta t}{\tilde{\sigma}} e^{\dot{\iota}\Tilde{\rho}\Delta t}$, and $\mathrm{tr}_{2}(.)$ is partial trace over second subsystem.
	
	Density matrix exponentiation is often used with the quantum phase estimation technique. Once we have exponentiated the density matrix, we can extract the eigenvalues through quantum phase estimation. To do so, we need to prepare the powers of $U^{k}=e^{\dot{\iota}\Tilde{\rho} k \Delta t}$.
	
	\citet{Lloyd2014QPCA} showed that using the exponential density matrix along with the estimation of the quantum phase, $O(\epsilon^{-3})$ copies of $\Tilde{\rho}$ are required for this, where $\epsilon$  represents the precision to estimate the eigenvalues. These copies are then combined with an ancilla register of $n$ qubits in superposition to obtain
	\begin{equation}
		\rho_{\xi_4}=\sum_{k=0}^{2^{n}-1}\ket{k}\bra{k} \otimes {\tilde{\sigma}} \otimes \Tilde{\rho}^{(1)} \otimes \Tilde{\rho}^{(2)} \otimes \cdots \otimes \Tilde{\rho}^{(2^{n})}.
	\end{equation}
	We now perform a sequence of two-qubit conditional SWAP operators, each of which swaps the first qubit state ${\tilde{\sigma}}$ with the $g$th copy of $\Tilde{\rho}$ conditioned on the ancilla qubit $\ket{k}$. The SWAP operators are entangled with the ancillary register, so the index $\ket{k}$ governs the sequence of SWAP operators up to the copy $\Tilde{\rho}^{(k)}$ as
	
	\begin{equation}
		\rho_{\xi_5}= \frac{1}{K}\sum_{k=0}^{2^{n}-1}\ket{k\Delta t}\bra{k\Delta t} \otimes \prod_{g=1}^{k}e^{-\dot{\iota}S_{g}t}.
	\end{equation}
	After taking the partial trace over all the copies of $\Tilde{\rho}$, we obtain the result
	\begin{equation}
		\rho_{\xi_6}= \sum_{k=0}^{2^{n}-1}\ket{k}\bra{k} \otimes e^{-\dot{\iota} k \Tilde{\rho} \Delta t} {\tilde{\sigma}} e^{\dot{\iota} k \Tilde{\rho} \Delta t} +O(\Delta t^{2}).
	\end{equation}
	This mixed quantum state representation is suitable for applying the QFT \citep{Schuld2021QML}. Letting ${\tilde{\sigma}}=\Tilde{\rho}$, it calculates the eigenvalues and eigenvectors of the operator $\Tilde{\rho}$. Obtaining eigenvalues and eigenvectors in this way is known as quantum principal component analysis.  
	
	The SWAP operator is a sparse matrix and can be simulated efficiently. \citet{Lloyd2014QPCA} showed that this algorithm can be implemented with a computational complexity of $O(\log N)$ on a quantum computer when multiple copies of the density operator are already given as quantum states. This qPCA is qubit efficient as long as we do not have to estimate the eigenvalue to a precision that does not grow exponentially. According to \citet{Lloyd2014QPCA}, this technique is only suitable when the density matrix is dominated by a few eigenvalues that do not require high precision for estimation.  
	
	
	
	
	\section{\label{sec: Quantum GPQ} Quantum Hilbert space low-rank Gaussian process quadrature}
	
	In this section, we provide the main contribution of our paper, which explores how quantum computers can be used to evaluate integrals using the Hilbert space low-rank Gaussian process quadrature. 
	For this, we implement the Hilbert space kernel approximation developed by \citet{Farooq2024QAHSGPR} for quantum-assisted Gaussian process regression and extend it to the GPQ rule. We start with the data matrix $\mathbf{X} \in \mathbb{R}^{N \times M}$ computed on a classical computer. This data matrix is exactly the same as used in Eqs. (\ref{mean:1}) and (\ref{varianve:1}). We initially consider the amplitude encoding scheme in Appendix \ref{app: Encoding} to encode the data matrix $\mathbf{X}$ with entries $x_{n}^{m}$ in the form
	\begin{equation}
		\ket{\psi_{\mathbf{X}}} = \sum_{m = 0}^{M-1}\sum_{n = 0}^{N-1} x_{n}^{m}\ket{m}\ket{n}.
		\label{eq: quantumX}
	\end{equation}
	The state $\ket{\psi_{\mathbf{X}}}$ has to be normalized, then, the entries $x_{n}^{m}$ satisfy the condition $\sum_{n,m} \|x_{n}^{m} \|^2 = 1$. 
	The amplitude data encoding implicates a computational complexity $O(NM)$ in standard quantum state preparation methods \citep{Schuld2021QML}. However, efficient quantum amplitude encoding methods have been recently proposed, achieving a $O(\operatorname{poly}(\log (NM)))$ computational complexity \citep{Nakaji2022ApproximateAmplitudeEncoding}.
	
	Starting from the quantum state in Eq. (\ref{eq: quantumX}), we consider its Gram-Schmidt decomposition \citep{Schuld2016QLinearRegresion} which gives
	\begin{equation}
		\ket{\psi_{\mathbf{X}}} = \sum_{r = 1}^{R} \lambda_{r} \ket{v_r} \ket{u_r}.
		\label{eq: QX GS decompostion}
	\end{equation}
	The Gram-Schmidt decomposition works as a quantum analog of the SVD performed in (\ref{eq: SVD mean GPQ}). Further, consider the density matrix $\rho_{\mathbf{X}^{\top}\mathbf{X}}=\operatorname{Tr}_{n} 
	\{ \ket{\psi_{\mathbf{X}}} \bra{\psi_{\mathbf{X}}}\}$ given by tracing out the $\ket{n}$ register of the quantum state in Eq. (\ref{eq: quantumX}). The resulting density matrix can be written in terms of the Gram-Schmidt decomposition in Eq. (\ref{eq: QX GS decompostion}) as 
	\begin{equation}
		\rho_{\mathbf{X}^{\top}\mathbf{X}}=\operatorname{Tr}_{n} \{ \ket{\psi_{\mathbf{X}}} \bra{\psi_{\mathbf{X}}}\} = \sum_{r = 1}^{R} \lambda_r^2 \ket{v_r}\bra{v_r}.
		\label{eq: rho_XTX}
	\end{equation}
	Once the density operator is built, it is possible to apply it as an exponential evolution operator to the $\ket{m}$ register of $\ket{\psi_{\mathbf{X}}}$ following the qPCA technique \citep{Lloyd2014QPCA} explained in Section \ref{sec: QPCA}. Since the state $\ket{\psi_{\mathbf{X}}}$ is the eigenstate of the operator in Eq. (\ref{eq: rho_XTX}), it is also the eigenstate of $\exp({-\dot{\iota} \rho_{\mathbf{X}^{\top}\mathbf{X}} t})$. With this argument, it is possible to implement the QPE algorithm (see Section \ref{sec: QPE}) to estimate the eigenvalues of $\rho_{\mathbf{X}^{\top}\mathbf{X}}$.
	
	The algorithm starts by building a specific state $\ket{\psi_1}$ that stores the information of the evaluation points. Fig. \ref{fig: psi_1 circuit} shows the quantum circuit used to create this state.  
	\begin{figure}[tb!]
		\centering
		\begin{quantikz}
			\lstick{$\ket{0}_a$}&&&&&&&& \gate{R_{y}(\theta_1)}\slice[style =black, label style={pos=0}]{$\ket{\xi_2}$}&&&&&\rstick[4]{$\ket{\psi_1}$}\\
			\lstick{$\ket{0}_\tau$}& & \qwbundle[style={xshift=-2mm}]{\tau}&&\gate[style={color = black, fill=Color2!20}]{H}\gategroup[3,steps=3,style={dashed,color = Color2, fill=Color2!20, rounded corners},background,label style={label position = below,anchor=north,yshift=-0.2cm}]{{QPE}} & \ctrl{1}&\gate[style={color = black, fill=Color2!20}]{\operatorname{QFT}^{\dagger}}&\slice[style =black, label style={pos=0}]{$\ket{\xi_1}$}& \ctrl{-1}& \gate{\operatorname{QFT}}&& \ctrl{1}& \gate{H}&\\
			& \setwiretype{n}\lstick[2]{$\ket{\psi_{\mathbf{X}}}$}& \setwiretype{q}\qwbundle[style={xshift=-2mm}]{\log_2(M)}&& &\gate[style={color = Color1,fill=Color1!20}]{e^{- \dot{\iota} \rho_{X^{\top}X} t} }&&&\setwiretype{q}&&&\gate[style={color = Color1, fill=Color1!20}]{e^{\dot{\iota} \rho_{X^{\top}X}t} } &&\\
			&\setwiretype{n}&\setwiretype{q}\qwbundle[style={xshift=-2mm}]{\log_2(N)}&&&&&&&&&&&
		\end{quantikz}
		\caption{\label{fig: psi_1 circuit} Quantum circuit to prepare the $\ket{\psi_1}$ quantum state. The red gates represent the parts of the algorithm where the qPCA is implemented, meanwhile, the green part envelopes the implementation of the QPE algorithm.}
	\end{figure}
	Four quantum registers are needed, the last two are the $\ket{n}_n$ and $\ket{m}_m$ registers needed to store $\ket{\psi_{\mathbf{X}}}$; then, we need a register $\ket{0}_\tau$ that will store the eigenstates; finally, we need an ancilla register $\ket{0}_a$. From Fig. \ref{fig: psi_1 circuit}, we can see that the ancilla register contains only one qubit, the second register contains  $\tau$ qubits, and the last two registers together contain a total of $\log\left(NM\right)$ qubits.
	
	Conditioning on $\ket{0}_\tau$, we implement the QPE algorithm with unitary evolution performed through qPCA.
	Denoting intermediate states as $\ket{\xi_i}$, the resulting state after the QPE algorithm is
	\begin{equation}
		\ket{\xi_1} = \sum_{r=1}^{R}\lambda_{r} \ket{v_{r}}_m \ket{u_{r}}_n \ket{\lambda_{r}^{2}}_\tau \ket{0}_a.
	\end{equation}
	The binary representation of the eigenvalue $\lambda_r^2$ can be used to condition a $R_{y}(\theta_1)$ rotation around the $y$ axis of angle $\theta_1 = 2\arcsin \left(\frac{c_1}{\lambda_r^2 + \sigma^2}\right)$ in the ancilla register. The eigenvalue estimation and the conditioned rotation correspond to the matrix inversion in (\ref{eq: HSGPQ mean}). After the rotation, the state of the quantum circuit takes the form
	\begin{equation}
		\ket{\xi_2} = \sum_{r=1}^{R}\lambda_{r} \ket{v_{r}}_m \ket{u_{r}}_n \ket{\lambda_{r}^{2}}_\tau \left[\sqrt{1 - \left(\frac{c_1}{\lambda_r^2 + \sigma^2}\right)^2}\ket{0}_a + \left(\frac{c_1}{\lambda_r^2 + \sigma^2}\right)\ket{1}_a\right].
	\end{equation}
	The term $\frac{c_1}{\lambda_r^2 + \sigma^2}$ has to be smaller than one, this condition is satisfied by an appropriate selection of the constant $c_1$ \citep{Cleve1998QAR}. Finally, it is possible to reverse the operation over the $\tau$ register, which provides the state
	\begin{equation}
		\ket{\psi_1} = \sum_{r=1}^{R}\lambda_{r} \ket{v_{r}}_m\ket{u_{r}}_n \ket{0}_\tau \left[\sqrt{1-\left(\frac{c_1}{\lambda_{r}^{2}+\sigma^{2}}\right)^2} \ket{0}_a +\frac{c_1}{\lambda_{r}^{2}+\sigma^{2}}\ket{1}_a\right].
	\end{equation}
	
	It is important to note that when the ancilla qubit is in the state $\ket{1}_a$, the state of the qubit registers $m$ and $n$ represents part of the quadrature expected value. 
	
	We can estimate the quadrature expected value $Q_{\mathrm{BQ}}$ with quantum states by introducing the information of the vector $\mathbf{X}_{\mu}$ and the evaluation points $\mathbf{y}$. For this, we need an additional quantum state named $\ket{\psi_2}$ with the same number of registers as in $\ket{\psi_1}$. By considering the quantum states $\ket{\mathbf{X}_\mu}_m = \sum_m x_\mu^m \ket{m} $ and $\ket{\mathbf{y}}_n = \sum_n y_n \ket{n}$, we use the amplitude encoding scheme to build the state $\ket{\psi_2} = \ket{\mathbf{X}_\mu}_m \ket{\mathbf{y}}_n \ket{0}_\tau \ket{1}_a$, which is normalized.  Note the ancilla qubit in the state $\ket{1}_a$ for $\ket{\psi_{2}}$. This ensures that when we perform the dot product between $\braket{\psi_1\mid\psi_2}$, we always obtain the amplitude coefficient corresponding to  the ancilla state $\ket{1}_a$ of $\ket{\psi_{1}}$ as 
	the dot product $\braket{0|1}_a$ is always zero.
	
	Consider the dot product between the $\ket{\psi_1}$ and $\ket{\psi_2}$ states, which takes the form
	\begin{equation}
		\braket{\psi_1\mid\psi_2} = c_1\sum_{r=1}^{R}\frac{\lambda_{r}}{\lambda_{r}^{2}+\sigma^{2}} \braket{\mathbf{X_\mu} \mid v_{r}}_m\braket{\mathbf{y} \mid u_{r}}_n.
		\label{eq: mean QuantumHSQ}
	\end{equation}
	Dividing the dot product by the rotation constant $c_1$, the result corresponds to the expected value  $Q_{\mathrm{BQ}}= \braket{\psi_1\mid\psi_2}/c_1$ given in Eq. (\ref{eq: SVD mean GPQ}). The dot product between two states can be implemented in a quantum computer through a Hadamard test, which is explained in the Appendix \ref{app: Hadamard}. As a result, the expected value in the quadrature can be written in terms of the probability $p_0$ of the ancilla qubit to be in the state $\ket{0}$ as
	\begin{equation}
		Q_{\mathrm{BQ}}= \frac{2p_0 - 1}{c_1}.
	\end{equation}
	
	The variance of the quadrature can be calculated using a similar circuit.
	After the quantum phase estimation, the circuit is in the intermediate state $\ket{\xi_1}$, now the conditioned rotation $R_y(\theta_2)$ has a different angle $\theta_2 = 2\arcsin\left(\frac{c_2}{\lambda_r\sqrt{\lambda_r^2 + \sigma^2}}\right)$. This results in the state
	\begin{equation}
		\ket{\xi_3} = \sum_{r=1}^{R}\lambda_{r} \ket{v_{r}}_m \ket{u_{r}}_n \ket{\lambda_{r}^{2}}_\tau \left[\sqrt{1 - \left(\frac{c_2}{\lambda_r \sqrt{\lambda_r^2 + \sigma^2}}\right)^2}\ket{0}_a + \left(\frac{c_2}{\lambda_r \sqrt{\lambda_r^2 + \sigma^2}}\right)\ket{1}_a\right],
	\end{equation}
	where the constant $c_2$ satisfies the condition $c_2\leq\lambda_r \sqrt{\lambda_r^2 + \sigma^2}$.
	The algorithm proceeds by conditioning the ancilla register to be in the state $\ket{1}_a$ and including the vector $\mathbf{X}_{\mu}$. In this case, we need an additional register, with the same length as $\ket{m}$, to store the quantum state $\ket{\psi_2 '} = \ket{\mathbf{X}_\mu}.$ By including an additional ancilla register $\ket{0}_b$ a SWAP test can be implemented between the $\ket{\mathbf{X}_\mu}$ and the $\ket{m}$ register of the circuit. Appendix \ref{app: swap} provides the detailed steps for implementing the SWAP test on quantum computers.  Measuring both ancillary qubits to be in the quantum state $\ket{1}$, the probability is
	\begin{equation}
		p_{11} = \frac{c_2^2}{2}\left( \sum_{r=1}^R \frac{1}{\lambda_r^2 + \sigma^2} - \sum_{r=1}^R \frac{1}{\lambda_r^2 + \sigma^2} |\braket{\mathbf{X}_\mu\mid v_r}|^2 \right),
	\end{equation}
	where the second term is proportional to the variance of the quadrature given in Eq. (\ref{eq: SVD variance GPQ}), then it can be written as
	\begin{equation}
		V_{\mathrm{BQ}}= \sigma^2\left( \sum_{r=1}^R \frac{1}{\lambda_r^2 + \sigma^2} - \frac{2p_{11}}{c_2^2}\right),
	\end{equation}
	giving the Gaussian process quadrature variance. The quantum circuit to estimate the quadrature variance is shown in Fig. \ref{fig: var qc}.
	\begin{figure}[tb!]
		\centering
		\begin{quantikz}[]
			\lstick{$\ket{0}_{a}$}&&&&&&&&\gate{R_{y}(\theta_2)}&\meter{}\\
			\lstick{$\ket{0}_{\tau}$}&&\qwbundle[style={xshift=-2mm}]{\tau}&&\gate[style={color = black, fill=Color2!20}]{H}\gategroup[3,steps=3,style={dashed,color = Color2, fill=Color2!20, rounded corners},background,label style={label position = above,anchor=south,yshift=-0.25cm}]{{QPE}}&\ctrl{1}& \gate[style={color = black, fill=Color2!20}]{\operatorname{QFT}^{\dagger}}&&\ctrl{-1}&\\
			\setwiretype{n}&\lstick[2]{$\ket{\psi_{\mathbf{X}}}$}&\setwiretype{q}\qwbundle[style={xshift=-2mm}]{\log_2(M)}&&&\gate[style={color = Color1, fill=Color1!20}]{e^{-i \rho_{X^{\top}X} t}}&&\swap{2}&&\\
			\setwiretype{n}&&\setwiretype{q}\qwbundle[style={xshift=-2mm}]{\log_2(N)}&&&&&&&\\
			\lstick{$\ket{X_\mu}$}&&\qwbundle[style={xshift=-2mm}]{\log_2(M)}&&&&&\targX{}&&\\
			\lstick{$\ket{0}_b$}&&&&&&\gate{H}&\ctrl{-1}&\gate{H}&\meter{}
		\end{quantikz}
		\caption{\label{fig: var qc} Quantum circuit to estimate the quadrature variance. The red gates represent the parts of the algorithm where the qPCA is implemented, meanwhile, the green part envelopes the implementation of the QPE algorithm.}
	\end{figure}
	Once the quantum computational method for Gaussian quadrature computation has been constructed we are interested in the computational complexity of the algorithm with respect to the classical Gaussian process quadratures.
	The quantum state preparation subroutines in our algorithm can be implemented using the quantum state preparation technique described by \citet{Nakaji2022ApproximateAmplitudeEncoding}, which efficiently prepares the quantum state $\ket{\psi_\mathbf{X}}$ with a computational complexity of $O(\operatorname{poly}(\log (NM)))$ when the dataset consists of real numbers. We then apply qPCA along with QPE, which has a total computational cost $O(\log(M)\epsilon^{-3})$ \citep{Schuld2016QLinearRegresion}. The next step involves the conditional unitary, which has a low computational complexity of $O(\log(\frac{1}{\epsilon}))$, so we can neglect its computation cost. We perform measurements in the variance circuit before the SWAP test, which can be implemented with amplitude amplification techniques in $O(\kappa^2)$. We then employ the Hadamard and SWAP tests, whose complexity is linear in the number of qubits; thus, the measurement accounts for only a constant factor, which can be ignored. We use two separate quantum circuits, both of which have the same computational complexity and only introduce a constant multiplicative factor of $2$ to the total computational complexity, which can be ignored, but allows us to implement more qubits in the numerical simulations. Therefore, the total computational cost to estimate the expected value and variance of our algorithm for the quantum Gaussian process quadrature rule is $O(\operatorname{poly}(\log (NM))\log(M) \kappa^2 \epsilon^{-3})$. On the other hand, the classical version of the GPQ rule requires a computational cost of $O(M^3)$, which shows that our algorithm is polynomially faster than the classical algorithm.
	
	\section{\label{sec: Simulations}Numerical Simulations}
	The simulations performed in this paper correspond to executions of quantum circuits in a classical computer that simulate the result that would be obtained on quantum hardware. The classical simulations already pose several challenges for different factors. The QPE algorithm needs several qubits to provide reasonable estimates of the eigenvalues. Moreover, since we estimate multiple eigenvalues in this step, it is important to differentiate them properly. Implementing the qPCA evolution operator is sensitive to the parameter $t$. In this case, we select $t = 2\pi/\delta$ and modify the value of $\delta$ as suggested by \citet{Cleve1998QAR}, following the inequality $\delta > \lambda_{max}^2$ where $\lambda_{max}$ is the largest eigenvalue of the decomposition. For a good approximation, $\delta$ should be slightly greater than $\lambda_{max}^2$. 
	
	As an example for simple demonstration purposes, we considered the integral $\mathcal{I}_1 = \int_\Omega f(x)\mu(x)dx$ of a simple one-dimensional sinusoidal function $f(x) = 1+\sin(x)$, similar to the one used for regression by \citet{Farooq2024QAHSGPR}, integrated over the interval $\Omega = [-\pi, \pi]$ with weight function $\mu(x) = 1$. 
	We choose $\zeta$ within the same range $[-L, L]$ with $L = \pi$ to implement the framework outlined in this work. 
	In this domain, the eigenfunctions corresponding to the Laplace operator are the sinusoidal functions $\phi_j(x) = L^{-1/2}\sin(\pi j(x+L)/2L)$, each associated with its respective eigenvalue $\lambda_{j} = (\pi j/2L)^2$. 
	
	\begin{figure}[tb!]
		\centering
		\includegraphics[width = \linewidth]{integral_gaussian_distribution.pdf}
		\caption{Estimate of the quadrature for the integral $\mathcal{I}_1$. The estimation results deliver the expected value and variance of the Gaussian distributions plotted above that approximates the integral. The dotted black plot corresponds to the GPQ, the dashed red plot corresponds to the HSQ and the solid green plots correspond to the estimates using our QHSQ algorithm with $R = 1,2,3,4$.}
		\label{fig: Quadrature}
	\end{figure}
	The first demonstration of the algorithm was implemented using a constant value of $\delta = \lambda_{max}^2 + 0.01$, $N = 8$ evaluation points, $\tau = 16$ qubits to store the eigenvalues, $M=4$ eigenfunctions to approximate the kernel and $10^6$ shots per circuit execution. The simulations were implemented using a classical simulator of a quantum computer using the QISKIT library \citep{Qiskit}. For the simulations, we consider the square exponential kernel in Eq. (\ref{eq: squared exp kernel}) with hyperparameters $\sigma_f = 1$ and $\ell = 1$, whose spectral density is $S(\omega)=\sigma_f^2 \sqrt{2 \pi} l \exp \left(-\frac{l^2 \omega^2}{2}\right)$. The noise $\varepsilon$ of the evaluation points has a Gaussian distribution with zero mean and variance $\sigma = 0.05$. 
	
	Fig. \ref{fig: Quadrature} shows the distribution that estimates the integral $\mathcal{I}_1$ given by the GPQ in Eqs. (\ref{eq: Classic GPQ mean}) and (\ref{eq: Classic GPQ var}), the low-rank Hilbert space quadrature in Eqs. (\ref{eq: HSGPQ mean}) and (\ref{eq: HSGPQ var}) and our proposed quantum Hilbert space quadrature method.
	The distributions given by $R = 1,2$ are quite distant from the integral value, meaning those low-rank approximations do not have enough information to approximate the integral. Also, these two distributions have a similar expected value, which is expected since the vector $X_\mu$ of this problem has ${X_{\mu}}_2 ={X_{\mu}}_4 = 0$. Similarly, the distributions with $R = 3,4$ have a similar expected value, this time around the expected value of the integral given by the classical HSQ, showing the effectiveness of our proposed algorithm. 
	
	\begin{figure}[tb!]
		\centering
		\includegraphics[width = \linewidth]{integral_mean_variance.pdf}
		\caption{Expected value and variance of the Gaussian quadrature methods against the number of evaluation points. The dotted black line corresponds to the GPQ, the dashed red to the HSQ and the solid green lines correspond to our proposed QHSQ method with $R=1,2,3,4$.}
		\label{fig: mean and variance}
	\end{figure}
	The expected value and variance of the method were plotted in Fig.\ \ref{fig: mean and variance} against the number of evaluation points used for the quadrature, which are symmetrically distributed around $x = 0$. Besides the number of evaluation points, all simulation parameters are the same as in the previous example. The implementation of the algorithm is susceptible to the parameter $\delta$, occasionally, it can lead to wrong results due to the small rotations implemented in the circuit and the limited number of $\tau$ qubits used to approximate the eigenvalues $\lambda_r^2$. To avoid outlier results, we implemented the algorithm with six different values of $\delta = \lambda_{max}^2 + \epsilon$, with $\epsilon \in \{0.01, 0.009, 0.008, 0.007, 0.006\}$, and we took the median of these results for each number of evaluation points.
	
	
	For the quantum implementation, it can be seen that as $R$ increases, the expected value approaches the integral real value and also the classical counterpart of the method. It can also be seen that the quantum method has greater fluctuation than the classical methods, mainly attributed to the limited amount of qubits used in the simulations, the finite amount of shots, and the small rotations implemented in the algorithm. 
	On the other hand, we can see how the variance of the quadratures decreases with the number of evaluation points as expected. 
	
	
	\section{\label{sec: Conclusions}Conclusion}
	In this paper, we introduced a novel quantum algorithm for low-rank Gaussian process quadrature.
	The method combines the uncertainty quantification properties of Bayesian quadrature methods for numerical integration \citep{Hagan1991BHQuadrature, Minka2000QuadraturesGPR} with the speedups provided by quantum computation. 
	Our method provides a polynomial complexity reduction compared to the classical GPQ rule, enhancing the solution of analytically intractable integration problems.
	We have also demonstrated the performance of the method using numerical simulations run on a classical simulator of a quantum computer, which shows that the algorithm works in practice. Our quantum algorithm computes the same solution as obtained from a classical computer but with accelerated speed. The simulation results slightly deviate from the classical GPQ because we use an approximate implementation of the unitary operation for qPCA along with a limited number of qubits.
	
	The proposed quantum algorithm is suitable for a fault-tolerant quantum computer with enough quantum registers to implement the algorithm precisely.  Recent developments in materials research have shown promise in the realization of fully scalable quantum computers with higher coherence times \citep{Acharya2024EnrichedSi}, which opens a window for implementing the method in real hardware. Also, we discussed the sensitivity of the method concerning the QPE $\delta$ parameter. However, this drawback can be overcome with advanced quantum simulation techniques \citep{Zulehner2019AdvancedQS} that improve the classical demonstration of the method.
	
	Future work could include the optimization of the quantum circuit needed for the algorithm implementation and demonstrations using advanced classical simulation techniques for quantum computers.
	Besides the well-known data encoding problem in quantum computing, the complexity of the algorithm is mainly dominated by the QPE and the qPCA parts of the algorithm. 
	These steps can be reduced using iterative approaches for the QPE algorithm \citep{Smith2022IterativeQPE}, allowing it to be parallelized. 
	The qPCA implementation can be enhanced using a hybrid approach, despite increasing the classical resources needed to implement the algorithm, a variational circuit can be implemented to reduce the complexity of the quantum circuit that is executed \citep{Xin2021EsperimentalQPCA}. This could enable the algorithm to be executed in a noisy intermediate-scale quantum computer.
	
	\subsubsection*{Author Contributions}
	C.A. Galvis-Florez and A. Farooq contributed equally to this work. All the authors contributed to the original idea, writing, and finalizing the paper results.
	
	\subsubsection*{Acknowledgments}
	C.A. Galvis-Florez, A. Farooq and Simo Srkk thank the Research Council of Finland for funding this research (project 350221). 
	
	\bibliography{main.bib}
	\bibliographystyle{tmlr}
	\appendix
	\section{\label{app: basics}Review of Fundamental Concepts of Quantum Computing}
	In this section, we will review the fundamental concepts used to implement our algorithm, known as the quantum-assisted Hilbert space Gaussian process quadrature rule. We follow and use the symbols and notations as used by \citet{Schuld2021QML}.
	\subsection{\label{app: qubits}Qubits}
	
	The basic unit of a quantum computer is the qubit, which is the analog of the bit in classical computers. A single qubit can be either in quantum state $\ket{0}=\begin{bmatrix}
		1 \\0
	\end{bmatrix}$ or $\ket{1}=\begin{bmatrix}
		0 \\1
	\end{bmatrix}$. Moreover, it can also be represented as a linear combination of both states, known as superposition, given by \citep{Nielsen2011QuantumComputing}
	\begin{equation}
		\ket{q}=\alpha\ket{0}+\beta\ket{1},
	\end{equation}
	with $\alpha,\beta \in \mathbb{C}$. Moreover, the squared amplitudes $|\alpha|^{2}$ and $|\beta|^{2}$ correspond to the probability of measuring the qubit $\ket{q}$ in the state $\ket{0}$ or $\ket{1}$ respectively. Then, they satisfy the condition $|\alpha|^{2}+|\beta|^{2}=1$.
	An $n$-qubit unentangled quantum system can be represented as the tensor product of single-qubit states
	\begin{equation}
		\ket{\psi}=\ket{q_{1}}\otimes \cdots \otimes \ket{q_{n}}.
	\end{equation}
	More generally, it can be written as the superposition
	\begin{equation}
		\ket{\psi}=\sum_{i=0}^{2^{n}-1}\alpha_{i}\ket{i},
	\end{equation}
	where $\alpha_{i}\in \mathbb{C}$, $\sum_{i=0}^{2^{n}-1}|\alpha_{i}|^{2}=1$, and $\{\ket{i}\}$ represents the computational basis $\{\ket{0\cdots 0}=\ket{0},\cdots,\ket{1\cdots 1}=\ket{2^{n}-1}\}$.
	Interactions between qubits can lead them into entangled states, which play an important role in the development of quantum algorithms and applications of quantum technologies. Entangled states cannot be represented as the product of individual states, for example, an entangled two-qubit quantum state can be represented as 
	\begin{equation}
		\ket{\psi} = \frac{1}{\sqrt{2}}\left(\ket{0}_{q_1}\ket{0}_{q_2} + \ket{1}_{q_1}\ket{1}_{q_2} \right) = \frac{1}{\sqrt{2}}\left(\ket{00}_{12} + \ket{11}_{12}\right).
	\end{equation}
	The complete state cannot be written as $\ket{q_1}\otimes\ket{q_2}$, however, it corresponds to the superposition of specific states of the computational basis.
	
	It is important to notice that neither qubit 1 nor 2 in the quantum state has a definite value, but when the state of one qubit is measured, a probabilistic process, then the state of the other qubit is completely defined. When the qubit $q_1$ is measured in a specific state, for example, $\ket{0}$, the qubit $q_2$ immediately takes the value $\ket{0}$. The same logic applies when the measurement is performed over the qubit $q_2$.
	\subsection{\label{app: Quantum gates}Quantum gates}
	Quantum logic gates are the fundamental building blocks of quantum circuits,  whose action on the quantum states defines the dynamics of the circuit. This section follows \citet{Schuld2021QML} to give a brief introduction to single and multiqubit gates.
	Quantum gates are realized through unitary transformation. Unlike the classical gates, quantum gates are reversible.  For a single qubit, there are three fundamental gates  known as Pauli matrices
	\begin{equation}
		\sigma_{x}=\begin{pmatrix}
			0&1\\
			1&0
		\end{pmatrix} , \hspace{1cm}     \sigma_{y}=\begin{pmatrix}
			0&-\dot{\iota}\\
			\dot{\iota}&0
		\end{pmatrix}, \hspace{1cm} \sigma_{z}=\begin{pmatrix}
			1&0\\
			0&-1
		\end{pmatrix}.
	\end{equation}
	
	
	The  Pauli $X$-gate is equivalent to the classical NOT gate for a quantum system.  This gate flips $\ket{0}$ to $\ket{1}$ and vice versa. This can be represented in matrix form as
	\begin{equation}
		\sigma_{x}\ket{0}=\begin{pmatrix}
			0&1\\
			1&0
		\end{pmatrix}\begin{pmatrix}
			1\\0
		\end{pmatrix}=\begin{pmatrix}
			0\\1
		\end{pmatrix}=\ket{1}, \hspace{1cm}
		\sigma_{x}\ket{1}=\begin{pmatrix}
			0&1\\
			1&0
		\end{pmatrix}\begin{pmatrix}
			0\\1
		\end{pmatrix}=\begin{pmatrix}
			1\\0
		\end{pmatrix}=\ket{0}.
	\end{equation}
	
	The Pauli $Z$-gate introduces a phase of -1 to the $\ket{1}$ state and keeps $\ket{0}$ state unchanged
	\begin{equation}
		\sigma_{z}\ket{0}=\begin{pmatrix}
			1&0\\
			0&-1
		\end{pmatrix}\begin{pmatrix}
			1\\0
		\end{pmatrix}=\begin{pmatrix}
			1\\0
		\end{pmatrix}=\ket{0}, \hspace{1cm}
		\sigma_{z}\ket{1}=\begin{pmatrix}
			1&0\\
			0&-1
		\end{pmatrix}\begin{pmatrix}
			0\\1
		\end{pmatrix}=\begin{pmatrix}
			0\\-1
		\end{pmatrix}=-\ket{1}.
	\end{equation}
	Similarly, the Pauli $Y$ gate flips the quantum state and performs a phase flip operation on a quantum state 
	\begin{equation}
		\sigma_{y}\ket{0}=\begin{pmatrix}
			0&-\dot{\iota}\\
			\dot{\iota}&0
		\end{pmatrix}\begin{pmatrix}
			1\\0
		\end{pmatrix}=\begin{pmatrix}
			0\\\dot{\iota}
		\end{pmatrix}=\dot{\iota}\ket{1}, \hspace{1cm}
		\sigma_{y}\ket{1}=\begin{pmatrix}
			0&-\dot{\iota}\\
			\dot{\iota}&0
		\end{pmatrix}\begin{pmatrix}
			0\\1
		\end{pmatrix}=\begin{pmatrix}
			-\dot{\iota}\\0
		\end{pmatrix}=-\dot{\iota}\ket{0}.
	\end{equation}
	A more general form of single-qubit gates is the rotations around the $x,y,z$ axis of the Bloch sphere \citet{Nielsen2011QuantumComputing}. These rotations are parametrized with respect to an angle and have the form
	\begin{equation}
		\begin{aligned}
			R_{x}(\theta) &=  e^{-i\frac{\theta}{2}\sigma_x} = \begin{pmatrix}
				\cos(\frac{\theta}{2})   & -i\sin(\frac{\theta}{2}) \\
				-i\sin(\frac{\theta}{2}) & \cos(\frac{\theta}{2})
			\end{pmatrix},\\
			R_{y}(\theta) &=  e^{-i\frac{\theta}{2}\sigma_y} = \begin{pmatrix}
				\cos(\frac{\theta}{2})  & -\sin(\frac{\theta}{2}) \\
				\sin(\frac{\theta}{2}) & \cos(\frac{\theta}{2})
			\end{pmatrix},\\
			R_{z}(\theta) &=  e^{-i\frac{\theta}{2}\sigma_z} = \begin{pmatrix}
				e^{-i\frac{\theta}{2}} & 0                     \\
				0                      & e^{i\frac{\theta}{2}}
			\end{pmatrix}.
		\end{aligned}
	\end{equation}
	The previous rotations can now defined a general rotation of a qubit parametrized by three angles given by 
	\begin{equation}
		U(\theta,\phi,\lambda) = R_z(\phi)R_x(-\frac{\pi}{2})R_z(\theta)R_x(\frac{\pi}{2})R_z(\lambda) \doteq \begin{pmatrix}
			\cos\frac{\theta}{2} & -e^{-i\lambda}\sin\frac{\theta}{2}\\
			e^{i\phi}\sin\frac{\theta}{2}& e^{i(\phi + \lambda)}\cos\frac{\theta}{2}
		\end{pmatrix}	.
	\end{equation}
	
	Besides single qubit gates, multiqubit gates act over multiple qubits through unitary quantum operations. These gates are usually responsible for entanglement between qubits. The Controlled-NOT (CNOT) gate is the most common two-qubit quantum gate. The quantum state is changed only when the control qubit is in the $\ket{1}$ state and remains unchanged if the control qubit is in the $\ket{0}$  state. The matrix representation of the Controlled-NOT gate is defined as follows
	\begin{equation}
		CNOT=\ket{0}\bra{0}\otimes I+\ket{1}\bra{1}\otimes X=\begin{pmatrix}
			1 &0&0&0 \\
			0 &1&0&0 \\
			0 &0&0&1 \\
			0 &0&1&0 \\
		\end{pmatrix}.
	\end{equation}
	The operation of the CNOT gate on two-qubit quantum states is given by
	\begin{equation}
		CNOT\ket{00}=\ket{00}, \hspace{0.2cm}  CNOT\ket{01}=\ket{01}, \hspace{0.2cm}  CNOT\ket{10}=\ket{11}, \hspace{0.2cm}  CNOT\ket{11}=\ket{10}.
	\end{equation}
	The concept of controlled gates is extended to controlled general rotations, which can be represented in a matrix of the form
	\begin{equation}
		CU(\theta,\phi,\lambda)=\ket{0}\bra{0}\otimes I+\ket{1}\bra{1}\otimes U(\theta,\phi,\lambda)=\begin{pmatrix}
			1 &0&0&0 \\
			0 &1&0&0 \\
			0 &0&\cos\frac{\theta}{2}& -e^{-i\lambda}\sin\frac{\theta}{2} \\
			0 &0&e^{i\phi}\sin\frac{\theta}{2}& e^{i(\phi + \lambda)}\cos\frac{\theta}{2}\\
		\end{pmatrix}.
	\end{equation}
	
	A SWAP gate is often used in quantum algorithms to exchange qubits in two quantum registers. The SWAP gate is given by
	\begin{equation}
		SWAP = \begin{pmatrix}
			1&0&0&0 \\
			0&0&1&0 \\
			0&1&0&0 \\
			0&0&0&1\\
		\end{pmatrix}.
	\end{equation}
	Acting over two arbitrary states $\ket{\phi}\ket{\psi}$ it will give
	\begin{equation}
		SWAP\ket{\phi}\ket{\psi} = \ket{\psi}\ket{\phi}.
	\end{equation}
	We have reviewed some of the most common quantum gates, however, with one two-qubit gate and four single-qubit gates, is possible to build any multiqubit quantum circuit \citep{Divicenzo1995UniversalQGates}.
	\subsection{\label{app: Measurement}Quantum measurements}
	We need to perform measurements to retrieve quantum information from quantum state $\ket{\psi}$. The outcomes of these measurements are random, whose probabilities are given by the elementary operators $\mathbf{M}_{i}=\ket{\phi_{i}}\bra{\phi_{i}}$ through Born's rule \citep{Nielsen2011QuantumComputing}
	\begin{equation}
		p_{i}=\braket{\psi|\mathbf{M}_{i}|\psi}.
	\end{equation}
	The measurement operators are a collection of rank-$1$ projectors that sum to the identity $\sum_{i}\mathbf{M}_{i}=\mathbb{I}$. Generally, we take the computational basis, which is orthonormal, to define the measurement operators for different quantum algorithms.
	
	Similarly, we can perform partial measurements over some fraction of the qubits that compose the system. For example, performing measurements only on the first qubit in the $\ket{0}$ basis gives
	\begin{equation}
		p_{0}=\bra{\psi}\ket{0}\bra{0}\otimes \mathbb{I}\cdots \otimes \mathbb{I}\ket{\psi}.
	\end{equation}
	Partial measurements are used in the Hadamard and SWAP tests to estimate the inner product between quantum states.
	\subsection{\label{app: Encoding}Amplitude encoding}
	
	The amplitude encoding scheme encodes each element of the normalized vector into a coefficient of the quantum state. Suppose we have a vector
	$
	\mathbf{x}=[
	x_{1},
	\cdots,
	x_{2^{n}}
	]
	$, where $\mathbf{x}\in \mathbb{C}^{2^{n}}$. We first normalize the vector such that $\sum_{i}^{2^{n}}|x_{i}|^{2}=1$ and encode the vector into the quantum state as \citep{Schuld2021QML}
	\begin{equation}
		\ket{\psi_{\mathbf{X}}}=\sum_{i=0}^{2^{n}-1}x_{i}\ket{i}.
	\end{equation}
	We can also encode the matrix $\mathbf{A}\in \mathbb{C}^{2^{n}\times 2^{n}}$ with entries $a_{ij}$ that fulfill $\sum_{ij}\|a_{ij}\|^{2}=1$ in a similar fashion to 
	\begin{equation}
		\ket{\psi_{\mathbf{A}}}=\sum_{i=0}^{2^{n}-1}\sum_{j=0}^{2^{n}-1}a_{ij}\ket{i}\ket{j},
	\end{equation}
	with the enlarged Hilbert space accordingly. The index registers $\ket{i}$, $\ket{j}$ refer to the $i$th row and the $j$th column of the matrix $\mathbf{A}$.
	\subsection{\label{app: Hadamard}Hadamard test}
	
	The Hadamard test calculates the inner product between two states along with correct signs. The additional overhead with the Hadamard test is that the ancilla qubit is entangled with both quantum states $\ket{\psi_1}$ and $\ket{\psi_2}$ as follows \citep{Schuld2021QML}:
	\begin{equation}
		\ket{\psi_3}=\frac{1}{\sqrt{2}}\left(\ket{0}\ket{\psi_1}+\ket{1}\ket{\psi_2}\right).
	\end{equation}
	Once we have the state available in this entangled form, we then apply a Hadamard gate to the ancilla qubit, resulting in
	\begin{equation}
		\ket{\psi_3}=\frac{1}{2} \ket{0} \otimes \left(\ket{\psi_1}+\ket{\psi_2}\right)+\frac{1}{2}\otimes \left(\ket{\psi_1}-\ket{\psi_2}\right).
	\end{equation}
	After this, we measure on the computational basis. The real and imaginary parts of the inner product between the two quantum states are given by
	\begin{equation}
		Re(\braket{\psi_1|\psi_2})=2p_0 - 1,\quad Im(\braket{\psi_1|\psi_2})=1-2p_0.
	\end{equation}
	
	When the states are real, the calculation of the real part becomes the actual inner product between the two quantum states.
	\subsection{\label{app: swap}SWAP test}
	The SWAP test in quantum computers is used to extract the square of the absolute value of the inner product between two quantum states $\ket{\psi_1}$ and $\ket{\psi_2}$ \citep{Schuld2021QML}. Suppose that we are given these two quantum states in the tensor product form $\ket{\psi_1}\ket{\psi_2}$. We start by adding an extra ancilla qubit $\ket{0}\ket{\psi_1}\ket{\psi_2}$. Then, we apply a Hadamard gate to the ancilla qubit, resulting in the following quantum state
	\begin{equation}
		\frac{1}{\sqrt{2}}\left(\ket{0}+\ket{1}\right)\ket{\psi_1}\ket{\psi_2}=\frac{1}{\sqrt{2}}\left(\ket{0}\ket{\psi_1}\ket{\psi_2}+\ket{1}\ket{\psi_1}\ket{\psi_2}\right).
	\end{equation}
	Next, we apply the swap operation between the two states conditioned on the ancilla qubit being in the quantum state $\ket{1}$ as
	\begin{equation}
		\frac{1}{\sqrt{2}}\left(\ket{0}\ket{\psi_1}\ket{\psi_2}+\ket{1}\ket{\psi_1}\ket{\psi_2}\right) \xrightarrow[\text{SWAP}]{\text{Controlled}}     \frac{1}{\sqrt{2}}\left(\ket{0}\ket{\psi_1}\ket{\psi_2}+\ket{1}\ket{\psi_2}\ket{\psi_1}\right) .
	\end{equation}
	Applying another Hadamard gate on the ancilla qubit we have
	\begin{equation}
		\frac{1}{2}\left(\ket{0}\otimes\left(\ket{\psi_1}\ket{\psi_2}+\ket{\psi_2}\ket{\psi_1}\right)+\ket{1}\otimes\left(\ket{\psi_1}\ket{\psi_2}-\ket{\psi_2}\ket{\psi_1}\right)\right).
	\end{equation}
	When we measure the ancilla qubit, the probability of measuring the state $\ket{0}$ is
	\begin{equation}
		p_{0}=\frac{1}{2}+\frac{1}{2}|\braket{\psi_1|\psi_2}|^{2}.
	\end{equation}
	This method evaluates the inner product, however, the squared of the absolute value does not allow us to estimate the sign of the inner product.
	
	
	
\end{document}