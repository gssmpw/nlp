\section{Introduction}

Characterizing the atomic structure in functional materials is fundamental for understanding and optimizing materials for e.g. new energy technologies. Such characterization can be done using X-ray diffraction (XRD), and in particular, powder X-ray diffraction (PXRD) is widely used in materials chemistry and related fields as a main characterization tool~\cite{Cheetham2014}. PXRD patterns contain information on the arrangement of atoms in a sample. However, direct structure solution (i.e. determining all atomic coordinates) is challenging from PXRD data. Instead, PXRD data analysis is generally done based on a known structural model, while parameters can be refined against experimental data~\cite{young1995rietveld}. Identifying a model for such analysis is often referred to as \textit{fingerprinting}, and can be a very challenging task, where chemical intuition as well as extensive database searches are required. Even then, model identification is not always successful, and this hinders further material development. Several advances in machine learning (ML) have shown promise in aiding analysis of diffraction data over the last decade~\cite{Tatlier2011, Bunn2016, Oviedo2019, Wang2020}, including recent advances in generative models for crystal structure prediction (CSP)~\cite{jiao2023crystal, mohanty2024crystext, antunes2024crystalstructuregenerationautoregressive}. While these conventional ML-aided CSP methods typically explore the structural phase space guided by energy evaluations or high-level descriptors like material composition, emerging \textit{data-informed CSP} approaches have started to integrate diffraction data directly into the generative process~\cite{kjaer2023deepstruc, guo2024abinitiostructuresolutions, riesel2024crystal, lai2025endtoendcrystalstructureprediction}. This shift represents a fundamental departure from conventional CSP.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.695\textwidth]{Figures/illustrations/deCIFer_overview_final_version.png}}
\caption{deCIFer performs autoregressive crystal structure prediction (CSP) from PXRD data, optionally guided by tokenized crystal descriptors. PXRD embeddings are prepended to the CIF token sequence, enabling the generation of structurally consistent CIFs directly from diffraction data.} \label{fig:overview}
\end{center}
\end{figure}


In this work, we introduce deCIFer (Figure~\ref{fig:overview}), a transformer-based model that directly generates Crystallographic Information Files (CIFs) -- text files that encode for crystal structures -- {\em conditioned} on the rich structural information explicitly represented in experimental data, such as PXRD patterns. We thus combine the strength of CSP with the possibility to include experimental data in the workflow. This approach can significantly advance the workflow used for atomic structure characterization based on PXRD methods. Instead of conditioning on the raw scattering data, in this work we learn PXRD embeddings based on a variety of data transformations. These transformations are designed to mimic noise due to experimental variations. 

We evaluate the performance of deCIFer on a diverse set of PXRD patterns, demonstrating its robustness to noise and its ability to generate high-quality crystal structures. By using learned conditioning embeddings to extract fine-grained structural features from PXRD patterns, deCIFer is the first of its kind that produces syntactically correct and structurally meaningful CIFs while agreeing with reference PXRD patterns.

Our key contributions in this work are:
\vspace{-0.35cm}
\begin{enumerate}
    \itemsep0em 
    \item  A novel autoregressive transformer model for CSP that incorporates materials science domain knowledge by conditioning on PXRD data to generate viable crystal structures in CIF format. This extends the use of CSP models, as experimental data can be incorporated in the material analysis.
    \item An efficient conditioning mechanism for autoregressive models when dealing with variable length input.
    \item High-fidelity simulation of experimental settings for PXRD data with Gaussian noise and peak broadening. 
    \item Systematic evaluation on two large-scale datasets: NOMA\footnote{NOMA stands for \textbf{N}OMAD~\cite{nomad2019} \textbf{O}QMD~\cite{kirklin2015open} \& \textbf{M}P~\cite{materialsproject2013} \textbf{A}ggregation.
    }
    \& CHILI-100K~\cite{FriisJensenJohansen2024}.
\end{enumerate}

\subsection*{Background and related work} 

PXRD is the most widely used method for structural characterization in materials chemistry. A PXRD pattern contains diffraction peaks, whose position and intensity contain information on the periodic structure in crystalline materials, i.e. their atomic positions and structure symmetry. Most quantitative analysis of PXRD data is done using structure refinement, where the parameters in a structural model is refined against experimental data. This requires a structural starting model of the correct structure type and symmetry. Identifying such a model is often a bottleneck for materials characterization.

The transformer architecture~\cite{vaswani2017attentionneed} with its capacity to capture long-range dependencies has inspired applications beyond natural language processing, including applications in materials science, where transformer-based large language models (LLMs) have been employed for CSP and property analysis. While LLMs have seen emerging use in automation of chemical syntheses~\cite{hocky2022natural, szymanski2023autonomous, m2024augmenting}, data extraction~\cite{gupta2022matscibert, dagdelen2024structured, polak2024extracting, schilling2025text}, and materials simulation and property prediction~\cite{zhang_dpa-2_2024, rubungo2024llmmatbench, jablonka2024leveraging}, they are not yet as widely used in tasks like materials design. ChemCrow~\cite{m2024augmenting}, for instance, is an LLM-powered chemistry search engine designed to automate reasoning tasks in materials design and other domains.

Recent works have used fine-tuning to adapt LLMs for CSP. \citet{gruver2024finetuned} fine-tuned Llama-2 models~\cite{touvron2023llama} on text-encoded atomistic data, enabling tasks like unconditional generation of stable materials. Similarly, \citet{mohanty2024crystext} fine-tuned LLaMA-3.1-8B~\cite{dubey2024llama} using QLoRA~\cite{dettmers2024qlora} for efficient CIF generation conditioned on material composition and space group. In contrast, the work on CrystalLLM~\cite{antunes2024crystalstructuregenerationautoregressive} relies on pre-training alone to generate CIFs. It is trained on an extensive corpus of CIFs representing millions of inorganic compounds. 

CrystalLLM relies on composition- and symmetry- level crystal descriptors. Its utility, as most other CSP methods, is thus to generate structures without direct reference to experimental data. deCIFer, on the other hand, conditions on PXRD signals, produces structures agreeing with observed diffraction patterns and address the need for models that bridge predictive power with experimental reality.

In parallel to LLM-based approaches, generative models using diffusion- or flow-based frameworks have also emerged for CSP~\cite{jiao2023crystal, millerflowmm,zeni2025generative}. These methods rely on composition or partial structural constraints to guide generation, which requires domain knowledge and can limit direct incorporation of experimental data. While they show promise in producing stable crystal configurations, the gap between purely computational crystal structures and experimental data remains a challenge. The recent diffusion-based framework MatterGen~\cite{zeni2025generative} addresses some of these challenges by enabling conditioning on a broad range of property constraints, and it can generate structures that are more likely to be synthesizable than previous methods; nevertheless, conditioning on PXRD has not been demonstrated in its current implementation, leaving room for further exploration in integrating experimental data more directly.

\section{Methods}

Consider a crystal structure in CIF (text input) format that is tokenized into a sequence of length $T_i$: $\xbf^i = (x^i_1,x^i_2, \dots, x^i_{T_i})$ (see Appendix~\ref{sup-sec:cif_tokenization} for details on tokenization). The corresponding scattering pattern, which in our case is the PXRD pattern denoted by $\ybf^i$, is a continuous-valued vector representing the intensity profile of the scattering pattern. The dataset then consists of pairs of CIFs and PXRD patterns, $\Dcal = [(\xbf^i,\ybf^i)]_{i=1}^{N}$. 

Given this dataset, we want to minimize the negative conditional log-likelihood over the training data: 
%
\begin{equation}
    \Lcal(\Xbf|\Ybf;\Thetabf) = \frac{1}{N} \sum_{i=1}^{N} \left( -\sum_{t=1}^{T_i} \log P_\Thetabf(x^i_t|x^i_{<t},\ybf^i)\right). 
\end{equation}
This is achieved using a conditional autoregressive model $f_\Thetabf(\cdot)$ with trainable parameters $\Thetabf$ based on the transformer architecture~\cite{vaswani2017attentionneed}. Our autoregressive model, deCIFer, generates structures in CIF format when conditioned with an initial prompt and the PXRD data.

\subsection{PXRD Conditioning}\label{sec:PXRD_conditioning}

The PXRD data can be seen as the structural fingerprint of the structures in CIFs. We use this PXRD data to steer the CSP by using it as a conditioning input. 

Following the standard procedure in materials chemistry, for each CIF, we generate the discrete diffraction peak data, given as the set $\Pcal = \{(q_k,i_k)\}_{k=1}^n$ using \texttt{\small pymatgen}~\cite{Ong2013}. During the model training, $\Pcal$ is transformed into the PXRD data, $\ybf$, using different simulated experimental conditions. Formally, let $\Tcal$ be a set of transformations that can be applied to each $\Pcal$.

The family of transformations used in this work are designed to closely mimic the experimental conditions. 
We define a distribution of transformations $\Tcal$ such that each $\tau \sim \Tcal$ comprises 1) a random peak broadening with full width at half maximum (FWHM) $\sim\mathcal{U}(0.001, 0.100)$ and 2) additive noise with variance $\sigma_{\mathrm{noise}}^2\sim\mathcal{U}(0.001, 0.050)$. Concretely, each $\tau$ is the composition of these two steps, and new values for broadening and noise are sampled on each draw, $\ybf = \tau(\Pcal)$. Hence for each CIF, the PXRD pattern is transformed slightly differently every time it appears during training. Later for evaluation, we use $\tau_{\mathrm{fixed}}$ with manually specified parameters to test the robustness of the models. The \textit{clean} transformation $\tau_0$ fixes FWHM = $0.05$ and $\sigma_{\mathrm{noise}}^2=0$. Examples from $\Tcal$ on a PXRD are shown in Figure~\ref{noise_broadeness_levels} (in Appendix).

{\bf Conditioning model:} A multi-layered perceptron (MLP) with trainable parameters $\Phibf$ is used to embed the PXRD data into a learnable conditioning vector $\ebf = f_\Phibf(\ybf) \in \mathbb{R}^D$. This embedding is inserted at the start of the tokenized CIF sequence as a conditioning signal. Jointly training the conditioning network, $f_\Phibf$, and the autoregressive model, $f_\Thetabf$, enables the integration of structural information from the PXRD profiles in the autoregressive generation of CIFs. This joint training along with transformations to the PXRD data results in the final training objective $\Lcal(\Xbf|\Ybf;\Thetabf,\Phibf)$.

\subsection{Sequence packing and Isolating CIFs} \label{sec:EfficientBatching}

To efficiently train on variable-length CIF sequences, we implement a batching strategy during training, inspired by recent sequence-packing methods that prevent cross-contamination and improve throughput~\cite{KosecFuKrell}. Our approach concatenates multiple tokenized CIF sequences, each of length $T_i$ (excluding the conditioning embedding) into segments of fixed context length, $C$. Formally, consider a sequence $\mathbf{S} = [\mathbf{e}^{1}, \tbf^{1}_1, \dots, \tbf^{1}_{T_1}, \mathbf{e}^{2}, \tbf^{2}_1, \dots, \tbf^{n}_k]$, where $n$ is the last fully or partially included CIF. In this notation $\ebf^{i}$ denotes a $D$-dimensional conditioning embedding for the $i$-th CIF, while $\tbf_j^i$ is the $D$-dimensional input embedding for $x_j^i$, the $j$-th token of the $i$-th CIF. We choose $k$ such that $|\mathbf{S}| = C$. If adding another CIF exceeds $C$, that CIF is split at the boundary and continued in the next segment. In practice, we set $C=3076$ based on the available GPU memory to balance throughput and memory constraints, which exceeds the length of the longest tokenized CIFs in the NOMA dataset. Although our method guarantees efficient batch utilization, it does occasionally split exceptionally long CIFs between batches ($\approx\!0.04$\% of samples in the NOMA dataset; see Figure~\ref{sup-fig:dataset_statistics_NOMA} in the Appendix). To mitigate this, we shuffle the training set at the start of each epoch so that previously split CIFs are more likely to appear in full in subsequent mini-batches, allowing the model to learn from complete sequences.

To isolate different CIFs in the same sequence, we employ an attention mask $\Mbf$, defined such that $M_{kl} = 1$ iff tokens $k$ and $l$ belong to the same CIF, and $M_{kl} = 0$ otherwise. This results in a block-wise diagonal, upper-triangular, attention matrix as shown in Figure~\ref{fig:attn_masking}. To prevent positional information from leaking across CIF boundaries we also reset positional encodings at the start of each CIF by assigning positions from $0$ to $T_{i}-1$ for the $i$-th sequence. 

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.695\textwidth]{Figures/data_statistics/attention_masking.png}}
\caption{Visualization of the attention masking strategy, showing the log-mean attention weights (averaged over all heads) for an example sequence and highlighting how CIFs are isolated using the attention mask. The figure also illustrates how the embeddings of the second CIF attend to the conditioning PXRD embedding. Lighter shades indicate stronger attention.
} \label{fig:attn_masking}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Evaluation Metrics}

We evaluate generated structures using four metrics. 

1) {\bf Residual weighted profile} ($R_{\mathrm{wp}}$) is applied to convolved continuous PXRD profiles, measuring the discrepancy between a reference PXRD profile, $\ybf$, and a generated PXRD profile, $\ybf^*$. Formally:
\vspace{-0.2cm}
\begin{equation}
        R_\mathrm{wp} = \sqrt{\frac{\sum_{i} w_i (y_i - y_i^*)^2}{\sum_{i} w_i y_i^2}},
\end{equation}
with all weights $w_i = 1$ here, following convention.

2) {\bf Wasserstein distance} (WD) is used to compare the non-convolved, discrete intensity peaks of the PXRD patterns. WD measures the minimal transport cost of moving peaks in the reference set $\Pcal = \{(q_k,i_k)\}_{k=1}^n$ to peaks in the generated set $\Pcal^* = \{(q_j^*,i_j^*)\}_{j=1}^m$. Letting $\pi_{kj}$ be a coupling between intensities at positions $q_k$ and $q_j^*$, we have in 1D:
\begin{equation}
    \mathrm{WD}(\Pcal, \Pcal^*) = \min_{\pi} \sum_{k=1}^{n} \sum_{j=1}^{m} \pi_{kj} \| q_k - q_j^* \|,
\end{equation}
where $\sum_{j=1}^{n} \pi_{kj} = i_k \text{ and } \sum_{k=1}^{m} \pi_{kj} = i_j^*$.

3) {\bf Match rate} (MR) uses \texttt{\small StructureMatcher}~\cite{Ong2013} to assess structural similarity between reference and generated CIFs. Two structures are considered a match if their lattice parameters, atomic coordinates, and space group symmetries are within the defined tolerances. MR is the fraction of matching structures. See Section~\ref{sup-sec:match_rate} in the Appendix for more information.

4) {\bf Validity} (Val.) ensures the internal consistency of each generated CIF by checking formula consistency, site multiplicity, bond lengths, and space group alignment. A CIF is deemed \textit{overall valid} only if it passes all four checks. Detailed explanations of these metrics are provided in Section~\ref{sup-sec:validity} in the Appendix.

Figure~\ref{fig:evaluation-pipeline} summarizes the evaluation pipeline.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.695\textwidth]{Figures/illustrations/evaluation_pipeline_new_notation.png}}
\caption{Evaluation pipeline. A reference CIF from the test set is used to generate discrete peaks, $\mathcal{P} = \{(q_k, i_k)\}_{k=1}^n$. These peaks are then transformed into a PXRD profile $\mathbf{y} = \tau_{\mathrm{fixed}}(\mathcal{P})$, where $\tau_{\mathrm{fixed}}$ is selected according to the dired experimental setting. The resulting PXRD, along with any optional crystal descriptors, is tokenized and passed to deCIFer to produce a new CIF ($\text{CIF}^*$). For evaluation, the generated structures are compared using a \emph{clean} reference transformation $\tau_0$ (FWHM$=0.05$, $\sigma_{\mathrm{noise}^2} = 0$).
} \label{fig:evaluation-pipeline}
\end{center}
\vskip -0.2in
\end{figure}

\section{Dataset and Experiments}

{\bf Dataset}: We use two datasets here. The first, NOMA, is a large-scale compilation of crystal structures curated in CrystaLLM~\cite{antunes2024crystalstructuregenerationautoregressive}, which draws from the Materials Project (April 2022)~\cite{materialsproject2013}, OQMD (v. 1.5, October 2023)~\cite{kirklin2015open}, and NOMAD (April 2023)~\cite{nomad2019} databases. The second, CHILI-100K~\cite{FriisJensenJohansen2024}, is a large-scale dataset of experimentally determined crystal structures obtained from a curated subset of the Crystallography Open Database (COD)~\cite{COD2009}. We employ NOMA for both training and testing our models, whereas CHILI-100K is used exclusively for testing. Both datasets are open-source and available for download.\footnote{NOMA: \url{github.com/lantunes/CrystaLLM} (CC-BY 4.0 licence), CHILI-100K: \url{github.com/UlrikFriisJensen/CHILI} (Apache 2.0 licence).}

{\bf Preprocessing}: We begin be applying the standard data preprocessing steps outlined in CrystaLLM before performing additional curation to address complexities such as ionic charges and to ensure consistent structural representation between NOMA and CHILI-100K. The resulting NOMA dataset comprises approximately $2.3$M CIFs, spanning $1$--$10$ elements, including elements up to atomic number $94$ (excluding polonium, astatine, radon, francium, and radium). Duplicate structures were filtered by selecting the configuration with the lowest volume per formula unit. All structures were converted to a standardized CIF format using the \texttt{\small pymatgen} library~\cite{Ong2013} and all floating point values were rounded to four decimal places. The resulting CHILI-100K dataset consists of $\approx\!8.2$K CIFs, spanning $1$--$8$ elements, including elements up to atomic number $85$. Figures~\ref{sup-fig:dataset_statistics_NOMA} and~\ref{sup-fig:dataset_statistics_CHILI_100K} (in Appendix) show the distribution of elemental compositions, space groups, and more for NOMA and CHILI-100K.

A challenge in NOMA is the disproportionate representation of high-symmetry structures. To account for the skew in the distribution, we derive the \textit{space group}, a numerical label from $1$ to $230$ that encodes crystal symmetry, from each CIF. We stratify the data by grouping these labels into bins of $10$, e.g., space groups $1$--$10$, $11$--$20$, etc., and use these strata to split data into $90$\% training, $7.5$\% validation, and $2.5$\% testing subsets while preserving the overall distribution of crystal symmetries. 

The dataset was tokenized into a vocabulary of $373$ tokens, encompassing CIF tags, space group symbols, element symbols, numeric digits, punctuation marks, padding tokens and a special conditioning token. For a detailed explanation of the standardization and tokenization processes, refer to Section~\ref{sup-sec:cif_standardization} and \ref{sup-sec:cif_tokenization} in the Appendix.

\paragraph{Hyperparameters:} deCIFer has two learnable components, $f_{\Phibf}$ and $f_{\Thetabf}$. The encoder $f_{\Phibf}$ is a 2-layer MLP that takes a PXRD profile of dimension $1000$ and outputs a $512$-dimensional embedding. This embedding is prepended as the conditioning-token to the sequence of CIF tokens, where each CIF token is also of $D=512$ dimensions. $f_{\Thetabf}$ is a decoder-only transformer~\cite{vaswani2017attentionneed} with $8$ layers, each containing $8$ attention heads. We use a context length of $3076$ and a batch size of $32$. A linear warm-up of the learning rate is applied over the first $100$ epochs, followed by a cosine decay schedule using AdamW~\cite{AdamW2017} ($\beta_1 = 0.9, \beta_2 = 0.95$, weight decay $10^{-1}$) for $50$K epochs with gradient accumulation of $40$ steps on a single NVIDIA A100 GPU with mixed-precision acceleration. All aspects of the model architecture were implemented in Pytorch~\cite{Pytorch2019}. $f_{\Phibf}$ has $\approx\!0.78$M trainable parameters and $f_{\Thetabf}$ contains $\approx\!26.94$M, resulting in a deCIFer model with $27.72$M parameters. The code is open-source and additional details can be found in Section~\ref{sup-sec:model_architecture} in the Appendix. 

\paragraph{Experiments:} We conducted a series of experiments to evaluate deCIFer's ability to perform CSP when conditioned on PXRD profiles, space group, and composition. First, we compare the baseline performance of deCIFer against an unconditioned variant (U-deCIFer) to assess the direct impact of PXRD conditioning on the generated structures. Next, we introduce controlled noise and peak broadening into the input PXRD data, examining deCIFer's robustness under more challenging scenarios resembling real-world PXRD data. Finally, we apply deCIFer (trained on NOMA) to CHILI-100K~\cite{FriisJensenJohansen2024} to demonstrate its scalability and performance on more complex crystal systems. In these experiments, we generate one CIF for each reference sample in the test sets. For consistent evaluation, each reference CIF is also processed through the fixed, \textit{clean} transformation $\tau_0$ (FWHM $= 0.05$, $\sigma_{\mathrm{noise}}^2 = 0$).

For some experiments we allow space group and composition to be specified as crystal descriptors. Since CIFs inherently encode space group and composition as text, we treat these descriptors as standard tokens in our vocabulary (see Section~\ref{sup-sec:cif_tokenization} in the Appendix). The space group appears in the CIF header, while composition is stored as element–count pairs. During inference, these tokens are optionally inserted into the CIF.

\section{Results}
\label{sec:results}

\subsection{Importance of PXRD Conditioning}

We compare deCIFer and U-deCIFer to study the effect of PXRD conditioning. We evaluate each model in three settings: without any crystal descriptors (``none''), with only compositional information (``comp.''), and with both compositional and space group information (``comp. + s.g.''). Both models share the same architectural backbone, but deCIFer receives structural guidance via the PXRD conditioning. 

Table~\ref{tab:baseline_performance} shows that deCIFer achieves lower average $R_{\mathrm{wp}}$ and WD than U-deCIFer across all descriptor settings. Figure~\ref{fig:violinplot_baseline} visualizes the distributions of $R_{\mathrm{wp}}$ and WD for both models, highlighting the large performance improvement due to PXRD conditioning, and a smaller but still notable improvement due to composition conditioning. Figure~\ref{fig:barplot_baseline} shows that structures of more common crystal systems lead to better outcomes, while under-represented, lower symmetry crystal systems remain challenging. Figure~\ref{fig:gen_samples} further illustrates the effectiveness of PXRD conditioning with three examples from the NOMA test set, showcasing the range of generated structures, from near-perfect alignment to structural mismatch. While U-deCIFer benefits from the addition of crystal descriptors, it never reaches the accuracy of deCIFer. Both models achieve high rates of syntactic validity, with the unconditional model marginally surpassing deCIFer (Table~\ref{tab:baseline_validity}). Overall, these results demonstrate that conditioning on PXRD data and crystal descriptors substantially enhances the model's ability to generate CIFs with close alignment to desired PXRD profiles.

\begin{figure}[t!]
\begin{center}
\centerline{\includegraphics[width=0.695\textwidth]{Figures/baseline/baseline_violin_plot_metrics.pdf}}
\caption{Distribution of $R_{\mathrm{wp}}$ and WD for deCIFer and U-deCIFer on the NOMA test set, presented as violin plots with overlain boxplots; the median is shown for each distribution. Lower $R_{\mathrm{wp}}$ and WD values indicate a better alignment between the reference- and generated CIFs in terms of the reference PXRD profile. The models are evaluated in three settings: ``none” (no crystal descriptors), ``comp.” (composition only), and ``comp. + s.g.” (composition plus space group).}
\label{fig:violinplot_baseline}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[t!]
\begin{center}
\centerline{\includegraphics[width=0.695\textwidth]{Figures/baseline/baseline_crystal_systems_metrics.pdf}}
\caption{
Average metric values by crystal systems for deCIFer on the NOMA test set show better performance for well-represented systems, while rarer, low-symmetry systems lead to worse performance ($\uparrow R_{\mathrm{wp}}, \uparrow$WD), reflecting their modelling complexity. The right-most plot shows crystal system distribution of the test set.
}
\label{fig:barplot_baseline}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.695\textwidth]{Figures/generated_examples/generated_samples_threecomparisons.png}}
\caption{
Three examples from the NOMA test set showcasing deCIFer generations. PXRD data is used for conditioning, with composition as a crystal descriptor. For each example, 100 CIFs are generated, and the median structure (based on $R_{\mathrm{wp}}$) is shown alongside the reference structure and PXRD profiles (noise-free, FWHM = $0.05$). The examples illustrate: (top) near-perfect PXRD conformity, (middle) structural match with suboptimal PXRD alignment, and (bottom) structural mismatch.
} \label{fig:gen_samples}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}[t]
\caption{Performance comparison for $20$K samples from the NOMA test set using deCIFer and U-deCIFer with descriptors. \textbf{none}: No crystal descriptors, \textbf{comp.}: compositional information only, and \textbf{comp.+ s.g.}: both compositional and space group information.}
\label{tab:baseline_performance}
\vskip 0.15in
\begin{center}
\scriptsize
\begin{tabular}{llcc}
\toprule
{\bf Desc.} & Model & $R_{wp}$ { $(\mu \pm \sigma) \downarrow$} & WD { $(\mu \pm \sigma) \downarrow$} \\
\midrule
\multirow{2}{*}{\bf none}           & U-deCIFer & $1.24$ {$\pm 0.26$} & $0.49$ { $\pm 0.31$} \\
                                & deCIFer   & $0.32$ { $\pm 0.34$} & $0.14$ { $\pm 0.21$} \\
\midrule
\multirow{2}{*}{\bf comp.}          & U-deCIFer & $0.82$ {\scriptsize $\pm 0.41$} & $0.28$ {\scriptsize $\pm 0.29$} \\
                                & deCIFer   & $0.25$ { $\pm 0.29$} & $0.10$ { $\pm 0.20$} \\
\midrule
\multirow{2}{*}{\bf comp. + s.g.}  & U-deCIFer & $0.65$ {\scriptsize $\pm 0.36$} & $0.23$ {\scriptsize $\pm 0.28$} \\
                                & deCIFer   & $0.24$ { $\pm 0.29$} & $0.10$ { $\pm 0.20$} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-0.3cm}
\end{table}

\begin{table}[ht!]
\setlength{\tabcolsep}{2pt} % Default value: 6pt
\caption{Validity of generated CIFs for the NOMA test set using deCIFer and U-deCIFer. Abbreviations: Form = formula validity, SG = space group validity, BL = bond length validity, SM = site multiplicity validity. Overall validity (Val.) is calculated as the percentage of CIFs that satisfy all four validity metrics simultaneously. Match rate (MR) represents the percentage of generated CIFs that replicate the reference CIF.}
\label{tab:baseline_validity}
\vskip 0.1in
\begin{center}
\tiny
\begin{tabular}{llcccccc} 
\toprule
{\bf Desc.} & {Model} & Form (\%) $\uparrow$ & SG (\%) $\uparrow$ & BL (\%) $\uparrow$ & SM (\%) $\uparrow$ & Val. (\%) $\uparrow$ & MR (\%) $\uparrow$\\
\midrule
\multirow{2}{*}{\bf none} 
& U-deCIFer & 99.82 & 98.87 & 94.30 & 99.47 & 93.49 & 0.00 \\
& deCIFer   & 99.42 & 98.85 & 93.69 & 99.46 & 92.66 & 5.01 \\
\midrule
\multirow{2}{*}{\bf comp.} 
& U-deCIFer & 99.87 & 99.09 & 94.40 & 99.46 & 93.78 & 49.30 \\
& deCIFer   & 99.68 & 99.21 & 94.37 & 99.55 & 93.73 & 91.50 \\
\midrule
\multirow{2}{*}{\bf comp.+s.g.} 
& U-deCIFer & 99.85 & 98.88 & 94.51 & 99.47 & 93.72 & 87.07 \\
& deCIFer   & 99.74 & 99.26 & 94.38 & 99.58 & 93.90 & 94.53 \\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Robustness to Perturbations in PXRD Conditioning}

We evaluated deCIFer's ability to generate accurate CIFs under varying levels of additive noise and peak broadening in the PXRD conditioning input, while also providing the composition as a crystal descriptor. Building upon the baseline scenario of clean, noise-free PXRD data, we tested several increasingly challenging conditions: maximum noise, maximum peak broadening, combined noise and broadening, and out-of-distribution (OOD) broadening levels beyond the model's training range.

Table~\ref{tab:robustness_performance} 
(and Figure~\ref{fig:violinplot_robustness} in Appendix) show that while additive noise results in slight performance degradation, the model remains robust to in-distribution noise and peak broadening. We also observe moderate performance degradation with OOD broadening or significantly higher noise levels. Unsurprisingly, we observe that lower-symmetry crystal systems remain more challenging to predict under perturbed conditions across all crystal systems. This is depicted in Figure~\ref{fig:barplot_robustness} in the Appendix.

\begin{table}[t]
\setlength{\tabcolsep}{2pt}
\caption{Performance of deCIFer under varying levels of noise and peak broadening in PXRD inputs for NOMA and CHILI-100K test sets, evaluated with compositional descriptors. We evaluate two in-distribution (ID) scenarios and one out-of-distribution (OOD) scenario for the PXRD input. Reported metrics include residual weighted profile ($R_{\mathrm{wp}}$), Wasserstein distance (WD), overall validity, and match rate.}
\label{tab:robustness_performance}
\vskip 0.15in
\begin{center}
\tiny
\begin{tabular}{lccccc}
\toprule
 {\bf Dataset} & ($\sigma_{\mathrm{noise}}^2$, {FWHM}) & $R_{\mathrm{wp}} \downarrow$  & WD$\downarrow$ & Val. (\%)$\;\uparrow$ & MR (\%)$\;\uparrow$\\
\midrule
\multirow{3}{*}{\bf NOMA} & ID: (0.00, 0.05)
& 0.25$\pm$0.29 & 0.10$\pm$0.20 & 93.73 & 91.50\\
 & ID: (0.05, 0.10)
& 0.31$\pm$0.30 & 0.12$\pm$0.21 & 93.77 & 89.28\\
\cmidrule{2-6}
 & OOD: (0.10, 0.20)
& 0.65$\pm$0.34 & 0.23$\pm$0.29 & 91.66 & 77.66\\
\midrule
\multirow{3}{*}{\bf CHILI-100K} &  ID: (0.00, 0.05)
& 0.70$\pm$0.37 & 0.22$\pm$0.21 & 41.83 & 37.34 \\
&  ID: (0.05, 0.10)
& 0.73$\pm$0.36 & 0.22$\pm$0.21 & 40.95 & 35.97\\
\cmidrule{2-6}
 & OOD: (0.10, 0.20)
& 0.87$\pm$0.33 & 0.24$\pm$0.20 & 33.62& 26.09\\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table}

\subsection{OOD Evaluation on CHILI-100K}

To evaluate the generalization of deCIFer to more complex crystal systems, we tested its performance on the CHILI-100K dataset~\cite{FriisJensenJohansen2024} which had no overlap with the NOMA training data for deCIFer. Unlike synthetic datasets, CHILI-100K presents a closer approximation to real-world challenges, including complex structural motifs and a broader distribution of crystal symmetries. Additionally, CHILI-100K contains a significantly higher proportion of lower-symmetry structures compared to synthetic datasets like NOMA (see the sample distribution in Appendix Figure~\ref{fig:barplot_chili}). 

The results on CHILI-100k dataset are summarized in Table~\ref{tab:robustness_performance} and Figure~\ref{fig:violinplot_chili}. These show that deCIFer maintains a reasonable level of structural accuracy on this challenging dataset. The relatively low validity score is partly due to challenges with bond length validity, which was notably lower than the other validity metrics. A full breakdown of the validity metrics for this experiment is available in Section~\ref{sup-sec:additional_results} in the Appendix.

Despite the performance drop, deCIFer demonstrated robustness to added noise and peak broadening in PXRD inputs, with stable $R_{\mathrm{wp}}$ and WD values across perturbed conditions as seen in Figure~\ref{fig:violinplot_chili}. This stability, along with its ability to generate a variety of structural features, suggests that deCIFer could be useful for practical applications involving experimentally derived PXRD data.

\begin{figure}[t!]
\begin{center}
\centerline{\includegraphics[width=0.695\textwidth]{Figures/chili100k/chili100k_violin_plot_metrics.pdf}}
\caption{Distribution of $R_{\mathrm{wp}}$ and WD for the CHILI-100K test set, presented as violin plots with overlain boxplots, for deCIFer using both PXRD conditionings and composition (\textbf{comp.}). The distributions show predictions under three PXRD conditioning scenarios: (top) clean, (middle) maximum noise and broadening, and (bottom) an out-of-distribution PXRD scenario.}
\label{fig:violinplot_chili}
\end{center}
\vspace{-0.4cm}
\end{figure}

\section{Discussion and Outlook}\label{sec:discussion}

{\bf PXRD-steered structure generation:} The experiments on NOMA and CHILI-100k in Section~\ref{sec:results} clearly show the utility of including conditioning data such as PXRD when performing CSP. Unlike unconditional structure generation, generating crystal structures that are steered by scattering data such as PXRD is more useful in applications where a specific target property might be desirable. 

By embedding the PXRD data, $\ybf$, into a learnable conditional embedding, $\ebf = f_\Phibf(\ybf)$, and prepending it onto prompts for deCIFer, we have demonstrated a general paradigm for integrating additional conditional data in materials design, an idea which can be extended to the incorporation of other material properties. In cases where multiple experimental data sources are available, they can be easily injected into the generative process using specific conditioning models. That is, if $P$ properties are available such that $\ybf = \{\ybf_1,\dots,\ybf_P\}$, then deCIFer can be extended to incorporate these additional data using additional conditioning models, $f_{\Phibf_1},\dots,f_{\Phibf_P}$, that can be jointly trained with the generative model, $f_\Thetabf$, via a training objective of the form $\Lcal(\Xbf|\Ybf_1,\dots,\Ybf_P; \Thetabf, \Phibf_1,\dots,\Phibf_P)$.

{\bf Consistency in CIF generation:} To investigate the consistency and variability of deCIFer, we generated 16K CIFs for the same PXRD profile with different crystal descriptors (``none'', ``comp.'', and ``comp+s.g.''). Figure~\ref{fig:self_consistency} illustrates our findings for a challenging monoclinic crystal system of Sr$_2$Cd$_2$Se$_4$ from the NOMA test set. When unconstrained by crystal descriptors, the model generates a wide diversity of cell-parameters, compositions, and space groups, yet the $R_{\mathrm{wp}}$ values tend to cluster in a relatively narrow range. In contrast, imposing compositional and, especially, space group constraints yields much tighter cell-parameter distributions and a broader $R_{\mathrm{wp}}$ distribution. The broader distributions highlight how the $R_{\mathrm{wp}}$ metric is highly sensitive to even small structural deviations and shows the importance of complementing less sensitive metrics like the WD with other validation methods. Despite these variations, the overall structural match rate to the reference CIF remains high. If the composition or space group is known with high confidence, incorporating these constraints can speed up convergence to a more accurate structural solution. Conversely, if the objective is to explore the material landscape more broadly, unconstrained generation with PXRD can be advantageous.

\begin{figure}[ht!]
\begin{center}
\centerline{\includegraphics[width=0.69\textwidth]{Figures/data_statistics/CIF_generateion_consistency.png}}
\vspace{-0.3cm}
\caption{Illustration of deCIFer-sampled structures for a single PXRD profile of the monoclinic crystal system of Sr$_2$Cd$_2$Se$_4$ (16K samples). $R_{\mathrm{wp}}$ comparisons were made with transformation $\tau_0$, void of noise and with FWHM$=0.05$, and with three crystal descriptor scenarios. a) Reference structure. b) Empiric probability distribution of generating a CIF with a given $R_{\mathrm{wp}}$ to the reference structure. c) Representative examples of generated structures showing best (lowest $R_{\mathrm{wp}}$), median, worst (highest $R_{\mathrm{wp}}$), and additional diverse examples. d) Distribution of the sampled crystal systems. e) Histograms of cell- lengths (a,b,c) and angles ($\alpha, \beta, \gamma$) for the 16K sampled structures, with reference cell parameters shown as dotted lines.}
\label{fig:self_consistency}
\end{center}
\vspace{-0.65cm}
\end{figure}

{\bf Limitations}: 
In materials science, the concept of data leakage is nuanced due to the challenges in defining novelty and diversity, as emphasized in recent discussions on scaling AI for materials discovery~\cite{cheetham2024ai_materials_discovery}. Although we stratify the NOMA dataset by space group bins and use CHILI-100K exclusively for testing, there remains the possibility of implicit overlaps, such as structurally similar crystal entries or shared compositional biases. However, the inherent redundancy in materials data, where many ``new" compounds are variations of existing structures, might simply reflect the natural landscape of materials science. Furthermore, the vast size and diversity of the NOMA dataset as well as the thorough preprocessing steps (e.g., de-duplication, filtering, and standardization), coupled with the independent curation of CHILI-100K, significantly mitigate the impact of any potential overlaps. 

We rely on two metrics to validate the performance of deCIFer, $R_{\mathrm{wp}}$ and WD. While $R_{\mathrm{wp}}$ is the standard in Rietveld refinement, and is highly sensitive to small deviations in atomic positions or cell parameters, it can overemphasize noise or strong peaks. Conversely, WD captures broad differences more robustly, but can be disproportionally sensitive to outlier peaks. Each reveal different aspects of pattern agreement, so employing them in tandem, along with comparing structural similarities is necessary for robust validation.

{\bf Future Work:} This work opens several directions for further exploration and improvement. 

One promising area for improvement lies in exploring more advanced decoding strategies, such as beam search, to enhance the generative model's capabilities in downstream tasks. By maintaining multiple hypotheses during decoding, beam search could produce diverse candidate CIFs for a given PXRD profile, improving structure determination accuracy by ranking outputs based on metrics like $R_{\mathrm{wp}}$. This method could also support optimization strategies that prioritize structural validity and relevance.

Another direction could be to integrate reinforcement learning from human feedback (RLHF) to guide the model more directly toward generating accurate and chemically valid structures~\cite{ziegler2019fine}. By defining a reward function tailored to properties such as low $R_{\mathrm{wp}}$ values, structural integrity, and adherence to chemical constraints, and interaction with a human expert, RLHF could further refine the model's outputs. 

\section{Conclusion}

In this work, we introduced deCIFer as a data-informed approach to CSP. deCIFer is an autoregressive transformer-based language model that generates CIFs directly from PXRD data. By conditioning on simulated PXRD profiles, deCIFer captures fine-grained structural information beyond what is possible with composition- or symmetry-based information alone. Evaluations on large synthetic and experimentally derived datasets highlight deCIFer's robust performance in scenarios with varying levels of noise and peak broadening. deCIFer was developed using lab-scale resources with a single GPU. This holds potential for diverse experiments going forward. The conditional prompting paradigm introduced in deCIFer can be used to integrate other experimental data. This can be essential to accelerating material design processes. 