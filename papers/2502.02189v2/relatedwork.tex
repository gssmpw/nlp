\section{Background and related work}
PXRD is the most widely used method for structural characterization in materials chemistry. A PXRD pattern contains diffraction peaks, whose position and intensity contain information on the periodic structure in crystalline materials, i.e. their atomic positions and structure symmetry. Most quantitative analysis of PXRD data is done using structure refinement, where the parameters in a structural model is refined against experimental data. This requires a structural starting model of the correct structure type and symmetry. Identifying such a model is often a bottleneck for materials characterization.

The transformer architecture~\cite{vaswani2017attentionneed} with its capacity to capture long-range dependencies has inspired applications beyond natural language processing, including applications in materials science, where transformer-based large language models (LLMs) have been employed for CSP and property analysis. While LLMs have seen emerging use in automation of chemical syntheses~\cite{hocky2022natural, szymanski2023autonomous, m2024augmenting}, data extraction~\cite{gupta2022matscibert, dagdelen2024structured, polak2024extracting, schilling2025text}, and materials simulation and property prediction~\cite{zhang_dpa-2_2024, rubungo2024llmmatbench, jablonka2024leveraging}, they are not yet as widely used in tasks like materials design. ChemCrow~\cite{m2024augmenting}, for instance, is an LLM-powered chemistry search engine designed to automate reasoning tasks in materials design and other domains.

Recent works have used fine-tuning to adapt LLMs for CSP. \citet{gruver2024finetuned} fine-tuned Llama-2 models~\cite{touvron2023llama} on text-encoded atomistic data, enabling tasks like unconditional generation of stable materials. Similarly, \citet{mohanty2024crystext} fine-tuned LLaMA-3.1-8B~\cite{dubey2024llama} using QLoRA~\cite{dettmers2024qlora} for efficient CIF generation conditioned on material composition and space group. In contrast, the work on CrystalLLM~\cite{antunes2024crystalstructuregenerationautoregressive} relies on pre-training alone to generate CIFs. It is trained on an extensive corpus of CIFs representing millions of inorganic compounds. 

CrystalLLM relies on composition- and symmetry- level crystal descriptors. Its utility, as most other CSP methods, is thus to generate structures without direct reference to experimental data. deCIFer, on the other hand, conditions on PXRD signals, produces structures agreeing with observed diffraction patterns and address the need for models that bridge predictive power with experimental reality.

In parallel to LLM-based approaches, generative models using diffusion- or flow-based frameworks have also emerged for CSP~\cite{jiao2023crystal, millerflowmm,zeni2025generative}. These methods rely on composition or partial structural constraints to guide generation, which requires domain knowledge and can limit direct incorporation of experimental data. While they show promise in producing stable crystal configurations, the gap between purely computational crystal structures and experimental data remains a challenge. The recent diffusion-based framework MatterGen~\cite{zeni2025generative} addresses some of these challenges by enabling conditioning on a broad range of property constraints, and it can generate structures that are more likely to be synthesizable than previous methods; nevertheless, conditioning on PXRD has not been demonstrated in its current implementation, leaving room for further exploration in integrating experimental data more directly.