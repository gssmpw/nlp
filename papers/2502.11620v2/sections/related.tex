Numerous approaches have been proposed for LLM evaluation in NLG. One class of methods involves training a separate model specifically for evaluation~\cite{DBLP:journals/corr/abs-2306-05087,DBLP:journals/corr/abs-2308-04592}. However, these evaluation models are typically limited in scale, remain static after training, and lack the flexibility to adapt to evolving LLM capabilities, shifting output distributions, and varying task requirements. Another direction relies on external proprietary LLMs, such as ChatGPT, as evaluators~\cite{DBLP:journals/corr/abs-2303-04048,DBLP:conf/eamt/KocmiF23}. However, subsequent studies have revealed issues with LLM-based evaluation, particularly related to biases~\cite{DBLP:conf/acl/WangLCCZLCKLLS24}.
In this paper, we focus on LLM evaluation without external oracles and therefore do not explore these approaches.

On the other hand, several approaches have been proposed to evaluate the quality of LLM responses for NLG without relying on external oracles. Among these, some methods leverage self-assessment by the model~\cite{DBLP:conf/icml/0001TDM24,DBLP:conf/acl/Xia0WCZ24,DBLP:journals/tmlr/LinHE22,DBLP:journals/corr/abs-2207-05221}, while others utilize internal model signals---such as the log-probabilities of generated tokens---to estimate uncertainty in responses~\cite{abbasi2024believe,farquhar2024detecting,kuhnsemantic,DBLP:conf/iclr/MalininG21}. A work closely related to Kuhn~\etal~\cite{kuhnsemantic} is by Kadavnath~\etal~\cite{DBLP:journals/corr/abs-2207-05221}, which offloads uncertainty computation to a specifically trained model designed to predict entropy. However, this approach is computationally intensive and highly dependent on the quality of the training samples.
In contrast, Jiang~\etal~\cite{jiang2021can} use the log-likelihood of responses as a straightforward proxy for correctness, though they demonstrate the limited effectiveness of this metric.

%The area of uncertainty estimation for LLMs is relatively well-studied; however, existing techniques are primarily designed for natural language generation. While this work extensively discusses the methods proposed by Kuhn~\etal~\cite{kuhnsemantic} and Abbasi~\etal~\cite{abbasi2024believe}, there are notable precursors to these techniques.

%Malinin~\etal~\cite{DBLP:conf/iclr/MalininG21} tackle the uncertainty problem using an \emph{ensemble} framework, combining multiple models to measure the level of agreement or disagreement in their predictions.

%Fomicheva~\etal~\cite{fomicheva2020unsupervised} take an unsupervised approach to uncertainty estimation, similar to our work, but rely on lexical similarity due to their focus on the domain of neural machine translation, an approximation that would be unsuitable for our domain of code generation. 

While all the aforementioned works focus on natural language, most code generation studies rely on external oracles to assess the quality of LLM-generated code.
For example, works such as \cite{liu2024your} and \cite{vaithilingam2022expectation} conduct large-scale evaluations of LLM-generated code against existing correctness oracles. Similarly, Honarvar~\etal~\cite{honarvar2024turbulencesystematicallyautomaticallytesting} introduce a test suite expansion technique to generate additional code generation problems with oracles, enabling a more comprehensive evaluation of LLM performance.

Another closely related area is hallucination detection, where studies such as \cite{liu2024exploring} and \cite{eghbali2024hallucinator} focus on evaluating LLMs using problems specifically designed to expose inconsistencies and hallucinations.
Liu et al.\cite{liu2024exploring} establish a taxonomy of hallucinations in LLM-generated code and propose a benchmark to evaluate the ability of code LLMs to recognize hallucinations. Eghbali and Pradel\cite{eghbali2024hallucinator} introduce a technique that grounds LLM predictions by retrieving relevant API references and iteratively refining prompts with increasingly relevant context, leading to more reliable LLM-generated code.

In contrast to these works, to the best of our knowledge, we are the first to explore techniques for estimating LLM uncertainty in code generation, and use it as a proxy for correctness without access to external oracles.

%While all of the aforementioned techniques work within the domain of LLM-based code generation quality, they focus on empirical evaluation schemes which are in turn heavily reliant on the quality of problems being used in the dataset. 
%Our work, to the best of our knowledge, provides a first attempt at using theoretically grounded techniques for estimating uncertainty and hence the quality of LLM-generated code.
