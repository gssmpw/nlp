% \AS{TODO:Redo this section, their is a sizeable gap between the theory of the paper and its practical implementation. Make sure that these gaps are explained in this section.
% Algorithm 2 and 3 of the paper work quite differently, so ensure that the explanation matches the implementation, where needed.}
This section presents an adaptation of the mutual information-based approach for quantifying epistemic uncertainty by Abbasi~\etal~\cite{abbasi2024believe} to the domain of code generation.
We follow the steps from the original work: iterative prompting for generating LLM responses, clustering, and mutual information estimation. 
However, similar to Section~\ref{sec:symex}, we diverge with respect to the methodology for clustering responses and the way we estimate the distribution of generated LLM responses.

\subsubsection{Iterative Prompting for Code Generation}
Iterative prompting is used for generating multiple responses from the LLM and consequently in constructing a pseudo joint distribution of outputs. 

% Beginning with the original prompt \(F_0(x) = x\) for \(i = 1, 2, \ldots, n\) we get a \emph{family of prompts} where the \(i\)-th response prompt in the family would be:
% \[
%   F_i(x, s_1, \ldots, s_i) = \text{``Original prompt: } x \text{. Previous responses: } s_1, \ldots, s_i \text{."}
% \]
% where $s_i$ is the \(i\)-th response from the LLM. 

More precisely, the LLM is sampled to produce $n$ responses while also getting their respective probabilities, $\mu(X_j)$ for \(j = 1, 2, \ldots, n\).
These responses are first used to construct iterative prompts by appending the response to the original prompt and asking the LLM to produce more responses. 
This step then makes use of softmax-style normalization to obtain values that can then be treated as probabilities which are used in the subsequent steps.  

As explained in Section~\ref{sec:probcomp} and Section~\ref{sec:symex}, this process can lead to numerical underflows. 
To mitigate this, we use length-normalization.
As opposed to the approach in Section~\ref{sec:symex}, here we did not use the uniform distribution approximation, as the actual LLM-reported probabilities are needed to distinguish between aleatoric and epistemic uncertainties.
%

Following length normalization, we compute conditional probabilities,  $\mu(X_m|X_n)$ for \(m,n = 1, 2, \ldots, n\), by looking at the response probabilities received from the LLM when subjected to the aforementioned iterative prompts.


% \CD{Are these $X_i$ rather than $s_i$ or that's just for clusters? Also, where do we get the probabilities $\mu(X_i)$ and $\mu(X_j \mid X_i)$ that are used below?}

\subsubsection{Clustering via Symbolic Execution}
To handle functional diversity, the generated program snippets are clustered based on their semantic equivalence using Algorithm~\ref{alg:clustering}. 
%symbolic execution. 
%Symbolic execution analyses each program to compute execution paths and constraints. 
%Two programs \(s^{(i)}\) and \(s^{(j)}\) are considered functionally equivalent if their symbolic execution traces \(T(s^{(i)})\) and \(T(s^{(j)})\) satisfy:
%\[
%T(s^{(i)}) \equiv T(s^{(j)}) \quad \text{or} \quad T(s^{(i)}) \subseteq T(s^{(j)}).
%\]

% Let \(C = \{c_1, c_2, \ldots, c_k\}\) denote the clusters formed, where each cluster \(c_j\) groups semantically equivalent programs. 
%The clustering procedure ensures that semantically redundant responses are grouped together, focusing on functional equivalence rather than syntactic similarity.
%The algorithm used is same as Algorithm~\ref{alg:clustering} from the previous section.

\subsubsection{Mutual Information Estimation}
% Once clustering is complete, mutual information is computed over the resulting clusters to quantify epistemic uncertainty. 
% The pseudo joint distribution is defined as:
% \[
% \tilde{p}(s_1, \ldots, s_n \mid x) = p(s_1 \mid F_0(x)) \prod_{i=2}^n p(s_i \mid F_{i-1}(x, s_1, \ldots, s_{i-1})).
% \]
% The marginal distribution is then:
% \[
% \tilde{p}^\otimes(s_1, \ldots, s_n) = \prod_{i=1}^n p(s_i \mid F_0(x)).
% \]

% The mutual information is then computed as:
% \[
% I(\tilde{p}) = D_{KL}(\tilde{p} \| \tilde{p}^\otimes).
% \]

Once clustering is complete, mutual information is computed over the resulting clusters to quantify epistemic uncertainty. 

The aggregated probabilities are defined as:
\[
\mu_1'(X_i) = \sum_{j \in D(i)} \mu(X_j), \quad
\mu_2'(X_t \mid X_i) = \sum_{j \in D(t)} \mu(X_j \mid X_i),
\]
where \( X_i \) and \( X_t \) are clusters, \( \mu(X_j) \) represents the probability of the output \( X_j \), and \( \mu(X_j \mid X_i) \) is the conditional probability of \( X_j \) given \( X_i \). The set \( D(i) \) contains all outputs assigned to the cluster \( X_i \).

The normalized empirical distributions are:
\[
\hat{\mu}_1(X_i) = \frac{\mu_1'(X_i)}{Z}, \quad \text{where} \quad Z = \sum_{j \in S} \mu_1'(X_j),
\]
\[
\hat{\mu}_2(X_t \mid X_i) = \frac{\mu_2'(X_t \mid X_i)}{Z_i}, \quad \text{where} \quad Z_i = \sum_{j \in S} \mu_2'(X_j \mid X_i).
\]
Here, \( \hat{\mu}_1(X_i) \) is the normalized marginal distribution for cluster \( X_i \), and \( \hat{\mu}_2(X_t \mid X_i) \) is the normalized conditional distribution for \( X_t \) given \( X_i \). The terms \( Z \) and \( Z_i \) are normalization constants to ensure that the distributions sum to 1.

The joint and pseudo-joint distributions are defined as:
\[
\hat{\mu}(X_i, X_t) = \hat{\mu}_1(X_i) \hat{\mu}_2(X_t \mid X_i), \quad
\hat{\mu}^\otimes(X_i, X_t) = \hat{\mu}_1(X_i) \sum_{j \in S} \hat{\mu}_1(X_j) \hat{\mu}_2(X_t \mid X_j).
\]
The joint distribution \( \hat{\mu}(X_i, X_t) \) combines the marginal and conditional distributions, while the pseudo-joint distribution \( \hat{\mu}^\otimes(X_i, X_t) \) assumes independence between clusters.

Finally, the mutual information is computed as:
\[
\hat{I}(\gamma_1, \gamma_2) = \sum_{i, t \in S} \hat{\mu}(X_i, X_t) \ln \left( \frac{\hat{\mu}(X_i, X_t) + \gamma_1}{\hat{\mu}^\otimes(X_i, X_t) + \gamma_2} \right).
\]
Here, \( \gamma_1 \) and \( \gamma_2 \) are small stabilization parameters to prevent division by zero, and \( S \) is the set of clusters.

% \subsubsection{Finite-Sample Estimation with Clusters}
% To estimate mutual information from the finite sample of clusters \(C = \{c_1, \ldots, c_k\}\), the probability of a cluster \(c\) is computed as:
% \[
% p(c \mid x) = \sum_{s \in c} p(s \mid x).
% \]
% The empirical mutual information is then:
% \[
% \hat{I}_k = \sum_{c \in C} \hat{p}(c \mid x) \ln \left( \frac{\hat{p}(c \mid x)}{\prod_{i=1}^n \hat{p}(c_i \mid x)} \right),
% \]
% where \(\hat{p}(c \mid x)\) is the observed cluster probability. 
% The empirical mutual information is then:
% \[
% \hat{I}_k(\gamma_1, \gamma_2) = \sum_{i, t \in S} p(X_i, X_t) \ln \left( \frac{p(X_i, X_t) + \gamma_1}{p^\otimes(X_i, X_t) + \gamma_2} \right),
% \]
% where $\hat{I}_k(\gamma_1, \gamma_2)$ is the estimated mutual information, $p(X_i, X_t)$ represents the joint empirical distribution over clusters $X_i$ and $X_t$, and $p^\otimes(X_i, X_t)$ denotes the product of their marginal probabilities for cluster pairs. 
% The stabilization parameters $\gamma_1$ and $\gamma_2$ are included to handle cases where $p(X_i, X_t)$ or $p^\otimes(X_i, X_t)$ might be zero, preventing undefined logarithmic terms. 
% This formulation allows for a robust estimation of mutual information by quantifying the dependencies between cluster distributions while addressing numerical instabilities.

% Entropy regularization is then applied for stability:
% \[
% \hat{I}_k(\gamma) = \sum_{c \in C} \hat{p}(c \mid x) \ln \left( \frac{\hat{p}(c \mid x) + \gamma}{\prod_{i=1}^n (\hat{p}(c_i \mid x) + \gamma)} \right).
% \]

This mutual information score serves as a proxy for epistemic uncertainty. 
High \(\hat{I}\) values signal significant uncertainty.
% \[
% a_\lambda(x) =
% \begin{cases} 
% 1 & \text{if } \hat{I}_k \geq \lambda, \\
% 0 & \text{otherwise.}
% \end{cases}
% \]

\paragraph{Motivating example revisited for \textnormal{\gptturbo}.}
We now illustrate the MI computation for our motivating example from Section~\ref{sec:motivating}. 
We use 3 samples with an iteration prompt length of 2.
All responses from \gptturbo fall in the same cluster and hence following the earlier formula for MI we get:

\begin{align*}
    \hat{I}(\gamma_1, \gamma_2) = \hat{\mu}(X_1, X_1) \ln \left( \frac{\hat{\mu}(X_1, X_1) + \gamma_1}{\hat{\mu}^\otimes(X_1, X_1) + \gamma_2} \right) 
    = 1.0 \ln \left( \frac{1.0 + \gamma_1}{1.0 + \gamma_2} \right).
\end{align*}

Since $\gamma_1$ and $\gamma_2$ are zero as per Abbasi~\etal~\cite{abbasi2024believe}:
%\begin{align*}
   $\ln \left( \frac{1.0 + \gamma_1}{1.0 + \gamma_2} \right) = \ln(1.0) = 0$.
%\end{align*}
Therefore, $\hat{I} = 1.0 \cdot 0 = 0$.
%\begin{align*}
%    \hat{I} &= 1.0 \cdot 0 = 0.
%\end{align*}
%
% EXPLANATION: Zero MI is expected as the paper says this "For the S.E. and M.I. methods, the responses for a large number of queries can be clustered into a single group, and therefore the semantic entropy and mutual information scores are zero."
%
A \emph{zero} MI indicates very low uncertainty in the model's responses, suggesting a high likelihood of correctness as we will show later in the paper.

\paragraph{Motivating example revisited for \textnormal{\salesforce}.}
For \codegenmonoC, all three responses fall in their own separate clusters \ie  $R_1 \in X_1$, $R_2 \in X_2$, $R_3 \in X_3$ and hence,
%
%\begin{align*}
    $P(X_1) = 0.9995, P(X_2) = 0.0004, P(X_3) = 0.0001$.
%\end{align*}

For brevity, we omit the computation of the marginals which was carried out using the same formula used by Abbasi~\etal~\cite{abbasi2024believe} which for this example looks like:

\[
\hat{I}(\gamma_1, \gamma_2)
= \sum_{i=1}^3 \sum_{j=1}^3
  \hat{\mu}(X_i, X_j)
  \ln\!\Biggl(\frac{\hat{\mu}(X_i, X_j) + \gamma_1}
                   {\hat{\mu}^\otimes(X_i, X_j) + \gamma_2}\Biggr).
\]

Computing the terms individually then looks like:

\[
\begin{aligned}
    (X_1, X_1) &: 0.800 \ln \left( \frac{0.800}{0.9990} \right) = -0.1788, & (X_1, X_2) &: 0.120 \ln \left( \frac{0.120}{0.0004} \right) = 0.6845, \\
    (X_1, X_3) &: 0.040 \ln \left( \frac{0.040}{0.0001} \right) = 0.2397, & (X_2, X_1) &: 0.050 \ln \left( \frac{0.050}{0.0004} \right) = 0.5050, \\
    (X_2, X_2) &: 0.010 \ln \left( \frac{0.010}{0.00000016} \right) = 0.1104, & (X_2, X_3) &: 0.020 \ln \left( \frac{0.020}{0.00000004} \right) = 0.2624, \\
    (X_3, X_1) &: 0.020 \ln \left( \frac{0.020}{0.0001} \right) = 0.1609, & (X_3, X_2) &: 0.020 \ln \left( \frac{0.020}{0.00000004} \right) = 0.2624, \\
    (X_3, X_3) &: 0.010 \ln \left( \frac{0.010}{0.00000001} \right) = 0.1382.
\end{aligned}
\]

Then total MI then is:
%\begin{align*}
    $\hat{I} = -0.1788 + 0.6845 + 0.2397 + 0.5050 + 0.1104 + 0.2624 + 0.1609 + 0.2624 + 0.1382 = 2.1847$.
%\end{align*}

%\begin{align*}
%    \hat{I} &= -0.1788 + 0.6845 + 0.2397 + 0.5050 + 0.1104 \\
%    &\quad + 0.2624 + 0.1609 + 0.2624 + 0.1382 = 2.1847.
%\end{align*}


While lower uncertainty is intuitively desirable, as discussed in Section~\ref{sec:symex}, the uncertainty estimate is assessed against an abstention threshold to derive meaningful conclusions (see Section~\ref{sec:usability}).

%This shows \emph{moderately} high uncertainty, due to all responses being in their own respective clusters. 

%By integrating symbolic execution-based clustering with iterative prompting and mutual information estimation, this methodology captures both functional diversity and epistemic uncertainty in program generation. 
