This section presents our adaptation of the semantic entropy-based approach by Kuhn~\etal~\cite{kuhnsemantic} for code generation. 
We follow the main steps from the original work while diverging in two key aspects: the way we estimate the distribution of generated LLM responses and the clustering methodology.

%This section presents our methodology for assessing semantic uncertainty in code generation by leveraging clustering based on symbolic execution. 

\subsubsection{Generation}
The first step involves sampling $M$ code snippets (using the same hyperparameters as Kuhn~\etal~\cite{kuhnsemantic}), $\{s^{(1)}, \ldots, s^{(M)}\}$, from the LLM's output distribution $p(s \mid x)$ for a given prompt $x$. 
%Given a code generation prompt, the model generates $M$ samples, $\{s^{(1)}, \ldots, s^{(M)}\}$, from its predictive distribution $p(s \mid x)$.
% Sampling is carried out using multinomial techniques, with hyperparameters such as temperature and nucleus sampling selected based on those used by Kuhn~\etal~\cite{kuhnsemantic}.
The probabilities of the collected samples are processed using a softmax-style normalization function, ensuring that the resulting values can be interpreted as a valid probability distribution.

As explained in Section~\ref{sec:probcomp}, this process can lead to numerical underflows. 
To mitigate this, we approximate the probability distribution of the LLM responses using either length-normalization or a uniform distribution.
%
Following this approximation, let \(\tilde{p}(s \mid x)\) denote the probability of a snippet \(s\) according to the adjusted distribution.

%softmax normalized
%Then, we have: 
 %\[
 %\tilde{p}(s \mid x) = \frac{p(s \mid x)}{|s|^\alpha},
 %\]
 %where \(|s|\) is the length of the snippet \(s\), and \(\alpha\) is a hyperparameter controlling the degree of normalization. 

%\CD{fix softmax normalization}

%This is similar to the approach taken by Kuhn~\etal~\cite{kuhnsemantic} and Abbasi~\etal~\cite{abbasi2024believe} and is a necessary step to prevent the probability computations from becoming invalid within the various formulae.
% \CD{Do we use these optional techniques?}

\subsubsection{Clustering via Symbolic Execution}
The second step works by grouping the aforementioned snippets into clusters based on semantic equivalence. %, determined through symbolic execution traces.
%To determine semantic equivalence, we employ symbolic execution, a program analysis technique that computes execution paths and constraints for a given snippet. 
%Two snippets $s^{(i)}$ and $s^{(j)}$ are deemed functionally equivalent if their symbolic execution traces are identical or exhibit subsumption.
%\CD{We need details on the semantic equivalence. This is, in principle, undecidable, so we need to explain a bit more on how this works.}
%
This process, as shown in Algorithm~\ref{alg:clustering} from Section~\ref{sec:symexclustering}, is based on symbolic execution. %ensures that clustering is driven by functional, rather than syntactic or lexical, similarities, aligning with the stricter requirements of code quality evaluation.
%It is important to note that two syntactically different responses can still end up in the same cluster if they are semantically equivalent.
% This is due to our adapted symbolic execution based clustering algorithm . 

\subsubsection{Entropy Estimation}
The final step computes uncertainty as the semantic entropy over clusters, reflecting the diversity of functional behaviors.

%Semantic entropy quantifies the uncertainty in functional behaviour by measuring the probability distribution over clusters.
%
First, the probability associated with a cluster \(c\) is calculated as:
\begin{equation*}
    \tilde{p}(c \mid x) = \sum_{s \in c} \tilde{p}(s \mid x),
    %p(c \mid x) = \sum_{s \in c} p(s \mid x),    
\end{equation*}
where \(s \in c\) indicates that the snippet \(s\) belongs to the cluster \(c\).
%
Then, the entropy \(H(C \mid x)\) over the set of clusters \(C\) is defined as:
\begin{equation*}
    H(C \mid x) = -\sum_{c \in C} \log \tilde{p}(c \mid x),
    %H(C \mid x) = -\sum_{c \in C} \log p(c \mid x),    
\end{equation*}
where \(C\) denotes all semantic clusters obtained from Algorithm~\ref{alg:clustering} in Section~\ref{sec:symexclustering}. 
%
%This formulation captures both the diversity and confidence of the model's outputs while accounting for the length-based normalization, offering a self-contained metric independent of external validation.
A higher entropy indicates greater semantic diversity and hence higher uncertainty in the functional behavior captured by the clusters. 
Conversely, a lower entropy suggests that the model's outputs are concentrated around a few semantically equivalent behaviors, reflecting higher confidence.

\paragraph{Motivating example revisited for \textnormal{\gptturbo}.} To illustrate our uncertainty computation, we'll go back to the motivating example from Section~\ref{sec:motivating}.

As discussed there, Figure~\ref{fig:good-llm-snippets} (Listings~\ref{lst:good1}, \ref{lst:good2}, and \ref{lst:good3}) contains code snippets generated by \gptturbo. % that are semantically equivalent. %, all three snippets exhibit the same behaviour.
%while Figure~\ref{fig:bad-llm-snippets} (Listings~\ref{lst:bad1}, \ref{lst:bad2}, and \ref{lst:bad3} from \salesforce/\codegenmonoC) contains code snippets that are \textbf{semantically distinct}, none of the snippets share the same functional behaviour.
We denote these snippets by $s^{(1)}, s^{(2)}, s^{(3)}$, and, according to Algorithm~\ref{alg:clustering} from Section~\ref{sec:symexclustering}, they are all grouped in the same functional cluster $c_1$ as they are semantically equivalent.
%Now for the \gptturbo case, we have three generated snippets (Listings~\ref{lst:good1}, \ref{lst:good2}, and \ref{lst:good3} from Figure~\ref{fig:good-llm-snippets}), denoted $s^{(1)}, s^{(2)}, s^{(3)}$, all found by using the Algorithm~\ref{alg:clustering} from \S\ref{sec:symexclustering}  to be in one functional cluster $c_1$.
In other words, $C = \{ c_1 \}$ with $c_1 = \{ s^{(1)}, s^{(2)}, s^{(3)} \}$.

%Given the very small probabilities reported by the LLM as shown in Section~\ref{sec:motivating} (which would cause underflows in our implementation), here we use length normalized log-probabilties according to the normalization formula in Section~\ref{sec:probcomp}.
Following softmax normalization, we obtain the following probabilities for the three snippets: %, where we use $\tilde{p}$ to denote the probability based on length-normalization: 
%For these snippets, following are the normalized probabilities based on the token-level \texttt{logprobs} data obtained from \gptturbo:
\(\tilde{p}(s^{(1)} \mid x) = \GPTsnipNormProbA,\; \tilde{p}(s^{(2)} \mid x) = \GPTsnipNormProbB,\;
\tilde{p}(s^{(3)} \mid x) = \GPTsnipNormProbC.\)
Since all responses belong to the single cluster $c_1$, its cluster probability is:
\[
   \tilde{p}(c_1 \mid x)
   \;=\;
   \sum_{s \in c_1} \tilde{p}(s \mid x)
   \;=\;
   \GPTsnipNormProbA \;+\; \GPTsnipNormProbB \;+\; \GPTsnipNormProbC
   \;=\;
   1.0
\]
Thus the distribution over clusters is \(\tilde{p}(c_1 \mid x) = 1,\) and the entropy of clusters is:
\[
   H(C \mid x)
   \;=\;
   -\sum_{c \in C} \log \tilde{p}(c \mid x)
   \;=\;
   -\log(1.0)
   \;=\;
   0.
\]
A \emph{zero} semantic entropy indicates high confidence in the model's response for this prompt.

\paragraph{Motivating example revisited for \textnormal{\salesforce}.} Let's now consider the three snippets $s^{(1)}, s^{(2)}, s^{(3)}$ from Figure~\ref{fig:bad-llm-snippets} generated by the \salesforce/\codegenmonoC model. 
Their respective probabilities are:
$\tilde{p}(s^{(1)}\!\mid x) = \SFsnipNormProbA,\;
 \tilde{p}(s^{(2)}\!\mid x) = \SFsnipNormProbB,\;
 \tilde{p}(s^{(3)}\!\mid x) = \SFsnipNormProbC$.
These snippets get categorized in three distinct semantic clusters
$C = \{ c_1, c_2, c_3 \}$, with $c_1 = \{ s^{(1)} \}$,
$c_2 = \{ s^{(2)} \}$, and $c_3 = \{ s^{(3)} \}.$
 Because each snippet resides in its own cluster, the cluster probabilities are:
   $\tilde{p}(c_1 \mid x) = \SFsnipNormProbA, \tilde{p}(c_2 \mid x) = \SFsnipNormProbB, \tilde{p}(c_3 \mid x) = \SFsnipNormProbC$.
%\[
%   \tilde{p}(c_1 \mid x) = \SFsnipNormProbA,\quad
%   \tilde{p}(c_2 \mid x) = \SFsnipNormProbB,\quad
%   \tilde{p}(c_3 \mid x) = \SFsnipNormProbC.
%\]
%
The entropy then is:
\[
\begin{aligned}
   H(C \mid x)
   &=
   -\!\sum_{c \in \{c_1,c_2,c_3\}}
   \log \tilde{p}(c \mid x)
   \\
   &=
   -\,\Bigl(
      \log(\SFsnipNormProbA)\;+\;\log(\SFsnipNormProbB)\;+\;\log(\SFsnipNormProbC) 
   \Bigr) \approx \SFSE.
\end{aligned}
\]
%Numerically, this is approximately \SFSE.
%A \emph{higher entropy} in this example indicates more disagreement or diversity in the model's functional outputs: the \salesforce/\codegenmonoC model produced three \textbf{distinctly incorrect} solutions, each forming its own cluster.


Intuitively, the uncertainty estimate reflects the strength of our belief in the LLM's prediction. 
Based on this, an \textit{abstention policy} can be implemented, whereby the system abstains from making a prediction if the entropy exceeds a predefined \emph{uncertainty threshold}. 
This approach minimizes the likelihood of committing to incorrect or suboptimal solutions. The abstention threshold is empirically determined by analyzing the entropy distribution. 
The methodology for computing this threshold will be detailed in Section~\ref{sec:eval}.

%Thus, these two scenarios exemplify how \emph{semantic clustering} and the corresponding \emph{entropy measure} can capture both the diversity (or uniformity) of model generations \emph{and} the model's confidence in those generations' functional behaviour.
