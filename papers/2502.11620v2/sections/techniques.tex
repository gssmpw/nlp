\section{Semantic Equivalence Based Program Clustering}
\label{sec:symexclustering}

The NLG techniques proposed by Kuhn~\etal~\cite{kuhnsemantic} and Abbasi~\etal~\cite{abbasi2024believe} rely on semantic clustering, where semantically equivalent programs are grouped together. 
Achieving this requires an effective method for assessing program equivalence. Kuhn~\etal employ the DeBERTa-large model~\cite{he2020deberta} for this task, while Abbasi~\etal determine equivalence using an F1 score based on token inclusion~\cite{DBLP:journals/corr/JoshiCWZ17}.

In the domain of code generation, program equivalence has a precise definition: two programs are considered equivalent if they produce identical behavior for all possible inputs. 
Consequently, a domain-specific equivalence check is required.
In this paper, we base the semantic equivalence check on \emph{symbolic execution}, where, instead of executing a program with concrete inputs, \emph{symbolic variables} are used to represent inputs, generating constraints that describe the program's behavior across all possible input values~\cite{symex_klee}.

The particular flavor of symbolic execution we use in this work is inspired by the lightweight \emph{peer architecture} described in Bruni~\etal~\cite{Bruni2011APA}. 
Unlike traditional approaches that require building a standalone symbolic interpreter, this architecture embeds the symbolic execution engine as a lightweight library operating alongside the target program. 
Their design is based on the insight that languages that provide the ability to dynamically dispatch primitive operations (\eg Python) allow symbolic values to behave as native values and be tracked at runtime.

Symbolic execution typically traverses the program's control flow graph, maintaining a symbolic state consisting of \emph{path constraints} (\ie logical conditions that must be satisfied for a given execution path to be feasible) and \emph{symbolic expressions} (\ie representations of program variables as functions of the symbolic inputs). 

\paragraph{Equivalence check.} Given two code snippets \(s^{(1)}\) and \(s^{(2)}\), we check semantic equivalence between them by comparing their symbolic traces. 
One such symbolic trace, \eg \(T(s^{(1)})\), consists of the corresponding path constraint and symbolic expressions denoting all the variables encountered on the corresponding execution path. 
Intuitively, for two code snippets to be semantically equivalent, all their corresponding symbolic traces must align. 
Specifically, for each path constraint, the traces produced by both snippets must be identical meaning that there is no concrete counterexample input for which the execution of the two snippets diverges.

%\CD{I'm not sure whether it's worth formalising this a bit.}

%Semantic equivalence between two code snippets \(s^{(i)}\) and \(s^{(j)}\) is determined by comparing their symbolic traces, \(T(s^{(i)})\) and \(T(s^{(j)})\), respectively:
%\begin{multline}
%    T(s^{(i)}) \equiv T(s^{(j)}) \iff \text{Path Constraints and Symbolic Expressions of } \\
%    s^{(i)} \text{ and } s^{(j)} \text{ are identical.}
%\end{multline}

Since program equivalence is undecidable in general, we perform a bounded equivalence check. 
This approach verifies that no counterexample input exists when exploring traces up to a given depth.

%Exact equivalence is often \emph{undecidable} due to the complexity of symbolic traces. 
%Instead, we employ \emph{subsumption}, where one trace subsumes another if all behaviours of the latter are captured by the former. 
%This allows us to approximate equivalence effectively.

%By using this lightweight symbolic execution approach, our clustering methodology emphasizes functional semantics, avoiding overfitting to syntactic similarities. 
%This methodology strikes a balance between precision and efficiency, leveraging the extensibility and simplicity of the peer architecture to scale across diverse programming scenarios.


\begin{algorithm}[ht!]
    \caption{Clustering with Symbolic Execution}
    \label{alg:clustering}
    \begin{algorithmic}[1]
    \Require Set of generated code snippets $\{s^{(1)}, \ldots, s^{(M)}\}$
    \Ensure Clusters of semantically equivalent snippets $C = \{c_1, c_2, \ldots, c_k\}$
    
    \State Initialize an empty cluster set $C \gets \emptyset$, and an equivalence map $E \gets \emptyset$ \label{alg:clustering:init}
    
    \For{each snippet $s^{(i)}$}
        \If{$s^{(i)}$ is invalid}
            \State $E[s^{(i)}] \gets \{\,s^{(i)}\}$ 
            \Comment{Assign invalid snippet to its own equivalence class}
        \EndIf
    \EndFor
    
    \For{each pair of valid snippets $(s^{(i)}, s^{(j)})$} \label{alg:clustering:pairwise}
        \State Perform symbolic execution on $s^{(i)}$ and $s^{(j)}$ to extract traces $T(s^{(i)})$ and $T(s^{(j)})$ \label{alg:clustering:trace}
        \If{$T(s^{(i)}) \equiv T(s^{(j)})$} \label{alg:clustering:check}
            \State $E[s^{(i)}] \gets E[s^{(i)}] \cup \{\,s^{(j)}\}$
            \State $E[s^{(j)}] \gets E[s^{(j)}] \cup \{\,s^{(i)}\}$ \label{alg:clustering:update}
        \EndIf
        \State \Comment{Enforce transitivity of equivalences}
        \If{$s^{(i)} \sim s^{(j)}$ and $s^{(j)} \sim s^{(k)}$ for some $s^{(k)}$}
            \State $E[s^{(i)}] \gets E[s^{(i)}] \cup \{\,s^{(k)}\}$
            \State $E[s^{(k)}] \gets E[s^{(k)}] \cup \{\,s^{(i)}\}$
        \EndIf
    \EndFor
    
    \State Identify equivalence classes in $E$ to form final clusters $C$ \label{alg:clustering:extract}
    
    \State \Return $C$ \label{alg:clustering:return}
    \end{algorithmic}
    \end{algorithm}

%\CD{we need to modify the alg so that it's obvious that we are talking about sets of traces at line 8.}

\paragraph{Semantic clustering.}
Algorithm~\ref{alg:clustering} illustrates how to cluster code snippets based on their functional semantics, with an additional check for invalid snippets. 
We first create empty structures for storing the final clusters ($C$) and an equivalence map ($E$) to track relationships (line~\ref{alg:clustering:init}). 

Next, in the \emph{invalid snippet handling phase}, each code snippet $s^{(i)}$ is examined and if it is detected to be invalid, it is immediately placed in its own equivalence class in $E$ and is thus isolated from further consideration. 

In the \emph{pairwise comparison phase} (line~\ref{alg:clustering:pairwise}), each pair of \emph{valid} snippets $(s^{(i)}, s^{(j)})$ is symbolically executed to produce traces $T(s^{(i)})$ and $T(s^{(j)})$ (line~\ref{alg:clustering:trace}). 
If the traces are equivalent (line~\ref{alg:clustering:check}), indicating identical functional behavior, both snippets are added to each other's equivalence classes (line~\ref{alg:clustering:update}). 
In reality, $T(s^{(i)})$ and $T(s^{(j)})$ actually denote sets of traces, and the equivalence check involves comparing individual traces from each set that share the same path constraint.
For brevity, in Algorithm~\ref{alg:clustering}, we represent this as $T(s^{(i)}) \equiv T(s^{(j)})$.
To maintain consistency, transitivity is enforced: if $s^{(i)}$ is equivalent to $s^{(j)}$, and $s^{(j)}$ is equivalent to $s^{(k)}$, then $s^{(i)}$ must also be in the same equivalence class as $s^{(k)}$. 

Finally, the equivalence map $E$ is processed to derive the clusters themselves (line~\ref{alg:clustering:extract}), and the resulting set of clusters is returned (line~\ref{alg:clustering:return}). 
% By isolating invalid snippets in single-item clusters, the algorithm cleanly separates non-functional or syntactically invalid code from semantically consistent groups. 
% This ensures that the final clustering reflects the functional semantics of valid snippets while transparently segregating invalid code. 

\section{Estimating the Probability Distribution of LLM Responses}
\label{sec:probcomp}

Both NLG techniques we adapt for code generation, at certain points, query the LLM, sample responses along with the log-probabilities of their tokens, and apply a softmax-style normalization to interpret them as a valid probability distribution.
However, since the LLM responses in this setting are programs, they are longer than the natural language responses used in the original studies---while the evaluation for Kuhn~\etal and Abbasi~\etal considered question-answer datasets typically involving one word answers, the programs produced in this work are around 200 tokens per response. 

The probability of a response is represented as the joint probability of its tokens, meaning that it decreases exponentially with length, often leading to \emph{numerical underflow}. 
This ultimately compromises the effectiveness of the technique. 
For instance, if the probabilities for all response programs underflow, then softmax returns NaN, which then propagates through the computation.

To address the issue of exponentially decaying probabilities, we propose two methods for approximating the probability distribution of LLM responses, as outlined below.

\paragraph{Length normalization.}

%The techniques we propose require computing the probability of responses generated by language models, where the probability of a response is represented as the joint probability of its tokens. 
%However, for longer responses, this probability decreases exponentially with length, which adversely impacts our estimation of uncertainty.

%In NLG tasks such as those addressed by prior works~\cite{kuhnsemantic,abbasi2024believe}, this issue is less pronounced because the goal in their problem domains (\eg TriviaQA) is to exactly match short reference answers. 
%In contrast, for code generation, the outputs are often longer. 
%While the evaluation for Kuhn~\etal~\cite{kuhnsemantic} and Abbasi~\etal~\cite{abbasi2024believe} considered question-answer datasets while typically involve one word answers, the program snippets produced in this work were typically around 200 tokens per response. 
% \CD{For our experiments, the average number of tokens in the results generated by the LLM is ...}. 
%Notably, while it is true that accuracy tends to decrease with length, existing research demonstrates that LLMs can generate high-quality code snippets, even up to 100 lines~\cite{codetranslation2}. %Consequently, the drastic reduction in probability with length disproportionately affects these scenarios. 

One solution is to use \emph{length normalization}~\cite{DBLP:conf/wmt/MurrayC18,DBLP:conf/aclnmt/KoehnK17}, more precisely length-normalizing the log probability of a program, a technique  used by other existing works, \eg to compute length-normalized predictive entropy~\cite{DBLP:conf/iclr/MalininG21}. %This also allows compar uncertainties of sequences of different length

More concretely, to compute a length-normalized probability from log probabilities, we begin by calculating the sum of the log probabilities. 
Let \(\ell_1, \ell_2, \dots, \ell_n\) denote the log probabilities associated with each token in the sequence that forms the response.
The log probability of the response is given by:
$S = \sum_{i=1}^{n} \ell_i$.
%
Next, the log probability is normalized by the sequence length \(L\) and the normalizing factor \(\gamma\). 
The normalized log probability is computed as:
$\ell_{\text{norm}} = \frac{S}{L \cdot \gamma}$.
%
Finally, the normalized probability \(P\) of the response is obtained by exponentiating the normalized log probability:
$P = e^{\ell_{\text{norm}}}$.

Intuitively, when using length-normalization in the context of uncertainty computation, probabilities remain comparable across responses of different lengths, whereas uncertainty is linked to the semantic differences between responses. %In our experimental evaluation, we compute uncertainty measures both with length-normalization and without.

\paragraph{Uniform distribution of LLM-generated responses.}
Intuitively, when multiple LLM responses are semantically equivalent, it indicates a higher degree of certainty in the (semantics of the) generated output.
To test this intuition, we propose disregarding the log-probabilities reported by the LLM and instead assuming a uniform distribution over the sampled responses. 
Specifically, if we sample $n$ responses, each response is assigned an equal probability of $1/n$.

Our experimental results demonstrated that using the semantic equivalence-based approach (described in Section~\ref{sec:symex}) in conjunction with this distribution reveals a negative correlation between the LLM's uncertainty and correctness---see Section~\ref{sec:results-discussion}. 
Furthermore, applying an uncertainty threshold derived from this technique to filter LLM responses—allowing only those above a specified correctness score (measured as the percentage of passed unit tests)—leads to high accuracy---see Section~\ref{sec:usability}.


%This approach ensures that the probabilities are appropriately scaled with respect to the sequence length.


%%For illustration, consider below as an example response produced by \gptturbo for the prompt used in Figure~\ref{fig:sampleproblem} from \S\ref{sec:motivating}:
%% % ``\texttt{Write a Python function that counts how many people older than 60 appear in a data list.}''

%% \begin{lstlisting}[language=Python]
%%     def candidate1(details):
%%      count = 0
%%         for detail in details:
%%             age = int(detail[11:13])
%%             if age > 60:
%%                 count += 1
%%         return count
%% \end{lstlisting}

%% For illustration, let us consider a code snippet generated by the LLM of $N=20$ tokens. ,  each with an individual token probability of $0.7$. 

%% \CD{Can we actually find out the number of tokens and log probabilities?}
%% When working with language models, each token in a generated completion has an associated log probability.  
%% 

%% Then if we simply sum the log probabilities of each token in a generated response as shown earlier:
%% \[
%%    \sum_{i=1}^{20} \log(0.7)
%%    \;=\;
%%    20 \,\log(0.7)
%%    \;\approx\;
%%    -7.1335,
%% \]
%% and exponentiate this sum yields:
%% \[
%%    \exp(-7.1335)
%%    \;\approx\;
%%    0.0008.
%% \]


%% Moreover, The probabilities of individual responses tend to decay exponentially with length, which can lead to disproportionately low values for valid but lengthy outputs.

%% If we merely sum these log probabilities over $N$ tokens,



%% Hence, although $0.7$ per token is fairly high, the \emph{total} probability from multiplying all 20 token probabilities becomes quite small ($0.0008$). 
%% A shorter completion, having fewer tokens, might end up with a larger total probability even if its average token confidence is slightly lower. 
%% This demonstrates how \emph{lengthier responses} can be unfairly penalized if we only sum or multiply all token probabilities, hence motivating \textbf{length-based normalisation}.


%% Length-normalization also helps when the responses generated for a query have different lengths.




% To address this, we perform a length-based normalisation to ensure that probabilities remain comparable across responses of different lengths. Specifically, for a generated snippet \(s\), its normalized probability is defined as:
% \begin{equation}
%     \tilde{p}(s \mid x) = \frac{p(s \mid x)}{|s|^\alpha},
% \end{equation}
% where \(|s|\) is the length of the snippet \(s\), and \(\alpha\) is a hyperparameter that controls the degree of normalisation. This adjustment prevents the probabilities of longer responses from dominating or vanishing entirely, ensuring a fair representation in subsequent entropy computations.

%% When working with language models, each token in a generated completion has an associated log probability.  
%% If we merely sum these log probabilities over $N$ tokens,
%% \[
%%    \text{sum\_logprob} \;=\; \sum_{i=1}^{N} \log \bigl(p(\mathrm{token}_i)\bigr),
%% \]
%% longer completions tend to accumulate more negative values simply due to having more tokens. 
%% This can make them appear less likely, even if each token is reasonably probable.

%% To address this, we perform a length-based normalisation to ensure that probabilities remain comparable across responses of different lengths. Specifically, for a generated snippet \(s\), its normalized probability is defined as:
%% \begin{equation}
%%     \tilde{p}(s \mid x) = \frac{p(s \mid x)}{|s|^\alpha},
%% \end{equation}
%% where \(|s|\) is the length of the snippet \(s\), and \(\alpha\) is a hyperparameter that controls the degree of normalisation. 
%% This adjustment prevents the probabilities of longer responses from dominating or vanishing entirely, ensuring a fair representation in subsequent entropy computations.

% To mitigate this effect, we use \emph{length-based normalisation}.
% Instead of summing the log probabilities, we compute the \textbf{average} log probability per token:
% \[
%    \text{avg\_logprob} \;=\; \frac{1}{N} \; \sum_{i=1}^{N} \log \bigl(p(\mathrm{token}_i)\bigr).
% \]
% We then exponentiate this average to obtain a \textbf{length-normalized probability}:
% \[
%    \text{length\_normalized\_prob} \;=\;
%    \exp\!\bigl(\text{avg\_logprob}\bigr).
% \]
% Although this value is not a ``true'' probability for the entire sequence, it is 
% a fairer score for comparing completions of different lengths, 
% because a longer completion is not automatically penalized by virtue of having more tokens.


%% Hence, instead of relying on the total product of probabilities, we use the 
%% \textbf{average log probability} per token:
%% \[
%%    \text{avg\_logprob} 
%%    \;=\;
%%    \frac{1}{N} 
%%    \sum_{i=1}^{N} \log\bigl(p(\mathrm{token}_i)\bigr)
%%    \;=\;
%%    \log(0.7),
%% \]
%% for $N=20$ tokens in this simplified scenario. 
%% Exponentiating that average log probability gives:
%% \[
%%    \exp\!\bigl(\text{avg\_logprob}\bigr)
%%    \;=\;
%%    \exp(\log(0.7))
%%    \;=\;
%%    0.7.
%% \]

%% Thus, while the raw product \(\prod_{i=1}^{20} p(\mathrm{token}_i)\) is about $0.0008$, the \emph{length-normalized} probability is $0.7$, reflecting the fairer notion that each token has a $70\%$ likelihood on average. This avoids unfairly penalizing longer responses merely due to having more tokens multiplied together. 
%% Length-based normalisation is therefore crucial for comparing or ranking completions of different lengths.

% A different completion with more lines of code and docstrings might sum to a lower total log probability (due to more tokens), but its average log probability could be similar (say, $0.95$). When using length-based normalisation, these two scores ($0.97$ vs.\ $0.95$) are directly comparable as per-token likelihoods, rather than an apples-to-oranges comparison of total log probabilities.


% If you want to interpret these normalized scores as a \emph{distribution} over completions, 
% you can renormalize across all candidates so that they sum to 1:
% \[
%   \hat{p}_i \;=\; 
%   \frac{\exp\!\bigl(\text{avg\_logprob}_i\bigr)}{\sum_{j=1}^{k} \exp\!\bigl(\text{avg\_logprob}_j\bigr)},
% \]
% where $k$ is the total number of candidate completions. 
% This transforms the length-normalized scores into valid probabilities for comparing or sampling.

\section{Semantic Uncertainty via Symbolic Clustering}
\label{sec:symex}
\input{sections/symex.tex}

\section{Mutual Information Estimation via Symbolic Clustering}
\label{sec:mi}
\input{sections/mi.tex}


