We explored the first techniques for estimating LLM uncertainty in code generation without relying on external oracles, using it as a proxy for correctness. By adapting entropy- and mutual information-based techniques from natural language generation, we demonstrated a strong negative correlation between uncertainty and correctness. 
Additionally, we proposed a simplified version of the entropy-based method that assumes a uniform probability distribution, achieving comparable results. 
Our abstention policy effectively prevents incorrect outputs, reducing the false positive rate to near zero. 
Experimental results on \livecodebench confirm the effectiveness of our approach, outperforming a baseline method that relies solely on log-probabilities. 
These findings suggest that uncertainty estimation can enhance the reliability of LLM-generated code and provide a foundation for further exploration in this area.

