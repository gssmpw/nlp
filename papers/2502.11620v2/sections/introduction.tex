% \AS{I am not sure yet how to number the refs properly, I thought it was automatically done?}\CD{they look good to me..}
Large Language Models (LLMs) have demonstrated remarkable capabilities in automated software development tasks such as code synthesis~\cite{synthesis1,synthesis2}, bug fixing~\cite{AutoCodeRoverPre,AutoCodeRover}, code refactoring~\cite{coderefactoring1,coderefactoring2}, and code translation~\cite{codetranslation1,codetranslation2,codetranslation3}. 
As the solutions generated by LLMs lack inherent correctness guarantees, correctness is typically assessed by running the code against \emph{external oracles}---test suites, reference solutions, or other expert specifications. 
While these oracle-based techniques can provide concrete evidence of functional correctness, such as passing unit tests, they are not always available. 
Even when present, they may be insufficiently precise, \eg unit tests may only cover a limited subset of the code's behavior. 
%Ideally, we need a reliable, intrinsic measure of the quality of the code generated by an LLM that does not rely on external oracles.


On the other hand, several approaches have been proposed to evaluate the quality of LLM responses for Natural Language Generation (NLG) without relying on external oracles. 
Among these, some methods leverage self-assessment by the model~\cite{DBLP:conf/icml/0001TDM24,DBLP:conf/acl/Xia0WCZ24,DBLP:journals/tmlr/LinHE22,DBLP:journals/corr/abs-2207-05221}, while others utilize internal model signals---such as the log-probabilities of generated tokens---to estimate uncertainty in responses~\cite{abbasi2024believe,farquhar2024detecting,kuhnsemantic,DBLP:conf/iclr/MalininG21}.

In this paper, we focus on two recent state-of-the-art techniques for uncertainty estimation in NLG with the goal of adapting them for code generation: one based on \emph{entropy}\cite{farquhar2024detecting,kuhnsemantic} and the other on \emph{mutual information}\cite{abbasi2024believe}. 
Both methods are grounded in the intuition that uncertainty is highest when the generated output is minimally informative.

In the first work, Kuhn~\etal~\cite{kuhnsemantic} estimate the uncertainty of a model's prediction as the entropy of the output distribution. 
Notably, in order to account for responses that may look syntactically different while having the same meaning, they introduce the notion of \emph{semantic entropy}.
This technique operates by grouping the generated responses into semantically equivalent clusters and estimating uncertainty across these clusters instead.

In the second work, Abbasi~\etal~\cite{abbasi2024believe} further distinguish between ``epistemic'' uncertainty (caused by incomplete or incorrect knowledge) and ``aleatoric'' uncertainty (resulting from inherent randomness, such as multiple valid answers). 
Intuitively, only epistemic uncertainty serves as a meaningful indicator of the correctness of the model's response.
To isolate epistemic uncertainty, they propose an iterative prompting technique in which the model is repeatedly queried, with its prior responses incorporated into the context. 
This method assesses the mutual information between successive responses to capture their dependencies: a high degree of dependency indicates elevated epistemic uncertainty, revealing the model's uncertainty about the ground truth. 
Similar to the work of Kuhn~\etal,Abbasi~\etal cluster responses into semantic equivalence classes, ensuring that the mutual information reflects semantic relationships.


%intuition that uncertainty tends to peak when the generated output is minimally correct.

%A solution comes from natural language generation (NLG), where researchers have explored various \emph{self-contained} measures to approximate the output's quality without relying on external verifiers.  In particular, given a query and a set of candidate responses returned by the model, these works are concerned with estimating the uncertainty in the model's responses. The underlying intuition is that uncertainty peaks when the generated output is minimally informative. %, making it a reasonable proxy for detecting hallucinations. While rigorous methods for uncertainty quantification remain highly challenging for deep neural networks~\cite{DBLP:conf/nips/AntoranAH20,DBLP:journals/fini/FerranteBT24}, existing research explores a variety of heuristic approaches~\cite{abbasi2024believe,farquhar2024detecting,kuhnsemantic,DBLP:journals/corr/abs-2207-05221,DBLP:journals/tmlr/DwaracherlaWOLAR23,DBLP:journals/corr/abs-2405-01563}. In this work, we focus on two very recent state-of-the-art techniques that make use of information-theoretic measures: one leveraging \emph{entropy}~\cite{farquhar2024detecting,kuhnsemantic}, and the other employing \emph{mutual information}~\cite{abbasi2024believe}. Importantly, these approaches are unsupervised, and require no additional training or modification of the existing models.


%These methods propose information-theoretic metrics to estimate the \emph{uncertainty in the model's response} by analysing the probability distribution over the model's responses.
% analysing relationships among generated responses or leveraging patterns within the outputs themselves.

%offers a novel approach to uncertainty estimation in natural language generation by focusing on the meaning of text rather than its exact lexical or syntactic form. 
%Traditional methods often measure uncertainty at the token or sequence level, which can overstate uncertainty when different forms convey the same meaning. 


%rather than superficial syntactic similarities.

%This mutual information serves as a principled metric to detect hallucinations and assess the model's reliability, particularly in scenarios where traditional measures may fail to distinguish between different types of uncertainty.

%As evident, both Kuhn \etal and Abbasi \etal leverage clustering techniques: the first clusters semantically equivalent responses to measure uncertainty over meanings, while the second aggregates responses to compute mutual information, capturing dependencies between successive outputs.

\paragraph{Our approach.} While these methods are not directly applicable to code generation due to the fundamental differences between natural language and code, we adapt them to this domain, exploring the use of symbolic reasoning to address its unique challenges.

A key aspect of this adaptation involves accounting for the distinct semantic properties of code. 
For example, clustering semantically equivalent code follows principles that differ significantly from those used in natural language. 
In code, semantic equivalence is precisely defined: two programs are considered equivalent if they exhibit identical behavior for all possible inputs. 
To ensure accurate evaluation, we base our semantic equivalence check on symbolic execution.

Another major challenge stems from the nature of the tasks being evaluated. 
While the original techniques were designed for assessing short natural language responses, code generation typically produces relatively longer snippets. 
This introduces additional complexities in computing response probabilities, as they decrease exponentially with length, often leading to \emph{numerical underflows}.

To address the issue of exponentially decaying probabilities in this work, we propose two methods for approximating the probability distribution of LLM responses. 
The first approach applies \emph{length-normalization}, adjusting for the bias against longer sequences. 
The second assumes a \emph{uniform distribution} of LLM responses (effectively disregarding the reported probability distribution), testing the assumption that semantic equivalence is a stronger indicator of correctness than the model's reported log-probabilities. 
Both methods demonstrated comparable effectiveness in our experimental evaluation (see Section~\ref{sec:eval}).

%The probability of a response is represented as the joint probability of its tokens, meaning that it decreases exponentially with length, often leading to \emph{numerical underflow}. This ultimately compromises the effectiveness of the technique. For instance, if the probabilities for all response programs underflow, then softmax returns NaN, which then propagates through the computation.

%distorting uncertainty estimates. Addressing these issues is crucial to effectively adapting uncertainty estimation techniques from natural language to code generation.

%\AS{TODO: Mention a bit the changes made to vanilla symex algo for this.}

%\CD{Probably we need more discussion on differences between NL and code.}
%\AS{I think this quote summarises one difference between code gen and NLG quite well}

%The following excerpt from Kuhn~\etal~\cite{kuhnsemantic} highlights another key distinction between the domains of NLG and code generation. 
%\begin{tcolorbox}[
%    colback=white, % Background color
%    colframe=white, % Frame color
%    boxrule=0pt, % No border
%    enhanced jigsaw, % Smooth edges
%    sharp corners, 
%    breakable,
%    left=10pt, right=10pt, top=5pt, bottom=5pt,
%    before skip=10pt, after skip=10pt
%  ]
%  \textbf{\textquotedblleft}
%  \textit{We therefore interpret length-normalising the log-probabilities when estimating
%    the entropy as asserting that the expected uncertainty of generations is
%    independent of sentence length. Sometimes, this is approximately valid. Other
%    times, longer sentences may well be usually more uncertain (e.g., when the
%    goal is to exactly match a typically short reference answer)}
%  \textbf{\textquotedblright}
%\end{tcolorbox}

%While this is reasonable assumption that allows the authors to work around longer responses in their domain of NLG, it is not quite applicable to our domain of code generation. 
%Long responses are not only common within this domain but might also be the more correct ones (consider cases where the problem specification demands heavy error checking).
%Hence, this requires special treatment and adaptations as we highlight in the remiander of this work. 

%% Bringing these two strands of research together reveals two key gaps:
%% \begin{itemize}[leftmargin=*] 
%%     \item Adapting NLG-based measures for code generation tasks requires significant modification, given the more rigid syntactic and semantic requirements of programmatic text.

%%     \item While the NLG-based measures are designed to evaluate semantic coherence, code generation introduces a unique challenge: syntactically different programs may exhibit identical semantics, complicating the alignment of measures with functional correctness. 
%%           Consequently, there is little guidance on \emph{which} of the \emph{adapted} measures, if any, offers the best performance or how they might be combined for evaluating LLM-generated code without external oracles.
%% \end{itemize}

%% \begin{itemize}[leftmargin=*] 

%%     \item \textbf{Mutual Information (MI) Analysis.} Following the direction of \cite{abbasi2024believe}, we adapt the mutual information-based methodology to quantify uncertainty for code generation tasks. 
%%             By leveraging iterative prompting and an information-theoretic metric, this approach aims to detect code hallucinations and quantify code uncertainty. 
%%             However, as our findings reveal, the assumptions that make MI effective for NLG---such as leveraging semantic diversity---can conflict with the constraints of correctness in code generation.
    
%%     \item \textbf{Semantic Uncertainty via Symbolic Clustering.} Building on \cite{farquhar2024detecting,kuhnsemantic}, we cluster multiple code snippets generated by an LLM based on functional behaviour (\eg using symbolic execution traces or similar neuro-symbolic representations), rather than just textual similarity. 
%%             This allows us to differentiate solutions that are textually dissimilar yet semantically equivalent or vice versa \emph{without} relying on external oracles. 
%%             Interestingly, SE's straightforward aggregation of uncertainty aligns more closely with the binary nature of correctness in code generation, offering unexpected advantages over MI in this domain.
    
%% \end{itemize}

%\CD{The contributions can be a bit clearer. There is some repetition in the rest of the section.}

%operating under the assumption that higher confidence from the LLM corresponds to higher quality generated code. Our goal is to evaluate the validity of this assumption for each technique and compare their performance against a baseline approach that uses the model's reported log-probabilities as an indicator of response correctness.

%To address these gaps, in this work we make the following contributions:
\paragraph{Contributions.}
\begin{itemize}[leftmargin=*]
    \item \emph{To the best of our knowledge, we are the first to explore techniques for estimating LLM uncertainty in code generation and leveraging it as a proxy for correctness without relying on external oracles}. To achieve this, we use symbolic reasoning to adapt two techniques from NLG—one based on entropy and the other on mutual information—and demonstrate a \emph{weak negative correlation between the computed uncertainty scores and correctness}.
    \item Building on the intuition that semantic equivalence is a stronger indicator of the model's (un)certainty than its reported log-probabilities, we propose a \emph{simplified version of the entropy-based technique} assuming a uniform distribution over LLM responses. 
        We show that this approach produces results comparable to the adaptation of the original entropy-based technique.

%Building on the intuition that semantic equivalence among LLM responses is a more reliable indicator of the model's (un)certainty than its reported log-probabilities, we propose a \emph{simplified entropy-based technique} that assumes a uniform distribution over the LLM's responses, effectively disregarding the reported probability distribution. We demonstrate a negative correlation between the thus computed uncertainty scores and correctness.
    %We demonstrate that this approach produces results comparable to the adapted version of the original entropy-based technique.

%Building on the intuition that the set of semantic meanings corresponding to the LLM's responses is more indicative of the LLM's uncertainty than the actual reported log-probabilities, we propose a simplified version of the entropy-based technique that assumes a uniform distribution over the LLM’s responses, effectively discarding the probability distribution reported by the model. We demonstrate that this approach produces results comparable to the adapted version of the original entropy-based technique.

    \item We show that the correlation between uncertainty (as computed by the proposed techniques) and correctness remains stable across different problem complexities and sizes.      

    \item We leverage the proposed techniques to define an \emph{abstention policy}, where the model should refrain from generating a response if the estimated correctness of the response falls below a predefined threshold. 
    Our results show that, under this policy, the LLM achieves a false positive rate of just 0.02\%, effectively ensuring that it never outputs responses below the correctness threshold.


%We adapt two NLG techniques for assessing LLM uncertainty to the domain of code generation and demonstrate a correlation between the computed uncertainty scores and correctness. 
 %  \item We compare the adapted techniques to their original, unmodified NLG counterparts, emphasizing the need to tailor them specifically for code generation.
 %  \item We show that these techniques outperform a baseline approach that relies on the model's reported log-probabilities to assess response correctness. 
%\item We propose a methodology for establishing an uncertainty threshold to be integrated into an LLM abstention policy, whereby the model abstains from making a prediction if the
%uncertainty exceeds the threshold.
\end{itemize}

\paragraph{Roadmap.}

Following a motivating example in Section~\ref{sec:motivating}, we outline the key challenges in computing uncertainty measures for code generation, namely semantic equivalence-based clustering (Section~\ref{sec:symexclustering}) and the probability computation for code snippets (Section~\ref{sec:probcomp}). 
We then present our adaptations of the uncertainty estimation techniques proposed in~\cite{kuhnsemantic} and~\cite{abbasi2024believe} for code generation, as detailed in Sections~\ref{sec:symex} and~\ref{sec:mi}, respectively. 
This is followed by an experimental evaluation in Section~\ref{sec:eval}, a discussion of related work in Section~\ref{sec:related}, and our conclusions in Section~\ref{sec:conclusion}.


% Taken together, these contributions underscore our central idea: it is possible to gauge the quality of LLM-generated code \emph{without} relying on external oracles. 
% Rather than presenting yet another benchmark of LLM performance, we \emph{focus on assessing the proxies themselves}, examining how well they correlate with overall code quality when external verification is either unavailable or too costly. 
% In doing so, we aim to equip researchers and practitioners with reliable, interpretable metrics for evaluating LLM-generated code, particularly in specialized or time-constrained environments where traditional test suites are out of reach.

%Taken together, these contributions establish a comprehensive framework for adapting and evaluating NLG-inspired correctness measures in the domain of code generation. By addressing the unique challenges posed by the stricter semantic and functional constraints of code, we provide a systematic approach to quantify the quality of LLM-generated code. We not only illustrate the necessity of adapting existing methods but also reveal the domain-specific dynamics that lead to a reversal in the performance hierarchy of MI and SE-based methods. These findings highlight the importance of understanding the underlying assumptions of evaluation frameworks and adapting them thoughtfully for new domains, equipping researchers and practitioners with actionable insights for assessing LLM performance in scenarios where traditional validation techniques are unavailable or impractical. 

% \CD{Is the following really true? I thought that, in the NLG paper, high semantic diversity generally is correlated with high uncertainty and low correctness. If it is true, we need a citation. My impression was that when going from NLG to code, the main difference consists in the way we measure equivalence, which is very much binary for code.}

% In natural language generation (NLG) tasks, \emph{semantic diversity} often aligns with correctness because multiple distinct responses may each validly address an open-ended query. 
% However, these assumptions do not directly transfer to code generation. Unlike typical NLG outputs, where diverse expressions can all be correct, code is subject to stricter logical and functional requirements, creating a largely \emph{binary} notion of correctness. 
% Different syntactic implementations (\eg iterative vs.\ recursive) may still produce the same valid functionality, but genuinely different solutions (such as computing a power instead of a factorial) are inherently incorrect. 
% This single-ground-truth nature of most programming tasks highlights the critical role of \emph{semantic correctness} in code, where even slight deviations in logic can render a solution invalid.


% To that end, in this work we \emph{adapt} multiple NLG-inspired methods originally designed for language-based tasks and tailor them specifically to the domain of code generation. 
% A central theme is to render these techniques sensitive to a program's \emph{semantics} and \emph{functionality} rather than merely its textual features. 
% We then evaluate these adaptations against both their NLG analogues and each other, highlighting a surprising and intriguing reversal: while MI-based methods outperform SE-based methods in NLG tasks, the opposite holds true for code generation. 
% This reversal underscores the critical importance of domain-specific considerations and offers a cautionary tale about directly transplanting evaluation frameworks from one domain to another without adaptation.

