In this section, we discuss the two works on estimating uncertainty in NLG, which we will adapt to code generation in Section~\ref{sec:tech}.
%We now describe the foundational techniques upon which this work builds. \S\ref{sec:background_SE} discusses the method presented in~\cite{kuhnsemantic} and its focus on semantic uncertainty for natural language generation while \S\ref{sec:background_MI} details the mutual information-based approach from~\cite{abbasi2024believe}, which quantifies epistemic uncertainty through iterative prompting. These techniques serve as the basis for our adaptations to the domain of code generation, addressing the unique challenges posed by its strict semantic and functional requirements.

%Finally in \S\ref{sec:background_SymEx} we briefly describe the idea of symbolic execution as it pertains to the domain of this work. 

\subsection{Semantic Uncertainty in NLG}
\label{sec:background_SE}

The work of Kuhn~\etal~\cite{kuhnsemantic} addresses a critical challenge in estimating uncertainty for NLG: how to account for the semantic equivalence of outputs. 
%Unlike traditional tasks such as image classification, where outputs are mutually exclusive (e.g., an image is either a cat or a dog, but not both), NLG outputs can take multiple forms while conveying the same underlying meaning. 
%For instance, the sentences \emph{"France's capital is Paris"} and \emph{"Paris is the capital of France"} differ lexically but are semantically identical. 
Conventional uncertainty estimation methods, which rely on token-level probabilities, fail to capture these linguistic invariances. 
Consequently, these methods can overestimate uncertainty when the model is confident about a meaning but uncertain about its exact wording.

To address this limitation, Kuhn~\etal~introduce \emph{semantic entropy}, a method that shifts the focus from token-level outputs to equivalence classes of semantically identical sentences. 
Their approach involves three main steps:

%\begin{enumerate}[leftmargin=*]
    \noindent\textbf{1. Generation:} Given the model's output distribution, $p(s \mid x)$, for a given input context $x$,  Kuhn~\etal sample multiple sequences, $\{s^{(1)}, \ldots, s^{(M)}\}$. 
            These sequences represent possible responses of the model for the given input. 
            %Sampling reflects the uncertainty inherent in the model's prediction process and captures the variability in how the model may express a particular concept.


    \noindent\textbf{2. Clustering:} The generated sequences are grouped into \emph{semantic equivalence classes}, $C = \{c_1, c_2, \ldots\}$, using a bidirectional entailment algorithm. 
            Two sequences, $s$ and $s'$, are considered semantically equivalent if and only if each entails the other within the context:
    %
    $E(s, s') \iff \text{entailment}(s \rightarrow s') \land \text{entailment}(s' \rightarrow s)$.
    %
    %This ensures that only sequences sharing the same underlying meaning are grouped together, regardless of differences in word choice or syntactic structure. 
    The entailment relation is determined by verifying whether the meaning of one sequence logically implies the other. 
    For instance, “France's capital is Paris” and “Paris is the capital of France” both convey the same information, and their entailment holds in both directions. 
    In particular, their work makes use of the DeBERTa-large model~\cite{he2020deberta} which allows for detecting such bi-directional entailments.  

    
    \noindent\textbf{3. Entropy Computation:} Once clustered, the method computes the semantic entropy over the equivalence classes. 
    %This is defined as:
    %\[
    %\text{SE}(x) = -\sum_{c \in C} \log p(c \mid x),
    %\]
    %where the probability of an equivalence class, $p(c \mid x)$, is calculated by summing the probabilities of all sequences in the class:
    %\[
    %p(c \mid x) = \sum_{s \in c} p(s \mid x).
    %\]
    The entropy captures the model's uncertainty over possible meanings rather than individual token sequences. 
    When most of the model's outputs cluster into a single equivalence class, the entropy is low, reflecting high confidence in the responses. 
    Conversely, if the outputs are distributed across multiple equivalence classes, the entropy is higher, indicating greater uncertainty.

Kuhn~\etal~validate their approach on question-answering datasets such as TriviaQA~\cite{triviaqa} and CoQA~\cite{coqa}, demonstrating its superior performance compared to baselines like predictive entropy and lexical similarity. 
%Notably, semantic entropy achieves higher predictive accuracy, as measured by the area under the receiver operating characteristic curve (AUROC), and scales effectively with model size and the number of sampled outputs. 
%Furthermore, the method is unsupervised and requires no fine-tuning or architectural modifications, making it compatible with existing pre-trained models \emph{off-the-shelf}.

% This work underscores the importance of addressing semantic invariances in NLG tasks and establishes semantic entropy as a reliable and practical tool for uncertainty estimation in free-form text generation.

\subsection{Mutual Information Based Uncertainty} %Quantification and Hallucination Detection}
\label{sec:background_MI}

\CD{Add a sentence mentioning where semantic clustering is used}

Abbasi~\etal~\cite{abbasi2024believe} address the challange of distinguishing between \emph{epistemic uncertainty} and \emph{aleatoric uncertainty}.
Epistemic uncertainty arises from a model's lack of knowledge about the task, typically due to insufficient or incomplete training data. 
            High epistemic uncertainty often corresponds to hallucinations—outputs that deviate significantly from the ground truth.
 Conversely, aleatoric uncertainty stems from inherent ambiguity or variability in the task itself, such as the existence of multiple equally valid outputs. 
            Unlike epistemic uncertainty, aleatoric uncertainty cannot be reduced with additional data.

In order to isolate epistemic uncertainty, the authors propose a novel framework leveraging \emph{mutual information} (MI), which measures dependencies between multiple generated responses to a single query. 
%Their approach is particularly well-suited for detecting hallucinations in LLM outputs, as it focuses on isolating epistemic uncertainty while being robust to aleatoric uncertainty.
%
%Central to their framework is the concept of \emph{iterative prompting}, a method for constructing a \emph{pseudo joint distribution} over multiple responses by sequentially querying the model. 
%This distribution models the dependencies between successive responses, enabling the disentanglement of different types of uncertainty. 
The process proceeds in three key steps:

%\begin{enumerate}[leftmargin=*]
    \noindent\textbf{1. Initial Prompting:} For a given query \(x\), the model generates an initial response \(Y_1\) using a base prompt \(F_0(x)\). %This response serves as the starting point for subsequent iterations and provides the first piece of information about how the model interprets the query.

    \noindent\textbf{2. Iterative Prompting:} Additional responses \(Y_2, Y_3, \ldots, Y_n\) are generated by incorporating all previous responses into the prompt. For illustration, the \(t\)-th prompt explicitly conditions the next response on both the query and all prior responses:
    \begin{multline}
        F_t(x, Y_1, \ldots, Y_{t-1}) = \text{``What is the answer to [query]} \\
        \text{given that previous responses are [responses]?''}
        \label{eq:iterative_prompt}
    \end{multline}
    This approach introduces dependencies among responses, meaning each successive response builds on earlier ones. 
    These dependencies allow the construction of a \emph{pseudo joint distribution}, defined as:
    \[
    \tilde{\mu}(Y_1, \ldots, Y_n \mid x) = \prod_{t=1}^n \mu(Y_t \mid F_{t-1}(x, Y_1, \ldots, Y_{t-1})).
    \]
    where and $\mu(Y_t \mid F_{t-1}(x, Y_1, \ldots, Y_{t-1}))$ is the conditional probability of $Y_t$ given $F_{t-1}(x, Y_1, \ldots, Y_{t-1})$.
    
    Unlike a true joint distribution, which reflects the actual probabilities of all responses occurring together, the pseudo joint distribution is constructed iteratively by combining conditional probabilities at each step. 
    This approximation is sufficient for capturing the dependencies required to measure epistemic uncertainty.

    \noindent\textbf{3. Mutual Information Estimation:} To quantify epistemic uncertainty, the MI of the pseudo joint distribution is computed. MI measures how much information is shared between successive responses in the pseudo joint distribution.
    %, and it is defined as:
    %\[
    %I(\tilde{\mu}) = D_{\text{KL}}(\tilde{\mu} \parallel \tilde{\mu}^\otimes),
    %\]
    %where \(\tilde{\mu}^\otimes\) represents the product of marginals derived from \(\tilde{\mu}\). 
    %The term \(D_{\text{KL}}\), known as the Kullback-Leibler divergence, quantifies the difference between two probability distributions. 
    %Specifically, it measures how much the pseudo joint distribution \(\tilde{\mu}\) deviates from the assumption of independence (\ie \(\tilde{\mu}^\otimes\)).

    %In this context, \(D_{\text{KL}}\) is used to detect dependencies. 
    %If \(\tilde{\mu}\) equals \(\tilde{\mu}^\otimes\), the responses are independent, implying no epistemic uncertainty. 
    %A higher value of \(D_{\text{KL}}\), and implicitly of MI, indicates stronger dependencies, signalling that the model's lack of knowledge about the query is influencing the responses.

    %However, Abbasi~\etal~\cite{abbasi2024believe} compute a lower bound for  \(D_{\text{KL}}\) which is then used as a proxy throughout their evaluation.
    %Details for why that is a reasonable approximation can be found in their paper.

    %Mutual information captures how much uncertainty in later responses is explained by earlier ones. 
    %If the responses are highly dependent (indicating the model is uncertain and guessing based on prior answers), the MI is high. 
    %Conversely, if the responses are independent (indicating the model is confident), the MI is low.
%\end{enumerate}

The authors validate their method on datasets such as TriviaQA~\cite{triviaqa} and AmbigQA~\cite{ambigqa}, demonstrating that MI outperforms semantic entropy (SE) in tasks with mixed single- and multi-label responses. 
%While SE conflates epistemic and aleatoric uncertainties, MI provides a clearer signal, particularly for hallucination detection. Notably, MI also scales effectively with model size, reflecting improvements in model knowledge while retaining sensitivity to hallucinations.

%This work underscores the importance of disentangling uncertainty types in LLM evaluation. 
%By leveraging iterative prompting and MI, Abbasi~\etal present a principled framework for hallucination detection, achieving state-of-the-art performance in uncertainty quantification and disentangled evaluation.

%\subsection{Symbolic Execution}
%\label{sec:background_SymEx}

%Symbolic execution is a program analysis technique widely used to explore the behaviour of programs under a variety of inputs~\cite{symex_klee}. 
%Instead of executing a program with concrete inputs, symbolic execution uses \emph{symbolic variables} to represent inputs, generating constraints that describe the program's behaviour across all possible values of these inputs.

%The technique typically traverses the program's control flow graph, maintaining a symbolic state consisting of:
%\begin{itemize}[leftmargin=*]
%    \item \textbf{Path Constraints:} Logical conditions that must be satisfied for a given execution path to be feasible.
%    \item \textbf{Symbolic Expressions:} Representations of program variables as functions of the symbolic inputs.
%\end{itemize}

% When the program encounters a conditional statement (\eg \texttt{if (x > 0)}), symbolic execution forks the execution into two paths:
% \begin{itemize}[leftmargin=*]
%     \item The \emph{true path}, where \texttt{x > 0} is added to the path constraints.
%     \item The \emph{false path}, where \texttt{x <= 0} is added to the path constraints.
% \end{itemize}
% To ensure feasibility, an external SMT solver is consulted to validate these constraints and guide exploration. Unlike heavyweight interpreters or compilers, this architecture reduces complexity by dynamically invoking callbacks to monitor and manipulate symbolic values at runtime.

%In particular, our flavour of symbolic execution used in this work is inspired by the lightweight \emph{peer architecture} described in Bruni~\etal~\cite{Bruni2011APA}. 
%Unlike traditional approaches that require building a standalone symbolic interpreter, this architecture embeds the symbolic execution engine as a lightweight library operating alongside the target program. 
%This design leverages dynamic operator dispatching in languages like Python, enabling symbolic values to behave as native data types while recording their interactions transparently. 

%This paper proposes an alternative peer architecture for
%symbolic execution, where the symbolic semantics is implemented as a library in the target language, rather than as a
%new implementation of the language.

%Our approach is based
%on the insight that languages that provide the ability to dynamically dispatch primitive operations (e.g. many scripting
%languages such as Python) allow us to track symbolic values
%at runtime.

%For our work, equivalence between two code snippets \(s^{(i)}\) and \(s^{(j)}\) is determined by comparing their symbolic traces. Symbolic execution extracts the path constraints and symbolic expressions for both snippets. 
%The equivalence relation is defined as:
%\begin{multline}
%    T(s^{(i)}) \equiv T(s^{(j)}) \iff \text{Path Constraints and Symbolic Expressions of } \\
%    s^{(i)} \text{ and } s^{(j)} \text{ are identical.}
%\end{multline}

%Exact equivalence is often \emph{undecidable} due to the complexity of symbolic traces. 
%Instead, we employ \emph{subsumption}, where one trace subsumes another if all behaviours of the latter are captured by the former. 
%This allows us to approximate equivalence effectively.

%By using this lightweight symbolic execution approach, our clustering methodology emphasizes functional semantics, avoiding overfitting to syntactic similarities. 
%This methodology strikes a balance between precision and efficiency, leveraging the extensibility and simplicity of the peer architecture to scale across diverse programming scenarios.
