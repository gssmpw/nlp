
We first outline the LLM and dataset selection process in Sections~\ref{sec:techchallenge} and~\ref{sec:dataset} respectively, followed by our setup in Section~\ref{sec:setup}.
We then report our results and a subsequent discussion in Sections~\ref{sec:results} and~\ref{sec:results-discussion} respectively, before concluding with a note on the usability of our techniques in Section~\ref{sec:usability}.

\subsection{LLM Selection}
\label{sec:techchallenge}
%This work faced several technical challenges arising from the limitations of available large language models (LLMs) for code generation. 
%The issues primarily stem from two areas: the quality of open-source models and the restricted capabilities of certain proprietary models.

%\textbf{Open-Source Model Quality:} Open-source models available on HuggingFace~\cite{huggingface} exhibit significant quality issues in code generation tasks. 
%            Initial experiments with general-purpose LLMs, such as \gemini, on a code hallucinations focused dataset~\cite{codehalu} revealed a negligible pass rate. 


We selected the open-source model \codegenmonoC from \salesforce~\cite{salesforcecodegen} because it was specifically pre-trained for \python, the language used in our dataset.
%
%We tested models of different sizes from \salesforce~\cite{salesforcecodegen}: \codegenmonoA, \codegenmonoB and \codegenmonoC given that they are tailored for code generation for the specific language \ie \python           
 %           Despite these models being tailored for code generation for the specific language \ie \python, their performance remained inadequate, with the best-performing model (\codegenmonoC) achieving only a \bestSalesforcePassrate top pass rate on our benchmark. 
 %           We also applied \llama to our benchmark and it achieved a comparable correctness percentage to \salesforce/\codegenmonoC.
 %           This highlights the current gap in the quality of open-source models for advanced code-generation tasks.
%
%\textbf{API Limitations of Proprietary Models:}
%
We also picked a proprietary model, namely \gptturbo~\cite{gpt35turboinstruct} from \openai~\cite{openai}.
%, demonstrate stronger performance in code generation, they pose challenges for tasks requiring fine-grained uncertainty quantification. 
            Unfortunately, the latest models from \openai don't expose the \texttt{logprobs} functionality through their API, which is needed for our computations. 
%            The only exception was \gptturbo~\cite{gpt35turboinstruct}, which supports \texttt{logprobs} and was therefore selected for our evaluation in this work. These API limitations restrict the range of models that can be used for tasks involving detailed uncertainty analysis.

%These challenges underscore the trade-offs between model quality and technical compatibility when evaluating uncertainty in code generation. 
%Improvements in both open-source and proprietary LLM capabilities are necessary to advance this area of research.



\subsection{Dataset Selection}\label{sec:dataset}
To evaluate the performance of our techniques, we use \livecodebench~\cite{livecodebench}, a contamination-free benchmark for assessing LLMs on code-related tasks
containing \totalProbsComb~problems, divided into \emph{Easy} (\totalProbs problems), \emph{Medium} (\totalProbsMedium problems) and \emph{Hard} (\totalProbsHard problems). 
\livecodebench is designed to address key challenges in code evaluation by incorporating diverse problems from programming competition platforms such as LeetCode, AtCoder, and CodeForces. 
Notably, \livecodebench's contamination-free design ensures that the selected problems have not been seen during the training of most modern LLMs, thereby eliminating data leakage concerns. 
%This guarantees that the benchmark accurately measures a model's generalisation ability rather than its capacity to memorize training data, making it a reliable benchmark for evaluating novel techniques in code generation.

%In our evaluation, we use the entirety of \livecodebench~dataset \ie \totalProbsComb~problems, divided into \emph{Easy, Medium} and \emph{Hard}.

To start with, we focused on the subset of \emph{Easy} problems in \livecodebench, where the two selected LLMs exhibited average testcase passing rates of
\SFSolutionsPassRate~for \codegenmonoC~and \GPTSolutionsPassRateSmall~ for \gptturbo.
Due to limited compute resources and the fact that \codegenmonoC already struggled on the \emph{Easy} problems, 
we then extended our evaluation to the \emph{Medium} and \emph{Hard} classes of problems only for \gptturbo. %, in order for us to get a better understanding of the applicability of our proposed techniques.
For \emph{Medium} problems, \gptturbo achieved a passing rate of \GPTSolutionsPassRateMedium, while for \emph{Hard} problems, the passing rate was \GPTSolutionsPassRateHard.
%reasons described in \S\ref{sec:techchallenge}.
%These problems are curated from high-quality competition datasets and are suitable for testing the functional correctness of generated solutions. 

Each problem is accompanied by a natural language description and Input/Output test cases. We use the natural language description in the query provided to the LLM (as shown in Figure~\ref{fig:sampleproblem}), and the test cases for evaluating our techniques.


%\begin{itemize}[leftmargin=*]
%    \item \textbf{Natural Language Problem Description:} A clear and detailed task description, written in natural language.
%    \item \textbf{Input/Output Test Cases:} A set of test cases used for verifying the correctness of the generated code.
%\end{itemize}

%The decision to use the \emph{Easy} subset stems from the observation that the performance of the LLMs under evaluation was insufficient on harder problems. 
%Higher-difficulty problems often require complex reasoning and sophisticated algorithms, which are challenging for the current generation of LLMs. 
%These limitations could result in poor-quality outputs, undermining our ability to effectively evaluate semantic uncertainty and functional clustering. 

%By selecting problems where the models exhibit a reasonable baseline performance, we ensure a meaningful evaluation of our proposed techniques. 
%The \emph{Easy} subset provides a controlled evaluation setting by avoiding extreme algorithmic complexity while maintaining sufficient diversity in problem types. 
%This allows us to focus on analysing semantic uncertainty and functional clustering in code generation without the confounding effects of low model performance.

%\livecodebench's contamination-free design ensures that the selected problems have not been seen during the training of most modern LLMs, making it a reliable benchmark for the purpose of this work.

\subsection{Experimental Setup}\label{sec:setup}

%In this study, we evaluate the performance of our techniques on a benchmark of \totalProbs problems.
Our experiments were conducted on a machine running \texttt{Ubuntu 20.04.5 LTS (Focal Fossa)} with one \texttt{NVIDIA A100 GPU (80GB)}.
 
%The evaluation involves querying a large language model (LLM) for each problem and applying both SE-based and MI-based approaches to compute uncertainty scores. 
%
%Additionally, in order to investigate the impact of our decision of using \textit{length-normalised response probablities}, we carry out an ablation study by using both the \textit{unnormalised, raw response probablities} alongside our \textit{length-normalised one}. 
%
%Finally, we assess the correctness of the generated solutions based on their performance on test cases provided by the benchmark.

For the semantic equivalence approach in Section~\ref{sec:symex}, we ask the LLM to generate \numSamples responses for each problem along with their respective \texttt{log-probabilities}.
%, as returned by the model's API. Using these \texttt{logprobs}, we compute the probability of each response. 
On top of that, for the MI-based approach described in Section~\ref{sec:mi}, we perform \numIterations iterations of prompting for each of the \numSamples generated responses. The first iteration involves querying the model with the original prompt, while the second iteration uses a concatenated prompt, combining the original prompt and the response from the first iteration. 

In order to perform symbolic execution for finding clusters, we use \crosshair~\cite{crosshair} with a per condition timeout of \CrosshairPerConditiontimeout and the same per path timeout of \CrosshairPerPathtimeout. 
In addition to this, we also impose an overall timeout of \Crosshairtotaltimeout for each pair of programs that is being checked for equivalence.
If a counterexample showing the difference in behavior is not found within this timeout, we assume that the programs belong in the same cluster. 
% \CD{clarify this}

%The SE score is then calculated based on the distribution of probabilities over the generated responses, as described in Section~\ref{sec:symex}.

We evaluate the following techniques:
\begin{itemize}[leftmargin=*]
\item \textbf{\SESymbolicRaw:} The semantic equivalence approach described in Section~\ref{sec:symex}.
%  For our SE-based approach, we query the LLM to generate \numSamples responses for each problem along with their respective \texttt{logprobs}, as returned by the model's API. 
 %                       Using these \texttt{logprobs}, we compute the probability of each response. 
    
  %                      For the \crosshair~\cite{crosshair} runs that are used to compute the clusters symbolically, we use a per condition timeout of \CrosshairPerConditiontimeout and an identical per path timeout of \CrosshairPerPathtimeout. 
   %                     Pairs of programs that hit the timeout are considered to be in the same cluster. 

    %                    The SE score is then calculated based on the distribution of probabilities over the generated responses, as described in Section~\ref{sec:symex}.
\item \textbf{\SESymbolic:} The semantic equivalence approach, but with length normalization (see Section~\ref{sec:probcomp}).
\item \textbf{\SESymbolicUnif:} The semantic equivalence approach, but with uniform distribution (see Section~\ref{sec:probcomp}).  
  %\emph{length-normaised response probablities} \ie while using \textit{unnormalised, raw response probablities}.  

    \item \textbf{\MISymbolicRaw:} The MI approach described in Section~\ref{sec:mi}.
                        %The response probabilities are computed using the same mechanism as the SE-based approach. 
                        %The \crosshair specific timeout remains the same as the SE-based approach. 
                        
                        %The MI score is then calculated as outlined in Section~\ref{sec:mi}.
    \item \textbf{\MISymbolic:} The MI approach, but with length normalization. %This is our adapted MI-based approach, but without the \textit{length-normaised response probablities} \ie while using \textit{unnormalised, raw response probablities}.
    \item \textbf{\SEOriginal:} The original implementation from Kuhn et al.~\cite{kuhnsemantic}. This incorporates length normalization, as omitting it led to probability underflows, resulting in NaN values.
    \item \textbf{\MIOriginal:} The original implementation from Abbasi et al.~\cite{abbasi2024believe}, with length normalization.
    \item \textbf{\LLMProbability:} Baseline technique where the model's response probablities are used as a proxy for correctness. 
\end{itemize}

As mentioned in Section~\ref{sec:mi}, for the MI approach, we did not explore the uniform distribution variant, as the LLM-reported probabilities are central to the technique being used to distinguish between aleatoric and epistemic uncertainties. Additionally, we prioritized experimentation with the better-performing technique---the semantic equivalence-based one.

For each problem in the benchmark, we compute:
\begin{itemize}[leftmargin=*]
\item The corresponding \emph{uncertainty score} for \SESymbolicRaw, \SESymbolic, \SESymbolicUnif, \SEOriginal, \MISymbolicRaw, \MISymbolic and \MIOriginal.
  \item The \emph{probablity of the top-ranked response} for \LLMProbability. 
  \item The \emph{correctness score}, which is calculated as follows: the top-ranked response generated by the LLM (as determined by the API's ranking) is executed against the benchmark's test cases. The percentage of test cases successfully passed is recorded as the correctness score. A generous timeout of \CorrectnessTimeout is applied to each test case. If the candidate solution exceeds this timeout, the test case is considered ``failed''.
    %\item The Pearson correlation factor and the resultant p-value for each of the collected scores and the correctness values for each problem. 
\end{itemize}


\subsection{Results}\label{sec:results}

In Table ~\ref{tab:correlation_results}, we compute the Pearson correlation between the uncertainty scores and the correctness scores for \SESymbolic, \SESymbolicUnif, \SEOriginal, \MISymbolic and \MIOriginal, respectively, and the correlation between the probablity of the top-ranked response and the correctness score for \LLMProbability. We report the Pearson correlation coefficient and p-value, with statistically significant results highlighted in bold.
%
We excluded the results for \SESymbolicRaw and \MISymbolicRaw from Table ~\ref{tab:correlation_results}, as probabilities underflowed (discussed in Section~\ref{sec:probcomp}) causing them to yield NaN values.

Table~\ref{tab:average_lines_tokens} shows an overview of the size of the responses received from the two models considered in this work: \gptturbo and \codegenmonoC. % (from \salesforce).
%As per the earlier discussed in \S\ref{sec:probcomp}, the length of the response from the models affects the performance of the techniques used.

%As such, the responses are \emph{small} with regards to Lines of Code (LOC), with \gptturbo producing, on average, \GPTSolutionsLines~lines long solutions and \salesforce/\codegenmonoC producing \SFSolutionsLines~lines on average. 

%However, the average token length is \emph{large} enough for both models (\GPTSolutionsToken~for \gptturbo and \SFSolutionsToken~for \salesforce/\codegenmonoC) such that it can significantly impact the effectiveness of the techniques used, as we will see later in the section. 

% \begin{table*}[ht!]
%     \centering
%     \caption{Correlation Results for Different Techniques}
%     \label{tab:correlation_results}

%     \begin{tabular}{l r r}
%         \toprule
%         \textbf{Technique} & \multicolumn{2}{c}{\textbf{Model (Pearson Correlation, p-value)}} \\
%         \cmidrule(r){2-3}
%          & \textbf{\gptturbo} & \textbf{\codegenmonoC} \\
%         \midrule
%         \SEOriginal & \SENLGPearsonGPT ~(\SENLGPearsonPValueGPT) & \SENLGPearsonSF ~(\SENLGPearsonPValueSF) \\

%         \SESymbolic & \SESymbolicPearsonGPT ~(\SESymbolicPearsonPValueGPT) & \SESymbolicPearsonSF ~(\SESymbolicPearsonPValueSF) \\
%         \SESymbolicRaw & NaN (NaN) & NaN (NaN) \\
%         \midrule
%         \MIOriginal & - & - \\
  
%         \MISymbolic & \MISymbolicPearsonGPT ~(\MISymbolicPearsonPValueGPT) & \MISymbolicPearsonSF ~(\MISymbolicPearsonPValueSF) \\
%         \MISymbolicRaw & - & - \\
%         \midrule
%         \LLMProbability & \LLMProbabilityPearsonGPT ~(\LLMProbabilityPearsonPValueGPT) & \LLMProbabilityPearsonSF ~(\LLMProbabilityPearsonPValueSF) \\
%         \bottomrule
%     \end{tabular}

% \end{table*}

\begin{table*}[ht!]
    \centering
    \small
    \caption{Correlation Results---Pearson coefficient(p-value)---for Different Techniques on Different Difficulty Problems (\gptturbo: Easy/Medium/Hard; \codegenmonoC: Easy). Statistically significant results are highlighted in bold.}
    \label{tab:correlation_results}
    \begin{tabularx}{\linewidth}{Xrrrr}
        \toprule
        \multirow{2}{*}{\textbf{Technique}} 
        & \multicolumn{3}{c}{\textbf{\gptturbo}} & \textbf{\codegenmonoC} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-5}
        & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Easy} \\
        \midrule

        \SEOriginal
          & \SENLGPearsonGPTUpdated (\SENLGPearsonPValueGPTUpdated)  
          & \SENLGPearsonGPTMediumUpdated (\SENLGPearsonPValueGPTMediumUpdated)                                         
          & \SENLGPearsonGPTHardUpdated (\SENLGPearsonPValueGPTHardUpdated)                                        
          & \SENLGPearsonSFUpdated (\SENLGPearsonPValueSFUpdated)    
          \\

        \SESymbolic
          & \textbf{\SESymbolicPearsonGPTUpdated (\SESymbolicPearsonPValueGPTUpdated)}
          & \textbf{\SESymbolicPearsonGPTMediumUpdated (\SESymbolicPearsonPValueGPTMediumUpdated)}
          & \textbf{\SESymbolicPearsonGPTHardUpdated (\SESymbolicPearsonPValueGPTHardUpdated)}
          & \textbf{\SESymbolicPearsonSFUpdated (\SESymbolicPearsonPValueSFUpdated)}
          \\

        \SESymbolicUnif
         & \textbf{\SESymbolicUnifPearsonGPTUpdated (\SESymbolicUnifPearsonPValueGPTUpdated)}
         & \textbf{\SESymbolicUnifPearsonGPTMediumUpdated (\SESymbolicUnifPearsonPValueGPTMediumUpdated)}
         & \textbf{\SESymbolicUnifPearsonGPTHardUpdated (\SESymbolicUnifPearsonPValueGPTHardUpdated)}
         & \textbf{\SESymbolicUnifPearsonSFUpdated (\SESymbolicUnifPearsonPValueSFUpdated)}
         \\

        \midrule

        \MIOriginal
         & \MINLGPearsonGPTUpdated (\MINLGPearsonPValueGPTUpdated)  
         & \MINLGPearsonGPTMediumUpdated (\MINLGPearsonPValueGPTMediumUpdated)                                         
         & \MINLGPearsonGPTHardUpdated (\MINLGPearsonPValueGPTHardUpdated)                                        
         & \MINLGPearsonSFUpdated (\MINLGPearsonPValueSFUpdated)    
         \\

        \MISymbolic
          & \textbf{\MISymbolicPearsonGPTUpdated (\MISymbolicPearsonPValueGPTUpdated)}
          & \textbf{\MISymbolicPearsonGPTMediumUpdated (\MISymbolicPearsonPValueGPTMediumUpdated)}
          & \textbf{\MISymbolicPearsonGPTHardUpdated (\MISymbolicPearsonPValueGPTHardUpdated)}
          & \textbf{\MISymbolicPearsonSFUpdated (\MISymbolicPearsonPValueSFUpdated)}
          \\

        \midrule

        \LLMProbability
          & \LLMProbabilityPearsonGPTUpdated (\LLMProbabilityPearsonPValueGPTUpdated)
          & \LLMProbabilityPearsonGPTMediumUpdated (\LLMProbabilityPearsonPValueGPTMediumUpdated)
          & \LLMProbabilityPearsonGPTHardUpdated (\LLMProbabilityPearsonPValueGPTHardUpdated)
          & \LLMProbabilityPearsonSFUpdated (\LLMProbabilityPearsonPValueSFUpdated)
          \\

        \bottomrule
    \end{tabularx}
\end{table*}

% \begin{table*}[ht!]
%     \centering
%     \caption{Average Number of Lines and Tokens for Solutions Generated by \gptturbo and \salesforce/\codegenmonoC}
%     \label{tab:average_lines_tokens}

%     \begin{tabular}{l r r}
%         \toprule
%         \textbf{Metric} & \multicolumn{2}{c}{\textbf{Model}} \\
%         \cmidrule(r){2-3}
%          & \textbf{\gptturbo} & \textbf{\codegenmonoC} \\
%         \midrule
%         Average Lines of Code & \GPTSolutionsLines & \SFSolutionsLines \\
%         Average Tokens & \GPTSolutionsToken & \SFSolutionsToken \\
%         \bottomrule
%     \end{tabular}
% \end{table*}

\begin{table*}[ht!]
    \centering
    \caption{Average Number of Lines and Tokens for Solutions Generated by \gptturbo and \salesforce/\codegenmonoC for Different Classes of Problems}
    \label{tab:average_lines_tokens}

    \begin{tabular}{l r r r r}
        \toprule
        \multirow{2}{*}{\textbf{Metric}} 
          & \multicolumn{3}{c}{\gptturbo}
          & \multicolumn{1}{c}{\codegenmonoC} \\
        \cmidrule(r){2-4}\cmidrule(r){5-5}
          & \textbf{Easy}
          & \textbf{Medium}
          & \textbf{Hard}
          & \textbf{Easy} \\

        \midrule
        Average Lines of Code
          & \GPTSolutionsLines
          & \GPTSolutionsLinesMedium
          & \GPTSolutionsLinesHard
          & \SFSolutionsLines \\
        Average Tokens
          & \GPTSolutionsToken
          & \GPTSolutionsTokenMedium
          & \GPTSolutionsTokenHard
          & \SFSolutionsToken \\
        \bottomrule
    \end{tabular}
\end{table*}

%Table~\ref{tab:correlation_results} summarises the results obtained for the various techniques considered in this experimental evaluation. 
\subsection{Discussion of Results}\label{sec:results-discussion}


\paragraph{RQ1: Is there a correlation between the uncertainty computed by the proposed techniques and correctness for code generation?}

%The experiments presented in Figure~\ref{tab:correlation_results}, we observe

The results in Table~\ref{tab:correlation_results} allow us to make the following observations:

$\bullet$ There is a \emph{\textbf{strong negative correlation between correctness and the uncertainty}} computed by \SESymbolic and \SESymbolicUnif and a \emph{\textbf{weak negative correlation}} for \MISymbolic.

$\bullet$ \emph{\textbf{Semantic equivalence among the LLM responses is a stronger indicator of correctness than the actual reported log-probabilities}}---while the probabilities assigned by the LLM carry minimal signal (see RQ4), semantic clustering provides significant insight. This is suggested by the fact that \SESymbolicUnif, which assumes uniform probabilities over LLM reponses, exhibits a correlation similar to that of \SESymbolic with length-normalization.
%
This finding is important as \SESymbolicUnif to be applied even when the LLM does not expose log-probabilities, such as in the latest GPT models.

An open question emerging from this is whether we can prove a direct correlation between the number of semantic clusters and correctness. As our current focus is on the relationship between uncertainty and correctness, we leave this investigation for future work.

$\bullet$ \emph{\textbf{The correlation between uncertainty and correctness remains stable across different problem complexities and sizes}} (i.e. easy, medium, hard). Our experiments for \SESymbolic, \SESymbolicUnif and \MISymbolic show no significant degradation with increased size and difficulty for \gptturbo.

$\bullet$ \emph{\textbf{There is little variation in the correlation between uncertainty and correctness across the two LLMs of different sizes}}. The results for \SESymbolic, \SESymbolicUnif, and \MISymbolic exhibit slight degradation on \livecodebench-Easy when transitioning from \gptturbo to \codegenmonoC.  However, the correlation remains weak in both cases.

%In contrast to this, our symbolic execution based adaptation that makes use of \textit{length-normaised response probablities} performed better than its contemporaries.
%For \gptturbo, \SESymbolic obtained a moderate negative correlation score of \SESymbolicPearsonGPT~with a statistically significant p-value of \SESymbolicPearsonPValueGPT. 
%The correlation is expected to be negative here as we are working with uncertainty scores hence a higher uncertainty score should be reflective of lower correctness.
%The story for \salesforce/\codegenmonoC was also the same. 


\paragraph{RQ2: How do the techniques based on semantic equivalence compare against those based on mutual information?}


We observed a stronger correlation between uncertainty and correctness in techniques based on semantic equivalence compared to those relying on mutual information. We hypothesize that this is due to the unambiguous nature of the problems in our dataset, reducing the need to account for aleatoric uncertainty. We leave the exploration of ambiguity in the problem formulation as future work.

\paragraph{RQ3: Are the techniques designed for natural language directly applicable to code generation?}
In all our experiments, \SEOriginal and \MIOriginal yield results that are not statistically significant, highlighting the necessity of adapting these techniques to account for the unique characteristics of code.
%The story for \SEOriginal is similar, but from a different perspective. 
%For almost all of the responses, the natural language based clustering mechanism puts all of the model's responses into a single cluster.
%This meant that the semantic-based clustering mechanism did not get a chance to contribute towards the uncertainty scores.
%Consequently, for both \gptturbo and \salesforce/\codegenmonoC, the results are \emph{not} statistically significant with the respective p-value scores of \SENLGPearsonPValueGPT~and \SENLGPearsonPValueSF, which are both larger than the threshold of \pvalue.



%\paragraph{\textbf{RQ4: Is length normalization useful for code generation?}}
%For \SESymbolicRaw, there \textit{raw response probabilities} underflowed to zero for all responses. 
%This is in accordance with the observation from Kuhn~\etal~\cite{kuhnsemantic} when responses have a large number of tokens, which is the case for our generation queries for both models, as shown in Table~\ref{tab:average_lines_tokens}.
%This, in turn, results in all uncertainty computations to decay to zero, which results in an NaN during the correlation computation.

\paragraph{RQ4: Are the probabilities computed by the LLM sufficient as a proxy for correctness?}
In all our experiments, except for \gptturbo on \livecodebench-Easy, the results are not statistically significant. This suggests that there is no meaningful correlation between correctness and the log-probabilities reported by the LLM.

%For \gptturbo while the \LLMProbability metric shows a \emph{small}, statistically significant (with~\LLMProbabilityPearsonPValueGPT~as the p-value) positive\footnote{The correlation here is positive since a high probability of the top-ranked response from the model should indicate a higher level of correctness.} correlation of~\LLMProbabilityPearsonGPT, for \salesforce/\codegenmonoC the results are \emph{not} statistically significant given the p-value \LLMProbabilityPearsonPValueSF~which is greater than the standard \pvalue. This shows the lack of effectiveness of simply using the model's probablities directly to decide the uncertainty in the model's outputs.


\subsection{Using the Uncertainty Estimation for Correctness}
\label{sec:usability}
We envision uncertainty scores being utilized as part of an abstention policy, similar to the approach described by Abbasi~\etal~\cite{abbasi2024believe}.
In this policy, an uncertainty threshold is set such that LLM responses with scores above the threshold are assumed to be good according to some metric, while those below the threshold are considered bad.
For the latter, the LLM may abstain from presenting these responses to the user.

The primary metric of interest in this work is functional correctness. LLM responses are classified as correct if their correctness score exceeds a predefined threshold and incorrect otherwise. To determine the \emph{uncertainty threshold} for the abstention policy, we select the value that maximizes correctness accuracy.

For our experimental evaluation, we considered the best performing techniques for uncertainty assessment, \SESymbolic and \SESymbolicUnif, and \LLMProbability as the baseline.
We then considered our more successful model~\gptturbo and the \livecodebench dataset, using a correctness score threshold of 90\%. 
%
As discussed in Section~\ref{sec:dataset}, \gptturbo's responses exhibit a low unit test passing rate, leading to an imbalanced dataset where incorrect responses significantly outnumber correct ones. This imbalance risks trivializing the classification task, as an ``all-incorrect'' model would dominate. To address this, we followed standard practice and applied random downsampling, removing a portion of incorrect responses to prevent the model from defaulting to a trivial solution. We split the dataset 50/50 into training and validation.
To mitigate overfitting and reduce bias,  we employed 2-fold cross-validation, alternating between training and validation for each fold.

%The metric we are conserned with in this work is functional correctness. Thus, we label LLM responses as \emph{correct}  if their correctness score exceeds the correctness threhold, or \emph{incorrect} otherwise. Then, we obtain the \emph{uncertainty threshold} to be used by the abstention policy by picking the one that maximizes accuracy. 
%In the experiments summarized in Table~\ref{tab:abstention_metrics}, we used the responses provided by \gptturbo to the \livecodebench dataset, with a correctness threshold of 90\%. To reduce overfitting and prevent biases, we used a 2-fold cross-validation, rotating the role of each fold between training and validation. as explained in Section~\ref{sec:dataset}, these responses exhibit relatively low unit test passing rate resulting in an imbalanced dataset with many more incorrect responses, causing a trivial ``all incorrect'' solution to dominate. Following common practice, we downsampled by at random dropping a certain percentage of points that are causing the trivial ``all-incorrect'' response.


%As it is the practice, if extreme class imbalance (\eg many near-zero correctness cases) causes a trivial ``all incorrect'' solution to dominate, we down-sample or partially discard excessively low-correctness samples to ensure a more balanced dataset (and thus avoid collapsing to a trivial threshold).

%Finally, we fix this threshold and measure performance on the validation set, whereby any response with an uncertainty \emph{below} the learned threshold is deemed sufficiently reliable, and any response above it is \emph{abstained} from the user.

%Concretely, we first combine each response's \emph{correctness score}, a measure of how well the solution aligns with ground truth and \emph{uncertainty score} into a single dataset.
%fraction exceeds a specified cutoff (\eg 0.9) 
% Next, we split the dataset into a training set and a validation set, ensuring we do not overfit our threshold to a single subset.

%Next, we carry out training while employing a k-fold cross validation, rotating the role of each fold between training and validation to ensure robust threshold selection.
%We evaluate the accuracy of these predictions against the ground-truth labels and select the threshold that maximizes this accuracy.


%Table~\ref{tab:abstention_metrics} summarizes results for \SESymbolic, \SESymbolicUnif and \LLMProbability for \gptturbo on the whole \livecodebench dataset, with a correctness threshold of 90\%. We used a cross-validation with $k=2$ folds. 


% More precisely, on the training set, we \emph{sort} responses by ascending uncertainty, then \emph{sweep} through possible threshold values; for each candidate threshold, all responses with uncertainty \emph{below} the threshold are predicted as \texttt{correct}, and the rest as \texttt{incorrect}.

%Table~\ref{tab:abstention_metrics}~summarises the performance of using learned abstention threshold for \SESymbolic, \SESymbolicUnif and \LLMProbability for \gptturbo. 

The evaluation results for the uncertainty threshold are presented in Table~\ref{tab:abstention_metrics}. 
Both \SESymbolic and \SESymbolicUnif achieved high accuracy scores of \SENormAcc~and \SEUnifAcc, respectively, whereas \LLMProbability~performed significantly worse, with an accuracy of only~\LLMProbabilityAcc.

A key advantage of our techniques is their extremely low False Positive (FP) rate, with \SENormFP~for both \SESymbolic and \SESymbolicUnif. 
This indicates that our methods are more conservative in accepting LLM-generated responses, a particularly valuable property for code generation. 
Given the potential safety risks associated with incorrect code, especially when LLM performance is inconsistent, this cautious approach enhances the reliability and safety of LLM-assisted coding.


\begin{table*}[ht!]
    \centering
    \caption{Abstention Metrics for \SESymbolic, \SESymbolicUnif and \LLMProbability with \gptturbo}
    \label{tab:abstention_metrics}

    \begin{tabular}{l r r r}
        \toprule
        \textbf{Technique}
          & \multicolumn{1}{c}{\textbf{Accuracy}}
          & \multicolumn{1}{c}{\textbf{False Positives}}
          & \multicolumn{1}{c}{\textbf{False Negatives}} \\
        \midrule
        \SESymbolic
          & \SENormAcc
          & \SENormFP
          & \SENormFN \\
        \SESymbolicUnif
          & \SEUnifAcc
          & \SEUnifFP
          & \SEUnifFN \\
        \LLMProbability
          & \LLMProbabilityAcc
          & \LLMProbabilityFP
          & \LLMProbabilityFN \\
        \bottomrule
    \end{tabular}
\end{table*}

% In practice, one may also tune the threshold for desired precision/coverage trade-offs or use calibration techniques (e.g.\ isotonic regression)
% so that the ``uncertainty score'' better reflects the actual probability of correctness.


%To implement this, we designed a pipeline that processes uncertainty scores and correctness fractions to derive an optimal abstention threshold. 

%The threshold training phase uses a split of the dataset into training and testing subsets, typically with 80\% of the data for training and 20\% for testing. 
%During training, the pipeline iterates over unique uncertainty score thresholds to identify the threshold that maximizes the accuracy metric \ie the number of correct abstentions made for a given correctness criterion (90\% in our case). 
% This metric accounts for both correct predictions, where uncertainty scores are below the threshold and correctness is above a specified value, and correct abstentions, where uncertainty scores are above the threshold and correctness is below this value. 
% The redefinition ensures that the abstention policy rewards both effective predictions and abstentions, promoting balanced decision-making.

%In the evaluation phase, the learned threshold is applied to the test set, where accuracy and abstention rates are calculated. 
% Accuracy is computed as the ratio of correctly handled samples (whether predicted or abstained) to the total number of samples, while the abstention rate reflects the proportion of samples where predictions were withheld due to high uncertainty. 

%We ran this pipeline on the better performing \gptturbo model and achieved a test-set accuracy of \SEAcc~for the SE scores and \MIAcc~for the MI scores. 

% \begin{figure}[ht]
%     \centering
%     % Subfigure 1
%     \begin{subfigure}[b]{0.45\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figures/average_se_bin_090.png}
%         \caption{Correctness Bin with SE scores: 0-90\% and 90-100\%}
%     \end{subfigure}
%     \hfill
%     % Subfigure 2
%     \begin{subfigure}[b]{0.45\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figures/average_mi_bin_090.png}
%         \caption{Correctness Bin with MI scores: 0-90\% and 90-100\%}
%     \end{subfigure}
    
%     \caption{Average SE/MI Scores for \gptturbo responses, showing an abstention threshold value for achieving 90\% correctness.}
%     \label{fig:mise_scores_bins}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     % Subfigure 1
%     \begin{subfigure}[b]{0.45\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figures/average_mi_bin_060.png}
%         \caption{Correctness Bin: 0-60\% and 60-100\%}
%     \end{subfigure}
%     \hfill
%     % Subfigure 2
%     \begin{subfigure}[b]{0.45\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figures/average_mi_bin_070.png}
%         \caption{Correctness Bin: 0-70\% and 70-100\%}
%     \end{subfigure}
    
%     % Subfigure 3
%     \begin{subfigure}[b]{0.45\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figures/average_mi_bin_080.png}
%         \caption{Correctness Bin: 0-80\% and 80-100\%}
%     \end{subfigure}
%     \hfill
%     % Subfigure 4
%     \begin{subfigure}[b]{0.45\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figures/average_mi_bin_090.png}
%         \caption{Correctness Bin: 0-90\% and 90-100\%}
%     \end{subfigure}
    
%     \caption{Average MI Scores for Different Correctness Bins. Each plot shows the average MI scores computed for two bins.}
%     \label{fig:mi_scores_bins}
% \end{figure}
