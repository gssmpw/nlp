To illustrate the relationship between uncertainty and correctness for code generation, we refer to the problem described by the prompt in Figure~\ref{fig:sampleproblem}. 

\begin{figure}[htbp]
    \centering
    \begin{tcolorbox}[
        enhanced,
        width=0.85\textwidth,
        colback=white,
        colframe=blue!50!black,
        boxrule=1pt,
        arc=5pt,              
        title=LLM Prompt,
        fonttitle=\bfseries,
        attach boxed title to top center={yshift=-2mm},
        varwidth boxed title=0.7\linewidth
      ]
    \textbf{User Prompt:}

    \vspace{0.5em}
    You are given a \texttt{0-indexed} array of strings \texttt{details}. Each element of \texttt{details} provides information about a given passenger compressed into a string of length 15. The format is as follows:
    \begin{itemize}
        \item The first 10 characters: the passenger's phone number
        \item The 11th character: the passenger's gender
        \item The 12th and 13th characters: the passenger's age
        \item The 14th and 15th characters: the seat allotted
    \end{itemize}
    Return the number of passengers whose age is \textbf{strictly greater} than 60.

    % \vspace{0.5em}
    % \textbf{Example:}
    % \begin{verbatim}
    % Input: details = ["7869194042M58A", "0741234567F75B", "6280984567F13C"]
    % Output: 1
    % Explanation: Only one passenger is older than 60.
    % \end{verbatim}

    \vspace{0.5em}
    \textbf{Task:} 
    Please provide a function in your preferred programming language that takes \texttt{details} and returns how many passengers are older than 60.
    \end{tcolorbox}
    \caption{An example LLM prompt for the ``number-of-senior-citizens'' coding problem.}
    \label{fig:sampleproblem}
\end{figure}

\begin{figure}[ht!]
    \centering
    
    \begin{subfigure}[t]{0.3\textwidth}
        
        \lstinputlisting{code/good_llm_snippet1.py}
        \vspace{2.5em} 
        \caption{Snippet 1}
        \label{lst:good1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        
        \lstinputlisting{code/good_llm_snippet2.py}
        \vspace{3.4em}
        \caption{Snippet 2}
        \label{lst:good2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        
        \lstinputlisting{code/good_llm_snippet3.py}
        \vspace{2.5em}
        \caption{Snippet 3}
        \label{lst:good3}
    \end{subfigure}

    \caption{Three code snippets from the \gptturbo, all correct, semantically equivalent.}
    \label{fig:good-llm-snippets}
\end{figure}

%Figure~\ref{fig:good-llm-snippets} shows the top 3 responses from the well-tuned model \ie \gptturbo. Closer observation shows that the three responses shown in Listing~\ref{lst:good1}, Listing~\ref{lst:good2} and Listing~\ref{lst:good3} of Figure~\ref{fig:good-llm-snippets} are semantically equivalent thereby indicating that the model has high confidence in its response. It also turns out to be the case that these responses all pass the testsuite for this problem. 

\begin{figure}[ht!]
    \centering
    
    \begin{subfigure}[t]{0.3\textwidth}
       
        \vspace{0.5em} 
        \lstinputlisting{code/bad_llm_snippet1.py}
        \caption{Snippet 1}
        \label{lst:bad1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        
        \vspace{0.5em}
        \lstinputlisting{code/bad_llm_snippet2.py}
        \caption{Snippet 2}
        \label{lst:bad2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        
        \vspace{0.5em}
        \lstinputlisting{code/bad_llm_snippet3.py}
        \caption{Snippet 3}
        \label{lst:bad3}
    \end{subfigure}

    \caption{Three code snippets from the \salesforce/\codegenmonoC, all incorrect, and semantically distinct.}
    \label{fig:bad-llm-snippets}
\end{figure}

%However, for a contemporary, worse-performing model from \salesforce (\codegenmonoC), the 3 responses shown in Listing~\ref{lst:bad1}, Listing~\ref{lst:bad2} and Listing~\ref{lst:bad3} of Figure~\ref{fig:bad-llm-snippets}, are all semantically different and hence fall in their own respective clusters. Unsurprisingly, all 3 of these responses are incorrect and do not pass any testcases of the testsuite. 

Figure~\ref{fig:good-llm-snippets} displays the top three responses generated by our first model \gptturbo, all of which are semantically equivalent and correct. 
In contrast, Figure~\ref{fig:bad-llm-snippets} presents the first three responses generated by \salesforce (\codegenmonoC), all of which are incorrect.

The log-probabilities reported by the LLMs are as follows: for the better performing \gptturbo, the three snippets have log-probabilities of \GPTsnipLogProbA, \GPTsnipLogProbB, and \GPTsnipLogProbC, respectively. 
For \codegenmonoC, the log-probabilities are \SFsnipLogProbA, \SFsnipLogProbB, and \SFsnipLogProbC. 
However, these log-probabilities alone are insufficient as proxies for correctness. 
In particular, there is no clear way to infer from the log-probabilities that all the snippets generated by \gptturbo are correct, while all those generated by \codegenmonoC are incorrect.

%At first glance, the differences in responses might suggest that both LLMs exhibit low confidence in their responses. However, a closer examination of the snippets in Listings~\ref{lst:good1}, \ref{lst:good2}, and \ref{lst:good3} in Figure~\ref{fig:good-llm-snippets} reveals that, although syntactically distinct, they are semantically equivalent. In contrast, the responses in Figure~\ref{fig:bad-llm-snippets} are both syntactically and semantically distinct.

The techniques outlined in Section~\ref{sec:symex} and Section~\ref{sec:mi} effectively identify the semantic equivalence of the snippets in Figure~\ref{fig:good-llm-snippets} and incorporate this information during the computation of entropy and mutual information, respectively. 
As shown later in the paper, both methods ultimately conclude that \gptturbo exhibits high confidence in its responses, while the model from \salesforce demonstrates significant uncertainty. 
Furthermore, we will show that uncertainty negatively correlates with correctness.
%When using uncertainty as a proxy for correctness, the results suggest that \gptturbo's responses are likely accurate, whereas \salesforce's are likely flawed. This conclusion is corroborated by the test suite results: all three responses from \gptturbo pass successfully, whereas none of \salesforce's responses do.
