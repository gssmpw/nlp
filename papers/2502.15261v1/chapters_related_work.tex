\section{Related Work}
\label{sec:related_works}

\subsection{Explainable GEC}
Many GEC systems focus solely on correction without offering explanations~\citep{davis-etal-2024-prompting,ye2022focus,ye2023system}. To address this limitation, recent research has investigated multiple techniques to enhance the explainability of GEC. One technique is Example-based GEC~\citep{kaneko-etal-2022-interpretability,vasselli-watanabe-2023-closer}, which boosts explainability by retrieving examples that are similar to the input instance based on defined grammar rules. GEE~\citep{song-etal-2024-gee} develops a two-step pipeline for GEC explanation generation.~\citet{kaneko-okazaki-2024-controlled} explore the generation of natural language explanations by prompting large language models (LLMs). Another relevant task is feedback comment generation (FCG)~\citep{nagata-2019-toward,nagata-etal-2021-shared, hanawa-etal-2021-exploring}, which aims to automatically create feedback comments, like hints or explanatory notes, to aid in writing learning. However, it suffers from expensive costs associated with data annotation~\citep{nagata2020creating}. Furthermore, it is often explored with limited access to only a subset of English grammatical error types due to the complexity of the task~\citep{nagata-2019-toward}.

Despite these efforts, no research has comprehensively examined the positive interaction between correction and explanation tasks during training. In contrast, our work focuses on studying whether learning a multi-task model can outperform the respective single-task models.


\subsection{Learning with Explanations}
Explainability of NLP tasks is a critical research direction and has been given serious attention recently, especially due to the ``black box'' nature of LLMs~\citep{dalal-etal-2024-inference,hu2024learning,yu2024mind,saeed2024sumex,DBLP:conf/aaai/YuJLHWLCLLTZZXH24,DBLP:conf/coling/XuLD0CJZLXH25}. Prior studies have shown that training models to produce task predictions and explanations concurrently can boost performance in vision-language tasks~\citep{majumder2022knowledge,DBLP:conf/icassp/LiCLXCZ23,DBLP:conf/aaai/LiL0LHYY024} and several downstream NLP tasks, including text classification~\citep{li2022unifying,DBLP:conf/nips/LiZLML0HY24}, commonsense reasoning~\citep{veerubhotla-etal-2023-shot,yan2025position,DBLP:conf/coling/HuangMLHZ0Z24,DBLP:conf/sigir/LiLHYS022,DBLP:journals/tkde/LiHZZLLCZS23}, and complaint detection~\citep{singh-etal-2023-peeking}. An essential aspect of this research is the development of self-Rationalization models that generate task predictions along with corresponding explanations to enhance the explainability or task performance of neural networks. There are two main methods for building self-Rationalization models: 1) \textit{extracting key input tokens responsible for task predictions}, referred to as extractive rationales~\citep{deyoung-etal-2020-eraser}, and 2) \textit{creating natural language explanations}~\citep{narang2020wt5}, which serve as a natural interface between models and human users. To refine the performance and trustworthiness of Seq2Seq models,~\citet{lakhotia-etal-2021-fid} developed an extractive fusion-in-decoder architecture within the ERASER benchmark~\citep{deyoung-etal-2020-eraser}, a well-known benchmark for rationale extraction across various datasets and tasks.~\citet{li2022unifying} introduced a combined text classification and rationale extraction model to improve explainability and robustness. Recognizing the synergy between extractive rationales and natural language explanations,~\citet{majumder2022knowledge} integrated both components into a self-Rationalization framework.

\paragraph{Explanation-augmented knowledge distillation.}
Leveraging in-context learning~\citep{brown2020language} and the chain-of-thought (CoT) reasoning~\citep{chu2023survey} of LLMs, many recent studies employ the natural language explanations produced by LLMs with chain-of-thought prompting~\citep{lampinen-etal-2022-language,li2023symbolic} to enhance the development of smaller reasoning models using knowledge distillation~\citep{zhang2024elad}, thereby boosting task performance~\citep{li2024explanations,ho-etal-2023-large,hsieh-etal-2023-distilling} or improving faithfulness~\citep{wang-etal-2023-scott}.
However, convincing and wrong explanations generated by LLMs can foster unwarranted confidence in tackling NLP tasks~\citep{madsen-etal-2024-self,pruthi-etal-2022-evaluating}, particularly in educational contexts emphasizing faithfulness~\citep{lyu-etal-2024-towards} and correctness~\citep{huang-etal-2024-chatgpt}. Consequently, this paper emphasizes model training with human-annotated datasets.
