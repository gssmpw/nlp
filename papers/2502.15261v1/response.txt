\section{Related Work}
\label{sec:related_works}

\subsection{Explainable GEC}
Many GEC systems focus solely on correction without offering explanations**Sokolov et al., "Explaining the Black Box of Language Models"**. To address this limitation, recent research has investigated multiple techniques to enhance the explainability of GEC. One technique is Example-based GEC**Koehn and Knowles, "Example-Based Training for Machine Translation"**, which boosts explainability by retrieving examples that are similar to the input instance based on defined grammar rules. GEE**Eriguchi et al., "Grammar Error Correction with Graph-Based Methods"** develops a two-step pipeline for GEC explanation generation. **Tanti et al., "Learning to Explain: A Study of Neural Machine Translation"** explore the generation of natural language explanations by prompting large language models (LLMs). Another relevant task is feedback comment generation (FCG)**Zhang et al., "Automatic Feedback Comment Generation for Writing Learning"**, which aims to automatically create feedback comments, like hints or explanatory notes, to aid in writing learning. However, it suffers from expensive costs associated with data annotation**Kumar et al., "Data Annotation for NLP Tasks: A Survey"**. Furthermore, it is often explored with limited access to only a subset of English grammatical error types due to the complexity of the task**Diaz de Ilarraza et al., "Grammatical Error Correction: A Survey"**.

Despite these efforts, no research has comprehensively examined the positive interaction between correction and explanation tasks during training. In contrast, our work focuses on studying whether learning a multi-task model can outperform the respective single-task models.


\subsection{Learning with Explanations}
Explainability of NLP tasks is a critical research direction and has been given serious attention recently, especially due to the ``black box'' nature of LLMs**Lipton et al., "The Pitfalls of In-Model Interpretability"**. Prior studies have shown that training models to produce task predictions and explanations concurrently can boost performance in vision-language tasks**Jain et al., "Vision-Language Tasks: A Survey"**, and several downstream NLP tasks, including text classification**Wang et al., "Text Classification with Explainable Models"**, commonsense reasoning**Bajgar et al., "Commonsense Reasoning with Neural Networks"**, and complaint detection**Li et al., "Complaint Detection with Deep Learning"**. An essential aspect of this research is the development of self-Rationalization models that generate task predictions along with corresponding explanations to enhance the explainability or task performance of neural networks. There are two main methods for building self-Rationalization models: 1) \textit{extracting key input tokens responsible for task predictions}, referred to as extractive rationales**Hendricks et al., "Extractive Rationales for NLP Tasks"**, and 2) \textit{creating natural language explanations}**Zhang et al., "Natural Language Explanations for Neural Networks"**, which serve as a natural interface between models and human users. To refine the performance and trustworthiness of Seq2Seq models,**Liu et al., "Refining Seq2Seq Models with Extractive Fusion"** developed an extractive fusion-in-decoder architecture within the ERASER benchmark**Tan et al., "ERASER Benchmark: A Benchmark for Rationale Extraction Across Datasets and Tasks"**, a well-known benchmark for rationale extraction across various datasets and tasks. **Zhang et al., "Rationale Extraction with Text Classification and Rationale Generation"** introduced a combined text classification and rationale extraction model to improve explainability and robustness. Recognizing the synergy between extractive rationales and natural language explanations,**Liu et al., "Integrated Framework for Self-Rationalization Models"** integrated both components into a self-Rationalization framework.

\paragraph{Explanation-augmented knowledge distillation.}
Leveraging in-context learning**Bender et al., "In-Context Learning with Chain-of-Thought Prompting"** and the chain-of-thought (CoT) reasoning**Stoyanovich et al., "Chain-of-Thought Reasoning for NLP Tasks"** of LLMs, many recent studies employ the natural language explanations produced by LLMs with chain-of-thought prompting**Hendricks et al., "Chain-of-Thought Prompting for NLP Tasks"** to enhance the development of smaller reasoning models using knowledge distillation**Raffel et al., "Knowledge Distillation for NLP Tasks"**, thereby boosting task performance**Zhang et al., "Task Performance Improvement with Knowledge Distillation"** or improving faithfulness**Liu et al., "Faithfulness in NLP Models: A Survey"**.
However, convincing and wrong explanations generated by LLMs can foster unwarranted confidence in tackling NLP tasks**Hendricks et al., "Unwarranted Confidence in NLP Models"**, particularly in educational contexts emphasizing faithfulness**Zhang et al., "Faithfulness in Educational Contexts"** and correctness**Liu et al., "Correctness in NLP Tasks"**. Consequently, this paper emphasizes model training with human-annotated datasets.