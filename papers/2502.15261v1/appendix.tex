\input{detailed_results}
\input{hyper_parameters}
\input{exp_loss}

\section{Fine-tuning Details}
\label{app:training_details}
We update all parameters of BART-Large and T5-Base during fine-tuning. For Llama3-8B, we leverage LoRA~\citep{hu2022lora} to implement parameter-efficient fine-tuning. Detailed hyper-parameter configurations are provided in Table~\ref{tab:hyper-parameter}. 

\section{Detailed Results}
\label{app:detailed_results}
Table~\ref{tab:detailed_results} lists detailed results on the \Dataset{}-\textit{dev} set, providing further insight into model behaviors in different training settings. Infusion (with Evidence and with Evidence \& Type) models gain higher correction TP but lower FP and FN, demonstrating that evidence words significantly benefit the correction task. Additionally, pre-explaining models tend to extract more evidence words ($\approx$ 4200\footnote{This is equal to TP plus FP.}) but predict fewer correction edits ($\approx$ 2300).
On the other hand, post-explaining models are declined to extract fewer evidence words ($\approx$ 3700), but predict more correction edits ($\approx$ 2900). We speculate that models are more likely to make predictions when prior information is unavailable. However, models become more cautious with the available prior information. Therefore, pre-explaining models achieve higher correction precision, whereas post-explaining models exhibit higher correction recall.

\section{Impact of Loss Weighting}
\label{app:loss_weight}
This section examines the balance between learning for correction and explanation tasks by adjusting the loss weight $\lambda$ in Equation~\eqref{eq:loss}. An elevated value of $\lambda$ indicates an increased focus on learning the explanation task. We experiment with various models employing different loss weights $\lambda$ selected from $\{0.5, 1.0, 1.5, 2.0\}$, and present our results in \Dataset{}-$\textit{dev}$ in Table~\ref{tab:exp_loss}. The findings indicate that focusing predominantly on a single task may cause a deterioration in the performance of both tasks. We hypothesize that with a reduced $\lambda$, the weak supervisory signals in the explanations hinder the model's capacity to generate accurate explanations, thereby compromising the correction learning process. Conversely, a larger $\lambda$ might neglect correction learning, leading to subpar explanation performance since the quality of explanations depends on the predicted corrections in post-explaining models. An optimal $\lambda$ must be chosen to attain a balance between the learning of both tasks and achieve mutually beneficial results.
