\begin{abstract}

Grammatical Error Correction (GEC) faces a critical challenge concerning explainability, notably when GEC systems are designed for language learners. Existing research predominantly focuses on explaining grammatical errors extracted in advance, thus neglecting the relationship between explanations and corrections. To address this gap, we introduce \textbf{EXGEC}, a unified explainable GEC framework that integrates explanation and correction tasks in a generative manner, advocating that these tasks mutually reinforce each other. Experiments have been conducted on EXPECT, a recent human-labeled dataset for explainable GEC, comprising around 20k samples. Moreover, we detect significant noise within EXPECT, potentially compromising model training and evaluation. Therefore, we introduce an alternative dataset named \Dataset{}, ensuring a more objective framework for training and evaluation. Results on various NLP models (BART, T5, and Llama3) show that EXGEC models surpass single-task baselines in both tasks, demonstrating the effectiveness of our approach.



% 2024-11-22 Deprecated
% Grammatical Error Correction (GEC) faces a critical challenge concerning explainability, notably when GEC systems are designed for language learners who frequently struggle to understand the correction outcomes without clear and practical explanations. Existing research predominantly focuses on explaining grammatical errors extracted in advance, thus neglecting the relationship between explanations and corrections. To address this gap, we introduce \textbf{EXGEC}, a unified explainable GEC framework that integrates explanation and correction tasks in a consistent generative approach, advocating that these tasks mutually reinforce each other. Experiments have been conducted on EXPECT, a recent human-labeled dataset for explainable GEC, comprising around 20k samples. Moreover, we detect significant noise within EXPECT, potentially compromising model training and evaluation. As a result, we reconstruct EXPECT to remove noise in 1.82\% training samples, 54.33\% development samples, and 54.76\% testing samples. Subsequently, we introduce an alternative dataset named \Dataset{}, ensuring a more objective framework for training and evaluation in explainable GEC. The results show that when training BART-based EXGEC models on \Dataset{}, the average correction F$_{0.5}$ and explanation F$_{0.5}$ scores are improved by 12.9\% and 4.5\%, indicating that the denoised samples are of significantly good quality. Additionally, when multi-task models concurrently predict corrections and offer explanations, they surpass single-task baseline models in both tasks. Moreover, the framework is general enough to be adapted to various NLP models (BART, T5, Llama3) and has proven effective under different training settings.


% 2024-09-22 Deprecated
% Grammatical Error Correction (GEC) faces a critical challenge concerning explainability, notably when GEC systems are designed for language learners who frequently struggle to understand the correction outcomes without clear and practical explanations. This paper emphasizes two major components of GEC explanations: extractive evidence words and grammatical error types. Nonetheless, existing research predominantly focuses on extracting evidence words and predicting grammatical error types based on a source and/or target sentence, thereby neglecting the relationship between explanations and corrections. To address this gap, we introduce \textbf{EXGEC}, a unified explainable GEC framework that integrates explanation and correction tasks in a sequence-to-sequence generation approach, advocating that these tasks mutually reinforce each other. When multi-task models jointly predict corrections and explanations, their performance in both tasks surpasses that of models trained on each task separately. Comprehensive experiments across various language models (BART, T5, Llama3) validate our method's effectiveness. Furthermore, we identify substantial noise in EXPECT, a recent explainable GEC dataset, which could undermine model training and evaluation. Consequently, we reconstruct EXPECT to eliminate the noise and propose \Dataset{}, ensuring a more objective training and evaluation framework in the field of explainable GEC.
% All source codes and data will be made available post-review.
\end{abstract}
