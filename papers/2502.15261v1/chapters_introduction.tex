\section{Introduction}\label{sec:intro}
Writing proficiently poses significant challenges for language learners who often struggle to produce grammatically correct and coherent texts~\citep{li2024rethinking,DBLP:conf/acl/LiZLLLSWLCZ22,DBLP:conf/emnlp/YeLZLM0023}. Therefore, GEC systems~\citep{bryant2023grammatical} are developed to detect and rectify all grammatical errors in texts~\citep{ye-etal-2023-mixedit,huang-etal-2023-frustratingly}. Research advancements in GEC encompass multi-language~\citep{rothe-etal-2021-simple,DBLP:journals/corr/abs-2307-09007,DBLP:conf/emnlp/MaLSZHZLLLCZS22}, multi-modality~\citep{fang-etal-2023-improving,DBLP:conf/emnlp/LiMZLLHLLC022,DBLP:conf/acl/LiXC0LMJLZZS24}, and document-level~\citep{yuan-bryant-2021-document,DBLP:conf/emnlp/DuW0D0LZVZSZGL024}.


However, the explainability of GEC~\citep{hanawa-etal-2021-exploring,DBLP:journals/corr/abs-2407-00924,DBLP:journals/corr/abs-2407-00934} remains under-explored due to its intrinsic difficulties. Given that neural GEC systems generally function as intricate black-box models, their internal processes are not transparent~\citep{zhao2023explainability}. The absence of explainability can result in inadequacies in educational scenarios, where L2 learners might find it difficult to completely understand outputs from GEC systems without knowing the rationale behind corrections. Providing corrections with explanations fosters appropriate trust by clarifying the linguistic principles and logical mechanisms underpinning model decisions in a comprehensible way, thereby aiding educationally K-12 students and L2 speakers~\citep{bitchener2005effect,sheen2007effect,DBLP:journals/eswa/LiMCHHLZS25,ye2025position}. Moreover, explainability offers insights that help identify unintentional biases and risks for researchers, functioning as a debugging tool to enhance model performance~\citep{ludan-etal-2023-explanation,DBLP:conf/icassp/ZhangLZMLCZ23}.

\input{intro}

The paper focuses on EXPECT~\citep{fei-etal-2023-enhancing}, an explainable GEC dataset characterized by human-labeled \textit{evidence words} and \textit{grammatical error types} annotations, designed to assist language learners in understanding the corrections from GEC systems. These evidence words, referred to as extractive rationales\footnote{Following EXPECT~\citep{fei-etal-2023-enhancing}, we use the term ``evidence words'' throughout the paper except in Section~\ref{sec:related_works}.}, provide precise cues for corrections, thereby enabling learners to comprehend the rationale underlying the corrections. The error types within EXPECT encompass 15 categories grounded in pragmatism~\citep{skehan1998cognitive,shichungui}, facilitating learners in inferring abstract grammatical rules from particular errors through inductive reasoning. However, existing studies~\citep{song-etal-2024-gee} primarily concentrate on post hoc explanation, neglecting the interaction between the explanation and correction tasks as represented in Figure~\ref{fig:intro}.

To explore the interaction between explanation and correction tasks, we introduce \textbf{EXGEC} (\textbf{EX}plainable \textbf{G}rammatical \textbf{E}rror \textbf{C}orrection), a unified multi-task explainable GEC framework that formulates the multi-task problem as a generative task. The framework can jointly correct ungrammatical sentences, extract evidence words, and classify grammatical errors~\cite{zou2025revisiting} in different prediction orders within an architecture. Our research indicates that learning correction and explanation tasks together can be mutually beneficial and the prediction orders affect the task performance. More specifically, pre-explaining models achieve better correction performance but lower explanation performance compared to post-explaining models. Nevertheless, both models show improved or comparable correction and explanation performance compared to their respective baselines.

Moreover, we find that EXPECT is not an ideal dataset for explainable GEC. This is due to the presence of numerous unidentified grammatical errors in EXPECT, which would disturb the extraction of evidence words and the prediction of grammatical errors. Therefore, it will lead to a bias in the training and evaluation process. Consequently, we reconstruct EXPECT to correct the unidentified errors while ensuring each sentence contains only one distinct error~\citep{fei-etal-2023-enhancing}. The resulting dataset is called \Dataset{}. By training and evaluating EXGEC models on our proposed \Dataset{}, we can obtain unbiased results reflecting their real abilities in both the correction and the explanation tasks. In summary, our contributions are three folds:

\begin{itemize}
\item [(1)] We present EXGEC, a comprehensive framework that integrates correction and explanation components. This adaptable design facilitates the investigation of the interplay between correction and explanation tasks when utilizing various prediction sequences.

\item [(2)] We recognize a potential critical limitation in EXPECT and reconstruct it into \Dataset{}, thereby enhancing the training and evaluation framework for EXGEC models.

\item [(3)] We perform extensive experiments employing three language models (BART, T5, and Llama3) to demonstrate the beneficial interaction between the two tasks and substantiate the efficacy of our approach.

\end{itemize}

% The organization of this paper is as follows. Section~\ref{sec:related_works} provides a literature review of existing studies on explainable GEC and relevant techniques. Section~\ref{sec:dataset} delineates the drawback of EXPECT and the construction of the \Dataset{} dataset. Section~\ref{sec:method} presents the details of the presented framework (EXGEC). The experimental design and results are discussed in Section~\ref{sec:experiments}. Section~\ref{sec:analyses} provides a discussion and analysis of the results and limitations of the presented framework. Section~\ref{sec:discussion} discusses the implications and limitations of our work. Finally, Section~\ref{sec:conclusion} presents the conclusion and future work directions.

