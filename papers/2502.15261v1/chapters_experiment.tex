\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Settings}
\paragraph{Backbone models.} We employ the pre-trained language models BART-Large~\citep{lewis-etal-2020-bart} and T5-Base~\citep{raffel2020exploring}, in conjunction with the decoder-only Llama3-8B~\citep{dubey2024llama}, as our foundational models. These models have been demonstrated to serve as effective backbones for state-of-the-art GEC models in prior research~\citep{ye-etal-2023-mixedit,zhang-etal-2023-bidirectional,wang-etal-2024-improving-grammatical}. It is worth noting that tokenization may divide a word into multiple BPE units, thereby complicating the extraction of evidence words. Considering that evidence words are typically concise and contiguous, we mandate that the pointer indices encompass all BPE units of the evidence words. In cases where an instance does not contain an evidence word, the target does not predict any pointer index. The specifics of fine-tuning and the hyperparameter configurations are detailed in Appendix~\ref{app:training_details}.


\paragraph{Training settings.}
As outlined in Section~\ref{subsec:model}, we aim to conduct experiments within four distinct training settings using a single unified framework with minimal adjustments. In line with the standard approach, the \textit{Baseline} and \textit{Explanation} settings train models to generate corrections or explanations from input source sentences. In contrast, the \textit{Infusion} models are trained to generate corrections from input source sentences and human-labeled explanations. Significantly, the \textit{Self-Rationalization} setting is further categorized into two sub-settings based on the sequence of generating corrections and explanations: 
1) \textit{pre-explaining} models produce the explanation first, followed by the correction, whereas 2) \textit{post-explaining} models generate the correction first and then the explanation. Generally, we first extract evidence words and subsequently predict error types, as our preliminary experiments indicate that the prediction order of evidence words and error types does not substantially impact performance.


\paragraph{Baselines.}
We regard single-task models as our baseline models. We denote the single-task correction baseline model by \textit{Baseline} and the single-task explanation baseline model by \textit{Explanation}. Additionally, we integrate BERT~\citep{devlin-etal-2019-bert} as an extra explanation baseline. Self-Rationalization models are comparable to the two baseline models since they use only source sentences as input. On the other hand, Infusion models cannot be compared to any baselines since they incorporate extra explanations in their input.


\paragraph{Evaluation.} We evaluate model performance in our experiments from three critical aspects. 1) \textit{Correction}. Following the conventional evaluation setup of W\&I+LOCNESS dataset~\citep{bryant-etal-2019-bea}, we report correction performance using ERRANT~\citep{bryant-etal-2017-automatic}.
2) \textit{Extraction of evidence words}. Following~\citet{fei-etal-2023-enhancing}, we utilize token-level evaluation metrics such as Precision, Recall, F$_1$, and F$_{0.5}$ but exclude the exact match (EM) metric. Interestingly, we observe that \textit{do-nothing} systems achieve higher EM scores than most well-trained systems, yet they still receive zero F$_1$ and F$_{0.5}$ scores, indicating EM may not be a reliable measure in the task. According to~\citet{fei-etal-2023-enhancing}, the F$_{0.5}$ score has the highest correlation with human evaluation (Pearson's coefficient), followed by the F$_1$ score. 3) \textit{Classification of grammatical errors}. We report label accuracy for grammatical error classification performance. Unlike previous studies~\citep{fei-etal-2023-enhancing}, we separate the evaluation of extraction and classification, offering a clearer view of model performance. Specifically, we consider an evidence word a True Positive (TP) if all its BPEs (or their variants) are detected. This approach deviates from previous evaluations~\citep{fei-etal-2023-enhancing}, which classify an evidence word as TP only when both its BPEs and error type are correctly identified. Results are averaged over three runs with different random seeds on the training set, using \Dataset{}-\textit{dev} as the validation set.

\subsection{Comparison of Original and \Dataset{} Datasets}
\label{subsec:exp_datasets}
We first demonstrate the effectiveness of our reconstruction method. To this end, we independently train models respectively on the original EXPECT dataset and its reconstructed version. As shown in Table~\ref{tab:exp_datasets_detailed}, our \Dataset{} dataset significantly enhances performance across both tasks, with the average correction F$_{0.5}$ and explanation F$_{0.5}$ scores increasing by 12.9\% and 4.5\%, respectively. These improvements underscore the high quality of the denoised samples, which result from detecting and rectifying previously overlooked grammatical errors. Consequently, all subsequent experiments will be conducted using the \Dataset{} dataset.

\input{exp_datasets_detailed}
\input{exp_main}


\subsection{Main Results}
Here, we inspect and evaluate the relationship between the correction and the explanation tasks through various experimental training setups. The main results presented in Table~\ref{tab:exp_main} reveal the following insights.

\paragraph{Evidence words offer more valuable information than grammatical error types for corrections.}
First, we investigate the Infusion setting, where we append different supplementary explanation data to the input source. Infusion models serve as an oracle since human-annotated explanations are typically lacking in real-world applications, allowing us to understand the impact of explanations on the correction task. Recent research has demonstrated that adding human-annotated explanations as additional input can improve task performance~\citep{hase-etal-2020-leakage,yao-etal-2023-human}, and we have observed similar results in the \textit{Infusion} setting. Specifically, we find that the additional information provided by grammatical error types enhances the correction performance of Llama3, but does not positively affect the correction performance of BART and T5. We suspect the significant enhancement of Llama3's correction performance is due to that Llama3 can understand the semantic meaning of error types. In contrast, offering evidence words can consistently boost the correction F$_{0.5}$ scores by 5$\sim$20 points for three different language models, even though only about 60\% of the samples in the dev and test sets are annotated with evidence words, showing that accurate evidence words are very beneficial for the correction task.

\paragraph{Jointly learning the two tasks is advantageous.}
In practical applications, explanations are generally unavailable during inference. Thus, Self-Rationalization models investigate whether incorporating explanations as additional output can enhance performance. The Self-Rationalization models' correction F$_{0.5}$ scores improve by an average of 1.3 points for BART and 0.88 points for T5 compared to the \textit{Baseline} setting. Similarly, explanation F$_{0.5}$ scores increase by an average of 1.3 points for BART, 1.8 points for T5, and 17.7 points for Llama3, compared to the \textit{Explanation} setting. The error classification accuracy scores see an average improvement of 9.8 points for BART, 0.9 points for T5, and 6.8 points for Llama3.

\paragraph{Post-explaining models perform better in the explanation performance.} Experiments reveal variations in performance between models that explain beforehand and those that explain afterward across three language models. We observe that post-explaining models predict evidence words and error types more accurately. This could be attributed to the challenge of predicting evidence words and error types without a specific correction. Furthermore, pre-explaining models exhibit marginally better correction performance for BART, contrasting with the trends observed in other models.

% Empirical studies reveal variations in performance between models that provide explanations prior to prediction and those that do so subsequently, evaluated across three distinct language models. Our observations indicate that models incorporating post-explanation demonstrate superior accuracy in predicting evidence words and error types. This phenomenon may be ascribed to the inherent difficulty of predicting evidence words and error types in the absence of a definitive correction. Additionally, models that furnish pre-explanations show slightly enhanced correction capabilities for BART, in contrast to the patterns discerned in the remaining models.


Furthermore, a BERT model based on sequence labeling is trained under consistent training and evaluation conditions from~\citet{fei-etal-2023-enhancing}. The generative language models exhibit significantly lower performance in grammatical error type classification compared to BERT-based models, which we hypothesize is due to intrinsic biases introduced by the differences in auto-regressive text generation and BERT's masked language model (MLM) pre-training objectives. This hypothesis is supported by the experiments in Section~\ref{subsec:exp_tag}, indicating that sequence labeling is not essential for grammatical error type classification. We report detailed results of EXGEC models in Appendix~\ref{app:detailed_results} and provide an investigation of varying loss weights $\lambda$ in Appendix~\ref{app:loss_weight}.


\section{Analyses}
\label{sec:analyses}
In this section, we adopt BART-Large for further analyses, aiming to provide insights into our framework design and the effects of explanations.

\subsection{Does Sequence Labeling Help?}
\label{subsec:exp_tag}
Motivated by recent studies in multi-task GEC frameworks~\citep{zhao-etal-2019-improving,yuan-etal-2021-multi}, which combine a sequence labeling task with a sentence-level correction task, we also develop a multi-task baseline for explainable GEC, keeping the experimental setup the same as our other experiments. Specifically, we append a randomly initialized tagging head after the encoder to perform the explanation task as a sequence labeling task, like BERT-based models. To predict each token's tag, we pass the encoder's hidden representation $\mathbf{H}^e$ through a softmax after an affine transformation, which is computed as follows:
\begin{equation}\begin{aligned}
P(T \mid X) = \operatorname{softmax}(W^{\top} \mathbf{H}^e),
\end{aligned}\end{equation}
\noindent where $T$ denotes the tagging sequence in the BIO scheme. To replace the pointer mechanism, we employ a token-level sequence labeling task, focusing solely on the correction task in the decoder. We also implement loss weighting to balance the correction generation and sequence labeling losses, defined as follows:
\begin{equation}\begin{aligned}
\mathcal{L} = \mathcal{L}_{cor} + \gamma \cdot \mathcal{L}_{tag},
\end{aligned}\end{equation}
\noindent where $\gamma$ represents the trade-off factor, and we minimize the cross-entropy between predicted tokens/labels and ground truth tokens/labels.

The outcomes of varying $\gamma$, chosen from the alternative set $\{0.5,0.8,1.0,1.5,2.0\}$, are presented in Table~\ref{tab:exp_tag}. Compared to Self-Rationalization models, sequence labeling-based multi-task models demonstrate inferior correction performance; however, they provide an intermediary level of explanation performance between pre-explaining and post-explaining models. So it can be inferred that our proposed EXGEC exhibits greater efficacy than sequence labeling-based baselines.


% Furthermore, by comparing with the generative ``BART Explanation'' baseline in Table~\ref{tab:exp_main}, we can conclude that appending a sequence tagging head to BART's encoder could not significantly improve explanation performance.


\input{exp_tag}
\input{exp_synthesize}

\subsection{Position Leakage}
One might suspect that the improved performance of Infusion models is driven, at least in part, by a positional leakage effect—where evidence words often appear within the first- or second-order nodes of correction words in the dependency parsing tree~\citep{fei-etal-2023-enhancing}. To address this concern, we synthesize datasets with artifact explanations using two methods: (1) \textit{random explanations}, which are randomly sampled from the entire set of source tokens, and (2) \textit{adjacent explanations}, which are randomly chosen from candidate source tokens located within a distance of 1 to 5 tokens from the correction position (noting that over 90\% of evidence words in EXPECT fall within this range).

Given that many samples lack annotated evidence words, we generate a number of synthesized evidence words matching the count of the ground-truth annotations to ensure experimental fairness. While our models are trained using these synthesized evidence words, evaluation is conducted with the ground-truth evidence words. This setup allows us to investigate whether the models can effectively learn to extract evidence words through an unsupervised approach. The results of these experiments are presented in Table~\ref{tab:exp_synthesize}.

Under the Infusion setting, it is unsurprising that incorporating random evidence words does not enhance correction performance as we anticipated. However, we observe that using adjacent synthesized evidence words produces a noticeable impact—yielding a moderate improvement over random evidence words, albeit still falling short of the gains achieved with ground-truth evidence words. This indicates that a positional leakage effect does exist, although it alone cannot fully realize the benefits of ground-truth evidence.

In the pre-explaining and post-explaining settings, generating adjacent evidence words yields a modest improvement in correction performance. However, this gain remains inferior to the enhancement achieved when incorporating ground-truth evidence words, underscoring the critical importance of jointly training the correction and explanation tasks. In contrast, employing random evidence words offers no discernible benefit for the correction task, and further evaluations reveal that their contributions are largely disregarded in the model’s explanations. Notably, the absence of ground-truth evidence words results in a marked decline in explanation performance—reflecting the inherent challenge of aligning model-generated explanations with human preferences in an unsupervised context.

These findings emphasize that high-quality and contextually relevant evidence is essential for improving correction accuracy and producing explanations that resonate with human evaluators.

