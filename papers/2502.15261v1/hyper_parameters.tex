\begin{table*}[!tbh]
\centering
\renewcommand{\arraystretch}{1.2}
% \renewcommand{\tabcolsep}{4pt}
%\caption{Hyper-parameter values used in our experiments.}
\label{tab:hp}
\scalebox{0.80}{
%\begin{tabular}{CLCCC}
\begin{tabular}{clcccc}
\toprule

& \textbf{Configuration}  & \textbf{BART}  & \textbf{T5}  & \textbf{LLaMA3} \\ 
\midrule

\multirow{9}{*}{\textbf{Training}}
& Backbone  & BART-Large & T5-Base & LLaMA3-8B  \\
& Epochs    & 60 & 80 & 3 (LoRA) \\
& Batch size per GPU & 4096 tokens & 4096 tokens & 8192 tokens \\
& Gradient Accumulation & 4 & 4 & 8 \\

\cdashline{2-5}

& Loss weight $\lambda$& \multicolumn{3}{c}{ 1.0 } \\
& Learning rate &  \multicolumn{3}{c}{$3 \times 10^{-5}$ } \\
& Devices   & \multicolumn{3}{c}{1 Tesla A100 GPU (80GB)} \\
& \multirow{2}{*}{Optimizer} & \multicolumn{3}{c}{Adam \citep{kingma2014adam}}  \\
& & \multicolumn{3}{c}{($\beta_1=0.9,\beta_2=0.999,\epsilon=1\times 10^{-8}$) } \\

\midrule

\multirow{2}{*}{\textbf{Inference}}
& Beam size          & 5 & 5 & 5 \\
& Max length         & 256 & 512 & 512 \\

\bottomrule
\end{tabular}}
\caption{Hyper-parameters used in our experiments.}
\label{tab:hyper-parameter}
\end{table*}
