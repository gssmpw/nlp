\section{Motivation and Methodology}
\label{sec:method}

\subsection{Problem Definition}
\label{subsec:problem_definition}
The objective of this study is to simultaneously address the correction and explanation tasks using a Seq2Seq-based generation approach. Specifically, given an ungrammatical input sentence $X = \{x_0, x_1, \cdots, x_n\}$, where $n$ represents the length of the input sentence, the joint models are designed to learn both tasks. The correction task involves converting the ungrammatical input into a grammatical output $Y = \{y_0, y_1, \cdots, y_m\}$, where $m$ denotes the length of the output. The explanation task is divided into two sub-tasks: 1) \textbf{classifying} grammatical errors, and 2) \textbf{extracting} evidence words. For the classification sub-task, the joint models are required to output a grammatical error type label $c$ ($c \in C$), where $C$ consists of 15 possible grammatical error type classes as defined in EXPECT. For the extraction sub-task, the models must extract evidence words $E(X) = \{e_0, e_1, \cdots, e_k\} \subset X$ that provide clear and comprehensive clues for corrections.

\input{figure_overview}

\subsection{Explainable GEC as Generation Task}
\label{subsec:model}
We introduce four distinct training settings to examine the interaction between explanation and correction tasks during the training phase, as illustrated in Figure~\ref{fig:overview}. These configurations are as follows: 1) the absence of explanations (\textit{Baseline}), which represents the conventional GEC setup, 2) the integration of explanations as supplementary input (\textit{Infusion}), 3) the generation of explanations as output (\textit{Explanation}), and 4) the inclusion of explanations as an auxiliary output (\textit{Self-Rationalization}).

This paper presents EXGEC, a unified generative framework designed for explainable grammatical error correction, which seamlessly integrates multiple settings within a singular architecture. In the Infusion setting, a special token, ``<sep>'', acts as a delimiter between the source sentence and the ensuing explanation, comprising evidence words and the identified error type. Conversely, in the Explanation setting, the model derives an explanation based solely on the information provided by the source sentence. Within the Self-Rationalization paradigm, the models concurrently generate both a correction and an explanation. The Self-Rationalization setting is depicted in detail in Figure~\ref{fig:overview}. Other settings can be adapted with minimal modifications. Moreover, the sequence in which corrections and explanations are predicted can be altered, facilitating a deeper understanding of task interactions.


We first describe how our EXGEC addresses tasks in a unified generative framework within the Self-Rationalization setting. Utilizing a pointing mechanism~\citep{vinyals2015pointer}, EXGEC can identify evidence words by directly generating the source indices of an ungrammatical sentence, thus eliminating the possibility of generating invalid evidence words. Given an ungrammatical source sentence $X$, the encoder translates $X$ into a hidden representation $\mathbf H$ as follows:
\begin{equation}
\mathbf{H}^e = \operatorname{Encoder}(X),
\end{equation}
\noindent where $\mathbf{H}^e\in\mathbb{R}^{n\times d}$, and $d$ is the hidden size. At each time step $t$, the decoder produces the hidden state $\mathbf{h}^d_t\in\mathbb{R}^d$ based on the previous output sequence $\hat{Y}_{<t}$, which is computed as follows:
\begin{equation}
\mathbf{h}^d_t = \operatorname{Decoder}(\mathbf{H}^e,\hat{Y}_{<t}).
\end{equation}

Next, the hidden state $\mathbf{h}^d_t\in\mathbb{R}^d$ is utilized to compute three forms of logits: 1) \textit{correction token logits}, responsible for the correction phase~\citep{vaswani2017attention}, 2) \textit{evidence pointer logits}, which determine the probabilities of source indices for evidence extraction, and 3) \textit{error type logits}, used for classifying error types.
Drawing inspiration from~\citet{yan-etal-2021-unified-generative}, we add an extra MLP layer~\cite{DBLP:journals/patterns/LiuLTLZ22} and calculate the probability distribution $P_t$ as follows:
\allowdisplaybreaks
\begin{align}
\mathbf{E}^e &= \operatorname{TokenEmbed}(X) \in \mathbb{R}^{n\times d}, \\[10pt]
\bar{\mathbf{H}}^e &= \alpha \mathbf{E}^e + (1-\alpha) \operatorname{MLP}(\mathbf{H}^e) \in \mathbb{R}^{n\times d}, \\[10pt]
\mathbf{V}^d &= \operatorname{TokenEmbed}(V) \in \mathbb{R}^{|V|\times d}, \\[10pt]
\mathbf{C}^d &= \operatorname{TypeEmbed}(C) \in \mathbb{R}^{|C|\times d}, \\[10pt]
P_t &= \operatorname{softmax}([\mathbf{V}^d \otimes \mathbf{h}^d_t; \bar{\mathbf{H}}^e \otimes \mathbf{h}^d_t; \mathbf{C}^d \otimes \mathbf{h}^d_t]),
\end{align}
\noindent where TokenEmbed refers to the embeddings that are shared between the encoder and decoder, $\alpha \in \mathbb{R}$ is a hyper-parameter responsible for balancing the trade-off between embeddings and encoder hidden representation, $V$ represents the token vocabulary, $[\cdot \ ; \ \cdot]$ denotes the concatenation operation in the first dimension, the symbol $\otimes$ means the dot product operation, and
$P_t\in\mathbb{R}^{|V| + n + |C|}$ represents the probability distribution at the current time step $t$.

The pointer index cannot be directly inputted into the decoder. So we utilize the Index2Token conversion to transform the indexes into tokens~\citep{yan-etal-2021-unified-generative}. Furthermore, the sequence of generating corrections and explanations can be restructured, potentially offering valuable insights into the deeper understanding of the interaction between both tasks. The probability distribution of Baseline and Infusion models is restricted to the token vocabulary. Conversely, the probability distribution of Explanation models is confined to the combination of pointer indexes and error types.


\subsection{Loss Weighting}
\label{subsec:loss_weighting}

Taking into account the heterogeneity of correction and explanation tasks, we construct the overall loss function in the form of a weighted sum, which is defined as follows:
\begin{equation}\begin{small}\begin{aligned}
\mathcal{L} &= \mathcal{L}_{cor} + \lambda \cdot \mathcal{L}_{exp} \\
&= -\sum^m_{i=0} \Big[ \mathbb{I}(y_i \in V)\log p_i + \lambda \mathbb{I}(y_i \not\in V) \log p_i \Big],
\end{aligned}\label{eq:loss}\end{small}\end{equation}
\noindent where $\lambda$ is responsible for balancing both tasks and $\mathbb{I}$ is the indicator function. During the inference stage, we generate the entire target sequence in an autoregressive manner and then separate different parts from the target.
