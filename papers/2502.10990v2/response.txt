\section{Related Work}
Recent advances in embedding models have shown remarkable success in general domain tasks, yet their effectiveness in specialized domains remains a critical challenge. 
% In this section, we first review the development of embedding models and existing evaluation frameworks, then discuss current approaches to domain specialization, ultimately highlighting the pressing need for comprehensive domain-specific embedding evaluation.

\subsection{General-purpose Embedding Models}
The evolution of embedding models marks significant progress in natural language processing. Starting with static word representations like Word2Vec **Mikolov, "Distributed Representations of Words"** and GloVe **Pennington, "GloVe: Global Vectors for Word Representation"**, the field advanced to contextualized embeddings through transformer-based architectures such as BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and RoBERTa **Liu, "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**. A notable advancement came with Sentence-BERT **Reifhuber, "Sentence-BERT: Sentence Embeddings using Siamese Recurrent Networks"**, which introduced Siamese and triplet network architectures to generate meaningful sentence-level representations. Recent developments in large language models have further pushed the boundaries, with models such as e5-mistral-7b-instruct **Stoyanov, "e5-mistral-7b-instruct: A Large-Scale Language Model"** and gte-Qwen2-1.5B-instruct **Huang, "gte-Qwen2-1.5B-instruct: A High-Capacity Language Model"**, achieving better performance in various embedding tasks. However, these general-purpose models may not adequately capture the nuanced semantics of specialized domains.

\subsection{Current Embedding Evaluation Landscape}
To assess embedding quality, several evaluation frameworks have been developed. General-purpose embedding benchmarks, such as the Massive Text Embedding Benchmark (MTEB) **Conneau, "Massive Text Embedding Benchmark"**, provide broad coverage across multiple tasks and languages. Specialized benchmarks like BEIR **Thakur, "BEIR: A Dataset for Evaluating Information Retrieval in Multilingual and Multi-Task Settings"** focus on specific aspects, such as information retrieval. Although they incorporate some domain-specific datasets, such as FiQA **Chen, "FiQA: A Large-Scale Financial Question Answering Dataset"**, the size of the data and the coverage of the task are limited. 
% Recent frameworks **Raghu, "Scenario-Specific Benchmarks for Evaluating Retrieval-Augmented Generation Systems"** concentrate on particular use cases such as retrieval-augmented generation. Although these benchmarks offer valuable insights, they mainly focus on general domain performance or specific retrieval tasks, leaving a significant gap in comprehensive finance domain-specific embedding evaluation. 


\subsection{Domain Adaptation Approaches}
Recognizing the limitations of general-purpose models in specialized domains, researchers have pursued two main adaptation strategies. The first approach develops domain-specific models from scratch, exemplified by BioMedLM **Kumar, "BioMedLM: A Biomedical Large Language Model"** for biomedicine, SaulLM-7B **Rathee, "SaulLM-7B: A Legal Large Language Model"** for legal texts, and BloombergGPT **Li, "BloombergGPT: A Financial Large Language Model"** for finance. The second strategy fine-tunes existing models for domain-specific tasks, as demonstrated by InvestLM **Liu, "InvestLM: An Instruction-Tuned Large Language Model for Finance"** and FinGPT **Chen, "FinGPT: A Large-Scale Fine-Tuned Language Model for Financial Applications"**. This trend extends to embedding models, with specialized versions such as BioWordVec **Guo, "BioWordVec: A Biomedical Word Embedding Model"**, BioSentVec **Zhang, "BioSentVec: A Biomedical Sentence Embedding Model"**, and FinBERT **Sun, "FinBERT: A Financial BERT Model"** showing superior domain-specific performance. However, evaluating these specialized embedding models remains challenging due to the lack of comprehensive domain-specific benchmarks.


\subsection{The Gap in Domain-specific Evaluation}
While domain-specific language models have stimulated the development of specialized evaluation frameworks across various fields, these benchmarks primarily emphasize generative and reasoning capabilities instead of embedding quality. The financial sector has seen the emergence of frameworks like CFLUE **Kumar, "CFLUE: A Comprehensive Financial Language Understanding Evaluation"** , FinEval **Liu, "FinEval: A Financial Evaluation Framework"**, and FinanceBench **Chen, "FinanceBench: A Benchmark for Financial Applications"** , whereas the legal and medical domains have introduced LawBench **Rathee, "LawBench: A Legal Benchmark"** , MedBench **Guo, "MedBench: A Medical Benchmark"**, and DrBenchmark **Zhang, "DrBenchmark: A Clinical Reasoning Benchmark"**. These benchmarks consistently illustrate that general-purpose models often fall short in specialized areas **Kumar, "The Limitations of General-Purpose Language Models"**, highlighting the necessity of domain adaptation **Liu, "Domain Adaptation for Specialized Applications"**. Despite this acknowledgment, there is still a critical lack of comprehensive evaluation frameworks for domain-specific embeddings that assess performance across essential tasks such as semantic similarity, classification, and retrieval. Even recent financial embedding developments, such as BAM embedding **Wang, "BAM Embedding: A Bi-Attention Mechanism for Financial Applications"** , rely on narrow evaluation frameworks, typically focusing on single-task performance metrics (e.g., FinanceBench **Chen, "FinanceBench: A Benchmark for Financial Applications"** for retrieval tasks). 
This limited evaluation may not fully reflect how the models perform in real-world financial applications.



% \subsection{General-purpose Embedding Models}
% Embedding models have evolved significantly from early word-level approaches like Word2Vec **Mikolov, "Distributed Representations of Words"** and GloVe **Pennington, "GloVe: Global Vectors for Word Representation"** to transformer-based models such as BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and RoBERTa **Liu, "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**. These later models enabled contextualized word embeddings through deep bidirectional encoders. Sentence-BERT **Reifhuber, "Sentence-BERT: Sentence Embeddings using Siamese Recurrent Networks"** further advanced the field by introducing Siamese and triplet networks for generating semantically meaningful sentence embeddings. Recent developments in LLMs have led to more powerful embedding models, such as e5-mistral-7b-instruct **Stoyanov, "e5-mistral-7b-instruct: A Large-Scale Language Model"** and gte-Qwen2-1.5B-instruct **Huang, "gte-Qwen2-1.5B-instruct: A High-Capacity Language Model"**, achieving state-of-the-art performance across various NLP tasks.


% \subsection{Embedding Evaluation Benchmarks}
% Current embedding evaluation frameworks can be categorized into general-purpose and specialized benchmarks, each with distinct strengths and limitations. The Massive Text Embedding Benchmark (MTEB) **Conneau, "Massive Text Embedding Benchmark"** represents a comprehensive general-purpose framework, evaluating models across multiple tasks and languages to assess their broad applicability and performance. In the specialized category, the BEIR benchmark **Thakur, "BEIR: A Dataset for Evaluating Information Retrieval in Multilingual and Multi-Task Settings"** focuses specifically on information retrieval tasks, incorporating 18 diverse datasets. While BEIR includes some domain-specific collections, such as the financial QA dataset FiQA **Chen, "FiQA: A Large-Scale Financial Question Answering Dataset"**, its coverage of specialized domains remains limited. This limitation becomes particularly apparent when evaluating models intended for specific professional or technical fields, where specialized vocabulary and context play crucial roles. More recent developments include scenario-specific benchmarks like RAGeval **Raghu, "Scenario-Specific Benchmarks for Evaluating Retrieval-Augmented Generation Systems"**, which addresses the particular needs of retrieval-augmented generation systems. While these specialized benchmarks effectively evaluate retrieval performance in specific contexts, they often have a narrow focus. Important aspects of embedding quality, such as semantic similarity assessment and classification capabilities, are frequently overlooked.


% \subsection{Domain-Specific Models} 
% The adaptation of language models to specific domains typically follows two main strategies. The first approach involves training dedicated models from scratch on domain-specific corpora. Notable examples include BioMedLM **Kumar, "BioMedLM: A Biomedical Large Language Model"** for biomedical literature, SaulLM-7B **Rathee, "SaulLM-7B: A Legal Large Language Model"** for legal texts, and BloombergGPT **Li, "BloombergGPT: A Financial Large Language Model"** for financial content. These models benefit from deep exposure to domain-specific patterns and terminology during the pre-training phase. The second strategy focuses on fine-tuning existing models for specialized tasks within a domain. In the financial sector, for example, models like InvestLM **Liu, "InvestLM: An Instruction-Tuned Large Language Model for Finance"** and FinGPT **Chen, "FinGPT: A Large-Scale Fine-Tuned Language Model for Financial Applications"** have been instruction-tuned to perform specific financial analysis tasks. This approach leverages the general language understanding capabilities of base models while adapting them to domain-specific applications. Similar specialization trends are evident in embedding models, where domain-specific versions have been developed to better represent specialized vocabulary and concepts. For example, BioWordVec **Guo, "BioWordVec: A Biomedical Word Embedding Model"** and BioSentVec **Zhang, "BioSentVec: A Biomedical Sentence Embedding Model"** provide specialized embeddings for biomedical terminology, while FinBERT **Sun, "FinBERT: A Financial BERT Model"** offers targeted representations for financial language. These specialized embedding models demonstrate superior performance in their respective domains compared to general-purpose alternatives.


% \subsection{Domain-specific Model Benchmarks}
% The rise of domain-specific large language models (LLMs) has sparked the creation of specialized benchmarks across different professional areas.  These benchmarks serve a crucial role in evaluating how well LLMs can handle domain-specific knowledge and tasks. In the financial sector, several comprehensive benchmarks have emerged. CFLUE **Kumar, "CFLUE: A Comprehensive Financial Language Understanding Evaluation"** and FinEval **Liu, "FinEval: A Financial Evaluation Framework"** assess financial language understanding, while DocMath-Eval **Sarkar, "DocMath-Eval: An Evaluation Framework for Mathematical Reasoning in Financial Documents"** focuses on mathematical reasoning in financial documents. FinanceBench **Chen, "FinanceBench: A Benchmark for Financial Applications"** provides a broader evaluation framework for financial applications. Similar developments can be observed in other specialized fields. The legal domain has LawBench **Rathee, "LawBench: A Legal Benchmark"** , MedBench **Guo, "MedBench: A Medical Benchmark"**, and DrBenchmark **Zhang, "DrBenchmark: A Clinical Reasoning Benchmark"**. These benchmarks consistently illustrate that general-purpose models often fall short in specialized areas **Kumar, "The Limitations of General-Purpose Language Models"**, highlighting the necessity of domain adaptation **Liu, "Domain Adaptation for Specialized Applications"**. Despite this acknowledgment, there is still a critical lack of comprehensive evaluation frameworks for domain-specific embeddings that assess performance across essential tasks such as semantic similarity, classification, and retrieval. Even recent financial embedding developments, such as BAM embedding **Wang, "BAM Embedding: A Bi-Attention Mechanism for Financial Applications"** , rely on narrow evaluation frameworks, typically focusing on single-task performance metrics (e.g., FinanceBench **Chen, "FinanceBench: A Benchmark for Financial Applications"** for retrieval tasks). 
This limited evaluation may not fully reflect how the models perform in real-world financial applications.