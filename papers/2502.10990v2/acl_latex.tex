% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs} 
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{array} 
\usepackage{comment}
\usepackage{adjustbox}
\usepackage{float}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
% \usepackage{filecontents}
\usepackage{subcaption}

\newcommand{\refine}[1]{\textcolor{blue}{{\bf #1}}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\title{FinMTEB: Finance Massive Text Embedding Benchmark}
\newcommand\modelname{Fin-E5}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Yixuan Tang , Yi Yang \\
The Hong Kong University of Science and Technology\\
\texttt{ytangch@connect.ust.hk, imyiyang@ust.hk}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the \textbf{Fin}ance \textbf{M}assive \textbf{T}ext \textbf{E}mbedding \textbf{B}enchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, Fin-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. 
Through extensive evaluation of 15 embedding models, including Fin-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.
\footnote{Github: https://github.com/yixuantt/FinMTEB}

\end{abstract}

\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/wordcloud.pdf}
    \caption{Word cloud visualization of Fin-E5's training data, contain common financial terms.}
    \label{fig:wordcloud}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=.9\linewidth]{fig/overview.pdf}
    \caption{An overview of tasks and datasets used in FinMTEB. All the dataset descriptions and examples are provided in the Appendix \ref{append: datasets}. }
    \label{fig: overview}
\end{figure*}
Embedding models, which transform text sequences into dense vector representations, serve as fundamental building blocks in natural language processing (NLP) tasks \citep{word2vector, pennington-etal-2014-glove, peters-etal-2018-deep}. The quality of text embeddings directly impacts the effectiveness of information retrieval, semantic understanding, and other downstream applications. Although recent large language model (LLM)-based embedding models have shown remarkable performance on general benchmarks \citep{e5,gte,SFR-embedding-2}, their effectiveness in specialized domains, particularly finance, remains understudied. Financial text analysis requires precise handling of domain-specific terminology, temporal sensitivity, and complex numerical relationships \citep{alphafin,anderson-etal-2024-finance_text_embedding}. This raises two critical question: 
\begin{itemize}
    \item How effectively do modern embedding models capture domain-specific financial information?
    \item Can domain adaptation improve LLM-based embeddings for financial applications?
\end{itemize}


% \textit{How effectively do modern embedding models capture and represent domain-specific financial information?}

% Embedding models, which transform text sequences into dense vector representations, serve as fundamental building blocks in natural language processing (NLP) tasks \citep{word2vector, pennington-etal-2014-glove, peters-etal-2018-deep}. The quality of text embeddings directly impacts the effectiveness of information retrieval, semantic understanding, and other downstream applications. Although recent large language model (LLM)-based embedding models have achieved remarkable performance on general benchmarks \citep{e5,gte,SFR-embedding-2}, their effectiveness in specialized domains remains understudied. This raises a critical question: \textit{How do modern embedding models perform on domain-specific tasks?}

These questions are motivated by three key insights. First, financial semantics often diverge from general language usage. For example, the term \textit{"liability"} inherently conveys negative sentiment in financial contexts due to its association with obligations and risks, whereas in general usage, it neutrally denotes legal responsibility. Such semantic divergence becomes particularly crucial for real-world applications such as Retrieval Augmented Generation (RAG) systems, where accurate document retrieval underpins effective knowledge enhancement. While recent work adapts RAG frameworks for finance \citep{alphafin,malandri-etal-2025-fin}, the fundamental role of embedding quality in retrieval performance remains overlooked.


Second, empirical evidence increasingly suggests that domain adaptation is crucial for achieving optimal performance in specialized fields~\citep{domainiskey,dont_stop}, even with recent advanced LLMs. This necessity for domain specialization has led to the development of field-specific models across various domains: BiMedLM \citep{biomedlm} for biomedical texts, SaulLM-7B \citep{saullm} for legal documents, and BloombergGPT \citep{bloomberggpt} for financial applications. This specialization trend extends to embedding models, where domain-specific variants have demonstrated superior performance in capturing specialized vocabulary and semantic relationships. For instance, BioWordVec \citep{biowordvec} and BioSentVec \citep{biosentvec} are used in biomedical text analysis, while FinBERT \citep{finbert} shows promising results in financial applications. 
However, while financial domain embedding models have shown promising improvement (e.g., BAM~\citep{anderson-etal-2024-finance_text_embedding}, a RoBERTa-based~\citep{liu2019roberta} model outperforming the general model in retrieval tasks), they are still based on traditional architectures. Compared to the general domain, there is a gap in the current landscape for finance NLP: \textbf{while commercial solutions like voyage-finance-2 \citep{voyage} exist, there remains a lack of open-source LLM-based financial embedding models available to researchers.}

Third, financial NLP lacks comprehensive evaluation frameworks for embedding models. Current benchmarks \citep{financebench, finqa} primarily assess text generation rather than embedding quality. Even embedding-specific evaluations \citep{FiQA,finsts} focus narrowly on single task types (e.g., classification) or limited text genres (e.g., earnings call transcripts). This gap is deepened by financial texts' unique characteristics, such as the prevalence of boilerplate language (e.g., \textit{"The company's performance is subject to various risks..."}) that creates noise in semantic representation. These standardized legal disclaimers appear frequently across documents but offer little information, complicating the models' ability to differentiate meaningful business insights from routine compliance text. Thus, there is a critical need for comprehensive financial embedding benchmarks. 

% Third, while financial institutions increasingly rely on text embeddings for critical applications like sentiment analysis and document similarity matching, there remains a notable absence of comprehensive evaluation frameworks specifically designed for embedding models in this domain. This gap is particularly concerning given the unique linguistic characteristics of financial texts. For example, corporate disclosures frequently contain repetitive boilerplate statements (e.g., \textit{"The company's operations and financial performance are subject to various risks and uncertainties..."}) that carry minimal informational value despite their high frequency. Such patterns create a false signal abundance that challenges models' ability to distinguish substantive business insights from routine legal disclaimers.

% Existing financial NLP benchmarks like FinanceBench \citep{financebench} and FinQA \citep{finqa} partially address this issue but focus predominantly on text generation tasks rather than embedding quality assessment. Meanwhile, embedding-specific evaluations in finance \citep{FiQA,finsts} tend to adopt narrow scopes, either targeting singular task types (e.g., retrieval accuracy) or limited text genres (e.g., earnings call transcripts). This isolated approach fails to provide practitioners with actionable insights about how embedding models generalize across diverse financial embedding tasks.

% Third, there is limited work on benchmarking the performance of embedding models on finance domain corpora across different tasks but the evaluation of embedding models in the finance domain is very critical. Financial institutions widely utilize text embeddings for tasks such as sentiment analysis and document similarity matching. Furthermore, finance texts present unique challenges due to their specialized language patterns and domain knowledge requirements. For instance, financial reports often contain boilerplate statements like \textit{"The company's operations and financial performance are subject to various risks and uncertainties..."}. These standardized legal disclaimers appear frequently across documents but offer little information, complicating the models' ability to differentiate meaningful business insights from routine compliance text. Besides, current evaluation approaches in finance primarily focus on specific tasks or text types instead of providing a comprehensive assessment framework~\citep{FiQA,financebench,finsts}. the absence of comprehensive embedding-specific evaluation frameworks, particularly in the financial domain, also presents a significant limitation in assessing the quality and utility of domain-specific embedding models across different tasks. while domain-specific evaluation frameworks exist, such as FinanceBench \citep{financebench} and FinQA \citep{finqa}, these benchmarks primarily focus on generation tasks. 

% Second, empirical evidence increasingly suggests that domain adaptation is crucial for achieving optimal performance in specialized fields~\citep{domainiskey,dont_stop}, even with recent advanced LLMs. This is exemplified by models such as BiMedLM \citep{biomedlm} for biomedical texts, SaulLM-7B \citep{saullm} for legal documents, and BloombergGPT \citep{bloomberggpt} for financial applications. This specialization trend extends to embedding models, where domain-specific variants have been developed to better capture specialized vocabulary and concepts. Examples include BioWordVec \citep{biowordvec} and BioSentVec \citep{biosentvec} for biomedical corpus, and FinBERT \citep{finbert} for financial language. \textbf{However, to our knowledge, aside from the commercial model voyage-finance-2~\citep{voyage}, there are no available open-source financial LLM-based embedding models nowadays.} There are also many specialized evaluation benchmarks, such as LawBench \citep{lawbench}, MedEval \citep{medbench}, and FinanceBench \citep{financebench}. However, there are two limitations. those benchmarks are still designed for generation tasks. To our knowledge, there is limited work on benchmarking the performance of embedding models on domain corpora across different tasks, especially in finance. 

% The evaluation of embedding models in the finance domain is also critical. Financial institutions widely utilize text embeddings for tasks such as sentiment analysis and document similarity matching. Furthermore, finance texts present unique challenges due to their specialized language patterns and domain knowledge requirements. For instance, financial reports often contain boilerplate statements like \textit{"The company's operations and financial performance are subject to various risks and uncertainties..."}. These standardized legal disclaimers appear frequently across documents but offer little information, complicating the models' ability to differentiate meaningful business insights from routine compliance text. Besides, current evaluation approaches in finance primarily focus on specific tasks or text types instead of providing a comprehensive assessment framework~\citep{FiQA,financebench,finsts}.


To bridge this gap, we introduce the \textbf{Fin}ance \textbf{M}assive \textbf{T}ext \textbf{E}mbedding \textbf{B}enchmark (FinMTEB), a comprehensive evaluation framework specialized for the financial domain. FinMTEB comprises 64 domain-specific datasets that span both Chinese and English, covering seven distinct tasks: classification, clustering, retrieval, pair classification, reranking, summarization, and semantic textual similarity. We also develop Fin-E5, a finance-adapted version of e5-Mistral-7B-Instruct~\citep{e5}, utilizing a persona-based data synthesis method. As shown in Figure \ref{fig:wordcloud}, our training data encompasses a diverse range of financial topics concepts. Experimental results show that LLM-based embedding models consistently outperform traditional approaches, while domain adaptation further improves performance.  Interestingly, in the STS task, we find that the simple Bag-of-Words (BoW) model outperforms all dense models. This indicates that current embedding models still encounter difficulties in interpreting complex financial texts. 

Our main contributions are twofold: First, we propose FinMTEB, the first comprehensive financial domain evaluation benchmark encompassing 64 datasets across seven distinct tasks in both Chinese and English. Second, we develop and release Fin-E5, a finance-adapted embedding model that achieves state-of-the-art performance on FinMTEB. To support future research, we will make both the FinMTEB benchmark and our Fin-E5 model available as open source. 

% \begin{enumerate}

% \item We propose FinMTEB, the first comprehensive financial domain evaluation benchmark covering 64 datasets across seven distinct tasks in both Chinese and English;

% \item We develop and release Fin-E5, a finance-adapted embedding model that achieves state-of-the-art performance on FinMTEB. 

% \item We release a persona-based financial training dataset, specifically curated to enhance financial task awareness in embedding models.

% \item Through extensive experiments, we quantitatively demonstrate the importance of domain-specific evaluation;

% \end{enumerate}


\section{Related Work}

Recent advances in embedding models have shown remarkable success in general domain tasks, yet their effectiveness in specialized domains remains a critical challenge. 
% In this section, we first review the development of embedding models and existing evaluation frameworks, then discuss current approaches to domain specialization, ultimately highlighting the pressing need for comprehensive domain-specific embedding evaluation.

\subsection{General-purpose Embedding Models}
The evolution of embedding models marks significant progress in natural language processing. Starting with static word representations like Word2Vec \citep{word2vector} and GloVe \citep{pennington-etal-2014-glove}, the field advanced to contextualized embeddings through transformer-based architectures such as BERT \citep{Bert} and RoBERTa \citep{roberta}. A notable advancement came with Sentence-BERT \citep{sentence-bert}, which introduced Siamese and triplet network architectures to generate meaningful sentence-level representations. Recent developments in large language models have further pushed the boundaries, with models such as e5-mistral-7b-instruct \citep{e5} and gte-Qwen2-1.5B-instruct \citep{qwen2} achieving better performance in various embedding tasks. However, these general-purpose models may not adequately capture the nuanced semantics of specialized domains.

\subsection{Current Embedding Evaluation Landscape}
To assess embedding quality, several evaluation frameworks have been developed. General-purpose embedding benchmarks, such as the Massive Text Embedding Benchmark (MTEB) \citep{mteb}, provide broad coverage across multiple tasks and languages. Specialized benchmarks like BEIR \citep{beir} focus on specific aspects, such as information retrieval. Although they incorporate some domain-specific datasets, such as FiQA \citep{FiQA}, the size of the data and the coverage of the task are limited. 
% Recent frameworks \citep{rageval} concentrate on particular use cases such as retrieval-augmented generation. Although these benchmarks offer valuable insights, they mainly focus on general domain performance or specific retrieval tasks, leaving a significant gap in comprehensive finance domain-specific embedding evaluation. 


\subsection{Domain Adaptation Approaches}
Recognizing the limitations of general-purpose models in specialized domains, researchers have pursued two main adaptation strategies. The first approach develops domain-specific models from scratch, exemplified by BioMedLM \citep{biomedlm} for biomedicine, SaulLM-7B \citep{saullm} for legal texts, and BloombergGPT \citep{bloomberggpt} for finance. The second strategy fine-tunes existing models for domain-specific tasks, as demonstrated by InvestLM \citep{investlm} and FinGPT \citep{fingpt}. This trend extends to embedding models, with specialized versions such as BioWordVec \citep{biowordvec}, BioSentVec \citep{biosentvec}, and FinBERT \citep{finbert} showing superior domain-specific performance. However, evaluating these specialized embedding models remains challenging due to the lack of comprehensive domain-specific benchmarks.


\subsection{The Gap in Domain-specific Evaluation}
While domain-specific language models have stimulated the development of specialized evaluation frameworks across various fields, these benchmarks primarily emphasize generative and reasoning capabilities instead of embedding quality. The financial sector has seen the emergence of frameworks like CFLUE \citep{CFLUE}, FinEval \citep{fineval}, and FinanceBench \citep{financebench}, whereas the legal and medical domains have introduced LawBench \citep{lawbench}, MedBench \citep{medbench}, and DrBenchmark \citep{drbenchmark}. These benchmarks consistently illustrate that general-purpose models often fall short in specialized areas \citep{CFLUE, lawbench}, highlighting the necessity of domain adaptation \citep{domainiskey}. Despite this acknowledgment, there is still a critical lack of comprehensive evaluation frameworks for domain-specific embeddings that assess performance across essential tasks such as semantic similarity, classification, and retrieval. Even recent financial embedding developments, such as BAM embedding~\citep{anderson-etal-2024-finance_text_embedding}, rely on narrow evaluation frameworks, typically focusing on single-task performance metrics (e.g., FinanceBench~\citep{financebench} for retrieval tasks). 
This limited evaluation may not fully reflect how the models perform in real-world financial applications.



% \subsection{General-purpose Embedding Models}
% Embedding models have evolved significantly from early word-level approaches like Word2Vec \citep{word2vector} and GloVe \citep{pennington-etal-2014-glove} to transformer-based models such as BERT \citep{Bert} and RoBERTa \citep{roberta}. These later models enabled contextualized word embeddings through deep bidirectional encoders. Sentence-BERT \citep{sentence-bert} further advanced the field by introducing Siamese and triplet networks for generating semantically meaningful sentence embeddings. Recent developments in LLMs have led to more powerful embedding models, such as e5-mistral-7b-instruct \citep{e5} and gte-Qwen2-1.5B-instruct \citep{qwen2}, achieving state-of-the-art performance across various NLP tasks.


% \subsection{Embedding Evaluation Benchmarks}
% Current embedding evaluation frameworks can be categorized into general-purpose and specialized benchmarks, each with distinct strengths and limitations. The Massive Text Embedding Benchmark (MTEB) \citep{mteb} represents a comprehensive general-purpose framework, evaluating models across multiple tasks and languages to assess their broad applicability and performance. In the specialized category, the BEIR benchmark \citep{beir} focuses specifically on information retrieval tasks, incorporating 18 diverse datasets. While BEIR includes some domain-specific collections, such as the financial QA dataset FiQA \citep{FiQA}, its coverage of specialized domains remains limited. This limitation becomes particularly apparent when evaluating models intended for specific professional or technical fields, where specialized vocabulary and context play crucial roles. More recent developments include scenario-specific benchmarks like RAGeval \citep{rageval}, which addresses the particular needs of retrieval-augmented generation systems. While these specialized benchmarks effectively evaluate retrieval performance in specific contexts, they often have a narrow focus. Important aspects of embedding quality, such as semantic similarity assessment and classification capabilities, are frequently overlooked.


% \subsection{Domain-Specific Models} 
% The adaptation of language models to specific domains typically follows two main strategies. The first approach involves training dedicated models from scratch on domain-specific corpora. Notable examples include BioMedLM \citep{biomedlm} for biomedical literature, SaulLM-7B \citep{saullm} for legal texts, and BloombergGPT \citep{bloomberggpt} for financial content. These models benefit from deep exposure to domain-specific patterns and terminology during the pre-training phase. The second strategy focuses on fine-tuning existing models for specialized tasks within a domain. In the financial sector, for example, models like InvestLM \citep{investlm} and FinGPT \citep{fingpt} have been instruction-tuned to perform specific financial analysis tasks. This approach leverages the general language understanding capabilities of base models while adapting them to domain-specific applications. Similar specialization trends are evident in embedding models, where domain-specific versions have been developed to better represent specialized vocabulary and concepts. For example, BioWordVec \citep{biowordvec} and BioSentVec \citep{biosentvec} provide specialized embeddings for biomedical terminology, while FinBERT \citep{finbert} offers targeted representations for financial language. These specialized embedding models demonstrate superior performance in their respective domains compared to general-purpose alternatives.


% \subsection{Domain-specific Model Benchmarks}
% The rise of domain-specific large language models (LLMs) has sparked the creation of specialized benchmarks across different professional areas.  These benchmarks serve a crucial role in evaluating how well LLMs can handle domain-specific knowledge and tasks. In the financial sector, several comprehensive benchmarks have emerged. CFLUE \citep{CFLUE} and FinEval \citep{fineval} assess financial language understanding, while DocMath-Eval \citep{DocMath-Eval} focuses on mathematical reasoning in financial documents. FinanceBench \citep{financebench} provides a broader evaluation framework for financial applications. Similar developments can be observed in other specialized fields. The legal domain has LawBench \citep{lawbench}, which evaluates legal reasoning and comprehension across multiple jurisdictions. In healthcare, MedBench \citep{medbench}, MedEval \citep{medeval}, and DrBenchmark \citep{drbenchmark} have been established to assess medical knowledge understanding and clinical reasoning capabilities. Most of these benchmarking papers conclude that general-purpose LLMs may fall short on domain tasks \citep{CFLUE, lawbench}. The importance of domain adaptation has gradually gained attention \citep{domainiskey}. However, to our knowledge, there is little work benchmarking the embedding model's performance on domain texts across different tasks. 

\section{The FinMTEB Benchmark}


In this section, we introduce the Finance MTEB (FinMTEB) benchmark. As illustrated in Figure \ref{fig: overview}, FinMTEB encompasses seven embedding tasks, following a structure similar to MTEB \citep{mteb} but with datasets specifically curated for the finance domain.


\subsection{FinMTEB Tasks}

\textbf{Semantic Textual Similarity (STS)} evaluates the semantic similarity between pairs of financial text. This task is crucial for automated financial analysis and risk management; for example, detecting subtle semantic differences between quarterly earnings statements could reveal important shifts in a company's financial strategy that impact investment decisions. To ensure comprehensive evaluation, we incorporate diverse financial datasets, including FinSTS \citep{finsts} and FINAL \citep{final} from company annual reports, and BQ-Corpus \citep{bq-corpus} from banking documents. Model performance is quantified using Spearman's rank correlation, which measures the alignment between predicted cosine similarity scores and human-annotated similarity ratings.

\textbf{Retrieval} evaluates a model's capability to identify and extract relevant financial information in response to specific queries. Unlike general domain retrieval, financial information retrieval presents unique challenges, requiring precise handling of complex numerical data, temporal dependencies, and regulatory context. For comprehensive evaluation, we leverage established finance QA datasets including FinanceBench \citep{financebench}, FiQA2018 \citep{FiQA}, and HPC3 \citep{hpc3}. To further assess models' understanding of professional financial terminology, we introduce TheGoldman dataset, constructed from the Goldman Sachs Financial Dictionary. Performance is measured using NDCG@10, a metric that evaluates both the relevance of retrieved information and its ranking position, reflecting the real-world requirement for highly precise top results in financial applications.

\textbf{Clustering} evaluates a model's ability to automatically group similar financial texts based on their semantic content. To ensure comprehensive evaluation, we developed multiple specialized datasets that capture different aspects of financial text clustering: (1) FinanceArxiv-s2s and FinanceArxiv-p2p, constructed from titles and abstracts of finance-related papers on arXiv, providing rich academic financial content; (2) CompanyWiki2Industry dataset, derived from Wikipedia company descriptions, offering diverse industry categorization scenarios; and (3) complementary resources including consumer complaints from CFPB\footnote{https://huggingface.co/datasets/CFPB/consumer-finance-complaints}, financial intent detection data \citep{Intent_Detection,Synthetic}, and other established datasets. Model performance is quantified using the V-measure \citep{v-Measure}, a comprehensive metric that evaluates cluster quality through both completeness (all members of a class are assigned to the same cluster) and homogeneity (each cluster contains only members of a single class).

\textbf{Classification} evaluates a model's ability to categorize financial texts into predefined classes based on their semantic content. This capability is essential for automated financial decision-making; for example, in algorithmic trading, accurately classifying sentiment in earnings calls or news articles can directly influence trading strategies and portfolio adjustments. The classification task encompasses diverse financial scenarios through multiple specialized datasets, including: financial sentiment analysis \citep{fpb,FiQA,semeval, bbt}, Federal Reserve monetary policy classification \citep{fomc}, organization's strategy classification, and forward-looking statement identification \citep{investlm}. Performance is measured using Mean Average Precision (MAP), which provides a comprehensive assessment of classification accuracy while accounting for ranking quality and confidence scores.


\textbf{Reranking} evaluates the model's ability to order retrieved documents based on their relevance to financial queries. We utilize financial question-answering datasets such as Fin-Fact and FinQA\citep{rangapur2023finfact,finqa} to construct the reranking tasks. Specifically, for each query in these datasets, we retrieve top-k relevant documents along with the ground truth answers to construct the reranking training and evaluation pairs. The main evaluation metric for reranking in Finance MTEB is Mean Average Precision (MAP).


\textbf{Pair-Classification} evaluates a model's ability to determine semantic relationships between financial text pairs. This task includes two datasets: (1) the AFQMC dataset\footnote{https://tianchi.aliyun.com/dataset/106411} for customer intention, and (2) three financial news headline datasets \citep{headline}. We use Average Precision (AP) as the evaluation metric to assess model performance across different decision thresholds.

\textbf{Summarization} is evaluated based on the correlation between dense embeddings derived from the summarized texts and those of the original texts, utilizing Spearman's correlation coefficient as the main metric.  The evaluation corpus encompasses a comprehensive range of financial texts, including earnings call transcripts \citep{ectsum}, financial news articles \citep{bbt}, and SEC Form 10-K filings \citep{fns2022}, ensuring robust assessment across diverse financial contexts and writing styles.


\subsection{Characteristics of FinMTEB}
FinMTEB contains \textbf{35} English datasets and \textbf{29} Chinese datasets. Detailed information about these datasets is provided in Appendix \ref{append: datasets}.

% \textbf{Datasets.} The benchmark contains 35 English datasets and 29 Chinese datasets. Detailed information about these datasets is provided in Appendix \ref{append: datasets}.

\textbf{Linguistic Pattern.} Table \ref{tab:compare_benchmark} presents a comparative analysis of linguistic features between MTEB~\citep{mteb} and FinMTEB benchmarks, examining aspects such as average sentence length, token length, syllables per token, and dependency distance \citep{dependency_distance}. The results indicate that texts in FinMTEB consistently exhibit longer and more complex sentences than those in MTEB, with an average sentence length of 26.37 tokens compared to MTEB's 18.2 tokens. This highlights the linguistic differences between financial and general domain texts.


\textbf{Semantic Diversity.} We examine the inter-dataset semantic similarity within FinMTEB. Using the all-MiniLM-L6-v2 model\footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}, we embed 1,000 randomly sampled texts from each dataset, compute their mean embeddings to represent each dataset, and measure inter-dataset similarities using cosine similarity. As shown in Figure \ref{fig: semantic_diveristy}, most datasets in FinMTEB display inter-dataset similarity scores below 0.6, with a mean cosine similarity of 0.4, indicating semantic distinctions among various types of financial texts.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/pie.pdf}
    \caption{Distribution analysis of 5000 randomly sampled training data showing the breakdown of Tasks and Person Types.  Left: Persona distribution. Right: Task distribution. }
    \label{fig:pie}
\end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.46\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/job_pie.pdf}
%         % \caption{Finance-related persona distribution within training data.}
%         \label{fig:persona}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.46\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/task_pie.pdf}
%         % \caption{Task distribution within training data.}
%         \label{fig:task}
%     \end{subfigure}
%     \caption{Distribution analysis of 5000 randomly sampled training data showing the breakdown of Tasks and Person Types.  Left: Persona distribution. Right: Task distribution. }
%     \label{fig:pie}
% \end{figure*}
\section{Fin-E5: Finance-Adapted Text Embedding Model}
Data is important for domain adaptation~\citep{domainiskey}. However, existing public financial retrieval datasets exhibit significant limitations in their scope and applicability. For example, FiQA~\citep{FiQA}, a widely used financial retrieval dataset, primarily focuses on opinion-based content from online platforms, neglecting crucial aspects such as fundamental financial knowledge, technical terminology, and important investment data. This narrow task focus creates a substantial gap in training comprehensive financial embedding models. Therefore, we use persona-based data generation to address this problem and synthesize a diverse range of tasks, as illustrated in Figure \ref{fig:pie}.

\subsection{Data Formation}
We aim to construct each training instance as a triplet structure $(q, d^+, D^-)$, where $q$ represents a financial query, $d^+$ denotes a relevant document that provides substantive information addressing the query, and $D^-$ comprises carefully selected negative examples that share the financial domain but differ in semantic intent. 

\subsection{Training Data Construction}
To create a comprehensive dataset tailored for financial embedding training, we employ a systematic approach that combines expert-curated seed data with persona-based synthetic data generation. 


\textbf{Seed Data. } Our seed data comes from the finance-specific QA dataset provided by InvestLM \citep{investlm}, which offers expert-validated financial content across various domains, such as market analysis, investment strategies, and corporate finance. To ensure evaluation integrity, we conduct rigorous overlap checks between our training data and the FinMTEB benchmark, guaranteeing no overlap.



\textbf{Persona-based Data Augmentation. } To enhance the diversity of financial task representations, we develop a persona-based data augmentation framework derived from QA data generation~\citep{ge2024scaling}. Our framework employs a three-stage process that specifically targets the expansion of task coverage while preserving domain consistency. 

\begin{itemize}
    \item \textbf{Persona and Task Identification:} We first employ Qwen2.5-72B-Instruct~\citep{qwen2.5} to analyze each question-answer pair in the seed data, aiming to identify the persona of the intended user (e.g., equity analyst, risk manager, financial advisor, retail investor) by using the prompt "Who is likely to use this text?" Different personas correspond to various tasks.
    \item \textbf{Contextual Query Expansion:} For each identified persona-task pair, we generate context-specific queries $q$ that reflect the persona’s unique information needs and risk preferences, using the prompt \textit{"Guess a prompt (i.e., instructions) that the following persona may ask you to do:"}. For example, a pension fund manager’s query might emphasize long-term asset allocation, while a venture capitalist’s query would prioritize startup valuation metrics. 
    \item \textbf{Synthetic Document Generation:} We used LLMs to synthesize financial documents $d^+$ tailored to each persona’s task, ensuring that the dataset represents diverse perspectives in financial decision-making. This step improves the representativeness of the dataset, ensuring that the embeddings are trained in real-world financial contexts. The prompt is \textit{"Please synthesize some real context information, which is related to this question:"}.
\end{itemize}
We randomly sample 5,000 data points from the training data, then use GPT-4o~\citep{openai_chat} to annotate the job-related persona and task for the query. Visualized in Figure \ref{fig:pie}, it is clear that our data generation process produces a diverse range of tasks and finance persona.

\subsection{Training Pipeline}
Following the training recipe of e5-mistral-7b-instruct~\citep{e5}, utilizing the last token pooling method, we construct training pairs by selecting queries as anchor points and their corresponding answers as positive samples. To enhance the effectiveness of contrastive learning, we identify challenging negative samples using the all-MiniLM-L12-v2 model~\citep{sentence-bert}. The training process applies the InfoNCE loss~\citep{oord2018infonce}, calculated over in-batch negative samples. The detailed training parameter is illustrated in Appendix \ref{append: e5}.
% For a given data $(q, d^+, D^-)$, we adopt an instruction-based methodology for embedding training. The instruction template is as follows:

% \begin{equation}
%     q_{\text{inst}} = \text{Instruct: } \{task\_definition\} \textbackslash n \{q\}
% \end{equation}

% where $\{task\_definition\}$ represents a concise single-sentence description of the embedding task. 

% Interestingly, while the finance-adapted model excels in the FinMTEB benchmark with a higher average score (0.6735 vs 0.6475), it shows slightly lower performance in the general MTEB benchmark (0.6320 vs 0.6463). This trade-off suggests that domain adaptation, while beneficial for domain-specific tasks, may come at a small cost to general-domain performance. 


\section{Experiment}

In this section, we benchmark several existing models on FinMTEB, and then provide an in-depth analysis. Since most models are trained on English corpora, we only evaluate their performance on English datasets. 
% First, we examine the performance of different language models on this benchmark. Then, we conduct an in-depth analysis of the models' behaviors across different financial tasks. 

\begin{center}
   \input{content/benchmark_tab} 
\end{center}


% \subsection{Fin-E5 Performance Overview}
% Interestingly, while the finance-adapted model excels in the FinMTEB benchmark with a higher average score (0.6735 vs 0.6475), it shows slightly lower performance in the general MTEB benchmark (0.6320 vs 0.6463). This trade-off suggests that domain adaptation, while beneficial for domain-specific tasks, may come at a small cost to general-domain performance. 

\subsection{Models}
In addition to Fin-E5, we also evaluate four categories of embedding models on the FinMTEB benchmark in Table \ref{tab:benchmark}. The benchmark time is reported in Appendix \ref{append: time}.

\textbf{Bag-of-Words (BOW).} As a simple baseline, we implement the traditional BOW approach that represents text as sparse vectors based on word frequencies, providing a reference point for comparing more sophisticated methods.

\textbf{Encoder-based Models.} We evaluate various transformer encoder architectures, including: (1) classical models like BERT (CLS pooling)~\citep{Bert} and domain-specific FinBERT~\citep{finbert}; (2) optimized models such as msmarco-bert-base-dot-v5 and all-MiniLM-L12-v2~\citep{sentence-bert}; and (3) advanced architectures including bge-large-en-v1.5~\citep{bge_embedding}, AnglE-BERT~\citep{li2023angle} and instructor-base ~\citep{instructor}.

\textbf{LLM-based Models.} We investigate several state-of-the-art decoder-based embedding models: (1) Mistral-7B-based models including bge-en-icl~\citep{bge_embedding} , e5-mistral-7b-instruct~\citep{e5} and Echo~\citep{echo}; (2) NV-Embed v2~\citep{NV-Embed}; and (3) gte-Qwen1.5-7B-instruct~\citep{gte} built on the Qwen2~\citep{qwen2} architecture. These models utilize the powerful representation capabilities of LLMs to generate high-quality embeddings.



\textbf{Commercial Models.} To provide a comprehensive comparison with commercial solutions, we include industry-leading closed-source models, specifically OpenAI's text-embedding-3-large, text-embedding-3-small ~\citep{openai_embedding} and voyage-3-large~\citep{voyage}\footnote{We thank VoyageAI for supporting us in conducting the evaluation.}.

% \textbf{Benchmarking Time Reporting.} The benchmarking was conducted on the NVIDIA H800 GPU using a batch size of 512. Echo Embedding required the longest processing time at 12 hours, followed by BeLLM at 11.98 hours. AnglE-BERT completed the evaluation in 8 hours, while NV-Embed v2 demonstrated the highest efficiency, completing all tasks in just 5.6 hours. 

\subsection{Analysis}

Based on the results presented in Table~\ref{tab:benchmark}, our analysis focuses on three key findings.

\subsubsection{Impact of Domain Adaptation}
As illustrated in Table~\ref{tab:benchmark}, domain specialization considerably boosts performance: FinBERT outperforms BERT by 15.6\% (0.6721 vs. 0.5812), while Fin-E5 exceeds its general-domain counterpart e5-mistral-7b-instruct by 4.5\% (0.6767 vs. 0.6475), particularly excelling in classification (0.842 vs. 0.807) and semantic textual similarity (0.721 vs. 0.685).  The finance-adapted Fin-E5 also achieves state-of-the-art performance (0.6767 average score) on the FinMTEB benchmark, exceeding both general-purpose and commercial models. Notably, this peak performance is achieved with just 100 training steps, showcasing a cost-effective adaptation without the risk of data leakage.
% The experimental results demonstrate the clear advantages of domain adaptation in embedding tasks. While both BERT and its domain-adapted variant FinBERT show moderate performance on this benchmark, FinBERT consistently outperforms its general-purpose counterpart, highlighting the benefits of financial domain specialization. In particular, the Fin-E5 model achieves state-of-the-art (SoTA) average performance with a score of 0.6767. This is closely followed by the best performing commercial model, voyage-3-large, which is only marginally better by 0.002. Fin-E5 also demonstrates superior results across specialized tasks such as classification. 
% Compared to its counterpart in the general domain, e5-mistral-7b-instruct (average score of 0.6475), the finance-adapted Fin-E5 model exhibits substantial improvements in STS, retrieval, and classification tasks. It is particularly noteworthy that these significant performance gains were achieved with merely 100 training steps, without any overlap between our training data and the FinMTEB benchmark. This underscores the substantial impact of domain adaptation for embedding models.

\subsubsection{The Role of Model Architecture and Size}

Our experiments reveal three distinct performance tiers across architectural paradigms (Table~\ref{tab:benchmark}). Traditional bag-of-words (BOW) models achieve baseline performance (STS: 0.4845) and show notable limitations in retrieval tasks. Encoder-based architectures, such as bge-large-en-v1.5, demonstrate significant improvements, increasing retrieval performance by 107\% (0.6463) and STS by 38\% (0.6692) over BOW. A paradigm shift occurs with LLM-based models; e5-mistral-7b-instruct sets new standards with an average score of 0.6475. This progression from BOW (lexical) to LLM-based (contextual) architectures reveals a 52\% overall performance improvement, suggesting that model capacity plays a critical role in capturing financial semantics.


\subsubsection{Limitations of Current Models in Financial STS Tasks}
The STS results reveal a counterintuitive finding: BOW models (0.4845) outperform all dense architectures (maximum 0.4342) in terms of financial document similarity. This reversal of typical NLP performance hierarchies arises from two characteristics of the corpus: (1) Extensive boilerplate content in annual reports introduces noise for contextual embeddings, and (2) Specialized terminology (27\% unique financial terms per document) decreases lexical overlap.BOW benefits from exact term matches in standardized disclosures; the best dense model only captures 64\% of human-annotated similarity relationships, revealing fundamental limitations in current strategies for financial documents.

% Our analysis reveals an interesting pattern in the Semantic Textual Similarity (STS) task results from Table~\ref{tab:benchmark}, where the simple Bag-of-Words (BOW) model unexpectedly outperforms dense models. This counterintuitive finding may be attributed to the corpus source of our financial STS datasets: annual reports. These financial documents contain substantial boilerplate text and specialized terminology, creating a unique challenge for semantic similarity assessment. The notably low STS performance across all dense models, with even the highest score reaching only 0.4342, indicates a significant gap in current approaches' ability to effectively process and understand complex financial documents.



\section{Domain-specific Embedding Benchmark is needed}
This section addresses another research question. \textit{To what extent do general-purpose embedding evaluations appropriately capture domain-specific performance?} 
To solve this question, we run a quantitative comparison between MTEB~\citep{mteb} and FinMTEB.
% We provide quantitative evidence highlighting the limitations of existing benchmarks and underscore the need for specialized domain evaluation frameworks.}

\textbf{Models.} We evaluate \textbf{seven} state-of-the-art general-purpose embedding model. Specifically, we consider the following models: bge-en-icl \citep{bge_embedding} and e5-mistral-7b-instruct \citep{e5}, which are developed from Mistral-7B-v0.1 \citep{mistral}; gte-Qwen2-1.5B-instruct \citep{gte}, developed from Qwen2 \citep{qwen2}; bge-large-en-v1.5 \citep{bge_embedding} and all-MiniLM-L12-v2 \citep{sentence-bert}, both developed from BERT \citep{Bert}; instructor-base \citep{instructor} from T5Encoder \citep{t5Encoder}; and OpenAI’s text-embedding-3-small \citep{openai_embedding}. 

\textbf{Method.} To ensure robust statistical analysis, we use bootstrapping methods to generate a large sample dataset. For each task in both MTEB and FinMTEB, we aggregate the datasets associated with the task into a task pool. From each task pool, we randomly select 50 examples to create a bootstrap sample and evaluate the embedding model’s performance on this bootstrap. We repeat this process 500 times, resulting in 500 bootstraps for each combination. Thus, we have 14 unique combinations (model and domain), each with 500 bootstraps and their corresponding performance scores.

% \textbf{Correlation Analysis.} We first examine the relationship between model performance on general (MTEB) and financial domain (FinMTEB) tasks using Spearman's rank correlation. As shown in Table \ref{tab: ranking}, the correlation coefficients vary across tasks, ranging from -0.80 (Classification) to 0.60 (Summarization). However, none of these correlations achieve statistical significance at the conventional p < 0.05 level. This weak and statistically insignificant correlation suggests that a model's performance on general benchmarks may not be a reliable indicator of its capabilities in financial domain tasks.



\textbf{Analysis of Variance.} We conduct an Analysis of Variance (ANOVA) that examines the effects of both the model and the domain. The results reveal that the Domain Factor demonstrates statistical significance across all tasks (p < 0.001), with notably large F statistics in classification (F = 2086.30), clustering (F = 32161.37), and STS (F = 25761.71). Furthermore, the Domain Factor generally accounts for a greater share of the variance than the Model Factor, as indicated by the Sum of Squares (e.g., in Classification: Domain = 56.82 vs. Model = 4.17). These findings suggest that domain-specific characteristics significantly impact model performance, reinforcing the importance of specialized evaluation frameworks such as FinMTEB for financial applications.

\section{Conclusion}
This paper introduces FinMTEB, the first comprehensive benchmark for evaluating embedding models in the financial domain. Our main contributions include establishing a large-scale evaluation framework with 64 datasets across seven tasks in Chinese and English, and developing Fin-E5, a finance-adapted embedding model demonstrating competitive performance through persona-based data augmentation. Our empirical results highlight the importance of domain-specific adaptation and reveal current limitations in financial text embeddings. We believe FinMTEB will serve as a valuable resource for both researchers and practitioners in advancing financial language models.

\section{Limitation}
This work has two primary limitations. First, it relies on several existing financial datasets that could potentially overlap with the training data of contemporary embedding models. This overlap may introduce contamination, making it difficult to ensure completely fair comparisons between different models. Second, our adapted model and evaluation methods are currently limited to the English language, which restricts their applicability to non-English financial texts.

\bibliography{acl_latex}
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

\appendix

% \section{Appendix}
% \label{sec:appendix}

\section{Datasets}
\label{append: datasets}
\input{content/appendix/datasets}

\section{Dataset Characteristic}\label{sec:characteristic}
Figure \ref{fig: semantic_diveristy} presents the semantic similarity across all datasets in the FinMTEB benchmark. The semantic similarity is calculated by cosine similarity. Table \ref{tab:compare_benchmark} presents a comparative analysis of linguistic features between MTEB~\citep{mteb} and FinMTEB benchmarks, examining aspects such as average sentence length, token length, syllables per token, and dependency distance \citep{dependency_distance}.

\begin{center}
    \begin{table*}[htb]
    \centering
    \begin{adjustbox}{width=0.8\linewidth}
    \begin{tabular}{ccccc}
        \toprule 
        \textbf{Benchmark} & \textbf{Sentence Length} & \textbf{Token Length}  & \textbf{Syllables Per Token} & \textbf{Dependency Distance}  \\
        \midrule
         MTEB & 18.20 & 4.89 & 1.49 & 2.49\\
        FinMTEB & 26.37 & 5.12 & 1.52 & 2.85\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Comparison of Text Characteristics Between FinMTEB and MTEB. The numbers represent the average scores across all samples from all datasets.}
    \label{tab:compare_benchmark}
    \end{table*}
\end{center}

\begin{center}
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{fig/cosine_similarity.pdf}
    \caption{Semantic similarity across all the datasets in FinMTEB benchmark.}
    \label{fig: semantic_diveristy}
    \label{append: semantic_diveristy}
    \end{figure*} 
\end{center}


\section{Training Details For Fin-E5}\label{append: e5}
The training dataset size is 19,467. The model is trained for 100 steps using the augmented dataset with a batch size of 128. For optimization, we use the AdamW optimizer with a learning rate of 1e-5 and implement a linear warmup schedule.  For a given data $(q, d^+, D^-)$, we adopt an instruction-based methodology for embedding training. The instruction template is as follows:

\begin{equation}
    q_{\text{inst}} = \text{Instruct: } \{task\_definition\} \textbackslash n \{q\}
\end{equation}

where $\{task\_definition\}$ represents a concise single-sentence description of the embedding task. 

\section{Benchmarking Time Reporting.} \label{append: time}
The benchmarking was conducted on the NVIDIA H800 GPU using a batch size of 512. Echo Embedding~\citep{echo} required the longest processing time at 12 hours, followed by BeLLM~\citep{li2023angle} at 11.98 hours. AnglE-BERT~\citep{li2023angle} completed the evaluation in 8 hours, while NV-Embed v2~\citep{NV-Embed} demonstrated the highest efficiency, completing all tasks in just 5.6 hours. 

\section{Spearman’s Correlation of Embedding Models’ Performance}
We evaluate the performance ranking of embedding models on both the general MTEB and FinMTEB datasets, calculating Spearman’s rank correlation between the two. The results, shown in Table  \ref{tab: ranking}, indicate that the ranking correlation is not statistically significant (p-values all greater than 0.05). In other words, a general-purpose embedding model performing well on MTEB does not necessarily perform well on domain-specific tasks.
\label{append: spearman}
\begin{center}

    \begin{table*}[ht]
    \centering
    \resizebox{0.7\linewidth}{!}{
        \begin{tabular}{lccccccc}
        \toprule
        & \textbf{STS} & \textbf{Class.} & \textbf{Ret.} & \textbf{Rerank.} & \textbf{Clust.} & \textbf{PairClass.} & \textbf{Summ.} \\
        \midrule
        \textbf{Correlation} & 0.30 & -0.80 & 0.30 & -0.10 & -0.70 & -0.30 & 0.60 \\
        \textbf{p-value} & 0.62 & 0.10 & 0.62 & 0.87 & 0.18 & 0.62 & 0.28 \\
        \bottomrule
        \end{tabular}
    }
    \caption{Spearman’s correlation of embedding models’ performance on MTEB and FinMTEB across different tasks. The p-value indicates that all correlations are statistically insignificant, suggesting a lack of evidence for a relationship between embedding model performance on the two benchmarks.}
    \label{tab: ranking}
    \end{table*}

\end{center}

\section{Analysis of Variance (ANOVA)}
\input{content/appendix/anova}


\end{document}
