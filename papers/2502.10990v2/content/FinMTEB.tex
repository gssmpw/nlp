\begin{center}
    \begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{fig/cosine_similarity.pdf}
    \caption{Semantic similarity across all the datasets in FinMTEB benchmark.}
    \label{fig: semantic_diveristy}
    \label{append: semantic_diveristy}
    \end{figure*} 
\end{center}



In this section, we introduce the Finance MTEB (FinMTEB) benchmark. Financial text embeddings face unique challenges due to the domain's specialized vocabulary, complex relationships between financial concepts, and the critical importance of precise interpretation. While general-purpose embedding benchmarks exist, they fail to capture these domain-specific nuances. As illustrated in Figure \ref{fig: overview}, FinMTEB encompasses seven embedding tasks, following a structure similar to MTEB \citep{mteb} but with datasets specifically curated for the finance domain.


\subsection{FinMTEB Tasks}

\textbf{Semantic Textual Similarity (STS)} evaluates the semantic similarity between pairs of financial text. This task is particularly challenging in finance due to the technical jargon and subtle differences in meaning that can have significant implications. We incorporate datasets from various financial sources, including FinSTS \citep{finsts} and FINAL \citep{final} from company annual reports, and BQ-Corpus \citep{bq-corpus} from banking documents. The main metric used to measure performance in this task is Spearman's rank correlation of predicted cosine similarity scores with the true similarity score.

\textbf{Retrieval} assesses the ability to identify relevant financial information in response to queries. Unlike general domain retrieval, financial retrieval must handle complex numerical data, temporal dependencies, and regulatory context. We utilize established finance QA datasets such as FinanceBench \citep{financebench}, FiQA2018 \citep{FiQA}, and HPC3 \citep{hpc3}. Additionally, we constructed TheGoldman dataset from the Goldman Sachs Financial Dictionary, which provides a comprehensive evaluation of financial terminology understanding. This specialized dataset helps assess models' grasp of professional financial concepts and technical terms. The main evaluation metric employed in this task is NDCG@10, which assesses the quality of the results based on their relevance and position in the list returned. 


\textbf{Clustering} evaluates the model's capability to group similar financial texts, which is essential for market analysis and risk management. To create a comprehensive evaluation suite, we constructed several specialized datasets: (1) FinanceArxiv-s2s and FinanceArxiv-p2p from titles and abstracts of finance-related papers on arXiv, providing a rich source of academic financial content; (2) CompanyWiki2Industry dataset from Wikipedia company descriptions, offering diverse industry categorization scenarios; and (3) we also incorporate consumer complaints from CFPB\footnote{https://huggingface.co/datasets/CFPB/consumer-finance-complaints}, financial intent detection data \citep{Intent_Detection,Synthetic} and other established datasets. The main evaluation metric for this task is the V-measure \citep{v-Measure}, which assesses the quality of the clusters by examining both the completeness and the homogeneity of the data within each group.


\textbf{Classification} involves predicting the label of a financial text based on its text embedding. The classification task includes multiple datasets, such as financial sentiment analysis \citep{fpb,FiQA,semeval, bbt}, Fed's monetary policy classification \citep{fomc}, organization's strategy classification, and forward-looking statement classification \citep{investlm}. The main evaluation metric for reranking in Finance MTEB is Mean Average Precision (MAP).

\textbf{Reranking} evaluates the model's ability to order retrieved documents based on their relevance to financial queries. We utilize financial question-answering datasets such as Fin-Fact\citep{rangapur2023finfact} to construct the reranking tasks. Specifically, for each query in these datasets, we retrieve top-k relevant documents along with the ground truth answers to construct the reranking training and evaluation pairs. The main evaluation metric for reranking in Finance MTEB is Mean Average Precision (MAP).


\textbf{Pair-Classification} evaluates a model's ability to determine semantic relationships between financial text pairs. This task includes two datasets: (1) the AFQMC dataset\footnote{https://tianchi.aliyun.com/dataset/106411} for customer intention, and (2) three financial news headline datasets \citep{headline}. We use Average Precision (AP) as the evaluation metric to assess model performance across different decision thresholds.


\textbf{Summarization} is evaluated based on the correlation between dense embeddings derived from the summarized texts and those of the original texts, utilizing Spearman's correlation coefficient as the main metric, which is crucial for financial text analysis. The evaluation corpus encompasses diverse financial texts including earnings call transcripts \citep{ectsum}, financial news \citep{bbt}, and Form 10-K filings \citep{fns2022}.


\subsection{Characteristics of FinMTEB}
Having built the FinMTEB benchmark, we now provide an analysis to understand its characteristics. 

\textbf{Linguistic Pattern.} Table \ref{tab:compare_benchmark} presents a comparative analysis of linguistic features between MTEB~\citep{mteb} and FinMTEB benchmarks, examining aspects such as average sentence length, token length, syllables per token, and dependency distance \citep{dependency_distance}. The results indicate that texts in FinMTEB consistently exhibit longer and more complex sentences than those in MTEB, with an average sentence length of 26.37 tokens compared to MTEB's 18.2 tokens. This highlights the linguistic differences between financial and general domain texts.


\textbf{Semantic Diversity.} We examine the inter-dataset semantic similarity within FinMTEB. Using the all-MiniLM-L6-v2 model\footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}, we embed 1000 randomly sampled texts from each dataset, compute their mean embeddings to represent each dataset, and measure inter-dataset similarities using cosine similarity. As shown in Figure \ref{fig: semantic_diveristy}, most datasets in FinMTEB exhibit inter-dataset similarity scores below 0.6, with a mean cosine similarity of 0.4, suggesting semantic distinctions among different financial text types.


\begin{center}
    \begin{table*}[htb]
    \centering
    \begin{adjustbox}{width=0.8\linewidth}
    \begin{tabular}{ccccc}
        \toprule 
        \textbf{Benchmark} & \textbf{Sentence Length} & \textbf{Token Length}  & \textbf{Syllables Per Token} & \textbf{Dependency Distance}  \\
        \midrule
         MTEB & 18.20 & 4.89 & 1.49 & 2.49\\
        FinMTEB & 26.37 & 5.12 & 1.52 & 2.85\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Comparison of Text Characteristics Between FinMTEB and MTEB. The numbers represent the average scores across all samples from all datasets.}
    \label{tab:compare_benchmark}
    \end{table*}
\end{center}