
In this study, we introduce FinMTEB, a comprehensive benchmark for evaluating text embedding models in the financial domain. Our work makes several key contributions: (1) We establish the first large-scale, multi-task evaluation framework specifically designed for financial text embeddings, covering 64 datasets across seven distinct tasks in both Chinese and English; (2) Through extensive experiments with various embedding models, we provide empirical observations about model performance on financial tasks; (3) Our analysis reveals distinct performance patterns between general and domain-specific evaluations, suggesting the value of specialized evaluation frameworks; and (4) We contribute to the research community by open-sourcing both the FinMTEB benchmark and our finance-adapted model FinE5. We believe FinMTEB will serve as a valuable resource for researchers and practitioners working on financial NLP applications, facilitating more reliable evaluation and development of domain-specific embedding models.