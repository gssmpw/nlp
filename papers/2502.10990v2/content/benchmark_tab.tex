\begin{table*}[htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llccccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{7}{c}{\textbf{Tasks}} & \multirow{2}{*}{\textbf{Avg.}} \\
\cmidrule(lr){3-9}  
& & \textbf{STS} & \textbf{Retrieval} & \textbf{Class.} & \textbf{Cluster.} & \textbf{Rerank.} & \textbf{PairClass.} & \textbf{Summ.} & \\
\cmidrule(lr){3-9}  
& & \textbf{2} & \textbf{10} & \textbf{8} & \textbf{6} & \textbf{3} & \textbf{3} & \textbf{3} & \\ 
\midrule
BOW & - &\textbf{0.4845} & 0.2084 & 0.4696 & 0.2547 & 0.7628 & 0.7143 & 0.2584 & 0.4504 \\
\hline
\multicolumn{9}{l}{\textbf{Encoder based Models}} \\
\hline
BERT & 110M  & 0.3789 & 0.0207 & 0.5496 & 0.1744 & 0.3930 & 0.7111 & 0.1686 & 0.3423 \\
FinBERT & 110M & 0.4198 & 0.1102 & 0.5923 & 0.2833 & 0.6404 & 0.6967 & 0.2010 & 0.4205 \\
instructor-base & 110M & 0.3732 & 0.5772 & 0.6208 & 0.5300 & 0.9734 & 0.6138 & 0.4315 & 0.5886 \\
bge-large-en-v1.5 & 335M & 0.3396 & 0.6463 & 0.6436 & 0.5725 & 0.9825 & 0.7400 & 0.4857 & 0.6301 \\
AnglE-BERT & 335M & 0.3080 & 0.5730 & 0.6439 & 0.5774 & 0.9650 & 0.6891 & 0.5049 & 0.6088 
\\
\hline
\multicolumn{9}{l}{\textbf{LLM-based Models}} \\
\hline
gte-Qwen1.5-7B-instruct & 7B & 0.3758 & 0.6697 & 0.6438 & 0.5854 & 0.9890 & 0.6998 & 0.5354 & 0.6427 \\
Echo & 7B & \underline{0.4380} & 0.6443 & 0.6525& 0.5776 & 0.9765 & 0.6261 & 0.4722 & 0.6267 \\
bge-en-icl & 7B & 0.3233 & 0.6789 & 0.6569 & 0.5742 & 0.9898 & 0.6738 & 0.5197 & 0.6309 \\
NV-Embed v2 & 7B & 0.3739 & 0.7061 & 0.6393 & 0.6096 & 0.9822 & 0.6043 & 0.5103 & 0.6322 \\
e5-mistral-7b-instruct & 7B & 0.3800 & 0.6749 & 0.6449 & 0.5783 & 0.9875 & \underline{0.7394} & 0.5275 & 0.6475 \\
\hline
\multicolumn{9}{l}{\textbf{Commercial Models}} \\
\hline
text-embedding-3-small & - & 0.3254 & 0.6641 & 0.6387 & 0.5802 & 0.9825 & 0.5957 & 0.5085 & 0.6136 \\
text-embedding-3-large & - & 0.3615 & \underline{0.7112} & 0.6596 & \textbf{0.6081} & \underline{0.9910} & 0.7309 & \underline{0.5671} & 0.6613 \\
voyage-3-large & - & 0.4145 & \textbf{0.7463} & \underline{0.6861} & \underline{0.5944} & \textbf{0.9938} & 0.6519 & \textbf{0.6484} & \underline{0.6765}\\
\hline
\multicolumn{9}{l}{\textbf{Finance Adapted LLM-based Models}} \\
\hline
Fin-E5 & 7B & 0.4342 & 0.7105 & \textbf{0.7565} & 0.5650 & 0.9896 & \textbf{0.8014} & 0.4797 & \textbf{0.6767} \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison across different embedding models on FinMTEB benchmark. The evaluation metrics include semantic textual similarity (STS), retrieval, classification (Class.), clustering (Cluster.), reranking (Rerank.), pair classification (PairClass.), and summarization (Summ.). \textbf{Best} results are in bold. The underline represents the \underline{second-best} performance.}
\label{tab:benchmark}
\end{table*}