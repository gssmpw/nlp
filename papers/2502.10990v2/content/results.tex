
In this section, we present a comprehensive analysis of FinMTEB from three perspectives. 
First, we examine the performance of different language models on this benchmark. Then, we conduct an in-depth analysis of the models' behaviors across different financial tasks. 

\label{tab:benchmark}
\begin{center}
   \input{content/benchmark_tab} 
\end{center}


\subsection{Models}
Except for the FinE5, we aslo conduct comprehensive evaluations across four categories of embedding models on the FinMTEB benchmark in Table \ref{tab:benchmark}. Since most models are trained on English corpora, we only evaluate their performance on English datasets. 

\textbf{Bag-of-Words (BOW).} As a simple baseline, we implement the traditional BOW approach that represents text as sparse vectors based on word frequencies, providing a reference point for comparing more sophisticated methods.

\textbf{General Encoder-based Models.} We evaluate various transformer encoder architectures, including: (1) classical models like BERT (CLS pooling)~\citep{Bert} and domain-specific FinBERT~\citep{finbert}; (2) optimized models such as msmarco-bert-base-dot-v5 and all-MiniLM-L12-v2~\citep{sentence-bert}; and (3) advanced architectures including bge-large-en-v1.5~\citep{bge_embedding}, AnglE-BERT~\citep{li2023angle} and instructor-base ~\citep{instructor}.

\textbf{LLM-based Models.} We investigate several state-of-the-art decoder-based embedding models: (1) Mistral-7B-based models including bge-en-icl~\citep{bge_embedding} and e5-mistral-7b-instruct~\citep{e5}; (2) NV-Embed v2~\citep{NV-Embed}; and (3) gte-Qwen1.5-7B-instruct~\citep{gte} built on the Qwen2~\citep{qwen2} architecture. These models utilize the powerful representation capabilities of LLMs to generate high-quality embeddings.



\textbf{Commercial Models.} To provide a comprehensive comparison with commercial solutions, we include industry-leading closed-source models, specifically OpenAI's text-embedding-3-large and text-embedding-3-small ~\citep{openai_embedding}.

\textbf{Benchmarking Time Reporting.} The benchmarking was conducted on the NVIDIA H800 GPU using a batch size of 512. Echo Embedding required the longest processing time at 12 hours, followed by BeLLM at 11.98 hours. AnglE-BERT completed the evaluation in 8 hours, while NV-Embed v2 demonstrated the highest efficiency, completing all tasks in just 5.6 hours. 

\subsection{Analysis}


We conduct a comprehensive analysis of various embedding models' performance across seven financial NLP tasks in table \ref{tab:benchmark}. 
Our analysis focuses on two key aspects that significantly influence model effectiveness: the impact of domain adaptation, the role of model architecture and size and the limitations of Current Models.

\subsubsection{Impact of Domain Adaptation}

The experimental results demonstrate the clear advantages of domain adaptation in embedding tasks. While both BERT and FinBERT show moderate performance on this benchmark, the domain-adapted model (FinBERT) still outperforms its general-purpose version (BERT), highlighting the benefits of financial domain adaptation. The FinE5 model achieves both the highest average performance (0.6735) and superior results across specialized tasks. Compared to its general-domain counterpart e5-mistral-7b-instruct (0.6475), the finance-adapted version demonstrates particularly strong improvements in classification (0.7565 vs 0.6449) and pair classification (0.8014 vs 0.7394) tasks. This performance gap highlights how domain-specific training enhances the model's ability to understand nuanced financial relationships and categories. The finance-adapted model also maintains robust performance across other key tasks, achieving competitive scores in retrieval (0.6989) and semantic textual similarity (0.4281), demonstrating its versatility in handling diverse financial language understanding challenges. These comprehensive results emphasize the value of domain adaptation in financial NLP tasks.

% To better understand the impact of domain adaptation, we further evaluate the FinE5 and e5-mistral-7b-instruct on both FinMTEB and MTEB benchmarks, as shown in Table~\ref{tab:model_comparison}. 
% Interestingly, while the finance-adapted model excels in the FinMTEB benchmark with a higher average score (0.6735 vs 0.6475), it shows slightly lower performance in the general MTEB benchmark (0.6320 vs 0.6463). This trade-off suggests that domain adaptation, while beneficial for domain-specific tasks, may come at a small cost to general-domain performance.


\subsubsection{The Role of Model Architecture and Size}

The evolution of model architectures and size reveals an interesting progression in capabilities. Traditional BOW approaches, while simple, show surprising strength in semantic textual similarity (0.4845), though they struggle with complex tasks such as retrieval and summarization. The introduction of encoder-decoder architectures, particularly exemplified by bge-large-en-v1.5 and AnglE-BERT, brought more balanced performance across tasks, marking a significant advancement over traditional approaches. The latest LLM-based architectures further push the boundaries, particularly excelling in complex tasks. This progression is most evident in tasks like retrieval, where NV-Embed v2 achieves a score of 0.7061 compared to bge-large-en-v1.5's 0.6463, and in summarization, where LLM-based models consistently score above 0.5000 while encoder-decoder models typically remain below 0.2000, with AnglE-BERT being a notable exception.

\subsubsection{Limitations of Current Models in Financial STS Tasks}

From table \ref{tab:benchmark}, one important point is that the best performance on STS task is BOW model. This unexpected result can be largely attributed to the nature of our STS datasets, which are primarily based on annual reports. These financial documents contain substantial boilerplate text and specialized terminology, where surface-level word overlap might be more indicative of semantic similarity than deeper contextual understanding. This explains why dense models, despite their advanced capabilities, do not outperform the simple BOW approach. The relatively low STS scores across all models (with the highest being only 0.4281) suggest that current approaches need to be specifically adapted to better handle the unique characteristics of financial documents, particularly the balance between boilerplate content and meaningful variations.