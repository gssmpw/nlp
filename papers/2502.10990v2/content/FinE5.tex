
\begin{center}
    \input{content/fin_e5_tab}
\end{center}

To advance the performance of embedding models in the financial domain, we develop FinE5, a finance-adapted version of the e5-mistral-7b-instruct model \citep{e5}. This section details our adaptation approach and training methodology.

\subsection{Training Data Construction}
\textbf{Seed Data. }We leverage a human-written finance-specific QA dataset that was originally used in the development of InvestLM \citep{investlm}, a finance LLM, as the seed data for augmentation. To ensure the robustness of our experiment, we carefully verified that there is no overlap between this training seed data and our FinMTEB benchmark. The dataset covers diverse financial topics including market analysis, investment strategies, and corporate finance. 

\textbf{Data Augmentation. }To meet the large data requirements for embedding model training, we augment the seed data following the methodology proposed in Persona-hub \citep{ge2024scaling}. Specifically, for each instruction in the seed data, we prompt LLMs to identify potential users by asking \textit{"Who is likely to use this text?"}. The generated personas are then paired with the seed data to create new, contextually relevant instructions using \textit{"Create a finance problem with the following persona:"}. Through this augmentation process, we expanded our dataset to 122,575 samples.

\subsection{Training Pipeline}
Following the training framework of e5-mistral-7b-instruct ~\citep{e5}, we construct training pairs using queries as anchor points and their corresponding answers as positive samples. For effective contrastive learning, we mine hard negative samples using all-MiniLM-L12-v2 \citep{sentence-bert}. 

% Given a relevant query-document pair $(q^+, d^+)$, we employ an instruction-based approach for embedding. The instruction template is formulated as:

% \begin{equation}
%     q^+_{\text{inst}} = \text{Instruct: } \{task\_definition\} \textbackslash n \{q^+\}
% \end{equation}

% where $\{task\_definition\}$ represents a concise single-sentence description of the embedding task. 
The model is trained for 300 steps using the augmented dataset with a batch size of 128. For optimization, we use the AdamW optimizer with a learning rate of 1e-5 and implement a linear warmup schedule . The model training employs the InfoNCE loss $\mathcal{L}$ computed over both in-batch negatives and hard negatives. We evaluate the performance of our adapted model on both the MTEB benchmark and compare it with general-purpose counterparts, as shown in Table \ref{tab:model_comparison}.

Interestingly, while the finance-adapted model excels in the FinMTEB benchmark with a higher average score (0.6735 vs 0.6475), it shows slightly lower performance in the general MTEB benchmark (0.6320 vs 0.6463). This trade-off suggests that domain adaptation, while beneficial for domain-specific tasks, may come at a small cost to general-domain performance.


