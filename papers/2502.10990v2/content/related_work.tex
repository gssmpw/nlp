\subsection{General-purpose Embedding Models}
Embedding models have evolved significantly from early word-level approaches like Word2Vec \citep{word2vector} and GloVe \citep{pennington-etal-2014-glove} to transformer-based models such as BERT \citep{Bert} and RoBERTa \citep{roberta}. These later models enabled contextualized word embeddings through deep bidirectional encoders. Sentence-BERT \citep{sentence-bert} further advanced the field by introducing Siamese and triplet networks for generating semantically meaningful sentence embeddings. Recent developments in LLMs have led to more powerful embedding models, such as e5-mistral-7b-instruct \citep{e5} and gte-Qwen2-1.5B-instruct \citep{qwen2}, achieving state-of-the-art performance across various NLP tasks.


\subsection{Embedding Evaluation Benchmarks}
To comprehensively evaluate embedding models, benchmarks like the Massive Text Embedding Benchmark (MTEB) \citep{mteb} have been established. MTEB assesses embedding models across a wide array of tasks using numerous datasets and languages. This extensive evaluation provides insights into a model's generalizability and effectiveness across different linguistic contexts and task types. Similarly, the BEIR benchmark \citep{beir} focuses on the information retrieval task, encompassing 18 diverse datasets. While BEIR includes some domain-specific datasets such as FiQA \citep{FiQA}, it is not tailored for comprehensive domain analysis. The inclusion of a few specialized datasets does not fully address the unique challenges posed by domain-specific language and terminology. There are also scenario-specific RAG evaluation benchmarks like RAGeval \citep{rageval}. These benchmarks acknowledge the necessity for domain-specific evaluations, particularly highlighting the impact of accurate retrieval in specialized contexts. However, they primarily focus on retrieval tasks and often overlook other crucial embedding tasks such as semantic similarity and clustering.

\subsection{Domain-Specific Models} 
Different domains exhibit distinct linguistic patterns and terminologies, often requiring domain-specific models or adaptations for specialized tasks. Researchers have advocated for training domain-specific models or fine-tuning general models for particular domains \citep{dont_stop}. For instance, domain-specific LLMs like BioMedLM \citep{biomedlm} for biomedical text, SaulLM-7B \citep{saullm} for legal documents, and BloombergGPT \citep{bloomberggpt} for financial applications are pre-trained on large domain-specific corpora. In addition, instruction-tuned domain-specific models such as InvestLM \citep{investlm} and FinGPT \citep{fingpt} are fine-tuned for specific downstream tasks in the finance domain. Similarly, domain-specific embedding models have been developed across various fields - BioWordVec \citep{biowordvec} and BioSentVec \citep{biosentvec} for biomedical texts, and FinBERT \citep{finbert} for financial documents. 


\subsection{Domain-specific Model Benchmarks}
Numerous benchmarks tailored to specific domains have been developed with the emergence of domain-specific large language models (LLMs). For example, in the finance domain, benchmarks such as CFLUE \citep{CFLUE}, FinEval \citep{fineval}, DocMath-Eval \citep{DocMath-Eval}, and FinanceBench \citep{financebench} have been introduced to assess the comprehension capabilities of LLMs within financial contexts. Similarly, in the legal domain, LawBench \citep{lawbench} has been established to evaluate LLMs across a variety of legal tasks. Besides, MedBench \citep{medbench}, MedEval \citep{medeval}, and DrBenchmark \citep{drbenchmark} have been developed to test the proficiency in understanding and generating medical information. Most of these benchmarking papers conclude that general-purpose LLMs may fall short on domain tasks \citep{CFLUE, lawbench}. The importance of domain adaptation has gradually gained attention \citep{domainiskey}. However, to our knowledge, there is little work benchmarking the embedding model's performance on domain texts. 
