\begin{figure}
    \centering
    \includegraphics[width=0.88\linewidth]{fig/task.pdf}
    \caption{FinMTEB contains diverse financial documents. The sunburst chart illustrates the hierarchical relationship between seven task categories (Classification, Retrieval, Summarization, etc.) and their associated financial text types (Financial News, Annual Reports, Regulatory Filings, etc.).}
    \label{fig:enter-label}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/overview.pdf}
    \caption{An overview of tasks and datasets used in FinMTEB. All the dataset descriptions and examples are provided in the Appendix \ref{append: datasets}. }
    \label{fig: overview}
\end{figure*}
Embedding models, which transform text sequences into dense vector representations, play a crucial role in various natural language processing (NLP) tasks \citep{word2vector, pennington-etal-2014-glove, peters-etal-2018-deep}. The quality of text embeddings directly impacts the effectiveness of information retrieval, semantic understanding, and other downstream applications. Recent advancements in NLP have led to embedding models achieving impressive performance on general-purpose benchmarks \citep{e5,gte,SFR-embedding-2}. However, the success of these models on general benchmarks may not accurately reflect their effectiveness in specialized domains, where unique linguistic patterns and domain-specific knowledge are critical.

The gap between general and domain-specific performance has sparked growing interest in specialized evaluation frameworks ~\citep{gururangan-etal-2020-dont,domainiskey}. Several domain-specific evaluation efforts have emerged in recent years, such as CoIR for evaluating embedding models in coding tasks \citep{li2024coir}, demonstrating the growing recognition of domain-specific evaluation's importance. Among various domains, finance stands out as an ideal testing ground due to its well-established importance in both academia and industry. The finance domain not only has a wealth of existing NLP datasets \citep{FiQA,financebench,finsts} but also presents unique challenges in representing complex financial concepts, market sentiments, and regulatory information. For instance, boilerplate statements in financial reports like \textit{"The company's operations and financial performance are subject to various risks and uncertainties..."} often carry little informational value and require models to distinguish between meaningful content and routine disclaimers. However, existing evaluation approaches in finance are often limited in scope, focusing on specific tasks or textual types rather than providing a comprehensive assessment framework.


To bridge this gap, we introduce the \textbf{Fin}ance \textbf{M}assive \textbf{T}ext \textbf{E}mbedding \textbf{B}enchmark (FinMTEB), a comprehensive evaluation framework specialized for the financial domain. FinMTEB comprises 64 domain-specific datasets spanning both Chinese and English, encompassing seven distinct tasks: classification, clustering, retrieval, pair-classification, reranking, summarization, and semantic textual similarity. We also develop FinE5, a finance-adapted version of e5-mistral-7b-instruct~\citep{e5}. Our experimental results show that LLM-based embedding models consistently outperform traditional approaches, while domain adaptation further improves performance. Notably, the low Spearman's correlation between model rankings on MTEB and FinMTEB indicates that strong performance on general-purpose benchmarks does not guarantee similar success on financial tasks. Interestingly, in the STS task, we find that the simple BOW model outperforms dense models, likely due to the prevalence of boilerplate text and specialized terminology in financial documents. This suggests that current embedding models still face challenges in handling the unique characteristics of financial texts. To facilitate future research, we will open-source both the FinMTEB benchmark and our FinE5 model.


Our main contributions can be summarized as follows:

\begin{enumerate}

\item We propose FinMTEB, the first comprehensive financial domain evaluation benchmark covering 64 datasets across seven distinct tasks in both Chinese and English;

\item We develop and release FinE5, a finance-adapted embedding model that achieves state-of-the-art performance on FinMTEB;

\item Through extensive experiments, we quantitatively demonstrate the importance of domain-specific evaluation;

\end{enumerate}
