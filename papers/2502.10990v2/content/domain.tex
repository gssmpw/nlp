

To further confirm the necessity of proposed benchmark, we employ various quantitative measures to reveal the importance of domain evaluation.

\textbf{Models.} We consider \textbf{seven} state-of-the-art, general-purpose embedding models in this experiments. Specifically, we consider the following models: bge-en-icl \citep{bge_embedding} and e5-mistral-7b-instruct \citep{e5}, which are developed from Mistral-7B-v0.1 \citep{mistral}; gte-Qwen2-1.5B-instruct \citep{gte}, developed from Qwen2 \citep{qwen2}; bge-large-en-v1.5 \citep{bge_embedding} and all-MiniLM-L12-v2 \citep{sentence-bert}, both developed from BERT \citep{Bert}; instructor-base \citep{instructor} from T5Encoder \citep{t5Encoder}; and OpenAI’s text-embedding-3-small \citep{openai_embedding}. 

\textbf{Method.} To facilitate statistical analysis, we employ bootstrapping methods to generate a large sample dataset. For each task in both MTEB and FinMTEB, we aggregate the task’s datasets into a task pool. From each task pool, we randomly sample 50 examples to form a bootstrap sample and evaluate the embedding model’s performance on this bootstrap. We repeat this process 500 times, yielding 500 bootstraps for each combination. Thus, we have 14 unique combinations (model and domain), each with 500 bootstraps and corresponding performance scores.

\textbf{Correlation Analysis.} We first examine the relationship between model performance on general (MTEB) and financial domain (FinMTEB) tasks using Spearman's rank correlation. As shown in Table \ref{tab: ranking}, the correlation coefficients vary across tasks, ranging from -0.80 (Classification) to 0.60 (Summarization). However, none of these correlations achieve statistical significance at the conventional p < 0.05 level. This weak and statistically insignificant correlation suggests that a model's performance on general benchmarks may not be a reliable indicator of its capabilities in financial domain tasks.

% \textbf{Correlation Analysis.} We first examine the relationship between model performance on general (MTEB) and financial domain (FinMTEB) tasks using Spearman's rank correlation. As shown in Table \ref{tab: ranking}, the correlation coefficients vary across tasks, ranging from -0.80 (Classification) to 0.60 (Summarization). However, none of these correlations achieve statistical significance at the conventional p < 0.05 level. This weak and statistically insignificant correlation suggests that a model's performance on general benchmarks may not be a reliable indicator of its capabilities in financial domain tasks.


\textbf{Analysis of Variance.} To further investigate performance variations, we conducted a two-way ANOVA examining both Model and Domain effects. The results reveal that the Domain Factor demonstrates consistent statistical significance across all tasks (p < 0.001), with notably large F-statistics in Classification (F = 2086.30), Clustering (F = 32161.37), and STS (F = 25761.71). Moreover, the Domain Factor typically accounts for a larger portion of variance than the Model Factor, as evidenced by the Sum of Squares (e.g., in Classification: Domain = 56.82 vs. Model = 4.17). These findings indicate that domain-specific characteristics substantially influence model performance, supporting the value of specialized evaluation frameworks like FinMTEB for financial applications.


% \textbf{Analysis of Variance (ANOVA). }Having observed a performance discrepancy between general-purpose embedding models across the two benchmarks, we conduct an Analysis of Variance (ANOVA) to understand the impact of different factors. The ANOVA results (detailed in Appendix \ref{append:anova}) reveal that while the choice of embedding models shows varying significance across individual tasks, the Domain factor demonstrates consistently significant effects across all tasks. Such findings provide compelling evidence for the necessity of domain-specific benchmarks, as general-purpose evaluations may not adequately reflect a model's capability in specialized domains.
