\section{Related Work}
Recent advances in embedding models have shown remarkable success in general domain tasks, yet their effectiveness in specialized domains remains a critical challenge. 
% In this section, we first review the development of embedding models and existing evaluation frameworks, then discuss current approaches to domain specialization, ultimately highlighting the pressing need for comprehensive domain-specific embedding evaluation.

\subsection{General-purpose Embedding Models}
The evolution of embedding models marks significant progress in natural language processing. Starting with static word representations like Word2Vec \citep{word2vector} and GloVe \citep{pennington-etal-2014-glove}, the field advanced to contextualized embeddings through transformer-based architectures such as BERT \citep{Bert} and RoBERTa \citep{roberta}. A notable advancement came with Sentence-BERT \citep{sentence-bert}, which introduced Siamese and triplet network architectures to generate meaningful sentence-level representations. Recent developments in large language models have further pushed the boundaries, with models such as e5-mistral-7b-instruct \citep{e5} and gte-Qwen2-1.5B-instruct \citep{qwen2} achieving better performance in various embedding tasks. However, these general-purpose models may not adequately capture the nuanced semantics of specialized domains.

\subsection{Current Embedding Evaluation Landscape}
To assess embedding quality, several evaluation frameworks have been developed. General-purpose embedding benchmarks, such as the Massive Text Embedding Benchmark (MTEB) \citep{mteb}, provide broad coverage across multiple tasks and languages. Specialized benchmarks like BEIR \citep{beir} focus on specific aspects, such as information retrieval. Although they incorporate some domain-specific datasets, such as FiQA \citep{FiQA}, the size of the data and the coverage of the task are limited. 
% Recent frameworks \citep{rageval} concentrate on particular use cases such as retrieval-augmented generation. Although these benchmarks offer valuable insights, they mainly focus on general domain performance or specific retrieval tasks, leaving a significant gap in comprehensive finance domain-specific embedding evaluation. 


\subsection{Domain Adaptation Approaches}
Recognizing the limitations of general-purpose models in specialized domains, researchers have pursued two main adaptation strategies. The first approach develops domain-specific models from scratch, exemplified by BioMedLM \citep{biomedlm} for biomedicine, SaulLM-7B \citep{saullm} for legal texts, and BloombergGPT \citep{bloomberggpt} for finance. The second strategy fine-tunes existing models for domain-specific tasks, as demonstrated by InvestLM \citep{investlm} and FinGPT \citep{fingpt}. This trend extends to embedding models, with specialized versions such as BioWordVec \citep{biowordvec}, BioSentVec \citep{biosentvec}, and FinBERT \citep{finbert} showing superior domain-specific performance. However, evaluating these specialized embedding models remains challenging due to the lack of comprehensive domain-specific benchmarks.


\subsection{The Gap in Domain-specific Evaluation}
While domain-specific language models have stimulated the development of specialized evaluation frameworks across various fields, these benchmarks primarily emphasize generative and reasoning capabilities instead of embedding quality. The financial sector has seen the emergence of frameworks like CFLUE \citep{CFLUE}, FinEval \citep{fineval}, and FinanceBench \citep{financebench}, whereas the legal and medical domains have introduced LawBench \citep{lawbench}, MedBench \citep{medbench}, and DrBenchmark \citep{drbenchmark}. These benchmarks consistently illustrate that general-purpose models often fall short in specialized areas \citep{CFLUE, lawbench}, highlighting the necessity of domain adaptation \citep{domainiskey}. Despite this acknowledgment, there is still a critical lack of comprehensive evaluation frameworks for domain-specific embeddings that assess performance across essential tasks such as semantic similarity, classification, and retrieval. Even recent financial embedding developments, such as BAM embedding~\citep{anderson-etal-2024-finance_text_embedding}, rely on narrow evaluation frameworks, typically focusing on single-task performance metrics (e.g., FinanceBench~\citep{financebench} for retrieval tasks). 
This limited evaluation may not fully reflect how the models perform in real-world financial applications.



% \subsection{General-purpose Embedding Models}
% Embedding models have evolved significantly from early word-level approaches like Word2Vec \citep{word2vector} and GloVe \citep{pennington-etal-2014-glove} to transformer-based models such as BERT \citep{Bert} and RoBERTa \citep{roberta}. These later models enabled contextualized word embeddings through deep bidirectional encoders. Sentence-BERT \citep{sentence-bert} further advanced the field by introducing Siamese and triplet networks for generating semantically meaningful sentence embeddings. Recent developments in LLMs have led to more powerful embedding models, such as e5-mistral-7b-instruct \citep{e5} and gte-Qwen2-1.5B-instruct \citep{qwen2}, achieving state-of-the-art performance across various NLP tasks.


% \subsection{Embedding Evaluation Benchmarks}
% Current embedding evaluation frameworks can be categorized into general-purpose and specialized benchmarks, each with distinct strengths and limitations. The Massive Text Embedding Benchmark (MTEB) \citep{mteb} represents a comprehensive general-purpose framework, evaluating models across multiple tasks and languages to assess their broad applicability and performance. In the specialized category, the BEIR benchmark \citep{beir} focuses specifically on information retrieval tasks, incorporating 18 diverse datasets. While BEIR includes some domain-specific collections, such as the financial QA dataset FiQA \citep{FiQA}, its coverage of specialized domains remains limited. This limitation becomes particularly apparent when evaluating models intended for specific professional or technical fields, where specialized vocabulary and context play crucial roles. More recent developments include scenario-specific benchmarks like RAGeval \citep{rageval}, which addresses the particular needs of retrieval-augmented generation systems. While these specialized benchmarks effectively evaluate retrieval performance in specific contexts, they often have a narrow focus. Important aspects of embedding quality, such as semantic similarity assessment and classification capabilities, are frequently overlooked.


% \subsection{Domain-Specific Models} 
% The adaptation of language models to specific domains typically follows two main strategies. The first approach involves training dedicated models from scratch on domain-specific corpora. Notable examples include BioMedLM \citep{biomedlm} for biomedical literature, SaulLM-7B \citep{saullm} for legal texts, and BloombergGPT \citep{bloomberggpt} for financial content. These models benefit from deep exposure to domain-specific patterns and terminology during the pre-training phase. The second strategy focuses on fine-tuning existing models for specialized tasks within a domain. In the financial sector, for example, models like InvestLM \citep{investlm} and FinGPT \citep{fingpt} have been instruction-tuned to perform specific financial analysis tasks. This approach leverages the general language understanding capabilities of base models while adapting them to domain-specific applications. Similar specialization trends are evident in embedding models, where domain-specific versions have been developed to better represent specialized vocabulary and concepts. For example, BioWordVec \citep{biowordvec} and BioSentVec \citep{biosentvec} provide specialized embeddings for biomedical terminology, while FinBERT \citep{finbert} offers targeted representations for financial language. These specialized embedding models demonstrate superior performance in their respective domains compared to general-purpose alternatives.


% \subsection{Domain-specific Model Benchmarks}
% The rise of domain-specific large language models (LLMs) has sparked the creation of specialized benchmarks across different professional areas.  These benchmarks serve a crucial role in evaluating how well LLMs can handle domain-specific knowledge and tasks. In the financial sector, several comprehensive benchmarks have emerged. CFLUE \citep{CFLUE} and FinEval \citep{fineval} assess financial language understanding, while DocMath-Eval \citep{DocMath-Eval} focuses on mathematical reasoning in financial documents. FinanceBench \citep{financebench} provides a broader evaluation framework for financial applications. Similar developments can be observed in other specialized fields. The legal domain has LawBench \citep{lawbench}, which evaluates legal reasoning and comprehension across multiple jurisdictions. In healthcare, MedBench \citep{medbench}, MedEval \citep{medeval}, and DrBenchmark \citep{drbenchmark} have been established to assess medical knowledge understanding and clinical reasoning capabilities. Most of these benchmarking papers conclude that general-purpose LLMs may fall short on domain tasks \citep{CFLUE, lawbench}. The importance of domain adaptation has gradually gained attention \citep{domainiskey}. However, to our knowledge, there is little work benchmarking the embedding model's performance on domain texts across different tasks.