\section{Related Works}
\subsection{High-dimensional Black-Box Optimization}
%Optimizing high-dimensional black-box functions is a challenging task. Furthermore, modeling large search spaces often requires a significant amount of observed data, necessitating scalable approaches. %While Bayesian Optimization (BO) has demonstrated effectiveness in low-dimensional black-box optimization, its performance deteriorates in high-dimensional settings due to the curse of dimensionality ____.

%Various approaches have emerged to solve the high-dimensional black-box optimization. In the literature on Bayesian Optimization (BO), most methods are based on local modeling or partitioning the search space to improve scalability. Specifically, TuRBO ____
%fits a collection of local models and restricts the candidate set to a small trust region. LA-MCTS ____ learns to partition the search space and identifies promising regions for sampling.
%Some approaches make structural assumptions that the high-dimensional objective function has a low active dimensional subspace ____ or that the objective function can be a sum of several low-dimensional functions ____. However, they often do not align with real-world applications and are not scalable to thousands of evaluations. Additionally, meta-algorithms, which add extra modules to the existing high-dimensional BO methods such as adding MCMC (Markov Chain Monte Carlo) steps ____ or CMA strategy ____ also emerged to improve sample efficiency.

%On the other hand, evolutionary search methods such as covariance matrix adaptation evolution strategy (CMA-ES) ____ also showed powerful capability. 
 
%Other approaches, which rely on specific assumptions about the objective functions , often make strong assumptions that do not align well with real-world problems, thereby limiting their practical applicability.

%MCMC-BO ____ incorporates Markov Chain Monte Carlo (MCMC) steps, and CMA-BO ____ adopts a Covariance Matrix Adaptation strategy to existing BO algorithms, improving their performance. 

Various approaches have been proposed to address high-dimensional black-box optimization. In Bayesian optimization (BO), some methods assume that objective function can be decomposed into a sum of several low-dimensional functions and train a large number of Gaussian processes (GPs) ____. However, relying on training multiple GPs reduces its scalability to a large number of evaluations.
Other approaches assume that high-dimensional objective functions reside in a low-dimensional active subspace and introduce mapping to low-dimensional spaces
____. However, these methods make strong assumptions that often fail to align with real-world problems.

Another line of BO methods utilizes local modeling or partitioning of the search space to address high dimensionality and scalability. TuRBO ____ fits multiple local models and restricts the search space to small trust regions to improve scalability. LA-MCTS ____ trains an unsupervised K-means classifier to partition the search space, identifies promising regions for sampling, and then employs BO-based optimizers such as BO or TuRBO. Including LA-MCTS, meta-algorithms that can be adapted to existing high-dimensional BO methods have also emerged. MCMC-BO ____ adopts Markov Chain Monte Carlo (MCMC) to adjust a set of candidate points towards more promising positions, and CMA-BO ____ utilizes covariance matrix adaptation strategy to define a local region that has the highest probability of containing global optimum.

Evolutionary Algorithms (EAs) are also widely applied in high-dimensional black-box optimization. The most representative method, the covariance matrix adaptation evolution strategy (CMA-ES) ____, operates by building and refining a probabilistic model of the search space. It iteratively updates the covariance matrix of a multivariate Gaussian distribution to capture the structure of promising solutions and uses feedback from the objective function to guide the search towards the optimal solution.

%Various approaches have been proposed to address high-dimensional black-box optimization. In Bayesian Optimization (BO), some methods propose local modeling or partitioning the search space for scalability. TuRBO ____ fits multiple local models and restricts the search space to small trust regions to improve scalability. LA-MCTS ____ trains an unsupervised K-means classifier to partition the search space, identifies promising regions for sampling, and then employs BO-based optimizers such as BO or TuRBO.
%Similar to LA-MCTS, meta-algorithms that can be adapted to existing high-dimensional BO methods have also emerged. For example, MCMC-BO ____ incorporates Markov Chain Monte Carlo (MCMC) steps, and CMA-BO ____ adopts a Covariance Matrix Adaptation (CMA) strategy to other BO methods. 

%Another line of BO assumes that high-dimensional objective functions reside in a low active dimensional subspace and leverage strategies like embeddings or variable selection ____. 
%Some methods combine multiple Gaussian Processes (GPs), assuming that the objective function can be decomposed into a sum of several low-dimensional functions ____. However, these assumptions often fail to align with real-world problems and lack scalability when thousands of evaluations are required.

%Besides BO-based strategies, Evolutionary Algorithms (EAs) are also widely applied in high-dimensional black-box optimization. The most representative method, the Covariance Matrix Adaption evolution strategy (CMA-ES) ____, works by maintaining a probabilistic model of the optimal solution and iteratively improving this model by adjusting the covariance matrix based on feedback from the objective function. 


\subsection{Generative Model-based Optimization}
%Generative models ____ have shown remarkable capabilities in modeling high-dimensional data across various domains, including images ____, text ____, and more ____. 


%Flow: 1. 다양한 method 들이 Generative model 을 이용해서 black-box optimization을 풀었다. 대부분은 inverse mapping과 target conditioning으로 generated 된 sample을 query 한느 방식으로 접근했음.
%Expressivity 내용 언급 필요, Uncertainty 추정 안된다는 내용 언급 필요. 

Several methods have been developed that utilize generative models to optimize black-box functions. Most approaches learn an inverse mapping from function values to the input domain and propose promising solutions by sampling from the trained model, conditioned on a high score ____.

%최근 Diffusion model이 high-dimensional 한 상황에서도 잘 된다는 연구가 계속 등장하고 있음.
%이를 발판으로 여러 diffusion model based algorithm들이 등장하기 시작함.
%대표적인 method로 DDOM 그리고, Diff-BBO가 있음. 

Building on the success of diffusion models, widely known for their expressivity in high-dimensional spaces ____, leveraging diffusion models for black-box optimization has also emerged 
____. 
DDOM ____ trains a conditional diffusion model with classifier-free guidance ____ and incorporates reweighted training to enhance the performance. 
Diff-BBO ____ trains an ensemble of conditional diffusion models, then employs an uncertainty-based acquisition function to select the conditioning target value during candidate sampling.

% While Diff-BBO is closely related to our work, particularly its utilization of diffusion models and its focus on online scenarios, it introduces significant computational burdens. %This stems from the necessity of training multiple diffusion models, evaluating uncertainty through repeated sampling, and optimizing acquisition functions, which requires multiple uncertainty evaluations. 
% Furthermore, it struggles to scale up to high dimensionality, losing its ability to capture uncertainty accurately
% In contrast, our approach alleviates the computational burden by introducing a moving dataset and effectively scaling up to high-dimensional tasks with our posterior sampling strategy.

Diff-BBO is closely related to our work, particularly in its use of diffusion models and its focus on online scenarios. However, utilizing multiple diffusion models results in a significant computational burden throughout the optimization process. Additionally, estimating uncertainty using diffusion models in high-dimensional spaces remains challenging, which can impact the effectiveness of uncertainty-based acquisition and may lead to suboptimal results.
In contrast, our approach alleviates the computational burden by introducing a moving dataset and effectively scaling up to high-dimensional tasks with our posterior sampling strategy.

%Various generative model-based algorithms have recently been applied to black-box optimization, particularly in offline settings. Most approaches learn an inverse mapping from function values to the input domain and propose promising solutions by sampling from the trained model, conditioned on a high score ____. %or by approximating it ____.

%Building on the success of diffusion models in various fields, several diffusion-based optimization approaches have been proposed ____. For instance, DDOM ____ leverages a conditional diffusion model trained with classifier-free guidance and incorporates reweighted training to enhance performance. Diff-BBO ____ follows a similar method to DDOM. It further utilizes ensembles of conditional diffusion models to estimate epistemic uncertainty and selects the appropriate conditioning target value, optimizing an acquisition function based on uncertainty.

%While Diff-BBO is closely related to our work, particularly its utilization of diffusion models and its focus on online scenarios, it introduces significant computational burdens. This stems from the necessity of training multiple diffusion models, evaluating uncertainty through repeated sampling, and optimizing acquisition functions, which requires multiple uncertainty evaluations. Furthermore, it struggles to scale up to high dimensionality, losing its ability to capture uncertainty accurately. In contrast, our approach alleviates the computational burden by introducing a moving dataset and effectively scaling up to high-dimensional tasks with our posterior sampling strategy.


% DIff-BBO가 우리가 풀려고 하는 문제랑 가장 가까운 세팅임을 언급하고, 한계도 설명하면 좋을듯 (diffusion ensemble이 시간이 오래 걸린다던가 등등)
% For example, the method CbAS ____ employs a Variational Autoencoder (VAE) to approximate the posterior \( p_\theta(\mathbf{x}|S) \), where \( S \) denotes the target set, using sequential training with importance sampling iteratively. Additionally, Model Inversion Networks (MINs) ____ leverage conditional GANs with reweighted training and employ approximated Thompson sampling to generate candidate solutions.

\subsection{Amortized Inference in Diffusion Models}
As diffusion models generate samples through a chain of stochastic transformations, sampling from the posterior distribution $p^{\text{post}}(\mathbf{x}) \propto p_\theta(\mathbf{x})r(\mathbf{x})$ is intractable. One of the widely used methods is estimating the guidance term by training a classifier on noised data ____. However, such data is unavailable in most cases, and it is often hard to train such a classifier in high-dimensional settings. Although Reinforcement learning (RL) methods have recently been proposed and shown interesting results ____, naive RL fine-tuning does not provide an unbiased sampler of the target distribution ____. To this end, we choose a relative trajectory balance proposed by ____ to obtain an unbiased sampler of the posterior distribution without training an additional classifier. Furthermore, we propose two post-processing strategies, local search and filtering, to improve the sample efficiency of our method.