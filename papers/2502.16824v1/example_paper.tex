%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage[dvipsnames]{xcolor}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{comment}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{multirow}

% if you use cleveref..
\usepackage[noabbrev,capitalise,nameinlink]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
% \usepackage{xcolor}
% \usepackage[dvipsnames]{xcolor}
% \usetikzlibrary{shapes.misc}

% Define a new command for the small orange star
\newcommand{\fcircle}[2][red,fill=red]{\tikz[baseline=-0.5ex]\draw[#1,radius=#2] (0,0.03) circle ;}
\newcommand{\fstarorange}[2][red,fill=red]{\tikz[baseline=-0.5ex]\node [star, star point height=5.0, star point ratio=0.5, minimum size=0.01cm, rotate=36, draw, fill=orange] at (0,0.03) {};}
\newcommand{\fstarorchid}[2][red,fill=red]{\tikz[baseline=-0.5ex]\node [star, star point height=5.0, star point ratio=0.5, minimum size=0.01cm, rotate=36, draw, fill=Orchid] at (0,0.03) {};}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Exploring High-dimensional Space with Diffusion Models for Scalable Black-box Optimization}
\icmltitlerunning{Posterior Inference with Diffusion Models
for High-dimensional Black-box Optimization}

\begin{document}

\twocolumn[
% \icmltitle{Amortizing Intractable Inference in Diffusion Models \\
% for High-dimensional Black-box Optimization}
% \icmltitle{Exploring High-dimensional Space with Diffusion Models \\
% for Scalable Black-box Optimization}
% \icmltitle{Diffusion Models Solve High-dimensional Black-box Optimization}
\icmltitle{Posterior Inference with Diffusion Models \\ for High-dimensional Black-box Optimization}

% Exploring Chemical Space with Score-based Out-of-distribution Generation
% Amortizing intractable inference in diffusion models for vision, language, and control
% Diffusion Models for Black-Box Optimization

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Taeyoung Yun}{yyy,equal}
\icmlauthor{Kiyoung Om}{yyy,equal}
\icmlauthor{Jaewoo Lee}{yyy}
\icmlauthor{Sujin Yun}{yyy}
\icmlauthor{Jinkyoo Park}{yyy}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Korea Advanced Institute of Science and Technology (KAIST)}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Taeyoung Yun}{99yty@kaist.ac.kr}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.



\begin{abstract}
% High-dimensional black-box optimization (BBO) has extensive applications but remains formidable. 
% The most suggestive method, Bayesian Optimization 
% (BO), is sample efficient while it struggles with a curse of dimensionality and a large number of evaluations.
% While several approaches in high-dimensional BO have been suggested to mitigate the problem by constructing trust regions or partitioning search space, they still suffer from poor sample efficiency. (Abstract 너무 길어서 이 문장 일단 뺌)
% Generative model-based methods have shown excellent capabilities in BBO, but these are often limited to offline, one-shot settings.
% To solve these challenges, we propose \textit{Posterior Inference Black-Box Optimization} (PI-BBO), an efficient, scalable method that leverages generative models for high-dimensional BBO.
% Our method iteratively repeats two stages: 
% First, training a diffusion model to capture the underlying data distribution along an ensemble of proxies for predicting function values with uncertainty estimation; 
% Second, we formulate the candidate selection process as a posterior inference problem and amortize it fine-tuning the diffusion model to efficiently sample from the posterior distribution.
% Second, we select candidates to query the black-box functions by utilizing the trained models. Specifically, we formulate the candidate selection as a posterior inference problem to balance exploration and exploitation trade-offs. Then, we employ fine-tuning diffusion models to amortize the sampling from the posterior distribution efficiently. 
% Extensive experiments on high-dimensional and large-scale BBO tasks demonstrate that our method outperforms its competitive baselines.

% While several approaches in high-dimensional BO have been suggested to mitigate the problem by constructing trust regions or partitioning search space, they still suffer from poor sample efficiency. (Abstract 너무 길어서 이 문장 일단 뺌)

% To accomplish this, we fine-tune the diffusion model that amortizes the sampling from the product distribution. We find that our candidate selection strategy efficiently balances exploration and exploitation in high-dimensional settings. (이것도 길어서 뺌)

% Second, we sample candidates from the product distribution of the upper confidence bound of proxy predictions and the likelihood of the diffusion model to balance exploration and exploitation in high-dimensional spaces.

% Recently, leveraging expressive generative models to solve optimization problems has emerged as a promising framework, especially in offline settings. 
% However, extending this framework into an online setting remains underexplored.
% In this paper, we introduce \textbf{DiBO}, a novel generative model-based approach for solving high-dimensional black-box optimization problems. 

Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications.
While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations. 
Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework.
However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces.
To overcome these issues, we introduce \textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems.
Our method iterates two stages. First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification.
Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces. Concretely, we fine-tune diffusion models to amortize posterior inference.
Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across various synthetic and real-world black-box optimization tasks. Our code is publicly available \href{https://github.com/umkiyoung/DiBO}{here}.
\end{abstract}

% 기영 다른 Bayesian optimization논문들에서 많이 인용하는 논문들을 넣어주는 게 좋을듯 내가 참고할 만한 논문들 몇개 적어줄게
% Scalable Global Optimization via Local Bayesian Optimization
% Discovering Many Diverse Solutions with Bayesian Optimization
% Sample-Efficient Bayesian Optimization with Transfer Learning for Heterogeneous Search Spaces

% Bayesian Optimization (BO) \cite{7352306, brochu2010tutorialbayesianoptimization} is a powerful method for solving black-box optimization. However, BO scales poorly to high dimensions due to the curse of dimensionality \cite{kandasamy2015high, letham2020re} and struggles to handle large-scale evaluations 
% \cite{wang2018batched, eriksson2019scalable}. While several approaches have been suggested to scale up BO for high-dimensional optimization problems \cite{eriksson2019scalable,wang2020learning,yi2024improving,ngo2024high}, most of them include partitioning the search space and performing optimization in local regions, which still requires a large number of evaluations to discover promising regions.

% Several approaches have been suggested to scale up BO for high-dimensional optimization problems 
% including variable selection \cite{chen2012joint}, linear embedding \cite{garnett2014active, letham2020re}, assuming additive structures of the target function \cite{duvenaud2011additive}.
% In addition, partitioning the search space and optimizing in local regions \cite{eriksson2019scalable, wang2020learning} were suggested to handle a large number of evaluations. 
% However, these methods still suffer from low sample efficiency. %require a large number of evaluations to discover promising regions. 

% which may not escape from local optimum within a limited number of evaluations.


% A variety of BO approaches aim to alleviate these issues. Some rely on structural assumptions—for instance, that the high-dimensional objective function lies in a low-dimensional subspace \cite{chen2012joint, letham2020re} or can be decomposed into a sum of low-dimensional functions \cite{duvenaud2011additive}. Alternatively, methods such as local modeling \cite{eriksson2019scalable} or search space partitioning \cite{wang2020learning} emerged to address scalability. However, while the former often makes strong assumptions that do not hold for many real-world problems, the latter still suffers from sample inefficiency. 

%Recently, generative model-based approaches have shown promising results in black-box optimization \cite{kumar2020model, brookes2019conditioning,krishnamoorthy2023diffusion}. These approaches utilize an inverse mapping from function values to the input domain and propose promising designs via sampling from the trained model conditioned by a high score.  
%However, most existing approaches focus on offline settings or rely on access to large, labeled datasets, making the application of these approaches into real-world applications challenging. In practice, we lack large-scale datasets and should repeat the procedure of proposing promising candidates and updating the model with the new evaluations to find an optimal design \cite{jain2022biological, wu2024diff}. Therefore, developing an efficient exploration strategy and continuously adapting generative models with evolving datasets is crucial for the success of generative model-based approaches in practical black-box optimization problems. Unfortunately, research on this perspective has yet to be explored.
% While most approaches focus on offline settings, some methods have also explored online scenarios \cite{kumar2020model, brookes2019conditioning, wu2024diff}. 
% or the  difficulty of estimating uncertainty in generative models %with an inverse mapping
%\cite{wu2024diff}.
%However, the limited expressiveness of earlier generative models, such as VAEs \cite{brookes2019conditioning} or GANs \cite{kumar2020model}, often fails to capture high-dimensional data distributions. While there is a more expressive diffusion model-based approach \cite{wu2024diff}, it struggles to estimate uncertainty in higher-dimensional spaces, 
% Furthermore, developing an efficient exploration strategy and continuously adapting generative models with evolving datasets is crucial for the success of generative model-based approaches. Unfortunately, research on this perspective has yet to be explored.
% However, most existing approaches focus on offline settings, giving access to a substantial, fixed dataset and employing a generative model to capture its distribution. In practice, we lack large-scale datasets and require efficient exploration to acquire new data from the black-box function. Furthermore, it leads to the distributional shift of the dataset. Therefore, adapting generative models to this evolving distribution is a critical and challenging research direction that has yet to be explored.

% To this end, we introduce a novel generative model-based approach for high-dimensional black-box optimization that enables scalable evaluations with sample efficiency. Three key intuitions drove our method. 

% Exploring both high-value and uncertain regions is crucial to achieving higher sample efficiency. We trained an ensemble of neural network proxies capable of predicting function values and their uncertainties, utilizing the Upper Confidence Bound (UCB) to encourage exploration.

% In many cases, evaluating a large number of observations involves querying samples in batches \cite{wang2018batched}. We adopted a sampling-based approach to ensure diversity within each batch and maximize information gain by preventing redundant evaluations. 

% Trust-region methods \cite{eriksson2019scalable} have demonstrated significant capabilities in high-dimensional settings. These methods consider not only the predicted values but also focus on searching within reliable regions, thereby enhancing the stability and efficiency of the optimization process.

% By incorporating these elements, we formulate the candidate selection process as sampling from a posterior distribution, considering UCB, and the prior data distribution. This integrated approach significantly enhances the efficiency and reliability of batch selection for high-dimensional, large-scale observation evaluations. 

%In many cases, evaluating a large number of observations involves querying samples in batches \cite{wang2018batched}. Three key factors must be considered to improve sample efficiency in this high-dimensional, batch-wise context: exploration toward high-value estimates, diversity within batches, and querying reliable regions.  

%To address these factors, we first train an ensemble of neural network proxies to estimate function values and uncertainties, utilizing the Upper Confidence Bound (UCB) to encourage exploration. Secondly, we employ a stochastic sampling approach to ensure diversity within each batch, maximizing information gain by preventing redundant evaluations. Finally, inspired by trust region methods \cite{ eriksson2019scalable}, we query samples within reliable regions not too far from the existing data distribution to utilize our evaluation budget efficiently. 

%By incorporating all of these, we formulate the candidate selection process as a sampling from a posterior, which considers both the UCB and the data distribution prior. This integrated approach enhances the efficiency and reliability of batch selection for high-dimensional, large-scale observation evaluations. 

% Our methods iterates two phases: 

%\textbf{Phase 1: Model Training}  
%We begin by training an ensemble of neural network proxies to predict function values and model uncertainty, utilizing the UCB to encourage the exploration. Simultaneously, we train a diffusion model to capture the data distribution. While training both models, we selectively stored high-value samples in the buffer along with a weighted sampler to concentrate effectively on valuable data points.  

%\textbf{Phase 2: Selecting Candidates}  
%In the second phase, we fine-tune the diffusion model to approximate the intractable posterior distribution, enabling efficient sampling from the desired regions. Additionally, we perform an extra local search to enhance sample efficiency, ensuring that the selected samples achieve high log-posterior. 

% \textbf{Phase 1: Model Training}  
% We begin by training an ensemble of neural network proxies to predict the function value and estimate the uncertainty, along with a diffusion model to capture the current data distribution. Subsequently, we fine-tune the diffusion model to approximate the intractable posterior distribution, enabling efficient sampling from the desired regions.

% \textbf{Phase 2: Selecting Candidates}  
% In the second phase, we sample candidates from the diffusion posterior. Then, we perform an extra local search of the sampled candidates to enhance efficiency and ensure that the selected samples achieve better log-posterior.

% Extensive experiments on high-dimensional synthetic functions and Real-world tasks show that our proposed method strictly outperforms existing SOTA Bayesian Optimization and generative model-based algorithms.


%\begin{itemize}
%    \item Black-box optimization (BBO) is important 
%    \item Most widely used method: Bayesian Optimization (BO)
%    \begin{itemize}
%        \item Challenges: Curse of dimensionality, Hard to scale up thousands of evaluations
%    \end{itemize}
%    \item Recently, generative models have demonstrated powerful capabilities\~ and also in optimization, especially in offline setting (MBO)
%    \begin{itemize}
%        \item Challenges: Focused on one-shot setting
%    \end{itemize}
%    \item Propose a novel method for solving high-dimensional black-box optimization problem with generative modeling
%    \item Our method consists of two stages:
%    \begin{itemize}
%        \item Train diffusion model to capture data distribution and surrogate model for prediction
%        \item Select candidates to query at the next round. We formulate candidate selection as a posterior inference problem and amortize inference by fine-tuning diffusion models
%    \end{itemize}
%    \item Conduct extensive experiments on several black-box functions. Competitive results compared to state-of-the-art baselines
%\end{itemize}
% To this end, we introduce \textbf{Di}ffusion models for high-dimensional \textbf{B}lack-box \textbf{O}ptimization (\textbf{DiBO}), a novel generative model-based approach for scalable black-box optimization. Our method consists of two stages. First, we train a diffusion model to effectively capture high-dimensional data distribution and an ensemble of proxies to predict the function values with uncertainty quantification. 
% Second, we sample candidates from the posterior distribution where the diffusion model serves as a prior and the upper confidence bound (UCB) of proxy predictions as a likelihood to balance exploration and exploitation in high-dimensional spaces.

%다루어야 할 내용들. Sample efficiency가 떨어지는 BO method -> Efficient해짐.
%앞서 uncertainty estimation이 잘 안된다고 했는데, 여기서는 왜 잘되는지?가 해소가 안되는 느낌. 
%Expressivity (solved)
%Efficient Exploration Strategy
%adapting models with evolving datasets

%다루어야 할 내용들. Sample efficiency가 떨어지는 BO method -> Efficient해짐.
%앞서 uncertainty estimation이 잘 안된다고 했는데, 여기서는 왜 잘되는지?가 해소가 안되는 느낌. 
%Expressivity (solved)
%Efficient Exploration Strategy
%adapting models with evolving datasets
 
% Version 1
% To address these issues, we introduce \textbf{Di}ffusion models for high-dimensional \textbf{B}lack-box \textbf{O}ptimization (\textbf{DiBO}), a novel generative model-based approach for high-dimensional black-box optimization. 
% Our method consists of two stages. 
% First, we train a diffusion model to effectively capture high-dimensional data distribution and an ensemble of proxies to predict the function values with uncertainty quantification.
% Second, we sample candidates from the posterior distribution where the diffusion model serves as a prior and the upper confidence bound (UCB) \cite{auer2002using} of proxy predictions as a likelihood to balance exploration and exploitation in high-dimensional spaces.

% In the training stage, we adopt a reweighted training scheme proposed in prior generative model-based approaches to focus on high-scoring data points \cite{kumar2020model,krishnamoorthy2023diffusion,kim2024bootstrapped}. Furthermore, to promote exploration during sampling stage, we estimate the uncertainty in neural networks by introducing ensembles of proxies \cite{lakshminarayanan2017simple}.

% In the sampling stage, we propose candidates to query the black-box function. In BO literature, most approaches search for an input that maximizes the acquisition function, such as the UCB score. However, in high-dimensional spaces, the acquisition function is often highly non-convex and contains numerous local optima, making it challenging to find such input points \cite{ament2023unexpected}. Furthermore, as the data points lie on a tiny manifold compared to the whole search space, the uncertainty tends to become extremely high in the regions too far from the dataset \cite{oh2018bock}. Thus, naively choosing the candidate that maximizes the acquisition function may lead to sub-optimal results, as illustrated in \Cref{fig:motivation}.

% % To overcome these issues, our key idea is to generate samples to query the black-box function from the product distribution of the likelihood and UCB score.
% To overcome these issues, our key idea is to cast the candidate selection problem as a posterior inference problem. Specifically, we sample promising candidates from the posterior distribution where a pre-trained diffusion model serves as a prior that is multiplied by the UCB score.
% Instead of directly searching for input that maximizes the UCB score, sampling from the posterior distribution prevents us from choosing the samples that lie too far from the current dataset, which may lead to sub-optimal results. As the sampling from the posterior distribution is intractable, we train an amortized sampler that can generate unbiased samples from the posterior distribution by fine-tuning the diffusion model.  
% While the posterior distribution is still highly non-convex, we can capture the complex multi-modal distribution by exploiting the expressivity of the diffusion models.

% Our key observation is that it is much easier to estimate the uncertainty of predictions than that of generative models. While one can build an acquisition function akin to prior BO methods and search for an input that maximizes the function using generative models, there are several remaining challenges. 
% First, it is challenging to find such inputs in high-dimensional spaces since the acquisition function is often highly non-convex and contains numerous local optima \cite{ament2023unexpected}.
% Thus, naively choosing the candidate that maximizes the acquisition function may lead to sub-optimal results.
% While \citet{wu2024diff} employ diffusion models and propose a novel acquisition function for selecting conditioning value, it is hard to estimate the uncertainty of the diffusion model in high-dimensional spaces 
% and choosing a single value that maximizes the acquisition function based on inaccurate uncertainty can lead to sub-optimal results.
% Version 2
% While several prior methods search for an input that maximizes the acquisition function, it is challenging to find such input points in high-dimensional spaces since the acquisition function is often highly non-convex and contains numerous local optima \cite{ament2023unexpected}. Furthermore, as the data points lie on a tiny manifold compared to the entire search space, the uncertainty becomes extremely high in regions too far from the dataset \cite{oh2018bock}. Thus, naively choosing the candidate that maximizes the acquisition function may lead to sub-optimal results. 

% One possible approach for scaling generative model-based approaches up to high-dimensional black-box optimization is measuring uncertainty with the proxy model and finding an input that maximizes the acquisition function using generative models. However, it is challenging to find such input points in high-dimensional spaces since the acquisition function is often highly non-convex and contains numerous local optima \cite{ament2023unexpected}. Furthermore, as the data points lie on a tiny manifold compared to the entire search space, the uncertainty becomes extremely high in regions too far from the dataset \cite{oh2018bock}. Thus, naively choosing the candidate that maximizes the acquisition function may lead to sub-optimal results.

% Our key motivation is that it is much easier to estimate the uncertainty of predictions than that of generative models. While one can build an acquisition function akin to prior BO methods and search for an input that maximizes the function using generative models, there are several remaining challenges. First, it is challenging to find such inputs in high-dimensional spaces since the acquisition function is often highly non-convex and contains numerous local optima \cite{ament2023unexpected}. Furthermore, as the data points lie on a tiny manifold compared to the entire search space, the uncertainty becomes extremely high in regions too far from the dataset \cite{oh2018bock}. 
% % Thus, naively choosing the candidate that maximizes the acquisition function may lead to sub-optimal results.
% Specifically, we sample promising candidates from the posterior distribution, where a pre-trained diffusion model serves as a prior that is multiplied by the upper confidence bound (UCB) score from proxy predictions.
% While several prior methods search for an input that maximizes the acquisition function, it is challenging to find such input points in high-dimensional spaces since the acquisition function is often highly non-convex and contains numerous local optima \cite{ament2023unexpected}. Furthermore, as the data points lie on a tiny manifold compared to the entire search space, the uncertainty becomes extremely high in regions too far from the dataset \cite{oh2018bock}. Thus, naively choosing the candidate that maximizes the acquisition function may lead to sub-optimal results. 
% During training, we adopt a reweighted training scheme to focus on high-scoring data points.
% Second, we sample candidates from the posterior distribution to balance exploration and exploitation in high-dimensional spaces.

\section{Introduction}\label{sec:intro}
Optimizing high-dimensional and complex black-box functions is crucial in various scientific and engineering applications, including hyperparameter optimization \cite{snoek2012practical, turner2021bayesian}, chemical engineering \cite{hernandez2017parallel}, drug discovery  \cite{negoescu2011knowledge}, and control systems \cite{candelieri2018bayesian}. 

Bayesian optimization (BO) \cite{brochu2010tutorial, shahriari2015taking} is a powerful method for solving black-box optimization. 
BO constructs a surrogate model from observed data and finds an input that maximizes the acquisition function to query the black-box function.
However, it scales poorly to high dimensions due to the curse of dimensionality \cite{kandasamy2015high, letham2020re} and struggles scaling to thousands of evaluations \cite{wang2018batched, eriksson2019scalable}. 
To address these challenges, several approaches have been suggested to scale up BO for high-dimensional optimization problems. 
Some works propose a mapping from high-dimensional space into low-dimensional subspace \cite{nayebi2019framework, letham2020re} or assume additive structures of the target function \cite{duvenaud2011additive, rolland2018high} to perform optimization in low-dimensional spaces. However, these methods often rely on unrealistic assumptions.


Other works partition the search space into promising local regions and search candidates within such regions \cite{eriksson2019scalable, wang2020learning}, which exhibits promising results.
However, these methods may struggle to escape from local optima within a limited number of evaluations due to several challenges.
First, it is notoriously difficult to find an input that maximizes the acquisition function in high-dimensional spaces since the function is often highly non-convex and contains numerous local optima \cite{ament2023unexpected}.
Furthermore, as the data points lie on a tiny manifold compared to the entire search space, the uncertainty becomes extremely high in regions too far from the dataset \cite{oh2018bock}. 


Recently, generative model-based approaches have shown promising results in black-box optimization \cite{ brookes2019conditioning,kumar2020model,wu2024diff}. These methods utilize an inverse mapping from function values to the input domain and propose candidates via sampling from the trained model conditioned on a high score. 
While they alleviate aforementioned issues in BO by converting optimization problem as sampling \cite{janner2022planning}, their performance degrades in higher dimensions due to the limited expressivity of underlying models \cite{brookes2019conditioning,kumar2020model} or the difficulty of uncertainty estimation in high-dimensional spaces \cite{wu2024diff}.
 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/motivation.pdf}
    \vspace{-5pt}
    \caption{Motivating example of our method. In high-dimensional spaces, directly searching for input that maximizes UCB score may lead to sub-optimal results (As depicted in \fstarorange[fill=orange]{3pt} of the second figure). Sampling from the posterior distribution prevents overemphasized exploration in the boundary of search space and leads to efficient exploration (As depicted in \fstarorchid[fill=Orchid]{3pt} of the last figure).}
    \label{fig:motivation}
    \vspace{-10pt}
\end{figure*}

To overcome these issues, we introduce \textbf{Di}ffusion models for \textbf{B}lack-box \textbf{O}ptimization (\textbf{DiBO}), a novel generative model-based approach for high-dimensional black-box optimization. 
Our key idea is to cast the candidate selection problem as a posterior inference problem.
Specifically, we first train a proxy and diffusion model, which serves as a reward function and the prior. Then, we construct a posterior distribution by multiplying two components and sample candidates from the posterior to balance exploration and exploitation in high-dimensional spaces.
Instead of searching for input that maximizes the acquisition function, sampling from the posterior prevents us from choosing the samples that lie too far from the dataset, as illustrated in \Cref{fig:motivation}.

Our method iterates two stages. First, we train a diffusion model to effectively capture high-dimensional data distribution and an ensemble of proxies to predict function values with uncertainty quantification. 
During training, we adopt a reweighted training scheme proposed in prior generative model-based approaches to focus on high-scoring data points \cite{kumar2020model,krishnamoorthy2023diffusion}. 
Second, we sample candidates from the posterior distribution.
As the sampling from the posterior distribution is intractable, we train an amortized sampler to generate unbiased samples from the posterior by fine-tuning the diffusion model. 
While the posterior distribution remains highly non-convex, we can capture such complex and multi-modal distribution by exploiting the expressivity of the diffusion models. 
By repeating these two stages, we progressively get close to the high-scoring regions of the target function.


As we have a limited budget for evaluations, it is beneficial to choose modes of the posterior distribution as proposing candidates. To accomplish this, we propose two post-processing strategies after training the amortized sampler: local search and filtering. Concretely, we generate many samples from the amortized sampler and improve them via local search. Then, we filter candidates with respect to the unnormalized posterior density. By incorporating these strategies, we can further boost the sample efficiency of our method across various optimization tasks.

We conduct extensive experiments on four synthetic and three real-world high-dimensional black-box optimization tasks. We demonstrate that our method achieves superior performance on a variety of tasks compared to state-of-the-art baselines, including BO methods, generative model-based methods, and evolutionary algorithms.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\textwidth]{figures/overview.pdf}
    \caption{Overview of our method. \textbf{Phase 1:} Train diffusion models and ensemble of proxies. \textbf{Phase 2:} Sampling candidates from the posterior distribution and post-processing via local search and filtering. Then, we evaluate samples, update the dataset, and repeat the process until we find an optimal design.}
    \label{fig:overview}
    \vspace{-10pt}
\end{figure*}


% where the search is performed over $R$ rounds, each containing $B$ queries. 
% The evaluation of function $f$ is assumed to be expensive, which is common in many real-world scenarios\cite{}, achieving high sample efficiency becomes critical.


\section{Preliminaries}
\subsection{Black-box Optimization}
In black-box optimization, our objective is to find a design \(\mathbf{x}\in\mathcal{X}\) that maximizes the target black-box function \(f(\mathbf{x})\). 
\begin{align}
    &\text{find }\mathbf{x}^{*}=\arg\max_{\mathbf{x}\in\mathcal{X}}f(\mathbf{x} )\nonumber
    \\
    &\text{ with }R\text{ rounds of }B\text{ batch of queries}
\end{align}
Querying designs in batches is practical in many real-world scenarios, such as biological sequence designs \citep{jain2022biological, kim2024improved}. 
As the evaluation process is expensive in most cases, developing an algorithm with high sample efficiency is critical in black-box optimization.


% (where $\mathbf{x}_0=\mathbf{x}$) to a target distribution \(q_{0}\), i.e., $p_{\theta}(\mathbf{x}_0)\approx q_0(\mathbf{x}_0)$. 
% Specifically, we introduce latent variables \(\mathbf{x}_{1}, \dots, \mathbf{x}_{T}\) and the marginal distribution \(p_{\theta}(\mathbf{x}_0)\) can be obtained from the joint distribution \(p_{\theta}(\mathbf{x}_{0:T})\) as follows:
% More specifically, the model \(p_{\theta}(\mathbf{x}_0)\) is obtained by marginalizing out latent variables \(\mathbf{x}_{1}, \dots, \mathbf{x}_{T}\) from a joint distribution \(p_{\theta}(\mathbf{x}_{0:T})\):
% \begin{align}
%     p_\theta(\mathbf{x}_0)=\int p_{\theta}(\mathbf{x}_{0:T}) \, d\mathbf{x}_{1:T},
% \end{align}
% where \(\mathbf{x}_0\) and \(\mathbf{x}_{1}, \dots, \mathbf{x}_{T}\) share the same dimensionality. 

\subsection{Diffusion Probabilistic Models}
Diffusion probabilistic models~\cite{sohl2015deep, ho2020denoising} are a class of generative models that aim to approximate the true distribution $q_0(\mathbf{x}_0)$ with a parametrized model of the form: $p_{\theta}(\mathbf{x}_0)=\int p_{\theta}(\mathbf{x}_{0:T}) \, d\mathbf{x}_{1:T}$, where $\mathbf{x}_0$ and latent variables $\mathbf{x}_1,\cdots,\mathbf{x}_{T}$ share the same dimensionality. 
The joint distribution \(p_{\theta}(\mathbf{x}_{0:T})\), often referred to as the \emph{reverse process}, is defined via a Markov chain that starts from a standard Gaussian prior \(p_{T}(\mathbf{x}_{T}) = \mathcal{N}(\mathbf{0}, \mathbf{I})\):
\begin{align}
    \label{eq:reverse process}
        p_{\theta}(\mathbf{x}_{0:T}) 
    = p_{T}(\mathbf{x}_{T})
      \prod_{t=1}^T p_{\theta}(\mathbf{x}_{t-1}\vert\mathbf{x}_{t}),
    \\
    p_{\theta}(\mathbf{x}_{t-1}\vert\mathbf{x}_{t}) 
    = \mathcal{N}\bigl(\mu_{\theta}(\mathbf{x}_{t}, t), \mathbf{\Sigma}_{t}\bigr).
\end{align}
\(p_{\theta}(\mathbf{x}_{t-1}\vert\mathbf{x}_{t})\) is a Gaussian transition from step \(t\) to \(t-1\).

We choose \emph{forward process} as fixed to be a Markov chain that progressively adds Gaussian noise to the data according to a variance schedule \(\beta_{1}, \dots, \beta_{T}\):
\begin{align}
    \label{eq:forward process}
        q(\mathbf{x}_{1:T}\vert\mathbf{x}_{0}) 
    = \prod_{t=1}^T q\bigl(\mathbf{x}_{t}\vert\mathbf{x}_{t-1}\bigr),
    \\
    q\bigl(\mathbf{x}_{t}\vert\mathbf{x}_{t-1}\bigr) 
    = \mathcal{N}\bigl(\sqrt{1-\beta_{t}} \, \mathbf{x}_{t-1}, \beta_{t} \mathbf{I}\bigr).
\end{align}

\paragraph{Training Diffusion Models.}
We can train diffusion models by optimizing the variational lower bound of negative log-likelihood, \(\mathbb{E}_{q_0}[-\log p_\theta(\mathbf{x}_0)]\). Following \citet{ho2020denoising}, we use a simplified loss with noise parameterization:
% Diffusion models can be trained by maximizing the variational lower bound on \(\mathbb{E}_{q_0}[\log p_\theta(\mathbf{x}_0)]\). Following~\cite{ho2020denoising}, we use a simplified loss function and noise parameterization:
\begin{align}
    \mathcal{L}(\theta)
    % &= \mathbb{E}_{\mathbf{x}_0\sim q_0}[-\log p_{\theta}(\mathbf{x}_0)] \\
    &= \mathbb{E}_{\mathbf{x}_0 \sim q_0,\; t \sim U(1,T),\; \epsilon \sim \mathcal{N}(0,I)}
    \bigl[\|\epsilon - \epsilon_{\theta}(\mathbf{x}_{t}, t)\|^2\bigr]
\end{align}
Here, \(\epsilon_{\theta}(\mathbf{x}_{t}, t)\) is the learned noise estimator, and
\begin{align}
    \mu_{\theta}(\mathbf{x}_{t}, t)
    = \frac{1}{\sqrt{\alpha_{t}}}
      \Bigl(
        \mathbf{x}_{t}
        - \frac{\sqrt{\beta_{t}}}{\sqrt{1 - \bar{\alpha}_{t}}}
          \,\epsilon_{\theta}(\mathbf{x}_{t}, t)
      \Bigr),
\end{align}
where \(\alpha_t = 1 - \beta_t\) and \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\).

% In various fields that utilize diffusion models, downstream problems require sampling product distributions. A pretrained diffusion model serves as a prior $p(\mathbf{x})$ that is multiplied by an auxiliary constraint $r(\mathbf{x})$ where $r: \mathbb{R}^d \rightarrow \mathbb{R}_{>0}$, resulting in the posterior distribution $p^\text{post}(\mathbf{x}) \propto p_\theta(\mathbf{x})r(\mathbf{x})$. 



%We often require sampling from the posterior distribution, where a pre-trained diffusion model serves as a prior $p_{\theta}(\mathbf{x})$ that is multiplied by a reward function $r(\mathbf{x})$, i.e., $p^{\text{post}}(\mathbf{x})\propto p_{\theta}(\mathbf{x})r(\mathbf{x})$ \cite{chung2023diffusion,lu2023contrastive,venkatraman2024amortizing}. For example, in offline reinforcement learning, if we train a conditional diffusion model $\mu(a\vert s)$ as a behavior policy, it requires sampling from the product distribution of behavior policy and Q-function \cite{nair2020awac}, i.e., $\pi(a\vert s)\propto \mu(a\vert s)\exp(\beta\cdot Q(s,a))$.

% Because sampling from $p^\text{post}(\mathbf{x})$ is intractable, several approaches solved this problem 
% \cite{venkatraman2024amortizing, lu2023contrastive} fine-tuning the diffusion model to amortize intractable posterior inference.

% Specifically, we treat $r_\phi(\mathbf{x}_0)$, $r_\phi: \mathbb{R}^d \rightarrow \mathbb{R}$ as a Boltzmann factor with temperature $\beta$. 
% The resulting posterior distribution is as follows:
% \begin{align}
%     p_\psi \propto \exp\bigl(\frac{1}{\beta}r_\phi(\mathbf{x})\bigr)p_\theta(\mathbf{x})
% \end{align}


\paragraph{Fine-tuning Diffusion Models for Posterior Inference.}
Given a diffusion model \( p_\theta(\mathbf{x}) \) and a reward function \( r(\mathbf{x}) \), we can define a posterior distribution \( p^{\text{post}}(\mathbf{x}) \propto p_\theta(\mathbf{x})r(\mathbf{x}) \), where the diffusion model serves as a prior. 
Sampling from the posterior distribution enables us to solve various downstream tasks \cite{chung2023diffusion,lu2023contrastive,venkatraman2024amortizing}. 
For example, in offline reinforcement learning, if we train a conditional diffusion model $\mu(a\vert s)$ as a behavior policy, it requires sampling from the product distribution of behavior policy and Q-function \cite{nair2020awac}, i.e., $\pi(a\vert s)\propto \mu(a\vert s)\exp(\beta\cdot Q(s,a))$.

When the prior is modeled as a diffusion model, the sampling from the posterior distribution is intractable due to the hierarchical nature of the sampling process. 
Fortunately, we can utilize relative trajectory balance (RTB) loss suggested by \citet{venkatraman2024amortizing} to learn amortized sampler $p_{\psi}$ that approximates the posterior distribution by fine-tuning the prior diffusion model as follows:
\begin{align}
    \label{eq:RTB}
    \mathcal{L}(\mathbf{x}_{0:T};\psi)=\left(\log\frac{Z_{\psi}\cdot p_{\psi}(\mathbf{x}_{0:T})}{r(\mathbf{x}_0) \cdot  p_{\theta}(\mathbf{x}_{0:T})}\right)^2
\end{align}
where $\mathbf{x}_0=\mathbf{x}$ and $Z_\psi$ is the partition function estimator. If the loss converges to zero for all possible trajectories $\mathbf{x}_{0:T}$, the amortized sampler matches the posterior distribution.

One of the main advantages of RTB loss is that we can train the model in an off-policy manner. In other words, we can train the model with the samples from the distribution different from the current policy $p_{\psi}$ to ensure mode coverage \cite{sendera2024improved,akhound2024iterated}.

%In this section, we introduce \textbf{DiBO}, a novel approach for solving high-dimensional black-box optimization by leveraging generative models. Our method iterates two stages. First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. %Then, we sample candidates for the next round from the product distribution of the likelihood from the diffusion model and the UCB score from the proxies. 
%Then, we evaluate the selected candidates, update the dataset, and repeat the process until we find an optimal design. \Cref{fig:overview} shows the overview of our method.
% Secondly, we fine-tune the diffusion model to approximate posterior distribution, which is the product of diffusion prior and UCB score from the trained proxies. 

\section{Method}
In this section, we introduce \textbf{DiBO}, a novel approach for high-dimensional black-box optimization by leveraging diffusion models. Our method iterates two stages to find an optimal design in high-dimensional spaces. 
First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. During training, we apply a reweighted training scheme to focus on high-scoring regions.
Next, we sample candidates from the posterior distribution. To further improve sample efficiency, we adopt local search and filtering to select diverse modes of posterior distribution as candidates.
We then evaluate the selected candidates, update the dataset, and repeat the process until we find an optimal design. \Cref{fig:overview} shows the overview of our method.

\subsection{Phase 1: Training Models}
In each round $r$, we have a pre-collected dataset of input-output pairs $\mathcal{D}_{r}=\{(\mathbf{x}_i, y_i)\}_{i=1}^{I}$, where $I$ is the number of data points.

\vspace{4pt}
\noindent\textbf{Training Diffusion Model.}
We first train a diffusion model $p_{\theta}(\mathbf{x})$ using $\mathcal{D}_{r}$ to capture the data distribution. We choose a diffusion model as it has a powerful capability to learn the distribution of high-dimensional data across various domains \cite{ramesh2022hierarchical, ho2022imagen}.

\vspace{4pt}
\noindent\textbf{Training Ensemble of Proxies.}
We also train a proxy model to predict function values using the dataset $\mathcal{D}_{r}$. As it is notoriously difficult to accurately predict all possible regions in high-dimensional spaces with a limited amount of samples, we need to properly quantify the uncertainty of our proxy model. To this end, we train an ensemble of proxies $f_{\phi_1},\cdots,f_{\phi_{K}}$ to estimate the epistemic uncertainty of the model \cite{lakshminarayanan2017simple}.

% In Phase 1, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. We adopt a reweighted training approach and maintain a buffer that contains only high-scoring samples, encouraging our model to focus on and improve accuracy in high-value regions.

% Our objective is to identify an optimal design $\mathbf{x}$ that maximizes the target black-box function $y = f(\mathbf{x})$. Consequently, we prioritize high-scoring regions that accurately capture the data distribution and predict values. A straightforward approach to achieve this is by weighting the models or training exclusively on high-scoring subsets of the training data.

% Building on this intuition, prior generative model-based methods have employed reweighted training schemes \cite{kumar2020model, krishnamoorthy2023diffusion, kim2024bootstrapped}. In offline settings, where samples can only be queried once, maintaining conservatism is crucial. These methods utilize the entire dataset and apply smoothed weights, such as rank-based reweighting \cite{kim2024bootstrapped} or assigning equal weights within each $y$-value bin \cite{kumar2020model, krishnamoorthy2023diffusion}.

% In contrast, our online approach allows for iterative querying, enabling us to impose exact value information within the weights to enhance concentration on high-value regions. 

% Given a dataset $\mathcal{D}$, we define the weight $w(y|\mathcal{D})$ of each sample as follows:
% \begin{align}
%     \label{weighted sampler}
%     w(y, \mathcal{D}) = \frac{\exp(y)}{\sum_{(\mathbf{x}', y') \in \mathcal{D}} \exp(y')}
% \end{align}

\paragraph{Reweighted Training.}
In the training stage, we introduce a reweighted training scheme to focus on high-scoring data points since our objective is to find an optimal design that maximizes the target black-box function. Reweighted training has been widely used in generative modeling for black-box optimization, especially in offline settings \cite{kumar2020model, krishnamoorthy2023diffusion, kim2024bootstrapped}. Formally, we can compute the weight for each data point as follows:
\begin{align}
    \label{eq:weighted sampler}
    w(y, \mathcal{D}_r)=\frac{\exp(y)}{\sum_{(\mathbf{x}', y') \in \mathcal{D}_{r}}\exp(y')}
\end{align}
Then, our training objective for proxies and diffusion models can be described as follows:
\begin{align}
    \label{eq:proxy}
    &\mathcal{L}(\phi_{1:K}) = \sum_{k=1}^{K}\sum_{(\mathbf{x}, y) \in \mathcal{D}_{r}} w(y, \mathcal{D}_{r}) \left(y - f_{\phi_{k}}(\mathbf{x})\right)^2\\
    \label{eq:prior}
    &\mathcal{L}(\theta) = -\sum_{(\mathbf{x}, y) \in \mathcal{D}_{r}} w(y, \mathcal{D}_{r}) \log p_{\theta}(\mathbf{x}).
\end{align}


% The resulting objectives for training the ensemble of proxies $f_{\phi_i}$ and the diffusion model $p_\theta$ are:
% \begin{equation}
%     \label{proxy}
%     \mathcal{L}(\phi_{i}) = \sum_{(\mathbf{x}, y) \in \mathcal{D}_{t}} w(y, \mathcal{D}_t) \left(y - f_{\phi_{i}}(\mathbf{x})\right)^2 \quad \forall i = 1, \dots, M
% \end{equation}
% \begin{equation}
%     \label{Prior}
%     \mathcal{L}(\theta) = -\sum_{(\mathbf{x}, y) \in \mathcal{D}_{t}} w(y, \mathcal{D}_t) \log p_{\theta}(\mathbf{x})
% \end{equation}
% After training, we sample candidates to query the black-box function using the trained models. 
% In high-dimensional spaces, the non-convex function with numerous local optima hinders finding optimal inputs \cite{ament2023unexpected}. 
% While most prior works try to find an input that maximizes the acquisition function, it is hard to find such input in high-dimensional spaces as the landscape of the acquisition function is highly non-convex and contains numerous local optima \cite{ament2023unexpected}.  
% Furthermore, the sparsity of the dataset leads to high uncertainty in regions far from the dataset, which leads to overemphasized exploration in boundaries of the search space \cite{oh2018bock}. Therefore, simply choosing an input that maximizes the acquisition function may result in a sub-optimal design.

\subsection{Phase 2: Sampling Candidates}
\label{sec:Phase 2}
After training models, we sample candidates from the posterior distribution to query the black-box function. As sampling from the posterior is intractable, we introduce an amortized sampler that generates unbiased samples from the posterior by fine-tuning the pre-trained diffusion model. Then, to further boost the sample efficiency, we apply post-processing strategies, local search and filtering, to select diverse modes of the posterior distribution as candidates.

% To address these challenges and induce controllability of exploration and exploitation, we formulated candidate selection as a posterior inference problem, which is the product of the upper confidence bound (UCB) and the trained diffusion prior. We fine-tune the diffusion models following the approach suggested by \cite{venkatraman2024amortizing} to approximate this intractable posterior and enable efficient sampling. 

% \paragraph{Problem Formulation}
% We define the candidate selection process as sampling from a target distribution \( p_{\text{tar}}(\mathbf{x}) \) designed to achieve two objectives: (1) guide the search toward regions that are both high-scoring and uncertain and (2) ensure the sampled candidates do not deviate excessively from the data distribution. These objectives lead to the following optimization problem:
% \begin{align}
% \label{KL Regularization}
%     p_{\text{tar}}(\mathbf{x}) = \arg\max_{p \in \mathcal{P}} \mathbb{E}_{\mathbf{x} \sim p}\left[ r_{\phi}(\mathbf{x}) \right] - \beta \cdot D_{\text{KL}}\left(p(\mathbf{x}) \,\|\, p_{\theta}(\mathbf{x})\right)
% \end{align}
% where \( \mathcal{P} \) denotes the set of feasible probability distributions and \( \beta \) is a regularization coefficient. We employ the Upper Confidence Bound (UCB) \cite{auer2002using} as a acquisition function \(    r_{\phi}(\mathbf{x}) = \mu_{\phi}(\mathbf{x}) + \gamma \cdot \sigma_{\phi}(\mathbf{x}) \),
% where \( \mu_{\phi}(\mathbf{x}) \), and \( \sigma_{\phi}(\mathbf{x}) \) are the prediction mean and standard deviation from an ensemble of proxy models. The parameter \( \gamma \) balances exploration and exploitation by encouraging sampling in regions with higher uncertainty.
%where:
%\begin{align}
%    \mu_{\phi}(\mathbf{x}) &= \frac{1}{M} \sum_{i=1}^{M} f_{\phi_i}(\mathbf{x}) \\
%    \sigma_{\phi}(\mathbf{x}) &= \sqrt{\frac{1}{M} \sum_{i=1}^{M} \left( f_{\phi_i}(\mathbf{x}) - \mu_{\phi}(\mathbf{x}) \right)^2 }
%\end{align}

% We can analytically drive following equation from \eqref{KL Regularization}, the $p_\text{tar}$ becomes the posterior distribution with inverse temperature $\beta$. 
% \begin{align}
%      p_{\text{tar}}(\mathbf{x}) \propto p_{\theta}(\mathbf{x}) \exp\left(\frac{1}{\beta} r_{\phi}(\mathbf{x})\right)
% \end{align}
% Since sampling directly from the product space is intractable, we leverage the Relative Trajectory Balance (RTB) loss \eqref{RTB} to fine-tune the diffusion model. 

% where $\mathbf{x}_0 = \mathbf{x}$, $Z_\psi \approx \int_{\mathbf{x} \in \mathcal{X}} p_\theta(\mathbf{x}) \exp\left(\beta\cdot r_\phi(\mathbf{x})\right) d\mathbf{x}$ is a parameterized partition function for numerical stability. $p(\mathbf{x}_{0:T})$ denotes the joint probability distribution of $\mathbf{x}$ across the diffusion timesteps $0:T$, $p_\theta$ is the diffusion prior, and $p_\psi$ is the posterior. 
% Following \cite{venkatraman2024amortizing}, we initialize $\psi \leftarrow \theta$, and assume $p_\theta(\mathbf{x}_0)=p_\psi(\mathbf{x}_0)$ to be fixed to the standard normal. The equation \eqref{Posterior} reduces to:
% \begin{align}
%     \label{Posterior Log}
%     \mathcal{L}(\psi)=\left(\log Z_{\psi}- \frac{1}{\beta}r_{\phi}(\mathbf{x}_0) + \sum_{t=1}^T\log \frac{p_{\psi}(\mathbf{x}_{t-1}|\mathbf{x}_{t})}{p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t})}\right)^2
% \end{align}
%Note that, \eqref{Posterior Log} can be trained both on-policy and off-policy. 

% This approach amortizes the intractable inference, enabling the direct generation of unbiased samples from the target posterior distribution. All the proofs can be found in \cite{venkatraman2024amortizing}.
% Furthermore, we post-processed the candidates sampled from the approximated posterior, by performing a local search and filtering out low-posterior candidates. This approach ensured that the most promising candidates were evaluated, improving sample efficiency

\vspace{3pt}
\noindent\textbf{Amortizing Posterior Inference.}
Our key idea is to sample candidates from the probability distribution that satisfies two desiderata: (1) promote exploration towards both high-rewarding and highly uncertain regions and 
(2) prevent the sampled candidates from deviating excessively from the data distribution. To accomplish these objectives, we can define our target distribution as follows:
\begin{align}
\label{eq:KL Regularization}
    p_{\text{tar}}(\mathbf{x}) = \arg\max_{p \in \mathcal{P}} \mathbb{E}_{\mathbf{x} \sim p}\left[ r_{\phi}(\mathbf{x}) \right] - \frac{1}\beta\cdot D_{\text{KL}}\left(p \,\|\, p_{\theta}\right)
\end{align}
where \(    r_{\phi}(\mathbf{x}) = \mu_{\phi}(\mathbf{x}) + \gamma \cdot \sigma_{\phi}(\mathbf{x}) \) is UCB score from the proxy, and $\mu_{\phi}(\mathbf{x}), \sigma_{\phi}(\mathbf{x})$ indicate mean and the standard deviation from proxy predictions, respectively. \( \mathcal{P} \) denotes the set of feasible probability distributions and \( \beta \) is an inverse temperature. Target distribution that maximizes the right part of the \Cref{eq:KL Regularization} can be analytically derived as follows \cite{nair2020awac}: %awac으로 설명 되는거 아닌가. venkatraman에도 있긴함 awac인용하자
\begin{align}\label{eq:p_target}
     p_{\text{tar}}(\mathbf{x}) =\frac{1}{Z}\cdot p_{\theta}(\mathbf{x}) \exp\left(\beta\cdot r_{\phi}(\mathbf{x})\right)
\end{align}
where $Z=\int_{\mathbf{x} \in \mathcal{X}} p_\theta(\mathbf{x}) \exp\left(\beta\cdot r_\phi(\mathbf{x})\right)$ is a partition function. As we do not know the partition function, sampling from $p_{\text{tar}}$ is intractable. Therefore, we introduce amortized sampler $p_{\psi}\approx p_{\text{tar}}$, which can be obtained by fine-tuning the trained diffusion model with relative trajectory balance loss suggested by \citet{venkatraman2024amortizing}. Formally, we train the parameters of $p_{\psi}$ with the following objective:  
\begin{align}
    \label{eq:Posterior}
     \mathcal{L}(\mathbf{x}_{0:T};\psi)=\left(\log\frac{Z_{\psi}\cdot p_{\psi}(\mathbf{x}_{0:T})}{\exp\left(\beta\cdot r_{\phi}(\mathbf{x}_{0})\right)\cdot p_{\theta}(\mathbf{x}_{0:T})}\right)^2
\end{align}
where $\mathbf{x}_0 = \mathbf{x}$, and $Z_\psi$ is a parameterized partition function.

As mentioned in the previous section, we can employ off-policy training to effectively match the target distribution. To this end, we train $p_{\psi}$ with the on-policy trajectories from the model mixed with the trajectories generated by samples from the pre-collected dataset $\mathcal{D}_r$. Please refer to \Cref{app:fine-tuning} for more details on off-policy training.

After training $p_{\psi}$, we can generate unbiased samples from our target distribution. However, as we have a limited evaluation budget, it is advantageous to refine candidates to exhibit a higher probability density of target distribution, i.e., modes of distribution. To achieve this, we introduce two post-processing strategies: local search and filtering.


\input{algorithms/method}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/synthetic_tasks.png}
    \vspace{-20pt}
    \caption{Comparison between our method against baselines in synthetic tasks. Experiments are conducted with four random seeds, and the mean and one standard deviation are reported.}
    \label{fig:synthetic_tasks}
    \vspace{-15pt}
\end{figure*}

% To enhance sample efficiency, capturing the modes of the target distribution $p_\text{tar}(\mathbf{x})$ is much more desirable. % To this end, we post-processed sampled candidates with local search and subsequent filtering.
% to estimate the marginal likelihood \(\tilde{p}_\theta(\mathbf{x}_i) \approx p_\theta (\mathbf{x}_i) \).

\vspace{3pt}
\noindent\textbf{Local Search.}
First, we generate a set of candidates $\{\mathbf{x}_i\}_{i=1}^M$ by sampling from $p_\psi$. For each candidate $\mathbf{x}_i$, we perform a local search using gradient ascent to move it toward high-density regions. Formally, we update the original candidate $\mathbf{x}_i$ to $\mathbf{x}_i^{*}$ as follows:
% The update rule for the $j$-th iteration is given by:
\begin{align}
\label{eq:local search1}
    &\mathbf{x}_i^{j+1}\leftarrow \mathbf{x}_i^j + \eta \cdot\nabla_{\mathbf{x}=\mathbf{x}_i^j} \left({p}_\theta(\mathbf{x})\cdot\exp(\beta\cdot r_\phi(\mathbf{x})) \right), \\
\label{eq:local search2}
    &\text{for } j = 0, \dots, J-1,\quad\text{where }\mathbf{x}_i^{0}=\mathbf{x}_i, \;\mathbf{x}_i^{J}=\mathbf{x}_i^{*}
\end{align}
where $\eta$ is the step size, and $J$ is the number of updates. To estimate the marginal probability $p_\theta(\mathbf{x})$, we employ the probability flow ordinary differential equation (PF ODE) \citet{song2021score} with a differentiable ODE solver. Please refer to \Cref{subsec:EstimatingMarginalLikelihood} for more details on local search.

As the amortized sampler $p_\psi$ is parametrized as a diffusion model, we can expect that it generates samples across diverse possible promising regions. Then, the local search procedure guides samples towards modes of each promising region \citep{kim2024local}. % Through this process, we can find diverse modes of high-dimensional and multi-modal distribution.

\vspace{3pt}
\noindent\textbf{Filtering.}
After the local search, we introduce filtering to select $B$ candidates for evaluation among generated samples $\{\mathbf{x}_1^{*},\cdots,\mathbf{x}_{M}^{*}\}$. To be specific, we select the top-$B$ samples with respect to the unnormalized target density, $p_{\theta}(\mathbf{x})\cdot\exp(\beta\cdot r_{\phi}(\mathbf{x}))$. Through filtering, we can effectively capture the high-quality modes of the posterior distribution, thereby significantly improving the sample efficiency.
% a subset of high-target probability samples.
% \begin{align}
%     \label{filtering}
%     S_\text{query} = \operatorname*{\arg \max}_{S \subseteq \{\mathbf{x}_i^J\}_{i=1}^M,\, |S|=B} \sum_{\mathbf{x}_i^J \in S} \left( \frac{1}{\beta} r_\phi(\mathbf{x}_i^J) + \log \tilde{p}_\theta(\mathbf{x}_i^J) \right),
% \end{align}
% where $S_\text{query}$ is the selected subset of samples to query, $B$ is the fixed batch size where $B\leq M$, and $\{\mathbf{x}_i^J\}_{i=1}^M$ represents the set of samples after $J$ iterations of local search.

% These post-processing steps ensure that evaluation budgets are focused on the most promising candidates, thereby improving sample efficiency.
% Throughout these post-processing strategies, we can use a limited evaluation budget in a more efficient way by choosing the modes of the target distribution, thereby achieving high sample efficiency. 

%Where $B$ is the query batch size $B \leq M$, we employ the Probability Flow Ordinary Differential Equation (PF ODE) \cite{song2021score} to estimate the marginal likelihood \(\tilde{p}_\theta(\mathbf{x}) \approx p_\theta (\mathbf{x}) \). %Here, $r_\phi(\mathbf{x})$ and the $\tilde{p}_\theta(\mathbf{x})$ are both differentiable. 
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/realworld_tasks.png}
    \vspace{-10pt}
    \caption{Comparison between our method against baselines in real-world tasks. Experiments are conducted with ten random seeds, and the mean and one standard deviation are reported.}
    \vspace{-10pt}
    \label{fig:realworld_tasks}
\end{figure*}

% After querying the samples \( S_{\text{query}} \), we append them to the buffer while keeping a fixed number of high-scoring samples. 
%Additionally, a fixed number of training sets ensures consistency, which was helpful to the model adaptivity.
% \begin{align}
%     \label{buffer management}
%     \mathcal{D}_r \leftarrow \mathcal{D}_r \setminus \left\{ (\mathbf{x}, y) \in \mathcal{D}_r \ \bigg| \ f(\mathbf{x}) < f(\mathbf{x}_{(N)}) \right\},
% \end{align}
% where $N$ is the buffer size, and $\mathbf{x}_{(N)}$ is the $N^\text{th}$ highest-valued sample within $\mathcal{D}_r$.
% Our overall algorithm is outlined in \cref{alg}.

\subsection{Evaluation and Moving Dataset} \label{sec:3.3_moving_dataset}
After selecting candidates, we evaluate their function values by querying the black-box function. Then, we update the dataset with new observations. When updating the dataset, we remove the samples with the lowest function values if the size of the dataset is larger than the buffer size $L$.
We empirically find that it reduces the computational complexity during training and ensures that the model concentrates more on the high-scoring regions.

\section{Experiments}
In this section, we present experimental results on high-dimensional black-box optimization tasks. First, we conduct experiments on four synthetic functions commonly used in the BO literature \cite{eriksson2019scalable, wang2020learning}. Then, we perform experiments on three high-dimensional real-world tasks, including  HalfCheetah-102D from MuJoCo Locomotion, RoverPlanning-100D, and DNA-180D from LassoBench. 
The description of each task is available in \cref{sec:Task Details}.
\subsection{Baselines}
We evaluate our method against state-of-the-art (SOTA) baselines for high-dimensional black-box optimization, 
including BO methods: 
TuRBO \cite{eriksson2019scalable},  
LA-MCTS \cite{wang2020learning},
MCMC-BO \cite{yi2024improving},
CMA-BO \cite{ngo2024high}, 
an evolutionary search approach: CMA-ES \cite{hansen2006cma},
and existing generative model-based algorithms:
CbAS \cite{brookes2019conditioning},
MINs \cite{kumar2020model},
DDOM \cite{krishnamoorthy2023diffusion}, and 
Diff-BBO \cite{wu2024diff}.
Details about baseline implementation can be found in \cref{sec:Baseline Details}.

\subsection{Synthetic function tasks}
We benchmark four synthetic functions that are widely used for evaluating high-dimensional black-box optimization algorithms: \textit{Rastrigin, Ackley, Levy,} and \textit{Rosenbrock}. 
We evaluate each function in both $D=200$ and $400$ dimensions and set the search space $\mathcal{X}=[\text{lb}, \text{ub}]^D$ for each function following previous works \cite{wang2020learning, yi2024improving}. All experiments are conducted with initial dataset size $\vert\mathcal{D}_0\vert=200$, batch size $B=100$, and $10,000$ as the maximum evaluation limit.\footnotemark

%thereby mitigating the risk of local collapse.

As shown in \cref{fig:synthetic_tasks}, our method significantly outperforms all baselines across four synthetic functions. 
Furthermore, we observe that our method not only discovers high-scoring designs but also achieves high sample efficiency. 
Specifically, our method exhibits a significant gap compared to baselines on \textit{Rastrigin} and \textit{Ackley} tasks, which have numerous local optima near the global optimum. It highlights that our key idea, sampling candidates from the posterior distribution, enables effective exploration of promising regions in high-dimensional spaces and mitigates the risk of converging sub-optimal regions.
%Specifically, our method excels particularly on \textit{Rastrigin} and \textit{Ackley}, highlighting that its success comes not only from the expressivity of the diffusion model but also from our carefully designed sampling strategy. This enables effective exploration of promising regions in high-dimensional spaces and mitigates the risk of local collapse.
% The results underscore that our approach effectively explores promising regions in high-dimensional spaces. 

% Generative methods like CbAS and MINs perform well initially but often become trapped in local optima due to limited exploration. 

%We find that generative model-based approaches such as CbAS and MINs perform well in the early stage but struggle to improve the performance throughout iterations. Those methods are not sufficient to capture the data distribution due to the limited expressivity of generative models.
%While diffusion model-based approaches show consistent improvements across various tasks, the performance lags behind recent BO methods. It demonstrates that the superiority of our method comes from not only the expressivity of the diffusion model but also our novel framework calibrated for high-dimensional black-box optimization. 

We find that generative model-based approaches such as CbAS and MINs perform well in the early stage but struggle to improve the performance through subsequent iterations. 
While diffusion-based methods (DDOM, Diff-BBO) show consistent improvements across various tasks, the performance lags behind that of recent BO methods. These results demonstrate that the superiority of our method stems not just from using diffusion models but also from our novel framework calibrated for high-dimensional black-box optimization. We also conduct an ablation study on each component of our method in the following section. 

%We also compare our method with high-dimensional Bayesian optimization methods. 
%While these approaches also exhibit comparable performance and often outperform generative model-based baselines, they remain sample-inefficient compared to our method. This reveals that our method can be a potential alternative %to the existing Bayesian optimization methods 
%for solving high-dimensional black-box optimization problems.
% This reveals that a carefully designed generative model-based approach can be a good candidate for solving high-dimensional black-box optimization. 
%This reveals that our method can be an alternative for solving high-dimensional black-box optimization problems. 

We also compare our method with high-dimensional Bayesian optimization methods.
While these approaches exhibit comparable performance and often outperform generative model-based baselines, they remain relatively sample-inefficient compared to our method. 
It reveals that sampling diverse candidates from the posterior distribution can be a sample-efficient solution for high-dimensional black-box optimization compared to choosing inputs that maximize the acquisition function.









% ㅋㅋㅋㅋㅋㅋ 이거?ㅇㅇ
%Rastrigin/Ackley 에서 잘하는데 Levy Rosenbrock에서 못한다고 까고 우리 method robust하다는건 어떰?
% 전체적으로 못하지 않음?ㅋㅋㅋㅋ 그냥 sample efficient하지 않다고 말해도 될듯 종종 generative model-based보다는 잘하지만
% ~, ~ing 처럼 쓰면 한 문장이 너무 길어져 간결한 게 좋음
% 그래서 우리 method가 potential한 alternative framework가 될 수 있다고 말하면 될 것 같음 (그전까지는 거의 다 BO니까)



% However, Diff-BBO, which employs an uncertainty-based acquisition function for target conditioning, has difficulty estimating uncertainty in higher dimensions (400D), resulting in lower sample efficiency than DDOM. % Diff-BBO가 DDOM보다 못하는 이유는 설명할 필요가 없음
% This results demonstrates the superiority of our model comes not only from generative modeling or the diffusion model's expressivity but also from our sampling from the product distribution strategy.
% BO에 대한 이야기도 좀 써주면 좋겠네 한 번 써보실 



%demonstrating exceptional performance in high-dimensional and large-scale black-box optimization.

%BO methods (TuRBO, MCMC-BO, and CMA-BO) show consistent improvements, they remain relatively sample-inefficient.
%Notably, generative methods such as CbAS, and MINs exhibit initial promise but often become trapped in local optima due to the limited exploration. 
%Diffusion-based models (DDOM and Diff-BBO) show consistent improvements. However, Diff-BBO, which employs an uncertainty-based acquisition function for target conditioning, has difficulty estimating uncertainty in higher dimensions (400D), resulting in lower sample efficiency than DDOM. This results demonstrates the superiority of our model comes not only from generative modeling or the diffusion model's expressivity but also from our sampling from the product distribution strategy.
% In contrast, our method excels particularly on \textit{Rastrigin} and \textit{Ackley}, highlighting that its success comes not only from the expressivity of the diffusion model but also from our carefully designed sampling strategy. This enables effective exploration of promising regions in high-dimensional spaces and mitigates the risk of local collapse.


\subsection{Real-World Tasks}
To evaluate the performance and adaptability of our method in real-world scenarios, we conduct experiments on three additional tasks: HalfCheetah-102D, RoverPlanning-100D, and DNA-180D. Each experiment starts with $\vert\mathcal{D}_0\vert=100$ initial samples, a batch size of $B=50$, and a maximum evaluation limit of $2,000$. 

The results are illustrated in \cref{fig:realworld_tasks}. Our method exhibits superiority in terms of both the performance and the sample efficiency compared to baseline approaches. While other methods show inconsistent performance—excelling on some tasks but underperforming on others—our method consistently surpassed the baselines, highlighting its robustness across a broader range of tasks.
\footnotetext{For MCMC-BO, we report the score of budget $6,000$ on tasks with $D=400$ due to memory constraints.} 
%Below, we provide a description of each task. 
%\textbf{MuJoCo Locomotion} \cite{todorov2012mujoco} is established benchmark in Reinforcement Learning (RL). Our objective is to find the optimal linear policy \( W \) that maximizes the average return. We conducted experiments using the \textit{HalfCheetah-102D} task and calculated the average rewards from three episodes.
%MuJoCo Locomotion tasks are widely recognized benchmarks in Reinforcement Learning (RL). In this context, we optimize a linear policy represented by the equation \(\mathbf{a} = \mathbf{W}\mathbf{s}\). The average return of this policy serves as our objective, and our goal is to identify the weight matrix that maximizes this return. We focus specifically on the HalfCheetah task, which has a dimensionality of 102. Each entry of the weight matrix \(\mathbf{W}\) is constrained to the range \([-1, 1]\), and we utilize 3 rollouts for each evaluation.

%\textbf{Rover trajectory optimization }\cite{wang2018batched} involves determining a rover's path in a 2D environment while maximizing a predefined reward. We experimented with a more challenging 100-dimensional variant \cite{ngo2024high}.
%Rover Trajectory Optimization \cite{wang2018batched} is determining the trajectory of a rover in a 2D environment, maximizing the pre-determined location-wise reward function. We used 100-dimensional variant \cite{ngo2024high}, optimizing 50 distinct points. 

%\textbf{LassoBench} \cite{vsehic2022lassobench} is a challenge focused on optimizing the hyperparameters of Weighted LASSO (Least Absolute Shrinkage and Selection Operator) regression. The goal is to fine-tune a set of hyperparameters to achieve a balance between least-squares estimation and the sparsity-inducing penalty term. We focus on \textit{DNA-180}, which utilizes a DNA dataset from a microbiological study.    
%LassoBench \cite{vsehic2022lassobench} is a challenge focused on optimizing the hyperparameters of Weighted LASSO (Least Absolute Shrinkage and Selection Operator) regression. The goal is to fine-tune a set of hyperparameters to achieve a balance between least-squares estimation and the sparsity-inducing penalty term. We focus on \textit{DNA}, 180-dimensional hyperparameter optimization utilizing DNA dataset from a microbiological study.

\begin{figure*}[t]
\centering
\begin{minipage}[t]{\textwidth}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_reweighting.png}
        \subcaption{Reweighted Training}
        \label{fig:ablation_reweighted}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_sampling.png}
        \subcaption{Sampling Procedure}
        \label{fig:ablation_sampling}
    \end{subfigure}
    \vspace{1.5em}
    
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_beta.png}
        \subcaption{Analysis on $\beta$}
        \label{fig:ablation_beta}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_L.png}
        \subcaption{Analysis on $L$}
        \label{fig:ablation_L}
    \end{subfigure}
    \vspace{-5pt}
    \caption{Ablation on various components in DiBO. Experiments are conducted on the Rastrigin-200D and HalfCheetah-102D tasks.}
    \label{fig:add_analysis}
\end{minipage}
\vspace{-5pt}
\end{figure*}

% As shown in \Cref{fig:ablation_reweighted}, reweighting is crucial in enhancing the model's ability to focus on high-scoring regions, improving sample efficiency.
% 's는 웬만하면 쓰지 않는 게 좋고 ~, ing도 마찬가지
%In phase 1, we use a reweighted training scheme. To assess the impact, we conduct experiments by omitting the reweighting components and analyze the resulting performance differences. As depicted in \Cref{fig:ablation_reweighted}, and \Cref{fig:ablation_reweighted}, reweighting plays a crucial role in enhancing the model's ability to focus on and explore high-scoring regions, thereby significantly improving sample efficiency.
%We observe consistent effects across various tasks. We present additional figures in \Cref{app:additional_analysis}
%the model struggles to explore high-scoring regions without reweighting, leading to suboptimal convergence. In contrast, the reweighting scheme encourages the model to focus on high-scoring regions, thereby increasing the likelihood of escaping local optima by allowing more frequent sampling from promising areas.
% encourages the model to effectively explore high-scoring regions. 딱 여기까지만 쓰는 것도 괜찮을듯 Halfcheetah도 잘 되었으니까 다른 task에서도 robust하게 비슷한 현상을 발견하였고 figure appendix에 있다고 말하고 넘겨도 될듯
% We conduct further analysis on each component in the Appendix.

\section{Additional Analysis}
In this section, we carefully analyze the effectiveness of each component of our method. We conduct additional analysis in Rastrigin-200D and HalfCheetah-102D tasks.

\paragraph{Ablation on Reweighted Training.} 
We examine the impact of reweighted training during the model training stage. We conduct experiments by omitting the reweighting component. 
As shown in \Cref{fig:ablation_reweighted}, the performance of our method significantly drops if we remove the reweighted training. It underscores that focusing on high-scoring regions accelerates the optimization process. We also conduct analysis on the number of training epochs in \Cref{app:ablation_epoch}. We find that naively increasing the number of training epochs does not lead to an improvement in performance.


\paragraph{Ablation on Sampling Procedure.} We analyze the effect of strategies we have proposed during the sampling stage. We conduct experiments without filtering, local search, and finally completely remove the amortized inference stage and propose samples from the diffusion model $p_{\theta}$.

As depicted in \Cref{fig:ablation_sampling}, each component of our method significantly affects the performance. Notably, when we remove both local search and filtering strategies, we observe that the sample efficiency of our method significantly drops, demonstrating the effectiveness of the proposed components. We also conduct further analysis on the number of local search steps $J$ in \Cref{app:ablation_j} and the effect of off-policy training for amortized inference in \Cref{app:ablation_offpolicy}.

\paragraph{Analysis on Inverse Temperature $\beta$.} 
The parameter $\beta$ in \cref{eq:KL Regularization} governs the trade-off between exploitation and exploration. If $\beta$ is too small, the method tends to exploit already discovered high-scoring regions. On the other hand, if $\beta$ is too large, we generate samples that deviate too far from the current dataset and overemphasize exploration of the boundary of search space.

As shown in \cref{fig:ablation_beta}, when $\beta$ is too small, it often leads to convergence on a local optimum due to limited exploration.
Conversely, if $\beta$ is too large, the model becomes overly dependent on the proxy function, resulting in excessive exploration and ultimately slowing the convergence.

% As shown in \cref{fig:ablation_beta}, when $\beta$ is too small, the KL term dominates, causing the model to rely excessively on the prior. This limits exploration, often leading to convergence on a local optimum. 
% We demonstrate that small $\beta$ impedes the exploration, resulting in degraded sample efficiency. In case of larger $\beta$, It performs well during the early stages of evaluation due to weaker regularization. Nevertheless, DiBO with larger $\beta$ overly relies on the proxy function, ultimately hindering the model from achieving optimal performance.



\paragraph{Analysis on Buffer size $L$.} As we described in \cref{sec:3.3_moving_dataset}, we introduce buffer size $L$ to maintain the dataset with high-scoring samples collected during the evaluation cycles. The choice of $L$ impacts both the time complexity and the sample efficiency of our method. 

As illustrated in \cref{fig:ablation_L}, using a small buffer size results in reaching suboptimal results, also causing early performance saturation. In contrast, a larger buffer size can reach the optimal value as in our default setting but significantly decelerate the rate of performance improvement.


\paragraph{Analysis on initial dataset size $\vert\mathcal{D}_0\vert$ and batch size $B$.} Initial dataset size and batch size for each round can be crucial in the performance of the method. To verify the effect of these components, we conduct additional analysis in \Cref{app:ablation_D0,app:ablation_B} and find that our method is robust to different initial experiment settings.

\paragraph{Analysis on uncertainty estimation.} 
We further analyze how the uncertainty estimation affects the performance of our method in \Cref{app:uncertainty_estimation}.
We find that using ensembles to measure uncertainty is a powerful approach, and using the UCB score based on the uncertainty can effectively guide the model to explore promising regions.

\paragraph{Time complexity of the method.} We conduct analysis on the time complexity of our method in \Cref{app:time_complexity}. Our findings show that our method exhibits a relatively low or comparable running time compared to other baselines. 


\section{Related Works}
\subsection{High-dimensional Black-Box Optimization}
%Optimizing high-dimensional black-box functions is a challenging task. Furthermore, modeling large search spaces often requires a significant amount of observed data, necessitating scalable approaches. %While Bayesian Optimization (BO) has demonstrated effectiveness in low-dimensional black-box optimization, its performance deteriorates in high-dimensional settings due to the curse of dimensionality \cite{kandasamy2015high, letham2020re}.

%Various approaches have emerged to solve the high-dimensional black-box optimization. In the literature on Bayesian Optimization (BO), most methods are based on local modeling or partitioning the search space to improve scalability. Specifically, TuRBO \cite{eriksson2019scalable}
%fits a collection of local models and restricts the candidate set to a small trust region. LA-MCTS \cite{wang2020learning} learns to partition the search space and identifies promising regions for sampling.
%Some approaches make structural assumptions that the high-dimensional objective function has a low active dimensional subspace \cite{chen2012joint, spagnol2019global, letham2020re, garnett2014active} or that the objective function can be a sum of several low-dimensional functions \cite{gardner2017discovering}. However, they often do not align with real-world applications and are not scalable to thousands of evaluations. Additionally, meta-algorithms, which add extra modules to the existing high-dimensional BO methods such as adding MCMC (Markov Chain Monte Carlo) steps \cite{yi2024improving} or CMA strategy \cite{ngo2024high} also emerged to improve sample efficiency.

%On the other hand, evolutionary search methods such as covariance matrix adaptation evolution strategy (CMA-ES) \cite{hansen2006cma} also showed powerful capability. 
 
%Other approaches, which rely on specific assumptions about the objective functions , often make strong assumptions that do not align well with real-world problems, thereby limiting their practical applicability.

%MCMC-BO \cite{yi2024improving} incorporates Markov Chain Monte Carlo (MCMC) steps, and CMA-BO \cite{ngo2024high} adopts a Covariance Matrix Adaptation strategy to existing BO algorithms, improving their performance. 

Various approaches have been proposed to address high-dimensional black-box optimization. In Bayesian optimization (BO), some methods assume that objective function can be decomposed into a sum of several low-dimensional functions and train a large number of Gaussian processes (GPs) \cite{duvenaud2011additive, kandasamy2015high, gardner2017discovering}. However, relying on training multiple GPs reduces its scalability to a large number of evaluations.
Other approaches assume that high-dimensional objective functions reside in a low-dimensional active subspace and introduce mapping to low-dimensional spaces
\cite{chen2012joint, garnett2014active, nayebi2019framework, letham2020re}. However, these methods make strong assumptions that often fail to align with real-world problems.

Another line of BO methods utilizes local modeling or partitioning of the search space to address high dimensionality and scalability. TuRBO \cite{eriksson2019scalable} fits multiple local models and restricts the search space to small trust regions to improve scalability. LA-MCTS \cite{wang2020learning} trains an unsupervised K-means classifier to partition the search space, identifies promising regions for sampling, and then employs BO-based optimizers such as BO or TuRBO. Including LA-MCTS, meta-algorithms that can be adapted to existing high-dimensional BO methods have also emerged. MCMC-BO \cite{yi2024improving} adopts Markov Chain Monte Carlo (MCMC) to adjust a set of candidate points towards more promising positions, and CMA-BO \cite{ngo2024high} utilizes covariance matrix adaptation strategy to define a local region that has the highest probability of containing global optimum.

Evolutionary Algorithms (EAs) are also widely applied in high-dimensional black-box optimization. The most representative method, the covariance matrix adaptation evolution strategy (CMA-ES) \cite{hansen2006cma}, operates by building and refining a probabilistic model of the search space. It iteratively updates the covariance matrix of a multivariate Gaussian distribution to capture the structure of promising solutions and uses feedback from the objective function to guide the search towards the optimal solution.

%Various approaches have been proposed to address high-dimensional black-box optimization. In Bayesian Optimization (BO), some methods propose local modeling or partitioning the search space for scalability. TuRBO \cite{eriksson2019scalable} fits multiple local models and restricts the search space to small trust regions to improve scalability. LA-MCTS \cite{wang2020learning} trains an unsupervised K-means classifier to partition the search space, identifies promising regions for sampling, and then employs BO-based optimizers such as BO or TuRBO.
%Similar to LA-MCTS, meta-algorithms that can be adapted to existing high-dimensional BO methods have also emerged. For example, MCMC-BO \cite{yi2024improving} incorporates Markov Chain Monte Carlo (MCMC) steps, and CMA-BO \cite{ngo2024high} adopts a Covariance Matrix Adaptation (CMA) strategy to other BO methods. 

%Another line of BO assumes that high-dimensional objective functions reside in a low active dimensional subspace and leverage strategies like embeddings or variable selection \cite{chen2012joint, spagnol2019global, letham2020re, garnett2014active}. 
%Some methods combine multiple Gaussian Processes (GPs), assuming that the objective function can be decomposed into a sum of several low-dimensional functions \cite{kandasamy2015high, gardner2017discovering}. However, these assumptions often fail to align with real-world problems and lack scalability when thousands of evaluations are required.

%Besides BO-based strategies, Evolutionary Algorithms (EAs) are also widely applied in high-dimensional black-box optimization. The most representative method, the Covariance Matrix Adaption evolution strategy (CMA-ES) \cite{hansen2019pycma}, works by maintaining a probabilistic model of the optimal solution and iteratively improving this model by adjusting the covariance matrix based on feedback from the objective function. 


\subsection{Generative Model-based Optimization}
%Generative models \cite{kingma2013auto, goodfellow2014generative, rezende2016variationalinferencenormalizingflows, ho2020denoising} have shown remarkable capabilities in modeling high-dimensional data across various domains, including images \cite{oord2016conditionalimagegenerationpixelcnn, goodfellow2014generative}, text \cite{vaswani2017attention}, and more \cite{liu2024sorareviewbackgroundtechnology}. 


%Flow: 1. 다양한 method 들이 Generative model 을 이용해서 black-box optimization을 풀었다. 대부분은 inverse mapping과 target conditioning으로 generated 된 sample을 query 한느 방식으로 접근했음.
%Expressivity 내용 언급 필요, Uncertainty 추정 안된다는 내용 언급 필요. 

Several methods have been developed that utilize generative models to optimize black-box functions. Most approaches learn an inverse mapping from function values to the input domain and propose promising solutions by sampling from the trained model, conditioned on a high score \cite{brookes2019conditioning, kumar2020model, krishnamoorthy2023diffusion, wu2024diff, kim2024bootstrapped}.

%최근 Diffusion model이 high-dimensional 한 상황에서도 잘 된다는 연구가 계속 등장하고 있음.
%이를 발판으로 여러 diffusion model based algorithm들이 등장하기 시작함.
%대표적인 method로 DDOM 그리고, Diff-BBO가 있음. 

Building on the success of diffusion models, widely known for their expressivity in high-dimensional spaces \cite{ramesh2022hierarchical, ho2022imagen}, leveraging diffusion models for black-box optimization has also emerged 
\cite{krishnamoorthy2023diffusion, wu2024diff, kong2024diffusion, yun2024guided}. 
DDOM \cite{krishnamoorthy2023diffusion} trains a conditional diffusion model with classifier-free guidance \cite{ho2021classifier} and incorporates reweighted training to enhance the performance. 
Diff-BBO \cite{wu2024diff} trains an ensemble of conditional diffusion models, then employs an uncertainty-based acquisition function to select the conditioning target value during candidate sampling.

% While Diff-BBO is closely related to our work, particularly its utilization of diffusion models and its focus on online scenarios, it introduces significant computational burdens. %This stems from the necessity of training multiple diffusion models, evaluating uncertainty through repeated sampling, and optimizing acquisition functions, which requires multiple uncertainty evaluations. 
% Furthermore, it struggles to scale up to high dimensionality, losing its ability to capture uncertainty accurately
% In contrast, our approach alleviates the computational burden by introducing a moving dataset and effectively scaling up to high-dimensional tasks with our posterior sampling strategy.

Diff-BBO is closely related to our work, particularly in its use of diffusion models and its focus on online scenarios. However, utilizing multiple diffusion models results in a significant computational burden throughout the optimization process. Additionally, estimating uncertainty using diffusion models in high-dimensional spaces remains challenging, which can impact the effectiveness of uncertainty-based acquisition and may lead to suboptimal results.
In contrast, our approach alleviates the computational burden by introducing a moving dataset and effectively scaling up to high-dimensional tasks with our posterior sampling strategy.

%Various generative model-based algorithms have recently been applied to black-box optimization, particularly in offline settings. Most approaches learn an inverse mapping from function values to the input domain and propose promising solutions by sampling from the trained model, conditioned on a high score \cite{kumar2020model, brookes2019conditioning, krishnamoorthy2023diffusion, wu2024diff, kim2024bootstrapped}. %or by approximating it \cite{brookes2019conditioning}.

%Building on the success of diffusion models in various fields, several diffusion-based optimization approaches have been proposed \cite{krishnamoorthy2023diffusion, wu2024diff, kong2024diffusion, yun2024guided}. For instance, DDOM \cite{krishnamoorthy2023diffusion} leverages a conditional diffusion model trained with classifier-free guidance and incorporates reweighted training to enhance performance. Diff-BBO \cite{wu2024diff} follows a similar method to DDOM. It further utilizes ensembles of conditional diffusion models to estimate epistemic uncertainty and selects the appropriate conditioning target value, optimizing an acquisition function based on uncertainty.

%While Diff-BBO is closely related to our work, particularly its utilization of diffusion models and its focus on online scenarios, it introduces significant computational burdens. This stems from the necessity of training multiple diffusion models, evaluating uncertainty through repeated sampling, and optimizing acquisition functions, which requires multiple uncertainty evaluations. Furthermore, it struggles to scale up to high dimensionality, losing its ability to capture uncertainty accurately. In contrast, our approach alleviates the computational burden by introducing a moving dataset and effectively scaling up to high-dimensional tasks with our posterior sampling strategy.


% DIff-BBO가 우리가 풀려고 하는 문제랑 가장 가까운 세팅임을 언급하고, 한계도 설명하면 좋을듯 (diffusion ensemble이 시간이 오래 걸린다던가 등등)
% For example, the method CbAS \cite{brookes2019conditioning} employs a Variational Autoencoder (VAE) to approximate the posterior \( p_\theta(\mathbf{x}|S) \), where \( S \) denotes the target set, using sequential training with importance sampling iteratively. Additionally, Model Inversion Networks (MINs) \cite{kumar2020model} leverage conditional GANs with reweighted training and employ approximated Thompson sampling to generate candidate solutions.

\subsection{Amortized Inference in Diffusion Models}
As diffusion models generate samples through a chain of stochastic transformations, sampling from the posterior distribution $p^{\text{post}}(\mathbf{x}) \propto p_\theta(\mathbf{x})r(\mathbf{x})$ is intractable. One of the widely used methods is estimating the guidance term by training a classifier on noised data \cite{dhariwal2021diffusion, lu2023contrastive}. However, such data is unavailable in most cases, and it is often hard to train such a classifier in high-dimensional settings. Although Reinforcement learning (RL) methods have recently been proposed and shown interesting results \cite{black2024training, fan2024reinforcement}, naive RL fine-tuning does not provide an unbiased sampler of the target distribution \cite{uehara2024fine, domingo2024adjoint}. To this end, we choose a relative trajectory balance proposed by \citet{venkatraman2024amortizing} to obtain an unbiased sampler of the posterior distribution without training an additional classifier. Furthermore, we propose two post-processing strategies, local search and filtering, to improve the sample efficiency of our method.




\section{Conclusion}
%We introduced DiBO, a novel generative model-based approach tailored for high-dimensional and scalable black-box optimization. We repeat the training of diffusion models and the ensemble of proxies and sampling candidates from the product distribution to effectively balance exploration and exploitation in high-dimensional spaces. To generate samples from the product distribution, we train the amortized sampler by fine-tuning the trained diffusion model. We observe that our method surpasses various black-box optimization methods across synthetic and real-world tasks. 

%We introduced DiBO, a novel generative model-based approach tailored for high-dimensional and scalable black-box optimization. We repeat the training of diffusion models and the ensemble of proxies and sampling candidates from the product distribution to effectively balance exploration and exploitation in high-dimensional spaces. To generate samples from the product distribution, we train the amortized sampler by fine-tuning the trained diffusion model. We observe that our method surpasses various black-box optimization methods across synthetic and real-world tasks. 

In this work, we introduce DiBO, a novel generative model-based framework for high-dimensional and scalable black-box optimization. We repeat the process of training models and sampling candidates with diffusion models to find a global optimum in a sample-efficient manner. Specifically, by sampling candidates from the posterior distribution, we can effectively balance exploration and exploitation in high-dimensional spaces. We observe that our method surpasses various black-box optimization methods across synthetic and real-world tasks. 

\paragraph{Limitation and Future work.} While our method shows superior performance on a variety of tasks, we need to train the diffusion model with the updated dataset in every round. Although we analyze that our method exhibits low computational complexity compared to the state-of-the-art baselines, one may consider constructing a framework that can efficiently reuse the trained models from the previous rounds.


% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.
\clearpage
\section*{Impact Statement}
% Optimization for real-world designs presents both opportunities and risks. For instance, while the
% design of new pharmaceuticals holds the promise of curing previously untreatable diseases, there is
% the potential for misuse, such as creating harmful biochemical agents. Researchers should be diligent
% to ensure that their innovations are employed in ways that contribute positively to societal welfare.

Real-world design optimization can lead to impressive breakthroughs, but it also carries certain risks. For example, while advanced pharmaceutical research could treat diseases once seen as incurable, those same methods could be misused to create harmful biochemical substances. Researchers have a responsibility to prioritize the well-being of society.








% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.

% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., "in previous work \cite{langley00}, we
% have shown \ldots").

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term "\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as "Supplementary Material" into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements. Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% "This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here."

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix. Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

% \section{Additional Analysis}
% \label{sec:Appendix A}
% It might be better to conduct several additional experiments in Ackley with 200 dimensions or HalfCheetah with 102 dimensions.
% \subsection{Analysis on Phase 1}
% \subsubsection{Ablation on Reweighting Scheme \textcolor{violet}{(Priority 1)}}
% We use reweighted training where the weight is proportional to $y$. We can compare our method with no reweighting and reweighting scheme from offline model-based optimization, which considers the number of data points in the bins to ensure conservatism. Furthermore, we can only apply the reweighting scheme to the diffusion model or proxy. 


% Related part: \url{https://github.com/umkiyoung/PIBO/blob/main/baselines/algorithms/pibo.py#L81-L84}

% \subsubsection{Analysis on training steps \textcolor{orange}{(Priority 3)}}
% We empirically find that when we increase the training steps of proxy and diffusion models, the performance increases rapidly at the beginning but converges to the local optimum quickly. Analyzing this part will be interesting. 

% Related argument: \texttt{num\_proxy\_epochs}, \texttt{num\_prior\_epochs}.

% \subsubsection{Analysis on Uncertainty Estimation \textcolor{cyan}{(Priority 3)}}
% We use an ensemble to measure the epistemic uncertainty of the proxy. There are several other ways, such as MC dropout. It might be better that there are no big differences in the choice for estimating uncertainty. 
% We can also analyze the effect of the number of proxies to estimate uncertainty, which is 5 as a default setting. It might be better that there are no big differences when the number of proxies becomes larger than 5, while with a small number of proxies, such as 3, the performance drops.

% Related part: \url{https://github.com/umkiyoung/PIBO/blob/main/baselines/algorithms/pibo.py#L87}

% \subsubsection{Analysis on $\gamma$ \textcolor{cyan}{(Priority 2)}}
% We empirically find that there is no big difference when we change the value of $\gamma$, but sometimes too large $\gamma$, such as 10, leads to poor performance.

% Related argument: \texttt{gamma}

% \subsection{Analysis on Phase 2}
% \subsubsection{Ablation on main components \textcolor{violet}{(Priority 1)}}
% There are mainly three components in phase 2. (1) Train an amortized sampler to sample from the product distribution, (2) local search via gradient ascent, and (3) generate many samples and filter top-k samples in terms of unnormalized product density. 

% First, remove the filtering part and conduct experiments, i.e., generate $B$ samples from the amortized sampler and conduct a local search, and evaluate those samples. Second, we also remove the local search part. Finally, we generate samples from the diffusion model instead of the amortized sampler.

% Related argument: \texttt{local\_search}
% Related part: \url{https://github.com/umkiyoung/PIBO/blob/main/baselines/algorithms/pibo.py#L173-L175}

% \subsubsection{Analysis on $\beta$ \textcolor{violet}{(Priority 1)}}
% We empirically find that when we increase $\beta$, the performance increases rapidly at the beginning but converges to the local optimum quickly. Analyzing this part will be interesting. Be careful that as large $\beta$ leads to some numerical issues, we use $\alpha$ instead for implementation.

% Related argument: \texttt{alpha}.

% \subsubsection{Analysis on local search steps \textcolor{cyan}{(Priority 2)}}
% %We empirically find that when we increase the local search steps, the performance increases rapidly at the beginning but converges to the local optimum quickly. Analyzing this part will be interesting.
% %Related argument: \texttt{local\_search\_epochs}.
% % \begin{figure*}[t]
% %     \centering
% %     \includegraphics[width=0.9\textwidth]{figures/}
% %     \caption{Plot of the diverse local search steps}
% %     \textcolor{red}{(Placeholder)}.
% %     \label{fig:Local Search}
% % \end{figure*}

% We analyzed the local search step in \cref{sec:Phase 2}, as shown in \cref{fig:Local Search}. While numerous local search steps may initially boost performance, they lead to earlier convergence at a local optimum. This can be viewed as an exploration-exploitation trade-off: Although the adequate local search improves sample efficiency, with the excessive steps, the batch-wise nature of black-box optimization causes most sampled candidates to converge to the same region, reducing diversity and sample efficiency. Additionally, the deterministic nature of climbing the predefined target distribution limits the chances of escaping the local optimum and prevents exploration of other potentially promising areas.


% \subsubsection{Analysis on filtering \textcolor{orange}{(Priority 3)}}
% We generate $B\times100$ samples and filter top-$B$ samples for evaluation. Using different numbers of total samples and analyzing the effect on the performance can be interesting.

% Related part: \url{https://github.com/umkiyoung/PIBO/blob/main/baselines/algorithms/pibo.py#L173-L175}

% \subsubsection{Analysis on amortized sampler \textcolor{cyan}{(Priority 2)}}
% We use RTB loss, which can be trained in an off-policy manner. We use the following procedure for training an amortized sampler. With probability 0.5, generate on-policy samples and update parameters with the RTB loss. Otherwise, gather samples from the reward-prioritized buffer and update parameters with the RTB loss. Therefore, we can conduct some analysis on the training part, such as on-policy vs. off-policy, the number of training steps, and the use of a reward-prioritized buffer.

% Related part: \url{https://github.com/umkiyoung/PIBO/blob/main/baselines/algorithms/pibo.py#L134-L161}

% \subsection{Analysis on other parts}
% \subsubsection{Analysis on moving dataset \textcolor{violet}{(Priority 1)}}
% We remove low-scoring input-output pairs from the dataset as the optimization process goes by. Analyzing the effect of dataset size can be interesting. We empirically observe that when we remove a high ratio of samples, it can be easily trapped to local optima, while not removing the dataset leads to slow convergence.

% Related argument: \texttt{buffer\_size}

% \subsubsection{Analysis on experiment setting \textcolor{cyan}{(Priority 2)}}
% We can change the size of the initial dataset (mostly 200) and batch size (mostly 100). By using different sizes of the initial dataset (e.g., 100, 200, 500, 100) or different batch sizes (e.g., 10, 20, 50, 100, 200), we can analyze the effect of those components on the performance.

% Related argument: \texttt{n\_init}, \texttt{batch\_size}

% \subsubsection{Analysis on Diffusion Timesteps \textcolor{orange}{(Priority 3)}}
% We use 30 as a default diffusion timestep as it should be run fast. We also observe that there is no significant difference in the performance even if we use a large diffusion step. Analysis of this part may also be beneficial.

% Related argument: \texttt{diffusion\_steps}

\newpage
\section{Task Details}

In this section, we present a detailed description of the benchmark tasks used in our experiments.
\label{sec:Task Details}
\subsection{Synthetic Functions}
We conduct experiments on four complex synthetic functions that are widely used in BO literature: \textit{Rastrigin, Ackley, Levy,} and \textit{Rosenbrock}. Levy and Rosenbrock have global optima within long, flat valleys, whereas Rastrigin and Ackley have numerous local optima. This characteristic makes all four functions particularly challenging as the dimensionality increases. Following previous studies \cite{wang2020learning, yi2024improving}, we set the search space for each function as, Rastrigin: $[-5, 5]^D$, Ackley: $[-5, 10]^D$, Levy: $[-10, 10]^D$, and Rosenbrock: $[-5, 10]^D$.

\subsection{MuJoCo locomotion}
MuJoCo locomotion task \cite{todorov2012mujoco} is a popular benchmark in Reinforcement Learning (RL). In this context, we optimize a linear policy $\mathbf{W}$ described by the equation \(\mathbf{a} = \mathbf{W}\mathbf{s}\). The average return of this policy serves as our objective, and our goal is to identify the weight matrix that maximizes this return. We specifically focus on the \textit{HalfCheetah} task, which has a dimensionality of $102$. Each entry of the weight matrix \(\mathbf{W}\) is constrained to the range \([-1, 1]\), and we utilize $3$ rollouts for each evaluation.  We followed the implementation of these tasks from the prior work \citet{ngo2024high}. \footnote{\url{https://github.com/LamNgo1/cma-meta-algorithm}} 




\subsection{Rover Trajectory Optimization}
Rover Trajectory Optimization is a task determining the trajectory of a rover in a 2D environment suggested by \citet{wang2018batched}. Following previous work \citet{ngo2024high}, we utilized a much harder version with a 100-dimensional variant, optimizing 50 distinct points. This task requires specifying a starting position \( s \), a target position \( g \), and a cost function applicable to the state space. We can calculate the cost \( c(\mathbf{x}) \) for a specific trajectory solution by integrating the cost function along the trajectory \( \mathbf{x} \in [0,1]^{100} \). The reward function is defined as:
\(
f(\mathbf{x}) = c(\mathbf{x}) + \lambda\left(\|\mathbf{x}_{0,1} - s\|_1 + \|\mathbf{x}_{99,100} - g\|_1\right) + b
\). We followed implementation from \citet{wang2018batched}. \footnote{\url{https://github.com/zi-w/Ensemble-Bayesian-Optimization}}

\subsection{LassoBench}
LassoBench \cite{vsehic2022lassobench} \footnote{\url{https://github.com/ksehic/LassoBench}} is a challenge focused on optimizing the hyperparameters of Weighted LASSO (Least Absolute Shrinkage and Selection Operator) regression. The goal is to fine-tune a set of hyperparameters to achieve a balance between least-squares estimation and the sparsity-inducing penalty term. LassoBench serves both synthetic (simple, medium, high, hard) and real-world tasks, including (Breast cancer, Diabetes, Leukemia, DNA, and RCV1). Specifically, we focused on the DNA task, which is a 180-dimensional hyperparameter optimization task that utilizes a DNA dataset from a microbiological study. In \cref{fig:realworld_tasks}, we present the original results multiplied by -1 for improved visibility. 

\newpage
\section{Methodology Details}
\label{sec:Methodology Details}
In this section, we provide a detailed overview of the methodology, covering model implementations and architectures, training procedures, hyperparameter settings, and computational resources.

\subsection{Training Models}
\subsubsection{Training Proxy Model}
We train five ensembles of proxies.
To implement the proxy function, we use MLP with three hidden layers, each consisting of 256 (512 for 400 dim tasks) hidden units and GELU \cite{hendrycks2016gaussian} activations. 
We train a proxy model using Adam \cite{kingma2014adam} optimizer for 50 (100 for 400 dim tasks) epochs per round, with a learning rate $1 \times 10^{-3}$. We set the batch size to 256. The hyperparameters related to the proxy are listed in \cref{table:proxy hyperparams}.
\input{tables/proxy}



\subsubsection{Training Diffusion Model}
\label{sec:prior hyperparams}
We utilize the temporal Residual MLP architecture from \citet{venkatraman2024amortizing} as the backbone of our diffusion model. The architecture consists of three hidden layers, each containing 512 hidden units. We implement GELU activations alongside layer normalization \cite{ba2016layer}. During training, we use the Adam optimizer for 50 epochs (100 for 400 dim tasks) per round with a learning rate of \(1 \times 10^{-3}\). We set the batch size to 256. We employ linear variance scheduling and noise prediction networks with 30 diffusion steps for all tasks. The hyperparameters related to the diffusion model are summarized in \cref{table:prior hyperparams}.
\input{tables/prior}

\clearpage
\subsection{Sampling Candidates}
\subsubsection{Fine-tuning diffusion model}
\label{app:fine-tuning}
We use relative trajectory balance (RTB) loss to fine-tune the diffusion model for obtaining an amortized sampler of the posterior distribution.
\begin{align}
    \label{eq:RTB; Appendix}
     \mathcal{L}(\mathbf{x}_{0:T};\psi)=\left(\log\frac{Z_{\psi}\cdot p_{\psi}(\mathbf{x}_{0:T})}{\exp\left(\beta\cdot r_{\phi}(\mathbf{x}_{0})\right)\cdot p_{\theta}(\mathbf{x}_{0:T})}\right)^2
\end{align}


As stated in the original work by \citet{venkatraman2024amortizing}, the gradient of this objective concerning \(\psi\) does not necessitate backpropagation into the sampling process that generates a trajectory $\mathbf{x}_{0:T}$. Consequently, the loss can be optimized in an off-policy manner. Specifically, we can optimize \cref{eq:RTB; Appendix} with (1): on-policy trajectories $\mathbf{x}_{0:T} \sim p_\psi(\mathbf{x}_{0:T})$ %where $\psi$ can be different from $\psi$ itself
or (2): off-policy trajectories $\mathbf{x}_{0:T}$ generated by noising process given $\mathbf{x}_0$ sampled from the buffer.

% This provides benefits where we can use $\mathbf{x}\in \mathcal{D}_r$ to train our model with off-policy training. Furthermore, we can use a weighted sampler when sampling $\mathbf{x}$ from $\mathcal{D}_r$ to focus learning on certain points.

To effectively fine-tune our diffusion model, we train $p_\psi$ using both methods. For each iteration, we select a batch of on-policy trajectories with a probability of 0.5 and off-policy trajectories otherwise. When sampling from the buffer, we use reward-prioritized sampling to focus on data points with high UCB scores.
% Additionally, to focus more on the high-UCB ($r_\phi(\mathbf{x})$) region during training, we utilize $\exp(r_\phi(\mathbf{x}))$ weighted sampler during (2). $w(r_\phi(\mathbf{x}), \mathcal{D}_r)=\frac{\exp(r_\phi(\mathbf{x}))}{\sum_{(\mathbf{x}') \in \mathcal{D}_{r}}\exp({r_\phi(\mathbf{x})}')}$.
We conducted additional analysis on off-policy training in \cref{app:ablation_offpolicy}.

We initialize $\psi \leftarrow \theta$ with each iteration, so the architecture and diffusion timestep is the same with \cref{table:prior hyperparams}. During training, we use Adam optimizer for $50$ epochs (100 epochs for 400D) with learning rate $1 \times 10^{-4}$. We set the batch size to 256.
The hyperparameters for fine-tuning the diffusion model are summarized in \cref{table:posterior hyperparams}.
\input{tables/posterior}

 
All the training is done with a Single NVIDIA RTX 3090 GPU.

\subsubsection{Estimating Marginal Likelihood}
\label{subsec:EstimatingMarginalLikelihood}
During the \textbf{local search} and \textbf{filtering} in \cref{sec:Phase 2}, we use the probability flow ordinary differential equation (PF ODE) to estimate the marginal log-likelihood of the diffusion prior \(\log p_\theta(\mathbf{x})\). We consider the diffusion forward process as the following stochastic differential equation (SDE):

\begin{equation}
\label{eq:forward_sde}
d\textbf x = \textbf f(\textbf x, t) dt + g (t) d\textbf w
\end{equation}

and the corresponding reverse process is 

\begin{equation}
\label{eq:reverse_sde}
d\textbf x = [\textbf f(\textbf x, t) -g(t)^2\nabla_{\textbf x} \log p_t(\textbf x)]dt + g (t) d \bar {\textbf w}
\end{equation}

where \(\mathbf{w}\) and \(\bar{\mathbf{w}}\) are forward and reverse Brownian motions, and $\textbf f$ and $g$ are drift coefficient and diffusion coefficient respectively. The quantity \(p_t(\mathbf{x})\) denotes the marginal distribution of \(\mathbf{x}\) at time \(t\). As we do not have direct access to score $\nabla_\mathbf{x} \log p_t(\mathbf{x})$, it should be modeled with network approximation $s_\theta(\mathbf{x},t) \approx \nabla_\mathbf{x} \log p_t(\mathbf{x})$, while in our case implicitly modeled by noise prediction network $\epsilon_\theta(\mathbf{x}, t)$ \cite{kingma2021variational}.

There also exists a deterministic PF ODE,
\begin{equation}
    d\textbf x = [\textbf f(\textbf x, t) - \frac 1 2 g(t)^2\nabla_\mathbf{x} \log p_t(\mathbf{x})]dt
\end{equation}
which evolves the sample \(\mathbf{x}\) through the same marginal distributions \(\{p_t(\mathbf{x})\}\) as \cref{eq:forward_sde,eq:reverse_sde}, under suitable regularity conditions. \cite{song2021score}

%With the trained score estimator network $s_\theta(\mathbf{x}, t) \approx \log p_t(\mathbf{x})$, we have the PF ODE formulation as follows:

%\begin{equation}
%    d\textbf x = [\textbf f(\textbf x, t) - \frac 1 2 %g(t)^2s_\theta(\textbf x, t)]dt
%\end{equation}


With the trained $s_\theta(\mathbf{x}, t)$, we can estimate \(\log p_0(\mathbf{x}_0)=\log p_\theta(\mathbf{x})\) by applying the instantaneous change-of-variables formula \cite{chen2018neural} to the PF ODE:
\begin{equation}
\label{eq:marginal_likelihood}
\log p_0(\mathbf{x}_0)
\;=\;
\log p_T(\mathbf{x}_T)
\;+\;
\int_0^T
\,\nabla \cdot  {\bar {\textbf f}}_\theta(\mathbf{x}(t), t)
\,dt
\end{equation}

where 
\begin{equation}
\bar {\textbf {f}}_\theta(\textbf{x}(t), t) := \textbf f(\textbf{x}, t) - \frac 1 2 g(t)^2 s_\theta(\textbf{x}, t).
\end{equation}
However, directly computing the trace of \(\bar {\textbf f}_\theta\) is computationally expensive. Following \citet{grathwohl2019ffjord, song2021score}, we use the Skilling-Hutchinson trace estimator \cite{skilling1989eigenvalues, hutchinson1989stochastic} to estimate the trace efficiently:

\begin{equation}
\nabla\cdot \bar {\textbf f}_\theta(\mathbf{x}, t) = \mathbb E_{\nu}[\nu ^\intercal \nabla\bar {\textbf f}_\theta(\mathbf{x}, t) \nu],
\end{equation}

where the $\nu$ is sampled from the Rademacher distribution.

We solve \cref{eq:marginal_likelihood} using a differentiable ODE solver \texttt{torchdiffeq} \cite{torchdiffeq} with 4th-order Runge--Kutta (RK4) integrator, accumulating the divergence term in the integral to approximate \(\log p_0(\mathbf{x}_0)\). Since the PF ODE is deterministic, this entire simulation is fully differentiable, enabling gradient-based optimization with respect to the $\mathbf{x}_0$, thereby supporting the local search stage.


% During the \textbf{local search} and \textbf{filtering} in \cref{sec:Phase 2}, we use the probability flow ordinary differential equation (PF ODE) to estimate the marginal log-likelihood of the diffusion prior \(\log p_\theta(\mathbf{x})\). Following \citet{song2021score}, the discrete-time diffusion probabilistic model \cite{ho2020denoising} can be viewed in the continuous setting as a Variance Preserving (VP) SDE. Specifically, the forward process is
% \begin{equation}
% \label{eq:forward_sde}
% d\mathbf{x} = -\tfrac{1}{2}\beta(t)\,\mathbf{x}\,dt \;+\; \sqrt{\beta(t)}\,d\mathbf{w},
% \end{equation}
% and the corresponding reverse process is
% \begin{equation}
% \label{eq:reverse_sde}
% d\mathbf{x} = -\tfrac{1}{2}\beta(t)\,\mathbf{x}\,dt \;-\; \beta(t)\,\nabla_{\mathbf{x}}\log p_t(\mathbf{x})\,dt \;+\; \sqrt{\beta(t)}\,d\bar{\mathbf{w}},
% \end{equation}
% where \(\mathbf{w}\) and \(\bar{\mathbf{w}}\) are forward and reverse Brownian motions, respectively, and \(\beta(t)\) is a continuous function interpolating the discrete variance schedule \(\{\beta_i\}_{i=1}^T\). The quantity \(p_t(\mathbf{x})\) denotes the distribution of \(\mathbf{x}\) at time \(t\). As we do not have direct access to score $\nabla_\mathbf{x} \log p_t(\mathbf{x})$, it should be modeled with network approximation $s_\theta(\mathbf{x}(t),t) \approx \nabla_\mathbf{x} \log p_t(\mathbf{x})$, while in our case implicitly modeled by noise prediction network $\epsilon_\theta(\mathbf{x}(t), t)$ \cite{kingma2023variationaldiffusionmodels}.


% There also exists a deterministic PF ODE,
% \begin{equation}
% \label{eq:pf_ode}
% \frac{d \mathbf{x}}{dt}
% \;=\;
% -\tfrac{1}{2}\beta(t)\,\mathbf{x}
% \;-\;
% \tfrac{1}{2}\beta(t)\,\nabla_{\mathbf{x}}\log p_t(\mathbf{x}),
% \end{equation}
% which evolves the sample \(\mathbf{x}\) through the same marginal distributions \(\{p_t(\mathbf{x})\}\) as \cref{eq:forward_sde,eq:reverse_sde}, under suitable regularity conditions. To estimate \(\log p_0(\mathbf{x}_0)=\log p_\theta(\mathbf{x})\), we can apply the instantaneous change-of-variables formula \cite{chen2018neural} to the PF ODE:
% \begin{equation}
% \label{eq:marginal_likelihood}
% \log p_0(\mathbf{x}_0)
% \;=\;
% \log p_T(\mathbf{x}_T)
% \;+\;
% \int_0^T
% \,\nabla \cdot f_\theta(\mathbf{x}(t), t)
% \,dt,
% \end{equation}
% where
% \[
% f_\theta(\mathbf{x}(t),t) 
% = -\tfrac{1}{2}\beta(t)\,\mathbf{x} (t)
% \;-\; 
% \tfrac{1}{2}\beta(t)\,s_\theta(\mathbf{x}(t),t).
% \]
% We solve \cref{eq:marginal_likelihood} with a \texttt{torchdiffeq} \cite{torchdiffeq} 4th-order Runge--Kutta (RK4) integrator, accumulating the divergence term in the integral to approximate \(\log p_0(\mathbf{x}_0)\). Because the PF ODE is deterministic, this entire simulation is fully differentiable, allowing gradient-based optimization with respect to the $\mathbf{x}_0$, thereby supporting the local search stage.

%In practice, we do not have direct access to \(\log p_t(\mathbf{x})\), so we approximate \(\nabla_{\mathbf{x}}\log p_t(\mathbf{x})\) (i.e., the score) with a time-dependent neural network trained via denoising score matching. In this case, we parameterize noise where the model learns to predict \(\epsilon_\theta(\mathbf{x}_t, t)\), the Gaussian noise added at step \(t\). Under a Variance Preserving assumption, if \(\mathbf{x}_t\) has standard deviation \(\sigma_t\) of the forward noise, then
% \begin{equation}
% \label{eq:score_epsilon_relation}
% \nabla_{\mathbf{x}} \log p_t(\mathbf{x}_t)
% \;\approx\;
% -\frac{1}{\sigma_t}\,\epsilon_\theta(\mathbf{x}_t,t),
% \end{equation}
% which implicitly gives us the score. Here, \(\sigma_t\) captures the noise level at continuous time \(t\), and can be derived from the integrated noise schedule (for example, \(\sigma_t^2 = 1 - \exp(-\!\int_0^t \beta(s)\,ds)\) in a VP SDE).

\subsubsection{Hyperparameters}
% During phase 2, there are several hyperparameters while modeling target distribution, selecting candidates, and moving datasets.
For the upper confidence bound (UCB), we fixed $\gamma = 1.0$ that controls the exploration-exploitation. 
For the target posterior distribution, the inverse temperature parameter $\beta$ controls the trade-off between the influence of $\exp(r_\phi(\mathbf{x}))$ and $p_\theta(\mathbf{x})$.
When selecting querying candidates, we sample $M=B \times 10^2$ candidates from $p_\psi(\mathbf{x})$, perform a local search for $J$ steps, and retain $B$ candidates for batched querying. 
% We maintain a fixed ratio of $M = 10^2 \times B$ across all tasks.
After querying and adding candidates, we maintain our training dataset to contain $L$ high-scoring samples. We present the detailed hyperparameter settings in \cref{table:candidate selection}. We also conduct several ablation studies to explore the effect of each hyperparameter on the performance.
\input{tables/candidate_selection}
%\subsection{Running Time}

\newpage
\section{Baseline Details}
\label{sec:Baseline Details}  
In this section, we provide details of the baseline implementation and hyperparameters used in our experiments.


\textbf{TuRBO} \cite{eriksson2019scalable}: We use the original code \footnote{\url{https://github.com/uber-research/TuRBO}} and keep all settings identical to those in the original paper. For all the algorithms utilizing TuRBO as a base algorithm (TuRBO, LA-MCTS, MCMC-BO), we use TuRBO-1 (No parallel local models). 

\textbf{LA-MCTS} \cite{wang2020learning}: We use the original code \footnote{\url{https://github.com/facebookresearch/LA-MCTS}} and keep all settings identical to those in the original paper.

\textbf{MCMC-BO} \cite{yi2024improving}: We utilize the original code \footnote{\url{https://drive.google.com/drive/folders/1fLUHIduB3-pR78Y1YOhhNtsDegaOqLNU?usp=sharing}} and adjust the standard deviation of the proposal distribution during the Metropolis-Hastings steps to replicate the results from the original paper. Due to memory constraints during the MCMC steps, we report a total of 6,000 evaluations for the $D=400$ tasks.

\textbf{CMA-BO} \cite{ngo2024high}: We use the original code \footnote{\url{https://github.com/LamNgo1/cma-meta-algorithm}} and keep all settings identical to those in the original paper.

\textbf{CbAS} \cite{brookes2019conditioning}: We reimplement the original code \footnote{\url{https://github.com/dhbrookes/CbAS}} with PyTorch and keep all settings identical to those in the original paper.

\textbf{MINs} \cite{kumar2020model}: 
We reimplement the code from \citet{trabucco2022design} \footnote{\url{https://github.com/brandontrabucco/design-bench}} with PyTorch and keep all settings identical to those in the original paper.

\textbf{DDOM} \cite{krishnamoorthy2023diffusion}: To ensure a fair comparison, we reimplement the original code \footnote{\url{https://github.com/siddarthk97/ddom}} to work with our diffusion models and add classifier free guidance for conditional generation. We use the same method-specific hyperparameters following the original paper and tune the training epochs ($200$ as a default, $400$ for $D=400$ tasks) for each task to optimize performance.

\textbf{Diff-BBO} \cite{wu2024diff}: As there is no open-source code, we implement it according to the details provided in the original paper. As with DDOM, we use the original method-specific hyperparameters and tune the training epochs ($200$ as a default, $400$ for $D=400$ tasks) for each task to improve performance.

\textbf{CMA-ES} \cite{hansen2006cma}: We use an existing library pycma \cite{hansen2019pycma} and adjust the initial standard deviation as $\sigma_0 = 0.1$, which gives better performance on all tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Extended Additional Analysis}
In this section, we present additional analysis on DiBO that is not included in the main manuscript due to the page limit.
\subsection{Effect of Training Epochs}\label{app:ablation_epoch}
The number of epochs for training models can be crucial in the performance of black-box optimization algorithms. If we use too small a number of epochs, the proxy may underfit, and the diffusion model may find it hard to capture the complex data distribution accurately. On the other hand, if we use too large a number of epochs, the proxy and the diffusion may overfit to the dataset, and the overall procedure takes longer time for each round.

To this end, we conduct experiments on Rastrigin-200D and HalfCheetah-102D by varying training epochs. As shown in the \Cref{fig:ablation_epoch}, when we use large training epochs, the performance improves significantly at the early stage but eventually converges to the sub-optimal results due to the overfitting of the proxy and diffusion model, which may hinder exploration towards promising regions.

\begin{figure*}[h]
\centering
\begin{minipage}[t]{0.8\textwidth}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_epoch_rastrigin.png}
        % \subcaption{Reweighted Training}
        \label{fig:ablation_epoch_rastrigin}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_epoch_halfcheetah.png}
        % \subcaption{Sampling Procedure}
        \label{fig:ablation_epoch_halfcheetah}
    \end{subfigure}
    \vspace{-10pt}
    \caption{Performance of DiBO in Rastrigin-200D and HalfCheetah-102D by varying training epochs. Experiments are conducted with four random seeds. Mean and one standard deviation are reported.}
    \label{fig:ablation_epoch}
\end{minipage}
\end{figure*}



\clearpage
\subsection{Analysis on Local Search Steps $J$}\label{app:ablation_j}
We conduct additional analysis on local search steps $J$. Through local search, we can capture the modes of the target distribution, which leads to high sample efficiency. However, using too large local search steps may focus on exploiting a single mode with the highest density of the target distribution, resulting in sub-optimal results. 

To this end, we conduct experiments on Rastrigin-200D and HalfCheetah-102D by varying $J$. As shown in the \Cref{fig:ablation_j}, our method shows a relatively slow learning curve when we remove the local search. On the other hand, if we use too large $J$, it struggles to escape from local optima and eventually results in sub-optimal results.

\begin{figure*}[h]
\centering
\begin{minipage}[t]{0.8\textwidth}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_j_rastrigin.png}
        % \subcaption{Reweighted Training}
        \label{fig:ablation_j_rastrigin}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_j_halfcheetah.png}
        % \subcaption{Sampling Procedure}
        \label{fig:ablation_j_halfcheetah}
    \end{subfigure}
    \vspace{-10pt}
    \caption{Performance of DiBO in Rastrigin-200D and HalfCheetah-102D by varying $J$. Experiments are conducted with four random seeds. Mean and one standard deviation are reported.}
    \label{fig:ablation_j}
\end{minipage}
\end{figure*}





\subsection{Effect of Off-policy Training in Amortized Inference}\label{app:ablation_offpolicy}

During the fine-tuning stage, we employ off-policy training with the RTB loss function, as detailed in \Cref{app:fine-tuning}. To assess the impact of this approach, we conduct a comparative experiment using only on-policy training.

As illustrated in \Cref{fig:ablation_offpolicy}, off-policy training demonstrates a significant performance advantage over on-policy training. In on-policy training, the model is restricted to learning only from the generated samples. Consequently, crucial data points, particularly those associated with significant events, are rarely encountered during training. In contrast, off-policy training effectively captures these critical regions by directly leveraging information from a replay buffer, enabling the model to learn from informative data points efficiently.



\begin{figure*}[h]
\centering
\begin{minipage}[t]{0.8\textwidth}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_offpolicy_rastrigin.png}
        % \subcaption{Reweighted Training}
        \label{fig:ablation_policy_rastrigin}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_offpolicy_halfcheetah.png}
        % \subcaption{Sampling Procedure}
        \label{fig:ablation_policy_halfcheetah}
    \end{subfigure}
    \vspace{-10pt}
    \caption{Performance of DiBO in Rastrigin-200D and HalfCheetah-102D with and without off-policy training. Experiments are conducted with four random seeds. Mean and one standard deviation are reported.}
    \label{fig:ablation_offpolicy}
\end{minipage}
\end{figure*}

%\subsection{Effect of Generative Modeling}
%As demonstrated in \Cref{fig:motivation}, due to the substantial increase in uncertainty distant from the available dataset, directly maximizing the UCB score within high-dimensional spaces can lead to significant performance degradation.To investigate this phenomenon, we conduct experiment utilizing a diffusion sampler that draws samples proportional to the exponential of UCB: $\exp(r_\phi(\mathbf{x}))$ instead of our product distribution: $\exp(\beta \cdot r_\phi(\mathbf{x}))p_\theta(\mathbf{x})$.

\clearpage
\subsection{Analysis on Initial Dataset Size $\vert\mathcal{D}_0\vert$}\label{app:ablation_D0}
The size of the initial dataset $\vert\mathcal{D}_{0}\vert$ can be crucial in the performance of black-box optimization algorithms. If the initial dataset is too small and concentrates on a small region compared to the whole search space, it is hard to explore diverse promising regions without proper exploration strategies. 

To this end, we conduct experiments by varying $\vert\mathcal{D}_0\vert$ on the synthetic tasks. As shown in the \Cref{fig:ablation_D0}, our method demonstrates robustness regarding the size of the initial dataset. It indicates that our exploration strategy, proposing candidates by sampling from the posterior distribution, is powerful for solving practical high-dimensional black-box optimization problems.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ablation_D0.png}
    \vspace{-10pt}
    \caption{Performance of DiBO in synthetic functions by varying $\vert\mathcal{D}_0\vert$. Experiments are conducted with four random seeds. Mean and one standard deviation are reported.}
    \label{fig:ablation_D0}
\end{figure*}

\subsection{Analysis on Batch Size $B$}\label{app:ablation_B}
The batch size $B$ can be crucial in the performance of black-box optimization algorithms. As the number of evaluations is mostly limited, if we use too large $B$, it is hard to focus on high-scoring regions. On the other hand, if we use too small $B$, it hinders exploration, and it is hard to escape from local optima. 

To this end, we conduct experiments by varying $B$ on the synthetic tasks. Note that we fix the batch size for all main experiments as $B=100$. We visualize the experiment results in \Cref{fig:ablation_B}. We can observe that our method shows robust performance across different $B$ while using a large batch size leads to slightly slow convergence compared to using a small batch size. Using a smaller batch size shows better sample efficiency but also leads to an increase in computational time.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ablation_B.png}
    \vspace{-10pt}
    \caption{Performance of DiBO in synthetic functions by varying $B$. Experiments are conducted with four random seeds. Mean and one standard deviation are reported.}
    \label{fig:ablation_B}
\end{figure*}

\clearpage
\subsection{Analysis on Uncertainty Estimation}\label{app:uncertainty_estimation}
To promote exploration, we estimate uncertainty with an ensemble of proxies and adopt an upper confidence bound (UCB) to define the target distribution. 
Specifically, we use: $r_\phi(\mathbf{x}) = \mu_\phi(\mathbf{x}) + \gamma\cdot \sigma_\phi(\mathbf{x})$, where $\gamma$ controls the degree of uncertainty bonus. We evaluated two aspects of this approach in the HalfCheetah-102D task.

To analyze the effectiveness of the ensemble strategy for uncertainty estimation, besides our ensemble method, we test Monte Carlo (MC) dropout \cite{gal2016dropout} and a setup without uncertainty estimation (one proxy).
As shown in \Cref{fig:ablation_uncertainty_halfcheetah}, the ensemble strategy effectively estimates the uncertainty and improves sample efficiency compared to others.

To analyze if UCB with uncertainty bonus promotes exploration, we conduct experiments by varying the parameter $\gamma$ and analyzing its impact on performance. 
\cref{fig:ablation_gamma_halfcheetah} demonstrate that increasing $\gamma$ leads to more extensive search space exploration. 
% enabling the model to identify high-performing regions that might otherwise be overlooked. 
However, excessively large $\gamma$ values ($\gamma = 10.0$) dilute the focus on exploitation, slowing convergence. 

%To evaluate the effectiveness of UCB in encouraging broader exploration, we conducted experiments by varying the parameter $\gamma$ and analyzing its impact on performance. 
%\cref{fig:ablation_j} demonstrate that increasing $\gamma$ leads to more extensive exploration of the search space, enabling the model to identify high-performing regions that might otherwise be overlooked. However, excessively large $\gamma$ values can dilute the focus on exploitation, potentially slowing convergence. 

\begin{figure*}[h]
\centering
\begin{minipage}[t]{0.8\textwidth}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_uncertainty_halfcheetah.png}
        \subcaption{Analysis on uncertainty estimation methods}
        \label{fig:ablation_uncertainty_halfcheetah}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
        \includegraphics[width=\textwidth]{figures/ablation_gamma_halfcheetah.png}
        \subcaption{Analysis on $\gamma$}
        \label{fig:ablation_gamma_halfcheetah}
    \end{subfigure}
    \vspace{-5pt}
    \caption{Performance of DiBO in HalfCheetah-102D with varying uncertainty estimation methods and gamma $\gamma$. Experiments are conducted with four random seeds. Mean and one standard deviation are reported.}
    \label{fig:ablation_uncertainty}
\end{minipage}
\end{figure*}

These findings demonstrate that an ensemble of proxies effectively captures uncertainty even in high-dimensional spaces. Moreover, designing a target distribution that incorporates UCB helps balance exploration and exploitation, improving sample efficiency during optimization. 


\clearpage
\subsection{Time Complexity of our method}\label{app:time_complexity}
We report the average running time per each round in \Cref{table:time_compleixty}. All training is done with a single NVIDIA RTX 3090 GPU and Intel Xeon Platinum CPU @ 2.90GHZ. As shown in the table, the running time of our method is similar to generative model-based approaches and mostly faster than BO methods. It demonstrates the efficacy of our proposed method.
\input{tables/time_complexity}
% \input{tables/time_complexity_realworld}
% \input{tables/time_complexity_all}

\subsection{Analysis on more computation time}\label{app:sota}
In this section, we present supplementary results demonstrating that more computing time can significantly improve performance beyond the results reported in the main text. %Although we constrained our search space there to ensure fair comparisons across methods, we observe that task-specific tuning can yield notably better outcomes. 
For example, on the Ackley-200D benchmark, with the local search steps $J=50$ and buffer size, $L=2000$ improves performance from the default configuration value of \(-0.643\) to a score of \(-0.260\). %This example highlights how deeper, task-tailored tuning can further boost performance across various benchmarks.
While increasing the number of local search steps has a possibility of converging to the local optimum, a large buffer size complements this issue. However, using larger $J$ and $L$ leads to an increase in computational complexity. Exploring methods that reduce the complexity of training while maintaining large buffer sizes and local search steps may be a promising research direction. We leave it as a future work.
\begin{figure*}[h]
\centering
\begin{minipage}[t]{0.8\textwidth}
    \centering
    % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=0.4\textwidth]{figures/ablation_bestofall_Ackley.png}
    \vspace{-5pt}
    \caption{Performance of DiBO in Ackley-200D with local search epochs $J=50$, and buffer size $L=2000$. Experiments are conducted with four random seeds. Mean and one standard deviation are reported.}
    \label{fig:ablation_bestofall_Ackley}
\end{minipage}
\vspace{-10pt}
\end{figure*}













\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.