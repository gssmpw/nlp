\section{Related Works}
\label{sec:related}
Many filter pruning methods have been studied without considering such a case where we cannot use any data, whether real or synthetic, to restore a pruned network. They mostly go through an expensive retraining process either iteratively Li et al., "DPP: Differentiable Pruning Process" or lastly as a fine-tuning phase Chen et al., "Filter Pruning via Structured Sparsity". In this article, we focus on the opposite case, which is more challenging in practice, where fine-tuning with any data is not available.

\smalltitle{Pruning with less fine-tuning} 
Most of the existing filter pruning methods have tried to avoid an expensive fine-tuning process by means of a carefully designed criterion for identifying unimportant filters such that the loss of information is minimized when they are pruned. To this end, they often make the best use of data-dependent values like channels and gradients that can be obtained by making inferences with some data. Huang et al., "Global Filter Pruning via Joint Optimization" proposes a greedy algorithm that prunes the filters whose corresponding channels minimize the layer-wise reconstruction error. Similarly, He et al., "Lasso Regression Based Channel Selection for Pruned Networks" formulates this problem of channel selection as lasso regression and least square reconstruction. \revfour{Wang et al., "Global Redundant Filter Selection via Feature Discrimination" globally chooses the redundant filters in the pre-trained model by utilizing feature-discrimination-based filter importance. Zhang et al., "Class-Aware Trace Ratio Optimization for Uninformative Channel Selection" suggests uninformative channel selection method via class-aware trace ratio optimization, which measures joint impact across channels in a convolutional layer.} Li et al., "Data-Independent Pruning Criterion Based on Coreset" introduces data-independent pruning criterion based on coreset together with an intermediate recovery method without training in order to reduce the overhead of fine-tuning. Despite their proposed methods on effective channel or neuron selection, a few epochs of fine-tuning process as well as some training data is inevitable for the pruned network to be sufficiently recovered. \revfour{To speedup filter pruning, Xu et al., "Coarse-to-Fine Neural Architecture Search via Contrastive Knowledge Transfer" utilizes a finetuning structure based on constrastive knowledge transfer, thereby proposing a coarse-to-fine neural architecture search algorithm.}


\smalltitle{Pruning with less fine-tuning and less data}
For the case of pruning with limited data, several methods Li et al., "Knowledge Distillation Based Filter Pruning" utilize knowledge distillation Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning" not to use the entire original data in the recovery process. Wang et al., "CURL: A Data-Free Method for Global Filter Pruning via KL-Divergence" recently proposes that globally prunes filters using a KL-divergence based criterion and perform knowledge distillation with a small dataset as a fine-tuning process. Huang et al., "Transforming Filters into Compact Ones via Limited Data" tackles the similar problem and devise a way of transforming all the filters in a layer into compact ones, called reborn filters, using only the limited data. These methods do not use the entire original data, but they still require some kinds of retraining process with a small amount of data. 

\smalltitle{Pruning with no fine-tuning and no data} In the literature of filter pruning, there are only a few works Zhang et al., "Pure Recovery Process for Pruned Networks via Similarity Based Compensation" that adopt the same problem setting as ours, that is, pruning filters without any fine-tuning and data. \rev{It is worth mentioning that our work focuses on this pure recovery process with respect to pruned networks without any support of extra memory and hardware, as in RED++.} Both methods are based on the strong assumption that we can always find a pair of filters that are quite similar to each other and therefore a pruned filter can be compensated by its similar unpruned one. However, this assumption does not hold in modern convolutional neural networks (CNNs) having a number of filters in a fairly deep architecture. As even presented in Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning", the cosine similarity between each filter and its nearest one gets extremely low (\textit{e.g.}, around 0.2) in back layers to the point that it is better to do nothing for some pruned neurons that do not have sufficiently similar preserved neurons. In this article, we remedy this problem by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works. \rev{Meanwhile, the RED++ method Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning" more concentrates on how to make the best use of extra memory buffer (i.e., RAM) other than GPU in order to reduce the number of GPU computations. Therefore, it requires high-speed memory access with a specific hardware support, which makes orthogonal to our method.}




% \lee{It is worth mentioning that Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning" methods concentrate on recovery to performance drop after pruning without any soft/hardware supports in same way as we do. However, Li et al., "LBYL: A Data-Free and Hardware-Agnostic Method for Filter Pruning" concentrates on how to compress network without performance drop. That is, Li et al., "LBYL: A Data-Free and Hardware-Agnostic Method for Filter Pruning" needs special soft/hardware supports to speed up memory access and allocation than computation to accelerate network inference.
% } ____

% Both methods are based on the strong assumption that we can always find a pair of filters that are quite similar to each other and therefore a pruned filter can be compensated by its similar unpruned one. However, this assumption does not hold in modern convolutional neural networks (CNNs) having a number of filters in a fairly deep architecture. As even presented in Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning", the cosine similarity between each filter and its nearest one gets extremely low (\textit{e.g.}, around 0.2) in back layers to the point that it is better to do nothing for some pruned neurons that do not have sufficiently similar preserved neurons. In this article, we remedy this problem by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works.


\smalltitle{Data-free compression with heavy retraining}
\rev{
There is a large branch of data-free compression methods Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning" that still fine-tune or retrain the compressed model with synthetically generated data but not real data. Thus, in some sense, they are not literally data-free, but free of original data. Most of these methods involves extracting a generator that can generate synthetic samples, which itself independently takes a considerable amount of training time, and rely on knowledge distillation, which thus makes them often called \textit{data-free knowledge distillation}. As in Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning", some of the works employ pruned networks as a student model, instead of a lightweight network randomly initialized, but still focus on retraining process with synthetic data. Note that these data-free methods still suffer from heavy recovery process for fine-tuning (or distillation) as well as training the generator, which is not our focus in this article.
}






% \lee{ ____ focuses on reducing the number of computations by changing identical operations to identical memory access to a unique operation. That is, Li et al., "LBYL: A Data-Free and Hardware-Agnostic Method for Filter Pruning" needs the hypothesis that memory access and allocation should run faster than mathematical operations, while our LBYL method do not need a specific hardware assumption. Therefore ____ is orthogonal to our method. In this article, we remedy problem of similarity based compensation Zhang et al., "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning" by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works.}


\smalltitle{Clarification regarding overlapping work}
We have become aware of two recently published papers—one in ICCV 2023 titled ``\textit{Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning}” and another in Pattern Recognition titled ``\textit{Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning}.” Both appear to share core derivations and theoretical components with an earlier version of our submission, which was not publicly accessible at the time those papers were written. We have formally reported our concerns about these similarities to the relevant committees. As we await a resolution, we are refraining from listing these papers in our references to avoid lending legitimacy to them while the plagiarism dispute remains unsettled.