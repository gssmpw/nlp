\section{Introduction} \label{sec:intro}


% 1. Network pruning has been extensively studied in neural networks. 
%    its simplicity, reasonable performance
Network pruning is one of the most well-studied model compression technique for an overparameterized deep neural network, and many different pruning strategies have been introduced with the goal of removing less important parameters from the original network. Due to its simplicity in terms of both implementation and methodology, network pruning has been pretty popular to the point that it is provided as a default model compression function in standard libraries like TensorFlow\footnote{https://www.tensorflow.org/model\_optimization/guide/pruning} and PyTorch\footnote{https://pytorch.org/tutorials/intermediate/pruning\_tutorial.html}, along with the other compression options such as quantization \cite{Coreset}. Among two major pruning schemes, namely weight pruning (\textit{a.k.a.} unstructured pruning) \cite{Lottery,Songhan,SNIP,Synaptic,OptimalPruning,SparseNet} and filter pruning (\textit{a.k.a.} structured pruning), filter pruning is more actively studied \cite{FPGM,Lasso,EagleEye,Rethinking,Thinet,DCP} due to its strength of reducing the actual computation cost at inference time without any special hardware support. In this sense, this article also focuses on filter pruning.

% \cite{GlobalRanking,AMC,FPGM,Lasso,EagleEye,Dynamic,Rethinking,CURL,Thinet,Importance,Reborn,NISP,DCP}

% To deploy large neural networks to resource-constrained mobile and edge devices, network pruning has been extensively studied due to implementation-friendly and compatibility with other network compression methods \cite{DBLP:conf/eccv/HeLLWLH18, DBLP:conf/cvpr/HeLWHY19} In general, Network pruning methods is fall in to two parts : Weight pruning(Unstructured pruning) and Filter pruning(Structured pruning). Weight pruning encourage parameters of neural network sparse during the training process or post process. As a result, Number of parameters in neural networks can be reduced parameters more than 90\% \cite{NIPS2015_ae0eb3ee} However, Because of irregular sparse connections Weight pruning need a special software to accelerate inference speed. On the other hand, Filter pruning removes unimportant filter so can accelerate inference time without special software. 

% 2. Most of pruning methods require an expensive training process
%    e.g., iterative training(on the fly), fine-tuning(post process)
% 3. A long fine-tuning eventually determines the final accuracy 
%    That is why existing pruning methods use only a few 2-3 epochs of fine-tuning

Even though many of the existing pruning methods achieve a reasonable level of the final accuracy, it mostly comes at an expensive cost of retraining at the end of (or in the middle of) pruning phase. For instance, iterative filter pruning methods \cite{Soft,FPGM,Dynamic} alternately perform pruning phase and retraining phase so that the original model can gradually be compressed as well as recovered over the hundreds of epochs. Another major approach is to conduct \textit{one-shot pruning} and then fine-tune the pruned network for its original performance to be recovered \cite{EagleEye,CURL,Importance,NISP,DCP}. In most one-shot pruning techniques, the fine-tuning phase does not take longer than a few dozens of epochs probably because there is no big difference in accuracy among pruning criteria beyond that point. In fact, it is reported that even a randomly pruned network can reach comparable performance after fine-tuning of hundreds or thousands of epochs \cite{Rethinking}. Thus, if we do not have any constraints on time, computing resources, or training data for a long fine-tuning process, one-shot pruning methods would be less meaningful.

% 4. Therefore, they obviously need a quiet complete set of training data
%    (mostly exactly the same as the original train set) or need to collect as 
%    many samples as the origin
% 5. However, in practice, the train data is neither available nor easy to collect
% 6. Therefore, a few recent works have tried to address above with using much small data ( still need data, train process anyway)
% 7. Meanwhile, a few works commonly employ the neuron similarity in order to 
%    reduce the bad effect of pruning Thus, a similar neuron can be replaced
%    limitations of NM, 
%    1) it is based on strong naive assumption, which is not the case in NNs.
%    2) a single neuron can't compensate due to the error term we found yet not discussed

\begin{figure*}[t!]
   \centering 
    \includegraphics[width=1.9\columnwidth]{./figure/LBYL_figures.pdf}
   \caption{The conceptual overview of our LBYL method, showing how the original output resulting from a pruned filter at $\ell$-th layer, that is, the output of $(\ell+1)$-th convolutional layer, can be recovered by all the other preserved filters at the same layer (\textit{i.e.}, $\ell$-th layer) through convolution, batch normalization, and activation function (\textit{i.e.}, ReLU), where $s, s',$ and $s^*$ are the coefficients that quantify how much each preserved filter should carry the information of the pruned filter.}
   \label{fig:framework}
\end{figure*}


Somewhat fortunately, however, this is not the case in many practical scenarios where we cannot access or re-collect the original data due to its large volume or some commercial issues \cite{DBLP:conf/eccv/MahajanGRHPLBM18}. Even though some recent works \cite{CURL,Reborn} attempt to use only a small amount of training data instead, but they still suffer from a time-consuming process of fine-tuning to achieve satisfactory performance. Furthermore, it would not be always possible to collect samples in the same domain as the original data, regardless of whether they are labeled or not. Only quite a few works \cite{NM,Data-free} have been introduced to recover pruned networks without using any data and fine-tuning process. Their common approach is what we call \textit{one-to-one compensation}, where the most similar unpruned neuron takes the role of a pruned neuron. This approach is based on the strong assumption that there exists such a neuron quite similar to each one being pruned at the same layer. Not surprisingly, this assumption cannot be guaranteed in deep neural networks, which is even reported in the result of \cite{NM} showing that the pairwise similarity on filters gets extremely low particularly in deeper layers. In addition, according to our theoretical analysis on the reconstruction error, a single neuron can only lead to a very rough approximation to the original output even if the neuron is similar enough to its pruned counterpart.



% However, they obviously still require a quiet complete set of train data to recover a original network performance because as presented in \cite{DBLP:conf/aaai/TangYXH0SXZ20}, compressed network hard to reach original performance with a few data. Moreover, in many practice application, the train data is neither accessible nor easy to collect due to commercial profits \cite{DBLP:conf/eccv/MahajanGRHPLBM18}. There are two trials to fine-tune the pruned model with small data using knowledge distilation \cite{Knowledge_Distilation} and extra trainable parameters. \cite{CURL} propose fine-tuning method based on knowledge distillation using a few data. More specifically, they employ soft target of the pre-trained model regarding the small data to fine-tuning the pruned model. \cite{Reborn} appends the additional convolution modules due to hardness to fine-tune using the small data and then retrain the extra modules with existing modules fixed. after fine-tuning on the supplementary modules with sparsity, the compressed model combines with additional convolution module and original modules to recover the original performance. However, they obviously exploit the small data to restore performance of original network.


% A few works \cite{NM, Data-free} commonly employ the neuron similarity in order to reduce catastrophic loss induced by removed filters without using any data and any fine-tuning process. They have a strong assumption that a filter can be replaced with a most similar filter but these assumption in neural networks have the limitations. It depends on that pruned filters have a single similar filter in neural networks. As presented in \cite{NM}, if the layer in networks become more deeper, the correlation between the filters in layer gradually decrease. Thus, preserved filter which is a most similar with removed filter isn't actually similar with pruned filter. That is, it turn out that a single similar filter can't significantly compensate. Moreover, they have ignored the error term we found yet not discussed in \cite{NM}.



% As shown above the pruned model, if filters in layer be pruned, the corresponding channels of filter in $i$+1-th layer are removed, which leads to loss of the feature maps in $i$+1-th layer. However, LBYL restores catastrophic loss pruned from filters in $i$-th layer by compensating the channels of filter $i$+1-th with a our assumption that compensation of pruned filter in $i$-th layer can be made with relationship between the pruned filter and preserved filters in $i$-th layer. The overall recovery process is as follow: first, we use a assumption that pruned filter in i-th layer is represented by multiple preserved filters on the same layer. Secondly, we can obtain the approximation scale to compensate pruned filter by extending the our assumption to normalization, activation layer, and convolution outputs (\textit{e.g.} feature maps in i+1 layer). Thirdly, before removing the channels of filters in $i$+1-th resulted from the pruned filter in $i$-th layer, they would be multipled by approximation scale and then add their weights to preserved filter in i+1 layer as shown in the last part above. Finally, we significantly recover the information loss induced by pruned filters in $i$-th layer


In this article, we propose a theoretically more rigorous and robust method, called \textit{Leave Before You Leave} (\textit{LBYL}), to effectively compensate for pruned filters, which is also free of any data and fine-tuning. As depicted in Figure \ref{fig:framework}, we significantly relax the strong assumption by observing the fact that a pruned filter is better to be represented by a linear combination of as many preserved filters as possible than it is done by a single filter. We first mathematically analyze how the reconstruction error can be formulated between the original network and its pruned network, and thereby discover three components of the error. Based on our formulation, we provide a closed form solution that can minimize the reconstruction error to compensate for information loss caused by pruned filters, which does not need any training process at all. Through extensive experiments, we show that LBYL outperforms the existing one-to-one compensation scheme \cite{NM} regardless of pruning criterion. For instance, LBYL achieves at pruning ratio 30\% on the average 17.92\% higher accuracy than those of \cite{NM} for ResNet-101 \cite{ResNet} trained with ImageNet \cite{deng2009imagenet}.



% LBYL achieves at pruning ratio 40\% on average 3.53\% higher accuracy than \cite{NM} for ResNet-50 \cite{ResNet} on CIFAR-100 \cite{krizhevsky2009learning} and also in case of large dataset (i.e., ImageNet \cite{deng2009imagenet}), .



% using CIFAR \cite{krizhevsky2009learning} and ImageNet \cite{deng2009imagenet}






