\section{Related Works}
\label{sec:related}
Many filter pruning methods have been studied without considering such a case where we cannot use any data, whether real or synthetic, to restore a pruned network. They mostly go through an expensive retraining process either iteratively \cite{Soft,FPGM,Dynamic,ZhangF24} or lastly as a fine-tuning phase \cite{GlobalRanking,Importance,NISP}. In this article, we focus on the opposite case, which is more challenging in practice, where fine-tuning with any data is not available.

\smalltitle{Pruning with less fine-tuning} 
Most of the existing filter pruning methods have tried to avoid an expensive fine-tuning process by means of a carefully designed criterion for identifying unimportant filters such that the loss of information is minimized when they are pruned. To this end, they often make the best use of data-dependent values like channels and gradients that can be obtained by making inferences with some data. \cite{Thinet} proposes a greedy algorithm that prunes the filters whose corresponding channels minimize the layer-wise reconstruction error. Similarly, \cite{Lasso} formulates this problem of channel selection as lasso regression and least square reconstruction. \revfour{\cite{URC} globally chooses the redundant filters in the pre-trained model by utilizing feature-discrimination-based filter importance. \cite{catro} suggests uninformative channel selection method via class-aware trace ratio optimization, which measures joint impact across channels in a convolutional layer.} \cite{CoreSet_ICLR} introduces data-independent pruning criterion based on coreset together with an intermediate recovery method without training in order to reduce the overhead of fine-tuning. Despite their proposed methods on effective channel or neuron selection, a few epochs of fine-tuning process as well as some training data is inevitable for the pruned network to be sufficiently recovered. \revfour{To speedup filter pruning, \cite{LeeS24} utilizes a finetuning structure based on constrastive knowledge transfer, thereby proposing a coarse-to-fine neural architecture search algorithm.}


\smalltitle{Pruning with less fine-tuning and less data}
For the case of pruning with limited data, several methods \cite{CURL,Reborn} utilize knowledge distillation \cite{Knowledge_Distilation} not to use the entire original data in the recovery process. \cite{CURL} recently proposes \textit{CURL} that globally prunes filters using a KL-divergence based criterion and perform knowledge distillation with a small dataset as a fine-tuning process. \cite{Reborn} tackles the similar problem and devise a way of transforming all the filters in a layer into compact ones, called \textit{reborn filters}, using only the limited data. These methods do not use the entire original data, but they still require some kinds of retraining process with a small amount of data. 

\smalltitle{Pruning with no fine-tuning and no data} In the literature of filter pruning, there are only a few works \cite{NM, Data-free,RedPlus} that adopt the same problem setting as ours, that is, pruning filters without any fine-tuning and data. \rev{It is worth mentioning that our work focuses on this \textit{pure} recovery process with respect to pruned networks without any support of extra memory and hardware, as in \cite{NM,Data-free}.} Both methods are based on the strong assumption that we can always find a pair of filters that are quite similar to each other and therefore a pruned filter can be compensated by its similar unpruned one. However, this assumption does not hold in modern convolutional neural networks (CNNs) having a number of filters in a fairly deep architecture. As even presented in \cite{NM}, the cosine similarity between each filter and its nearest one gets extremely low (\textit{e.g.}, around 0.2) in back layers to the point that it is better to do nothing for some pruned neurons that do not have sufficiently similar preserved neurons. In this article, we remedy this problem by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works. \rev{Meanwhile, the RED++ method \cite{RedPlus} more concentrates on how to make the best use of extra memory buffer (i.e., RAM) other than GPU in order to reduce the number of GPU computations. Therefore, it requires high-speed memory access with a specific hardware support, which makes orthogonal to our method.}




% \lee{It is worth mentioning that \cite{NM, Data-free} methods concentrate on recovery to performance drop after pruning without any soft/hardware supports in same way as we do. However, \cite{Red++} concentrates on how to compress network without performance drop. That is, \cite{Red++} needs special soft/hardware supports to speed up memory access and allocation than computation to accelerate network inference.
% } \cite{NM, Data-free}

% Both methods are based on the strong assumption that we can always find a pair of filters that are quite similar to each other and therefore a pruned filter can be compensated by its similar unpruned one. However, this assumption does not hold in modern convolutional neural networks (CNNs) having a number of filters in a fairly deep architecture. As even presented in \cite{NM}, the cosine similarity between each filter and its nearest one gets extremely low (\textit{e.g.}, around 0.2) in back layers to the point that it is better to do nothing for some pruned neurons that do not have sufficiently similar preserved neurons. In this article, we remedy this problem by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works.


\smalltitle{Data-free compression with heavy retraining}
\rev{
There is a large branch of data-free compression methods \cite{KEGNET,DAFL019,DFAD_BEFORE,DFAD,DeepInversion,Data-Free-NetworkPruning} that still fine-tune or retrain the compressed model with synthetically generated data but not real data. Thus, in some sense, they are not literally data-free, but free of original data. Most of these methods involves extracting a generator that can generate synthetic samples, which itself independently takes a considerable amount of training time, and rely on knowledge distillation, which thus makes them often called \textit{data-free knowledge distillation}. As in \cite{Data-Free-NetworkPruning,DeepInversion}, some of the works employ pruned networks as a student model, instead of a lightweight network randomly initialized, but still focus on retraining process with synthetic data. Note that these data-free methods still suffer from heavy recovery process for fine-tuning (or distillation) as well as training the generator, which is not our focus in this article.
}






% \lee{ \cite{RedPlus} focuses on reducing the number of computations by changing identical operations to identical memory access to a unique operation. That is, \cite{RedPlus} needs the hypothesis that memory access and allocation should run faster than mathematical operations, while our LBYL method do not need a specific hardware assumption. Therefore \cite{RedPlus} is orthogonal to our method. In this article, we remedy problem of similarity based compensation \cite{NM, Data-free} by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works.}


\smalltitle{Clarification regarding overlapping work}
We have become aware of two recently published papers—one in ICCV 2023 titled ``\textit{Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning}” and another in Pattern Recognition titled ``\textit{Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning}.” Both appear to share core derivations and theoretical components with an earlier version of our submission, which was not publicly accessible at the time those papers were written. We have formally reported our concerns about these similarities to the relevant committees. As we await a resolution, we are refraining from listing these papers in our references to avoid lending legitimacy to them while the plagiarism dispute remains unsettled.