\section{Proposed Method of Restoring Pruned Filters} \label{sec:method}
In this section, we first theoretically examine how the reconstruction error of Eq. (\ref{eq:goal}) can be formulated in a way free of using data-dependent parameters. Based on this analysis, we present our train-free recovery method that has a closed form solution for minimizing the formulated reconstruction loss.

%is composed of with respect to our proposed methodology using multiple filters to approximate a pruned filter, so-called \textit{many-to-one}. Our approach is in fact a generalized version of the existing one-to-one compensation methods \cite{NM,Data-free}. Finally, we present our train-free recovery method that has a closed form solution for minimizing the formulated reconstruction error.

\subsection{Data-Independent Reconstruction Loss} \label{sec:method:a}
For brevity, we deal with a simple case of pruning only a single filter, which can trivially be extended to cover multiple filters. Without loss of generality, let the $j$-th filter of the $\ell$-th layer be pruned. \revfour{Thus, here after, $j$ is a fixed value that represents a particular filter being pruned.}

\smalltitle{Leave Before You Leave (LBYL) assumption}
In order to transform Eq. (\ref{eq:goal}) to a data-free version, our proposed assumption is that each filter being pruned can \textit{leave} its pieces of information for the remaining ones by adding its corresponding pruned channel to each of the other channels in the next layer. Since the quantity of information depends on how much the pruned filter is related to a particular remaining filter, we need to multiply the pruned channel with a different value before addition. Based on this assumption, \revfour{for the $j$-th filter being pruned}, the delivery matrix $\boldsymbol{\S^*} \in \mathbb{R}^{m \times (m-1)}$ is defined to be in the following form:
\begin{eqnarray}\begin{split}\label{eq:matrix}
\boldsymbol{\S^*}_{i,k} = 
  \begin{cases} 
   1 & \text{if } i = i'_k \\
   s_{i'_k} & \text{if } i = j \\
   0 & \text{otherwise}
  \end{cases}
  \end{split}
\end{eqnarray}
 $\text{s.t. } i, i'_k \in [1, m] \text{ and } k \in [1, m-1],$ where $s_{i'_k}$ is a scalar value that quantifies to what extent the $i'_k$-th filter is related to the $j$-th filter being pruned. Figure \ref{fig:matrix:b} shows an example of the delivery matrix for our LBYL method.


\smalltitle{Without BN and an activation function}
Starting with Eq. (\ref{eq:goal}), we first derive its data-free version without considering both a BN procedure and an activation function, and then make an extension for them. With the proposed form of the delivery matrix in Eq. (\ref{eq:matrix}) together with Eq. (\ref{eq:fmap_approx}), the $i$-th channel of $\mathbf{\hat{Z}}^{{(\ell+1)}}$ is now represented as:
\begin{equation}
{{\mathbf{\hat{Z}}}_{i}^{{(\ell+1)}}} = \sum\limits_{k = 1, k \neq j}^{m} \mathbf{A}_{k}^{(\ell)} \circledast (\mathbf{W}_{i,k}^{(\ell+1)}+{s_{k}} \mathbf{W}_{i,j}^{(\ell+1)}).
\nonumber
\end{equation}
Then, the reconstruction error for the $i$-th channel of the feature map at the $(\ell+1)$-th layer can be derived as:
\begin{equation}
{\mathbf{Z}_{i}^{{(\ell+1)}}}-{\mathbf{\hat{Z}}_{i}^{{(\ell+1)}}}=(\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)})\circledast \mathbf{W}_{i,j}^{(\ell+1)}.
\label{eq:eq5}
\end{equation}
By Eq. (\ref{eq:eq5}), we have the following form of the reconstruction error for the case of pruning the $j$-th filter at the $\ell$-th layer:
\begin{eqnarray}\begin{split}
\sum\limits_{{i} = 1}^{m^{\prime}}\|{\mathbf{Z}_{i}^{{(\ell+1)}}-\mathbf{\hat{Z}}_{i}^{{(\ell+1)}}}\|_1 ~~~~~~~~~~~~~~~~~\\ 
=\sum\limits_{{i} =  1}^{m^{\prime}}\|(\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)})\circledast\mathbf{W}_{i,j}^{(\ell+1)}\|_1.
\end{split}
\label{eq:eq6}
\end{eqnarray}
Based on Eq. (\ref{eq:eq6}), we can reduce the problem of minimizing Eq. (\ref{eq:goal}) into the one of minimizing the following error term:
\begin{equation} \label{eq:goal2}
\|(\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)})\|_1,
\end{equation}
where we need to find scalars $s_{k}$'s such that ${ \mathbf{A}_{j}^{(\ell)} \approx \sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}}$. This is because the term $\mathbf{W}_{i,j}^{(\ell+1)}$ in Eq. (\ref{eq:eq6}) is a constant with respect to a given pretrained model. Therefore, minimizing Eq. (\ref{eq:goal2}) makes the same effect as minimizing Eq. (\ref{eq:eq6}). \revfour{More specifically, this is based on the fact that the convolution operation $\mathbf{Z} = \mathbf{A} \circledast \mathbf{W}$ is linear with respect to both participating terms $\mathbf{A}$ and $\mathbf{W}$. Therefore, if $\mathbf{W}$ is constant, reducing the values in $\mathbf{A}$ will directly reduce the output values $\mathbf{Z}$ resulting from the convolution operation, as $\mathbf{Z}$ is directly proportional to $\mathbf{A}$ for fixed $\mathbf{W}$.} Without considering BN and any activation function, we simply have $\mathbf{A}^{(\ell)} = \mathbf{Z}^{(\ell)}$, and consequently Eq. (\ref{eq:goal2}) can be represented as:
\begin{gather*}
    %  \|\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}\|_{1} \\
    % = \| (\mathbf{A}^{(\ell-1)}  \circledast \mathbf{W}_{j}^{(\ell)} ) - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times (\mathbf{A}^{(\ell-1)}  \circledast \mathbf{W}_{k}^{(\ell)} ) \|_{1} \nonumber \\
    % = 
     \| (\mathbf{A}^{(\ell-1)}  \circledast (\mathbf{W}_{j}^{(\ell)} - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \mathbf{W}_{k}^{(\ell)} ) \|_{1}.
\end{gather*}
Since $\mathbf{A}^{(\ell-1)}$ is data-dependent yet independent to pruned filters, it can be regarded as a constant factor of the reconstruction error that depends on which filters are pruned. Thus, it suffices to minimize the other part of the term, which we call \textit{Residual Error} (\textit{RE}) denoted by $\boldsymbol{\E}$ as the following definition:
\begin{definition} \label{def:re}
Given a pruned filter $\mathbf{W}_{j}^{(\ell)}$ at the $\ell$-th layer, the Residual Error (RE) is defined as:
$$
\boldsymbol{\E} = \mathbf{W}_{j}^{(\ell)} - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \mathbf{W}_{k}^{(\ell)}.
$$
\end{definition}
Note that $\boldsymbol{\E}$ can be minimized by using only the original network without any support from data.


% Since we cannot access $\mathbf{A}^{(\ell-1)}$ without data, the only thing we can do is minimizing the other part of the term. We call this error term \textit{Residual Error} (\textit{RE}) and denote it by $\boldsymbol{\E}$

\smalltitle{Considering batch normalization} 
We now incorporate a BN procedure in the process of minimizing Eq. (\ref{eq:goal2}). In this case, as we have $\mathbf{A}^{(\ell)} = \N(\mathbf{Z}^{(\ell)})$, Eq. (\ref{eq:goal2}) can be derived as Lemma \ref{lem:bn}.
\begin{lemma} \label{lem:bn}
If there is only batch normalization between a feature map and its activation map, the reconstruction error can be formulated as follows:
\begin{equation} \label{eq:bn}
\| \frac{\gamma_{j}}{\sigma_{j}}{(\mathbf{A}^{(\ell-1)} \circledast \boldsymbol{\E})} + \boldsymbol{\B}\|_{1},
\end{equation}
% \begin{equation} \label{eq:bn}
% \| \frac{\gamma_{j}}{\sigma_{j}}{(\mathbf{A}^{(\ell-1)} \circledast \boldsymbol{\E})} +
% \frac{\gamma_{j}}{\sigma_{j}} 
% \{\sum\limits_{k = 1, k \neq j }^{m} s_{k}\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{k}}{\sigma_{k}} (\mu_{k} -  \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j + \frac{\sigma_{j}}{\gamma_{j}}\beta_j\} \|_{1},
% \end{equation}
where $\boldsymbol{\B} = \frac{\gamma_{j}}{\sigma_{j}} 
 \{\sum\limits_{k = 1, k \neq j }^{m} s_{k}\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{k}}{\sigma_{k}} (\mu_{k} -  \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j + \frac{\sigma_{j}}{\gamma_{j}}\beta_j\}$, $\mu$, $\gamma$, $\sigma$, and $\beta$ are batch normalization parameters.
\end{lemma}
\begin{proof}
See Appendix.
\end{proof}
Hereafter, the notation $\boldsymbol{\E}$ is a bit differently defined such that $s_k$ from Definition \ref{def:re} should be substituted with $s_k \frac{\sigma_j}{\gamma_j} \frac{\gamma_k}{\sigma_k}$ due to the BN procedure. We call the second term $\boldsymbol{\B}$ of Eq. (\ref{eq:bn}) Batch Normalization Error (BE).

\begin{definition}
Given a pruned filter $\mathbf{W}_{j}^{(\ell)}$ at the $\ell$-th layer, the Batch Normalization Error (BE) is defined as:
$$
\boldsymbol{\B} = \frac{\gamma_{j}}{\sigma_{j}} 
\{\sum\limits_{k = 1, k \neq j }^{m} s_{k}\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{k}}{\sigma_{k}} (\mu_{k} -  \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j  + \frac{\sigma_{j}}{\gamma_{j}}\beta_j \}.
$$
\end{definition}
\revsec{It is noteworthy that BN parameters are obtained through the pretraining process just like the other learned weights, and stored in the pretrained model to be used as a linear transform layer at inference time.}\footnote{https://en.wikipedia.org/wiki/Batch\_normalization}



% below Eq. (\ref{eq:eq9}).
% \begin{eqnarray} \begin{split}\label{eq:eq9}
%      \|\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}\|_{1} = &~\| \N(\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{j}^{(l)} ) - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \N (\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{k}^{(l)} ) \|_{1}
% \end{split}
% \end{eqnarray}
% Eq. (\ref{eq:eq9}) is further derived and finally we can conclude to the following lemma:


\smalltitle{Considering activation function}
Lastly, we proceed our analysis to cover a general CNN architecture including both BN and the ReLU activation function. For an activation function $\F$, we now have $\mathbf{A}^{(\ell)} = \F(\N(\mathbf{Z}^{(\ell)}))$, and thereby Eq. (\ref{eq:goal2}) is derived to be the following theorem:
\begin{theorem} \label{thm:relu}
If there are both batch normalization and a ReLU function between a feature map and its activation map, the reconstruction error can be formulated as follows:
    \begin{equation} \label{eq:relu}
     \| \frac{\gamma_{j}}{\sigma_{j}}{(\mathbf{A}^{(\ell-1)}\circledast\boldsymbol{\E})}+ \boldsymbol{\B} + \boldsymbol{\R}\|_{1},
     \end{equation}
     where $\boldsymbol{\R} = \sum\limits_{k=1, k \neq j}^{m} s_{k}  \min(0,\N(\mathbf{Z}_{k}^{(\ell)}))-\min(0,\N(\mathbf{Z}_{j}^{(\ell)}))$ represents the \textit{Activation Error} (\textit{AE}).
\end{theorem}
\begin{proof}
See Appendix.
\end{proof}





\smalltitle{Data-free loss function}
\rev{As concluded in Theorem \ref{thm:relu}, we discover the fact that the final reconstruction error of Eq. (\ref{eq:goal}) consists of three different components, namely RE, BE, and AE.} \revsec{Since it is not feasible to find a training-free solution for minimizing Eq. (\ref{eq:relu}) itself, we focus on the fact that Eq. (\ref{eq:relu}) is smaller than equal to:
\begin{eqnarray}
         \| \frac{\gamma_{j}}{\sigma_{j}}{(\mathbf{A}^{(\ell-1)}\circledast\boldsymbol{\E})}\|_{1} + \|\boldsymbol{\B}\|_{1} + \|\boldsymbol{\R}\|_{1}. \nonumber
\end{eqnarray}
This further brings us to formulate the following reconstruction loss that does not require any training data and nicely has a closed form solution as derived in Theorem \ref{thm:closedform}:
\begin{equation}
     \mathcal{L}_{re} =  \|\boldsymbol{\E}\|_2^{2} + \lambda_1 \|\boldsymbol{\B}\|_2^{2} + \lambda_2 \|\mathbf{s}\|_{2}^{2},
\label{eq:loss}
\end{equation}
where $\mathbf{s} =  [s_{1}~...~s_{j-1}~s_{j+1}~...~s_{m}]$.
}

Unlike RE \revsec{(\textit{i.e.,} $\boldsymbol{\E}$)} and BE \revsec{(\textit{i.e.,} $\boldsymbol{\B}$)}, we cannot exactly control the AE term \revsec{(\textit{i.e.,} $\boldsymbol{\R}$)} as $\mathbf{Z}^{(\ell)}$ can only be obtained by using the actual data. Therefore, we instead introduce a regularization term $\|\mathbf{s}\|_{2}^{2}$ for the purpose of mitigating the explosion of $\boldsymbol{\R}$, \revsec{inspired by the fact that the upper bound of $\|\boldsymbol{\R}\|_{1}$ is proportional to $\sum_{k=1,k\neq j}^{m}\|s_k\|_{1}$, as derived in the following lemma:
\begin{lemma} \label{lem:ae}
    \begin{equation} \label{eq:ae}
     \|\boldsymbol{\R}\|_{1} \leq \sum_{k=1,k\neq j}^{m}\|s_k\|_{1} \cdot \|\N(\mathbf{Z}_{k}^{(\ell)})\|_{1} + c,
     \end{equation}
     where $c \geq 0$ is a constant with respect to $\mathbf{s}$.
\end{lemma}
\begin{proof}
See Appendix.
\end{proof}}
Furthermore, since our intention is to participate as many preserved filters as possible in restoring a pruned filter, it would be better for the $s_k$ values to be more uniformly distributed. Both purposes can be fulfilled by this L2-based regularization term. 

\begin{algorithm}[t]
  \small
  \caption{LBYL method}
  \label{alg:Recovery}
  \KwIn{$\mathbf{\Theta}$ = $[\mathbf{W}^{{(1)}}$, .... ,$\mathbf{W}^{{(L)}}] := $ Original model}
  \KwOut{$\hat{\mathbf{\Theta}}$ = $[{\mathbf{\hat{W}}}^{{(1)}}$, .... ,$\mathbf{\hat{W}}^{(L)}] : = $ Pruned \& restored model}
   
  \For{each layer $\ell \in [1, L]$ that has been pruned}%$\in$ $\mathbb{R}^{n_{l} \times k \times k}$   $\mathbf{in}$ $\mathbf{W}^{(\ell)}$}
    {
    ${\boldsymbol{\S}}^{(\ell)} \leftarrow $ a zero-valued matrix
    
    \For{each filter $\mathbf{W}_i^{(\ell)}$ s.t. $i$ $\in$ $[1,m]$}{
        \If{$\mathbf{W}_i^{(\ell)}$ is not pruned} 
            {
            $\boldsymbol{\S}_{i,k}^{(\ell)} = 
              \begin{cases} 
              1  & \text{if  } i = i'_k \\
              0  & \text{otherwise } \\
              \end{cases}
             $
            }
        \Else{        
        $\boldsymbol{\S}_{i,:}^{(\ell)} \leftarrow {\argmin\limits_{\mathbf{s}}~\|\boldsymbol{\E}\|_2^{2} + \lambda_1 \|\boldsymbol{\B}\|_2^{2} + \lambda_2 \|\mathbf{s}\|_{2}^{2}}$  \tcp{obtained by Theorem \ref{thm:closedform}} 
        }
      }
    $\hat{\mathbf{W}}^{(\ell+1)} = \mathbf{W}^{(\ell+1)} \times_2 \boldsymbol{\S}^{(\ell)^T} $
    }
\Return $\hat{\mathbf{\Theta}}$
\end{algorithm}




\smalltitle{One-to-one vs. LBYL}
In one-to-one compensation approaches \cite{NM,Data-free}, they deliver the missing information for a pruned filter only to the most similar one of the remaining filters as shown in Figure \ref{fig:matrix:c}. Thus, this assumption is identical to the case that we have only one $s_{k}$ in Eq. (\ref{eq:goal2}), which would return a quite rough solution for the problem of minimizing the reconstruction error. Indeed, this one-to-one approach particularly fails to reduce the RE term as shown in Figure \ref{fig:error_components} of our experiments.

\begin{figure}[t!]
   \centering 
    \includegraphics[width=0.6\columnwidth]{./figure/appendix_figure.pdf}
   \caption{A neuron pruning scenario in fully-connected layers}
   \label{fig:appendix_figure}
\end{figure}


% \begin{table*}[t] % 
% \centering
% \small
% \begin{tabular}{c||ccc||ccc||ccc||c}\hline Criterion& \multicolumn{3}{c||}{L2-norm}& \multicolumn{3}{c||}{L2-GM}& \multicolumn{3}{c||}{L1-norm}& \multirow{2}{*}{Coreset} \\ \cline{1-10}
% Pruning Ratio& \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune & \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune & \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune &\\ \hline \Xhline{2\arrayrulewidth}
% 50 \%& \multicolumn{1}{c|}{\textbf{88.83}} & \multicolumn{1}{c|}{87.86}          & 87.86 & \multicolumn{1}{c|}{\textbf{88.69}}  & \multicolumn{1}{c|}{88.57} & 88.08 & \multicolumn{1}{c|}{\textbf{89.03}} & \multicolumn{1}{c|}{88.69} & 88.40 & 79.34\\ \hline
% 60 \%& \multicolumn{1}{c|}{87.75}& \multicolumn{1}{c|}{\textbf{88.07}} & 83.03 & \multicolumn{1}{c|}{\textbf{88.15}}  & \multicolumn{1}{c|}{88.10} & 85.82 & \multicolumn{1}{c|}{\textbf{87.55}} & \multicolumn{1}{c|}{86.92} & 85.17 & 69.41\\ \hline
% 70 \%& \multicolumn{1}{c|}{\textbf{83.92}} & \multicolumn{1}{c|}{83.27}& 71.21 & \multicolumn{1}{c|}{85.92}& \multicolumn{1}{c|}{86.39} & 78.38 & \multicolumn{1}{c|}{\textbf{84.57}} & \multicolumn{1}{c|}{82.75} & 71.26 & 62.31\\ \hline
% 80 \%& \multicolumn{1}{c|}{\textbf{78.05}} & \multicolumn{1}{c|}{77.11}& 63.90 & \multicolumn{1}{c|}{\textbf{77.63}} & \multicolumn{1}{c|}{77.49} & 64.19 & \multicolumn{1}{c|}{\textbf{80.55}} & \multicolumn{1}{c|}{80.02} & 66.76 & 49.68 \\ \Xhline{2\arrayrulewidth}
% \end{tabular}
% \vspace{2mm}
% \caption{Recovery results of LeNet-300-100 on FashionMNIST }
% \label{tab:LeNet_FashionMNIST}
% \end{tabular}

\begin{table*}[t] % 
\centering
{\small 
\begin{tabular}{c||c|c|c||c|c|c||c|c|c||c|c|c}  \Xhline{2\arrayrulewidth}
\multicolumn{13}{c}{\textbf{VGG-16 (Acc. 93.7)}}
\\ \Xhline{2\arrayrulewidth} %\hline
Criterion & \multicolumn{3}{c||}{L2-norm} & \multicolumn{3}{c||}{L2-GM} & \multicolumn{3}{c||}{L1-norm}& \multicolumn{3}{c}{Random}\\ \hline
Pruning Ratio& Ours& NM& Prune& Ours& NM& Prune& Ours& NM& Prune& Ours& NM& Prune\\ \Xhline{2\arrayrulewidth}
10\%& \textbf{92.04} & 91.93 & 89.43 & \textbf{92.15} & 91.82 & 88.83 & 91.85 & \textbf{91.97} & 89.85 & \textbf{88.05} & 86.52 & 74.87 \\\hline
20\%& \textbf{87.84} & 87.24 & 71.77 & \textbf{87.97} & 86.42 & 72.04 & \textbf{89.51} & 86.93 & 73.64 & \textbf{75.24} & 63.33 & 37.19 \\ \hline
30\%& \textbf{83.25} & 76.91 & 56.95 & \textbf{82.83} & 75.58 & 52.40 & \textbf{81.80} & 75.01 & 49.79 & 45.67 & \textbf{46.86} & 12.81 \\ \hline
40\%& \textbf{66.81} & 54.32 & 31.74 & \textbf{67.19} & 52.24 & 30.46 & \textbf{56.63} & 52.54 & 17.78 & \textbf{31.34} & 26.79 & 10.00 \\ \hline
50\%& \textbf{45.71} & 32.58 & 12.37 & \textbf{49.46} & 32.13 & 11.71 & \textbf{43.32} & 27.87 & 10.43 & \textbf{21.56} & 18.08 & 10.00 \\ \Xhline{2\arrayrulewidth}
% Average Acc. & \textbf{75.13} & 68.60 & 52.45 & \textbf{75.92} & 67.64 & 51.09 & \textbf{72.62} & 66.86 & 48.30 & \textbf{52.37} & 48.32 & 28.97\\ \Xhline{2\arrayrulewidth}   
\end{tabular}
}
\vspace{2mm}
\caption{Recovery results of VGG-16 on CIFAR-10}
\label{tab:VGG16-CIFAR10}
\vspace{1.5mm}

\centering 
{\small
\begin{tabular}{c||c|c|c||c|c|c||c|c|c||c|c|c}  \Xhline{2\arrayrulewidth}
\multicolumn{13}{c}{\textbf{ResNet-50 (Acc. 78.82) }}
\\ \Xhline{2\arrayrulewidth} %\hline
Criterion & \multicolumn{3}{c||}{L2-norm} & \multicolumn{3}{c||}{L2-GM} & \multicolumn{3}{c||}{L1-norm}& \multicolumn{3}{c}{Random}\\ \hline
Pruning Ratio& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune
\\ \Xhline{2\arrayrulewidth}
10\%& \textbf{78.14} & 77.28 & 75.14 & \textbf{78.01} & 76.92 & 74.49 & \textbf{78.25} & 77.21 & 75.07 & \textbf{76.53} & 72.46 & 59.32 \\ \hline
20\%& \textbf{76.15} & 72.73 & 63.39 & \textbf{76.36} & 72.32 & 63.54 & \textbf{75.71} & 72.24 & 61.84 & \textbf{73.29} & 59.44 & 19.27 \\ \hline
30\%& \textbf{73.29} & 64.47 & 39.96 & \textbf{72.97} & 64.01 & 39.01 & \textbf{72.07} & 63.07 & 35.77 & \textbf{69.05} & 40.42 & 3.25  \\ \hline
40\%& \textbf{65.21} & 46.40 & 15.32 & \textbf{65.78} & 46.17 & 13.14 & \textbf{62.64} & 45.98 & 12.59 & \textbf{59.49} & 20.89 & 2.59  \\ \hline
50\%& \textbf{52.61} & 25.98 & 5.22  & \textbf{54.04} & 23.44 & 4.32  & \textbf{49.07} & 21.98 & 4.25  & \textbf{43.51} & 8.77  & 2.77  \\ \Xhline{2\arrayrulewidth}
Average Acc. & \textbf{69.08} & 57.37 & 39.81 & \textbf{69.43} & 56.57 & 38.90 & \textbf{67.55} & 56.10 & 37.90 & \textbf{64.37} & 40.40 & 17.44\\ \Xhline{2\arrayrulewidth}   
\end{tabular}%
}
\vspace{2mm}
\caption{Recovery results of ResNet-50 on CIFAR-100}
\label{tab:ResNet50-CIFAR100}

\end{table*}


\subsection{Our Train-Free Recovery Method with a Closed Form Solution}
Based on our data-free loss function in Eq. (\ref{eq:loss}), we now present our proposed train-free recovery method, called LBYL. First of all, we theoretically prove that the problem of minimizing $\mathcal{L}_{re}$ of Eq. (\ref{eq:loss}) has a closed form solution at the following theorem.
\begin{theorem} \label{thm:closedform}
The loss function $\mathcal{L}_{re}$ of Eq. (\ref{eq:loss}) is convex with a unique optimal solution as follows:
\begin{equation}
% \small
    \boldsymbol{s}=[X^{T}X+\lambda_{1}\frac{\gamma_{j}^2}{\sigma_{j}^2} \boldsymbol{p}\boldsymbol{p}^{T}+\lambda_{2}I]^{-1}[X^{T}\boldsymbol{y}+\frac{\lambda_{1}\gamma_{j}}{\sigma_{j}}(\frac{\mu_{j}\gamma_{j}}{\sigma_{j}}-\beta_{j})\boldsymbol{p}],
    \nonumber
\end{equation}
where (1) $X = [\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{1}}{\sigma_{1}}\boldsymbol{f_{1}}~~...~~\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{j-1}}{\sigma_{j-1}}\boldsymbol{f_{j-1}}~\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{j+1}}{\sigma_{j+1}}\boldsymbol{f_{j+1}}~~...~~ \\
\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{m}}{\sigma_{m}}\boldsymbol{f_{m}}]$ such that $\boldsymbol{f_{i}}$ is the vectorized $i$-th filter in the $\ell$-th layer, (2) $\boldsymbol{y}$ is the vectorized $j$-th filter in the $\ell$-th layer, and (3) $\boldsymbol{p} =  [p_{1}~...~p_{j-1}~p_{j+1}~...~p_{m}]^T$ such that $p_{i} ={\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{i}}{\sigma_{i}}}{(\mu_{i}-\frac{\sigma_{i}}{\gamma_{i}}\beta_{i})}$.
% where (1) $X = [~...~\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{j-1}}{\sigma_{j-1}}\boldsymbol{f_{j-1}}~\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{j+1}}{\sigma_{j+1}}\boldsymbol{f_{j+1}}~...~\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{m}}{\sigma_{m}}\boldsymbol{f_{m}}]$ such that $\boldsymbol{f_{i}}$ is the vectorized $i$-th filter in the $\ell$-th layer, (2) $\boldsymbol{y}$ is the vectorized $j$-th filter in the $\ell$-th layer, and (3) $\boldsymbol{p} =  [p_{1}~...~p_{j-1}~p_{j+1}~...~p_{m}]^T$ such that $p_{i} ={\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{i}}{\sigma_{i}}}{(\mu_{i}-\frac{\sigma_{i}}{\gamma_{i}}\beta_{i})}$.
\end{theorem}
\begin{proof}
See Appendix.
\end{proof}

Finally, given $\lambda_{1}$ and $\lambda_{2}$, Algorithm \ref{alg:Recovery} presents the whole procedure of our LBYL method using the closed form solution in Theorem \ref{thm:closedform}.


\smalltitle{Extension of LBYL to neuron pruning}
Even though this article focuses on filter pruning, our LBYL method can also be applied to prune and restore neurons in vanilla feed-forward neural networks consisting of only FC layers such as LeNet-300-100 \cite{LeNet}. Let $\alpha_{i}^{\ell}$ denote the output of the $i$-th neuron in the $\ell$-th layer, and let $W^{\ell}_{j,k}$ denote a weight between the $j$-th neuron in the $\ell$-th layer and the $k$-th neuron in the $(\ell+1)$-th layer as illustrated in Figure \ref{fig:appendix_figure}. The goal of our method is still minimizing the reconstruction error between a restored network and its original network by making a linear combination of remained neurons to compensate each pruned neuron. If the $j$-th neuron in the $\ell$-th layer is pruned, the reconstruction error can be formulated as: 
\begin{equation}
\sum\limits_{n = 1}^{k}\|{{\mathbf{\alpha}}_{n}^{{\ell+1}}-{\hat{\mathbf{\alpha}}}_{n}^{{\ell+1}}}\|_1 
\nonumber
\end{equation}
As explained in Section \ref{sec:probdef}, the reconstruction error for the $k$-th neuron at the $(\ell+1)$-th layer can be derived as:
\begin{eqnarray}
 \|{\mathbf{\alpha}}_{k}^{{\ell+1}} - (\sum\limits_{n = 1, n \neq j}^{j}{ W^{\ell}_{n,k} * \mathbf{\alpha}_{n}^{\ell}} +{\sum\limits_{n = 1, n \neq j}^{j} s_nW^{\ell}_{j,k} *  \mathbf{\alpha}_{n}^{\ell}} ) \|_1 
 \\ 
 = \|{ W_{j,k}^{\ell} * (\mathbf{\alpha}_{j}^{(\ell)} - \sum\limits_{n = 1, n \neq j}^{j}{s_{n}} \mathbf{\alpha}_{n}^{(\ell)})}\|_1 ~~~~~~~~~~~
% = \|{ W^{\ell}_{j,n} * (\sum\limits_{n' = 1, n' \neq j}^{j}{s_{n'}}\mathbf{\alpha}_{n'}^{\ell})}\|_1 ,
\nonumber
\end{eqnarray}
where we need to find scalars $s_{n}$'s such that ${ \mathbf{\alpha}_{j}^{(\ell)} - \sum\limits_{n = 1, n \neq j}^{j}{s_{n}} \mathbf{\alpha}_{n}^{(\ell)}}$ to minimize the reconstruction error. Note that $\alpha$ is data dependent and computed through a multiplication procedure with weights followed by an activation function (e.g., ReLU) without batch normalization process in LeNet-300-100. Therefore, by the similar derivation of Section \ref{sec:method:a}, we have:
\begin{equation}
     \mathcal{L}_{re} =  \|\boldsymbol{\E}\|_2^{2} + \lambda \|\mathbf{s}\|_{2}^{2}, ~~\text{where}~\mathbf{s} =  [s_{1}~...~s_{j-1}].
    \nonumber
\end{equation}

% based on LBYL assumption in real time using the closed form solution in Theorem 1. Actually, this restoration algorithm can be extended to multiple pruned filters through just implementing above works iteratively. We represent the whole procedure in Algorithm \ref{alg:Recovery}.



% \smalltitle{Reasonable loss function}
% As shown in Lemma 2, there are three error components in (12), which are related with Sum of Squared Error(SSE) and Batch normalization Error(BE) and Activation Error(AE), respectively. Unfortunately, we cannot control data driven feature map $\mathbf{Z}^{(\ell-1)$ and matrix $R$. So we alternatively try to minimize the norm of two terms, $\boldsym{\epsilon}$ and $B$ respectively. Moreover, we place regularization term in loss function lastly for the purpose of mitigating explosion of matrix $R$. For example, one element of matrix $R$ maybe too large if some of the coefficients are positively large and elements corresponding to that coefficients are negative, but elements corresponding to others are positive. Finally, we formulate the reasonable restoration loss function as below Eq.(16).








% \begin{lemma} If there are both BN and ReLU between feature map and activation map, the reconstruction error can be formulated in a data-free manner as follows:
%     \begin{equation} 
%     \| \mathbf{A}_{j,:,:}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k,:,:}^{(\ell)}\|_{1}=
%      \| \frac{\gamma_{j}}{\sigma_{j}}{(\mathbf{Z}^{(\ell-1)}\circledast\boldsymbol{\epsilon})}+ B + R\|_{1},
%      \end{equation}\nonumber
%      \label{lemma}
% where $\mathbf{\epsilon}=\mathbf{W}_{j}^{(l)} - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \mathbf{W}_{k}^{(l)}$, $B$=$\gamma_{j} \over \sigma_{j}$$\{ \sum\limits_{i = 1, i \neq j }^{m} s_{i} (\mu_{i} -  \frac{\sigma_{i}}{\gamma_{i}}\beta_i) - \mu_j \} + \beta_j $, $R$ is w $\times$ h matrix where $ R_{u,v}$ = |$\sum\limits_{k=1, k \neq j}^{m} s_{k}min(0,\mathbf{Z}_{k,u,v}^{(l+1)})-min(0,\mathbf{Z}_{j,u,v}^{(l+1)})$|, and $\mu,\gamma,\sigma,\beta$ are the batch normalization parameters.
% \end{lemma}
% where,$\mathbf{\epsilon}=\mathbf{W}_{j}^{(l)} - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \mathbf{W}_{k}^{(l)}$, $B$=$\gamma_{j} \over \sigma_{j}$$\{ \sum\limits_{i = 1, i \neq j }^{m} s_{i} (\mu_{i} -  \frac{\sigma_{i}}{\gamma_{i}}\beta_i) - \mu_j \} + \beta_j $, $R$ is w $\times$ h matrix where $ R_{u,v}$ = |$\sum\limits_{k=1, k \neq j}^{m} s_{k}min(0,\mathbf{Z}_{k,u,v}^{(l+1)})-min(0,\mathbf{Z}_{j,u,v}^{(l+1)})$|, and $\mu,\gamma,\sigma,\beta$ are the batch normalization parameters.



 

% Our approximation scale for pruned filter in $\ell$-th layer is computed to recovery filters in $\ell+1$-th layer. Algorithm of our proposed method is largely two parts. In the case of remained filter, we do not compute the approximation scale. And in the case of pruned filter, we compute the approximation scale on remained filters. Through the process to minimize Eq. (\ref{eq:eq10}), we can get the approximation scales for preserved filter. Approximation scale is computed each $\ell$-th layers that perform pruning.

 
 
 
%  is vectorized $i$-th filter in $l$-th layer for $i \in [1,m_{l}] \setminus$ \{$j$\}, $\boldsymbol{y}$ is vectorized $j$-th filter in $l$-th layer, $\boldsymbol{p}$ is ($m_{l}-1$)-dimensional vector whose i-th component is $\mu_{i}-\frac{\sigma_{i}}{\gamma_{i}}\beta_{i}$ for $i \in [1,m_{l}] \setminus$ \{$\textit{j}$\}.

% Meanwhile, our LBYL method can have more flexibility in Error components. We consider a reasonable loss function that reflect all error components comprehensively. Through this loss function, we can find more optimal solution $\boldsymbol{s}$ in Eq.(10) than One-to-One compensation method.





% They consider SSE and BE and AE to a degree, but their methods are so heuristic and can have large disadvantage of SSE. For example, Kim \textit{et.al} \cite{NM} choose one of the preserved filters based on their customized score which reflect SSE and BE at different degree. This degree is decided by parameter $\lambda$ which need to be searched. Furthermore, Kim \textit{et.al} \cite{NM} decide whether to compensate or not based on cosine similarity between the preserved and pruned filter at the end. That is, they are both depend on the similarity between pruned and preserved filter, and they suffer from little similarity in some layers as shown in \cite{NM} which result in large SSE. On the other hand, they may have advantage in terms of BE since they have only one BN parameter in $\boldsymbol{B}$ term. 




% We should consider batch normalization to minimize reconstruction form. In case of BN, we also assume that the the feature map $\mathbf{Z}_{j,:,:}^{(\ell+1)}$ generated by pruned filter $\mathbf{W}_{j,:,:,:}^{(\ell)}$ can be represented linear combination of feature maps $\mathbf{Z}_{i,:,:}^{(\ell+1)}$ generated by preserved filters $\mathbf{W}_{i,:,:,:}^{(\ell)} i \in [1,m] \setminus {\{$\textit j$\}}$.

% \begin{eqnarray} \begin{split}\label{eq:eq8}
%      \text{Eq.} (\ref{equation1}). &= \| \N(\mathbf{Z}_{j,:,:,:}^{(\ell)}) - \sum\limits_{i = 1, i \neq j }^{m} {s}_{i}^{\prime} \times \N(\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{i,:,:,:}^{(l)} ) \|_{1} \\ 
%  &=\| \frac{\gamma_{j}(\mathbf{Z}_{j,:,:,:}^{(l)} - \mu_j)}{\sigma_{j}} + \beta_{j}- \sum\limits_{i = 1, i \neq j }^{m}{{s}_{i}^{\prime} \times \{ \frac{\gamma_{i}(\mathbf{Z}_{i,:,:,:}^{(l)} - \mu_i)}{\sigma_{i}} + \beta_{i}}\} +B \|_{1}\\
% \end{split}
% \end{eqnarray}
% where ${s}_{i}^{\prime}$ = $\frac{\gamma_{j}\sigma_{i}}{\sigma_{j}\gamma_{i}}{s}_{i}$ is the scale considered by batch normalization and
% we want bias caused by considering the batch normalization to be zeroed. $B=\frac{\sigma_{j}}{\gamma_{j}} \{ \sum\limits_{i = 1, i \neq j }^{m} s_{i} (\mu_{i} -  \frac{\sigma_{i}}{\gamma_{i}}\beta_i) - \mu_j \} + \beta_j$ \} and $\mu,\gamma, \sigma$ is the batch normalization parameters $\in \mathbb{R}^{n \times w \times h}$ for feature maps. 


% % $\mathbf{B} = \frac{\gamma_{j}}{\sigma_{j}} \{ \sum\limits_{i = 1, i \neq j }^{m} s_{i} (\mu_{i} -  \frac{\sigma_{j}}{\gamma_{j}}\beta_i) - \mu_j + \frac{\gamma_{j}}{\sigma_{j}}(\mathbf{Z}^{(\ell)}\circledast\boldsymbol{\epsilon} \} + \beta_j$ \} to $\mathbf{B}=\frac{\gamma_{j}}{\sigma_{j}} \{ \sum\limits_{i = 1, i \neq j }^{m} s_{i} (\mu_{i} -  \frac{\sigma_{j}}{\gamma_{j}}\beta_i) - \mu_j \} + \beta_j$ \} and $\mu,\gamma, \sigma$ is the batch normalization parameters $\in \mathbb{R}^{n \times w \times h}$ for feature maps. 

% Through the lemma below, we can find the reconstruction error caused by batch normalization layer.
% \newtheorem*{lemma}{Lemma} 
% \begin{lemma} From the above equation, we can get the reconstruction error considering batch normalization.
%     \begin{equation} 
%     \| \mathbf{A}_{j,:,:}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k,:,:}^{(\ell)}\|_{1}  = \| \frac{\sigma_{j}(\mathbf{Z}^{(\ell-1)} \circledast \mathbf{\epsilon})}{\gamma_{j}} + B \|_{1}\\
%     \end{equation}\nonumber
% \end{lemma}

% Therefore, we want both $B$ and  $\mathbf{\epsilon}$ zero..

% \smalltitle{Considering Activation Function}
% Moreover, our assumption should extend to next layer that is activation layer. Therefore we assume that activation map generated by pruned filter can be represented by linear combination of activation maps generated by remained filters.

% \begin{eqnarray} \begin{split}\label{eq:eq}
%      \text{Eq.} (\ref{equation1}). &= \| \F\circ(\N(\mathbf{Z}_{j,:,:,:}^{(\ell)})) - \sum\limits_{i = 1, i \neq j }^{m} {s}_{i}^{\ast} \times \F\circ(\N(\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{i,:,:,:}^{(l)})) \|_{1} \\ 
%      &= \| \F\{\N(\boldsymbol{\alpha}_{j,:,:}^{(l+1)}),0\} - \sum\limits_{i = 1, i \neq j }^{m_{i}} \F\{\boldsymbol{s}_{i}^{\ast} \times \N(\boldsymbol{\alpha}_{i,:,:}^{(l+1)})\} +\F(\boldsymbol{K}) \} \|_1
%      \end{split}
% \end{eqnarray}

% \begin{lemma} From the above equation, we can get the reconstruction error considering activation. More detailed derivation is in appendix.
%     \begin{equation} 
%      \| \frac{\sigma_{j}}{\gamma_{j}}(\mathbf{Z}^{(\ell)}\circledast\boldsymbol{\epsilon})+ B + AE\|_{1}
%      \end{equation}\nonumber
%      \label{lemma}
% \end{lemma}
% where, $ AE = \sum\limits_{m=1}^{h^{\prime}}\sum\limits_{n=1}^{w^{\prime}}-min(0,\mathbf{Z}_{j,m,n}^{(l+1)})+\sum\limits_{k=1, k \neq j}^{m} s_{k}min(0,\mathbf{Z}_{k,m,n}^{(l+1)}) \in \boldsymbol{R}^{h^{\prime} \times w^{\prime}}$

% Finally, we should minimize $B$ and  $\mathbf{\epsilon}$ and activation error zero..


% \smalltitle{Objective Function}
% As shown in lemma(\ref{lemma}) above, we should minimize the $b$ to reduce the reconstruction error,which leads to reduction of the error caused from activation. Plus, we also reduce the difference between pruned filter and remained filters. Therefore, we design the loss to minimize reconstruction with errors we consider above.  
% \begin{equation}
%      \mathcal{L}_{re} =  \| \mathbf{W}_{j,:,:,:}^{(\ell)} - \sum\limits_{i = 1, i \neq j }^{m} s_{i} \mathbf{W}_{i,:,:,:}^{(\ell)} \|_2^{2} + \lambda_1 \|B\|_2^{2} + \lambda_2 \|s_i \|_{2}^{2},
% \label{eq:eq10}
% \end{equation}
% where $i \in [1,m] \setminus {\{$\textit j$\}}$.

% By using the loss function to find the optimized scale, we can control the three error components. First, Sum of Squared Error(\textit{SSE}) that means difference between pruned filter and linear combination of remained filters can be minimized by first terms in Eq. (\ref{eq:eq10}). Second, Batch normalization Error (\textit{BE}) that means error caused by batch normalization layer also can be reduced. Finally, the Activation Error (\textit{AE}) that error from the activation error can be decreased weakly by third term in Eq. (\ref{eq:eq10}) in the sense of preventing amplification of some coefficient $s_{i}$ that may amplify the activation error term. Through this loss function, we can find more optimal solution 
% $s$ than one-to-one compensation method. Furthermore, we investigate that minimizing loss function is truly helpful in Theorem 1 because we substitute scale ($\boldsymbol{\mathcal{S}}^{\prime}$) for scale ($\boldsymbol{\mathcal{S}}^{\ast}$) considering the activation.

% % The difference between  $\boldsymbol{Z}_{j,:,:}^{(l+1)}$ and $ \sum\limits_{n = 1, n \neq j}^{m_{l}}{S_{n}} \boldsymbol{Z}_{n,:,:}^{(l+1)}$ can be quantified as below Theorem 1.
% % \smalltitle{\textit{Theorem 1}} If we assume Eq. (\ref{eq:eq6})., we can assume s_{i}^{\prime} = \frac{\gamma_{j}}{\sigma_{j}}\frac{\sigma_{i}}{\gamma_{i}}s_{i}$ for $i \in [1,m] \setminus$ \{$j$\} and $\| \boldsymbol{Z}_{j,:,:}^{(l+1)} - \sum\limits_{n = 1, n \neq k}^{m}{S_{k}} \boldsymbol{Z}_{k,:,:}^{(l+1)}\|_{1}$ can be represented as below. The detailed derivation process is in appendix.



% From Theorem 1, we get to know that we should minimize $b$ and $\boldsymbol{\epsilon}$ and the last term concerned with activation layer to minimize the reconstruction error. Thus, in general compensation method, we should find the scalars $s_{i}$ for $i \in [1,m] \setminus$ \{$\textit{j}$\} which minimize the three error components.



% \subsection{Our Train-Free Recovery Method with a Closed Form Solution}

% Moreover, as mentioned earlier, To propose data-free and training-free method. We use the closed form solution and then compensate pruned filter in a real time. Our loss function is the loss function is convex and there exists a unique solution as shown below at Theorem 2.

% \smalltitle{\textit{Theorem 2}} The loss function in Eq. (\ref{eq:eq10}). is convex and there exists a unique optimal solution. More detailed derivation is in appendix.
% \begin{equation}
%     s_{i}=[X^{T}X+\lambda_{1}\frac{\gamma_{j}^2}{\sigma_{j}^2} \boldsymbol{p}\boldsymbol{p}^{T}]^{-1}[X^{T}\boldsymbol{y}+\frac{\lambda_{1}\gamma_{j}}{\sigma_{j}}(\frac{\mu_{j}\gamma_{j}}{\sigma_{j}}-\beta_{j})\boldsymbol{p}]
% \label{eq:eq}\nonumber
% \end{equation}
%  where $X_{:,i}$ is vectorized $i$-th filter in $l$-th layer for $i \in [1,m_{l}] \setminus$ \{$j$\}, $\boldsymbol{y}$ is vectorized $j$-th filter in $l$-th layer, $\boldsymbol{p}$ is ($m_{l}-1$)-dimensional vector whose i-th component is $\mu_{i}-\frac{\sigma_{i}}{\gamma_{i}}\beta_{i}$ for $i \in [1,m_{l}] \setminus$ \{$\textit{j}$\}.

% Finally, given $\lambda_{1}$ and $\lambda_{2}$, we can implement many-to-one compensation Algorithm in real time using the closed form solution in Theorem 2. Actually, this compensation algorithm can be extended to multiple pruned filters through just implementing above works iteratively. We call this compensation Algorithm LBYL. We represent how LBYL works in Algorithm \ref{alg:Recovery}. Our approximation scale for pruned filter in $\ell$-th layer is computed to recovery filters in $\ell+1$-th layer. Algorithm of our proposed method is largely two parts. In the case of remained filter, we do not compute the approximation scale. And in the case of pruned filter, we compute the approximation scale on remained filters. Through the process to minimize Eq. (\ref{eq:eq10}), we can get the approximation scales for preserved filter. Approximation scale is computed each $\ell$-th layers that perform pruning.

% \begin{algorithm}[htb]
%   \caption{LBYL Algorithm}
%   \label{alg:Recovery}
%   \KwIn{$\boldsymbol{W}^{{(\ell)}}$ $\in$ $\boldsymbol{R}^{m_l \times n_l\times k \times k}$, set of remained filter $R = \{r_1, r_2, ..., r_{t_l}\}$} 
%   \KwOut{Approximation matrix $s^{(\ell)}$ $\in$ $\boldsymbol{R}^{m_{l} \times t_l}$}
   
%   Initialize ${s}^{(\ell)}$ $\in$ $\boldsymbol{R}^{m_{l} \times t_l}$ with 0
  
%   \For{each filter $w_m$ $\in$ $\mathbb{R}^{n_{l} \times k \times k}$   $\mathbf{in}$ $\mathbf{W}^{(\ell)}$}
%     {
%     \If{w_m $\in$ R} 
%         {
%         ${i} \leftarrow $index of $w_m$ within $\boldsymbol{W}^{{(\ell)}}$
        
%         $s_{i,m}^{(\ell)} \leftarrow$ 1
%         }
%     \Else{        
%     approximation~scale~list $\leftarrow$ $\underset{s^{\ell}}{\mathrm{argmin}}$ Eq. \ref{eq:eq10}.
    
%     ${i} \leftarrow $ index of $w_m$
    
%         \For{each scale $\mathbf{in}$ approximation scale list}
%             {
%                 ${j} \leftarrow $index of scale within approximation scale list
                
%                 $s_{i,j}^{(\ell)} \leftarrow $scale \times \frac{\gamma_{j}\sigma_{i}}{\sigma_{j}\gamma_{i}} 
                
%           }
%       }
%     }
% \Return $s^{(\ell)}$
% \end{algorithm}


% \smalltitle{Discussion on one-to-one vs. many-to-one approach}
% In one-to-one compensation \cite{NM,Data-free}, they prudently choose one of the preserved filters corresponding to one pruned filter, and then compensate like general compensation method. so in this case, there is one $s_{i}$ in Eq. (\ref{eq:eq6}).
% They consider SSE and BE and AE to a degree, but their methods are so heuristic and can have large disadvantage of SSE. For example, Kim \textit{et.al} \cite{NM} choose one of the preserved filters based on their customized score which reflect SSE and BE at different degree. This degree is decided by parameter $\lambda$ which need to be searched. Furthermore, Kim \textit{et.al} \cite{NM} decide whether to compensate or not based on cosine similarity between the preserved and pruned filter at the end. That is, they are both depend on the similarity between pruned and preserved filter, and they suffer from little similarity in some layers as shown in \cite{NM} which result in large SSE. On the other hand, they may have advantage in terms of BE since they have only one BN parameter in $\boldsymbol{B}$


%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Proposed Method} \label{sec:method}

% Most of filter pruning methods try to select filters to be pruned prudently so that pruned network's output be similar to the original network's. To this end, they prune the unimportant filters and then fine-tune the pruned network with using the train data. How can we restore the the pruned networks without any data? In other words, it implies that we cannot use any data-driven values(i.e., activation maps) and we can only exploit the values of original filters. In that case, the only thing we can do maybe changing the weights of remained filters appropriately not to amplify the difference between pruned and unpruned network's outputs through the information of original filters.



% \smalltitle{Problem Statement} For a Convolutional Neural Network (CNN) with $L$ layers, we denote $\mathbf{Z}{^{(\ell-1)}}$ $\in$ $\mathbb{R}^{n_{l} \times h_{l} \times w_{l}} $ is activation maps at $l$-th layer, where $n_{l}$, $h_{l}$ and $w_{l}$ are the number of channels, height and width in activation maps, respectively. and we denote $\mathbf{W}^{{(\ell)}}$ $\in$  $\mathbb{R}^{m_l \times n_l\times k \times k}$ is covolution filters in $l$ -th layer,where $m_l$, $n_l$ and $k$ are the number of filters, number of channels and kernel size, respectively. Trough the convolution operation using activation map $\mathbf{Z}{^{(\ell-1)}}$ and convolution filter $\mathbf{W}^{{(\ell)}}$ in $l$-th layer, the feature maps $\boldsymbol{\mathcal{A}}^{{(l+1)}}$ $\in$ $\mathbb{R}^{m_{l} \times h_{l+1} \times w_{l+1}}$ is computed as shown in Eq. (\ref{eq:eq1}).

% \begin{equation}
% \boldsymbol\alpha^{(l+1)} = {\mathbf{Z}^{(\ell)} \circledast {\mathbf{W}}^{(\ell)}}
% \label{eq:eq1}
% \end{equation}

% and the feature maps passed through the BN and ReLU layer are activation maps $\mathbf{Z}{^{(l+1)}}$ $\in$ $\mathbb{R}^{m_{l} \times h_{l+1} \times w_{l+1}} $ in $i+1$ -th layer as shown in Eq. (\ref{eq:eq2}).

% \begin{equation}
% \mathbf{Z}^{(l+1)} = \mathcal{F}({\mathbf{Z}^{(\ell)} \circledast {\mathbf{W}}^{(\ell)}})
% \label{eq:eq2}
% \end{equation}
% where $\mathcal{F}$ is the function that implement batch normalization and non-linear activation(e.g., ReLU).

% If the filter pruning is performed in $l$-th layer, the shape of original filters $\mathbf{W}^{{(\ell)}}$ $\in$ $\mathbb{R}^{m_l \times n_l\times k \times k}$ is modified to ${\hat {\mathbf{W}}^{(\ell)}}$ $\in$ $\mathbb{R}^{t_l \times n_l\times k \times k}$, where $t_l$ $<$ $m_l$ by pruning criterion. Therefore, the pruned activation maps ${\hat {\mathbf{Z}}}{^{(l+1)}}$ $\in$ $\mathbb{R}^{t_{l} \times h_{l+1} \times w_{l+1}}$ in ($l+1$)-th layer is computed by Eq. (\ref{eq:eq3}).

% \begin{equation}
% \mathbf{\hat{Z}}^{(l+1)} = \mathcal{F}({\mathbf{Z}^{(\ell)} \circledast {\mathbf{\hat{W}}}^{(\ell)}})
% \label{eq:eq3}
% \end{equation}

% Moreover, corresponding channels of each filters in ($l+1$) -th layer are sequentially removed. As a result, shape of original filters $\mathbf{W}^{{(l+1)}}$ $\in$ $\mathbb{R}^{m_{l+1} \times m_l\times k \times k}$ in ($l+1$) -th layer is changed to  ${\hat {\mathbf{W}}^{(l+1)}}$ $\in$ $\mathbb{R}^{m_{l+1} \times t_l\times k \times k}$. Although the pruned feature maps $\boldsymbol{\hat{\alpha}}^{{(l+2)}}$ $\in$ $\mathbb{R}^{m_{l+1} \times h_{l+1} \times w_{l+1}}$ in ($l+2$) -th layer have same shape with original feature maps $\boldsymbol{\mathcal{A}}^{{(l+2)}}$ $\in$ $\mathbb{R}^{m_{l+1} \times h_{l+1} \times w_{l+1}}$, the pruned feature maps $\boldsymbol{\hat{\alpha}}^{{(l+2)}}$ are damaged. Thus, we make approximation scales $\testit{s}$ $\in$ $\mathbb{R}^{m_{l} \times t_l}$ with relationship between the pruned filter and preserved filters in $l$-th layer and then apply it to the original filters in ($l+1$) -th layer to compensate for pruned feature maps $\boldsymbol{\hat{\alpha}}^{{(l+2)}}$ as shown in Eq. (\ref{eq:eq4}).
% (i.e., change $\hat{\mathbf{W}}^{(l+1)}$ to ${\mathbf{W}}^{(l+1)}$ \circledast  ${\boldsymbol{\mathcal{S}}} $)

% \begin{equation}
% \boldsymbol\alpha^{(l+2)} = {\mathbf{Z}}^{(l+1)} \circledast {\mathbf{W}}^{(l+1)}
% \approx {\hat{\mathbf{Z}}^{(l+1)} \circledast {\mathbf{W}}^{(l+1)} \circledast  {\boldsymbol{\mathcal{S}}} = \boldsymbol{\hat{\alpha}}^{{(l+2)}}}
% \label{eq:eq4}
% \end{equation}

% Our goal is finding the approximation scales $\testit{s}$ to minimize the reconstruction error between the pruned model and the original model without any data, and consequently compensating for pruned filters in neural network using this approximation scales $\testit{s}$,which can be represented as below in Eq. (\ref{eq:eq5}).

% \begin{equation}
% \boldsymbol{\mathcal{S}} =  \underset{{\mathcal{S}}}{\mathrm{argmin}} \sum\limits_{{i} = 1}^{m_{l+1}}\|{\boldsymbol{\alpha}_{i,:,:}^{{(l+2)}}-\boldsymbol{\hat{\alpha}}_{i,:,:}^{{(l+2)}}}\|_1
% \label{eq:eq5}
% \end{equation}


% \kim{From this part(below), should be checked thoroughly again!! this part is so important. There are too many mathematical expression error.....}
% \subsection{One-to-one compensation}
% As mentioned in introduction, in one-to-one compensation, they assume that one pruned filter can be approximated by one remained filter\cite{NM,Data-free}(i.e., $ W_{pruned} \approx S\circledastW_{remained} $ ). And more clearly, This can be represented as below.\kim{the below equation and below sentences are not matched each other! You should check it carefully!}


% \begin{equation}
% % \boldsymbol{W}_{j,:,:,:}^{(\ell)} ={\boldsymbol\beta}_{:,j}^{(\ell)} \times \boldsymbol{W}_{i,:,:,:}^{(\ell)}  + \boldsymbol{\epsilon}
% \boldsymbol{W}_{j,:,:,:}^{(\ell)} = \boldsymbol{S} \times \boldsymbol{W}_{i,:,:,:}^{(\ell)} 
% \label{eq:eq6}
% \end{equation}
% where $i$-th filter is the the preserved filter in $l$ -th layer and $j$-th filter is the pruned filter in the same layer and $\mathcal{S}$ is the scale and ${\boldsymbol{\epsilon}}$ is the residual. \kim{I think that the definition of scale more clearly.}

% the reconstruction error formula can be represented as below.
% \begin{eqnarray} \begin{split}\label{eq:eq7}
%         \mathcal{L}_{re} = \sum\limits_{{i} = 1}^{m_{l+1}} \|{\boldsymbol{\alpha}_{i,:,:}^{{(l+2)}}- \mathcal{F}(\boldsymbol{\hat{\alpha}}_{i,:,:}^{{(l+1)}}}) \circledast  \mathbf{W}_{i,:,:,:}^{(l+1)} \circledast \boldsymbol {\mathcal{S}}_{i,:}^{(\ell)}\|_1 
% \end{split}
% \end{eqnarray}
% The channels of preserved filter in ($l+1$)-th layer are compensated using the scale $\boldsymbol{\mathcal{S}}_{i,:}^{(\ell)}$ obtained from the relationship between pruned filter and preserved filter in $l$-th layer \cite{NM,Data-free}. \cite{NM} first prudently choose one of the preserved filters corresponding to each pruned filter in $l$-th layer based on the cosine similarity and bias caused by batch normalization. Then calculate the scale $\boldsymbol{\mathcal{S}}_{i,:}^{(\ell)}$ reflecting the batch normalization scale. Finally, \cite{NM} determine whether to apply the scale $\boldsymbol{\mathcal{S}}_{i,:}^{(\ell)}$ to the original filters or not.\kim{I think the exact description about S is needed!!}

% \subsection{Many-to-one compensation}
% We propose the many-to-one compensation method to sufficiently compensate for the pruned filter. To this end, we approximate the pruned filter with the linear combination of preserved filter. Actually, one-to-one compensation ignore the residual term. in real case, there exists non trivial residual. Therefore it can be showed more clearly as below Eq. (\ref{eq:eq7}).

% \begin{equation}
% \boldsymbol{W}_{j,:,:,:}^{(\ell)} =\sum\limits_{i = 1, i \neq j }^{m_{i}} \boldsymbol{S}_{i} \times \boldsymbol{W}_{i,:,:,:}^{(\ell)}  + \boldsymbol{\epsilon} 
% \label{eq:eq8}
% \end{equation}

% The reconstruction error about the many-to-one compensation can be represented using Eq. (\ref{eq:eq9}).


% \begin{eqnarray} \begin{split}\label{eq:eq9}
%     \mathcal{L}_{re} = \sum\limits_{{i} = 1}^{m_{l+1}}\|{\boldsymbol{\alpha}_{i,:,:}^{{(l+2)}}- \mathcal{F}(\boldsymbol{\hat{\alpha}}_{i,:,:}^{{(l+1)}}}) \circledast  \mathbf{W}_{i,:,:,:}^{(l+1)} \circledast \boldsymbol{{\mathcal{S}}}_{i,:}^{(\ell)}\|_1 
% \end{split}
% \end{eqnarray}
% where $\boldsymbol{\mathcal{K}}$ = $\frac{\gamma_{j}}{\sigma_{j}}[\sum\limits_{{i} = 0 , i \neq j}^{m_{i}}\{(\boldsymbol{S}_{i}\mu_{i} -\frac{\sigma_{i}}{\gamma_{i}}\beta_{i}) - \mu_{j} \} ]+ \beta_{j}$ is bias term about the batch normalization. The derivation process for the above reconstruction error formula is in appendix.\kim{Where is K?? and to add it in appendix, you prefer to make it Lemma or Theorem!!}


% \smalltitle{\textit{Approximation for scale}} To find out the optimized approximation scale for compensation, We start from our assumption pruned filter is represented by linear combination of multiple filters in $l$-th layer and extend our assumption to up to next activation maps in $l+1$-th layer and then compute the approximation scale. First, we can extend our assumption to the followed feature map generated by pruned filter using our assumption as shown below.
% \begin{equation}
% \boldsymbol{\alpha}_{j,:,:}^{(l+1)} =\sum\limits_{i = 1, i \neq j }^{m_{i}} \boldsymbol{S}_{i} \times \boldsymbol{Z}_{i,:,:}^{(\ell)} \circledast \boldsymbol{W}_{i,:,:,:}^{(\ell)}   +  \boldsymbol{Z}_{i,:,:}^{(\ell)} \circledast \boldsymbol{\epsilon}
% \label{eq:eq10}
% \end{equation}



% As we assume that the pruned filter is represented by linear combination of preserved filter, in batch normalization layer, we can also assume that the the feature map generated by pruned filter can be represented linear combination of feature maps generated by preserved filters as shown in Eq. (\ref{eq:eq11}). \kim{at below equation, what is mi? is m depend on i? this is strange. }

% \begin{eqnarray} \begin{split}\label{eq:eq11}
%     \boldsymbol{BN}(\boldsymbol{\alpha}_{j,:,:}^{(l+1)}) &= \sum\limits_{i = 1, i \neq j }^{m_{i}} \boldsymbol{S}_{i}^{\prime} \times \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)})    +\boldsymbol{K} \\
%  \frac{\gamma_{j}(\boldsymbol{\alpha}_{j,:,:}^{(\ell)} - \mu_j)}{\sigma_{j}} + \beta_{j} & =  \sum\limits_{i = 1, i \neq j }^{m_{i}}{\boldsymbol{S}_{i}^{\prime} \times \{ \frac{\gamma_{i}(\boldsymbol{\alpha}_{i,:,:}^{(\ell)} - \mu_i)}{\sigma_{i}} + \beta_{i}}\} +\boldsymbol{K} \\
% \end{split}
% \end{eqnarray}
% \kim{K dimension error!! Add operation cannot be used!}
% where $\boldsymbol{S}_{i}^{\prime} = \frac{\gamma_{j}\sigma_{i}}{\sigma_{j}\gamma_{i}}\boldsymbol{S}_{i}$ and $\boldsymbol{K} = \frac{\gamma_{j}}{\sigma_{j}} \{ \sum\limits_{i = 1, i \neq j }^{m_{i}}(\boldsymbol{\alpha}_{j,:,:}^{(\ell)}\mu_{i} -  \frac{\sigma_{j}}{\gamma_{j}}\beta_i) - \mu_j + (\boldsymbol{Z}_{i,:,:}^{(\ell)} \circledast \boldsymbol{\epsilon})\} + \beta_j$.

% And our assumption should extend to next layer that is activation layer as shown in Eq. (\ref{eq:eq12}). 
% \begin{eqnarray} \begin{split}\label{eq:eq12}
%     \boldsymbol{max}\{\boldsymbol{BN}(\boldsymbol{\alpha}_{j,:,:}^{(l+1)}),0\} &= \sum\limits_{i = 1, i \neq j }^{m_{i}} \boldsymbol{max}\{\boldsymbol{S}_{i}^{\ast} \times \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)}),0\} +\boldsymbol{max}(\boldsymbol{K},0)
% \end{split}
% \end{eqnarray}
% Unfortunately, we can't extend our assumption without any data. That is, it means we should consider all cases for possible branches. Therefore, we extend our assumption feature maps passed through the batch normalization layer in $l+1$-th layer as shown in Eq. (\ref{eq:eq13}). 
% \begin{eqnarray} \begin{split}\label{eq:eq13}
%     \boldsymbol{\alpha}_{j,:,:}^{(l+2)} & = \boldsymbol{BN}(\boldsymbol{\alpha}_{:,:,:}^{(l+1)} \circledast \boldsymbol{W}_{j,:,:,:}^{(l+1)}) \\
%     &\approx \sum\limits_{i = 1, i \neq j}^{m_{i}} \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)} \circledast \boldsymbol{\acute{W}}_{j,i,:,:}^{(l+1)}) = \sum\limits_{i = 1, i \neq j}^{m_{i}} \boldsymbol{BN}\{\boldsymbol{\alpha}_{i,:,:}^{(l+1)} \circledast (\boldsymbol{W}_{j,i,:,:}^{(l+1)}+\boldsymbol{S}_{i}^{\prime} \times \boldsymbol{W}_{j,j,:,:}^{(l+1)}) \}\\
%     &=\sum\limits_{i = 1, i \neq j}^{m_{i}} \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)} \circledast \boldsymbol{W}_{j,i,:,:}^{(l+1)}) + \sum\limits_{i = 1, i \neq j}^{m_{i}} \boldsymbol{S}_{i}^{\prime} \times \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)}) \circledast  \boldsymbol{W}_{j,j,:,:}^{(l+1)}\\
%     &=\sum\limits_{i = 1, i \neq j}^{m_{i}} \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)} \circledast \boldsymbol{W}_{j,i,:,:}^{(l+1)}) + (\boldsymbol{BN}(\boldsymbol{\alpha}_{j,:,:}^{(l+1)}) - \boldsymbol{K})\circledast  \boldsymbol{W}_{j,j,:,:}^{(l+1)}   % \text{ by Eq. }(\ref{eq:eq11})
% \end{split}
% \end{eqnarray}

% we should minimize $\boldsymbol{K}$,which leads to ultimately reduce the reconstruction error in $l$-th layer induced from removed filter in $l$-th layer the from Eq. (\ref{eq:eq13}). 
% Moreover, we decompose $\boldsymbol{K}$ to optimize the the approximation scale that minimize the reconstruction error and discover that the $\boldsymbol{K}$ is composed of three error terms,which are Sum of Squares Error (\textit{SSE}, $\boldsymbol{\varepsilon}$), Batch Normalization Error (\textit{BE}, \boldsymbol{K}) and Activation Error (\textit{AE}, $\boldsymbol{\mathds{1}}$).
% \begin{eqnarray} \begin{split}\label{eq:eq14}
%         \boldsymbol{\|\boldsymbol{K}\|_1} & =  \|\boldsymbol{BN}(\boldsymbol{\alpha}_{j,:,:}^{(l+1)}) - \sum\limits_{i = 1, i \neq j }^{m_{i}} \boldsymbol{S}_{i}^{\prime} \times \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)})  \|_1 \\
%         & = \|\boldsymbol{\varepsilon} -\boldsymbol{\mathds{1}}\times\boldsymbol{BN}(\boldsymbol{\alpha}_{j,:,:}^{(l+1)}) + \sum\limits_{i = 0, i \neq j }^{m_i}\boldsymbol{\mathds{1}_{i}} \times\boldsymbol{S}_{i}^{\prime} \times \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)}) \|_1 \\
%         & \le  \|\boldsymbol{\varepsilon}\|_1 + \|\sum\limits_{i = 0, i \neq j }^{m_i}\boldsymbol{\mathds{1}_{i}} \times\boldsymbol{S}_{i}^{\prime} \times \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)})  -\boldsymbol{\mathds{1}}\times\boldsymbol{BN}(\boldsymbol{\alpha}_{j,:,:}^{(l+1)})\|_1
% \end{split}
% \end{eqnarray}
% where $\boldsymbol{\mathds{1}}$ is the indicator variable$\boldsymbol{\varepsilon}$ = $\boldsymbol{BN}(\boldsymbol{\alpha}_{j,:,:}^{(l+1)}) - \sum\limits_{i = 1, i \neq j }^{m_{i}} \boldsymbol{S}_{i}^{\prime} \times \boldsymbol{BN}(\boldsymbol{\alpha}_{i,:,:}^{(l+1)})$. \kim{Is it better than the direc delta? and the indicator notation is not correct!! you should describe about indicator more clearly!}

% Therefore, we set a reasonable loss function as below that can overall control SSE and BE (and weakly AE through L2-regularization) Eq. (\ref{eq:eq14}) and optimizing this loss function, we can obtain nicely a closed form solution as Theorem 1.

% \smalltitle{\textit{Theorem 1}} The unique closed form solution. we will use the property of positive definite matrix and weak vector calculus to prove that (in appendix). \kim{The sentence in Theorem 1 is not that!!}
% \begin{equation}
%     \mathcal{L}_{re} =  \| \boldsymbol{W}_{j,:,:,:}^{(\ell)} - \sum\limits_{i = 1, i \neq j }^{m_{i}} \boldsymbol{\mathcal{S}}^{l} \times \boldsymbol{W}_{i,:,:,:}^{(\ell)} \|_2 + \boldsymbol{\lambda_1} {\|\boldsymbol{K}\|_2} + \boldsymbol{\lambda_1} {\|\boldsymbol{\mathcal{S}}^{(\ell)}\|_2}
% \label{theorem:theorem1}
% \end{equation}

% Using this closed form solution, we can compensate pruned filter in a real time. we call this compensation method LBYL. LBYL can have advantage in the perspective of SSE. Furthermore, LBYL also reduces other error components (BE, AE) so that pruned filter can be compensated more optimally.\kim{there are some nuance difference.. for example.. are we reducing all BE and AE??(specifically we are not controlling AE clearly!!) I think the previous representation that I wrote is more valid.......} There is the whole procedure of LBYL described in Algorithm 1.


% \begin{algorithm}[tb]
%   \caption{Compensation Algorithm}
%   \label{alg:Compensation}
%   \KwIn{$\mathbf{W}^{{(\ell)}}$ $\in$ $\mathbb{R}^{m_l \times n_l\times k \times k}$, set of remained filter $R = \{r_1, r_2, ..., r_{t_l}\}$, set of pruned filter $P = \{p_1, p_2, ..., p_{m_{l}-{t_l}}\}$}
%   \KwOut{Approximated scale matrices $\boldsymbol{\mathcal{S}}^{l}$ $\in$ $\mathbb{R}^{m_{l} \times t_l}$}
   
%   Initialize $\boldsymbol{\mathcal{S}}^{l}$ $\in$ $\mathbb{R}^{m_{l} \times t_l}$ with 0 \;
  
%   \For {each filter w_m $\in$ $\mathbb{R}^{n_{l} \times k \times k}$ \  $\mathbf{in}$ \ \mathbf{W}^{(\ell)}}
%     {
%     \If{w_m $\in$ $R$} 
%         {
%         ${i} \leftarrow $index of $w_m \ $within \mathbf{W}^{{(\ell)}} \\
%         $\boldsymbol{\mathcal{S}}_{i,m}^{(\ell)} \leftarrow$ 1
%         }
        
%     \Else{
%         ${approximation \ scale \ list} \leftarrow $ \underset{\mathcal{S}^{l}}{\mathrm{argmin}} Eq. \ref{eq:eq12} \\
%         ${i} \leftarrow $index of w_m \\
        
%         \For {scale $\mathbf{in}$ approximation scale list}
%         {
%             ${j} \leftarrow $index of scale within approximation scale list \\
%             ${\boldsymbol{\mathcal{S}}_{i,j}^{(\ell)}} \leftarrow scale $\\
            
%         }
%         }
%     }
%     \Return $\boldsymbol{\mathcal{S}}^{(\ell)}$
% \end{algorithm}





% \begin{equation}
% {\boldsymbol{\tilde{\mathbf{Z}}}_{i}^{{(\ell+1)}}} = \sum\limits_{k = 1, k \neq j}^{m} \mathbf{A}_{k}^{(\ell)} \circledast \mathbf{W}_{i,k}^{(\ell+1)}
% \label{eq:eq}
% \end{equation}




% By compensation in a manner like Eq.(4), the pruned feature map (5) change to the compensated feature map ${{\hat{\mathbf{Z}}}_{i}^{{(\ell+1)}}}$ as below Eq.(7).

% \begin{equation}
% {{\hat{\mathbf{Z}}}_{i}^{{(\ell+1)}}} = \sum\limits_{k = 1, k \neq j}^{m} \mathbf{A}_{k}^{(\ell)} \circledast (\mathbf{W}_{i,k}^{(\ell+1)}+{s_{k}} \mathbf{W}_{i,j}^{(\ell+1)})
% \label{eq:eq}
% \end{equation}




% By using Eq.(8), the reconstruction error of feature map in $(\ell+1)$-th layer can be represented as below Eq.(9).





% \smalltitle{Without BN, ReLU}
% As shown in Eq.(11), we should find scalars $s_{k}$ such that $\mathbf{A}_{j}^{(\ell+1)} \approx \sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(l+1)}$ to minimize the reconstruction error (5). We start the error analysis cosidering the situation where there are no BN and ReLU between feature map and activation map. BN and ReLU will be reflected sequentially. In that case, $\mathbf{A}^{(\ell)} = \mathbf{Z}^{(\ell)}$ and as a result, (12) can be represented as below Eq.(13).

% \begin{eqnarray} \begin{split}\label{eq:eq8}
%      \|\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}\|_{1} = &~\| (\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{j}^{(l)} ) - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times (\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{k}^{(l)} ) \|_{1} \\
%      = &~\| (\mathbf{Z}^{(\ell-1)}  \circledast (\mathbf{W}_{j}^{(l)} - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \mathbf{W}_{k}^{(l)} ) \|_{1}
% \end{split}
% \end{eqnarray}

% Since we cannot control the data driven feature map $\mathbf{Z}^{(\ell-1)}$, the only thing we can do is minimizing the term $(\mathbf{W}_{j}^{(l)} - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \mathbf{W}_{k}^{(l)} )$ which is called SSE.

% In order for the information carried by the pruned filter to be delivered to the other filters at the same layer, we have to examine how the value of the pruned filter is related to those of the others. Our proposed assumption is that a pruned filter can be represented as a \textit{linear combination} of all the preserved filters. Let $\mathbf{W}_{j,:,:,:}^{(\ell)}$ denote such a pruned filter at the $\ell$-th layer. Then, it can be represented as follows:
% \begin{equation}
% \mathbf{W}_{j,:,:,:}^{(\ell)} =\sum\limits_{i = 1, i \neq j }^{m} s_{i} \times \mathbf{W}_{i,:,:,:}^{(\ell)}  + \epsilon,
% \label{eq:linear}
% \end{equation}
% where $s_i$ quantifies to what extent each $i$-th preserved filter can contribute to form the pruned filter and $\epsilon$ is an approximation error.


% With the proposed assumption of Eq. (\ref{eq:linear}), let us examine how the reconstruction error of Eq. (\ref{eq:goal}) can further be derived in a way of not using any data-dependent values.

% Meanwhile, $i$-th channel of feature map $\mathbf{Z}_{i,:,:}^{{(\ell+1)}}$ in $(\ell+1)$-th layer can be showed as below.

% \begin{equation}
% {{\mathbf{Z}}_{i,:,:}^{{(\ell+1)}}} = \sum\limits_{k = 1}^{m} {\mathbf{A}}_{k,:,:}^{(\ell)} \circledast \mathbf{W}_{i,k,:,:}^{(\ell+1)}
% \label{eq:eq3}
% \end{equation}

% The pruned feature map in the same position can be showed as below.

% \begin{equation}
% {\boldsymbol{\tilde{\mathbf{Z}}}_{i,:,:}^{{(\ell+1)}}} = \sum\limits_{k = 1, k \neq j}^{m} \mathbf{A}_{k,:,:}^{(\ell)} \circledast \mathbf{W}_{i,k,:,:}^{(\ell+1)}
% \label{eq:eq}\nonumber
% \end{equation}

% And the compensated feature map in the same position can be showed as below.

% \begin{equation}
% {\tilde{\mathbf{Z}}_{i,:,:}^{{(\ell+1)}}} = \sum\limits_{k = 1}^{t}\mathbf{\tilde{A}}_{k,:,:}^{(\ell)}\circledast({\mathbf{W}}^{(\ell+1)} \times_{2} \boldsymbol{\mathcal{S}})_{i,k,:,:}
% \label{eq:eq4}
% \end{equation}

% By a few manipulation, we can make approximation matrix $\boldsymbol{\mathcal{S}}$ so that a pruned channel corresponding to a pruned filter in $l$-th layer is divided into preserved channels in each filters in $(l+1)$-th layer. Through this matrix, we can make compensated feature map ${\tilfde{\mathbf{Z}}_{i,:,:}^{{(l+1)}}}$ which is represented as below.

% \begin{equation}
% {{\tilde{\mathbf{Z}}}_{i,:,:}^{{(\ell+1)}}} = \sum\limits_{k = 1, k \neq j}^{m} \mathbf{A}_{k,:,:}^{(\ell)} \circledast (\mathbf{W}_{i,k,:,:}^{(\ell+1)}+{s_{k}} \mathbf{W}_{i,j,:,:}^{(\ell+1)})
% \label{eq:eq}\nonumber
% \end{equation}

% Then, using Eq. (\ref{eq:eq3}). and Eq. (\ref{eq:eq4})., the reconstruction error after pruning and compensation can be represented as below.

% \begin{equation}
% {\mathbf{Z}_{i,:,:}^{{(\ell+1)}}}-{\mathbf{\tilde{Z}}_{i,:,:}^{{(l+1)}}}=(\mathbf{A}_{j,:,:}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k,:,:}^{(\ell)})\circledast\mathbf{W}_{i,j,:,:}^{(l+1)}
% \label{eq:eq5}
% \end{equation}

% And using Eq. (\ref{eq:eq5})., the whole reconstruction error of feature maps in $(\ell+1)$-th layer can be represented as below Eq. \ref{eq:eq6}).

% \begin{equation}
% \sum\limits_{{i} = 1}^{m^{\prime}}\|{\mathbf{Z}_{i,:,:}^{{(\ell+1)}}-\mathbf{\hat{Z}}_{i,:,:}^{{(\ell+1)}}}\|_1 = \sum\limits_{{i} = 1}^{m^{\prime}}\|(\mathbf{A}_{j,:,:}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k,:,:}^{(\ell)})\circledast\mathbf{W}_{i,j,:,:}^{(\ell+1)}\|_1
% \label{eq:eq6}
% \end{equation}


% \smalltitle{Without BN, ReLU}
% As shown in Eq. (\ref{eq:eq6})., we should find scalars $s_{k}$ such that $\mathbf{A}_{j,:,:}^{(\ell+1)} \approx \sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k,:,:}^{(l+1)}$ to minimize the reconstruction error. 

% \begin{equation}
% \mathbf{Z}_{j,:,:}^{(\ell)} =\sum\limits_{i = 1, i \neq j }^{m} {s}_{i} \times \mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{i,:,:,:}^{(l)} +  \mathbf{Z}^{(\ell-1)} \circledast \boldsymbol{\epsilon}
% \label{eq:eq7}
% \end{equation}

% \smalltitle{Considering BN} Now we should consider Batch Normalization to minimize Eq. (\ref{eq:goal2}). In this case, $\mathbf{A}^{(\ell)} = \N(\mathbf{Z}^{(\ell)})$, where $\N$ is Batch Normalization function and as a result (12) can be represented as below Eq.(14).

% \begin{eqnarray} \begin{split}\label{eq:eq8}
%      \|\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}\|_{1} = &~\| \N(\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{j}^{(l)} ) - \sum\limits_{k = 1, k \neq j }^{m} {s}_{k} \times \N (\mathbf{Z}^{(\ell-1)}  \circledast \mathbf{W}_{k}^{(l)} ) \|_{1}
% \end{split}
% \end{eqnarray}































% % In that case, reconstruction error formula Eq.(\ref{eq:eq5}) can be represented as theorem (\ref{theorem:theorem2})
% % If we compensate one filter with multiple filters, the reconstruction error formula can be obtained as below.
% To optimize the the approximation scale that minimize the reconstruction error, we decompose the how the reconstruction error consist of and discover that the reconstruction error is composed of three error terms,which are Sum of Squares Error(SSE), Batch Normalization Error(BE) and Activation Error(AE).

% \smalltitle{\textit{Theorem 1}} The reconstruction error is decomposed as below.
% \begin{eqnarray} \begin{split}\label{Theorem:Theorem1}
%         \boldsymbol{\|\mathcal{K}\|_1} & =  \|\boldsymbol{\alpha}_{j}^{{(l+1)}} - \sum\limits_{i = 0, i \neq j }^{m_i} {\beta}_{i} \boldsymbol{{\alpha}}_{i}^{{(l+1)}} \|_1 \\
%         & = \|\boldsymbol{\varepsilon} -\boldsymbol{\mathds{1}}\times\boldsymbol{\alpha}_{j}^{{(l+1)}} + \sum\limits_{i = 0, i \neq j }^{m_i}(\boldsymbol{\mathds{1}_{i}} \times {\beta}_{i} \boldsymbol{{\alpha}}_{i}^{{(l+1)}}) \|_1 \\
%         & \le  \|\boldsymbol{\varepsilon}\|_1 + \|\sum\limits_{i = 0, i \neq j }^{m_i}(\boldsymbol{\mathds{1}_{i}} \times {\beta}_{i} \boldsymbol{{\alpha}}_{i}^{{(l+1)}})  -\boldsymbol{\mathds{1}}\times\boldsymbol{\alpha}_{j}^{{(l+1)}}\|_1
% \end{split}
% \end{eqnarray}
% where, $\boldsymbol{\mathds{1}}$ is the indicator variable, $\boldsymbol{\varepsilon}$ = $\|\boldsymbol{\alpha}_{j}^{{(l+1)}} - \sum\limits_{i = 0, i \neq j }^{m_i} {\beta}_{i} \boldsymbol{{\alpha}}_{i}^{{(l+1)}} \|_1$. More detailed derivation above formula is in appendix. \kim{Is it better than the direc delta?}

% As we can see, Theorem (\ref{theorem:theorem2}) is generalized version of Theorem (\ref{theorem:theorem1}).
% We set a reasonable loss function as below that can overall control SSE and BE (and weakly AE through L2-regularization) Eq. \ref{eq:eq12} and optimizing this loss function, we can obtain nicely a closed form solution as Theorem \ref{theorem:theorem3}

% \begin{equation}
% loss = \| \boldsymbol{W}_{j,:,:,:}^{(\ell)} - \boldsymbol{\beta} \times \boldsymbol{W}_{i,:,:,:}^{(\ell)} \|_2 + \boldsymbol{\lambda_1} {\|\boldsymbol{\mathcal{K}}\|_2} + \boldsymbol{\lambda_1} {\|\boldsymbol{\beta}\|_2}
% \label{eq:eq12}
% \end{equation}

% \smalltitle{\textit{Theorem 3}} \kim{the unique closed form solution. we will use the property of positive definite matrix and weak vector calculus to prove that (in appendix).}
% \begin{eqnarray} \begin{split}\label{theorem:theorem3}
%     \mathcal{L}_{re} = 
% \end{split}
% \end{eqnarray}

% Using this closed form solution, we can compensate pruned filter in a real time. we call this compensation method LBYL(leave Before You Leave). LBYL can have advantage in the perspective of SSE. Furthermore, LBYL also relect other error components(BE,AE) so that pruned filter can be compensated more optimally.\kim{The whole procedure of LBYL is described in Algorithm 1.}



% As the pruned filter is represented by similar preserved filter, the activation maps generated by pruned filter can represent as shown in Eq. (\ref{eq:eq7}) because both pruned filter and preserved filter have same activation maps $\mathbf{Z}^{(\ell)}$

% \begin{equation}
% \boldsymbol{\hat{\alpha}}_{j,:,:}^{(l+1)} =\mathcal{S} \times \boldsymbol{\hat{\alpha}}_{i,:,:}^{{(l+1)}}  + \boldsymbol{\epsilon}  \circledast \mathbf{Z}^{(\ell)} 
% \label{eq:eq7}
% \end{equation}

% However, \cite{NM} chooses a most similar the pruned filter with preserved filter based on $\boldsymbol{\mathcal{B}}$ which consider the batch normalization as shown Eq. (\ref{eq:eq8}) and then recompute scale $\mathcal{S}$ as adjusted scale $\mathcal{K}$. 

% \begin{equation}
% \boldsymbol{BN}(\boldsymbol{\hat{\alpha}}_{j,:,:}^{{(l+1)}}) =\mathcal{K} \times \boldsymbol{BN}(\boldsymbol{\hat{\alpha}}_{i,:,:}^{{(l+1)}})  + \boldsymbol{\mathcal{B}}
% \label{eq:eq8}
% \end{equation}
% where $\mathcal{K}$ = s$\frac{\gamma_{j}\sigma_{i}}{\gamma_{i}\sigma_{j}}$, $\boldsymbol{\mathcal{B}}$ = $\frac{\gamma_{j}}{\sigma_{j}}[s(-\frac{\sigma_{i}\beta_{i}}{\gamma_{i}} + \mu_{i}) - \mu_{j} ] + \beta_{j}$ are the adjusted scale and bias considering the batch normalization respectively, here s = $\frac{||\boldsymbol{W}_{j,:,:,:}^{(\ell)}||_2}{||\boldsymbol{W}_{i,:,:,:}^{(\ell)}||_2}$


% Finally, \cite{NM} consider the activation layer (\textit{e.g.,} ReLU) using the non-negative cosine similarity between the pruned filter and preserved filter. In other words, scale matrices $ {\boldsymbol\beta}^{(\ell)}$ in $l$-th layer is computed for all pairs of be 

% $\mathcal{K}$  has only non-negative entries with at most one strictly positive entry per column. Therefore, the pruned filter with negative similarity is not compensated.

% % limitations
% Therefore, there are limitations of one-to-one compensation. First of all, they should decide a similarity threshold about how similar pruned filter is. That is, if not exist a sufficiently similar filter, pruned filter is not compensated. Moreover, Even if there is a quiet similar filter about the pruned filter, its residual term  is not still zero because the scale is computed based on magnitude about the pruned filter and preserved filter. In other words, they only focus on how pruned filter is similar to preserved filter. 

