\section{Experiments} \label{sec:experiments}
In this section, we empirically validate the performance and effectiveness of our proposed LBYL method, compared to a one-to-one compensation method, called \textit{Neuron Merging} (\textit{NM})  \cite{NM} as well as a baseline called \textit{Prune} that does not perform any recovery process after pruning. For a fair comparison, we exploit the code provided by the authors of \cite{NM} and then implement our proposed method using PyTorch \cite{PyTorch}. LBYL uses two hyperparameters, namely $\lambda_1$ and $\lambda_2$, that adjust the weights of loss terms BE and AE, respectively. The details of hyperparameter settings are presented in Appendix. Also, our implementation is available at our anonymized github.\footnote{https://github.com/anonymous}

\smalltitle{Pruning criteria and ratio}
We evaluate LBYL using the following four data-independent pruning criteria: L1-norm \cite{PFEC}, L2-norm \cite{Soft}, L2-GM \cite{FPGM}, and lastly even random. We prune only convolution filters in CNNs, and hence the pruning ratio is the ratio of pruned filters for each convolution layer.
% Random pruning is intended to show the robustness of each method when a norm-based criterion is not used.

\subsection{Experiments using CIFAR-10 and CIFAR-100}
\smalltitle{Dataset and pretrained model} 
The CIFAR-10 dataset \cite{krizhevsky2009learning} consists 60K images of 10 different classes, where each class includes 5K training images along with 1K validation images and the size of every image is 32 × 32. CIFAR-100 \cite{krizhevsky2009learning} also include 60K images for 100 different classes, where 500 training images along with 100 validation images are included for each class. We test VGG-16 \cite{VGG} for CIFAR-10 and ResNet-50 \cite{ResNet} for CIFAR-100, and present the result of CIFAR-100 only in Appendix as 
their experimental results of CIFAR-10 and CIFAR-100 show the similar trend.


% In case of CIFAR-100, we evaluate the our proposed method using the ResNet-50 \cite{ResNet}. To train the ResNet-50, we exploit SGD with Nesterov momentum of 0.9. the initial learning rate is 0.1 and it is divided by 5 at 60, 120, 160 epochs. the weight decay is set to 5e-4, the train epochs is 200 with batch size 128. 

\smalltitle{Results}
We first prune VGG-16 on CIFAR-10, which is a representative single-branched CNN, using the layer-by-layer pruning scheme with four different criteria, and then update the remaining filters in each layer to restore the pruned VGG-16. As shown in Table \ref{tab:VGG16-CIFAR10}, our LBYL method achieves up to 6.16\% higher average accuracy than NM \cite{NM}. In particular, LBYL outperforms NM with clear margins when the pruning ratio increases. This is probably because NM might have trouble finding a filter sufficiently close to each pruned filter with less remaining filters due to a higher pruning ratio.

\subsection{Experiments using ImageNet (ILSVRC2012)}

\smalltitle{Dataset and pre-trained model} 
The ImageNet dataset \cite{deng2009imagenet} contains 1,000 classes and consist of 1.28M training images and 50K validation images of size 256 $\times$ 256. For ImageNet, we use pretrained ResNet-34 and ResNet-101, both of which are released by PyTorch\footnote{https://pytorch.org/vision/stable/models.html}. ResNet-34 and ResNet-101 respectively represent two different types of the ResNet architecture, namely the one consisting of basic blocks and the other one consisting of bottleneck blocks.

\smalltitle{Results}
To prune a ResNet architecture, we exploit the common strategy to remove each residual block, that is, pruning the first two convolutional layers and not changing the output dimension of the block. Tables \ref{tab:ResNet34-ImageNet} and \ref{tab:ResNet101-ImageNet} present the experimental results using ResNet-34 and ResNet-101 on ImageNet. Through the results, LBYL can still recover the damaged filters to a satisfactory extent.

\begin{figure}[t]
	\centering 
	% vanilla WA
	\includegraphics[height=2mm]{./figure/error_key.pdf} \\
    \subfigure[\label{fig:expcos_:a}{RE} ]{\hspace{0mm}\includegraphics[width=0.32\columnwidth]{./figure/LBYL_figure_3_RE.pdf}\hspace{0mm}}
    %CTL + WA
    \subfigure[\label{fig:expcos_:b}{BE} ]{\hspace{0mm}\includegraphics[width=0.32\columnwidth]{./figure/LBYL_figure_3_BE.pdf}\hspace{0mm}}
    % ART+WA
    \subfigure[\label{fig:expcos_:c}{WARE} ]{\hspace{0mm}\includegraphics[width=0.32\columnwidth]{./figure/LBYL_figure_3_WARE.pdf}\hspace{0mm}}
    \caption{Comparison on the three error components with NM \cite{NM}, where each $m$\_$n\_k$ in the x-axis represents the $k$-th conv module in the $n$-th block at the $m$-th layer in ResNet-50}
	\label{fig:error_components}
	\vspace{-2mm}
\end{figure}


\subsection{Effectiveness in terms of Reconstruction}
In order to see the effectiveness of our loss function on minimizing the reconstruction error, we compute the values of three error components, namely RE, BE, and AE, using ResNet-50 \cite{ResNet} on CIFAR-100 \cite{krizhevsky2009learning}. Both RE and BE can be measured without any inference with sample data, but AE is not easy to obtain. Therefore, we adopt the \textit{weighted average reconstruction error} (\textit{WARE}) introduced in \cite{NISP}, which shows the difference between the layer-wise output of the original model and that of the pruned model.

Figure \ref{fig:error_components} shows layer-wise values for each of RE, BE and WARE. It is clearly observed that NM returns a very rough approximation in terms of RE, compared to our LBYL method. This is due to the fact that NM uses only a single remaining filter to receive the missing information of a pruned filter, and consequently the final output can be quite far from the original.
In both BE and WARE, there is not much difference between LBYL and NM in early layers, but we can observe that the error values of NM become higher in the end. Thus, even though NM might be effective to reconstruct the original output in some layers, it shows unstable performance depending on the existence of similar filters and finally ends up with less accurate approximation. On the other hand, our LBYL method is less varied across layers and managed to keep the error values lower in the end for all three metrics.

\begin{table}[t]
\centering
\tiny
\begin{tabular}{c||c|c|c|c|c} \Xhline{2\arrayrulewidth}
Pruning Ratio& Ours & NM& Coreset&Prune & From Scratch (80 epochs) \\ \Xhline{2\arrayrulewidth}
10 \% & \textbf{78.3}   & 77.57  &76.55 & 77.67  & 48.22 (72.07)\\\hline
20 \% & \textbf{77.22}  &  75.62  &74.39&  75.4   & 47.85 (72.01)\\\hline 
30 \% & \textbf{75.99}  & 73.03  &71.88&  73.18  & 46.7 (72.24)\\\hline
40 \% & \textbf{74.13}  & 69.71  &68.14&  70.12  & 46.53 (72.88)\\\hline
50 \% & \textbf{71.15}  & 65.03  &62.88&  65.72  & 44 (69.33)\\\Xhline{2\arrayrulewidth}
\end{tabular}
\caption{Fine-tuned or trained accuracies of ResNet-50 on CIFAR-100 using L2-norm criterion, where we fine-tune each model for 20 epochs and train the same-sized small architecture for 20 epochs (80 epochs)}
\label{tab:finetune_scratch}
\vspace{-3mm}
\end{table}

\begin{figure}[t]
	\centering 
	\includegraphics[height=2mm]{./figure/f4_KEY.pdf} \\
    \subfigure[\label{fig:expcos:a}{10 \%} ]{\hspace{0mm}\includegraphics[width=0.32\columnwidth]{./figure/f4_10.pdf}\hspace{0mm}}
    \subfigure[\label{fig:expcos:b}{30 \%} ]{\hspace{0mm}\includegraphics[width=0.32\columnwidth]{./figure/f4_30.pdf}\hspace{0mm}}
    \subfigure[\label{fig:expcos:c}{50 \%} ��]{\hspace{0mm}\includegraphics[width=0.32\columnwidth]{./figure/f4_50.pdf}\hspace{0mm}}
    \caption{Comparison on learning curves of fine-tuning restored networks for 20 epochs and that of training the same-sized small architecture from scratch for 80 epochs at different pruning ratios}
	\label{fig:Fine tuning and From Scratch}
 	\vspace{-2mm}
\end{figure}

\smalltitle{Performance estimation}
��) can also be utilized in such a case. As shown in Figure \ref{fig:error_components}, we can roughly estimate WARE, which is also highly relevant to the final performance, by combining RE and BE.

\subsection{Practical Test with Data and Fine-Tuning}
Although this paper focuses on the problem of restoring pruned networks without data, we additionally test whether our method is also effective when the data becomes available later on. Table \ref{tab:finetune_scratch} shows the experimental results on the final accuracy of each pruned or restored model after fine-tuning along with that of the same-sized architecture trained from scratch. In this experiment, we take the data-independent pruning method based on coreset \cite{CoreSet_ICLR} as another competitor, which also introduces a training-free recovery method as an intermediate process. We fine-tune and train each model for 20 epochs, and also show the resulting accuracy of the model trained from scratch after 80 epochs. In all cases, our LBYL method outperforms its competitors particularly when the pruning ratio increases. Our best guess on why NM becomes eventually worse than just pruned model is that NM can choose a dissimilar filter to recover each pruned one in some layers, which makes it hard for fine-tuning to improve the performance.
\textit{Coreset} \cite{CoreSet_ICLR} shows the lowest accuracy probably because it does not focus on training-free recovery and hence 20 epochs of fine-tuning is not sufficient. As shown in Figure \ref{fig:Fine tuning and From Scratch}, LBYL shows the fastest convergence speed during the fine-tuning process. Both of these observations justify the fact that our LBYL method is practically useful when the data is available. 

% \TODO{Add Coreset method in Table 4 and Figure 4. Change Table 4 to have arrows}
% We also compare the Coreset\cite{CoreSet_ICLR} from the standpoint that update the weights of pruned model as shown in the Figure 4. \ref{fig:Fine tuning and From Scratch},where we present average accuracy of the Coreset \cite{CoreSet_ICLR}. The reason why Coreset \cite{CoreSet_ICLR} shows the lowest accuracy is that Coreset \cite{CoreSet_ICLR} do not focus on directly minimizing the output between pruned model and pre-trained model but concentrate on proving the error bound for output between pruned model and pre-trained model, which results in insignificant compensation.


% We measure the reconstruction errors (RE) generated by pruned filter and preserved filter and batch normalization error, which can minimize the reconstruction error, where both RE and BE are taken l2-norm to show magnitude. LBLY optimize the scale parameter to minimize the both RE and BE. Therefore, Our method has a lower error compared to NM \cite{NM} as shown in Figure \ref{fig:error_components}.Moreover we also calculate the Weighted Average Reconstruction Error (WARE) used in \cite{NISP} to measure the difference between original model and recovered model. From Figure \ref{fig:error_components}, LBLY tends to increase more gently in last layer because our method is based on the number of filters,leads to small reconstruction error than NM with considering relationship between pruned filter and only one remained filter.\cite{NM} 




% \begin{figure*}[t]
% 	\centering
% 	{
%     \includegraphics[height=2.5mm]{./figure/key.pdf}	\vspace{1mm}

% 		\begin{tabular}{cccc}
% 			\subfigure[\label{fig:cifar:a}2 tasks] {\includegraphics[height=32mm]{./figure/LBYL_figure_2}}  & 
% 		\end{tabular}
% 		\vspace{-3mm}
% 		\caption{Comparison on the accuracy for each incremental task using CIFAR-100}
% 		\label{fig:cifar}
% 	}

% \end{figure*}



% \begin{table}[]
% {\scriptsize

% \begin{tabular}{c||c|c|c||c|c|c||c|c|c||c|c|c}  \Xhline{2\arrayrulewidth}
% \multicolumn{13}{c}{\textbf{ResNet-50 (Acc. 73.27) }}
% \\ \Xhline{2\arrayrulewidth} %\hline
% Criterion & \multicolumn{3}{c||}{L2-norm} & \multicolumn{3}{c||}{L2-GM} & \multicolumn{3}{c||}{L1-norm}& \multicolumn{3}{c}{Random}\\ \hline
% Pruning Ratio& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune
% \\ \Xhline{2\arrayrulewidth}
% 10\%& \textbf{77.31}& 77.28& 75.14& \textbf{77.20}& 76.92& 74.49& \textbf{77.55}& 77.21& 75.07& \textbf{76.14}& 72.18& 58.12\\ \hline
% 20\%& \textbf{73.24}& 72.73& 63.39& \textbf{73.46}& 72.32& 63.54& \textbf{73.20}& 72.24& 61.84& \textbf{70.59}& 59.62& 26.87\\ \hline
% 30\%& \textbf{66.63}& 64.47& 39.96& \textbf{67.07}& 64.01& 39.01& \textbf{66.30}& 63.07& 35.77& \textbf{62.22}& 39.45& 2.59 \\ \hline
% 40\%& \textbf{51.46}& 46.40& 15.32& \textbf{50.60}& 46.17& 13.14& \textbf{48.07}& 45.98& 12.59& \textbf{41.90}& 17.77& 3.08 \\ \hline
% 50\%& \textbf{28.74}& 25.98& 5.22& \textbf{30.66}& 23.44& 4.32& \textbf{22.06}& 21.98& 4.25& \textbf{22.07}& 7.17 & 2.20 \\ \Xhline{2\arrayrulewidth}
% Average Acc.& \textbf{59.48}& 57.37& 39.81& \textbf{59.80}& 56.57& 38.90& \textbf{57.44}& 56.10& 37.90& \textbf{54.58}& 39.24& 18.57 \\ 
% \Xhline{2\arrayrulewidth}
% \end{tabular}
% }
% \caption{Compenssation results of ResNet-50 on CIFAR-100}
% \label{tab:ResNet50-CIFAR100}

% \end{table}