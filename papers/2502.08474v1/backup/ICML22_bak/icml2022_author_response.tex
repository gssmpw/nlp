\documentclass{article}

% Please use the following line and do not change the style file.
\usepackage{icml2022_author_response}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}       % hyperlinks
\usepackage{booktabs} % for professional tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{lipsum}
\usepackage{caption}
\newcommand{\smalltitle}[1]{ \vspace{1mm}{\noindent\textbf{#1.}\hspace{1mm}}}

\newcommand{\lee}[1]{{\color{blue} \sc \small Lee: #1}}

\begin{document}
% Uncomment the following line if you prefer a single-column format
%\onecolumn
Thank you for the constructive feedback of all the reviewers. 

% To best appreciate all the reviewers' comments, we response to each of the detailed questions below.

\subsection*{Response to Reviewer \#3}

% \smalltitle{Low accuracy at higher pruning ratios}
% \lee{In the experiment of vanilla feed-forward neural network, our method remarkably shows low accuracy drop at higher pruning ratios compared to the experiments of CNNs, which means LBYL can be applied to various models. Moreover, our method has a strength in terms of preprocess of compressing pre-trained model without any resources (e.g., time, computing resource, training data) and can also be useful when the final accuracy is not the most critical factor, but the model size reduction is most important (e.g., recommendation for mobile apps).}


\smalltitle{About strong assumptions of LBYL}
As the problem tackled in this paper is very challenging, we had to introduce some assumptions for a training-free and closed-form solution. Although we significantly relax the assumption made by the existing works such as NM (Kim et al., NeurIPS 2020), we agree that it would be a nice future work if we incorporate how the data-dependent variables are likely to be distributed in practice.

\smalltitle{Comparison with more baselines}
In Table 4, we carefully choose Coreset (Mussay et al., 2020) as another SOTA competitor because it is a \textit{data-independent} pruning method without using any data-dependent values for evaluating the importance of each filter. In contrast, most of the other channel pruning methods suggest their own pruning criteria relying on data-dependent activation maps. Since our method neither employs any data nor focuses on devising a new pruning criterion, we think Coreset would be a proper competitor to investigate the pure effect of finetuning regardless of pruning criteria. 

\smalltitle{Ablation study}
By following the reviewer's suggestion, we have conducted an ablation study using ResNet-34 with ImageNet for L2-norm criterion, as shown in the following table.
\begin{table}[h]
\scriptsize
\centering
\begin{tabular}{cccc}
\multicolumn{1}{c||}{Pruning Ratio} & \multicolumn{1}{c|}{Best Acc.} & \multicolumn{1}{c||}{$\lambda_1$ Zero} & $\lambda_2$ Zero \\ \hline \hline
\multicolumn{1}{c||}{10\%}& \multicolumn{1}{c|}{69.22}& \multicolumn{1}{c||}{68.23}& 64.83\\ \hline
\multicolumn{1}{c||}{20\%}& \multicolumn{1}{c|}{62.49}& \multicolumn{1}{c||}{58.68}& 46.74\\ \hline
\multicolumn{1}{c||}{30\%}& \multicolumn{1}{c|}{47.59}& \multicolumn{1}{c||}{39.11}& 23.99\\ \hline 
\end{tabular}
% \label{tab:ablation}
\vspace{-1mm}
\end{table}
It is well observed that both $\lambda_1$ and $\lambda_2$ are necessary to achieve the best performance. Particularly, minimizing AE by the regularization term $||\mathbf{s}||$ is a bit more effective for recovery than minimizing BE. This experimental result can be added in our final supplement material.

% In Figure 3, we show the extent of three error terms (i.e., RE, BE and AE), which together contribute to the reconstruction error. Also, b

\subsection*{Response to Reviewer \#4}
\smalltitle{Why not $\mathbf{\tilde{A}}$ in Equation (6)}
In Section 4.1, we concentrate on the case of a single pruned filter (i.e., $j$-th filter) unlike we formulate a more general and abstract representation in Section 3.2. Thus, the damaged activation map (i.e., $\mathbf{\tilde{A}}$ of Eq. (3)) is more specifically represented as $\sum_{k=1, k\neq j}^m\mathbf{A_k}$ in Eq. (6), which is supposed to be $\sum_{k=1}^m\mathbf{A_k}$ in the original unpruned model. This specification is intended to clearly present how each of the pruned filter can be recovered by our method, as mentioned in Lines 215-218.


% \lee{We represent the $\tilde{A}$ in order to show damaged activation maps after pruning convolution filter in Eq (3). The reason why we derive the equation without representing the $\tilde{A}$ in section 4 is that we donâ€™t need to consider previous activation maps because previous activation maps can be recovered using linear assumption, which means we assume that $\tilde{A}$ is equal to $A$ after recovering process.}

\smalltitle{Title of the paper}
We agree with the reviewer that it is better to remove `without finetuning' from the title in the final version.

\smalltitle{1-mode product vs. 2-mode product}
We utilize the 1-mode product to represent the resulting tensor after pruning in the $\ell$-th layer and exploit the 2-mode product to represent the corresponding tensor in the $(\ell+1)$-th layer due to the pruning in the $\ell$-th layer. Both are just mathematical expressions that precisely formulate what happens to tensors in the $\ell$-th and $(\ell+1)$-th layers after pruning filters of the $\ell$-th layer, and therefore do not make any difference in terms of loss analysis.


\smalltitle{Comparison on higher pruning rates}
It is true that the higher the pruning rates, the more difficult to recover the pruned networks without using any data for finetuning. At the same time, however, please note that LBYL more significantly works better than the existing methods in such a higher pruning rate, as also mentioned by Reviewer \#3. Furthermore, as shown in Table 4 and Figure 4, LBYL (1) reaches a fairly practical level of the accuracy (i.e., 71.15 \%) even with a pruning ratio 50\% if we finetune the pruned model just for 20 epochs, and (2) clearly outperforms all the other methods including a SOTA data-independent pruning method, Coreset (Mussay et al., 2020).

\smalltitle{Why linear combination in LBYL assumption}
This is due to our empirical observation where filters of pretrained CNNs tend to be linearly independent to each other. It is our hope to see more following works trying a more sophisticated combination that can improve the recovery performance.

\subsection*{Response to Reviewer \#6}
\smalltitle{About the assumption of Residual Error}
In Line 227 (right), we found our explanation might have made readers a bit misleading, and hence we would like to revise the sentence as follows: ``Since $\mathbf{A}^{(\ell-1)}$ is data-dependent yet independent to pruned filters, it can be regarded as a constant factor of the reconstruction error that depends on which filters are pruned. Thus, it suffices to minimize the other part of the term, which we call \textit{Residual Error} (\textit{RE}) denoted by $\mathbf{\mathcal{E}}$.'' Thus, once input data is determined, $\mathbf{A}^{(\ell-1)}$ is also fixed, and is not changed by which filters we prune and recover with respect to the $\ell$-th layer. In fact, without BN and activation function, there would be no approximation error as long as we can minimize RE to be 0.

% It is possible to minimize the reconstruction error equation in line 227(right) using any assumption as mentioned feedback of Reviewer $\#3$ or random input data between pre-trained model and pruned one. However, we would like to demonstrate that reconstruction error without any data also can be minimize actual reconstruction error in Eq. 4. As shown in Figure  3-(c), we  empirically verify that performance of pruned model can be recovered by minimizing data-free loss function consisting of error components (RE ,BE, AE).

\smalltitle{Dropping $\mathbf{W}$ from Eq. (7) to Eq. (8)}
$\mathbf{W}$ in Eq. (7) is a common factor of the error part (i.e., $(\mathbf{A}_j - \sum_{k\neq j}s_k\mathbf{A}_k)$), and therefore it is not damaged by pruning the $j$-th filter.

% \lee{We don't intend to minimize reconstruction error considering the weight W because it can be led to further assumption or adjustment for BN and Activation layer. }

% \smalltitle{Approximation quality in Residual Error}
% % i think this question is same question about the assumption residual error..

\smalltitle{Performance with finetuning}
Although we did not finetune all the pruned models in Tables 1, 2, and 3 in the sense that this paper focuses on restoring those models without finetuning, we have also tested whether such a finetuning can further improve the performance and how much LBYL works well therein. In Section 5.4, LBYL outperforms its competitors particularly including a SOTA data-independent pruning method, Coreset (Mussay et al., 2020).


% \lipsum
% \lipsum
\end{document}
