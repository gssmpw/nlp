\section{Related Works} \label{sec:related}
Many filter pruning methods have been studied without considering such a case where we cannot use any data, whether real or synthetic, to restore a pruned network. They mostly go through an expensive retraining process either iteratively \cite{Soft,FPGM,Dynamic} or lastly as a fine-tuning phase \cite{GlobalRanking,Importance,NISP}. In this paper, we focus on the opposite case, which is more challenging in practice, where fine-tuning with any data is not available.

\smalltitle{Pruning with less fine-tuning} 
Most of the existing filter pruning methods have tried to avoid an expensive fine-tuning process by means of a carefully designed criterion for identifying unimportant filters such that the loss of information is minimized when they are pruned. To this end, they often make the best use of data-dependent values like channels and gradients that can be obtained by making inferences with some data. \citet{Thinet} propose a greedy algorithm that prunes the filters whose corresponding channels minimize the layer-wise reconstruction error. Similarly, \citet{Lasso} formulate this problem of channel selection as lasso regression and least square reconstruction. \citet{CoreSet_ICLR} introduce data-independent pruning criterion based on coreset together with an intermediate recovery method without training in order to reduce the overhead of fine-tuning. Despite their proposed methods on effective channel or neuron selection, a few epochs of fine-tuning process as well as some training data is inevitable for the pruned network to be sufficiently recovered. 


\smalltitle{Pruning with less fine-tuning and less data}
For the case of pruning with limited data, several methods \cite{CURL,Reborn} utilize knowledge distillation \cite{Knowledge_Distilation} not to use the entire original data in the recovery process. \citet{CURL} recently propose \textit{CURL} that globally prunes filters using a KL-divergence based criterion and perform knowledge distillation with a small dataset as a fine-tuning process. \citet{Reborn} tackle the similar problem and devise a way of transforming all the filters in a layer into compact ones, called \textit{reborn filters}, using only the limited data. These methods do not use the entire original data, but they still require some kinds of retraining process with a small amount of data. Meanwhile, there is a branch of data-free pruning methods \cite{Data-Free-NetworkPruning, DeepInversion} that still fine-tune the pruned model with synthetically generated data but not real data. In addition to the heavy fine-tuning process, these methods require another non-trivial training process to build a generative model for obtaining synthetic data.

\smalltitle{Pruning with no fine-tuning and no data} In the literature of filter pruning, there are only a couple of works \cite{NM, Data-free} that adopt the same problem setting as ours, that is, pruning filters without any data and fine-tuning. Both methods are based on the strong assumption that we can always find a pair of filters that are quite similar to each other and therefore a pruned filter can be compensated by its similar unpruned one. However, this assumption does not hold in modern convolutional neural networks (CNNs) having a number of filters in a fairly deep architecture. As even presented in \cite{NM}, the cosine similarity between each filter and its nearest one gets extremely low (\textit{e.g.}, around 0.2) in back layers to the point that it is better to do nothing for some pruned neurons that do not have sufficiently similar preserved neurons. In this paper, we remedy this problem by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works.

% \smalltitle{



% \lee{ \citet{RedPlus} focuses on reducing the number of computations by changing identical operations to identical memory access to a unique operation. That is, \citet{RedPlus} needs the hypothesis that memory access and allocation should run faster than mathematical operations, while our LBYL method do not need a specific hardware assumption. Therefore \citet{RedPlus} is orthogonal to our method. In this paper, we remedy problem of similarity based compensation \cite{NM, Data-free} by using as many preserved filters as possible to compensate for each pruned filter, and thereby propose a more robust method that successfully relaxes the assumption made by these existing works.}
