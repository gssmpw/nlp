\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title (Leave Before You Leave: Data-Free Restoration of Pruned Neural Networks Without Fine-Tuning)
/Author ()
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
\usepackage{color} %(if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Leave Before You Leave: Data-Free Restoration of Pruned Neural Networks Without Fine-Tuning}
\author{
    %Authors
    % All authors must be in the same font size and format.
}
\affiliations{
    % %Afiliations
    % \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % % If you have multiple authors and multiple affiliations
    % % use superscripts in text and roman font to identify them.
    % % For example,

    % % Sunil Issar, \textsuperscript{\rm 2}
    % % J. Scott Penberthy, \textsuperscript{\rm 3}
    % % George Ferguson,\textsuperscript{\rm 4}
    % % Hans Guesgen, \textsuperscript{\rm 5}.
    % % Note that the comma should be placed BEFORE the superscript for optimum readability

    % 2275 East Bayshore Road, Suite 160\\
    % Palo Alto, California 94303\\
    % % email address must be in roman text type, not monospace or sans serif
    % publications22@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name,\textsuperscript{\rm 1}
%     Second Author Name, \textsuperscript{\rm 2}
%     Third Author Name \textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1} Affiliation 1\\
%     \textsuperscript{\rm 2} Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% END REMOVE bibentry\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{appendix}
\usepackage[linesnumbered, lined, boxed, commentsnumbered, ruled, vlined]{algorithm2e}
\usepackage[switch]{lineno} %
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage{kotex}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{makecell}
% \usepackage{algorithm,algpseudocode}
\usepackage{amsthm}

\newcommand{\eat}[1]{}
\newcommand{\TODO}[1]{{\color{red} \sc \scriptsize ToDo: #1}}
\newcommand{\dchoi}[1]{{\color{blue} \sc \small Choi: #1}}
\newcommand{\lee}[1]{{\color{orange} \sc \small Lee: #1}}
\newcommand{\kim}[1]{{\color{teal} \sc \small Kim: #1}}
\newcommand{\smalltitle}[1]{ \vspace{0.5mm}{\noindent\textbf{#1.}\hspace{0.5mm}}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}


\def\T{\mathcal{T}}
\def\D{\mathcal{D}}
\def\M{\mathcal{M}}
\def\L{\mathcal{L}}
\def\S{\mathcal{S}}
% \def\A{\mathcal{A}}
% \def\Z{\mathcal{Z}}
\def\F{\mathcal{F}}
\def\N{\mathcal{N}}
\def\E{\mathcal{E}}
\def\B{\mathcal{B}}
\def\R{\mathcal{R}}
\DeclareMathOperator*{\argmin}{arg\,min}


\begin{document}

\section{General response}
Thank you for the constructive feedback of all the reviewers. Even though all reviewers find our work is theoretically solid and easy to read, they commonly request us to compare our method with additional related baselines. As a rebuttal summary, we first would like to address this major concern as follows.

\smalltitle{Comparison with more baselines}
Reviewers suggest to compare LBYL with more baselines, but most of them are either weight pruning, the pruning methods using data (synthetic or real), or the methods essentially requiring fine-tuning. There are indeed a number of pruning methods in the literature, but they cannot be fairly compared to ours as most of them need to re-train the pruned network with data as a post-processing step. Please note that our work is not focused on designing a new (data-independent) filter pruning criterion, but intends to devise another post-processing recovery method neither using any single data samples nor fine-tuning the pruned network. Our recovery method can be applicable to any pruned networks instead of fine-tuning as long as we are given their original one.

To best appreciate all the reviewers' comments, we response to each of the detailed questions below.

%%% To our best knowledge, only a couple of works have been proposed (Kim et al. 2020; Srinivas and Babu 2015) in the same problem setting as ours, and we thoroughly tested the performance of our method, compared to the state-of-the-art one (Kim et al. 2020). Furthermore, we also conducted an ablation study to see whether our method can be effective as a preprocessing stage for fast fine-tuning process, as shown in Table 4. Nevertheless, we generally agree with the reviewers that we have to include some missing references in the related work section, which should be done in our final version.

%%% To best appreciate all the reviewers' comments, we response to each of the detailed questions below.

\section{Response to Reviewer 1}
% In my opinion, this work misses some of the data independent neural pruning baselines, which involve coreset based layer-wise approximation in general. I recommend authors add a section in the related work, as in how their work is similar/different to the existing neural pruning works using coreset (for both the filter and fully connected layers). In particular, please have a look at [1/2]. [1] Data-Independent Structured Pruning of Neural Networks via Coresets (IEEE TNNLS) [2] Data-Independent Neural Pruning via Coresets (ICLR'20).
\smalltitle{Q1} Data-Independent Pruning via Coresets (ICLR'20 and TNNLS) is a weight pruning method, not a filter pruning method. As remarked in our introduction, this work deals with only structured pruning, and proposes a recovery method for models pruned by some criterion of structured pruning.

% Where is the closed-form expression for the case when we prune more than 1 filter?
\smalltitle{Q2} We iteratively use the closed-form solution for each single pruned filter to form the delivery matrix in each layer as shown in Algorithm 1. This process would restore the loss caused by multiple pruned filters in each layer.

%How does this scale to pruning and restoring the fully connected layers? I think this should be mentioned, in an appendix (with appropriate pointers to the main paper)
\smalltitle{Q3} We did not prune any neurons in FC layers in this paper, but actually our method would easily be extended to FC layers like NM (Kim et al. 2020).

% Is there any expression for the activation error, in general with an activation function \phi which is just a monotone non-decreasing function with some properties (such as Lipschitz constant?)
\smalltitle{Q4} It is challenging to approximate any non-linear activation function, and therefore we consider only ReLU in this work as shown by AE of Theorem 1. It is our future work to incorporate other activation functions than ReLU.

% Since the final expression doesn't involve the activation error, the paper just uses the L2 norm penalty. While in the case of no batch norm it may seem intuitive that reducing s will reduce the "AE", but with the batch norm, how can one be sure that reducing s is a good choice? Moreover, how does L2 make sure that s are uniform? Two weight vectors, one being uniform and the other being skewed can have equal L2 norms.
\smalltitle{Q5} In Theorem 2, we aim to minimize three error terms (i.e., RE, BE, and AE). Batch normalization (BN) can perfectly be managed by our BE term as BN is just another linear function. What is challenging here is minimizing AE, that is, a data-dependent error term. To this end, we discovered that minimizing the scale of $s$'s is effective for mitigating AE. For example, one abnormally large $s$ is hard to be cancelled out by the other $s$'s in AE. Meanwhile, in the ridge regression, L2 regularization shrinks the values of parameters in a way that larger values would have more severe penalty (i.e., forced to shrink more) than L1 regularization. In this sense, the L2 regularization would make the coefficients be more uniform whereas L1 regularization will make them more sparse.

% and the degree of minimizing the scale (i.e., $\lambda_2$ in equation (10)) is decided with the relationship with other terms (i.e., ``RE'', ``BE'') to minimize the reconstruction error optimally.


% It may be true that considering the L1 absolute error eases the mathematical analysis, to get the final closed-form expression, but have the authors tried using L2 norm square error, and using gradient descent (in case it doesn't lead to a closed-form expression)?
\smalltitle{Q6} As described in Theorem 2, our closed form solution is actually to minimize Eq. (10) consisting of three L2 norm errors, namely $\|\textbf{RE}\|_2^2$, $\|\textbf{BE}\|_2^2$, and a regularization term $\|\mathbf{s}\|_2^2$ instead of AE. Furthermore, our finally derived loss of Theorem 1 is not necessarily L1 norm. Every lemma and theorem still holds even if we use L$P$ norm for any $P \in [1, \infty]$. This is because we did not use any specific properties of L1 norm in our proofs (please refer to Appendix).

% In the definition of residual error, if instead of considering the plain linear combination of remaining filters, one considered using s as a matrix, then can one approximate the error much better (when used say gradient descent to optimize)?
\smalltitle{Q7} This suggestion is indeed interesting for our future work.


% In the tables, does the pruning ratio only consider filters? In the case of VGG-16 most of the parameters are in fully connected layers, so does the pruning ratio only consider the CNN filters? The fact that fully connected layers involve a huge number of parameters further enforces to have an algorithm for linear layers.
\smalltitle{Q8} We prune only convolution filters in CNNs. 
% we prune final convolutional filter therefore we compensate first fully connected layer like NM in VGG-16.


\section{Response to Reviewer 2}
% As far as I know, model restore often leads to distribution variation in output, which demands re-calculation of parameters in batch normalization (mean/variance) thus data inference. Does the proposed method meet similar problems
\smalltitle{Q1} In Theorem 1, the component related to the Batch Normalization (i.e., $\boldsymbol{\B}$) in the reconstruction error is considered as a constant. Therefore, we need not re-calculate the batch normalization parameters which are dependent on data.

%%% We make the reconstruction loss capable of re-scaling batch normalization parameters by the BE term.

% May I consider the proposed method as reassignment of remaining channels (features) to compensate pruned channels? If so, I think [1] should be considered as a comparison method as they share the similar idea that the remaining features are a linear combination of the origin (remaining or pruned) channels.
\smalltitle{Q2} As already discussed in our related work section, [1] (He, Zhang, and Sun 2017) is a data-dependent pruning method with fine-tuning.

% Does further fine-tuning improve the restored pruned model?
\smalltitle{Q3} In Table 4 and Figure 4, our restored model shows the fast convergence speed compared to the others.

% {Detailed Feedback for the Authors}  1. [2] should also be considered as comparison, for they similarly use few (or no) data for pruning restore. [2] compensates the remaining parameters after pruning individual parameter (element-wise pruning). Though [2] focused on pruning while the submission focused on model restore after pruning. Author may compare them from a perspective of model restore.\

\smalltitle{Q4} [2] proposes a layer-wise weight pruning method, which prunes unimportant parameters to minimize the reconstruction error caused by pruning. Although [2] avoids the re-training process after pruning, they need the whole data to compute the reconstruction error, which is therefore orthogonal to our method.

%%% Our LBYL method does not rely on the filter pruning criteria, and therefore even the model pruned by [2] can further be restored by LBYL. We can include [2] as a pruning method with less fine-tuning and less data in the related work section.

%%% (revise,,?) [2] proposes a layer-wise weight pruning method,which prunes unimportant parameters with keeping reconstruction error small using training data. Although [2] avoid the re-training process after pruning, they need whole data set to compute reconstruction error.




% 2. More experiments using fine-tune for filter pruning should be listed as reference
\smalltitle{Q5} We do not suggest a new pruning criterion but propose a data-free restoration of already pruned models. Therefore, it is somewhat pointless to compare various filter pruning methods proposing their own criterion. Nonetheless, it is highly expected for LBYL to improve the final performance even after fine-tuning as reported in Table 4, not to mention that LBYL is superior to its competitors in the other experiments without fine-tuning.


% Nonetheless, it is expected that the overall performance after fine-tuning in different experiments would be similar to what is shown in Table 4 since our restoration method is superior to the other methods across the different experiments when the fine-tuning is not implemented.
%%% (revise,,?) As showed in Table 3. Our proposed method shows better performance than the others in experiment on ImageNet compared to experiment on CIFAR-100. Our method will show better performance after fine-tuning in experiment on ImageNet.




\section{Response to Reviewer 3}
% o	The related works about pruning with less data is also a practical problem setting. It is better to compare the two problem settings (pruning with no data or few data). Besides, a few training data are available, could this method achieves better performance?
\smalltitle{Q1} Our method shows the fastest convergence speed in Table 4 and Figure 4. It means that our restored model can more quickly reach to the best performance with fine-tuning.

\section{Response to Reviewer 4}
% % o	1. Insufficient comparison are done in comparison experiments, which doesn’t meet the requirements for the top meeting to publish.
% \smalltitle{Q1} As you mentioned, it is possible to utilize the synthesized data sets using generative model in order to recovery pruned model. However, our method is a "data-free" restoration method on pruned model,which means we do not use any data like synthesized images.

% o	(1) What is the limitation of this method? Can it be used to various pruned networks?
\smalltitle{Q1} As shown in Appendix, we would say our limitation is that LBYL is more effective in restoring larger CNNs than in already tiny networks like MobileNets of using 1x1 filters.

% Therefore, we will further study in order to apply our method on flop efficient convolutional filter (e.g., 1x1 conv, group conv).

% (2) The proposed method is only compared with NM and pruned, I’m confused that why NM is worse than pruned, as listed in Table 4?
\smalltitle{Q2} In terms of the final accuracy, as mentioned in the introduction, we would say there is no big difference among pruning criteria as long as we can fine-tune the network with lots of data for a very long training time (Liu et al. 2019). Our best guess on why NM becomes eventually worse than just pruned model is that NM can choose a dissimilar filter to recover each pruned one in some layers, which makes hard for fine-tuning to improve the performance. 

% Therefore, NM is hard to fine-tuned than pruned model.

% similarity based restoration like NM ignores that as a layer is deeper, pairwise similarity on filters gets extremely low. That is, 

% Our method proposes a restoration method on pruned model using filter pruning. Therefore we compared with NM,which is the latest data-free compensation method as mentioned in Related Works. 

% (3)The author should provide more comparisons with the related latest methods in the quantitative results, such as
\smalltitle{Q3} Our LBYL method is a data-free restoration method for pruned models without fine-tuning. All the works suggested by the reviewer require a heavy fine-tuning process with synthetic data (not real data though) generated by some kinds of GAN. The term \textit{data-free} we meant in this work is not training any data samples regardless of whether they are real or synthetic for the fast restoration of pruned networks.

% a huge training time and computation resource 

% The latest data-free compression methods that you mentioned is based on generative model like GANs,which requires a huge training time and computation resource while our method use a little computation compared to training generative models,which is quite practical scenarios like edge devices.

\end{document}


