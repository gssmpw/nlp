\onecolumn


\section*{Cover Letter}

\vspace{10mm}

\setcounter{table}{0}
\setcounter{figure}{0}

\renewcommand{\thetable}{C\arabic{table}}  
\renewcommand{\thefigure}{C\arabic{figure}}
\renewcommand{\thesubsection}{C\arabic{subsection}}
		
Dear ACs and reviewers for AAAI-22,

We would like to submit our paper entitled ``Leave Before You Leave: Data-Free Restoration of Pruned Neural Networks Without Fine-Tuning,'' for your consideration in AAAI-22 conference. In this version, we have made the following revisions to best appreciate all the previous comments raised by reviewers of NeurIPS-21.

\begin{enumerate}
    \item We have confirmed by new experimental results that LBYL is practically useful even when fine-tuning with data is available. Please refer to Table \ref{tab:finetune_scratch} and Figure \ref{fig:Fine tuning and From Scratch}.
    \item We have shown by supplementary experimental results on MobileNet-V2 that LBYL is also effective in tiny architectures. Please refer to Table \ref{tab:mobilenet}.
    \item We further improved our presentation quality by revising figures more descriptive, presenting a notation table in Appendix, and correcting all the typos and ambiguous expressions. Please particularly refer to Figure \ref{fig:framework} and Table \ref{tab:notations}.
    \item All the additional experimental results have been shown in either manuscript or Appendix.
\end{enumerate}


The followings are the detailed responses to each of the reviewers' comments.

\subsection*{Reviewer fQob}

\smalltitle{Comment 1}
\textit{By LBYL, the pruned network can recover the outputs of the original model with minimal approximation. But how to explain why LBYL can lead to even better performance than the original model.}

\smalltitle{Response 1}
As our goal is to approximate the layer-wise output after pruning, it would be impossible for a restored model to beat the original model. This question is probably due to the result of ResNet-50 on CIFAR-100 in Table A1 of the previous version of Appendix, where there was a typo in the table such that the accuracy of the original ResNet-50 should be 78.82, not 73.27. We have corrected this typo in this version.

\smalltitle{Comment 2}
\textit{The authors do not provide how to tackle the residual issue. How to make sure that the output is still lossless when residual connections are presented.}

\smalltitle{Response 2}
For a CNN with skip connections, we have used the common pruning scheme that prunes only first two conv layers within each residual block so that the dimension of each block's output remains the same after pruning. Furthermore, since our restoration method is only applied to each pruned layer, the output of each block would properly be restored regardless of residual connections.


\smalltitle{Comment 3}
\textit{It seems that the LBYL works well in models with much redundancy. How about the performance of trunk pruning in efficient models such as mobilenet.}

\smalltitle{Response 3}
Computationally efficient architectures such as MobileNet are already highly optimized in terms of model size, and hence pruning these compact models would lead to severe drop in accuracy. Furthermore, these tiny architectures commonly have depth-wise convolution layers, which makes us difficult to apply our restoration method in a straightforward way. Nevertheless, we have conducted a series of experiments using MobileNet-v2 using the scheme that prunes only the first layer of each block. This scheme can be seen as a naive adaptation of our method for MobileNet-v2. Table \ref{tab:mobilenet} summarizes the corresponding experimental results, where we can clearly observe that LBYL outperforms NM in most cases and is quite effective to recover the performance of pruned networks.


\smalltitle{Comment 4}
\textit{It would be nice to provide an algorithm table for a better understanding of the method.}

\smalltitle{Response 4}
Due to page limits, we have added a notation table in Appendix. Please refer to Table \ref{tab:notations}, which would further help to understand our methodology.



%%% Limitations And Societal Impact:
%%% The authors do not address the limitations and potential negative societal impact of their work. The authors can discuss how the proposed pruning method affects the development of edge AI.

% \smalltitle{Limitations and Societal Impact}
% The potential negative societal impact of our work can be to make less accurate yet lightweight models highly prevalent in the era of edge AI. This is because our method is extremely simple to implement and easy to apply to any heavyweight models even without using any data and fine-tuning.


%--> it is difficult to prune the computation efficient models such as mobile net without fine-tuning process becacues they are developed with considering the computation. However we experment the mobilenet-V2 using the scheme that prune first layer for each block and calculate the scale, compensating in the last layer. Actually, this compensation method is not fully match with our proposed method due to depthwise convolutional layer in block. but we can loose approximate the output.






\subsection*{Reviewer VxV3}

% Thank you for the very detailed and structured reviews.

%%% I don't think these implications are true. implications: The nonexistence of data implies the following two facts. First, we have to use a pruning criterion exploiting only the values of filters themselves such as L1-norm, Secondly, since we cannot make appropriate changes in the remaining filters by fine-tuning.....
%There are techniques like conserving synaptic flow... just because there's no training data doesn't mean that synthetic inputs can't be fed to the network.

\smalltitle{Comment 1}
\textit{I'd like to have seen comparisons to other data-free approaches, even if they require fine-tuning, since there can be time for fine-tuning, even if there's no data available.}


\smalltitle{Response 1}
This work is intended not to use any data regardless of whether it is real or synthetic. Nevertherless, we provide the comparison between NM and LBYL on the accuracy after fine-tuning with real data. As shown in Table \ref{tab:finetune_scratch} and Figure \ref{fig:Fine tuning and From Scratch}, we observe that LBYL converges much faster than NM in the middle of fine-tuning, and thereby achieves the best accuracy after 20 epochs of fine-tuning.





% In the CRC version, we can further clarify this issue. However, we would still like to point out the following two facts in response to your detailed comments.

% 1. \textit{conserving synaptic flow} [27] is a weight pruning (a.k.a. unstructured pruning) method, and hence it cannot be directly compared to any structured pruning methods like filter pruning that this paper focuses on. 

% 2. To the best of our knowledge, there is no existing work on fine-tuning with synthetic data a compressed model whose filters are pruned. In general, however, we agree that it would be valuable to study whether it is effective to use synthetic data generated by approaches like \textit{Deep Inversion} [2] in order to fine-tune a pruned network even though it will require heavy computation as mentioned by the reviewer.


% %%% I'd like to have seen comparisons to other data-free approaches, even if they require fine-tuning, since there can be time for fine tuning, even if there's no data available.

% Unfortunately, we have time limit for rebuttal, and therefore we cannot implement such a fine-tuning method via synthetic data for the comparison. Alternatively, we provide the comparison between NM and LBYL on the accuracy after fine-tuning with real data. As shown table below, we observe that LBYL converges much faster than NM in the middle of fine-tuning, and thereby achieves the best accuracy after 20 epochs of fine-tuning.

% \begin{table}[h]
% \centering
% \scriptsize
% \begin{tabular}{c||c|c|c} \Xhline{2\arrayrulewidth}
% Pruning ratio& LBYL& NM& Prune \\ \Xhline{2\arrayrulewidth}
% 10 \% & \textbf{78.14} $\rightarrow$ \textbf{78.3}  & 77.28 $\rightarrow$ 77.57 & 75.14 $\rightarrow$ 77.67\\\hline
% 20 \% & \textbf{76.15} $\rightarrow$ \textbf{77.22} &  72.73 $\rightarrow$ 75.62 &  63.39 $\rightarrow$ 75.4\\\hline
% 30 \% & \textbf{73.29} $\rightarrow$ \textbf{75.99} & 64.47 $\rightarrow$ 73.03 &  39.96 $\rightarrow$ 73.18\\\hline
% 40 \% & \textbf{65.21} $\rightarrow$ \textbf{74.13} & 46.4 $\rightarrow$ 69.71 &  15.32 $\rightarrow$ 70.12\\\hline
% 50 \% & \textbf{52.61} $\rightarrow$ \textbf{71.15} & 25.98 $\rightarrow$ 65.03 &  5.22 $\rightarrow$ 65.72\\\Xhline{2\arrayrulewidth}
% \end{tabular}%
% \vspace{2mm}
% \caption{Fine-tuned results of ResNet-50 on CIFAR100 using L2-norm criterion, where all the pruned models are fine-tuned for 20 epochs.}
% \label{cov:tab:fine-tuning}
% \end{table}


%%% While the writing is mostly understandable with few typos, I still may not understand how the technique works. If layer i is pruned, and the remaining filters share its duties, is the output of layer i the original size, or half-sized due to half the neurons being pruned? Relatedly, does the next layer downstream see a reduced-size input, and correspondingly have its own input dimension pruned?

\smalltitle{Comment 2}
\textit{While the writing is mostly understandable with few typos, I still may not understand how the technique works. If layer i is pruned, and the remaining filters share its duties, is the output of layer i the original size, or half-sized due to half the neurons being pruned? Relatedly, does the next layer downstream see a reduced-size input, and correspondingly have its own input dimension pruned?}

\smalltitle{Response 2}
The reviewer's understanding is indeed correct. If half of filters in layer $i$ are pruned, the output of layer $i$ is half-sized. This leads to the reduction of the number of input channels in layer $(i+1)$, what the reviewer calls the input dimension of the next layer. More specifically, if $j$-th filter of $\ell$-th layer is pruned, its corresponding channel of filters in $(\ell+1)$-th layer is consequently removed. However, it does not imply that the output size of $(\ell+1)$-th layer is also reduced, but its value is \textit{damaged} by the loss of $j$-th input channel.

In this version, we have added the following description to the caption of Figure \ref{fig:framework}:
$s, s',$ and $s^*$ are the coefficients that quantify how much each preserved filter should carry the information of the pruned filter.

We have double-checked the paper and revised all the typos therein as well.


\smalltitle{Comment 3}
\textit{You can always show top-1 or top-5 accuracy on a data set, but in the absence of such a data set, how will the deployer know when to stop pruning? There's a big accuracy gap between 10\% and 20\% pruning on the ImageNet results.}

\smalltitle{Response 3}
First of all, if there is no available data, we have no choice but to compress the target network in a data-free manner anyway, which is equally applied to both LBYL and NM. A possible way to determine when to stop pruning is to keep track of how the values of our two data-free error terms (i.e., RE and BE) are changing. As shown in Figure \ref{fig:error_components}, we can roughly guess how good the final performance would be by observing both RE and BE, which strongly contribute the reconstruction error. 

Also, our method can still be useful when we are given a target model size yet the final accuracy is not the most critical factor (e.g., recommendation app running in mobile devices).

\smalltitle{Comment 4}
\textit{This technique also seems like it would be useful as a pre-processing step before performing fine-tuning, potentially encouraging accuracy recovery or reducing the amount of fine-tuning required.}

\smalltitle{Response 4}
We agree with the reviewer that our method can also be seen as a preprocessing step before fine-tuning. As shown in Table \ref{tab:finetune_scratch} and Figure \ref{fig:Fine tuning and From Scratch}, we found that LBYL is indeed effective for fast convergence during the fine-tuning phase. 





% As you said, the accuracy of restored model decreases dramatically as pruning ratio increases. If we have a table of restored model's performances according to pruning ratio in experimental setting and the situation where our method is applied is similar with the experimental setting, the performance can be guessed through this table.

% But, when this is not the case, we suggest a naive solution to know when to stop pruning. A naive solution is to employ random noisy images(e.g., gaussian random samples) and then measure weighted average reconstruction error(WARE) between original model and pruned model. Increasing the pruning ratio and Checking the WARE, we may be able to avoid drastic loss. but this is just a rough method.

% On the other hand, our proposed methods can be used practically when users need to compress the original model and performance loss is relatively not critical.(e.g., recommendation system) For example, Choosing whether to update or not the smartphone is relatively robust to poor performance of model.


%--> As you said, it is true that accuracy of restored model in pruning ratio 30$\%$ is dropped with large margin compared to original model. Therefore we suggest a naive solution to stop whether prune more or not without using any data. A naive solution is to employ random images and then measure weighted average reconstruction error(WARE) between pre-trained model and pruned model. Based on WARE, we can choose whether to prune more or not.

%our proposed methods can practically be used when users need to compress the pre-trained model without considering performance such as recommendation system. for example, choosing whether to update or not does smartphone not affects smartphone's performance much because our method can correctly increase the performance after pruning. 




%%% Q4) Figure 1 has elements labeled 's' - these should be explained in the caption.

% Thanks for your reminder. In figure 1, $s_{i}$s mean coefficients by which a pruned filter is approximated using remained filters in l-th layer. and this is same with the coefficients by which a pruned channel is approximated using remained channels after CONV.

% Similarly, $s^{'}_{i}$s and $s^{*}_{i}$s are the coefficients by which a pruned channel is approximated using remained channels after BN and ReLU, respectively.




% \begin{table}[]
% \centering
% \normalsize
% \begin{tabular}{c||c|c|c|c} \Xhline{2\arrayrulewidth}
% Pruning ratio& OURS& NM& Prune & From Scratch (80epochs) \\ \Xhline{2\arrayrulewidth}
% 10 \% & \textbf{78.3}   & 77.57  & 77.67  & 48.22 (72.07)\\\hline
% 20 \% & \textbf{77.22}  &  75.62  &  75.4   & 47.85 (72.01)\\\hline
% 30 \% & \textbf{75.99}  & 73.03  &  73.18  & 46.7 (72.24)\\\hline
% 40 \% & \textbf{74.13}  & 69.71  &  70.12  & 46.53 (72.88)\\\hline
% 50 \% & \textbf{71.15}  & 65.03  &  65.72  & 44 (69.33)\\\Xhline{2\arrayrulewidth}
% \end{tabular}%
% \vspace{2mm}
% \caption{Fine-tuned results of ResNet-50 on CIFAR100 using L2-norm criterion. We fine-tune pruned and restored models for 20 epochs to compare methods after fine-tuning, where we use a Nesterov SGD optimizer with 0.9 momentum and the initial learning rate to 0.00001 divided by 10 at 5, 10 and 15 epochs. In case of from scratch, we train the randomly initialized model for 20 epochs where we use a Nesterov SGD optimizer with 0.9 momentum and the initial learning rate to 0.1}
% \label{tab:params:Fine-tuned results of ResNet-50 on CIFAR100 using L2-norm criterion}
% \end{table}




% show the comparison between our method and other data-free approaches that uses synthetic data for fine-tuning. Alternatively, we provide the comparison result between NM and our method LBYL after fine-tuning. the result is shown below. Table \ref{tab:param:ResNet50-CIFAR100 using l2-norm} presents that 





% You mentioned about "conserving synaptic flow"[27], but as far as I know, that method is introduced for weight pruning. It is possible to prune convolutional filters using weight pruning methods but pruning filters with weight pruning can't guarantee performance of compressed model because many kinds of weight pruning methods rely on fine-tuning. On the contrary, filter pruning methods try to reduce the fine-tuning as mentioned in related works in our paper. Therefore as mentioned in our paper, we focused on the filter pruning not weight pruning. 

% However, I think it will be a valuable study that uses fine-tuning via synthetic data generated by pre-trained generating models although it may require a lot of computation time as you mentioned. and If that method is applicable in filter pruning, our second sentence "since we cannot make appropriate changes in the remaining filters by fine tuning" should be changed as you mentioned.










%--> Thank for your guidance, In figure 1, $s_{i}$ means coefficient considering the linearity between preserved filters and the pruned filter and $s^{'}_{i}$ means coefficient considering both linearity and a batch normalization layer and $s^{*}_{i}$ means optimized scales considering both linearity and layers such as batch normalization and activation layer.

% \smalltitle{Q5}
% %%% Q5) Typos noticed

% Thanks for reminder. We will correct all typo you said in the final CRC version. 




% \begin{table}[]
% \centering
% \normalsize
% \begin{tabular}{c||c|c|c} \Xhline{2\arrayrulewidth}
% Pruning ratio& OURS& NM& Prune \\ \Xhline{2\arrayrulewidth}
% 10 \% & \textbf{78.14}   & 77.28  & 75.14  \\\hline
% 20 \% & \textbf{76.15}  &  72.73  &  63.39 \\\hline
% 30 \% & \textbf{73.29}  & 64.47  &  39.96  \\\hline
% 40 \% & \textbf{65.21}  & 46.4  &  15.32  \\\hline
% 50 \% & \textbf{52.61}  & 25.98  &  5.22  \\\Xhline{2\arrayrulewidth}
% \end{tabular}%
% \vspace{2mm}
% \caption{Recovery results of ResNet-50 on CIFAR-100 using L2-norm criterion}
% \label{tab:param:ResNet50-CIFAR100 using l2-norm}
% % \end{table}
% % \begin{table}[]
% \centering
% \normalsize
% \begin{tabular}{c||c|c|c|c} \Xhline{2\arrayrulewidth}
% Pruning ratio& OURS& NM& Prune & From Scratch (80epochs) \\ \Xhline{2\arrayrulewidth}
% 10 \% & \textbf{78.3}   & 77.57  & 77.67  & 48.22 (72.07)\\\hline
% 20 \% & \textbf{77.22}  &  75.62  &  75.4   & 47.85 (72.01)\\\hline
% 30 \% & \textbf{75.99}  & 73.03  &  73.18  & 46.7 (72.24)\\\hline
% 40 \% & \textbf{74.13}  & 69.71  &  70.12  & 46.53 (72.88)\\\hline
% 50 \% & \textbf{71.15}  & 65.03  &  65.72  & 44 (69.33)\\\Xhline{2\arrayrulewidth}
% \end{tabular}%
% \vspace{2mm}
% \caption{Fine-tuned results of ResNet-50 on CIFAR100 using L2-norm criterion. We fine-tune pruned and restored models for 20 epochs to compare methods after fine-tuning, where we use a Nesterov SGD optimizer with 0.9 momentum and the initial learning rate to 0.00001 divided by 10 at 5, 10 and 15 epochs. In case of from scratch, we train the randomly initialized model for 20 epochs where we use a Nesterov SGD optimizer with 0.9 momentum and the initial learning rate to 0.1}
% \label{tab:params:Fine-tuned results of ResNet-50 on CIFAR100 using L2-norm criterion}
% \end{table}

\subsection*{Reviewer AsvP}

% Thank you for the positive feedback and insightful comments that we have not thought about.

\smalltitle{Comment 1}
\textit{Would like to see analysis on how merged filters are changed to accommodate the information from pruned filters. How much information in the un-pruned kernel is lost as a result of the process? Does classic kernel behavior in terms of learning specific features hold after the kernel merging process?}

\smalltitle{Response 1}
Even though it is not trivial to quantify how much information in the preserved filter is lost as a result of the restoration process, we can claim that such a side effect is not much in LBYL, compared to NM. This is because LBYL minimizes the amount of those changes in remaining filters by making as many filters as possible to participate in the restoration process. On the other hand, NM forces each pruned filter to deliver its information to only one remaining filter, which can dramatically change the role of the remaining filter. As shown the table below, the average and maximum of absolute scaling factors (i.e., coefficients) of LBYL are much smaller than those of NM. More specifically, each un-pruned filter in LBYL is multiplied by only 0.00005 on the average, and consequently the information loss of the filter cannot be significant.
\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{c||c|c}\Xhline{2\arrayrulewidth}
Statistic& LBYL & NM \\ \Xhline{2\arrayrulewidth}
Mean & 0.00005 & 0.00045 \\ \hline
Max & 0.03977 & 0.34971 \\ \hline
Min & 0 & 0 \\ \hline
\end{tabular}%
\label{cov:tab:scale}
\vspace{2mm}
\caption{Comparison on absolute scale coefficients between LBYL and NM}
\end{table}



\smalltitle{Comment 2}
\textit{One limitation of this method is that it looks at each layer individually when recovering pruned kernel information -- it might be interesting to see if this method could not be generalized for compatible cross-layer information recovery.}

\smalltitle{Response 2} 
Although our strategy is to minimize the layer-wise output error in this paper, it would be a promising future work to incorporate cross-layer information as well to further optimize the restoration process.



%that the large scales can lead to distortion of remained filter's property.


%--> Original property on remained filters is changed to capture features that the pruned filter captured. However, the amount of changes in remained filters is very small than NM because we assume linearity between pruned filter and preserved filters. That means our method give a appropriate information on pruned filter to the remained filters compared to NM that give a scale based on norm ratio between pruned and remained filter. In our github, we will show that scale on pruned filter is very small than NM that the large scales can lead to distortion of remained filter's property.


% \begin{figure}[!htbp]
% \centering
%   \includegraphics[width=14cm]{Figure_OURS}
%   \hfil
% \caption{OURS scale}
% \label{Figure_OURS}
% \end{figure} 

% \begin{figure}[!htbp]
% \centering
%   \includegraphics[width=14cm]{Figure_NM}
%   \hfil
% \caption{NM scale}
% \label{Figure_NM}
% \end{figure} 



\subsection*{Reviewer VcLn }

\smalltitle{Comment 1}
\textit{Understanding the entire idea took me a long time due to the constant switching between layers, notation (A vs Z) etc ... . I think this can be done in a much cleaner way.}

\smalltitle{Response 1}
We would like to point out that most reviewers other than this one commonly said our manuscript is well written and easy to understand.

Having said that, we have provided the table of notations frequently used throughout the paper in Appendix, which would help to understand how our method works. We have attached our notations in Figure 1 as well in order to clarify what each notation actually means in Figure \ref{fig:framework}. For example, $\mathbf{Z}^{(\ell)}$ are illustrated by blue squares, and $\mathbf{A}^{(\ell)}$ are represented by green squares in Figure 1.



\smalltitle{Comment 2}
\textit{While the experiments show it works better than a method based on a similar idea, it is not clear to me whether the approach works any better than training a smaller network to start with. (in essence training a smaller model from scratch). It would also be interesting to see how it compares against fine-tuning, or whether this approach can benefit from fine tuning.}

\smalltitle{Response 2}
Although our method is motivated by a data-free scenario and therefore is compared to another data-free pruning method, we totally agree with the reviewer that it is essential to test whether our method is also effective when the data becomes available later on. Therefore, we additionally present new experimental results in Table \ref{tab:finetune_scratch} on the final accuracy of each pruned or restored model after fine-tuning along with that of the same-sized architecture trained from scratch. In all the cases, our LBYL method outperforms the competitors particularly when the pruning ratio increases. In addition, as shown in Figure \ref{fig:Fine tuning and From Scratch}, LBYL shows the fastest convergence speed during the fine-tuning process. Both of these observations justify the fact that our LBYL method is useful in practice even when the data is available.


% Our approach is essentially based on data-free assumption. so our method may have benefit compared to the smaller network that requires fine-tuning in the sense that our method can be used when the training data is not available or the training data is available but the computing resource is constrained.(e.g., mobile device...)

% Even when the training data and fine-tuning process are available, our method can have benefit compared to the smaller network which is trained from scratch in the sense that the restored model converge much more faster than From Scratch model shown in the table \ref{tab:params:Fine-tuned results of ResNet-50 on CIFAR100 using L2-norm criterion}.

% Our method can be used when the model compression is needed but training data is not available or heavy computation is burdensome. But as reviewer 2 mentioned, you may wonder when to stop the pruning in real world because the accuracy of restored model decreases dramatically as pruning ratio increases. In this case, 
% If we have a table of restored model's performances according to pruning ratio in experimental setting and the situation where our method is applied is similar with the experimental setting, the performance can be guessed through this table.

% But, when this is not the case, we suggest a naive solution to know when to stop pruning. A naive solution is to employ random noisy images(e.g., gaussian random samples) and then measure weighted average reconstruction error(WARE) between original model and pruned model. Increasing the pruning ratio and checking the WARE, we may be able to avoid drastic loss. but this is just a rough method.

% On the other hand, our proposed methods can be used practically when users need to compress the original model and performance loss is relatively not critical.(e.g., recommendation system) For example, Choosing whether to update or not the smartphone is relatively robust to poor performance of model.

% Or, As reviewer 2 mentioned and shown in the table \ref{tab:params:Fine-tuned results of ResNet-50 on CIFAR100 using L2-norm criterion}, our method can also be used as the pre-processing step before performing fine-tuning. We found the converge time is reduced by comparison with NM and From Scratch.

% \kim{continued}
%--> Our method shows better performance compared to randomly initialized model with same fine-tuning epochs. However, if we train predefined small model from scratch during a lot of epochs, result will show similar performance regardless of restored methods because a few works[1,2] show that randomly initialized small models reaches the similar performance regardless of pruning methods. 
%Actually, performing the fine-tuning process assumes that there are sufficient data and computation such as GPU. Unfortunately, in practice, these assumptions are not always satisfactory. Therefore, our proposed method can be used in the following situations. Our method can be used in resource-constrained devices such as edge device and also pre-processing step before fine-tuning as the reviewer3 said too.

%[1] rethinking of network pruning
%[2] pruning from scratch 





