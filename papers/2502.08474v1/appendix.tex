\onecolumn

\section*{Appendix \\``Training-Free Restoration of Pruned Neural Networks''} \label{sec:supp}

% \vspace{10mm}

\setcounter{table}{0}
\setcounter{figure}{0}

\renewcommand{\thetable}{A\arabic{table}}  
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thesubsection}{A\arabic{subsection}}

In this appendix, we first present the proofs of Lemma \ref{lem:bn}, Theorem \ref{thm:relu}, and Theorem \ref{thm:closedform}, respectively. Then, we describe more details about our experimental results including the information about the pretrained model and the values of hyperparameters.

\subsection{Notation Table}
Let us first present the following table of notations frequently used throughout the manuscript and Appendix.

\begin{table}[h]
\centering
\begin{tabular}{l||l}
\hline
$\mathbf{W}^{(\ell)}$ and $\Tilde{\mathbf{W}}^{(\ell)}$                 &  original and damaged (due to pruning) filters in $\ell$-th layer            \\ \hline
$\mathbf{A}^{(\ell)}$ and $\Tilde{\mathbf{A}}^{(\ell)}$         & original and damaged (due to pruning) activation maps in $\ell$-th layer                                 \\ \hline
$\mathbf{Z}^{(\ell)}$  and $\Tilde{\mathbf{Z}}^{(\ell)}$         & original and damaged (due to pruning) feature maps in $\ell$-th layer                                    \\ \hline
$\N(\cdot)$                   & batch normalization function                                     \\ \hline
$\F(\cdot)$                   & activation function such as ReLU                                             \\ \hline
$\boldsymbol{\S}$                         & pruning matrix or delivery matrix                                        \\ \hline
$\gamma, \beta, \sigma, \mu$                    & batch normalization parameters              \\ \hline
$\boldsymbol{\E}$                         & residual error (RE)                                       \\ \hline
$\boldsymbol{\B}$                         & batch normalization error (BE)                              \\ \hline
$\boldsymbol{\R}$                         & activation error (AE)                              \\ \hline
$\mathbf{s}$                              & scale coefficients for preserved filters to restore pruned filters              \\ \hline
\end{tabular}%
\vspace{2mm}
\caption{Table of notations}
\label{tab:notations}
\end{table}


\subsection{Proofs on Our Theoretical Results} \label{sec:appendix:proof}
For the ease of presentation, we assume the operators like addition `$+$', subtraction `$-$', and minimum `$\min(\cdot)$' or maximum `$\max(\cdot)$' to support \textit{broadcasting} as well as \textit{element-wise}. For example, given two equal-sized matrices $A$, $B$ and a constant $b$, $A+b$ means that every element of $A$ is added by $b$, whereas $A+B$ indicates a normal element-wise addition of two matrices.

\subsubsection{Proof of Lemma \ref{lem:bn}}
\begin{proof}
If there is only batch normalization between a feature map and its activation map, $\mathbf{A}^{(\ell)} = \N(\mathbf{Z}^{(\ell)})$. In this case, the reconstruction error can be formulated as below.
\small
\begin{eqnarray}
\begin{split}
     \|\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}\|_{1} = &~\| \N(\mathbf{Z_j^{(\ell)}}) - \sum\limits_{k = 1, k \neq j }^{m} s_{k} \N(\mathbf{Z}_k^{(\ell)}) \|_{1} \\
     =&~\| \frac{\gamma_{j}({\mathbf{Z}_j^{(\ell)} - \mu_j})}{\sigma_{j}} + \beta_j -\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \{ \frac{\gamma_{k}({\mathbf{Z}_k^{(\ell)} - \mu_k})}{\sigma_{k}} + \beta_k \} \|_{1}\\
     =&~\|\frac{\gamma_{j}}{\sigma_{j}}\mathbf{Z}_j^{(\ell)} - \sum\limits_{k = 1, k \neq j}^{m}\frac{s_{k}{\gamma_{k}}}{\sigma_{k}}\mathbf{Z}_k^{(\ell)}  -\frac{\gamma_{j}\mu_j}{\sigma_{j}} + \beta_j + \sum\limits_{k = 1, k \neq j}^{m}\{\frac{s_{k}{\gamma_{k}}\mu_k}{\sigma_{k}} - s_{k}\beta_k \}\|_{1} \\
     =&~\|\frac{\gamma_{j}}{\sigma_{j}}(\mathbf{A}^{(\ell-1)} \circledast \mathbf{W}_j^{(\ell)}) - \sum\limits_{k = 1, k \neq j}^{m}\frac{s_{k}{\gamma_{k}}}{\sigma_{k}}(\mathbf{A}^{(\ell-1)} \circledast \mathbf{W}_k^{(\ell)}) +  \\
     &~\sum\limits_{k = 1, k \neq j}^{m} \{{s_{k}} \frac{\gamma_{k}}{\sigma_{k}}(\mu_k - \frac{\sigma_{k}}{\gamma_{k}}\beta_k)\} -\frac{\gamma_{j}\mu_j}{\sigma_{j}} + \beta_j\|_{1}\\
     =&~\| \frac{\gamma_{j}}{\sigma_{j}} \{ (\mathbf{A}^{(\ell-1)} \circledast \mathbf{W}_j^{(\ell)})  - \sum\limits_{k = 1, k \neq j}^{m}s_k\frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}(\mathbf{A}^{(\ell-1)} \circledast \mathbf{W}_k^{(\ell)}) \}  + \\
     &~\frac{\gamma_{j}}{\sigma_{j}}\sum\limits_{k = 1, k \neq j}^{m} \{s_k \frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}(\mu_k - \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j +\frac{\sigma_{j}}{\gamma_{j}}\beta_j \}\|_{1}\\
     =&~\|\frac{\gamma_{j}}{\sigma_{j}} \{\mathbf{A}^{(\ell-1)} \circledast (\mathbf{W}_j^{(\ell)} - \sum\limits_{k = 1, k \neq j}^{m}s_k\frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}\mathbf{W}_k^{(\ell)})\} \\
     &~+ \frac{\gamma_{j}}{\sigma_{j}}\sum\limits_{k = 1, k \neq j}^{m} \{s_k \frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}(\mu_k - \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j +\frac{\sigma_{j}}{\gamma_{j}}\beta_j \} \|_{1}\\
     =&~\|\frac{\gamma_{j}}{\sigma_{j}}(\mathbf{A}^{(\ell-1)} \circledast \boldsymbol{\E}) + \frac{\gamma_{j}}{\sigma_{j}}\sum\limits_{k = 1, k \neq j}^{m} \{s_k \frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}(\mu_k - \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j +\frac{\sigma_{j}}{\gamma_{j}}\beta_j \} \|_{1},
     \nonumber
\end{split}
\end{eqnarray}
where $\boldsymbol{\E}$ = $\mathbf{W}_j^{(\ell)} - \sum\limits_{k = 1, k \neq j}^{m}s_k\frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}\mathbf{W}_k^{(\ell)} $.
\end{proof}

\subsubsection{Proof of Theorem \ref{thm:relu}}

\begin{proof}
If there are both batch normalization and a ReLU function between a feature map and its activation map, $\mathbf{A}^{(\ell)} = \F(\N(\mathbf{Z}^{(\ell)}))$. In this case, the reconstruction error can be formulated as below.
% \scriptsize
\small
\begin{eqnarray}
\begin{split}
 \|\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}\|_{1} &=\|\max(\N(\mathbf{Z}_{j}^{(\ell)}),0) - \sum\limits_{k=1, k \neq j}^{m} s_{k} \max(\N(\mathbf{Z}_{k}^{(\ell)}),0)\|_{1}\\
  &=\|\N(\mathbf{Z}_{j}^{(\ell)}) - \sum\limits_{k = 1, k \neq j}^{m}{s_{k}}\N(\mathbf{Z}_k^{(\ell)}) + \sum\limits_{k=1, k \neq j}^{m} s_{k} \min(0,\N(\mathbf{Z}_{k}^{(\ell)})) - \min(0, \N(\mathbf{Z}_j^{(\ell)}))\|_{1}\\
\end{split}\nonumber
\end{eqnarray}
From the procedure of Proof of Lemma \ref{lem:bn}, we obtained $\N(\mathbf{Z}_j^{(\ell)}) - \sum\limits_{k = 1, k \neq j }^{m} s_{k} \N(\mathbf{Z}_k^{(\ell)}) $ = $\frac{\gamma_{j}}{\sigma_{j}}(\mathbf{A}^{(\ell-1)} \circledast \boldsymbol{\E}) + \boldsymbol{\B}$, where $\boldsymbol{\E}$ = $\mathbf{W}_j^{(\ell)} - \sum\limits_{k = 1, k \neq j}^{m}s_k\frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}\mathbf{W}_k^{(\ell)} $ and $\boldsymbol{\B} =\frac{\gamma_{j}}{\sigma_{j}}\sum\limits_{k = 1, k \neq j}^{m} \{s_k \frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}(\mu_k - \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j +\frac{\sigma_{j}}{\gamma_{j}}\beta_j \}$.
Therefore, we have:
\begin{equation}
    \|\mathbf{A}_{j}^{(\ell)}-\sum\limits_{k = 1, k \neq j}^{m}{s_{k}} \mathbf{A}_{k}^{(\ell)}\|_{1} = \|\frac{\gamma_{j}}{\sigma_{j}}(\mathbf{A}^{(\ell-1)} \circledast \boldsymbol{\E}) + \boldsymbol{\B} + \boldsymbol{\R} \|_{1},
    \nonumber
\end{equation}
where $\boldsymbol{\R} = \sum\limits_{k=1, k \neq j}^{m} s_{k}  \min(0,\N(\mathbf{Z}_{k}^{(\ell)}))-\min(0,\N(\mathbf{Z}_{j}^{(\ell)}))$.
\end{proof}


\subsubsection{\revsec{Proof of Lemma \ref{lem:ae}}}
\revsec{
\begin{proof}
\begin{eqnarray}
    \|\boldsymbol{\R}\|_{1} & =  & \|\sum_{k=1, k \neq j}^{m} s_{k}  \min(0,\N(\mathbf{Z}_{k}^{(\ell)}))-\min(0,\N(\mathbf{Z}_{j}^{(\ell)})) \|_{1} \nonumber \\
    & =  & \|\sum_{k=1, k \neq j}^{m} s_{k}  \min(0,\N(\mathbf{Z}_{k}^{(\ell)}))\|_{1}-\min(0,\N(\mathbf{Z}_{j}^{(\ell)})) \nonumber \\
    & \leq & \sum_{k=1,k\neq j}^{m} \| s_k \min(0,\N(\mathbf{Z}_{k}^{(\ell)}))\|_{1} -\min(0,\N(\mathbf{Z}_{j}^{(\ell)})) \nonumber \\
    & \leq & \sum_{k=1,k\neq j}^{m}\|s_k \cdot \N(\mathbf{Z}_{k}^{(\ell)})\|_{1} -\min(0,\N(\mathbf{Z}_{j}^{(\ell)})) \nonumber \\
    & = & \sum_{k=1,k\neq j}^{m}\|s_k\|_{1} \cdot \|\N(\mathbf{Z}_{k}^{(\ell)})\|_{1} +c, \nonumber 
\end{eqnarray}
where $c = -\min(0,\N(\mathbf{Z}_{j}^{(\ell)})) \geq 0$.
\end{proof}
}



\subsubsection{Proof of Theorem \ref{thm:closedform}}
\begin{proof}
Our loss function is as follows.
\small
\begin{equation}
     \mathcal{L}_{re} =  \|\boldsymbol{\E}\|_2^{2} + \lambda_1 \|\boldsymbol{\B}\|_2^{2} + \lambda_2 \|\mathbf{s}\|_{2}^{2},
\nonumber
\end{equation}
where $\boldsymbol{\E}$ = $\mathbf{W}_j^{(\ell)} - \sum\limits_{k = 1, k \neq j}^{m}s_k\frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}\mathbf{W}_k^{(\ell)} $,~$\boldsymbol{\B} =\frac{\gamma_{j}}{\sigma_{j}}\sum\limits_{k = 1, k \neq j}^{m} \{s_k \frac{\sigma_{j}\gamma_{k}}{\gamma_{j}\sigma_{k}}(\mu_k - \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j +\frac{\sigma_{j}}{\gamma_{j}}\beta_j \},~\mathbf{s} =  [s_{1}~...~s_{j-1}~s_{j+1}~...~s_{m}]^T,~\lambda_1 , \lambda_2 > 0$. \\
\\
\\
Let (1) $\mathbf{X}=[\frac{\sigma_{j}\gamma_{1}}{\gamma_{j}\sigma_{1}}\boldsymbol{f_1},...,\frac{\sigma_{j}\gamma_{j-1}}{\gamma_{j}\sigma_{j-1}}\boldsymbol{f_{j-1}},\frac{\sigma_{j}\gamma_{j+1}}{\gamma_{j}\sigma_{j+1}}\boldsymbol{f_{j+1}},...\frac{\sigma_{j}\gamma_{m}}{\gamma_{j}\sigma_{m}}\boldsymbol{f_m}]$ such that $\boldsymbol{f_i}$ is the vectorized  $\mathbf{W}_i^{(\ell)}$ for $i \in [1,m] \setminus \{j\}$, (2) $\mathbf{y}$ is the vectorized $\mathbf{W}_j^{(\ell)}$ and (3) $\mathbf{p}=[p_1,...,p_{j-1}, p_{j+1}, ... p_n]^{T}$ such that $p_i = \frac{\sigma_{j}\gamma_{i}}{\gamma_{j}\sigma_{i}}(\mu_i - \frac{\sigma_{i}}{\gamma_{i}}\beta_i)$.\\
Then, Our loss function can be represented as below.
\begin{equation}
 \mathcal{L}_{re}(\mathbf{s}) = (\mathbf{y} - \mathbf{X}\mathbf{s})^{T}(\mathbf{y} - \mathbf{X}\mathbf{s}) + \lambda_1\{\frac{\gamma_{j}}{\sigma_{j}}(\mathbf{s}^{T}\mathbf{p} - \mu_j + \frac{\sigma_{j}}{\gamma_{j}}\beta_j)\}^2 + \lambda_2\mathbf{s}^T\mathbf{s}.
    \nonumber
\end{equation}
The first derivative of the loss function is: 
\begin{equation}
\begin{split}
\frac{\partial\mathcal{L}_{re}(\mathbf{s})}{\partial\mathbf{s}} &=  -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\mathbf{s} + \lambda_1 \{\frac{\gamma_{j}^2}{\sigma_{j}^2}(2\mathbf{p}\mathbf{p}^T)\mathbf{s} - 2 \frac{\gamma_{j}^2}{\sigma_{j}^2}\mu_j\mathbf{p} + 2\frac{\gamma_{j}}{\sigma_{j}}\beta_j\mathbf{p}\} + \lambda_2(2\mathbf{s}) \\
&= [2X^{T}X+2\lambda_{1}\frac{\gamma_{j}^2}{\sigma_{j}^2}\boldsymbol{p}\boldsymbol{p}^{T}+2\lambda_{2}I]\mathbf{s} -2 X^{T}\boldsymbol{y}-2\frac{\lambda_{1}\gamma_{j}}{\sigma_{j}}(\frac{\mu_{j}\gamma_{j}}{\sigma_{j}}-\beta_{j})\boldsymbol{p}
\end{split}
 \nonumber
\end{equation}


The second derivative of the loss function(\textit{i.e.} Hessian) is: 
\begin{equation}
 \mathbf{H}_{\mathcal{L}_{re}} = \frac{\partial^2\mathcal{L}_{re}(\mathbf{s})}{\partial\mathbf{s}\partial\mathbf{s}^T} = 2\mathbf{X}^T\mathbf{X} + 2\lambda_1\frac{\gamma_{j}^2}{\sigma_{j}^2}(\mathbf{p}\mathbf{p}^T)+2\lambda_2I.
 \nonumber
\end{equation}
Since $\mathbf{H}_{\mathcal{L}_{re}}$ is positive definite, $\mathcal{L}_{re}$ is a convex function and thus there exists a unique optimal solution $\mathbf{s}$ such that $\frac{\partial\mathcal{L}_{re}(\mathbf{s})}{\partial\mathbf{s}} = \mathbf{0}$. From the first derivative above, we obtain the solution as below.

\begin{equation}
 \boldsymbol{s}=[X^{T}X+\lambda_{1}\frac{\gamma_{j}^2}{\sigma_{j}^2}\boldsymbol{p}\boldsymbol{p}^{T}+\lambda_{2}I]^{-1}[X^{T}\boldsymbol{y}+\frac{\lambda_{1}\gamma_{j}}{\sigma_{j}}(\frac{\mu_{j}\gamma_{j}}{\sigma_{j}}-\beta_{j})\boldsymbol{p}].
\nonumber
\end{equation}
\end{proof}

\subsection{Experimental Details} \label{sec:appendix:exp}
% \smalltitle{Neuron version of LBYL in pretrained LeNet-300-100 \cite{LeNet} for FashionMNIST}
% %  Even though this paper focuses on filter pruning, our LBYL method can also be applied to prune and restore neurons in vanilla feed-forward neural networks consisting of only FC layers. We present the details in Appendix due to the space limitation
% Our LBYL method can also be applied to pruning neurons in a vanilla feed-forward neural network without convolution layers such as LeNet-300-100 \cite{LeNet}. Let $\alpha_{i}^{\ell}$ denote the output of the $i$-th neuron in the $\ell$-th layer, and let $W^{\ell}_{j,k}$ denote a weight between the $j$-th neuron in the $\ell$-th layer and the $k$-th neuron in the $(\ell+1)$-th layer as illustrated in Figure \ref{fig:appendix_figure}. The goal of our method is still minimizing the reconstruction error between a restored network and its original network by making a linear combination of remained neurons to compensate each pruned neuron. If the $j$-th neuron in the $\ell$-th layer is pruned, the reconstruction error can be formulated as: 
% \begin{equation}
% \sum\limits_{n = 1}^{k}\|{{\mathbf{\alpha}}_{n}^{{\ell+1}}-{\hat{\mathbf{\alpha}}}_{n}^{{\ell+1}}}\|_1 
% \nonumber
% \end{equation}
% As explained in Section \ref{sec:probdef}, the reconstruction error for the $k$-th neuron at the $(\ell+1)$-th layer can be derived as:
% \begin{equation}
%  \|{\mathbf{\alpha}}_{k}^{{\ell+1}} - (\sum\limits_{n = 1, n \neq j}^{j}{ W^{\ell}_{n,k} * \mathbf{\alpha}_{n}^{\ell}} +{\sum\limits_{n = 1, n \neq j}^{j} s_nW^{\ell}_{j,k} *  \mathbf{\alpha}_{n}^{\ell}} ) \|_1  = \|{ W_{j,k}^{\ell} * (\mathbf{\alpha}_{j}^{(\ell)} - \sum\limits_{n = 1, n \neq j}^{j}{s_{n}} \mathbf{\alpha}_{n}^{(\ell)})}\|_1
% % = \|{ W^{\ell}_{j,n} * (\sum\limits_{n' = 1, n' \neq j}^{j}{s_{n'}}\mathbf{\alpha}_{n'}^{\ell})}\|_1 ,
% \nonumber
% \end{equation}
% where we need to find scalars $s_{n}$'s such that ${ \mathbf{\alpha}_{j}^{(\ell)} - \sum\limits_{n = 1, n \neq j}^{j}{s_{n}} \mathbf{\alpha}_{n}^{(\ell)}}$ to minimize the reconstruction error. Note that $\alpha$ is data dependent and computed through a multiplication procedure with weights followed by an activation function (e.g., ReLU) without batch normalization process in LeNet-300-100. Therefore, by the similar derivation of Section \ref{sec:method:a}, we have:
% \begin{equation}
%      \mathcal{L}_{re} =  \|\boldsymbol{\E}\|_2^{2} + \lambda \|\mathbf{s}\|_{2}^{2}, ~~\text{where}~\mathbf{s} =  [s_{1}~...~s_{j-1}].
%     \nonumber
% \end{equation}

% Table \ref{tab:LeNet_FashionMNIST} shows the experimental results of LeNet-300-100 on FashionMNIST by following the same setup of \textit{Coreset} \cite{CoreSet_ICLR}. It is well observed that LBYL outperforms the training-free recovery method of Coreset with a clear margin. Although LBYL does not show the outstanding performance to a huge extent compared to NM \cite{NM} unlike it does in filter pruning on modern architectures, our LBYL method still shows the best accuracy in most of the cases.


% \begin{figure*}[t!]
%   \centering 
%     \includegraphics[width=0.35\columnwidth]{./figure/appendix_figure.pdf}
%   \caption{A neuron pruning scenario in fully-connected layers}
%   \label{fig:appendix_figure}
% \end{figure*}


% % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% \begin{table}[]
% \centering
% \begin{tabular}{c||ccc||ccc||ccc||c}\hline Criterion& \multicolumn{3}{c||}{L2-norm}& \multicolumn{3}{c||}{L2-GM}& \multicolumn{3}{c||}{L1-norm}& \multirow{2}{*}{Coreset} \\ \cline{1-10}
% Pruning Ratio& \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune & \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune & \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune &\\ \hline \Xhline{2\arrayrulewidth}
% 50 \%& \multicolumn{1}{c|}{\textbf{88.83}} & \multicolumn{1}{c|}{87.86}          & 87.86 & \multicolumn{1}{c|}{\textbf{88.69}}  & \multicolumn{1}{c|}{88.57} & 88.08 & \multicolumn{1}{c|}{\textbf{89.03}} & \multicolumn{1}{c|}{88.69} & 88.40 & 79.34\\ \hline
% 60 \%& \multicolumn{1}{c|}{87.75}& \multicolumn{1}{c|}{\textbf{88.07}} & 83.03 & \multicolumn{1}{c|}{\textbf{88.15}}  & \multicolumn{1}{c|}{88.10} & 85.82 & \multicolumn{1}{c|}{\textbf{87.55}} & \multicolumn{1}{c|}{86.92} & 85.17 & 69.41\\ \hline
% 70 \%& \multicolumn{1}{c|}{\textbf{83.92}} & \multicolumn{1}{c|}{83.27}& 71.21 & \multicolumn{1}{c|}{85.92}& \multicolumn{1}{c|}{86.39} & 78.38 & \multicolumn{1}{c|}{\textbf{84.57}} & \multicolumn{1}{c|}{82.75} & 71.26 & 62.31\\ \hline
% 80 \%& \multicolumn{1}{c|}{\textbf{78.05}} & \multicolumn{1}{c|}{77.11}& 63.90 & \multicolumn{1}{c|}{\textbf{77.63}} & \multicolumn{1}{c|}{77.49} & 64.19 & \multicolumn{1}{c|}{\textbf{80.55}} & \multicolumn{1}{c|}{80.02} & 66.76 & 49.68\\ \hline \Xhline{2\arrayrulewidth}
% \end{tabular}
% \caption{Recovery results of LeNet-300-100 on FashionMNIST }
% \label{tab:LeNet_FashionMNIST}
% \end{table}


% \smalltitle{Pretrained models for CIFAR-10 and CIFAR-100}
% We adopt a pretrained VGG-16 released by NM \cite{NM} in the experiments on CIFAR-10. For CIFAR-100, we use ResNet-50 \cite{ResNet} that we train from scratch in a way that:
% (1) SGD with Nesterov momentum of 0.9 is used, (2) the initial learning rate is 0.1 and then divided by 5 at 60, 120, and 160 epochs, (3) the weight decay factor is set to 5e-4, and (4) the total duration of training is 200 epochs with the batch size 128.

% \begin{table}[htb!]
% \centering 
% {\scriptsize
% \begin{tabular}{c||c|c|c||c|c|c||c|c|c||c|c|c}  \Xhline{2\arrayrulewidth}
% \multicolumn{13}{c}{\textbf{ResNet-50 (Acc. 78.82) }}
% \\ \Xhline{2\arrayrulewidth} %\hline
% Criterion & \multicolumn{3}{c||}{L2-norm} & \multicolumn{3}{c||}{L2-GM} & \multicolumn{3}{c||}{L1-norm}& \multicolumn{3}{c}{Random}\\ \hline
% Pruning Ratio& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune
% \\ \Xhline{2\arrayrulewidth}
% 10\%& \textbf{78.14} & 77.28 & 75.14 & \textbf{78.01} & 76.92 & 74.49 & \textbf{78.25} & 77.21 & 75.07 & \textbf{76.53} & 72.46 & 59.32 \\ \hline
% 20\%& \textbf{76.15} & 72.73 & 63.39 & \textbf{76.36} & 72.32 & 63.54 & \textbf{75.71} & 72.24 & 61.84 & \textbf{73.29} & 59.44 & 19.27 \\ \hline
% 30\%& \textbf{73.29} & 64.47 & 39.96 & \textbf{72.97} & 64.01 & 39.01 & \textbf{72.07} & 63.07 & 35.77 & \textbf{69.05} & 40.42 & 3.25  \\ \hline
% 40\%& \textbf{65.21} & 46.40 & 15.32 & \textbf{65.78} & 46.17 & 13.14 & \textbf{62.64} & 45.98 & 12.59 & \textbf{59.49} & 20.89 & 2.59  \\ \hline
% 50\%& \textbf{52.61} & 25.98 & 5.22  & \textbf{54.04} & 23.44 & 4.32  & \textbf{49.07} & 21.98 & 4.25  & \textbf{43.51} & 8.77  & 2.77  \\ \Xhline{2\arrayrulewidth}
% Average Acc. & \textbf{69.08} & 57.37 & 39.81 & \textbf{69.43} & 56.57 & 38.90 & \textbf{67.55} & 56.10 & 37.90 & \textbf{64.37} & 40.40 & 17.44\\ \Xhline{2\arrayrulewidth}   
% \end{tabular}%
% }
% \caption{Recovery results of ResNet-50 on CIFAR-100}
% \label{tab:ResNet50-CIFAR100}
% \vspace{2mm}
% % \end{table}

% % \begin{table}[h]
% \centering
% \scriptsize
% \begin{tabular}{c ||c |c |c |c |c |c |c |c |c } \Xhline{2\arrayrulewidth}
% \multicolumn{10}{c}{\textbf{MobileNet-V2 (Acc. 71.88) }} \\ \Xhline{2\arrayrulewidth} %\hline
% \multicolumn{1}{l||}{Pruning criterion} & \multicolumn{3}{c|}{ L2 - norm} & \multicolumn{3}{c|}{ L2 - GM} & \multicolumn{3}{c}{L1 - norm} \\ \hline
% Pruning ratio & Ours & NM & prune & Ours & NM & prune & Ours & NM & prune \\ \hline
% 5\% & \textbf{67.46} & 67.1 & 61.97 & \textbf{67.64} & 63.68 & 62.53 & 67.25 & \textbf{67.64} & 63.27 \\ \hline
% 10\% & \textbf{53.41} & 52.66& 29.06 & \textbf{52.26} & 46.8& 28.21 & \textbf{54.95} & 52.1& 32.08 \\ \hline
% \end{tabular}%
% \vspace{2mm}
% \caption{Recovery results of MobileNet-V2 on ImageNet}
% \label{tab:mobilenet}
% \end{table}

% \smalltitle{Result of ResNet-50 on CIFAR-100}
% We present here our experimental results of CIFAR-100 on ResNet-50 in Table \ref{tab:ResNet50-CIFAR100}. In all the cases, it is surely confirmed that our LBYL outperforms NM with clear margins.

% \smalltitle{Result of MobileNet-V2 on ImageNet}
% Table \ref{tab:mobilenet} shows the experimental results of ImageNet on MobileNet-V2, where we prune only the first layer of each block. This scheme can be seen as a naive adaptation of our method for MobileNet-V2, but LBYL still manages to beat NM in such a tiny architecture.


% test our proposed method on CIFAR-100 using ResNet50. As shown in \ref{tab:ResNet50-CIFAR100}, Our method achieves in all cases higher accuracy than \cite{NM}. The reason why our method show better performance than NM \cite{NM} is that ResNet50 has a number of filters in layer and deeper layers. That is, our method sufficiently recovers the pruned filters in large networks.

\smalltitle{Hyperparameters}
LBYL uses two hyperparameters, namely $\lambda_1$ and $\lambda_2$, which adjust the weights of loss terms BE and AE, respectively. NM \cite{NM} also needs two hyperparameters, namely $\lambda$ and $t$, where $\lambda$ is the ratio for balancing between the cosine distance and the bias distance and $t$ is the threshold for avoiding erroneous compensation caused by the low similarity. In \cite{NM}, $\lambda$ is 0.85 and $t$ is 0.1 by default, which we also adopt in our experiments. However, these values do not work well in random pruning and therefore we had to find the best values of $\lambda$ and $t$ as well as $\lambda_1$ and $\lambda_2$. All hyperparameter values are found by grid search, and presented in Tables \ref{tab:param:vgg16:cifar10}, \ref{tab:param:ResNet50:cifar100}, \ref{tab:param:ResNet34:ImageNet}, and \ref{tab:param:ResNet101:ResNet101}.


\begin{table*}[h]
\centering 
\scriptsize
\begin{tabular}{c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & L2-norm& L2-GM & L1-norm\\ \cline{2-4} 
& OURS& OURS& OURS\\ \hline
Pruning Ratio& $\lambda$  & $\lambda$    & $\lambda$ \\\Xhline{2\arrayrulewidth}
50\%  & $0.3$  & $1.2$ & $0.7$  \\ \hline
60\%  & $0.6$  & $0.5$ & $0.8$  \\ \hline
70\%  & $0.3$  & $1.3$ & $0.2$   \\ \hline
80\%  & $1\times10^{-6}$  & $0.003$ & $0.3$  \\ \hline\Xhline{2\arrayrulewidth}
\end{tabular}% 
\caption{hyperparameters of LeNet-300-100 on FashionMNIST}
\label{tab:param:LeNet:fashionmnist}
\vspace{4mm}
% \end{table*}


% \begin{table*}[h]
\centering 
\scriptsize
\begin{tabular}{c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & L2-norm& L2-GM & L1-norm\\ \cline{2-4} 
& OURS& OURS& OURS\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$  & $\lambda_1$ / $\lambda_2$  & $\lambda_1$ / $\lambda_2$  \\\Xhline{2\arrayrulewidth}
10\%  & $6\times10^{-6}$ / $1\times10^{-4}$ & $1\times10^{-6}$ / $4\times10^{-3}$ & $2\times10^{-6}$ / $0.06$  \\ \hline
20\%  & $4\times10^{-6}$ / $6\times10^{-3}$ & $4\times10^{-6}$ / $4\times10^{-3}$ & $2\times10^{-6}$ / $1\times10^{-4}$ \\ \hline
30\%  & $1\times10^{-6}$ / $0.01$           & $1\times10^{-6}$ / $5\times10^{-3}$ & $4\times10^{-6}$ / $1\times10^{-4}$ \\ \hline
40\%  & $2\times10^{-6}$ / $0.01$           & $1\times10^{-6}$ / $8\times10^{-3}$ & $1\times10^{-6}$ / $1\times10^{-4}$ \\ \hline
50\%  & $4\times10^{-5}$ / $2\times10^{-4}$ & $2\times10^{-5}$ / $1\times10^{-4}$ & $1\times10^{-6}$ / $0.01$  \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\vspace{4mm}
% \end{table*}
\begin{tabular}{c|c|c|c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & \multicolumn{2}{c|}{Random\_1}& \multicolumn{2}{c|}{Random\_2}& \multicolumn{2}{c}{Random\_3}\\ \cline{2-7} 
    & OURS& NM& OURS& NM& OURS& NM\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ \\ \Xhline{2\arrayrulewidth}
10\%& 0.9 / $4\times10^{-4}$& 0.2 / 0.7   & $5\times10^{-6}$ / 0.2& 0.25 / 0.65     & $1\times10^{-6}$ / 0.2  & 0.05 / 0.8\\ \hline
20\%& $1\times10^{-5}$ / 0.6& 0 / 0.75    & $6\times10^{-6}$ / 0.2& 0.15 / 0.6& $2\times10^{-6}$ / 0.08 & 0.35 / 0.05 \\ \hline
30\%& $1\times10^{-5}$ / $2\times10^{-4}$ & 0 / 0.75    & $6\times10^{-6}$ / 0.09& 0.15/ 0.75& $2\times10^{-6}$ / $9\times10^{-3}$   & 0.05 / 1.0  \\ \hline
40\%& $1\times10^{-5}$ / $9\times10^{-3}$ & 0.15 / 0.7  & $3\times10^{-6}$ / 0.02& 0.05/ 1.0 & $2\times10^{-6}$ / $9\times10^{-3}$   & 0.15 / 1.0  \\ \hline
50\%& $1\times10^{-5}$ / $3\times10^{-3}$ & 0.15 / 0.75 & $1\times10^{-5}$ /~$7\times10^{-5}$ & 0.45 / 0.95     & $4\times10^{-6}$ / $7\times10^{-5}$   & 0.15 / 1.0  \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\caption{hyperparameters of VGG16 on CIFAR-10}
\label{tab:param:vgg16:cifar10}
\vspace{4mm}
% \end{table*}
% \begin{table*}[h]
\centering 
\scriptsize
\begin{tabular}{c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & L2-norm& L2-GM & L1-norm\\ \cline{2-4} 
& OURS& OURS& OURS\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$  & $\lambda_1$ / $\lambda_2$  & $\lambda_1$ / $\lambda_2$  \\\Xhline{2\arrayrulewidth}
10\%  & $2\times10^{-5}$ / $6\times10^{-3}$ & $2\times10^{-5}$ / $1\times10^{-3}$ & $2\times10^{-5}$ / $1\times10^{-3}$ \\ \hline
20\%  & $1\times10^{-5}$ / $2\times10^{-3}$ & $1\times10^{-5}$ / $1\times10^{-3}$ & $2\times10^{-5}$ / $1\times10^{-3}$ \\ \hline
30\%  & $1\times10^{-5}$ / $2\times10^{-3}$ & $1\times10^{-5}$ / $2\times10^{-3}$ & $1\times10^{-5}$ / $1\times10^{-3}$ \\ \hline
40\%  & $1\times10^{-5}$ / $1\times10^{-3}$ & $1\times10^{-5}$ / $1\times10^{-3}$ & $1\times10^{-5}$ / $1\times10^{-3}$ \\ \hline
50\%  & $1\times10^{-6}$ / $1\times10^{-3}$ & $1\times10^{-6}$ / $1\times10^{-4}$ & $1\times10^{-6}$ / $1\times10^{-3}$  \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\vspace{1mm}

\begin{tabular}{c|c|c|c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & \multicolumn{2}{c|}{Random\_1}& \multicolumn{2}{c|}{Random\_2}& \multicolumn{2}{c}{Random\_3}\\ \cline{2-7} 
    & OURS& NM& OURS& NM& OURS& NM\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ \\ \Xhline{2\arrayrulewidth}
10\%& $1\times10^{-5}$ / $2\times10^{-3}$ & 0.2 / 0.5    & $2\times10^{-5}$ / $2\times10^{-3}$   & 0.15 / 0.65  & $3\times10^{-6}$ / $2\times10^{-3}$   & 0.2 / 0.95 \\ \hline
20\%& $1\times10^{-5}$ / $3\times10^{-3}$ & 0.2 / 0.5    & $4\times10^{-5}$ / $1\times10^{-4}$   & 0.05 / 0.7   & $4\times10^{-6}$ / $5\times10^{-3}$   & 0.3 / 0.55 \\ \hline
30\%& $1\times10^{-5}$ / $2\times10^{-3}$ & 0.4 / 0.6    & $3\times10^{-6}$ / $2\times10^{-3}$   & 0.15/ 0.7    & $1\times10^{-6}$ / $4\times10^{-3}$   & 0.05 / 0.9  \\ \hline
40\%& $1\times10^{-5}$ / $2\times10^{-3}$ & 0.2 / 0.5    & $8\times10^{-6}$ / $7\times10^{-4}$   & 0.4 / 0.9     & $1\times10^{-6}$ / $4\times10^{-3}$   & 0.25 / 0.55  \\ \hline
50\%& $1\times10^{-5}$ / $2\times10^{-3}$ & 0.4 / 1.0    & $6\times10^{-6}$ /~$9\times10^{-4}$   & 0.1 / 0.7    & $1\times10^{-6}$ / $5\times10^{-3}$   & 0.05 / 1.0  \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\caption{hyperparameters of ResNet50 on CIFAR-100}
\label{tab:param:ResNet50:cifar100}
\vspace{2mm}
\end{table*}

\begin{table*}[h]
\centering 
\scriptsize
\begin{tabular}{c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & L2-norm& L2-GM & L1-norm\\ \cline{2-4} 
& OURS& OURS& OURS\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$    & $\lambda_1$ / $\lambda_2$  & $\lambda_1$ / $\lambda_2$  \\\Xhline{2\arrayrulewidth}
10\%  & $7\times10^{-5}$ / 0.05 & $1\times10^{-5}$ / 0.1 &  $3\times10^{-5}$ / 0.08 \\ \hline
20\%  & $2\times10^{-5}$ / 0.07 & $2\times10^{-4}$ / 0.06 & $5\times10^{-4}$ / 0.03 \\ \hline
30\%  & $5\times10^{-4}$ / 0.03 & $4\times10^{-4}$ / 0.04 & $8\times10^{-5}$ / 0.05 \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\vspace{1mm}

\begin{tabular}{c|c|c|c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & \multicolumn{2}{c|}{Random\_1}& \multicolumn{2}{c|}{Random\_2}& \multicolumn{2}{c}{Random\_3}\\ \cline{2-7} 
    & OURS& NM& OURS& NM& OURS& NM\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ \\ \Xhline{2\arrayrulewidth}
10\%& $4\times10^{-4}$ / 0.07 & 0.15 / 0.1    & $1\times10^{-5}$ / 0.3   & 0.05 / 0.75   & $1\times10^{-5}$ / 0.08   & 0.1 / 0.6 \\ \hline
20\%& $1\times10^{-5}$ / 0.1  & 0.15 / 1.0    & $1\times10^{-5}$ / 0.3   & 0.05 / 0.65   & $1\times10^{-5}$ / 0.2    & 0.1 / 0.8 \\ \hline
30\%& $2\times10^{-5}$ / 0.2  & 0.15 / 0.9    & $4\times10^{-5}$ / 0.2   & 0.05 / 0.65   & $2\times10^{-5}$ / 0.07   & 0.05 / 0.85  \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\caption{hyperparameters of ResNet34 on ImageNet}
\label{tab:param:ResNet34:ImageNet}
\vspace{2mm}

% \end{table*}

% \begin{table*}[h]
\centering 
\scriptsize
\begin{tabular}{c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & L2-norm& L2-GM & L1-norm\\ \cline{2-4} 
& OURS& OURS& OURS\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$    & $\lambda_1$ / $\lambda_2$  & $\lambda_1$ / $\lambda_2$  \\\Xhline{2\arrayrulewidth}
10\%  & $4\times10^{-6}$ / 0.02 & $8\times10^{-6}$ / $9\times10^{-4}$   &  $1\times10^{-6}$ / $5\times10^{-3}$ \\ \hline
20\%  & $1\times10^{-6}$ / 0.02 & $1\times10^{-6}$ / 0.01               & $1\times10^{-6}$ / 0.02 \\ \hline
30\%  & $2\times10^{-6}$ / 0.03 & $1\times10^{-6}$ / 0.02               & $1\times10^{-6}$ / 0.03 \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\vspace{1mm}
\begin{tabular}{c|c|c|c|c|c|c}\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Criterion} & \multicolumn{2}{c|}{Random\_1}& \multicolumn{2}{c|}{Random\_2}& \multicolumn{2}{c}{Random\_3}\\ \cline{2-7} 
    & OURS& NM& OURS& NM& OURS& NM\\ \hline
Pruning Ratio& $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ & $\lambda_1$ / $\lambda_2$ & t / $\lambda$ \\ \Xhline{2\arrayrulewidth}
10\%& $1\times10^{-6}$ / 0.2 & 0.1 / 1.0      & $2\times10^{-6}$ / 1.0   & 0.15 / 0.35   & $2\times10^{-6}$ / 0.06   & 0.35 / 0.9 \\ \hline
20\%& $1\times10^{-6}$ / 0.3  & 0.15 / 0.1    & $4\times10^{-5}$ / 0.2   & 0.15 / 0.45   & $6\times10^{-6}$ / 0.05    & 0.35 / 1.0 \\ \hline
30\%& $1\times10^{-6}$ / 0.2  & 0.5 / 0.85    & $1\times10^{-6}$ / 0.2   & 0.25 / 0.3   & $2\times10^{-6}$ / $1\times10^{-4}$   & 0.1 / 0.1 \\ \Xhline{2\arrayrulewidth}
\end{tabular}% 
\caption{hyperparameters of ResNet101 on ImageNet}
\label{tab:param:ResNet101:ResNet101}
\end{table*}








% \begin{table}[hp]
% {\scriptsize
% \resizebox{!}{%
% \begin{tabular}{c||c||c||c||c||c}\Xhline{2\arrayrulewidth}
% \hline
% \multirow{2}{*}{Criterion}& L2-norm& L2-GM& L1-norm& \multicolumn{2}{c}{Random}\\ \cline{2-6} 
% & OURS& OURS& OURS& OURS& NM\\ \hline
% Pruning Ratio& $\lambda_1$ / $\lambda_2$ & $\lambda_1$ / $\lambda_2$ & $\lambda_1$ / $\lambda_2$ & $\lambda_1$ / $\lambda_2$ & $t$  $ / $\lambda$ \\ \hline\Xhline{2\arrayrulewidth}
% 10\%  & 3\times10^{-6}$ / $1.4 & 3\times10^{-6}$ / $1.6 & 1\times10^{-6}$ / $1.2 & 2\times10^{-6}$ / $0.6& 0.1$ / $1   \\ \hline
% 20\%  & 1\times10^{-6}$ / $1.1 & 3\times10^{-6}$ / $0.9 & 9\times10^{-6}$ / $1.8 & 1\times10^{-6}$ / $0.3& 0.15$ / $0.1   \\ \hline
% 30\%  & 3\times10^{-3}$ / $ 1.3 & 3\times10^{-6}$ / $1.1 & 1\times10^{-6}$ / $1.8 & 6\times10^{-6}$ / $0.3& 0.5$ / $0.85   \\ \Xhline{2\arrayrulewidth}
% \end{tabular}%
% }
% }
% \caption{hyperparameters of ResNet101 on ImageNet}
% \end{table}



% \begin{equation} 
% \| \frac{\gamma_{j}}{\sigma_{j}}{(\mathbf{A}^{(\ell-1)} \circledast \boldsymbol{\E})} + \frac{\gamma_{j}}{\sigma_{j}} 
% \{\sum\limits_{k = 1, k \neq j }^{m} s_{k}\frac{\sigma_{j}}{\gamma_{j}}\frac{\gamma_{k}}{\sigma_{k}} (\mu_{k} -  \frac{\sigma_{k}}{\gamma_{k}}\beta_k) - \mu_j + \frac{\sigma_{j}}{\gamma_{j}}\beta_j\} \|_{1},
% \end{equation}

% NM cifar-10
% we employ SGD with the momentum of 0.9. The learning rate starts at
% 0.1, with different annealing strategies per model. For LeNet, the learning rate is reduced by one-tenth
% for every 15 of the total 60 epochs. Weight decay is set to 1e-4, and batch size to 128. For VGG and
% ResNet, the learning rate is reduced by one-tenth at 100 and 150 of the total 200 epochs

% \smalltitle{CIFAR-100 Result on CIFAR10}

