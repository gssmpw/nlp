\section{Experiments} \label{sec:experiments}
In this section, we empirically validate the performance and effectiveness of our proposed LBYL method, compared to a one-to-one compensation method, called \textit{Neuron Merging} (\textit{NM})  \cite{NM} as well as a baseline called \textit{Prune} that does not perform any recovery process after pruning. For a fair comparison, we exploit the code provided by the authors of \cite{NM} and then implement our proposed method using PyTorch \cite{PyTorch}. LBYL uses two hyperparameters, namely $\lambda_1$ and $\lambda_2$, that adjust the weights of loss terms BE and AE, respectively. The details of hyperparameter settings are presented in Appendix. Also, our implementation is available at our github.\footnote{https://github.com/bigdata-inha/LBYL}

\smalltitle{Pruning criteria and ratio}
\rev{We evaluate LBYL using the following four data-independent pruning criteria: L1-norm \cite{PFEC}, L2-norm \cite{Soft}, L2-GM \cite{FPGM}, and lastly even random. Random pruning is intended to show the robustness of each method when a norm-based criterion is not used. We prune only convolution filters in CNNs, and hence the pruning ratio is the ratio of pruned filters for each convolution layer.}



\subsection{Experiments Using CIFAR-10 and CIFAR-100}
\smalltitle{Dataset and pretrained model} 
The CIFAR-10 dataset \cite{krizhevsky2009learning} consists 60K images of 10 different classes, where each class includes 5K training images along with 1K validation images and the size of every image is 32 Ã— 32. CIFAR-100 \cite{krizhevsky2009learning} also include 60K images for 100 different classes, where 500 training images along with 100 validation images are included for each class. We test VGG-16 \cite{VGG} for CIFAR-10 and ResNet-50 \cite{ResNet} for CIFAR-100, and present the result of CIFAR-100 only in Appendix as 
their experimental results of CIFAR-10 and CIFAR-100 show the similar trend.


We adopt a pretrained VGG-16 released by NM \cite{NM} in the experiments on CIFAR-10. For CIFAR-100, we use ResNet-50 \cite{ResNet} that we train from scratch in a way that:
(1) SGD with Nesterov momentum of 0.9 is used, (2) the initial learning rate is 0.1 and then divided by 5 at 60, 120, and 160 epochs, (3) the weight decay factor is set to 5e-4, and (4) the total duration of training is 200 epochs with the batch size 128.

\begin{table*}[htb!]
% \begin{table}[t]
{\small

\begin{tabular}{c||c|c|c||c|c|c||c|c|c||c|c|c}  \Xhline{2\arrayrulewidth}
\multicolumn{13}{c}{\textbf{ResNet34 - ImageNet (Acc. 73.27)}}
\\ \Xhline{2\arrayrulewidth} %\hline
Criterion & \multicolumn{3}{c||}{L2-norm} & \multicolumn{3}{c||}{L2-GM} & \multicolumn{3}{c||}{L1-norm}& \multicolumn{3}{c}{Random}\\ \hline
Pruning Ratio& Ours& NM& Prune& Ours& NM& Prune& Ours& NM& Prune& Ours& NM& Prune\\ \Xhline{2\arrayrulewidth}
10\%& \textbf{69.22} & 66.96 & 63.74 & \textbf{69.12} & 66.59 & 61.76 & \textbf{69.07} & 66.30 & 62.05 & \textbf{65.90} & 64.75& 52.97 \\ \hline
20\%& \textbf{62.49} & 55.70 & 42.81 & \textbf{62.27} & 55.39 & 43.45 & \textbf{61.18} & 53.95 & 40.61 & \textbf{50.64} & 48.40& 18.62 \\ \hline
30\%& \textbf{47.59} & 39.22 & 17.02 & \textbf{49.43} & 37.97 & 15.74 & \textbf{46.88} & 35.56 & 12.58 & 22.92 & \textbf{23.69} & 1.35  \\ \Xhline{2\arrayrulewidth}
% Average Acc. & \textbf{59.77} & 53.96 & 41.19 & \textbf{60.27} & 53.32 & 40.32 & \textbf{59.04} & 51.94 & 38.41 & \textbf{46.49} & 45.61& 19.10\\ \Xhline{2\arrayrulewidth}
\end{tabular}
}
\vspace{2mm}
\caption{Recovery results of ResNet-34 on ImageNet}
\label{tab:ResNet34-ImageNet}
\vspace{1.5mm}

% \end{table}
% \begin{table}[t]
{\small

\begin{tabular}{c||c|c|c||c|c|c||c|c|c||c|c|c}  \Xhline{2\arrayrulewidth}
\multicolumn{13}{c}{\textbf{ResNet101 - ImageNet (Acc. 77.31)}}
\\ \Xhline{2\arrayrulewidth} %\hline
Criterion & \multicolumn{3}{c||}{L2-norm} & \multicolumn{3}{c||}{L2-GM} & \multicolumn{3}{c||}{L1-norm}& \multicolumn{3}{c}{Random}\\ \hline
Pruning Ratio& Ours& NM& Prune& Ours& NM& Prune& Ours& NM& Prune& Ours& NM& Prune
\\ \Xhline{2\arrayrulewidth}
10\%& \textbf{74.59} & 72.36 & 68.90 & \textbf{74.54} & 72.38 & 68.38 & \textbf{74.19} & 72.05 & 68.33 & \textbf{59.78} & 57.13 & 46.55 \\ \hline
20\%& \textbf{68.47} & 61.42 & 45.78 & \textbf{69.03} & 61.02 & 46.25 & \textbf{68.52} & 60.57 & 44.57 & \textbf{34.85} & 15.78 & 6.83  \\ \hline
30\%& \textbf{55.51} & 37.38 & 10.32 & \textbf{56.65} & 33.74 & 7.65  & \textbf{56.11} & 36.43 & 9.32  & \textbf{13.27} & 2.31  & 0.60  \\ \Xhline{2\arrayrulewidth}
% Average Acc. & \textbf{66.19} & 57.05 & 41.67 & \textbf{66.74} & 55.71 & 40.76 & \textbf{66.27} & 56.35 & 40.74 & \textbf{35.97} & 25.07 & 17.99\\\Xhline{2\arrayrulewidth}
\end{tabular}
}
\vspace{2mm}
\caption{Recovery results of ResNet-101 on ImageNet} \label{tab:ResNet101-ImageNet}
\vspace{1.5mm}
% \end{table}

% \begin{table}[h]
\centering
\small
\begin{tabular}{c||c |c |c ||c |c |c ||c |c |c } \Xhline{2\arrayrulewidth}
\multicolumn{10}{c}{\textbf{MobileNet-V2 (Acc. 71.88) }} \\ \Xhline{2\arrayrulewidth} %\hline
\multicolumn{1}{c||}{Criterion} & \multicolumn{3}{c||}{ L2 - norm} & \multicolumn{3}{c||}{ L2 - GM} & \multicolumn{3}{c}{L1 - norm} \\ \hline
Pruning Ratio & Ours & NM & prune & Ours & NM & prune & Ours & NM & prune \\ \Xhline{2\arrayrulewidth}
5\% & \textbf{67.46} & 67.1 & 61.97 & \textbf{67.64} & 63.68 & 62.53 & 67.25 & \textbf{67.64} & 63.27 \\ \hline
10\% & \textbf{53.41} & 52.66& 29.06 & \textbf{52.26} & 46.8& 28.21 & \textbf{54.95} & 52.1& 32.08 \\ \Xhline{2\arrayrulewidth}
\end{tabular}%
\vspace{2mm}
\caption{Recovery results of MobileNet-V2 on ImageNet}
\label{tab:mobilenet}
\end{table*}

% In case of CIFAR-100, we evaluate the our proposed method using the ResNet-50 \cite{ResNet}. To train the ResNet-50, we exploit SGD with Nesterov momentum of 0.9. the initial learning rate is 0.1 and it is divided by 5 at 60, 120, 160 epochs. the weight decay is set to 5e-4, the train epochs is 200 with batch size 128. 

\smalltitle{Results}
We first prune VGG-16 on CIFAR-10, which is a representative single-branched CNN, using the layer-by-layer pruning scheme with four different criteria, and then update the remaining filters in each layer to restore the pruned VGG-16. As shown in Table \ref{tab:VGG16-CIFAR10}, our LBYL method achieves up to 6.16\% higher average accuracy than NM \cite{NM}. In particular, LBYL outperforms NM with clear margins when the pruning ratio increases. This is probably because NM might have trouble finding a filter sufficiently close to each pruned filter with less remaining filters due to a higher pruning ratio. Table \ref{tab:ResNet50-CIFAR100} presents the experimental results of CIFAR-100 on ResNet-50. In all the cases, it is surely confirmed that our LBYL outperforms NM with clear margins.

\subsection{Experiments Using ImageNet (ILSVRC2012)}

\smalltitle{Dataset and pre-trained model} 
The ImageNet dataset \cite{deng2009imagenet} contains 1,000 classes and consist of 1.28M training images and 50K validation images of size 256 $\times$ 256. For ImageNet, we use pretrained ResNet-34 and ResNet-101, both of which are released by PyTorch\footnote{https://pytorch.org/vision/stable/models.html}. ResNet-34 and ResNet-101 respectively represent two different types of the ResNet architecture, namely the one consisting of basic blocks and the other one consisting of bottleneck blocks.

\smalltitle{Results}
To prune a ResNet architecture, we exploit the common strategy to remove each residual block, that is, pruning the first two convolutional layers and not changing the output dimension of the block. Tables \ref{tab:ResNet34-ImageNet} and \ref{tab:ResNet101-ImageNet} present the experimental results using ResNet-34 and ResNet-101 on ImageNet. Through the results, LBYL can still recover the damaged filters to a satisfactory extent.


\smalltitle{Result of MobileNet-V2 on ImageNet}
Table \ref{tab:mobilenet} shows the experimental results of ImageNet on MobileNet-V2, where we prune only the first layer of each block. This scheme can be seen as a naive adaptation of our method for MobileNet-V2, but LBYL still manages to beat NM in such a tiny architecture.



\begin{figure*}[t]
	\centering 
	\includegraphics[height=2mm]{./figure/error_key.pdf} \\
    \subfigure[\label{fig:expcos_:a}{RE} ]{\hspace{0mm}\includegraphics[width=0.45\columnwidth]{./figure/LBYL_figure_4_RE_.pdf}\hspace{2mm}}
    \subfigure[\label{fig:expcos_:b}{BE} ]{\hspace{0mm}\includegraphics[width=0.45\columnwidth]{./figure/LBYL_figure_4_BE_.pdf}\hspace{2mm}}
    \subfigure[\label{fig:expcos_:d}{WARE} ]{\hspace{0mm}\includegraphics[width=0.45\columnwidth]{./figure/LBYL_figure_4_WARE_.pdf}\hspace{2mm}}
    \subfigure[\label{fig:expcos_:c}{Average Norm of Scales} ]{\hspace{0mm}\includegraphics[width=0.45\columnwidth]{./figure/LBYL_figure_4_Scale_.pdf}\hspace{2mm}}
    \caption{Comparison on the three error components with NM \cite{NM}, where each $m$\_$n\_k$ in the x-axis represents the $k$-th conv module in the $n$-th block at the $m$-th layer in ResNet-50 and when pruning the first and second convolution layers of each block by 30\%}
	\label{fig:error_components}
	\vspace{-2mm}
\end{figure*}


\subsection{\revfour{Experiments Using COCO2017}}


\revfour{

\smalltitle{Dataset and pre-trained model}
The COCO2017 dataset \cite{cocodataset} is widely used for training and evaluating object detection, segmentation, and captioning tasks, which contains 330K training images alongside 5K validation images across 80 object categories. We use the SSD (Single Shot Multibox Detector \cite{ssd}) model based on ResNet-50 for evaluating the object detection task on COCO2017, obtained from a PyTorch implementation library \footnote{https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch}. 

\smalltitle{Results}
In the SSD detector, we prune the ResNet-50 backbone using the same pruning criterion as in the experiments using ImageNet, and measure the recovered performance of pruned ResNet-50 by each compared recovery method. In terms of implementation, here we do not utilize the Batch Normalization Error (BE) as the values of $\gamma$ and $\sigma$ of batch normalization layers of the pre-trained SSD model are too tiny (less than 10$^{-6}$), which incurs zero division error in the implementation level. Table \ref{tab:COCO_res} demonstrates the experimental results on the recovered performance of the pruned SSD model on COCO2017. Our LBYL method shows better performance than NM \cite{NM} by clear margins. More specifically, as the pruning ratio increases, LBYL achieves up to approximately 5\% higher AP$_{50}$ on average than NM.
}

% Also, we set $\lambda$ to 0.05 in the recovery process to show the robustness of lambda value in various pruning criteria.


% \begin{table}[t!]
% \color{blue}
% \centering
% \small
% \begin{tabular}{c||c|c|c} \Xhline{2\arrayrulewidth}
% \multicolumn{4}{c}{\textbf{SSD based on ResNet-50 on COCO2017 (AP 25.35)}} \\
% \Xhline{2\arrayrulewidth}
% Pruning Ratio& Ours & NM& Prune\\ \Xhline{2\arrayrulewidth}
% 10 \% & \textbf{22.62}   & 22.19  &20.70 \\\hline
% 20 \% & \textbf{17.10}  & 15.47  &12.69 \\\hline 
% 30 \% & \textbf{8.22}  & 5.95  &4.33 \\\Xhline{2\arrayrulewidth}
% \end{tabular}
% \vspace{3mm}
% \caption{ Average Precisions of Single Shot Object Detetion based on ResNet-50 on COCO2017 using l2-norm pruning criterion. }
% \label{tab:COCO}
% \end{table}


\begin{table*}[htb!]
\centering
\small
% \color{blue}
\begin{tabular}{cc||c|c|c|c|c|c|c}
\hline \Xhline{2\arrayrulewidth}

\multicolumn{2}{c||}{} & Method & AP& AP$_{50}$ & AP$_{75}$& AP$_{S}$& AP$_{M}$ & AP$_{L}$          \\ \hline  \Xhline{2\arrayrulewidth}
\multicolumn{1}{l|}{Pruning   Criterion}      & Pruning Ratio         & SSD (ResNet-50)    & 25.35              & 42.75            & 26.19              &       7.13        & 27.31              & 40.88             \\ \hline \Xhline{2\arrayrulewidth}
\multicolumn{1}{c|}{\multirow{9}{*}{L2-norm}} & \multirow{3}{*}{10\%} & Prune  & 20.7           & 37.08          & 20.64          & 6.38          & 22.3           & 32.46          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 22.2           & 38.27          & 22.53          & 6.31          & 23.58          & 36.02          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{22.63} & \textbf{39.83} & \textbf{22.85} & \textbf{6.5}  & \textbf{24.02} & \textbf{36.37} \\ \cline{2-9} 
\multicolumn{1}{c|}{}                         & \multirow{3}{*}{20\%} & Prune  & 12.69          & 24.38          & 12.06          & 3.8           & 15.14          & 19.51          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 15.47          & 27.39          & 15.37          & 3.72          & 16.26          & 25.53          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{17.11} & \textbf{32.23} & \textbf{16.39} & \textbf{5.05} & \textbf{18.36} & \textbf{27.66} \\ \cline{2-9} 
\multicolumn{1}{c|}{}                         & \multirow{3}{*}{30\%} & Prune  & 4.33           & 9.32           & 3.58           & 1.52          & 6.49           & 5.47           \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 5.95           & 11.51          & 5.59           & 1.52          & 7.34           & 8.73           \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{8.22}  & \textbf{18.62} & \textbf{6.15}  & \textbf{2.59} & \textbf{9.86}  & \textbf{12.8}  \\   \hline \Xhline{2\arrayrulewidth}
\multicolumn{1}{c|}{\multirow{9}{*}{L1-norm}} & \multirow{3}{*}{10\%} & Prune  & 20.69          & 37.31          & 20.68          & 6.15          & 22.77          & 31.96          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 22.1           & 38.16          & 22.54          & 5.98          & 23.27          & 35.71          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{22.63} & \textbf{39.66} & \textbf{22.97} & \textbf{6.45} & \textbf{24.11} & \textbf{36.15} \\ \cline{2-9} 
\multicolumn{1}{c|}{}                         & \multirow{3}{*}{20\%} & Prune  & 10.87          & 21.95          & 9.78           & 3.51          & 13.62          & 15.82          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 14.59          & 25.6           & 14.39          & 3.5           & 15.6           & 23.59          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{16.06} & \textbf{31.18} & \textbf{15.08} & \textbf{4.95} & \textbf{17.28} & \textbf{25.77} \\ \cline{2-9} 
\multicolumn{1}{c|}{}                         & \multirow{3}{*}{30\%} & Prune  & 2.98           & 6.86           & 2.12           & 1.26          & 4.56           & 3.63           \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 5.85           & 11.57          & 5.36           & 1.59          & 7.07           & 7.97           \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{7.1}   & \textbf{16.73} & \textbf{4.97}  & \textbf{2.39} & \textbf{9.07}  & \textbf{10.85} \\  \hline \Xhline{2\arrayrulewidth}
\multicolumn{1}{c|}{\multirow{9}{*}{L2-GM}}   & \multirow{3}{*}{10\%} & Prune  & 20.68          & 37.09          & 20.68          & 6.42          & 22.3           & 32.1           \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 22.11          & 38.06          & 22.53          & 6.18          & 23.42          & 36.03          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{22.88} & \textbf{39.96} & \textbf{23.35} & \textbf{5.59} & \textbf{24.5}  & \textbf{36.45} \\ \cline{2-9} 
\multicolumn{1}{c|}{}                         & \multirow{3}{*}{20\%} & Prune  & 12.98          & 24.78          & 12.16          & 3.99          & 15.2           & 20.16          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 15.58          & 27.55          & 15.73          & 3.96          & 16.57          & 25.37          \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{17.87} & \textbf{33.24} & \textbf{17.34} & \textbf{5.31} & \textbf{19.26} & \textbf{28.85} \\ \cline{2-9} 
\multicolumn{1}{c|}{}                         & \multirow{3}{*}{30\%} & Prune  & 4.17           & 9.15           & 3.3            & 1.51          & 6.21           & 5.35           \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & NM     & 6.14           & 11.88          & 5.85           & 1.49          & 7.66           & 8.96           \\ \cline{3-9} 
\multicolumn{1}{c|}{}                         &                       & Ours   & \textbf{10.36} & \textbf{21.75} & \textbf{8.59}  & \textbf{3.17} & \textbf{11.65} & \textbf{16.3}  \\  \hline \Xhline{2\arrayrulewidth}
\end{tabular}
\vspace{3mm}
\caption{ Recovery Results of Single Shot Object Detetion based on ResNet-50 on COCO2017}
\label{tab:COCO_res}
\end{table*}


\subsection{Effectiveness in terms of Reconstruction}
In order to see the effectiveness of our loss function on minimizing the reconstruction error, we compute the values of three error components, namely RE, BE, and AE, using ResNet-50 \cite{ResNet} on CIFAR-100 \cite{krizhevsky2009learning}. Both RE and BE can be measured without any inference with sample data, but AE is not easy to obtain. Therefore, we adopt the \textit{weighted average reconstruction error} (\textit{WARE}) introduced in \cite{NISP}, which shows the difference between the layer-wise output of the original model and that of the pruned model.

Figure \ref{fig:error_components} shows layer-wise values for each of RE, BE and WARE. It is clearly observed that NM returns a very rough approximation in terms of RE, compared to our LBYL method. This is due to the fact that NM uses only a single remaining filter to receive the missing information of a pruned filter, and consequently the final output can be quite far from the original.
In both BE and WARE, there is not much difference between LBYL and NM in early layers, but we can observe that the error values of NM become higher in the end. Thus, even though NM might be effective to reconstruct the original output in some layers, it shows unstable performance depending on the existence of similar filters and finally ends up with less accurate approximation. On the other hand, our LBYL method is less varied across layers and managed to keep the error values lower in the end for all three metrics.

\revsec{Furthermore, we measure the average scale coefficients for each layer. As shown in Figure \ref{fig:expcos_:c}, the average norm of $\mathbf{s}$ of LBYL becomes smaller as the number of filters increases in deeper layers due to our regularization term $\|\mathbf{s}\|_{2}^2$, while the scale norm of NM gets rather larger in those deeper layers. This is somewhat consistent to the comparison result of WARE in Figure \ref{fig:expcos_:d}, where the performance gap between ours and NM gets larger in deeper layers.}

\begin{table*}[t]
\centering
% \color{blue}
\begin{tabular}{c||c|c|c|c|c} \Xhline{2\arrayrulewidth}
Pruning Ratio& Ours & NM& Coreset&Prune & From Scratch (80 epochs) \\ \Xhline{2\arrayrulewidth}
10 \% & \textbf{78.14 â†’ 78.3}   & 77.28 â†’ 77.57  & 50.28 â†’ 76.55& 75.14 â†’ 77.67  & 48.22 (72.07)\\\hline
20 \% & \textbf{76.15 â†’ 77.22}  & 72.73 â†’ 75.62  & 23.9 â†’ 74.39& 63.39 â†’ 75.4   & 47.85 (72.01)\\\hline 
30 \% & \textbf{73.29 â†’ 75.99}  & 64.47 â†’ 73.03  & 17.81 â†’ 71.88&  39.96 â†’ 73.18  & 46.7 (72.24)\\\hline
40 \% & \textbf{65.21 â†’ 74.13}  & 46.4 â†’ 69.71   & 6.13 â†’ 68.14&  15.32 â†’ 70.12  & 46.53 (72.88)\\\hline
50 \% & \textbf{52.61 â†’ 71.15}  & 25.98 â†’ 65.03  & 1.28 â†’ 62.88&  5.22 â†’ 65.72  & 44 (69.33)\\\Xhline{2\arrayrulewidth}
\end{tabular}
\vspace{2mm}
\caption{Fine-tuned or trained accuracies of ResNet-50 on CIFAR-100 using L2-norm criterion, where we fine-tune each model for 20 epochs and train the same-sized small architecture for 20 epochs (80 epochs)}
\label{tab:finetune_scratch}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table*}[]
% \color{blue}
% \centering
% \small
% \begin{tabular}{c||c|c|c} \Xhline{2\arrayrulewidth}
% { \textbf{Pruning   ratio}} & { \textbf{LBYL (before â†’ after   fine-tuning)}} & { \textbf{NM (before â†’ after   fine-tuning)}} & { \textbf{Prune (before â†’ after   fine-tuning)}} \\ \Xhline{2\arrayrulewidth}
% { 10\%}                     & { 78.14 â†’ 78.30}                                & { 77.28 â†’ 77.57}                              & { 75.14 â†’ 77.67}                                 \\ \hline
% { 20\%}                     & { \textbf{76.15 â†’ 77.22}}                       & { 72.73 â†’ 75.62}                              & { 63.39 â†’ 75.4}                                  \\ \hline
% { 30\%}                     & { 73.29 â†’ 75.99}                                & { 64.47 â†’ 73.03}                              & { 39.96 â†’ 73.18}                                 \\ \hline
% { 40\%}                     & { 65.21 â†’ 74.13}                                & { 46.4 â†’ 69.71}                               & { 15.32 â†’ 70.12}                                 \\ \hline
% { 50\%}                     & { 52.61 â†’ 71.15}                                & { 25.98 â†’ 65.03}                              & { 5.22 â†’ 65.72}                                  \\ \hline
% \end{tabular}
% \vspace{2mm}
% \caption{Fine-tuned or trained accuracies of ResNet-50 on CIFAR-100 using L2-norm criterion, where we fine-tune each model for 20 epochs and train the same-sized small architecture for 20 epochs (80 epochs)}
% \end{table*}


\begin{figure*}[t]
	\centering 
	\includegraphics[height=2mm]{./figure/f4_KEY.pdf} \\
    \subfigure[\label{fig:expcos:a}10\%]{\hspace{0mm}\includegraphics[width=0.45\columnwidth]{./figure/f4_10.pdf}\hspace{2mm}}
    \subfigure[\label{fig:expcos:b}30\%]{\hspace{0mm}\includegraphics[width=0.45\columnwidth]{./figure/f4_30.pdf}\hspace{2mm}}
    \subfigure[\label{fig:expcos:c}50\%]{\hspace{0mm}\includegraphics[width=0.45\columnwidth]{./figure/f4_50.pdf}\hspace{2mm}}
    \vspace{2mm}
    \caption{Comparison on learning curves of fine-tuning restored networks for 20 epochs and that of training the same-sized small architecture from scratch for 80 epochs at different pruning ratios}
	\label{fig:Fine tuning and From Scratch}
\end{figure*}

\smalltitle{Performance estimation}
One can be curious about how to set the appropriate pruning ratio for a particular target size and accuracy if we cannot use any data during the entire pruning and recovery process. Our two data-free error terms (i.e., RE and BE) can also be utilized in such a case. As shown in Figure \ref{fig:error_components}, we can roughly estimate WARE, which is also highly relevant to the final performance, by combining RE and BE.

\subsection{Practical Test with Data and Fine-Tuning} \label{sec:praticaltest}
Although this article focuses on the problem of restoring pruned networks without data, we additionally test whether our method is also effective when the data becomes available later on. Table \ref{tab:finetune_scratch} shows the experimental results on the \revfour{initial and} final accuracy of either pruned or restored model after fine-tuning along with that of the same-sized architecture trained from scratch. In this experiment, we take the data-independent pruning method based on \textit{Coreset} \cite{CoreSet_ICLR} as another competitor, which also introduces a training-free recovery method as an intermediate process. \revfour{We examine how much the initially measured performance right after recovery has been improved with the help of light finetuning for 20 epochs, and also show the resulting accuracy of the model trained from scratch after 80 epochs.} In all cases, our LBYL method outperforms its competitors particularly when the pruning ratio increases. Our best guess on why NM becomes eventually worse than just pruned model is that NM can choose a dissimilar filter to recover each pruned one in some layers, which makes it hard for fine-tuning to improve the performance. Coreset \cite{CoreSet_ICLR} shows the lowest accuracy probably because it does not focus on training-free recovery and hence 20 epochs of fine-tuning is not sufficient. As shown in Figure \ref{fig:Fine tuning and From Scratch}, \revfour{LBYL shows the fastest convergence speed during the fine-tuning process to the point that it takes only a few epochs to reach the maximum accuracy. Both of these observations justify the fact that our LBYL method is not just theoretically rigorous but also practically meaningful when the data is available.}


% \TODO{Add Coreset method in Table 4 and Figure 4. Change Table 4 to have arrows}
% We also compare the Coreset\cite{CoreSet_ICLR} from the standpoint that update the weights of pruned model as shown in the Figure 4. \ref{fig:Fine tuning and From Scratch},where we present average accuracy of the Coreset \cite{CoreSet_ICLR}. The reason why Coreset \cite{CoreSet_ICLR} shows the lowest accuracy is that Coreset \cite{CoreSet_ICLR} do not focus on directly minimizing the output between pruned model and pre-trained model but concentrate on proving the error bound for output between pruned model and pre-trained model, which results in insignificant compensation.


% We measure the reconstruction errors (RE) generated by pruned filter and preserved filter and batch normalization error, which can minimize the reconstruction error, where both RE and BE are taken l2-norm to show magnitude. LBLY optimize the scale parameter to minimize the both RE and BE. Therefore, Our method has a lower error compared to NM \cite{NM} as shown in Figure \ref{fig:error_components}.Moreover we also calculate the Weighted Average Reconstruction Error (WARE) used in \cite{NISP} to measure the difference between original model and recovered model. From Figure \ref{fig:error_components}, LBLY tends to increase more gently in last layer because our method is based on the number of filters,leads to small reconstruction error than NM with considering relationship between pruned filter and only one remained filter.\cite{NM} 




% \begin{figure*}[t]
% 	\centering
% 	{
%     \includegraphics[height=2.5mm]{./figure/key.pdf}	\vspace{1mm}

% 		\begin{tabular}{cccc}
% 			\subfigure[\label{fig:cifar:a}2 tasks] {\includegraphics[height=32mm]{./figure/LBYL_figure_2}}  & 
% 		\end{tabular}
% 		\vspace{-3mm}
% 		\caption{Comparison on the accuracy for each incremental task using CIFAR-100}
% 		\label{fig:cifar}
% 	}

% \end{figure*}



% \begin{table}[]
% {\scriptsize

% \begin{tabular}{c||c|c|c||c|c|c||c|c|c||c|c|c}  \Xhline{2\arrayrulewidth}
% \multicolumn{13}{c}{\textbf{ResNet-50 (Acc. 73.27) }}
% \\ \Xhline{2\arrayrulewidth} %\hline
% Criterion & \multicolumn{3}{c||}{L2-norm} & \multicolumn{3}{c||}{L2-GM} & \multicolumn{3}{c||}{L1-norm}& \multicolumn{3}{c}{Random}\\ \hline
% Pruning Ratio& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune& Ours& NM& prune
% \\ \Xhline{2\arrayrulewidth}
% 10\%& \textbf{77.31}& 77.28& 75.14& \textbf{77.20}& 76.92& 74.49& \textbf{77.55}& 77.21& 75.07& \textbf{76.14}& 72.18& 58.12\\ \hline
% 20\%& \textbf{73.24}& 72.73& 63.39& \textbf{73.46}& 72.32& 63.54& \textbf{73.20}& 72.24& 61.84& \textbf{70.59}& 59.62& 26.87\\ \hline
% 30\%& \textbf{66.63}& 64.47& 39.96& \textbf{67.07}& 64.01& 39.01& \textbf{66.30}& 63.07& 35.77& \textbf{62.22}& 39.45& 2.59 \\ \hline
% 40\%& \textbf{51.46}& 46.40& 15.32& \textbf{50.60}& 46.17& 13.14& \textbf{48.07}& 45.98& 12.59& \textbf{41.90}& 17.77& 3.08 \\ \hline
% 50\%& \textbf{28.74}& 25.98& 5.22& \textbf{30.66}& 23.44& 4.32& \textbf{22.06}& 21.98& 4.25& \textbf{22.07}& 7.17 & 2.20 \\ \Xhline{2\arrayrulewidth}
% Average Acc.& \textbf{59.48}& 57.37& 39.81& \textbf{59.80}& 56.57& 38.90& \textbf{57.44}& 56.10& 37.90& \textbf{54.58}& 39.24& 18.57 \\ 
% \Xhline{2\arrayrulewidth}
% \end{tabular}
% }
% \caption{Compenssation results of ResNet-50 on CIFAR-100}
% \label{tab:ResNet50-CIFAR100}

% \end{table}




\begin{table*}[t]
\centering
\small
\begin{tabular}{c||c|c|c}
\hline \Xhline{2\arrayrulewidth}
Method        & FLOPs (G) & Top-1 Acc (\%) &  Time for Pruning + Fine-Tuning (epochs)\\ \hline \Xhline{2\arrayrulewidth}
ResNet-50 (Original) & 4.1    & 76.1 & - \\ \hline \Xhline{2\arrayrulewidth}
DeepInversion\cite{DeepInversion} (w/ 0.1M ImageNet) & 2.9    & \textbf{74.9} & $\leq$ 22 (2.66 GFlops) + 30  \\ \hline
Ours (w/ 0.1M ImageNet)    &      2.9  &   70.9 & \textbf{0 + 1}     \\ \hline \Xhline{2\arrayrulewidth}
% DeepInversion\cite{DeepInversion} & 2.9    & 73.3 & 35673.6 + \underline{$\alpha$} \\ \hline
% Ours (w/o fine-tuning )       &    2.9    &   49.7 & 16.1   \\ \hline
% \Xhline{2\arrayrulewidth}
\end{tabular}
\vspace{2mm}
\caption{Fine-tuned accuracies of ResNet-50 on partial ImageNet}
\label{tab:finetune_synthetic}
\end{table*}
 

% \begin{table*}[t]
% \centering
% \small
% \begin{tabular}{c||c}
% \hline \Xhline{2\arrayrulewidth}
% Method        & latency (sec) \\ \hline \Xhline{2\arrayrulewidth}
% Red++\cite{Red++} & 5.01 \\ \hline
% OURS  &       2.08 (x2.41)  \\ \hline
% \Xhline{2\arrayrulewidth}
% \end{tabular}
% \vspace{2mm}
% \caption{Latency test on CPU (Intel Core Xeon Gold5122 ).}
% \label{tab:inference_res}
% \end{table*}


\subsection{Comparison with SOTA Data-Free Compression Methods}
\rev{
Since this article focuses on the problem of restoring pruned networks without any retraining, it is not trivial to make a fair comparison with data-free compression methods fully supported by retraining. Among various data-free methods, we compare our method with DeepInversion \cite{DeepInversion}, as it also utilizes pruning together with retraining process using a generator. DeepInversion fine-tunes a pruned network using synthetic data that is generated by its own generator, which however cannot be used to fine-tune our restored networks as the generator is tailored to the iterative pruning strategy of DeepInversion. Therefore, we employ a partial training dataset (0.1M ImageNet) to fine-tune either the network pruned by DeepInversion or our restored network. As shown in Table \ref{tab:finetune_synthetic}, our methods shows 4\% drop in accuracy, compared to DeepInversion. In fact, this performance gap comes from their different pruning schemes, namely one-shot pruning for ours and iterative pruning for DeepInversion. Thus, our LBYL method performs one-shot pruning for a pretrained model, and then we fine-tune the recovered model only for one epoch to achieve 70.9\% accuracy, while DeepInversion iteratively prunes filters during fine-tuning, which usually takes a long time to reach to a target compression ratio (e.g., 10 epochs for 3.27 GFlops and 22 epochs for 2.66 GFlops \cite{DeepInversion}).
}

% we also compare \cite{DeepInversion} method, which prunes network and then fine tunes model for 30 epochs using partial training (0.1M ImageNet) or synthetic data (315K images) from using \cite{DeepInversion}. First of all, we fine tune recovered model to enhance performance using partial ImageNet data. Our method shows 3.2 accuracy \% drop compared to DeepInversion \cite{DeepInversion} . These performance difference in Table \ref{tab:finetune_synthetic} is derived from pruning method, which means one-shot or iterative pruning. Our method use one-shot pruning scheme using l2-norm and then fine-tuning such as fine-tuning experiments \ref{tab:finetune_scratch}, while DeepIversion\cite{DeepInversion} iteratively prunes filter using first-order Taylor expansion during fine-tuning, which takes a long time to prune model until the pre-defined compression ratio is met. 

% Therefore, Our method can fast recover the performance drop after pruning. Moreover, we compare to \cite{DeepInversion} without fine-tuning, which shows considerably speed of accuracy recovery. Furthermore, we also compare with \cite{Red++} to measure inference speed on off-the-shelf soft/hardware. It is worth mentioning that we use identical pruning ratio for input-wise splitting and output-wise merging and assume preserved in-channels of convolution filters in input-wise merging step are independent to accelerate inference speed on off-the-shelf soft/hardware,which means we use not indexing but slicing in matrix multiplication. We test a 3x3 convolution with 5 channels on random noise of shape 3x224x224x3 for 10 times. As shown in \ref{tab:inference_res}, Our method shows a speed accelerated by 2.4 times compared with \cite{Redplus}. In fact, difference of inference time between ours and \cite{Red++} on Intel Core Xeon Gold5122 arises from a duplication operation after matrix multiplication. More experiments details for inference test with \cite{Red++}, we post our implementation on our github\footnote{https://github.com/bigdata-inha/LBYL}. General structured pruning methods without any soft/hardware support including our method can accelerate network inference speed on off-the-shelf soft/hardware. That is, pruning method that don't need customized soft/hardware can more accelerate on general hardware. For examples, we can optimize pruned model on general hardware(e.g., NVIDIA GPU, Intel CPU, Qualcomm CPU, etc.) using backend optimization library (e.g,.TensorRT \footnote{https://github.com/NVIDIA/TensorRT} and OpenVINO \footnote{https://github.com/openvinotoolkit/openvino}, AIMET {https://github.com/quic/aimet}
% ).
% \TODO{add experimental results}


\subsection{\revfour{Neuron Version of LBYL with LeNet-300-100}}
%  Even though this paper focuses on filter pruning, our LBYL method can also be applied to prune and restore neurons in vanilla feed-forward neural networks consisting of only FC layers. We present the details in Appendix due to the space limitation
\revfour{As discussed earlier, our LBYL method can also be extended to \textit{neuron pruning}, instead of filter pruning, in a vanilla feed-forward neural network such as LeNet-300-100 \cite{LeNet}. Table \ref{tab:LeNet_FashionMNIST} shows the experimental results of LeNet-300-100 on FashionMNIST by following the same setup of \textit{Coreset} \cite{CoreSet_ICLR}. It is well observed that LBYL outperforms the training-free recovery version of Coreset with clear margins. Although LBYL occasionally takes the second place, it still manages to achieve the best accuracy in overall.}


\begin{table*}[t!]
\centering
% \color{blue}
\begin{tabular}{c||ccc||ccc||ccc||c}
\Xhline{2\arrayrulewidth}
\multicolumn{11}{c}{\textbf{LeNet-300-100 (Acc. 89.51) }} \\ \Xhline{2\arrayrulewidth} %\hline
\hline Criterion& \multicolumn{3}{c||}{L2-norm}& \multicolumn{3}{c||}{L2-GM}& \multicolumn{3}{c||}{L1-norm}& \multirow{2}{*}{Coreset} \\ \cline{1-10}
Pruning Ratio& \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune & \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune & \multicolumn{1}{c|}{Ours}& \multicolumn{1}{c|}{NM}& Prune &\\ \hline \Xhline{2\arrayrulewidth}
50 \%& \multicolumn{1}{c|}{\textbf{88.83}} & \multicolumn{1}{c|}{87.86}          & 87.86 & \multicolumn{1}{c|}{\textbf{88.69}}  & \multicolumn{1}{c|}{88.57} & 88.08 & \multicolumn{1}{c|}{\textbf{89.03}} & \multicolumn{1}{c|}{88.69} & 88.40 & 79.34\\ \hline
60 \%& \multicolumn{1}{c|}{87.75}& \multicolumn{1}{c|}{\textbf{88.07}} & 83.03 & \multicolumn{1}{c|}{\textbf{88.15}}  & \multicolumn{1}{c|}{88.10} & 85.82 & \multicolumn{1}{c|}{\textbf{87.55}} & \multicolumn{1}{c|}{86.92} & 85.17 & 69.41\\ \hline
70 \%& \multicolumn{1}{c|}{\textbf{83.92}} & \multicolumn{1}{c|}{83.27}& 71.21 & \multicolumn{1}{c|}{85.92}& \multicolumn{1}{c|}{\textbf{86.39}} & 78.38 & \multicolumn{1}{c|}{\textbf{84.57}} & \multicolumn{1}{c|}{82.75} & 71.26 & 62.31\\ \hline
80 \%& \multicolumn{1}{c|}{\textbf{78.05}} & \multicolumn{1}{c|}{77.11}& 63.90 & \multicolumn{1}{c|}{\textbf{77.63}} & \multicolumn{1}{c|}{77.49} & 64.19 & \multicolumn{1}{c|}{\textbf{80.55}} & \multicolumn{1}{c|}{80.02} & 66.76 & 49.68\\ \hline \Xhline{2\arrayrulewidth}
\end{tabular}
\vspace{2mm}
\caption{Recovery results of LeNet-300-100 on FashionMNIST }
\label{tab:LeNet_FashionMNIST}
\end{table*}

\begin{table*}[h]
% \color{blue}
\centering
\small
\begin{tabular}{c||c |c |c|c|c |c |c |c |c } \Xhline{2\arrayrulewidth}
% \multicolumn{7}{c}{\textbf{ResNet-50 (Top-1 Acc. 78.82 / MACs. 64.27 G / Params. 19.68M) }} \\ \Xhline{2\arrayrulewidth} %\hline
\multicolumn{10}{c}{\textbf{ResNet-50 (Top-1 Acc. 78.82 / MACs. 1.31 G / Params. 23.71M) }} \\ \Xhline{2\arrayrulewidth} %\hline
\multicolumn{1}{c||}{Thresholds} & \multicolumn{3}{c|}{ 0.1 } & \multicolumn{3}{c|}{0.3} & \multicolumn{3}{c}{0.5} \\ \hline
 Methods & Ours & NM  &Prune & Ours & NM &Prune & Ours & NM   &Prune\\ \Xhline{2\arrayrulewidth}
% Top-1 Acc (\%) & \textbf{76.82} & 76.28 & \textbf{69.66} & 66.53 &  \textbf{50.11}& 29.4  \\ \hline
% Parmas (M) & \textbf{19.86} & 20.39& \textbf{17.47} & 19.45&  \textbf{16.47} &17.18  \\ \hline
% MACs (G) & \textbf{50.01} & 50.85&  \textbf{36.44} & 42.14&  \textbf{29.76} & 31.58  \\ \hline 
Top-1 Acc (\%) & \textbf{69.66} & 66.07 & 62.31 & \textbf{65.58} & 47.1 & 24.61 & \textbf{41.53}& 6.6 & 4.65\\ \hline
Parmas (M) & \textbf{17.47} & 20.01& 20.13 & \textbf{14.80} & 14.83& 14.85& 10.49 &10.49  & 10.49\\ \hline
MACs (G) & \textbf{0.74} & 0.90& 0.95 &  0.95 & \textbf{0.70}& 0.73& 0.53 & 0.53 & 0.53 \\ \hline 

\Xhline{2\arrayrulewidth}
\end{tabular}
\vspace{2mm}
\caption{Recovered accuracies of ResNet-50 on CIFAR100 in global adaptive pruning}
\label{tab:global_pruning_resnet50_cifar100}
\end{table*}

\begin{table}[t!]
\centering
% \color{blue}
\begin{tabular}{c||c|c|c}
\hline
      & \ MACs(G) & \ Params(M) & Accuracy(\%) \\ \hline \Xhline{2\arrayrulewidth}
Ours  & 0.74    & 17.47        & 94.05        \\ \hline
NM    & 0.90     & 20.01      & 93.88        \\ \hline
Prune & 0.95     & 20.13        & 93.98        \\ \hline
Fine-tuning  & 1.31    & 23.72        & 82.66        \\ \hline
\end{tabular}
\vspace{2mm}
\caption{Transfer-learning accuracies of ResNet-50 from CIFAR-100 to CIFAR-10}
\label{tab:downstream_cifar}
\end{table}




\begin{table}[t!]
\centering
% \color{blue}
\begin{tabular}{c||c|c|c}
\hline
      & \ MACs(G) & \ Params(M) & Accuracy(\%) \\ \hline \Xhline{2\arrayrulewidth}
Ours  & 2.69     & 15.46        & 91.98        \\ \hline
NM    & 2.84   & 17.09       & 91.36        \\ \hline
Prune & 2.96    & 17.13        & 91.8        \\ \hline
Fine-tuning  & 3.68    & 21.8       & 92.54        \\ \hline

\end{tabular}
\vspace{2mm}
\caption{Transfer-learning accuracies of ResNet-34 from ImageNet to CUB-200-2011}
\label{tab:downstream_cub200}
\end{table}


\subsection{Recovery Performance with Global Pruning}
\revsec{We examine how the recovery performance of LBYL would be if we adopt a global pruning scheme, where we adaptively changes pruning ratios across layers rather than applying the same pruning ratio to every layer. In order to measure the pruning intensity of each layer, we employ WARE between the original model and the model being pruned and recovered, and set a threshold value, which indicates the maximum WARE value during the entire pruning procedure. More specifically, we repeat an iterative procedure that first prunes a small number of filters (\textit{e.g.,} 10\%) for each layer, followed by a corresponding recovery process without training. Then, we check if the resulting WARE value exceeds a pre-defined threshold value, and stop pruning for the layer if it is the case. Varying the threshold values from 0.1 to 0.5, Table \ref{tab:global_pruning_resnet50_cifar100} demonstrates that our LBYL method outperforms NM \cite{NM} in particular for the highest pruning intensity with the threshold 0.5. Notably, it is also observed that LBYL can prune more filters and hence ends up with smaller compressed models, while still achieving higher accuracies than NM.}




\subsection{\revfin{Effectiveness of Recovering Transferable Knowledge}}

\revfin{Finally, we examine whether our method can be effective to make pruned models well-adapted to downstream tasks. To this end, we first prune and recover pretrained models on more comprehensive datasets (e.g., CIFAR-100 and ImageNet) with global pruning scheme, and then fine-tune the recovered models on less complicated (e.g., CIFAR-10) or less generic (e.g., CUB-200-2011) datasets during only 20 epochs. As clearly observed in Tables \ref{tab:downstream_cifar} and \ref{tab:downstream_cub200}, our LBYL method reaches to the highest accuracy, even with a higher compression ratio (i.e., a smaller model size). This explains the effectiveness of LBYL at recovering more transferable knowledge from a pretrained model to downstream tasks.}








