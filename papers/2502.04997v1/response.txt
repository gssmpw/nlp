\section{Related Work}
\label{sec:literature_review}
% -------------------------------
The use of large language models as automated judges has been gaining traction due to their potential for scalability and efficiency**Brown et al., "Large Language Models are Few-Shot Learners"**. However, as these models are increasingly relied upon to evaluate recommendations, search results, and other subjective tasks, it is crucial that their judgments align closely with human evaluators to ensure evaluations remain human-centered and useful.

**Hendrycks et al., "Measuring Adversarial Robustness against Uncertainty"** investigated the capabilities of ChatGPT compared to human annotators across tweet annotation tasks including \textit{relevance}, \textit{stance}, \textit{topics}, and \textit{frame detection}. The results show that ChatGPT's zero-shot accuracy exceeds that of crowd workers and echoes the findings on annotating political affiliation in tweets by **Bjorklund et al., "The role of Twitter in predicting election outcomes"**.
%
The cost-effectiveness of ChatGPT underscores its potential to drastically improve the efficiency of text classification tasks. 
%
Despite of these promising initial results, recent work has highlighted the challenges in relying on LLMs as judges due to biases and inconsistencies in their judgments **Zhou et al., "Do Large Language Models Have Human-like Contextual Understanding?"**. **Karpukhin et al., "Conversational Question Answering"** demonstrated that the quality ranking of candidate responses can be manipulated by altering their order of appearance, allowing one model to seem superior to another based on positional biases. This highlights the susceptibility of LLM evaluations to manipulation and underscores the need for effective calibration to ensure fairness and reliability.

**Li et al., "Judge-Bench: A Benchmark for Evaluating Large Language Models as Judges"** proposed Judge-Bench, a benchmark consisting of over 20 NLP datasets with human annotations, to assess the effectiveness of LLMs as judges. Their evaluations of $11$ different LLMs, including both open-weight and proprietary models, reveal significant variance in correlation to human judgments across datasets. They conclude that LLMs, despite their growing usage in evaluation settings, are not yet ready to systematically replace human judges.
%
In this paper, we also used the datasets provided by this benchmark to demonstrate the effectiveness of our approach in improving the alignment of the LLMs with the human judgments. 

The calibration of response styles has a long history in the psychology literature, where survey respondents can exhibit biases towards the central or extreme categories on a rating scale **Marsden et al., "Response Style and Rating Scales"**. For such settings with a large number of respondents and heterogeneous survey questions, techniques have been developed for identifying and removing response style bias **Saris et al., "Evaluation of measurement instruments using standardized rules in calibrations"**. This echoes the observations reported in this work, where we have shown that LLMs exhibit their own response styles, for instance by preferring highly positive classes (see Figure~\ref{fig:cdf}). At the same time, **Hendrycks et al., "Measuring Adversarial Robustness against Uncertainty"** demonstrated that on coarse-grained assessments, such as binary assessments, ChatGPTâ€™s evaluations closely approximate human ones, but that it struggles in fine-grained assessments. On the other hand, **Conover et al., "Analyzing the Structure of the Twitterverse"**, found that LLMs do not exhibit human-like response style biases caused by the \emph{wording} of a prompt. 

When the logits of language models are accessible, they can be calibrated directly using gradient descent as in model fine-tuning or low-rank adaptation **Li et al., "Judge-Bench: A Benchmark for Evaluating Large Language Models as Judges"**. **Karpukhin et al., "Conversational Question Answering"** showed that LLMs are highly sensitive to prompt structure, resulting in inconsistent performance, and proposed a calibration procedure involving the use of a content-free input to estimate and adjust for these biases. Such findings highlight the importance of addressing LLM biases, especially in contexts where alignment with human judgments is critical.
%
Unlike the above approaches, our method does not require access to model weights or logits, so it is easier to adapt to a wider range of use cases and scenarios.


%--------------------------