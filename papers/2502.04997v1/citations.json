[
  {
    "index": 0,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging {LLM-as-a-Judge} with {MT}-{B}ench and {C}hatbot {A}rena"
      },
      {
        "key": "bavaresco2024llms",
        "author": "Anna Bavaresco and Raffaella Bernardi and Leonardo Bertolazzi and Desmond Elliott and Raquel Fern\u00e1ndez and Albert Gatt and Esam Ghaleb and Mario Giulianelli and Michael Hanna and Alexander Koller and Andr\u00e9 F. T. Martins and Philipp Mondorf and Vera Neplenbroek and Sandro Pezzelle and Barbara Plank and David Schlangen and Alessandro Suglia and Aditya K Surikuchi and Ece Takmaz and Alberto Testoni",
        "title": "{LLMs} instead of Human Judges? {A} Large Scale Empirical Study across 20 {NLP} Evaluation Tasks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "gilardi2023chatgpt",
        "author": "Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\\\"e}l",
        "title": "{ChatGPT} outperforms crowd workers for text-annotation tasks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "tornberg2023chatgpt",
        "author": "Petter T{\\\"o}rnberg",
        "title": "{ChatGPT-4} Outperforms Experts and Crowd Workers in Annotating Political {Twitter} Messages with Zero-Shot Learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu2023style",
        "author": "Wu, Minghao and Aji, Alham Fikri",
        "title": "Style over substance: Evaluation biases for large language models"
      },
      {
        "key": "zheng2023large",
        "author": "Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie",
        "title": "Large language models are not robust multiple choice selectors"
      },
      {
        "key": "koo2024benchmarking",
        "author": "Koo, Ryan  and\nLee, Minhwa  and\nRaheja, Vipul  and\nPark, Jong Inn  and\nKim, Zae Myung  and\nKang, Dongyeop",
        "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
      },
      {
        "key": "hada2024large",
        "author": "Hada, Rishav  and\nGumma, Varun  and\nWynter, Adrian  and\nDiddee, Harshita  and\nAhmed, Mohamed  and\nChoudhury, Monojit  and\nBali, Kalika  and\nSitaram, Sunayana",
        "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?"
      },
      {
        "key": "pavlovic2024effectiveness",
        "author": "Pavlovic, Maja  and\nPoesio, Massimo",
        "title": "The Effectiveness of {LLM}s as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2023large",
        "author": "Wang, Peiyi and Li, Lei and Chen, Liang and Cai, Zefan and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang",
        "title": "Large language models are not fair evaluators"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bavaresco2024llms",
        "author": "Anna Bavaresco and Raffaella Bernardi and Leonardo Bertolazzi and Desmond Elliott and Raquel Fern\u00e1ndez and Albert Gatt and Esam Ghaleb and Mario Giulianelli and Michael Hanna and Alexander Koller and Andr\u00e9 F. T. Martins and Philipp Mondorf and Vera Neplenbroek and Sandro Pezzelle and Barbara Plank and David Schlangen and Alessandro Suglia and Aditya K Surikuchi and Ece Takmaz and Alberto Testoni",
        "title": "{LLMs} instead of Human Judges? {A} Large Scale Empirical Study across 20 {NLP} Evaluation Tasks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jackson1958content",
        "author": "Jackson, Douglas N. and Messick, Samuel",
        "title": "Content and style in personality assessment"
      },
      {
        "key": "paulhus1991measurement",
        "author": "Paulhus, Delroy L.",
        "title": "Measurement and Control of Response Bias"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "van2010identifying",
        "author": "Van Rosmalen, Joost and Van Herk, Hester and Groenen, Patrick J. F.",
        "title": "Identifying response styles: A latent-class bilinear multinomial logit model"
      },
      {
        "key": "schoonees2015constrained",
        "author": "Schoonees, Pieter C. and Van de Velden, Michel and Groenen, Patrick J. F.",
        "title": "Constrained dual scaling for detecting response styles in categorical data"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "huang-etal-2024-chatgpt",
        "author": "Huang, Fan  and\nKwak, Haewoon  and\nPark, Kunwoo  and\nAn, Jisun",
        "title": "{C}hat{GPT} Rates Natural Language Explanation Quality like Humans: But on Which Scales?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "tjuatja2024llms",
        "author": "Tjuatja, Lindia and Chen, Valerie and Wu, Tongshuang and Talwalkwar, Ameet and Neubig, Graham",
        "title": "{Do {LLMs} Exhibit Human-like Response Biases? {A} Case Study in Survey Design}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "reif2024beyond",
        "author": "Reif, Yuval  and\nSchwartz, Roy",
        "title": "Beyond Performance: Quantifying and Mitigating Label Bias in {LLM}s"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhao2021calibrate",
        "author": "Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer",
        "title": "Calibrate Before Use: Improving Few-shot Performance of Language Models"
      }
    ]
  }
]