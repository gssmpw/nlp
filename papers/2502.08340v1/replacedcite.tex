\section{Related Works}
\textbf{Constructive Methods.} The learning-based constructive method aims to develop an efficient and (near-)optimal end-to-end neural solver for combinatorial optimization problems (COPs). Among them, the PointerNet____ and the Transformer____ are two commonly employed architectures trained using either SL____ or RL____, to progressively infer complete solutions with the aid of the autoregressive mechanism. Additionally, some methods consider inherent properties in VRPs, such as multiple optima____ and symmetry____, to enhance the quality of solutions. However, the delayed rewards in the training of RL policies lead to the high GPU memory demands for gradient backpropagation. Thus, SL-driven policies, such as BQ____, LEHD____ and SIL____, have resurfaced to alleviate training difficulties and moderately improve generalization. Moreover, some approaches employ techniques such as meta-learning____, knowledge distillation____, or ensemble learning____ to enhance the generalization of neural solvers. However, these constructive methods might still experience performance deterioration when encountering substantial distribution or scale shifts.

\noindent \textbf{Iterative Methods.} In comparison to constructive methods, iterative methods offer the benefit of consistently improving a given solution until convergence. Both L2I____ and NeuRewriter____ utilize RL policies to choose from local improvement operators to refine the given initial solutions. Likewise,____ interleaves the use of heuristic destroy operators and a set of learning-based repair policies to generate a new solution. Moreover, DACT____ focuses on the expressive representation of solution encodings provided to the RL policy. Additionally, both NeuralLKH____ and Neural k-Opt____ utilize the RL policy to substitute the heuristic rule for edge exchanges in k-opt algorithms. However, these iterative methods trade efficiency for improved performance and still rely heavily on well-crafted rules.

\noindent \textbf{Divide-and-Conquer Methods.} The divide-and-conquer paradigm can leverage local topological features that are insensitive to distribution or scale shifts to mitigate performance deterioration. ____, ____ and____ attempt to transfer a standard neural solver for larger instances by sampling multiple small-scale subgraphs using heuristic rules. In contrast, both L2D____ and RGB____ learn a policy to choose among heuristically constructed subgraphs for iterative enhancement. However, the used heuristic rules may lead to solutions being trapped in local optima. Unlike the above methods, TAM____, GLOP____ and UDC____ opt to use a learning-based policy to globally partition the entire instance into subproblems that are solved by the pretrained local construction policy. In addition,____ resort to a hierarchical RL model where a local policy solves subproblems assigned by a jointly trained global policy. However, these neural partition policies may suffer performance degradation due to compounded errors during the partition process.