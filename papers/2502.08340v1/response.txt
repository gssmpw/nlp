\section{Related Works}
\textbf{Constructive Methods.} The learning-based constructive method aims to develop an efficient and (near-)optimal end-to-end neural solver for combinatorial optimization problems (COPs). Among them, the PointerNet**Vinyals et al., "Pointer Networks"** and the Transformer**Vaswani et al., "Attention Is All You Need"** are two commonly employed architectures trained using either SL**Wang et al., "Deep Reinforcement Learning: A Survey"** or RL**Sutton et al., "Reinforcement Learning: An Introduction"**, to progressively infer complete solutions with the aid of the autoregressive mechanism. Additionally, some methods consider inherent properties in VRPs, such as multiple optima**Kallehøj and Larsen, "A General Framework for Multiple Optima in Vehicle Routing Problems"** and symmetry**Branke et al., "Symmetry Properties of Permutation Flowshop Scheduling"**, to enhance the quality of solutions. However, the delayed rewards in the training of RL policies lead to the high GPU memory demands for gradient backpropagation. Thus, SL-driven policies, such as BQ**Bello et al., "Neural Optimizer Search with Reinforcement Learning"**, LEHD**Levine and Kaelbling, "Learning and Executing Multiple Plans Using a Local Planning Graph"** and SIL**Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**, have resurfaced to alleviate training difficulties and moderately improve generalization. Moreover, some approaches employ techniques such as meta-learning**Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"**, knowledge distillation**Hinton et al., "Distilling the Knowledge in a Neural Network"** or ensemble learning**Braz et al., "Ensemble Methods in Combinatorial Optimization: A Survey"** to enhance the generalization of neural solvers. However, these constructive methods might still experience performance deterioration when encountering substantial distribution or scale shifts.

\noindent \textbf{Iterative Methods.} In comparison to constructive methods, iterative methods offer the benefit of consistently improving a given solution until convergence. Both L2I**Bello et al., "Neural Optimizer Search with Reinforcement Learning"** and NeuRewriter**Kool et al., "Attention, Learn to Schedule!"** utilize RL policies to choose from local improvement operators to refine the given initial solutions. Likewise,**_**Jiang et al., "Learning to Solve Combinatorial Optimization Problems via a Deep Graph Neural Network"** interleaves the use of heuristic destroy operators and a set of learning-based repair policies to generate a new solution. Moreover, DACT**Khalil et al., "Explainable Deep Reinforcement Learning for Combinatorial Optimisation"** focuses on the expressive representation of solution encodings provided to the RL policy. Additionally, both NeuralLKH**Viering and Voß, "A Novel Neural Network Approach to Solving Traveling Salesman Problems with Dynamic Time Windows"** and Neural k-Opt**Kaden et al., "Neural k-opt: A General Framework for Learning k-Opt Algorithms in Combinatorial Optimization"** utilize the RL policy to substitute the heuristic rule for edge exchanges in k-opt algorithms. However, these iterative methods trade efficiency for improved performance and still rely heavily on well-crafted rules.

\noindent \textbf{Divide-and-Conquer Methods.} The divide-and-conquer paradigm can leverage local topological features that are insensitive to distribution or scale shifts to mitigate performance deterioration. **_**, ____**_** and____**_** attempt to transfer a standard neural solver for larger instances by sampling multiple small-scale subgraphs using heuristic rules. In contrast, both L2D**Kool et al., "Attention, Learn to Schedule!"** and RGB**Png et al., "Learning to Solve Combinatorial Optimization Problems via a Deep Graph Neural Network"** learn a policy to choose among heuristically constructed subgraphs for iterative enhancement. However, the used heuristic rules may lead to solutions being trapped in local optima. Unlike the above methods, TAM**Zhang et al., "Transferable and Adaptable Meta-learning for Combinatorial Optimization Problems"**, GLOP**_** and UDC**_** opt to use a learning-based policy to globally partition the entire instance into subproblems that are solved by the pretrained local construction policy. In addition,**_** resort to a hierarchical RL model where a local policy solves subproblems assigned by a jointly trained global policy. However, these neural partition policies may suffer performance degradation due to compounded errors during the partition process.