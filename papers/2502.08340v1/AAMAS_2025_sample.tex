%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
%\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
\documentclass[sigconf,nonacm]{aamas} 


%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

\usepackage{booktabs}
\usepackage{amsmath}
%\frenchspacing  % DO NOT CHANGE THIS
\usepackage{subfigure}
%\usepackage{bbm}
%\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\usepackage{eucal}
\usepackage[cal=cm]{mathalfa}
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.2\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{by}}
  \end{minipage}\hfill
  \begin{minipage}{0.8\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
\makeatother

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Now√©  (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your OpenReview submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<OpenReview submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 Formatting Instructions]{Hierarchical Learning-based Graph Partition for \\Large-scale Vehicle Routing Problems}

% Add the subtitle below for an extended abstract
%\subtitle{Extended Abstract}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Yuxin Pan}
\affiliation{
  \institution{The Hong Kong University of Science and Technology}
  \city{Hong Kong}
  \country{China}
  }
\email{yuxin.pan@connect.ust.hk}

\author{Ruohong Liu}
\affiliation{
  \institution{University of Oxford}
  \city{Oxford}
  \country{United Kingdom}}
\email{ruohong.liu@eng.ox.ac.uk}

\author{Yize Chen}
\affiliation{
  \institution{University of Alberta}
  \city{Edmonton}
  \country{Canada}}
\email{yize.chen@ualberta.ca}


\author{Zhiguang Cao}
\affiliation{
  \institution{Singapore Management University}
  \city{Singapore}
  \country{Singapore}}
\email{zhiguangcao@outlook.com}

\author{Fangzhen Lin}
\affiliation{
  \institution{The Hong Kong University of Science and Technology}
  \city{Hong Kong}
  \country{China}}
\email{flin@cs.ust.hk}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). Additionally, we decompose the synchronized training into individual training of each component to circumvent the instability issue. Furthermore, we point out the importance of viewing the subproblems encountered during the partition process as individual training instances. Extensive experiments conducted on various CVRP benchmarks demonstrate the effectiveness and generalization of the HLGP framework. The source code is available at \href{https://github.com/panyxy/hlgp_cvrp}{https://github.com/panyxy/hlgp\_cvrp}.
\renewcommand{\thefootnote}{}
\footnotetext{This paper has been accepted as a Full Paper at the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025).}
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Vehicle Routing Problem; Combinatorial Optimization Problem; Hierarchical Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The Vehicle Routing Problem (VRP) is a widely-studied NP-hard problem which has many real-world applications including transportation~\cite{garaix2010vehicle}, logistic~\cite{cattaruzza2017vehicle}, and digital e-commerce~\cite{liu2017capacitated}. Exact methods for solving VRP often use mixed integer linear programming (MILP) techniques and employ MILP solvers to generate optimal solutions with theoretical guarantees~\cite{laporte1983branch}. However, these methods so far are not computationally efficient enough to handle large-scale instances, particularly for the applications with time-sensitive and dynamically changing VRP scenarios. In contrast, heuristic methods such as LKH3~\cite{helsgaun2017extension} and HGS~\cite{vidal2012hybrid} aim to generate high-quality solutions quickly. They commonly improve the quality of the existing solution incrementally by local search techniques. However, in addition to the heavy reliance on the quality of handcrafted local operators, these methods are not robust and often need to start from scratch for the problem instances with slight variations.


More recently, there has been much work on neural network-based solvers for VRP. Experimentally they have been shown capable of inferring near-optimal efficiently solutions for instances which fall within the training data distribution. These learning-based solvers typically use one of the following methods: constructive, iterative, and divide-and-conquer. The constructive method, as a pioneering paradigm, incrementally deduces a complete solution starting from an empty state~\cite{vinyals2015pointer, nazari2018reinforcement, kool2018attention, xin2020step, kwon2020pomo, kim2022sym, liu20242d}. However, challenges arise when dealing with out-of-distribution instances due to the limited expressivity of neural network and the intricate search landscape. To mitigate this performance degradation, the iterative method merges a neural network-based policy with heuristic local operators to progressively refine the current solution~\cite{lu2019learning, chen2019learning, hottung2020neural, ma2021learning, xin2021neurolkh, ma2024learning}. Yet, this approach relies on numerous improvement steps with well-crafted local operators for satisfactory solutions. By comparison, the divide-and-conquer approach embraces either a heuristic-based partition policy~\cite{fisher1981generalized, fu2021generalize, kim2021learning, li2021learning, zong2022rbg, cheng2023select} or a neural partition policy~\cite{pan2023h, hou2023generalize, ye2024glop, zheng2024udc} to globally divide the entire graph into subgraphs and employ a local construction policy to solve subproblems. However, a failure in either component policy may lead to a significant performance drop. Moreover, heuristic-based partition rules often result in local optima, and neural partition policies may be vulnerable to distribution shifts. Hence, there is a pressing need for a more generalizable and meticulous partition policy, which is the focus of this paper.


In the divide-and-conquer paradigm, the local construction policy agent benefits from the local topological features within subproblems insensitive against distribution and scale shifts, contributing to the (near-)optimality of solutions of subproblems~\cite{jiang2023multi, gao2023towards, fang2024invit}. However, during the multi-step decoding process of the learning-based partition policy for Capacitated VRP (CVRP) instances~\cite{hou2023generalize, ye2024glop}, the decoding of clustered nodes in each step relies on the partial partition solution from the preceding step. This implies that errors in the clustering from earlier steps have a tendency to propagate and result in a chain of misclusterings in subsequent steps, called as compounded errors. Consequently, even with an optimal local construction policy, deficiencies in the partition task lead to substantial deviations from the ideal policy in the overall system. Therefore, we argue that the partition task holds a critical position in the overall decomposition-based system for solving CVRP. Furthermore, the success of the local construction policy inspires us to introduce a local partition policy which aims to progressively alleviate compounded errors by harnessing local topological features in the partition task. We thus consider to implement a hierarchical learning (HL) framework specifically designed for the partition task in CVRP, which is capable of seamlessly integrating both global and local partition policies. In prevailing HL frameworks, a high-level policy is adopted to derive a series of simpler sub-tasks which are then delegated to the low-level policy, with the aim of facilitating exploration~\cite{pateria2021hierarchical}. These frameworks predominantly focus on reinforcement learning and undergo joint training of the associated policies~\cite{levy2017learning}. Yet, these HL frameworks have not been extensively explored in addressing compounded errors within the graph partition task of large-scale CVRP. In contrast, our study extends the HL framework to the partition task of CVRP and demonstrates its efficacy in mitigating compounded errors.


In this paper, we present a versatile Hierarchical Learning-based Graph Partition (HLGP) framework specifically tailored for the partition task in CVRP, which synergistically integrates both global and local partitioning policies. To be specific, our method formulates the partition problem of CVRP using a multi-level HL framework. At the global partition level, the global partition policy is responsible for initiating a coarse multi-way partition to create a series of simpler 2-way partition subtasks. These subtasks stand as the starting point for the subsequent \emph{K} local partition levels. At each local partition level, a tailored sequence of subtasks is derived from the partition solution of the preceding level. These subtasks are then fed into the local partition policy. Such a setup allows the local partition policy to mitigate the potential misclustering arising from the previous level by leveraging the insensitive local topological features inherent in the subtask. By enabling the local partition policy to traverse through the subtasks at each local partition level,  the compounded misclusterings can be mitigated progressively across levels as a consequence.

Our proposed HLGP framework is versatile, featuring a unified objective that effortlessly accommodates both reinforcement learning (RL) and supervised learning (SL) for training the partition policies. It is worth noting that unlike prior approaches that utilized SL to directly train the neural solver, our method explores the application of SL for training the partition policy, usually omitting the need for permutation information. Additionally, the joint training of the involved policies is disentangled to mitigate training instability. Moreover, by conducting in-depth analyses in both the RL and SL settings, we shed light on the importance of viewing the subproblems encountered during the partition process as individual training instances. Empirically, the proposed HLGP framework convincingly demonstrates its superiority through extensive experiments on various CVRP benchmarks over previous SOTA methods. In particular, our method can scale up to CVRP10K instances with around $10\%$ performance improvement over current literature. 


\section{Related Works}
Learning-based methods for solving combinatorial optimization problems (COPs) typically fall into three main categories: \textbf{constructive methods}, \textbf{iterative methods}, and \textbf{divide-and-conquer methods}. Constructive methods aim to progressively infer complete solutions using the autoregressive mechanism~\cite{vinyals2015pointer, nazari2018reinforcement, kool2018attention, kwon2020pomo, kim2022sym, son2023meta, zhou2023towards, manchanda2022generalization, qiu2022dimes, gao2023towards, jiang2024ensemble, grinsztajn2023winner, bi2022learning}. Impressively, SL-driven constructive policies, such as BQ~\cite{drakulic2024bq}, LEHD~\cite{luo2024neural} and SIL~\cite{luo2024self} can mitigate the high GPU memory demands associated with gradient backpropagation by eliminating the need for delayed rewards in the training of RL algorithms. Iterative methods offer the benefit of consistently improving a given solution until convergence~\cite{lu2019learning, chen2019learning, hottung2020neural, ma2021learning, xin2021neurolkh, ma2024learning} by integrating local improvement operators into RL policies. The divide-and-conquer paradigm can exploit local topological features that remain insensitive to distribution or scale shifts, thus alleviating performance degradation. Some methods harness heuristic rules for the partitioning process~\cite{fu2021generalize, kim2021learning, cheng2023select, li2021learning, zong2022rbg}. In contrast, H-TSP~\cite{pan2023h}, TAM~\cite{hou2023generalize}, GLOP~\cite{ye2024glop}, and UDC~\cite{zheng2024udc} opt to use a learning-based policy to globally divide the entire instance into subproblems, which are then addressed by a pretrained local construction policy. \emph{For a more detailed review of related algorithms used to solve VRPs, please refer to the Appendix-D.}


%\textbf{Constructive Methods.} The learning-based constructive method aims to develop an efficient and (near-)optimal end-to-end neural solver for combinatorial optimization problems (COPs). Among them, the PointerNet~\cite{vinyals2015pointer} and the Transformer~\cite{vaswani2017attention} are two commonly employed architectures trained using either SL~\cite{vinyals2015pointer} or RL~\cite{nazari2018reinforcement, kool2018attention}, to progressively infer complete solutions with the aid of the autoregressive mechanism. Additionally, some methods consider inherent properties in VRPs, such as multiple optima~\cite{kwon2020pomo} and symmetry~\cite{kim2022sym}, to enhance the quality of solutions. However, the delayed rewards in the training of RL policies lead to the high GPU memory demands for gradient backpropagation. Thus, SL-driven policies, such as BQ~\cite{drakulic2024bq}, LEHD~\cite{luo2024neural} and SIL~\cite{luo2024self}, have resurfaced to alleviate training difficulties and moderately improve generalization. Moreover, some approaches employ techniques such as meta-learning~\cite{son2023meta, zhou2023towards, manchanda2022generalization, qiu2022dimes}, knowledge distillation~\cite{bi2022learning}, or ensemble learning~\cite{gao2023towards, jiang2024ensemble, grinsztajn2023winner} to enhance the generalization of neural solvers. However, these constructive methods might still experience performance deterioration when encountering substantial distribution or scale shifts.

%\noindent \textbf{Iterative Methods.} In comparison to constructive methods, iterative methods offer the benefit of consistently improving a given solution until convergence. Both L2I~\cite{lu2019learning} and NeuRewriter~\cite{chen2019learning} utilize RL policies to choose from local improvement operators to refine the given initial solutions. Likewise,~\citet{hottung2020neural} interleaves the use of heuristic destroy operators and a set of learning-based repair policies to generate a new solution. Moreover, DACT~\cite{ma2021learning} focuses on the expressive representation of solution encodings provided to the RL policy. Additionally, both NeuralLKH~\cite{xin2021neurolkh} and Neural k-Opt~\cite{ma2024learning} utilize the RL policy to substitute the heuristic rule for edge exchanges in k-opt algorithms. However, these iterative methods trade efficiency for improved performance and still rely heavily on well-crafted rules.

%\noindent \textbf{Divide-and-Conquer Methods.} The divide-and-conquer paradigm can leverage local topological features that are insensitive to distribution or scale shifts to mitigate performance deterioration. ~\citet{fu2021generalize}, ~\citet{kim2021learning} and~\citet{cheng2023select} attempt to transfer a standard neural solver for larger instances by sampling multiple small-scale subgraphs using heuristic rules. In contrast, both L2D~\cite{li2021learning} and RGB~\cite{zong2022rbg} learn a policy to choose among heuristically constructed subgraphs for iterative enhancement. However, the used heuristic rules may lead to solutions being trapped in local optima. Unlike the above methods, TAM~\cite{hou2023generalize}, GLOP~\cite{ye2024glop} and UDC~\cite{zheng2024udc} opt to use a learning-based policy to globally partition the entire instance into subproblems that are solved by the pretrained local construction policy. In addition,~\citet{pan2023h} resort to a hierarchical RL model where a local policy solves subproblems assigned by a jointly trained global policy. However, these neural partition policies may suffer performance degradation due to compounded errors during the partition process. 





\section{Preliminaries}
\textbf{CVRP Formulation. }A CVRP instance $I$ is defined as a tuple, represented by $I=(G, D, N_{\mathrm{max}})$. The graph $G$ consists of a depot node $v_{0}$ and $N_{v}$ customer nodes $v_{i}$ ($1 \leq i \leq N_{v}$). $D$ and $N_{\mathrm{max}}$ denote the vehicle capacity and maximum allowable number of times vehicles returning to the depot, respectively. Each node is associated with its coordinates $(a_{i}, b_{i})$ and each customer node is further associated with a demand $d_{i}$. $N_{\mathrm{max}}$ is accordingly defined as $\lceil\sum_{i=1}^{N_{v}} d_{i}/D\rceil + 1$. The distance between any pair of nodes can be measured by the Euclidean distance. In the CVRP, the vehicle needs to visit each customer exactly once, fulfills their demands without exceeding $D$, and returns to the depot to reload goods if necessary. A feasible solution $\mathcal{T} \in \mathbb{S}_{\mathcal{T}}$ can be described as a node permutation where the depot node can occur multiple times, while each customer node appears only once. Furthermore, the feasible solution $\mathcal{T}$ also can be decomposed into $N_{\tau}$ feasible subtours. In each subtour $\tau_{i}$ ($1 \leq i \leq N_{\tau}$), the starting and ending nodes are the depot, and the intermediate nodes are customers. The travel cost $e(\tau_{i})$ associated with $\tau_{i}$ is the sum of Euclidean distances along this subtour. Thus, the objective is to minimize $e(\mathcal{T})=\sum_{i=1}^{N_{\tau}}e(\tau_{i})$. $\mathbb{S}_{\mathcal{T}}$ denotes the space of feasible solutions, as does the notation used for $\mathbb{S}$ in the following sections. 



\noindent \textbf{Global Partition and Local Construction (GPLC). } In the context of CVRP, the partition task involves clustering nodes into distinct groups, ensuring that each cluster includes the depot node, each customer node belongs to a single cluster, and the total demand within each cluster does not surpass $D$. Each cluster of nodes forms a subgraph. A feasible partition solution $\mathcal{C} \in \mathbb{S}_{\mathcal{C}}$ comprises $N_{c}$ subgraphs. Each subgraph $c_{i}$ ($1 \leq i \leq N_{c}$) consists of the depot node along with various customer nodes distinct from those in other subgraphs (i.e., $\cup_{i=1}^{N_{c}}c_{i} = G$ and $\cap_{i=1}^{N_{c}}c_{i} = \{ v_{0} \}$). The feasible partition solution $\mathcal{C}$ can also be represented as a node list where the order of customer nodes is ignored between the two consecutively visited depot nodes. Therefore, a feasible solution $\mathcal{T}$ can be equivalently seen as a feasible partition solution $\mathcal{C}$ when disregarding the node permutation information in $\mathcal{T}$. 

Obviously, both the original CVRP and the partition problem within CVRP revolve around feasible solutions $\mathcal{T}$ and $\mathcal{C}$ composed of discrete value variables. This fact prompts prevalent learning-based methods to employ stochastic policies as the neural solver to handle whatever types of problems (permutation or partition) within the context of CVRP. Let $\Delta(\cdot)$ denote the space of the probability measure. In the GPLC paradigm~\cite{hou2023generalize, ye2024glop}, a stochastic partition policy $\pi_{\mathrm{part}}(\mathcal{C}|I) \in \Delta(\mathbb{S}_{\mathcal{C}})$ is used to derive a feasible partition solution $\mathcal{C}$ by dividing the graph $G$. Then, a (near-)optimal local permutation policy $\pi_{\mathrm{perm}}^{\ast}(\mathcal{T}|\mathcal{C}) \in \Delta(\mathbb{S}_{\mathcal{T}})$ can generate the feasible subtour $\tau_{i}$ for each subproblem $(c_{i}, D, 1)$. The objective is to identify an optimal partition policy $\pi_{\mathrm{part}}^{\ast}$ that minimizes $e(\mathcal{T})$. \emph{However, prior GPLC methods lack theoretical foundations of the partition problem. Therefore, we introduce Theorem~\ref{thm:gplc} to establish the rationality of the partition problem for CVRP.} Please see Appendix-C.1 for proofs.
\begin{theorem}
\label{thm:gplc}
    The objective in solving an original CVRP instance $I$ is to identify a (permutation) policy $\pi(\mathcal{T}|I) \in \Delta(\mathbb{S}_{\mathcal{T}})$ so as to minimize the expected cost $\mathbb{E}_{\mathcal{T} \sim \pi}[e(\mathcal{T})]$. If $\pi_{\mathrm{perm}}^{\ast} \in \Delta(\mathbb{S}_{\mathcal{T}})$ is optimal for each subproblem $(c_{i}, D, 1)$, then the original objective can be reframed as identifying an optimal partition policy $\pi_{\mathrm{part}}^{\ast} \in \Delta(\mathbb{S}_{\mathcal{C}})$ to minimize the expected cost, expressed as:
    \begin{equation}
    \label{equ:gplc} \min_{\pi_{\mathrm{part}}} \; \mathbb{E}_{\mathcal{C} \sim \pi_{\mathrm{part}}} [\sum_{i=1}^{N_{c}} \mathbb{E}_{\tau_{i} \sim \pi_{\mathrm{perm}}^{\ast}} (e(\tau_{i})) ],
    \end{equation}
    where $\pi_{\mathrm{perm}}^{\ast}(\mathcal{T}|\mathcal{C}) = \prod_{i=1}^{N_{c}} \pi_{\mathrm{perm}}^{\ast}(\tau_{i}|c_{i})$ implies that $\tau_{i}$ is independently sampled given the corresponding $c_{i}$. As aforementioned, viewing a feasible partition solution $\mathcal{C}$ as a node list implies that the partition policy can incrementally construct the partition solution. This process involves conditioning the current selected node $\mathcal{C}[n]$ on the partial partition solution $\mathcal{C}[0:n-1]$ ($\mathcal{C}[0]=\emptyset$) and the given problem instance $I$, written as:
    \begin{equation}
    \label{equ:part_pi}
        \pi_{\mathrm{part}}(\mathcal{C}|I)=\prod_{n=1}^{N_{\mathrm{sol}}}\pi_{\mathrm{part}}(\mathcal{C}[n]|\mathcal{C}[0:n-1], I),
    \end{equation}
    where $N_{\mathrm{sol}}$ denotes the length of partition solution. Please note that we abuse the notation of $N_{\mathrm{sol}}$ to denote the length of different partition solutions for brevity in the following sections. Since the objective defined in Equation~\ref{equ:gplc} essentially aligns with that associated with RL, the common approach involves training neural policies using RL.
\end{theorem}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{figures/hlgp.pdf}
    \caption{The proposed HLGP framework. $I_{j\geq1}^{k}$ represents a sequence of subproblems. Following the HLGP framework, the sequence of subproblems $I_{j\geq1}^{K}$ are fed to a permutation policy to derive the respective subtours.
    }
    \label{fig:hlgp}
    \vspace{-10pt}
\end{figure}
\section{Hierarchical Learning-based Graph Partition}
Our proposed HLGP framework is built upon the GPLC paradigm. Likewise, we assume the optimal local permutation policy $\pi_{\mathrm{perm}}^{\ast}$ is obtainable by leveraging LKH3~\cite{helsgaun2017extension} or the neural solver used in GLOP~\cite{ye2024glop}. It is evident from the partition policy expression in Equation~\ref{equ:part_pi} that decoding the nodes at each step hinges on the partial partition solution obtained in the preceding steps. Consequently, inaccuracies in clustering from earlier steps tend to propagate, resulting in a chain of misclustering in subsequent steps. These misclusterings in the partition policy exacerbate notably when confronted with substantial distribution or scale shifts. This empirical challenge in CVRP thus motivates us to develop a HL framework for solving the partition problem in CVRP. We anticipate that this HL framework can progressively mitigate compounded errors by incorporating both global and local partition policies.


\subsection{HL Formulation of Partition Problem}
In this section, we begin by introducing the \emph{feasible cost function} $f(\mathcal{C})$ for a feasible partition solution $\mathcal{C}$ as defined in Definition~\ref{def:part_cost}. Following this, various forms of $f(\mathcal{C})$ will be presented in the subsequent sections to align with both RL and SL objectives for training the partition policies. 
\begin{definition}
\label{def:part_cost}
Let $\pi_{\mathrm{part}}^{\ast}$ denote the optimal partition policy obtained by optimizing the objective in Equation~\ref{equ:gplc}. Given a cost function $f(\mathcal{C}): \mathbb{S}_{\mathcal{C}} \rightarrow \mathbb{R}$, if $\pi_{\mathrm{part}}^{\ast}$ can be derived by optimizing the objective $\min_{\pi_{\mathrm{part}}}\mathbb{E}_{\mathcal{C}\sim\pi_{\mathrm{part}}}[f(\mathcal{C})]$, then $f(\mathcal{C})$ is a feasible cost function.
\end{definition}

By leveraging this well-defined feasible cost function $f(\mathcal{C})$, the goal of the partition problem is to minimize $\mathbb{E}_{\mathcal{C}} [f(\mathcal{C})]$. Then, we reformulate the partition problem using a multi-level HL framework. In this framework, the global partition policy $\pi_{\mathrm{Gpart}}$ and the local partition policy $\pi_{\mathrm{Lpart}}$ work together in synergy to execute the partition task, as depicted in Figure~\ref{fig:hlgp}. At the global partition level, $\pi_{\mathrm{Gpart}}$ creates an initial coarse feasible partition $\mathcal{C}^{(0)}=\{ c^{(0)}_{1}, \ldots, c^{(0)}_{N_{c}} \}$, where $c^{(0)}_{i}$ denotes the subgraph at this level. In this partition solution $\mathcal{C}^{(0)}$, each pair of subgraphs $(c^{(0)}_{i}, c^{(0)}_{i\%N_{c}+1})$ (where $1 \leq i \leq N_{c}$) is stipulated as neighboring subgraphs as defined by a specific heuristic rule. For instance, a simple heuristic involves rearranging subgraphs in $\mathcal{C}^{(0)}$ based on the polar angles of their centroids in a Polar coordinate system centered at the depot node. This coarse multi-way partition $\mathcal{C}^{(0)}$ serves as the entry point of the subsequent $K$ local partition levels. At each local partition level $k \in \{1, ..., K\}$, the subproblems are sequentially formed by reuniting pairs of neighboring subgraphs from $\mathcal{C}^{(k-1)}$. Each subproblem $I^{(k-1)}_{j} (1 \leq j \leq \lfloor\frac{N_{c}}{2}\rfloor)$ is defined as:
\begin{equation}
\label{equ:loc_subp}
\begin{split}
    &I^{(k-1)}_{j} = (G_{j}^{(k-1)}, D, 2); \\
    &G_{j}^{(k-1)} = c_{(m+k-1) \% N_{c} + 1}^{(k-1)} \cup c_{(m+k) \% N_{c} + 1}^{(k-1)},
\end{split}
\end{equation}
where $m=2(j-1)$. There are $\lfloor\frac{N_{c}}{2}\rfloor$ subproblems in each local partition level. For each subproblem, the vehicle is only allowed to return twice to the depot by subproblem definition. Please note that each pair of consecutive subproblems $I^{(k-1)}_{j}$ and $I^{(k-1)}_{j+1}$ do not overlap in terms of the subgraphs they contain. Additionally, this technique for creating subproblems can be described as initially left-shifting the subgraphs in $\mathcal{C}^{(k-1)}$ by $k-1$ places and then merging the neighboring subgraphs without overlaps. At each local partition level $k \geq 1$, the subproblem sequence is directed to the local partition policy $\pi_{\mathrm{Lpart}}$. This allows the local partition policy $\pi_{\mathrm{Lpart}}$ to address potential misclusterings from the preceding level by utilizing the robust local topological features. As a result, the local partition policy can traverse through subproblems at each local partition level, gradually reducing accumulated misclusterings across levels. Moreover, upon completion of solving the subproblem $I^{(k-1)}_{j}$, the pair of subgraphs $(c_{(m+k-1)\%N_{c}+1}^{(k-1)}, c_{(m+k)\%N_{c}+1}^{(k-1)})$ is transitioned to the corresponding subgraph pair $(c_{(m+k-1)\%N_{c}+1}^{(k)}, c_{(m+k)\%N_{c}+1}^{(k)})$. Consequently, the resolution of the subproblem sequence results in an update from $\mathcal{C}^{(k-1)}$ to $\mathcal{C}^{(k)}$.


Within the overall HLGP framework, the global partition policy $\pi_{\mathrm{Gpart}}$ is formulated identical to the partition policy in the GPLC method, written as:
\begin{equation}
\label{equ:glb_part_pi}
\pi_{\mathrm{Gpart}}(\mathcal{C}^{(0)}|I)=\prod_{n=1}^{N_{\mathrm{sol}}}\pi_{\mathrm{Gpart}}(\mathcal{C}^{(0)}[n]|\mathcal{C}^{(0)}[0:n-1], I),
\end{equation}
where $\mathcal{C}^{(0)}[n]$ and $\mathcal{C}^{(0)}[0:n-1]$ denote the $n$-th selected node and the partial solution in $\mathcal{C}^{0}$, respectively. In contrast, the local partition policy addresses the series of subproblems produced from the previous partition solution $\mathcal{C}^{(k-1)}$ to construct the partition solution $\mathcal{C}^{(k)}$. Let $\mathcal{C}_{j}^{(k-1)}$ denote the partition solution for the subproblem $I_{j}^{(k-1)}$. Again, the partition solution $\mathcal{C}_{j}^{(k-1)}$ can be either represented as a node list where $\mathcal{C}_{j}^{(k-1)}[n]$ and $\mathcal{C}_{j}^{(k-1)}[0:n-1]$ indicate the $n$-th node and partial solution within it respectively, or decomposed into two subgraphs $c_{(m+k-1) \% N_{c} + 1}^{(k)}$, $ c_{(m+k) \% N_{c} + 1}^{(k)}$ both of which also belong to $\mathcal{C}^{(k)}$. Thus, it can be expressed as:
\begin{equation}
\label{equ:loc_part_pi}
\begin{split}
    &\pi_{\mathrm{Lpart}}(\mathcal{C}^{(k)} | \mathcal{C}^{(k-1)}, k) 
    = \prod_{j=1}^{\lfloor\frac{N_{c}}{2}\rfloor} \pi_{\mathrm{Lpart}}(\mathcal{C}_{j}^{(k-1)} | I_{j}^{(k-1)}) \\
    = &\prod_{j=1}^{\lfloor\frac{N_{c}}{2}\rfloor} \prod_{n=1}^{N_{\mathrm{sol}}} \pi_{\mathrm{Lpart}} (\mathcal{C}_{j}^{(k-1)}[n]|\mathcal{C}_{j}^{(k-1)}[0:n-1] , I_{j}^{(k-1)}).
\end{split}
\end{equation}
Please note that in the LHS of Equation~\ref{equ:loc_part_pi}, the parameter $k$ representing the level is included as an input to the local partition policy. This inclusion is necessary as the parameter $k$ governs the construction of different subproblem sequences for each level. As a result, the objective of HLGP framework is to minimize the expected cost by optimizing both $\pi_{\mathrm{Gpart}}$ and $\pi_{\mathrm{Lpart}}$, written as: 
\begin{equation}
\label{equ:hl_obj}
 \min_{\pi_{\mathrm{Gpart}}, \pi_{\mathrm{Lpart}}} \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [f(\mathcal{C}^{(K)})],
\end{equation}
where $\mathcal{C}^{(0)}$ and $\mathcal{C}^{(k)}$ ($k\geq1$) are sampled from $\pi_{\mathrm{Gpart}}(\mathcal{C}^{(0)}|I)$ and $\pi_{\mathrm{Lpart}}(\mathcal{C}^{(k)}|\mathcal{C}^{(k-1)},k)$, respectively.


\subsection{RL-driven HLGP}

In the HLGP framework, the imperative task at hand involves the joint optimization for the global and local partition policies, as illustrated in Equation~\ref{equ:hl_obj}. To address this intricate optimization challenge through RL algorithms, a rigorous formulation utilizing a multi-level Markov Decision Process (MDP) is required. However, Equation~\ref{equ:hl_obj} essentially revolves around evaluating $\mathcal{C}^{(K)}$ at the $K$-th level. The absence of direct evaluations for $\mathcal{C}^{(k)}, k < K$, primarily contributes to the instability concern during the joint training via RL. We thus equivalently convert it to one involving direct evaluations at each level, as outlined in Theorem~\ref{thm:rl_obj}.

\begin{theorem}
\label{thm:rl_obj}
Let $g(c_{i})$ denote $\mathbb{E}_{\tau_{i} \sim \pi_{\mathrm{perm}}^{\ast}(\cdot|c_{i})}(e(\tau_{i}))$. It is clear that $f(\mathcal{C}) = \sum_{i=1}^{N_{c}} g(c_{i})$ acts as a feasible cost function. Then, the optimization problem defined in Equation~\ref{equ:hl_obj} can be transformed equivalently as follows:
\begin{equation}
\label{equ:trans_obj}
\begin{split}
     \min_{\pi_{\mathrm{Gpart}}, \pi_{\mathrm{Lpart}}} &\mathbb{E}_{\mathcal{C}^{(0)}} [f(\mathcal{C}^{(0)})] +  \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}}[f(\mathcal{C}^{(1)}) - f(\mathcal{C}^{(0)})]+ \\
    &  \cdots + \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [f(\mathcal{C}^{(K)}) - f(\mathcal{C}^{(K-1)})].
\end{split}
\end{equation}
The evaluation for $\mathcal{C}^{(k)}, k \geq 1$, can further be derived as:
\begin{align}
\label{equ:eval_C}
    &f(\mathcal{C}^{(k)}) - f(\mathcal{C}^{(k-1)}) = \sum_{j=1}^{\lfloor\frac{N_{c}}{2}\rfloor}
    [h(\mathcal{C}^{(k)}, k, m) - h(\mathcal{C}^{(k-1)}, k, m)]; \nonumber \\
    &h(\mathcal{C}^{(k)}, k, m) = g(c_{(m+k-1)\%N_{c}+1}^{(k)}) + g(c_{(m+k)\%N_{c}+1}^{(k)}),
\end{align}
where $m=2(j-1)$.
\end{theorem}

Please see Appendix-C.2 for proofs. Theorem~\ref{thm:rl_obj} breaks down the objective described in Equation~\ref{equ:hl_obj} into ${K+1}$ components, with each component associated with the direct evaluation of the respective partition solution. Notably, except for the evaluation of $\mathcal{C}^{(0)}$ which solely considers its own cost $f(\mathcal{C}^{(0)})$, the evaluation of $\mathcal{C}^{(k)}$, $k \geq 1$ hinges on the difference between its own cost $f(\mathcal{C}^{(k)})$ and the cost $f(\mathcal{C}^{(k-1)})$ from the preceding level. At each local partition level $k \geq 1$, the local partition policy is responsible for resolving each subproblem $I_{j}^{k-1}$, leading to the modification of each pair of subgraphs $(c_{(m+k-1)\%N_{c}+1}^{(k-1)}, c_{(m+k)\%N_{c}+1}^{(k-1)})$ to the respective subgraph pair $(c_{(m+k-1)\%N_{c}+1}^{(k)}, c_{(m+k)\%N_{c}+1}^{(k)})$. We are thus allowed to proceed with the derivation as indicated in Equation~\ref{equ:eval_C}. Given the optimization problem stated above, we present the formulation utilizing a multi-level MDP in Proposition~\ref{prop:mlmdp}. 


\begin{proposition}
\label{prop:mlmdp}
In the multi-level MDP framework, at the global partition level, for $t\geq1$, the state $x_{t}^{(0)}\in\mathbb{X}^{(0)}$ comprises problem instance $I$ and the partial partition solution $\mathcal{C}^{(0)}[0:t-1]$ ($\mathcal{C}^{(0)}[0] = \emptyset$). The initial distribution $\mu^{(0)}$ aligns with the problem instance distribution $p_{I}$. The action $u_{t}^{(0)}\in\mathbb{U}^{(0)}$ involves selecting a node denoted as $\mathcal{C}^{(0)}[t]$, from unvisited customer nodes and the depot node. Let $i_{t}$ index subgraphs such that at timestep $t$, the agent is constructing $i_{t}$-th subgraph $c_{i_{t}}^{(0)}$. If the subgraph $c_{i_{t}}^{(0)}$ is created, then the reward $r_{t}^{(0)}$ is set as $-g(c_{i_{t}}^{(0)})$; otherwise, it remains at $0$. The global partition policy, parameterized by $\theta_{G}$, is thus specified as $\pi_{\theta_{G}}(u_{t}^{(0)}|x_{t}^{(0)})$. 

At each local partition level $k\geq1$, the local partition policy is tasked with solving the sequence of subproblems obtained from $\mathcal{C}^{(k-1)}$. In this context, we use $j_{t}$ as an index for subproblems, indicating that the $j_{t}$-th subproblem denoted as $I_{j_{t}}^{(k-1)}$, is currently being addressed but remains incomplete at timestep $t$. The state $x_{t}^{(k)}\in\mathbb{X}^{(k)}$ consists of the subproblem sequence and the partial solution of $I_{j_{t}}^{(k-1)}$. The initial state distribution $\mu^{(k)}$ corresponds to the distribution of the subproblem sequence. The action $u_{t}^{(k)}\in\mathbb{U}^{(k)}$ involves selecting a node for solving $I_{j_{t}}^{(k-1)}$. When $I_{j_{t}}^{(k-1)}$ is successfully solved, the index $j_{t}$ will proceed to the next subproblem, and the reward $r_{t}^{(k)}$ is set as $-(h(\mathcal{C}^{(k)}, k, m) - h(\mathcal{C}^{(k-1)}, k, m))$ (where $m=2(j_{t}-1)$). Otherwise, the reward remains at $0$. Thus, the local partition policy parameterized by $\theta_{L}$, is defined as $\pi_{\theta_{L}}(u_{t}^{(k)}|x_{t}^{(k)})$. The objective is to maximize the sum of expected returns across levels, as defined below:
\begin{equation}
\label{equ:rl_obj}
    J(\theta_{G}, \theta_{L}) = \mathbb{E}_{\omega^{(0)}} [\sum_{t=1}^{T^{(0)}}r_{t}^{(0)}] + \cdots+
\mathbb{E}_{\omega^{(0)}}\cdots\mathbb{E}_{\omega^{(K)}} [\sum_{t=1}^{T^{(K)}}r_{t}^{(K)}],
\end{equation}
where $T^{(k)}$ and $\omega^{(k)}$ denote the horizon and the trajectory at level $k$.
\end{proposition}


\begin{figure*}[t]
\centering
\subfigure[RL-driven HLGP]{\label{fig:rl_hlgp}\includegraphics[width=0.4\textwidth]{figures/rl_hlgp.pdf}}
\hspace{0.1\textwidth}
\subfigure[SL-driven HLGP]{\label{fig:sl_hlgp}\includegraphics[width=0.4\textwidth]{figures/sl_hlgp.pdf}}
\caption{RL-driven HLGP replaces the initially generated partial partition solution with the complete partition solution of subproblems within $\mathcal{C}^{(0)}$ at level 0. SL-driven HLGP requires labeled instances for training $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$.
\vspace{-10pt}
}
\label{fig:training_flow}
\end{figure*}


Notably, although Equation~\ref{equ:rl_obj} isolates the evaluation exclusively for $\omega^{(k)}$, the evaluation impacted by the trajectories $\omega^{(k+1)}, ..., \omega^{(K)}$ still remains. This implies that the underlying MDP at level $k$ remains nonstationary. 
%\textcolor{red}{Comment a bit if this approximation is accurate? What affects such approximation performance?}
We thus take the following optimization problem as an approximation:
\begin{equation}
\begin{split}
%\begin{align}
\label{equ:approx_rl_obj}
& \hat{J}(\theta_{G}, \theta_{L}) = L(\theta_{G}, \lambda_{G}, 0) + \sum_{k=1}^{K}L(\theta_{L}, \lambda_{L}, k); \\
& L(\theta, \lambda, k) = \mathbb{E}_{\omega^{(k)} \sim \hat{\mu}^{(k)}, \pi_{\theta}}[\sum_{t=1}^{T^{(k)}}r_{t}^{(k)}] + \lambda \mathcal{H}(\pi_{\theta}), %\nonumber
%\end{align}
\end{split}
\end{equation}
where $\hat{\mu}^{(k)}$ is a surrogate initial state distribution at level $k$, $\mathcal{H}(\pi_{\theta})$ is the entropy term, and $\lambda$ denotes the hyperparameter. The entropy term is typically defined to minimize the KL divergence between the policy and a uniform distribution. In Equation~\ref{equ:rl_obj}, $\omega^{(k)}, k\geq1$ is drawn from the initial distribution $\mu^{(k)}$ and the local partition policy $\pi_{\theta_{L}}$. However, $\mu^{(k)}$ heavily relies on preceding partition solutions derived from both the global and local partition policies. Therefore, in Equation~\ref{equ:approx_rl_obj}, the surrogate initial distribution $\hat{\mu}^{(k)}$ is introduced to eliminate this dependency. Please note that $\hat{\mu}^{(0)} = \mu^{(0)}$. As a result, the optimization for $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$ is decoupled. 

In the context of RL-driven HLGP, we incorporate the surrogate objective defined in Equation~\ref{equ:approx_rl_obj} into the REINFORCE algorithm~\cite{williams1992simple} to update $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$. In each iteration $n \geq 0$ of REINFORCE, the existing global partition policy denoted as $\pi_{\theta_{G}^{n}}$ is employed to sample $\omega^{(0)}$ for the update of $\pi_{\theta_{G}^{n}}$. At each local partition level $k \geq 1$, the current local partition policy denoted as $\pi_{\theta_{L}^{n}}$ is additionally leveraged to sample the partition solution $\mathcal{C}^{k-1}$, crucial for $\hat{\mu}^{(k)}$. Following this, $\omega^{(k)}$ is sampled to update $\pi_{\theta_{L}^{n}}$. Please refer to Appendix-B for the pseudocode.


Furthermore, in the standard theoretical analysis of REINFORCE algorithm conducted in~\cite{zhang2021sample}, the upper bound of regret includes the term represented by $||\frac{d}{\mu}||_{\infty}$, where $d$ and $\mu$ stand for the stationary state distribution and the initial state distribution. However, the existing method using REINFORCE algorithm for whatever types of problems (permutation or partition) in the context of CVRP ignores the potential risks highlighted in the regret bound. We exemplify the partition problem as a case study to elucidate this issue. The support set of $\mu$ consists solely of the problem instances $I$. Let $N_{v}(t)$ denote the number of customer nodes selected before timestep $t$. In contrast, during the partition process, at each step $t > 1$, the partition policy is indeed responsible to solve the subproblem denoted as $I_{N_{v}(t)}$ in which the graph comprises depot and unvisited customers. Let $c_{i_{t}}$ denote the subgraph under construction. The capacity in $I_{N_{v}(t)}$ is accordingly subtracted from the total demand of the visited node in $c_{i_{t}}$, reverting back to $D$ once $c_{i_{t}}$ is fully formed. The support set of $d$ thus includes the subproblems $I_{N_{v}(t)}$.  This significant discrepancy in support sets inevitably results in an infinite $||\frac{d}{\mu}||_{\infty}$ in the regret bound. This observation inspires us to incorporate the subproblems $I_{N_{v}(t)}$ encountered during the partition process into the training of involved partition policies to reduce the mismatch between support sets.


In the practical implementation, a problem instance $I$ is initially generated from the instance distribution $p_{I}$, which is used to train $\pi_{\theta_{G}}$ via RL. If a new subgraph $c_{i_{t}}$ is formed at timestep $t$, then the subproblem $I_{N_{v}(t)}$ is treated as an individual problem instance, denoted as $I \leftarrow I_{N_{v}(t)}$, for the training of $\pi_{\theta_{G}}$. This procedure continues until $G=\emptyset$ in $I$, and reverts back to $p_{I}$ for a new instance $I$. For efficiency reasons, we do not include all subproblems. In inference, the partition solution $\mathcal{C}^{(0)}$ is formed by sequentially replacing the partial partition solution with the corresponding complete partition solution of the subproblem. An example is shown in Figure~\ref{fig:rl_hlgp}. The training and inference procedure utilizing the encountered subproblem can similarly be applied to $\pi_{\theta_{L}}$. Additionally, we utilize the isomorphic Graph Neural Netwok (GNN) as presented in GLOP~\cite{ye2024glop} to serve as the backbones of $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$ correspondingly. 


\subsection{SL-driven HLGP}
In this section, we pivot towards an SL training strategy to optimize the objective of the partition problem as defined in Equation~\ref{equ:hl_obj}. Here, the optimal partition solver $\pi_{\mathrm{part}}^{\ast}$ is presumed to be available in advance. The optimal partition solution $\bar{\mathcal{C}}=\{\bar{c}_{1}, \ldots, \bar{c}_{N_{c}}\}$ for each instance $I$ is accordingly obtainable from $\pi_{\mathrm{part}}^{\ast}$. We thus adopt $f(\mathcal{C})=-\mathds{1}(\mathcal{C} = \bar{\mathcal{C}}) = -\mathds{1}(c_{1}=\bar{c}_{1}, \ldots, c_{N_{c}}=\bar{c}_{N_{c}})$, where $\mathds{1}(\cdot)$ denotes the indicator function. Recall that the optimal solution $\bar{\mathcal{T}}$ of the CVRP instance can be equivalently seen as the optimal partition solution $\bar{\mathcal{C}}$ when disregarding the node permutation information in $\bar{\mathcal{T}}$. Therefore, by setting $\bar{\mathcal{C}} = \bar{\mathcal{T}}$, the feasible cost function can be defined as $f(\mathcal{C}) = -\mathds{1}(\mathcal{C}[0] = \bar{\mathcal{C}}[0], \ldots, \mathcal{C}[N_{\mathrm{sol}}] = \bar{\mathcal{C}}[N_{\mathrm{sol}}])$. Upon utilizing this design of $f(\mathcal{C})$, Theorem~\ref{thm:sl_obj} simplifies the optimization objective of HLGP framework.
\begin{theorem}
\label{thm:sl_obj}
Given $f(\mathcal{C})=-\mathds{1}(\mathcal{C} = \bar{\mathcal{C}})$, the optimization objective in the HLGP framework for a problem instance $I$ is to identify $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$ so as to minimize:
\begin{equation}
    L(\theta_{G}, \theta_{L}, \bar{\mathcal{C}}) = 
    - \log\pi_{\theta_{G}}(\bar{\mathcal{C}}|I) - \sum_{i=1}^{N_{c}} \log\pi_{\theta_{L}}(\bar{\mathcal{C}}_{i}|I_{i}),
\end{equation}
where $I_{i} = (G_{i}, D, 2)$ denotes the subproblem, with $G_{i} = \bar{c}_{i} \cup \bar{c}_{i\%N_{c}+1}$, and $\bar{\mathcal{C}}_{i} = \{\bar{c}_{i}, \bar{c}_{i\%N_{c}+1} \}$ represents the corresponding label.
\end{theorem}

\begin{table*}[t]
    \caption{Comparative results on uniformly distributed CVRP instances. OOM stands for out-of-memory. The symbol $\ast$ indicates that the results are obtained from the original paper. The notation $\downarrow$ indicates that a lower value is better.}
    \setlength\tabcolsep{0.9pt}
    \renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{lccccccccccccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{Methods}  & \multicolumn{3}{c}{CVRP1K} & \multicolumn{3}{c}{CVRP2K} & \multicolumn{3}{c}{CVRP5K} & \multicolumn{3}{c}{CVRP7K} & \multicolumn{3}{c}{CVRP10K} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16}
        & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} \\
    \midrule[0.7pt]
    LKH & 42.17 & 0.80 & 14.18s & 58.06 & 1.06 & 31.92s	& 126.59 & 2.81 & 6.80m & 172.80 & 4.04 & 18.21m & 240.23 & 5.42 & 41.29m \\
    HGS & 41.12 & 0.77 & 4.57m & 56.24 & 1.07 & 12.68m & 122.84 & 2.87 & 18.80m & 165.37 & 3.95 & 20.15m & 226.59 & 5.29 &	24.64m \\
    \midrule[0.7pt]
    AM (ICLR'19) & 59.18 & 2.81 &	8.84s &\multicolumn{3}{c}{OOM}	&\multicolumn{3}{c}{OOM}  &\multicolumn{3}{c}{OOM}	& \multicolumn{3}{c}{OOM} \\
    POMO (NeurIPS'20) & 100.99 & 7.43 & 4.63s & 255.13 &30.02 &39.35s &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM}	&\multicolumn{3}{c}{OOM} \\	
    Sym-POMO (NeurIPS'22)& 98.04 & 2.86 & 5.71s & 157.89 & 2.96 &45.23s	&\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} \\
    AMDKD (NeurIPS'22)& 84.16 & 0.98 & 4.27s & 188.75 & 4.39 & 34.39s &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} \\
    Omni-POMO (ICML'23)& 47.80 &0.77	& 4.45s	& 74.01 & 1.05 &35.86s	&\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} \\
    ELG-POMO (IJCAI'24)& 46.41 & 0.40 & 9.53s & 66.07 & 0.55 & 67.19s &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} \\
    INViT (ICML'24)& 46.56 & 0.81 & 17.08s & 64.94 & 1.09 & 36.67s & 139.45 & 2.86 & 141.07s & 186.57 & 3.93 & 4.81m & 254.17 & 5.39 & 6.96m \\
    \midrule[0.7pt]
    LEHD (NeurIPS'23) & 42.80 & 0.82 & 40.25s & 60.48 & 1.12 & 72.48s & 136.80 &2.86 & 3.22m & 188.11 & 4.00 & 6.52m & 266.06 & 5.56 & 11.92m \\		
    BQ (NeurIPS'23)& 43.12 & 0.80 & 4.75s & 60.95 & 1.10 &15.66s & 137.14 & 3.00 & 79.89s & 188.78 & 4.23 & 1.88m & 265.81 & 5.97 & 3.30m \\
    \midrule[0.7pt]
    L2I (ICLR'20)& 49.79 & 1.10 & 18.60s & 95.58 & 5.44 & 44.88s & 262.84 & 9.99 & 2.64m & 506.73 & 25.25	& 3.61m & 1263.23  & 4.00 & 4.07m \\
    NLNS (ECAI'20) & 53.51 & 0.83 & 12.08s & 81.54 & 1.12 & 12.15s & 180.84 & 2.99 & 12.62s & 243.50 &4.17 &13.16s & 331.27 & 5.53 & 13.97s	\\
    DACT (NeurIPS'21) & 50.43 & 0.35	& 75.47s & 71.17 & 0.51	& 5.40m &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM} &\multicolumn{3}{c}{OOM}\\
    \midrule[0.7pt]
    L2D (NeurIPS'21) & 46.45 & 0.87 & 4.67s & 64.04 & 1.21 & 7.54s & 135.09 & 3.02 & 16.11s & 182.21 & 4.13 & 24.37s & 246.38 & 5.55 & 27.59s \\
    RBG$^{\ast}$ (KDD'22) & 74.00 & -	&13.00s	& 137.60 &-& 42.00s	&\multicolumn{3}{c}{-}&\multicolumn{3}{c}{-}&\multicolumn{3}{c}{-} \\
    TAM-AM$^{\ast}$ (ICLR'23) & 50.06 & 0.98	& 0.76s	& 74.31 & 1.42	&2.20s &172.22&- &11.78s	&233.44&- &26.47s &\multicolumn{3}{c}{-}	\\
    TAM-LKH$^{\ast}$ (ICLR'23) & 46.34 & 0.84	& 1.82s	& 64.78 & 1.18	&5.63s	&144.64&- &17.19s &196.91&- &33.21s &\multicolumn{3}{c}{-} \\
    GLOP-G (AAAI'24) &47.21&0.90	&0.73s	&63.60&1.41	&1.74s	&141.67&3.67	&2.37s	&191.75&4.99	&3.50s	&266.96&6.46 &13.74s \\
    GLOP-LKH (AAAI'24) &46.03&0.99	&0.78s	&63.10&1.43	&1.83s	&140.51&3.72	&4.31s	&191.45&5.06	&7.34s	&267.50&6.50 &16.47s \\
    \midrule[0.7pt]
    RL-driven HLGP &43.78&0.85	&3.72s &59.58&1.17 &10.03s	&128.12 &3.06 &82.59s & 173.71 &4.39 & 1.96m & 238.62 & 6.03 & 5.13m \\
    SL-driven HLGP &\textbf{41.95}&0.78	&8.31s &\textbf{57.67}&1.08 &32.40s	&\textbf{124.13}&2.79 &97.27s &\textbf{166.73}&3.91 & 2.15m & \textbf{227.07} & 5.25 & 3.39m \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:1}
    \vspace{-10pt}
\end{table*}

Please see Appendix-C.3 for proofs. We can observe that the optimization objective for $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$ is totally disentangled in Theorem~\ref{thm:sl_obj}. Accordingly, the optimization objective for instances sampled from $p_{I}$ is to minimize:
\begin{equation}
\label{equ:sl_obj}
    J(\theta_{G}, \theta_{L})= \mathbb{E}_{(I, \bar{\mathcal{C}})\sim p_{I}, \pi_{\mathrm{part}}^{\ast}} [L(\theta_{G}, \theta_{L}, \bar{\mathcal{C}})].
\end{equation}
This objective involves evaluating the performance of $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$ on the sampled trajectories induced by $\pi_{\mathrm{part}}^{\ast}$. However, $\pi_{\mathrm{part}}^{\ast}$ is practically unavailable for the NP-hard partition problem, since it is impossible to directly get supervised labels. Inspired by recent techniques known as self-imitation learning~\cite{son2023meta, luo2024self}, our goal is to acquire high-quality labeled instances from a behavioral policy $\hat{\pi}_{\mathrm{part}}$. The working pipeline of $\hat{\pi}_{\mathrm{part}}$ can be described as follows (see Figure~\ref{fig:sl_hlgp}): At the global partition level, $\hat{\mathcal{C}}^{(0)}$ is first generated using beam search with $\pi_{\theta_{G}}$. Then, at each local partition level $k\geq1$, the partition solution is further refined locally using beam search with $\pi_{\theta_{L}}$ to obtain the ultimate partition solution $\hat{\mathcal{C}}^{(K)}$. Thus, during training, $\bar{\mathcal{C}}=\hat{\mathcal{C}}^{(K)}$ serves as the label. The practical loss function is thus defined as:
\begin{equation}
\label{equ:sl_loss}
\hat{J}(\theta_{G}, \theta_{L}) = \mathbb{E}_{(I, \bar{\mathcal{C}})\sim p_{I}, \hat{\pi}_{\mathrm{part}}} [L(\theta_{G}, \theta_{L}, \bar{\mathcal{C}})]  + \mathrm{reg}(\theta_{G}, \theta_{L}),
\end{equation}
where $\mathrm{reg}(\theta_{G}, \theta_{L}) = \lambda_{G}\frac{||\theta_{G}||^{2}}{2} + \lambda_{L}\frac{||\theta_{L}||^{2}}{2}$, with hyperparameters $\lambda_{G}$ and $\lambda_{L}$. Therefore, this loss function is incorporated into the imitation learning algorithm to iteratively optimize $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$. In each iteration $n \geq 0$, the algorithm deploys $\hat{\pi}_{\mathrm{part}}^{n}$ (which relies on $\theta_{G}^{n}$ and $\theta_{L}^{n}$) and gathers the labeled instance $(I, \bar{\mathcal{C}})$. Online gradient updates are then executed based on estimated gradients to update $\theta_{G}^{n}$ and $\theta_{L}^{n}$. Please refer to Appendix-B for the pseudocode.


Here, let us delve deeper into illustrating the training process for the global partition policy $\pi_{\theta_{G}}$ as a case study to elucidate the intricacies of the SL algorithm for the partition problem. A similar analysis can be conducted for the local partition policy $\pi_{\theta_{L}}$. The global partition policy $\pi_{\theta_{G}}$ requires to imitate the whole trajectory induced by the behavioral policy $\hat{\pi}_{\mathrm{part}}$. Following the formulation in Equation~\ref{equ:glb_part_pi}, the global partition policy can directly output the node sequence. Subsequently, the log-probability of this node sequence in $\hat{\mathcal{C}}$ is maximized to update $\theta_{G}$. This log-probability of the node sequence contains the product of a series of conditional probabilities, represented as $\log \prod_{t=1}^{N_{\mathrm{sol}}} \pi_{\theta_{G}}(\bar{\mathcal{C}}[t] | \bar{\mathcal{C}}[0:t-1], I)=\sum_{t=1}^{N_{\mathrm{sol}}} \log \pi_{\theta_{G}}(\bar{\mathcal{C}}[t] | \bar{\mathcal{C}}[0:t-1], I)$. This sum of log-probabilities prompts us to consider $(\bar{\mathcal{C}}[0:t-1], I)$ as an individual training sample, with its corresponding label being a singular $\bar{\mathcal{C}}[t]$. In this context, at timestep $t \geq 1$, the global partition policy is addressing a subproblem $I_{N_{v}(t)}$ determined by $(\bar{\mathcal{C}}[0:t-1], I)$, and it aligns with the same definition as in the RL setting. 
Hence, rather than generating the entire node sequence for behavioral imitation, the labeled instance $(I_{N_{v}(t)}, \bar{\mathcal{C}}[t])$ is fed to the global partition policy to imitate one-step behavior at each time step $t \geq 1$. In practice, we employ a variant Transformer model of BQ~\cite{drakulic2024bq} as the backbones of $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$, which aligns with the analysis above. Therefore, we underscore the importance of viewing the subproblems encountered during training as individual training instances within both the contexts of RL and SL training paradigms.





\begin{table*}[]
\caption{Comparative results on various distributed CVRP instances. ``G" denotes the Gaussian distribution. ``E" denotes the Explosion distribution. ``R" denotes the Rotation distribution. The notation $\downarrow$ indicates that a lower value is better.}
\setlength\tabcolsep{0.9pt}
\renewcommand{\arraystretch}{0.9}
\small
\centering
\begin{tabular}{lcccccccccccccccccc}
\toprule[1.2pt]
\multirow{2}{*}{Methods}  & \multicolumn{3}{c}{CVRP1K+G} & \multicolumn{3}{c}{CVRP1K+E} & \multicolumn{3}{c}{CVRP1K+R} & \multicolumn{3}{c}{CVRP2K+G} & \multicolumn{3}{c}{CVRP5K+E} &\multicolumn{3}{c}{CVRP7K+R} \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16} \cmidrule(r){17-19}
& \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} & \normalsize{$Avg._{\downarrow}$} & \normalsize{$Std._{\downarrow}$} & \normalsize{$Time_{\downarrow}$} \\
\midrule[0.7pt]
LKH3            & 32.52           & 1.21   & 37.35s & 38.01            & 1.48  & 15.55s  & 37.50           & 1.33  & 15.35s  & 42.60           & 1.62  & 64.06s  & 103.45           & 4.39   & 6.67m  & 156.04           & 6.81  & 25.69m \\
HGS             & 31.84           & 1.19   & 15.57m & 37.13            & 1.46  & 6.52m   & 36.62           & 1.32  & 7.78m   & 41.64           & 1.61  & 19.80m  & 101.16           & 4.40   & 16.53m & 151.04           & 6.72  & 21.04m \\
\midrule[0.7pt]
AM (ICLR'19) & 93.62           & 20.23  & 9.32s  & 58.99            & 4.79  & 8.74s   & 60.80           & 5.42  & 8.72s   & \multicolumn{3}{c}{OOM}           & \multicolumn{3}{c}{OOM}            & \multicolumn{3}{c}{OOM}           \\
POMO (NeurIPS'20) & 56.83           & 2.72   & 4.78s  & 74.88            & 4.84  & 4.54s   & 75.26           & 5.52  & 4.41s   & 101.75          & 7.32  & 38.31s  & \multicolumn{3}{c}{OOM}            & \multicolumn{3}{c}{OOM}           \\
Sym-POMO (NeurIPS'22) & 97.59           & 5.35   & 5.57s  & 95.08            & 5.11  & 5.65s   & 106.88          & 6.11  & 5.53s   & 134.32          & 5.20  & 40.56s  & \multicolumn{3}{c}{OOM}            & \multicolumn{3}{c}{OOM}           \\
AMDKD (NeurIPS'22) & 58.71           & 1.98   & 4.14s  & 71.10            & 2.04  & 4.17s   & 73.32           & 2.00  & 4.11s   & 108.11          & 3.82  & 32.96s  & \multicolumn{3}{c}{OOM}            & \multicolumn{3}{c}{OOM}           \\
Omni-POMO (ICML'23) & 35.47           & 1.20   & 4.30s  & 41.80            & 1.47  & 4.30s   & 41.30           & 1.34  & 4.28s   & 51.02           & 1.76  & 35.31s  & \multicolumn{3}{c}{OOM}            & \multicolumn{3}{c}{OOM}           \\
ELG-POMO (IJCAI'24) & 36.49           & 0.63   & 9.86s  & 41.64            & 0.75  & 9.67s   & 41.31           & 0.69  & 9.48s   & 49.34           & 0.86  & 68.73s  & \multicolumn{3}{c}{OOM}            & \multicolumn{3}{c}{OOM}           \\
INViT (ICML'24) & 35.67           & 1.28   & 19.80s  & 42.11            & 1.53  & 19.80s   & 41.22           & 1.37  & 19.74s  & 47.31           & 1.75  & 46.92s  & 113.26           & 4.79   & 3.17m  & 169.38           & 7.47  & 4.66m  \\
\midrule[0.7pt]
LEHD   (NeurIPS'23)         & 33.99           & 1.23   & 36.27s & 38.96            & 1.50  & 36.19s  & 38.44           & 1.36  & 36.16s  & 47.48           & 1.65  & 64.80s   & 116.70           & 4.38   & 2.85m  & 176.14           & 6.91  & 5.93m  \\
BQ   (NeurIPS'23)           & 34.71           & 1.25   & 3.88s  & 39.64            & 1.50  & 3.90s   & 39.17           & 1.39  & 3.91s   & 47.74           & 1.67  & 11.34s  & 120.23           & 4.75   & 72.12s & 181.04           & 7.59  & 76.47s \\
\midrule[0.7pt]
L2I    (ICLR'20)         & 37.42           & 1.28   & 13.56s & 44.05            & 1.58  & 14.15s  & 43.56           & 1.41  & 14.01s  & 63.33           & 3.26  & 26.14s  & 204.51           & 10.31  & 2.10m  & 348.70           & 17.47 & 4.37m  \\
NLNS    (ECAI'20)        & 41.31           & 1.27   & 12.15s & 46.52            & 1.51  & 12.15s  & 47.44           & 1.36  & 12.16s  & 60.38           & 1.89  & 12.22s  & 142.87           & 4.65   & 12.73s & 221.69           & 6.51  & 13.02s \\
DACT  (NeurIPS'21)   & 37.03           & 0.57   & 68.63s & 43.10            & 0.66  & 67.92s  & 43.50           & 0.57  & 67.98s  & 49.30           & 0.83  & 4.52m   & \multicolumn{3}{c}{OOM}            & \multicolumn{3}{c}{OOM}           \\
\midrule[0.7pt]
L2D    (NeurIPS'21)         & 35.26           & 1.24   & 2.60s  & 41.09            & 1.50  & 2.52s   & 40.40           & 1.37  & 2.63s   & 46.29           & 1.69  & 4.24s   & 108.95           & 4.63   & 10.15s & 162.90           & 6.99  & 19.04s \\
GLOP-G    (AAAI'24)      & 39.20            & 1.40   & 0.44s  & 43.44            & 1.63  & 0.43s   & 43.46           & 1.46  & 0.41s   & 50.55           & 1.97  & 1.88s   & 117.65           & 4.80   & 7.07s  & 178.37           & 6.84  & 7.98s  \\
GLOP-LKH   (AAAI'24)   & 38.71           & 1.42   & 1.22s  & 42.83            & 1.67  & 0.93s   & 42.80           & 1.49  & 0.77s   & 50.42           & 1.98  & 3.84s   & 117.04           & 4.83   & 9.85s  & 178.08           & 6.90  & 11.35s \\
\midrule[0.7pt]
RL-driven HLGP         & 34.58           & 1.26   & 3.47s  & 39.85            & 1.54  & 3.43s   & 39.36           & 1.38  & 3.47s   & 44.80           & 1.70  & 10.37s  & 106.27           & 4.52   & 70.80s & 160.73           & 7.20  & 3.04m  \\
SL-driven HLGP  & \textbf{32.55}  & 1.21   & 7.21s  & \textbf{37.96}   & 1.48  & 7.55s   & \textbf{37.40}  & 1.34  & 7.41s   & \textbf{42.85}  & 1.65  & 30.61s  & \textbf{102.27}  & 4.47   & 84.12s & \textbf{152.47}  & 6.91  & 98.49s \\
\bottomrule[1.2pt]
\end{tabular}
\label{tab:2}
\vspace{-10pt}
\end{table*}



\section{Experiments}

\subsection{Training and Evaluation Settings}

In the training for both RL-driven and SL-driven HLGP, we adhere to the problem settings used in GLOP~\cite{ye2024glop}. During the training phase, each CVRP instance consists of 1000 customer nodes distributed uniformly, with a vehicle capacity of 200. During the evaluation phase, our focus is on assessing the generalization capabilities of various models. Therefore, we utilize diverse datasets with varying scales and distributions. Each evaluation dataset can specify the number of customer nodes as 1000, 2000, 5000, 7000, or 10000. The customer nodes in each dataset are distributed according to a Uniform distribution, a Gaussian distribution, an explosion pattern, or a rotation pattern. Except for the dataset with 1000 customer nodes setting capacity as 200, the capacity for the other datasets is set to 300. Each dataset comprises 128 instances. The process of generating problem instances aligns with the methodologies outlined in~\cite{kool2018attention, zhou2023towards}. Please refer to Appendix-A.1 for more training details. Note that the code of our implementation, along with the Appendix, has been uploaded as the supplementary material.


During the evaluation phase, we compare our proposed RL-driven and SL-driven HLGP models with various methods. The classical heuristic methods include LKH3~\cite{helsgaun2017extension} and HGS~\cite{vidal2012hybrid}. In learning-based constructive methods, AM~\cite{kool2018attention}, POMO~\cite{kwon2020pomo}, and Sym-POMO~\cite{kim2022sym} serve as baselines trained with RL. AMDKD~\cite{bi2022learning} and Omni-POMO~\cite{zhou2023towards} target generalization issues specifically. ELG-POMO~\cite{gao2023towards} and INViT~\cite{fang2024invit} harness local topological features. Within the realm of iterative methods, L2I~\cite{lu2019learning}, NLNS~\cite{hottung2020neural}, and DACT~\cite{ma2021learning} integrate RL-based policies with local operators to iteratively refine a given solution. In the context of the divide-and-conquer paradigm, RBG~\cite{zong2022rbg} and L2D~\cite{li2021learning} employ heuristic rules for the partition policy, while GPLC paradigms TAM~\cite{hou2023generalize} and GLOP~\cite{ye2024glop} utilize neural partition policies. We adhere to the official implementations of these methods and the instructions provided by other works~\cite{hou2023generalize, ye2024glop, fang2024invit} that cite these methods to replicate the experimental results. For RBG and TAM, we directly use the reported results from the papers. For a fair comparison, we only consider baseline methods that either have official code available for reproduction or have been extensively reported in previous papers. Further details are deferred to the Appendix-A.6. In addition, for comparison, the metrics include the average solution costs ($Avg.$), the standard deviation of solution costs ($Std.$), and average inference time ($Time$).

\subsection{Performance Comparisons}
In Table~\ref{tab:1}, our proposed RL-driven and SL-driven HLGP are compared with various previous methods on cross-size datasets. When compared to the state-of-the-art method, LEHD, RL-driven HLGP shows only a slight performance drop, notably on the CVRP1K dataset. Across other cross-size datasets, RL-driven HLGP consistently delivers superior solutions and is notably more efficient than LEHD. In comparison to all other learning-based baselines, SL-driven HLGP consistently demonstrates its superiority in terms of average cost while maintaining efficiency. Moreover, compared to classical heuristic solvers, SL-driven HLGP can swiftly produce high-quality solutions. In the most challenging case, CVRP10K, SL-driven HLGP stands out as the only method capable of generating high-quality solutions within 4 minutes for each instance. Additionally, owing to the adopted HL framework to mitigate compounded errors, both RL-driven HLGP and SL-driven HLGP outperform their respective baselines (GLOP and BQ).

Table~\ref{tab:2} displays the comparison of our proposed methods with various existing methods on both cross-distribution datasets and cross-size and distribution datasets. When compared to BQ and LEHD, our RL-driven HLGP exhibits a minor performance decline on cross-distribution datasets. However, on the cross-size and distribution datasets, RL-driven HLGP consistently showcases superior generalization by efficiently producing improved solutions. In comparison to all previous learning-based methods, SL-driven HLGP consistently outperforms on both cross-size and cross-size and distribution datasets. Moreover, the performance of SL-driven HLGP closely approaches that of HGS while being dramatically more efficient. This justifies the use of supervised partition policy especially for larger instances. Please refer to the Appendix-A.3 and Appendix-A.4 for the hyperparameter studies, ablation studies, and visualization results.


\section{Conclusion}
In this work, we propose a novel hierarchical learning-based framework for the graph partition in CVRP. The global partition policy and local partition policy synergistically tackle the partition task to progressively alleviate the compounded misclusterings. Our methods adopt a unified objective function harmoniously compatible with both RL and SL training methods. Meanwhile, we thoroughly analyze the significance of treating the subproblems encountered during training as individual instances in both the RL and SL settings. Experimental results unequivocally demonstrate the generalization capability of proposed HLGP framework in finding low-cost CVRP solutions under distribution and scale shifts. 
%At present, we mainly demonstrate the effectiveness of our method by solving CVRP. 
In future, we plan to extend our HLGP to more different VRP variants like CVRPTW and Min-max CVRP, as well as other types of COPs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

\begin{acks}
This research is supported by a generous research grant from Xiaoi Robot Technology Limited, the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-RP-2022-031), and the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grant.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{bibfile}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\input{appendix}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

