\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\setcounter{section}{0}
\setcounter{equation}{0}
\renewcommand\thesection{\Alph{section}} 

\section{Experiments}

In this section, we present the specifics of both training and evaluation for our proposed HLGP methods, including the problem settings, hyperparameter configurations, and practical training protocols. Subsequently, we conduct both a hyperparameter study and an ablation study to scrutinize the influence of key hyperparameters and modules in the HLGP framework on overall performance. Additionally, we include visualization results for intuitive comparisons. Furthermore, detailed implementations of baseline methods are provided.

\subsection{Training Details}


During the training phase for both RL-driven and SL-driven HLGP, we strictly adhere to the standard problem settings used in GLOP~\cite{ye2024glop}. In these settings, each CVRP instance consists of 1000 nodes uniformly distributed within a unit square area. The vehicle capacity, denoted as $D$, is fixed at 200. The maximum permissible number of times the vehicle returns to the depot is accordingly calculated as $\lceil \sum_{i=1}^{N_{v}} d_{i} / D \rceil + 1$, where $N_{v}$ represents the number of customer nodes and $d_{i}$ signifies the demands associated with each customer node. The demand $d_{i}$ is uniformly sampled from the range $\{1, \ldots, 9 \}$.

The RL-driven HLGP adopts the same isomorphic GNN architecture~\cite{joshi2019efficient, qiu2022dimes} as employed in GLOP~\cite{ye2024glop} for both the global and local partition policies to generate partition heatmaps. These heatmaps, specific to each CVRP instance, aid in progressively decoding feasible partition solutions that adhere to the predefined CVRP constraints. Subsequently, the same pretrained local permutation policy, as utilized in GLOP~\cite{ye2024glop}, is applied to resolve the subproblems derived from these feasible partition solutions. The resulting subsolutions to these subproblems constitute the overall CVRP solution. Consequently, the cost of the CVRP solution corresponds to the evaluation of the feasible partition solution.

The training phase of the RL-driven HLGP comprises 20 epochs, with each epoch consisting of 256 iterations. During each iteration, 5 CVRP instances are randomly generated to train both the global and local partition policies. For detailed training procedures, please refer to the Algorithm Pseudocodes. Consistent with the approach in GLOP~\cite{ye2024glop}, 20 solutions are simultaneously generated for both global and local partition policies to calculate the baseline, thereby reducing gradient variance. The Adam Optimizer is employed with an initial learning rate of $3 \times 10^{-4}$ for the involved partition policies. Additionally, a Cosine Annealing scheduler is utilized to gradually decrease the learning rate as the training epoch progresses. Similarly, we impose a constraint to maintain the maximum L2 norm of gradients below 1. The RL-driven HLGP is trained on a NVIDIA RTX 3090 GPU and an INTEL(R) XEON(R) GOLD 5218R CPU@2.10GHz.


\begin{table*}[hbt]
    \caption{Hyperparameter study for the number of levels in RL-driven HLGP (denoted as $K$). ``G" denotes the Gaussian distribution. ``U" denotes the Uniform distribution.}
    \setlength\tabcolsep{0.9pt}
    \renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{cccccccccccccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{RL-driven HLGP}  & \multicolumn{3}{c}{$K=1$} & \multicolumn{3}{c}{$K=2$} & \multicolumn{3}{c}{$K=3$} & \multicolumn{3}{c}{$K=4$} & \multicolumn{3}{c}{$K=5$} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16}
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} \\
    \midrule[0.7pt]
    CVRP1K+G & 35.13 & 1.26 & 1.18s & 34.84 & 1.26 & 1.78s	& 34.69 & 1.25 & 2.37s & 34.63 & 1.25 & 2.91s & 34.58 & 1.26 & 3.47s \\
    CVRP2K+U & 60.32 & 1.18 & 3.94s & 59.85 & 1.18 & 5.50s & 59.70 & 1.17 & 7.07s & 59.61 & 1.16 & 8.51s & 59.58 & 1.17 & 10.03s \\
    \midrule[0.7pt]
    \multirow{2}{*}{RL-driven HLGP}  & \multicolumn{3}{c}{$K=6$} & \multicolumn{3}{c}{$K=7$} & \multicolumn{3}{c}{$K=8$} & \multicolumn{3}{c}{$K=9$} & \multicolumn{3}{c}{$K=10$} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16}
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} \\
    \midrule[0.7pt]
    CVRP1K+G & 34.56 & 1.25 & 4.10s & 34.51 & 1.24 & 4.57s	& 34.51 & 1.24 & 5.11s & 34.51 & 1.24 & 5.77s & 34.51 & 1.24 & 6.31s \\
    CVRP2K+U & 59.56 & 1.17 & 11.40s & 59.55 & 1.17 & 13.00s & 59.50 & 1.17 & 14.70s & 59.50 & 1.17 & 16.39s & 59.50 & 1.17 & 17.66s \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:rl_K}
    %\vspace{-8pt}
\end{table*}



\begin{table*}[hbt]
    \caption{Hyperparameter study for the number of levels in SL-driven HLGP (denoted as $K$). ``G" denotes the Gaussian distribution. ``U" denotes the Uniform distribution.}
    \setlength\tabcolsep{0.9pt}
    \renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{cccccccccccccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{SL-driven HLGP}  & \multicolumn{3}{c}{$K=1$} & \multicolumn{3}{c}{$K=2$} & \multicolumn{3}{c}{$K=3$} & \multicolumn{3}{c}{$K=4$} & \multicolumn{3}{c}{$K=5$} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16}
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} \\
    \midrule[0.7pt]
    CVRP1K+G & 32.65 & 1.21 & 4.38s & 32.60 & 1.21 & 5.06s	& 32.58 & 1.21 & 5.78s & 32.56 & 1.21 & 6.49s & 32.55 & 1.21 & 7.21s \\
    CVRP2K+U & 57.73 & 1.80 & 19.54s & 57.69 & 1.08 & 23.48s & 57.67 & 1.08 & 26.96s & 57.67 & 1.08 & 29.97s & 57.67 & 1.08 & 32.40s \\
    \midrule[0.7pt]
    \multirow{2}{*}{SL-driven HLGP}  & \multicolumn{3}{c}{$K=6$} & \multicolumn{3}{c}{$K=7$} & \multicolumn{3}{c}{$K=8$} & \multicolumn{3}{c}{$K=9$} & \multicolumn{3}{c}{$K=10$} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16}
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} \\
    \midrule[0.7pt]
    CVRP1K+G & 32.55 & 1.21 & 7.92s & 32.55 & 1.21 & 8.62s	& 32.55 & 1.21 & 9.34s & 32.55 & 1.21 & 10.05s & 32.54 & 1.21 & 10.76s \\
    CVRP2K+U & 57.66 & 1.08 & 34.96s & 57.66 & 1.08 & 37.60s & 57.66 & 1.08 & 40.00s & 57.66 & 1.08 & 42.37s & 57.66 & 1.08 & 44.77s \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:sl_K}
    %\vspace{-8pt}
\end{table*}


The SL-driven HLGP leverages the same variant Transformer architecture utilized in BQ~\cite{drakulic2024bq} for both the global and local partition policies to incrementally generate the partition solution. Although BQ~\cite{drakulic2024bq} was initially designed to serve as a neural solver for producing the CVRP solution directly, it can also be repurposed to generate the partition solution. This adaptability arises from the fact that each subtour in the CVRP solution can be extracted by rearranging the node sequence within the subgraph of the partition solution. Consequently, the decoded node sequence from BQ~\cite{drakulic2024bq} can be interpreted as the partition solution, regardless of the specific order of decoding. This justifies that any neural solver crafted for generating CVRP solutions can be repurposed as the partition policy. Subsequently, the same pretrained local permutation policy utilized in the RL-driven HLGP is applied to address the subproblems derived from partition policies. Similarly, the cost of the CVRP solution corresponds to the evaluation of the partition solution.


The training phase of the SL-driven HLGP consists of two stages. Notably, given that the two partition policies are initialized randomly, within our proposed multi-level hierarchical learning (HL) framework, incorporating beam search into the global and local partition policies at each level to generate partition solutions as labels for large-scale instances does not ensure that these solutions are of sufficiently high quality to effectively supervise the training of the involved partition policies. Additionally, there are no optimal solvers specifically tailored for the partition task in the context of CVRP. However, as previously mentioned, the CVRP solution can be regarded as a partition solution, irrespective of the node order, with the optimal CVRP solution being synonymous with the optimal partition solution. Thus, in the first stage, we employ curriculum learning to train both the global and local partition policies on CVRP instances comprising 100 nodes. The labels for the small-scale instances of 100 nodes are generated from the procedure employed in BQ~\cite{drakulic2024bq}, and we adhere to the training methodology outlined in BQ~\cite{drakulic2024bq} during this stage. In the second stage, we use CVRP instances with 1000 nodes to train the partition policies. For detailed training procedures in this stage, please refer to the Algorithm Pseudocodes. This stage comprises 20 epochs, each consisting of 500 iterations. Within each epoch, 100 problem instances with 1000 nodes are randomly sampled. The corresponding labels are generated by executing the involved policies with beam search at each level within our proposed multi-level HL framework (as illustrated in Figure 2(b) in the main body), with a beam size of 16. These labeled instances constitute the training dataset. During each iteration, batches of data from the training dataset are utilized to train the global partition policy, with a batch size set at 50. Additionally, we employ the Adam Optimizer with an initial learning rate of $1 \times 10^{-5}$. The learning rate is subject to decay by a factor of 0.9 every 5 update steps. We enforce a constraint to keep the maximum L2 norm of gradients below 1. The coefficients for the L2 regularization terms in the loss function are set at $1 \times 10^{-6}$, denoted as $\lambda_{G}=\lambda_{L}=1 \times 10^{-6}$, which aligns with common practices in Transformer-based neural solvers~\cite{kwon2020pomo,kim2022sym,zhou2023towards,gao2023towards}. The SL-driven HLGP is trained on 4 NVIDIA RTX 3090 GPUs and an INTEL(R) XEON(R) GOLD 5218R CPU@2.10GHz.





\subsection{Evaluation Details}


During the evaluation phase, our focus is on assessing the generalization capabilities of various models. Therefore, we utilize diverse datasets with varying scales and distributions. Each evaluation dataset can specify the number of customer nodes as 1000, 2000, 5000, 7000, or 10000. The customer nodes in each dataset are distributed according to a Uniform distribution, a Gaussian distribution, an explosion pattern, or a rotation pattern. Except for the dataset with 1000 customer nodes setting capacity as 200, the capacity for the other datasets is set to 300. Each dataset comprises 128 instances. The process of generating problem instances aligns with the methodologies outlined in~\cite{kool2018attention, zhou2023towards}. In addition, for comparison, the metrics include the average solution costs ($Avg.$), the standard deviation of solution costs ($Std.$), and average inference time ($Time$). 
For both RL-driven and SL-driven HLGP, the number of levels, denoted as $K$ is set as 5 across various evaluation settings. Additionally, the beam size in SL-driven HLGP for the evaluation is set as 16, 16, 8, 4, and 4 for the CVRP datasets with node counts of 1000, 2000, 5000, 7000, and 10000, respectively.


\subsection{Hyperparameter Studies}

We begin by examining the influence of the number of levels in the multi-level HL framework, denoted as $K$, on the performance of both RL-driven and SL-driven HLGP. In Table~\ref{tab:rl_K}, we assess RL-driven HLGP using the CVRP1K dataset with Gaussian distributed nodes (CVRP1K+G) and the CVRP2K dataset with uniformly distributed nodes (CVRP2K+U). Here, $K$ ranges from 1 to 10. It is evident that the average costs gradually converge to 34.51 when $K=7$ for the CVRP1K+G dataset and to 59.50 when $K=8$ for the CVRP2K+U dataset. Concurrently, the time consumption increases as the number of levels rises. Table~\ref{tab:sl_K} illustrates the performance variations of SL-driven HLGP on the CVRP1K+G and CVRP2K+U datasets as $K$ varies. Notably, for the CVRP1K+G dataset, the average cost has already converged to 32.55 when $K=5$, while for the CVRP2K+U dataset, the convergent average cost is 57.66 at $K=6$. Likewise, the time taken for computation escalates with the increase in the number of levels. In practice, to trade off the performance against efficiency, we select $K=5$ for both RL-driven and SL-driven HLGP.



We then investigate the impact of the coefficients of the entropy terms corresponding to the global and local partition policies, denoted as $\lambda_{G}$ and $\lambda_{L}$, in the loss function of RL-driven HLGP. Since the value of $\lambda_{G}$ directly influences the performance of the global partition policy, as shown in Table~\ref{tab:rl_lambda_G}, we analyze how the performance of the global partition policy varies across four datasets with node numbers ranging from 1000 to 7000 as $\lambda_{G}$ changes. Notably, setting $\lambda_{G}$ to 0.1 consistently yields the best performance across all datasets. In Table~\ref{tab:rl_lambda_L}, we display the performance of RL-driven HLGP on the same four datasets as $\lambda_{L}$ varies. The model with $\lambda_{L}=0.005$ excels on three out of four datasets. Therefore, we decide to set $\lambda_{G}=0.1$ and $\lambda_{L}=0.005$. It is worth noting that the coefficients of L2 regularization terms in the loss function are commonly set to $1 \times 10^{-6}$ in Transformer-based neural solvers~\cite{kwon2020pomo,kim2022sym,zhou2023towards,gao2023towards}. Therefore, we maintain this consistent setting for the coefficients of the L2 regularization terms for both the global and local partition policies, denoted as $\lambda_{G}=\lambda_{L}=1 \times 10^{-6}$. This configuration has proven effective in our SL-driven HLGP setup.




\begin{table}[t]
    \caption{Hyperparameter study for $\lambda_{G}$ in RL-driven HLGP on uniformly distributed CVRP instances.}
    \setlength\tabcolsep{0.9pt}
    \renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{lcccccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{$\lambda_{G}$}  & \multicolumn{2}{c}{CVRP1K} & \multicolumn{2}{c}{CVRP2K} & \multicolumn{2}{c}{CVRP5K} & \multicolumn{2}{c}{CVRP7K} \\
    \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} 
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} \\
    \midrule[0.7pt]
0.1 & 46.40 & 0.91 & 63.55 & 1.19 & 138.37 & 3.51 & 187.91 & 4.99 \\
0.05 & 46.48 & 0.82 & 63.68 & 1.21 & 138.68 & 3.57 & 188.65 & 5.09 \\
0.01 & 46.51 & 0.92 & 63.69 & 1.19 & 139.02 & 3.57 & 189.31 & 5.13 \\
0.005 & 46.54 & 0.92 & 63.77 & 1.20 & 138.74 & 3.54 & 188.71 & 5.05 \\
0.001 & 46.64 & 0.93 & 63.84 & 1.20 & 139.49 & 3.61 & 190.13 & 5.18 \\
0.0005 & 46.54 & 0.92 & 63.78 & 1.20 & 139.20 & 3.62 & 189.80 & 5.23 \\
0.0001 & 46.60 & 0.92 & 63.70 & 1.20 & 138.89 & 3.60 & 189.04 & 5.19 \\
0.0 & 46.55 & 0.94 & 63.75 & 1.21 & 139.16 & 3.58 & 189.30 & 5.14 \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:rl_lambda_G}
    %\vspace{-8pt}
\end{table}


\begin{table}[t]
    \caption{Hyperparameter study for $\lambda_{L}$ in RL-driven HLGP on uniformly distributed CVRP instances.}
    \setlength\tabcolsep{0.9pt}
    \renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{lcccccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{$\lambda_{L}$}  & \multicolumn{2}{c}{CVRP1K} & \multicolumn{2}{c}{CVRP2K} & \multicolumn{2}{c}{CVRP5K} & \multicolumn{2}{c}{CVRP7K} \\
    \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} 
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} \\
    \midrule[0.7pt]
0.1 & 43.92 & 0.82 & 59.81 & 1.11 & 128.31 & 3.04 & 173.78 & 4.36 \\
0.05 & 43.91 & 0.82 & 59.73 & 1.12 & 128.16 & 3.00 & 173.63 & 4.35 \\
0.01 & 43.91 & 0.81 & 59.80 & 1.12 & 128.52 & 3.01 & 174.02 & 4.32 \\
0.005 & 43.78 & 0.85 & 59.58 & 1.17 & 128.12 & 3.06 & 173.71 & 4.39 \\
0.001 & 44.01 & 0.81 & 60.01 & 1.13 & 128.38 & 3.02 & 173.40 & 4.22 \\
0.0005 & 44.05 & 0.82 & 60.14 & 1.11 & 128.38 & 3.03 & 173.35 & 4.23 \\
0.0001 & 44.47 & 0.94 & 60.14 & 1.25 & 133.39 & 3.41 & 182.30 & 4.78 \\
0.0 & 43.94 & 0.86 & 59.70 & 1.17 & 129.07 & 3.08 & 175.10 & 4.27 \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:rl_lambda_L}
    %\vspace{-8pt}
\end{table}




\subsection{Ablation Studies}



\begin{table*}[hbt]
    \caption{Ablation study for RL-driven HLGP.}
    %\setlength\tabcolsep{0.9pt}
    %\renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{lcccccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{RL-driven HLGP}  & \multicolumn{2}{c}{glob.} & \multicolumn{2}{c}{glob.+subp.} & \multicolumn{2}{c}{glob.+loc.} & \multicolumn{2}{c}{glob.+loc.+subp.} \\
       \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} 
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} \\
    \midrule[0.7pt]
CVRP1K+G & 39.12 & 1.41 & 37.98 & 1.35 & 35.04 & 1.28 & 34.58 & 1.26 \\
CVRP2K+U & 64.38 & 1.25 & 63.55 & 1.19 & 60.33 & 1.14 & 59.58 & 1.17 \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:rl_abl}
    %\vspace{-8pt}
\end{table*}


\begin{table*}[hbt]
    \caption{Ablation study for SL-driven HLGP.}
    %\setlength\tabcolsep{0.9pt}
    %\renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{lcccccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{SL-driven HLGP}  & \multicolumn{2}{c}{SS.+glob.} & \multicolumn{2}{c}{SS.+LS.+glob.} & \multicolumn{2}{c}{SS.+glob.+loc.} & \multicolumn{2}{c}{SS.+LS.+glob.+loc.} \\
       \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} 
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Avg.$} & \normalsize{$Std.$} \\
    \midrule[0.7pt]
CVRP1K+G & 33.28 & 1.26 & 32.73 & 1.21 & 32.88 & 1.23 & 32.55 & 1.21 \\
CVRP2K+U & 59.51 & 1.09 & 57.77 & 1.08 & 59.10 & 1.09 & 57.67 & 1.08 \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:sl_abl}
    %\vspace{-8pt}
\end{table*}

\begin{table*}[hbt]
    \caption{Time overheads analysis for HLGP.}
    %\setlength\tabcolsep{0.9pt}
    %\renewcommand{\arraystretch}{0.9}
    \small
    \centering
    \begin{tabular}{lcccccc}
    \toprule[1.2pt]
       \multirow{2}{*}{RL-driven HLGP} & \multicolumn{3}{c}{global partition process} & \multicolumn{3}{c}{overall partition process} \\
       \cmidrule(r){2-4} \cmidrule(r){5-7}  
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} \\
    \midrule[0.7pt]
CVRP1K+G & 37.98 & 1.35 & 0.58s & 34.58 & 1.26 & 3.47s \\
CVRP2K+U & 63.55 & 1.19 & 2.37s & 59.58 & 1.17 & 10.03s \\
    \midrule[0.7pt]
    \multirow{2}{*}{SL-driven HLGP} & \multicolumn{3}{c}{global partition process} & \multicolumn{3}{c}{overall partition process} \\
       \cmidrule(r){2-4} \cmidrule(r){5-7}  
        & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} & \normalsize{$Avg.$} & \normalsize{$Std.$} & \normalsize{$Time$} \\
    \midrule[0.7pt]
        CVRP1K+G & 32.73 & 1.21 & 3.69s & 32.55 & 1.21 & 7.21s \\
        CVRP2K+U & 57.77 & 1.08 & 15.11s & 57.67 & 1.08 & 32.40s \\
    \bottomrule[1.2pt]
    \end{tabular}
    \label{tab:overhead}
    %\vspace{-8pt}
\end{table*}


In this section, we first evaluate the individual contributions of each module in RL-driven HLGP on the CVRP1K+G and CVRP2K+U datasets. In Table~\ref{tab:rl_abl}, ``glob." denotes that only the global partition policy is utilized, trained solely on randomly sampled CVRP1K instances. ``glob.+subp." indicates that the global partition policy is trained on both the random CVRP1K instances and the encountered subproblems during training. On the other hand, ``glob.+loc." and ``glob.+loc.+subp." signify that these models incorporate the local partition policy to progressively rectify misclusterings. It is evident from Table~\ref{tab:rl_abl} that both ``subp" and ``loc." modules contribute to performance enhancement. However, the ``loc." module notably makes a more substantial contribution to the final performance on both evaluation datasets. Additionally, Figure~\ref{fig:rl_curves} presents the training and validation curves of the global partition policy of RL-driven HLGP. It is clear that incorporating the encountered subproblems during training for the global partition policy can expedite the training process. 


In Table~\ref{tab:sl_abl}, we explore the individual contributions of each module in SL-driven HLGP on the CVRP1K+G and CVRP2K+U datasets. "SS.+glob." signifies that only the global partition policy trained on small-scale instances (100 nodes) is used for evaluation. "SS.+LL.+glob." indicates that the global partition policy is additionally trained on large-scale instances (1000 nodes). Similarly, "SS.+glob.+loc." and "SS.+LS.+glob.+loc." represent models incorporating the local partition policy. The table indicates that when evaluating the CVRP1K+G dataset, the "LS." and "loc." modules roughly make the equivalent contributions to the performance. However, on the larger-scale dataset (CVRP2K+U), the "LS." module clearly outweighs the "loc." module in importance. This suggests that our proposed multi-level HL framework, coupled with beam search for each involved policy, can indeed yield dependable high-quality solutions serving as labels for supervised training, thereby enhancing the generalization capability of the partition policy.

\begin{figure}[t]
\centering
\subfigure[]{\label{fig:rl_curve_1}\includegraphics[width=0.85\columnwidth]{figures/curves/train_curves.pdf}}
\subfigure[]{\label{fig:rl_curve_2}\includegraphics[width=0.85\columnwidth]{figures/curves/val_curves.pdf}}
\caption{The training curve (a) and the validation curve (b) of the global partition policy in RL-driven HLGP.
%\vspace{-10pt}
}
\label{fig:rl_curves}
\end{figure}

\subsection{Time Overheads Analysis}
In this section, we analyze the time overhead contributions of the global partition process and the local partition process for both RL-driven HLGP and SL-driven HLGP, as illustrated in Table~\ref{tab:overhead}. In the RL-driven HLGP, it is evident that the local partition process contributes significantly more to the average time overhead compared to the global partition process. This disparity arises due to the local partition process typically involving multiple levels of partitioning. Conversely, in the SL-driven HLGP, we observe a similar average time overhead between the global and local partition processes. This similarity can be attributed to the nature of the global partition process in the SL-driven HLGP. In this process, with each step, a new encountered subproblem is derived for the current node selection, necessitating each encountered subproblem to be processed by the policy network. As the number of encountered subproblems is linearly proportional to the graph size, the time overhead of the global partition process increases accordingly. In contrast, in the global partition process of the RL-driven HLGP, only an encountered subproblem is formed and fed to the policy network when a new subgraph is constructed. Consequently, the time overhead associated with the global partition process is less significant compared to that of the local partition process.

\subsection{Visualization Results}

In Figure~\ref{fig:rl_visual} and Figure~\ref{fig:sl_visual}, we present the visualized CVRP solutions generated by RL-driven and SL-driven HLGP, respectively, on the CVRP1K datasets with distributions including Uniform, Gaussian, Explosion, and Rotation.



\subsection{Implementation details of baselines}
In this section, we provide the implementation details of the baseline methods, including both both non-learning and learning-based approaches, selected for comparison with our proposed approaches.

\textbf{LKH3.} We adhere to the settings outlined in Omni-POMO~\cite{zhou2023towards} to reproduce the results of LKH3. For solving each CVRP instance with varying scales and distributions, we set the maximum number of trials to 10000.


\textbf{HGS.} In order to decrease the runtime, we adjust the number of iterations of HGS for the CVRP datasets with varying scales. Specifically, we set the number of iterations to 20000, 5000, 2000, 1500, and 1000 for the CVRP instances with node counts of 1000, 2000, 5000, 7000, and 10000, respectively.

\textbf{AM.} We follow the implementation guidelines of AM as outlined by~\citet{ye2024glop}, where the checkpoint trained on CVRP100 is adapted for comparison purposes. In the evaluation stage, to enhance the quality of solutions generated by AM, we employ a sampling decoding strategy with 1280 solutions for each instance. The temperature of the softmax function in the output layer is set to 0.1 to prevent solutions from diverging towards lower-quality outcomes.


\textbf{POMO.} To benchmark against POMO, we generalize the checkpoint trained on CVRP100 to generate results across different CVRP datasets. The POMO size is adjusted to align with the number of nodes in each CVRP instance. Furthermore, data augmentation is implemented for each CVRP instance with an augmentation factor of 8.


\textbf{Sym-POMO.} We stick to the original implementations of Sym-POMO, where the POMO size is adjusted to match the number of nodes, and each CVRP instance is augmented by a factor of 8. The checkpoint trained on CVRP100 is generalized for comparisons.


\textbf{AMDKD.} AMDKD aims to improve the generalization capacity of neural solvers on CVRP datasets with scale and distribution shifts through knowledge distillation. We adhere to the original evaluation settings with the model trained via knowledge distillation on CVRP100 datasets with various distributions. The default POMO size and augmentation factor are employed in our evaluations.


\textbf{Omni-POMO.} Omni-POMO aims to transfer neural solvers to problem instances with diverse scales and distributions through meta-learning techniques. Therefore, we strictly follow the evaluation configurations by utilizing the provided model trained via MAML with 250000 epochs to replicate the results of Omni-POMO across various CVRP datasets. The POMO size is configured to match the number of nodes in each CVRP instance. Additionally, data augmentation is applied to every CVRP instance with an augmentation factor of 8.


\textbf{ELG-POMO.} ELO-POMO integrates both a global neural solver and a local neural solver to leverage insensitive local topological features, thereby enhancing the overall system's generalization capability. We generalize the checkpoint trained on CVRP100 with 250000 epochs for comparison purposes. Throughout the evaluation phase, we maintain the default values of the POMO size and augmentation factors as recommended in the original implementations.


\textbf{INViT.} INViT utilizes multiple encoders to capture the features of subgraphs of varying scales within each CVRP instance, enabling the utilization of local topological features for the generalization improvement. In our study, we generalize the checkpoint trained on CVRP100 to evaluate the generalization ability against other baselines. During the evaluation phase, given that each instance in the evaluation dataset consists of at least 1000 nodes, the beam size is accordingly set to 16 for each evaluation CVRP dataset, as recommended in the original implementation. All other settings strictly conform to the original implementations.


\textbf{LEHD.} LEHD incorporates iterative local revision (known as RRC) by utilizing a SL-trained neural solver to improve the generalization capability of the constructive method. We leverage the officially provided checkpoint to compare its generalization ability with other methods. During the evaluation phase, as all baseline methods are evaluated on large-scale instances with a minimum of 1000 nodes, to balance performance and efficiency, the number of RRC is set to 200, 50, 5, and 1 for CVRP datasets with node counts of 1000, 2000, 5000, and 7000. For the CVRP10K dataset, the greedy model is directly deployed for comparison purposes.


\textbf{BQ.} BQ enhances generalization ability by leveraging the inherent recursive property of the CVRP instance. We utilize the officially provided checkpoint to generalize its application to various CVRP datasets. During the evaluation phase, to achieve a trade-off between performance and efficiency, the beam size is set to 16, 16, 8, 4, and 4 for the CVRP datasets with node counts of 1000, 2000, 5000, 7000, and 10000, respectively. Other settings remain aligned with the original implementations.


\textbf{L2I.} L2I is an iterative approach that merges an RL-based policy with predefined local operators to iteratively enhance a given solution. We directly apply the provided official checkpoint of the RL-based policy and the predefined set of local operators to diverse CVRP datasets for generalization comparisons. Additionally, to prevent unexpected increases in makespan due to excessive iterations, we limit the maximum number of rollout steps for the RL-based policy to 40, 10, 6, 4, and 2 for the CVRP datasets with node counts of 1000, 2000, 5000, 7000, and 10000, respectively.


\textbf{NLNS.} NLNS seamlessly integrates heuristic destroy operators and a collection of learning-based repair policies to iteratively enhance a given solution. We closely follow the official implementations of NLNS for generalization comparisons, with the exception of the setting for maximum running time per instance. Since NLNS is intended to generalize to large-scale instances, the maximum running time in our study is extended to approximately 1200ms per instance for various CVRP datasets.


\textbf{DACT.} DACT, as an iterative method, redesigns the solution representation for the RL-based policy to improve the given solution. We thus generalize the official checkpoint trained on CVRP100 to various CVRP datasets for comparisons. Following the guidelines in the official implementation, to tailor DACT to larger CVRP instances, smaller values are recommended for the number of perturbations and the dummy rate. As a result, we set the dummy rate to 0.05 and the number of perturbations to 2. Furthermore, to prevent unexpected increases in makespan resulting from an excessive number of iterations, the maximum rollout steps of the RL-based policy are set to 400.


\textbf{L2D.} L2D is replicated by adhering to the publicly available open-source implementation across various CVRP datasets. It is impressive to observe L2D's strong performance on CVRP datasets exhibiting distribution and scale shifts. This success can be attributed to L2D's heavy reliance on near-optimal solutions from numerous neighboring subgraphs, derived from classical heuristic solvers, to guide both clustering subgraphs and resolving subproblems.


\textbf{GLOP.} We rigorously follow the official implementations of GLOP to benchmark it against other baseline methods. Moreover, for the selected baseline methods that align with those in GLOP, we rely on GLOP's recommendations to reproduce these methods, thereby ensuring that our replicated results are at least on par with those documented in GLOP.





%\section{Algorithm Details}
%\subsection{Algorithm Pseudocodes}

\section{Algorithm Pseudocodes}
The algorithm pseudocodes for both RL-driven and SL-driven HLGP are illustrated in Algorithm~\ref{alg:rl} and Algorithm~\ref{alg:sl}, respectively.



\begin{algorithm}[h]
    \caption{RL-driven HLGP}
    \label{alg:rl}
    %\textbf{Initialize}: global partition policy $\pi_{\theta_{G}^{0}}$, local partition policy $\pi_{\theta_{L}^{0}}$; \\
    %\textbf{Input}: CVRP instance generator
    %$\mathrm{Gen}_{\mathrm{CVRP}}$, number of iterations $N$; \\
    %\textbf{Parameter}: number of levels $K$, learning rate lr; \\
    %\textbf{Output}: updated global partition policy $\pi_{\theta_{G}^{N}}$, updated local partition policy $\pi_{\theta_{L}^{N}}$ \\
    \begin{algorithmic}[1] %[1] enables line numbers
    \REQUIRE Problem instance distribution $p_{I}$, permutation policy $\pi_{\mathrm{perm}}$
    \ENSURE global partition policy $\pi_{\theta_{G}}$, local partition policy $\pi_{\theta_{L}}$
        \FOR{$n=0$ to $N$}
        %\STATE \texttt{\# sample a CVRP instance}
        \STATE Sample an instance: $I=(G, D, N_{\mathrm{max}}) \sim p_{I}$
        \STATE Initialize $\mathcal{C}^{(0)}$: $\mathcal{C}^{(0)} = \{\}$
        \STATE \textbf{while} $G \neq \emptyset$ \textbf{do}
        \STATE \quad \texttt{\# sample a partition solution}
        \STATE \quad $\hat{\mathcal{C}}^{(0)} = \{\hat{c}_{1}^{(0)}, \ldots,  \hat{c}_{N_{c}}^{(0)}\} \sim \pi_{\theta_{G}^{n}}(\cdot|I)$
        \STATE \quad \texttt{\# construct the subproblem}
        \STATE \quad $N_{\mathrm{max}} \leftarrow N_{\mathrm{max}} - 1$, $I \leftarrow (\cup_{i=2}^{N_{c}}\hat{c}_{i}^{(0)}, D, N_{\mathrm{max}})$
        \STATE \quad Update $\mathcal{C}^{(0)}$: $\mathcal{C}^{(0)} \leftarrow \mathcal{C}^{(0)} \cup \{ \hat{c}_{1}^{(0)} \}$
        \STATE \quad \texttt{\# train global partition policy}
        \STATE \quad CVRP solution and Cost: $\mathcal{T} \sim \pi_{\mathrm{perm}}(\cdot|\hat{\mathcal{C}}^{(0)})$, $e(\mathcal{T})$
        \STATE \quad Update parameter: $\theta_{G}^{n} \leftarrow
        \mathrm{AdamOpt}(\theta_{G}^{n}, \nabla_{\theta_{G}}\hat{J}(\theta_{G}^{n}, \theta_{L}^{n}))$
        \STATE \textbf{end while}
        \FOR{$k=1$ to K}
        \STATE Construct subproblems: $I_{j}^{(k-1)}, 1 \leq j \leq \lfloor \frac{N_{c}}{2}\rfloor$
        \STATE \texttt{\# sample partition solutions }
        \STATE $\mathcal{C}_{j}^{(k-1)} \sim \pi_{\theta_{L}^{n}}(\cdot|I_{j}^{(k-1)}), 1 \leq j \leq \lfloor \frac{N_{c}}{2}\rfloor$
        \STATE \texttt{\# train local partition policy}
        \STATE CVRP solutions and Costs: $\mathcal{T}_{j}\sim\pi_{\mathrm{perm}}(\cdot|\mathcal{C}_{j}^{(k-1)}), e(\mathcal{T}_{j})$
        \STATE Update parameter: $\theta_{L}^{n} \leftarrow
        \mathrm{AdamOpt}(\theta_{L}^{n}, \nabla_{\theta_{L}}\hat{J}(\theta_{G}^{n}, \theta_{L}^{n}))$
        \ENDFOR
        \STATE $\theta_{G}^{n+1} \leftarrow \theta_{G}^{n}$; $\theta_{L}^{n+1} \leftarrow \theta_{L}^{n}$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}



\begin{algorithm}[h]
    \caption{SL-driven HLGP}
    \label{alg:sl}
    %\textbf{Initialize}: global partition policy $\pi_{\theta_{G}^{0}}$ parameterized by $\theta_{G}^{0}$, local partition policy $\pi_{\theta_{L}^{0}}$ parameterized by $\theta_{L}^{0}$; \\
    %\textbf{Input}: CVRP instance generator $\mathrm{Gen}_{\mathrm{CVRP}}$, number of iterations $N$; \\
    %\textbf{Parameter}: number of levels $K$, learning rate lr; \\
    %\textbf{Output}: updated global partition policy $\pi_{\theta_{G}^{N}}$, updated local partition policy $\pi_{\theta_{L}^{N}}$
    \begin{algorithmic}[1] %[1] enables line numbers
    \REQUIRE Problem instance distribution $p_{I}$, permutation policy $\pi_{\mathrm{perm}}$
    \ENSURE global partition policy $\pi_{\theta_{G}}$, local partition policy $\pi_{\theta_{L}}$
        \FOR{$N=0$ to $N$}
        %\STATE \texttt{\# sample a CVRP instance}
        \STATE Sample an instance: $I=(G, D, N_{\mathrm{max}}) \sim p_{I}$
        \STATE \texttt{\# generate the label}
        \STATE $\bar{\mathcal{C}}^{(0)} \sim \mathrm{Beam Search}
        (I, \pi_{\theta_{G}^{n}}, \pi_{\mathrm{perm}})$
        \FOR{$k=1$ to K}
        \STATE Construct subproblems $I_{j}^{(k-1)}, 1 \leq j \leq \lfloor \frac{N_{G}}{2}\rfloor$
        \STATE \texttt{\# solve each subproblem}
        \STATE $\bar{\mathcal{C}}_{j}^{(k-1)} \sim \mathrm{Beam Search}(I_{j}^{(k-1)}, \pi_{\theta_{L}^{n}}, \pi_{\mathrm{perm}})$
        \STATE Construct $\bar{\mathcal{C}}^{(K)}$ from the set of $\bar{\mathcal{C}}_{j}^{(k-1)}$.
        \ENDFOR
        \STATE $\bar{\mathcal{C}} \leftarrow \bar{\mathcal{C}}^{(K)}$
        \STATE \texttt{\# train global partition policy}
        \STATE Generate labeled instances: $(I_{N_{v}(t)}, \bar{\mathcal{C}}[t]), t \geq 1$
        \STATE Update parameter: $\theta_{G}^{n} \leftarrow
        \mathrm{AdamOpt}(\theta_{G}^{n}, \nabla_{\theta_{G}}\hat{J}(\theta_{G}^{n}, \theta_{L}^{n}))$
        \STATE \texttt{\# train local partition policy}
        \STATE Generate labeled instances: $(I_{i}, \bar{C}_{i}), i \geq 1$
        \STATE Update parameter: $\theta_{L}^{n} \leftarrow
        \mathrm{AdamOpt}(\theta_{L}^{n}, \nabla_{\theta_{L}}\hat{J}(\theta_{G}^{n}, \theta_{L}^{n}))$
        \STATE $\theta_{G}^{n+1} \leftarrow \theta_{G}^{n}$; $\theta_{L}^{n+1} \leftarrow \theta_{L}^{n}$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


\newpage

\section{Proofs}

\subsection{Proof of Theorem 1}
\begin{theorem}
\label{thm:gplc}
    The objective in solving an original CVRP instance $I$ is to identify a (permutation) policy $\pi(\mathcal{T}|I) \in \Delta(\mathbb{S}_{\mathcal{T}})$ so as to minimize the expected cost $\mathbb{E}_{\mathcal{T} \sim \pi}[e(\mathcal{T})]$. If $\pi_{\mathrm{perm}}^{\ast} \in \Delta(\mathbb{S}_{\mathcal{T}})$ is optimal for each subproblem $(c_{i}, D, 1)$, then the original objective can be reframed as identifying an optimal partition policy $\pi_{\mathrm{part}}^{\ast} \in \Delta(\mathbb{S}_{\mathcal{C}})$ to minimize the expected cost, expressed as:
    \begin{equation}
    \label{equ:gplc}
    \min_{\pi_{\mathrm{part}}} \; \mathbb{E}_{\mathcal{C} \sim \pi_{\mathrm{part}}} [\sum_{i=1}^{N_{c}} \mathbb{E}_{\tau_{i} \sim \pi_{\mathrm{perm}}^{\ast}} (e(\tau_{i})) ],
    \end{equation}
    The partition policy $\pi_{\mathrm{part}}$ and the permutation policy $\pi_{\mathrm{perm}}$ are formulated respectively as follows:
    \begin{equation}
    \label{equ:part_perm_pi}
    \begin{split}
    &\pi_{\mathrm{part}}(\mathcal{C}|I)=\prod_{n=1}^{N_{\mathrm{sol}}}\pi_{\mathrm{part}}(\mathcal{C}[n]|\mathcal{C}[0:n-1], I), \\
    &\pi_{\mathrm{perm}}(\mathcal{T}|\mathcal{C}) = \prod_{i=1}^{N_{c}} \pi_{\mathrm{perm}}(\tau_{i}|c_{i})
    \end{split}
    \end{equation}
    where $N_{\mathrm{sol}}$ denotes the length of partition solution.
\end{theorem}

\begin{proof}
    For a CVRP instance $I$, the feasible CVRP solution $\mathcal{T}$ comprises $N_{\tau}$ subtours $\tau_{i}, 1 \leq i \leq N_{\tau}$. If a partition solution $\mathcal{C}=\{c_{1}, \ldots, c_{N_{c}}\}$ corresponds to $\mathcal{T}$, then $\tau_{i}$ can be obtained by rearranging the nodes in $c_{i}$, and $N_{\tau} = N_{c}$. This observation indicates that a given CVRP solution $\mathcal{T}$ can be represented by the corresponding partition solution and the order of the nodes in each subgraph, denoted as $(\mathcal{C}, \mathcal{T}) = \{c_{1}, \ldots, c_{N_{c}}, \tau_{1}, \ldots, \tau_{N_{c}}\}$. Therefore, we can derive the following expressions for the primary objective.
    \begin{equation}
    \label{equ:thm1_1}
    \begin{split}
        &\min_{\pi} \;\mathbb{E}_{\mathcal{T}} [e(\tau)] \\
    = &\min_{\pi} \; \sum_{\mathcal{T}} \pi(\mathcal{T}|I) e(\mathcal{T}) \\
    = &\min_{\pi} \; \sum_{(\mathcal{C}, \mathcal{T})} \pi((\mathcal{C}, \mathcal{T})|I) e(\mathcal{T}) \\
    = &\min_{\pi_{\mathrm{part}}, \pi_{\mathrm{perm}}} \sum_{\mathcal{C}}\pi_{\mathrm{part}}(\mathcal{C}|I) \sum_{\mathcal{T}}\prod_{i=1}^{N_{c}}\pi_{\mathrm{perm}}(\tau_{i}|c_{i}) \sum_{i}^{N_{c}}e(\tau_{i}).
    \end{split}
    \end{equation}
    We can simplify the expectation term associated with $\pi_{\mathrm{perm}}$ further as follows:
    \begin{equation}
    \label{equ:thm1_2}
    \begin{split}
\sum_{\mathcal{T}}\prod_{i=1}^{N_{c}}\pi_{\mathrm{perm}}(\tau_{i}|c_{i}) \sum_{i}^{N_{c}}e(\tau_{i}) =  \sum_{i=1}^{N_{c}} \sum_{\tau_{i}} \pi_{\mathrm{perm}}(\tau_{i}|c_{i})e(\tau_{i}).
    \end{split}
    \end{equation}
Thus, we can plug Equation~\ref{equ:thm1_2} into Equation~\ref{equ:thm1_1} to derive the following objective function:
\begin{equation}
\label{equ:thm1_3}
\begin{split}
    &\min_{\pi_{\mathrm{part}}, \pi_{\mathrm{perm}}} \sum_{\mathcal{C}}\pi_{\mathrm{part}}(\mathcal{C}|I) \sum_{\mathcal{T}}\prod_{i=1}^{N_{c}}\pi_{\mathrm{perm}}(\tau_{i}|c_{i}) \sum_{i}^{N_{c}}e(\tau_{i}) \\
    = & \min_{\pi_{\mathrm{part}}, \pi_{\mathrm{perm}}} \sum_{\mathcal{C}}\pi_{\mathrm{part}}(\mathcal{C}|I) \sum_{i=1}^{N_{c}} \sum_{\tau_{i}} \pi_{\mathrm{perm}}(\tau_{i}|c_{i})e(\tau_{i}) \\
    = & \min_{\pi_{\mathrm{part}}} \sum_{\mathcal{C}}\pi_{\mathrm{part}}(\mathcal{C}|I) \sum_{i}^{N_{c}}\min_{\pi_{\mathrm{perm}}} \sum_{\tau_{i}} \pi_{\mathrm{perm}}(\tau_{i}|c_{i})e(\tau_{i}) \\
\end{split}
\end{equation}
If we denote the optimal permutation policy as $\pi_{\mathrm{perm}}^{\ast}$, then the primary objective can be expressed as:
\begin{equation}
    \label{equ:thm1_4}
        \min_{\pi_{\mathrm{part}}} \; \mathbb{E}_{\mathcal{C} \sim \pi_{\mathrm{part}}} [\sum_{i=1}^{N_{c}} \mathbb{E}_{\tau_{i} \sim \pi_{\mathrm{perm}}^{\ast}} (e(\tau_{i})) ].
    \end{equation}
\end{proof}

\subsection{Proof of Theorem 2}
\begin{theorem}
\label{thm:rl_obj}
Let $g(c_{i})$ denote $\mathbb{E}_{\tau_{i} \sim \pi_{\mathrm{perm}}^{\ast}(\cdot|c_{i})}(e(\tau_{i}))$. It is clear that $f(\mathcal{C}) = \sum_{i=1}^{N_{c}} g(c_{i})$ acts as a feasible cost function. Then, the optimization problem in the multi-level HL framework can be transformed equivalently as follows:
\begin{equation}
\label{equ:trans_obj}
\begin{split}
     \min_{\pi_{\mathrm{Gpart}}, \pi_{\mathrm{Lpart}}} &\mathbb{E}_{\mathcal{C}^{(0)}} [f(\mathcal{C}^{(0)})] +  \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}}[f(\mathcal{C}^{(1)}) - f(\mathcal{C}^{(0)})]+ \\
    &  \cdots + \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [f(\mathcal{C}^{(K)}) - f(\mathcal{C}^{(K-1)})].
\end{split}
\end{equation}
The evaluation for $\mathcal{C}^{(k)}, k \geq 1$, can further be derived as:
\begin{align}
\label{equ:eval_C}
    &f(\mathcal{C}^{(k)}) - f(\mathcal{C}^{(k-1)}) = \sum_{j=1}^{\lfloor\frac{N_{c}}{2}\rfloor}
    [h(\mathcal{C}^{(k)}, k, m) - h(\mathcal{C}^{(k-1)}, k, m)]; \nonumber \\
    &h(\mathcal{C}^{(k)}, k, m) = g(c_{(m+k-1)\%N_{c}+1}^{(k)}) + g(c_{(m+k)\%N_{c}+1}^{(k)}),
\end{align}
where $m=2(j-1)$.
\end{theorem}

\begin{proof}
The objective function of the multi-level HL framework can be equivalently transformed as follows:
\begin{equation}
\label{equ:thm2_1}
\begin{split}
    &\min_{\pi_{\mathrm{Gpart}}, \pi_{\mathrm{Lpart}}} \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [f(\mathcal{C}^{(K)})] \\
= & \min_{\pi_{\mathrm{Gpart}}, \pi_{\mathrm{Lpart}}} \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [f(\mathcal{C}^{(K)}) - f(\mathcal{C}^{(K-1)}) \\
+ & f(\mathcal{C}^{(K-1)}) - f(\mathcal{C}^{(K-2)}) + \ldots 
+ f(\mathcal{C}^{(1)}) - f(\mathcal{C}^{(0)}) + f(\mathcal{C}^{(0)})]
\end{split}
\end{equation}
The RHS of Equation~\ref{equ:thm2_1} can be further decomposed as follows:
\begin{equation}
\label{equ:thm2_2}
\begin{split}
&\mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [ f(\mathcal{C}^{(K)}) - f(\mathcal{C}^{(K-1)}) ]  + \\
 &\mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K-1)}} [f(\mathcal{C}^{(K-1)}) - f(\mathcal{C}^{(K-2)}) + \ldots \\
&+ f(\mathcal{C}^{(1)}) - f(\mathcal{C}^{(0)}) + f(\mathcal{C}^{(0)})]
\end{split}
\end{equation}
Due to the decomposition demonstrated in Equation~\ref{equ:thm2_2}, the original objective can be iteratively simplified as:
\begin{equation}
\label{equ:thm2_3}
\begin{split}
     \min_{\pi_{\mathrm{Gpart}}, \pi_{\mathrm{Lpart}}} &\mathbb{E}_{\mathcal{C}^{(0)}} [f(\mathcal{G}^{(0)})] +  \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}}[f(\mathcal{C}^{(1)}) - f(\mathcal{C}^{(0)})]+ \\
    &  \cdots + \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [f(\mathcal{C}^{(K)}) - f(\mathcal{C}^{(K-1)})].
\end{split}
\end{equation}
Then for any $k \geq 1$, the evaluation for $\mathcal{C}^{(k)}$ can be written as:
\begin{equation}
\label{equ:thm2_4}
\begin{split}
    &f(\mathcal{C}^{(k)}) - f(\mathcal{C}^{(k-1)}) \\
    = &\sum_{i=1}^{N_{c}} g(c_{i}^{(k)}) - \sum_{i=1}^{N_{c}} g(c_{i}^{(k-1)}) \\
    = &\sum_{j=1}^{\lfloor \frac{N_{c}}{2} \rfloor} [g(c_{(k+m-1)\%N_{c}+1}^{(k)}) + g(c_{(k+m)\%N_{c}+1}^{(k)})] \\
    - &\sum_{j=1}^{\lfloor \frac{N_{c}}{2} \rfloor}[g(c_{(k+m-1)\%N_{c}+1}^{(k-1)}) + g(c_{(k+m)\%N_{c}+1}^{(k-1)}) ],
\end{split}
\end{equation}
where $m=2(j-1)$.
\end{proof}


\subsection{Proof of Theorem 3}
\begin{theorem}
\label{thm:sl_obj}
Given $f(\mathcal{C})=-\mathds{1}(\mathcal{C} = \bar{\mathcal{C}})$, the optimization objective in the HLGP framework for a problem instance $I$ is to identify $\pi_{\theta_{G}}$ and $\pi_{\theta_{L}}$ so as to minimize:
\begin{equation}
    L(\theta_{G}, \theta_{L}, \bar{\mathcal{C}}) = 
    - \log\pi_{\theta_{G}}(\bar{\mathcal{C}}|I) - \sum_{i=1}^{N_{c}} \log\pi_{\theta_{L}}(\bar{\mathcal{C}}_{i}|I_{i}),
\end{equation}
where $I_{i} = (G_{i}, D, 2)$ denotes the subproblem, with $G_{i} = \bar{c}_{i} \cup \bar{c}_{i\%N_{c}+1}$, and $\bar{\mathcal{C}}_{i} = \{\bar{c}_{i}, \bar{c}_{i\%N_{c}+1} \}$ represents the corresponding label.
\end{theorem}

\begin{proof}
Given that $f(\mathcal{C})=-\mathds{1}(\mathcal{C} = \bar{\mathcal{C}})$ serves as the feasible cost function, the original objective function can be written as:
\begin{equation}
\label{equ:thm3_1}
\begin{split}
& \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [\mathds{1}(\mathcal{C}^{(K)} = \bar{\mathcal{C}})] \\
= & \frac{1}{K+1} \mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [\mathds{1}(\mathcal{C}^{(K)} = \bar{\mathcal{C}}) + \ldots + \mathds{1}(\mathcal{C}^{(0)} = \bar{\mathcal{C}})]
\end{split}
\end{equation}
Subsequently, we can apply the same recursive decomposition as demonstrated in Equation~\ref{equ:thm2_2} and Equation~\ref{equ:thm2_3} to Equation~\ref{equ:thm3_1} as follows:
\begin{equation}
\label{equ:thm3_2}
\begin{split}
&\mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [\mathds{1}(\mathcal{C}^{(K)} = \bar{\mathcal{C}}) + \ldots + \mathds{1}(\mathcal{C}^{(0)} = \bar{\mathcal{C}})] \\ =
&\mathbb{E}_{\mathcal{C}^{(0)}} \mathbb{E}_{\mathcal{C}^{(1)}} \cdots \mathbb{E}_{\mathcal{C}^{(K)}} [\mathds{1}(\mathcal{C}^{(K)} = \bar{\mathcal{C}})]  + \dots + \mathbb{E}_{\mathcal{C}^{(0)}} [\mathds{1}(\mathcal{C}^{(0)} = \bar{\mathcal{C}})]
\end{split}
\end{equation}
Each term on the RHS of Equation~\ref{equ:thm3_2} corresponds to a distinct objective. The global partition policy at level $k=0$ is designed to maximize the probability of the solution $\bar{\mathcal{C}}$ for a specific problem instance $I$. On the other hand, the local partition policy at level $k>0$ aims to maximize the probability of the solution $\bar{\mathcal{C}}_{i}$ for a given subproblem $I_{i}$ as defined earlier. In practical scenarios, maximizing the log-probability objective can be utilized without affecting the theoretical optimal policies.

\end{proof}


\subsection{Proof of Proposition 1}

\begin{proposition}
\label{prop:mlmdp}
In the multi-level MDP framework, at the global partition level, for $t\geq1$, the state $x_{t}^{(0)}\in\mathbb{X}^{(0)}$ comprises problem instance $I$ and the partial partition solution $\mathcal{C}^{(0)}[0:t-1]$ ($\mathcal{C}^{(0)}[0] = \emptyset$). The initial distribution $\mu^{(0)}$ aligns with the problem instance distribution $p_{I}$. The action $u_{t}^{(0)}\in\mathbb{U}^{(0)}$ involves selecting a node denoted as $\mathcal{C}^{(0)}[t]$, from unvisited customer nodes and the depot node. Let $i_{t}$ index subgraphs such that at timestep $t$, the agent is constructing $i_{t}$-th subgraph $c_{i_{t}}^{(0)}$. If the subgraph $c_{i_{t}}^{(0)}$ is created, then the reward $r_{t}^{(0)}$ is set as $-g(c_{i_{t}}^{(0)})$; otherwise, it remains at $0$. The global partition policy, parameterized by $\theta_{G}$, is thus specified as $\pi_{\theta_{G}}(u_{t}^{(0)}|x_{t}^{(0)})$. 

At each local partition level $k\geq1$, the local partition policy is tasked with solving the sequence of subproblems obtained from $\mathcal{C}^{(k-1)}$. In this context, we use $j_{t}$ as an index for subproblems, indicating that the $j_{t}$-th subproblem denoted as $I_{j_{t}}^{(k-1)}$, is currently being addressed but remains incomplete at timestep $t$. The state $x_{t}^{(k)}\in\mathbb{X}^{(k)}$ consists of the subproblem sequence and the partial solution of $I_{j_{t}}^{(k-1)}$. The initial state distribution $\mu^{(k)}$ corresponds to the distribution of the subproblem sequence. The action $u_{t}^{(k)}\in\mathbb{U}^{(k)}$ involves selecting a node for solving $I_{j_{t}}^{(k-1)}$. When $I_{j_{t}}^{(k-1)}$ is successfully solved, the index $j_{t}$ will proceed to the next subproblem, and the reward $r_{t}^{(k)}$ is set as $-(h(\mathcal{C}^{(k)}, k, m) - h(\mathcal{C}^{(k-1)}, k, m))$ (where $m=2(j_{t}-1)$). Otherwise, the reward remains at $0$. Thus, the local partition policy parameterized by $\theta_{L}$, is defined as $\pi_{\theta_{L}}(u_{t}^{(k)}|x_{t}^{(k)})$. The objective is to maximize the sum of expected returns across levels, as defined below:
\begin{equation}
\label{equ:rl_obj}
    J(\theta_{G}, \theta_{L}) = \mathbb{E}_{\omega^{(0)}} [\sum_{t=1}^{T^{(0)}}r_{t}^{(0)}] + \cdots+
\mathbb{E}_{\omega^{(0)}}\cdots\mathbb{E}_{\omega^{(K)}} [\sum_{t=1}^{T^{(K)}}r_{t}^{(K)}],
\end{equation}
where $T^{(k)}$ and $\omega^{(k)}$ denote the horizon and the trajectory at level $k$.
\end{proposition}

\begin{proof}
We aim to prove that, at any partition level $k \geq 0$, the definitions of state and action, as presented in Proposition~\ref{prop:mlmdp}, are essential for preserving the Markovian property of the corresponding MDP. At the global partition level, the partial partition solution evolves from $\mathcal{C}^{0}[0:t-1]$ to $\mathcal{C}^{0}[0:t]$ by selecting an unvisited node $\mathcal{C}^{0}[t]$. It is evident that the dynamics remains Markovian at this level. At the local partition level $k \geq 1$, the state consists of the subproblem sequence and the partial solution of the subproblem $I_{j_{t}}^{k-1}$ addressed at time step $t$. When solving $I_{j_{t}}^{k-1}$, executing the action $u_{t}^{k}$ augments the partial solution of $I_{j_{t}}^{k-1}$ with an unvisited node for the subsequent partial solution of $I_{j_{t}}^{k-1}$. Apart from $I_{j_{t}}^{k-1}$, the remaining subproblems remain unaltered. Upon the completion of $I_{j_{t}}^{k-1}$, the partial solution of $I_{j_{t+1}}^{k-1}$ begins to be addressed. Consequently, the dynamics of each local partition level retain their Markovian nature.
\end{proof}

\section{Related Works}

\textbf{Constructive Methods.} The learning-based constructive method aims to develop an efficient and (near-)optimal end-to-end neural solver for combinatorial optimization problems (COPs). Among them, the PointerNet~\cite{vinyals2015pointer} and the Transformer~\cite{kool2018attention} are two commonly employed architectures trained using either SL~\cite{vinyals2015pointer} or RL~\cite{nazari2018reinforcement, kool2018attention}, to progressively infer complete solutions with the aid of the autoregressive mechanism. Additionally, some methods consider inherent properties in VRPs, such as multiple optima~\cite{kwon2020pomo} and symmetry~\cite{kim2022sym}, to enhance the quality of solutions. However, the delayed rewards in the training of RL policies lead to the high GPU memory demands for gradient backpropagation. Thus, SL-driven policies, such as BQ~\cite{drakulic2024bq}, LEHD~\cite{luo2024neural} and SIL~\cite{luo2024self}, have resurfaced to alleviate training difficulties and moderately improve generalization. Moreover, some approaches employ techniques such as meta-learning~\cite{son2023meta, zhou2023towards, manchanda2022generalization, qiu2022dimes}, knowledge distillation~\cite{bi2022learning}, or ensemble learning~\cite{gao2023towards, jiang2024ensemble, grinsztajn2023winner} to enhance the generalization of neural solvers. However, these constructive methods might still experience performance deterioration when encountering substantial distribution or scale shifts.

\noindent \textbf{Iterative Methods.} In comparison to constructive methods, iterative methods offer the benefit of consistently improving a given solution until convergence. Both L2I~\cite{lu2019learning} and NeuRewriter~\cite{chen2019learning} utilize RL policies to choose from local improvement operators to refine the given initial solutions. Likewise,~\citet{hottung2020neural} interleaves the use of heuristic destroy operators and a set of learning-based repair policies to generate a new solution. Moreover, DACT~\cite{ma2021learning} focuses on the expressive representation of solution encodings provided to the RL policy. Additionally, both NeuralLKH~\cite{xin2021neurolkh} and Neural k-Opt~\cite{ma2024learning} utilize the RL policy to substitute the heuristic rule for edge exchanges in k-opt algorithms. However, these iterative methods trade efficiency for improved performance and still rely heavily on well-crafted rules.

\noindent \textbf{Divide-and-Conquer Methods.} The divide-and-conquer paradigm can leverage local topological features that are insensitive to distribution or scale shifts to mitigate performance deterioration. ~\citet{fu2021generalize}, ~\citet{kim2021learning} and~\citet{cheng2023select} attempt to transfer a standard neural solver for larger instances by sampling multiple small-scale subgraphs using heuristic rules. In contrast, both L2D~\cite{li2021learning} and RGB~\cite{zong2022rbg} learn a policy to choose among heuristically constructed subgraphs for iterative enhancement. However, the used heuristic rules may lead to solutions being trapped in local optima. Unlike the above methods, TAM~\cite{hou2023generalize}, GLOP~\cite{ye2024glop} and UDC~\cite{zheng2024udc} opt to use a learning-based policy to globally partition the entire instance into subproblems that are solved by the pretrained local construction policy. In addition,~\citet{pan2023h} resort to a hierarchical RL model where a local policy solves subproblems assigned by a jointly trained global policy. However, these neural partition policies may suffer performance degradation due to compounded errors during the partition process. 


\section{Toy Examples}
The toy examples for the overall HLGP framework, the RL-driven HLGP training framework and the SL-driven HLGP training framework
are illustrated in Figure~\ref{fig:hlgp_toy_example}, Figure~\ref{fig:rl_hlgp_toy_example}, and Figure~\ref{fig:sl_hlgp_toy_example}, respectively.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/toy_examples/toy_example_1.pdf}
    \caption{The toy example of the overall HLGP framework.
    }
    \label{fig:hlgp_toy_example}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/toy_examples/toy_example_2.pdf}
    \caption{The toy example of the RL-driven HLGP training framework.
    }
    \label{fig:rl_hlgp_toy_example}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/toy_examples/toy_example_3.pdf}
    \caption{The toy example of the SL-driven HLGP training framework.
    }
    \label{fig:sl_hlgp_toy_example}
\end{figure*}


\newpage


\begin{figure*}[t]
\centering
\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Uniform}}
\subfigure[\shortstack{Cost=59.37 \\ glob.}]{\label{fig:u_rl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset0_model0_instance70_cost59.368.pdf}}
\subfigure[\shortstack{Cost=53.99 \\ glob.+loc.}]{\label{fig:u_rl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset0_model1_instance70_cost53.992.pdf}}
\subfigure[\shortstack{Cost=56.49 \\ glob.+subp.}]
{\label{fig:u_rl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset0_model2_instance70_cost56.488.pdf}}
\subfigure[\shortstack{Cost=52.73 \\ glob.+loc.+subp.}]
{\label{fig:u_rl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset0_model3_instance70_cost52.733.pdf}}


\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Gaussian}}
\subfigure[\shortstack{Cost=51.61 \\ glob.}]{\label{fig:g_rl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset1_model0_instance47_cost51.610.pdf}}
\subfigure[\shortstack{Cost=43.59 \\ glob.+loc.}]{\label{fig:g_rl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset1_model1_instance47_cost43.593.pdf}}
\subfigure[\shortstack{Cost=49.08 \\ glob.+subp.}]
{\label{fig:g_rl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset1_model2_instance47_cost49.083.pdf}}
\subfigure[\shortstack{Cost=42.18 \\ glob.+loc.+subp.}]
{\label{fig:g_rl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset1_model3_instance47_cost42.183.pdf}}


\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Explosion}}
\subfigure[\shortstack{Cost=46.55 \\ glob.}]{\label{fig:e_rl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset2_model0_instance85_cost46.553.pdf}}
\subfigure[\shortstack{Cost=40.90 \\ glob.+loc.}]{\label{fig:e_rl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset2_model1_instance85_cost40.901.pdf}}
\subfigure[\shortstack{Cost=43.75 \\ glob.+subp.}]
{\label{fig:e_rl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset2_model2_instance85_cost43.750.pdf}}
\subfigure[\shortstack{Cost=39.73 \\ glob.+loc.+subp.}]
{\label{fig:e_rl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset2_model3_instance85_cost39.733.pdf}}

\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Rotation}}
\subfigure[\shortstack{Cost=51.28 \\ glob.}]{\label{fig:r_rl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset3_model0_instance85_cost51.276.pdf}}
\subfigure[\shortstack{Cost=42.27 \\ glob.+loc.}]{\label{fig:r_rl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset3_model1_instance85_cost42.271.pdf}}
\subfigure[\shortstack{Cost=48.65 \\ glob.+subp.}]
{\label{fig:r_rl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset3_model2_instance85_cost48.648.pdf}}
\subfigure[\shortstack{Cost=40.91 \\ glob.+loc.+subp.}]
{\label{fig:r_rl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/rl/dataset3_model3_instance85_cost40.910.pdf}}

\caption{visualization of RL-driven HLGP routes.}
%\vspace{-10pt}}
\label{fig:rl_visual}
\end{figure*}



\begin{figure*}[t]
\centering
\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Uniform}}
\subfigure[\shortstack{Cost=49.70 \\ SS.+glob.}]{\label{fig:u_sl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset0_model0_instance99_cost49.695.pdf}}
\subfigure[\shortstack{Cost=48.70 \\ SS.+glob.+loc.}]{\label{fig:u_sl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset0_model1_instance99_cost48.696.pdf}}
\subfigure[\shortstack{Cost=47.99 \\ SS.+LS.+glob.}]
{\label{fig:u_sl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset0_model2_instance99_cost47.986.pdf}}
\subfigure[\shortstack{Cost=47.79 \\ SS.+LS.+glob.+loc.}]
{\label{fig:u_sl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset0_model3_instance99_cost47.786.pdf}}




\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Gaussian}}
\subfigure[\shortstack{Cost=33.90 \\ SS.+glob.}]{\label{fig:g_sl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset1_model0_instance56_cost33.899.pdf}}
\subfigure[\shortstack{Cost=33.09 \\ SS.+glob.+loc.}]{\label{fig:g_sl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset1_model1_instance56_cost33.087.pdf}}
\subfigure[\shortstack{Cost=32.25 \\ SS.+LS.+glob.}]
{\label{fig:g_sl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset1_model2_instance56_cost32.252.pdf}}
\subfigure[\shortstack{Cost=31.85 \\ SS.+LS.+glob.+loc.}]
{\label{fig:g_sl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset1_model3_instance56_cost31.851.pdf}}




\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Explosion}}
\subfigure[\shortstack{Cost=58.14 \\ SS.+glob.}]{\label{fig:e_sl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset2_model0_instance35_cost58.141.pdf}}
\subfigure[\shortstack{Cost=56.97 \\ SS.+glob.+loc.}]{\label{fig:e_sl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset2_model1_instance35_cost56.971.pdf}}
\subfigure[\shortstack{Cost=56.20 \\ SS.+LS.+glob.}]
{\label{fig:e_sl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset2_model2_instance35_cost56.195.pdf}}
\subfigure[\shortstack{Cost=56.01 \\ SS.+LS.+glob.+loc.}]
{\label{fig:e_sl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset2_model3_instance35_cost56.014.pdf}}



\rotatebox{90}{~~~~~~~~~~~~\scriptsize{CVRP1000+Rotation}}
\subfigure[\shortstack{Cost=47.27 \\ SS.+glob.}]{\label{fig:r_sl_1}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset3_model0_instance76_cost47.269.pdf}}
\subfigure[\shortstack{Cost=46.56 \\ SS.+glob.+loc.}]{\label{fig:r_sl_2}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset3_model1_instance76_cost46.555.pdf}}
\subfigure[\shortstack{Cost=45.49 \\ SS.+LS.+glob.}]
{\label{fig:r_sl_3}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset3_model2_instance76_cost45.489.pdf}}
\subfigure[\shortstack{Cost=44.86 \\ SS.+LS.+glob.+loc.}]
{\label{fig:r_sl_4}\includegraphics[width=0.23\textwidth]{figures/visualizations/sl/dataset3_model3_instance76_cost44.861.pdf}}


\caption{visualization of SL-driven HLGP routes.}
%\vspace{-10pt}}
\label{fig:sl_visual}
\end{figure*}