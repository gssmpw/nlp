\section{Related Works}
\textbf{Constructive Methods.} The learning-based constructive method aims to develop an efficient and (near-)optimal end-to-end neural solver for combinatorial optimization problems (COPs). Among them, the PointerNet~\cite{vinyals2015pointer} and the Transformer~\cite{kool2018attention} are two commonly employed architectures trained using either SL~\cite{vinyals2015pointer} or RL~\cite{nazari2018reinforcement, kool2018attention}, to progressively infer complete solutions with the aid of the autoregressive mechanism. Additionally, some methods consider inherent properties in VRPs, such as multiple optima~\cite{kwon2020pomo} and symmetry~\cite{kim2022sym}, to enhance the quality of solutions. However, the delayed rewards in the training of RL policies lead to the high GPU memory demands for gradient backpropagation. Thus, SL-driven policies, such as BQ~\cite{drakulic2024bq}, LEHD~\cite{luo2024neural} and SIL~\cite{luo2024self}, have resurfaced to alleviate training difficulties and moderately improve generalization. Moreover, some approaches employ techniques such as meta-learning~\cite{son2023meta, zhou2023towards, manchanda2022generalization, qiu2022dimes}, knowledge distillation~\cite{bi2022learning}, or ensemble learning~\cite{gao2023towards, jiang2024ensemble, grinsztajn2023winner} to enhance the generalization of neural solvers. However, these constructive methods might still experience performance deterioration when encountering substantial distribution or scale shifts.

\noindent \textbf{Iterative Methods.} In comparison to constructive methods, iterative methods offer the benefit of consistently improving a given solution until convergence. Both L2I~\cite{lu2019learning} and NeuRewriter~\cite{chen2019learning} utilize RL policies to choose from local improvement operators to refine the given initial solutions. Likewise,~\citet{hottung2020neural} interleaves the use of heuristic destroy operators and a set of learning-based repair policies to generate a new solution. Moreover, DACT~\cite{ma2021learning} focuses on the expressive representation of solution encodings provided to the RL policy. Additionally, both NeuralLKH~\cite{xin2021neurolkh} and Neural k-Opt~\cite{ma2024learning} utilize the RL policy to substitute the heuristic rule for edge exchanges in k-opt algorithms. However, these iterative methods trade efficiency for improved performance and still rely heavily on well-crafted rules.

\noindent \textbf{Divide-and-Conquer Methods.} The divide-and-conquer paradigm can leverage local topological features that are insensitive to distribution or scale shifts to mitigate performance deterioration. ~\citet{fu2021generalize}, ~\citet{kim2021learning} and~\citet{cheng2023select} attempt to transfer a standard neural solver for larger instances by sampling multiple small-scale subgraphs using heuristic rules. In contrast, both L2D~\cite{li2021learning} and RGB~\cite{zong2022rbg} learn a policy to choose among heuristically constructed subgraphs for iterative enhancement. However, the used heuristic rules may lead to solutions being trapped in local optima. Unlike the above methods, TAM~\cite{hou2023generalize}, GLOP~\cite{ye2024glop} and UDC~\cite{zheng2024udc} opt to use a learning-based policy to globally partition the entire instance into subproblems that are solved by the pretrained local construction policy. In addition,~\citet{pan2023h} resort to a hierarchical RL model where a local policy solves subproblems assigned by a jointly trained global policy. However, these neural partition policies may suffer performance degradation due to compounded errors during the partition process.