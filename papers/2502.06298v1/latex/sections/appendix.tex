\appendix
\onecolumn
\section{Benchmark Details}\label{sec:appendix_benchmark}

\subsection{SeaExam} \label{app:seaexam}
Following the construction of M3Exam dataset~\cite{zhang_m3exam_2023}, we engage native speakers from the SEA region to collect official exam papers, along with their corresponding answers, typically taken at the end of each educational levelâ€”primary school, middle school, and high school graduation exams. 

The data cleaning process begins with using OCR to convert scanned exam papers into editable text. Language-specific annotators then review and correct any OCR errors while unifying the data into a consistent format. Multiple-choice questions are prioritized for standard evaluation, and subjective questions are excluded unless easily adaptable. Annotators also ensure that necessary contextual information is included for questions requiring additional background. Special formats, like equations, are converted into LaTeX, and multiple rounds of quality checks ensure the final dataset closely mirrors real exam conditions.

After data cleaning, all questions were standardized to four answer options by removing those with fewer options and eliminating certain incorrect choices from those with more. The final SeaExam comprises a total of 5,451 test samples and the statistics of the SeaExam is shown in Table~\ref{tab:stats_m3exam}, following the original classification framework of M3Exam. We also map the subjects to MMLU categories, with the mapping shown in Table~\ref{tab:subject_category}. 


\input{latex/tables/stats_m3exam}

\input{latex/tables/subject_categorization}

\subsection{SeaBench}
Table \ref{tab:stats_seabench} shows the Distribution of subject categories by language for SeaBench and Table \ref{tab:priority_aspects} the categories and their corresponding priority aspects in SeaBench.

\input{latex/tables/stats_seabench}

\input{latex/tables/seabench_priority}


\subsection{Translated Benchmarks}
We compare SeaExam and SeaBench with the translated MMLU and the translated MT-bench. For an effective comparison with the two datasets, we process the datasets using the following procedures:
\paragraph{MMLU} We randomly select 50 questions from each subject, totaling 2850 questions. Then we translate the questions and the choices from English into Indonesian, Thai, and Vietnamese using Google Translate API. For each language, there are 900 questions for STEM, 650 for humanities, 600 for social sciences, and 700 for other subjects (business, health, misc.). We call the curated benchmark MMLU-SEA. 

\paragraph{MT-bench} We translated MT-bench into Indonesian, Thai, and Vietnamese using the Google Translate API. Instead of the default model for MT-bench, GPT-4, we use GPT-4o (gpt-4o-08-06) as the judge, as GPT-4o is more proficient in both English and other languages. In addition, we utilize GPT-4o to generate reference answers for reasoning, math, and coding questions. We refer to the translated version of MT-bench as MT-bench-SEA. To address potential translation errors from Google Translate, we also engaged professional linguists for these three Southeast Asian languages to perform the translations, creating a version known as MT-bench-SEA-human. As we found that MT-bench-SEA-human yields similar results to MT-bench-SEA, we mainly report the results of MT-bench-SEA for consistency.

\subsection{Comparison of Dataset Distribution}

Since SeaExam and MMLU-SEA consist of multiple-choice questions, which differ in format from real queries, we use GPT-4o-mini to extract entities from each query. The specific prompt used for entity extraction is detailed in Figure \ref{fig:prompt_entity_extract} in the appendix. After that, we bge-multilingual-gemma2 model to embed each entity. For SeaBench and MT-bench-SEA queries, we embed the entire query. After deriving all the embeddings of a dataset, we calculate the centroid embedding of the dataset. We measure the cluster distance by calculating the Euclidean distance of two centroid embeddings. The distributions of the datasets are shown in Figure~\ref{fig:data_distribution}.


\section{Experiment Details} \label{app:exp_details}

\subsection{Evaluation Setup} \label{app:eval_setup}
We evaluate on SeaExam with 3-shot setting in the completion mode. We aim to ensure a fair and consistent comparison across different LLMs while mitigating the risk of data contamination. We have designed four instruction templates to provide a fair comparison and reduce LLMs' dependence on specific prompt templates. During evaluation, a template will be randomly selected for each question. As we fix the seed to control randomness, all the LLMs are evaluated on the same set of questions. Additionally, users have the option to change the seed value to generate a different set of questions for evaluation purposes.

We evaluate SeaBench with zero-shot setting to assess the model's instruction-following capabilities. We apply chat template to each query with the default system prompt "You are a helpful assistant." If the model does not support the system prompt, we leave it empty. We run all the evaluations on Nvdia A100 GPUs.

\subsection{Additional Results}

\input{latex/tables/result_SeaExam}

\input{latex/tables/result_SeaBench}



\begin{figure*}[htb]
    \centering
    % First subfigure with 3 grouped figures
    \begin{subfigure}[b]{\textwidth}
        \centering
         \includegraphics[width=0.325\linewidth]{latex/figs/data_distribution_v2/SeaExam_entities/t-SNE_indonesian_500_wild.pdf}
        \includegraphics[width=0.325\linewidth]{latex/figs/data_distribution_v2/SeaExam_entities/t-SNE_thai_500_wild.pdf}
        \includegraphics[width=0.325\linewidth]{latex/figs/data_distribution_v2/SeaExam_entities/t-SNE_vietnamese_500_wild.pdf}
        \caption{}
        \label{fig:subfigure1}
    \end{subfigure}
    \hfill
    % Second subfigure with 3 grouped figures
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.325\linewidth]{latex/figs/data_distribution_v2/SeaBench/t-SNE_indonesian_200_wild.pdf}
        \includegraphics[width=0.325\linewidth]{latex/figs/data_distribution_v2/SeaBench/t-SNE_thai_200_wild.pdf}
        \includegraphics[width=0.325\linewidth]{latex/figs/data_distribution_v2/SeaBench/t-SNE_vietnamese_200_wild.pdf}
        \caption{}
        \label{fig:subfigure2}
    \end{subfigure}
    
    \caption{(a) Entity embedding distribution for Wild Queries, SeaExam, and MMLU-SEA, with each benchmark sampled up to 500 data points. (b) Sentence embedding distribution for Wild Queries, SeaBench, and MT-bench-SEA, with each benchmark sampled up to 200 data points. Wild Queries are represented by \textcolor{orange}{orange} dots, and other benchmarks by \textcolor{blue}{blue} dots. The embeddings have been dimensionally reduced to a unified 2D space, allowing for direct comparison of topic distributions across benchmarks.}
    \label{fig:data_distribution}
\end{figure*}




\begin{table}[htb]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{}           & \textbf{id} & \textbf{th} & \textbf{vi} & \textbf{total}\\ 
\midrule
\textbf{Queries}    & 1,954                & 517           & 2,184       &  4,658         \\ 
\bottomrule
\end{tabular}
\caption{Number of queries for each language in Wild Queries.}
\label{tab:stats_wild_queries}
\end{table}
% \section{Prompt}
\section{Human Evaluation}
\subsection{SeaBench Evaluation}
The prompt templates for reference-guided single-answer grading for SeaBench are shown in Figure~\ref{fig:prompt_template_turn1} and~\ref{fig:prompt_template_turn2}. To compare the entity distributions between SeaExam, MMLU-SEA, and Wild Queries, we employ the prompt in Figure \ref{fig:prompt_entity_extract} to extract the entities from each query. 


\subsection{Agreement Evaluations} \label{app:agreement}
To verify the reliability of LLMs as multilingual judges, we calculate their agreement rate with human judges by engaging three professional linguists to compare response pairs. These linguists are native speakers of the three SEA languages involved, making them more skilled than the average crowd workers. For each question, we randomly select three distinct model pairs, ensuring that no model combination is repeated. Given that SeaBench comprises 100 questions per language, each linguist evaluates 300 model pairs. Considering the two-turn structure of each question, this approach results in 600 votes per language for analysis. During the annotation process, the linguists are unaware of which two models generated each response pair. The annotation instructions for the human judges are provided in Figure \ref{fig:human_instruction} in the Appendix. To ensure a more balanced set of labels, we treat responses as ties when their scores differ by 1 point or less, given that the model scores range from 1 to 10. Additionally, we calculate the average scores of the six judges to form the ensemble setting.


\begin{figure*}[!ht]
    \centering
    \begin{subfigure}[b]{0.38\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/figs/corr_judges/corr_id_avg.pdf}
        \caption{Indonesian}
        \label{fig:subfig1}
    \end{subfigure}
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/figs/corr_judges/corr_th_avg.pdf}
        \caption{Thai}
        \label{fig:subfig2}
    \end{subfigure}
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/figs/corr_judges/corr_vi_avg.pdf}
        \caption{Vietnamese}
        \label{fig:subfig3}
    \end{subfigure}


    \caption{The ranking correlation for SeaBench between six judges for each language.}
    \label{fig:corr_SeaBench_judge.}
\end{figure*}

For human evaluation, we report the number of counts to calculate the agreement rates when a tie is recorded if two scores differ by 1 or less, as shown in Table \ref{tab:seabench_agreement_count_1}. The agreement rates and the number of counts when a tie is recorded if two responses receive equal scores are shown in Table \ref{tab:seabench_agreement} and Table \ref{tab:seabench_agreement_count_0}. The instructions for human judges to compare the model performance are shown in Figure \ref{fig:human_instruction}.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{latex/figs/prompts/prompt_judge_turn1.pdf}
    \caption{The prompt for reference-guided single-turn single-answer grading.}
    \label{fig:prompt_template_turn1}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.98\linewidth]{latex/figs/prompts/prompt_judge_turn2.pdf}
    \caption{The prompt for reference-guided multi-turn single-answer grading.}
    \label{fig:prompt_template_turn2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.5\linewidth]{latex/figs/prompts/prompt_entity_extract.pdf}
    \caption{The prompt to extract entities from a query  .}
    \label{fig:prompt_entity_extract}
\end{figure*}

\begin{figure*}[htb]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.98\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/figs/prompts/human_instruction.pdf}
        \caption{}
        \label{subfig:}
    \end{subfigure}
    % \hfill
    % Subfigure 2
    \begin{subfigure}[b]{0.98\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/figs/prompts/human_instruction_turn2.pdf}
        \caption{}
        \label{subfig:}
    \end{subfigure}
    \caption{Instructions for humans to compare the model performance in (a) turn 1, and (b) turn 2.}
    \label{fig:human_instruction}
\end{figure*}

\input{latex/tables/agreement/seabench_agreement_valid_count_1}
\input{latex/tables/agreement/seabench_agreement_thres_0}
\input{latex/tables/agreement/seabench_agreement_valid_count_0}






