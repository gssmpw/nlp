\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{latex/figs/figure1_example_jiahao.pdf}
    \caption{ 
    Compared with local usage queries in Vietnamese, questions in English-based translations show more American context (Hawaii). To better illustrate this discrepancy, we extracted the object in these questions and visualised their distribution. The results show that the objects in translated questions cover only a small portion of those in local usage queries.
    }
    \label{fig: intro}  
\end{figure}


Large Language Models (LLMs) have shown remarkable performance across various English benchmarks, including both human exam datasets such as MMLU~\cite{hendrycks_measuring_2021}, or instruction-following datasets such as MT-Bench~\cite{zheng_judging_2023}, indicating their strong capabilities \cite{openai_gpt-4_2023,dubey_llama_2024,gemma_team_gemma_2024}. 
As these LLMs are increasingly deployed globally, there is growing interest in their ability to handle multiple languages and adapt to a wide range of multilingual applications \cite{huang_not_2023,qin_multilingual_2024,huang_survey_2024,dou_sailor_2024,nguyen_seallms_2023,zhang_seallms_2024}. 

This led to the development of multiple multilingual benchmarks to assess the multilingual capabilities of LLMs \cite{lai_chatgpt_2023,ahuja_mega_2023,zhang_m3exam_2023}. 
Among them, many datasets such as MGSM~\cite{shi_language_2022}, XNLI~\cite{conneau_xnli_2018}, and Multilingual MMLU~\cite{hendrycks_measuring_2021,openai_gpt-4_2023} are typically constructed by translating the English set into target languages.
Considering that original English test sets are often carefully designed, such translations provide an effective way to leverage the task categorization, evaluation targets, and construction methods of the monolingual dataset into the multilingual context.


However, such translated questions focus merely on evaluating the same contextual elements as their monolingual counterparts.
In other words, they focus primarily on the application scenarios relevant to the original benchmarks rather than adapting to a wide range of multilingual applications in the real world.
Instead, a truly effective multilingual benchmark must also consider the content typically used in the practical application of the target language \cite{liu_is_2024}.
For example, as shown in Figure~\ref{fig: intro}, we visualize the distribution of objects in questions collected from local usage queries versus those translated from English.
Compared to local usage queries, translated questions based on English exhibit more of an American context, e.g., involving the place ``Hawaii''.  It shows that translated questions cover only a small portion of the entities in local usage queries, indicating a significant divergence in the query context.


Considering the scarcity of such effective multilingual benchmarks, this paper introduces two new benchmarks, SeaExam and SeaBench.
These benchmarks are specifically designed to address the unique application scenarios and cultural contexts of Southeast Asian (SEA) countries, which often differ significantly from western-centric datasets.
Following the design principles of two widely used English-based datasets, MMLU and MT-bench, we do not simply translate the original English questions but incorporate real-world usage scenarios from SEA natives into the content --- allowing us to measure a model's adaptability in multilingual application scenarios.
Specifically, SeaExam is a multitask exam dataset sourced from real exams in SEA countries that cover a wide range of subjects including local history, geography, and literature. 
SeaBench, following MT-Bench's approach, focuses on multi-turn instruction-following tasks spanning ten task categories. It incorporates scenarios and instructions that are commonly encountered in SEA cultures and daily life.


Our experimental analysis quantitatively demonstrates that, \textbf{1)} Compared to the translated benchmarks MMLU and MT-bench, our SeaExam and SeaBench benchmarks include questions that are more aligned with the daily usage of regional languages (Section~\ref{subsection is_more_aligned}). \textbf{2)} Furthermore, using SeaExam and SeaBench, we are able to more effectively discern the capabilities of models in real-world multilingual applications (Section~\ref{finding1}). Further analysis reveals that \textbf{3)} While multiple-choice questions in exam datasets can objectively measure model capabilities, open-ended questions are more effective in highlighting differences in model performance across various languages (Section~\ref{finding2} and Section~\ref{finding3}). Additionally, we find that \textbf{4)} The nine models involved generally perform poorly in the ``safety'' category --- evaluating whether the models generate harmful responses in the local context (Section~\ref{finding4}). Therefore, we advocate for enhanced safety measures in multilingual applications to adapt to a broader range of scenarios.

The key contribution can be summarized as:
\begin{itemize}
   \item We introduce two new benchmarks, SeaExam and SeaBench, which extend the scope of the translated MMLU and MT-bench frameworks to better accommodate the unique linguistic features and practical content contexts of the Southeast Asian (SEA) region.
    \item We compare these benchmarks with translated counterparts, such as MMLU and MT-Bench, and find that SeaExam and SeaBench have closer distribution to real-world queries. Utilizing these benchmarks allows for a better differentiation of model performance across different language uses. 
\end{itemize}




