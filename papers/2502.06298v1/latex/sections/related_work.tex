\section{Related Work} 

\paragraph{SEA Benchmarks.}
Several benchmarks have been developed to evaluate LLMs on SEA languages. SeaEval \cite{wang_seaeval_2023} includes 28 datasets covering classic NLP tasks, reasoning, and cultural comprehension. For the newly created datasets, Cross-MMLU and Cross-LogiQA, the questions were translated from English using Google Translate and proofread by native speakers. SeaCrowd benchmarks \cite{lovenia_seacrowd_2024} cover 4 NLU tasks with 131 data subsets and 7 NLG tasks with 100 subsets. BHASA \cite{leong_bhasa_nodate} offers a holistic evaluation suite for assessing linguistic and cultural aspects in LLMs tailored to SEA languages. These benchmarks aim to provide a comprehensive evaluation for SEA languages, with a focus on NLP tasks. However, none of the existing benchmarks evaluate open-ended questions or multi-turn conversations. In contrast, SeaExam focuses on real-world exam questions, and SeaBench offers the first SEA benchmark designed specifically for open-ended and multi-turn evaluations.

\paragraph{LLM-as-a-Judge}
Strong LLMs have emerged as judges to evaluate model capabilities on open-ended questions.
\citet{zheng_judging_2023} proposed MT-bench, with GPT-4 as the judge to test multi-turn conversation and instruction-following ability. \citet{alpaca_eval} introduced AlpacaEval, a method for assessing a model’s performance by determining the percentage of instances in which a powerful LLM favors the model’s outputs compared to those from a reference model. Building on this, \citet{dubois2024length} proposed length-controlled AlpacaEval to mitigate length gameability, as judge LLMs prefer longer outputs. To effectively distinguish model capabilities and capture human preferences in practical scenarios, \citet{li_crowdsourced_2024} developed Arena-Hard, a data pipeline designed to create high-quality benchmarks using live data from Chatbot Arena \cite{zheng_judging_2023}. Similarly, \citet{lin_wildbench_2024} proposed Wildbench to benchmark LLMs with real user queries. These benchmarks are limited to use LLMs as English judges. \citet{hada-etal-2024-large} expand the evaluation of LLM-based evaluators to eight languages, but not including SEA languages. To our knowledge, SeaBench is the first open-ended multi-turn benchmark for SEA languages. 

