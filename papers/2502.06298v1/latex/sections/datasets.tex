\section{SeaExam and SeaBench}

We aim to build multilingual benchmarks to comprehensively evaluate model adaptability to Southeast Asia applications, focusing on both linguistic style and content essence that cannot be fully measured with translated questions.
Following the design principle of MMLU and MT-bench, two comprehensive datasets in measuring the English capabilities of large language models, we incorporate real local exams of each country for SeaExam and engage native speakers to craft instructions commonly used in the corresponding language communities for SeaBench. 
This approach ensures that our benchmarks reflect real-world usage in SEA contexts. We outline the detailed creation processes for SeaExam and SeaBench in Section~\ref{subsection SeaExam} and Section~\ref{subsection SeaBench}, respectively.


\begin{figure*}[!ht]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.98\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/figs/examples/SeaExam_Examples.pdf}
        \caption{}
        \label{subfig:seaexam_example}
    \end{subfigure}
    % \hfill
    % Subfigure 2
    \begin{subfigure}[b]{0.98\textwidth}
        \centering
        \includegraphics[width=\linewidth]{latex/figs/examples/SeaBench_Examples_v1.pdf}
        \caption{}
        \label{subfig:seabench_example}
    \end{subfigure}
    \caption{Data Examples for the three languages in (a) SeaExam and (b) SeaBench. The correct answer for SeaExam is in \textbf{bold}. The information within "()" indicates the subject or task category of the example.}
    \label{fig:seaexam_examples}
\end{figure*}


\subsection{SeaExam Construction} \label{subsection SeaExam}
Evaluating LLMs using human exam questions can provide valuable insights into the model's performance, as these questions encompass a wide range of knowledge types.
However, relying solely on translations of monolingual exam questions can introduce content biases into model evaluations. For example, the widely used MMLU benchmark includes categories such as ``US History'', which may be more relevant to American users. 


To address this, we decide to manually collect exam questions from the SEA region (Indonesian (id), Thai (th), and Vietnamese (vi)). We follow the construction of M3Exam~\cite{zhang_m3exam_2023}, one of the few guidelines for compiling multilingual regional exam datasets. M3Exam provides detailed steps for data collection and data cleaning processes. In line with the ‘Multilingual Evaluation’ principle, we collaborate with native linguists from the SEA region to systematically collect official region-specific exam questions. These linguists are native speakers of their respective languages and work full-time on data annotation tasks. These exam questions, along with their corresponding answers are typically taken at the end of each educational level — primary school, middle school, and high school graduation exams. These questions undergo detailed data processing and annotation, ensuring their transformation into multiple-choice format with four answer options (examples are provided in Figure~\ref{fig:seaexam_examples}). Further details regarding the data curation process for SeaExam are provided in Appendix~\ref{app:seaexam}.

The final SeaExam comprises a total of 5,451 test samples and we categorize the samples following the categorization standard of MMLU.  The statistics of the SeaExam are shown in Table~\ref{tab:stats_m3exam_mmlu_cates}. 


\input{latex/tables/stats_m3exam_mmlu_format}


\subsection{SeaBench Construction}~\label{subsection SeaBench}


Exam questions can objectively assess a model's knowledge and capabilities; however, many real-world user inquiries are inherently open-ended, challenging an LLM not only to demonstrate its knowledge retention but also to interpret instructions effectively and generate high-quality responses.

Currently, MT-bench~\cite{zheng_judging_2023}, widely regarded as the most authoritative and systematically categorized open-ended benchmark, is composed of manually crafted, English-based instructions, thus it predominantly suits the usage scenarios of English-speaking users. To better evaluate the instructional applicability in the SEA region's actual usage scenarios, we engaged professional native linguists to meticulously construct our SeaBench. Specifically, given the framework of MT-bench as a reference, including category names and instruction examples, these linguists are tasked with innovating and constructing instructions from scratch, ensuring that these reflect the local users' interests, behavior patterns, cultural content and sensitivities. Three detailed examples are shown in Figure~\ref{fig:seaexam_examples}(b).

Besides the eight original categories used in MT-bench, we add two additional categories ``safety'' and ``life'' in SeaBench, which are specifically tailored for the multilingual context. Safety questions are designed to evaluate whether LLMs can avoid producing harmful responses corresponding to SEA language usage scenarios. Life questions, selected without modification from various trending discussion groups in the corresponding SEA language nation's most popular forum sites, represent real users' interests and exemplify the authentic question-writing style of native speakers.

Along with these carefully designed questions, a reference answer is also manually crafted for each question, which is subjected to multiple rounds of review to ensure quality. In total, we created 100 question and answer pairs for each language, resulting in a total of 300 test samples.
Detailed statistical results are presented in Table~\ref{tab:stats_seabench}.



\input{latex/tables/dataset_C-dist}