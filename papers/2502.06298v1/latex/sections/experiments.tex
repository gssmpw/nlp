\section{Experiment}
Given the meticulously built SeaExam and SeaBench, we then conduct experiments to quantitatively demonstrate how our benchmarks could better evaluate models' abilities on multilingual applications from: 1) how our datasets align more closely with the daily usage of regional languages (Section~\ref{subsection is_more_aligned}), and 2) how it effectively distinguishing differences in model performance across various languages (Section~\ref{finding1}) and distinguishing performance variations within the same model across different languages ((Section~\ref{finding2}) and (Section~\ref{finding3})). Through our fine-grained analysis using SeaBench, we have uncovered significant deficiencies in LLMs' response safety across multilingual usage scenarios. Consequently, we advocate for enhanced safety measures in models for multilingual contexts to better adapt to actual usage realities (Section~\ref{finding4})).


\begin{figure*}[!ht]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/cluster_distance/cluster_distance_SeaExam.pdf}
        \caption{}
        \label{subfig:seaexam_example}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/cluster_distance/cluster_distance_SeaBench.pdf}
        \caption{}
        \label{subfig:seabench_example}
    \end{subfigure}
    \caption{Cluster distance between each benchmark and Wild Queries. (a) Cluster distance of entity embeddings between each exam dataset and Wild Queries. (b) Cluster distance of sentence embeddings between each multi-turn dataset and Wild Queries.  A smaller value means more similar to Wild Queries.}
    \label{fig:cluster_distance}
\end{figure*}


\subsection{Are the Contructed SeaExam and SeaBench More Aligned with Actual Local Usage?}\label{subsection is_more_aligned}

Despite utilizing local exams and engaging native language experts specifically to tailor questions to the local context, the critical question remains unsolved: How do these questions more accurately reflect the actual local usage compared to those derived from translations? 
To evaluate the alignment of our benchmarks with actual local usage, we conduct a quantitative comparison between SeaExam and SeaBench and real-world user queries. As the first step, we construct the real-world user queries dataset ``Wild Queries'' as follows:

\textbf{Wild Queries} is constructed based on LMSYS-Chat-1M~\cite{zheng_lmsys-chat-1m_2023} and WildChat-1M~\cite{zhao2024wildchat,deng2024wildvisopensourcevisualizer}, which are databases of real-world human queries with millions of conversations across various application scenarios. Using these conversation data, we conducted a meticulous post-filtering process to obtain high-quality queries in SEA languages. First, we conducted 1) \textbf{Language Filter} for the corresponding SEA language using the original language labels and further refined our selection using the Google Translate API to confirm the query language. Given corresponding SEA queries, we have 2) \textbf{Data Balance Control} --- removing overly long conversations, limiting the data to extracting user inputs up to five rounds per conversation, to ensure data balance across different usage scenarios. Finally, we employ a capable multilingual model, GPT-4o, to process 3) \textbf{LLM-Based Heuristic Filter} to further filter out questions that are not queries or instructions.
After these three steps, we get a total of 4,658 queries real-world user queries in SEA languages. The statistic result is shown in Table~\ref{tab:stats_wild_queries} in the appendix.


Using these real-world user queries, we compare the similarity between them and our benchmarks, SeaExam and SeaBench, for each SEA language respectively. Specifically, we utilize the cluster distance (C-Dist) of sentence embeddings derived from the bge-multilingual-gemma2 model~\cite{bge-m3} to measure similarity. We also deploy translated MMLU (MMLU-SEA) and MT-bench (MT-bench-SEA) on SEA languages as baselines (more details on the datasets and the embedding calculation are shown in Appendix \ref{app:exp_details}).


As shown in Figure~\ref{fig:cluster_distance}, \textbf{SeaExam and SeaBench have a more similar distribution with Wild Queries than translated benchmarks}, with a smaller cluster distance by an average of 6 units. This demonstrates that our benchmarks could better evaluate model performance in real-world multilingual application scenarios.


\subsection{Can SeaExam and SeaBench better distinguish models across SEA language?}~\label{subsection better distinguish models}


We have quantitatively demonstrated that the constructed SeaExam and SeaBench benchmarks are more aligned with actual local usage questions (Section~\ref{subsection is_more_aligned}). However, does this greater alignment also improve our ability to distinguish between different models? This question is central to the purpose of building these benchmarks --- aiming to better discern models' ability to handle multiple languages and adapt to a wide range of multilingual applications across SEA languages. To answer the question, we evaluate nine LLMs, a detailed experiment setting as follows:

\textbf{Models:}
We consider multiple factors when selecting nine models for evaluation. First, instruction-following capability is a key requirement, as SeaBench necessitates models that can effectively adhere to given instructions. Second, we select only those with parameters ranging from 7B to 9B, as they offer a good balance between performance and inference speed. Based on these criteria, we select models from three groups: (1) the most popular open-source models, including Meta-Llama-3.1-8B-Instruct (Llama-3.1-8B)\cite{dubey_llama_2024}, Gemma-2-9b-it (Gemma-2-9B)\cite{gemma_team_gemma_2024}, Mistral-7B-Instruct-v0.3 (Mistral-7B)\cite{jiang_mistral_2023}, and Qwen2-7B-Instruct (Qwen2-7B)\cite{yang_qwen2_2024}; (2) models optimized for multilingual capabilities, including glm-4-9b-chat (glm-4-9b)\cite{glm_chatglm_2024} and Aya-23-8B\cite{aryabumi_aya_2024}; and (3) models specifically optimized for Southeast Asian languages, including SeaLLMs-v3-7B-Chat (SeaLLMs-v3-7B)\cite{zhang_seallms_2024}, llama3-8b-cpt-sealionv2-instruct (sealionv2)\cite{sea_lion_2024}, and Sailor-7B-Chat (Sailor-7B)~\cite{dou_sailor_2024}.

\textbf{Metrics and Setups:} 
For SeaExam, we conduct evaluation in 3-shot and use accuracy (\%) as the evaluation metric. For SeaBench, we employ LLMs-as-a-Judge~\cite{zheng_judging_2023, bai2023benchmarkingfoundationmodelslanguagemodelasanexaminer,ying2024automatingdatasetupdatesreliable}, setting GPT-4o as the judge model to evaluate LLM's responses based on the reference answers (construction details in Section~\ref{subsection SeaBench}). Considering that different categories of questions focus on assessing different aspects of model performance, we have designed a list of priority evaluation aspects for each category to facilitate a comprehensive judgment. We prompt GPT-4o to rate each response on a scale from 1 to 10.
These evaluation aspects are detailed in Table~\ref{tab:priority_aspects} and the evaluation prompt is shown in Figure~\ref{fig:prompt_template_turn1} and Figure~\ref{fig:prompt_template_turn2} in the appendix. More experimental and model setups is shown in Appendix \ref{app:eval_setup}.


Following this experimental setup, we conduct tests using SeaExam and SeaBench, with results presented in Table~\ref{tab:main_results}. Upon analyzing these results, we identify several interesting findings as follows:




\input{latex/tables/result_main}
\subsubsection{Finding 1: SeaExam and SeaBench can better distinguish different models} \label{finding1}
\begin{figure}[htb]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/std_benchmarks/std_seaexam_mmlu_v2.pdf}
        \caption{}
        \label{subfig:seaexam_example}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/std_benchmarks/std_seabench_mtbench_v2.pdf}
        \caption{}
        \label{subfig:seabench_example}
    \end{subfigure}
    \caption{(a) Accuracy standard deviation across the nine models for each language on SeaExam and MMLU-SEA. (b) Score standard deviation across the nine models for each language  on SeaBench and MT-bench-SEA.}
    \label{fig:finding1}
\end{figure}


We compare the performance of tested models between SeaExam and MMLU-SEA, examining the standard deviation of model performances across three SEA languages. Results, as shown in Figure~\ref{fig:finding1}, indicate that the variances in SeaExam are significantly higher than those in MMLU-SEA by 9.3\%.
A similar phenomenon was observed when comparing SeaBench with MT-bench-SEA by 8.7\%.
This consistency suggests that, compared to direct translations, our benchmarks more effectively discern the capabilities of models in real-world application scenarios.


In Figure~\ref{fig:finding1}, we find the abnormal phenomenon that SeaExam has no distinct advantage in differentiating among models for the Indonesian language. 
This may be due to the poor performance across the models on Indonesian, each showing a decline of more than 4.5\% compared to MMLU-SEA, resulting in a lower standard deviation in differentiation.
This observation prompts us to explore further whether the ability to effectively separate models extends to aiding in a more nuanced analysis across different languages. 

% \input{latex/tables/corr_SeaExam_SeaBench}

\subsubsection{Finding 2: SeaBench can better distinguish performance variations within the same model across different languages} \label{finding2}



\begin{figure}[htb]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/std_benchmarks/std_lang_seaexam_mmlu_v2.pdf}
        \caption{}
        \label{subfig:seaexam_example}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/std_benchmarks/std_lang_seabench_mtbench.pdf}
        \caption{}
        \label{subfig:seabench_example}
    \end{subfigure}
    \caption{(a) Accuracy standard deviation across three SEA languages for the nine models on SeaExam and MMLU-SEA. (b) Score standard deviation across three SEA languages for the nine models on SeaBench and MT-bench-SEA.}
    \label{fig:finding2}
\end{figure}


We conduct a comparison of nine models' performance standard deviations on SeaExam across three SEA languages and compared these with performances on MMLU-SEA. As shown in Figure~\ref{fig:finding2}, SeaExam does not demonstrate a significant advantage in distinguishing language differences. In contrast, a notable distinction emerges when comparing SeaBench to MT-Bench. Specifically, the performance gaps across the three languages in SeaBench are significantly larger than those in the translated MT-bench-SEA, by 6.7\% on average, indicating that SeaBench more effectively highlights the performance variations within the same model across different languages. Additionally, we identified a few models, such as Sailor-7B, SeaLLMs-v3-7B, and Sealionv2, that exhibited more balanced performances across SEA languages in SeaBench. This is because these models were specifically trained with a focus on SEA daily scenarios, which resulted in a more balanced performance on SEA language tests.



Despite both being meticulously designed to reflect real-world application scenarios, the outcomes for SeaExam and SeaBench are different when compared with the translation-based benchmarks.
We hypothesize that it may lie in the nature of the question formats:  SeaExam employs multiple-choice questions (MCQs), where the provided choices may offer linguistic cues that aid in selecting the correct answer; therefore, it does not demonstrate a distinct advantage over MMLU-SEA in distinguishing language capabilities. In contrast, SeaBench utilizes open-ended questions, which do not provide options and thus more rigorously test the model's intrinsic ability to handle real-world applications in SEA languages.
To further validate our hypothesis, we conducted an in-depth analysis, which led to our third finding.

\subsubsection{Finding 3: Open-Ended Question Formats More Effectively Distinguish Model Capabilities} \label{finding3}

\begin{figure}[htb]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/std_benchmarks/std_seaexam_seabench.pdf}
        \caption{}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{latex/figs/std_benchmarks/std_lang_seaexam_seabench.pdf}
        \caption{}
    \end{subfigure}
    \caption{(a) Accuracy standard deviation across the models for each language on SeaExam and SeaBench. (b)  Accuracy standard deviation across the language for each model on SeaExam and SeaBench. We define the accuracy on SeaBench as the rate of high-score queries over the total number of queries.}
    \label{fig:find3}
\end{figure}


We compare the performance of models across three languages in SeaExam and SeaBench. Since SeaExam employs accuracy (\%) as its metric and SeaBench uses scores from a judge model, the scoring methods are not directly comparable. To standardize the evaluation, we converted the latter’s scores to accuracy rates and full mark rates (where a response is considered correct only if it achieves full marks on all aspects). The results, depicted in Figure~\ref{fig:find3}, reveal that the deviations among the nine models across the three languages are greater in SeaBench compared to SeaExam by 1.37 times. This observation supports our earlier hypothesis that open-ended question formats, requiring more extensive language use, better highlight differences in model capabilities. 


\subsubsection{Finding 4: LLMs Perform Poorly on Safety Questions} \label{finding4}
Through extensive experimental analysis, we have demonstrated that our benchmarks more effectively evaluate models' abilities in real-world multilingual applications. Building on this, we conduct a fine-grained analysis, with the results for SeaBench shown in Figure~\ref{fig:seabench_mean_cate}. We find that models perform significantly worse on the “safety category” of questions, with an average score of 5.02, which is 20\% lower than the highest-performing “STEM category”. These questions assess the model’s ability to avoid generating harmful responses. This finding highlights a notable deficiency in the models' safety performance in relevant usage scenarios. We speculate that most alignment efforts are conducted using data on the models' primary languages and overlooking other multilingual application contexts. Consequently, \textbf{we advocate for enhanced safety measures in models for multilingual contexts to better adapt to actual usage}.


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{latex/figs/std_benchmarks/seabench_mean_cate_v2.pdf}
    \caption{The average scores of the nine LLMs on 8 categories of SeaBench. The models performs poorly on the safety questions.}
    \label{fig:seabench_mean_cate}
\end{figure}


\section{Human Evaluation}

\input{latex/tables/agreement/seabench_agreement_thres_1}

For both constructed benchmarks, SeaExam and SeaBench, each question and its corresponding reference answer are meticulously crafted by engaged native linguists, ensuring high quality. To further validate the reliability of our experimental results—particularly the evaluation scores assigned by GPT-4o for SeaBench—we conduct a human agreement evaluation. For each question, we randomly sample three distinct model pairs, ensuring that no model combination is repeated. Since SeaBench consists of 100 questions per language, each linguist evaluates 300 model pairs. As each language involves two turns, this approach results in a total of 600 votes per language. 


Annotators judge which of the two models produces a better response. If both responses are equally good, the result is marked as a tie. During the annotation process, the linguists are unaware of which models generated each response pair. The instructions for the human judges are provided in Figure \ref{fig:human_instruction} in the appendix. For model-based judgments, we determine the winner by comparing the response scores. To ensure a more balanced distribution of labels, we treat responses as ties if their scores differ by 1 point or less, as the model scores range from 1 to 10. Finally, we compare the human-generated votes with the model-derived votes to assess the level of agreement between them.


Results in Table~\ref{tab:seabench_agreement_thres_1} show that GPT-4o has a high agreement with human evaluations—64.9\% on average (with tie votes) and 91.3\% (without tie votes). In comparison, \citet{zheng_judging_2023} report 65\% agreement for human evaluators on MT-bench when including tie votes and 81.5\% when excluding them. This suggests that GPT-4o’s judgments align well with human preferences on SeaBench, confirming the reliability of our findings.


In addition to evaluating the results using GPT-4o as the judge in our experiment (more details in Section~\ref{subsection better distinguish models}), we expand our evaluation to include more judges, including GPT-4o-mini, Claude-3.5-Sonnet, Claude-3-Haiku, Gemini-Pro-1.5, and Gemini-Flash-1.5 and assess their results. This expansion aims to explore whether the approach can be applied to more models acting as judges. Considering that relying solely on GPT-4o might introduce biases, such as self-preference, especially when employing the LLMs-as-a-Judge approach, using different models helps mitigate the bias associated with exclusively using one judge~\cite{bai2023benchmarkingfoundationmodelslanguagemodelasanexaminer, ying2024automatingdatasetupdatesreliable, zhao2024auto}.
The result is shown in Table~\ref{tab:seabench_agreement_thres_1}. More details on the experimental setup and results are discussed in Appendix \ref{app:agreement}. 
