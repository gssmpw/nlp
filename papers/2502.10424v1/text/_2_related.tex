\section{Related Work}

\subsection{Efficient Long-Context Inference}
An important challenge in optimizing long-context inference lies in reducing memory and computation requirements while retaining high performance on tasks that involve long sequences.
Sparse attention mechanisms~\citep{liu2021sparseattn,xiao2023streamllm,yao2024sirllm,tang2024quest,yang2024doublesparse,liu2024scissorhands,ge2023model,jiang2024minference} have been widely adopted to manage the quadratic complexity of traditional full attention in long contexts. 
% By selectively attending only to a subset of tokens, sparse attention reduces both computational load and memory usage, enabling the processing of longer sequences. 
These techniques typically maintain efficiency by dropping non-essential Key-Value (KV) pairs from the cache.
% Another line of research maintains the full key-value pairs but dynamically loads them from HBM, and usually achieves higher performance at the cost of higher memory consumption.
Token pruning~\citep{fu2024lazyllm} selectively computes the KV for tokens relevant for next token prediction. KV Prediction~\citep{horton2024kvpredictionimprovedtime} improves prompt processing time by predicting the KV cache needed for autoregressive generation. 
Retrieval-augmented generation~\citep{tan2024lloco,liu2024retrievalattention} enhances the accuracy of language model outputs by combining generative models with external retrieval mechanisms whose context length is very long.


\subsection{Quantization}
Quantization has emerged as a powerful technique to reduce the memory footprint and computational complexity in large-scale neural networks.
Weight-only quantization~\cite{lin2024awq,kim2023squeezellm,shao2023omniquant,chee2024quip} focuses on reducing the precision of model weights to reduce the memory requirements of the model.
As models grow larger, the memory footprint of KV caches can become substantial, especially for long input sequences. KV cache quantization~\cite{liu2024kivi,hooper2024kvquant,kang2024gear} addresses this issue by quantizing the key and value caches to enable longer sequence inference.

\subsection{Speculative Decoding}
Speculative decoding has become an important technique for improving the inference efficiency of LLMs~\cite{leviathan2023fast,chen2023accelerating,kim2024speculative}. It uses a smaller draft model to rapidly generate candidate tokens, which are then verified by a larger target model to ensure correctness. Parallelization in speculative decoding has also been studied to enhance the efficiency by predicting multiple tokens at one time~\cite{cai2024medusa,bhendawade2024speculative,li2024eagle,chen2024sequoia}. We include additional related works in Appendix~\ref{appendix:rel_works}.

\textbf{Self-speculative decoding} is the class of speculative decoding methods in which the draft model shares the same architecture as that of target model for better alignment. Recent works like Magicdec \cite{magicdec} and TriForce~\cite{sun2024triforce} have shown that self-speculation with sparse KV can effectively speedup the draft model in long-context settings, where KV is the main bottleneck. While this design avoids loading the entire KV cache throughout the autoregressive generation process, KV cache sparsification can lead to noticeable performance degradation as evidenced in previous works~\cite{zhang2024h2o,liu2024scissorhands,ge2023model,zhou2024sirius}.
This can potentially yield a mismatch between the draft and target model's predictions (i.e., lower acceptance rate), which is a critical factor in overall speedup.
\OURS\ addresses this limitation by proposing a draft model with a novel hierarchical quantized KV cache, which maintains a higher acceptance rate between the draft and target models, therefore leading to better speedup. Note that our method can be combined with sparse KV methods \cite{sun2024triforce, magicdec} for additional speedup, which we leave for future work.