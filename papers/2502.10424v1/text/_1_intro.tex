

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/Teaser.pdf}
    \caption{Throughput in tokens/sec of various decoding methods. \OURS\ achieves $>1.78\times$ speedup over the autoregressive baseline across several context lengths. Benchmarked on LWM-Text-Chat-128k.}
    \label{fig:placeholder}
    \vspace{-0.2cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Large Language Models (LLMs) have been widely used in recent years, revolutionizing natural language processing (NLP) and artificial intelligence (AI) applications. 
As their applications expand, there is a growing demand to deploy LLMs in long-context settings -- handling extended text inputs such as document summarization, lengthy conversations, or comprehensive instructions. 
The model must maintain coherence in such contexts and track intricate details across extended sequences. 
However, long-context inference presents significant challenges in terms of efficiency and scalability.
For example, token eviction~\cite{zhang2024h2o,ge2023model,liu2024scissorhands} and KV cache quantization~\cite{liu2024kivi,kang2024gear,hooper2024kvquant} have been proposed to improve the efficiency for long-context inference. However, they often entail noticeable degradation in generation quality.


One promising alternative to enhance the efficiency of LLMs while preserving generation quality is speculative decoding~\cite{leviathan2023fast,chen2023accelerating,kim2024speculative}. 
This method accelerates inference by using a smaller (draft) model to rapidly generate candidate tokens, and uses the original (target) model to verify these tokens to ensure generation quality. However, the efficient application of speculative decoding in long-context settings has not been thoroughly explored.

Traditional speculative decoding approaches often rely on using smaller models as the draft model in order to minimize the memory-bandwidth overhead of loading the \textit{model weights} of the larger target model.
In long-context scenarios, however, the primary bottleneck shifts from model weights to the \textit{KV cache}, which grows linearly with the context length. Additionally, since small models do not usually possess good long-context understanding ability, the acceptance rates of the candidate tokens by the target model drop significantly, leading to suboptimal speedup.
Moreover, traditional speculative decoding methods maintain the KV cache for both the target model and the draft model, causing a large memory footprint. Therefore, finding a solution that both optimizes the KV cache's memory efficiency and improves the acceptance rate within speculative decoding is essential for performant LLMs in long-context applications.


To mitigate these issues and to enable efficient and accurate long-context inference, we propose \OURS, a self-speculative decoding method that utilizes 4-bit weights and a 4-bit hierarchical KV cache to speedup long-context inference. In particular we make the following contributions: 
\begin{itemize}
    \item We perform a comprehensive analysis of LLM inference to identify bottlenecks across various context lengths, demonstrating that quantizing the KV cache improves efficiency for long contexts, while quantizing model weights is more beneficial for short contexts (see Section \ref{sec:multi-regime-analysis-inference-bottlenecks}).
    \item We introduce a novel hierarchical quantization technique that enables bit-sharing between the target and draft models' KV caches, eliminating the need for additional memory for the draft model (see Section \ref{subsec:Hierarchical_kv_cache}).
    \item We propose a double full-precision cache buffer used for storing the most recent KV cache in full precision to improve acceptance rates and also eliminate wasteful quantization and dequantization operations (see Section \ref{subsec:full_precision_buffer}).
    \item We show that using a quantized KV cache leads to better acceptance rates between the target and the draft model, and thus leads to better overall speedups (see Section \ref{sec:speedup}).
    \item We implement custom CUDA kernels for attention with our hierarchical quantized KV cache achieving up to $\sim 2.88 \times$ speedups at 4-bit precision relative to FP16 FlashAttention kernels. (see Section \ref{sec:kernel_speedups})
\end{itemize}