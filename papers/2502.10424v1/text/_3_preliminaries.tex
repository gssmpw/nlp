\section{LLM Inference Bottlenecks}

\subsection{Arithmetic Intensity}
\label{sec:multi-regime-analysis-inference-bottlenecks}
To understand the primary bottlenecks in LLM inference and to motivate our method, we perform a thorough analysis of inference under several different regimes. These regimes include a combination of small versus large batch sizes and short versus long context lengths during both the prefill and decoding stages. We use arithmetic intensity as the central metric in our analysis, where arithmetic intensity is defined as the number of floating point operations (FLOPs) that can be performed per byte loaded from memory, or memory operations (MOPs) \cite{williams2009roofline}:
\[\text{Arithmetic Intensity} = \frac{\#\text{ FLOPs}}{\#\text{ MOPs}}.\] 

Arithmetic intensity allows us to classify which regimes of LLM inference are compute-bound or memory-bound and determine appropriate optimizations to improve latency. Compute-bound operations are limited by the hardware's peak FLOP/s (FLOPs per second) performance and benefit from algorithmic improvements that reduce computational complexity (e.g., subquadratic attention).
On the other hand, memory-bound operations are limited by the hardware's memory bandwidth (GB/s) and benefit from techniques that optimize memory load-store operations, such as quantizing the weights of a model even if they are later scaled up to a higher precision during computation to preserve accuracy.

For a finer-grained analysis, we break down the major operations in the Transformer into two categories: \textbf{linear}, which consists of the weight-to-activation matrix multiplications (i.e., $W_Q, W_K, W_V, W_{out}$, \texttt{mlp\_up\_proj}, \texttt{mlp\_down\_proj}, and the linear classification layer), and \textbf{attention}, which consists of the activation-to-activation matrix multiplications (i.e., query $\times$ key and attention weights $\times$ values). Note that the \textbf{aggregate} of all Transformer operations includes the above operations as well as non-linear operations like activation functions in the feed-forward network, softmax in the attention mechanism, and layer normalization. Because we are interested in studying the linear and attention operations, we do not explicitly focus on the non-linear operations and classification layer in our asymptotic analysis, although we include them in our final results.

\subsubsection{Asymptotic Analysis of Arithmetic Intensity for Prefill and Decoding}

% ASYMPTOTIC ANALYSIS TABLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% Define colors for headers
\definecolor{headergray}{RGB}{240,240,240}

\begin{table*}[h]
\caption{Asymptotic analysis of arithmetic intensity for linear, attention, and aggregate operations under prefill and decoding for batch size $B$, sequence length $S_L$, hidden dimension $d$, and generation length of $k$ tokens.}\label{tab:asymptotic_analysis}
\vspace{2mm}
\centering
\begin{tabularx}{\textwidth}{l|Y|Y|Y}
  \toprule
  \multicolumn{4}{c}{\textbf{Prefill}} \\
  \midrule
   & \cellcolor{headergray}Linear & \cellcolor{headergray}Attention & \cellcolor{headergray}Aggregate \\
  \midrule

  \cellcolor{headergray}FLOPs 
    & $\mathcal{O}(B \cdot S_L \cdot d^2)$ 
    & $\mathcal{O}(B \cdot {S_L}^2 \cdot d)$
    & $\! \! \mathcal{O}(B \!\cdot\! S_L \!\cdot\! d^2) + \mathcal{O}(B \!\cdot\! {S_L}^2 \!\cdot\! d) \!$ \\


  \midrule
  \cellcolor{headergray}MOPs 
    & $\underbrace{\mathcal{O}(B \cdot S_L \cdot d)}_{\text{activations}}\; + \;\underbrace{\mathcal{O}(d^2)}_{\text{weights}}$
    & $\!\underbrace{\mathcal{O}(B \cdot S_L)}_{\text{flash-attn scores}} + \! \! \! \underbrace{\mathcal{O}(B \cdot S_L \cdot d)}_{\text{activations }\{Q, C_K, C_V\}}$
    &  $\mathcal{O}(B \cdot S_L \cdot d) + \mathcal{O}(d^2)$ \\

  \midrule
  \cellcolor{headergray}Arithmetic Intensity 
      & 
        $\begin{array}{ll}
            \approx \begin{cases}
                \mathcal{O}(B \cdot S_L), & S_L \ll d \\
                \mathcal{O}(d), & S_L \gg d
            \end{cases}
        \end{array}$
      & 
        $\begin{array}{ll}
            \approx  \begin{cases}
                \mathcal{O}(S_L), & S_L \ll d \\
                \mathcal{O}(S_L), & S_L \gg d
            \end{cases}
        \end{array}$
      & 
        $\begin{array}{ll}
            \approx \begin{cases}
                \mathcal{O}(B \cdot S_L), & S_L \ll d \\
                \mathcal{O}(S_L), & S_L \gg d
            \end{cases}
        \end{array}$ \\
  \midrule
  \multicolumn{4}{c}{\textbf{Decode}} \\
  \midrule
   & \cellcolor{headergray}Linear & \cellcolor{headergray}Attention & \cellcolor{headergray}Aggregate \\
  \midrule

  \cellcolor{headergray}FLOPs 
    & $\mathcal{O}(k \cdot B \cdot d^2)$ 
    & $\mathcal{O}(k \cdot B \cdot S_L \cdot d)$ 
    & $\! \! \mathcal{O}(k \!\cdot\! B \!\cdot\! d^2) + \mathcal{O}(k \!\cdot\! B \!\cdot\! S_L \!\cdot\! d)$ \\

  \midrule
  \cellcolor{headergray}MOPs 
    & $\underbrace{\mathcal{O}(k \cdot B \cdot d)}_{\text{activations}}\; + \;\underbrace{\mathcal{O}(k \cdot d^2)}_{\text{weights}}$
    & $\! \underbrace{\mathcal{O}(k \!\cdot\! B \!\cdot\! S_L)}_{\text{attention scores}} + \!\underbrace{\mathcal{O}(k \!\cdot\! B \!\cdot\! S_L \!\cdot\! d)}_{\text{activations }\{C_K, C_V\}}$
    & $\mathcal{O}(k \cdot d^2) + \mathcal{O}(k \cdot B \cdot S_L \cdot d)$ \\

  \midrule
  \cellcolor{headergray}Arithmetic Intensity 
      & 
        $\begin{array}{ll}
            \approx \begin{cases}
                \mathcal{O}(B), & S_L \ll d \\
                \mathcal{O}(B), & S_L \gg d
            \end{cases}
        \end{array}$
      & 
        $\begin{array}{ll}
            \approx  \begin{cases}
                \mathcal{O}(1), & S_L \ll d \\
                \mathcal{O}(1), & S_L \gg d
            \end{cases}
        \end{array}$
      & 
        $\begin{array}{ll}
            \approx \begin{cases}
                \mathcal{O}(B), & S_L \ll d \\
                \mathcal{O}(1), & S_L \gg d
            \end{cases}
        \end{array}$ \\
  \bottomrule
\end{tabularx}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

During \textbf{prefill}, the model weights are only loaded once to process all tokens in the input and generate the first token. Because the context length can range from a couple thousand to hundreds of thousands of tokens, this phase consists of large matrix-matrix multiplications (matmuls) with high arithmetic intensities. Table \ref{tab:asymptotic_analysis} shows asymptotic analysis of arithmetic intensity for prefill and decoding broken up into linear, attention, and aggregate operations for batch size $B$, sequence length $S_L$, hidden dimension $d$, and a generation length of $k$ tokens. During prefill, the aggregate arithmetic intensity is similar to the arithmetic intensity of the linear projections when $S_L \ll d$ because self-attention is relatively inexpensive for short contexts. Thus the linear projections dominate latency in this regime. However, as the context length increases and $S_L \gg d$, the aggregate arithmetic intensity reflects the arithmetic intensity of attention, which begins to dominate latency since self-attention incurs additional cost with longer context lengths. Note that our analysis assumes the use of FlashAttention \cite{flashattention}, such that the attention scores matrix which grows on the order of $\mathcal{O}(B \cdot {S_L}^2)$ is never fully materialized, and thus the memory operations for this matrix are limited to $\mathcal{O}(B \cdot S_L)$.

On the other hand, in the \textbf{decoding} stage, generating $k$ tokens requires loading and storing the weights and KV cache $k$ times. Since the input at each iteration is a single token per sequence in the batch ($x\in\mathbb{R}^{B\times 1\times d}$), these operations mainly consist of small matmuls with low arithmetic intensity. For short context lengths where $S_L \ll d$, the aggregate arithmetic intensity for decoding again reflects the arithmetic intensity of the linear projections as loading and storing a small KV cache is relatively inexpensive compared to loading and storing the model weights. However, as the context length grows ($S_L \gg d$), the load-store operations for the large KV cache exacerbate and dominate latency, and the aggregate arithmetic intensity reflects the arithmetic intensity of attention. Ultimately, the aggregate arithmetic intensity for decoding is much lower than that of prefill:
\[
\underbrace{
\begin{cases}
    \mathcal{O}(B \cdot S_L), & S_L \ll d \\
    \mathcal{O}(S_L), & S_L \gg d
\end{cases}}_{\text{prefill}}
\quad \gg \quad
\underbrace{
\begin{cases}
    \mathcal{O}(B), & S_L \ll d \\
    \mathcal{O}(1), & S_L \gg d
\end{cases}}_{\text{decode}}.
\]
While the aggregate arithmetic intensity for prefill scales proportionally to the context length which can be in the hundreds of thousands, the aggregate arithmetic intensity for decoding does not scale with the context length at all. Moreover, using larger batch sizes only seems to increase the arithmetic intensity for decoding in the short-context setting. For long contexts, decoding has an extremely low arithmetic intensity irrespective of the batch size since every sequence in the batch undergoes self-attention separately and therefore cannot benefit from batching in the same way linear layers do.

% Combined Arithmetic Intensity Analysis Figures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/decode_ai_analysis.png}
    \caption{Breakdown of how arithmetic intensity changes during decoding as the context length and batch size are scaled logarithmically for linear, attention, and aggregate operations. All regimes lie below the ridge plane and thus are memory-bound. The ridge plane is calculated for an NVIDIA A6000 GPU. The colors for the linear and attention surface plots simply represent the magnitude of the arithmetic intensity. The aggregate plot is colored by attention's runtime as a percentage of the total latency. Prefill results in Appendix~\ref{appendix:prefill_ai_analysis}.}
    \label{fig:decode_arithmetic_intensity_analysis}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Compute versus Memory-Bound Regimes}
The asymptotic analysis suggests that in general, decoding suffers from low arithmetic intensities compared to prefill in all regimes. However, to decide which optimizations will most effectively improve latency, all regimes must be classified as either compute-bound or memory-bound. 
Whether an operation is compute or memory-bound depends on the hardware it is being run on as well as the magnitude of the arithmetic intensity achieved by the operation.

We utilize an analytical roofline model \cite{williams2009roofline, kim2023squeezellm, kim2023stackoptimizationtransformerinference} to help determine which regimes are compute or memory-bound in a practical inference setting. The roofline model defines a \textit{ridge point} which is calculated as
\[\frac{\text{peak compute performance (FLOP/s)}}{\text{peak memory-BW (GB/s)}}.\]
Note that the ridge point has the same units as arithmetic intensity (FLOPs/byte). In the roofline model, any operation with an arithmetic intensity smaller than the ridge point 
is memory-bound, and any operation with an arithmetic intensity greater than the ridge point is compute-bound. For our analysis, we extrapolate this to a \textit{ridge plane} and use hardware specifications for an NVIDIA A6000 GPU to study inference for the Llama-2-7B model in 16 bit precision. 

For optimizing speculative decoding, we specifically focus on the decoding phase, although we include results for prefill in Appendix~\ref{appendix:prefill_ai_analysis}. Figure~\ref{fig:decode_arithmetic_intensity_analysis} shows the arithmetic intensity for generating 1k tokens at different context lengths and batch sizes for the Linear/Attention components as well as the aggregate arithmetic intensity.
To decide the ideal quantization strategy for different regimes, we consider the aggregate arithmetic intensity, which is colored by the percentage of the total latency taken up by attention and provides a complete view of decoding in all regimes. 
Based on these results, we can clearly see that in the small batch + short context regime, the memory operations for the linear projections dominate latency, so \textbf{weight quantization} could provide considerable speedup in this regime. In the small batch + long context, large batch + short context, and large batch + long context regimes, attention dominates latency due to the expensive load-store operations for the large KV cache. \textbf{KV cache quantization} could help provide performance improvements in these regimes. In the small batch + medium context and short context + medium batch regimes, the linear and attention operations are approximately equivalent in their contributions to total latency. Thus, both \textbf{weight and KV cache quantization} are ideal here.