\section{Conclusions}
In this paper, we have introduced a novel approach to enhance the efficiency and scalability of Large Language Models (LLMs) in long-context settings through quantized speculative decoding. Our method addresses the increasing memory and computational demands by optimizing the Key-Value (KV) cache operations, which become a significant bottleneck as the context length grows. We propose a double full-precision cache buffer to resolve conflicts between per-group quantization and speculative decoding. Our comprehensive approach shows that by integrating advanced quantization techniques with speculative decoding, it is possible to significantly improve processing speed without compromising the accuracy and performance of LLMs. This work paves the way for more scalable and effective deployment of LLMs in applications that require extensive contextual understanding, offering a robust solution to the challenges posed by long-context settings.