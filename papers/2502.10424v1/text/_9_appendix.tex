\appendix
\clearpage
\onecolumn
\leftline{ {\Large Appendix } }
% \section{Appendix}

\section{Attention Module's Inference Workflow}
The inference of LLMs can be divided into 2 parts: the prefill stage and the decoding stage. In the \textbf{prefill stage}, for the input sequence $X\in\mathbb{R}^{B\times S_L\times d}$, the KV cache update rule can be calculated as \[Q = XW_Q, ~~C_K = XW_K, ~~C_V = XW_V,\]
where we denote the query, key, and value weight matrices as $W_Q, W_K, W_V \in \mathbb{R}^{d\times d}$ and denote the key and value caches as $C_K$ and $C_V$ respectively. $B$ refers to batch size, $S_L$ refers to sequence length, and $d$ refers to hidden size. We then calculate the multi-head attention (MHA) as:
\[ O = \operatorname{MultiHeadAttn}(Q,~C_K,~C_V). \]

In the \textbf{decode stage}, for input token $x\in\mathbb{R}^{B\times 1\times d}$, we first calculate the query, key, and value of the current token:
\[q = xW_Q, ~~c_k = xW_K, ~~c_v = xW_V,\]
then concatenate the KV cache with the current token's key and value to update the KV cache:
\[ C_K = \operatorname{concat}(C_K, c_k), ~~C_V = \operatorname{concat}(C_V, c_v).\] 
Then, the multi-head attention output is calculated:
\[ O = \operatorname{MultiHeadAttn}(q,~C_K,~C_V). \]



\section{More Related Works}\label{appendix:rel_works}
We list some related works that we find interesting, but can not elaborate on in the related works section due to space limitations.

\paragraph{Efficient Long Context Inference}
Some research maintains the full key-value pairs but dynamically loads them from high-bandwidth memory~\cite{yang2024doublesparse,tang2024quest}, and usually achieves higher performance at the cost of higher memory consumption. Shared KV cache across tokens~\cite{nawrot2024dynamic} and layers~\cite{brandon2024reducing} provides a new way to reduce the KV cache budget through sharing.

\paragraph{Quantization} Any Precision representation~\cite{park2024any} incorporates multiple precision levels (e.g., INT2, INT4, and INT8) within a single representation, eliminating the need to store separate KV caches for each precision and allowing the framework to dynamically select the optimal precision based on the complexity of the task. Training quantization~\cite{peng2023fp8,xi2024jetfire,fishman2024fp8trillion,xi2024coatcompressingoptimizerstates} reduces the bit precision of various model parameters, gradients, and activations to accelerate training.
Attention quantization~\cite{chen2024int8attn,zhang2024sageattention,shah2024flashattention3} reduces the computational overhead associated with attention computations, which becomes dominant in the prefill stage of the long-context inference setting.

\paragraph{Speculative Decoding} Zhao et al.,~\cite{zhao2024qspec} explored complementary quantization schemes in speculative decoding with QSpec, enhancing efficiency without significant performance degradation. 
Sirius~\cite{zhou2024sirius} finds that contextual sparsity will lead to poor performance under the speculative decoding setting since the model performance is degraded, and thus it cannot accelerate LLM inference.

\section{Additional LLM Inference Bottlenecks Analysis}
\label{appendix:appendix_inference_bottlenecks_analysis}
\subsection{Prefill Arithmetic Intensity Analysis}
\label{appendix:prefill_ai_analysis}

Keeping in line with the asymptotic analysis in Table ~\ref{tab:asymptotic_analysis}, the arithmetic intensity of attention during prefill does not scale with batch size at all, as attention is unable to benefit from batching in the same way that linear operations do. Moreover, for long contexts, attention entirely dominates the linear operations due to the quadratic nature of self-attention. For short contexts however, this quadratic cost is relatively inexpensive when compared to the linear operations. As shown in Figure~\ref{fig:prefill_arithmetic_intensity_analysis}, the arithmetic intensity for all prefill operations in all regimes is above the ridge plane, which means that prefill is entirely compute-bound.

% Combined Arithmetic Intensity Analysis Figures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/prefill_ai_analysis.png}
    \caption{During prefill, all regimes lie above the ridge plane and thus are compute-bound. }
    \label{fig:prefill_arithmetic_intensity_analysis}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Modern GPU Hardware VRAM Size Constraints}
\label{appendix:gpu_memory_constraints}
% KV Cache vs. GPU Memory Figure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/kv_cache_vs_gpu_memory.png}
    \caption{KV cache memory usage by Llama-2-7B on a single node (8 GPUs) as context length and batch size are scaled logarithmically. The surface plot's color represents the ratio of KV cache memory to the model weights memory. The dotted-lines represent GPU DRAM capacities for several different GPUs. At ($B=16, S_L=262$k), the KV cache takes up 160x more memory than the model weights.}
    \label{fig:kv_cache_vs_gpu_memory}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The relatively higher linear arithmetic intensities observed in decoding for batch sizes greater than 8 in Figure~\ref{fig:decode_arithmetic_intensity_analysis} are misleading due to the limited VRAM sizes in modern GPUs. As shown in Figure~\ref{fig:kv_cache_vs_gpu_memory}, the size of the KV cache for a Llama-2-7B model exceeds the total VRAM capacities of a single node equipped with 8 A100/H100 GPUs with 80 GB of memory each. This means that simply scaling the batch size for decoding will not translate the memory-bound nature of generation to being compute-bound.

\section{Quantization Strategies for KV cache}
\label{appendix:quantization_strategies}
Here we provide a visualization of our quantization scheme in Figure~\ref{fig:group_quant_asymmetric_quant}. We apply asymmetric quantization for both the keys and values cache, and apply channel-wise quantization to the key cache and apply token-wise quantization to the value cache. We also provide a table to show that this quantization scheme offers the best performance in Table~\ref{tab:token_channel_wise_quant} by showing that it gives the lowest perplexity.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/quant_method.png}
    \caption{We apply asymmetric and per-group quantization for both the key cache and value cache, along the channel axis and token axis, respectively. This figure describes how it works when only the upper-4 bit cache is applied.}\label{fig:group_quant_asymmetric_quant}
\end{figure}

\begin{table}[h]
    \centering
    \begin{tabular}{c|cc}
        \toprule
         & \multicolumn{2}{c}{Key Cache} \\
        \midrule
        Value Cache & token-wise & channel-wise \\
        \midrule
        token-wise & 6.587 & \textbf{6.507} \\
        channel-wise & 7.041 & 6.911 \\
        \bottomrule
    \end{tabular}
    \caption{Perplexity of Llama-2-7B on WikiText-2 dataset with different quantization strategies. Group size $G = 128$. Channel-wise quantization for key cache and token-wise quantization for value cache gives the best performance.}\label{tab:token_channel_wise_quant}
\end{table}

\label{appendix:fp_cache_buffer}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/DoubleCacheBuffer.pdf}
    \caption{Our KV cache with 2 full precision cache buffers for recent KV cache.}
    \label{fig:double_size_fp_buffer}
\end{figure}

\section{Compatibility with Flash Decoding}\label{appendix:flash_decoding}
Our full-precision buffer design, as shown in Figure~\ref{fig:double_size_fp_buffer}, is fully compatible with Flash Decoding~\cite{flashdecoding}, a fast attention implementation available for the decoding stage. In this setup, the quantized section divides naturally into several chunks, aligning perfectly with the structure of Flash Decoding. For the full-precision buffer, given its upper bound length of $2G$, it can be processed independently with minimal overhead. This full-precision segment can be treated as an additional chunk, which can then be summed with the quantized segments and seamlessly integrated into the Flash Decoding algorithm.



\section{Details about the Datasets Used}
\label{appendix:datasets}
We provide an overview of the datasets used in our experiments, highlighting their key characteristics.

\begin{itemize}
    \item \textbf{WikiText-2} \cite{merity2016pointer}: WikiText-2 is a widely used dataset for language modeling. It is a subset of the larger WikiText dataset and consists of high-quality, clean, and well-structured English text extracted from Wikipedia articles. 
    \item \textbf{C4} \cite{raffel2020exploring}: C4 is a large scale web-crawled language modelling dataset mostly used for pretraining LLMs.
    \item \textbf{PG-19} \cite{raecompressive2019pg19}: It is a dataset of books from Project Gutenberg, designed for long-context language modeling.
    \item \textbf{$\infty$B{\scriptsize ENCH} Sum} \cite{zhang2024inftybenchextendinglongcontext}: InfiniteBench benchmark is tailored for evaluating the capabilities of language models to process, understand, and reason over super long contexts. We used one of its summarization datasets where the task is to summarize a fake book created by core entity substitution. The average length of input prompt is $\sim$171k.
    \item \textbf{Multi-LexSum} \cite{shen2022multilexsum}: Multi-LexSum is a multi-doc summarization dataset for civil rights litigation lawsuits. The average length of prompt in this dataset is $\sim$90k.
\end{itemize}


\section{Hyperparameter Search}\label{appendix:hparam_search}
Here we present details about the hyperparameter search done to select optimal $\gamma$ for each experiment. We search $\gamma$ for each dataset and method pair using a prompt length of 8192 and use the same value for all other context length experiments. Table \ref{tab:hparam_search} shows the results of the search. We find that sparse-based methods achieves a maximum performance when $\gamma$ equals to 1, while our quantization-based method usually achieves the best performance with a larger $\gamma$, such as 4 or 6.

\begin{table*}[h]
\centering
\caption{Hyperparameter Search for Llama-2-7B-32K and LWM-Text-Chat-128k models on PG19, Multi-LexSum, and $\infty$B{\scriptsize ENCH} Sum datasets. Context length is kept as 8k.}
\label{tab:hparam_search}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.85\linewidth}{!}{
\begin{subtable}{0.48\linewidth}
    \centering
    \caption{Llama-2-7B-32k on PG19}
    \label{tab:llama_pg19}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{$\gamma$} & \textbf{Acceptance Rate $\uparrow$} & \textbf{Speedup $\uparrow$} \\
        \midrule
        \multirow{3}{*}{StreamingLLM}
            & 1 & 90.78 & \textbf{39.1} \\
            & 2 & 89.42 & 38.5 \\
            & 3 & 90.21 & 38.66 \\
        \midrule
        \multirow{3}{*}{SnapKV}
            & 1 & 94.39 & \textbf{40.34} \\
            & 2 & 91.03 & 39.38 \\
            & 3 & 91.84 & 39.28 \\
        \midrule
        \multirow{4}{*}{\OURS}
            & 1 & 91.88 & 41.52 \\
            & 2 & 89.88 & \textbf{44.51} \\
            & 4 & 83.17 & 43.84 \\
            & 6 & 77.07 & 41.88 \\
        \bottomrule
    \end{tabular}
\end{subtable}
\hfill
\begin{subtable}{0.48\linewidth}
    \centering
    \caption{Llama-2-7B-32k on Multi-LexSum}
    \label{tab:llama_multilex}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{$\gamma$} & \textbf{Acceptance Rate $\uparrow$} & \textbf{Speedup $\uparrow$} \\
        \midrule
        \multirow{3}{*}{StreamingLLM}
            & 1 & 90.78 & \textbf{39.17} \\
            & 2 & 86.82 & 37.84 \\
            & 3 & 83.29 & 36 \\
        \midrule
        \multirow{3}{*}{SnapKV}
            & 1 & 55.55 & \textbf{31.05} \\
            & 2 & 43.96 & 24.39 \\
            & 3 & 36.61 & 19.78 \\
        \midrule
        \multirow{4}{*}{\OURS}
            & 1 & 96.58 & 42.83 \\
            & 2 & 96.61 & 47.51 \\
            & 4 & 95.59 & 49.47 \\
            & 6 & 91.23 & \textbf{49.62} \\
        \bottomrule
    \end{tabular}
\end{subtable}
}

\vspace{1em}
\resizebox{0.85\linewidth}{!}{
\begin{subtable}{0.48\linewidth}
    \centering
    \caption{LWM-Text-Chat-128k on $\infty$B{\scriptsize ENCH} Sum}
    \label{tab:LWM-Text-Chat-128k_infbench}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{$\gamma$} & \textbf{Acceptance Rate $\uparrow$} & \textbf{Speedup $\uparrow$} \\
        \midrule
        \multirow{3}{*}{StreamingLLM}
            & 1 & 81.79 & \textbf{37.17} \\
            & 2 & 74.86 & 34.33 \\
            & 3 & 64.48 & 30.14 \\
        \midrule
        \multirow{3}{*}{SnapKV}
            & 1 & 85.55 & \textbf{38.27} \\
            & 2 & 82.92 & 36.97 \\
            & 3 & 77.13 & 34.40 \\
        \midrule
        \multirow{4}{*}{\OURS}
            & 1 & 93.83 & 42.01 \\
            & 2 & 94.38 & 46.74 \\
            & 4 & 90.33 & \textbf{47.30} \\
            & 6 & 82.13 & 45.51 \\
        \bottomrule
    \end{tabular}
\end{subtable}
\hfill
\begin{subtable}{0.48\linewidth}
    \centering
    \caption{LWM-Text-Chat-128k on Multi-LexSum}
    \label{tab:LWM-Text-Chat-128k_multilex}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{$\gamma$} & \textbf{Acceptance Rate $\uparrow$} & \textbf{Speedup $\uparrow$} \\
        \midrule
        \multirow{3}{*}{StreamingLLM}
            & 1 & 83.96 & \textbf{37.80} \\
            & 2 & 77.41 & 35.13 \\
            & 3 & 71.28 & 32.37 \\
        \midrule
        \multirow{3}{*}{SnapKV}
            & 1 & 89.25 & \textbf{39.04} \\
            & 2 & 83.53 & 37.22 \\
            & 3 & 80.04 & 35.48 \\
        \midrule
        \multirow{4}{*}{\OURS}
            & 1 & 95.94 & 42.79 \\
            & 2 & 95.06 & 47.10 \\
            & 4 & 92.55 & 48.15 \\
            & 6 & 87.73 & \textbf{48.20} \\
        \bottomrule
    \end{tabular}
\end{subtable}
}
\end{table*}



\section{Comparing Acceptance Rates} \label{app:acc_rate}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/acc_rate.png}
    \caption{Acceptance rate of self-speculative decoding methods at different speculation length measured for model LWM-Text-Chat-128k on Multi-LexSum dataset.}
    \label{fig:acc_rate_vs_gamma}
\end{figure*}
Although the acceptance rates in Table~\ref{tab:results} for the sparse KV baselines and \OURS\ do not seem exceedingly different, this is misleading because the acceptance rates shown are for the optimal $\gamma$ values observed from our hyperparameter search in Table~\ref{tab:hparam_search}. Table~\ref{tab:results} effectively compares the acceptance rates of the baselines at very low $\gamma$ (e.g. 1) with those of \OURS\ at much higher $\gamma$ (e.g. 6). Here, we compare the acceptance rates of different self-speculative decoding algorithms. 
For fair comparison, Figure \ref{fig:acc_rate_vs_gamma} illustrates the acceptance rate between the draft and target models as a function of speculation length. We observe that \OURS\ consistently outperforms sparse KV approaches in terms of acceptance rate. Notably, as speculation length increases, the acceptance rate of sparse KV methods degrades much faster, whereas our method maintains high acceptance rates.


\input{text/_algorithm_large}
