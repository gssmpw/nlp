\section{Evaluation}
\label{sec:results}
In this section, we evaluate the performance of \OURS\ across multiple datasets and context lengths. Our evaluation focuses on three key dimensions: (1) the acceptance ratio between the draft and target models, (2) GPU memory consumption, and (3) end-to-end serving speedup. We begin by presenting a detailed benchmarking of acceptance rate, memory usage, and end-to-end speedup across different datasets. Then, we highlight the performance gains achieved by our custom kernels for quantized KV cache. 

\subsection{Setup}
All experiments are performed on a node equipped with 8 NVIDIA RTX A6000 GPUs. We evaluate \OURS\ using long-context variants of LLaMA-2 and LWM models as target models. For benchmarking decoding speedup, we use PG-19 \cite{raecompressive2019pg19}, 
(a language modeling benchmark)
% an open-vocabulary language modeling benchmark derived from books; 
and two long context summarization datasets, namely $\infty$B{\scriptsize ENCH} Sum \cite{zhang2024inftybenchextendinglongcontext, yen2024helmet} and Multi-LexSum \cite{shen2022multilexsum, yen2024helmet}. More details about the datasets are provided in Appendix \ref{appendix:datasets}. Following ~\citet{magicdec}, we compare against two recent sparse KV-based self-speculative decoding baselines: StreamingLLM \cite{magicdec, xiao2023streamingllm} and SnapKV \cite{magicdec, li2024snapkv}. To ensure a fair comparison, the draft KV budget for the baselines is set to one-fourth of the context length, matching our 4-bit quantized KV cache. We fix the quantization group size at 128, the residual length $R$ for the KV cache at 256, and limit the number of output tokens to 90. The optimal speculation length $\gamma$ for each dataset is determined through a hyperparameter search for each dataset-model pair. Details of the hyperparameter search are provided in Appendix \ref{appendix:hparam_search}.
 

\subsection{Speedup Evaluation}
\label{sec:speedup}
 Table \ref{tab:results} shows the acceptance rate, GPU memory required, and speedup achieved compared to autoregressive decoding. We observe that \OURS\ provides consistently better speedups for all context lengths. For short and medium context lengths (e.g. 8k and 32k prompt length), \OURS\ achieves $\sim$1.61$\times$ to $\sim$2.08$\times$ speedups respectively on the Multi-LexSum dataset. For longer context lengths (e.g. 128k), our speedups are even greater, up to $\sim$\textbf{2.49}$\times$, all while using lower GPU memory than the baselines. We also see that acceptance rates of \OURS\ are considerably higher than the baselines for summarization tasks (refer to Appendix \ref{app:acc_rate} for detailed comparison); this shows that for such tasks where the whole context is important, sparse KV cache methods are much more lossy, whereas quantization preserves most of the information in the context. Consequently, \OURS\ proves to be a more reliable choice, delivering consistent speedups across varying context lengths and query complexities.

\subsubsection{Kernel Speedups} \label{sec:kernel_speedups}
In Table \ref{tab:kernel_speedup} we show the speedup achieved using our custom attention kernel that makes use of quantized KV cache versus the standard FP16 FlashAttention kernels. For a context length of 128k, our INT4 attention kernel is $\sim2.88\times$ faster than the standard FlashAttention kernel.

\begin{table}[h!]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c}
    \toprule
     & \multicolumn{2}{c}{Context Length} \\
    \midrule
    Kernels & 64k & 256k \\
    \midrule
    FlashAttention (FP16) & 3.07 ms & 6.16 ms \\
    \OURS\ INT 8 & 1.08 ms (1.44x) & 4.06 ms (1.51x) \\
    \OURS\ INT 4 & 0.54 ms (2.88x) & 2.15 ms (2.86x) \\
    \bottomrule
\end{tabular}}
\caption{Latency benchmark of our custom attention kernels for calculating attention with quantized hierarchical KV cache. QuantSpec INT4 refers to only loading the upper-4-bit, QuantSpec INT8 refers to loading both the upper and lower 4-bit. Benchmarked on kernel level.}
\label{tab:kernel_speedup}
\end{table}

\subsection{Ablation Results}
We present an extensive ablation study of \OURS{} focusing on the contribution of weight versus KV cache quantization in the final speedup.

\textbf{Weight versus KV Quantization}: Figure \ref{fig:ablation} illustrates the speedup ratio of QuantSpec compared to autoregressive baseline as context length increases. The figure benchmarks QuantSpec with KV cache-only quantization, weight-only quantization, and both. The results are aligned with the analysis done in Section \ref{sec:multi-regime-analysis-inference-bottlenecks}, showing that for short contexts most of the speedup comes from quantizing weights, for medium length prompts both weight and KV cache quantization contribute to the final speedup, and KV cache quantization is most effective for long contexts.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/AblateKVW_png.png}
    \vspace{-7mm}
    \caption{Speedup ratio of \OURS{} compared to autoregressive baseline as we scale the context length. We report \OURS{} with KV cache-only quantization, weight-only quantization, and both. Benchmarked on Llama-2-7B-32k-Instruct using PG-19.}
    \label{fig:ablation}
\end{figure}