%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
% \onecolumn

\section{Limitations and Future Directions}

Although the current results are promising, there are also limitations. Firstly, although the generated videos using the compressed latent space exhibit good visual quality and high quality scores, they differ from those generated in the original latent space.  Additionally, the lack of retraining prevents the generative model from fully exploiting the advantages of a dynamic latent space. Future research could address these limitations by: 1. Developing automated frame rate schedulers integrated with the generative process. 2. Designing new positional encoding mechanisms tailored to dynamic latent spaces. 3. Training generative models end-to-end in such spaces to maximize efficiency and performance.

\section{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning, particularly in the domain of video generation. By introducing DLFR-VAE (Dynamic Latent Frame Rate Variational Auto Encoder), we propose a training-free framework that dynamically adjusts the latent frame rate based on video content complexity, significantly reducing computational overhead while maintaining high reconstruction quality. This innovation has the potential to make video generation more efficient and scalable, enabling longer and higher-resolution video synthesis with reduced computational resources.

The broader impact of this work includes potential applications in various fields such as entertainment, education, and virtual reality, where efficient video generation is crucial. By lowering the computational barriers, DLFR-VAE could democratize access to advanced video generation technologies, allowing smaller organizations and researchers with limited resources to leverage state-of-the-art video synthesis tools.

However, as with any generative technology, there are ethical considerations to be mindful of. The ability to generate high-quality videos efficiently could be misused for creating deepfakes or other forms of misinformation. It is important for the community to develop robust detection mechanisms and ethical guidelines to mitigate such risks. Additionally, the environmental impact of reduced computational requirements could be positive, as it may lead to lower energy consumption in data centers. While this work primarily aims to advance the technical capabilities of video generation, we encourage ongoing discussions around its ethical implications and societal consequences to ensure that the technology is used responsibly and for the benefit of society.

\section{Design Details of Dynamic Upsampler / Downsampler}
We illustrate the design based on the 3D VAE with the 884 architecture adopted by HunyunVideo. The encoder and decoder of this VAE consist of four down blocks / up blocks and a mid block, respectively.

\textbf{In the encoder}, temporal downsampling is achieved by the DownsampleCausal3D modules within certain down blocks, which apply convolutions with stride $> 1$ along the temporal dimension. 

\textbf{In the decoder}, temporal upsampling is not performed via transposed convolution. Instead, each up block restores the temporal sequence length using interpolation (e.g., nearest-neighbor), followed by convolutions with stride $= 1$ for feature fusion and smoothing.

DLFR operates \textbf{only} along the temporal dimension. The design and positional selection of the upsampler/downsampler are as follows:

\paragraph{Temporal Downsampling in the Encoder:}
We modify the temporal stride of the DownsampleCausal3D modules in selected down blocks — for example, changing the stride from $(1,2,2)$ to $(2,2,2)$, or from $(2,2,2)$ to $(4,2,2)$ — where $(T, H, W)$ denotes temporal, height, and width dimensions, respectively. This enables a further increase in the temporal downsampling ratio.

\textbf{Temporal Downsampling Positional Selection:} Among the four down blocks, only the first three (closer to the input) contain DownsampleCausal3D modules, thus offering three selectable positions. For \textbf{2$\times$ downsampling}, one of the three down blocks is selected and its stride is modified; for \textbf{4$\times$ downsampling}, two of them are selected for stride adjustment.

\paragraph{Temporal Upsampling in the Decoder:}
Each up block contains three ResNet blocks and one interpolation module. To perform upsampling, we insert an additional nearest-neighbor interpolation module either \textit{before or after} a selected ResNet block to increase the temporal resolution.

\textbf{Temporal Upsampling Positional Selection:} Across the four up blocks, there are 12 ResNet blocks. Considering both pre- and post-insertion for each, we obtain 16 candidate positions. For \textbf{2$\times$ upsampling}: one interpolation location is selected; for \textbf{4$\times$ upsampling}: two positions are selected.

\paragraph{Optimal Combination Selection:}
To determine the best combinations of upsampling and downsampling positions, we exhaustively evaluate all possible configurations by constructing corresponding VAE models. For \textbf{2\,Hz} frame rate: there are 48 possible combinations; for \textbf{4\,Hz} frame rate: there are 546 possible combinations.These are tested on 500 videos at 15\,Hz and 540p resolution. Evaluation metrics include \textbf{SSIM}, \textbf{PSNR}, and \textbf{LPIPS}.
\vspace{-1.5em} 
\begin{figure}[H]
    \centering
    \hspace*{-1cm}
    \includegraphics[width=1.1\linewidth]{figures/psnr_ssim_lpips_comparison.pdf}
    \caption{VAE reconstruction metrics (SSIM, PSNR, LPIPS) under different frame rates and combinations.}
    \label{fig:vae_metrics}
\end{figure}
\paragraph{Results:}
The results (see Figure~\ref{fig:vae_metrics}) indicate that the three evaluation metrics (SSIM, PSNR, LPIPS) follow a highly consistent pattern across all configuration combinations. Specifically, configurations achieving the highest SSIM and PSNR values also tend to yield the lowest LPIPS scores. Notably, the most effective upsampling and downsampling positions are predominantly located near the input end of the encoder and the output end of the decoder.

Based on this analysis, we fix the optimal position combinations under each frame rate setting for use in subsequent experiments and system deployment.



\section{Video Generation with Modified RoPE}

For video generation on the dynamic frame rate latent space, we modify the RoPE in of the diffusion transformer (DiT). To handle latent frames with varying frame rates, where a frame can represent either a short or long duration of time, we adjust the positional embedding for RoPE (Rotary Position Embedding) in DiT. Specifically, for tokens in the compressed latent space that each token represents a longer duration, we reduce the frequency of the RoPE parameters to generate the cosine and sine components.
`
\subsection{Concept of RoPE}

The RoPE (Rotary Position Embedding) mechanism is a technique used in transformer-based models~\cite{su2024roformer}, including those applied in diffusion models like Diffusion Transformer. We first provides a detailed introduction to the RoPE mechanism in the context of Diffusion Transformer.

In transformer models, position embeddings are crucial for the model to understand the order of input elements (e.g., tokens in a sequence). Traditional position embeddings (like those in the original Transformer model) use fixed vectors to represent positions, which can be inefficient for long sequences and may not generalize well to sequences of different lengths. RoPE addresses these limitations by introducing a more dynamic way of encoding positions.

RoPE represents positions using a combination of sine and cosine functions. For a token at position $m$ with embedding $\mathbf{x}_m \in \mathbb{R}^D$, RoPE applies rotation matrices $\mathbf{R}_m$ to queries ($\mathbf{q}_m$) and keys ($\mathbf{k}_n$) in the attention mechanism:

\begin{equation}
    \mathbf{q}_m = \mathbf{R}_m \mathbf{W}_q \mathbf{x}_m, \quad \mathbf{k}_n = \mathbf{R}_n \mathbf{W}_k \mathbf{x}_n
\end{equation}

where $\mathbf{R}_m$ is defined block-diagonally for each dimension pair $(2i, 2i+1)$:

\begin{equation}
    \mathbf{R}_m = \bigoplus_{i=1}^{D/2} \begin{pmatrix}
        \cos m\theta_i & -\sin m\theta_i \\
        \sin m\theta_i & \cos m\theta_i
    \end{pmatrix}, \quad \theta_i = 10000^{-2i/D}
\end{equation}

The attention score $A_{m,n}$ naturally encodes relative positions:

\begin{equation}
    A_{m,n} = (\mathbf{R}_m \mathbf{q}_m)^\top (\mathbf{R}_n \mathbf{k}_n)
    \label{eq_raw_rope}
\end{equation}
The frequency of the RoPE is ${10000^{-2i/D}}$.

\subsection{Resample RoPE Parameters for DLFR}

\begin{figure}[tb] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.47\textwidth]{figures/resample.pdf} %ims/xx.png
    \caption{Illustration of resampling for DLFR. (a) represents the original RoPE sampling method, while (b) depicts the resampling method adapted for DLFR. The green areas highlight the extended positional relationships for tokens that represent longer time spans.}
\label{im_resample}
\end{figure}

When a latent token corresponds to a longer time span, the RoPE parameters should be adjusted. This is because such tokens encode positional relationships over longer temporal intervals compared to adjacent frames.

To address this, we replace the $m$ in Equation~\ref{eq_raw_rope} with $P_m$ based on the frame rates of different tokens. For a token with longer time span, its positional distance to neighboring tokens increases proportionally. Using this relationship, we compute the new RoPE parameters as follows:

    
In this way, we generate the new RoPE parameters:

\begin{equation}
    \mathbf{R}_m = \bigoplus_{i=1}^{D/2} \begin{pmatrix}
        \cos P_{m}\theta_i & -\sin P_{m}\theta_i \\
        \sin P_{m}\theta_i & \cos P_{m}\theta_i
    \end{pmatrix}, \quad \theta_i = 10000^{-2i/D}
\end{equation}

As shown in Figure~\ref{im_resample}, the interval of different RoPE sample points is different according to the frame rate.

\subsection{Speedup of generating videos in compressed space}


\begin{table}[!tb]
\centering
\caption{Latency of one diffusion step under different latent space on HunyuanVideo Diffusion model.}
\label{table_gen_latency}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{@{}cccccc@{}}
\toprule
                      &         & Original & DLFR  & DLFR  & DLFR \\ \midrule
                      & CR      & 4x       & 6x    & 8x    & 12x  \\ \midrule
\multirow{2}{*}{720p} & latency & 59.90    & 29.36 & 19.26 & 9.58 \\
                      & speedup & 1.00     & 2.04  & 3.11  & 6.25 \\ \midrule
\multirow{2}{*}{540p} & latency & 24.93    & 11.61 & 7.73  & 4.05 \\
                      & speedup & 1.00     & 2.15  & 3.23  & 6.16 \\ \bottomrule
\end{tabular}
}
\vspace{-0.2in}
\end{table}

% 我们测试了xxx，从表中可以观察到，DLFR可以显著降低开销，在CR=6x，平均可以获得2x的speedup，在CR=8x，xxx，在CR=12x，xxx。速度提升主要来源于diffusion的token数量的降低，由于Transformer的Attention的计算量是随着token数量平方级别，因此可以显著降低计算量。

We evaluated the latency of denoising step of different compression ratio.
From Table~\ref{table_gen_latency}, we observe that denoising in the compressed latent space can significantly reduces the generation cost. At CR=6x, it achieves an average speedup of 2x for one step of denoising. At CR=12x, DLFR achieves an speedup of more than 6x. These improvements primarily stem from the reduction in the number of tokens processed by the diffusion model. Since the computational complexity of the Transformer's attention mechanism scales quadratically with the number of tokens, reducing the token count leads to substantial computational savings.

% \section{Visualization}

% \subsection{Open-Sora Reconstruction Visualization}
% We provides more 

% \subsection{}