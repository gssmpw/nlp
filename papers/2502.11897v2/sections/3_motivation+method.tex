

\section{Method}
In this section, we present \dlfr, a training-free solution for dynamic frame rate control in the latent space. We first establish our theoretical foundation by analyzing the temporal frequency of video signals in both pixel and latent spaces (Sec.\ref{subsec:motivation}). Based on this analysis, we propose our dynamic latent frame rate space (Sec.\ref{subsec:dflr_space}). To realize this design, we propose two key technical components: \raisebox{-1.1pt}{\ding[1.1]{182\relax}} a Dynamic Latent Frame Rate Scheduler that determines optimal frame rates based on content complexity (Sec.\ref{subsec:dflr_Scheduler}), and \raisebox{-1.1pt}{\ding[1.1]{183\relax}} a training-free adaptation mechanism that enables pretrained VAE architectures to process variable frame rate features (Sec.\ref{subsec:dflr_vae}).
Beyond these components, we provide a more straightforward explanation for why our simple yet effective approach can work (Sec.\ref{subsec:discuss}).



\subsection{Motivation}
\label{subsec:motivation}
Traditional frame rate optimization in video processing and compression has predominantly focused on raw video signals \cite{song2001rate,mackin2015study}. Previous studies in video content analysis have demonstrated that video information density exhibits strong temporal non-uniformity. This non-uniformity manifests as significant variations in temporal frequency across different video segments \cite{menon2022vca,papakonstantinou2023content}. 
For instance, in our analysis of the BVI-HFR dataset (Fig.~\ref{fig:video_examples}), fast-motion sequences like ``books'' exhibit 5--8× higher temporal frequency magnitude compared to static scenes like ``flowers'' (see Fig.\ref{fig:analysis_on_video_seg} Top). 

However, with the growing adoption of deep learning for video generation, videos are increasingly mapped into a latent space via an encoder~\cite{fan2024fluid,kuaishou2024,zheng2024open,kong2024hunyuanvideo}. This raises crucial questions: \textit{(i) How do temporal characteristics transfer into the latent space? (ii) Does the latent space preserve the frequency variations observed in the original pixel domain? (iii) Can we apply dynamic frame rates within the latent space?} Although these questions are highly relevant, all of them remain underexplored.




\paragraph{Frequency Analysis of Signals.}

A continuous-time signal can be denoted as $x(t)$. Its frequency spectrum $X(f)$ is obtained via the Fourier transform, and for a  band-limited signal, it is nonzero only up to a maximum frequency $f_{\max}$:
\begin{equation}
X(f) = 0, \quad \forall|f| > f_{\max}.
\end{equation}
Sampling the continuous signal at sampling frequency $F_s$ discretizes $x(t)$ into frames:
\begin{equation}
x[n] = x(nT), \quad T = \frac{1}{F_s}.
\end{equation}
According to the Nyquist-Shannon sampling theorem~\cite{Nyquist_theory,shannon_noise}, $F_s$ must satisfy
\begin{equation}
F_s \geq 2f_{\max}.
\end{equation}
to prevent aliasing. As $f_{\max}$ varies across different segments of the signal, different segments naturally require different frame rates, motivating adaptive frame rate strategies.

\paragraph{Temporal Frequency Analysis of Latent Space.}
Let the video luminance signal be $x(t)$, and its encoder mapping be $\mathcal{E}$. In the latent space, the signal becomes $z(t) = \mathcal{E}(x(t))$ with a  corresponding frequency spectrum $Z(f)$:
\begin{equation}
Z(f) = \int_{-\infty}^{\infty} z(t)e^{-j2\pi ft} dt.
\end{equation}
Note that $\mathcal{E}$ is a complex nonlinear transformation, it can alter the amplitude, phase, and frequency characteristics of a signal or generate new frequency components, leading to changes in the video signal's shape and spectrum.
Therefore, we first analyze the signal in the latent space.
% Hence, the highest frequency in the latent domain, $f'_{\max}$, usually satisfies:
% \begin{equation}
% f'_{\max} \leq g(f_{\max}),
% \end{equation}
% where $g(\cdot)$ denotes the encoder's frequency response function~\cite{smith1997scientist}.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/frequency_analyse.pdf}
%     \caption{Frequency analysis of original video and latent space in temporal dimension. The video segments are 1080p, 60Hz, and 10-second length in BVI-HFR dataset~\cite{mackin2018study}.}
%     \vspace{-0.2in}
%     \label{fig:analysis_on_video_seg}
% \end{figure}


Through empirical analysis on multiple video segments (see Fig.\ref{fig:analysis_on_video_seg}), we observe pronounced frequency variability in both the original video domain and latent domain. High-speed segments—such as rapid camera pans or fast-moving objects—often retain higher temporal frequencies, whereas low-motion segments focus on lower-frequency components. This suggests that adaptive frame rate optimization remains viable in latent space, just as it is in raw pixel space.

In line with the Nyquist-Shannon theorem, when the latent-space sampling rate $F'_s$ meet
\begin{equation}
F'_s \geq 2f'_{\max},
\end{equation}
aliasing can be prevented. The $f'_{\max}$ is its maximum frequency in latent space. Hence, segment-wisely estimating $f'_{\max}$ and adjusting $F'_s$ provides an opportunity to implement a variable frame rate in latent space. 

\begin{figure*}[!tbhp] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.98\textwidth]{figures/Dynamic_VAE.pdf} %ims/xx.png
    \vspace{-0.1in}
    \caption{\textbf{Architecture overview of the Dynamic Latent Frame Rate (DLFR) VAE.} The input video is first divided into segments. The dynamic encoder processes these segments through a series of 3D convolution layers interspersed with dynamic downsample operations (Eq.\ref{eq:encoder} in Sec.\ref{subsec:dflr_vae}), where the execution of downsample is determined by the schedule (Sec.\ref{subsec:dflr_Scheduler}). The resulting latent representations maintain varying temporal resolutions according to segment complexity (Sec.\ref{subsec:dflr_space}). The dynamic decoder then reconstructs the video through corresponding upsampling operations (Eq.\ref{eq:decoder} in Sec.\ref{subsec:dflr_vae}), restoring the original frame rate while preserving temporal consistency. Each segment can be processed at different frame rates, enabling content-adaptive temporal compression in latent space.}
    \vspace{-0.1in}
    \label{fig:dynamic_vae}
\end{figure*}

\subsection{Dynamic Frame Rate Latent Space}
\label{subsec:dflr_space}
Building on the above analysis, we propose a dynamic frame rate latent space wherein each video segment can have a distinct frame rate, allocated based on its temporal complexity. Specifically, suppose an input video is divided into $M$ segments $\{S_1,\ldots,S_M\}$, each comprising $N$ frames. For segment $S_i$, its latent representation is $z_i(t)$. The frequency spectrum $Z_i(f)$ is defined as
\begin{equation}
Z_i(f) = \int_{t_i}^{t_i+NT} z_i(t)e^{-j2\pi ft} dt,
\end{equation}
where $T = 1/F_s$ is the sampling interval in the raw video domain.



From empirical observations, certain high-frequency components in $Z_i(f)$ have negligible amplitude and minimal impact on overall fidelity. We thus define an effective maximum frequency $f_{\text{eff},i}$ for each segment, identifying the point where the amplitude remains above a threshold $\epsilon$:
\begin{equation}
f_{\text{eff},i} = \max\{f \mid |Z_i(f)| \geq \epsilon\}.
\end{equation}

By the Nyquist-Shannon principle~\cite{ash2012information}, the corresponding latent-space frame rate for segment $S_i$ can be lowered to
\begin{equation}
F'_{s,i} = 2f_{\text{eff},i}.
\end{equation}
This adaptive sampling ensures each segment maintains only the minimum frame rate necessary to preserve perceptually significant temporal details. To maintain temporal consistency across segment boundaries, we implement a
smooth transition mechanism that gradually adjusts frame
rates between adjacent segments.


\subsection{DLFR Scheduler} 
\label{subsec:dflr_Scheduler} 
While theoretically sound, computing exact frequency spectra for real-time video processing presents significant computational challenges. We address this through a practical approximation strategy that maintains the benefits of dynamic frame rates while ensuring computational efficiency. Our approach discretizes the continuous space of temporal complexities into $N$ distinct levels.
Each level $k \in \{1,\ldots,N\}$ is associated with an effective frequency $f'_{\text{eff},k}$~\footnote{For example, we use \{1, 2, 4\}Hz for the 16 FPS video, which have \{16x, 8x, 4x\} temporal downsample ratio.}. % TODO: the number ablation
If segment $S_i$ falls into complexity class $k$, its latent frame rate becomes
\begin{equation}
F'_{s,i} = 2f'_{\text{eff},k}.
\end{equation}
Formally, we express this as:
\begin{equation}
F'_{s,i} = \sum_{k=1}^N (2f'_{\text{eff},k}) \mathbb{I}_{C_k}(S_i),
\end{equation}
where $\mathbb{I}_{C_k}(S_i)$ is an indicator function that is 1 if $S_i$ belongs to class $C_k$ and 0 otherwise.

\begin{figure}[!tb] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.9\linewidth]{figures/frequency_analyse_eff.pdf} %ims/xx.png
    \caption{Content complexity experiment on HunyuanVideo VAE. The upper figure illustrates the relationship between content complexity and effective frequency, with  $\epsilon=1.8$ used in this experiment. The lower figure demonstrates the alignment between content complexity and reconstruction LPIPS, indicating a strong correlation.}
    \label{im_content_complexity}
\end{figure}

Directly evaluating $\mathbb{I}_{C_k}(S_i)$ from raw or latent signals can still be challenging. 
Instead, we use a practical content complexity metric $C(S_i)$ as a proxy, which considers the SSIM of adjacent frames in a raw video segment: 
\begin{equation}
C(S_i) = \frac{1}{N}\sum_{j=1}^{N-1} (1-\text{SSIM}(x[j],x[j+1])),
\end{equation}
This metric can efficiently distinguish high-motion segments from low-motion ones without explicitly analyzing the latent frequency spectrum. The scheduling logic then maps each segment's metric value to an appropriate complexity class $k$, and hence to a frame rate $F'_{s,i}$. 
As illustrated in Figure~\ref{im_content_complexity}, the content complexity metric exhibits a high correlation with both the effective frequency and the VAE reconstruction performance.
We use this metric and thresholds $Th$ to determine the frame rate:
\begin{equation} 
\label{eq_complexity_threshold}
\mathbb{I}_{C_k}(S_i) = 
\begin{cases} 1, & \text{if}~~~~Th^{down}_{i}<C(S_i)\le Th^{up}_{i} \\
0, & \text{else}. \end{cases} 
\end{equation} 


% \[
% \mathbb{I}_{C_k}(S_i) = 
% \begin{cases} 
% 1 & \text{if } S_i \in C_k, \\
% 0 & \text{otherwise}.
% \end{cases}
% \]




% we cannot get the real the temporal space of input video is quantized, 
% 如何评价，提出Content Information

% 具体而言，我们引入了一个动态帧率Scheduler（Dynamic Frame Rate Encoder Scheduler）。该模块首先评估视频每个chunk的信息密度，并动态分配对应的潜空间帧率。

\subsection{Transform Static VAE to Dynamic VAE}
\label{subsec:dflr_vae}

% 如何从原始视频生成动态帧率的潜空间，以及如何从动态潜空间还原回原始视频。我们注意到：以往的视频VAE模型通过在大规模包含各种动作与节奏的视频数据上训练，已经具备了将视频压缩到固定帧率潜空间并还原回原始视频的能力。这一特性启发我们对现有的静态帧率VAE模型进行改造，扩展出动态帧率的潜空间生成能力。我们需要一个运行的过程中动态根据输入scheduler的变化而变化的模型（Encoder模型的输出维度动态变化，Decoder模型的输入维度动态变化），因此我们需要将静态模型转换为一个动态模型。

% 具体来说，我们在VAE的Encoder中插入一个dynamic down下采样器我们在VAE模型中的合适位置根据分配的帧率对VAE中间特征进行动态下采样，从而得到包含不同帧率的chunk特征。在该module之后的VAE模型以相同方式推理具有不同的chunk帧率的features，最终输出的结果就是在dynamic frame rate的latent space。在解码过程中，我们在Deocoder模型中的合适位置根据编码时确定的帧率对中间特征进行上采样，将所有chunk特征恢复到一致的帧率，最终重建出原始视频。我们将其称为DLFR-VAE（Dynamic Latent Frame Rate VAE）


To convert a pretrained static-frame VAE into a dynamic-frame version, we exploit the existing capacity of modern video VAEs, which have learned to compress videos into fixed-frame latent spaces~\cite{chen2024deep,xing2024large,zhu2023designing}. As illustrated in Fig.~\ref{fig:dynamic_vae}, our approach introduces two key modifications to the pretrained VAE: a dynamic downsampling module in the VAE's encoder and a corresponding upsampling module in the decoder. This design allows us to leverage the robust compression capabilities of pretrained VAEs while enabling variable frame rate processing without requiring additional training.

\paragraph{Encoder Modification.} Let the input video be $\{x_1,x_2,\ldots,x_T\}$. A pretrained video VAE encoder $E$ typically processes this input into a latent representation $z$. To support variable frame rates, we introduce a dynamic downsampler at a strategically chosen point in the encoder. Given a frame-rate schedule $\{F'_{s,1},F'_{s,2},\ldots,F'_{s,M}\}$, the downsampler transforms encoder features $h_i$ for each segment $S_i$ into a reduced-rate feature $h'_i$:
\begin{equation}
h'_i = \text{Downsample}(h_i, F'_{s,i}).
\label{eq:encoder}
\end{equation}
These reduced-rate features are then passed through the remaining encoder layers, denoted $E_{\text{post}}$, to yield segment-wise latent codes $\{z_1,\ldots,z_M\}$.

\paragraph{Decoder Modification.} Decoding requires reversing the frame rate changes. A dynamic upsampler is inserted at the corresponding decoder stage. For each segment's latent code $z_i$, the initial decoder layers $D_{\text{pre}}$ produce intermediate features $h''_i$:
\begin{equation}
h''_i = D_{\text{pre}}(z_i).
\end{equation}
The upsampler then restores the original frame rate $F_s$:
\begin{equation}
h'''_i = \text{Upsample}(h''_i, F_s),
\label{eq:decoder}
\end{equation}
after which the remaining decoder layers $D_{\text{post}}$ reconstruct the final segment $\hat{S}_i$. The overall video reconstruction $\hat{V}$ is formed by concatenating $\{\hat{S}_1,\ldots,\hat{S}_M\}$.

Crucially, these modifications allow the pretrained encoder and decoder weights to remain largely unchanged, except for the newly inserted downsampling and upsampling operators. Consequently, \dlfr\ can be deployed as a training-free extension on top of mainstream video VAEs, seamlessly enabling dynamic latent frame rate control.

\subsection{Discussion on \dlfr}
\label{subsec:discuss}
In addition to our information-theoretic formulation, we offer an intuitive explanation for why our \textbf{simple but effective} \dlfr\ can compress latent space with minimal reconstruction loss.
At its core, \dlfr\ dynamically downsamples the pretrained VAE encoder and, in turn, upsamples its decoder—effectively achieving content-dependent spatial-temporal compression without additional training as shown in Fig.\ref{fig:dynamic_vae}.

Pretrained video VAEs~\cite{chen2024deep,xing2024large,zhu2023designing} are typically trained on large-scale datasets that include diverse motion types, ranging from slow to fast, and from sparse to dense. Many training pipelines also involve augmentation techniques (e.g., temporal interpolation) that effectively expose the VAE to slower versions of the same content. Consequently, the VAE develops an internal capacity to represent video content at different temporal scales, but its default latent space is merely configured to operate at a fixed frame rate. \dlfr~can thus be seen as a mechanism that ``reactivates'' this dormant flexibility. By strategically downsampling and upsampling in the encoder and decoder, respectively, \textit{we allow the VAE to adapt to each segment’s temporal complexity, leveraging the latent representational power that was already learned but not previously utilized for frame rate variation.}
