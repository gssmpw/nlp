\section{Introduction}


Video is a fundamental medium for capturing real-world dynamics, making the generation of diverse video content a crucial capability for AI systems~\cite{videoworldsimulators2024,agarwal2025cosmos}. 
Recent advances in diffusion models~\cite{ho2020denoising,rombach2022high,esser2024scaling,jin2024pyramidal} and autoregressive models~\cite{kuaishou2024,fan2024fluid} have led to notable breakthroughs in producing high-fidelity and long-duration videos.

At the heart of these video generation frameworks lies the Variational Autoencoder (VAE) \cite{kingma2013auto,rombach2022high,xing2024large}, which jointly reduces spatial and temporal dimensions to create compact latent representations. 
This latent space not only provides a more structured manifold for downstream generative tasks, but also substantially lowers computational and memory requirements compared to operating in the original video domain~\cite{agarwal2025cosmos,videoworldsimulators2024}. Based on the latent space, modern architectures, such as Diffusion Transformers~\cite{peebles2023scalable} or Auto-Regressive Transformers~\cite{esser2021taming}, can then effectively learn the distribution of these latent representations, with the VAE decoder ultimately reconstructing them back into complete videos \cite{jin2024pyramidal,kuaishou2024,kong2024hunyuanvideo}.

The computational complexity of video generation is largely determined by the size of the latent representation (\textit{i.e.}, the number of latent tokens)~\cite{kong2024hunyuanvideo,zheng2024open,kondratyuk2023videopoet}. 
% . For a typical eight-second video processed at 24 frames per second with an 8$\times$ spatial compression ratio, the model must generate 10$\times$24$\times$512 = 122,880 tokens in the latent space
This substantial token count imposes significant computational overhead, primarily due to the quadratic complexity of attention in diffusion and autoregressive Transformers~\cite{vaswani2017attention,kondratyuk2023videopoet,peebles2023scalable}. Therefore, addressing this bottleneck is pivotal for extending video generation to longer durations and higher resolutions. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/overview.pdf}    
    \vspace{-0.2in}
    \caption{\textbf{\dlfr}: A training-free approach that accelerates video generation through content-adaptive spatial-temporal compression. This module can seamlessly integrate with existing pretrained video generative models.}
    % (both diffusion- and autoregressive-based) while preserving their generative ability.}
    \vspace{-0.3in}
    \label{fig:intro}
\end{figure}

In order to achieve more efficient video generation in a training-free manner, we explore the possibility of adjusting latent frame rates based on information-theoretic content density.
Prior studies have demonstrated that video data exhibits significant temporal non-uniformity~\cite{feichtenhofer2019slowfast,yang2020temporal,li2022nuta}. For example, high-motion segments (e.g., soccer shots) have more content complexity than static scenes (e.g., stationary players). 
% Overcompressing sparse-content segments causes information loss, whereas undercompressing dense-content segments causes redundancy.
Our empirical observations reveal that in standard fixed-rate latent spaces, roughly 35\% of latent units carry minimal information, indicating considerable computational waste. 
These findings support our key motivation: sparse-content segments (e.g., slow-motion sequences) can be efficiently represented with fewer latent elements, whereas dense-content segments require a higher token density to preserve temporal complexity~\cite{yu2024efficient,xiang2020zooming,lin2023spvos,ravanbakhsh2024deep}.

Based on this motivation, we propose \textbf{Dynamic Latent Frame Rate VAE, \dlfr}, a novel framework that enables dynamic frame rate in the latent space without additional training. 
We proposes two key technical innovations for \dlfr: \raisebox{-1.1pt}{\ding[1.1]{182\relax}} a Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on content complexity, and \raisebox{-1.1pt}{\ding[1.1]{183\relax}} a training-free adaptation mechanism that enables pretrained VAE architectures to process features with variable frame rates. 
This dynamic frame rate strategy can significantly reduce both computational overhead and memory requirements for video generative models (See Fig.\ref{fig:intro}).

The first component \raisebox{-1.1pt}{\ding[1.1]{182\relax}} centers on our efficient content complexity metric based on inter-frame differences, which enables real-time frame rate adaptation during inference with minimal computational overhead.
We can then schedule each video chunk's adaptive frame rate based on this metric. 
Building on this scheduler, we develop \raisebox{-1.1pt}{\ding[1.1]{183\relax}} a training-free mechanism to augment pretrained VAE models with dynamic frame rate capabilities. Leveraging the robust compression and reconstruction capabilities of large-scale pretrained video VAEs~\cite{wu2024improved,xing2024large,chen2024deep}, we strategically inject dynamic sampling modules into the network architecture. 
In the encoder, we incorporate dynamic downsampling operators that modulates frame rates, producing chunk features with varying temporal resolutions. 
% These variable-rate features are processed through the remaining encoder layers to produce the dynamic latent representation. 
The decoder mirrors this design with dynamic upsampling operators that restores features to a consistent frame rate.
% based on encoding-time schedules. 
This architectural approach preserves pretrained weights while enabling dynamic frame rate control, making \dlfr~a plug-and-play module for downstream video generative models.
% whether under diffusion- or autoregressive-based~\cite{zheng2024open,kondratyuk2023videopoet} frameworks.

Experimental results show that DLFR maintains a reconstruction quality (SSIM change $<$ 0.03) while reducing the average number of latent space elements by 50\%. We integrated DLFR into an existing diffusion-based video generation model without any fine-tuning, and the model was able to generate acceptable videos with a significant speedup. The latency of a diffusion step is reduced by 2 to 6 times, and the end to end speedup is 2x to generate a video.

% The key contributions of this work are as follows:
% \begin{itemize}
%     \item We establish, for the first time, a quantitative relationship between video content complexity and latent space representation efficiency, proposing a dynamic frame rate decision criterion based on information entropy.
%     \item We design a plug-and-play dynamic frame rate processing module that is compatible with existing VAE architectures (e.g., HunyuanVideo~\cite{kong2024hunyuanvideo}, OpenSora~\cite{zheng2024open}) without requiring fine-tuning. We develop a multi-scale feature fusion mechanism to effectively address reconstruction consistency challenges in variable frame rate latent spaces.
%     \item Experimental results on the UCF-101 and Kinetics-400 datasets demonstrate that DLFR-VAE maintains a reconstruction quality of PSNR $>$ 32dB and SSIM $>$ 0.91, while reducing the average number of latent space elements by xxx\% (with peak reductions of up to xxx\%), accelerating diffusion model inference by xx times, and reducing memory usage significantly.
% \end{itemize}


% Version 0
% On the other hand, we observe that the characteristics of content variation differ significantly across videos and even within different segments of the same video. Prior studies [1-3] have shown that the information density of video content exhibits substantial non-uniformity. For example, as illustrated in Figure 2, frames in high-motion segments of sports videos (e.g., a soccer shot) may contain 5-8 times more inter-frame differences than low-motion segments (e.g., players standing still). Similarly, in nature documentaries, the temporal complexity of animal predation sequences exceeds that of static landscape shots by over 60\%. This dynamic nature of video content fundamentally conflicts with fixed compression rate strategies: sparse-content segments become redundant under overly high compression rates, while dense-content segments suffer information loss when compression rates are too low. Our preliminary experiments reveal that under a fixed frame rate in the latent space, approximately 35\% of latent units contribute less than 10\% of meaningful information. This inefficiency forces subsequent diffusion or auto-regressive models to process a large number of ineffective tokens. Therefore, we hypothesize that sparse-content segments, such as slow-motion or less dynamic shots, can be represented using fewer latent elements, whereas dense-content segments require more latent elements to capture their complexity.

% Based on this observation, we propose a novel latent space design with Dynamic Frame Rate Control. Specifically, we divide the latent space into temporal chunks, each corresponding to an equal-length segment of the original video. Each chunk is assigned a different frame rate in the latent space based on the richness of its content. Chunks with low frame rates contain fewer latent elements, indicating lower information density, while chunks with high frame rates include more elements to capture complex content. The key challenge lies in efficiently evaluating the richness of each chunk and dynamically adjusting its frame rate accordingly. To address this, we explored several methods and ultimately developed a content information density metric based on inter-frame differences. This metric effectively quantifies the complexity of each chunk while maintaining high computational efficiency, making it suitable for real-time inference.

% The next challenge is to generate a dynamic frame rate latent space from the original video and reconstruct the video from this dynamic latent space. Notably, existing video VAE models, trained on large-scale datasets containing diverse actions and rhythms, are capable of compressing videos into a fixed frame rate latent space and reconstructing them. This inspired us to adapt these static frame rate VAEs to support dynamic frame rates. Specifically, we introduce a Dynamic Frame Rate Encoder Module into the VAE architecture. This module first evaluates the information density of each chunk and dynamically assigns its latent frame rate. It then down-samples intermediate features within the VAE according to the assigned frame rates, generating chunk features with varying frame rates. The subsequent VAE model processes these features as usual, resulting in a dynamic frame rate latent space representation. During decoding, we introduce a Dynamic Frame Rate Decoder Module, which up-samples intermediate features back to a unified frame rate based on the frame rates determined during encoding. This ensures all chunk features are aligned and reconstructs the original video. The modified dynamic VAE is referred to as the DLFR-VAE (Dynamic Latent Frame Rate VAE).

% To validate the effectiveness of DLFR-VAE, we conducted experiments on the HunyuanVideo and OpenSora VAE models. Results demonstrate that DLFR-VAE can efficiently compress videos of varying resolutions into a dynamic frame rate latent space and successfully reconstruct them without the need for additional fine-tuning of the original VAEs. The reconstructed videos exhibit excellent subjective visual quality and objective evaluation scores. Furthermore, compared to static frame rate VAEs, DLFR-VAE significantly reduces the number of latent elements in the latent space, achieving a 20\%-80\% reduction in computation and storage costs. This translates to substantial improvements in the efficiency of subsequent Diffusion or Auto-Regressive models. These results highlight the broad applicability and performance advantages of DLFR-VAE in video generation tasks.

% The key contributions of this work are as follows:

% We establish, for the first time, a quantitative relationship between video content complexity and latent space representation efficiency, proposing a dynamic frame rate decision criterion based on information entropy.
% We design a plug-and-play dynamic frame rate processing module that is compatible with existing VAE architectures (e.g., HunyuanVideo, OpenSora) without requiring fine-tuning.
% We develop a multi-scale feature fusion mechanism to effectively address reconstruction consistency challenges in variable frame rate latent spaces.
% Experimental results on the UCF-101 and Kinetics-400 datasets demonstrate that DLFR-VAE maintains a reconstruction quality of PSNR > 32dB and SSIM > 0.91, while reducing the average number of latent space elements by xxx\% (with peak reductions of up to xxx\%), accelerating diffusion model inference by xx times, and reducing memory usage significantly.

% Version 1
% Motivated by these insights, we propose a novel latent space design featuring a \textbf{Dynamic Latent Frame Rate Scheduler}. 
% This scheduler partitions the latent space into temporal chunks of fixed duration and adaptively assigns a frame rate to each chunk according to its information density. Segments with low information demand fewer latent elements, while complex segments retain a denser token arrangement to preserve fidelity. 
% We adopt an inter-frame difference measure that balances accuracy in quantifying complexity without only slightly marginal computational overhead. Through this efficient content complexity metric, we can enable real-time frame rate adaptation during inference, i.e., Dynamic Latent Frame Rate adjustment in a training-free manner. 

% After we have the efficient metric to determine the frame rate in the latent space, the subsequent technical challenge lies in adapting existing VAE models to support dynamic frame rates while leveraging their pre-trained capabilities, i.e, make the Video-VAEs can effectively process the video chunk with dynamic frame rate in a training-free manner. 
% Modern video VAEs, trained on large-scale datasets with diverse motion patterns, have demonstrated robust performance in fixed-rate latent space compression and reconstruction~\cite{wu2024improved}. Building upon this foundation, we introduce \textbf{\dlfr} (\textbf{Dynamic Latent Frame Rate VAE}) by augmenting conventional VAE architectures with dynamic sampling modules. Specifically, we inject a dynamic downsampling operator into the encoder that adjusts feature dimensions according to content-dependent frame rates, producing chunk features with varying temporal resolutions. The modified encoder processes these variable-rate features through its remaining layers to generate the dynamic latent representation. Correspondingly, we incorporate an adaptive upsampling module in the decoder that restores the features to a consistent frame rate based on the encoding-time schedules, enabling accurate video reconstruction. This architectural modification enables dynamic frame rate control while preserving the pre-trained weights and compression capabilities of the original VAE~\cite{wu2024improved}. This makes our \dlfr~can be served as an plug-and-play module for all the mainstream pretrained video generative models.