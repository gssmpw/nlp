\begin{abstract}
In this paper, we propose the Dynamic Latent Frame Rate VAE (\textbf{\dlfr}), a training-free paradigm that can make use of adaptive temporal compression in latent space.
While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. 
Based on this insight, \dlfr~dynamically adjusts the latent frame rate according to the content complexity.
Specifically, \dlfr~comprises two core innovations: \raisebox{-1.1pt}{\ding[1.1]{182\relax}} a Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and
\raisebox{-1.1pt}{\ding[1.1]{183\relax}} a training-free adaptation mechanism that transform pretrained VAE architectures to dynamic VAE that can process features with variable frame rates.
Our simple but effective \dlfr~can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process. 
% Project link: \href{https://github.com/thu-nics/DLFR-VAE}{github.com/thu-nics/DLFR-VAE}.
% Experiments on UCF-101 and Kinetics-400 demonstrate that DLFR-VAE maintains high reconstruction fidelity, while reducing latent space elements by xxx\%, resulting in zz× faster inference and yy× lower memory usage in downstream generative models.
\end{abstract}