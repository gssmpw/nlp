\section{Experiment}

% 为了验证DLFR-VAE的性能，我们将分别使用了HunyuanVideo-VAE与Open-Sora VAE，将其转化为Dynamic VAE，并在不同视频上测试的视频重建性能。


To evaluate the performance of the proposed \textbf{DLFR-VAE} framework, we applied it to two state-of-the-art pretrained VAE models: \textit{HunyuanVideo VAE} \cite{kong2024hunyuanvideo} and \textit{Open-Sora 1.2 VAE} \cite{zheng2024open}. These models were converted into Dynamic VAEs by incorporating our dynamic frame rate mechanism. We then tested their video reconstruction performance on a diverse set of videos.

\subsection{Video Reconstruction}

We conducted extensive experiments to compare the reconstruction quality of videos processed by the original VAE, Static VAE, and Dynamic VAE under different temporal compression ratios (\textbf{CR}). For evaluation, we used three commonly employed metrics: \textbf{SSIM}, \textbf{PSNR}, and \textbf{LPIPS} \cite{zhang2018unreasonable}. Lower LPIPS values indicate better perceptual quality, while higher SSIM and PSNR values signify better structural and pixel-level fidelity.

\begin{table}[!t]
\centering
\caption{Video reconstruction performance on HunyuanVideo VAE. CR indicates the temporal compression ratio. FVMD\cite{liu2024fr} (Frame-wise Video Motion Deviation) is specifically designed to capture the temporal coherence of inter-frame motion. Compared to the traditional rFVD metric, FVMD is more sensitive to unnatural motion transitions and provides a more accurate assessment of video smoothness and temporal consistency, aligning more closely with human perceptual judgments.} 
\label{tab:hunyuan_results}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}cccccccc@{}}
\toprule
Size/FPS & VAE & CR & SSIM↑ & PSNR↑ & LPIPS↓ & rFVD↓ & FVMD↓ \\ \midrule
240p/15 & Original & 4x & 0.698 & 23.82 & 0.08 & 116 & 623 \\ 
240p/15 & Static & 8x & 0.582 & 22.21 & 0.12 & 268 & 16514 \\ 
240p/15 & Static & 16x & 0.487 & 20.71 & 0.17 & 1244 & 17855 \\ 
240p/15 & Dynamic & 6x & 0.696 & 23.8 & 0.08 & 134 & 687 \\ 
240p/15 & Dynamic & 8x & 0.684 & 23.64 & 0.09 & 278 & 812 \\ 
240p/15 & Dynamic & 12x & 0.631 & 22.77 & 0.12 & 620 & 2345 \\ 
240p/30 & Original & 4x & 0.66 & 22.7 & 0.09 & 108 & 494 \\ 
240p/30 & Static & 8x & 0.59 & 21.7 & 0.11 & 196 & 13192 \\ 
240p/30 & Static & 16x & 0.5 & 20.6 & 0.15 & 1099 & 14777 \\ 
240p/30 & Dynamic & 6x & 0.661 & 22.7 & 0.09 & 127 & 508 \\ 
240p/30 & Dynamic & 8x & 0.655 & 22.6 & 0.09 & 324 & 815 \\ 
240p/30 & Dynamic & 12x & 0.62 & 22 & 0.11 & 571 & 1907 \\ \midrule
540p/15 & Original & 4x & 0.806 & 27.27 & 0.071 & 57 & 331 \\ 
540p/15 & Static & 8x & 0.668 & 24.27 & 0.128 & 218 & 15134 \\ 
540p/15 & Static & 16x & 0.535 & 21.83 & 0.187 & 1281 & 16135 \\ 
540p/15 & Dynamic & 6x & 0.8 & 27.18 & 0.074 & 124 & 392 \\ 
540p/15 & Dynamic & 8x & 0.779 & 26.74 & 0.082 & 170 & 614 \\ 
540p/15 & Dynamic & 12x & 0.695 & 24.51 & 0.119 & 410 & 1766 \\ 
540p/30 & Original & 4x & 0.789 & 26.29 & 0.075 & 60 & 272 \\ 
540p/30 & Static & 8x & 0.682 & 24.05 & 0.118 & 155 & 11784 \\ 
540p/30 & Static & 16x & 0.562 & 21.84 & 0.172 & 1083 & 13222 \\ 
540p/30 & Dynamic & 6x & 0.786 & 26.23 & 0.076 & 131 & 300 \\ 
540p/30 & Dynamic & 8x & 0.773 & 26 & 0.082 & 156 & 526 \\ 
540p/30 & Dynamic & 12x & 0.712 & 24.2 & 0.109 & 485 & 1825 \\ \midrule 
720p/15 & Original & 4x & 0.971 & 38.95 & 0.041 & 8 & 201 \\ 
720p/15 & Static & 8x & 0.806 & 30.71 & 0.109 & 163 & 15180 \\ 
720p/15 & Static & 16x & 0.65 & 25.32 & 0.173 & 1257 & 16345 \\ 
720p/15 & Dynamic & 6x & 0.962 & 37.97 & 0.045 & 34 & 291 \\ 
720p/15 & Dynamic & 8x & 0.927 & 35.47 & 0.057 & 132 & 463 \\ 
720p/15 & Dynamic & 12x & 0.834 & 30.95 & 0.095 & 304 & 1687 \\ 
720p/30 & Original & 4x & 0.97 & 38.95 & 0.041 & 8 & 200 \\ 
720p/30 & Static & 8x & 0.832 & 30.07 & 0.097 & 189 & 15180 \\ 
720p/30 & Static & 16x & 0.684 & 25.68 & 0.162 & 1257 & 16348 \\ 
720p/30 & Dynamic & 6x & 0.964 & 38.07 & 0.04 & 94 & 291 \\ 
720p/30 & Dynamic & 8x & 0.944 & 36 & 0.049 & 156 & 411 \\ 
720p/30 & Dynamic & 12x & 0.867 & 31.32 & 0.081 & 505 & 1687 \\ \bottomrule 
\end{tabular}
}
\vspace{-0.2in}
\end{table}


\begin{table}[!t]
\centering
\caption{Video reconstruction performance on Open-Sora VAE. CR indicates the temporal compression ratio.} 
\label{tab:Open-Sora_results}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{@{}cccccccc@{}}
\toprule
Size/FPS & VAE & CR & SSIM↑ & PSNR↑ & LPIPS↓ & rFVD↓ & FVMD↓ \\ \midrule 
540p/15 & Original & 4x & 0.856 & 29.62 & 0.199 & 435 & 899 \\ 
540p/15 & Static & 8x & 0.746 & 27.31 & 0.245 & 1103 & 14311 \\ 
540p/15 & Static & 16x & 0.659 & 25.71 & 0.27 & 2164 & 15311 \\ 
540p/15 & Dynamic & 6x & 0.849 & 29.39 & 0.199 & 516 & 998 \\ 
540p/15 & Dynamic & 8x & 0.822 & 28.8 & 0.21 & 862 & 1085 \\ 
540p/15 & Dynamic & 12x & 0.762 & 27.44 & 0.229 & 1352 & 3577 \\ 
540p/30 & Original & 4x & 0.866 & 29.91 & 0.181 & 311 & 789 \\ 
540p/30 & Static & 8x & 0.781 & 27.99 & 0.229 & 986 & 13299 \\ 
540p/30 & Static & 16x & 0.702 & 26.23 & 0.257 & 1877 & 14311 \\ 
540p/30 & Dynamic & 6x & 0.864 & 29.9 & 0.177 & 340 & 823 \\ 
540p/30 & Dynamic & 8x & 0.849 & 29.3 & 0.188 & 499 & 1001 \\ 
540p/30 & Dynamic & 12x & 0.793 & 27.92 & 0.213 & 1277 & 3571 \\ \midrule  
720p/15 & Original & 4x & 0.884 & 30.73 & 0.179 & 305 & 799 \\ 
720p/15 & Static & 8x & 0.779 & 28.33 & 0.221 & 1003 & 13656 \\ 
720p/15 & Static & 16x & 0.692 & 26.6 & 0.244 & 2071 & 13699 \\ 
720p/15 & Dynamic & 6x & 0.877 & 30.42 & 0.182 & 551 & 863 \\ 
720p/15 & Dynamic & 8x & 0.85 & 29.78 & 0.192 & 737 & 1070 \\ 
720p/15 & Dynamic & 12x & 0.792 & 28.35 & 0.211 & 1232 & 2946 \\ 
720p/30 & Original & 4x & 0.897 & 31.22 & 0.159 & 320 & 534 \\ 
720p/30 & Static & 8x & 0.818 & 29.22 & 0.207 & 940 & 8848 \\ 
720p/30 & Static & 16x & 0.739 & 27.3 & 0.234 & 2131 & 8849 \\ 
720p/30 & Dynamic & 6x & 0.896 & 31.16 & 0.158 & 519 & 654 \\ 
720p/30 & Dynamic & 8x & 0.882 & 30.51 & 0.168 & 925 & 659 \\ 
720p/30 & Dynamic & 12x & 0.828 & 29.09 & 0.194 & 1345 & 1660 \\ \bottomrule 
\end{tabular}
}
\vspace{-0.2in}
\end{table}

% 为了测试不同的动态特性的视频的影响，我们选取了BVI-HFR dataset，这个数据集涵盖了多种场景类型和运动模式，包括运动模糊和动态纹理。例如，有运动场景、静态背景下的快速运动物体等。我们分别测试了分辨率为540p和720p的视频在15fps、30fps下的性能。

% 原始的VAE的temporal压缩率为4x，为了将Static VAE转换为Dynamic VAE，我们在Encoder中原本的前两个temporal strided conv后，分别加入一个Dynamic downsample operator，在Decoder中原本的最后两个temporal strided conv后，分别加入一个Dynamic upsample operator。
% 为了验证动态根据输入信息进行动态判断下采样比例的好处，我们还设置了一个插入静态下采样和上采样算子的VAE对比对象，它们的temporal压缩率分别为8x和16x。

% 在Table 1和Table 2中，我们展示了将Threshold设置为平均temporal compression ratio为8x和12x的位置时的性能（Section 4.3中有关于threshold的详细ablation实验）。从实验结果可以有以下发现：1. 相比于原始的VAE，直接通过插入Static downsample和upsample operations会大幅度降低VAE的视频重建性能。2. Dynamic VAE相比Static VAE可以大幅度提升视频重建性能。在12x压缩率的情况下甚至比static VAE在8x压缩率的情况下有更好的性能。 3. Dynamic VAE在不同模型、不同分辨率、不同原始视频帧率下均有不错的表现。这证明了DLFR-VAE方法的通用性。

To test the effects of dynamic characteristics in videos, we used the \textbf{BVI-HFR dataset} \cite{mackin2018study}, which includes a variety of scene types and motion patterns, such as dynamic textures and fast-moving objects against static backgrounds. We evaluated videos at two resolutions (\textbf{540p} and \textbf{720p}) and two frame rates (\textbf{15fps} and \textbf{30fps}).

For the original VAE, the temporal compression ratio is 4x. To convert the Static VAE into a Dynamic VAE, we modified the encoder and decoder by introducing Dynamic Downsampling and Dynamic Upsampling operators. For HunyuanVideo VAE, the encoder's first two temporal strided convolution layers were augmented with a Dynamic Downsampling operator, while the decoder's last two temporal strided convolution layers were enhanced with a Dynamic Upsampling operator.
For Open-Sora VAE, we add the dynamic downsampling operators after the 2D VAE encoder, and the dynamic upsampling operators before the 2D VAE decoder. Both are a bilinear sample.

The results in Tables~\ref{tab:hunyuan_results} and~\ref{tab:Open-Sora_results} highlight the following key findings: 1. \textbf{Static VAE significantly reduces reconstruction quality:} Compared to the original VAE, inserting static downsampling and upsampling operators at higher compression ratios (\textbf{8x} and \textbf{16x}) leads to a noticeable drop in SSIM, PSNR, and LPIPS performance. This demonstrates the limitations of static compression in preserving video fidelity. 2. \textbf{Dynamic VAE outperforms Static VAE:} The proposed DLFR-VAE framework consistently achieves higher reconstruction quality than its static counterpart across all resolutions, frame rates, and compression ratios. 
At a compression ratio of \textbf{6x}, the performance of dynamic VAE is comparable to the Original VAE. At a compression ratio of \textbf{12x}, the Dynamic VAE even surpasses the Static VAE at \textbf{8x} in terms of reconstruction metrics, demonstrating the effectiveness of the adaptive approach in optimizing information retention. 3. \textbf{Generalizability of DLFR-VAE:} The performance gains of Dynamic VAE hold across different pretrained models, resolutions, and frame rates. This highlights the generality and robustness of the DLFR-VAE framework, making it suitable for a wide range of video generation tasks.

\subsection{Visualization}

\begin{figure*}[!tb] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.95\linewidth]{figures/visualization.pdf} %ims/xx.png
    \caption{Comparison of the (a) original video, (b) the reconstruction result using the original HunyuanVideo VAE, and (c) the reconstruction result using our proposed DLFR-VAE. The figure illustrates the effectiveness of our dynamic frame rate adaptation in preserving video quality while reducing computational overhead. (d,e) The generated video in the dynamic latent space using the prompt: \textit{Realistic style. A man stands at a quiet bus stop on a sunny afternoon. Then, a bright yellow bus approaches.} and \textit{A woman strolls into a café and approaches a wooden table. She picks up a newspaper and starts reading it}.}
    \label{im_visualization}
\vspace{-0.2in}
\end{figure*}

% 如图所示，我们对DLFR-VAE的重建结果进行展示。可以发现，在摄像机视角快速变化的场景使用了较低的temporal compression ratio，也就是较高的Frame rate；在摄像机视角相对静止位置，使用较高的compression ratio，也就是较低的Frame rate。从主观视觉感受来说，视频质量在整体语意表达和xx，但在细节上有小幅下降。仔细观察可以发现，尤其是16x的压缩率，有一定的视频质量损失，尤其是在一些局部快速变化的物体上，会出现唯影等问题。但需要注意的是，我们是一个Training free的方法。未来可通过VAE finetune或者重头训练等方式缓解，或者通过对画面不同位置使用不同的frame rate来解决局部与整体差异的问题。

To visually demonstrate the effectiveness of our DLFR-VAE framework, we present a comparison between the original video, the reconstruction result from the original HunyuanVideo VAE, and the reconstruction result from DLFR-HunyuanVideo VAE. As shown in Figure~\ref{im_visualization}(a,b,c), our method dynamically adjusts the temporal compression ratio based on the complexity of the video content. Specifically, in scenes with rapid camera movements or high motion, DLFR-VAE employs a lower temporal compression ratio (i.e., a higher frame rate) to preserve temporal details. Conversely, in scenes with relatively static camera views or low motion, a higher compression ratio (i.e., a lower frame rate) is used to reduce computational overhead.

From a subjective visual perspective, the overall semantic content and motion coherence of the reconstructed video are well-preserved. However, there is a slight degradation in fine-grained details, particularly in regions with rapid local motion. For instance, at a compression ratio of 16x, some artifacts such as motion blur or ghosting may appear in areas with fast-moving objects.

It is important to note that DLFR-VAE is a training-free approach, meaning it does not require additional training or fine-tuning of the underlying VAE model. While this makes our method highly efficient and easy to integrate with existing systems \cite{kong2024hunyuanvideo, zheng2024open}, the observed quality degradation in high-compression scenarios suggests potential areas for future improvement. For example, fine-tuning the VAE or training it from scratch with dynamic frame rate adaptation could help mitigate these artifacts. Additionally, a more sophisticated approach could involve applying different frame rates to different regions of the video frame, addressing the trade-off between global and local motion preservation.

\begin{figure}[!tb] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ablation_threshold_ssim.png} %ims/xx.png
\vspace{-0.2in}
    \caption{The influence of different threshold settings on Open-Sora 720p 30fps videos. The threshold grid is made to cover the whole range of content complexity.}
    \label{im_ablation_threshold}
\vspace{-0.2in}
\end{figure}

\subsection{Ablation Study on Threshold}


% \textbf{Dynamic VAE Encoder Structure}

% \textbf{Dynamic VAE Decoder Structure}

% \subsubsection{The Influence of Threshold} 


% 公式12中的threshold会影响结果，为了研究不同的threshold设置对结果的影响，我们将下采样的threshold做了一个grid，并测试了不同threshold下的reconstruction SSIM。结果如图5所示。从图中可以看出，不同的threshold下，Dynamic VAE的性能都比Static的性能要高；同时，dynamic VAE也有清晰明显的帕累托最优曲线，这也显示了dynamic VAE的通用性，可以满足不同压缩率的需求。

The threshold parameter, introduced in Equation~\ref{eq_complexity_threshold}, plays a critical role in determining the performance of the proposed DLFR-VAE. To better understand its impact, we conducted a grid search over different threshold values and evaluated the reconstruction performance. The results are visualized in Figure~\ref{im_ablation_threshold}.

From Figure~\ref{im_ablation_threshold}, the following observations can be drawn: 
1. Across all tested threshold values, the performance of the \textbf{Dynamic VAE} consistently surpasses that of the \textbf{Static VAE}. This demonstrates the robustness and adaptability of the Dynamic VAE framework, as it effectively balances compression and reconstruction quality.
2. The presence of a well-defined Pareto curve \cite{blanchet2022generalized} underscores the versatility of the Dynamic VAE across varying compression levels. This generalizability suggests that the model is capable of adapting to diverse use cases, from low-bitrate streaming to high-fidelity video reconstruction.

% \textbf{Content Complexity Metric}. 
% We tried different kinds of metric to compute the difference between the frames, including MSE SSIM PSNR.

\subsection{Video Generation in Dynamic Latent Space}


% 我们尝试了在不经过训练的情况下，在Dynamic Frame Rate Latent Space中进行视频，由于当前图像生成网络几乎都是DiT架构，我们只需要调整positional encoding即可实现不同的frame rate的token在同时生成。在生成之前，我们需要人工设置每一段生成的帧率，我们使用了两种帧率设置，第一种是前面高帧率后面低帧率，第二种是前面低帧率后面高帧率。我们在图8中展示了生成的结果。结果显示，即使不经过训练，DiT神经网络可以在Dynamic Frame Rate Latent Space中生成正常的视频。

% 当然，本文的主要内容是如何构建一个Dynamic的Latent Space，而不是探索如何在这个空间中进行生成，这里只是展示一个可能性。我们认为在未来，可以根据用户的prompt，由神经网络自动判断每一段视频应该生成多少帧率的视频。或者从头训练在Dynamic空间中的视频生成模型。

% \subsection{}

In this section, we explore the feasibility of generating videos directly within a \textit{Dynamic Frame Rate Latent Space} without any additional training. Given that contemporary image and video generation models are predominantly based on the Diffusion Transformer (DiT) architecture \cite{peebles2023scalable,esser2024scaling}, adapting these models to work in a dynamic latent space requires only minor adjustments. 

% To get the latent frame rates for different temporal segments of the generated video, we first generate a video with a very small number of denoising steps (such as 3 steps) and then we using the DLFR scheduler to determine the frame ratios of video segments. Next we use the frame rate configuration to generate the videos in the dynamic frame rate latent space. Finally, we use our DLFR-VAE decoder to convert the latents into video.


To obtain the latent frame rates for different temporal segments of the generated video, we follow a multi-step process. First, we generate a preliminary video using a minimal number of denoising steps (e.g., 5 steps), which has a 10\% computation cost of generating the video with 50 steps. Although this preliminary video has very low quality due to the limited number of steps, it still provides a clear indication of whether each video segment contains fast or slow motion. Based on this information, we then use the DLFR scheduler to assess the content complexity of each segment and determine the appropriate frame rate ratios. With these frame rate configurations, we proceed to generate the final videos in the dynamic frame rate latent space. To handle latent frames with varying frame rates, where a frame can represent either a short or long duration of time, we adjust the positional embedding for RoPE (Rotary Position Embedding) in DiT. Because tokens in the compressed latent space that each token represents a longer duration, we resample the cosine and sine parameters for RoPE~\footnote{See Appendix for detail of the positional embedding parameter generation.}. Finally, we apply our DLFR-VAE decoder to convert the latent representations into the final video output.

\begin{table}[tb]
\centering
\caption{Comparing the quality of video generation in raw latent space and our DLFR latent space. The end-to-end latency includes the preliminary video generation, dynamic frame rate schedule and the video generation in the compressed latent space.}
\label{tab:gen_result}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}cccccc@{}}
\toprule
     & CLIPT  & CLIPSIM & VQA   & FLICKER & Latency (s) \\ \midrule
raw  & 0.9991 & 0.1869  & 97.56 & 0.9781  & 186         \\
ours & 0.9994 & 0.1874  & 97.44 & 0.9852  & 95          \\ \bottomrule
\end{tabular}
}
\end{table}

In Table~\ref{tab:gen_result}, we present the results of generating videos at 480p resolution with 97 frames, based on a subset of prompts collected from VBench \cite{huang2024vbench} with 21 prompts. To evaluate the alignment between text and video, we use CLIPSIM \cite{wu2021godiva}. For assessing temporal consistency, we employ CLIP-Temp \cite{esser2023structure}. Additionally, we use VQA \cite{wu2022fastvqa} to evaluate the aesthetic quality of the generated videos. We also measure temporal consistency, focusing on local and high-frequency details, using the temporal flickering score \cite{huang2024vbench}. Our observations indicate that our approach achieves scores comparable to raw video generation while offering a 2x speedup.

In Figure~\ref{im_visualization}(d,e), we show two videos with 1. High-to-Low Latent Frame Rate: Higher frame rates were applied to the initial temporal segments, followed by lower frame rates in subsequent segments.
2. Low-to-High Latent Frame Rate: Lower frame rates were applied to the earlier segments, gradually increasing to higher frame rates in later segments. Remarkably, even without retraining, the DiT-based neural network was able to produce coherent and visually plausible videos in the dynamic latent space. 

% Since our method focuses on transforming static VAE into dynamic VAE, we did not explore the generation process in depth. This experiment reveals several interesting possibilities for future research: 1. Frame Rate Prediction: Future models could leverage prompts or contextual information to automatically infer the appropriate frame rate for each segment. 2. Training in Dynamic Latent Space: While our approach demonstrates that pretrained generative models can produce videos in a dynamic latent space, training models within this space could yield optimized results. 


