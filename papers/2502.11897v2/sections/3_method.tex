
\section{Method}

\subsection{Dynamic Frame Rate Latent Space}

% 根据上面的分析，我们可以知道，不同的视频片段需要的frame rate不同。因此，我们提出一种动态的Latent Space，具体定义：我们将输入的视频按照帧分段，例如每64帧一段。每段视频都对应于Latent Space的一段区域，这段区域的frame rate动态根据输入来决定。

% 从频谱图中可以看到，一些高频信息的振幅非常小，对结果影响很小，我们允许将Latent space中部分高频信号去除，以降低帧率，降低开销。因此，我们有：

Based on the preceding analysis, we observe that different video segments require varying frame rates. To address this, we propose the dynamic frame rate latent space. Specifically, we segment the input video into fixed-length intervals, such as 1-second segments. Each segment corresponds to a region in the latent space, where the frame rate is dynamically determined based on the temporal characteristics of the segment.

Let the input video be divided into \( M \) segments, denoted as \( \{S_1, S_2, \dots, S_M\} \), where each segment \( S_i \) consists of \( N \) consecutive frames. For each segment \( S_i \), the corresponding latent representation is \( z_i(t) \). The temporal frequency spectrum of \( z_i(t) \), denoted as \( Z_i(f) \), is computed as:

\[
Z_i(f) = \int_{t_i}^{t_i + N T} z_i(t) e^{-j 2 \pi f t} \, dt.
\]

where \( T \) is the sampling interval of the original video. From the frequency spectrum, we observe that the amplitude of certain high-frequency components is negligible, contributing minimally to the overall signal fidelity. Therefore, we allow for the removal of such high-frequency components in the latent space to reduce the frame rate and computational overhead. Specifically, for each segment \( S_i \), we determine the effective maximum frequency \( f_{\text{eff}, i} \) as the highest frequency component with an amplitude above a predefined threshold \( \epsilon \):

\[
f_{\text{eff}, i} = \max \{ f \mid |Z_i(f)| \geq \epsilon \}.
\]

The frame rate \( F_{s,i}' \) for the latent representation of segment \( S_i \) is then dynamically adjusted according to the Nyquist-Shannon sampling theorem:

\[
F_{s,i}' = 2 f_{\text{eff}, i}.
\]

This adaptive frame rate allocation ensures that the latent space representation retains the essential temporal information while minimizing redundancy.

\subsection{DLFR Scheduler}

In practice, we cannot use any frequencies on the latent space. Therefore, the temporal complexity of latent signals is categorized into \( N \) classes, where each class \( k \in \{1, 2, \dots, N\} \) has a corresponding effective frequency \( f_{\text{eff},k}' \). Let \( C_k \) denote the set of all video segments belonging to class \( k \). For a given segment \( S_i \), the frame rate allocation \( F_{s,i}' \) is determined by its class membership:

\[
F_{s,i}' = 2 f_{\text{eff},k}', \quad \text{if } S_i \in C_k.
\]

This can be formally expressed as:

\[
F_{s,i}' = \sum_{k=1}^{N} 2 f_{\text{eff},k}' \cdot \mathbb{I}_{C_k}(S_i),
\]

where \( \mathbb{I}_{C_k}(S_i) \) is the indicator function. This adaptive strategy ensures that the latent space signals are sampled with a small loss of information while minimizing the required frame rate.

In practice, it is also difficult to compute the indicator \( \mathbb{I}_{C_k}(S_i) \), because we cannot directly obtain the latent representation of the time-contiguous \( z_i(t) \) and its frequency spectrum.
Therefore, we propose to use a heuristic approximation to this function. 

% 这里还要再做做实验确定下
DEFINE of Content Information. We explore different metrics to vague the information in a video clip. We propose xxx $C(\hat{X})=A+B+C$. $\hat{X}\in R^{B \times H\times W}$ is the discrete input video, which is what we can get from digital camera. xxxx 


% \[
% \mathbb{I}_{C_k}(S_i) = 
% \begin{cases} 
% 1 & \text{if } S_i \in C_k, \\
% 0 & \text{otherwise}.
% \end{cases}
% \]




% we cannot get the real the temporal space of input video is quantized, 
% 如何评价，提出Content Information

% 具体而言，我们引入了一个动态帧率Scheduler（Dynamic Frame Rate Encoder Scheduler）。该模块首先评估视频每个chunk的信息密度，并动态分配对应的潜空间帧率。

\subsection{Transform Static VAE to Dynamic VAE}

% 如何从原始视频生成动态帧率的潜空间，以及如何从动态潜空间还原回原始视频。我们注意到：以往的视频VAE模型通过在大规模包含各种动作与节奏的视频数据上训练，已经具备了将视频压缩到固定帧率潜空间并还原回原始视频的能力。这一特性启发我们对现有的静态帧率VAE模型进行改造，扩展出动态帧率的潜空间生成能力。我们需要一个运行的过程中动态根据输入scheduler的变化而变化的模型（Encoder模型的输出维度动态变化，Decoder模型的输入维度动态变化），因此我们需要将静态模型转换为一个动态模型。

% 具体来说，我们在VAE的Encoder中插入一个dynamic down下采样器我们在VAE模型中的合适位置根据分配的帧率对VAE中间特征进行动态下采样，从而得到包含不同帧率的chunk特征。在该module之后的VAE模型以相同方式推理具有不同的chunk帧率的features，最终输出的结果就是在dynamic frame rate的latent space。在解码过程中，我们在Deocoder模型中的合适位置根据编码时确定的帧率对中间特征进行上采样，将所有chunk特征恢复到一致的帧率，最终重建出原始视频。我们将其称为DLFR-VAE（Dynamic Latent Frame Rate VAE）

To generate a dynamic frame rate latent space from raw video and reconstruct the original video from this dynamic latent space, we build upon the capabilities of existing video Variational Autoencoders (VAEs). Previous video VAE models, trained on large-scale datasets containing diverse actions and rhythms, have demonstrated the ability to compress videos into a fixed frame rate latent space and reconstruct the original video. This observation inspires us to extend the static frame rate VAE framework to support dynamic frame rate latent space generation.


Let the input video be represented as a sequence of frames \( \{x_1, x_2, \dots, x_T\} \), where \( T \) is the total number of frames. The VAE encoder \( E \) maps the input video into a latent space. To introduce dynamic frame rates, we insert a \textit{dynamic downsampler} into the encoder. This downsampler adjusts the frame rate of intermediate features based on a schedule provided by a scheduler. 

Formally, let \( F_{s,i}' \) denote the frame rate assigned to the \( i \)-th segment \( S_i \) by the scheduler. The dynamic downsampler operates on the intermediate features \( h_i \) of the encoder, producing downsampled features \( h_i' \) as follows:

\[
h_i' = \text{Downsample}(h_i, F_{s,i}'),
\]

where \( \text{Downsample}(\cdot) \) is a function that reduces the temporal resolution of \( h_i \) to match the target frame rate \( F_{s,i}' \). The downsampled features \( h_i' \) are then processed by the subsequent layers of the encoder, which infer the latent representation \( z_i \) for each segment \( S_i \):

\[
z_i = E_{\text{post}}(h_i'),
\]

where \( E_{\text{post}} \) represents the remaining layers of the encoder after the dynamic downsampler. The final latent space representation \( z \) is a sequence of segment-wise latent codes \( \{z_1, z_2, \dots, z_M\} \), each corresponding to a segment with a dynamically adjusted frame rate.

During decoding, the VAE decoder \( D \) reconstructs the original video from the dynamic frame rate latent space. To ensure consistency, we insert a \textit{dynamic upsampler} into the decoder at the corresponding location where the dynamic downsampler was applied in the encoder. The upsampler restores the temporal resolution of the intermediate features based on the frame rates determined during encoding.

For each latent code \( z_i \), the decoder first processes it to produce intermediate features \( h_i'' \):

\[
h_i'' = D_{\text{pre}}(z_i),
\]

where \( D_{\text{pre}} \) represents the initial layers of the decoder. The dynamic upsampler then adjusts the temporal resolution of \( h_i'' \) to match the original frame rate \( F_s \):

\[
h_i''' = \text{Upsample}(h_i'', F_s),
\]

where \( \text{Upsample}(\cdot) \) is a function that increases the temporal resolution of \( h_i'' \) to the target frame rate \( F_s \). The upsampled features \( h_i''' \) are further processed by the remaining layers of the decoder to reconstruct the original video segment \( \hat{S}_i \):

\[
\hat{S}_i = D_{\text{post}}(h_i''').
\]

The final reconstructed video \( \hat{V} \) is obtained by concatenating all the reconstructed segments \( \{\hat{S}_1, \hat{S}_2, \dots, \hat{S}_M\} \).
