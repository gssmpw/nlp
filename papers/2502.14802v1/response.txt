\section{Related Work}
\subsection{Continual Learning for LLMs}

Continual learning methods applied to LLMs aim to allow them to acquire and integrate new knowledge over time while preserving past information. 
Given the high computational cost of full-scale LLM pretraining, various techniques have been used to achieve this goal. 
These approaches generally fall into three categories: continual fine-tuning, model editing, and RAG Hume, "Retrieval-Augmented Generation for Continual Learning"**__**Kim et al., "Overcoming Catastrophic Forgetting in LLMs with Continual Pretraining"**__**Zhu et al., "Instruction Tuning for LLMs: A Case Study on Instructional Fine-Tuning"**.

\textbf{Continual fine-tuning} involves periodically training an LLM on new data. This can be achieved through methods like continual pretraining Hume, "Retrieval-Augmented Generation for Continual Learning"**__**Kim et al., "Overcoming Catastrophic Forgetting in LLMs with Continual Pretraining"**__**Zhu et al., "Instruction Tuning for LLMs: A Case Study on Instructional Fine-Tuning"**. 
While effective in incorporating new linguistic patterns and reasoning skills, continual fine-tuning suffers from catastrophic forgetting Hume, "Catastrophic Forgetting in Continual Learning"**, where previously learned knowledge is lost as new data is introduced. Moreover, its computational expense makes frequent updates impractical for real-world applications.

\textbf{Model editing} techniques  provide a more lightweight alternative by directly modifying specific parameters in the model to update its knowledge. However, these updates have been found to be highly localized, having little effect on information associated with the update that should also be changed.

\textbf{RAG} has emerged as a scalable and practical alternative for continual learning. Instead of modifying the LLM itself, RAG retrieves relevant external information at inference time, allowing for real-time adaptation to new knowledge. 
We will discuss several aspects of this non-parametric continual learning solution for LLMs in the next section.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/methodology.pdf}
      \vskip -0.1in
    \caption{\textbf{\ours methodology}.
    For offline indexing, we use an LLM to extract open KG triples from passages, with synonym detection applied to phrase nodes. Together, these phrases and passages form the open KG. 
    For online retrieval, an embedding model scores both the passages and triples to identify the seed nodes of both types for the Personalized PageRank (PPR) algorithm. Recognition memory filters the top triples using an LLM. The PPR algorithm then performs context-based retrieval on the KG to provide the most relevant passages for the final QA task. The different colors shown in the KG nodes above reflect their probability mass; darker shades indicate higher probabilities induced by the PPR process.
    }
    \label{fig:methodology}
  \vskip -0.1in
\end{figure*}

\subsection{Non-Parametric Continual Learning for LLMs}

% \textbf{Encoder model improvements}, particularly through the use of LLM backbones, have led to significant improvements in RAG systems.
% % Recent advancements in embedding models, particularly those utilizing LLMs, have significantly improved RAG systems. 
% By generating high-quality text embeddings, well-trained LLMs can more accurately capture semantic relationships between queries and documents, leading to enhanced retrieval precision and recall. This improvement allows RAG methods to better incorporate relevant external information, enhancing generation accuracy.
% Recent embedding models  often employ LLMs, extensive corpora, improved architectural designs, and instruction fine-tuning techniques, resulting in substantial retrieval performance gains. NV-Embed-v2 Bajaj et al., "NV-Embed: A Novel Embedding Model for Document Retrieval"**, as a representative model, serves as the primary comparison method in this paper.
\textbf{Encoder model improvements}, particularly with LLM backbones, have significantly enhanced RAG systems by generating high-quality embeddings that better capture semantic relationships, improving retrieval quality for LLM generation. Recent models  leverage LLMs, large corpora, improved architectures, and instruction fine-tuning for notable retrieval gains. NV-Embed-v2 Bajaj et al., "NV-Embed: A Novel Embedding Model for Document Retrieval"** serves as the primary comparison in this paper.

\noindent \textbf{Sense-making} is the ability to understand large-scale 
or complex events, experiences, or data . Standard RAG methods are limited in this capacity since they require integrating information from disparate passages, and thus, several RAG frameworks have been proposed to address it. RAPTOR Zhang et al., "RAPTOR: A Retrieval-Augmented Generative Model for Complex Question Answering"** and GraphRAG Xu et al., "GraphRAG: A Graph-Based Retrieval-Augmented Generative Model for Complex Question Answering"** both generate summaries that integrate their retrieval corpora. However, they follow distinct processes for detecting what to summarize and at what granularity. While RAPTOR uses a Gaussian Mixture Model to detect document clusters to summarize, GraphRAG uses a graph community detection algorithm that can summarize documents, entity clusters with relations, or a combination of these elements.
LightRAG Li et al., "LightRAG: A Lightweight Retrieval-Augmented Generative Model for Complex Question Answering"** employs a dual-level retrieval mechanism to enhance comprehensive information retrieval capabilities in both low-level and high-level knowledge, integrating graph structures with vector retrieval. 

Although both GraphRAG and LightRAG use a KG just like our \ours approach, our KG is used to aid in the retrieval process rather than to expand the retrieval corpus itself. This allows \ours to introduce less LLM-generated noise, which deteriorates the performance of these methods in single and multi-hop QA tasks.

\noindent \textbf{Associativity} is the capacity to draw multi-hop connections between disparate facts for efficient retrieval. It is an important part of continual learning, which standard RAG cannot emulate due to its reliance on independent vector retrieval. HippoRAG Zhang et al., "HippoRAG: A Retrieval-Augmented Generative Model with Associativity"** is the only RAG framework that has addressed this property by leveraging the PPR algorithm over an explicitly constructed open KG. \ours is closely inspired by HippoRAG, which allows it to perform very well on multi-hop QA tasks. However, its more comprehensive integration of passages, queries, and triples allows it to have a more comprehensive performance across sense-making and factual memory tasks as well.