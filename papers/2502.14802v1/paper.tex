%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[table,xcdraw]{xcolor}
\definecolor{lightgrey}{HTML}{E7E7E7}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\newcommand{\bjg}[1]{{\color{blue}#1}}
\usepackage{xspace}
% Public version
\newcommand{\hipporag}{HippoRAG\xspace}
\newcommand{\ours}{HippoRAG 2\xspace}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{\ours: Continual Knowledge Learning for Large Language Models}
\icmltitlerunning{From RAG to Memory: Non-Parametric Continual Learning for Large Language Models}

\begin{document}

\twocolumn[
\icmltitle{From RAG to Memory: Non-Parametric Continual Learning for \\ Large Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bernal Jim\'{e}nez Guti\'{e}rrez}{equal,yyy}
\icmlauthor{Yiheng Shu}{equal,yyy}
\icmlauthor{Weijian Qi}{yyy}
\icmlauthor{Sizhe Zhou}{zzz}
\icmlauthor{Yu Su}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{The Ohio State University, Columbus, OH, USA}
\icmlaffiliation{zzz}{University of Illinois Urbana-Champaign, IL, USA}

\icmlcorrespondingauthor{Bernal Jim\'{e}nez Guti\'{e}rrez}{jimenezgutierrez.1@osu.edu}
\icmlcorrespondingauthor{Yiheng Shu}{shu.251@osu.edu} 
\icmlcorrespondingauthor{Yu Su}{su.809@osu.edu} 

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. 
Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. 
However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. 
Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity.
However, their performance on more basic factual memory tasks drops considerably below standard RAG.
We address this unintended deterioration and propose \ours, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. 
% \ours combines the Personalized PageRank algorithm used by HippoRAG with deeper passage integration of other sense-making methods and effective online use of an LLM to move RAG systems closer to the effectiveness of human long-term memory.
\ours builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. 
This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7\% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities.
This work paves the way for non-parametric continual learning for LLMs. 
Our code and data will be released at \url{https://github.com/OSU-NLP-Group/HippoRAG}.

% Additionally, we note that \ours offers an efficient and simple-to-use solution that can be robustly integrated with any embedding and LLM models, providing a wide degree of usage flexibility.


\end{abstract}

\section{Introduction}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/intro.pdf}
    \caption{
    Evaluation of continual learning capabilities across three key dimensions: factual memory (NaturalQuestions, PopQA), sense-making (NarrativeQA), and associativity (MuSiQue, 2Wiki, HotpotQA, and LV-Eval). 
    \ours surpasses other methods across all benchmark categories, bringing it one step closer to a true long-term memory system.
    }
    \label{fig:introduction}
\end{figure*}

In an ever-evolving world, the ability to continuously absorb, integrate, and leverage knowledge is one of the most important features of human intelligence.
From lawyers navigating shifting legal frameworks to researchers tracking multifaceted scientific progress, much of our productivity relies on this incredible capacity for continual learning.
It is imperative for AI systems to approximate this capability in order to become truly useful human-level assistants.
% -long a central pursuit of the continual learning community.

In recent years, large language models (LLMs) have made remarkable progress in many aspects of human intelligence.
However, efforts to endow these models with our evolving long-term memory capabilities have faced significant challenges in both fully absorbing new knowledge \cite{zhong-etal-2023-mquake, hoelscher-obermaier-etal-2023-detecting} and avoiding catastrophic forgetting \cite{cohen-etal-2024-evaluating, gu-etal-2024-model}, due to the complex distributional nature of their parametric knowledge.
Retrieval-augmented generation (RAG) has emerged as a way to circumvent these obstacles and allow LLMs to access new information in a \textit{non-parametric} fashion without altering an LLM's parametric representation.
Due to their simplicity and robustness \cite{zhong-etal-2023-mquake, xie2024adaptive}, RAG has quickly become the \textit{de facto} continual learning solution for production LLM systems.
However, their reliance on simple vector retrieval results in the inability to capture two vital aspects of our interconnected long-term memory system: \textbf{sense-making} (\citet{klein2006making}; the ability to interpret larger, more complex, or uncertain contexts) and \textbf{associativity} (\citet{suzuki2005associative}; the capacity to draw multi-hop connections between disparate pieces of knowledge).

Several RAG frameworks that engage an LLM to explicitly structure its retrieval corpus have been recently proposed to address these limitations. 
To enhance sense-making, such \textit{structure-augmented} RAG methods allow an LLM to either generate summaries \cite{graphrag, raptor, chen2023walkingmemorymazecontext} or a knowledge graph (KG) structure \cite{lightrag} to link groups of disparate but related passages, thereby improving the RAG system's ability to understand longer and more complex discourse such as long stories.
To address the associativity gap, the authors of HippoRAG \cite{hipporag} use the Personalized PageRank algorithm \cite{david02topic} and an LLM's ability to automatically construct a KG and endow the retrieval process with multi-hop reasoning capabilities.

Although these methods demonstrate strong performance in both of these more challenging memory tasks, bringing RAG truly closer to human long-term memory requires robustness across simpler memory tasks as well.
% Although each of these methods demonstrate strong performance on one of these challenging memory tasks or the other, bringing RAG truly closer to human long-term memory requires a system that can not only mimic the strengths of standard RAG but also achieves robust performance on both of these complex tasks.
In order to understand whether these systems could achieve such robustness, we conduct comprehensive experiments that not only simultaneously evaluate their associativity and sense-making capacity through multi-hop QA and large-scale discourse understanding, but also test their \textbf{factual memory} abilities via simple QA tasks, which standard RAG is already well-equipped to handle.

As shown in Figure \ref{fig:introduction}, our evaluation reveals that all previous structure-augmented methods underperform against the strongest embedding-based RAG methods available on all three benchmark types.
Perhaps unsurprisingly, we find that each method type experiences the largest performance decay in tasks outside its own experimental setup. For example, HippoRAG's performance drops most on large-scale discourse understanding due to its lack of query-based contextualization, while RAPTOR's performance deteriorates substantially on the simple and multi-hop QA tasks due to the noise introduced into the retrieval corpora by its LLM summarization mechanism.

In this work, we leverage this experimental setting to help us address the robustness limitations of these innovative approaches while avoiding the pitfalls of focusing too narrowly on just one task.
Our proposed method, \ours, leverages the strength of HippoRAG's OpenIE and Personalized PageRank (PPR) methodologies while addressing its query-based contextualization limitations by integrating passages into the PPR graph search process, involving queries more deeply in the selection of KG triples as well as engaging an LLM in the online retrieval process to recognize when retrieved triples are irrelevant.

Through extensive experiments, we find that this design provides \ours with consistent performance improvements over the most powerful standard RAG methods across the board.
More specifically, our approach achieves an average $7$ point improvement over standard RAG in associativity tasks while showing no deterioration and even slight improvements in factual memory and sense-making tasks. 
Furthermore, we show that our method is robust to different retrievers as well as to the use of strong open-source and proprietary LLMs, allowing for a wide degree of usage flexibility.
All of these results suggest that \ours is a promising step in the development of a more human-like non-parametric continual learning system for LLMs.

\section{Related Work}

\subsection{Continual Learning for LLMs}

Continual learning methods applied to LLMs aim to allow them to acquire and integrate new knowledge over time while preserving past information. 
Given the high computational cost of full-scale LLM pretraining, various techniques have been used to achieve this goal. 
These approaches generally fall into three categories: continual fine-tuning, model editing, and RAG \cite{shi2024continual}.

\textbf{Continual fine-tuning} involves periodically training an LLM on new data. This can be achieved through methods like continual pretraining \cite{lifelong}, instruction tuning \cite{citb}, and alignment fine-tuning \cite{copr}. 
While effective in incorporating new linguistic patterns and reasoning skills, continual fine-tuning suffers from catastrophic forgetting \cite{huang24mitigating}, where previously learned knowledge is lost as new data is introduced. Moreover, its computational expense makes frequent updates impractical for real-world applications.

\textbf{Model editing} techniques \cite{yao23editing} provide a more lightweight alternative by directly modifying specific parameters in the model to update its knowledge. However, these updates have been found to be highly localized, having little effect on information associated with the update that should also be changed.

\textbf{RAG} has emerged as a scalable and practical alternative for continual learning. Instead of modifying the LLM itself, RAG retrieves relevant external information at inference time, allowing for real-time adaptation to new knowledge. 
We will discuss several aspects of this non-parametric continual learning solution for LLMs in the next section.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/methodology.pdf}
      \vskip -0.1in
    \caption{\textbf{\ours methodology}.
    For offline indexing, we use an LLM to extract open KG triples from passages, with synonym detection applied to phrase nodes. Together, these phrases and passages form the open KG. 
    For online retrieval, an embedding model scores both the passages and triples to identify the seed nodes of both types for the Personalized PageRank (PPR) algorithm. Recognition memory filters the top triples using an LLM. The PPR algorithm then performs context-based retrieval on the KG to provide the most relevant passages for the final QA task. The different colors shown in the KG nodes above reflect their probability mass; darker shades indicate higher probabilities induced by the PPR process.
    }
    \label{fig:methodology}
  \vskip -0.1in
\end{figure*}

\subsection{Non-Parametric Continual Learning for LLMs}

% \textbf{Encoder model improvements}, particularly through the use of LLM backbones, have led to significant improvements in RAG systems.
% % Recent advancements in embedding models, particularly those utilizing LLMs, have significantly improved RAG systems. 
% By generating high-quality text embeddings, well-trained LLMs can more accurately capture semantic relationships between queries and documents, leading to enhanced retrieval precision and recall. This improvement allows RAG methods to better incorporate relevant external information, enhancing generation accuracy.
% Recent embedding models \cite{gte,gritlm,nvembedv2} often employ LLMs, extensive corpora, improved architectural designs, and instruction fine-tuning techniques, resulting in substantial retrieval performance gains. NV-Embed-v2 \cite{nvembedv2}, as a representative model, serves as the primary comparison method in this paper.
\textbf{Encoder model improvements}, particularly with LLM backbones, have significantly enhanced RAG systems by generating high-quality embeddings that better capture semantic relationships, improving retrieval quality for LLM generation. Recent models \cite{gte,gritlm,nvembedv2} leverage LLMs, large corpora, improved architectures, and instruction fine-tuning for notable retrieval gains. NV-Embed-v2 \cite{nvembedv2} serves as the primary comparison in this paper.

\noindent \textbf{Sense-making} is the ability to understand large-scale 
or complex events, experiences, or data \cite{koli-etal-2024-sensemaking}. Standard RAG methods are limited in this capacity since they require integrating information from disparate passages, and thus, several RAG frameworks have been proposed to address it. RAPTOR \cite{raptor} and GraphRAG \cite{graphrag} both generate summaries that integrate their retrieval corpora. However, they follow distinct processes for detecting what to summarize and at what granularity. While RAPTOR uses a Gaussian Mixture Model to detect document clusters to summarize, GraphRAG uses a graph community detection algorithm that can summarize documents, entity clusters with relations, or a combination of these elements.
LightRAG \cite{lightrag} employs a dual-level retrieval mechanism to enhance comprehensive information retrieval capabilities in both low-level and high-level knowledge, integrating graph structures with vector retrieval. 

Although both GraphRAG and LightRAG use a KG just like our \ours approach, our KG is used to aid in the retrieval process rather than to expand the retrieval corpus itself. This allows \ours to introduce less LLM-generated noise, which deteriorates the performance of these methods in single and multi-hop QA tasks.

\noindent \textbf{Associativity} is the capacity to draw multi-hop connections between disparate facts for efficient retrieval. It is an important part of continual learning, which standard RAG cannot emulate due to its reliance on independent vector retrieval. HippoRAG \cite{hipporag} is the only RAG framework that has addressed this property by leveraging the PPR algorithm over an explicitly constructed open KG. \ours is closely inspired by HippoRAG, which allows it to perform very well on multi-hop QA tasks. However, its more comprehensive integration of passages, queries, and triples allows it to have a more comprehensive performance across sense-making and factual memory tasks as well.

\section{\ours}
\label{sec:long-term}

\begin{table*}[tb]
  \centering
  \small
    \caption{Dataset statistics}
    \vskip 0.1in
  \begin{tabular}{lrrrrrrr}
  \toprule
   & NQ & PopQA & MuSiQue & 2Wiki & HotpotQA & LV-Eval & NarrativeQA   \\ \midrule
  Num of queries & $1,000$ & $1,000$ & $1,000$ & $1,000$ & $1,000$ & $124$ & $293$ \\
  Num of passages & $9,633$ & $8,676$ & $11,656$ & $6,119$ & $9,811$ & $22,849$ & $4,111$ \\
  \bottomrule
  \end{tabular}
  \label{table:dataset statistics}
  % \vskip -0.1in
\end{table*}

\subsection{Overview}
% Give a figure for the overview method.

HippoRAG \cite{hipporag} is a neurobiologically inspired long-term memory framework for LLMs, with each component designed to emulate aspects of human memory. The framework consists of three primary components: the artificial neocortex (LLM), the parahippocampal region (PHR encoder), and the artificial hippocampus (open KG). These components collaborate to replicate the interactions observed in human long-term memory.

For HippoRAG offline indexing, an LLM processes passages into KG triples, which are then incorporated into the artificial hippocampal index. Meanwhile, the PHR is responsible for detecting synonymy to interconnect information.
For HippoRAG online retrieval, the LLM neocortex extracts named entities from a query, while the PHR encoder link these entities to the hippocampal index. 
Then, the Personalized PageRank (PPR) algorithm on the KG is conducted for context-based retrieval.
Although HippoRAG seeks to construct memory from non-parametric RAG, its effectiveness is hindered by a critical flaw: an entity-centric approach that causes context loss during both indexing and inference, as well as difficulties in semantic matching.
% Although HippoRAG attempts to build memory from non-parametric RAG, a key flaw hinders its effectiveness: its entity-centric approach, which leads to issues of context loss in both the indexing and inference stages and semantic matching difficulties.

Built on the neurobiologically inspired long-term memory framework proposed in HippoRAG \cite{hipporag}, the structure of \ours follows a similar two-stage process: offline indexing and online retrieval, as shown in Figure \ref{fig:methodology}. Additionally, however, \ours introduces several key refinements that improve its alignment with human memory mechanisms:
1) It seamlessly integrates conceptual and contextual information within the open KG, enhancing the comprehensiveness and atomicity of the constructed index (\S\ref{subsec:index_integration}).
2) It facilitates more context-aware retrieval by leveraging the KG structure beyond isolated KG nodes (\S\ref{subsec:query_to_triple}).
3) It incorporates recognition memory to improve seed node selection for graph search (\S\ref{subsec:recognition_memory}).
In the following sections, we introduce the pipeline in more detail and elaborate on each of these refinements.


\noindent \textbf{Offline Indexing.}
1) \ours leverages an LLM to extract triples from each passage and integrates them into a schema-less open KG. We call the subject or object of these triples \textbf{phrase} and the edge connecting them \textbf{relation edge}.
2) Next, the encoder identifies synonyms by evaluating phrase pairs within the KG, detecting those with vector similarity above a predefined threshold, and adding \textbf{synonym edge} between such pair.
This process enables the KG to link synonyms across different passages, facilitating the integration of both old and new knowledge during learning.
3) Finally, this phrase-based KG is combined with the original passages, allowing the final open KG to incorporate both conceptual and contextual information (\S\ref{subsec:index_integration}).

\noindent \textbf{Online Retrieval.}
1) The query is linked to relevant triples and passages using the encoder, identifying potential seed nodes for graph search (\S \ref{subsec:query_to_triple}).
2) During triple linkage, the recognition memory functions as a filter, ensuring only relevant triples are retained from the retrieved set (\S \ref{subsec:recognition_memory}).
3) Given seed nodes, the PPR algorithm is then applied for context-aware retrieval, refining the linking results to retrieve the most relevant passages.
4) Finally, the retrieved passages serve as contextual inputs for the final QA task.
Next, we describe each of the improvements in \ours in more detail.
% by \ours.

\subsection{Dense-Sparse Integration}
\label{subsec:index_integration}

The nodes in the \hipporag KG primarily consist of phrases describing concepts, which we refer to as \textbf{phrase nodes} in this paper. This graph structure introduces limitations related to the \textit{concept-context tradeoff}.
Concepts are concise and easily generalizable but often entail information loss. 
In contrast, context provide specific circumstances that shape the interpretation and application of these concepts, enriching semantics but increasing complexity. 
However, in human memory, concepts and contexts are intricately interconnected.
The dense and sparse coding theory offers insights into how the brain represents and processes information at different granularities \cite{Beyeler2019}. 
Dense coding encodes information through the simultaneous activation of many neurons, resulting in a distributed and \textit{redundant} representation. 
Conversely, sparse coding relies on minimal neural activation, engaging only a small subset of neurons to enhance \textit{efficiency} and storage \textit{compactness}.

Inspired by the dense-sparse integration observed in the human brain, we treat the phrase node as a form of sparse coding for the extracted concepts, while incorporating dense coding into our KG to represent the context from which these concepts originate.
First, we adopt an encoding approach similar to how phrases are encoded, using the embedding model.
These two types of coding are then integrated in a specific manner within the KG.
Unlike the document ensemble in \hipporag, which simply aggregates scores from graph search and embedding matching, we enhance the KG by introducing \textbf{passage nodes}, enabling more seamless integration of contextual information.
This approach retains the same offline indexing process as \hipporag while enriching the graph structure with additional nodes and edges related to passages during construction.
% Specifically, during the graph construction phase, we treat each passage in the corpus as a passage node and add them to the graph. 
% Subsequently, we add directed edges from each passage node to all the phrases the passage contains, where this type of edge is labeled ``contains''. 
Specifically, each passage in the corpus is treated as a passage node, with the \textbf{context edge} labeled ``\textit{contains}'' connecting the passage to all phrases derived from this passage. 
% They are set to be unidirectional edges since the PPR process is undirected.

\begin{table*}[!t]
  \centering
  \small
\caption{\textbf{QA performance} (F1 scores) on RAG benchmarks using Llama-3.3-70B-Instruct as the QA reader. No retrieval means evaluating the parametric knowledge of the readers. HippoRAG (and \ours) uses Llama-3.3-70B-Instruct as the extractor (and the triple filter) and NV-Embed-v2 as the retriever. This table, along with the following ones, highlight the \textbf{best} and \underline{second-best} results.}
\vskip 0.1in
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
& \multicolumn{2}{c}{\multirow{2}{*}{Simple QA}} 
& \multicolumn{4}{c}{\multirow{2}{*}{Multi-Hop QA}} 
& \multicolumn{1}{c}{\multirow{2}{*}{\shortstack{Discourse\\Understanding}}} & \\
&&&&&&&&\\
\cmidrule(lr){2-3} \cmidrule(lr){4-7} \cmidrule(lr){8-8}
Retrieval & NQ & PopQA & MuSiQue & 2Wiki & HotpotQA & LV-Eval & NarrativeQA & Avg \\
\midrule
\rowcolor{lightgrey}
\multicolumn{9}{c}{\textit{\textbf{Simple Baselines}}} \\
None & $54.9$ & $32.5$ & $26.1$ & $42.8$ & $47.3$ & \ \ $6.0$ & $12.9$ & $38.4$ \\ 
Contriever~\cite{contriever} & $58.9$ & $53.1$ & $31.3$ & $41.9$ & $62.3$ & \ \ $8.1$ & $19.7$ & $46.9$ \\
BM25~\cite{bm25} & $59.0$ & $49.9$ & $28.8$ & $51.2$ & $63.4$ & \ \ $5.9$ & $18.3$ & $47.7$ \\ 
GTR (T5-base)~\cite{gtr} & $59.9$ & $\underline{56.2}$ & $34.6$ & $52.8$ & $62.8$ & \ \ $7.1$ & $19.9$ & $50.4$\\ 
\rowcolor{lightgrey}
\multicolumn{9}{c}{\textit{\textbf{Large Embedding Models}}} \\
GTE-Qwen2-7B-Instruct~\cite{gte} & $\underline{62.0} $ & $\mathbf{56.3}$ & $40.9$ & $60.0$ & $71.0$ & \ \ $7.1$ & $21.3$ & $54.9$ \\
GritLM-7B~\cite{gritlm} & $61.3$ & $55.8$ & $44.8$ & $60.6$ & $73.3$ & \ \ $9.8$ & $23.9$ & $56.1$ \\ 
NV-Embed-v2 (7B)~\cite{nvembedv2} & $61.9$ & $55.7$ & $\underline{45.7}$ & $61.5$ & $\underline{75.3}$ & \ \ $9.8$ & $\underline{25.7}$ & $\underline{57.0}$ \\
\rowcolor{lightgrey}
\multicolumn{9}{c}{\textit{\textbf{Structure-Augmented RAG}}} \\
RAPTOR~\cite{raptor} & $50.7$ & $\underline{56.2}$ & $28.9$ &  $52.1$  & $69.5$  &  \ \ $5.0$ & $21.4$ & $48.8$ \\
GraphRAG~\cite{graphrag} & $46.9$ &  $48.1$ & $38.5$  &  $58.6$  & $68.6$ & $\underline{11.2}$ & $23.0$ & $49.6$ \\
LightRAG~\cite{lightrag} & $16.6$ & \ \ $2.4$  & \ \ $1.6$ &  $11.6$  & \ \ $2.4$ &  \ \ $1.0$ & \ \ $3.7$ & \ \ $6.6$ \\
HippoRAG~\cite{hipporag} &  $55.3$ &  $55.9$ &  $35.1$ & $\mathbf{71.8}$ &	$63.5$  & \ \ $8.4$ & $16.3$ & $53.1$ \\
\midrule
\ours  & $\mathbf{63.3}$ &  $\underline{56.2}$ &  $\mathbf{48.6}$ & $\underline{71.0}$ & $\mathbf{75.5}$ & $\mathbf{12.9}$ & $\mathbf{25.9}$ & $\mathbf{59.8}$ \\
\bottomrule
\end{tabular}}
\label{table:QA_results}
\vskip -0.1in
\end{table*}



\begin{table*}[!h]
  \centering
  \small
    \caption{\textbf{Retrieval performance} (passage recall@5) on RAG benchmarks. * denotes the report from the original paper. The compared structure-augmented RAG methods are reproduced with the same LLM and retriever as ours for a fair comparison. GraphRAG and LightRAG are not presented because they do not directly produce passage retrieval results. }
\vskip 0.1in
  \begin{tabular}{lcccccc}
  \toprule
    & \multicolumn{2}{c}{Simple QA} & \multicolumn{3}{c}{Multi-Hop QA} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-6}
  Retrieval & NQ & PopQA & MuSiQue & 2Wiki & HotpotQA & Avg \\
  \midrule
  \rowcolor{lightgrey}
  \multicolumn{7}{c}{\textit{\textbf{Simple Baselines}}} \\
  BM25~\cite{bm25} & $56.1$ & $35.7$ & $43.5$ & $65.3$ & $74.8$ & $55.1$  \\ 
  % \midrule
  Contriever~\cite{contriever} & $54.6$ & $43.2$ & $46.6$ & $57.5$ &  $75.3$ & $55.4$ \\
  GTR (T5-base)~\cite{gtr} & $63.4$ & $49.4$ & $49.1$ & $67.9$ & $73.9$ & $60.7$ \\
  \rowcolor{lightgrey}
  \multicolumn{7}{c}{\textit{\textbf{Large Embedding Models}}} \\
  GTE-Qwen2-7B-Instruct~\cite{gte} & $74.3$ & $50.6$ & $63.6$ & $74.8$ & $89.1$ & $70.5$  \\
  GritLM-7B~\cite{gritlm} & $\underline{76.6}$ & $50.1$ &  $65.9$ &  $76.0$ & $92.4$ & $72.2$ \\
  NV-Embed-v2 (7B)~\cite{nvembedv2} & $75.4$ & $51.0$ & $\underline{69.7}$ & $76.5$ & $\underline{94.5}$ & $\underline{73.4}$  \\ 
  \rowcolor{lightgrey}
  \multicolumn{7}{c}{\textit{\textbf{Structure-Augmented RAG}}} \\
  % RAPTOR~\cite{raptor} &$40.5$ / $69.4$ & $37.2$ / $48.1$ & $49.1$ / $61.0$ & $61.5$ / $70.6$ & $78.6$ / $90.2$  \\
  RAPTOR~\cite{raptor} & $68.3$  &  $48.7$  &  $57.8$ &  $66.2$ & $86.9$ & $65.6$  \\
  % GraphRAG & & & & & & & \\
  % LightRAG & & & & & & & \\
  HippoRAG*~\cite{hipporag} & $-$ & $-$ &  $51.9$ & $\underline{89.1}$ & $77.7$ & $-$ \\
  % HippoRAG~(GPT-4o-mini) & 45.1 & 52.2 & $52.4$ & $87.0$ & $78.5$ & 63.0 \\ 
  HippoRAG (reproduced) &  $44.4$ & $\mathbf{53.8}$ & $53.2$ &  $\mathbf{90.4}$ &  $77.3$ & $63.8$\\
   % KAG*~\cite{} & - & - & 48.5 / 65.7 & 65.4 / 91.9 & - \\
   \midrule
  % \ours (GPT-4o-mini) &  76.4 &  52.2 & 74.2 &  90.2 &  95.7 & 77.7   \\
  \ours &  $\mathbf{78.0}$ &  $\underline{51.7}$ & $\mathbf{74.7}$ & $\mathbf{90.4}$ & $\mathbf{96.3}$ & $\mathbf{78.2}$ \\
  % \ours & & & $\mathbf{55.4}$ / $\mathbf{73.3}$ & $\mathbf{74.1}$ / $\mathbf{87.4}$ & $\mathbf{84.9}$ / $\mathbf{95.9}$ % 4o filter\\
  \bottomrule
  \end{tabular}
  % \vspace{-10pt}
  \label{table:recall_results}
\vskip -0.1in
\end{table*}


\subsection{Deeper Contextualization}
\label{subsec:query_to_triple}

Building upon the discussion of the concept-context tradeoff, we observe that query parsing in \hipporag, which relies on Named Entity Recognition (NER), is predominantly concept-centric, often overlooking the contextual alignment within the KG. 
This entity-focused approach to extraction and indexing introduces a strong bias toward concepts, leaving many contextual signals underutilized \cite{hipporag}.
To address this limitation, we explore and evaluate different methods for linking queries to the KG, aiming to more effectively align query semantics with the starting nodes of graph searches. 
Specifically, we consider three approaches:
1) \textbf{NER to Node}: This is the original method used in \hipporag, where entities are extracted from the query and subsequently matched with nodes in the KG using text embeddings.
2) \textbf{Query to Node}: Instead of extracting individual entities, we leverage text embeddings to match the entire query directly to nodes in the KG.
3) \textbf{Query to Triple}: To incorporate richer contextual information from the KG, we match the entire query to triples within the graph using text embeddings. Since triples encapsulate fundamental contextual relationships among concepts, this method provides a more comprehensive understanding of the query's intent.
By default, \ours adopts the query-to-triple approach, and we evaluate all three methods later (\S\ref{subsec:ablation}).

\subsection{Recognition Memory}
\label{subsec:recognition_memory}

Recall and recognition are two complementary processes in human memory retrieval \cite{Uner2022}.
Recall involves actively retrieving information without external cues, while recognition relies on identifying information with the help of external stimuli.
Inspired by this, we model the query-to-triple retrieval as a two-step process.
1) \textbf{Query to Triple}: We use the embedding model to retrieve the top-k triples $T$ of the graph as described in \S\ref{subsec:query_to_triple}. 
2) \textbf{Triple Filtering}: We use LLMs to filter retrieved $T$ and generate triples $T' \subseteq T$. 
The detailed prompts are shown in Appendix \ref{sec:prompts}.

\subsection{Online Retrieval}
\label{subsec:online retrieval}

% After introducing the above improvements, we summarize the online retrieval process in \ours. 
% The task is identifying seed nodes for the PPR search and assigning them reset probabilities (detailed in Appendix \ref{PPR_init}).
% \ours employs query-to-triple linking and initially ranks the top-$k$ triples. 
% It then applies a triple filter to retain up to $k$ phrase nodes from the subject or object of the filtered triples. 
% Each selected phrase node is assigned a reset probability based on the ranking score of its corresponding triple, as determined by the dense retriever.
% For passage nodes, an embedding model is used to directly score all passages, which are then assigned reset probabilities. 
% A weight factor is applied to adjust these probabilities for passage nodes to balance the importance between phrase nodes and passage nodes (\S\ref{subsec:hyperparameter}).
% Finally, the top-$k$ phrase nodes and all passage nodes serve as the seed nodes for the PPR search. 
% After completing the PPR search, we directly rank the passages based on the PageRank scores of all passage nodes. 
% The top-ranked passages are then used for the subsequent QA reading process.

We summarize the online retrieval process in \ours after introducing the above improvements. 
The task involves selecting seed nodes and assigning reset probabilities for retrieval. \ours identifies phrase nodes from filtered triples generated by query-to-triple and recognition memory. If no triples are available, it directly retrieves top-ranked passages using the embedding model. Otherwise, up to $k$ phrase nodes are selected based on their average ranking scores across filtered triples they originate. 
All passage nodes are also taken as seed nodes, as broader activation improves multi-hop reasoning. 
Reset probabilities are assigned based on ranking scores for phrase nodes, while passage nodes receive scores proportional to their embedding similarity, adjusted by a weight factor (\S\ref{subsec:hyperparameter}) to balance the influence between phrase nodes and passage nodes. 
The PPR search is then executed, and passages are ranked by their PageRank scores, with the top-ranked passages used for downstream QA. An example of the pipeline is in Appendix \ref{sec:pipeline example} and the PPR initialization is detailed in Appendix \ref{PPR_init}, 



\section{Experimental Setup}
\label{section:experimental setup}

\subsection{Baselines}

We select three types of comparison methods: 1) The classic retrievers \textbf{BM25} \cite{bm25}, \textbf{Contriever} \cite{contriever} and \textbf{GTR} \cite{gtr}. 
2) Large embedding models that perform well on the BEIR leaderboard \cite{beir}, including \textbf{Alibaba-NLP/GTE-Qwen2-7B-Instruct}~\cite{gte}, \textbf{GritLM/GritLM-7B}~\cite{gritlm}, and \textbf{nvidia/NV-Embed-v2}~\cite{nvembedv2}.
3) Structure-augmented RAG methods, including \textbf{RAPTOR} \cite{raptor}, \textbf{GraphRAG} \cite{graphrag}, \textbf{LightRAG} \cite{lightrag}, and \textbf{\hipporag} \cite{hipporag}.


% \begin{itemize}[noitemsep, topsep=0pt]
%     \item nvidia/NV-Embed-v2 (\#1 in BEIR)
%     \item BAAI/bge-en-icl (\#2 in BEIR)
%     \item Alibaba-NLP/GTE-Qwen2-7B-Instruct (\#1 in BRIGHT)
%     \item GritLM/GritLM-7B (\#2 in BRIGHT)
%     \item Promptriever
%     \item Contextual Document Embeddings
% \end{itemize}

% \subsubsection{Direct Baselines}

% \begin{itemize}[noitemsep, topsep=0pt]
%     \item RAPTOR
%     \item GraphRAG
%     \item LightRAG
%     \item LongRAG (No hyperlinks to group documents)
%     \item Think-on-Graph 2.0 (No Code)
%     \item GraphReader (No Code)
% \end{itemize}



\subsection{Datasets}

To evaluate how well RAG systems retain factual memory while enhancing associativity and sense-making, we select datasets that correspond to three critical challenge types: 
1) Simple QA primarily evaluates the ability to recall and retrieve factual knowledge accurately.
2) Multi-hop QA measures associativity by requiring the model to connect multiple pieces of information to derive an answer.
3) Discourse understanding evaluates sense-making by testing the capability to interpret and reason over lengthy, complex narratives.
The statistics for our sampled dataset are summarized in Table \ref{table:dataset statistics}.

\noindent \textbf{Simple QA.}
This common type of QA task primarily involves questions centered around individual entities, making it particularly well-suited for embedding models to retrieve relevant contextual information intuitively.
We randomly collect $1,000$ queries from the \textbf{NaturalQuestions} (NQ) dataset (collected by \citet{rear}), which contains real user questions with a wide range of topics. 
Additionally, we select $1,000$ queries from \textbf{PopQA} \cite{popqa}, with the corpus derived from the December 2021 Wikipedia dump.\footnote{\url{https://github.com/facebookresearch/atlas?tab=readme-ov-file\#corpora}}
Both datasets offer straightforward QA pairs, enabling evaluation of single-hop QA capabilities in RAG systems. 
Notably, PopQA from Wikipedia is especially entity-centric, with entities being less frequent than NaturalQuestions, making it an excellent resource for evaluating entity recognition and retrieval in simple QA tasks.

\noindent \textbf{Multi-hop QA.}
% We randomly collect $1,000$ queries from three datasets using the same collection approach by HippoRAG \cite{hipporag}: \textbf{MuSiQue}, \textbf{2WikiMultihopQA}, and \textbf{HotpotQA}. Each of these datasets involves queries that require reasoning across multiple passages, making them ideal for evaluating the ability to connect information from different parts of the corpus.
% We also include all $124$ queries from \textbf{LV-Eval} (hotpotwikiqa-mixup 256k) \cite{lveval}, a dataset designed to reduce potential knowledge leakage. 
% This dataset is incredibly challenging as it demands multi-passage reasoning and applies keyword and phrase replacement to minimize the risk of overfitting specific knowledge snippets. 
% Compared to the above Wikipedia-based datasets, this dataset reduces leakage to enhance our ability to evaluate the ability to synthesize knowledge from different sources effectively.
% For the corpus collection, we collect all the long-form contexts of the queries and split them into shorter passages for retrieval, maintaining the same RAG setup as in the other multi-hop RAG scenarios.
We randomly collect $1,000$ queries from \textbf{MuSiQue}, \textbf{2WikiMultihopQA}, and \textbf{HotpotQA} following HippoRAG \cite{hipporag}, all requiring multi-passage reasoning. 
Additionally, we include all $124$ queries from \textbf{LV-Eval} (hotpotwikiqa-mixup 256k) \cite{lveval}, a challenging dataset designed to minimize knowledge leakage and reduce overfitting through keyword and phrase replacements. 
Thus, unlike Wikipedia-based datasets, LV-Eval better evaluates the model's ability to synthesize knowledge from different sources effectively. 
For corpus collection, we segment long-form contexts of LV-Eval into shorter passages while maintaining the same RAG setup as other multi-hop datasets.

\noindent \textbf{Discourse Understanding.}
This category consists of only \textbf{NarrativeQA}, a QA dataset that contains questions requiring a cohesive understanding of a full-length novel. This dataset's focus on large-scale discourse understanding allows us to leverage it in our evaluation of sense-making in our chosen baselines and our own method. 
We randomly select $10$ lengthy documents and their corresponding $293$ queries from NarrativeQA and collect a retrieval corpus just as in the above LV-Eval dataset.

% \noindent \textbf{Open-Domain QA} 

% \begin{itemize}[noitemsep, topsep=0pt]
%     \item PopQA
%     \item NQ
% \end{itemize}

% \noindent \textbf{Long-Form QA} 

% \begin{itemize}[noitemsep, topsep=0pt]
%     \item NarrativeQA
% \end{itemize}

% \noindent \textbf{Multi-hop RAG}

% \begin{itemize}[noitemsep, topsep=0pt]
%     \item MuSiQue
%     \item 2WikiMultihopQA
%     \item HotpotQA
%     \item LV-Eval Mixup Hotpot-2WikiQA 
% \end{itemize}

% \noindent \textbf{Complex QA}

% \begin{itemize}[noitemsep, topsep=0pt]
%     \item Mintaka
%     \item CLQA
% \end{itemize}


\begin{table}[tb]
  \centering
  \small
\caption{Ablations: passage recall@5 on multi-hop benchmarks.}
\vskip 0.1in
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lcccccccc}
  \toprule
   & MuSiQue & 2Wiki & HotpotQA & Avg \\
  \midrule
  \ours &  $\mathbf{74.7}$ &  $90.4$ &  $\mathbf{96.3}$ &  $\mathbf{87.1}$ \\  
  \midrule 
  \ \ w/ NER to node &  $53.8$ &  $\mathbf{91.2}$ & $78.8$ & $74.6$ \\
  \ \ w/ Query to node &  $44.9$ &  $65.5$ & $68.3$ &  $59.6$ \\  
  \midrule
  \ \ w/o Passage Node & $63.7$ & $90.3$ & $88.9$ & $81.0$ \\ 
  \midrule
  \ \ w/o Filter & $\underline{73.0}$ & $\underline{90.7}$ & $\underline{95.4}$ & $
  \underline{86.4}$ \\
  \bottomrule
  \end{tabular}%
  }
  % \vspace{-10pt}
  \label{table:ablations}
  \vskip -0.1in
\end{table}

\begin{table}[tb]
\centering
\small
\caption{Passage recall@5 with different weight factors for passage nodes on our MuSiQue dev set and NaturalQuestions (NQ) dev set, where each set has $1,000$ queries.}
\vskip 0.1in
% \begin{tabular}{lcccccc}
% \toprule
% Weight & 0.01 & 0.05 & 0.1 & 0.3 & 0.5 \\ \midrule
% MuSiQue & 59.9/79.9 & \textbf{63.3}/\textbf{80.5} & 62.7/79.8 & 63.1/78.4 & 62.8/77.9 \\ 
% NQ & 42.7/75.6 & 44.6/\textbf{76.9} & 45.4/76.9 & 45.9/76.7 & \textbf{46.5}/76.4 \\
% \bottomrule
% \end{tabular}
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
Weight & $0.01$ & $0.05$ & $0.1$ & $0.3$ & $0.5$ \\ \midrule
MuSiQue & $79.9$ & $\mathbf{80.5}$ & $79.8$ & $78.4$ & $77.9$ \\ 
NQ & $75.6$ & $\mathbf{76.9}$ & $76.9$ & $76.7$ & $76.4$ \\
\bottomrule
\end{tabular}%
% }
\label{table:weight factor}
\vskip -0.1in
\end{table}


\subsection{Metrics}

Following HippoRAG \cite{hipporag}, we use passage recall@5 to evaluate the retrieval task. 
For the QA task, we follow evaluation metrics from MuSiQue \cite{musique} to calculate F1 scores for the final answer.


\subsection{Implementation Details}
\label{subsec: implementation details}

For \ours, we use the open-source Llama-3.3-70B-Instruct \cite{llama3modelcard} as both the extraction (NER and OpenIE) and triple filtering model, and we use nvidia/NV-Embed-v2 as the retriever.
We also reproduce the compared structure-augmented RAG methods using the same extractor and retriever for a fair comparison.
For the triple filter, we use DSPy \cite{dspy} MIPROv2 optimizer and Llama-3.3-70B-Instruct to tune the prompt, including the instructions and demonstrations. 
The resulting prompt is shown in Appendix \ref{sec:prompts}.
We use top-5 triples ranked by retriever for filtering.
For hyperparameters, we follow the default settings from \hipporag. 
More implementation and hyperparameter details can be found in Appendix \ref{sec:appendix-hyper}.


\begin{table}[t]
\centering
\caption{Passage recall@5 on MuSiQue subset. \ours supports different dense retrievers.}
\vskip 0.1in
\small
% \begin{tabular}{lcc}
% \toprule
% Retriever & Dense Retrieval & \ours \\ \midrule
% GTE-Qwen2-7B-Instruct   & 48.1 / 63.6 & \textbf{52.3} / \textbf{68.8} \\
% GritLM-7B       & 49.7 / 66.0 & \textbf{53.9} / \textbf{71.6} \\
% NV-Embed-v2 &  52.7 / 69.7 & \textbf{56.1} / \textbf{74.7} \\
% \bottomrule
% \end{tabular}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
Retriever & Dense Retrieval & \ours \\ \midrule
GTE-Qwen2-7B-Instruct   &  $63.6$ & $\mathbf{68.8}$ \\
GritLM-7B               &  $66.0$ & $\mathbf{71.6}$ \\
NV-Embed-v2 (7B)        &  $69.7$ & $\mathbf{74.7}$ \\
\bottomrule
\end{tabular}
}
\label{table:dense retriever flexibility}
\vskip -0.1in
\end{table}


\begin{table*}[htb]
\centering
\small
 \caption{We show exemplary retrieval results (the title of passages) from \ours and NV-Embed-v2 on different types of questions. Bolded items denote the titles of supporting passages.}
 \vskip 0.1in
 \resizebox{\textwidth}{!}{%
\begin{tabular}{p{1.7cm} p{2.2cm} p{3.7cm} p{4.0cm} p{3.7cm}} 
\toprule
& \textbf{Question} & \textbf{NV-Embed-v2} Results & \textbf{\ours} Filtered Triples & \textbf{\ours} Results   \\
\midrule
\textbf{Simple QA} & In what city was I.P. Paul born? & \textbf{1. I. P. Paul}  \newline 2. Yinka Ayefele - Early life \newline  3. Paul Parker (singer) & (\textbf{I. P. Paul}, from, \textbf{Thrissur}) \newline (\textbf{I. P. Paul}, was mayor of, Thrissur municipal corporation) & \textbf{1. I. P. Paul} \newline \textbf{2. Thrissur} \newline 3. Yinka Ayefele \\ 
\midrule
\textbf{Multi-Hop QA} & What county is Erik Hort's birthplace a part of? & \textbf{1. Erik Hort} \newline 2. Horton Park (Saint Paul, Minnesota) \newline 3. Hertfordshire  & (\textbf{Erik Hort}, born in, \textbf{Montebello}) \newline (\textbf{Erik Hort}, born in, New York) &  \textbf{1. Erik Hort} \newline 2. Horton Park (Saint Paul, Minnesota) \newline \textbf{3. Monstebello, New York} \\
\bottomrule
\end{tabular}
}
   \label{table:qualitative analysis}
\vskip -0.1in
\end{table*}


\section{Results}

We now present our main QA and retrieval experimental results, where the QA process uses retrieved results as its context. 
More detailed experimental results are presented in Appendix \ref{sec:detailed results}. The statistics for all constructed KGs are shown in Appendix \ref{sec:prompts}.

% \noindent \textbf{QA Performance.} We show the QA performance of various retrievers across multiple RAG benchmarks in Table \ref{table:QA_results}, using Llama-3.3-70B-Instruct as the QA reader. 
% \ours outperforms all compared retrievers on the majority of these benchmarks, achieving the highest average F1 score. 
% It underscores the robustness and versatility across different evaluation settings. 
% Among the methods compared, the large embedding models are the strongest competitors and perform much better than previous smaller models. For example, NV-Embed-v2 (7B) achieves $6.6\%$ higher average F1 score than GTR (T5-base).
% These large embedding models also perform better than previous structure-augmented RAG methods while using less computation.
% But note that large embedding models are mainly strong in the simple QA case, and clearly lose out to the other scenarios. 
% Especially, \ours gets $9.5\%$ higher F1 compared to NV-Embed-v2 on 2Wiki.
% On the extremely challenging LV-Eval dataset, all methods perform poorly, but \ours still gets $3.1\%$ higher F1 than NV-Embed-v2.
% Compared to \hipporag, the improvements in \ours are even more pronounced, further validating the effectiveness of this neuropsychology-inspired framework.
% These results demonstrate that \ours not only enhances retrieval effectiveness (detailed later) but also significantly improves overall QA performance, establishing it as a state-of-the-art RAG system. 
% Meanwhile, it shows \ours can be well-driven by an open-source model.
\noindent \textbf{QA Performance.} Table \ref{table:QA_results} presents the QA performance of various retrievers across multiple RAG benchmarks using Llama-3.3-70B-Instruct as the QA reader. \ours achieves the highest average F1 score, demonstrating robustness across different settings. 
Large embedding models outperform smaller ones, with NV-Embed-v2 (7B) scoring $6.6\%$ higher on average than GTR (T5-base). 
These models also surpass structure-augmented RAG methods with lower computational costs but excel mainly in simple QA while struggling in complex cases. N
otably, \ours outperforms NV-Embed-v2 by $9.5\%$ F1 on 2Wiki and by $3.1\%$ on the challenging LV-Eval dataset. Compared to \hipporag, \ours shows even greater improvements, validating its neuropsychology-inspired approach. These results highlight \ours as a state-of-the-art RAG system that enhances both retrieval and QA performance while being effectively powered by an open-source model.
Table \ref{table:appendix qa performance} in Appendix \ref{sec:detailed results} presents additional QA results (EM and F1) using Llama or GPT-4o-mini as the QA reader, along with an extractor or triple filter. GPT-4o-mini follows Llama’s trend, with NV-Embed-v2 outperforming structure-augmented methods in most cases, except for HippoRAG in multi-hop QA. \ours consistently outperforms all other methods across nearly all settings.


\noindent \textbf{Retrieval Performance.} We report retrieval results for datasets with supporting passage annotations and models that explicitly retrieve passages in Table \ref{table:recall_results}. Large embedding models (7B) significantly outperform classic smaller LM-based models like Contriever and GTR, achieving at least a $9.8\%$ higher F1 score. While our reproduction of HippoRAG using Llama-3.3-70B-Instruct and NV-Embed-v2 shows slight improvements over the original paper, the gains are minimal, with only a $1.3\%$ increase in F1. Although \hipporag excels in entity-centric retrieval, achieving the highest recall@5 on PopQA, it generally lags behind recent dense retrievers and \ours. Notably, \ours achieves the highest recall scores across most datasets, with substantial improvements of $5.0\%$ and $13.9\%$ in Recall@5 on MuSiQue and 2Wiki, respectively, compared to the strongest dense retriever, NV-Embed-v2.
Additionally, the cost and efficiency analysis is presented in Appendix \ref{sec:cost and efficiency}.


\section{Discussions}


\subsection{Ablation Study}
\label{subsec:ablation}
We design ablation experiments for the proposed linking method, graph construction method, and triple filtering method, with the results reported in Table \ref{table:ablations}.
Each introduced mechanism boosts \ours.
First, the linking method with deeper contextualization leads to significant performance improvements. 
Notably, we do not apply a filtering process to the NER-to-node or query-to-node methods; however, the query-to-triple approach, regardless of whether filtering is applied, consistently outperforms the other two linking strategies. On average, query-to-triple improves Recall@5 by $12.5\%$ compared to NER-to-node. Moreover, query-to-node does not provide an advantage over NER-to-node, as queries and KG nodes operate at different levels of granularity, whereas both NER results and KG nodes correspond to phrase-level representations.

\subsection{Controlling Reset Probabilities}
\label{subsec:hyperparameter}

% ratio of phrase node and passage node
% \noindent \textbf{Reset Probability Ratio}

When setting the reset probability before starting PPR, we find that it is necessary to balance the reset probabilities between two types of nodes: phrase nodes and passage nodes. 
Specifically, the reset probability of all passage nodes is multiplied by a weight factor to balance the importance of two types of nodes during PPR.
Here, we present the results obtained on the validation set in Table \ref{table:weight factor}, which shows that this factor is crucial for the PPR results.
Considering the model performance across different scenarios, we set the factor to be $0.05$ by default.



% damping factor

\subsection{Dense Retriever Flexibility}

The dense retriever employed by \ours is fully plug-and-play, offering seamless integration. As demonstrated in Table \ref{table:dense retriever flexibility}, \ours consistently surpasses direct dense retrieval across various retrievers. 
Notably, these performance gains remain robust regardless of the specific dense retriever used.




\subsection{Qualitative Analysis}

We show examples from PopQA and MuSiQue in Table \ref{table:qualitative analysis}.
For the first example, ``\textit{In what city was I. P. Paul born?}'', NV-Embed-v2 ranks the entity mentioned in the query ``\textit{I. P. Paul}'' as the top 1, where the passage is enough to answer this question.
But \ours does even better. It directly finds the answer ``\textit{Thrissur}'' when linking the triples, and during the subsequent graph search, it places the passage corresponding to that entity in the second position, which is a perfect retrieval result.
For the second multi-hop question, ``\textit{What county is Erik Hort’s birthplace a part of?}'' NV-Embed-v2 also easily identifies the person mentioned, ``\textit{Erik Hort}.'' 
However, since this question requires two-step reasoning, it is not sufficient to fully answer the question. 
In contrast, \ours retrieves a passage titled ``\textit{Montebello}'' during the query-to-triple step, which contains geographic information that implies the answer to the question. In the subsequent graph search, this passage is also ranked at the top.
Apart from this, the error analysis of \ours is detailed in Appendix \ref{sec:error analysis}.

\section{Conclusion}

We introduced \ours, a novel framework designed to address the limitations of existing RAG systems in approximating the dynamic and interconnected nature of human long-term memory. It combining the strengths of the Personalized PageRank algorithm, deeper passage integration, and effective online use of LLMs. 
\ours opens new avenues for research in continual learning and long-term memory for LLMs by achieving comprehensive improvements over standard RAG methods across factual, sense-making, and associative memory tasks, showing capabilities that previous methods have either overlooked or been incapable of achieving in a thorough evaluation.
% , moving closer to mimicking the adaptability and efficiency of human learning. 
Future work could consider leveraging graph-based retrieval methods to further enhance the episodic memory capabilities of LLMs in long conversations.

\section*{Impact Statement}

This paper presents work on Retrieval-Augmented Generation (RAG) to advance the field of long-term memory for large language models. 
While our work may have various societal implications, we do not identify any concerns that warrant specific emphasis beyond those generally associated with large language models and information retrieval systems.

\section*{Acknowledgments}
 
We would also like to extend our appreciation to colleagues from the OSU NLP group for their constructive comments. 
This work is supported in part by ARL W911NF2220144, NSF 2112606, and a gift from Cisco. 
We also thank the Ohio Supercomputer Center for providing computational resources.
The views and conclusions contained herein are those of the
authors and should not be interpreted as representing the official policies, either expressed or implied,
of the U.S.\ government. The U.S.\ government is
authorized to reproduce and distribute reprints for
government purposes notwithstanding any copyright notice herein.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{anthology, custom}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section*{Appendices}

Within this supplementary material, we elaborate on the following aspects:

\begin{itemize}
    \item Appendix \ref{sec:prompts}: LLM Prompts
    \item Appendix \ref{sec:pipeline example}: \ours Pipeline Example
    \item Appendix \ref{sec:detailed results}: Detailed Experimental Results
    \item Appendix \ref{sec:graph statistics}: Graph Statistics
    \item Appendix \ref{sec:error analysis}: Error Analysis
    \item Appendix \ref{sec:cost and efficiency}: Cost and Efficiency
    \item Appendix \ref{sec:appendix-hyper}: Implementation Details and Hyperparameters
\end{itemize}

\section{LLM Prompts}
\label{sec:prompts}

We show LLM prompts for triple filter in Figure \ref{fig:prompts for triple filtering}, including the instruction, the few-shot demonstrations and the input format.

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/filter_prompt.pdf}
    \caption{LLM prompts for triple filtering (recognition memory).}
    \label{fig:prompts for triple filtering}
\end{figure*}


\section{Pipeline Example}
\label{sec:pipeline example}

We show a pipeline example of \ours online retrieval in Figure \ref{fig:pipeline example}, including query-to-triple, triple filtering and using seed nodes for PPR.

\begin{figure*}
    \centering
    \includegraphics[width=0.73\linewidth]{figure/pipeline.pdf}
    \caption{An example of \ours pipeline.}
    \label{fig:pipeline example}
\end{figure*}



\section{Detailed Experimental Results}
\label{sec:detailed results}

We show QA performance and retrieval performance with the proprietary model GPT-4o-mini as well as more metrics here, as shown in Table \ref{table:appendix qa performance} and Table \ref{table:appendix recall results}.

\noindent \textbf{QA Performance}
As shown in Table \ref{table:appendix qa performance}, when using GPT-4o-mini for indexing and QA reading, \ours consistently achieves competitive EM and F1 scores across most datasets. Notably, it leads in the MuSiQue and 2Wiki benchmarks. Our method also demonstrates superior performance in the NarrativeQA and LV-Eval tasks. When compared to the strong NV-Embed-v2 retriever, \ours exhibits comparable or enhanced F1 scores, particularly excelling in the LV-Eval dataset with reduced knowledge leakage.

\noindent \textbf{Retrieval Performance}
As shown in Table \ref{table:appendix recall results}, the improvement trend of \ours in recall@2 is similar to that in recall@5.


\begin{table*}[htb]
  \centering
  \small
    \caption{QA performance (EM / F1 scores) on RAG benchmarks. No retrieval means evaluating the parametric knowledge of the readers. HippoRAG (and \ours) uses the denoted LLM for OpenIE (triple filtering) and QA reading.}
    \vskip 0.1in
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{lccccccccccccc}
  \toprule
& \multicolumn{2}{c}{\multirow{2}{*}{Simple QA}} 
& \multicolumn{4}{c}{\multirow{2}{*}{Multi-Hop QA}} 
& \multicolumn{1}{c}{\multirow{2}{*}{\shortstack{Discourse\\Understanding}}} & \\
&&&&&&&&\\
  \cmidrule(lr){2-3} \cmidrule(lr){4-7} \cmidrule(lr){8-8}
  Retrieval & NQ & PopQA & MuSiQue & 2Wiki & HotpotQA & LV-Eval & NarrativeQA & Avg \\
   \midrule
   \multicolumn{9}{c}{Llama-3.3-70B-Instruct} \\
   \midrule
% \rowcolor{lightgrey}\multicolumn{9}{c}{\textit{\textbf{Simple Baselines}}} \\
  None & $40.2$ / $54.9$ & $28.2$ / $32.5$ & $17.6$ / $26.1$ & $36.5$ / $42.8$ & $37.0$ / $47.3$ & \ \ $4.0$ / \ \ $6.0$ &  \ \ $3.4$ / $12.9$ & $29.7$ / $38.4$ \\ 
  Contriever~\cite{contriever} & $45.0$ / $58.9$ & $41.6$ / $53.1$ & $24.0$ / $31.3$ & $38.1$ / $41.9$ & $51.3$ / $62.3$ & \ \ $5.7$ / \ \ $8.1$ & \ \ $6.5$ / $19.7$ & $37.4$ / $46.9$ \\
  BM25~\cite{bm25} & $44.7$ / $59.0$ & $39.1$ / $49.9$ & $20.3$ / $28.8$ & $47.9$ / $51.2$ & $52.0$ / $63.4$ & \ \ $4.0$ / \ \ $5.9$ & \ \ $4.4$ / $18.3$ & $38.0$ / $47.7$ \\ 
% \rowcolor{lightgrey}\multicolumn{9}{c}{\textit{\textbf{Large Embedding Models}}} \\
  GTR (T5-base)~\cite{gtr} & $45.5$ / $59.9$ & $\underline{43.2}$ / $\underline{56.2}$  & $25.8$ / $34.6$ & $49.2$ / $52.8$ & $50.6$ / $62.8$ & \ \ $4.8$ / \ \ $7.1$ & \ \ $6.8$ / $19.9$ & $40.0$ / $50.4$ \\
  GTE-Qwen2-7B-Instruct~\cite{gte} & $46.6$ / $\underline{62.0}$ & $\mathbf{43.5}$ / $\mathbf{56.3}$ & $30.6$ / $40.9$ & $55.1$ / $60.0$ & $58.6$ / $71.0$ & \ \ $5.7$ / \ \ $7.1$ & \ \ $7.9$ / $21.3$ & $43.8$ / $54.9$ \\
  GritLM-7B~\cite{gritlm} & $46.8$ / $61.3$ & $42.8$ / $55.8$ & $33.6$ / $44.8$ & $55.8$ / $60.6$ & $60.7$ / $73.3$ & \ \ $\underline{7.3}$ / \ \ $9.8$ & \ \ $\underline{8.2}$ / $23.9$ & $44.9$ / $56.1$ \\
  NV-Embed-v2 (7B)~\cite{nvembedv2} &  $\underline{47.3}$ / $61.9$	& $42.9$ / $55.7$ & $\underline{34.7}$ / $\underline{45.7}$ & $\underline{57.5}$ / $61.5$ &	$\mathbf{62.8}$ / $\underline{75.3}$ & \ \ $\underline{7.3}$ / \ \ $9.8$  & \ \ $\mathbf{8.9}$ / $\underline{25.7}$ & $\underline{45.9}$ / $\underline{57.0}$ \\
  % \rowcolor{lightgrey}\multicolumn{9}{c}{\textit{\textbf{Structure-augmented RAG}}} \\
  RAPTOR~\cite{raptor} & $36.9$ / $50.7$& $43.1$ / $\underline{56.2}$ & $20.7$ / $28.9$ & $47.3$ / $52.1$  & $56.8$ / $69.5$  & \ \ $2.4$ / \ \ $5.0$ & \ \ $5.1$ / $21.4$ & $38.1$ / $48.8$ \\
  GraphRAG~\cite{graphrag} & $30.8$ / $46.9$ & $31.4$ / $48.1$ &$27.3$ / $38.5$  &  $51.4$ / $58.6$  & $55.2$ / $68.6$ & \ \ $4.8$ / $\underline{11.2}$ & \ \ $6.8$ / $23.0$ & $36.7$ / $49.6$ \\
  LightRAG~\cite{lightrag} & \ \ $8.6$ / $16.6$  & \ \ $2.1$ / \ \ $2.4$ & \ \ $0.5$ / \ \ $1.6$ & \ \ $9.4$ / $11.6$  & \ \ $2.0$ / \ \ $2.4$ & \ \ $0.8$ / \ \ $1.0$ & \ \ $1.0$ / \ \ $3.7$ & \ \ $4.2$ / \ \ $6.6$ \\
  HippoRAG~\cite{hipporag} & $43.0$ / $55.3$ & $42.7$ / $55.9$ & $26.2$ / $35.1$ &	$\mathbf{65.0}$ / $\mathbf{71.8}$ &	$52.6$ / $63.5$  & \ \ $6.5$ / \ \ $8.4$ & \ \ $4.4$ / $16.3$ & $42.8$ / $53.1$ \\
  \ours  & $\mathbf{48.6}$ / $\mathbf{63.3}$ & $42.9$ / $\underline{56.2}$ & $\mathbf{37.2}$ / $\mathbf{48.6}$ & $\mathbf{65.0}$ / $\underline{71.0}$ & $\underline{62.7}$ / $\mathbf{75.5}$ & \ \ $\mathbf{9.7}$ / $\mathbf{12.9}$ & \ \ $\mathbf{8.9}$ / $\mathbf{25.9}$ & $\mathbf{48.0}$ /	$\mathbf{59.8}$ \\
  \midrule
     \multicolumn{9}{c}{GPT-4o-mini} \\ 
     \midrule
     % \rowcolor{lightgrey}\multicolumn{9}{c}{\textit{\textbf{Simple Baselines}}} \\
  None & $35.2$ / $52.7$ & $16.1$ / $22.7$ & $11.2$ / $22.0$ &  $30.2$ / $36.3$ & $28.6$ / $41.0$ & \ \ $3.2$ / \ \ $5.0$ & \ \ $2.7$ / $14.1$  & $22.6$ / $33.1$ \\
  % \rowcolor{lightgrey}\multicolumn{9}{c}{\textit{\textbf{Large Embedding Models}}} \\
  NV-Embed-v2 (7B)~\cite{nvembedv2} & $\mathbf{43.5}$ / $\underline{59.9}$ & $41.7$ / $\underline{55.8}$ & $\underline{32.8}$ / $\underline{46.0}$ & $54.4$ / $60.8$ & $\mathbf{57.3}$ / $\underline{71.0}$ &  \ \ $\underline{7.3}$ / $10.0$ & \ \ $5.1$ / $\underline{24.2}$ & $\underline{42.9}$ / $\underline{55.7}$ \\
  % \midrule
  % \rowcolor{lightgrey}\multicolumn{9}{c}{\textit{\textbf{Structure-augmented RAG}}} \\
  RAPTOR~\cite{raptor} & $37.8$ / $54.5$& $\underline{41.9}$ / $55.1$ & $27.7$ / $39.2$ & $39.7$ / $48.4$ & $50.6$ / $64.7$  & \ \ $5.6$ / \ \ $9.2$ & \ \ $4.1$ / $21.8$ & $36.9$ / $49.7$ \\
  GraphRAG~\cite{graphrag} & $38.0$ / $55.5$ & $30.7$ / $51.3$ & $27.0$ / $42.0$  & $45.7$ /\ $61.0$  & $51.4$ / $67.6$ & \ \ $4.9$ / $\underline{11.0}$  & \ \ $\underline{5.4}$ / $20.9$ & $36.0$ / $52.6$  \\
  LightRAG~\cite{lightrag} & \ \ $2.8$ / $15.4$ & \ \ $1.9$ / $14.8$& \ \ $2.0$ / \ \ $9.3$ & \ \ $2.5$ /\ $12.1$ & \ \ $9.9$ /\ $20.2$ & \ \ $0.9$ / \ \ $5.0$ & \ \ $1.0$ / \ \ $9.0$ & \ \ $3.6$ / $13.9$ \\
  HippoRAG~\cite{hipporag} & $37.2$ / $52.2$ & $\mathbf{42.5}$ / $\mathbf{56.2}$ & $24.0$ / $35.9$ & $\underline{59.4}$ / $\underline{67.3}$ & $46.3$ / $60.0$ & \ \ $4.8$ / \ \ $7.6$ & \ \ $2.1$ / $16.1$ & $38.9$ / $51.2$ \\ 
  % \midrule
  \ours & $\underline{43.4}$ / $\mathbf{60.0}$ & $41.7$ / $55.7$ & $\mathbf{35.0}$ / $\mathbf{49.3}$ & $\mathbf{60.5}$ / $\mathbf{69.7}$ & $\underline{56.3}$ / $\mathbf{71.1}$ & $\mathbf{10.5}$ / $\mathbf{14.0}$ & \ \ $\mathbf{5.8}$ / $\mathbf{25.2}$ & $\mathbf{44.3}$ / $\mathbf{58.1}$ \\
  \bottomrule
  \end{tabular}
  }
  \label{table:appendix qa performance}
  % \vspace{-10pt}
    \vskip -0.1in
\end{table*}


\begin{table*}[htb]
  \centering
  \small
      \caption{Passage recall@2 / @5 on RAG benchmarks. * denotes the report from the original paper while we reproduce the HippoRAG results with aligned LLM and retriever.}
          \vskip 0.1in
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{lcccccc}
  \toprule
  & \multicolumn{2}{c}{Simple}  & \multicolumn{3}{c}{Multi-hop} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-6}
   & NQ & PopQA & MuSiQue & 2Wiki & HotpotQA & Avg \\
  \midrule
\rowcolor{lightgrey}
  \multicolumn{7}{c}{\textit{\textbf{Simple Baselines}}} \\
  Contriever~\cite{contriever} & $29.1$ / $54.6$ & $27.0$ / $43.2$ & $34.8$ / $46.6$ & $46.6$ / $57.5$ & $58.4$ / $75.3$ & $39.2$ / $55.4$ \\
  BM25~\cite{bm25} & $28.2$ / $56.1$ & $24.0$ / $35.7$ & $32.4$ / $43.5$ & $55.3$ / $65.3$ & $57.3$ / $74.8$  & $39.4$ / $55.1$ \\ 
  % \midrule
  GTR (T5-base)~\cite{gtr} & $35.0$ / $63.4$ & $40.1$ / $49.4$ & $37.4$ / $49.1$ & $60.2$ / $67.9$ & $59.3$ / $73.9$ & $46.4$ / $60.7$ \\
  \rowcolor{lightgrey}
  \multicolumn{7}{c}{\textit{\textbf{Large Embedding Models}}} \\
  GTE-Qwen2-7B-Instruct~\cite{gte} & $44.7$ / $74.3$ & $\mathbf{47.7}$ / $50.6$ & $48.1$ / $63.6$ & $66.7$ / $74.8$ & $75.8$ / $89.1$ & $56.6$ / $70.5$  \\
  GritLM-7B~\cite{gritlm} & $\mathbf{46.2}$ / $\underline{76.6}$ & $44.0$ / $50.1$ & $49.7$ / $65.9$ & $67.3$ / $76.0$ & $79.2$ / $92.4$ & $57.3$ / $72.2$ \\
  NV-Embed-v2 (7B)~\cite{nvembedv2} & $45.3$ / $75.4$ & $\underline{45.3}$ / $51.0$ & $52.7$ / $69.7$ & $67.1$ / $76.5$ & $\mathbf{84.1}$ / $94.5$ & $58.9$ / $73.4$ \\ 
  \rowcolor{lightgrey}
  \multicolumn{7}{c}{\textit{\textbf{Structure-augmented RAG}}} \\
  % RAPTOR~\cite{raptor} &$40.5$ / $69.4$ & $37.2$ / $48.1$ & $49.1$ / $61.0$ & $61.5$ / $70.6$ & $78.6$ / $90.2$  \\
  RAPTOR~(GPT-4o-mini) & $40.5$ / $69.4$  & $37.2$ / $48.1$ &  $49.1$ / $61.0$ & $58.4$ / $66.0$ & $78.6$ / $90.2$ & $52.8$ / $67.0$  \\
  RAPTOR~(Llama-3.3-70B-Instruct) & $40.3$ / $68.3$  & $40.2$ / $48.7$   &  $47.0$ / $57.8$ & $58.3$ / $66.2$ & $76.8$ / $86.9$ & $52.5$ / $65.6$  \\
  % GraphRAG & & & & & & & \\
  % LightRAG & & & & & & & \\
  HippoRAG*~\cite{hipporag} & $-$ & $-$ & $40.9$ / $51.9$ & $70.7$ / $89.1$ & 
 $60.5$ / $77.7$ & $-$ \\
  HippoRAG~(GPT-4o-mini) & $21.6$ / $45.1$ & $36.5$ / $52.2$ & $41.8$ / $52.4$ & $68.4$ / $87.0$ & $60.1$ / $78.5$ & $45.7$ / $63.0$ \\ 
   HippoRAG~(Llama-3.3-70B-Instruct) & $21.3$ / $44.4$ & $40.0$ / $\mathbf{53.8}$ & $41.2$ / $53.2$ & $71.9$ / $\mathbf{90.4}$ & $60.4$ / $77.3$ & $47.0$ / $63.8$ \\
   % KAG*~\cite{} & - & - & 48.5 / 65.7 & 65.4 / 91.9 & - \\
   \midrule
  \ours (GPT-4o-mini) & $44.4$ / $76.4$ & $43.5$ / $\underline{52.2}$ &  $\underline{53.5}$ / $\underline{74.2}$ & $\underline{74.6}$ / $\underline{90.2}$ & $80.5$ / $\underline{95.7}$ & $\underline{59.3}$ / $\underline{77.7}$  \\
  \ours (Llama-3.3-70B-Instruct) & $\underline{45.6}$ / $\mathbf{78.0}$ & $43.9$ / $51.7$ & $\mathbf{56.1}$ / $\mathbf{74.7}$ & $\mathbf{76.2}$ / $\mathbf{90.4}$ & $\underline{83.5}$ / $\mathbf{96.3}$ & $\mathbf{61.1}$ / $\mathbf{78.2}$ \\
  % \ours & & & $\mathbf{55.4}$ / $\mathbf{73.3}$ & $\mathbf{74.1}$ / $\mathbf{87.4}$ & $\mathbf{84.9}$ / $\mathbf{95.9}$ % 4o filter\\
  \bottomrule
  \end{tabular}
  }
  % \vspace{-10pt}

  \label{table:appendix recall results}
    \vskip -0.1in
\end{table*}


\section{Graph Statistics}
\label{sec:graph statistics}

We show the knowledge graph statistics using Llama-3.3-70B-Instruct or GPT-4o-mini for OpenIE in Table \ref{tab:graph statistics}.

\begin{table}[]
    \centering
    \small
        \caption{Knowledge graph statistics using different LLMs for OpenIE. The nodes and triples are counted based on unique values. }
        \vskip 0.1in
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lrrrrrrr}
    \toprule
     & NQ & PopQA & MuSiQue & 2Wiki & HotpotQA & LV-Eval & NarrativeQA \\ 
     \midrule
     \multicolumn{8}{c}{Llama-3.3-70B-Instruct} \\
     \midrule
        \# of phrase nodes & $68,375$ & $76,539$ & $85,288$ & $44,004$ & $81,200$ & $175,195$ & 
$9,224$ \\
        \# of passage nodes & $9,633$ & $8,676$ & $11,656$ & $6,119$ & $9,811$ & $22,849$ & $4,111$ \\
        \# of total nodes & $78,008$ & $85,215$ & $96,944$ & $50,123$ & $91,011$ & $198,044$ & $13,335$ \\ 
        \# of extracted edges & $125,777$ & $124,579$ & $140,830$ & $68,881$ & $130,058$ & $314,324$ & $26,208$\\
        \# of synonym edges & $899,031$ & $845,014$ & $1,125,951$ & $593,298$ & $994,187$ & $2,674,833$ & $72,494$ \\
        \# of context edges & $126,757$ & $118,909$ & $132,586$ & $64,132$ & $122,437$ & $375,424$ & $33,395$ \\
        \# of total edges & $1,151,565$ & $1,088,502$ & $1,399,367$ & $726,311$ & $1,246,682$ & $3,364,581$ & $132,097$ \\ \midrule
    \multicolumn{8}{c}{GPT-4o-mini} \\ \midrule
           \# of phrase nodes & $86,904$ & $85,744$ & $101,641$ & $49,544$ & $95,105$ & $217,085$ & $15,365$ \\
        \# of passage nodes & $9,633$ & $8,676$ & $11,656$ & $6,119$ & $9,811$ & $22,849$ & $4,111$ \\
        \# of total nodes & $96,537$ & $94,420$ & $113,297$ & $55,663$ & $104,916$ & $239,934$ & $19,476$ \\ 
        \# of extracted edges & $114,900$ & $108,989$ & $125,903$ & $62,626$ & $119,630$ & $303,491$ & $24,373$ \\
        \# of synonym edges & $1,094,651$ & $901,528$ & $1,304,605$ & $715,763$ & $1,126,501$ & $3,268,084$ & $14,075$ \\
        \# of context edges & $142,419$ & $127,568$ & $146,293$ & $68,348$ & $133,220$ & $404,210$ & $38,632$ \\
        \# of total edges & $1,351,970$ & $494,082$ & $1,576,801$ & $846,737$ & $1,379,351$ & $3,975,785$ & $77,080$ \\ 
        \bottomrule
         & 
    \end{tabular}
    }
    \label{tab:graph statistics}
    \vskip -0.1in
\end{table}



\section{Error Analysis}
\label{sec:error analysis}

We provide an error analysis of 100 samples generated by \ours with recall@5 less than 1.0. 
Among these samples, 26\%, 41\%, and 33\% are classified as 2-hop, 3-hop, and 4-hop questions, respectively. 
Triple filtering and the graph search algorithm are the two main sources of errors.

\noindent \textbf{Recognition Memory} In 7\% of the samples, no phrase from the supporting documents is matched with the phrases obtained by the query-to-triple stage before triple filtering. 
In 26\% of the samples, no phrase from the supporting documents is matched with the phrases after triple filtering.
After the triple filtering step, 8\% of the samples show a decrease in the proportion of phrases in the triples that match phrases from the supporting passages. For instance, the first case from Table \ref{tab:error examples} shows an empty list after triple filtering, which eliminates all relevant phrases.
Additionally, 18\% of the samples are left with zero triples after filtering. Although not necessarily an error in filtering, this indicates that the attempt to link to the triples has failed, where \ours directly uses the results from dense retrieval as a substitute. Overall, though recognition memory is an essential component, the precision of the triple filter has room for further improvement.

\noindent \textbf{Graph Construction} Graph construction is challenging to evaluate, but we find that only 2\% of the samples do not contain any phrases from the supporting passages within the one-hop neighbors of the linked nodes. Given our dense-sparse integration, we can assume that the graphs we construct generally include most of the potentially exploitable information.

\noindent \textbf{Personalized PageRank} In 50\% of the samples, at least half of the linked phrase nodes appear in the supporting documents. However, the final results remain unsatisfactory due to the graph search component. For example, in the second case from Table \ref{tab:error examples}, the recognition memory identifies the key phrase "Philippe, Duke of Orléans" from the query, but the graph search fails to return perfect results among the top-5 retrieved passages.

\begin{table*}[]
\small
\centering
\caption{Two examples from MuSiQue where passage recall@5 is less than 1.0.}
    \vskip 0.1in
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lp{13cm}}
\toprule
\textbf{Query} & Where is the district that the person who wanted to reform and address Bernhard Lichtenberg's religion preached a sermon on Marian devotion before his death located? \\ 
\textbf{Answer} & Saxony-Anhalt \\ \midrule
\textbf{Supporting Passages (Title)} & 1. Mary, mother of Jesus 2. Reformation 3. Wittenberg (district)  4. Bernhard Lichtenberg \\
\textbf{Retrieved Passages (Title)} & \textbf{1. Bernhard Lichtenberg} \textbf{2. Mary, mother of Jesus} 3. Ambroise-Marie Carré \textbf{4. Reformation} 5. Henry Scott Holland (Recall@5 is 0.75) \\ \midrule
\textbf{Query to Triple (Top-5)} & \begin{tabular}[c]{@{}l@{}}("Bernhard Lichtenberg", "was", "Roman Catholic Priest")\\ ("Bernhard Lichtenberg", "beatified by", "Catholic Church")\\ ("Bernhard Lichtenberg", "died on", "5 November 1943")\\ ("Catholic Church", "beatified", "Bernhard Lichtenberg")\\ ("Bernhard Lichtenberg", "was", "Theologian")\\ All above subjects and objects appear in supporting passages\end{tabular} \\ \midrule
\textbf{Filtered Triple} & Empty \\ 

\bottomrule
\toprule

\textbf{Query} & Who is the grandmother of Philippe, Duke of Orléans? \\
\textbf{Answer} & Marie de' Medici \\ \midrule
\textbf{Supporting Passages (Title)} & 1. Philippe I, Duke of Orléans 2. Leonora Dori \\
\textbf{Retrieved Passages (Title)} & \textbf{1. Philippe I, Duke of Orléans} 2. Louise Élisabeth d'Orléans 3. Philip III of Spain 4. Anna of Lorraine 5. Louis Philippe I (Recall@5 is 0.5)\\ \midrule
\textbf{Query to Triple (Top-5)} & \begin{tabular}[c]{@{}l@{}}("Bank of America", "purchased", "Fleetboston Financial")\\ ("Fleetboston Financial", "was acquired by", "Bank of America")\\ ("Bank of America", "acquired", "Fleetboston Financial")\\ ("Bank of America", "announced purchase of", "Fleetboston Financial") \\ ("Bank of America", "merged with", "Fleetboston Financial")\\ All above subjects and objects appear in supporting passages\end{tabular} \\ \midrule
\textbf{Filtered Triple} & \begin{tabular}[c]{@{}l@{}}("Bank of America", "purchased", "Fleetboston Financial")\\ ("Fleetboston Financial", "was acquired by", "Bank of America")\\  All above subjects and objects appear in supporting passages\end{tabular} \\ 
\bottomrule
\end{tabular}
}
\label{tab:error examples}
    \vskip -0.1in
\end{table*}

\section{Cost and Efficiency}
\label{sec:cost and efficiency}

For LLM deployment, we run Llama-3.3-70B-Instruct on a machine equipped with four NVIDIA H100 GPUs, utilizing tensor parallelism via vLLM \cite{vllm}. We also employ the gpt-4o-mini-2024-07-18 model from OpenAI’s official endpoint, leveraging its batch API\footnote{\url{https://platform.openai.com/docs/guides/batch}}.
For offline indexing, we execute NER and Open IE on the MuSiQue corpus ($11,656$ passages). Processing each passage takes approximately $1.1$ seconds using Llama-3.3-70B-Instruct, while utilizing the gpt-4o-mini batch API allows indexing to complete within 24 hours at a cost of under \$$2$ USD.

\paragraph{Comparison With Structure-Augmented RAG Methods}

We count the token usage across different structure-augmented RAG methods when indexing the MuSiQue corpus using the Llama-3.3-70B-Instruct model, and we compare the number of input and output tokens against RAPTOR \cite{raptor}, LightRAG \cite{lightrag}, and GraphRAG \cite{graphrag} in Table \ref{tab:token_stats}.
\ours not only outperforms these RAG methods in QA and retrieval performance but also uses much fewer tokens compared to LightRAG and GraphRAG.

\begin{table}[ht]
\small
\centering
\caption{Token usage of different structure-augmented RAG methods for indexing the MuSiQue corpus ($11,656$ passages) and their relative proportions.}
    \vskip 0.1in
\label{tab:token_stats}
\begin{tabular}{lrrrr}
\toprule
    & \textbf{\ours} & \textbf{RAPTOR} & \textbf{LightRAG} & \textbf{GraphRAG} \\
    \midrule
    \textbf{Input Tokens}  & $9.2\mathrm{M}$ $(100.0\%)$   & $1.7\mathrm{M}$ $(18.5\%)$ & $68.5\mathrm{M}$ $(744.6\%)$      & $115.5\mathrm{M}$ $(1255.4\%)$              \\
    \textbf{Output Tokens} & $3.0\mathrm{M}$ $(100.0\%)$  & $0.2\mathrm{M}$ \ \ $(6.7\%)$& $18.3\mathrm{M}$ $(610.0\%)$      & $36.1\mathrm{M}$ $(1203.3\%)$        \\
    \bottomrule
\end{tabular}
\end{table}
% Ours: OPENIE 3385456, 600569; TRIPLES 5781526, 2371459
% lightrag total input: 68464264, total output: 18269261
% GraphRAG: 115486094, 36074863
% Raptor(1710270, 170783)

\section{Implementation Details and Hyperparameters}
\label{sec:appendix-hyper}


\subsection{\ours}
\label{ours_impl}

% and implement the personalized Pagerank with the PRPACK library\footnote{\url{}}. 

% \paragraph{Personalized PageRank}
\label{PPR_init}

% For the initialization of the PPR process, we use both phrase nodes and passage nodes as seed nodes.
% Phrase nodes undergo a two-stage process to initialize their scores. 
% In the first stage, we select the top-$5$ most relevant triples associated with each phrase node as input to our filtering mechanism. If the number of phrase nodes within the filtered triples exceeds a threshold of $5$, we compute an average node score based on the initial five triple scores. This average score is then used to initialize the scores of these phrase nodes ($N_{\text{entity}}\leq 5$), while other phrase nodes are set to $0$.
% For passage similarity, each passage node is initialized using the cosine similarity of its normalized embedding, since we find that activating a broader set of potential passages is more effective for uncovering passages along multi-hop reasoning chains, compared to focusing only on the top-ranked passages.


We provide a detailed explanation of the PPR initialization process used in \ours here. The key goal is to determine the seed nodes for the PPR search and assign appropriate reset probabilities to ensure an effective retrieval process.

\paragraph{Seed Node Selection} The seed nodes for the PPR search are categorized into two types: phrase nodes and passage nodes. All the scores given by the embedding model below use normalized embedding to calculate.
1) Phrase Nodes: These seed nodes are selected from the phrase nodes within the filtered triples, which are obtained through the recognition memory component. If recognition memory gives an empty triple list and no phrase node is available, \ours directly returns top passages using the embedding model without any graph search. 
Otherwise, we keep at most $5$ phrase nodes as the seed nodes, and the ranking score of each phrase node is computed as the average score of all filtered triples it appears in.
2) Passage Nodes: Each passage node is initially scored using an embedding-based similarity, and these scores are processed as follows. All passage nodes are taken as seed nodes since we find that activating a broader set of potential passages is more effective for uncovering passages along multi-hop reasoning chains compared to focusing only on the top-ranked passages.

\paragraph{Reset Probability Assignment}
After determining the seed nodes, we assign reset probabilities to control how likely the PPR algorithm will return to these nodes during the random walk. The rules are: 1) Phrase nodes receive reset probabilities directly as their ranking scores. 2) Passage nodes receive reset probabilities proportional to their embedding similarity scores, i.e., to balance the influence of phrase nodes and passage nodes, we apply a weight factor to the passage node scores. 
Specifically, the passage node scores are multiplied by the weight factor discussed in Section \ref{subsec:hyperparameter}. This ensures that passage nodes and phrase nodes contribute appropriately to the retrieval process.

\paragraph{PPR Execution and Passage Ranking}
Once the seed nodes and their reset probabilities are initialized, we run PPR over the constructed graph. The final ranking of passages is determined based on the PageRank scores of the passage nodes. Top-ranked passages are then used as inputs for the downstream QA reading process.
We manage our KG and run the PPR algorithm using the python-igraph library.\footnote{\url{https://python.igraph.org/en/stable/}}

By incorporating both phrase nodes and passage nodes into the PPR initialization, our approach ensures a more effective retrieval of relevant passages, especially for multi-hop reasoning tasks.

\paragraph{Hyperparameters} We perform hyperparameter tuning on $100$ examples from MuSiQue’s training data. 
The hyperparameters are listed in Table \ref{hp_ours}.


\begin{table}[h]
\centering
\caption{Hyperparameters set on \ours}
    \vskip 0.1in
\label{hp_ours}
\small
\begin{tabular}{cc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Synonym Threshold & $0.8$ \\
Damping Factor of PPR & $0.5$ \\
Temperature & $0.0$ \\
\bottomrule
\end{tabular}
    \vskip -0.1in
\end{table}


\subsection{Comparison Methods}

We use PyTorch \cite{pytorch} and HuggingFace \cite{huggingface} for dense retrievers and BM25s \cite{bm25s} for the BM25 implementation.
For GraphRAG \cite{graphrag} and LightRAG \cite{lightrag}, we adhere to their default hyperparameters and prompts. 
To ensure a consistent evaluation, the same QA prompt that \ours adopts from HippoRAG \cite{hipporag} is applied to rephrase the original response of GraphRAG and LightRAG. 

\paragraph{Hyperparameters}

We keep the default indexing hyperparameters for LightRAG and GraphRAG. 
For QA, we perform hyperparameter tuning on the same $100$ samples as Appendix \ref{ours_impl}.

\begin{table}[h]
\centering
\caption{Hyperparameters set on LightRAG and GraphRAG}
    \vskip 0.1in
% \resizebox{0.47\textwidth}{!}{%
\small
\begin{tabular}{lrr}
\toprule
\textbf{Hyperparameters} & \textbf{LightRAG} & \textbf{GraphRAG} \\
\midrule
Mode & Local & Local \\
Response Type & Short phrase & Short phrase \\
Top-k Phrases for QA & $60$ & $60$ \\
Chunk Token Size & $1,200$ & $1,200$ \\
Chunk Overlap Token Size & $100$ & $100$ \\
Community Report Max Length & $2,000$ & $-$ \\
Max Input Length & $8,000$ & $-$ \\
Max Cluster Size & $10$ & $-$ \\
Entity Summary Max Tokens & $-$ & $500$ \\
\bottomrule
\end{tabular}
% }
    \vskip -0.1in
\end{table}

% \begin{table}[h]
% \centering
% \caption{Parameters: LightRAG}
%     \vskip 0.1in
% \begin{tabular}{cc}
% \hline
% \textbf{Parameter} & \textbf{Value} \\
% \hline
% Mode & Local \\
% Response Type & Short phrase \\
% Top-k & 60 \\
% Chunk Token Size & 1200 \\
% Chunk Overlap Token Size & 100 \\
% Community Report Max Length & 2000 \\
% Max Input Length & 8000 \\
% Max Cluster Size & 10 \\
% \hline
% \end{tabular}
%     \vskip -0.1in
% \end{table}

% \begin{table}[h]
% \centering
% \caption{Parameters: GraphRAG}
%     \vskip 0.1in
% \begin{tabular}{cc}
% \hline
% \textbf{Parameter} & \textbf{Value} \\
% \hline
% Mode & Local \\
% Response Type & Short phrase \\
% Top-k & 60 \\
% Chunk Token Size & 1200 \\
% Chunk Overlap Token Size & 100 \\
% Entity Summary to Max Tokens & 500 \\
% \hline
% \end{tabular}
%     \vskip -0.1in
% \end{table}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
