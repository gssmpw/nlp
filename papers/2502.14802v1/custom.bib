% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{
xie2024adaptive,
title={Adaptive Chameleon  or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts},
author={Jian Xie and Kai Zhang and Jiangjie Chen and Renze Lou and Yu Su},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=auKAUJZMO6}
}

@misc{chen2023walkingmemorymazecontext,
      title={Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading}, 
      author={Howard Chen and Ramakanth Pasunuru and Jason Weston and Asli Celikyilmaz},
      year={2023},
      eprint={2310.05029},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.05029}, 
}

@inproceedings{david02topic,
  author       = {Taher H. Haveliwala},
  editor       = {David Lassner and
                  David De Roure and
                  Arun Iyengar},
  title        = {Topic-sensitive PageRank},
  booktitle    = {Proceedings of the Eleventh International World Wide Web Conference,
                  {WWW} 2002, May 7-11, 2002, Honolulu, Hawaii, {USA}},
  pages        = {517--526},
  publisher    = {{ACM}},
  year         = {2002},
  doi          = {10.1145/511446.511513},
  timestamp    = {Tue, 06 Nov 2018 16:57:09 +0100},
  biburl       = {https://dblp.org/rec/conf/www/Haveliwala02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
url={https://dl.acm.org/doi/10.1145/511446.511513}
}

@inproceedings{
jang2022towards_continual_knowledge_learning,
title={Towards Continual Knowledge Learning of Language Models},
author={Joel Jang and Seonghyeon Ye and Sohee Yang and Joongbo Shin and Janghoon Han and Gyeonghun KIM and Stanley Jungkyu Choi and Minjoon Seo},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vfsRB5MImo9}
}

@ARTICLE{continual_learning_wang,
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
  year={2024},
  volume={46},
  number={8},
  pages={5362-5383},
  keywords={Task analysis;Training;Surveys;Testing;Complexity theory;Stability analysis;Visualization;Continual learning;incremental learning;lifelong learning;catastrophic forgetting},
  doi={10.1109/TPAMI.2024.3367329}}


@article{gte,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}


@article{gritlm,
  author       = {Niklas Muennighoff and
                  Hongjin Su and
                  Liang Wang and
                  Nan Yang and
                  Furu Wei and
                  Tao Yu and
                  Amanpreet Singh and
                  Douwe Kiela},
  title        = {Generative Representational Instruction Tuning},
  journal      = {CoRR},
  volume       = {abs/2402.09906},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.09906},
  doi          = {10.48550/ARXIV.2402.09906},
  eprinttype    = {arXiv},
  eprint       = {2402.09906},
  timestamp    = {Thu, 14 Nov 2024 13:27:37 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-09906.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{raptor,
  author       = {Parth Sarthi and
                  Salman Abdullah and
                  Aditi Tuli and
                  Shubh Khanna and
                  Anna Goldie and
                  Christopher D. Manning},
  title        = {{RAPTOR:} Recursive Abstractive Processing for Tree-Organized Retrieval},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=GN921JHCRw},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/SarthiATKGM24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{graphrag,
      title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization}, 
      author={Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
      year={2024},
      eprint={2404.16130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16130}, 
}

@misc{lightrag,
      title={{LightRAG}: Simple and Fast Retrieval-Augmented Generation}, 
      author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
      year={2024},
      eprint={2410.05779},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2410.05779}, 
}


@misc{nvembedv2,
      title={{NV-Embed}: Improved Techniques for Training LLMs as Generalist Embedding Models}, 
      author={Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
      year={2025},
      eprint={2405.17428},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17428}, 
}


@inproceedings{hipporag,
      title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models}, 
      author={Bernal Jiménez Gutiérrez and Yiheng Shu and Yu Gu and Michihiro Yasunaga and Yu Su},
      booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
      year={2024},
      url={https://openreview.net/forum?id=hkujvAPVsg}
}

@inproceedings{bm25,
  author       = {Stephen E. Robertson and
                  Steve Walker},
  editor       = {W. Bruce Croft and
                  C. J. van Rijsbergen},
  title        = {Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic
                  Weighted Retrieval},
  booktitle    = {Proceedings of the 17th Annual International {ACM-SIGIR} Conference
                  on Research and Development in Information Retrieval. Dublin, Ireland,
                  3-6 July 1994 (Special Issue of the {SIGIR} Forum)},
  pages        = {232--241},
  publisher    = {ACM/Springer},
  year         = {1994},
  doi          = {10.1007/978-1-4471-2099-5\_24},
  timestamp    = {Thu, 14 Oct 2021 10:27:21 +0200},
  biburl       = {https://dblp.org/rec/conf/sigir/RobertsonW94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{rear,
  author       = {Yuhao Wang and
                  Ruiyang Ren and
                  Junyi Li and
                  Xin Zhao and
                  Jing Liu and
                  Ji{-}Rong Wen},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {{REAR:} {A} Relevance-Aware Retrieval-Augmented Framework for Open-Domain
                  Question Answering},
  booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16,
                  2024},
  pages        = {5613--5626},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.emnlp-main.321},
  timestamp    = {Thu, 14 Nov 2024 17:20:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/WangRLZLW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{popqa,
    title = "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
    author = "Mallen, Alex  and
      Asai, Akari  and
      Zhong, Victor  and
      Das, Rajarshi  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.546/",
    doi = "10.18653/v1/2023.acl-long.546",
    pages = "9802--9822",
}

@article{musique,
    title = "{MuSiQue}: Multihop Questions via Single-hop Question Composition",
    author = "Trivedi, Harsh  and
      Balasubramanian, Niranjan  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.31/",
    doi = "10.1162/tacl_a_00475",
    pages = "539--554",
}

@inproceedings{2wiki,
  author       = {Xanh Ho and
                  Anh{-}Khoa Duong Nguyen and
                  Saku Sugawara and
                  Akiko Aizawa},
  editor       = {Donia Scott and
                  N{\'{u}}ria Bel and
                  Chengqing Zong},
  title        = {Constructing {A} Multi-hop {QA} Dataset for Comprehensive Evaluation
                  of Reasoning Steps},
  booktitle    = {Proceedings of the 28th International Conference on Computational
                  Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13,
                  2020},
  pages        = {6609--6625},
  publisher    = {International Committee on Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.coling-main.580},
  doi          = {10.18653/V1/2020.COLING-MAIN.580},
  timestamp    = {Fri, 06 Aug 2021 00:39:51 +0200},
  biburl       = {https://dblp.org/rec/conf/coling/HoNSA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@misc{lveval,
      title={{LV-Eval}: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K}, 
      author={Tao Yuan and Xuefei Ning and Dong Zhou and Zhijie Yang and Shiyao Li and Minghui Zhuang and Zheyue Tan and Zhuyu Yao and Dahua Lin and Boxun Li and Guohao Dai and Shengen Yan and Yu Wang},
      year={2024},
      eprint={2402.05136},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.05136}, 
}

@article{contriever,
  author       = {Gautier Izacard and
                  Mathilde Caron and
                  Lucas Hosseini and
                  Sebastian Riedel and
                  Piotr Bojanowski and
                  Armand Joulin and
                  Edouard Grave},
  title        = {Unsupervised Dense Information Retrieval with Contrastive Learning},
  journal      = {Trans. Mach. Learn. Res.},
  volume       = {2022},
  year         = {2022},
  url          = {https://openreview.net/forum?id=jKN1pXi7b0},
  timestamp    = {Fri, 19 May 2023 11:20:42 +0200},
  biburl       = {https://dblp.org/rec/journals/tmlr/IzacardCHRBJG22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@misc{openai_gpt4o_mini,
  author       = {OpenAI},
  title        = {{GPT-4o} Mini},
  year         = {2024},
  url          = {https://platform.openai.com/docs/models#gpt-4o-mini},
  note         = {Accessed: 2025-01-13}
}

@inproceedings{gtr,
    title = "Large Dual Encoders Are Generalizable Retrievers",
    author = "Ni, Jianmo  and
      Qu, Chen  and
      Lu, Jing  and
      Dai, Zhuyun  and
      Hernandez Abrego, Gustavo  and
      Ma, Ji  and
      Zhao, Vincent  and
      Luan, Yi  and
      Hall, Keith  and
      Chang, Ming-Wei  and
      Yang, Yinfei",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.669/",
    doi = "10.18653/v1/2022.emnlp-main.669",
    pages = "9844--9855",
}

@inproceedings{dspy,
  title={{DSPy}: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{
    beir,
    title={{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
    author={Nandan Thakur and Nils Reimers and Andreas R{\"u}ckl{\'e} and Abhishek Srivastava and Iryna Gurevych},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
    year={2021},
    url={https://openreview.net/forum?id=wCu6T5xFjeJ}
}

@article{Beyeler2019,
  author = {Beyeler, M. and Rounds, E. L. and Carlson, K. D. and Dutt, N. and Krichmar, J. L.},
  title = {Neural correlates of sparse coding and dimensionality reduction},
  journal = {PLoS Comput Biol},
  year = {2019},
  volume = {15},
  number = {6},
  pages = {e1006908},
  doi = {10.1371/journal.pcbi.1006908},
  pmid = {31246948},
  pmcid = {PMC6597036},
}

@article{Uner2022,
  author    = {Uner, O. and Roediger III, H. L.},
  title     = {Do recall and recognition lead to different retrieval experiences?},
  journal   = {The American Journal of Psychology},
  year      = {2022},
  volume    = {135},
  number    = {1},
  pages     = {33--43},
}


@misc{bm25s,
      title={{BM25S}: Orders of magnitude faster lexical search via eager sparse scoring}, 
      author={Xing Han Lù},
      year={2024},
      eprint={2407.03618},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.03618}, 
}

@inproceedings{pytorch,
  author       = {Adam Paszke and
                  Sam Gross and
                  Francisco Massa and
                  Adam Lerer and
                  James Bradbury and
                  Gregory Chanan and
                  Trevor Killeen and
                  Zeming Lin and
                  Natalia Gimelshein and
                  Luca Antiga and
                  Alban Desmaison and
                  Andreas K{\"{o}}pf and
                  Edward Z. Yang and
                  Zachary DeVito and
                  Martin Raison and
                  Alykhan Tejani and
                  Sasank Chilamkurthy and
                  Benoit Steiner and
                  Lu Fang and
                  Junjie Bai and
                  Soumith Chintala},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {8024--8035},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/PaszkeGMLBCKLGA19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{huggingface,
  author       = {Thomas Wolf and
                  Lysandre Debut and
                  Victor Sanh and
                  Julien Chaumond and
                  Clement Delangue and
                  Anthony Moi and
                  Pierric Cistac and
                  Tim Rault and
                  R{\'{e}}mi Louf and
                  Morgan Funtowicz and
                  Jamie Brew},
  title        = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/1910.03771},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.03771},
  eprinttype    = {arXiv},
  eprint       = {1910.03771},
  timestamp    = {Tue, 02 Jun 2020 12:49:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@inproceedings{lifelong,
    title = "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
    author = "Jin, Xisen  and
      Zhang, Dejiao  and
      Zhu, Henghui  and
      Xiao, Wei  and
      Li, Shang-Wen  and
      Wei, Xiaokai  and
      Arnold, Andrew  and
      Ren, Xiang",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.351/",
    doi = "10.18653/v1/2022.naacl-main.351",
    pages = "4764--4780"
}

@inproceedings{citb,
    title = "{CITB}: A Benchmark for Continual Instruction Tuning",
    author = "Zhang, Zihan  and
      Fang, Meng  and
      Chen, Ling  and
      Namazi-Rad, Mohammad-Reza",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.633/",
    doi = "10.18653/v1/2023.findings-emnlp.633",
    pages = "9443--9455"
}

@misc{copr,
      title={COPR: Continual Learning Human Preference through Optimal Policy Regularization}, 
      author={Han Zhang and Lin Gui and Yuanzhao Zhai and Hui Wang and Yu Lei and Ruifeng Xu},
      year={2024},
      eprint={2310.15694},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.15694}, 
}

@inproceedings{yao23editing,
    title = "Editing Large Language Models: Problems, Methods, and Opportunities",
    author = "Yao, Yunzhi  and
      Wang, Peng  and
      Tian, Bozhong  and
      Cheng, Siyuan  and
      Li, Zhoubo  and
      Deng, Shumin  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.632/",
    doi = "10.18653/v1/2023.emnlp-main.632",
    pages = "10222--10240",
    abstract = "Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to alter the behavior of LLMs \textbf{efficiently} within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context."
}

@inproceedings{huang24mitigating,
    title = "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal",
    author = "Huang, Jianheng  and
      Cui, Leyang  and
      Wang, Ante  and
      Yang, Chengyi  and
      Liao, Xinting  and
      Song, Linfeng  and
      Yao, Junfeng  and
      Su, Jinsong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.77/",
    doi = "10.18653/v1/2024.acl-long.77",
    pages = "1416--1428",
    abstract = "Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model`s ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains."
}

@article{shi2024continual,
  title={Continual Learning of Large Language Models: A Comprehensive Survey},
  author={Shi, Haizhou and 
          Xu, Zihao and 
          Wang, Hengyi and 
          Qin, Weiyi and 
          Wang, Wenyuan and 
          Wang, Yibin and 
          Wang, Zifeng and 
          Ebrahimi, Sayna and 
          Wang, Hao},
  journal={arXiv preprint arXiv:2404.16789},
  year={2024}
}

@article{klein2006making,
  title={Making sense of sensemaking 1: Alternative perspectives},
  author={Klein, Gary and Moon, Brian and Hoffman, Robert R},
  journal={IEEE intelligent systems},
  volume={21},
  number={4},
  pages={70--73},
  year={2006},
  publisher={IEEE}
}

@article{suzuki2005associative,
  author = {Suzuki, Wendy A.},
  title = {Associative Learning and the Hippocampus},
  journal = {Psychological Science Agenda},
  publisher = {American Psychological Association},
  month = {February},
  year = {2005}
}
