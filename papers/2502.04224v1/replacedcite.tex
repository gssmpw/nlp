\section{Related Work}
\label{sec:related}

\vspace{-2mm}
{\bf Explainable GNNs:} XNNGs can be classified into \textit{decomposition-based}, \textit{gradient-based},  \textit{surrogate-based}, \textit{generation-based},  \textit{perturbation-based}, and \textit{causality-based} methods. \textit{Decomposition-based methods}____ treat the GNN prediction  as a score and decompose it backward layer-by-layer until reaching the input. The score of different parts of the input indicates the importance to the prediction. 
\textit{Gradient-based methods}____ 
take the gradient (implies sensitivity) of the prediction wrt. the input graph, and the sensitivity is used to explain the graph for that prediction.
\textit{Surrogate-based methods}____ replace the complex GNN model with a simple and interpretable surrogate model.
\textit{Generation-based methods} 
____
use generative models to generate explanations. 
E.g., RCExplainer____ 
applies reinforcement learning to generate subgraphs as explanations. 
\textit{Perturbation-based methods}____
uncover the important subgraph as explanations by perturbing the input graph. 
\emph{Causality-based methods} ____ explicitly build the structural causal model for a graph, based on the common assumption that a graph often consists of a underlying causal  subgraph. It then adopts the trainable neural causal model ____ to learn the cause-effect among nodes for causal explanation. 



{\bf Adversarial attacks on GNN classifiers and explainers:}
Almost all existing method  focus on attacking GNN classifiers. 
 They are classified as test-time attacks
____ and training-time attacks
____. 
Test-time attacks carefully perturb test graphs  
so that as many as them are misclassified by a pretrained GNN classifier, 
while training-time attacks carefully perturb training graphs during training, such that the learnt GNN classifier mispredicts as many test graphs as possible. 
____ is the only method on attacking GNN explainers.  
It is a black-box attack (i.e., attacker has no knowledge about XGNN) that aims to corrupt GNN explanations while preserving GNN predictions.

{\bf Certified defenses for GNN classifiers with probabilistic guarantees:} Existing certified defenses____ are for GNN classifiers--they guarantee the same predicted label for a testing graph with arbitrary graph perturbation. 
For instance,
____  generalized randomized smoothing____ from the continuous domain to the discrete graph domain.____ extended randomized ablation____ to build provably robust graph classifiers. 
However, these defenses only provide probabilistic guarantees and cannot be applied to XGNNs.

{\bf Majority voting-based certified defenses with deterministic guarantees:} 
This strategy has been widely used for classification models against adversarial tabular data____,
adversarial 3D points____, adversarial patches____, adversarial graphs____, and {data poisoning attacks____}. 
Their key differences are creating problem-specific voters for majority voting. 
 {However, these defenses  cannot be applied to 
robustify GNN explainers, which are drastically different from classification models.} 

{\bf Certified defenses of explainable non-graph models.} A few works____ propose to provably robustify explainable non-graph (image) models against adversarial perturbations. These methods mainly leverage the idea of randomized smoothing ____ and only provide probabilistic certificates.