\section{Evaluation}
\label{sec:eval}
\vspace{-2mm}

\subsection{Experimental Setup}
\vspace{-2mm}

{\bf Datasets:} 
As suggested by~\citep{agarwal2023evaluating}, we choose datasets with groundtruth explanations for evaluation. 
We adopt the synthetic dataset "SG-Motif", where each graph has a label and ``Motif" is the groundtruth explanation that can be "House", "Diamond", and "Wheel".
We also adopt two real-world graph datasets (i.e., Benzene and FC) with groundtruth explanations from~\cite{agarwal2023evaluating}. 
Their dataset statistics are described in Table~\ref{tab:Datasets} in Appendix~\ref{app:evaluation}.
For each dataset, we randomly sample 70\% graphs for training, 10\% for validation, and use the remaining 20\% graphs for testing. 

{\bf GNN Explainer and Classifier:}
Recent works \citep{funke2022zorro,agarwal2023evaluating} show many GNN explainers (including the well-known GNNExplainer~\cite{GNNEx19}) are unstable, i.e., they yield significantly different explanation results under different runs. We also validate this and show results in Table~\ref{tab:instable} in Appendix. This makes it hard to evaluate the explanation results in a consistent or predictable way. To avoid the issue, we carefully select XGNN baselines with stable explanations: PGExplainer~\citep{DBLP:journals/corr/abs-2011-04573/PGExplainer}, Refine~\citep{wang2021towards}, and GSAT~\citep{DBLP:journals/corr/abs-2201-12987/GSAT}. We also select three well-known GNNs as the GNN classifier: GCN~\citep{kipf2017semi}, GSAGE~\citep{hamilton2017inductive}, and GIN~\citep{xu2018how}.  
We implement  these explainers and classifiers using their publicly available source code. 
Appendix~\ref{app:evaluation} details our training strategy to learn the voting explainer and voting classifier in {\name}. 

{\bf Evaluation Metrics:} We adopt three metrics for evaluation.  
\emph{1) Classification Accuracy}: fraction of testing graphs that are correctly classified, e.g., by our voting classifier; \emph{2) Explanation Accuracy}: fraction of explanatory edges outputted, e.g., by our voting  explainer, are in the groundtruth across all testing graphs; 
\emph{3) Certified Perturbation Size  $M^*$ at Certified Explanation Accuracy (or $\lambda$)}: 
Given a testing graph with groundtruth ($k$) explanatory edges, and a predefined $\lambda$ (or certified explanation accuracy $\lambda/k$), our theoretical result outputs (at least) $\lambda$ explanatory edges on the perturbed testing graph are from the groundtruth, where the testing graph allows arbitrary {$M^*$} perturbations.  
{$M^*$ vs  $\lambda$} then reports the average {$M^*$} of all testing graphs for the given $\lambda$. 

{\bf Parameter Setting:} 
There are several hyperparameters in our {\name}. Unless otherwise mentioned, we use GCN as the default GNN classifier and PGExplainer as the default GNN explainer. 
we use MD5 as the hash function $h$ and we set $\lambda = 3$, $p=0.3$, $T = 70$, $\gamma = 0.3$ and $k$ as Table~\ref{tab:Datasets}. We will also study the impact of these hyperparameters on our defense performance. 

\begin{table}[!t]
    \centering
    \renewcommand\arraystretch{1.15}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|cccc|c|cccc|c|cccc}
		\toprule
      \multirow{2}*{\bf Datasets} & \multicolumn{5}{c|}{\bf PGExplainer} & \multicolumn{5}{c|}{\bf ReFine} &  \multicolumn{5}{c}{\bf GSAT}\\
         \cline{2-16}
         &\multirow{2}{*}{Orig.}&\multicolumn{4}{c|}{T}&\multirow{2}{*}{Orig.}&\multicolumn{4}{c|}{T}&\multirow{2}{*}{Orig.}&\multicolumn{4}{c}{T}\\
        \cline{3-6} \cline{8-11} \cline{13-16}
         & &30&50&70&90&  &30&50&70&90& &30&50&70&90\\
         \Xhline{0.7px}
         SG+House&0.740&0.658&0.725&{0.725}&0.673&0.707&0.588& {0.690}& 0.593&0.564&0.744& {0.759}&0.716&0.673&0.658\\
         \cline{1-16}
        SG+Diamond& 0.745&0.704& {0.730}&0.729& 0.620&0.569&0.440&0.499& 0.521&0.398&0.564&0.426&0.493&0.558&0.420\\
          \cline{1-16}
        SG+Wheel&0.629&0.587&0.612&0.571& 0.542&0.604&0.614& {0.626}& 0.606&0.462&0.568&0.491&0.544&0.612&0.562\\
         \cline{1-16}
         {Benzene}&0.552&0.421&0.497&0.468& 0.429&0.559&0.463& 0.474&  {0.512}&0.440&0.552&0.314&0.430&0.445&0.398\\
         \cline{1-16}
        {FC}&0.531&0.385&0.452&0.373& 0.328&0.503&0.369&0.447& 0.425&0.314&0.487&0.350&0.392&0.412& 0.373\\
        \bottomrule
    \end{tabular}}
    \vspace{-2mm}
    \caption{Explanation accuracy on the original GNN explainers and our {\name}.}
   \vspace{-2mm}
   \label{tab:Explanation}
\end{table}


\begin{table}[!t]
    \footnotesize
        \centering 
    \renewcommand\arraystretch{0.9}
     \resizebox{\textwidth}{!}{\begin{tabular}{c|c|cccc|c|cccc|c|cccc}
     \toprule
       \multirow{2}{*}{\bf Datasets}   & %{GCN} & 
       \multicolumn{5}{c|}{\bf GCN}&  \multicolumn{5}{c|}{\bf GIN}&  \multicolumn{5}{c}{\bf GSAGE} \\
         \cline{2-16} 
        & \multirow{2}{*}{Orig.} & \multicolumn{4}{c|}{T}&\multirow{2}{*}{Orig.} & \multicolumn{4}{c|}{T}&\multirow{2}{*}{Orig.} & \multicolumn{4}{c}{T}\\
        \cline{3-6} \cline{8-11} \cline{13-16}
         & &30&50&70&90& &30&50&70&90& &30&50&70&90\\
         \Xhline{0.7px}
         SG+House& 0.920&0.895&0.905&0.905&0.890&0.945&0.915&0.915&0.900&0.905&0.930&0.900&0.890&0.895&0.875\\
         \cline{1-16}
        SG+Diamond& 0.965&0.935&0.935&0.935&0.930& 0.975&0.935&0.955&0.955&0.955& 0.965&0.940&0.940&0.940&0.940\\
         \cline{1-16}
        SG+Wheel& 0.915
&0.905&0.905&0.900&0.885& 0.930
&0.915&0.905&0.900&0.895& 0.920
&0.910&0.910&0.895&0.890\\
         \cline{1-16}
         Benzene& 0.758&0.746&0.700&0.723&0.707& 0.792&0.736&0.754&0.754&0.754& 0.773&0.725&0.760&0.718&0.718\\
         \cline{1-16}
         FC&0.711&0.674&0.692&0.692&0.631&0.800&0.662&0.714&0.714&0.703&0.723&0.692&0.692&0.692&0.620\\
       \bottomrule
    \end{tabular}}
    \vspace{-2mm}
    \caption{Prediction accuracy on the original GNN classifiers and our {\name}.}
    \vspace{-2mm}
    \label{tab:Prediction}
\end{table}

\begin{table}[!t]
    \tiny
\begin{minipage}[c]{\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|cccc|ccc|ccc}
		\toprule
      \multirow{2}*{\bf Datasets}  &\multicolumn{4}{c|}{$p$}&\multicolumn{3}{c|}{$\gamma$}&\multicolumn{3}{c}{$h$}\\
         \cline{2-11}
         &0.0&0.2&0.3&0.4&0.2&0.3&0.4&{\fontsize{5}{5}\selectfont MD5}&{\fontsize{5}{5}\selectfont SHA1}&{\fontsize{4.5}{5}\selectfont SHA256}\\
         \Xhline{0.7px}
         SG+House&0.053&0.695&0.725&0.710& 0.715&0.725&0.720& 0.725&0.718&0.710\\
         \cline{1-11}
        SG+Diamond& 0.045&0.620&0.729&0.720& 0.712& 0.729&0.718&0.729&0.729&0.721\\
          \cline{1-11}
        SG+Wheel& 0.042&0.511&0.571&0.508& 0.550&0.571&0.564& 0.571&0.565&0.562\\
         \cline{1-11}
         {Benzene}&0.102&0.433&0.468&0.403& 0.440& 0.468&0.452&0.468&0.472&0.468\\
         \cline{1-11}
         {FC}&0.096&0.353&0.373&0.288& 0.345&0.373&0.385& 0.373& 0.382&0.390\\
        \bottomrule
    \end{tabular}
    }
   \vspace{-2mm}
    \caption{Explanation accuracy of our {\name} under different $p$, $\gamma$, and the hash function $h$}
    \label{tab:ablation}
\end{minipage}
\vspace{-4mm}
\end{table}


\subsection{Evaluation Results}
We first show the explanation accuracy and classification accuracy of {\name} under no attack, to validate it can behave similarly to the conventional GNN classifier and GNN explainer. We then show the guaranteed robustness performance of our {\name} against the graph perturbation attack. 


\subsubsection{Explanation Accuracy and Classification Accuracy}
\label{sec:res_emp}
{\bf {\name} maintains the explanation accuracy and classification accuracy on the original GNN explainers  and GNN classifiers:}
Table~\ref{tab:Explanation} shows the explanation accuracy of our {\name} and the original GNN explainers for reference.  
We can observe that {\name} can achieve close explanation accuracies (with a suitable number of subgraph $T$) as the original GNN explainers (which have   different explanation accuracies, due to their different explanation mechanisms). 
This shows the potential of {\name}
as an ensemble based XGNN.   
We also show the classification performance of our voting classifier in {\name} in Table~\ref{tab:Prediction} and  
the original GNNs  classifier for reference. Similarly, we can see our voting classifier learnt based on our training strategy can reach close classification accuracy as the original GNN classifiers.

{\bf Impact of hyperparameters in {\name}:} Next, we will explore the impact of important hyperparameters that  could affect the performance of {\name}. 

\emph{Impact of $T$:} 
Table~\ref{tab:Explanation} shows the 
explanation accuracy of {\name} with different $T$. 
We can see the performance depends on $T$ and the best $T$ in different datasets is different (often not the largest or smallest $T$). Note that the generated hybrid subgraphs use nonexistent edges from the complete graph. If $T$ is too small,  a hybrid subgraph   contains more nonexistent edges, which could exceed the tolerance of the voting explainer. In contrast, a too large $T$ yields very sparse subgraphs, making the useful information in the subgraph that can be used by the explainer be insufficient. This thus makes it hard to ensure explanatory edges have higher important scores than non-explanatory edges. 

\emph{Impact of $p$:} Table~\ref{tab:ablation} shows the explanation accuracy with different $p$, the fraction of the subgraphs generated by the complete graph that are combined with the clean graphs' subgraphs. 
We have a similar observation that a too small or too large $p$ would degrade the explanation performance, with $p=0.3$ obtaining the best performance overall. 
Note that when $p=0$, we only use the information of the original graphs, and the explanation performance is extremely bad. That means it is almost impossible to obtain the groundtruth explanatory edges. 
This thus inspires us to reasonably leverage extra information not in the original graph to guide finding the groundtruth explanatory edges. 

\emph{Impact of $\gamma$:} Table~\ref{tab:ablation} also shows the explanation accuracy with different $\gamma$, the fraction of the edges with the largest scores used for the voting explainer. We  can observe the results are relatively stable in the range $\gamma=[0.2,0.4]$. This is possibly due to that important edges in the original graph are mostly within these edges with the largest scores. 

\emph{Impact of $h$:} The explanation accuracy with different hash functions $h$ are shown in Table~\ref{tab:ablation}. We see the results are insensitive to $h$, suggesting we can simply choose the most efficient one in practice. 

\begin{figure*}[!t]
	\centering
 \subfloat[{SG+House}]
	{\centering\includegraphics[scale=0.225]{png/Pg-House.png}}
 \,
	\subfloat[{SG+Diamond}]
	{\centering \includegraphics[scale=0.22]{png/PG-Diamond.png}}
\,
\subfloat[{Beneze}]
	{\centering\includegraphics[scale=0.22]{png/PG-Benz.png}}
	\caption{Certified perturbation size over all testing graphs vs. $\lambda$ on PGExplainer. The maximum $\lambda$ in x-axis equals to $k$, the number of edges in the groundtruth explanation.} 
	\label{fig:CEA_T_pge}
	 \vspace{-8mm}
\end{figure*}



\begin{figure*}[!t]
	\centering
 \subfloat[{SG+House}]
	{\centering\includegraphics[scale=0.22]{png/House-p.png}}
 \,
	\subfloat[{SG+Diamond}]
	{\centering \includegraphics[scale=0.22]{png/Diamond-p.png}}
\,
 \subfloat[{Beneze}]
 	{\centering\includegraphics[scale=0.22]{png/Benzene-p.png}}
	\caption{Certified perturbation size over all testing graphs  vs. $p$ on PGExplainer.} 
	\vspace{-8mm}
	\label{fig:CEA_p_PG}
\end{figure*}


\begin{figure*}[!t]
	\centering
 \subfloat[{SG+House}]
	{\centering\includegraphics[scale=0.22]{png/House-g.png}}
 \,
	\subfloat[{SG+Diamond}]
	{\centering \includegraphics[scale=0.22]{png/Diamond-g.png}}
\,
 \subfloat[{Beneze}]
 	{\centering\includegraphics[scale=0.22]{png/Benzene-g.png}}
	\caption{Certified perturbation size over all testing graphs vs. $\gamma$ on PGExplainer.} 
	\label{fig:CEA_gamma_PG}
	 \vspace{-4mm}
\end{figure*}

\subsubsection{Certified Explanation Accuracy vs. Certified Perturbation Size}
The certified robustness results are shown in Figures~\ref{fig:CEA_T_pge}-\ref{fig:CEA_h_PG}. Due to limited space, we only show results on three datasets and put results on the other datasets and impact of hyperparameters in Appendix~\ref{app:evaluation}. 

{\bf Impact of $T$:} 
Figure~\ref{fig:CEA_T_pge} and Figures~\ref{fig:CEA_T_pge_2}-\ref{fig:CEA_T_gsat_2} in Appendix~\ref{app:moreresults} show the (average) maximum certified perturbation size vs $\lambda$ with different $T$. 
First, XGNNCert obtains reasonable certified explanation accuracy ($\lambda / k$) against the worst-case graph perturbation, when the \#perturbed edges is bounded by ${M^*}$. 
E.g., with average 6.2 edges are arbitrarily perturbed in SG+House, XGNNCert guarantees at least $\lambda=2$ edges are from the $k=5$ groundtruth explanatory edges. 
Second, there exists a tradeoff between the clean explanation accuracy and robust explanation accuracy. Specifically, as $T$ grows, the derived 
 certified perturbation size increases in general. This means that a larger number of generated subgraphs can enlarge the gap between the largest and  second-largest votes in ${\bf n}^{\gamma}$. On the other hand, the explanation accuracy (under no attack) can be decreased as shown in Section~\ref{sec:res_emp}. 


{\bf Impact of $p$:} The results are in Figure~\ref{fig:CEA_p_PG} and Figure~\ref{fig:CEA_p_PG-2} in Appendix~\ref{app:moreresults}. First, we  observe the certified perturbation size is 0 when $p=0$. This means, without using information in the complete graph, it is impossible to provably defend against the graph perturbation attack.  Second, the certified explanation accuracies are close when $p$ is within the range $[0.2,0.4]$ (which is different from the conclusions on explanation accuracy without attack). This implies, for each clean graph, we can use 20\%-40\% of the subgraphs generated by the complete graph for achieving stable certified explanation accuracy.    

{\bf Impact of $\gamma$:} The results are shown in Figure~\ref{fig:CEA_gamma_PG} and Figure~\ref{fig:CEA_gamma_PG-2} in Appendix~\ref{app:moreresults}. 
Similarly, the certified results are relatively stable 
in the range $\gamma = [0.2, 0.4]$. The key reason could be that important edges in the original graph are mostly within the edges in these range. 

{\bf Impact of $h$:} The results are shown in Figure~\ref{fig:CEA_h_PG}. Like results on explanation accuracy, we can see the hash function $h$ almost does not affect the certified explanation accuracy. Again, this suggests we can choose the most efficient one in practice. 

\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{c|c|c|c|c|c|c}
         \Xhline{0.8pt} 
      \multicolumn{2}{c|}{\bf Datasets} & {SG+House}&SG+Diamond&SG+Wheel&Benzene&FC\\
         \Xhline{0.8pt} 
        \multirow{2}{*}{\bf Exp. Acc.}&{\bf V-InfoR}&0.693&0.419 &0.439 &0.345 &0.217\\
         \cline{2-7}
         &{\bf {\name}}&0.740&0.729&0.571& 0.468&0.403 \\
         
         \Xhline{0.8pt} 
        {\bf Difference}
        &{\bf V-InfoR}&48.39\% & 73.07\% & 65.35\% &83.82\% &63.22\%\\
         \cline{2-7}
         
         {\bf Fraction}&{\bf {\name}}&7.44\% & 0.0\% & 4.20\% & 1.44\% & 1.41\%\\
         \Xhline{0.8pt} 
    \end{tabular}
    \caption{Explanation accuracy and the fraction of different edges under attack in \citet{li2024graph}.}
    \label{tab:Effectiveness}
    \vspace{-4mm}
\end{table}


{
\subsubsection{Defense Effectiveness Against Adversarial Attack on XGNN}
We further test {\name} in the default setting against the recent adversarial attack on XGNN~\citep{li2024graph}, and compare with the  state-of-the-art empirical defense V-InfoR~\citep{wang2023vinfor}. We evaluate their effectiveness by allowing the attacker to change two non-explanatory edges in the graph and taking the fraction of different explanatory edges (before and after the attack) as the metric. The test results are shown in Table~\ref{tab:Effectiveness}. We can observe that: Our {\name} not only achieves the theoretical defense performance and higher explanation accuracy, but also shows much better empirical defense performance than V-InfoR under the powerful attack. This is possibly due to our subgraph division and voting scheme design, which is ``inherently" robust to the strongest attack---it dilutes the adversarial perturbation effect into subgraphs, and at the same time, the number of subgraphs that are affected can be bounded. In contrast, V-InfoR is an empirical defense that constrains the attack capability and is unable to defend against the strong attack. 

\subsubsection{Complexity Analysis of {\name}}
Our {\name} divides each hybrid graph into $T$ subgraphs and applies a base GNN explainer to explain each subgraph. The final explanation is obtained via voting the explanation results of the $T$ subgraphs, whose computational complexity is negligible. Hence, the dominant computational complexity of {\name} is $T$ times of the base GNN explainerâ€™s. For instance, PGExplainer has a complexity of $O(S|V|+|E|)$, where $S$ is the number of optimization steps, and $|V|$ and $|E|$ are the number of nodes and edges, respectively. Therefore, {\name} with PGExplainer as the base explainer has complexity $O(TS|V|+|E|)$. 
Note that the explanation on $T$ subgraphs can be run in parallel, as they are independent of each other. 
{Furthermore, each hybrid subgraph needs to store $p|V|^2$ more edges from the complete graph, where an edge is represented as a pair of node indexes in the implementation. Hence, the extra memory cost per graph is $O(pT|V|^2))$.}
We highlight that the extra computation and memory cost is to ensure the robustness guarantee. In other words, our {\name} obtains a robustness-efficiency tradeoff.
}