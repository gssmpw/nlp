\section{Background and Problem Formulation}
\label{sec:background}


{\bf GNN and XGNN:} 
 We denote a graph as ${G} = (\mathcal{V}, \mathcal{E})$, that consists of a set of nodes $v \in \mathcal{V}$ and edges $e_{u,v} \in \mathcal{E}$. A GNN, denoted as $f$, takes a graph $G$ as input and outputs a predicted label $y = f(G) \in \mathcal{C}$, with $\mathcal{C}$ including all possible labels. For instance, $y$ can be defined on the graph $G$ in graph classification, or on a specific node $v \in \mathcal{V}$ in node classification. An XGNN, denoted as $g$, uncovers the key component in $G$ that contributes to the GNN prediction $y$. In this paper, we focus on the widely-studied edge explanations where $g$ takes $(G,y)$ as input and determines the important edges in $G$. Particularly, this type of XGNN  learns importance scores $\mathbf{m}$ for all edges $\mathcal{E}$; and selects the edges $\mathcal{E}_k \subseteq \mathcal{E}$ with the top $k$ scores in $\mathbf{m}$ as the explanatory edges, where $k$ is a hyperparameter of the XGNN. Formally, ${\bf m} = g(G,y), \mathcal{E}_k = \mathcal{E}.top_{k}({\bf m}) = \mathcal{E}.top_{k}(g(G,y))$. 

\begin{figure}[!t]
\vspace{-4mm}
    \centering
    \captionsetup[subfloat]{labelsep=none, format=plain, labelformat=empty}

    \subfloat[\small (a) GNN Explanation]{
        \includegraphics[width=0.48\linewidth]{before.jpg}
    }\hfil
    \subfloat[\small (b) Adversarial Attack on GNN Explanation]{
        \includegraphics[width=0.48\linewidth]{after.jpg}
    }
    \vspace{-2mm}
    \caption{(a) GNN for graph classification and GNN explanation. 
    A GNN classifier $f$ first predicts a label $y$ for the graph $G$, and then a GNN explainer $g$ interprets the predicted label $y$ to produce the explanatory edges $\mathcal{E}_k$. 
    (b) Two possible graph perturbation attacks on the GNN explainer $g$: 1) the GNN prediction $\hat{y}$ on the perturbed graph $\hat{G}$ is different from $y$; 
    2) the GNN prediction on $\hat{G}$ is kept, but the explanatory edges $\hat{\mathcal{E}}_k$ outputted by $g$ after the attack is largely different from $\mathcal{E}_k$.
    }
    \label{fig:Explanation}
    \vspace{-4mm}
\end{figure}


{\bf Adversarial Attack on XGNN:} Given a graph $G$ and a prediction $y$ (by a GNN $f$), an XGNN $g$ and its explanatory edges $\mathcal{E}_k$. We notice that an attacker can perturb the graph structure to mislead the XGNN $g$. To be specific, the attacker could delete edges from $G$ (to ensure stealthiness, the attacker does not delete edges in the explanatory edges   $\mathcal{E}_k$, as otherwise it can be easily identified) or add new edge into $G$. We denote the adversarially perturbed graph as $\hat{G} = (\mathcal{V}, \hat{\mathcal{E}})$, with an attack budget $M$ (i.e., the total number of deleted and added edges is no more than $M$). On the perturbed graph $\hat{G}$, $f$ gives a new prediction $\hat{y}=f(\hat{G})$, and $g$ produces new explanatory edges $\hat{\mathcal{E}}_k = \hat{\mathcal{E}}.top_k(g(\hat{G},\hat{y}))$.

We assume the attacker has two ways to attack an XGNN, as illustrated in Fig~\ref{fig:Explanation} (b). 
\begin{enumerate}[leftmargin=*]
\vspace{-2mm}
\item \emph{The attacker simply misleads the GNN prediction}. Note that if the prediction is changed (i.e., $\hat{y} \neq y$), even $\hat{\mathcal{E}}_k = \mathcal{E}_k$, the explanation is not useful as it explains the wrong prediction. This attack can be achieved via existing evasion attacks on GNNs, e.g., ~\cite{dai2018adversarial,zugner2018adversarial,xu2019topology,mu2021hard,wang2022bandits}.  


\item \emph{A more stealthy attack keeps the correct prediction, but largely deviates the explanation result.}  That is, the attacker aims to largely enlarge the difference 
between 
$\mathcal{E}_k$ and $\hat{\mathcal{E}}_k$ with  the prediction unchanged~\citep{li2024graph}. 

\end{enumerate}


{\bf Problem Formulation:} 
The above results show existing XGNNs are vulnerable to effective and stealthy graph perturbation attacks. Also, 
as various works~\citep{carlini2019evaluating,mujkanovic2022defenses} have demonstrated, empirical defenses often can be  broken by advanced/stronger attacks. Such observations and past experiences motivate us 
to design \emph{certifiably robust XGNNs}, i.e., that can defend against the \emph{worst-case} graph perturbation attacks with a bounded attack budget. 

\begin{definition}[$(M_{\lambda},\lambda)$-Certifiably robust XGNN]
\label{def:certxgnn}
We say an XGNN is $(M_{\lambda},\lambda)-$certifiably robust, 
if, for \emph{any} graph perturbation attack with no more than $M_{\lambda}$ perturbed edges on a graph $G$, the GNN prediction on the induced perturbed graph $\hat{G}$ always equals to the prediction $y$ on $G$, and there are at least $\lambda$ ($\leq k$) same edges in the explanatory edges $\hat{\mathcal{E}}_k$ after the attack and the explanatory edges $\mathcal{E}_k$ without the attack. 
We also call $M_{\lambda}$ the \emph{certified perturbation size}. 
{Further, we denote by $M_{\lambda}^*$ the \emph{maximal} $M_{\lambda}$  associated with a $\lambda$, for which a specific XGNN remains certifiably robust.} 
\vspace{-2mm}
\end{definition}

\emph{Remark:} A smaller $\lambda$ implies a larger {$M_\lambda^*$}.  
When $\lambda=k$, a robust XGNN should guarantee $\hat{\mathcal{E}}_k = \mathcal{E}_k$. 
In this paper, we focus on deriving the certifiably robust XGNN for the graph-level classification task. The adaptation of the proposed defense techniques (in Section \ref{sec:xgnncert}) to other graph-related tasks, such as node-level classification and edge-level classification, is discussed in Appendix \ref{app:discussion}.

