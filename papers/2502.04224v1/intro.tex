\section{Introduction}
\label{sec:intro}

Explainable Graph Neural Network (XGNN) has emerged recently  to foster the trust of using GNNs---it provides a human-understandable way to interpret the prediction by GNNs. Particularly, given a graph and a predicted node/graph label by a GNN, XGNN aims to uncover the \emph{explanatory edges} (and the connected nodes) from the raw graph that is crucial for predicting the label (see Figure \ref{fig:Explanation}(a) an example). Various XGNN methods~\citep{GNNEx19,DBLP:journals/corr/abs-2011-04573/PGExplainer,DBLP:journals/corr/abs-2102-05152/subgraphX,zhang2022gstarx,wang2023/gnninterpreter,behnam2024graph} have been proposed from different perspectives (more details see Section~\ref{sec:related}) and they have also been widely adopted in applications including  disease diagnosis~\citep{pfeifer2022gnn}, drug analysis~\citep{yang2022mgraphdta,Drug_repurposing2023}, fake news spreader detection~\citep{rath2021scarlet}, and molecular property prediction~\cite{wu2023chemistry}. 
 
While existing works focus on enhancing the explanation performance, the robustness of XGNNs is largely unexplored. 
\citet{li2024graph} observed that  
 well-known XGNN methods (e.g., GNNExplainer~\citep{GNNEx19}, PGExplainer~\citep{DBLP:journals/corr/abs-2011-04573/PGExplainer}) are vulnerable to graph perturbation attacks
---Given a graph, a  GNN model, and a GNN explainer, an adversary can slightly perturb a few edges such that the GNN predictions are accurate, but the explanatory edges outputted by the GNN explainer on the perturbed graph is drastically changed. This attack could cause serious issues in the safety/security-critical applications such as drug analysis. For instance, \cite{Drug_repurposing2023} designs an XGNN tool Drug-Explorer for drug repurposing (reuse existing drugs for new diseases), where users input a drug graph and the tool outputs the visualized explanation result (i.e., important chemical structure) useful for curing the diseases. 
If such tool is misled on adversarial purposes (i.e., adversary inputs a carefully designed perturbed graph), it may recommend invalid drugs with harmful side-effects. Therefore, it is crucial to design defenses for GNN explainers against these attacks.


Generally, defense strategies can be classified as \emph{empirical defense} and \emph{certified defense}. Empirical defenses often can be  broken by stronger/adaptive attacks, as verified in many existing works on defending against adversarial examples~\citep{carlini2019evaluating} and adversarial graphs~\citep{zhang2020backdoor,yang2024distributed}. 
We notice two empirical defense methods  \citep{bajaj2021robust,wang2023vinfor} have been proposed to  robustify XGNNs against graph perturbations. 
Likewise, we found they are ineffective against stronger attacks proposed in \cite{li2024graph} (see Table~\ref{tab:Effectiveness}). 
In this paper, we hence focus on designing certified defense for XGNNs against graph perturbation attacks. {An XGNN is said certifiably robust against a bounded graph perturbation if, for any graph perturbation attack with a perturbation budget that does not exceed this bound, the XGNN consistently produces the same correct explanation (formal definition is in Definition \ref{def:certxgnn}).} 
There are several technical challenges. 
First, GNN explanation and GNN classification are coupled in XGNNs. Robust GNN classifiers do not imply robust GNN explainers, and 
% it is meaningless to 
claiming robust explanations without correct GNN classification is meaningless\footnote{Our additional experiments in Figure~\ref{fig:deceive} in Appendix~\ref{app:evaluation} also validate that if the GNN classifier is deceived, the explanation result would be drastically different compared with the groundtruth explanation.}. It is thus necessary to ensure both robust GNN classification and robust GNN explanation. 
Second, there is a fundamental difference to guarantee the robustness of GNN classifiers and GNN explainers. This is because GNN classifiers map a graph to a label, while  GNN explainers map a graph to an edge set. All existing certified defenses~\citep{jia2020certified,jin2020certified, wang2021certified,xia2024gnncert,li2025agnncert} against graph perturbations focus on the robustness of GNN \emph{classifiers} and cannot be applied to robustify GNN explainers.     


In this work, we propose {\name}, the first {certifiably robust} XGNN against graph perturbation attacks. Given a testing graph, a GNN classifier, and 
 a GNN explainer,
{\name} consists of three main steps. First, we are inspired by 
existing defenses for  classification~\citep{levine2020randomized, jia2021intrinsic,jia2022certified,xia2024gnncert,yang2024distributed,li2025agnncert} that divide an input (e.g., image) into multiple non-overlapping parts (e.g., patches). 
However, directly applying the idea to divide the test graph into multiple non-overlapping subgraphs does not work well for robustifying GNN explainers. 
One reason is that it is hard for the GNN explainer to determine the groundtruth explanatory edges from each subgraph due to its sparsity. 
To address it, we propose to leverage both the test graph and its complete graph for "hybrid" subgraph generation. An innovation design here is only a bounded number of hybrid subgraphs could be affected when the 
test graph is adversarially perturbed with a bounded perturbation, which is the requirement for deriving the robustness guarantee.  
Second, we build a majority-vote classifier on GNN predictions for the generated hybrid subgraphs,  
and a majority-vote explainer on GNN explanations for interpreting 
the prediction 
of the hybrid subgraphs. 
Last, we derive the certified robustness guarantee. 
Particularly, {\name} guarantees the majority-vote classifier yields the same prediction, and majority-vote explainer outputs close explanatory edges for the perturbed testing graph under arbitrary graph perturbations, when the number of perturbed edges is bounded (which we call \emph{certified perturbation size}). 

We evaluate {\name} on multiple XGNN methods on both synthetic and real-world graph dataset with groundtruth explanations. 
Our results show {\name} does not affect the normal explanation accuracy without attack. Moreover, {\name} shows it can guarantees at least 2
edges are from the 5 groundtruth explanatory edges, when averaged 6.2 edges are arbitrarily perturbed in testing graphs from the SG+House dataset.  
Our major contributions are as follows:
\begin{itemize}[leftmargin=*]
\vspace{-2mm}
    \item We propose {\name}, to our best knowledge, the first certified defense for explainable GNN  against graph perturbation attacks. 
    \item We derive the deterministic robustness guarantee of {\name}. 
    \item We evaluate {\name} on multiple graph datasets and GNN explainers and show its effectiveness.  
\end{itemize}

