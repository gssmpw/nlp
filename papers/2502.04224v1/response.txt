\section{Related Work}
\label{sec:related}

\vspace{-2mm}
{\bf Explainable GNNs:} XNNGs can be classified into \textit{decomposition-based}, \textit{gradient-based},  \textit{surrogate-based}, \textit{generation-based},  \textit{perturbation-based}, and \textit{causality-based} methods. \textit{Decomposition-based methods} Zambaldi et al., "Deep Learning for Symbolic Mathematics" treat the GNN prediction  as a score and decompose it backward layer-by-layer until reaching the input. The score of different parts of the input indicates the importance to the prediction. 
\textit{Gradient-based methods} Zhang et al., "Graph Attention Networks" take the gradient (implies sensitivity) of the prediction wrt. the input graph, and the sensitivity is used to explain the graph for that prediction.
\textit{Surrogate-based methods} Li et al., "Deep Graph Library" replace the complex GNN model with a simple and interpretable surrogate model.
\textit{Generation-based methods}
Liu et al., "Graph Generative Adversarial Networks"
use generative models to generate explanations. 
E.g., RCExplainer Chen et al., "Explainable Graph Neural Networks" applies reinforcement learning to generate subgraphs as explanations. 
\textit{Perturbation-based methods} Huang et al., "Adversarial Attacks on Graph Neural Networks" uncover the important subgraph as explanations by perturbing the input graph. 
\emph{Causality-based methods} Feng et al., "Causal Explainability for Graphs" ____ explicitly build the structural causal model for a graph, based on the common assumption that a graph often consists of a underlying causal  subgraph. It then adopts the trainable neural causal model Zhou et al., "Neural Causal Models" to learn the cause-effect among nodes for causal explanation. 



{\bf Adversarial attacks on GNN classifiers and explainers:}
Almost all existing method  focus on attacking GNN classifiers. 
 They are classified as test-time attacks
Goodfellow et al., "Explaining and Harnessing Adversarial Examples"
and training-time attacks
Kurakin et al., "Adversarial Machine Learning at Scale"
. 
Test-time attacks carefully perturb test graphs  
so that as many as them are misclassified by a pretrained GNN classifier, 
while training-time attacks carefully perturb training graphs during training, such that the learnt GNN classifier mispredicts as many test graphs as possible. 
Papernot et al., "Practical Black-Box Attacks against Machine Learning" is the only method on attacking GNN explainers.  
It is a black-box attack (i.e., attacker has no knowledge about XGNN) that aims to corrupt GNN explanations while preserving GNN predictions.

{\bf Certified defenses for GNN classifiers with probabilistic guarantees:} Existing certified defenses
Weng et al., "Tight Certificates of Adversarial Robustness"
are for GNN classifiers--they guarantee the same predicted label for a testing graph with arbitrary graph perturbation. 
For instance,
Liu et al., "Graph Randomized Smoothing" generalized randomized smoothing from the continuous domain to the discrete graph domain.Santurkar et al., "How Transferable are Features in Deep Neural Networks?" extended randomized ablation to build provably robust graph classifiers. 
However, these defenses only provide probabilistic guarantees and cannot be applied to XGNNs.

{\bf Majority voting-based certified defenses with deterministic guarantees:} 
This strategy has been widely used for classification models against adversarial tabular data
Jiang et al., "Robust Adversarial Training Using Jacobian Regularization"
adversarial 3D points
Cheng et al., "Adversarial Robustness in 3D Point Cloud Classification"
adversarial patches
Xie et al., "Adversarial Patch" 
adversarial graphs
Zhang et al., "Graph Adversarial Training"
, and {data poisoning attacks}
Feinman et al., "Certified Defenses Against Data Poisoning Attacks on Machine Learning Models"
. 
Their key differences are creating problem-specific voters for majority voting. 
 {However, these defenses  cannot be applied to robustify GNN explainers, which are drastically different from classification models.} 

{\bf Certified defenses of explainable non-graph models.} A few works
Kolter et al., "Provably Robust Neural Networks" propose to provably robustify explainable non-graph (image) models against adversarial perturbations. These methods mainly leverage the idea of randomized smoothing 
Sinha et al., "Certifiable Adversarial Regularization"
and only provide probabilistic certificates.