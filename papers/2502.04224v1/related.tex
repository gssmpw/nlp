\section{Related Work}
\label{sec:related}

\vspace{-2mm}
{\bf Explainable GNNs:} XNNGs can be classified into \textit{decomposition-based}, \textit{gradient-based},  \textit{surrogate-based}, \textit{generation-based},  \textit{perturbation-based}, and \textit{causality-based} methods. \textit{Decomposition-based methods}~\citep{schnake2021higher,feng2023degree} treat the GNN prediction  as a score and decompose it backward layer-by-layer until reaching the input. The score of different parts of the input indicates the importance to the prediction. 
\textit{Gradient-based methods}~\citep{baldassarre2019explainability,pope2019explainability} 
take the gradient (implies sensitivity) of the prediction wrt. the input graph, and the sensitivity is used to explain the graph for that prediction.
\textit{Surrogate-based methods}~\citep{vu2020pgm,pereira2023distill} replace the complex GNN model with a simple and interpretable surrogate model.
\textit{Generation-based methods} 
~\citep{GEM,sui2022causal,shan2021reinforcement/RGExplainer,Wang_2023/RCExplainer}
use generative models to generate explanations. 
E.g., RCExplainer~\citep{Wang_2023/RCExplainer} 
applies reinforcement learning to generate subgraphs as explanations. 
\textit{Perturbation-based methods}~\citep{GNNEx19,DBLP:journals/corr/abs-2011-04573/PGExplainer,wang2021towards,funke2022zorro}
uncover the important subgraph as explanations by perturbing the input graph. 
\emph{Causality-based methods} \citep{behnam2024graph} explicitly build the structural causal model for a graph, based on the common assumption that a graph often consists of a underlying causal  subgraph. It then adopts the trainable neural causal model \citep{xia2021causal} to learn the cause-effect among nodes for causal explanation. 



{\bf Adversarial attacks on GNN classifiers and explainers:}
Almost all existing method  focus on attacking GNN classifiers. 
 They are classified as test-time attacks
\citep{dai2018adversarial,zugner2018adversarial,ma2020towards,mu2021hard,wang2022bandits,wang2023turning,wang2024efficient} and training-time attacks
\citep{xu2019topology,zugner2019adversarial,wang2019attacking,zhang2021backdoor,wang2023turning}. 
Test-time attacks carefully perturb test graphs  
so that as many as them are misclassified by a pretrained GNN classifier, 
while training-time attacks carefully perturb training graphs during training, such that the learnt GNN classifier mispredicts as many test graphs as possible. 
\citep{li2024graph} is the only method on attacking GNN explainers.  
It is a black-box attack (i.e., attacker has no knowledge about XGNN) that aims to corrupt GNN explanations while preserving GNN predictions.

{\bf Certified defenses for GNN classifiers with probabilistic guarantees:} Existing certified defenses~\citep{bojchevski2020efficient,wang2021certified,zhang2021backdoor} are for GNN classifiers--they guarantee the same predicted label for a testing graph with arbitrary graph perturbation. 
For instance,
\citet{wang2021certified}  generalized randomized smoothing~\citep{lecuyer2019certified,cohen2019certified,hong2022unicr} from the continuous domain to the discrete graph domain.~\citet{zhang2021backdoor} extended randomized ablation~\citep{levine2020robustness} to build provably robust graph classifiers. 
However, these defenses only provide probabilistic guarantees and cannot be applied to XGNNs.

{\bf Majority voting-based certified defenses with deterministic guarantees:} 
This strategy has been widely used for classification models against adversarial tabular data~\citep{hammoudeh2023feature},
adversarial 3D points~\citep{zhang2023pointcert}, adversarial patches~\citep{levine2020randomized,xiang2021patchguard}, adversarial graphs~\citep{xia2024gnncert,yang2024distributed,li2025agnncert}, and {data poisoning attacks~\citep{levine2020deep,jia2021intrinsic,wang2022improved,jia2022certified}}. 
Their key differences are creating problem-specific voters for majority voting. 
 {However, these defenses  cannot be applied to 
robustify GNN explainers, which are drastically different from classification models.} 

{\bf Certified defenses of explainable non-graph models.} A few works~\citep{levine2019certifiably,liu2022certifiably,tan2023robust} propose to provably robustify explainable non-graph (image) models against adversarial perturbations. These methods mainly leverage the idea of randomized smoothing \citep{lecuyer2019certified,cohen2019certified} and only provide probabilistic certificates.
