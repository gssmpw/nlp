\documentclass[11pt,letterpaper]{article}
% \usepackage[sc,osf,noBBpl]{mathpazo}


%\usepackage[letterpaper,top=1in,bottom=1in,left=1in,right=1in,marginparwidth=1in]{geometry}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} % control layout
\usepackage{setspace} % control line spacing
\linespread{1.1}
\usepackage{graphicx}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{dsfont}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref}
\usepackage{enumitem}
\setlist{topsep=3pt,itemsep=0pt}
\usepackage{bm}
\usepackage{bbm}
\usepackage{soul}
\usepackage{appendix,url}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{outlines}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{wrapfig}
\usepackage[outdir=./]{epstopdf}
\usepackage{multirow}
\usepackage{array}
\usepackage{adjustbox}
\usepackage{booktabs}     
\usepackage{float}

\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear}

\allowdisplaybreaks


 
\input{my-macros}
\input{math_commands}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

% %Header
% \pagestyle{fancy}
% \thispagestyle{empty}
% \rhead{ \textit{ }} 

% % Update your Headers here
% \fancyhead[CO]{ID policy (with reassignment) is asymptotically optimal for heterogeneous weakly-coupled MDPs}

\author{%
Xiangcheng Zhang$^{1}\footnotemark[1] ~~\footnotemark[2]$ \quad Yige Hong$^{2}\footnotemark[1]$ \quad  Weina Wang$^{2}\footnotemark[3]$   \medskip\\
$^1$Weiyang College, Tsinghua University  \\ $^2$ Computer Science Department, Carnegie Mellon University \\
\texttt{xc-zhang21@mails.tsinghua.edu.cn}\\
\texttt{\{yigeh,weinaw\}@cs.cmu.edu}
}
\title{ID policy (with reassignment) is asymptotically optimal for heterogeneous weakly-coupled MDPs}
\date{}


\begin{document}
\maketitle

\footnotetext[1]{These authors contributed equally to this work.}
\footnotetext[2]{Work done during a visit at Carnegie Mellon University.}
\footnotetext[3]{Corresponding author.}
\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}


\begin{abstract}%
Heterogeneity poses a fundamental challenge for many real-world large-scale decision-making problems but remains largely understudied.
In this paper, we study the \emph{fully heterogeneous} setting of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs).
Each WCMDP consists of $N$ arms (or subproblems), which have distinct model parameters in the fully heterogeneous setting, leading to the curse of dimensionality when $N$ is large.
We show that, under mild assumptions, a natural adaptation of the ID policy, although originally proposed for a homogeneous special case of WCMDPs, in fact achieves an $O(1/\sqrt{N})$ optimality gap in long-run average reward per arm for fully heterogeneous WCMDPs as $N$ becomes large.
This is the first asymptotic optimality result for fully heterogeneous average-reward WCMDPs.
Our techniques highlight the construction of a novel projection-based Lyapunov function, which witnesses the convergence of rewards and costs to an optimal region in the presence of heterogeneity.
\end{abstract}



\noindent \small\textbf{Keywords:} 
weakly-coupled Markov decision processes, fully heterogeneous systems, asymptotic optimality, planning, average-reward Markov decision processes, Lyapunov analysis





\tableofcontents

\section{Introduction}

Heterogeneity poses a fundamental challenge for many real-world decision-making problems, where each problem consists of a large number of interacting components.
However, despite its practical significance, heterogeneity remains largely understudied in the literature.
In this paper, we study \emph{heterogeneous} settings of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs) \citep{Haw_03}.
A WCMDP consists of $N$ \emph{arms} (or \emph{subproblems}), where each arm itself is a Markov decision process (MDP).
In a heterogeneous setting, the arms have distinct model parameters.
At each time step, the decision-maker selects an action for each arm, which affects the arm's transition probabilities and reward, and then the arms make state transitions independently.
However, these actions are subject to a set of global \emph{budget constraints}, where each constraint limits one type of total cost across all arms at each time step.
The objective is to find a policy that maximizes the long-run \emph{average reward} over an infinite time horizon.
We focus on the \emph{planning} setting, where all the model parameters are known.


WCMDPs have been used to model a wide range of applications, including online advertising \citep{boutilier2016budget,zhou2023rl}, job scheduling \citep{yu2018deadline}, healthcare \citep{biswas2021learning_arxiv}, surveillance \citep{villar2016indexability}, and machine maintenance  \citep{GlaMitAns_05_rb_repair}.
A faithful modeling of these applications calls for \emph{heterogeneity}.
For instance, in \citep{biswas2021learning_arxiv}, arms are beneficiaries of a health program and they could react to interventions differently; in \citep{villar2016indexability}, arms are targets of surveillance who have different locations and probabilities to be exposed; in \citep{GlaMitAns_05_rb_repair}, arms are machines that could require distinct repair schedules.  


Although heterogeneity is crucial in the modeling of these applications, most existing work on average-reward WCMDPs (or their special cases) establish asymptotic optimality only for the homogeneous setting where all arms share the same set of model parameters
\citep{WebWei_90,Ver_16_verloop,GasGauYan_23_exponential,GasGauYan_23_whittles,HonXieChe_23,HonXieChe_24,hong2024exponential,Yan_24,GolAvr_24_wcmdp_multichain}. 
An exception is \citep{Ver_16_verloop}, which considers the \emph{typed heterogeneous} setting, where the $N$ arms are divided into a constant number of types as $N$ scales up, with each type having distinct model parameters.
While heterogeneous WCMDPs have been studied under the finite-horizon total-reward and discounted-reward criteria, as we review in Appendix~\ref{app:additional-related-work}, these results do not extend to the average-reward setting we consider. 

The key distinction between the homogeneous (or typed heterogeneous) setting and the fully heterogeneous setting is whether the arms can be divided into a \emph{constant} number of homogeneous groups.
In the former, the system dynamics depends only on the fraction of arms in each state in each homogeneous group.
Thus, the effective dimension of the state space is polynomial in $N$.
In contrast, in the fully heterogeneous setting, the state space grows exponentially in $N$, making the problem truly high-dimensional.


\paragraph{Our contribution}
In this paper, we study \emph{fully heterogeneous} WCMDPs.
We consider a natural adaptation of the \emph{ID policy}, originally proposed by \citet{HonXieChe_24} for \emph{homogeneous} restless bandits---a renowned special case of WCMDPs.
Given that the ID policy was designed for a much simpler setting, its effectiveness in fully heterogeneous WCMDPs is far from obvious.
However, with proper adaptation and novel theoretical techniques, we show that the ID policy achieves an $O(1/\sqrt{N})$ optimality gap under mild assumptions as the number of arms $N$ becomes large.
Here, the optimality gap is the gap between the long-run average reward per arm achieved by the ID policy and that under the optimal policy.
This is the first result establishing asymptotic optimality for fully heterogeneous average-reward WCMDPs.


Here we briefly describe how the adapted ID policy works.
While it retains the core structure of the original ID policy, modifications are needed to handle heterogeneity.
The policy consists of two phases.
The first phase is a pre-processing phase, where we compute an \emph{optimal single-armed policy} for each arm (denoted as $\pibs_i$ for the $i$-th arm) that prescribes the \emph{ideal action} the arm would take at each state.
These optimal single-armed policies can be efficiently solved via a linear program. 
The second phase is the real-time phase.
At each time step, the policy iterates over the arms in a fixed order, allowing as many arms as possible to follow their respective ideal actions while satisfying the budget constraints. 
Unlike the original ID policy, which has only one optimal single-armed policy due to homogeneous arms, our adapted version computes $N$ optimal single-armed policies, one for each arm.  
Additionally, we introduce an ID reassignment procedure before the real-time phase that reorders arms to ensure a regularity property. 

Our assumptions are in terms of the optimal single-armed policies.
We assume that they induce aperiodic unichains and their transition probability matrices have spectral gaps bounded away from zero.
These assumptions generalize the one in \citep{HonXieChe_23} and are weaker than most assumptions in previous papers when specialized to their settings.


\paragraph{Technical novelty}
The main technical innovation of the paper is the introduction of a novel Lyapunov function for fully heterogeneous WCMDPs. 
Specifically, to prove the asymptotic optimality of a policy, a key step is to show that the system state is globally attracted to an \emph{optimal region} where most arms can follow the ideal actions generated by their respective optimal single-armed policies $\pibs_i$'s. 
Let $\mS_t = (S_{1,t}, \dots, S_{N, t})$ denote the joint state of the $N$ arms at time $t$.
Then this optimal region consists of those states $\mS_t$'s whose each coordinate $S_{i,t}$ is approximately an independent sample from a certain \emph{optimal state distribution} $\mu^*_i$ for the $i$-th arm, for $i=1,2,\dots,N$. 
In the \emph{homogeneous setting}, there is only one optimal state distribution $\mu^*$. Consequently, the optimal region in that case is the set of system states whose \emph{empirical distribution across coordinates} remains sufficiently close to $\mu^*$; global attraction to this region could be established by a Lyapunov function that depends on the empirical distribution of $\mS_t$. 
In the \emph{heterogeneous setting}, however, it has been unclear how such a Lyapunov function could be constructed. 
Intuitively, this Lyapunov function should focus on the collective properties of $\mS_t$ rather than the states of individual arms, so that the statistical patterns across the coordinates of $\mS_t$ can be captured. 
Our technique is to \emph{project} $\mS_t$ onto a set of carefully selected feature vectors, and define the Lyapunov function based on these projections. 
These feature vectors encode the minimal amount of information needed to evaluate the relevant functions of the system state (e.g., instantaneous reward or cost) and predict their future expectations.  
This projection-based Lyapunov function provides a principled way to measure deviations of the system state from the optimal region in a fully heterogeneous setting. 
A more detailed discussion of this approach can be found in \Cref{sec:result-tech-overview}. 


Beyond WCMDPs, our techniques have the potential to be applied to more general heterogeneous large stochastic systems.
Heterogeneity has been a topic of strong interest in these systems, but it is known to be a challenging problem with limited theoretical results.
Only recently have there been notable breakthroughs.
\citep{AllGas22_het,AllGas22_graphon} extended the popular mean-field analysis to a class of heterogeneous large stochastic systems for the first time, but the results are only for transient distributions.
Another line of work \citep{ZhaMukWu_24_data_loc_het,ZhaMuk_24_rate_mat_prun} studied heterogeneous load-balancing systems.
They first analyzed the transient distributions and then used interchange-of-limits arguments to extend the results to steady state.
Our method provides a more direct framework for steady-state analysis and has the potential to generalize to a broader range of heterogeneous stochastic systems.


\paragraph{Related work}
WCMDPs have been extensively studied with a rich body of literature.
Here we provide a brief overview of the most relevant work, and we refer the reader to Appendix~\ref{app:additional-related-work} for a more detailed survey.

We first focus on the \emph{average-reward} criterion.
As mentioned earlier, most existing work considers the \emph{homogeneous setting}.
Early work on WCMDPs primarily focuses on a special case known as the \emph{restless bandit (RB)} problem, where each arm's MDP has a binary action space (active and passive actions) and there is only one budget constraint that limits the total number of active actions across all arms at each time step.
The seminal work by \citet{Whi_88_rb} introduced the RB problem and the celebrated Whittle index policy, which was later shown to achieve an $o(1)$ optimality gap as $N\to\infty$ under a set of conditions \citep{WebWei_90}.
Subsequent work on RBs has focused on designing policies that achieve asymptotic optimality under more relaxed conditions \citep{Ver_16_verloop,HonXieChe_23,HonXieChe_24,Yan_24}, as well as improving the optimality gap to $O(1/\sqrt{N})$ \citep{HonXieChe_23,HonXieChe_24} or $O(\exp(-cN))$ \citep{GasGauYan_23_exponential,GasGauYan_23_whittles,hong2024exponential}.
Among these papers, only \citep{Ver_16_verloop} addresses \emph{heterogeneity}, but in the \emph{typed heterogeneous} setting, where the $N$ arms are divided into a constant number of types as $N\to\infty$.

Beyond RBs, work on general average-reward WCMDPs is scarce.
The closest to ours is \citep{Ver_16_verloop}, which considered \emph{typed heterogeneous} WCMDPs with a \emph{single} budget constraint and established an $o(1)$ optimality gap.
More recently, \citep{GolAvr_24_wcmdp_multichain} proved the first asymptotic optimality result for \emph{homogeneous} WCMDPs, also achieving an $o(1)$ optimality gap. 


Under the \emph{finite-horizon total-reward} or \emph{discounted-reward} criteria, there has been more work on heterogeneous settings, including both the typed heterogeneous setting \citep{DaeChoGri_23,GhoNagJaiTam_23_finite_discount} and, more recently, the fully heterogeneous setting \citep{BroSmi_19_rb,BroZha_22,BroZha_23,Zhang24_het}.
However, the optimality gap in these papers generally grow \emph{super-linearly} with the (effective) time horizon, except under restrictive conditions.
As a result, it would be difficult to extend these results to the average-reward setting and still achieve asymptotic optimality.


\paragraph{General notation.}
Let $\mathbb{R}$, $\mathbb{N}$, and $\mathbb{N}_+$ denote the sets of real numbers, nonnegative integers, and positive integers, respectively.
Let $[N]\triangleq\{1,2,\dots,N\}$ for any $N\in \mathbb{N}_+$ and $[n_1:n_2]\triangleq\{n_1,n_1+1,\dots,n_2\}$ for $n_1,n_2\in\mathbb{N}_+$ with $n_1\le n_2$.
Let $[0,1]_N=\{i/N\colon i\in\mathbb{N}, 0\le i/N\le 1\}$, the set of integer multiples of $1/N$ in $[0,1]$.
For a matrix ${A}\in\RR^{d\times d}$, we denote its operator norm as $\norm{{A}}_p=\sup_{x\neq \mathbf{0}}\frac{\norm{{A}x}_{p}}{\norm{x}_p}$, and refer to the $2$-norm as $\norm{A}$ for simplicity.
We use boldface letters to denote matrices, and regular letters to denote vectors and scalars. We write $\R^{\sspa}$ for the set of real-valued vectors indexed by elements of $\sspa$, or equivalently, the set of real-valued functions on $\sspa$; for each $v\in \R^{\sspa}$, let $v(s)$ to denote its element corresponding to $s\in\sspa$. 




\section{Problem setup}\label{sec:background}
We consider a weakly-coupled Markov decision process (WCMDP) that consists of $N$ arms.
Each arm $i\in[N]$ is associated with a smaller MDP denoted as $\cM_i=\left(\sspa, \aspa,\PP_i, r_i, (c_{k,i})_{k\in[K]}\right)$.
Here $\sspa$ and $\aspa$ are the state space and the action space, respectively, both assumed to be finite; $\PP_i$ describes the transition probabilities with $\PP_i(s'\mid s,a)$ being the transition probability from state $s$ to state $s'$ when action $a$ is taken.
The state transitions of different arms are independent given the actions.
When arm~$i$ is in state $s$ and we take action $a$, a reward $r_i(s,a)$ is generated, as well as $K$ types of costs $c_{k,i}(s,a), k\in[K]$. 
We assume that the costs are nonnegative, i.e., $c_{k,i}(s,a)\ge 0$ for all $i\in\mathbb{N}_+,k\in[K],s\in\sspa,$ and $a\in\aspa$.
Note that we allow the arms to be \emph{fully heterogeneous}, i.e., the $\cM_i$'s can be \emph{all distinct}.


When taking an action for each arm in this $N$-armed system, we are subject to cost constraints.
Specifically, suppose each arm $i$ is in state $s_i$.
Then the actions, $a_i$'s, should satisfy the following constraints
\begin{align}\label{eq:cost}
    \sum_{i\in[N]} c_{k,i}(s_i, a_i)\le \alpha_k N,\quad \forall k\in[K],
\end{align}
where each $\alpha_k>0$ is a constant independent of $N$, and $\alpha_k N$ is referred to as the \emph{budget} for type-$k$ cost.
We assume that there exists an action $0\in\aspa$ that does not incur any type of cost for any arm at any state, i.e., $c_{k,i}(s,0)=0$ for all $k\in[K],i\in[N],s\in\sspa$.
This assumption guarantees that there always exist valid actions (e.g., taking action $0$ for every arm) regardless of the states of the arms.


\paragraph{Policy and system state.}
A policy $\pi$ for the $N$-armed problem specifies the action for each of the $N$ arms, in a possibly history-dependent way.
Under policy $\pi$, let $S_{i,t}^\pi$ denote the state of the $i$th arm at time $t$, and we refer to $\bm{S}^\pi_t \triangleq (S_{i,t}^\pi)_{i\in[N]}$ as the \emph{system state}. Similarly,
let $A_{i,t}^\pi$ denote the action applied to arm $i$ at time $t$, and we refer to $\bm{A}^\pi_t \triangleq (A_{i,t}^\pi)_{i\in[N]}$ as the \emph{system action}. 
In this paper, we also use an alternative representation of the system state, denoted as $\mX_t^\pi$ and defined as follows.
Let $X_{i,t}^\pi=(X_{i,t}^\pi(s))_{s\in\sspa}\in\mathbb{R}^{|\sspa|}$ be a row vector where the entry corresponding to state $s$ is given by $X_{i,t}^{\pi}(s)=\mathbbm{1}\{S_{i,t}^\pi=s\}$; i.e., $X_{i,t}^\pi$ is a one-hot row vector whose $s$'s entry is $1$ if $S_{i,t}^\pi=s$ and is $0$ otherwise.
Then let $\mX_t^\pi$ be an $N\times|\sspa|$ matrix whose $i$th row is $X_{i,t}^\pi$.
It is easy to see that $\mX_t^\pi$ contains the same information as $\bm{S}^\pi_t$, and we refer to both of them as the system state.
In this paper, we often encounter vectors like $X_{i,t}^\pi=(X_{i,t}^\pi(s))_{s\in\sspa}$, whose entries correspond to different states in $\sspa$.
For such vectors, say $u$ and $v$, we use the inner product to write a sum for convenience $\innerproduct{u}{v}\triangleq \sum_{s\in\sspa}u(s)v(s)$.
We sometimes omit the superscript $\pi$ when it is clear which policy is being used.



\paragraph{Maximizing average reward.}
Our objective is to maximize the long-run time-average reward subject to the cost constraints.
To be more precise, we follow the treatment for maximizing average reward in \citep{Put_05}. 
For any policy $\pi$ and an initial state $\bm{S}_0$ of the $N$-armed system, 
consider the \emph{limsup} average reward $R^+(\pi,\bm{S}_0)$ and the \emph{liminf} average  $R^-(\pi,\bm{S}_0)$, defined as $R^+(\pi,\bm{S}_0)=\lim\sup_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{N}\sum_{i\in[N]}\E{r_i(S_{i,t}^\pi, A_{i,t}^\pi)}$ and $R^-(\pi,\bm{S}_0)=\lim\inf_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{N}\sum_{i\in[N]}\E{r_i(S_{i,t}^\pi, A_{i,t}^\pi)}$. 
If $R^+(\pi,\bm{S}_0)=R^-(\pi,\bm{S}_0)$, then the average reward of policy $\pi$ under initial condition $\bm{S}_0$ exists and is defined as
\begin{equation}
    R(\pi,\bm{S}_0) = R^+(\pi,\bm{S}_0) = R^-(\pi,\bm{S}_0)=\lim_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{N}\sum_{i\in[N]}\E{r_i(S_{i,t}^\pi, A_{i,t}^\pi)}.
\end{equation}
Note that these reward notions divide the total reward from all arms by the number of arms, $N$, measuring the reward \emph{per arm}.
The WCMDP problem is to solve the following optimization problem:
\begin{subequations}\label{eq:WCMDP}
\begin{align}
    \label{eq:N-arm-formulation} 
    \underset{\text{policy } \pi}{\text{maximize}} &\quad R^-(\pi,\bm{S}_0)\\
    \text{subject to}  
    &\mspace{12mu}  \sum_{i\in[N]} c_{k,i}(S_{i,t}^\pi, A_{i,t}^\pi)\le \alpha_k N,\quad \forall k\in[K],\forall t\ge 0. \label{eq:hard-budget-constraint}
\end{align}
\end{subequations}
Let the optimal value of this problem be denoted as $R^*(N,\bm{S}_0)$.
Note that since the WCMDP is an MDP with finite state and action space, if we replace the $R^-(\pi,\bm{S}_0)$ in the objective \eqref{eq:N-arm-formulation} with $R^+(\pi,\bm{S}_0)$, the optimal value stays the same.

\paragraph{Asymptotic optimality.}
Recall that exactly solving the WCMDP problem is PSPACE-hard.
In this paper, our goal is to design a policy $\pi$ that is \emph{efficiently computable} and \emph{asymptotically optimal} as $N\to\infty$, with the following notion for asymptotic optimality.
For any policy $\pi$, we define its \emph{optimality gap} as $R^*(N,\bm{S}_0)-R^-(\pi,\bm{S}_0)$.
We say the policy $\pi$ is \emph{asymptotically optimal} if as $N\to\infty$,
\begin{equation}
    R^*(N,\bm{S}_0)-R^-(\pi,\bm{S}_0)=o(1).
\end{equation}
When we take this asymptotic regime as $N\to\infty$, we keep the number of constraints, $K$, as well as the budget coefficients, $\alpha_1,\alpha_2,\dots,\alpha_K$, fixed.
We assume that the reward functions and cost functions are uniformly bounded, i.e., $\sup_{i\in\mathbb{N}_+}\max_{s\in\sspa,a\in\aspa}|r_i(s,a)|\triangleq r_{\max}<\infty$
and
$\sup_{i\in\mathbb{N}_+}\max_{k\in[K],s\in\sspa,a\in\aspa}c_{k,i}(s,a)\triangleq c_{\max}<\infty$.
This notion for asymptotic optimality is consistent with that in the existing literature (e.g., \citep[Definition 4.11]{Ver_16_verloop}). We are interested in not only achieving asymptotic optimality but also characterizing the \emph{order} of the optimality gap.


In the remainder of this paper, we focus on stationary Markov policies.
Under such a policy, the system state $\bm{S}_t$ forms a finite-state Markov chain.
Therefore, its time-average reward $R(\pi,\bm{S}_0)= R^+(\pi,\bm{S}_0) = R^-(\pi,\bm{S}_0)$ is well-defined. 


\paragraph{LP relaxation and an upper bound on optimality gap.}
We consider the linear program (LP) below, which will play a critical role in performance analysis and policy design:
\begin{subequations}\label{eq:lp}
\begin{align}
    R^{\rel}_N\triangleq&\underset{(y_i(s,a))_{i\in [N], s\in\sspa,a\in\aspa}}{\text{maximize}}\mspace{9mu}\frac{1}{N}\sum_{i\in[N]} \sum_{s\in\sspa,a\in\aspa} y_i(s,a)r_i(s,a)\\
    \label{eq:lp:budget-constraint}
    &\mspace{32mu}\text{subject to}\mspace{40mu}\frac{1}{N}\sum_{i\in[N]}\sum_{s\in\sspa,a\in\aspa} y_i(s,a)c_{k,i}(s,a)\leq \alpha_k,\quad\forall k\in[K],\\
    \label{eq:lp:stationarity-constraint}
    &\mspace{130mu} \sum_{s'\in\sspa,a'\in\aspa} \PP_i(s\mid s',a')y_i(s',a')=\sum_{a\in\aspa}y_i(s,a),\quad\forall s\in\sspa,\forall i\in[N],\\
    \label{eq:lp:probability-constraint}
   &\mspace{135mu}\sum_{s'\in\sspa,a'\in\aspa}y_i(s',a')=1, \qquad y_i(s,a)\geq 0, \quad \forall s\in\sspa,  \forall a\in \aspa,\forall i\in[N].
\end{align}
\end{subequations}
Lemma~\ref{lem:lp upper bound} below establishes a connection between this LP and the WCMDP.
\begin{restatable}{lemma}{lprelaxation}\label{lem:lp upper bound}
The optimal value of any $N$-armed WCMDP problem is upper bounded by the optimal value of the corresponding linear program in \eqref{eq:lp}, i.e.,
    \begin{align*}
        R^*(N,\bm{S}_0)\le R_N^{\rel},\quad\forall N,\forall \bm{S}_0.
    \end{align*}
\end{restatable}

An immediate implication of Lemma~\ref{lem:lp upper bound} is that for any policy $\pi$, its optimality gap is upper bounded as
\begin{equation}
    R^*(N,\bm{S}_0)-R^-(\pi,\bm{S}_0) \le R_N^{\rel}-R^-(\pi,\bm{S}_0).
\end{equation}
Therefore, to derive an upper bound for the optimality gap, it suffices to upper bound $R_N^{\rel}-R^-(\pi,\bm{S}_0)$, which is the route taken by this paper.

To see the intuition of Lemma~\ref{lem:lp upper bound}, we interpret the optimization variable $y_i(s,a)$ as the long-run fraction of time arm~$i$ spends in state $s$ and takes action $a$.
We refer to $y_i(s,a)$ as arm~$i$'s \emph{state-action frequency} for the state-action pair $(s,a)$.
Then the constraints in \eqref{eq:lp:budget-constraint} of the LP can be viewed as relaxations of the budget constraints in \eqref{eq:hard-budget-constraint} for the WCMDP.
The constraints in \eqref{eq:lp:stationarity-constraint}--\eqref{eq:lp:probability-constraint} guarantee that $y_i(s,a)$'s are proper stationary time fractions.
Therefore, the LP is a relaxation of the WCMDP and thus achieves a higher optimal value.
The formal proof of Lemma~\ref{lem:lp upper bound} is provided in Appendix~\ref{app:proof-lem-lp-upper-bound}.


Our LP \eqref{eq:lp} serves a similar role to the LP used in previous work on restless bandits and WCMDPs with \emph{homogeneous} arms \citep[see, e.g.,][]{WebWei_90,GasGauYan_23_exponential,HonXieChe_23}. 
Both our LP and the LP in previous work relax the hard budget constraints to time-average constraints.
However, in the homogeneous arm setting, the LP has only one set of state-action frequencies $y(s,a)$, and the LP is independent of $N$.
As a result, both the optimal value of the LP and the complexity of solving it are independent of $N$.
Some existing work \citep{Ver_16_verloop} considers heterogeneous arms, but only in the limited sense of having a constant number of arm types. 
This setting still closely resembles the homogeneous setting, and the LP remains independent of $N$.

In contrast, our work addresses \emph{fully heterogeneous} arms. 
Consequently, we must define separate state-action frequencies $y_i(s,a)$ for each arm $i\in[N]$, making our LP explicitly depend on $N$. 
Therefore, the optimal value $R_N^{\rel}$ depends on $N$, and the complexity of solving the LP grows with $N$. 
Nevertheless, because the number of variables and constraints scale linearly with $N$, our LP can still be solved in polynomial time.




\section{ID policy (with reassignment)}
In this section, we introduce our adapted version of the ID policy that was originally proposed in \citep{HonXieChe_24}. 
This adapted version retains the core structure of the original ID policy but includes an additional ID reassignment procedure. 
For simplicity, we will continue to refer to this adapted version as the ID policy.
We begin by introducing a building block of the ID policy, referred to as optimal single-armed policies, followed by a brief description of the ID reassignment algorithm.
We then present the complete adapted ID policy.


\paragraph{Optimal single-armed policies.}
Once we obtain a solution to the LP in \eqref{eq:lp}, we can construct a policy for each arm~$i$, which we refer to as an \emph{optimal single-armed policy} for arm~$i$.
In particular, let $(y^*_i(s,a))_{i\in [N], s\in\sspa,a\in\aspa}$ be an optimal solution to the LP in \eqref{eq:lp}.
Then for arm~$i$, the optimal single-armed policy, $\pibar^*_i$, is defined as
\begin{equation}\label{eq:optimal-single-armed-policy}
    \bar{\pi}_i^*(a\mid s) = \begin{cases}
        \frac{y^*_i(s,a)}{\sum_{a\in\aspa}y^*_i(s,a)}, & \text{if}~ \sum_{a\in\aspa}y^*_i(s,a) >0,\\
        \frac{1}{\cardA}, &  \text{if} ~\sum_{a\in\aspa}y^*_i(s,a)=0,
    \end{cases}
\end{equation}
where $\bar{\pi}_i^*(a\mid s)$ is the probability of taking action $a$ given that the arm's current state is $s$.
Note that due to heterogeneity, this optimal single-armed policy $\pibar^*_i$ can be different for different arms.


The rationale behind considering these policies is as follows.
If each arm $i$ individually follows its optimal single-armed policy $\pibar_i^*$, then the average reward per arm (total reward divided by $N$) achieves the upper bound $R_N^{\rel}$ given by the LP.
However, this strategy only guarantees that the cost constraints are satisfied in a \emph{time-average} sense, rather than conforming to the \emph{hard} constraints in the original $N$-armed WCMDP.
Thus, having each arm follow its optimal single-armed policy is not a valid policy for the original $N$-armed problem.
Nevertheless, these optimal single-armed policies $\pibar_i^*$'s serve as guidance for how the arms should ideally behave to maximize rewards.
The ID policy uses the $\pibar_i^*$'s as a reference.
It is then designed to ensure that even under the hard cost constraints, most arms follow their optimal single-armed policies most of the time, yielding a diminishing gap to $R_N^{\rel}$ in reward.

\paragraph{ID reassignment.}
The full ID reassignment algorithm is detailed in Section~\ref{sec:ID-reassignment}.
Here, we provide a brief high-level description of its guarantee.
When an arm follows its optimal single-armed policy, it incurs a certain amount of average cost for each cost type. 
The algorithm rearranges the arms so that the cost incurred by each contiguous segment of arms is approximately proportional to its length, which is a regularity property on which our subsequent analysis is built. 


\begin{algorithm}[t]
    \caption{ID policy (with reassignment)}
    \label{alg:id policy}
    \begin{algorithmic}[1]
        \STATE
        \textbf{Input:} $N$-armed WCMDP instance $(\cM_i)_{i\in[N]}$
        \STATE \textbf{Preprocessing:}        
        \STATE \hspace{0.7em} Solve the LP in \eqref{eq:lp} and obtain the optimal state-action frequencies $(y^*_i(s,a))_{i\in [N], s\in\sspa,a\in\aspa}$
        \STATE \hspace{0.7em} Calculate the optimal single-armed policies $(\pibar_i^*)_{i\in[N]}$ using \eqref{eq:optimal-single-armed-policy} 
        \STATE \hspace{0.7em} Perform ID reassignment using Algorithm~\ref{alg:id-assign}
        \STATE \textbf{Real-time:}
        \FOR{$t=0,1,2,\cdots$} 
            \STATE Sample ideal actions $\widehat{A}_{i,t}\sim \pibar_i^*(\cdot\mid S_{i,t})$ for all $i\in[N]$
            \STATE $I\gets 1$ 
            \WHILE{$\sum_{i\in[I]}c_{k,i}(S_{i,t},\widehat{A}_{i,t})\leq \alpha_k N, \forall k\in[K]$}
                \STATE For arm $I$, take action $A_{I,t}=\widehat{A}_{I,t}$; $\quad I\gets I+1$
            \ENDWHILE
        \STATE For each arm $i\in\{I,I+1,\dots,N\}$, take action $A_{i,t}=0$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}
\paragraph{Constructing ID policy.}
We are now ready to describe the ID policy, formalized in Algorithm~\ref{alg:id policy}.
The policy begins with a one-time preprocessing phase: we solve the associated LP, construct the optimal single-armed policies, and reassign arm IDs using the ID reassignment algorithm (Algorithm~\ref{alg:id-assign} in Section~\ref{sec:ID-reassignment}).
After the preprocessing, the policy proceeds at each time step $t$ as follows.
For each arm~$i$ (where $i$ is the reassigned ID), we first sample an action $\widehat{A}_{i,t}$, referred to as an \emph{ideal action}, from the optimal single-armed policy $\pibar_i^*(\cdot \mid S_{i,t})$.
We then attempt to execute these ideal actions, i.e., set the real actions equal to the ideal actions, in ascending order of arm IDs, starting from $i=1$, then $i=2$, and so on.
We continue the attempt until we have used up at least one type of cost budget, at which point we let the remaining arms take action $0$ (the no-cost action).
One can see that the ID policy follows the rationale of following the optimal single-armed policies as much as possible.
Its particular way of selecting which arms to follow these polices, based on the reassigned IDs, is key to achieving asymptotic optimality.




\section{Main result and technical overview}\label{sec:result-tech-overview}
Before we present the main result, we first state the main assumption we make.
This assumption is for the optimal single-armed policies $\pibar^*_i$'s.
Note that each $\pibar^*_i$ is a stationary Markov policy.
Therefore, under this policy, the state of arm~$i$ forms a Markov chain.
Let the transition probability matrix of this Markov chain be denoted as $P_i=(P_i(s'\mid s))_{s\in\sspa,s'\in\sspa}$, where the row index is the current state $s$ and the column index is the next state $s'$.
Then $P_i(s'\mid s)$ can be written as
\begin{equation}
    P_i(s'\mid s) = \sum_{a\in \aspa} \PP_i(s'\mid s,a)\bar{\pi}_i^*(a\mid s).
\end{equation}
One can verify that the stationary distribution of this Markov chain is $\mu_i^*=(\mu_i^*(s))_{s\in\sspa}$ with $\mu_i^*(s)=\sum_{a\in\aspa}y^*_i(s,a)$.
We call $\mu_i^*$ the \emph{optimal state distribution} for arm~$i$.
The mixing time of this Markov chain is closely related to its \emph{absolute spectral gap} $1-|\lambda_2(P_i)|$, where $\lambda_2(P_i)$ is the second largest eigenvalue of $P_i$ in absolute value.

\begin{assumption}\label{ass:unichain}
    For each arm $i\in\mathbb{N}_+$, the induced Markov chain under the optimal single-armed policy $\bar{\pi}_i^*$ is an aperiodic unichain. 
    Furthermore, the absolute spectral gap of the transition probability matrix $P_i$ is lower bounded by $1-\specrad$ for all $i\in\mathbb{N}_+$, where $0\le \specrad<1$ is a constant; i.e.,
    \begin{equation}
        1-|\lambda_2(P_i)| \ge 1-\specrad, \quad\forall i\in\mathbb{N}_+.
    \end{equation}
\end{assumption}
\begin{theorem}
\label{thm:opt-gap-bound}
    Consider any $N$-armed WCMDP with initial system state $\bm{S}_0$ and assume that it satisfies Assumption \ref{ass:unichain}. Let policy $\pi$ be the ID policy (Algorithm~\ref{alg:id policy}). Then the optimality gap of $\pi$ is bounded as
    \begin{align*}
        R^*(N,\bm{S}_0)-R(\pi,\bm{S}_0) \leq\frac{C_{\ID}}{\sqrt{N}},
    \end{align*}
    where $C_{\ID}$ is a positive constant independent of $N$. 
\end{theorem}



\subsection*{Technical overview}

Our technical approach uses the Lyapunov drift method, which has found widespread applications in queueing systems, Markov decision processes, reinforcement learning, and so on.
While the basic framework of the drift method is standard, the \emph{key challenge} lies in constructing the right Lyapunov function with the desired properties, where the difficulty is exacerbated by the full heterogeneity of the problem under study.
Our construction of such a Lyapunov function is highly novel, yet still natural.
We reiterate that fully heterogeneous, high-dimensional stochastic systems are poorly understood in the existing literature.
Our approach opens up the possibility of analyzing the steady-state behavior of such systems through the Lyapunov drift method.


In the remainder of this section, we consider the ID policy, sometimes also referred to as policy $\pi$.
Let $\mX_t$ denote the system state under it, with the superscript $\pi$ omitted for brevity.
To make this overview more intuitive, here let us assume that $\mX_t$ converges to its steady state $\mX_{\infty}$ in a proper sense such that taking expectations in steady state is the same as taking time averages.
However, note that our formal results do not need this assumption and directly work with time averages.
We call a function $V$ a Lyapunov/potential function if it maps each possible system state to a nonnegative real number.


\paragraph{General framework of the drift method.}
Here we briefly describe the general framework of the drift method when applied to our problem.
The goal is to construct a Lyapunov function $V$ such that
\begin{enumerate}[label=(\textbf{C\arabic*}),leftmargin=4em]
    \item\label{cond:upper} $R_N^{\rel}-R(\pi,\bm{S}_0) \le \E{V(\mX_{\infty})}$;
    \item\label{cond:drift} (Drift condition) $\EE\mbracket{V(\mX_{t+1})\mid \mX_t}-V(\mX_t)\leq -CV(\mX_t)+ O(\sqrt{N})$ for a constant $C$.
\end{enumerate}
The drift condition requires that on average, the value of $V$ approximately decreases (ignoring the additive $O(\sqrt{N})$) after a time step.
The drift condition implies a bound on $\E{V(\mX_{\infty})}$.
To see this, let $\mX_t$ follow the steady-state distribution, which means $\mX_{t+1}$ also follows the steady-state distribution, and take expectations on both sides of the inequality.
Then we get $0=\EE\mbracket{V(\mX_{t+1})}-\E{V(\mX_t)}\leq -C\E{V(\mX_t)}+ O(\sqrt{N})$, which implies $\E{V(\mX_{\infty})}=\E{V(\mX_t)}=O(\sqrt{N}).$
Combining this with \ref{cond:upper} proves the desired upper bound on the optimality gap.

\paragraph{Key challenge: constructing Lyapunov function.}
We highlight this challenge by contrasting the homogeneous setting and the heterogeneous setting.
In the \emph{homogeneous} setting, there is only one optimal state distribution, $\mu^*$.
The Lyapunov function in \citep{HonXieChe_24} is defined based on the distance between the \emph{empirical state distribution} across arms and $\mu^*$.  Specifically, it is based on a set of functions $(h(\mX_t,D))_{D\subseteq [N]}$ defined as:
\begin{equation}
    h(\mX_t,D)=\|\mX_t(D)-m(D)\mu^*\|,
\end{equation}
where $\mX_t(D)=(X_t(D,s))_{s\in\sspa}$ denotes within $D$, the number of arms in each state $s$, divided by $N$; $m(D)=|D|/N$; and the norm is a properly defined norm.
The idea is that if all arms in $D$ follow the optimal single-armed policy, the state distribution of each arm in $D$ gets closer to $\mu^*$, and thus $\mX_t(D)$ gets closer to $m(D)\mu^*$ over time. 


In the \emph{heterogeneous} setting, we also want to construct a Lyapunov function $h(\mX_t, D)$ to witness the convergence of any set of arms $D$ if they  follow the optimal single-armed policies. 
However, unlike the homogeneous setting, now it no longer makes sense to aggregate arm states into an empirical state distribution, since each arm's dynamics is distinct.
Instead, our Lyapunov function considers $X_{i,t}-\mu_i^*$, where recall $X_{i,t}(s)$ is the indicator that arm~$i$'s state is $s$ at time $t$. 
A naive first attempt is to construct the Lyapunov function from the pointwise distances, $\norm{X_{i,t}-\mu_i^*}$ for each arm~$i$, with a properly defined norm $\norm{\cdot}$. 
However, the pointwise distances are very noisy: $\norm{X_{i,t}-\mu_i^*}$  could be large even when the state of arm~$i$ independently follows the distribution $\mu_i^*$ for each $i$, a situation when we should view the set of arms as already converged. 

Intuitively, to make the Lyapunov function properly reflect the convergence of the set of arms (referred to as ``the system'' in the rest of the section) following the optimal single-armed policies, we would like it to depend less strongly on the state of each individual arm and focus more on the collective properties of the whole system. 
Our idea is to \emph{project} the system state onto a properly selected set of \emph{feature vectors}, and construct the Lyapunov function based on how far these projections are from the projections of $\mu^*$. 
Then what features of the system state do we need to determine whether it has converged in a proper sense? 
The first feature we consider is the instantaneous reward of the system, $\sum_{i\in D} \innerproduct{X_{i,t}}{r_i^*}$, where $r_i^*\in \R^{\sspa}$ is the reward function of arm $i$ under $\pibs_i$, and recall that the inner product is defined between two vectors whose entries correspond to states in $\sspa$. 
We also want to keep track of the \emph{$\ell$-step ahead expected reward}, $\sum_{i\in D} \innerproduct{X_{i,t}P_i^{\ell}}{r_i^*}$, for each $\ell\in \mathbb{N}_+$. 
Intuitively, if $\sum_{i\in D} \innerproduct{(X_{i,t} -\mu_i^*)P_i^{\ell}}{r_i^*}$ is small for each $\ell\in \mathbb{N}$, the reward of the system should remain close to that under the optimal stationary distribution $\mu^*$ for a long time; conversely, if the state of each arm~$i$ independently follows $\mu^*_i$, each of these features should be small as well. 
We also consider the \emph{$\ell$-step ahead expected type-$k$ cost} for each $\ell\in \mathbb{N}$ and $k\in [K]$ as features, defined analogously.  

Combining the above ideas, for any set of arms $D$, we let the Lyapunov function $h(\mX_t, D)$ be the supremum of the differences between $\mX_t$ and $\mu^*$ in all the features defined above with proper weightings: 
\begin{equation}\label{eq:h-def-overview}
    h(\mX_t,D) = \max_{g\in \mathcal{G}} \sup_{\ell\in\mathbb{N}} \abs{\sum_{i\in D} \innerproduct{(X_{i,t}-\mu_i^*) P_i^{\ell} / \gamma^{\ell}}{g_i}},
\end{equation}
where $\gamma$ is a constant with $\specrad<\gamma<1$; each element $g\in \mathcal{G}$ is either $g = (r_i^*)_{i\in[N]}$, or corresponds to the type-$k$ cost for some $k\in[K]$ (see \Cref{sec:proof-main-mainbody} for the definition of $\mathcal{G}$). 
Note that dividing each term by powers of $\gamma$ is another interesting trick, 
which makes the Lyapunov function \emph{strictly contract} under the optimal single-armed policies, as demonstrated in the proof of \Cref{lem:drift}. 

Now with the set of functions $(h(\mX_t,D))_{D\subseteq [N]}$ defined, we generalize the idea of focus sets in \citep{HonXieChe_24} to convert $(h(\mX_t,D))_{D\subseteq [N]}$ into a Lyapunov function $V(\mX_t)$.
We prove that $V$ satisfies \ref{cond:upper} and \ref{cond:drift} using the structure of $(h(\mX_t,D))_{D\subseteq [N]}$.


\begin{remark}
The idea for constructing $h(\mX_t, D)$ is potentially useful for analyzing other heterogeneous stochastic systems. 
At a high level, projecting the system state onto a set of feature vectors (and their future expectations) can be roughly viewed as aggregating system states whose relevant performance metrics \emph{remain close for a sufficiently long time}. 
This idea provides a new way to measure the distance between two system states in a heterogeneous system, and this distance notation enjoys similar properties as that in a homogeneous system, without resorting to symmetry.  
\end{remark}






\section{ID reassignment}\label{sec:ID-reassignment}
In this section, we introduce the ID reassignment algorithm (Algorithm~\ref{alg:id-assign}) used in the ID policy (Algorithm~\ref{alg:id policy}) and its key property, which will be used in later analysis.

We first define a few quantities that will be used in the ID reassignment algorithm.
For each arm $i\in[N]$ and each cost type $k\in[K]$, the expected cost under the optimal single-armed policy is defined as
\begin{align}\label{eq:C_k,i}
    C_{k,i}^* =\sum_{s\in\sspa,a\in\aspa}y_i^*(s,a)c_{k,i}(s,a).
\end{align}
Based on $C_{k,i}^*$'s, we divide the cost constraints into \emph{active} constraints and \emph{inactive} constraints as follows.
For each cost type $k\in[K]$, we say the type-$k$ cost constraint is \emph{active} if
\begin{equation}\label{eq:active}
    \sum_{i\in[N]}C_{k,i}^*\ge \frac{\alpha_k}{2}N,
\end{equation}
and \emph{inactive} otherwise.
Let $\cA\subseteq [K]$ denote the set of cost types corresponding to active constraints.


Now consider a subset $D\subseteq [N]$ of arms.
For each $k\in[K]$, let $C_k^*(D)=\sum_{i\in D}C_{k,i}^*$,
i.e., $C_k^*(D)$ is the total expected type-$k$ cost for arms in $D$ under the optimal single-armed policies.
In our analysis, we often need to consider a notion of \emph{remaining} budget, defined as
\begin{equation}\label{eq:remain-bud}
    \rembud_k(D)=
    \begin{cases}
        \alpha_k N-C_k^*(D),\quad&\text{if }k\in\cA,\\
        \alpha_k N-C_k^*(D)-\frac{\alpha_k}{3}|D|,\quad&\text{otherwise},
    \end{cases}
\end{equation}
where the $\frac{\alpha_k}{3}|D|$ is a correction term when type-$k$ constraint is inactive. 
Note that $\rembud_k(D)\ge 0$ and $C_k^*(D)+\rembud_k(D)\le \alpha_k N$ for all $k\in[K]$ and all $D\subseteq [N]$.


\begin{algorithm}[t]
     \caption{ID reassignment}\label{alg:id-assign}
     \begin{algorithmic}[1]
        \STATE \textbf{Input:} optimal state-action frequencies $(y^*_i(s,a))_{i\in [N], s\in\sspa,a\in\aspa}$, budgets $(\alpha_k)_{k\in[K]}$, \\
        \hspace{0.47in}parameter $\costIDreassign$ with $0<\costIDreassign<\alpha_{\min}/2\triangleq \min_{k\in[K]}\alpha_k/2$
        \STATE \textbf{Output:} new arm ID, $\newID(i)$, for each arm with old ID $i\in[N]$
        \STATE Compute $(C_{k,i}^*)_{i\in[N],k\in[K]}$ and the set of active constraints $\cA$ using \eqref{eq:C_k,i} and \eqref{eq:active}
        \IF{$\cA=\emptyset$}
        \STATE $\newID(i)=i$ for all $i\in[N]$  \hfill \emph{$\triangleright$ No need for ID reassignment}
        \ELSE 
        \STATE Initialize $\cF = \emptyset$ \hfill\emph{$\triangleright$ Set of arms that have been assigned new IDs}
        \STATE Initialize $\cD_k=\{i\in[N]\colon C_{k,i}^*\ge \costIDreassign\}$ for all $k\in\cA$\\
        \FOR{$\ell =0,1,\dots,\lfloor N/\gsizeIDreassign\rfloor-1$}
        \STATE $\cI(\ell) = [\ell\gsizeIDreassign+1:(\ell+1)\gsizeIDreassign]$; set $j=\ell\gsizeIDreassign+1$
        \FOR{$k\in\cA$}\label{alg:line:reassign-start}
            \IF{$\sum_{i\in \cF}C_{k,i}^*\mathbbm{1}\{\newID(i)\in \cI(\ell)\}<\costIDreassign$}
            \STATE Pick any $i$ from $\cD_k$ and set $\newID(i)=j$; remove $i$ from $\cD_{k'}$ for all $k'$; add $i$ to $\cF$
            \STATE $j\gets j+1$
            \ENDIF
        \ENDFOR\label{alg:line:reassign-end}
        \ENDFOR
        \STATE For all $i\in [N]\setminus \cF$, assign values to their $\newID(i)$'s randomly from $[N]\setminus \{\newID(i')\colon i'\in \cF\}$
        \ENDIF
     \end{algorithmic}
 \end{algorithm}
We are now ready to describe the ID reassignment algorithm, formalized in Algorithm~\ref{alg:id-assign}.
Roughly speaking, the goal of the ID reassignment is to ensure that when we expand a set of arms from $[n_1]$ to $[n_2]$ for some $n_1\le n_2$, the drop in the remaining budget of any type $k$, i.e., $\rembud_k([n_1])-\rembud_k([n_2])$, is (almost) at least linear in $n_2-n_1$.
Note that this property is automatically satisfied for $k$ if the type-$k$ constraint is inactive.
This property is formalized in Lemma~\ref{lem:positiveC}, and the need for it will become clearer in \Cref{sec:proof-main-mainbody} when we introduce the so-called focus set in our analysis. 


To achieve this desired property, we design our ID reassignment algorithm in the following way.
If the set of active constraints is empty, i.e., $\cA=\emptyset$, then there is no need to perform ID reassignment.
Otherwise, i.e., when $\cA\neq \emptyset$, we first carefully choose two parameters, a positive real number $\costIDreassign$ and a positive integer $\gsizeIDreassign$.
We then divide the full ID set $[N]$ into groups of size $\gsizeIDreassign$, i.e., $[\gsizeIDreassign],[\gsizeIDreassign+1:2\gsizeIDreassign],[2\gsizeIDreassign+1:3\gsizeIDreassign],\dots,[(\lfloor N/\gsizeIDreassign\rfloor-1)\gsizeIDreassign+1: \lfloor N/\gsizeIDreassign\rfloor \gsizeIDreassign]$, and the remainder.
We ensure that after the reassignment, each group contains at least one arm $i$ with $C_{k,i}^*\ge \costIDreassign$ for each active constraint type $k\in\cA$.


The key here is to choose $\costIDreassign$ and $\gsizeIDreassign$ properly so such a reassignment is feasible.
In particular, we choose $\costIDreassign$ to be any constant with $0<\costIDreassign<\alpha_{\min}/2$, where $\alpha_{\min}=\min_{k\in[K]}\alpha_k$, and let
$\gsizeIDreassign = \left\lceil \frac{(c_{\max}-\costIDreassign)K}{\alpha_{\min}/2-\costIDreassign}\right\rceil.$
Note that $\gsizeIDreassign\ge K$ since one can verify that $c_{\max}\ge \alpha_{\min}/2$ when $\cA\neq \emptyset$. 

More details of the ID reassignment are provided in Algorithm~\ref{alg:id-assign}.
We state Lemma~\ref{lem:positiveC} below and provide its proof in Appendix~\ref{app:proof-lem-positiveC}.
In the remainder of this paper, we use the reassigned IDs to refer to arms, i.e., arm $i$ refers to the arm whose new ID assigned by the ID reassignment algorithm is $i$.

\begin{restatable}{lemma}{positiveC}\label{lem:positiveC}
After performing the ID reassignment algorithm (Algorithm~\ref{alg:id-assign}), for any $n_1,n_2$ with $1\le n_1\le n_2\le N$, we have
\begin{equation}\label{eq:remaining-budget-decrease}
    \rembud_k([n_1])-\rembud_k([n_2])\ge \eta_c(n_2-n_1)-M_c,
\end{equation}
for all $k\in[K]$,
where $\eta_c>0$ and $M_c>0$ are constants determined by $\delta$, $\alpha_{\min}$, $c_{\max}$, and $K$.

Further, let $\rembud(D)=\min_{k\in[K]}\rembud_k(D)$ for all $D\subseteq [N]$.  Then the bound \eqref{eq:remaining-budget-decrease} implies that for any $n_1,n_2$ with $1\le n_1\le n_2\le N$,
\begin{equation}
    \rembud([n_1])-\rembud([n_2])\ge \eta_c(n_2-n_1)-M_c.
\end{equation}
\end{restatable}





\section{Proof of main result (Theorem~\ref{thm:opt-gap-bound})}
\label{sec:proof-main-mainbody}

As outlined in the technical overview in Section~\ref{sec:result-tech-overview}, the core of our proof is the construction of a Lyapunov function.
The Lyapunov function we construct is the following
\begin{equation}
    \label{eq:main-lyapunov-def}
    V(\mx) = h_{\ID}(\mx,m(\mx))+L_hN\cdot(1-m(\mx)).
\end{equation}
In the rest of this section, we first define the functions $h_{\ID}(\cdot,\cdot)$ and $m(\cdot)$, along with the constant $L_h$.
We then proceed to analyze the Lyapunov function $V$ to establish an upper bound on the optimality gap.

\paragraph{Defining $h_{\ID}(\cdot,\cdot)$ using subset Lyapunov functions.}
We first construct a Lyapunov function indexed by a subset of arms $D\subseteq [N]$, denoted as $h(\mx,D)$, which is viewed as a function of the system state $\mx$, and it is referred to as a \emph{subset Lyapunov function}.

For each cost type $k\in[K]$, let $c_{k,i}^*(s)=\sum_{a\in\aspa} \bar{\pi}_i^*(a|s)c_{k,i}(s,a)$,
and let $c_{k}^*=(c_{k,i}^*)_{i\in[N]}$ denote the vector of the functions $c_{k,i}^*$'s.
In addition, let $r_i^*(s)=\sum_{a\in\aspa}\bar{\pi}_i^*(a|s)r_i(s,a)$,
and let $r^*=(r_i^*)_{i\in[N]}$ denote the vector of the functions $r_i^*$'s.  We combine these vectors into a set $\mathcal{G}=\{c_1^*,c_2^*,\dots,c_K^*,r^*\}$.

The subset Lyapunov function is then defined as
\begin{equation}\label{eq:h-def}
    h(\mx,D) = \max_{g\in \mathcal{G}} \sup_{\ell\in\mathbb{N}} \abs{\sum_{i\in D} \innerproduct{(x_i-\mu_i^*) P_i^{\ell} / \gamma^{\ell}}{g_i}}.
\end{equation}
Here recall that $\mx$ is an $N\times\cardS$ matrix whose $i$th row, $x_i$, describes the state of arm~$i$; $P_i$ is the transition probability matrix for arm~$i$ under the optimal single-armed policy; $\gamma$ is any constant satisfying $\specrad < \gamma < 1$.
To build intuition for $h(\mx,D)$, consider the term corresponding to $g=r^*$ and $\ell = 0$.
In this case, the term $\sum_{i\in D}\innerproduct{x_i-\mu_i^*}{g_i}$ measures the difference between the reward obtained by applying the optimal single-armed policies to arms in $D$ and the reward upper bound given by the LP relaxation.
A similar interpretation holds for the differences in costs.


In Lemma~\ref{lem:drift} below, we show that $h(\mx,D)$ is well-defined and establish its two key properties, which play a critical role in our analysis.  The proof of \Cref{lem:drift} is given in \Cref{app:proof-lem-drift}.
\begin{restatable}{lemma}{hproperties}\label{lem:drift}
The Lyapunov function $h(\mx, D)$ defined in \eqref{eq:h-def} is finite for all system state $\mx$ and subset $D\subseteq[N]$. 
Moreover, $h(\mx, D)$ has the following properties.
\begin{enumerate}
    \item \textbf{(Lipschitz continuity)} There exists a Lipschitz constant $L_h$ such that for each system state $\mx$ and $D'\subseteq D\subseteq [N]$, we have
    \begin{equation}
        \label{eq:h-lipschitz}
        \abs{h(\mx,D)-h(\mx,D')}\leq L_h\abs{D/D'}\,.
    \end{equation}
    \item \textbf{(Drift condition)} If each arm in $D$ takes the action sampled from the optimal single-armed policy, i.e., ${A}_{i,t}\sim\bar{\pi}_i^*(\cdot\mid S_{i,t})$, then there exists a constant $C_h > 0$ such that
    \begin{equation}
        \label{eq:h-drift-condition}
        \EE\mbracket{\bracket{h(\mX_{t+1},D)-\gamma h(\mX_t,D)}^+\middle|\mX_t,{A}_{i,t}\sim\bar{\pi}_i^*(\cdot\mid S_{i,t}),\forall i\in D}\leq C_h\sqrt{N}.
    \end{equation}
\end{enumerate}
\end{restatable}
Note that \eqref{eq:h-drift-condition} implies the following more typical form of drift condition
\begin{equation}
    \EE\mbracket{h(\mX_{t+1},D)\middle |\mX_t,{A}_{i,t}\sim\bar{\pi}_i^*(S_{i,t}),\forall i\in D}-h(\mX_{t},D)\leq -(1-\gamma)h(\mX_{t},D)+C_h\sqrt{N}.
\end{equation}


We are now ready to define the function $h_{\ID}(\cdot,\cdot)$ used to construct the Lyapunov function $V$.
For any system state $\mx$ and $m\in[0,1]_N$ (where recall that $[0,1]_N$ is the set of integer multiples of $1/N$ within the interval $[0,1]$), $h_{\ID}(\mx,m)$ is defined as
\begin{equation}
    \label{eq:hid-def}
    h_{\ID}(\mx,m)=\maxmp h(\mx,[Nm']).
\end{equation}
That is, $h_{\ID}(\mx,m)$ is an upper envelope of the subset Lyapunov functions $h(\mx,[Nm'])$'s.
The function $h_{\ID}(\mx,m)$ has properties similar to those in Lemma~\ref{lem:drift}, which we state as Lemma~\ref{lem:hID drift} and prove in Appendix~\ref{sec:properties-hID}.

\paragraph{Focus set.}
We next introduce the concept of the focus set, which is directly tied to the function $m(\cdot)$ in the Lyapunov function $V$.
The focus set is a dynamic subset of arms based on the current system state.
Specifically, for any system state $\mx$, the focus set is defined as the set $[Nm(\mx)]$, where $m(\mx)$ is given by
\begin{align}
    m(\mx)= \max \sets{ m\in[0,1]_N: h_{\ID}(\mx,m)\leq \min_{k\in[K]}\rembud_k([Nm])}.\label{eq:focus set def}
\end{align}

The focus set is introduced because it has several desirable properties that are useful for the analysis.
First, under the ID policy, almost all the arms in the focus set, except for $O(\sqrt{N})$ arms, can follow the optimal single-armed policies.
Additionally, as the focus set evolves over time, it is almost non-shrinking, and its size is closely related to the value of the function $h_{\ID}(\cdot,\cdot)$.
These properties are formalized as Lemmas~\ref{lem:majority_conformal}, \ref{lem:non-shrinking} and \ref{lem:coverage}, which are presented in Appendix~\ref{sec:lem-proof-focus-set}.


\paragraph{Bounding the optimality gap via analyzing the Lyapunov function $V$.}

With the Lyapunov function $V$ fully defined, we now proceed to bound the optimality gap $R^*(N,\bm{S}_0)-R(\pi,\bm{S}_0)$, where the policy $\pi$ is the ID policy.
As outlined in the technical overview in Section~\ref{sec:result-tech-overview}, an upper bound on the optimality gap is established via the following two lemmas.

\begin{restatable}{lemma}{optgapbyV}\label{lem:optimality gap}
Consider any $N$-armed WCMDP with initial system state $\bm{S}_0$ and assume that it satisfies Assumption \ref{ass:unichain}. Let policy $\pi$ be the ID policy.
Consider the Lyapunov function $V$ defined in \eqref{eq:main-lyapunov-def}.
Then the optimality gap of $\pi$ is bounded as
\begin{align*}
    R^*(N,\bm{S}_0)-R(\pi,\bm{S}_0)\leq \frac{2r_{\max}+L_h}{L_h N}\lim_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}\EE\mbracket{V(\mX_{t})} + \frac{K_{\conf}}{\sqrt{N}}\,,
\end{align*}
where $L_h$ is the Lipschitz constant in Lemma~\ref{lem:drift} and $K_{\conf}$ is the positive constant in \Cref{lem:majority_conformal}. 
\end{restatable}


\begin{restatable}{lemma}{driftV}\label{lem:drift-V}
Consider any $N$-armed WCMDP with initial system state $\bm{S}_0$ and assume that it satisfies Assumption \ref{ass:unichain}.
Let $\mX_t$ be the system state at time $t$ under the ID policy.
Consider the Lyapunov function $V$ defined in \eqref{eq:main-lyapunov-def}.
Then its drift satisfies
\begin{equation}
    \label{eq:main-drift-inequality}
    \EE\mbracket{V(\mX_{t+1}) \givenplain \mX_t}-V(\mX_t) \leq -\rhov V(\mX_t) + \Kv \sqrt{N}, 
\end{equation}
which further implies that
\begin{equation}
    \label{eq:main-final-EV-bound}
    \lim_{T\to\infty} \frac{1}{T} \sum_{t=0}^{T-1} \E{V(\mX_{t})} \leq \frac{\Kv\sqrt{N}}{\rhov},
\end{equation}
where $\rhov$ and $\Kv$ are constants whose values are given in the proof. 
\end{restatable}

The proofs of Lemmas~\ref{lem:optimality gap} and \ref{lem:drift-V} are provided in Appendix~\ref{sec:proof-lem-main-theorem}.
It is then straightforward to combine these two lemmas to get Theorem~\ref{thm:opt-gap-bound}.








\bibliography{refs-weina}
\newpage
\appendix
\input{Appendix}
\end{document}



