@inproceedings{augenstein2019MultiFCRealWorldMultiDomain,
  title = {{{MultiFC}}: {{A Real-World Multi-Domain Dataset}} for {{Evidence-Based Fact Checking}} of {{Claims}}},
  shorttitle = {{{MultiFC}}},
  booktitle = {Proceedings of 2019 {{EMNLP-IJCNLP}}},
  author = {Augenstein, Isabelle and Lioma, Christina and Wang, Dongsheng and Chaves Lima, Lucas and Hansen, Casper and Hansen, Christian and Simonsen, Jakob Grue},
  year = {2019},
  month = nov,
  pages = {4685--4697},
  publisher = {Association for Computational Linguistics},
  urldate = {2024-07-29},
  abstract = {We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2\%, showing that this is a challenging testbed for claim veracity prediction.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Augenstein/augenstein_2019_multifc.pdf}
}

@misc{brown2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-30},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Brown/brown_2020_language_models_are_few-shot_learners.pdf}
}

@misc{devlin2019BERTPretrainingDeep,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-10-30},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Devlin/devlin_2019_bert.pdf}
}

@misc{eldifrawi2024AutomatedJustificationProduction,
  title = {Automated {{Justification Production}} for {{Claim Veracity}} in {{Fact Checking}}: {{A Survey}} on {{Architectures}} and {{Approaches}}},
  shorttitle = {Automated {{Justification Production}} for {{Claim Veracity}} in {{Fact Checking}}},
  author = {Eldifrawi, Islam and Wang, Shengrui and Trabelsi, Amine},
  year = {2024},
  month = jul,
  number = {arXiv:2407.12853},
  eprint = {2407.12853},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-29},
  abstract = {Automated Fact-Checking (AFC) is the automated verification of claim accuracy. AFC is crucial in discerning truth from misinformation, especially given the huge amounts of content are generated online daily. Current research focuses on predicting claim veracity through metadata analysis and language scrutiny, with an emphasis on justifying verdicts. This paper surveys recent methodologies, proposing a comprehensive taxonomy and presenting the evolution of research in that landscape. A comparative analysis of methodologies and future directions for improving fact-checking explainability are also discussed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Zotero/storage/9AYBNJGJ/Eldifrawi et al. - 2024 - Automated Justification Production for Claim Verac.pdf}
}

@inproceedings{ferreira2016EmergentNovelDataset,
  title = {Emergent: A Novel Data-Set for Stance Classification},
  shorttitle = {Emergent},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ferreira, William and Vlachos, Andreas},
  editor = {Knight, Kevin and Nenkova, Ani and Rambow, Owen},
  year = {2016},
  month = jun,
  pages = {1163--1168},
  publisher = {Association for Computational Linguistics},
  address = {San Diego, California},
  doi = {10.18653/v1/N16-1138},
  urldate = {2024-07-29},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Ferreira_Vlachos/ferreira_vlachos_2016_emergent.pdf}
}

@article{guo2022SurveyAutomatedFactChecking,
  title = {A {{Survey}} on {{Automated Fact-Checking}}},
  author = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  year = {2022},
  month = feb,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {178--206},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00454},
  urldate = {2024-11-11},
  abstract = {Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Guo/guo_2022_a_survey_on_automated_fact-checking2.pdf}
}

@inproceedings{hassan2017AutomatedFactCheckingDetecting,
  title = {Toward {{Automated Fact-Checking}}: {{Detecting Check-worthy Factual Claims}} by {{ClaimBuster}}},
  shorttitle = {Toward {{Automated Fact-Checking}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Hassan, Naeemul and Arslan, Fatma and Li, Chengkai and Tremayne, Mark},
  year = {2017},
  month = aug,
  pages = {1803--1812},
  publisher = {ACM},
  address = {Halifax NS Canada},
  doi = {10.1145/3097983.3098131},
  urldate = {2024-06-27},
  isbn = {978-1-4503-4887-4},
  langid = {english}
}

@misc{kaplan2020ScalingLawsNeural,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-10-29},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Kaplan/kaplan_2020_scaling_laws_for_neural_language_models.pdf}
}

@misc{kotonya2020ExplainableAutomatedFactChecking,
  title = {Explainable {{Automated Fact-Checking}}: {{A Survey}}},
  shorttitle = {Explainable {{Automated Fact-Checking}}},
  author = {Kotonya, Neema and Toni, Francesca},
  year = {2020},
  month = nov,
  primaryclass = {cs},
  urldate = {2024-06-27},
  abstract = {A number of exciting advances have been made in automated fact-checking thanks to increasingly larger datasets and more powerful systems, leading to improvements in the complexity of claims which can be accurately fact-checked. However, despite these advances, there are still desirable functionalities missing from the fact-checking pipeline. In this survey, we focus on the explanation functionality -- that is fact-checking systems providing reasons for their predictions. We summarize existing methods for explaining the predictions of fact-checking systems and we explore trends in this topic. Further, we consider what makes for good explanations in this specific domain through a comparative analysis of existing fact-checking explanations against some desirable properties. Finally, we propose further research directions for generating fact-checking explanations, and describe how these may lead to improvements in the research area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Kotonya_Toni/kotonya_toni_2020_explainable_automated_fact-checking.pdf}
}

@misc{lewis2021RetrievalAugmentedGenerationKnowledgeIntensive,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2021},
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.11401},
  urldate = {2023-10-27},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Lewis/lewis_2021_retrieval-augmented_generation_for_knowledge-intensive_nlp_tasks.pdf}
}

@inproceedings{maynez2020FaithfulnessFactualityAbstractive,
  title = {On {{Faithfulness}} and {{Factuality}} in {{Abstractive Summarization}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  pages = {1906--1919},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.173},
  urldate = {2024-07-29},
  abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Maynez/maynez_2020_on_faithfulness_and_factuality_in_abstractive_summarization.pdf}
}

@misc{nguyen2024SurveySmallLanguage,
  title = {A {{Survey}} of {{Small Language Models}}},
  author = {Nguyen, Chien Van and Shen, Xuan and Aponte, Ryan and Xia, Yu and Basu, Samyadeep and Hu, Zhengmian and Chen, Jian and Parmar, Mihir and Kunapuli, Sasidhar and Barrow, Joe and Wu, Junda and Singh, Ashish and Wang, Yu and Gu, Jiuxiang and Dernoncourt, Franck and Ahmed, Nesreen K. and Lipka, Nedim and Zhang, Ruiyi and Chen, Xiang and Yu, Tong and Kim, Sungchul and Deilamsalehy, Hanieh and Park, Namyong and Rimer, Mike and Zhang, Zhehao and Yang, Huanrui and Rossi, Ryan A. and Nguyen, Thien Huu},
  year = {2024},
  month = oct,
  number = {arXiv:2410.20011},
  eprint = {2410.20011},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.20011},
  urldate = {2025-01-19},
  abstract = {Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Nguyen/nguyen_2024_a_survey_of_small_language_models.pdf}
}

@misc{ouyang2022TrainingLanguageModelsa,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-25},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Zotero/storage/DXE4K7K2/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf}
}

@inproceedings{pan2023FactCheckingComplexClaims,
  title = {Fact-{{Checking Complex Claims}} with {{Program-Guided Reasoning}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {6981--7004},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2024-07-29},
  abstract = {Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Pan/pan_2023_fact-checking_complex_claims_with_program-guided_reasoning.pdf}
}

@article{radford2018ImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal = "",
  year = {2018},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/premtim.sahitaj/Zotero/storage/W83ZBYCD/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@misc{rafailov2023DirectPreferenceOptimization,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18290},
  eprint = {2305.18290},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-23},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Rafailov/rafailov_2023_direct_preference_optimization.pdf}
}

@article{russo2023BenchmarkingGenerationFact,
  title = {Benchmarking the {{Generation}} of {{Fact Checking Explanations}}},
  author = {Russo, Daniel and Tekiro{\u g}lu, Serra Sinem and Guerini, Marco},
  year = {2023},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1250--1264},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  urldate = {2024-07-29},
  abstract = {Fighting misinformation is a challenging, yet crucial, task. Despite the growing number of experts being involved in manual fact-checking, this activity is time-consuming and cannot keep up with the ever-increasing amount of fake news produced daily. Hence, automating this process is necessary to help curb misinformation. Thus far, researchers have mainly focused on claim veracity classification. In this paper, instead, we address the generation of justifications (textual explanation of why a claim is classified as either true or false) and benchmark it with novel datasets and advanced baselines. In particular, we focus on summarization approaches over unstructured knowledge (i.e., news articles) and we experiment with several extractive and abstractive strategies. We employed two datasets with different styles and structures, in order to assess the generalizability of our findings. Results show that in justification production summarization benefits from the claim information, and, in particular, that a claim-driven extractive step improves abstractive summarization performances. Finally, we show that although cross-dataset experiments suffer from performance degradation, a unique model trained on a combination of the two datasets is able to retain style information in an efficient manner.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Russo/russo_2023_benchmarking_the_generation_of_fact_checking_explanations.pdf}
}

@misc{schulhoff2024PromptReportSystematic,
  title = {The {{Prompt Report}}: {{A Systematic Survey}} of {{Prompting Techniques}}},
  shorttitle = {The {{Prompt Report}}},
  author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Da Costa, Hevander and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
  year = {2024},
  month = jun,
  number = {arXiv:2406.06608},
  eprint = {2406.06608},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-24},
  abstract = {Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area's nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Zotero/storage/Y46FYRY9/Schulhoff et al. - 2024 - The Prompt Report A Systematic Survey of Promptin.pdf}
}

@misc{si2024LargeLanguageModels,
  title = {Large {{Language Models Help Humans Verify Truthfulness}} -- {{Except When They Are Convincingly Wrong}}},
  author = {Si, Chenglei and Goyal, Navita and Wu, Sherry Tongshuang and Zhao, Chen and Feng, Shi and Daum{\'e} III, Hal and {Boyd-Graber}, Jordan},
  year = {2024},
  month = apr,
  urldate = {2024-07-28},
  abstract = {Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they get, LLMs should not only provide information but also help users fact-check it. Our experiments with 80 crowdworkers compare language models with search engines (information retrieval systems) at facilitating fact-checking. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than those using search engines while achieving similar accuracy. However, they over-rely on the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-reliance on LLMs, but cannot significantly outperform search engines. Further, showing both search engine results and LLM explanations offers no complementary benefits compared to search engines alone. Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Si/si_2024_large_language_models_help_humans_verify_truthfulness_--_except_when_they_are.pdf}
}

@misc{vaswani2023AttentionAllYou,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-10-30},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Vaswani/vaswani_2023_attention_is_all_you_need.pdf}
}

@inproceedings{wang2023ExplainableClaimVerification,
  title = {Explainable {{Claim Verification}} via {{Knowledge-Grounded Reasoning}} with {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Wang, Haoran and Shu, Kai},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {6288--6304},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.416},
  urldate = {2024-06-27},
  abstract = {Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Wang_Shu/wang_shu_2023_explainable_claim_verification_via_knowledge-grounded_reasoning_with_large.pdf}
}

