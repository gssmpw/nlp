\section{Related Work}
NLP provides the foundation for efficiently processing and interpreting text, making it critical for addressing misinformation detection. Advances in transformer-based architectures have significantly improved language modeling and generation. Vaswani et al., "Attention Is All You Need" **Vaswani et al., "Attention Is All You Need"**__**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**__**Radford et al., "Improving Language Understanding by Generative Models with Contrastive Divergence"** illustrate encoder-only and decoder-only applications, respectively. BERT is well-suited for sequence classification tasks, while GPT generates text autoregressively for sequence-to-sequence tasks. In the context of this study, we refer to these lightweight architectures as SLMs _____. **Liu et al., "Multitask Learning of General-Purpose Visual Features"**__**Li et al., "VisualBERT: A Simple and Performant Baseline for Vision and Language"** showed that the loss of a language scales as a power law with model parameter size, pre-training dataset size, and compute budget, guiding the development of current large language models (LLMs). Most LLMs follow the GPT architecture with modifications, aligning their behavior to specific tasks through instruction fine-tuning, reinforcement learning from human feedback (RLHF) _____, direct preference optimization (DPO) _____, or supervised fine-tuning. Prompts serve as inputs to these models, acting as task instructions. Effective prompt engineering, including template design and task-specific examples in a few-shot scenario, enhances their utility _____. **Lewis et al., "Pre-train, Prompt, Predict: A Systematic Comparison of Few-shot Learning Methods for Text Classification"** is a technique that combines retrieval mechanisms with NLG, allowing LLMs to ground their responses in external evidence. This approach can enhance factual accuracy of LLM generated responses by enforcing consistency with the evidence, and thus may reduce hallucinations in generative tasks. Automated fact-checking frameworks typically consist of claim detection, evidence retrieval, and claim verification _____. Claim detection identifies check-worthy claims _____, often guided by factors like relevance or harm. Evidence retrieval involves collecting and selecting relevant information to justify verdicts _____. Claim verification can be broken down into two main tasks: (a) verdict prediction and (b) justification production _____. Verdict prediction describes the task of classifying the veracity of a claim. Specific labeling schemes may be required to fit particular use cases or domains. **Wang et al., "Knowledge-based Fact-Checking with Different Labeling Schemes"** investigate various datasets for knowledge-based fact-checking with different labeling schemes, ranging from a simple binary classification task formulation to more nuanced labels that do not directly map onto a veracity scale. The authors highlight the challenge of standardizing these schemes across datasets. Several approaches to justification production exist. **Liu et al., "Explainable Automated Fact-Checking: A Survey"** survey explainable automated fact-checking, emphasizing the need for effective explanation functionality, comparing current methods against desirable properties, and highlighting existing limitations. Similarly, **Kumar et al., "Abstractive Summarization of Justifications using Pre-trained Language Models"** treat justification production as an abstractive summarization task using language models. However, **Pramanik et al., "Hallucinations in Abstractive Extraction Models for Fact-Checking"** note that abstractive extraction models are prone to hallucinations, underlining the necessity for more systematic approaches of generating justifications to ensure transparency and reliability. While LLMs can help produce justifications, users may over-rely on these explanations even when they are incorrect _____. **Hewlett et al., "Predict-then-justify: A Systematic Comparison of Few-shot Learning Methods for Text Classification"** explore various methods for producing justifications, such as the \textit{predict-then-justify} approach, where a classification model determines veracity followed by a separate model generating explanations, and the \textit{justify-then-predict} approach, which employs reasoning techniques like chain-of-thought ____ and multi-hop ____ with LLMs.