@inproceedings{atanasova2020GeneratingFactChecking,
  title = {Generating {{Fact Checking Explanations}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  year = {2020},
  pages = {7352--7364},
  publisher = {Association for Computational Linguistics},
  urldate = {2024-12-15},
  abstract = {Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process -- generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.},
  langid = {english},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Atanasova/atanasova_2020_generating_fact_checking_explanations.pdf}
}

@inproceedings{augenstein2019MultiFCRealWorldMultiDomain,
  title = {{{MultiFC}}: {{A Real-World Multi-Domain Dataset}} for {{Evidence-Based Fact Checking}} of {{Claims}}},
  shorttitle = {{{MultiFC}}},
  booktitle = {Proceedings of 2019 {{EMNLP-IJCNLP}}},
  author = {Augenstein, Isabelle and Lioma, Christina and Wang, Dongsheng and Chaves Lima, Lucas and Hansen, Casper and Hansen, Christian and Simonsen, Jakob Grue},
  year = {2019},
  month = nov,
  pages = {4685--4697},
  publisher = {Association for Computational Linguistics},
  urldate = {2024-07-29},
  abstract = {We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2\%, showing that this is a challenging testbed for claim veracity prediction.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Augenstein/augenstein_2019_multifc.pdf}
}

@article{brandtzaeg2017TrustDistrustOnline,
  title = {Trust and Distrust in Online Fact-Checking Services},
  author = {Brandtzaeg, Petter Bae and F{\o}lstad, Asbj{\o}rn},
  year = {2017},
  month = aug,
  journal = {Communications of the ACM},
  volume = {60},
  number = {9},
  pages = {65--71},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3122803},
  urldate = {2024-07-18},
  abstract = {Even when checked by fact checkers, facts are often still open to preexisting bias and doubt.},
  langid = {english}
}

@misc{brown2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-30},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Brown/brown_2020_language_models_are_few-shot_learners.pdf}
}

@misc{devlin2019BERTPretrainingDeep,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-10-30},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Devlin/devlin_2019_bert.pdf}
}

@misc{eldifrawi2024AutomatedJustificationProduction,
  title = {Automated {{Justification Production}} for {{Claim Veracity}} in {{Fact Checking}}: {{A Survey}} on {{Architectures}} and {{Approaches}}},
  shorttitle = {Automated {{Justification Production}} for {{Claim Veracity}} in {{Fact Checking}}},
  author = {Eldifrawi, Islam and Wang, Shengrui and Trabelsi, Amine},
  year = {2024},
  month = jul,
  number = {arXiv:2407.12853},
  eprint = {2407.12853},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-29},
  abstract = {Automated Fact-Checking (AFC) is the automated verification of claim accuracy. AFC is crucial in discerning truth from misinformation, especially given the huge amounts of content are generated online daily. Current research focuses on predicting claim veracity through metadata analysis and language scrutiny, with an emphasis on justifying verdicts. This paper surveys recent methodologies, proposing a comprehensive taxonomy and presenting the evolution of research in that landscape. A comparative analysis of methodologies and future directions for improving fact-checking explainability are also discussed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Zotero/storage/9AYBNJGJ/Eldifrawi et al. - 2024 - Automated Justification Production for Claim Verac.pdf}
}

@inproceedings{ferreira2016EmergentNovelDataset,
  title = {Emergent: A Novel Data-Set for Stance Classification},
  shorttitle = {Emergent},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ferreira, William and Vlachos, Andreas},
  editor = {Knight, Kevin and Nenkova, Ani and Rambow, Owen},
  year = {2016},
  month = jun,
  pages = {1163--1168},
  publisher = {Association for Computational Linguistics},
  address = {San Diego, California},
  doi = {10.18653/v1/N16-1138},
  urldate = {2024-07-29},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Ferreira_Vlachos/ferreira_vlachos_2016_emergent.pdf}
}

@misc{graves2018UnderstandingPromiseLimits,
  title = {Understanding the Promise and Limits of Automated Fact-Checking},
  author = {Graves, Lucas},
  year = {2018},
  publisher = {Reuters Institute for the Study of Journalism},
  urldate = {2024-06-27},
  abstract = {The furore over so-called `fake news' has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, drawing attention to the potential of various automated fact-checking (AFC) technologies to combat online misinformation. Based on a review of recent research and interviews with both fact-checkers and computer scientists working in this area, this Factsheet gives an overview of current efforts to automatically police false claims and misleading content online.},
  archiveprefix = {Reuters Institute for the Study of Journalism},
  copyright = {Creative Commons Attribution 2.0 Generic},
  langid = {english},
  file = {/Users/premtim.sahitaj/Zotero/storage/HI8FVM7S/Graves - 2018 - Understanding the promise and limits of automated .pdf}
}

@article{guo2022SurveyAutomatedFactChecking,
  title = {A {{Survey}} on {{Automated Fact-Checking}}},
  author = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  year = {2022},
  month = feb,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {178--206},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00454},
  urldate = {2024-11-11},
  abstract = {Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Guo/guo_2022_a_survey_on_automated_fact-checking2.pdf}
}

@misc{hanselowski2019RichlyAnnotatedCorpus,
  title = {A {{Richly Annotated Corpus}} for {{Different Tasks}} in {{Automated Fact-Checking}}},
  author = {Hanselowski, Andreas and Stab, Christian and Schulz, Claudia and Li, Zile and Gurevych, Iryna},
  year = {2019},
  month = oct,
  number = {arXiv:1911.01214},
  eprint = {1911.01214},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.01214},
  urldate = {2025-01-13},
  abstract = {Automated fact-checking based on machine learning is a promising approach to identify false information distributed on the web. In order to achieve satisfactory performance, machine learning methods require a large corpus with reliable annotations for the different tasks in the fact-checking process. Having analyzed existing fact-checking corpora, we found that none of them meets these criteria in full. They are either too small in size, do not provide detailed annotations, or are limited to a single domain. Motivated by this gap, we present a new substantially sized mixed-domain corpus with annotations of good quality for the core fact-checking tasks: document retrieval, evidence extraction, stance detection, and claim validation. To aid future corpus construction, we describe our methodology for corpus creation and annotation, and demonstrate that it results in substantial inter-annotator agreement. As baselines for future research, we perform experiments on our corpus with a number of model architectures that reach high performance in similar problem settings. Finally, to support the development of future models, we provide a detailed error analysis for each of the tasks. Our results show that the realistic, multi-domain setting defined by our data poses new challenges for the existing models, providing opportunities for considerable improvement by future systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Hanselowski/hanselowski_2019_a_richly_annotated_corpus_for_different_tasks_in_automated_fact-checking.pdf}
}

@inproceedings{hassan2017AutomatedFactCheckingDetecting,
  title = {Toward {{Automated Fact-Checking}}: {{Detecting Check-worthy Factual Claims}} by {{ClaimBuster}}},
  shorttitle = {Toward {{Automated Fact-Checking}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Hassan, Naeemul and Arslan, Fatma and Li, Chengkai and Tremayne, Mark},
  year = {2017},
  month = aug,
  pages = {1803--1812},
  publisher = {ACM},
  address = {Halifax NS Canada},
  doi = {10.1145/3097983.3098131},
  urldate = {2024-06-27},
  isbn = {978-1-4503-4887-4},
  langid = {english}
}

@misc{jiang2024TIGERScoreBuildingExplainable,
  title = {{{TIGERScore}}: {{Towards Building Explainable Metric}} for {{All Text Generation Tasks}}},
  shorttitle = {{{TIGERScore}}},
  author = {Jiang, Dongfu and Li, Yishan and Zhang, Ge and Huang, Wenhao and Lin, Bill Yuchen and Chen, Wenhu},
  year = {2024},
  month = may,
  urldate = {2024-12-10},
  abstract = {We present TIGERScore, a {\textbackslash}textbf\{T\}rained metric that follows {\textbackslash}textbf\{I\}nstruction {\textbackslash}textbf\{G\}uidance to perform {\textbackslash}textbf\{E\}xplainable, and {\textbackslash}textbf\{R\}eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output \${\textbackslash}rightarrow\$ error analysis). We collected the `system outputs' through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERScore can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8{\textbackslash}\% accurate. Through these experimental results, we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task. All the resourced are released in our project website: {\textbackslash}url\{https://tiger-ai-lab.github.io/TIGERScore/\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Jiang/jiang_2024_tigerscore.pdf}
}

@misc{kaplan2020ScalingLawsNeural,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-10-29},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Kaplan/kaplan_2020_scaling_laws_for_neural_language_models.pdf}
}

@misc{kotonya2020ExplainableAutomatedFactChecking,
  title = {Explainable {{Automated Fact-Checking}}: {{A Survey}}},
  shorttitle = {Explainable {{Automated Fact-Checking}}},
  author = {Kotonya, Neema and Toni, Francesca},
  year = {2020},
  month = nov,
  primaryclass = {cs},
  urldate = {2024-06-27},
  abstract = {A number of exciting advances have been made in automated fact-checking thanks to increasingly larger datasets and more powerful systems, leading to improvements in the complexity of claims which can be accurately fact-checked. However, despite these advances, there are still desirable functionalities missing from the fact-checking pipeline. In this survey, we focus on the explanation functionality -- that is fact-checking systems providing reasons for their predictions. We summarize existing methods for explaining the predictions of fact-checking systems and we explore trends in this topic. Further, we consider what makes for good explanations in this specific domain through a comparative analysis of existing fact-checking explanations against some desirable properties. Finally, we propose further research directions for generating fact-checking explanations, and describe how these may lead to improvements in the research area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Kotonya_Toni/kotonya_toni_2020_explainable_automated_fact-checking.pdf}
}

@misc{kotonya2024FrameworkEvaluatingExplanations,
  title = {Towards a {{Framework}} for {{Evaluating Explanations}} in {{Automated Fact Verification}}},
  author = {Kotonya, Neema and Toni, Francesca},
  year = {2024},
  month = may,
  number = {arXiv:2403.20322},
  eprint = {2403.20322},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-09},
  abstract = {As deep neural models in NLP become more complex, and as a consequence opaque, the necessity to interpret them becomes greater. A burgeoning interest has emerged in rationalizing explanations to provide short and coherent justifications for predictions. In this position paper, we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically. We also outline one such formal framework, tailored to rationalizing explanations of increasingly complex structures, from free-form explanations to deductive explanations, to argumentative explanations (with the richest structure). Focusing on the automated fact verification task, we provide illustrations of the use and usefulness of our formalization for evaluating explanations, tailored to their varying structures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Zotero/storage/YYLDX39V/Kotonya and Toni - 2024 - Towards a Framework for Evaluating Explanations in.pdf}
}

@misc{lewandowsky2020DebunkingHandbook2020,
  title = {Debunking {{Handbook}} 2020},
  author = {Lewandowsky, Stephan and Cook, John and Lombardi, Doug},
  year = {2020},
  publisher = {[object Object]},
  doi = {10.17910/B7.1182},
  urldate = {2024-05-09},
  abstract = {This handbook compiles expert data about debunking misinformation. You may download a PDF of the handbook in the "Highlights" section below.},
  copyright = {Databrary Access Agreement},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Lewandowsky/lewandowsky_2020_debunking_handbook_2020.pdf}
}

@misc{lewis2021RetrievalAugmentedGenerationKnowledgeIntensive,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2021},
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.11401},
  urldate = {2023-10-27},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Lewis/lewis_2021_retrieval-augmented_generation_for_knowledge-intensive_nlp_tasks.pdf}
}

@inproceedings{maynez2020FaithfulnessFactualityAbstractive,
  title = {On {{Faithfulness}} and {{Factuality}} in {{Abstractive Summarization}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  pages = {1906--1919},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.173},
  urldate = {2024-07-29},
  abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Maynez/maynez_2020_on_faithfulness_and_factuality_in_abstractive_summarization.pdf}
}

@article{nyhan2020FactsMythsMisperceptions,
  title = {Facts and {{Myths}} about {{Misperceptions}}},
  author = {Nyhan, Brendan},
  year = {2020},
  month = aug,
  journal = {Journal of Economic Perspectives},
  volume = {34},
  number = {3},
  pages = {220--236},
  issn = {0895-3309},
  doi = {10.1257/jep.34.3.220},
  urldate = {2025-01-11},
  abstract = {Misperceptions threaten to warp mass opinion and public policy on controversial issues in politics, science, and health. What explains the prevalence and persistence of these false and unsupported beliefs, which seem to be genuinely held by many people? Though limits on cognitive resources and attention play an important role, many of the most destructive misperceptions arise in domains where individuals have weak incentives to hold accurate beliefs and strong directional motivations to endorse beliefs that are consistent with a group identity such as partisanship. These tendencies are often exploited by elites who frequently create and amplify misperceptions to influence elections and public policy. Though evidence is lacking for claims of a ``post-truth'' era, changes in the speed with which false information travels and the extent to which it can find receptive audiences require new approaches to counter misinformation. Reducing the propagation and influence of false claims will require further efforts to inoculate people in advance of exposure (for example, media literacy), debunk false claims that are already salient or widespread (for example, fact-checking), reduce the prevalence of low-quality information (for example, changing social media algorithms), and discourage elites from promoting false information (for example, strengthening reputational sanctions).},
  langid = {english},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Nyhan/nyhan_2020_facts_and_myths_about_misperceptions.pdf}
}

@misc{ouyang2022TrainingLanguageModelsa,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-25},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Zotero/storage/DXE4K7K2/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf}
}

@inproceedings{pan2023FactCheckingComplexClaims,
  title = {Fact-{{Checking Complex Claims}} with {{Program-Guided Reasoning}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {6981--7004},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2024-07-29},
  abstract = {Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Pan/pan_2023_fact-checking_complex_claims_with_program-guided_reasoning.pdf}
}

@article{radford2018ImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal = "",
  year = {2018},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/premtim.sahitaj/Zotero/storage/W83ZBYCD/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}


@misc{rafailov2023DirectPreferenceOptimization,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18290},
  eprint = {2305.18290},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-23},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Rafailov/rafailov_2023_direct_preference_optimization.pdf}
}

@article{russo2023BenchmarkingGenerationFact,
  title = {Benchmarking the {{Generation}} of {{Fact Checking Explanations}}},
  author = {Russo, Daniel and Tekiro{\u g}lu, Serra Sinem and Guerini, Marco},
  year = {2023},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1250--1264},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  urldate = {2024-07-29},
  abstract = {Fighting misinformation is a challenging, yet crucial, task. Despite the growing number of experts being involved in manual fact-checking, this activity is time-consuming and cannot keep up with the ever-increasing amount of fake news produced daily. Hence, automating this process is necessary to help curb misinformation. Thus far, researchers have mainly focused on claim veracity classification. In this paper, instead, we address the generation of justifications (textual explanation of why a claim is classified as either true or false) and benchmark it with novel datasets and advanced baselines. In particular, we focus on summarization approaches over unstructured knowledge (i.e., news articles) and we experiment with several extractive and abstractive strategies. We employed two datasets with different styles and structures, in order to assess the generalizability of our findings. Results show that in justification production summarization benefits from the claim information, and, in particular, that a claim-driven extractive step improves abstractive summarization performances. Finally, we show that although cross-dataset experiments suffer from performance degradation, a unique model trained on a combination of the two datasets is able to retain style information in an efficient manner.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Russo/russo_2023_benchmarking_the_generation_of_fact_checking_explanations.pdf}
}

@misc{schlichtkrull2023AVeriTeCDatasetRealworld,
  title = {{{AVeriTeC}}: {{A Dataset}} for {{Real-world Claim Verification}} with {{Evidence}} from the {{Web}}},
  shorttitle = {{{AVeriTeC}}},
  author = {Schlichtkrull, Michael and Guo, Zhijiang and Vlachos, Andreas},
  year = {2023},
  month = nov,
  number = {arXiv:2305.13117},
  eprint = {2305.13117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13117},
  urldate = {2024-05-02},
  abstract = {Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of \${\textbackslash}kappa=0.619\$ on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through several question-answering steps against the open web.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Schlichtkrull/schlichtkrull_2023_averitec.pdf}
}

@misc{schulhoff2024PromptReportSystematic,
  title = {The {{Prompt Report}}: {{A Systematic Survey}} of {{Prompting Techniques}}},
  shorttitle = {The {{Prompt Report}}},
  author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Da Costa, Hevander and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
  year = {2024},
  month = jun,
  number = {arXiv:2406.06608},
  eprint = {2406.06608},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-24},
  abstract = {Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area's nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Zotero/storage/Y46FYRY9/Schulhoff et al. - 2024 - The Prompt Report A Systematic Survey of Promptin.pdf}
}

@inproceedings{setty2024SurprisingEfficacyFineTuned,
  title = {Surprising {{Efficacy}} of {{Fine-Tuned Transformers}} for {{Fact-Checking}} over {{Larger Language Models}}},
  booktitle = {Proceedings of the 47th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Setty, Vinay},
  year = {2024},
  month = jul,
  pages = {2842--2846},
  publisher = {ACM},
  address = {Washington DC USA},
  doi = {10.1145/3626772.3661361},
  urldate = {2024-07-28},
  isbn = {9798400704314},
  langid = {english},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Setty/setty_2024_surprising_efficacy_of_fine-tuned_transformers_for_fact-checking_over_larger.pdf}
}

@misc{si2024LargeLanguageModels,
  title = {Large {{Language Models Help Humans Verify Truthfulness}} -- {{Except When They Are Convincingly Wrong}}},
  author = {Si, Chenglei and Goyal, Navita and Wu, Sherry Tongshuang and Zhao, Chen and Feng, Shi and Daum{\'e} III, Hal and {Boyd-Graber}, Jordan},
  year = {2024},
  month = apr,
  urldate = {2024-07-28},
  abstract = {Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they get, LLMs should not only provide information but also help users fact-check it. Our experiments with 80 crowdworkers compare language models with search engines (information retrieval systems) at facilitating fact-checking. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than those using search engines while achieving similar accuracy. However, they over-rely on the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-reliance on LLMs, but cannot significantly outperform search engines. Further, showing both search engine results and LLM explanations offers no complementary benefits compared to search engines alone. Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Si/si_2024_large_language_models_help_humans_verify_truthfulness_--_except_when_they_are.pdf}
}

@misc{vaswani2023AttentionAllYou,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-10-30},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Vaswani/vaswani_2023_attention_is_all_you_need.pdf}
}

@inproceedings{vlachos2014FactCheckingTask,
  title = {Fact {{Checking}}: {{Task}} Definition and Dataset Construction},
  shorttitle = {Fact {{Checking}}},
  booktitle = {Proceedings of the {{ACL}} 2014 {{Workshop}} on {{Language Technologies}} and {{Computational Social Science}}},
  author = {Vlachos, Andreas and Riedel, Sebastian},
  editor = {{Danescu-Niculescu-Mizil}, Cristian and Eisenstein, Jacob and McKeown, Kathleen and Smith, Noah A.},
  year = {2014},
  month = jun,
  pages = {18--22},
  publisher = {Association for Computational Linguistics},
  address = {Baltimore, MD, USA},
  doi = {10.3115/v1/W14-2508},
  urldate = {2024-05-05},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Vlachos_Riedel/vlachos_riedel_2014_fact_checking.pdf}
}

@inproceedings{wang2017LiarLiarPants,
  title = {``{{Liar}}, {{Liar Pants}} on {{Fire}}'': {{A New Benchmark Dataset}} for {{Fake News Detection}}},
  shorttitle = {``{{Liar}}, {{Liar Pants}} on {{Fire}}''},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Wang, William Yang},
  editor = {Barzilay, Regina and Kan, Min-Yen},
  year = {2017},
  month = jul,
  pages = {422--426},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-2067},
  urldate = {2025-01-13},
  abstract = {Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Wang/wang_2017_“liar,_liar_pants_on_fire”.pdf}
}

@inproceedings{wang2023ExplainableClaimVerification,
  title = {Explainable {{Claim Verification}} via {{Knowledge-Grounded Reasoning}} with {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Wang, Haoran and Shu, Kai},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {6288--6304},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.416},
  urldate = {2024-06-27},
  abstract = {Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Wang_Shu/wang_shu_2023_explainable_claim_verification_via_knowledge-grounded_reasoning_with_large.pdf}
}

@misc{warner2024SmarterBetterFaster,
  title = {Smarter, {{Better}}, {{Faster}}, {{Longer}}: {{A Modern Bidirectional Encoder}} for {{Fast}}, {{Memory Efficient}}, and {{Long Context Finetuning}} and {{Inference}}},
  shorttitle = {Smarter, {{Better}}, {{Faster}}, {{Longer}}},
  author = {Warner, Benjamin and Chaffin, Antoine and Clavi{\'e}, Benjamin and Weller, Orion and Hallstr{\"o}m, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and Cooper, Nathan and Adams, Griffin and Howard, Jeremy and Poli, Iacopo},
  year = {2024},
  month = dec,
  number = {arXiv:2412.13663},
  eprint = {2412.13663},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.13663},
  urldate = {2025-01-17},
  abstract = {Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Warner/warner_2024_smarter,_better,_faster,_longer.pdf}
}

@misc{wei2023ChainofThoughtPromptingElicits,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  urldate = {2024-12-15},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Wei/wei_2023_chain-of-thought_prompting_elicits_reasoning_in_large_language_models.pdf}
}

@misc{willard2023EfficientGuidedGeneration,
  title = {Efficient {{Guided Generation}} for {{Large Language Models}}},
  author = {Willard, Brandon T. and Louf, R{\'e}mi},
  year = {2023},
  month = aug,
  urldate = {2025-01-14},
  abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Willard_Louf/willard_louf_2023_efficient_guided_generation_for_large_language_models.pdf}
}

@inproceedings{yao2023EndtoEndMultimodalFactChecking,
  title = {End-to-{{End Multimodal Fact-Checking}} and {{Explanation Generation}}: {{A Challenging Dataset}} and {{Models}}},
  shorttitle = {End-to-{{End Multimodal Fact-Checking}} and {{Explanation Generation}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Yao, Barry Menglong and Shah, Aditya and Sun, Lichao and Cho, Jin-Hee and Huang, Lifu},
  year = {2023},
  month = jul,
  eprint = {2205.12487},
  primaryclass = {cs},
  pages = {2733--2743},
  doi = {10.1145/3539618.3591879},
  urldate = {2025-01-13},
  abstract = {We propose end-to-end multimodal fact-checking and explanation generation, where the input is a claim and a large collection of web sources, including articles, images, videos, and tweets, and the goal is to assess the truthfulness of the claim by retrieving relevant evidence and predicting a truthfulness label (e.g., support, refute or not enough information), and to generate a statement to summarize and explain the reasoning and ruling process. To support this research, we construct Mocheg, a large-scale dataset consisting of 15,601 claims where each claim is annotated with a truthfulness label and a ruling statement, and 33,880 textual paragraphs and 12,112 images in total as evidence. To establish baseline performances on Mocheg, we experiment with several state-of-the-art neural architectures on the three pipelined subtasks: multimodal evidence retrieval, claim verification, and explanation generation, and demonstrate that the performance of the state-of-the-art end-to-end multimodal fact-checking does not provide satisfactory outcomes. To the best of our knowledge, we are the first to build the benchmark dataset and solutions for end-to-end multimodal fact-checking and explanation generation. The dataset, source code and model checkpoints are available at https://github.com/VT-NLP/Mocheg.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Yao/yao_2023_end-to-end_multimodal_fact-checking_and_explanation_generation.pdf}
}

@inproceedings{zhang2021ExplainPredictThen,
  title = {Explain and {{Predict}}, and Then {{Predict Again}}},
  booktitle = {Proceedings of the 14th {{ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Zhang, Zijian and Rudra, Koustav and Anand, Avishek},
  year = {2021},
  month = mar,
  series = {{{WSDM}} '21},
  pages = {418--426},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3437963.3441758},
  urldate = {2024-12-15},
  abstract = {A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, which uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. Next, we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- sentiment classification, fact-checking, and question answering -- and find that we substantially outperform existing approaches.},
  isbn = {978-1-4503-8297-7},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Zhang/zhang_2021_explain_and_predict,_and_then_predict_again.pdf}
}

@article{zhou2021SurveyFakeNews,
  title = {A {{Survey}} of {{Fake News}}: {{Fundamental Theories}}, {{Detection Methods}}, and {{Opportunities}}},
  shorttitle = {A {{Survey}} of {{Fake News}}},
  author = {Zhou, Xinyi and Zafarani, Reza},
  year = {2021},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {5},
  eprint = {1812.00315},
  primaryclass = {cs},
  pages = {1--40},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3395046},
  urldate = {2023-10-25},
  abstract = {The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news detection and intervention. This survey reviews and evaluates methods that can detect fake news from four perspectives: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its source. The survey also highlights some potential research tasks based on the review. In particular, we identify and detail related fundamental theories across various disciplines to encourage interdisciplinary research on fake news. We hope this survey can facilitate collaborative efforts among experts in computer and information sciences, social sciences, political science, and journalism to research fake news, where such efforts can lead to fake news detection that is not only efficient but more importantly, explainable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Social and Information Networks,survey},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Zhou_Zafarani/zhou_zafarani_2021_a_survey_of_fake_news.pdf}
}

@misc{nguyen2024SurveySmallLanguage,
  title = {A {{Survey}} of {{Small Language Models}}},
  author = {Nguyen, Chien Van and Shen, Xuan and Aponte, Ryan and Xia, Yu and Basu, Samyadeep and Hu, Zhengmian and Chen, Jian and Parmar, Mihir and Kunapuli, Sasidhar and Barrow, Joe and Wu, Junda and Singh, Ashish and Wang, Yu and Gu, Jiuxiang and Dernoncourt, Franck and Ahmed, Nesreen K. and Lipka, Nedim and Zhang, Ruiyi and Chen, Xiang and Yu, Tong and Kim, Sungchul and Deilamsalehy, Hanieh and Park, Namyong and Rimer, Mike and Zhang, Zhehao and Yang, Huanrui and Rossi, Ryan A. and Nguyen, Thien Huu},
  year = {2024},
  month = oct,
  number = {arXiv:2410.20011},
  eprint = {2410.20011},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.20011},
  urldate = {2025-01-19},
  abstract = {Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/premtim.sahitaj/Library/CloudStorage/OneDrive-Personal/phd/literature/DATABASE/Nguyen/nguyen_2024_a_survey_of_small_language_models.pdf}
}


