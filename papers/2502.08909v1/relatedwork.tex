\section{Related Work}
NLP provides the foundation for efficiently processing and interpreting text, making it critical for addressing misinformation detection. Advances in transformer-based architectures have significantly improved language modeling and generation. Vaswani et al. \cite{vaswani2023AttentionAllYou} introduced the transformer architecture, which leverages multi-head self-attention to capture contextual dependencies between tokens. Variants such as BERT \cite{devlin2019BERTPretrainingDeep} and GPT \cite{radford2018ImprovingLanguageUnderstanding} illustrate encoder-only and decoder-only applications, respectively. BERT is well-suited for sequence classification tasks, while GPT generates text autoregressively for sequence-to-sequence tasks. In the context of this study, we refer to these lightweight architectures as SLMs \cite{nguyen2024SurveySmallLanguage}. \\\citet{kaplan2020ScalingLawsNeural} showed that the loss of a language scales as a power law with model parameter size, pre-training dataset size, and compute budget, guiding the development of current large language models (LLMs). Most LLMs follow the GPT architecture with modifications, aligning their b ,ehavior to specific tasks through instruction fine-tuning, reinforcement learning from human feedback (RLHF) \cite{ouyang2022TrainingLanguageModelsa}, direct preference optimization (DPO) \cite{rafailov2023DirectPreferenceOptimization}, or supervised fine-tuning. Prompts serve as inputs to these models, acting as task instructions. Effective prompt engineering, including template design and task-specific examples in a few-shot scenario, enhances their utility \cite{schulhoff2024PromptReportSystematic, brown2020LanguageModelsAre}. Retrieval-Augmented Generation (RAG) \cite{lewis2021RetrievalAugmentedGenerationKnowledgeIntensive} is a technique that combines retrieval mechanisms with NLG, allowing LLMs to ground their responses in external evidence. This approach can enhance factual accuracy of LLM generated responses by enforcing consistency with the evidence, and thus may reduce hallucinations in generative tasks. Automated fact-checking frameworks typically consist of claim detection, evidence retrieval, and claim verification \cite{guo2022SurveyAutomatedFactChecking}. Claim detection identifies check-worthy claims \cite{hassan2017AutomatedFactCheckingDetecting}, often guided by factors like relevance or harm. Evidence retrieval involves collecting and selecting relevant information to justify verdicts \cite{ferreira2016EmergentNovelDataset}. Claim verification can be broken down into two main tasks: (a) verdict prediction and (b) justification production \cite{guo2022SurveyAutomatedFactChecking}. Verdict prediction describes the task of classifying the veracity of a claim. Specific labeling schemes may be required to fit particular use cases or domains. \citet{augenstein2019MultiFCRealWorldMultiDomain} investigate various datasets for knowledge-based fact-checking with different labeling schemes, ranging from a simple binary classification task formulation to more nuanced labels that do not directly map onto a veracity scale. The authors highlight the challenge of standardizing these schemes across datasets. Several approaches to justification production exist. \citet{kotonya2020ExplainableAutomatedFactChecking} survey explainable automated fact-checking, emphasizing the need for effective explanation functionality, comparing current methods against desirable properties, and highlighting existing limitations. Similarly, \citet{russo2023BenchmarkingGenerationFact} treat justification production as an abstractive summarization task using language models. However, \citet{maynez2020FaithfulnessFactualityAbstractive} note that abstractive extraction models are prone to hallucinations, underlining the necessity for more systematic approaches of generating justifications to ensure transparency and reliability. While LLMs can help produce justifications, users may over-rely on these explanations even when they are incorrect \cite{si2024LargeLanguageModels}. \citet{eldifrawi2024AutomatedJustificationProduction} explore various methods for producing justifications, such as the \textit{predict-then-justify} approach, where a classification model determines veracity followed by a separate model generating explanations, and the \textit{justify-then-predict} approach, which employs reasoning techniques like chain-of-thought \cite{pan2023FactCheckingComplexClaims} and multi-hop \cite{wang2023ExplainableClaimVerification} with LLMs.