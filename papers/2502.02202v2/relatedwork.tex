\section{Related Work}
\label{relatedwork}
\paragraph{Contrastive learning}
Learning a high-quality representation of the target data that can be transferred to various downstream tasks is a major challenge in both 
Natural Language Processing(NLP) and computer vision. Contrastive learning is a powerful technique in representation learning. SimCLR~\citep{chen2020simple} introduced a novel framework for acquiring high-quality representations from unlabeled images, achieving state-of-the-art results in the field. Its popularity stems from its simplicity, as it operates without the need for specialized architectures \citep{bachman2019learning, henaff2020data} or memory banks \citep{he2020momentum, wu2018unsupervised}, while demonstrating remarkable performance across diverse downstream tasks. SimCLR discriminates positive and negative pairs in the projection space, and large batch sizes with many negative pairs and various data augmentations are required for optimal performance. 
Several subsequent papers attempted to improve SimCLRâ€™s performance and overcome its limitations. For example, \citet{chuang2020debiased} presents a debiased contrastive loss that corrects the sampling bias of negative examples, while BYOL \citep{grill2020bootstrap} relies only on positive samples for training and is more robust to batch size and data augmentations. Furthermore, \citet{kalantidis2020hard} emphasizes the importance of hard negatives in contrastive learning and proposes a hard negative mixing strategy that improves performance and avoids the need for large batch sizes and memory banks \citep{mocov2}. Recently, \citet{wang2023adaptive} introduced an adaptive multiple projection head mechanism for self-supervised contrastive learning models to address better intra- and inter-sample similarities between different views of the samples. They also incorporate an adaptive temperature mechanism to re-weight each similarity pair. In contrast, our approach employs multiple projection heads to capture different aspects of the samples within a supervised framework, making it suitable for downstream tasks such as multi-label and hierarchical classifications.

\begin{figure*}
\subfloat[Two projection heads $g_1$ and $g_2$ are attached to the encoder's output to capture subclass and superclass similarities respectively.\label{fig:img-arch}]{%
  \includegraphics[height=4cm,width=.49\linewidth]{images/img_arch.png}%
}
\hfill
\subfloat[We employ an individual projection head for each label and an additional global head to capture the overall similarities between the two reviews. \label{fig:text-arch}]{%
  \includegraphics[height=4cm,width=.49\linewidth]{images/nlp_arch.png}%
}
\caption{
Our proposed architecture with multiple projection heads for a) hierarchical classification and b) multi-label classification. 
The final loss is computed as a weighted sum of the losses from each head, with an additional cross-entropy loss applied in the case of text classification.
}
%\label{}
\end{figure*}

\paragraph{Supervised Contrastive Learning}
The self-supervised contrastive learning approach can be extended to supervised settings with minimal modifications. SupCon~\citep{khosla2020supervised} developed a supervised contrastive learning method that leverages labels by modifying the contrastive loss function. Instead of comparing each sample with all other samples, as in SimCLR, SupCon pulls samples from the same class closer and pushes samples from different classes farther apart. Several studies have aimed to enhance SupCon. For example, \citet{barbano2022unbiased} focused on controlling the minimal distance between positive and negative samples and introduced a new debiasing regularization loss. SupCon suffers from imbalanced data distribution, leading to poor uniformity in the feature distribution on the hypersphere. \citet{li2022targeted} addressed this issue by proposing a targeted supervised contrastive learning loss to maintain a uniform feature distribution even with long-tail distribution data. Additionally, \citet{chen2022perfectly} suggested a weighted class-conditional InfoNCE loss to enhance the robustness of SupCon. Furthermore, certain methods leverage contrastive representation to learn from noisy labels \citep{ciortan2021framework, ghosh2021contrastive, li2021learning, li2020mopro, xue2022investigating, yi2022learning}.  While these approaches indicate the suitability of contrastive learning in the supervised setting, they do not provide the possibility of contrasting multiple aspects.


\paragraph{Multi-label and Hierarchical Contrastive Learning}
Recent research has extended supervised contrastive learning (SCL) to multi-label contrastive learning \citep{dao2021multi, malkinski2022multi, dao2021contrast, zaigrajewcontrastive, zhang2024multi}. These extensions aim to bring samples with shared labels closer together while separating those with different labels. For instance, \citet{zaigrajewcontrastive} utilized the Jaccard similarity measure to assess label overlap between sample pairs and \citet{zhang2024multi} introduced multiple positive sets to address this problem. Meanwhile, \citet{sajedi2023end} introduced a kernel-based approach that transforms embedded features into a mixture of exponential kernels in Gaussian RKHS for multi-label contrastive learning. Additionally, \citet{dao2021contrast} proposed a module that learns multiple label-level representations using multiple attention blocks. However, the method by \citet{dao2021contrast}, which trains label-specific features, involves two-stage training and produces task-specific representations not universally applicable across all downstream tasks.  In contrast, our approach considers comprehensive similarity aspects among samples beyond mere label overlap, ensuring applicability across a wide range of classification tasks. Incorporating the class hierarchies, \citet{landrieu2021leveraging} (
Guided) integrate class distance into a prototypical network to model the hierarchical class structure and \citet{zhang2022use} (HiMulConE) introduced a hierarchical framework for multi-label representation learning with a new loss function penalizing sample distance in label hierarchies. Existing approaches often overlook diverse similarity perspectives, tailoring solutions to specific scenarios rather than broader applications.