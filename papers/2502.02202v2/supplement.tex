% \documentclass[twoside]{article}

% \usepackage{aistats2025}
% \usepackage{microtype}
% \usepackage{graphicx}
% \usepackage{svg}
% \usepackage{subcaption}
% \usepackage{booktabs} 
% \usepackage{hyperref}
% \usepackage{amssymb}
% \usepackage{amsmath}
% \usepackage{commath}
% \usepackage{graphicx,bm}
% \usepackage{verbatim}
% \usepackage{csquotes}
% \usepackage{multirow}

% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

% \begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\onecolumn
\aistatstitle{Supplementary Materials}


\section{Temperature Analysis for $\tau \to +\infty$}
We showed that adjusting the temperature parameter in the contrastive loss influences the focus of the model on different samples. In the main paper, we derived the case where $\tau \to 0^+$, illustrating that this leads the model to concentrate exclusively on the hardest negative samples among all available negatives. Here, we extend the analysis to the case where $\tau \to +\infty$. The loss function for a given sample 
$i$ in the projection space $h$ is denoted as:
\begin{equation*}  
L^i_h = \frac{-1}{|P_h(i)|} \sum_{p\in P_h(i)} \log \frac{\exp\left(\frac{z_i^h \cdot z_p^h}{\tau_h}\right)}{\sum_{a \in A(i)} \exp\left(\frac{z_i^h \cdot z_a^h}{\tau_h}\right)}.
\end{equation*}
As before, we omit the index $h$ for easier readability:

\begin{align*}
\lim_{\tau \to +\infty} |P(i)| \times L^i &= \sum_{p\in P(i)} \lim_{\tau \to +\infty} \Bigg[-\log \frac{\exp(z_i \cdot z_p/\tau)}{\sum_{a \in A(i)} \exp(z_i \cdot z_a/\tau)}\Bigg] \\
&= \sum_{p\in P(i)} \lim_{\tau \to +\infty} \Bigg[-z_i \cdot z_p /\tau + \log\sum_{a\in A(i)}\exp(z_i \cdot z_a/\tau)\Bigg] \\
&= \sum_{p\in P(i)} \lim_{\tau \to +\infty} \Bigg[-z_i \cdot z_p /\tau + \log(2N-1) + \sum_{a \in A(i)}\frac{\exp(z_i \cdot z_a/\tau)-(2N-1)}{2N-1}\Bigg]\\
&= \sum_{p\in P(i)} \lim_{\tau \to +\infty} \Bigg[-z_i \cdot z_p /\tau + \log(2N-1) -1 + \sum_{a \in A(i)}\frac{\exp(z_i \cdot z_a/\tau)}{2N-1}\Bigg]\\
&= \sum_{p\in P(i)} \lim_{\tau \to +\infty} \Bigg[-z_i \cdot z_p /\tau + \log(2N-1) -1 + \sum_{a \in A(i)}\frac{1 + z_i \cdot z_a/\tau}{2N-1}\Bigg]\\
&= \sum_{p\in P(i)} \lim_{\tau \to +\infty} \Bigg[-z_i \cdot z_p /\tau + \log(2N-1) + \frac{1}{\tau}\sum_{a \in A(i)}\frac{z_i \cdot z_a}{2N-1}\Bigg]\\
&= \lim_{\tau \to +\infty} \Bigg[|P(i)|\log(2N-1) + \frac{1}{\tau}|P(i)|\sum_{a \in A(i)}\frac{z_i \cdot z_a}{2N-1} - \sum_{p\in P(i)} z_i \cdot z_p /\tau \Bigg]
\end{align*}
The third equality results from the Taylor expansion of $\log(\sum_{a\in A(i)}\exp (z_i\cdot z_a/\tau)$ as $\tau \to +\infty$, while the fifth equality follows from the Taylor expansion of $\exp (z_i\cdot z_a/\tau)$ in the same limit.\newline
We observe that as $\tau \to +\infty$, the loss treats all the negative samples equally, meaning their contribution to the loss becomes uniform. Using a higher temperature for the superclass projection head encourages more uniform learning across all negative samples, allowing for greater flexibility in capturing dissimilarities.

\section{Gradient Analysis}
For clarity, we omit the index $h$ in our gradient computation. The final loss function $L$ is the sum of the losses for each individual sample, $L = \sum_{t \in I} L_i$. Unlike SupCon which only provides the derivative of $L^i$ with respect to $z_i$, we compute the derivative for all individual loss terms $L^t$ where $t \in I$.

 We calculate the derivative of $L^t$ by dividing it to three different cases: 1. $t=i$,  2. $t \in P(i)$ and 3. $t \in A(i) \setminus P(i)$. The final gradient of $L$ with respect to $z_i$ is then obtained by summing these individual gradients. The relative similarity between samples $m$ and $n$ is defined as $S_{m,n}$, where

\[
S_{m,n} = \frac{\exp(z_m \cdot z_n / \tau)}{\sum_{a \in A(m)} \exp(z_m \cdot z_a / \tau)}
\]

For the first case, where $t=i$, the gradient is:

\begin{align*}
\frac{\partial {L^t}}{\partial z_i} &= \frac{\partial}{\partial z_i} \Bigg[\frac{-1}{|P(i)|} \sum_{p \in P(i)}\log 
\frac{\exp( z_i \cdot z_p / \tau )}{\sum_{a \in A(i)} \exp( z_i \cdot z_a / \tau )}\Bigg]
\\
&=\frac{-1}{|P(i)|}\sum_{p \in P(i)}\frac{\partial}{\partial z_i}\Bigg[ \frac{z_i \cdot z_p}{\tau} - \log\bigg[\sum_{a \in A(i)} \exp( {z_i \cdot z_a}/{\tau} )\bigg] \Bigg]
=\frac{-1}{|P(i)|\tau}\sum_{p \in P(i)}  \bigg[z_p  - \frac{\sum_{a \in A(i)} z_a \exp ( z_i \cdot z_a / \tau )}{\sum_{a \in A(i)} \exp ( z_i \cdot z_a / \tau )}\bigg]  \\
&= \frac{-1}{\tau |P(i)|} \sum_{p \in P(i)} \bigg[ z_p - \sum_{a \in A(i)} z_aS_{i,a}\bigg] = \frac{-1}{\tau |P(i)|}
\sum_{p \in P(i)} z_p + \frac{1}{\tau}\sum_{a \in A(i)} z_aS_{i,a}
\end{align*}

The second case where $t \in P(i)$:

\begin{align*}
\frac{\partial {L^t}}{\partial z_i} &= \frac{\partial}{\partial z_i} \Bigg[\frac{-1}{|P(t)|} \sum_{p \in P(t)}\log 
\frac{\exp( z_t \cdot z_p / \tau )}{\sum_{a \in A(t)} \exp( z_t \cdot z_a / \tau )}\Bigg]
\\
&=\frac{-1}{|P(t)|}\sum_{p \in P(t)}\frac{\partial}{\partial z_i}\Bigg[ \frac{z_t \cdot z_p}{\tau} - \log\bigg[\sum_{a \in A(t)} \exp( {z_t \cdot z_a}/{\tau} )\bigg] \Bigg]
=\frac{-1}{|P(i)|\tau}\bigg[z_t -\sum_{p \in P(t)} \frac{ z_t \exp ( z_t \cdot z_i / \tau )}{\sum_{a \in A(t)} \exp ( z_t \cdot z_a / \tau )}\bigg]  \\
&= \frac{-1}{\tau |P(t)|} \bigg[ z_t - |P(t)|z_tS_{t,i}\bigg] = \frac{-1}{\tau |P(i)|} z_t + \frac{1}{\tau} z_tS_{t,i}
\end{align*}

where we used the fact that $|P(i)| = |P(t)|$. Similarly for the third case where $t \in A(i) \setminus P(i)$:

\begin{align*}
\frac{\partial {L^t}}{\partial z_i} &= \frac{1}{\tau} z_tS_{t,i}
\end{align*}

Thus, the gradient of the loss function with respect to $z_i$ is:


\begin{align*}
\frac{\partial {L}}{\partial z_i} &=
\frac{\partial}{\partial z_i} L_i + \sum_{j \in P(i)} \frac{\partial}{\partial z_i} L_j + \sum_{n \in A(i) / P(i)} \frac{\partial}{\partial z_i} L_n 
\\
&= \frac{-1}{\tau |P(i)|}
\sum_{p \in P(i)} z_p + \frac{1}{\tau}\sum_{a \in A(i)} z_aS_{i,a} + \sum_{j \in P(i)} \Big( \frac{-1}{\tau |P(i)|} z_j + \frac{1}{\tau} z_jS_{j,i} \Big) + \sum_{n \in A(i) / P(i)}\frac{1}{\tau} z_nS_{n,i}
\\
&= -\frac{2}{\tau} \Bigg[ 
\frac{1}{|P(i)|}\sum_{p \in P(i)} z_p - \sum_{a \in A(i)} z_a\frac{S_{a,i}+S_{i,a}}{2} 
\Bigg]
\end{align*}
In each iteration, $z_i$ is updated by moving in the direction of the negative gradient, which adjusts $z_i$ towards the average of the positive samples, offset by the weighted average of all samples. This aligns with the intuition that each sample seeks to have a representation closer to its class while distinct from others. The effect of temperature is evident in the relative similarities $S_{i,a}$ and $S_{a,i}$. A lower temperature leads to a sharp $S_{i,a}$ (and $S_{a,i}$), which is mostly dependent on the direction of the sample with maximum similarity $z_i^{\max}=\arg\max_{a \in A(i)} z_i \cdot z_a$. In this case, easy positive pairs—those with large values of $S_{p,i}$ or $S_{i,p}$—will have their influence either canceled out or reversed by significant negative contributions in the second term of the derivative. Consequently, the model will primarily focus on bringing the representations of hard positive pairs closer together, which is undesirable at higher levels of hierarchical classification.  For example, at the second level of the CIFAR-100 classification, a hard positive pair could be trout and shark, which are distinct classes within the superclass 'fish', and some degree of separability between their representations is desired. This observation further supports the use of a higher temperature parameter for the projection spaces corresponding to higher levels of the hierarchy.



\section{Ablation Studies}
\label{ablation}
\subsection{Label Noise} We evaluate the impact of the global projection head in our approach by introducing uniform label noise into the TripAdvisor and BeerAdvocate training datasets. For a label with a total of $N$ classes, the noise level of $r\%$ implies that the label $y_k$ for sample $x_k$ remains $y_k$ with a probability of $1-\frac{r(N-1)}{N}$ after adding noise, and switches to any of the other $(N-1)$ labels with a probability of $\frac{r}{N}$. This results in a completely random label if $r=100\%$. We train the model with label noise rates of $30\%$, $50\%$, $70\%$, and $80\%$. The corresponding results are presented in Table \ref{noise}. Our observation indicates that models trained with a large weight for global projection loss, set at $0.6$ for this experiment, show increased robustness to label noise, which is consistent with our interpretation of the global head as a regularizer.
\begin{table*}[h]
\caption{The Top-1 classification accuracy is reported under uniform noise labels for the TripAdvisor and BeerAdvocate datasets. In this setup, the global projection head is assigned a weight of 
$0.6$, while the total weight for the other projection heads remains constant at 
$0.2$, resulting in a cross-entropy weight of 
$0.2$. Results indicate an improvement of up to 7\% in noisy label settings when assigning a large weight to the global projection head loss.}
\label{noise}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{cccccccccc}
\toprule
\textbf{Dataset} & \textbf{\#Samples} & \multicolumn{4}{c}{\textbf{Cross-Entropy}} & \multicolumn{4}{c}{\textbf{MSCL (ours)}} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10}
& & 30\% & 50\% & 70\% & 80\% & 30\% & 50\% & 70\% & 80\%\\
\midrule
\multirow{4}{*}{TripAdvisor} & 60 & 61.00 & 51.77 & 40.4 & 36.00 & \textbf{62.75} & \textbf{55.00} & \textbf{43.11} & \textbf{37.00}\\
 & 180 & 61.17 & 49.83 & 39.74 & 36.31 & \textbf{64.10} & \textbf{54.79} & \textbf{44.30} & \textbf{39.10}\\
 & 360 & 62.28 & 50.96 & 40.25 & 36.54 & \textbf{64.90} & \textbf{55.43} & \textbf{43.72} & \textbf{39.59}\\
 & 600 & 63.00 & 50.72 & 40.89 & 37.13 & \textbf{65.00}& \textbf{55.50} & \textbf{44.90} & \textbf{40.24}\\
\midrule
\multirow{4}{*}{BeerAdvocate} & 60 & 57.59 & 45.53 & 36.54 & 34.3 & \textbf{62.18} & \textbf{53.70} & \textbf{39.57} & \textbf{36.47} \\
 & 180 & 60.27 & 47.23 & 38.46 & 35.90 & \textbf{63.67} & \textbf{53.81} & \textbf{42.88} & \textbf{37.98}\\
 & 360 & 58.20 & 47.49 & 39.66 & 36.79 & \textbf{63.63} & \textbf{54.83} & \textbf{44.72} & \textbf{41.05}\\
 & 600 & 60.90 & 49.00 & 40.61 & 36.40 &  \textbf{64.78} & \textbf{55.86} & \textbf{43.30} & \textbf{39.58}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\subsection{Transfer Learning}
To demonstrate the generalizability of our model's learned features, we conducted a transfer learning experiment. First, we train the encoder on CIFAR-100 and then train a linear classifier on CIFAR-10 using the frozen encoder weights from CIFAR-100. This approach enables us to assess the transferability of features learned on CIFAR-100 to CIFAR-10. The results, presented in Table \ref{results-transferlearning}, compare the performance of SupCon and our method, demonstrating an increase of over 1\% across various training sample sizes.

\begin{table}[ht]
\caption{Features from CIFAR-100 transferred to CIFAR-10. Classification accuracy of a linear classifier trained on a frozen encoder from the CIFAR-100 dataset. The highest test accuracy is highlighted in bold.}
\label{results-transferlearning}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
\#Training Samples & SupCon & MSCL (ours) \\
\midrule
50K & 85.97 &  \textbf{86.88}  \\
40K & 83.70 & \textbf{84.91}  \\
30K & 82.54 & \textbf{83.69}  \\
20K & 78.62 & \textbf{79.44}  \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\newpage
\subsection{Feature Analysis}
We provide additional t-SNE visualizations of the feature space for our approach compared to SupCon in Figures \ref{fig:representations2} to \ref{fig:representations5}. The second projection head plays a crucial role in preserving the proximity of representations for samples belonging to the same superclass. \newline
% In Figure \ref{fig:representations2}, the second projection head clusters fruits and vegetables closely together in the representation space while positioning the \textit{cattle} class distinctly apart from them. Figure \ref{fig:representations3} shows that the superclass projection head groups flowers together and keeps the \textit{cattle} class far from the flower group. In Figure \ref{fig:representations4}, the \textit{mouse} class is shown to be distant from the container group, and in Figure \ref{fig:representations5}, the \textit{spider} class is clearly separated from the superclass furniture.
\begin{figure}[h]
\label{tsne-fig2}
\subfloat[SupCon\label{fig:supcon_rep2}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/1/x_normal_0.png}%
}
\hfill
\subfloat[MLCL\label{fig:mulhead_rep2}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/1/x_multihead_76.png}%
}
\caption{SupCon positions samples from the \textit{cattle} class close to \textit{pear} and \textit{sweet pepper}. In contrast, MLCL clusters the fruit and vegetable classes more closely in the representation space while separating them from the \textit{cattle} class.}
\label{fig:representations2}
\end{figure}

\begin{figure}
\label{tsne-fig3}
\subfloat[SupCon\label{fig:supcon_rep}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/2/x_normal_0.png}%
}
\hfill
\subfloat[MLCL\label{fig:mulhead_rep}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/2/x_multihead_42.png}%
}
\caption{MLCL groups the flower classes more tightly in the representation space while separating them from the class \textit{cattle}.}
\label{fig:representations3}
\end{figure}

\begin{figure}
\label{tsne-fig4}
\subfloat[SupCon\label{fig:supcon_rep}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/3/x_normal_0.png}%
}
\hfill
\subfloat[MLCL\label{fig:mulhead_rep}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/3/x_multihead_0.png}%
}
\caption{In SupCon, samples from the \textit{can} class are positioned farther from \textit{bottle}, \textit{cup}, and \textit{bowl} compared to the \textit{mouse} class. In contrast, MLCL clusters the container classes more closely in the representation space.}
\label{fig:representations4}
\end{figure}

\begin{figure}
\label{tsne-fig5}
\subfloat[SupCon\label{fig:supcon_rep}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/4/x_normal_0.png}%
}
\hfill
\subfloat[MLCL\label{fig:mulhead_rep}]{%
  \includegraphics[height=5cm,width=.49\linewidth]{images/tsne-vis/good_tsne/4/x_multihead_111.png}%
}
\caption{MLCL aligns the furniture classes more closely in the representation space and distinct from the class \textit{spider}.}
\label{fig:representations5}
\end{figure}

% \setlength{\tabcolsep}{5pt} % General space between cols (6pt standard)


% \section{FORMATTING INSTRUCTIONS}

% To prepare a supplementary pdf file, we ask the authors to use \texttt{aistats2025.sty} as a style file and to follow the same formatting instructions as in the main paper.
% The only difference is that the supplementary material must be in a \emph{single-column} format.
% You can use \texttt{supplement.tex} in our starter pack as a starting point, or append the supplementary content to the main paper and split the final PDF into two separate files.

% Note that reviewers are under no obligation to examine your supplementary material.

% \section{MISSING PROOFS}

% The supplementary materials may contain detailed proofs of the results that are missing in the main paper.

% \subsection{Proof of Lemma 3}

% \textit{In this section, we present the detailed proof of Lemma 3 and then [ ... ]}

% \section{ADDITIONAL EXPERIMENTS}

% If you have additional experimental results, you may include them in the supplementary materials.

% \subsection{The Effect of Regularization Parameter}

% \textit{Our algorithm depends on the regularization parameter $\lambda$. Figure 1 below illustrates the effect of this parameter on the performance of our algorithm. As we can see, [ ... ]}

\vfill

\end{document}
