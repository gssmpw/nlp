\section{Related Work}
\label{relatedwork}
\paragraph{Contrastive learning}
Learning a high-quality representation of the target data that can be transferred to various downstream tasks is a major challenge in both 
Natural Language Processing(NLP) and computer vision. Contrastive learning is a powerful technique in representation learning. SimCLR____ introduced a novel framework for acquiring high-quality representations from unlabeled images, achieving state-of-the-art results in the field. Its popularity stems from its simplicity, as it operates without the need for specialized architectures ____ or memory banks ____, while demonstrating remarkable performance across diverse downstream tasks. SimCLR discriminates positive and negative pairs in the projection space, and large batch sizes with many negative pairs and various data augmentations are required for optimal performance. 
Several subsequent papers attempted to improve SimCLRâ€™s performance and overcome its limitations. For example, ____ presents a debiased contrastive loss that corrects the sampling bias of negative examples, while BYOL ____ relies only on positive samples for training and is more robust to batch size and data augmentations. Furthermore, ____ emphasizes the importance of hard negatives in contrastive learning and proposes a hard negative mixing strategy that improves performance and avoids the need for large batch sizes and memory banks ____. Recently, ____ introduced an adaptive multiple projection head mechanism for self-supervised contrastive learning models to address better intra- and inter-sample similarities between different views of the samples. They also incorporate an adaptive temperature mechanism to re-weight each similarity pair. In contrast, our approach employs multiple projection heads to capture different aspects of the samples within a supervised framework, making it suitable for downstream tasks such as multi-label and hierarchical classifications.

\begin{figure*}
\subfloat[Two projection heads $g_1$ and $g_2$ are attached to the encoder's output to capture subclass and superclass similarities respectively.\label{fig:img-arch}]{%
  \includegraphics[height=4cm,width=.49\linewidth]{images/img_arch.png}%
}
\hfill
\subfloat[We employ an individual projection head for each label and an additional global head to capture the overall similarities between the two reviews. \label{fig:text-arch}]{%
  \includegraphics[height=4cm,width=.49\linewidth]{images/nlp_arch.png}%
}
\caption{
Our proposed architecture with multiple projection heads for a) hierarchical classification and b) multi-label classification. 
The final loss is computed as a weighted sum of the losses from each head, with an additional cross-entropy loss applied in the case of text classification.
}
%\label{}
\end{figure*}

\paragraph{Supervised Contrastive Learning}
The self-supervised contrastive learning approach can be extended to supervised settings with minimal modifications. SupCon____ developed a supervised contrastive learning method that leverages labels by modifying the contrastive loss function. Instead of comparing each sample with all other samples, as in SimCLR, SupCon pulls samples from the same class closer and pushes samples from different classes farther apart. Several studies have aimed to enhance SupCon. For example, ____ focused on controlling the minimal distance between positive and negative samples and introduced a new debiasing regularization loss. SupCon suffers from imbalanced data distribution, leading to poor uniformity in the feature distribution on the hypersphere. ____ addressed this issue by proposing a targeted supervised contrastive learning loss to maintain a uniform feature distribution even with long-tail distribution data. Additionally, ____ suggested a weighted class-conditional InfoNCE loss to enhance the robustness of SupCon. Furthermore, certain methods leverage contrastive representation to learn from noisy labels ____.  While these approaches indicate the suitability of contrastive learning in the supervised setting, they do not provide the possibility of contrasting multiple aspects.


\paragraph{Multi-label and Hierarchical Contrastive Learning}
Recent research has extended supervised contrastive learning (SCL) to multi-label contrastive learning ____. These extensions aim to bring samples with shared labels closer together while separating those with different labels. For instance, ____ utilized the Jaccard similarity measure to assess label overlap between sample pairs and ____ introduced multiple positive sets to address this problem. Meanwhile, ____ introduced a kernel-based approach that transforms embedded features into a mixture of exponential kernels in Gaussian RKHS for multi-label contrastive learning. Additionally, ____ proposed a module that learns multiple label-level representations using multiple attention blocks. However, the method by ____, which trains label-specific features, involves two-stage training and produces task-specific representations not universally applicable across all downstream tasks.  In contrast, our approach considers comprehensive similarity aspects among samples beyond mere label overlap, ensuring applicability across a wide range of classification tasks. Incorporating the class hierarchies, ____ (
Guided) integrate class distance into a prototypical network to model the hierarchical class structure and ____ (HiMulConE) introduced a hierarchical framework for multi-label representation learning with a new loss function penalizing sample distance in label hierarchies. Existing approaches often overlook diverse similarity perspectives, tailoring solutions to specific scenarios rather than broader applications.