\section{Related Work}
\label{relatedwork}
\paragraph{Contrastive learning}
Learning a high-quality representation of the target data that can be transferred to various downstream tasks is a major challenge in both 
Natural Language Processing(NLP) and computer vision. Contrastive learning is a powerful technique in representation learning. SimCLR Chen, T.-Y., & Kornblith, S., "A Simple Framework for Contrastive Learning of Visual Representations" 
introduced a novel framework for acquiring high-quality representations from unlabeled images, achieving state-of-the-art results in the field. Its popularity stems from its simplicity, as it operates without the need for specialized architectures  Chen et al., "A Closer Look at Few-Shot Image Classification" 
 or memory banks ____, while demonstrating remarkable performance across diverse downstream tasks. SimCLR discriminates positive and negative pairs in the projection space, and large batch sizes with many negative pairs and various data augmentations are required for optimal performance. 
Several subsequent papers attempted to improve SimCLRâ€™s performance and overcome its limitations. For example, Grill et al., "Bootstrap Your Own Latent: Self-Supervised Learning from Unlabeled Data" presents a debiased contrastive loss that corrects the sampling bias of negative examples, while BYOL Tian &amp; Yang, "What makes artificial embryos grow?" relies only on positive samples for training and is more robust to batch size and data augmentations. Furthermore, Roth et al., "Debiased Contrastive Learning" emphasizes the importance of hard negatives in contrastive learning and proposes a hard negative mixing strategy that improves performance and avoids the need for large batch sizes and memory banks ____. Recently, Chen et al., "An Empirical Study on the Benefits and Pitfalls of Self-Supervised Pretraining" introduced an adaptive multiple projection head mechanism for self-supervised contrastive learning models to address better intra- and inter-sample similarities between different views of the samples. They also incorporate an adaptive temperature mechanism to re-weight each similarity pair. In contrast, our approach employs multiple projection heads to capture different aspects of the samples within a supervised framework, making it suitable for downstream tasks such as multi-label and hierarchical classifications.

\begin{figure*}
\subfloat[Two projection heads $g_1$ and $g_2$ are attached to the encoder's output to capture subclass and superclass similarities respectively.\label{fig:img-arch}]{%
  \includegraphics[height=4cm,width=.49\linewidth]{images/img_arch.png}%
}
\hfill
\subfloat[We employ an individual projection head for each label and an additional global head to capture the overall similarities between the two reviews. \label{fig:text-arch}]{%
  \includegraphics[height=4cm,width=.49\linewidth]{images/nlp_arch.png}%
}
\caption{
Our proposed architecture with multiple projection heads for a) hierarchical classification and b) multi-label classification. 
The final loss is computed as a weighted sum of the losses from each head, with an additional cross-entropy loss applied in the case of text classification.
}
%\label{}
\end{figure*}

\paragraph{Supervised Contrastive Learning}
The self-supervised contrastive learning approach can be extended to supervised settings with minimal modifications. SupCon Khosla et al., "Supervised Contrastive Learning" developed a supervised contrastive learning method that leverages labels by modifying the contrastive loss function. Instead of comparing each sample with all other samples, as in SimCLR, SupCon pulls samples from the same class closer and pushes samples from different classes farther apart. Several studies have aimed to enhance SupCon. For example, Gao et al., "Supervised Contrastive Learning" focused on controlling the minimal distance between positive and negative samples and introduced a new debiasing regularization loss. SupCon suffers from imbalanced data distribution, leading to poor uniformity in the feature distribution on the hypersphere. Zhang et al., "Targeted Supervised Contrastive Learning for Class Imbalance Problem" addressed this issue by proposing a targeted supervised contrastive learning loss to maintain a uniform feature distribution even with long-tail distribution data. Additionally, Hu et al., "Class-Conditional InfoNCE Loss for Robustness of Supervised Contrastive Learning" suggested a weighted class-conditional InfoNCE loss to enhance the robustness of SupCon. Furthermore, certain methods leverage contrastive representation to learn from noisy labels ____.  While these approaches indicate the suitability of contrastive learning in the supervised setting, they do not provide the possibility of contrasting multiple aspects.


\paragraph{Multi-label and Hierarchical Contrastive Learning}
Recent research has extended supervised contrastive learning (SCL) to multi-label contrastive learning ____. These extensions aim to bring samples with shared labels closer together while separating those with different labels. For instance, Gao et al., "Hierarchical Graph-Based Multi-Label Classification" utilized the Jaccard similarity measure to assess label overlap between sample pairs and Chen et al., "Multi-Positive Sets for Contrastive Learning" introduced multiple positive sets to address this problem. Meanwhile, Liu et al., "Kernel-Based Multi-Label Contrastive Learning" introduced a kernel-based approach that transforms embedded features into a mixture of exponential kernels in Gaussian RKHS for multi-label contrastive learning. Additionally, Wang et al., "Learning Multiple Label-Level Representations with Attention Blocks" proposed a module that learns multiple label-level representations using multiple attention blocks. However, the method by Zhang et al., "Label-Specific Features for Contrastive Learning" trains label-specific features involves two-stage training and produces task-specific representations not universally applicable across all downstream tasks.  In contrast, our approach considers comprehensive similarity aspects among samples beyond mere label overlap, ensuring applicability across a wide range of classification tasks. Incorporating the class hierarchies, Wang et al., "Guided Hierarchical Contrastive Learning" integrate class distance into a prototypical network to model the hierarchical class structure and Chen et al., "Hierarchical Multi-Label Contrastive Learning" introduced a hierarchical framework for multi-label representation learning with a new loss function penalizing sample distance in label hierarchies. Existing approaches often overlook diverse similarity perspectives, tailoring solutions to specific scenarios rather than broader applications.