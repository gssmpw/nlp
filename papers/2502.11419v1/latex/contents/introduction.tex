\section{Introduction}

Instruction fine-tuning is widely adopted to refine pre-trained LLMs to accurately understand human instructions and provide precise, pertinent and harmless responses \citep{collection-flan-2022, ds-survey}. LIMA \citep{ift-lima} has proved that the quality and diversity of instruction data are significantly more critical than its sheer quantity for training, motivating recent efforts in instruction data selection to reduce unnecessary training costs by eliminating low-quality and redundant data \citep{ds-survey}. However, how to evolve the selected instruction subset in parallel with the development of the instruction data remains underexplored.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\columnwidth]{latex/figures/data-selection-scenarios.pdf}
\end{center}
% \caption{Illustration of InsBank evolution.} 
\caption{Illustration of InsBank evolution. It is initialized by data selection on all current available instruction data, and it will evolve itself as long as new instruction data are proposed. A smaller training subset can be obtained from InsBank according to user training budget.} 
\label{fig: data selection}
\end{figure}


Specifically, with the continuous emergence of instruction datasets (The timeline of part instruction datasets is shown in Appendix~\ref{appendix: timeline}), it becomes necessary to regularly update the instruction subset to incorporate the latest advanced instruction data in order to ensure ongoing improvements in the alignment capabilities of LLMs. Simultaneously, the subset size must be controlled to avoid excessive growth that could lead to increased training costs. To address these practical challenges, we propose a novel concept termed \textbf{InsBank} (Instruction Bank). As shown in Figure~\ref{fig: data selection}, it is initially built by selecting current available instruction data. When new datasets are proposed, the bank evolves by selecting new data while phasing out an equivalent amount of older data, thereby maintaining an optimized instruction subset. The data in InsBank is also ranked, enabling users to extract smaller subsets tailored to specific training budgets efficiently.

The orderliness of InsBank is achieved through an overall score that combines individual quality and diversity scores. Quality scores can be obtained through manual or model annotation, but measuring diversity requires a global comparison between data, leading to significant storage and computational costs. During InsBank evolution, the impact of new data on the overall distribution necessitates continuous adjustment of each sample's diversity score. A straightforward approach would be to re-select data from all available instruction data during each evolution iteration. However, the massive volume of instruction data \citep{ds-survey} and its rapid growth \citep{collection-flan-2022, collection-self-instruct, collection-wizardlm} make this approach prohibitively expensive. Moreover, existing methods struggle to effectively represent and combine diversity and quality scores for ranking purposes.

To address these challenges, we propose Progressive Instruction Bank Evolution (\textbf{PIBE}) for continuous and efficient selection of the optimal instruction subset. PIBE employs a gradual manner of selection to evolve InsBank, ensuring long-term efficiency. Unlike the naive approach, PIBE significantly reduces evolution costs by excluding previously filtered-out data and focusing only on newly proposed and current InsBank data. However, the absence of past data changes the distribution of candidates, making it critical to preserve historical distribution information during evolution. Existing diversity-driven data selection methods \citep{ds-deita, ds-self-evolve} fall into two main categories: k-nearest neighbor (k-NN) \citep{indiv_eval_semantic1} and geometry-based coreset sampling \citep{indiv_eval_coreset1}. However, both of them rely solely on local information from a few neighboring points, making it difficult to record and utilize the rich information of previously eliminated data. Consequently, they cannot capture global relationships between points or provide robust individual diversity scores for effective ranking.
Inspired by Affinity Propagation \citep{cluster-ap}, we frame InsBank data selection as an exemplar election process, where the representativeness of each data point is quantified through an iterative voting mechanism. The representativeness further serves as the individual diversity score, and the voting results are passed to the next iteration as historical information to preserve the distribution of absent data. Moreover, existing data selection methods either prioritize quality or diversity \citep{ds-alpagasus}, or address them sequentially \citep{ds-deita}, failing to consider both aspects equally. In contrast, our diversity score integrates seamlessly with the quality score, enabling comprehensive and flexible instruction selection and InsBank ranking.

We simulate the process of instruction set development with five datasets and perform InsBank evolution on them with PIBE. We evaluate the general instruction following capability of fine-tuned models on AlpacaEval \citep{benchmark-alpaca-eval}, MT-Bench \citep{benchmark-mtbench} and IFEval \citep{benchmark-ifeval}. Experimental results show that PIBE outperforms the baselines and successfully evolves the instruction bank in parallel with the development of instruction sets. Besides, analysis on orderliness of InsBank indicates that users can flexibly select a smaller subset based on their budget.
Ours contributions can be summarized as follows:
\begin{itemize}[nosep]
    \item We propose InsBank, a dynamic framework for evolving instruction subsets alongside the development of instruction data, enabling continuous alignment improvements.

    \item We develop Progressive Instruction Bank Evolution (PIBE), an efficient approach that leverages a memory-enhanced diversity score and seamlessly integrates it with quality scores for optimal subset selection.

    \item We introduce a unified scoring system for individual samples, ensuring an ordered InsBank and enabling flexible extraction of high-quality subsets tailored to user budgets.
    
    \item Extensive experiments demonstrate that PIBE not only outperforms baseline methods in evolving InsBank but also provides flexible, budget-aware data selection, highlighting its effectiveness and adaptability.

\end{itemize}
