\section{Progressive Instruction Bank Evolution}

\begin{figure*}[hbtp]
\begin{center}
\includegraphics[width=0.75\textwidth]{latex/figures/Framework.pdf}
\end{center}
\caption{The detailed pipeline of PIBE. It consists of four core elements: the gradual manner of evolution, the flow of historical information across evolution rounds, individual representation scoring for diversity evaluation, and the integration of quality and diversity scores for data selection and ranking}
\label{fig: PIBE}
\end{figure*}

In this section, we provide a detailed explanation of PIBE, whose pipeline is depicted in Figure~\ref{fig: PIBE}.

\subsection{Gradual Evolution Formulation}

In this work, we propose the instruction subset evolution task to build the InsBank. Denoting current available instruction data as $\mathcal{X}_0$, the instruction bank $\mathcal{B}_{\pi}^{0,m}$ of size $m$ is initialized through data selection which can be presented as $\mathcal{B}_{\pi}^{0,m} = \pi(\mathcal{X}_0)$. Then, when new instruction dataset $\mathcal{X}_1$ is proposed, $\mathcal{B}_{\pi}^{0,m}$ should evolve itself to adapt to changes in data distribution. The naive manner of InsBank evolution can be represented as $\mathcal{B}_{\pi}^{1,m} = \pi(\mathcal{X}_0, \mathcal{X}_1)$ which can be extended to $\mathcal{B}_{\pi}^{t+1,m} = \pi(\mathcal{X}_0, ..., \mathcal{X}_t, \mathcal{X}_{t+1})$ for future evolution. However, this manner requires substantial storage and computational resources to calculate diversity scores as $t$ continues to increase. To improve the long-term evolution efficiency, we propose a gradual manner where only the newly proposed instruction data $\mathcal{X}_{t+1}$ along with the data participated in last round of evolution $\mathcal{X}_t + \mathcal{B}_{\pi}^{t-1,m}$ are involved into the current round of evolution, and the evolution can be represented as $\mathcal{B}_{\pi}^{t+1,m} = \pi(\mathcal{X}_{t+1}, \mathcal{X}_t + \mathcal{B}_{t-1}^m)$.

In addition to the update of InsBank, we evaluate the diversity and quality of each sample $x_i$ and provide an overall individual score for data ranking. Users can quickly select a smaller subset according to the data ranking to suit their own training budget.

\subsection{Historical Information Flowing}


Although a large amount of data is eliminated during InsBank evolution for efficiency, preserving their distribution information is crucial for maintaining InsBank's global representativeness. To address this, we introduce a momentum matrix based on historical voting results to retain the distribution information of excluded data, which flows across iterations, allowing filtered-out data to re-engage in future exemplar selection and preventing suboptimal global representativeness.


As described in Section~\ref{sec: ap}, we evaluate individual diversity through AP. By analyzing the similarity between previously selected data and newly proposed candidates, we estimate the suitability of new data as exemplars for the existing data and vice versa, represented by the responsibility matrix. 


Formally, let $\mathcal{X}'_t = \mathcal{X}_t \cup \mathcal{B}^{t-1, m}_\pi$ denote the full candidate data set from the previous round of InsBank evolution, and $\mathcal{X}'_{t+1} = \mathcal{X}_{t+1} \cup \mathcal{B}^{t, m}_\pi$ denote the full candidate data set of the $(t+1)$-th evolution round.
Then, the matrix $Sim_{t+1}$ of size \( |\mathcal{X}'_t| \times |\mathcal{X}_{t+1}| \) represents the cosine similarity between $\mathcal{X}'_t$ and $\mathcal{X}_{t+1}$.
Given the historical information matrix \(H_t\) of size \(|\mathcal{X}'_t| \times |\mathcal{X}'_t|\), representing the responsibility matrix stored from the \(t\)-th round of InsBank evolution, we derive the momentum responsibility matrix \(M_t\) using \(H_t\) and \(Sim_{t+1}\):
\begin{equation}
\label{eq: estimate top-right}
\small
\begin{aligned}
        w_{jk} &= \frac{Sim[j,k]}{\sum_{l=1}^{|X'_{t}|} Sim[l,k]}, \\
        M_t[i,k] &= \sum_{j=1}^{|X'_{t}|} w_{jk} * R_t[i,j]     
\end{aligned} 
\end{equation}


\begin{equation}
\small
\label{eq: estimate bottom-left}
        M_t[i,k] = \sum_{j=1}^{|X'_{t}|} w_{ij} * R_t[j,k]    
\end{equation}
This allows the filtered-out data to participate in exemplar election during future history-aware AP processes.



The structure of $M_t$ is depicted in Appendix~\ref{appendix: momentum-responsibility-matrix}. The top-left part of $M_t$ contains responsibility values between data in \(\mathcal{B}^{t, m}_\pi\), taken directly from \(H_t\). The top-right part represents the suitability of newly proposed candidate data as exemplars for previously selected data, estimated using Eq.~\ref{eq: estimate top-right}. Similarly, the bottom-left part represents the suitability of previously selected data as exemplars for newly proposed candidate data, estimated using Eq.~\ref{eq: estimate bottom-left}. 
The bottom-right section is filled with the median values of the other three sections.

We regard $M_t$ as a continuously decaying momentum term for historical information preserving.
% during the message passing iterations of the $(t+1)$-th round of data selection. 
Specifically, we first calculate $R_{t+1}^{i}$ by Eq.~\ref{eq: ap}. Then, we apply a weighted sum of $M_t$ and $R_{t+1}^{i}$ to recall the historical information as shown in Eq.~\ref{eq: history-aware iter},
\begin{equation}
\small
\label{eq: history-aware iter}
        R_{t+1}^{i} = \alpha_i \cdot M_t + (1-\alpha_i) \cdot (\beta \cdot R_{t+1}^{i} + (1-\beta) \cdot R_{t+1}^{i-1}) 
\end{equation}
where $\alpha_i = \lambda \cdot \alpha_{i-1}$ is the momentum coefficient with a decay rate of $\lambda$, and $\beta$ is the official AP damping rate \citep{cluster-ap}.
% to prevent numerical oscillations between iterations \citep{cluster-ap}. 
Finally, $A_{t+1}^{i}$ is calculated by Eq.~\ref{eq: ap}. All $\alpha$, $\lambda$ and $\beta$ are predefined hyperparameters. 

\subsection{Representativeness Scoring}

The individual representativeness score encapsulates the results of the exemplar election, reflecting both how willing other samples are to be represented by a specific sample and how unwilling the specific sample is to be represented by others. 
As explained earlier, the responsibility value \(R[i,k]\) indicates the suitability of \(x_k\) to serve as the exemplar for \(x_i\), while the availability value \(A[i,k]\) reflects the appropriateness of \(x_i\) selecting \(x_k\) as its exemplar. The combined value \((A+R)[i,k]\) represents the total evidence supporting \(x_i\)'s selection of \(x_k\) as its exemplar \citep{cluster-ap}. Thus, the sum of the \(k\)-th column of \(A+R\) can be interpreted as the total votes received by \(x_k\), and the sum of the \(i\)-th row of \(A+R\) represents the total votes cast by \(x_i\) for different samples. Defining \(Z = A + R\), the representativeness score of \(x_k\) is then computed using Eq.~\ref{eq: representation score}.

\begin{equation}
\small
\label{eq: representation score}
        s^k_{rep} = \sum_{i=1}^{|X'_{t+1}|}Z[i, k] - \sum_{i=1}^{|X'_{t+1}|}Z[k, i] + Z[k,k]
\end{equation}

\subsection{Integration of Diversity and Quality}
\label{method: combination}

Both data quality and data diversity are crucial for instruction tuning, yet existing methods often focus on one or address them sequentially. We combine quality and diversity scores in three ways, both preceded by min-max normalization (Eq.~\ref{eq: min-max}) to ensure scale consistency, where $s_q^k$ refers to the quality score of $x_k$, and $s^k_{rep}$ refers to the corresponding diversity score. 



\begin{equation}
\small
\label{eq: min-max}
\begin{aligned}
     {s'}^{k}_{rep} &= \frac{s^{k}_{rep} - \min\limits_{x_{i} \in {B}^m_{t}} s^{i}_{rep}}
        {\max\limits_{x_{i} \in {X'}_{t+1}} s^{i}_{rep} - \min\limits_{x_{i} \in {B}^m_{t}} s^{i}_{rep}},\quad \\
    {s'}_{q}^{k} &= \frac{s_{q}^{k} - \min\limits_{x_{i} \in {X'}_{t+1}} s_{q}^{i}}
        {\max\limits_{x_{i} \in {X'}_{t+1}} s_{q}^{i} - \min\limits_{x_{i} \in {X'}_{t+1}} s_{q}^{i}}
\end{aligned}  
\end{equation}


\begin{equation}
\small
\label{eq: addition-combine}
    s^{k} = {s'}^{k}_{rep} + \gamma \cdot {s'}_{q}^{k}.
\end{equation}


\begin{equation}
\small
\label{eq: multi-combine}
s^{k} =  (1+{s'}_{rep}^{k}) * (1+{s'}_{q}^{k})^{\gamma}
\end{equation}

Eq.~\ref{eq: addition-combine} and Eq.~\ref{eq: multi-combine} illustrate the calculation of the individual overall score using the additive and multiplicative approaches, respectively, where $\gamma$ is the weighting coefficient that controls the focus between diversity and quality. 


In practice, we observe that further improving quality beyond a certain level can reduce the fine-tuned model's performance. Additionally, when combining quality and diversity using linear methods, diversity scores often dominate the selection process. This occurs because quality, as a linear score, increases at a constant rate, even when excessively large values provide diminishing benefits. More details can be found in our experimental analysis of score combination (Section~\ref{exp: combination}). 

\begin{table*}[htbp]
    \centering
    \small 
    \setlength{\tabcolsep}{5pt} 
    \begin{tabular}{l|ccc|ccc|ccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Llama3-8B} & \multicolumn{3}{c|}{Qwen2.5-7B} & \multicolumn{3}{c}{Mistral-7B} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
           & AlpacaEval & MT-Bench & IFEval & AlpacaEval & MT-Bench & IFEval & AlpacaEval & MT-Bench & IFEval \\
    \midrule
    Full   & 19.07      & 5.88     & \underline{40.29} & 20.37   & 6.11    & 41.37    & 13.12   & 4.98   &   \textbf{35.25}  \\
    Random & 17.93      & 5.13     & 38.13             & 22.80   & 6.00    & 43.53    & 11.93    & 4.39   & 9.95     \\
    kCenter & 15.28     & 4.99     & 37.29             & 27.39   & 6.12    & \underline{46.40}    & 9.20  & 3.97    & 1.92     \\
    DEITA  & \underline{43.60} & 6.03 & 38.25         & \underline{50.43} & \underline{6.86} & 45.44 & \underline{28.82} & \underline{4.93} & 33.57 \\
    kNN$_1$ & 40.62     & \underline{6.04} & 38.49     & 46.96   & 6.62    & 45.56    & 26.62    & 4.91    & \underline{33.81} \\
    PIBE (ours)   & \textbf{44.84} & \textbf{6.23} & \textbf{40.89} & \textbf{51.55} & \textbf{6.88} & \textbf{46.76} & \textbf{29.48} & \textbf{5.03} & 29.38 \\
    \bottomrule
    \end{tabular}
    \caption{Comparison between different methods. For AlpacaEval and MT-Bench, we employ gpt-4o as annotator. The \textbf{bold} text indicates the best results, and the \underline{underlined} text represents the second-best results.}
    \label{tab: result-main}
\end{table*}


To address this, we design a nonlinear mapping function for quality scores, shown in Eq.~\ref{eq: nonlinear-combine}. Here, \(Q_p\) denotes the \(p\)-th percentile, \(r_l\) and \(r_h\) represent the lower and upper percentiles, \(S'_q\) refers to the scaled quality scores, and \(\sigma(\cdot)\) is the sigmoid function. The function, illustrated in Figure~\ref{fig: nonlinear_fn}, leverages the sigmoid's steepness in \((-2, 2)\) to enhance the distinguishability of scores within \([\tau_l, \tau_h]\), while flattening growth for scores above \(\tau_h\). Data below \(\tau_l\) are less considered, as such low-quality data are rarely selected into InsBank. Finally, we combine diversity with the nonlinear-mapped quality scores.
% using the multiplication method.

\begin{equation}
\small
\begin{aligned}
    \tau_l &= Q_{r_l}(S'_q) \\
    \tau_h &= Q_{r_h}(S'_q) \\
    c_{mul} &= 4 / (\tau_h - \tau_l) \\
    c_{sub} &= \tau_l + 2 / c_{mul} \\
    {s''}^k_q &= \sigma(({s'}^k_q - c_{sub}) * c_{mul})
\end{aligned}
\label{eq: nonlinear-combine}    
\end{equation}

After getting the overall scores, in addition to serving as the criterion for InsBank data selection, users can quickly select a smaller subset according to the data ranking to suit their own training budget.
