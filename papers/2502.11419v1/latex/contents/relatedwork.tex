\section{Related Work}

Instruction fine-tuning is widely used to refine LLMs. Early methods focused on fine-tuning with large-scale instruction datasets \citep{collection-flan-2021, collection-super-natural-inst} manually aggregated from extensive NLP task collections \citep{collection-flan-2022}. With advancements in generative models, \citet{collection-self-instruct} has led the trend of synthetic data generation \citep{collection-alpaca, collection-ultrachat, collection-wizardlm}. 
As \citet{ift-lima} found, quality and diversity are more important than quantity, driving recent efforts to cut training costs by removing low-quality and redundant data.Existing selection methods can be broadly categorized into three types \citep{ds-survey}: quality-based, diversity-based, and model-specific importance-based selection.

\textbf{Quality-based Selection} 
Humpback \citep{ds-self-alignment} selects high-quality samples through an iterative self-curation process where quality predictions are produced by the fine-tuned model of each turn. 
Recent works typically employ a GPT-model to annotate the data quality.  For example, ALPAGASUS \citet{ds-alpagasus} employs ChatGPT to score the accuracy of instruction data and select data according to a threshold. 

\textbf{Diversity-based Selection} 
The diversity-based selection aims to deduplicate the instruction data and maximize the coverage of selected data. Recent methods typically achieve this purpose by control the nearest neighbor distance \citep{ds-deita} or maximize the average distance between the selected data through text embedding \citep{ds-self-evolve}. INSTAG \citep{ds-instag} identifies semantics and intentions of instructions by tags and it assumes that a dataset is considered more diverse if it covers more individual tags.

\textbf{Model-specific Importance-based Selection} Importance refers to the necessity of adding one sample into training set \citep{ds-deita} whose indicator are typically model-specific \citep{ds-less, ds-ifd}. However, this work focuses on the general data selection and emphasizes the quality and diversity of selected data.


InfoGrowth \citep{data-growth} also aims to address the continuous expansion of datasets, but it primarily focuses on image data and relabeling noisy samples, making it less relevant to this paper. While InfoGrowth and DEITA consider both quality and diversity, they handle them sequentially, without combining them into a unified score. Besides, previous efforts primarily aggregate all candidate data before data selection and are not experimented under the progressive instruction bank evolution task. In this paper, we propose PIBE to efficiently obtain the optimal current instruction subset with comprehensive characterization and integration of diversity and quality scores.

