\section{Preliminaries}
\label{appendix: preliminary}
\subsection{Instruction Data Selection Problem}
Following \citet{ds-deita}, given a collection of instruction data $\mathcal{X} = \{x_1, x_2, ..., x_n\}$ where $x_i$ is an individual instruction-response pair, data selection selects an instruction subset $\mathcal{P}_{\pi}^m$ of size $m$ from $\mathcal{X}$, where $\pi$ is the data selection strategy. Denote the performance evaluation function for $\pi$ as $Q$, the optimal data selection strategy $\pi ^*$ with subset size $m$ satisfies:


\begin{equation}
\small
    \pi^* = \arg \max_{\pi}Q(\mathcal{P}_\pi^m)
\end{equation}

\subsection{Selection Metrics}
Previous research \citep{ds-deita, ds-survey} highlight that the effectiveness of instruction set selection depends on both quality and diversity. In line with this, we focus on the two aspects in this paper:

\textbf{Quality} of instruction data primarily refers to the accuracy and rationality which estimate the consistency and coherence of the instruction context, as well as whether the response accurately corresponds to the instructions \citep{ds-survey}. In this work, we adopt the quality evaluation model of DEITA \citep{ds-deita} for quality annotation. 

\textbf{Diversity} of instruction data is critical to the generalization ability of the trained model \citep{ds-survey}. There are currently two major approaches to measure diversity: k-nearest neighbor (k-NN) \citep{indiv_eval_semantic1} and geometry-based coreset sampling \citep{indiv_eval_coreset1}. 
% More details about instruction data diversity can be found in Appendix~\ref{appendix: instruction-data-diversity}.
The kNN approach measures sample's diversity by its distance to its $j$-th k-nearest neighbor (k-NN) with the help of text embeddings as shown in Eq.~\ref{eq: knn}:
\begin{equation}
\small
\label{eq: knn}
    kNN_i^j = d(e(x_i), e(N_j(x_i)))
\end{equation}
where $N_j(x_i)$ denotes the $j$-th closest neighbor of $x_i$ in the embedding space projected by $e(\cdot)$, and $d(\cdot, \cdot)$ calculates the distance between $x_i$ and $N_j(x_i)$.
The geometry-based coreset sampling approach is to find the most informative-and-diverse subset that represents the entire dataset the most through controlling the minimum distance between any two samples for subset selection \citep{indiv_eval_coreset1, indiv_eval_coreset2}. However, both methods rely solely on local information from nearby points, making it difficult to capture the global distribution relationships or utilize historically eliminated points, resulting in inadequate individual diversity scores for subset evaluation.

\subsection{Affinity Propagation}
\label{sec: ap}


Affinity Propagation (AP) \citep{cluster-ap} is a clustering algorithm that leverages message-passing to uncover the global distribution of data. It identifies exemplars by iteratively transmitting two kinds of messages between data points:

\begin{itemize}[nosep]
    \item \textbf{Responsibility ($R[i,k]$)} This message sent from point \(i\) to point \(k\) represents how suitable point \(k\) is to serve as the exemplar for point \(i\).
    \item \textbf{Availability ($A[i,k]$)} This message sent from point \(k\) to point \(i\) represents how appropriate it would be for point \(i\) to choose point \(k\) as its exemplar, taking into account the current responsibilities sent from other points to \(k\).
\end{itemize}

The messages are updated iteratively based on the rules as shown in Eq.~\ref{eq: ap}.
Here, \(S[i,k]\) represents the similarity between point \(i\) and point \(k\) where $i \neq k$. And $S[k,k]$ is filled by the predefined preference value which represents the preference for sample $i$ as an exemplar.

\begin{equation}
\small
\label{eq: ap}
    \begin{aligned}
        R[i, k] &\leftarrow S[i, k] - \max_{k' \neq k} \left\{A[i, k'] + S[i, k']\right\}, \\
        A[i, k] &\leftarrow \min \left\{0, R[k, k] + \sum_{i' \notin \{i, k\}} \max \left\{0, R[i', k]\right\}\right\}, \\
        A[k, k] &\leftarrow \sum_{i' \neq k} \max \{0, R[i', k]\}, \\
    \end{aligned}
\end{equation}

At any given moment, the clustering result can be determined by summing $R$ and $A$. For $x_i$, let $k'$ be the index that maximizes $A[i,k] + R[i,k]$, the conclusion are as follows: (1) if $i=k'$, then $x_i$ is a cluster center, (2) if $i \neq k'$, then $x_i$ belongs to the cluster center $x_{k'}$. That is, for $R+A$, the $i$-th row represents the votes cast by $x_i$ for different points to represent itself, while the $j$-th column represents the votes received by $x_j$. Based on this, we obtain the representativeness of $x_i$ according to the voting results by subtracting the votes cast by $x_i$ for other samples from the votes received by $x_i$. This result serves as individual diversity score.

