\section{Experiment}
\subsection{Experimental Setup}


\textbf{Candidate Instruction Data}  We aggregate five instruction datasets for general instruction following capability: Self-Instruct \citep{collection-self-instruct}, Alpaca (GPT-4) \citep{collection-alpaca-gpt4}, Dolly \citep{collection-dolly}, ShareGPT\footnote{We filter out incomplete conversations.} \citep{collection-sharegpt} and WilzardLM (alpaca) \citep{collection-wizardlm}, resulting in a mixed dataset of 278k samples. The statistics of each dataset is presented in Table~\ref{tab: dataset-statistics}.

\textbf{Training and Evaluation} In this work, we fine-tune the Llama3-8B model \citep{model-llama3} on the selected InsBank unless otherwise specified. Following DEITA \citep{ds-deita}, we set the size of InsBank to 6k for the convenience of subset evolution. During training, we further restrict the trainable tokens and the number of conversation turns. We adopt AlpacaEval \citep{benchmark-alpaca-eval}, MT-Bench \citep{benchmark-mtbench} and IFEval \citep{benchmark-ifeval} for automatic model alignment performance evaluation. More details about training and evaluation can be found in Appendix~\ref{appendixs: hyperparameters}.


\textbf{Baselines} We compare proposed PIBE with the following baselines:
\begin{itemize}[nosep]
    \item \textbf{Full} Train model on all candidate data.
    \item \textbf{Random} Randomly select $m$ samples from all candidate data. 
    \item \textbf{kNN$_1$} Measure the diversity of one sample by its euclidean distance to the nearest neighbor (Eq.~\ref{eq: knn}). The diversity score is first normalized and then combine with the normalized quality score by $s_i = (1+kNN_1^i) * (1+{s'}_q^i)^{\gamma}$ for data selection.
    \item \textbf{kCenter Greedy} \citep{indiv_eval_coreset2} The original kCenter Greedy algorithm is shown in Alg.~\ref{alg:kcentergreedy}. We take $\operatorname*{min}_{x_j\in S_b}d(e(x_i),e(x_j))$ as the individual diversity score and combine it with quality score in the same manner of kNN$_1$.
    \item \textbf{DEITA} Traverse the instruction pool in descending order of quality scores and involve the current sample to the selected subset if the largest cosine similarity between the current sample and the samples in the selected subset is less than the threshold (i.e. 0.9 following the raw setting of DEITA \citep{ds-deita}).
\end{itemize}

\subsection{Performance of SFT with InsBank}

Table~\ref{tab: result-main} compares the performance of LLM trained on subsets selected by different approaches. PIBE consistently outperforms the baselines on such benchmarks, showing the superiority of our data selection method. We further fine-tune Qwen2.5 7B \citep{llm-qwen2.5} and Mistral 7B \citep{llm-mistral} for robustness analysis, and the results exhibit the same trends, demonstrating that our method is effective across different models. We also report the quality and diversity of subsets selected by different methods in Table~\ref{tab: main-subset-statistics}. 
From the results of data selection, PIBE and DEITA demonstrate higher quality and diversity compared to kCenter and kNN. DEITA produces subsets with the highest quality, primarily because it prioritizes quality during the data selection process by traversing candidates in descending order of quality. In contrast, PIBE treats quality and diversity equally, enabling the subset to achieve the highest diversity while maintaining decent quality. 
From the perspective of downstream task performance, models fine-tuned with high-quality data (DEITA, PIBE) generally outperform those fine-tuned on low-quality data (kCenter, kNN). However, despite achieving the highest quality, DEITA’s downstream performance falls short of the more diverse PIBE, validating the importance of data diversity when the quality level is acceptable.

\subsection{Orderliness of InsBank}
Each sample in the InsBank selected by PIBE is provided with an overall individual score reflects both the diversity and quality which shows the priority of each sample to be used to fine-tune models. We sort the InsBank in descending order based on the overall individual score, and compare the performance of models fine-tuned with the “top2k, mid2k, bottom2k” samples in InsBank. Here, we use the instruction subset obtained from the final evolution round, and restrict the trainable tokens to 0.9M and turns to 2.3k. The results are illustrated in Fig~\ref{fig: results-ordering}, showing that the top-ranked data generally achieved better performance, proving the orderliness of InsBank. 

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lcccc}
    \toprule
     Metric & kCenter & DEITA &  kNN$_1$ & PIBE \\
    \midrule
    Quality & 4.37 & \textbf{5.19} & 4.82 & 5.13 \\
    Diversity & 62.26 & 86.94 & 77.24 & \textbf{91.84} \\
    \bottomrule
\end{tabular}
\caption{The quality and diversity of subsets selected by different methods. The diversity here is measured by euclidean distance between data.}
\label{tab: main-subset-statistics}
\end{table}

\begin{figure}[hbtp]
\begin{center}
\includegraphics[width=0.9\columnwidth]{latex/figures/ordered.pdf}
\end{center}
\caption{Results of orderliness experiment.}
\label{fig: results-ordering}
\end{figure}

\subsection{Analysis}

In this section, we analyze the effectiveness of diversity and quality. We also experiment PIBE with different score combination methods. More analysis about overlap between progressive evolving and full data selection, InsBank evolution, PIBE hyper-parameters, time costs and selected data quality distribution can be found in Appendix~\ref{appendix: addition-analysis}.

\paragraph{\textbf{Effectiveness of Diversity and Quality}}

To validate the role of diversity in instruction data selection, we first construct a quality-controlled subset where all data have quality scores within the range of 4.5 to 5.0 (details in Appendix~\ref{appendix: qc construction}). Using PIBE, we compute individual diversity scores for the subset, sort the data in descending order, and select the top 6k samples as the most diverse subset and the bottom 6k as the least diverse subset. The distributions of the two subsets are shown in Fig.~\ref{fig: qc_diversity}. Before fine-tuning, we restrict the total trainable tokens to 2M. Results in Table~\ref{tab: qc_diversity} indicate that, with comparable quality, models trained on more diverse data achieve better performance.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{l|cc|cc}
    \toprule
    Method  & Qua & Div & AlpacaEval & MT-Bench \\  
    \midrule
    Top & 4.84 & 81.14 & \textbf{27.70} & \textbf{5.52} \\
    Bottom & 4.86 & 68.55 & 27.33 & 5.43 \\
    \bottomrule
    \end{tabular}
    \caption{The results of quality-controlled diversity effectiveness experiment. Here, Qua refers to the average quality score, and Div refers to the average diversity score.}
    \label{tab: qc_diversity}
\end{table}

When it comes to quality, the improvement from extremely low to high quality is clearly beneficial, as extremely low-quality subsets often contain noisy data, such as irrelevant or incomplete responses. However, \emph{is continuously improving quality always effective in the data selection process?} To address this, we compare model performance fine-tuned on data selected by the following strategies in the final evolution iteration: (1) \textbf{Diversity Greedy}: selecting data with the highest diversity scores; (2) \textbf{Quality Greedy}: selecting data with the highest quality scores; and (3) \textbf{PIBE}. The results shown in Table~\ref{tab: analysis_div_and_qua} reveal a clear trade-off between diversity and quality. A purely greedy approach focusing on either aspect leads to suboptimal outcomes, while a balanced consideration of both proves more effective. This finding aligns with the main experiment results and suggests the existence of a balance point between diversity and quality, which we further investigate through combination methods in Section~\ref{exp: combination}.


\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{l|cc|cc}
    \toprule
    Method  & Qua & Div & AlpacaEval & MT-Bench \\  
    \midrule
    DG & 5.02 & 93.06 & \textbf{41.93}  & \textbf{6.09} \\
    QG & 5.20 & 83.70 & 40.86  & 5.86 \\
    % \midrule
    PIBE & 5.13 & 91.84 & 44.84 & 6.23 \\
    \bottomrule
    \end{tabular}
    \caption{Analysis of diversity and quality contribution. Here, DG refers to diversity greedy, and QG refers to quality greedy}
    \label{tab: analysis_div_and_qua}
\end{table}

\paragraph{\textbf{Analysis of Score Combination}}
\label{exp: combination}
We experiment with the different combination methods to explore the contribution of quality and diversity in PIBE. 

\begin{table}[htbp]
    \centering
    \small
    \setlength{\tabcolsep}{2pt} 
    \begin{tabular}{l|cc|ccc}
    \toprule
    Param & AlpacaEval & MT-Bench & SP-Qua & SP-Div & Diff \\
    \midrule
    \multicolumn{6}{c}{Multiplication} \\
    \midrule
    $\gamma=1$    & 44.84 & 6.23 & 0.36 & 0.74 & 0.38 \\
    $\gamma=2$    & \textbf{46.77} & \textbf{6.15} & 0.51 & 0.70 & 0.19 \\
    $\gamma=3$    & 42.98 & 6.17 & 0.54 & 0.67 & 0.13 \\
    \midrule
    \multicolumn{6}{c}{Addition} \\
    \midrule
    $\gamma=1$    & 44.84 & 6.13 & 0.44 & 0.72 & 0.28 \\
    $\gamma=2$    & \textbf{47.08} & \textbf{6.10} & 0.54 & 0.68 & 0.14 \\
    $\gamma=3$    & 44.53 & 6.09 & 0.56 & 0.64 & 0.08 \\
    \midrule
    \multicolumn{6}{c}{Nonlinear} \\
    \midrule
    $r_h=0.80$ & 44.41 & 5.98 & 0.58 & 0.72 & 0.14 \\
    $r_h=0.90$ & 44.84 & 6.19 & 0.62 & 0.70 & 0.08 \\
    $r_h=0.95$ & \textbf{47.58} & \textbf{6.36} & 0.63 & 0.69 & 0.06 \\
    \bottomrule
\end{tabular}
\caption{The results of different combination methods. Here, SP- refers to Spearman Correlation Coefficient, Diff refers to the difference value between SP-Qua and SP-Div.}
\label{tab: result-combination}
\end{table}

We first explore the basic multiplication manner and the addition manner, and the results are reported in Table~\ref{tab: result-combination}. Overall, regardless of whether addition or multiplication is used as the combination method, the experimental results exhibit a distinct trend of initially increasing and then decreasing as the influence of quality grows (i.e., with the increase of the $ \gamma $ value). This finding supports the hypothesis that a balance point exists between diversity and quality.

We analyze the correlation between quality and selection flags, as well as diversity and selection flags, for the top 12k data sorted by overall score (details in Appendix~\ref{appendix: correlation-analysis}). As shown in Table~\ref{tab: result-combination}, Spearman for diversity consistently surpass those for quality, indicating diversity's priority during selection. While increasing \(\gamma\) reduces the gap, this approach presents limitations: (1) Even at \(\gamma=3\), a notable gap remains between SP-Qua and SP-Div, particularly with the multiplication method; (2) Increasing \(\gamma\) further improves downstream performance initially but leads to declines afterward. 

Examining the quality distribution of selected data (Figure~\ref{fig: selected-data-quality-distribution}), we observe that \(\gamma=1\) includes some low-quality data, while \(\gamma=3\) selects excessive high-quality data. As discussed in Section~\ref{method: combination}, this stems from quality's linear nature. To address this, we use a nonlinear quality mapping function. Fixing \(r_l=0.3\), we compare different \(r_h\) values, with results shown in Table~\ref{tab: result-combination}. Nonlinear mapping significantly mitigates diversity's dominance and improves fine-tuned model performance, particularly at \(r_h=0.95\). Unlike linear methods, which improve subset quality by selecting extreme high-quality values, the nonlinear approach raises overall quality by incorporating more moderately high-quality data, aligning with its design goals.
