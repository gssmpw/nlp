\begin{table}
        \renewcommand{\arraystretch}{0.8}
        \begin{threeparttable}
        \begin{tabular}{@{}lcc@{}}
        \toprule
        Parameter & Floating Allegro Hand & Bimanual Robot Arms \\
        \midrule
        % \makecell{Initial object translational \\ perturbation (cm) }& [$\pm1.5$, $\pm1.5$, 0] & [$\pm 5$, $\pm 5$, 0]\\
        % \makecell{Initial object rotational \\ perturbation (rad) }& [0, 0, $\pm 0.3$] & [0, 0, $\pm 0.3$]\\
        Init. obj. trans.  pert. (cm) & [$\pm1.5$, $\pm1.5$, 0] & [$\pm 5$, $\pm 5$, 0]\\
        Init. obj. rot. pert. (rad) & [0, 0, $\pm 0.3$] & [0, 0, $\pm 0.3$]\\
        Object side length (cm) & [5.8, 6.2]  & [28, 32] \\
        Object mass (kg) & [0.1, 0.3]  & [0.25, 0.75]  \\
        Friction coefficients & [0.7, 1.3] &  [0.2, 0.4]  \\
        Task horizon (s) & 25 & 50 / 260  (Panda / iiwa) \\
        \bottomrule
        \end{tabular}
        \end{threeparttable}
        \caption{Ranges of different physical parameters $\theta$. The initial object pose is only perturbed in yaw, x, and y to ensure the object sits stably on the table. }
        \label{tab:domain_randomization}
        \vspace{0.5em}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/trajopt_unittest.png}
	\caption{\textbf{Trajectory optimization is crucial for generating dynamically feasible trajectories}. (Top) Before trajectory optimization, the kinematically retargeted demos easily lose contact and drive the object out of reach with different physical parameters or slight deviations in object states. (Bottom) Trajectory optimization encourages robots to establish contact with and maintain good manipulability of the object. The tricolor axis indicates the object orientation.}
	\label{fig:trajopt_unittest}
\end{figure*}

\section{Trajectory Optimization Experiments}

While kinematic retargeting of demonstrations might suffice to generate data for simpler manipulation tasks such as pick and place, it often falls short for the more challenging contact-rich tasks requiring frequent contact mode switches and fine-grained actions. In this section, we demonstrate that trajectory optimization is crucial for generating diverse, dynamically feasible contact-rich trajectories on three high-dimensional dexterous manipulation systems: a floating Allegro hand, bimanual iiwa arms, and bimanual Panda arms.

Our data generation framework is agnostic to the choice of the trajectory optimizer. We implement 
% a contact-implicit model predictive controller based on smoothed contact dynamics \cite{suh2024dexterous} and
the cross-entropy method (CEM) \cite{de2005tutorial} to solve \eqref{eq:predictive_control} over a distribution of physical parameters and initial conditions, as specified in Table \ref{tab:domain_randomization}. 
%\russtcomment{Right... the SQP discussion tricked me, but I guess that's only for the retargeting. I thought you had replaced this. In this case, your approach is almost doing RL, but on a policy parameterized as a trajectory... right? why is that better than doing PPO on a small neural net policy, and generating data from that? If you stick with CEM, than this will be your burden of proof, i think?}

\underline{\textbf{Task}} Manipulating the object to a target pose on the table (Fig. \ref{fig:policy_rollouts}). The object is initially placed randomly on the table with an arbitrary face upward. Task success is defined as the object reaching within 3 cm and 0.2 rad of the target pose for the Allegro hand, and within 10 cm and 0.2 rad for the bimanual robot arms.  This task requires long-horizon reasoning of complex multi-contact interactions between the robot and the object. The necessary frequent contact mode switches and high-dimensional action space pose great challenges for traditional model-based planners, while the precise contact interactions require fine-grained control actions. 

\begin{table}
\centering
        \renewcommand{\arraystretch}{0.8}
        \begin{threeparttable}
        \begin{tabular}{@{}lcccc@{}}
        \toprule
        Perturbation & Allegro Hand & iiwa Arms & Panda Arms \\
        \midrule
        Original demo &4 / 24 & 5 / 24 & 6 / 24\\
        Object size & 2 / 24 & 1 / 24 & 4 / 24\\
        % Object mass & 1 / 24& 1 / 24 & \\
        % Friction coefficients & 3 / 24 & 2 / 24 & \\
        Initial object translation & 1 / 24 & 3 / 24 & 2 / 24\\
        Initial object orientation & 2 / 24 & 3 / 24& 3 / 24\\
        \midrule
        Trajectory optimization & 2164 / 3000 & 2252 / 3000 & 2462 / 3000 \\
        \bottomrule
        \end{tabular}
        \end{threeparttable}
        \caption{Success rates of replaying kinematically retargeted trajectories of the 24 original human demos, and trajectory optimization under random perturbations in physical parameters and object initial conditions. }
        \label{tab:kin_success_rate}
\end{table}

% \begin{table}
% \centering
%         \renewcommand{\arraystretch}{0.8}
%         \begin{threeparttable}
%         \begin{tabular}{@{}ccc@{}}
%         \toprule
%         Allegro Hand & iiwa Arms & Panda Arms \\
%         \midrule
%         0.721 & 0.65 & 0.803\\
%         % Task Horizon (s) & 25 & 280 & 50 \\
%         \bottomrule
%         \end{tabular}
%         % \begin{tablenotes}
%         % \itme{*} 
%         % \end{tablenotes}
%         \end{threeparttable}
%         \caption{Success rates of trajectory optimization under random perturbations in physical parameters and object initial conditions. }
%         \label{tab:trajopt_success_rate}
%         \vspace{0.5em}
% \end{table}



% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.7\textwidth]{figures/aug_traj_den.png}
% 	\caption{\textbf{Distribution of object trajectories generated from a single demonstration}. The original demonstration (orange) is locally perturbed and augmented to about 100 dynamically feasible contact-rich trajectories (blue) for each system. The density map represents the object pose distribution of the generated trajectories in the specific 2-dimensional slices.}
%     \label{fig:aug_data_distribution}
% \end{figure*}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{figures/aug_traj_snapshots.png}
% 	\caption{\textbf{Snapshots of trajectories generated from a single demonstration}. The original demonstration (orange) is locally perturbed and augmented to about 100 dynamically feasible contact-rich trajectories (blue) for each system. The density map represents the object pose distribution of the generated trajectories in the specific 2-dimensional slices.}
%     \label{fig:aug_data_distribution}
% \end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/density_snapshots_aug_traj.png}
	\caption{\textbf{Distribution and snapshots of trajectories generated from a single demonstration.} (a) The original demonstration (orange) is locally perturbed and augmented to about 100 dynamically feasible contact-rich trajectories (blue) for each system. The density map represents the object pose distribution of the generated trajectories in the specific 2-dimensional slices. (b) Snapshots of 30 dynamically feasible trajectories under random physical parameters and object initial poses for bimanual iiwa arms are visualized.}
    \label{fig:aug_data_distribution}
\end{figure*}


\underline{\textbf{Dynamic Feasibility}}
While kinematic motion retargeting can generate visually plausible robot and object trajectories, these trajectories often lack dynamical consistency due to the differences in physical parameters and embodiment between the human demonstrator and the target robot. To illustrate this, we replay the kinematically retargeted trajectories of the original 24 human demos and record the success rates for each system in Table \ref{tab:kin_success_rate}. Furthermore, we randomly sample object sizes and perturbations of initial object poses according to Table \ref{tab:domain_randomization} and roll out the nominal kinematically retargeted trajectories. Some trajectories still succeed under certain perturbations thanks to caging grasps or other strategies that encourage robustness during the human demonstration. For all the systems, the successful rollouts are relatively short, manipulating the object to the goal pose within only 1 or 2 rotations. 
% Notably, the successful trajectories for the iiwa and Panda arms vary significantly, despite being generated from the same initial set of demonstrations.

The low success rate of purely kinematically retargeted trajectories highlights the importance of trajectory optimization for locally refining the demos for the particular embodiments and physical parameters. Before trajectory optimization, the floating Allegro hand lightly touches the cube and easily loses contact when rotating it clockwise (demonstrated in Fig. \ref{fig:trajopt_unittest}a). After trajectory optimization, the hand increases the contact area, establishing a stable grip for rotation. In Fig. \ref{fig:trajopt_unittest}b, similar behavior that encourages contact can be observed for the bimanual iiwa arms: the demo trajectory tries to rotate the box clockwise only using a single arm, while trajectory optimization encourages the other arm to help hold the box and reorient the box more stably. These refinements that encourage contact are particularly helpful when the object is heavier or smaller, or when the friction coefficients are lower than expected. In addition, replaying the kinematically retargeted trajectory often fails when the object pose deviates slightly from the demonstration, driving the object out of reach (visualized in Fig. \ref{fig:trajopt_unittest}c). In contrast, trajectory optimization 
%stabilizes the system in a vicinity around the demonstration, ensuring higher success rates even when the object is perturbed
accounts for the system’s true dynamics and can adjust the robot’s actions accordingly. The success rates of trajectory optimization under random perturbations in physical parameters and object initial conditions for each system are recorded in Table \ref{tab:kin_success_rate}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/policy_rollouts.png}
    \caption{\textbf{Policy rollouts for different embodiments.} The object manipulation task requires the robots to frequently make and break contact with the object. It also requires precise control of the robot since small deviations in positions can result in missing contact interactions and lead to task failure. } 
    \label{fig:policy_rollouts}
\end{figure*}

\underline{\textbf{Cross-Embodiment Generalization}} We demonstrate that a single set of human demonstrations can be effectively repurposed to generate dynamically consistent, contact-rich trajectories across different robotic embodiments with varying task horizons. Specifically, human demonstrations involving two index fingers manipulating a small cube are retargeted to fixed-base bimanual Kuka LBR iiwa and Franka Emika Panda arms manipulating a larger box (visualized in Fig. \ref{fig:kinematic_retargeting}). This approach addresses key challenges in data collection for contact-rich tasks: directly teleoperating two real robot arms to flip a large box would be both physically demanding and cost-prohibitive due to hardware latency, limited feedback, and the embodiment gap--differences in kinematic structure, degrees of freedom, and workspace between human and robotic arms. In contrast, performing the same task on a smaller scale using human fingers is more intuitive, reduces physical effort, and enables faster, more consistent demonstration collection.

The iiwa and Panda arms differ in contact geometry, velocity limits, and joint constraints, all of which are explicitly modeled within the trajectory optimization framework described in \eqref{eq:predictive_control}. For safe hardware deployment, we enforce conservative velocity limits on the iiwa arms, while only applying soft velocity regularization on the Panda arms in simulation to allow for more aggressive motions.


\underline{\textbf{Data Diversity}} 
Trajectory optimization efficiently augments a single demonstration to a wide distribution of trajectories with locally perturbed physical parameters and initial conditions as visualized in Fig. \ref{fig:aug_data_distribution}. The diverse states in the generated dataset cover a larger training distribution and encourage smoother learned policies, as will be discussed in the next section.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/policy_failure.png}
    \caption{\textbf{Failure cases of baselines.} (a) The baseline policy trained on the original 24 demonstrations for the floating Allegro hand frequently misses contact or gets stuck on the cube. (b-c) The baseline policies for the bimanual robot arms often exhibit jittery motion, resulting in loss of contact, the box being kicked out of reach, or the robot arms running into and getting stuck on the box surface. } 
    \label{fig:policy_failure}
\end{figure*}

\begin{figure}
\centering
\includegraphics[width=0.42\textwidth]{figures/success_rate.png}
	\caption{Success rates of policy evaluation in simulation and hardware. }
	\label{fig:success_rate}
    \vspace*{-0.4cm}
\end{figure}

% \begin{figure}
% \centering
% \includegraphics[width=0.48\textwidth]{figures/jitteriness.png}
% 	\caption{\textbf{Joint angles of bimanual iiwa arms over time. } Each line represents the trajectory of a different joint of the iiwa arms. The policy trained on augmented datasets (b) demonstrates significantly smoother motion compared to the baseline policy (a). }
% 	\label{fig:jitteriness}
% \end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/hardware_rollout.png}
    \caption{\textbf{Policy rollouts on hardware.} The fixed-base bimanual iiwa arms perform a sequence of coordinated rolling, pitching, and yawing actions to reorient the box to the goal pose. } 
    \label{fig:hardware_rollout}
\end{figure*}

\section{Behavior Cloning Experiments}
We illustrate our framework's capability to efficiently produce diverse, high-quality contact-rich datasets for training behavior cloning policies across multiple robotic platforms, including the floating Allegro hand and the bimanual Panda arms in simulation as well as bimanual iiwa arms on hardware. We show that policies trained on the generated data generalize to a wide distribution of physical parameters and initial conditions, and are much more robust and performant than the ones trained only on the original demonstrations. 
\subsection{Policy Evaluation in Simulation}
\label{subsec:policy_eval_sim}
From only 24 human demonstrations, our data generation pipeline can efficiently generate thousands of dynamically feasible contact-rich trajectories using trajectory optimization. We train state-based diffusion policies \cite{chi2023diffusion} on the 24 original demo trajectories, as well as 500 and 1000 generated trajectories. While our method is compatible with any Behavior Cloning algorithm, we adopt diffusion policies due to its recent success in contact-rich tasks \cite{chi2024universal, zhu2024should, li2024planning}. Fig. \ref{fig:policy_rollouts} visualizes the policy rollouts. We evaluate the performance by conducting 48 policy rollouts for each embodiment in simulation and record the success rates in Fig. \ref{fig:success_rate}. The success criteria are the same as specified in the trajectory optimization experiments.
%For policy evaluation, we visualize the initial states for all evaluation episodes, typical failure cases of baseline policies, and final object pose errors in Fig. \ref{fig:policy_eval}.  
% and validate that the generated data help improve the policy's robustness and generalizability.

\subsubsection{Floating Allegro Hand} 
While the human demonstrator completes the task in approximately 5 seconds on average in the virtual reality environment, the demonstration trajectories are temporally scaled by a factor of 2.5 to ensure smoother, dynamically feasible motions on the floating Allegro hand, which is subject to velocity limits. We define the task horizon as 25 seconds to allow the policy sufficient time to recover from missed contacts and other errors during the execution. The task complexity arises from the 22-dimensional action space of the Allegro hand and the long-horizon nature of the task, which requires a sequence of coordinated rolling, pitching, and yawing actions to reorient the cube to an upright position. These factors together present significant challenges for traditional model-based planners without guidance.

The baseline behavior cloning policy trained on the original set of 24 demonstrations achieves a success rate of $10 / 48 = 21\%$ and exhibits significant jittery behavior when encountering out-of-distribution states. The workspace, characterized by diverse object orientations and translations, is sufficiently large that minor deviations during policy rollouts often drive the trajectory out of the demonstrated distribution. Common failure modes include the Allegro hand repeatedly missing contact with the cube or becoming stuck on its surface while attempting reorientation (visualized in Fig. \ref{fig:policy_failure}a), which often result in the object being trapped in intermediate orientations. In contrast, policies trained on the expanded dataset generated by our pipeline demonstrate a higher likelihood of re-establishing contact with the object after initial misses, resulting in significantly improved success rates up to $39 / 48 = 81\%$.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/hardware_eval.png}
    \caption{\textbf{Policy failure and recovery on hardware.} The baseline policy frequently (a) gets stuck on the box surface when small deviations from the demonstration trajectories occur, and (b) struggles to recover from out-of-distribution states, where the object is never intentionally lifted for accomplishing the task in the generated dataset. Policies trained on augmented datasets (c) sometimes fail due to unmodeled collision geometry, but (d) can recover from undesired sliding by employing firmer grasps found by trajectory optimization. } 
    \label{fig:hardware_eval}
\end{figure*}
\subsubsection{Bimanual Robot Arms}
The baseline policy trained on the original set of 24 human demonstrations achieves a success rate of $27 / 48 = 56\%$ on the bimanual iiwa system. We hypothesize that the restrictive velocity limits encourage more quasi-static behavior, leading to longer trajectories with a higher density of state-action pairs in the training data. In contrast, the baseline policy yields a success rate of $14/48=29\%$ on the bimanual Panda system, likely due to the more dynamic nature of the learned behavior under its looser velocity constraints. Both baseline policies exhibit remarkably jittery motion, frequently kicking the box out of reach, losing contact, or running into and getting stuck on the box surface during reorientation (visualized in Fig. \ref{fig:policy_failure}b and c). Policies trained on the augmented dataset, however, generate significantly smoother trajectories and are capable of re-establishing contact with the object after initial misses, resulting in as high as $44 / 48 = 92\%$ success rates for bimanual iiwa arms and $42 / 48 = 87.5\%$ for bimanual Panda arms. Additionally, the learned policies capture multimodal behaviors observed in the original human demonstrations, such as rotating the box either clockwise or counterclockwise for similar object poses. 


\subsection{Policy Evaluation on Hardware}
We zero-shot deploy the trained policies on hardware for bimanual iiwa arms to flip a 30 cm cubic box on a table (Fig. \ref{fig:hardware_rollout}). An OptiTrack motion capture system is employed to estimate the object pose. The baseline behavior cloning policy only achieves $6/23=26\%$ success rate, with most successful rollouts being relatively short-horizon, involving only 1 or 2 rotations. Common failure modes of the baseline policy include: 1) deviation from the demonstration trajectory, causing the arms to collide with the box surface (Fig. \ref{fig:hardware_eval}a), and 2) significant box sliding during rolling, resulting in the policy encountering out-of-distribution states and failing to recover (Fig. \ref{fig:hardware_eval}b). In contrast, as shown in Fig. \ref{fig:success_rate}b, the policy trained on 500 generated trajectories achieves $17 / 23 = 74\%$ success rate, while the policy trained on 1000 generated trajectories achieves $16/23=70\%$ success rate. Despite occasional box sliding during rolling, these policies demonstrate an improved ability to stabilize the box by using one arm to hold the opposite side more firmly to prevent further sliding (Fig \ref{fig:hardware_eval}d). However, as visualized in Fig \ref{fig:hardware_eval}c, both policies trained on the augmented datasets exhibit failure modes originating from unmodeled collision geometries on iiwa arms, which lead to significant undesired yaw motions of the box during pitch actions.\looseness=-1