\section{Data Collection}

We present a Virtual Reality (VR)-based data collection pipeline designed for intuitive and efficient collection of human demonstrations across multiple robot embodiments. 
The pipeline emphasizes simplicity and cross-embodiment generalization while minimizing the reliance on physical robot hardware. 
While we consider the data collection pipeline to be one of our contributions, we emphasize that the simulation-based large-scale data generation method presented in the next section is independent of this particular data collection approach.

\begin{figure}
\centering
	\includegraphics[width=0.42\textwidth]{figures/data_collection_pipeline.png}
	\caption{VR-based human-hand demonstration framework.}
	\label{fig:data_collection}
\end{figure}
\begin{figure*}[t]
\centering
	\includegraphics[width=1.0\textwidth]{figures/human_demo_kinematic_retargeting.png}
	\caption{Human hand demo in VR and kinematic retargeting for different embodiments. The blue spheres illustrate the demo hand landmarks scaled to the specific system.} 
	\label{fig:kinematic_retargeting}
	\vspace*{-0.2cm}
\end{figure*}
Our data collection pipeline (Fig. \ref{fig:data_collection}) is a human-hand demonstration interface in VR. We use an Apple Vision Pro to track the poses of the human demonstrator's hands and stream the poses to the Drake physics simulator \cite{drake}, which simulates the contact interaction between the object and the hands. The updated object pose is then sent back to Apple Vision Pro for real-time visualization in VR using Vuer~\cite{vuer}.
% Fig. \ref{fig:data_collection} outlines the simulation-based data collection pipeline.

Our demonstration interface is fast and cost-effective. Since the system operates entirely in simulation, it removes the dependency on robot hardware, significantly reducing the cost and complexity of data collection. 
In practice, it takes approximately 7 minutes to collect 24 long-horizon demos for each considered system.
The setup is also intuitive to use, as the human demonstrator does not have to mentally close the embodiment gap between the human body and the specific robot.

We demonstrate our pipeline on two different classes of robot embodiments: a dexterous hand and a bimanual manipulation setup.

\underline{\textbf{Floating Allegro Hand}} For the dexterous hand, we consider a 22-DOF free-floating Allegro hand manipulating a cube on a table as shown in Fig. \ref{fig:kinematic_retargeting}. Since the Allegro hand only has four fingers, we restrict the VR-based demonstrations to using four fingers on the right hand to interact with the object in simulation.

\underline{\textbf{Bimanual Robot Arms}} For the bimanual manipulation setup, we consider two different fixed-base bimanual manipulators: a pair of 7-DOF Kuka LBR iiwa arms, and a pair of Franka Emika Panda arms. Each pair of arms collaboratively manipulates a big box (Fig. \ref{fig:kinematic_retargeting}). During the VR demonstrations, the human demonstrator uses both index fingers to manipulate a small cube in VR and constrains their wrist movement to mimic the fixed base. During kinematic motion retargeting (detailed in Sec. \ref{subsec:kin_retarget}), the small cube and fingers are scaled to match the size of the larger box and the robot manipulators. 

This design facilitates two forms of cross-embodiment generalization. First, it utilizes easy-to-collect human finger demonstrations to guide planning for harder and higher-dimensional tasks, such as the dual-arm manipulators. Second, it supports the reuse of the same set of demonstrations across multiple robot platforms, as both the iiwa and Panda arms can leverage the same data to accomplish the manipulation task, eliminating the need for embodiment-specific demonstrations.

%We would like to note that our automated data generation framework is agnostic to the data collection approach. In particular, our method can adapt legacy datasets collected using outdated configurations to new robot settings, reducing the cost to collect large amount of data on the new robot setups from scratch. We outline our data collection pipeline for its simplicity, efficiency and capability for cross-embodiment generalization.

