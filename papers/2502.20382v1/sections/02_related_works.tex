\section{Related Works}
In this section, we review the most relevant approaches for generating diverse robot data for contact-rich tasks.
We categorize the methods into data collection, data augmentation, model-based planning, demonstration-guided reinforcement learning and cross-embodiment transfer.

\subsection{Data Collection for Imitation Learning}
Behavior Cloning \cite{pomerleau1988alvinn}, which trains robot policies to mimic expert behavior, has shown impressive empirical results in a wide range of dexterous manipulation tasks \cite{chi2023diffusion}. Collecting high-quality robot data has been an essential component of imitation learning (IL). Many such methods rely on human experts teleoperating a robot to accomplish specific tasks. Researchers have adopted interfaces such as 3D spacemouse \cite{chi2023diffusion, zhu2023viola}, 
%virtual and augmented reality (VR/AR) devices \cite{ebert2021bridge, zhang2018deep, duan2023ar2}, hand-held grippers \cite{chi2024universal, seo2024legato}, 
and puppeteering platforms \cite{fu2024mobile, zhao2024aloha} for end-effector \cite{o2023open} and whole-body control \cite{fu2024humanplus, he2024omnih2o}. 


% \subsection{AR/VR Interfaces for Data Collection}

Virtual and augmented reality (VR/AR) interfaces have recently gained traction as effective alternatives for robot data collection \cite{ebert2021bridge, zhang2018deep}, reducing cognitive load, physical strain, and user frustration compared to traditional techniques like kinesthetic teaching or 3D mouse control \cite{smith2024augmented}. These technologies offer a more intuitive data collection paradigm for complex tasks, especially in dexterous manipulation. AR2-D2 \cite{duan2023ar2} enables data collection without a physical robot by projecting a virtual robot into the physical workspace, but lacks real-time feedback necessary for precise control. 
DART~\cite{park2024dexhub} supports data collection entirely in simulation, visualized through a VR headset, but faces challenges bridging the sim-to-real gap for physical robot deployment.  
ARCap~\cite{chen2024arcap} integrates real-time AR feedback, but requires specialized hardware, including an RGBD camera, motion capture gloves, and VR controllers, in addition to the AR headset. ARMADA~\cite{nechyporenko2024armada} enables real-world manipulation data collection with bare hands through real-time virtual robot feedback, achieving high success rates when replayed on physical hardware. In contrast to these existing systems, our work focuses on scalable data generation from a small number of human demonstrations by leveraging trajectory optimization, facilitating generalization across different robot embodiments, initial conditions, and physical parameters.\looseness=-1

\subsection{Data Augmentation}
Despite many research efforts, collecting large datasets remains time-consuming and costly, requiring a large amount of human effort and resources. To address these challenges, significant effort has been devoted to automating the data generation process through data augmentation techniques. Existing approaches have leveraged state-of-the-art generative models for visual \cite{zhang2024diffusion, tian2024view, chen2024rovi} and semantic \cite{mandi2022cacti, chen2023genaug, yu2023scaling} augmentations. MimicGen \cite{mandlekar2023mimicgen} and its bimanual extension DexMimicGen \cite{ jiang2024dexmimicgen} automatically synthesize large-scale datasets from a small number of human demonstrations. These works decompose long-horizon tasks into object-centric subtasks and replay transformed demonstrations open loop in simulation. SkillMimicGen \cite{garrett2024skillmimicgen} extends this paradigm by segmenting tasks into motion and skill components, augmenting local manipulation skills with MimicGen-style replay and using motion planning to connect these skill segments.  RoboCasa \cite{nasiriany2024robocasa} leverages generative models to create diverse kitchen scenes with abundant 3D assets and utilizes MimicGen for automated trajectory generation. While these approaches have shown success in automating data generation, they primarily rely on kinematic replay of demonstrations, which is often inadequate for contact-rich manipulation tasks. Our work can be viewed as an important extension to MimicGen line of works to support dynamically feasible contact-rich data generation, which requires fine-grained control of the robot and continuous reasoning about making and breaking contacts with the environment.

\subsection{Trajectory Optimization for Contact-Rich Tasks}

% \bpgcomment{Motivate why we need some human demonstrations, because the planning problem is very hard to solve from scratch.}

Planning and control through contact remains a significant challenge for both learning-based and model-based methods due to the explosion of contact modes and the nonsmooth nature of contact dynamics. To tackle these challenges, researchers have explored various trajectory optimization formulations for multi-contact interactions.

% \noindent \textbf{Mixed-Integer Programming} \russtcomment{I don't actually think MIP deserves a subsection here? most people would not think about it for this pipeline. If you want to call out Bernhard's work (great!), then i would just add one sentence at the end of CITO saying "a new line of work tries to address this with efficient global optimization, but does not yet scale to the problems we consider here." or something like that.} Mixed-integer programming (MIP) approaches aim to explicitly reason about contact modes by introducing binary variables to represent the contact state at each time step \cite{marcucci2019mixed, marcucci2017approximate, aceituno2020global, hogan2020reactive}. While effective for small systems, MIP formulations suffer from poor scalability as the number of contact modes grows exponentially with the systemâ€™s complexity \cite{huang2021efficient}. To mitigate this limitation, some works have explored convex relaxations that encode contact modes without the need for binary variables \cite{graesdal2024towards}, but these approaches have so far been constrained to lower-dimensional systems.

\noindent \textbf{Contact-Implicit Trajectory Optimization} Existing works based on contact-implicit trajectory optimization (CITO) \cite{posa2014direct, mordatch2012discovery} have sought to formulate the combinatorial problem into a smooth optimization problem by using complementarity constraints. CITO has been applied in various domains, including planar manipulation \cite{onol2019contact, moura2022non}, dynamic pushing \cite{sleiman2019contact}, and locomotion tasks \cite{tassa2012synthesis, neunert2016efficient, winkler2018gait}. Recent efforts have extended CITO for real-time applications as model predictive control (MPC) \cite{wang2022contact, kurtz2023inverse}, with successful hardware deployment on quadrupeds using tailored solvers \cite{neunert2018whole, le2024fast}.  
Aydinoglu et al. \cite{aydinoglu2024consensus} parallelize the solution of linear complementarity problems using alternating direction method of multipliers (ADMM) and validate the method on hardware for multi-contact manipulation tasks.
While CITO shows promising scalability for handling contact modes, it suffers from poor global exploration and relies on good initial guesses \cite{ctr}.
A new line of work tries to address these issues with efficient global optimization \cite{graesdal2024towards}, but does not yet scale to the tasks we consider here.

\noindent \textbf{Sampling-Based Planning} Sampling-based methods have also shown great promise for solving trajectory optimization for contact-rich tasks. H{\"a}m{\"a}l{\"a}inen et al. \cite{hamalainen2015online} employ sampling-based belief propagation for humanoid balancing, juggling and locomotion. Carius et al. 
\cite{carius2022constrained} extend the path integral formulation to handle state-input constraints and validate the approach on quadruped stabilization on hardware.
More recently, Pezzato et al. \cite{pezzato2023sampling} applied sampling-based predictive control (SPC) for simpler contact tasks like pushing, while Howell et al. \cite{howell2022predictive} and Li et al. \cite{li2024drop} extended SPC to more complex, contact-rich tasks such as in-hand cube reorientation. Pang et al. \cite{pang2023global} use smoothed contact dynamics with global sampling to generate contact-rich plans in under a minute, with performance comparable to reinforcement learning. Cheng et al. introduce HiDex \cite{cheng2023enhancing}, a hierarchical planner that combines Monte-Carlo Tree Search with integrated contact projection, achieving rapid planning for dexterous manipulation tasks. 
Interestingly, directly applying sampling-based planners for contact-rich data generation in behavior cloning can be problematic, as the high entropy of the generated trajectories often degrades downstream policy performance \cite{belkhale2024data, zhu2024should}.

In this work, we leverage low-entropy human demonstrations to guide the global planning for multi-contact interactions and utilize trajectory optimization to locally refine the trajectories for specific physical parameters and robot embodiments. From a small number of demonstrations, the model-based planner can efficiently generate abundant, high-quality, contact-rich data for training robust robot policies.

\subsection{Demonstration-Guided Reinforcement Learning}
While IL often demands a large number of expert demonstrations to achieve robust and high-performing policies, reinforcement learning (RL) aims to solve tasks autonomously through reward-driven exploration. However, pure RL can suffer from inefficient exploration and the need for extensive reward shaping, especially in complex manipulation tasks \cite{chen2022system, qi2023hand}. To address these challenges, researchers have explored using demonstrations to guide RL, improving both sample efficiency and exploration quality.

Demonstrations have been integrated into RL pipelines in various ways, including adding them directly to the replay buffer \cite{vecerik2017leveraging, nair2018overcoming}, using behavior cloning for policy pretraining \cite{rajeswaran2017learning, hu2023imitation, hansen2022modem}, and augmenting task rewards with information extracted from demonstrations \cite{zhu2018reinforcement, peng2021amp, peng2018deepmimic}. Sleiman et al. \cite{sleiman2024guided} guide RL with demonstrations generated from a model-based trajectory optimizer for multi-contact loco-manipulation tasks, and validate their method on hardware with a quadrupedal mobile manipulator. While these approaches search over the parameters of a neural network policy and potentially optimize a more global objective, we leverage trajectory optimization as a complementary tool to \emph{locally} refine and expand demonstration trajectories. This enables the efficient generation of contact-rich data while avoiding the computational overhead, approximation errors, and unnecessary exploration associated with RL's high-dimensional search space.
%Moreover, these RL-based methods typically assume that demonstrations are collected using the same robot embodiment as the target policy, limiting their applicability for cross-embodiment generalization. Our approach, in contrast, leverages demonstrations collected in a flexible virtual reality framework, enabling their use across multiple robot platforms without requiring embodiment-specific data collection. 

\subsection{Cross-Embodiment Generalization}
% \bpgcomment{Is "Cross-Embodiment Transfer" a better title?}
Reusing datasets and policies across different embodiments unlocks the potential for large-scale robot learning. One line of work learns latent plans from videos of humans interacting with the environment and transfers this knowledge for robotic manipulation \cite{li2024okami, wang2023mimicplay}. Another approach involves portable data collection tools, such as hand-held grippers \cite{chi2024universal, seo2024legato}, for in-the-wild human demonstrations. While these methods enable policy deployment on multiple robot platforms, they are often constrained to robots with the same end-effector used during data collection, limiting generalization across platforms. On the other hand, to leverage large-scale datasets, recent works pull data from a heterogeneous set of robots ranging from navigation to manipulation, and train a robotic foundation model capable of accomplishing a diverse range of tasks \cite{yang2024pushing, doshiscaling}. Our proposed framework enables reusing the same set of easy-to-collect demonstrations for multiple robots, avoiding the need to collect embodiment-specific data for contact-rich tasks.
