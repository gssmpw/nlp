\section{APPENDIX}
% Tarik: Also mention the evaluation procedure, how many runs are recorded? What is the executed action horizon? Are we doing any action chunking/blending? 
In appendix, we present the implementation details of CEM and policy training.
% \begin{figure*}[t]
% \centering
% \includegraphics[width=1.0\textwidth]{figures/allegro_franka_snapshots.png}
% 	\caption{Snapshots of trajectories generated from a single demonstration for Allegro hand and bimanual Panda arms.}
%     \label{fig:allegro_franka_snapshots}
% \end{figure*}
\subsection{CEM Implementation Details}
\label{sec:appendix_cem}
\begin{table}
\centering
        \renewcommand{\arraystretch}{0.8}
        \begin{threeparttable}
        \begin{tabular}{@{}lccccc@{}}
        \toprule
        Parameter & $T$ & Plan Duration & $q_o$ & $q_r$ & $r_u$ \\
        \midrule
        Floating Allegro Hand & 6 & 1.25 s & 10 & 0.01 & 0.1 \\
        Bimanual iiwa Arms & 6 & 1.25 s & 10 & 0.01 & 10 \\
        Bimanual Panda Arms & 6 & 2.0 s & 10 & 0.01 & 10 \\
        \bottomrule
        \end{tabular}
        \end{threeparttable}
        \caption{\textbf{Parameters for CEM. } $T$: planning horizon. $q_o$: scalar weight for tracking object trajectories. $q_r$: scalar weight for tracking robot trajectories. $r_u$: scalar weight for control input.}
        \label{tab:cem_params}
\end{table}
We provide detailed parameters for the CEM implementation in Table \ref{tab:cem_params}. We optimize over the action knot points $u_{0:T-1}$, which are linearly interpolated to generate action commands sent to Drake. Drake simulates the contact dynamics $f$ at 200 Hz. The state cost matrix  $Q_t = diag(q_o \cdot \mathbf{1}_{n_o}, q_r \cdot \mathbf{1}_{n_r})$, where $n_o$ and $n_r$ denote the object and robot state dimensions, and $\mathbf{1}$ is a vector or all 1's. The terminal state cost matrix $Q_T = 10 \cdot Q_t$. The input cost matrix $R_t = diag(r_u \cdot \mathbf{1}_{n_u})$, where $n_u$ represents the control input dimension. All of the systems adopt 50 samples, 5 elites and initial standard deviation $\sigma = 0.05 \cdot \mathbf{1}_{n_u}$ for action sampling.
\subsection{Policy Implementation Details}
We train UNet-based diffusion policies \cite{chi2023diffusion} for all tasks. The action space is the robot configuration (joint angles, and additional floating base coordinates for the Allegro hand), while the observation space is the robot configuration and object pose (with orientations represented by rotation matrices). Detailed parameters are listed in Table \ref{tab:diffo_po_params}.

\begin{table}
\centering
        \renewcommand{\arraystretch}{0.8}
        \begin{threeparttable}
        \begin{tabular}{@{}lcccccc@{}}
        \toprule
        Parameter & $T_o$ & $T_a$ & Freq & Epochs & Obs. Dim. & Act. Dim. \\
        \midrule
        Floating Allegro Hand & 10 & 40 & 50 & 1000 & 34 & 22 \\
        Bimanual iiwa Arms & 10 & 40 & 20 & 800 & 26 & 14\\
        Bimanual Panda Arms & 10 & 40 & 50 & 800 & 26 & 14\\
        \bottomrule
        \end{tabular}
        \end{threeparttable}
        \caption{\textbf{Parameters for diffusion policies. } $T_o$: observation horizon. $T_a$: action horizon. Freq: environment frequency (Hz, both observations and actions).}
        \label{tab:diffo_po_params}
\end{table}

