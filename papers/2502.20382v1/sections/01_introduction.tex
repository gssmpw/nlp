% \bpgcomment{I took a crack at the abstract + introduction. What do you think?}
% \bpgcomment{There are some references missing in the introduction.}
\section{Introduction}

The emergence of foundation models has transformed fields such as natural language processing and computer vision, where models trained on massive, internet-scale datasets demonstrate remarkable generalization across diverse reasoning tasks \cite{achiam2023gpt, touvron2023llama, hoffmann2022training, anil2023palm, radford2021learning}. Motivated by this success, the robotics community is currently pursuing foundation models for generalist robot policies capable of flexible and robust decision-making across a wide range of tasks \cite{kim2024openvla, o2023open, team2024octo}, leading to significant industrial investments in large-scale robot learning \cite{reuters_figure_funding_2024}.
However, the pursuit for generalist robot policies remains constrained by the limited availability of high-quality datasets, especially for contact-rich robotic manipulation. Existing datasets \cite{o2023open, khazatsky2024droid, walke2023bridgedata, dasari2019robonet} are orders of magnitude smaller than those used to train foundation models in other domains, such as \acrfull*{llms}. The scarcity of diverse, high-fidelity manipulation data limits policy generalization across different embodiments, task contexts, and physical conditions.

% \russtcomment{I don't think you should dismiss MimicGen so much by only mentioning it quietly here. i think the opening should say that this can be viewed as an important extension to mimicgen to support dynamically feasible contact-rich demonstrations.} 

To address data scarcity, robot learning researchers often rely on a spectrum of data sources varying in cost, quality, and transferability. The most informative data typically consists of high-quality demonstrations specific to the task, environment, and embodiment \cite{o2023open, khazatsky2024droid}, but such data is costly and time-consuming to collect, as it requires human teleoperation with specialized hardware. At the opposite end of the spectrum, there is a wealth of lower-quality data in the form of internet videos showing humans and robots performing manipulation tasks \cite{damen2018scaling, radosavovic2023real, nair2022r3m, karamcheti2023language}. However, the significant embodiment gap and limited action labeling make this data difficult to transfer effectively to robot policies. Simulation data offers a middle ground, providing the potential to generate large, diverse, and high-quality datasets at relatively low cost \cite{xiang2020sapien, brockman2016openai, james2020rlbench}. In practice, effective policy learning can be achieved by co-training on a mixture of data from different points along this spectrum, reducing data collection costs while improving generalization \cite{wang2024scaling}.
%\bpgcomment{Remember to cite Adam's upcoming paper here}


\begin{figure}
\centering
	\includegraphics[width=0.47\textwidth]{figures/banner.pdf}
	\caption{\textbf{Physics-driven data generation overview.} Leveraging trajectory optimization, our framework automatically generates thousands of dynamically feasible contact-rich trajectories across a range of embodiments and physical parameters from only 24 human demonstrations. The policy trained with imitation learning from the generated dataset is more robust and performant.}
	\label{fig:banner}
    \vspace*{-0.2cm}
\end{figure}

% Researchers have explored simulation for automated data generation.
% Starting from a small source of human demonstrations, MimicGen \cite{mandlekar2023mimicgen} and its bimanual extension DexMimicGen \cite{ jiang2024dexmimicgen} decompose each task into object-centric subtasks and replay transformed demonstrations open loop in simulation to generate a large dataset.  

% + Demonstration-guided RL

A key insight in this work is that human demonstrations and model-based planners complement each other in critical ways for generating high-quality robot data. 
Human demonstrations, though costly to collect, offer valuable global information for solving complex tasks.
However, collecting real-world, contact-rich manipulation data through teleoperation is challenging due to the need for precise multi-contact interactions, which are difficult to achieve in practice due to hardware latency, embodiment mismatches between the human and robot, and the fine-grained control required \cite{chi2024universal}. 
% On the other hand, model-based planners, while capable of global search through sampling-based planning \cite{pang2023global, cheng2023enhancing, zhu2023efficient}, often generates high-entropy data that hurt the performance of behavior cloning \cite{belkhale2024data, zhu2024should}. 
In contrast, trajectory optimization has demonstrated success in generating locally-optimal trajectories for contact-rich tasks \cite{mordatch2012discovery, posa2014direct, howell2022calipso}, but often relies on global guidance in the form of good initial guesses.
% the combinatorial complexity of contact modes and 

% For example, CALIPSO , a nonconvex differentiable solver tailored for contact-rich robotics, efficiently handles second-order cone and complementarity constraints, but relies on good initial guesses. 


%While there are many methods for data collection in robotics, they have so far failed to produce datasets of the size required for foundation models.
%On the one hand, collecting extensive real-world contact-rich data through teleoperation is especially costly and challenging. Dexterous manipulation tasks often involve complex multi-contact interactions that require fine-grained control, which can be difficult to achieve due to hardware latency and embodiment mismatches between the human demonstrator and the robot \cite{chi2024universal}.
%Meanwhile, model-based planners have shown great promise in generating high-quality synthetic trajectories, particularly in structured tasks with complex contact dynamics. For example, Pang et al. \cite{pang2023global} use smoothed contact dynamics with global sampling to generate contact-rich plans in under a minute, with performance comparable to reinforcement learning. Howell et al. \cite{howell2022calipso} propose CALIPSO, a nonconvex differentiable solver tailored for contact-rich robotics by efficiently handling second-order cone and complementarity constraints. 
%While these planners can generate high-quality dexterous demonstrations, they often struggle with the high-dimensional search space and challenging nonconvexities introduced by contact mechanics.
%\russtcomment{the planners you've listed don't explicitly think about, and therefore don't struggle with, the ``explosion" of contact modes..?}

%To combine the strengths of both human demonstrations and model-based planning to efficiently generate abundant high-quality data for dexterous contact-rich manipulation, we present a low-cost pipeline that automatically generates physically consistent trajectories in simulation with highly contact-rich behavior as visualized in Fig. \ref{fig:banner}. 
% In this work, we 
% leverage global information from human demonstrations to guide model-based planners in simulation.
% The result is a low-cost pipeline that efficiently generates high-quality, dynamically feasible, and highly contact-rich trajectories in simulation for robotic manipulation tasks.
% The pipeline is visualized in Fig. \ref{fig:banner}. 

%  combines the strengths of physics-based simulation, human demonstrations, and model-based planning to generate high-quality training data that offers several advantages over a real-world setup alone. 
% \russtcomment{this is good, but I think it could be stronger. i would think you should emphasize (perhaps even earlier than here), that a key insight here is that the human demonstrations -- even if they are performed on a different morphology -- provide the "global" reasoning that makes trajopt struggle...  once you have a demonstration, local optimization can be highly effective. And in simulation, trajopt works well thanks to privileged information.}
In this work, we propose a data generation framework that leverages the strengths of both approaches: human demonstrations can provide global guidance, while trajectory optimization can locally refine these demonstrations to ensure dynamic feasibility. 
Starting with a small number of human demonstrations collected in a virtual reality (VR) environment, our method uses model-based trajectory optimization to generate large datasets of dynamically feasible, contact-rich trajectories in simulation. The demonstrations guide the planner through complex search spaces, while the planner ensures physical consistency and robustness across varying physical parameters and robot embodiments.
Our pipeline, visualized in Fig. \ref{fig:banner}, enables efficient cross-embodiment data transfer, where demonstrations collected with one robot configuration can be adapted to another, and supports domain randomization for improved generalization and robustness.
Additionally, it provides the potential to revive and adapt legacy datasets collected with different hardware or configurations, making old datasets valuable for new robot systems. \looseness=-1

Our key contributions include:
\begin{enumerate}
    \item We present an intuitive, embodiment-flexible demonstration interface based on virtual reality and physics simulation, enabling fast data collection for dexterous contact-rich manipulation.  
    \item  We propose a scalable framework that leverages trajectory optimization to transform a small number of human demonstrations into large-scale, physically consistent datasets, enabling generalization across embodiments, initial conditions, and physical parameters.
    \item We validate our approach by training policies on the generated dataset for challenging contact-rich manipulation tasks across multiple robot platforms, including bimanual robot arms and a floating base Allegro hand.
    \item We achieve high success rates in zero-shot hardware deployment on bimanual iiwa arms, highlighting the utility of augmented datasets in real-world scenarios.
    %We zero-shot deploy the learned policy on hardware for bimanual iiwa arms and achieve high success rates with augmented datasets. 
    
\end{enumerate}
 % \pangcomment{This could be the third contribution, showing that we can indeed train good policies from the demonstrations?}

% % \bpgcomment{Mention quickly improving simulators in terms of realism and range of tasks, maybe cite Drakes hydroelastic and/or soft-body?}
% %with quickly improving simulators both in terms of physical realism and range of tasks \bpgcomment{cite drake/hydroelastic?}
% Specifically, our pipeline consists of the following steps:
% we first collect a small number of embodiment-agnostic human task demonstrations in a simulation-driven \acrfull*{vr} environment, which are then refined for different robot embodiments using optimization-based kinematic retargeting.
% Importantly, because we collect embodiment-agnostic demonstrations, our method is able to generate demonstrations for different embodiments, varying in multiple robot arms
% % \bpgcomment{Update this sentence after we know exactly which embodiments we use.}.
% We then leverage \acrfull*{mpc} to adapt the demonstrations for a wide variety of physical parameters for the task at hand.
% The end result is a large collection of physically consistent demonstration trajectories for different embodiments and physical parameters, generated from only a handful of human demonstrations.
% We show the efficacy of our method by training \acrfull*{bc} Diffusion Policies \cite{chi2023diffusion} for several different robots on the generated datasets, achieving high success rates starting from only a few human demonstrations. 
% \bpgcomment{Mention exactly which robots we train for.}


% Old text draft
\begin{comment}
\subsection{intro draft 2}
Collecting high-quality robot training data is an important task in advancing robotics.
The rise of foundation models has transformed fields like natural language processing, where models trained on massive internet-scale datasets achieve remarkable generalization.
This has inspired the robotics community to pursue foundation models for generalist robot policies with similar reasoning capabilities, driving significant industrial investments \cite{reuters_figure_funding_2024}. 
However, this pursuit is constrained by the limited availability of high-quality robot data, especially for contact-rich robot manipulation, as existing datasets are orders of magnitude smaller than those used to train e.g. \acrfull*{llms}. \bpgcomment{Cite some dataset comparison studies?}

In robotics, researchers often refer to the "robot data diet", representing a spectrum of data sources of varying cost and quality.
The most desirable data is high-quality demonstrations for your specific task, environment, and embodiment, but this is also the most expensive to collect.
In contrast, there is an abundance of lower-quality data in the form of internet videos of humans and robots performing tasks, but the challenge of bridging the transfer gap makes this data less usable.
Simulation data occupies a middle ground on this spectrum, 
potentially offering a way to generate large, diverse, and high-quality datasets at low cost.

%Unlike real-world human demonstrations, which are time-intensive to set up and execute, simulation offers a scalable way of quickly generating diverse datasets.
The potential of physics-based simulators to generate high-quality data is big and quickly growing as simulators improve in realism and task diversity.
By leveraging co-training, robot policies can be bootstrapped with just tens of high-quality human demonstrations combined with hundreds or thousands of simulated demonstrations, significantly reducing data collection costs.
\bpgcomment{Cite Adam + Follow up on co-training/finetuning references}
However, automatically generating data in simulation remains a challenge, especially for contact-rich manipulation tasks, as current planners struggle to...
Simulation also offers important advantages over real-world setups alone:
it enables cross-embodiment data transfer, allowing data collected with one robot or configuration to be adapted to another, and supports domain randomization for improved generalization and robustness. 
Additionally, it also provides a way of reviving and adapting legacy datasets collected with outdated hardware or configurations, making old datasets valuable for new robot systems.
\bpgcomment{Make this a bit more about sim + planning (not just sim)?}

In this work, we present a novel low-cost pipeline for generating large amounts of high-quality training data across a variety of embodiments, leveraging the strength of physics-based simulation and planning.
We focus on contact-rich tasks for robotic manipulation.
\bpgcomment{motivate why we focus on contact-rich tasks, or say something more about contact-rich being especially difficult?}
Specifically, the first step in our pipeline is to collect a low number of embodiment-agnostic human task demonstrations in a simulation-driven \acrfull*{vr} environment.
The demonstrations are then refined for different robot embodiments using optimization-based kinematic retargeting, before we leverage local \acrfull*{mpc} to adapt the demonstrations for a wide variety of physical parameters for the task at hand.
The end result is a dataset containing thousands of physically consistent demonstration trajectories for different embodiments and physical parameters, generated from only a handful of human demonstrations.
Finally, we show the efficacy of our method by training \acrfull*{bc} Diffusion Policies \cite{chi2023diffusion} on the generated datasets, achieving high success rates starting from only a few human demonstrations.

\end{comment}

% Old text draft
\begin{comment}
\begin{itemize}
    \item Paragraph 1: Foundation models and the data diet.
    \item Paragraph 2: The promise of simulation data and why the timing is right.
    \item Paragraph 3: The specifics of this work.
\end{itemize}

\bpgcomment{I don't think we need to motivate the pursuit for foundation models as much as I am currently doing.}

\bpgcomment{High-level comment: This introduction is just a first pass, there are a number of things I haven't added that I will add!}

\bpgcomment{Should the first two paragraphs be one paragraph?}

The rise of foundation models has transformed fields like natural language processing, where models trained with \acrfull*{bc} on massive internet-scale datasets achieve remarkable generalization.
This has inspired the robotics community to pursue foundation models for generalist robot policies with similar reasoning capabilities, driving significant industrial investments. 
However, this pursuit is constrained by the limited availability of high-quality robot data, as existing datasets are orders of magnitude smaller than those used to train e.g. \acrfull*{llms}.

\bpgcomment{I don't think we need to motivate the pursuit for foundation models as much as I am currently doing.}

In robotics, researchers often refer to the "robot data diet", representing a spectrum of data sources varying in cost and quality.
The most desirable data is high-quality demonstrations tailored to the specific task, environment, and embodiment at hand, but this data is also the most expensive to collect.
In contrast, there is an abundance of lower-quality data in the form of internet videos of humans and robots performing tasks, but the challenge of bridging the transfer gap makes this data less usable.
Simulation data occupies a middle ground on this spectrum;
with rapid advancements in both simulators and planners, it potentially offers a way to generate diverse, high-quality datasets for varying embodiments at a low cost.

\bpgcomment{Not currently adressing: If you have some data, how to use it cross-embodiment?}

In this work, we present a novel low-cost pipeline for generating large amounts of high-quality training data across a variety of embodiments, leveraging the strength of physics-based simulation and planning.
We focus on contact-rich tasks for robotic manipulation \bpgcomment{motivate why this specifically}.
Specifically, the first step in our pipeline is to collect embodiment-agnostic human task demonstrations in a simulation-driven \acrfull*{vr} environment.
The demonstrations are then refined for different robot embodiments using optimization-based kinematic retargeting, before we leverage \acrfull*{mpc} to generate demonstrations for a wide variety of physical parameters for the task at hand.
Finally, we show the efficacy of the method by training a \acrshort*{bc} Diffusion Policy on the generated data, achieving high success rates from training on a dataset generated from only a handful of human demonstrations.
\bpgcomment{Mention co-training in a sentence?}
\bpgcomment{Address why we even need human demonstrations in the first place (planners are not strong enough).}
\bpgcomment{TODO: Mention reviving old data in sim, due to hardware updates, sim updates, etc etc}
\bpgcomment{TODO: Mention how to use data collected with a different embodiment on a new embodiment.}

\subsubsection{Lu's old notes}
revive old data which are not accurate enough
Moreover, when the hardware is updated (e.g., the gripper geometry is slightly modified), previous dataset collected with the old hardware 
data collection from sim
cross-embodiment transfer
\end{comment}

