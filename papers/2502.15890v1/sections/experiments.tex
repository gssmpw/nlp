\section{Experiments}
\label{sec:exp}

To verify the accuracy and efficiency of our framework, we run extensive experiments on a variety of settings, including different graph configurations, graph sizes, sampling methods, and sample sizes. We additionally evaluate the usefulness of our framework on the downstream task of comparing mean distances. We focus our experiments on synthetic graphs as our framework works using the specification of the degree distribution. We thus focus on accurately estimating the DSPD assuming a correct model; we view the task of deriving an accurate model of a real-world graph as a separate task. Our experiments are implemented in NetworkX~\cite{SciPyProceedings_11} and NumPy~\cite{harris2020array}.

\subsection{Graph Configurations}

We focus our experiments on three types of graphs: binomial graphs, power-law graphs~\cite{barabasi1999emergence}, and SBM graphs. The first two graphs do not model community structure, while SBM graphs do.

\subsubsection{Binomial Graphs}

Binomial graphs are graphs with $N$ nodes and probability parameter $p$, where every possible edge is in the graph with probability $p$, independent of every other edge. The degree distribution is given by $p(k) = \binom{N-1}{k}p^k(1-p)^{N-1-k}$, the $\text{Binomial}(N-1,p)$ distribution. Binomial graphs are simple and intuitive to implement, making them attractive to analyze, but are not as flexible as more complex models.

We run experiments on settings of $(N,p) = (20000, 0.0005)$, $(40000, 0.00025)$, and $(100000, 0.0001)$, wherein the graphs get larger but the expected degree of each node remains constant.

\subsubsection{Power-Law Graphs}

The power-law model is a way to generate scale-free graphs, where the degree distribution follows a power-law distribution. For a graph with $N$ nodes and power-law parameter $\gamma$, the degree distribution is $p(k) = ak^{-\gamma}$ where $a$ is a normalizing constant. Minimums and maximums can be enforced on the support of the distribution. Many social networks are scale-free, making power-law graphs a potentially good way to replicate real-world graphs.

\citet{barabasi1999emergence} suggest that values of $\gamma$ in large real-world graphs commonly fall between $2$ and $3$; we thus consider two configurations: Power Law A, with $\gamma = 2$ and support $6 \leq k \leq 29$; and Power Law B, with $\gamma = 3$ and support $6 \leq k \leq 19$. The supports were chosen empirically to allow for opposite preferences for sampling method when considering the downstream task, allowing us to better evaluate our framework's performance on downstream tasks. For each configuration of power law graph, we run experiments with $N = 20000, 40000,$ and $100000$.

\subsubsection{Stochastic Block Model}

The SBM generates graphs resembling real-world social networks with community structures. The graph is split into blocks with each edge within a block appearing with probability $p_1$ and each edge across two blocks appearing with probability $p_2$. In practice, blocks can be of different sizes, but for simplicity we assume each block is of the same size; this allows the degree distributions of each node to be the same, which is necessary for our framework. Then, for a graph with $B$ blocks and $N_B$ nodes per block, the within-block degree distribution is $p_w(k)=\binom{N_B-1}{k}p_1^{k}(1-p_1)^{N_b-1-k}$ and the across-block degree distribution is $p_a(k) = \binom{(B-1)N_B}{k}p_2^k (1-p_2)^{(B-1)N_B-k}$.

To inform our design decision for the SBM graphs, we examine the Facebook Large Page-Page Network graph~\cite{rozemberczki2019multiscale} from the Stanford Network Analysis Project (SNAP)~\cite{snapnets}, a graph with 22,470 nodes representing official pages on Facebook, with 171,002 undirected edges representing mutual likes between pages. We run Clauset-Newman-Moore greedy modularity maximization~\cite{clauset2004finding} to partition the graph into $130$ communities, each with an average block size of about $172$. We next determine the within-block and across-block density to match the number of edges after enforcing equal block sizes.

We obtain graphs of different sizes by scaling the number of blocks,  generating SBM graphs with $p_1=0.08323$, $p_2=0.00004718$, $N_B = 172$, and $B = 130, 260,$ and $650$. These settings correspond approximately to graphs of size $N = 22360, 44720,$ and $111800$, approximately matching the size settings of other graph configurations. When presenting results on these graphs, we will refer to these as graph size $20000, 40000,$ and $100000$ for consistency between graph configurations.

\subsection{Sampling Methods}
\label{sec:sampling_methods}

We experiment using two common sampling methods: random sampling and snowball sampling. Three sample sizes are used: $s=200, 400$ and $1000$. These sample sizes roughly correspond to $1\%$ of the nodes for each size of graph. Applications and evaluations on other sampling methods are left as future work.

\subsubsection{Random Sampling}

Random sampling is a straightforward way to sample from a graph. To generate a sample of size $s$, we select $s$ nodes from the graph uniformly at random, without replacement.

The sample supernode degree distribution $p_S$ can then be determined easily.
Let $K_i$ be the random variable representing the degree of the $i$th node in the sample with pmf $p(k)$.
For a sample of size $s$, we are then interested in the distribution of $K_S = \sum_{i=1}^s K_i$, or the sum of $s$ independent draws of $K_i$. This distribution can be efficiently calculated using polynomial exponentiation: let the polynomial $q(x) = \sum_{k=0}^\infty p(k) x^k$. 
In practice, as $K_i$ has finite support, $q(x)$ is a finite polynomial. Then, $q_S(x) = q(x)^s$, and $p_S(k)$ is simply the coefficient on $x^k$ in $q_S(x)$.
In the case of SBM graphs, the calculations are similar, except that the process is performed twice: once to determine $p_{Sw}(k)$ using $p_w(k)$ and once to determine $p_{Sa}(k)$ using $p_a(k)$.

A key assumption in these derivations is that there are negligible edges between two nodes in the sample, i.e. that every edge out of a node in the sample remains an edge counted in the degree of the contracted supernode. In cases where the sample size is small and the graph is large, this will generally be true.

\subsubsection{Snowball Sampling}

Snowball sampling~\cite{goodman1961snowball} is a form of sampling commonly used in sociology when sampling on hard-to-reach communities~\cite{hu2013survey}, where random sampling is not feasible.
In snowball sampling, we begin with a seed sample, then produce a frontier of all nodes adjacent to the seed. At each step, a node in the frontier is removed from the frontier and added to the sample with a certain probability. If the node is added, it is removed from the frontier and the frontier is expanded with that node's neighbors. This simulates the process of using referrals to find new subjects, which may be necessary if the population being sampled is stigmatized and standard surveys are ineffective. In our experiments, we begin with a single random seed node, nodes are added to the sample with probability $0.5$, and if the frontier is exhausted without having reached the sample size $s$, the process is continued from a new random seed node.

The process of deriving the sample supernode degree distribution $p_S$ becomes more complicated.
Under snowball sampling, all nodes except the seed nodes are reached via exploring an edge. Thus, the probability that a node is in the snowball sample is proportional to its degree. Moreover, each node in the sample contributes $2$ fewer degrees to the supernode than its actual degree as the edge used to reach it is fully within the supernode. We can approximate the degree distribution of the supernode as follows: let the degree distribution be $p(k)$ and let $p'(k) = \frac{k}{c}p(k)$ where $c$ is a normalizing constant such that $p'(k)$ is the degree distribution of a node reached via exploring an edge. For a sample of size $s$, we may now proceed as in the random sampling case: let the polynomial $q(x) = \sum_{k=0}^\infty p'(k)x^k$. Then $q_S(x) = q(x)^s$, and $p_S(k)$ is approximately the coefficient on $x^{k+2s}$. The correction of $2s$ accounts for degree contribution of $2$ less from each node in the sample. Note that this is an approximation as it assumes every node is reached via snowball sampling, while some nodes are indeed seed nodes. However, when the sample size is large and the retention probability is high relative to the degree distribution, this approximation is accurate.


\paragraph{SBM Graphs} For SBM graphs, the process is more complicated still as we must track whether the sample node is reached via a within-block or across-block edge. Let the within-block degree distribution be $p_w(k)$, the across block degree distribution be $p_a(k)$, $p'_w(k) = \frac{k}{c_w}p_w(k)$ be the within-block degree distribution of a node reached via a within-block edge, and $p'_a(k) = \frac{k}{c_a}p_a(k)$ be the across-block degree distribution of a node reached via an across-block edge, with $c_w$ and $c_a$ being normalizing constants.

We first determine the probability a node is reached via a within-block edge. Let the event a node is reached via a within-block edge be $W$, and let the event the node's parent is reached via a within-block edge and across-block edge be $W_P$ and $A_P$, respectively. Then:
\begin{align*}
    P(W) = P(W | W_P) P(W_P) + P(W | A_P) P(A_P),
\end{align*}
with the observation that $P(W_P) = P(W)$ and $P(A_P) = 1-P(W)$. To determine the conditional probabilities, we have
\begin{align*}
    P(W | W_P) &= 
    \sum_{0 < k_w < N-1} \sum_{0 < k_a < N-1} \frac{k_wp_w(k_w)}{c_w} p_a(k_a) \frac{k_w - 1}{k_w + k_a - 1}, \\
    P(W | A_P) &= 
    \sum_{0 < k_w < N-1} \sum_{0 < k_a < N-1} p_w(k_w) \frac{k_ap_a(k_a)}{c_a} \frac{k_w}{k_w + k_a - 1},
\end{align*}
from the law of total probability: for each possible within-block and across-block degree of the parent node, we find the probability that the parent had that probability (product of the first two terms, with the degree distribution of the reaching type of edge scaled by degree and normalized) and the probability that a within-block edge is traversed (last term). With this we may solve for $P(W)$.

We now build degree distributions for the ``average'' node in the sample. For example, the within-block degree distribution is a mixture of $p_w(k)$ with weight $1-P(W)$ and $p_w'(k)$ with weight $P(W)$: with probability $P(W)$ the node is reached via a within-block edge and so it's within-block degree comes from the weighted distribution, and with probability $1-P(W)$ the node is reached via an across-block edge and it's within-block degree comes from the unweighted distribution. We again express this average distribution using polynomials. Let
\begin{align*}
    q_w(x) &= \sum_{k=0}^\infty p_w(k)x^k, q_a(x) = \sum_{k=0}^\infty p_a(k)x^k, \\
    q'_w(x) &= \sum_{k=0}^\infty p'_w(k)x^k, q'_a(x) = \sum_{k=0}^\infty p'_a(k)x^k.
\end{align*}
Then 
\begin{align*}
    q^*_w(x) &= (1-P(W)) q_w(x) + P(W) q'_w(x)/x^2 \\
    q^*_a(x) &= P(W) q_a(x) + (1-P(W)) q'_a(x)/x^2
\end{align*}
are the polynomials representing the average distribution within the sample. Here we apply the correction with the $/x^2$ term; we reduce the degree contribution to the corresponding edge type by $2$ when the edge type is fully inside the sample. We may now conclude as in the random sampling case: $q^*_{Sw}(x) = q^*_w(x)^s$ with $p_{Sw}(k)$ approximately the coefficient on $x^k$, and similar for $p_{Wa}(k)$ and $q^*_{Sa}(x) = q^*_{Sa}(x)^s$.

\subsection{Experimental Design}

We run extensive experiments on these settings. For each graph setting, we generate $5$ graphs. On these graphs, for each sampling method and sample size, we generate $20$ samples and calculate the resulting DSPD. For each graph setting, sampling method, and sample size, we collect timing statistics on the $100$ runs and average the derived DSPDs to obtain our empirical distribution. We also run our framework $100$ times and collect timing information; however each run of our framework yields the same estimated DSPD.

\subsubsection{Downstream Task}

Our downstream task investigates which sampling method gives a smaller mean shortest-path distance to the sample for a given graph setting and sample size. This metric is of interest because a smaller mean distance can indicate better average performance for graph neural networks~\cite{ma2021subgroup} and the first moment of a distribution is an important measure in general.

Recall that the Power Law A and Power Law B configurations were chosen to provide different preferences on this downstream task. In the Power Law A configurations snowball sampling consistently gives smaller mean distances than random sampling, and vice versa in the Power Law B configurations.
