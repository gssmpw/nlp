\section{Related Work}
\label{sec:related}
%\my{Beef up this section...could cover some grounds about AI explanations, and previous studies that highlight explanations need to be provided selectively/progressively, etc.}

\subsection{AI-Assisted Decision Making}
The increasing prevalence of AI-assisted decision making has led to a growing line of research to investigate how people engage with, trust in, and rely on AI models in this new collaboration paradigm ~\cite{lai2021towards,chiang2023two,10.1145/3653708}. Early studies focus on empirically identifying factors that influence AI-assisted decision making, including the AI model's performance~\cite{rechkemmer2022confidence}, the explanation of the model recommendation~\cite{schemmer2023appropriate,schoeffer2024explanations,robbemond2022understanding},  the decision making workflow~\cite{chiang2024enhancing,rastogi2022deciding}, and the influence of task complexity on human-AI interactions~\cite{salimzadeh2023missing}.

While it is expected that the complementarity between AI models and humans could enable the human-AI team to outperform either party alone, in practice, the collaboration between humans and AI in decision making is widely observed to be suboptimal ~\cite{schemmer2022should}. It is observed that people usually exhibit inappropriate reliance on AI models~\cite{steyvers2023three}. For instance, the design of conversational interfaces can influence users' trust, sometimes causing overreliance on AI recommendations~\cite{gupta2022trust}. In addition, people may also blindly rely on AI in time-pressured environments, where the presence of AI suggestions may speed up decision making at the cost of accuracy~\cite{swaroop2024accuracy}. In contrast, people could also reject the AI model recommendation even when it is correct, noted as underreliance on AI~\cite{mahmud2022influences,ochmann2020influence,10.1145/3648188.3675130}. Recent research has also discussed how misaligned AI outputs can contribute to people's underreliance on AI systems despite their accuracy~\cite{guerdan2022under}. To help decision makers interact with and rely on the AI model more appropriately, a wide range of approaches was recently developed ~\cite{he2022walking,10.1145/3610219,10.1145/3555572,li2023modeling,lu2023strategic,li2024decoding,lu2024mix}. For instance, the cognitive forcing function encourages people to engage with AI more cognitively, thus potentially reducing people's overreliance on the AI model~\cite{buccinca2021trust,erlei2020impact,lai2021towards,salimzadeh2024dealing}. ~\citeauthor{ma2023should}~\cite{ma2023should} explored the calibration of user trust in AI-assisted decision making by inferring the correctness likelihood of both human and AI on a decision case, which informs the adaptive presentations of the AI model's decision recommendations. 
% \my{Discuss Shuai Ma's CHI 2023 paper here.}

In addition, providing AI explanations generated by various post-hoc explainable AI (XAI) methods~\cite{10.1145/2939672.2939778,10.5555/3295222.3295230} that reveal the decision rationale of AI models is another popular approach used, %with the 
aiming to improve humans' understanding of the AI model's behavior and enable humans to calibrate their trust in AI. 
%, which in turn allows users to calibrate their trust in the AI model accordingly. 
However, many empirical studies have observed that people often struggle to process and comprehend these explanations~\cite{vasconcelos2023explanations,lee2023understanding,wang2021explanations,li2024utilizing}, letting alone utilize the insights revealed from these explanations to trust AI more appropriately. %which fails to meet designers' expectations of positively influencing human engagement with AI models. 
To realize the positive utility of explanations in AI-assisted decision making, recent research highlights the need to provide explanations selectively or progressively to aid human comprehension~\cite{feng-boyd-graber-2022-learning,10.1145/3610206,springer2020progressive,springer2019progressive,li2024utilizing} . For instance, ~\citeauthor{10.1145/3610206}~\cite{10.1145/3610206} demonstrated that selectively highlighting AI explanations, which align with the userâ€™s own decision rationale, can increase agreement between human decisions and AI model predictions and reduce human overreliance on AI recommendations. ~\citeauthor{springer2019progressive}~\cite{springer2019progressive} showed that users may benefit from
initially simplified feedback that hides potential AI system errors and assists users in building working heuristics about how the AI system operates progressively. In this work, we make an initial attempt to explore that \textit{in the absence of AI explanations}, whether the incorporation of the natural-language-based, LLM-powered analysis of the AI recommendations on decision making tasks %as a  explanation 
can promote more appropriate reliance behavior of humans on AI models in decision making, and how to present such analysis in the most effective way.


% explanations are a popularly employed method to improve joint decision making via revealing the AI model decision rationales to humans~\cite{vasconcelos2023explanations,lee2023understanding}\hx{, and research has shown that task complexity plays a key role in shaping trust and reliance on AI systems, requiring tailored interventions to manage uncertainty and reliance behavior~\cite{salimzadeh2024dealing}}. Moreover, a series of interventions~\cite{erlei2020impact,lai2021towards} are designed to influence humans in AI-assisted decision making\cite{lai2021towards}.  
%Well-designed training process, as another approach, also hold its promise to guide human decision makers better utilize the AI model~\cite{erlei2020impact}. Despite the great insight provided, the effectiveness of the approaches are either mixed (e.g., in price of under-reliance) or could only be applied to a specific population (e.g., AI literacy). Thus, there has been an universal approach for appropriate and effective AI-assisted decision making.

\subsection{Human-LLM Interaction}

% \my{You should discuss the TalkToModel paper somewhere in this section.}
Recently, large language models (LLMs) have demonstrated their exceptional capabilities across various applications to assist humans, including creative writing~\cite{wang2024weaver,yuan2022wordcraft,li2024value}, software engineering~\cite{nam2024using,ni2023lever}, and generative design ~\cite{huang2024graphimind}, which has sparked significant interest within the HCI community to investigate the interaction between humans and LLMs ~\cite{gao2024taxonomy,kim2024understanding,10.1145/3613904.3642002,chiang2024enhancing,li-etal-2024-disclosure}. On the one hand, LLMs are increasingly utilized to directly create content or solve problems, which is shown to match or even surpass humans' performance. For example, \citeauthor{10.1145/3544548.3581225}~\shortcite{10.1145/3544548.3581225} presented the framework leveraging LLMs to create coherent scripts and screenplays with humans in the loop. 
% \my{What do you mean by ``with humans audition''?}. 
In other cases, LLM-based services provide foundational support for human creation, such as generating coding schemes for qualitative analysis~\cite{chew2023llm}. 
In these human-LLM collaboration scenarios, a key challenge is that laypeople often lack the skill to effectively prompt LLMs to generate the outputs that they desire ~\cite{zamfirescu2023johnny}. To address this challenge, novel approaches like AI Chains~\cite{wu2022ai}, automatic prompting methods ~\cite{shin2020autoprompt}, and interactive interfaces~\cite{wang2024lave,liu2024make} are developed to enhance the effectiveness of human-LLM interaction, either by improving LLMs' usability ~\cite{hong2024next,yang2024human} or by guiding humans' engagement with LLMs.

%\my{To update.}
%However, effective human-LLM interaction requires careful design to optimize usability and outcomes. Without this, such collaborations can lead to suboptimal results. 
%Exemplified by the overreliance on LLM led 
Researchers have also explored the potential of LLMs in AI-assisted decision making. For example, LLMs could directly provide decision recommendations. However, it was found that the 
overconfident and seemingly convincing LLM outputs can mislead people to believe them to be correct~\cite{steyvers2025large} and result in people's overreliance on LLM~\cite{do2024facilitating,kim2024m}. 
%, as the convincing yet incorrect outputs can potentially mislead users who may rely on them as if they were correct~\cite{kim2024m}. 
%\my{I'm not sure what does this sentence mean.}. 
%\my{This sentence is not grammatically correct.}. 
Recently, \citeauthor{slack2023explaining}~\cite{slack2023explaining} developed an interactive dialogue system that allows users to inquire about the reasons behind the AI model's predictions. This system leverages a LLM to parse user intent and match it with pre-specified, handcrafted answers, demonstrating significant potential to enhance user understanding and decision performance through conversational interactions with the AI model.
% \my{So in their paper, only human understanding is improved, not human decision performance? How is our work different from theirs? (Or what makes our setting different so that in the first experiment, we find providing LLM-powered analysis is not that helpful?)} \zy{they also see the improvements over the decision accuracy... In their work, the users have more freedom to chat with the system like why this prediction happens, how one feature affects the prediction, and what the typical errors the AI model would have; in addition, they used LLM mainly to identify user intent, they do not have LLM directly analyze the task and provide analysis. Their language-format explanations are directly parsing from the post0-hoc explanation. }. \my{Then, try to include these points in the text (in a succinct manner.} 
Different from the previous work, %which mostly relies on the heuristic design of human-LLM interaction flow, 
in this paper, we explore 
how to utilize LLMs to analyze an AI model's decision recommendations and augment them, and 
how to build an algorithmic framework to dynamically decide what information to present to humans from the rich information generated by LLMs.
%, thereby yielding enhanced human-AI team performance in decision making.