% Related Work

@article{lai2021towards,
  title={Towards a science of human-ai decision making: a survey of empirical studies},
  author={Lai, Vivian and Chen, Chacha and Liao, Q Vera and Smith-Renner, Alison and Tan, Chenhao},
  journal={arXiv preprint arXiv:2112.11471},
  year={2021}
}

@inproceedings{bansal2021does,
  title={Does the whole exceed its parts? the effect of ai explanations on complementary team performance},
  author={Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
  booktitle={Proceedings of the 2021 CHI conference on human factors in computing systems},
  pages={1--16},
  year={2021}
}

@article{krishna2022disagreement,
  title={The disagreement problem in explainable machine learning: A practitioner's perspective},
  author={Krishna, Satyapriya and Han, Tessa and Gu, Alex and Wu, Steven and Jabbari, Shahin and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2202.01602},
  year={2022}
}

@inproceedings{slack2020fooling,
  title={Fooling lime and shap: Adversarial attacks on post hoc explanation methods},
  author={Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={180--186},
  year={2020}
}

@article{lai2019many,
  title={Many faces of feature importance: Comparing built-in and post-hoc feature importance in text classification},
  author={Lai, Vivian and Cai, Jon Z and Tan, Chenhao},
  journal={arXiv preprint arXiv:1910.08534},
  year={2019}
}

@article{miro2024assessing,
  title={Assessing fidelity in xai post-hoc techniques: A comparative study with ground truth explanations datasets},
  author={Mir{\'o}-Nicolau, Miquel and Jaume-i-Cap{\'o}, Antoni and Moy{\`a}-Alcover, Gabriel},
  journal={Artificial Intelligence},
  volume={335},
  pages={104179},
  year={2024},
  publisher={Elsevier}
}

@article{chen2022machine,
  title={Machine explanations and human understanding},
  author={Chen, Chacha and Feng, Shi and Sharma, Amit and Tan, Chenhao},
  journal={arXiv preprint arXiv:2202.04092},
  year={2022}
}

@inproceedings{kim2018interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={International conference on machine learning},
  pages={2668--2677},
  year={2018},
  organization={PMLR}
}

@inproceedings{springer2019progressive,
  title={Progressive disclosure: empirically motivated approaches to designing effective transparency},
  author={Springer, Aaron and Whittaker, Steve},
  booktitle={Proceedings of the 24th international conference on intelligent user interfaces},
  pages={107--120},
  year={2019}
}

@inproceedings{chiang2023two,
  title={Are two heads better than one in ai-assisted decision making? comparing the behavior and performance of groups and individuals in human-ai collaborative recidivism risk assessment},
  author={Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2023}
}
@article{10.1145/3653708,
author = {Lu, Zhuoran and Wang, Dakuo and Yin, Ming},
title = {Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3653708},
doi = {10.1145/3653708},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {217},
numpages = {31},
keywords = {appropriate reliance, human-ai interaction, machine learning, second opinions}
}


@inproceedings{li-etal-2023-synthetic,
    title = "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
    author = "Li, Zhuoyan  and
      Zhu, Hangxiao  and
      Lu, Zhuoran  and
      Yin, Ming",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.647/",
    doi = "10.18653/v1/2023.emnlp-main.647",
    pages = "10443--10461",
}

@inproceedings{
li2024utilizing,
title={Utilizing Human Behavior Modeling to Manipulate Explanations in {AI}-Assisted Decision Making: The Good, the Bad, and the Scary},
author={Zhuoyan Li and Ming Yin},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=7XkwzaPMvX}
}

@inproceedings{rechkemmer2022confidence,
  title={When confidence meets accuracy: Exploring the effects of multiple performance indicators on trust in machine learning models},
  author={Rechkemmer, Amy and Yin, Ming},
  booktitle={Proceedings of the 2022 chi conference on human factors in computing systems},
  pages={1--14},
  year={2022}
}

@inproceedings{mahmood2024designing,
  title={Designing behavior-aware AI to improve the human-AI team performance in AI-assisted decision making},
  author={Mahmood, Syed Hasan Amin and Lu, Zhuoran and Yin, Ming},
  booktitle={Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
  pages={3106--3114},
  year={2024}
}

@article{steyvers2025large,
  title={What large language models know and what people think they know},
  author={Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas W and Smyth, Padhraic},
  journal={Nature Machine Intelligence},
  pages={1--11},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{lu2024mix,
  title={Mix and Match: Characterizing Heterogeneous Human Behavior in AI-assisted Decision Making},
  author={Lu, Zhuoran and Mahmoo, Syed Hasan Amin and Li, Zhuoyan and Yin, Ming},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={12},
  pages={95--104},
  year={2024}
}

@inproceedings{li2024decoding,
  title={Decoding AI’s Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making},
  author={Li, Zhuoyan and Lu, Zhuoran and Yin, Ming},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={9},
  pages={10083--10091},
  year={2024}
}

@inproceedings{lu2023strategic,
  title={Strategic Adversarial Attacks in AI-assisted Decision Making to Reduce Human Trust and Reliance.},
  author={Lu, Zhuoran and Li, Zhuoyan and Chiang, Chun-Wei and Yin, Ming},
  booktitle={IJCAI},
  pages={3020--3028},
  year={2023}
}

@inproceedings{li2023modeling,
  title={Modeling human trust and reliance in ai-assisted decision making: A markovian approach},
  author={Li, Zhuoyan and Lu, Zhuoran and Yin, Ming},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={5},
  pages={6056--6064},
  year={2023}
}

@inproceedings{li-etal-2024-disclosure,
    title = "How Does the Disclosure of {AI} Assistance Affect the Perceptions of Writing?",
    author = "Li, Zhuoyan  and
      Liang, Chen  and
      Peng, Jing  and
      Yin, Ming",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.279/",
    doi = "10.18653/v1/2024.emnlp-main.279",
    pages = "4849--4868",
}

@inproceedings{li2024value,
  title={The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing},
  author={Li, Zhuoyan and Liang, Chen and Peng, Jing and Yin, Ming},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--25},
  year={2024}
}

@inproceedings{chiang2024enhancing,
  title={Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate},
  author={Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  booktitle={Proceedings of the 29th International Conference on Intelligent User Interfaces},
  pages={103--119},
  year={2024}
}

@article{yaniv2004benefit,
  title={The benefit of additional opinions},
  author={Yaniv, Ilan},
  journal={Current directions in psychological science},
  volume={13},
  number={2},
  pages={75--78},
  year={2004},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{yaniv2000advice,
  title={Advice taking in decision making: Egocentric discounting and reputation formation},
  author={Yaniv, Ilan and Kleinberger, Eli},
  journal={Organizational behavior and human decision processes},
  volume={83},
  number={2},
  pages={260--281},
  year={2000},
  publisher={Elsevier}
}

@article{achanta2012slic,
  title={SLIC superpixels compared to state-of-the-art superpixel methods},
  author={Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={11},
  pages={2274--2282},
  year={2012},
  publisher={IEEE}
}

@article{springer2020progressive,
  title={Progressive disclosure: When, why, and how do users want algorithmic transparency information?},
  author={Springer, Aaron and Whittaker, Steve},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={10},
  number={4},
  pages={1--32},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{yu-etal-2020-interactive,
    title = "Interactive Classification by Asking Informative Questions",
    author = "Yu, Lili  and
      Chen, Howard  and
      Wang, Sida I.  and
      Lei, Tao  and
      Artzi, Yoav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.237",
    doi = "10.18653/v1/2020.acl-main.237",
    pages = "2664--2680",
    abstract = "We study the potential for interaction in natural language classification. We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions. At each turn, our system decides between asking the most informative question or making the final classification pre-diction. The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks. We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.",
}

@inproceedings{10.1145/3613904.3642883,
author = {Kim, Dajung and Vegt, Niko and Visch, Valentijn and Bos-De Vos, Marina},
title = {How Much Decision Power Should (A)I Have?: Investigating Patients’ Preferences Towards AI Autonomy in Healthcare Decision Making},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642883},
doi = {10.1145/3613904.3642883},
abstract = {Despite the growing potential of artificial intelligence (AI) in improving clinical decision making, patients' perspectives on the use of AI for their care decision making are underexplored. In this paper, we investigate patients’ preferences towards the autonomy of AI in assisting healthcare decision making. We conducted interviews and an online survey using an interactive narrative and speculative AI prototypes to elicit participants’ preferred choices of using AI in a pregnancy care context. The analysis of the interviews and in-story responses reveals that patients’ preferences for AI autonomy vary per person and context, and may change over time. This finding suggests the need for involving patients in defining and reassessing the appropriate level of AI assistance for healthcare decision making. Departing from these varied preferences for AI autonomy, we discuss implications for incorporating patient-centeredness in designing AI-powered healthcare decision making.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {439},
numpages = {17},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@article{10.1145/3579460,
author = {Banovic, Nikola and Yang, Zhuoran and Ramesh, Aditya and Liu, Alice},
title = {Being Trustworthy is Not Enough: How Untrustworthy Artificial Intelligence (AI) Can Deceive the End-Users and Gain Their Trust},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579460},
doi = {10.1145/3579460},
abstract = {Trustworthy Artificial Intelligence (AI) is characterized, among other things, by: 1) competence, 2) transparency, and 3) fairness. However, end-users may fail to recognize incompetent AI, allowing untrustworthy AI to exaggerate its competence under the guise of transparency to gain unfair advantage over other trustworthy AI. Here, we conducted an experiment with 120 participants to test if untrustworthy AI can deceive end-users to gain their trust. Participants interacted with two AI-based chess engines, trustworthy (competent, fair) and untrustworthy (incompetent, unfair), that coached participants by suggesting chess moves in three games against another engine opponent. We varied coaches' transparency about their competence (with the untrustworthy one always exaggerating its competence). We quantified and objectively measured participants' trust based on how often participants relied on coaches' move recommendations. Participants showed inability to assess AI competence by misplacing their trust with the untrustworthy AI, confirming its ability to deceive. Our work calls for design of interactions to help end-users assess AI trustworthiness.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {27},
numpages = {17},
keywords = {XAI, explainability, explainable AI, fairness, transparency, trustworthiness, trustworthy AI}
}

@article{10.1145/3449287,
author = {Bu\c{c}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z.},
title = {To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449287},
doi = {10.1145/3449287},
abstract = {People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {188},
numpages = {21},
keywords = {artificial intelligence, cognition, explanations, trust}
}

@INPROCEEDINGS{6751146,
  author={Kovashka, Adriana and Grauman, Kristen},
  booktitle={2013 IEEE International Conference on Computer Vision}, 
  title={Attribute Pivots for Guiding Relevance Feedback in Image Search}, 
  year={2013},
  volume={},
  number={},
  pages={297-304},
  keywords={Databases;Visualization;Binary search trees;Entropy;Uncertainty;Training;History;relative attributes;image retrieval;relevance feedback;active selection},
  doi={10.1109/ICCV.2013.44}}

@inproceedings{10.5555/3327144.3327183,
author = {Lee, Sang-Woo and Heo, Yu-Jung and Zhang, Byoung-Tak},
title = {Answerer in questioner's mind: information theoretic approach to goal-oriented visual dialog},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence. Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. To ask the adequate question, deep learning and reinforcement learning have been recently applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose "Answerer in Questioner's Mind" (AQM), a novel information theoretic algorithm for goal-oriented dialog. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer's intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialog tasks: "MNIST Counting Dialog" and "GuessWhat?!". In our experiments, AQM outperforms comparative algorithms by a large margin.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {2584–2594},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{BARTLETT202230,
title = {Consumer-lending discrimination in the FinTech Era},
journal = {Journal of Financial Economics},
volume = {143},
number = {1},
pages = {30-56},
year = {2022},
issn = {0304-405X},
doi = {https://doi.org/10.1016/j.jfineco.2021.05.047},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X21002403},
author = {Robert Bartlett and Adair Morse and Richard Stanton and Nancy Wallace},
keywords = {Discrimination, FinTech, GSE mortgages, Credit scoring, Algorithmic underwriting, Big-data lending, Platform loans, Statistical discrimination, Legitimate business necessity},
abstract = {U.S. fair-lending law prohibits lenders from making credit determinations that disparately affect minority borrowers if those determinations are based on characteristics unrelated to creditworthiness. Using an identification under this rule, we show risk-equivalent Latinx/Black borrowers pay significantly higher interest rates on GSE-securitized and FHA-insured loans, particularly in high-minority-share neighborhoods. We estimate these rate differences cost minority borrowers over $450 million yearly. FinTech lenders’ rate disparities were similar to those of non-Fintech lenders for GSE mortgages, but lower for FHA mortgages issued in 2009–2015 and for FHA refi mortgages issued in 2018–2019.}
}

@inproceedings{10.1145/3442381.3450077,
author = {Imana, Basileal and Korolova, Aleksandra and Heidemann, John},
title = {Auditing for Discrimination in Algorithms Delivering Job Ads},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450077},
doi = {10.1145/3442381.3450077},
abstract = {Ad platforms such as Facebook, Google and LinkedIn promise value for advertisers through their targeted advertising. However, multiple studies have shown that ad delivery on such platforms can be skewed by gender or race due to hidden algorithmic optimization by the platforms, even when not requested by the advertisers. Building on prior work measuring skew in ad delivery, we develop a new methodology for black-box auditing of algorithms for discrimination in the delivery of job advertisements. Our first contribution is to identify the distinction between skew in ad delivery due to protected categories such as gender or race, from skew due to differences in qualification among people in the targeted audience. This distinction is important in U.S.&nbsp;law, where ads may be targeted based on qualifications, but not on protected categories. Second, we develop an auditing methodology that distinguishes between skew explainable by differences in qualifications from other factors, such as the ad platform’s optimization for engagement or training its algorithms on biased data. Our method controls for job qualification by comparing ad delivery of two concurrent ads for similar jobs, but for a pair of companies with different de facto gender distributions of employees. We describe the careful statistical tests that establish evidence of non-qualification skew in the results. Third, we apply our proposed methodology to two prominent targeted advertising platforms for job ads: Facebook and LinkedIn. We confirm skew by gender in ad delivery on Facebook, and show that it cannot be justified by differences in qualifications. We fail to find skew in ad delivery on LinkedIn. Finally, we suggest improvements to ad platform practices that could make external auditing of their algorithms in the public interest more feasible and accurate.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3767–3778},
numpages = {12},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1016/j.inffus.2019.12.012,
author = {Barredo Arrieta, Alejandro and D\'{\i}az-Rodr\'{\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2019.12.012},
doi = {10.1016/j.inffus.2019.12.012},
journal = {Inf. Fusion},
month = {jun},
pages = {82–115},
numpages = {34},
keywords = {Responsible Artificial Intelligence, Accountability, Fairness, Privacy, Transparency, Comprehensibility, Interpretability, Data Fusion, Deep Learning, Machine Learning, Explainable Artificial Intelligence}
}

@misc{das2020opportunities,
      title={Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey}, 
      author={Arun Das and Paul Rad},
      year={2020},
      eprint={2006.11371},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{10.1145/3301275.3302310,
author = {Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Dugan, Casey},
title = {Explaining models: an empirical study of how explanations impact fairness judgment},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302310},
doi = {10.1145/3301275.3302310},
abstract = {Ensuring fairness of machine learning systems is a human-in-the-loop process. It relies on developers, users, and the general public to identify fairness problems and make improvements. To facilitate the process we need effective, unbiased, and user-friendly explanations that people can confidently rely on. Towards that end, we conducted an empirical study with four types of programmatically generated explanations to understand how they impact people's fairness judgments of ML systems. With an experiment involving more than 160 Mechanical Turk workers, we show that: 1) Certain explanations are considered inherently less fair, while others can enhance people's confidence in the fairness of the algorithm; 2) Different fairness problems-such as model-wide fairness issues versus case-specific fairness discrepancies-may be more effectively exposed through different styles of explanation; 3) Individual differences, including prior positions and judgment criteria of algorithmic fairness, impact how people react to different styles of explanation. We conclude with a discussion on providing personalized and adaptive explanations to support fairness judgments of ML systems.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {275–285},
numpages = {11},
keywords = {empirical studies, explanation, fairness, machine learning},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.1145/3178876.3186138,
author = {Grgic-Hlaca, Nina and Redmiles, Elissa M. and Gummadi, Krishna P. and Weller, Adrian},
title = {Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186138},
doi = {10.1145/3178876.3186138},
abstract = {As algorithms are increasingly used to make important decisions that affect human lives, ranging from social benefit assignment to predicting risk of criminal recidivism, concerns have been raised about the fairness of algorithmic decision making. Most prior works on algorithmic fairness normatively prescribe how fair decisions ought to be made. In contrast, here, we descriptively survey users for how they perceive and reason about fairness in algorithmic decision making. A key contribution of this work is the framework we propose to understand why people perceive certain features as fair or unfair to be used in algorithms. Our framework identifies eight properties of features, such as relevance, volitionality and reliability, as latent considerations that inform people»s moral judgments about the fairness of feature use in decision-making algorithms. We validate our framework through a series of scenario-based surveys with 576 people. We find that, based on a person»s assessment of the eight latent properties of a feature in our exemplar scenario, we can accurately (> 85\%) predict if the person will judge the use of the feature as fair. Our findings have important implications. At a high-level, we show that people»s unfairness concerns are multi-dimensional and argue that future studies need to address unfairness concerns beyond discrimination. At a low-level, we find considerable disagreements in people»s fairness judgments. We identify root causes of the disagreements, and note possible pathways to resolve them.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {903–912},
numpages = {10},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3465416.3483291,
author = {Nyarko, Julian and Goel, Sharad and Sommers, Roseanna},
title = {Breaking Taboos in Fair Machine Learning: An Experimental Study},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483291},
doi = {10.1145/3465416.3483291},
abstract = {Many scholars, engineers, and policymakers believe that algorithmic fairness requires disregarding information about certain characteristics of individuals, such as their race or gender. Often, the mandate to “blind” algorithms in this way is conveyed as an unconditional ethical imperative—a minimal requirement of fair treatment—and any contrary practice is assumed to be morally and politically untenable. However, in some circumstances, prohibiting algorithms from considering information about race or gender can in fact lead to worse outcomes for racial minorities and women, complicating the rationale for blinding. In this paper, we conduct a series of randomized studies to investigate attitudes toward blinding algorithms, both among the general public as well as among computer scientists and professional lawyers. We find, first, that people are generally averse to the use of race and gender in algorithmic determinations of “pretrial risk”—the risk that criminal defendants pose to the public if released while awaiting trial. We find, however, that this preference for blinding shifts in response to a relatively mild intervention. In particular, we show that support for the use of race and gender in algorithmic decision-making increases substantially after respondents read a short passage about the possibility that blinding could lead to higher detention rates for Black and female defendants, respectively. Similar effect sizes are observed among the general public, computer scientists, and professional lawyers. These findings suggest that, while many respondents attest that they prefer blind algorithms, their preference is not based on an absolute principle. Rather, blinding is perceived as a way to ensure better outcomes for members of marginalized groups. Accordingly, in circumstances where blinding serves to disadvantage marginalized groups, respondents no longer view the exclusion of protected characteristics as a moral imperative, and the use of such information may become politically viable.},
booktitle = {Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {14},
numpages = {11},
location = {--, NY, USA},
series = {EAAMO '21}
}

@article{10.1145/3359130,
author = {van Berkel, Niels and Goncalves, Jorge and Hettiachchi, Danula and Wijenayake, Senuri and Kelly, Ryan M. and Kostakos, Vassilis},
title = {Crowdsourcing Perceptions of Fair Predictors for Machine Learning: A Recidivism Case Study},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359130},
doi = {10.1145/3359130},
abstract = {The increased reliance on algorithmic decision-making in socially impactful processes has intensified the calls for algorithms that are unbiased and procedurally fair. Identifying fair predictors is an essential step in the construction of equitable algorithms, but the lack of ground-truth in fair predictor selection makes this a challenging task. In our study, we recruit 90 crowdworkers to judge the inclusion of various predictors for recidivism. We divide participants across three conditions with varying group composition. Our results show that participants were able to make informed decisions on predictor selection. We find that agreement with the majority vote is higher when participants are part of a more diverse group. The presented workflow, which provides a scalable and practical approach to reach a diverse audience, allows researchers to capture participants' perceptions of fairness in private while simultaneously allowing for structured participant discussion.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {28},
numpages = {21},
keywords = {perceived fairness, modelling bias, intelligible models, fairness, crowdsourcing, crime, chatbots, bias, artificial intelligence, algorithmic decision making}
}

@inproceedings{srivastava-etal-2019-learning,
    title = "Learning to Ask for Conversational Machine Learning",
    author = "Srivastava, Shashank  and
      Labutov, Igor  and
      Mitchell, Tom",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1426",
    doi = "10.18653/v1/D19-1426",
    pages = "4164--4174",
    abstract = "Natural language has recently been explored as a new medium of supervision for training machine learning models. Here, we explore learning classification tasks using language in a conversational setting {--} where the automated learner does not simply receive language input from a teacher, but can proactively engage the teacher by asking questions. We present a reinforcement learning framework, where the learner{'}s actions correspond to question types and the reward for asking a question is based on how the teacher{'}s response changes performance of the resulting machine learning model on the learning task. In this framework, learning good question-asking strategies corresponds to asking sequences of questions that maximize the cumulative (discounted) reward, and hence quickly lead to effective classifiers. Empirical analysis across three domains shows that learned question-asking strategies expedite classifier training by asking appropriate questions at different points in the learning process. The approach allows learning classifiers from a blend of strategies, including learning from observations, explanations and clarifications.",
}

@misc{yang2024llm,
      title={LLM Voting: Human Choices and AI Collective Decision Making}, 
      author={Joshua C. Yang and Damian Dailisan and Marcin Korecki and Carina I. Hausladen and Dirk Helbing},
      year={2024},
      eprint={2402.01766},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ma2024humanai,
      title={Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making}, 
      author={Shuai Ma and Qiaoyi Chen and Xinru Wang and Chengbo Zheng and Zhenhui Peng and Ming Yin and Xiaojuan Ma},
      year={2024},
      eprint={2403.16812},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@inproceedings{10.1145/3640543.3645199,
author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
title = {Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645199},
doi = {10.1145/3640543.3645199},
abstract = {Group decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group decision making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities exhibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommendation have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {103–119},
numpages = {17},
keywords = {AI-assisted decision making, Human-AI interaction, devil’s advocate, group-AI interaction, large language model},
location = {<conf-loc>, <city>Greenville</city>, <state>SC</state>, <country>USA</country>, </conf-loc>},
series = {IUI '24}
}

@inproceedings{10.1145/3544548.3580907,
author = {Petridis, Savvas and Diakopoulos, Nicholas and Crowston, Kevin and Hansen, Mark and Henderson, Keren and Jastrzebski, Stan and Nickerson, Jeffrey V and Chilton, Lydia B},
title = {AngleKindling: Supporting Journalistic Angle Ideation with Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580907},
doi = {10.1145/3544548.3580907},
abstract = {News media often leverage documents to find ideas for stories, while being critical of the frames and narratives present. Developing angles from a document such as a press release is a cognitively taxing process, in which journalists critically examine the implicit meaning of its claims. Informed by interviews with journalists, we developed AngleKindling, an interactive tool which employs the common sense reasoning of large language models to help journalists explore angles for reporting on a press release. In a study with 12 professional journalists, we show that participants found AngleKindling significantly more helpful and less mentally demanding to use for brainstorming ideas, compared to a prior journalistic angle ideation tool. AngleKindling helped journalists deeply engage with the press release and recognize angles that were useful for multiple types of stories. From our findings, we discuss how to help journalists customize and identify promising angles, and extending AngleKindling to other knowledge-work domains.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {225},
numpages = {16},
keywords = {Brainstorming, Generative AI, Ideation, Journalism, Large Language Models},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {CHI '23}
}

@misc{chen2024personalised,
      title={Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation}, 
      author={Hailin Chen and Amrita Saha and Steven Hoi and Shafiq Joty},
      year={2024},
      eprint={2310.18628},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yang2023interpretable,
      title={Towards Interpretable Mental Health Analysis with Large Language Models}, 
      author={Kailai Yang and Shaoxiong Ji and Tianlin Zhang and Qianqian Xie and Ziyan Kuang and Sophia Ananiadou},
      year={2023},
      eprint={2304.03347},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cegin2023chatgpt,
      title={ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness}, 
      author={Jan Cegin and Jakub Simko and Peter Brusilovsky},
      year={2023},
      eprint={2305.12947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{10.1145/3613905.3651029,
author = {Wang, Chao and Hasler, Stephan and Tanneberg, Daniel and Ocker, Felix and Joublin, Frank and Ceravola, Antonello and Deigmoeller, Joerg and Gienger, Michael},
title = {LaMI: Large Language Models for Multi-Modal Human-Robot Interaction},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651029},
doi = {10.1145/3613905.3651029},
abstract = {This paper presents an innovative large language model (LLM)-based robotic system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI systems relied on complex designs for intent estimation, reasoning, and behavior generation, which were resource-intensive. In contrast, our system empowers researchers and practitioners to regulate robot behavior through three key aspects: providing high-level linguistic guidance, creating "atomic actions" and expressions the robot can use, and offering a set of examples. Implemented on a physical robot, it demonstrates proficiency in adapting to multi-modal inputs and determining the appropriate manner of action to assist humans with its arms, following researchers’ defined guidelines. Simultaneously, it coordinates the robot’s lid, neck, and ear movements with speech output to produce dynamic, multi-modal expressions. This showcases the system’s potential to revolutionize HRI by shifting from conventional, manual state-and-flow design methods to an intuitive, guidance-based, and example-driven approach. Supplementary material can be found at https://hri-eu.github.io/Lami/},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {218},
numpages = {10},
keywords = {Assisting robot, Human-robot interaction, Large language model},
location = {<conf-loc>
<city>Honolulu</city>
<state>HI</state>
<country>USA</country>
</conf-loc>},
series = {CHI EA '24}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{10.1145/3613905.3650786,
author = {Gao, Jie and Gebreegziabher, Simret Araya and Choo, Kenny Tsu Wei and Li, Toby Jia-Jun and Perrault, Simon Tangi and Malone, Thomas W},
title = {A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650786},
doi = {10.1145/3613905.3650786},
abstract = {With ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow—planning, facilitating, iterating, and testing—to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {24},
numpages = {11},
keywords = {Human-LLM Interaction, Large Language Models, Taxonomy},
location = {<conf-loc>
<city>Honolulu</city>
<state>HI</state>
<country>USA</country>
</conf-loc>},
series = {CHI EA '24}
}

@misc{dong2023survey,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Lei Li and Zhifang Sui},
      year={2023},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{10.1145/3560815,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3560815},
doi = {10.1145/3560815},
abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {195},
numpages = {35},
keywords = {Pre-trained language models, prompting}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{10.1145/3613904.3641960,
author = {Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},
title = {Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641960},
doi = {10.1145/3613904.3641960},
abstract = {Large language models (LLMs) have shown remarkable performance across various natural language processing (NLP) tasks, indicating their significant potential as data annotators. Although LLM-generated annotations are more cost-effective and efficient to obtain, they are often erroneous for complex or domain-specific tasks and may introduce bias when compared to human annotations. Therefore, instead of completely replacing human annotators with LLMs, we need to leverage the strengths of both LLMs and humans to ensure the accuracy and reliability of annotations. This paper presents a multi-step human-LLM collaborative approach where (1) LLMs generate labels and provide explanations, (2) a verifier assesses the quality of LLM-generated labels, and (3) human annotators re-annotate a subset of labels with lower verification scores. To facilitate human-LLM collaboration, we make use of LLM’s ability to rationalize its decisions. LLM-generated explanations can provide additional information to the verifier model as well as help humans better understand LLM labels. We demonstrate that our verifier is able to identify potentially incorrect LLM labels for human re-annotation. Furthermore, we investigate the impact of presenting LLM labels and explanations on human re-annotation through crowdsourced studies.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {303},
numpages = {21},
keywords = {Human-LLM collaborative annotation, LLM annotation, NLP, self-rationalization, text annotation},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@misc{li2024robust,
      title={Robust Prompt Optimization for Large Language Models Against Distribution Shifts}, 
      author={Moxin Li and Wenjie Wang and Fuli Feng and Yixin Cao and Jizhi Zhang and Tat-Seng Chua},
      year={2024},
      eprint={2305.13954},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liang2023prompting,
      title={Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation}, 
      author={Yuanyuan Liang and Jianing Wang and Hanlun Zhu and Lei Wang and Weining Qian and Yunshi Lan},
      year={2023},
      eprint={2310.08395},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yin2019understanding,
  title={Understanding the effect of accuracy on trust in machine learning models},
  author={Yin, Ming and Wortman Vaughan, Jennifer and Wallach, Hanna},
  booktitle={Proceedings of the 2019 chi conference on human factors in computing systems},
  pages={1--12},
  year={2019}
}

@inproceedings{schemmer2023appropriate,
  title={Appropriate reliance on AI advice: Conceptualization and the effect of explanations},
  author={Schemmer, Max and Kuehl, Niklas and Benz, Carina and Bartos, Andrea and Satzger, Gerhard},
  booktitle={Proceedings of the 28th International Conference on Intelligent User Interfaces},
  pages={410--422},
  year={2023}
}

@inproceedings{schoeffer2024explanations,
  title={Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making},
  author={Schoeffer, Jakob and De-Arteaga, Maria and Kuehl, Niklas},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2024}
}

@inproceedings{chiang2024enhancing,
  title={Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate},
  author={Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  booktitle={Proceedings of the 29th International Conference on Intelligent User Interfaces},
  pages={103--119},
  year={2024}
}

@article{schemmer2022should,
  title={Should I follow AI-based advice? Measuring appropriate reliance in human-AI decision-making},
  author={Schemmer, Max and Hemmer, Patrick and K{\"u}hl, Niklas and Benz, Carina and Satzger, Gerhard},
  journal={arXiv preprint arXiv:2204.06916},
  year={2022}
}

@article{steyvers2023three,
  title={Three challenges for ai-assisted decision-making},
  author={Steyvers, Mark and Kumar, Aakriti},
  journal={Perspectives on Psychological Science},
  pages={17456916231181102},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{mahmud2022influences,
  title={What influences algorithmic decision-making? A systematic literature review on algorithm aversion},
  author={Mahmud, Hasan and Islam, AKM Najmul and Ahmed, Syed Ishtiaque and Smolander, Kari},
  journal={Technological Forecasting and Social Change},
  volume={175},
  pages={121390},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{ochmann2020influence,
  title={The influence of algorithm aversion and anthropomorphic agent design on the acceptance of AI-based job recommendations.},
  author={Ochmann, Jessica and Michels, Leonard and Zilker, Sandra and Tiefenbeck, Verena and Laumer, Sven},
  booktitle={ICIS},
  year={2020}
}

@article{vasconcelos2023explanations,
  title={Explanations can reduce overreliance on ai systems during decision-making},
  author={Vasconcelos, Helena and J{\"o}rke, Matthew and Grunde-McLaughlin, Madeleine and Gerstenberg, Tobias and Bernstein, Michael S and Krishna, Ranjay},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={7},
  number={CSCW1},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{lee2023understanding,
  title={Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making},
  author={Lee, Min Hun and Chew, Chong Jun},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={7},
  number={CSCW2},
  pages={1--22},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{buccinca2021trust,
  title={To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making},
  author={Bu{\c{c}}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={5},
  number={CSCW1},
  pages={1--21},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{erlei2020impact,
  title={Impact of algorithmic decision making on human behavior: Evidence from ultimatum bargaining},
  author={Erlei, Alexander and Nekdem, Franck and Meub, Lukas and Anand, Avishek and Gadiraju, Ujwal},
  booktitle={Proceedings of the AAAI conference on human computation and crowdsourcing},
  volume={8},
  pages={43--52},
  year={2020}
}


@article{wang2024weaver,
  title={Weaver: Foundation Models for Creative Writing},
  author={Wang, Tiannan and Chen, Jiamin and Jia, Qingrui and Wang, Shuai and Fang, Ruoyu and Wang, Huilin and Gao, Zhaowei and Xie, Chunzhao and Xu, Chuou and Dai, Jihong and others},
  journal={arXiv preprint arXiv:2401.17268},
  year={2024}
}

@inproceedings{nam2024using,
  title={Using an llm to help with code understanding},
  author={Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}


@article{huang2024graphimind,
  title={GraphiMind: LLM-centric Interface for Information Graphics Design},
  author={Huang, Qirui and Lu, Min and Lanir, Joel and Lischinski, Dani and Cohen-Or, Daniel and Huang, Hui},
  journal={arXiv preprint arXiv:2401.13245},
  year={2024}
}

@inproceedings{gao2024taxonomy,
  title={A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration},
  author={Gao, Jie and Gebreegziabher, Simret Araya and Choo, Kenny Tsu Wei and Li, Toby Jia-Jun and Perrault, Simon Tangi and Malone, Thomas W},
  booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--11},
  year={2024}
}

@inproceedings{kim2024understanding,
  title={Understanding large-language model (llm)-powered human-robot interaction},
  author={Kim, Callie Y and Lee, Christine P and Mutlu, Bilge},
  booktitle={Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={371--380},
  year={2024}
}

@inproceedings{10.1145/3544548.3581225,
author = {Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard},
title = {Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581225},
doi = {10.1145/3544548.3581225},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {355},
numpages = {34},
keywords = {co-creativity, computational creativity, human-computer interaction, improvisation, natural language evaluation, natural language generation, theatre},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{eigner2024determinants,
  title={Determinants of LLM-assisted Decision-Making},
  author={Eigner, Eva and H{\"a}ndler, Thorsten},
  journal={arXiv preprint arXiv:2402.17385},
  year={2024}
}

@inproceedings{kim2024m,
  title={" I'm Not Sure, But...": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust},
  author={Kim, Sunnie SY and Liao, Q Vera and Vorvoreanu, Mihaela and Ballard, Stephanie and Vaughan, Jennifer Wortman},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={822--835},
  year={2024}
}

@article{do2024facilitating,
  title={Facilitating Human-LLM Collaboration through Factuality Scores and Source Attributions},
  author={Do, Hyo Jin and Ostrand, Rachel and Weisz, Justin D and Dugan, Casey and Sattigeri, Prasanna and Wei, Dennis and Murugesan, Keerthiram and Geyer, Werner},
  journal={arXiv preprint arXiv:2405.20434},
  year={2024}
}

@article{tonmoy2024comprehensive,
  title={A comprehensive survey of hallucination mitigation techniques in large language models},
  author={Tonmoy, SM and Zaman, SM and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  journal={arXiv preprint arXiv:2401.01313},
  year={2024}
}

@inproceedings{zamfirescu2023johnny,
  title={Why Johnny can’t prompt: how non-AI experts try (and fail) to design LLM prompts},
  author={Zamfirescu-Pereira, JD and Wong, Richmond Y and Hartmann, Bjoern and Yang, Qian},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2023}
}

@inproceedings{wu2022ai,
  title={Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts},
  author={Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
  booktitle={Proceedings of the 2022 CHI conference on human factors in computing systems},
  pages={1--22},
  year={2022}
}

@article{shin2020autoprompt,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.15980},
  year={2020}
}

@inproceedings{wang2024lave,
  title={LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing},
  author={Wang, Bryan and Li, Yuliang and Lv, Zhaoyang and Xia, Haijun and Xu, Yan and Sodhi, Raj},
  booktitle={Proceedings of the 29th International Conference on Intelligent User Interfaces},
  pages={699--714},
  year={2024}
}

@inproceedings{liu2024make,
  title={Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions},
  author={Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Che, Xing and Wang, Dandan and Wang, Qing},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@inproceedings{ni2023lever,
  title={Lever: Learning to verify language-to-code generation with execution},
  author={Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Veselin and Yih, Wen-tau and Wang, Sida and Lin, Xi Victoria},
  booktitle={International Conference on Machine Learning},
  pages={26106--26128},
  year={2023},
  organization={PMLR}
}

@article{chew2023llm,
  title={LLM-assisted content analysis: Using large language models to support deductive coding},
  author={Chew, Robert and Bollenbacher, John and Wenger, Michael and Speer, Jessica and Kim, Annice},
  journal={arXiv preprint arXiv:2306.14924},
  year={2023}
}

@article{khan2024automating,
  title={Automating Thematic Analysis: How LLMs Analyse Controversial Topics},
  author={Khan, Awais Hameed and Kegalle, Hiruni and D'Silva, Rhea and Watt, Ned and Whelan-Shamy, Daniel and Ghahremanlou, Lida and Magee, Liam},
  journal={arXiv preprint arXiv:2405.06919},
  year={2024}
}

@inproceedings{wang2024human,
  title={Human-LLM collaborative annotation through effective verification of LLM labels},
  author={Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}
@article{hong2024next,
  title={Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL},
  author={Hong, Zijin and Yuan, Zheng and Zhang, Qinggang and Chen, Hao and Dong, Junnan and Huang, Feiran and Huang, Xiao},
  journal={arXiv preprint arXiv:2406.08426},
  year={2024}
}

@inproceedings{yang2024human,
  title={Human-centric autonomous systems with llms for user command reasoning},
  author={Yang, Yi and Zhang, Qingwen and Li, Ci and Marta, Daniel Sim{\~o}es and Batool, Nazre and Folkesson, John},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={988--994},
  year={2024}
}

@inproceedings{sushina2020artificial,
  title={Artificial intelligence in the criminal justice system: leading trends and possibilities},
  author={Sushina, Tatyana and Sobenin, Andrew},
  booktitle={6th International Conference on Social, economic, and academic leadership (ICSEAL-6-2019)},
  pages={432--437},
  year={2020},
  organization={Atlantis Press}
}

@article{ghasemi2021application,
  title={The application of machine learning to a general risk--need assessment instrument in the prediction of criminal recidivism},
  author={Ghasemi, Mehdi and Anvari, Daniel and Atapour, Mahshid and Stephen Wormith, J and Stockdale, Keira C and Spiteri, Raymond J},
  journal={Criminal Justice and Behavior},
  volume={48},
  number={4},
  pages={518--538},
  year={2021},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{khan2022stock,
  title={Stock market prediction using machine learning classifiers and social media, news},
  author={Khan, Wasiat and Ghazanfar, Mustansar Ali and Azam, Muhammad Awais and Karami, Amin and Alyoubi, Khaled H and Alfakeeh, Ahmed S},
  journal={Journal of Ambient Intelligence and Humanized Computing},
  pages={1--24},
  year={2022},
  publisher={Springer}
}

@inproceedings{lu2021human,
  title={Human reliance on machine learning models when performance feedback is limited: Heuristics and risks},
  author={Lu, Zhuoran and Yin, Ming},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--16},
  year={2021}
}

@inproceedings{he2022walking,
  title={Walking on Eggshells: Using Analogies to Promote Appropriate Reliance in Human-AI Decision Making},
  author={He, Gaole and Gadiraju, Ujwal},
  booktitle={Proceedings of the Workshop on Trust and Reliance on AI-Human Teams at the ACM Conference on Human Factors in Computing Systems (CHI’22)},
  year={2022}
}

@inproceedings{bussone2015role,
  title={The role of explanations on trust and reliance in clinical decision support systems},
  author={Bussone, Adrian and Stumpf, Simone and O'Sullivan, Dympna},
  booktitle={2015 international conference on healthcare informatics},
  pages={160--169},
  year={2015},
  organization={IEEE}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{misc_census_income_20,
  author       = {Kohavi,Ron},
  title        = {{Census Income}},
  year         = {1996},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5GP7S}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{huang2023can,
  title={Can large language models explain themselves? a study of llm-generated self-explanations},
  author={Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2310.11207},
  year={2023}
}

@article{dressel2018accuracy,
  title={The accuracy, fairness, and limits of predicting recidivism},
  author={Dressel, Julia and Farid, Hany},
  journal={Science advances},
  volume={4},
  number={1},
  pages={eaao5580},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{karinshak2023working,
  title={Working with AI to persuade: Examining a large language model's ability to generate pro-vaccination messages},
  author={Karinshak, Elise and Liu, Sunny Xun and Park, Joon Sung and Hancock, Jeffrey T},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={7},
  number={CSCW1},
  pages={1--29},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{10.1145/3648188.3675130,
author = {He, Gaole and Bharos, Abri and Gadiraju, Ujwal},
title = {To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance on AI Systems},
year = {2024},
isbn = {9798400705953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648188.3675130},
doi = {10.1145/3648188.3675130},
pages = {98–105},
numpages = {8},
location = {Poznan, Poland},
series = {HT '24}
}

@inproceedings{zhang2020effect,
  title={Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making},
  author={Zhang, Yunfeng and Liao, Q Vera and Bellamy, Rachel KE},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={295--305},
  year={2020}
}

@inproceedings{lai2020chicago,
  title={" Why is' Chicago'deceptive?" Towards Building Model-Driven Tutorials for Humans},
  author={Lai, Vivian and Liu, Han and Tan, Chenhao},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2020}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{clark2021all,
  title={All that's' human'is not gold: Evaluating human evaluation of generated text},
  author={Clark, Elizabeth and August, Tal and Serrano, Sofia and Haduong, Nikita and Gururangan, Suchin and Smith, Noah A},
  journal={arXiv preprint arXiv:2107.00061},
  year={2021}
}

@article{li2024pre,
  title={Pre-trained language models for text generation: A survey},
  author={Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={ACM Computing Surveys},
  volume={56},
  number={9},
  pages={1--39},
  year={2024},
  publisher={ACM New York, NY}
}

@article{wang2023pursuit,
  title={In pursuit of interpretable, fair and accurate machine learning for criminal recidivism prediction},
  author={Wang, Caroline and Han, Bin and Patel, Bhrij and Rudin, Cynthia},
  journal={Journal of Quantitative Criminology},
  volume={39},
  number={2},
  pages={519--581},
  year={2023},
  publisher={Springer}
}

@inproceedings{wang2020human,
  title={From human-human collaboration to Human-AI collaboration: Designing AI systems that can work together with people},
  author={Wang, Dakuo and Churchill, Elizabeth and Maes, Pattie and Fan, Xiangmin and Shneiderman, Ben and Shi, Yuanchun and Wang, Qianying},
  booktitle={Extended abstracts of the 2020 CHI conference on human factors in computing systems},
  pages={1--6},
  year={2020}
}


@article{jarrahi2018artificial,
  title={Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making},
  author={Jarrahi, Mohammad Hossein},
  journal={Business horizons},
  volume={61},
  number={4},
  pages={577--586},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{alufaisan2021does,
  title={Does explainable artificial intelligence improve human decision-making?},
  author={Alufaisan, Yasmeen and Marusich, Laura R and Bakdash, Jonathan Z and Zhou, Yan and Kantarcioglu, Murat},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={6618--6626},
  year={2021}
}

@article{reverberi2022experimental,
  title={Experimental evidence of effective human--AI collaboration in medical decision-making},
  author={Reverberi, Carlo and Rigon, Tommaso and Solari, Aldo and Hassan, Cesare and Cherubini, Paolo and Cherubini, Andrea},
  journal={Scientific reports},
  volume={12},
  number={1},
  pages={14952},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{ashtiani2023news,
  title={News-based intelligent prediction of financial markets using text mining and machine learning: A systematic literature review},
  author={Ashtiani, Matin N and Raahemi, Bijan},
  journal={Expert Systems with Applications},
  volume={217},
  pages={119509},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{he2023knowing,
  title={Knowing about knowing: An illusion of human competence can hinder appropriate reliance on AI systems},
  author={He, Gaole and Kuiper, Lucie and Gadiraju, Ujwal},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2023}
}

@inproceedings{prabhudesai2023understanding,
  title={Understanding uncertainty: how lay decision-makers perceive and interpret uncertainty in human-AI decision making},
  author={Prabhudesai, Snehal and Yang, Leyao and Asthana, Sumit and Huan, Xun and Liao, Q Vera and Banovic, Nikola},
  booktitle={Proceedings of the 28th International Conference on Intelligent User Interfaces},
  pages={379--396},
  year={2023}
}

@article{steyvers2022bayesian,
  title={Bayesian modeling of human--AI complementarity},
  author={Steyvers, Mark and Tejeda, Heliodoro and Kerrigan, Gavin and Smyth, Padhraic},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={11},
  pages={e2111547119},
  year={2022},
  publisher={National Acad Sciences}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{kerrigan2021combining,
  title={Combining human predictions with model probabilities via confusion matrices and calibration},
  author={Kerrigan, Gavin and Smyth, Padhraic and Steyvers, Mark},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4421--4434},
  year={2021}
}

@article{whitehill2009whose,
  title={Whose vote should count more: Optimal integration of labels from labelers of unknown expertise},
  author={Whitehill, Jacob and Wu, Ting-fan and Bergsma, Jacob and Movellan, Javier and Ruvolo, Paul},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}
@article{li2014confidence,
  title={A confidence-aware approach for truth discovery on long-tail data},
  author={Li, Qi and Li, Yaliang and Gao, Jing and Su, Lu and Zhao, Bo and Demirbas, Murat and Fan, Wei and Han, Jiawei},
  journal={Proceedings of the VLDB Endowment},
  volume={8},
  number={4},
  pages={425--436},
  year={2014},
  publisher={VLDB Endowment}
}
@article{raykar2010learning,
  title={Learning from crowds.},
  author={Raykar, Vikas C and Yu, Shipeng and Zhao, Linda H and Valadez, Gerardo Hermosillo and Florin, Charles and Bogoni, Luca and Moy, Linda},
  journal={Journal of machine learning research},
  volume={11},
  number={4},
  year={2010}
}

@article{zheng2017truth,
  title={Truth inference in crowdsourcing: Is the problem solved?},
  author={Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold},
  journal={Proceedings of the VLDB Endowment},
  volume={10},
  number={5},
  pages={541--552},
  year={2017},
  publisher={VLDB Endowment}
}

@misc{li2023large,
      title={Large Language Models Understand and Can be Enhanced by Emotional Stimuli}, 
      author={Cheng Li and Jindong Wang and Yixuan Zhang and Kaijie Zhu and Wenxin Hou and Jianxun Lian and Fang Luo and Qiang Yang and Xing Xie},
      year={2023},
      eprint={2307.11760},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}















% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@inproceedings{dunlop2012multidimensional,
  title={Multidimensional pareto optimization of touchscreen keyboards for speed, familiarity and improved spell checking},
  author={Dunlop, Mark and Levine, John},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={2669--2678},
  year={2012}
}

@inproceedings{arnold2016suggesting,
  title={On suggesting phrases vs. predicting words for mobile text composition},
  author={Arnold, Kenneth C and Gajos, Krzysztof Z and Kalai, Adam T},
  booktitle={Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
  pages={603--608},
  year={2016}
}

@inproceedings{kannan2016smart,
  title={Smart reply: Automated response suggestion for email},
  author={Kannan, Anjuli and Kurach, Karol and Ravi, Sujith and Kaufmann, Tobias and Tomkins, Andrew and Miklos, Balint and Corrado, Greg and Lukacs, Laszlo and Ganea, Marina and Young, Peter and others},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={955--964},
  year={2016}
}

@inproceedings{vertanen2015velocitap,
  title={VelociTap: Investigating fast mobile text entry using sentence-based decoding of touchscreen keyboard input},
  author={Vertanen, Keith and Memmi, Haythem and Emge, Justin and Reyal, Shyam and Kristensson, Per Ola},
  booktitle={Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
  pages={659--668},
  year={2015}
}

@inproceedings{bi2014both,
  title={Both complete and correct? multi-objective optimization of touchscreen keyboard},
  author={Bi, Xiaojun and Ouyang, Tom and Zhai, Shumin},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={2297--2306},
  year={2014}
}


@inproceedings{banovic2019limits,
  title={The limits of expert text entry speed on mobile keyboards with autocorrect},
  author={Banovic, Nikola and Sethapakdi, Ticha and Hari, Yasasvi and Dey, Anind K and Mankoff, Jennifer},
  booktitle={Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
  pages={1--12},
  year={2019}
}

@inproceedings{buschek2018researchime,
  title={ResearchIME: A mobile keyboard application for studying free typing behaviour in the wild},
  author={Buschek, Daniel and Bisinger, Benjamin and Alt, Florian},
  booktitle={Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2018}
}

@inproceedings{dalvi2016does,
  title={Does prediction really help in Marathi text input? Empirical analysis of a longitudinal study},
  author={Dalvi, Girish and Ahire, Shashank and Emmadi, Nagraj and Joshi, Manjiri and Joshi, Anirudha and Ghosh, Sanjay and Ghone, Prasad and Parmar, Narendra},
  booktitle={Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
  pages={35--46},
  year={2016}
}

@inproceedings{palin2019people,
  title={How do people type on mobile devices? Observations from a study with 37,000 volunteers},
  author={Palin, Kseniia and Feit, Anna Maria and Kim, Sunjun and Kristensson, Per Ola and Oulasvirta, Antti},
  booktitle={Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
  pages={1--12},
  year={2019}
}



@inproceedings{fowler2015effects,
  title={Effects of language modeling and its personalization on touchscreen typing performance},
  author={Fowler, Andrew and Partridge, Kurt and Chelba, Ciprian and Bi, Xiaojun and Ouyang, Tom and Zhai, Shumin},
  booktitle={Proceedings of the 33rd annual ACM conference on human factors in computing systems},
  pages={649--658},
  year={2015}
}

@inproceedings{quinn2016cost,
  title={A cost-benefit study of text entry suggestion interaction},
  author={Quinn, Philip and Zhai, Shumin},
  booktitle={Proceedings of the 2016 CHI conference on human factors in computing systems},
  pages={83--88},
  year={2016}
}

@inproceedings{chen2019gmail,
  title={Gmail smart compose: Real-time assisted writing},
  author={Chen, Mia Xu and Lee, Benjamin N and Bansal, Gagan and Cao, Yuan and Zhang, Shuyuan and Lu, Justin and Tsay, Jackie and Wang, Yinan and Dai, Andrew M and Chen, Zhifeng and others},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2287--2295},
  year={2019}
}

@inproceedings{yuan2022wordcraft,
  title={Wordcraft: story writing with large language models},
  author={Yuan, Ann and Coenen, Andy and Reif, Emily and Ippolito, Daphne},
  booktitle={27th International Conference on Intelligent User Interfaces},
  pages={841--852},
  year={2022}
}
@inproceedings{clark2018creative,
  title={Creative writing with a machine in the loop: Case studies on slogans and stories},
  author={Clark, Elizabeth and Ross, Anne Spencer and Tan, Chenhao and Ji, Yangfeng and Smith, Noah A},
  booktitle={23rd International Conference on Intelligent User Interfaces},
  pages={329--340},
  year={2018}
}
@inproceedings{lee2022coauthor,
  title={Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities},
  author={Lee, Mina and Liang, Percy and Yang, Qian},
  booktitle={Proceedings of the 2022 CHI conference on human factors in computing systems},
  pages={1--19},
  year={2022}
}

@inproceedings{chung2022talebrush,
  title={TaleBrush: Sketching stories with generative pretrained language models},
  author={Chung, John Joon Young and Kim, Wooseok and Yoo, Kang Min and Lee, Hwaran and Adar, Eytan and Chang, Minsuk},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@inproceedings{mirowski2023co,
  title={Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals},
  author={Mirowski, Piotr and Mathewson, Kory W and Pittman, Jaylen and Evans, Richard},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--34},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{Openaigpt4,
  title={GPT-4 Technical Report
},
  author={OpenAI},
  
  year={2023}
}

@inproceedings{fu2023comparing,
  title={Comparing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication},
  author={Fu, Liye and Newman, Benjamin and Jakesch, Maurice and Kreps, Sarah},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2023}
}

@inproceedings{dang2023choice,
  title={Choice over control: How users write with large language models using diegetic and non-diegetic prompting},
  author={Dang, Hai and Goller, Sven and Lehmann, Florian and Buschek, Daniel},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2023}
}

@inproceedings{buschek2021impact,
  title={The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non-native english writers},
  author={Buschek, Daniel and Z{\"u}rn, Martin and Eiband, Malin},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2021}
}

@article{singh2022hide,
  title={Where to hide a stolen elephant: Leaps in creative writing with multimodal machine intelligence},
  author={Singh, Nikhil and Bernal, Guillermo and Savchenko, Daria and Glassman, Elena L},
  journal={ACM Transactions on Computer-Human Interaction},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{yang2022ai,
  title={AI as an Active Writer: Interaction strategies with generated text in human-AI collaborative fiction writing},
  author={Yang, Daijin and Zhou, Yanpeng and Zhang, Zhiyuan and Li, Toby Jia-Jun and LC, Ray},
  booktitle={Joint Proceedings of the ACM IUI Workshops},
  volume={10},
  year={2022},
  organization={CEUR-WS Team}
}

@article{hohenstein2020ai,
  title={AI as a moral crumple zone: The effects of AI-mediated communication on attribution and trust},
  author={Hohenstein, Jess and Jung, Malte},
  journal={Computers in Human Behavior},
  volume={106},
  pages={106190},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{jakesch2019ai,
  title={AI-mediated communication: How the perception that profile text was written by AI affects trustworthiness},
  author={Jakesch, Maurice and French, Megan and Ma, Xiao and Hancock, Jeffrey T and Naaman, Mor},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2019}
}
@inproceedings{jakesch2023co,
  title={Co-writing with opinionated language models affects users’ views},
  author={Jakesch, Maurice and Bhat, Advait and Buschek, Daniel and Zalmanson, Lior and Naaman, Mor},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2023}
}

@inproceedings{zhou2023synthetic,
  title={Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions},
  author={Zhou, Jiawei and Zhang, Yixuan and Luo, Qianni and Parker, Andrea G and De Choudhury, Munmun},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--20},
  year={2023}
}

@article{acemoglu2018race,
  title={The race between man and machine: Implications of technology for growth, factor shares, and employment},
  author={Acemoglu, Daron and Restrepo, Pascual},
  journal={American economic review},
  volume={108},
  number={6},
  pages={1488--1542},
  year={2018},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203}
}

@article{chakrabarty2022help,
  title={Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing},
  author={Chakrabarty, Tuhin and Padmakumar, Vishakh and He, He},
  journal={arXiv preprint arXiv:2210.13669},
  year={2022}
}

@article{macneil2023prompt,
  title={Prompt Middleware: Mapping Prompts for Large Language Models to UI Affordances},
  author={MacNeil, Stephen and Tran, Andrew and Kim, Joanne and Huang, Ziheng and Bernstein, Seth and Mogil, Dan},
  journal={arXiv preprint arXiv:2307.01142},
  year={2023}
}

@misc{hoqueaugmenting,
  title={Augmenting Human-AI Co-Writing with Interactive Visualization},
  author={Hoque, Md Naimul and Elmqvist, Niklas}
}


@article{
doi:10.1126/science.adh2586,
author = {Shakked Noy  and Whitney Zhang },
title = {Experimental evidence on the productivity effects of generative artificial intelligence},
journal = {Science},
volume = {381},
number = {6654},
pages = {187-192},
year = {2023},
doi = {10.1126/science.adh2586},
URL = {https://www.science.org/doi/abs/10.1126/science.adh2586},
eprint = {https://www.science.org/doi/pdf/10.1126/science.adh2586},
}

@article{liang2023hidden,
  title={The hidden costs and benefits of monitoring in the gig economy},
  author={Liang, Chen and Peng, Jing and Hong, Yili and Gu, Bin},
  journal={Information Systems Research},
  volume={34},
  number={1},
  pages={297--318},
  year={2023},
  publisher={INFORMS}
}

@inproceedings{biermann2022tool,
  title={From tool to companion: Storywriters want AI writers to respect their personal values and writing strategies},
  author={Biermann, Oloff C and Ma, Ning F and Yoon, Dongwook},
  booktitle={Designing Interactive Systems Conference},
  pages={1209--1227},
  year={2022}
}

@inproceedings{holtz2022much,
  title={How much do platform workers value reviews? An experimental method},
  author={Holtz, David M and Scult, Liane and Suri, Siddharth},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--11},
  year={2022}
}

@article{ding2023mapping,
  title={Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks},
  author={Ding, Zijian and Chan, Joel},
  journal={arXiv e-prints},
  pages={arXiv--2303},
  year={2023}
}

@article{hedegaard2018price,
  title={The price of prejudice},
  author={Hedegaard, Morten St{\o}rling and Tyran, Jean-Robert},
  journal={American Economic Journal: Applied Economics},
  volume={10},
  number={1},
  pages={40--63},
  year={2018},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2425}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{mas2017valuing,
  title={Valuing alternative work arrangements},
  author={Mas, Alexandre and Pallais, Amanda},
  journal={American Economic Review},
  volume={107},
  number={12},
  pages={3722--3759},
  year={2017},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203}
}

@article{rosen1986theory,
  title={The theory of equalizing differences},
  author={Rosen, Sherwin},
  journal={Handbook of labor economics},
  volume={1},
  pages={641--692},
  year={1986},
  publisher={Elsevier}
}

@incollection{hart1988development,
  title={Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research},
  author={Hart, Sandra G and Staveland, Lowell E},
  booktitle={Advances in psychology},
  volume={52},
  pages={139--183},
  year={1988},
  publisher={Elsevier}
}

@inproceedings{roder2015exploring,
  title={Exploring the space of topic coherence measures},
  author={R{\"o}der, Michael and Both, Andreas and Hinneburg, Alexander},
  booktitle={Proceedings of the eighth ACM international conference on Web search and data mining},
  pages={399--408},
  year={2015}
}

@inproceedings{rhys2021directed,
  title={Directed diversity: Leveraging language embedding distances for collective creativity in crowd ideation},
  author={Rhys Cox, Samuel and Wang, Yunlong and Abdul, Ashraf and Von Der Weth, Christian and Y. Lim, Brian},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--35},
  year={2021}
}

@inproceedings{sentence_embed,
  title={Sentence transformers: paraphrase-MiniLM-L6-v2},
  author={https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2},
  year={2023}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@inproceedings{robbemond2022understanding,
  title={Understanding the Role of Explanation Modality in AI-assisted Decision-making},
  author={Robbemond, Vincent and Inel, Oana and Gadiraju, Ujwal},
  booktitle={Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization},
  pages={223--233},
  year={2022}
}

@inproceedings{salimzadeh2023missing,
  title={A missing piece in the puzzle: Considering the role of task complexity in human-ai decision making},
  author={Salimzadeh, Sara and He, Gaole and Gadiraju, Ujwal},
  booktitle={Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
  pages={215--227},
  year={2023}
}

@inproceedings{gupta2022trust,
  title={To trust or not to trust: How a conversational interface affects trust in a decision support system},
  author={Gupta, Akshit and Basu, Debadeep and Ghantasala, Ramya and Qiu, Sihang and Gadiraju, Ujwal},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={3531--3540},
  year={2022}
}

@inproceedings{salimzadeh2024dealing,
  title={Dealing with Uncertainty: Understanding the Impact of Prognostic Versus Diagnostic Tasks on Trust and Reliance in Human-AI Decision Making},
  author={Salimzadeh, Sara and He, Gaole and Gadiraju, Ujwal},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2024}
}

@inproceedings{swaroop2024accuracy,
  title={Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure},
  author={Swaroop, Siddharth and Bu{\c{c}}inca, Zana and Gajos, Krzysztof Z and Doshi-Velez, Finale},
  booktitle={Proceedings of the 29th International Conference on Intelligent User Interfaces},
  pages={138--154},
  year={2024}
}

@article{rastogi2022deciding,
  title={Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making},
  author={Rastogi, Charvi and Zhang, Yunfeng and Wei, Dennis and Varshney, Kush R and Dhurandhar, Amit and Tomsett, Richard},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW1},
  pages={1--22},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{guerdan2022under,
  title={Under-reliance or misalignment? How proxy outcomes limit measurement of appropriate reliance in AI-assisted decision-making},
  author={Luke M. Guerdan and Kenneth Holstein and Zhiwei Steven and Steven Wu},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248913428}
}


@article{10.1145/3610219,
author = {Chen, Valerie and Liao, Q. Vera and Wortman Vaughan, Jennifer and Bansal, Gagan},
title = {Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610219},
doi = {10.1145/3610219},
abstract = {AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition---beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments---with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the task outcome, features, and AI limitations. Building on these, we summarize three observed pathways for decision-makers to apply their own intuition and override AI predictions. We use these pathways to explain why (1) the feature-based explanations we used did not improve participants' decision outcomes and increased their overreliance on AI, and (2) the example-based explanations we used improved decision-makers' performance over feature-based explanations and helped achieve complementary human-AI performance. Overall, our work identifies directions for further development of AI decision-support systems and explanation methods that help decision-makers effectively apply their intuition to achieve appropriate reliance on AI.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {370},
numpages = {32},
keywords = {decision support, explainable AI, human-AI interaction, interpretability}
}

@article{10.1145/3555572,
author = {Cao, Shiye and Huang, Chien-Ming},
title = {Understanding User Reliance on AI in Assisted Decision-Making},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555572},
doi = {10.1145/3555572},
abstract = {Proper calibration of human reliance on AI is fundamental to achieving complementary performance in AI-assisted human decision-making. Most previous works focused on assessing user reliance, and more broadly trust, retrospectively, through user perceptions and task-based measures. In this work, we explore the relationship between eye gaze and reliance under varying task difficulties and AI performance levels in a spatial reasoning task. Our results show a strong positive correlation between percent gaze duration on the AI suggestion and user AI task agreement, as well as user perceived reliance. Moreover, user agency is preserved particularly when the task is easy and when AI performance is low or inconsistent. Our results also reveal nuanced differences between reliance and trust. We discuss the potential of using eye gaze to gauge human reliance on AI in real-time, enabling adaptive AI assistance for optimal human-AI team performance.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {471},
numpages = {23},
keywords = {decision making, decision support tools, gaze, human-ai interaction, trust}
}

@inproceedings{wang2021explanations,
  title={Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making},
  author={Wang, Xinru and Yin, Ming},
  booktitle={Proceedings of the 26th International Conference on Intelligent User Interfaces},
  pages={318--328},
  year={2021}
}

@inproceedings{10.1145/3613904.3642482,
author = {Ma, Zilin and Mei, Yiyang and Long, Yinru and Su, Zhaoyuan and Gajos, Krzysztof Z.},
title = {Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642482},
doi = {10.1145/3613904.3642482},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {872},
numpages = {15},
keywords = {Chatbot, Gender, Identity, LGBTQIA+ Health, Large Language Models, Mental health, Socio-technical AI, Stigma},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3642400,
author = {Suh, Sangho and Chen, Meng and Min, Bryan and Li, Toby Jia-Jun and Xia, Haijun},
title = {Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642400},
doi = {10.1145/3613904.3642400},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {644},
numpages = {26},
keywords = {Large language models, creativity support, design space, dimensional exploration, human-AI co-creation, human-AI interaction},
location = {Honolulu, HI, USA},
series = {CHI '24}
}


@inproceedings{feng-boyd-graber-2022-learning,
    title = "Learning to Explain Selectively: A Case Study on Question Answering",
    author = "Feng, Shi  and
      Boyd-Graber, Jordan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.573",
    doi = "10.18653/v1/2022.emnlp-main.573",
    pages = "8372--8382",
}


@article{10.1145/3610206,
author = {Lai, Vivian and Zhang, Yiming and Chen, Chacha and Liao, Q. Vera and Tan, Chenhao},
title = {Selective Explanations: Leveraging Human Input to Align Explainable AI},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610206},
doi = {10.1145/3610206},
month = {oct},
articleno = {357},
numpages = {35},
keywords = {explainable AI, human-AI decision making, selective explanations}
}


@inproceedings{10.1145/2939672.2939778,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
pages = {1135–1144},
numpages = {10},
keywords = {interpretable machine learning, interpretability, explaining machine learning, black box classifier},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.5555/3295222.3295230,
author = {Lundberg, Scott M. and Lee, Su-In},
title = {A unified approach to interpreting model predictions},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4768–4777},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@inproceedings{10.1145/3613904.3642002,
author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
title = {CollabCoder: A Lower-barrier, Rigorous Workflow for Inductive Collaborative Qualitative Analysis with Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642002},
doi = {10.1145/3613904.3642002},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {11},
numpages = {29},
keywords = {Collaborative Qualitative Analysis, Grounded Theory, Inductive Qualitative Coding, Large Language Models},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{bansal2019updates,
  title={Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoff},
  author={Bansal, Gagan and Nushi, Besmira and Kamar, Ece and Weld, Daniel S and Lasecki, Walter S and Horvitz, Eric},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={2429--2437},
  year={2019}
}


@inproceedings{green2019disparate,
  title={Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments},
  author={Green, Ben and Chen, Yiling},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={90--99},
  year={2019}
}
@inproceedings{wang2021explanations,
  title={Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making},
  author={Wang, Xinru and Yin, Ming},
  booktitle={Proceedings of the 26th International Conference on Intelligent User Interfaces},
  pages={318--328},
  year={2021}
}

@inproceedings{van2021effect,
  title={Effect of information presentation on fairness perceptions of machine learning predictors},
  author={Van Berkel, Niels and Goncalves, Jorge and Russo, Daniel and Hosio, Simo and Skov, Mikael B},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2021}
}
@article{slack2023explaining,
  title={Explaining machine learning models with interactive natural language conversations using TalkToModel},
  author={Slack, Dylan and Krishna, Satyapriya and Lakkaraju, Himabindu and Singh, Sameer},
  journal={Nature Machine Intelligence},
  volume={5},
  number={8},
  pages={873--883},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{ma2023should,
  title={Who should i trust: Ai or myself? leveraging human and ai correctness likelihood to promote appropriate trust in ai-assisted decision-making},
  author={Ma, Shuai and Lei, Ying and Wang, Xinru and Zheng, Chengbo and Shi, Chuhan and Yin, Ming and Ma, Xiaojuan},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2023}
}