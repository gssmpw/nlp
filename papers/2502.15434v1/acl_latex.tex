% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{amsmath} % 数学公式扩展包
\usepackage{amsfonts}
\usepackage{makecell}
% \usepackage{tabularx}
\usepackage{multirow} % 导入宏包
\usepackage{subcaption}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{float} 
\usepackage{graphicx}
\usepackage{booktabs} % 使⽤三线表


% \title{Mixup Model Merge: Unlocking Model Merging Potential via Randomized Linear Interpolation}
\title{Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation}
% \title{Mixup Model Merge: Unlocking Model Merging Potential via Randomization}
% \title{Mixup Model Merge: Enhancing Model Merging with Randomized Contribution Ratios}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
        Yue Zhou$^{1}$ \quad  Yi Chang$^{1,2,3}$ \quad Yuan Wu$^{1}$\footnotemark[1] \\
        $^{1}$School of Artificial Intelligence, Jilin University \\
        $^{2}$Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China \\
        $^{3}$International Center of Future Science, Jilin University\\
        yuezhou24@mails.jlu.edu.cn, yichang@jlu.edu.cn, yuanwu@jlu.edu.cn \\
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors}

\begin{abstract}
Model merging integrates the parameters of multiple models into a unified model, combining their diverse capabilities. Existing model merging methods are often constrained by fixed parameter merging ratios. In this study, we propose Mixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data augmentation technique. This method merges the parameters of two large language models (LLMs) by randomly generating linear interpolation ratios, allowing for a more flexible and comprehensive exploration of the parameter space. Extensive experiments demonstrate the superiority of our proposed M$^3$ method in merging fine-tuned LLMs: (1) it significantly improves performance across multiple tasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and adversarial robustness, (3) it achieves superior results when combined with sparsification techniques such as DARE, and (4) it offers a simple yet efficient solution that does not require additional computational resources. In conclusion, M$^3$ is a simple yet effective model merging method that significantly enhances the performance of the merged model by randomly generating contribution ratios for two fine-tuned LLMs.
The code is available at \url{https://github.com/MLGroupJLU/MixupModelMerge}.
\end{abstract}

\section{Introduction}
In the field of Natural Language Processing (NLP), the emergence of large language models (LLMs) \citep{brown2020language,touvron2023llama,openai2023gpt,chowdhery2023palm} represents a revolutionary breakthrough. With their remarkable capabilities, these models have demonstrated outstanding performance across various tasks \citep{jiao2023chatgpt,chang2024survey,nam2024using,xing2024designing,guo2024chbench}, significantly advancing NLP technologies. 
\begin{figure}[H]
  \centering
  % 第一张图像
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{M_3.pdf}
    \captionsetup{skip=2pt}  % 减小这一图像与caption的间距
    \caption{Performance of the merged models obtained through Task Arithmetic and TIES-Merging with/without M$^3$.}
    \label{fig:introduction_1}
  \end{subfigure}

  \vspace{0.1cm} % 添加垂直间距，可根据需要调整

  % 第二张图像
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{ood_and_adv.pdf}
    \captionsetup{skip=2pt}  % 减小这一图像与caption的间距
    \caption{(Left) Performance of merged models with/without M$^3$ on OOD datasets. (Right) Adversarial robustness (PDR) of merged models with/without M$^3$.}
    \label{fig:introduction_2}
  \end{subfigure}
 
  \vspace{0.1cm} % 添加垂直间距，可根据需要调整

  % 第三张图像
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{mixup_and_dare.pdf}
    \captionsetup{skip=2pt}  % 减小这一图像与caption的间距
    % \caption{performance comparison with DARE, Model Mixup, and their combination.}
    \caption{The performance trend of the merged models obtained through the following methods: TIES-Merging without M$^3$ and DARE, TIES-Merging with DARE, TIES-Merging with M$^3$, and TIES-Merging with both M$^3$ and DARE.}
    \label{fig:introduction_3}
  \end{subfigure}
  
  \caption{(a) M$^3$ significantly boosts the model merging performance. (b) OOD robustness, and adversarial robustness of the merged models. (c) Combined with DARE, M$^3$ delivers even better results. LM, MATH, and Code refer to the WizardLM13B, WizardMath-13B, and llama-2-13b-code-alpaca models. TA stands for Task Arithmetic and TIES stands for TIES-Merging.}
  \label{fig:overall_comparison}
\end{figure}
Supervised Fine-Tuning (SFT) is a crucial technique for adapting LLMs to specific tasks, refining their performance by training on domain-specific data \citep{hu2021lora,ding2023parameter,xia2024rethinking}. However, SFT requires substantial computational resources and long training times \citep{brown2020language,chang2024ba}. To address this challenge, Model Merging has emerged as an efficient solution, fusing the parameters of multiple fine-tuned LLMs into a unified model with diverse capabilities, without the need for additional training or computational costs \citep{yang2024model,akiba2024evolutionary}. It effectively reduces the resource-intensive demands of SFT while preserving and even enhancing model performance.

A simple analogy for model merging is the Super Mario game, where the protagonist gains special abilities by absorbing power-up items. Similarly, merging model parameters integrates the strengths of different models, enabling more effective multi-task learning \citep{yu2024language}. However, existing model merging methods have some limitations, these methods heavily rely on predefined or heuristic parameter fusion strategies \citep{wortsman2022model,ilharco2022editing,matena2022merging,jin2022dataless,yadav2023resolving,yu2024language} such that they fail to fully explore the parameter space, thereby restricting the potential of the merged model.

To address this issue, we draw inspiration from Mixup \citep{zhang2017mixup} and propose a novel technique called Mixup Model Merge (M$^3$). This method introduces randomness by dynamically adjusting the contribution ratios between models, making the model merging process more flexible and enabling a thorough exploration of the parameter space. M$^3$ further unlocks the potential of model merging, significantly enhancing the generalization performance of the merged model while improving its out-of-distribution (OOD) and adversarial robustness \citep{wang2023robustness,zhu2023promptrobust}. 
As shown in Figure~\ref{fig:HarryPotter}, the implementation of M$^3$ is similar to the process of proportionally mixing magical potions in \textit{Harry Potter}. Specifically, this method dynamically controls the parameter fusion ratio between two fine-tuned LLMs by randomly generating a linear interpolation ratio $\lambda_m$, where $\lambda_m \in (0, 1)$ and $\lambda_m \sim \text{Beta}(\alpha, \alpha)$. By adjusting the parameter $\alpha$, we can precisely control the distribution of $\lambda_m$, allowing M$^3$ to explore the parameter space of model merging more deeply and thus fully unleash the potential of the models, leading to improved overall performance.

We conducted extensive experiments with three fine-tuned LLMs: WizardLM-13B~\citep{xu2024wizardlm}, WizardMath-13B~\citep{luo2023wizardmath}, and llama-2-13b-code-alpaca~\citep{chaudhary2023code}, which specialize in instruction following, mathematical reasoning, and code generation, respectively. Inspired by Mixup's effectiveness in enhancing the robustness of neural networks when handling corrupted labels or adversarial examples \citep{zhang2017mixup}, we further performed comprehensive evaluations on LiveBench \citep{white2024livebench} and PromptBench \citep{zhu2024promptbench} to validate the potential of M$^3$ in improving the OOD robustness and adversarial robustness of merged models. The experimental results demonstrate that our proposed M$^3$ method can significantly improve merged models' performance across various tasks (as shown in Figure~\ref{fig:introduction_1}), enhance the OOD and adversarial robustness of the merged models (as shown in Figure~\ref{fig:introduction_2}), and boost model merging when combined with sparsification techniques like DARE \citep{yu2024language} (as shown in Figure~\ref{fig:introduction_3}).
%We observe that:

%(1) Model Mixup significantly improves merged models' performance across various tasks. As shown in Figure~\ref{fig:introduction_1}, applying TIES-Merging \citep{yadav2023resolving} with Model Mixup increases pass@1 by 25.6\% on HumanEval \citep{cobbe2021training} and 30.8\% on MBPP \citep{austin2021program}, demonstrating Model Mixup' effectiveness in unlocking the potential of model merging.

%(2) Model Mixup improves the OOD robustness and adversarial robustness of the merged models. As shown in Figure~\ref{fig:introduction_2}, Model Mixup increases the accuracy of LM \& Code on the OOD LiveBench-TypoFixing task \citep{white2024livebench} by 6\%, reduces the Performance Drop Rate (PDR) \citep{zhu2023promptrobust} of Math \& Code on the CoLA dataset \citep{warstadt2019neural} by 92.12\%.
 
%(3) Model Mixup further improves model merging when combined with sparsification techniques like DARE \citep{yu2024language}. In most cases, it outperforms DARE alone. As shown in Figure~\ref{fig:introduction_3}, LM \& Code merged with Model Mixup and DARE achieves the highest performance on AlpacaEval \citep{ding2023parameter}, HumanEval, and MBPP, while Model Mixup alone achieves the second highest.

\begin{figure*}[t]
  \centering  % 关键：让图片居中
  \includegraphics[width=0.9\textwidth]{HarryPotter.pdf}
  \caption{Implementation of m$^3$: A process analogous to proportionally mixing magical potions in harry potter. The proposed method controls the contribution ratio between two fine-tuned llms by randomly generating a linear interpolation ratio $\lambda_m$, where $\lambda_m \in (0, 1)$ and $\lambda_m \sim \text{Beta}(\alpha, \alpha)$. The distribution of $\lambda_m$ is controlled by adjusting $\alpha$.}
  \label{fig:HarryPotter}
\end{figure*}

\section{Related Works}
\paragraph{\textbf{{Model Merging}}}
Model merging is a technique that integrates the parameters of multiple models to create a unified model with enhanced or diverse capabilities \citep{wortsman2022model,ilharco2022editing,matena2022merging,jin2022dataless,yadav2023resolving,yu2024language,lin2024mitigating}. 
% This process leverages the unique strengths of each model to enhance overall performance without requiring original training data or costly computations.
%Model soups \citep{wortsman2022model} can improve accuracy and robustness by averaging the parameters of independently fine-tuned models with different hyperparameters. 
Task arithmetic \citep{ilharco2022editing} leverages task vectors for model merging through arithmetic operations, incorporating a predefined scaling term to weight the contribution of different models.
Fisher Merging \citep{matena2022merging} performs parameter fusion by applying weights derived from the Fisher information matrix \citep{fisher1922mathematical}, resulting in more precise parameter integration.
%Regression Mean (RegMean) \citep{jin2022dataless} minimizes the $\ell_2$ distance between the merged model and individual fine-tuned models, providing a computationally efficient closed-form solution. 
TIES-Merging \citep{yadav2023resolving} addresses task conflicts by removing low-magnitude parameters, resolving sign disagreements, and merging only the parameters that align with the final agreed-upon sign.
In \citep{yu2024language}, it is found that LLMs can enhance their capabilities through model merging. Additionally, it introduces DARE, a method for sparsifying the delta parameters of the model \citep{ilharco2022editing}, significantly improving the performance of various model merging techniques. 
% \citet{lin2024mitigating} use model averaging to alleviate alignment tax in Reinforcement Learning with Human Feedback (RLHF) %\citep{ouyang2022training} on NLP tasks and introduce Adaptive Model Averaging (AMA), which optimizes the averaging ratios of different model layers to maximize alignment reward.
%In this paper, we propose a simple yet effective model merging method, Model Mixup, which randomly generates contribution ratios between two fine-tuned models, resulting in a merged model with improved performance by exploring a broader parameter space.

\paragraph{\textbf{{Mixup}}}
%\citet{zhang2017mixup} proposed a novel data augmentation method \citep{simard2002transformation, lecun1998gradient, simonyan2014very, zhong2020random}, Mixup, in 2018, aiming to
%Traditional machine learning methods typically train models through ERM, which optimizes model parameters by minimizing the loss function on the training data. Under this approach, the model makes explicit predictions for each training sample, which often leads to overfitting, especially when the training data is limited or noisy.
Mixup is proposed to enhance the generalization ability of deep learning models by surpassing traditional Empirical Risk Minimization (ERM)~\citep{zhang2017mixup}. It is a simple, data-agnostic augmentation technique that trains models using virtual examples created by linearly interpolating pairs of random examples and their corresponding labels. Rooted in the Vicinal Risk Minimization (VRM) principle \citep{chapelle2000vicinal}, this approach improves generalization across a variety of datasets \citep{russakovsky2015imagenet, krizhevsky2009learning, warden2017launching, asuncion2007uci}, and helps reduce overfitting, sensitivity to adversarial examples, and training instability, all with minimal computational cost. Given two samples $(x_i, y_i)$ and $(x_j, y_j)$, Mixup generates a new sample using the following formulas:
\begin{align}
\begin{aligned}
\tilde{x} &= \lambda x_i + (1 - \lambda) x_j \\
\tilde{y} &= \lambda y_i + (1 - \lambda) y_j
\end{aligned}
\end{align}
$\tilde{x}$ and $\tilde{y}$ represent the generated synthetic sample and its label, respectively, with $\lambda$ determining their interpolation ratio, typically ranging from 0 to 1. Here, $\lambda$ is a hyperparameter sampled from a Beta distribution, i.e., $\lambda \sim \text{Beta}(\alpha, \alpha)$, where $\alpha$ controls the strength of the interpolation between feature-target pairs.
Inspired by Mixup, we propose a novel model merging method, M$^3$, which generates the parameters of the merged model by performing linear interpolation between the parameters of two fine-tuned LLMs, with the interpolation ratio being random.

\section{Methodology}
\subsection{Model Merging Problem}
Following \citet{yu2024language}, we focus on merging fine-tuned LLMs that have been optimized from the same pre-trained backbone \citep{touvron2023llama}. Specifically, we aim to fuse the parameters of these LLMs to create a unified model capable of handling multiple tasks. In this context, we restrict our attention to the merging of two models, as the mixup theory, which forms the basis of our approach, is generally applied to two entities.

Given two tasks \( t_1 \) and \( t_2 \) with the corresponding fine-tuned LLMs having parameters \( \theta_{\text{SFT}}^{t_1} \) and \( \theta_{\text{SFT}}^{t_2} \), model merging aims to combine the parameters of these two models into a single model with parameters \( \theta_M \). The resulting model should be able to effectively perform both tasks simultaneously, leveraging the knowledge learned from each individual model.

\subsection{Mixup Model Merge}
Inspired by Mixup \citep{zhang2017mixup}, we propose a simple yet effective model merging method called Mixup Model Merge (M$^3$). Unlike existing methods that use fixed merging ratios, M$^3$ generates the model contribution ratio randomly, harnessing randomness to inject fresh vitality into the model merging process. Specifically, M$^3$ further explores the parameter space of the merged model to unlock the potential of model merging. 

To further elaborate, given two fine-tuned LLMs with parameters \( \{\theta_{\text{SFT}}^{t_1}, \theta_{\text{SFT}}^{t_2}\} \), we combine M$^3$ with established model merging methods to fuse these parameters and obtain a single merged model with parameters \( \theta_M \). As illustrative examples, we consider two widely used merging methods: Average Merging \citep{wortsman2022model} and Task Arithmetic \citep{ilharco2022editing}. 

The official computation process for Average Merging is described as follows:
\begin{align}
\theta_M = \frac{1}{2} \left( \theta_{\text{SFT}}^{t_1} + \theta_{\text{SFT}}^{t_2} \right)
\end{align}

The official computation process for Task Arithmetic is:
\begin{align}
\theta_M &= \theta_{\text{PRE}} + \lambda \cdot (\delta^{t_1} + \delta^{t_2}) \nonumber \\
         &= \theta_{\text{PRE}} + \lambda \cdot \sum_{i=1}^2 (\theta_{\text{SFT}}^{t_i} - \theta_{\text{PRE}}),
\end{align}
where \( \theta_{\text{PRE}} \in \mathbb{R}^d \) represents the parameters of the pre-trained language model (PLM), such as Llama 2 \citep{touvron2023llama}. $\lambda$ is a scaling factor that weights the contribution of each model during the merging process. $\delta^{t_i}$ denotes the delta parameter \citep{ilharco2022editing}, which is defined as the difference between the parameters of the language models (LMs) before and after SFT, i.e., \( \delta^{t} = \theta_{\text{SFT}}^{t} - \theta_{\text{PRE}} \in \mathbb{R}^d \), where $t$ refers to task $t$.

When introducing M$^3$, the process for Average Merging is reformulated as:
\begin{align}
\theta_M = \lambda_m \theta_{\text{SFT}}^{t_1} + (1 - \lambda_m) \theta_{\text{SFT}}^{t_2},
\end{align}
while the process for Task Arithmetic is reformulated as:
\begin{align}
\theta_M &= \theta_{\text{PRE}} + \lambda_m \delta^{t_1} + (1 - \lambda_m) \delta^{t_2},
\end{align}
where \( \lambda_m \) determines the linear interpolation ratio between the two fine-tuned LLMs, and is generally a value between 0 and 1. \( \lambda_m \) is sampled from a Beta distribution, typically \( \lambda_m \sim \text{Beta}(\alpha, \alpha) \), where \( \alpha \) controls the shape of the Beta distribution. 

The hyperparameter $\alpha$ for M$^3$ is selected from the range $[0.2, 0.4, 0.5, 1, 2, 3, 5]$. As shown in Figure~\ref{fig:beta_distribution}: (1) When $\alpha = 1$, $\lambda$ follows a uniform distribution, meaning all values within the range $(0, 1)$ are equally likely to be sampled. (2) When $\alpha < 1$, the distribution of $\lambda$ exhibits a bimodal shape, with higher probabilities near the extremes (0 and 1). This indicates that the merged model is more likely to be dominated by one of the two models. (3) When $\alpha > 1$, the distribution of $\lambda$ becomes concentrated around the middle (e.g., 0.5), resulting in more balanced contributions from both models.
\begin{figure}[t]
  \centering  % 关键：让图片居中
  \includegraphics[width=0.8\columnwidth]{beta_distribution.pdf}
  \caption{The Beta distribution visualization for different $\alpha$ values.}
  \label{fig:beta_distribution}
\end{figure}

\subsection{Theoretical Analysis}
Mixup performs linear interpolations between data samples and their labels in the data space~\citep{zhang2017mixup}. Similarly, M$^3$ can be viewed as applying random linear interpolation in the parameter space, where the interpolation occurs between the parameters of two fine-tuned models trained on different tasks. M$^3$ represents a natural extension of the Vicinal Risk Minimization (VRM) principle \citep{chapelle2000vicinal} into the model parameter space. In data space, VRM introduces a vicinal distribution to simulate the true data distribution, thereby increasing the diversity of training data and enhancing the model's generalization ability. Similarly, M$^3$ extends this principle by constructing a virtual neighborhood in the model parameter space between two task-specific models, combining their knowledge in a way that is more natural and balanced.

In the context of model merging, interpolating between two sets of model parameters, \( \theta_{\text{SFT}}^{t_1} \)and \( \theta_{\text{SFT}}^{t_2} \), defines a new neighborhood distribution in the parameter space, which can be expressed as:
\begin{align}
\begin{aligned}
P_{\nu}(\theta_{M}) &= \int \nu(\theta_{M} \mid \theta_{\text{SFT}}^{t_1}, \theta_{\text{SFT}}^{t_2}) \, d\theta_{M} \\
& \quad \times P(\theta_{\text{SFT}}^{t_1}) P(\theta_{\text{SFT}}^{t_2}),
\end{aligned}
\end{align}
\noindent where \( P_{\nu}(\theta_{M}) \) represents the probability distribution of the new model parameters \( \theta_M \) generated through interpolation. The function \( \nu(\theta_{M} \mid \theta_{\text{SFT}}^{t_1}, \theta_{\text{SFT}}^{t_2}) \) defines the range and behavior of \( \theta_M \), depending on the interpolation strategy (such as linear interpolation). Additionally, \( P(\theta_{\text{SFT}}^{t_1}) \) and \( P(\theta_{\text{SFT}}^{t_2}) \) represent the prior distributions of the parameters of the two models, respectively.

Under the linear interpolation strategy, the function \( \nu(\theta_{M} \mid \theta_{\text{SFT}}^{t_1}, \theta_{\text{SFT}}^{t_2}) \) is defined as:
\begin{align}
\theta_M = \lambda_m \cdot \theta_{\text{SFT}}^{t_1} + (1 - \lambda_m) \cdot \theta_{\text{SFT}}^{t_2},
\end{align}
\noindent where \( \lambda_m \in (0, 1) \) is the interpolation ratio. The corresponding distribution \( P_{\nu}(\theta_{M}) \) is expressed as:
\vspace{-2.5mm} % 调整数值以减少间距
\begin{align}
\resizebox{\columnwidth}{!}{$
\begin{aligned}
P_{\nu}(\theta_{M}) &= \int \delta\left(\theta_{M} - \left( \lambda_m \cdot \theta_{\text{SFT}}^{t_1} + (1 - \lambda_m) \cdot \theta_{\text{SFT}}^{t_2} \right)\right) \\
&\quad \cdot P(\theta_{\text{SFT}}^{t_1}) P(\theta_{\text{SFT}}^{t_2}) \, d\theta_{\text{SFT}}^{t_1} d\theta_{\text{SFT}}^{t_2},
\end{aligned}
$}
\end{align}
\noindent where \( \delta(\cdot) \) is the Dirac delta function, ensuring that \( \theta_{M} \) satisfies the linear interpolation rule. By using this linear interpolation method, the parameters \( \theta_{\text{SFT}}^{t_1} \) and \( \theta_{\text{SFT}}^{t_2} \) are combined in the parameter space, forming a new neighborhood distribution \( P_{\nu}(\theta_{M}) \). This process effectively merges the knowledge of both models into a unified parameter space, thus achieving robust performance across different tasks while maintaining the original strengths of the models.

By interpolating the parameters, M$^3$ encourages the model to learn smoother decision boundaries. In this context, smoother decision boundaries can be understood as the boundaries between different tasks, such as instruction following, mathematical reasoning, and code generation, where the model must understand and adapt to each task differently. In tasks \( t_1 \) and \( t_2 \), M$^3$ creates a virtual neighborhood that seamlessly integrates knowledge from both tasks. This approach prevents the merged model from overfitting task-specific details, ensuring a balanced and effective performance across all tasks.

Secondly, M$^3$ introduces a linear inductive bias in the parameter space, encouraging the merged model parameters to lie on a linear manifold between the two source models. This linear structure offers significant advantages in terms of simplicity and generalization. According to Occam's Razor, simpler solutions tend to generalize better. By performing linear interpolation between two sets of parameters, M$^3$ avoids unnecessary complexity in the model merging process, leading to a more straightforward and efficient solution.

Thirdly, M$^3$ can improve the performance of the merged model across multiple tasks by mitigating task conflicts. Different tasks may require conflicting parameter values, which can lead to performance degradation on one task while optimizing for another. Linear interpolation helps balance these conflicts, resulting in a model that performs well among all tasks. 
%In sum, Model Mixup effectively explores the parameter space, blending the characteristics of both source models by adjusting the interpolation ratio. This approach enhances the generalization performance and robustness of the merged model, making it more effective in real-world scenarios.

\section{Experiments}
\subsection{Experimental Setup}
\paragraph{\textbf{{Task-Specific Fine-Tuned LLMs and Datasets}}}
Following the experimental setup given in \citet{yu2024language}, we select three task-specific fine-tuned LLMs: WizardLM-13B \citep{xu2024wizardlm}, WizardMath-13B \citep{luo2023wizardmath}, and llama-2-13b-code-alpaca \citep{chaudhary2023code}, all of which use Llama-2-13b \citep{touvron2023llama} as the pre-trained backbone. These models are respectively designed for instruction-following, mathematical reasoning, and code generation tasks.
To evaluate the instruction-following task we use AlpacaEval \citep{li2023alpacaeval}. For testing mathematical reasoning task, we employ GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring}. For estimating the performance of code-generating task, we use HumanEval \citep{chen2021evaluating} and MBPP \citep{austin2021program}. More details of these LLMs and datasets can be found in Appendix~\ref{sec:Task-Specific Fine-Tuned LLMs and Datasets Details}.

\paragraph{\textbf{{The Benchmarks for evaluating Out-of-Distribution and Adversarial Robustness}}}
To assess OOD robustness, we evaluate math \& code, LM \& math, and LM \& code models using instruction following (LiveBench-Instruction), coding (LiveBench-Coding), and language comprehension (LiveBench-TypoFixing) category in LiveBench \citep{white2024livebench}, respectively. More details on OOD benchmarks are given in Appendix~\ref{sec:Out-of-Distribution Dataset Selection Details}.

We utilize the Adversarial Prompt Attacks module in PromptBench \citep{zhu2024promptbench} to assess the robustness of LLMs against adversarial prompts. Specifically, we employ three attack methods: DeepWordBug (character-level) \citep{gao2018black}, BERTAttack (word-level) \citep{li2020bert}, and StressTest (sentence-level) \citep{naik2018stress}. The evaluation is conducted on two datasets supported by PromptBench: SST2 (sentiment analysis) \citep{socher2013recursive} and CoLA (grammatical correctness) \citep{warstadt2019neural}. For more details on PromptBench and attack methods, please refer to Appendix~\ref{sec:Adversarial Robustness Evaluation Experiments Setting Details}.

\paragraph{\textbf{{Evaluation Metrics}}}
We calculate win rate for AlpacaEval and LiveBench-Instruction, zero-shot accuracy for GSM8K and MATH, pass@1 for HumanEval, MBPP and LiveBench-Coding, Matthews correlation coefficient (MCC) for CoLA, accuracy for SST2, and zero-shot accuracy for LiveBench-TypoFixing.

\paragraph{\textbf{{Implementation Details}}}
Unless otherwise specified, the details of the model merging experiments are consistent with \citet{yu2024language}. The hyperparameter $\alpha$ for M$^3$ is chosen from the range $[0.2, 0.4, 0.5, 1, 2, 3, 5]$. 
For a detailed description of the hyperparameter settings in model merging methods, please refer to Appendix~\ref{sec:Hyperparameter Setting Details in Model Merging Methods}. Additionally, all experiments are conducted on NVIDIA GeForce RTX 4090 GPUs.

\subsection{Merging Task-Specific Fine-Tuned LLMs}
\input{sft_llm_merging}
We integrate M$^3$ into three prominent model merging techniques: Average Merging, Task Arithmetic, and TIES-Merging. The performance of merging task-specific fine-tuned LLMs is presented in Table~\ref{table:sft_models_merging_results}.

From Table~\ref{table:sft_models_merging_results}, we obtain the following observations: 
1) M$^3$ generally enhances Average Merging, Task Arithmetic, and TIES-Merging when merging fine-tuned LLMs. For example, the improvements achieved by Average Merging with M$^3$ for Math \& Code are 7.43\% on GSM8K, 3.74\% on Math, and 11.0\% on MBPP. For LM \& Code, Average Merging with M$^3$ shows improvements of 7.31\% on AlpacaEval, 7.32\% on HumanEval, and 2.4\% on MBPP. Task Arithmetic with M$^3$ results in improvements of 2.0\% on AlpacaEval and 2.44\% on HumanEval for LM \& Code, and 10.4\% on MBPP for Math \& Code. TIES-Merging with M$^3$ achieves an improvement of 4.01\% for LM \& Math on GSM8K. For LM \& Code, TIES-Merging with M$^3$ shows significant improvements of 3.11\% on AlpacaEval, 25.61\% on HumanEval, and 30.8\% on MBPP.
2) Compared to Task Arithmetic, Average Merging and TIES-Merging tend to benefit more from M$^3$. This is because both Average Merging and TIES-Merging use a fixed merging ratio of 1/2, whereas Task Arithmetic allows the merging ratio to vary within the range \([0.5, 1.0]\). Consequently, the randomness introduced by M$^3$ in the merging ratio has a more pronounced impact on Average Merging. This further highlights the critical role of an effective merging ratio in determining the performance of the merged model.
3) \citet{yu2024language} has indicated that the suboptimal results of merging WizardMath-13B with llama-2-13b-code-alpaca are due to llama-2-13b-code-alpaca not being well fine-tuned for code generation. In this context, the proposed M$^3$ approach improves the pass@1 score on MBPP by 10.4\% for the merged model of WizardMath-13B and llama-2-13b-code-alpaca. The improvement demonstrates that when one of the fine-tuned models to be merged is not well fine-tuned for the specific task, M$^3$ can effectively unlock the potential of both models, maximizing the performance of the merged model. The M$^3$ approach helps mitigate the impact of suboptimal fine-tuning on model merging performance.

\subsection{Model Robustness}
\paragraph{\textbf{{Out-of-distribution robustness}}}
\begin{figure}[t]
  \centering  % 关键：让图片居中
  \includegraphics[width=0.9\columnwidth]{ood_all.pdf}
  \caption{Performance of merged models (Math \& Code, LM \& Math, and LM \& Code) using three model merging methods (Average Merging, Task Arithmetic, and TIES-Merging) on OOD datasets.}
  \label{fig:ood_all}
\end{figure}
To ensure that the evaluation datasets are as representative as possible of OOD data, we select datasets with sufficiently recent release dates and ensure they cover domains that fine-tuned LLMs have not been specifically trained on. Consequently, Math \& Code is evaluated on LiveBench-Instruction, LM \& Math on LiveBench-Coding, and LM \& Code on LiveBench-TypoFixing. The performance of the merged LLMs is shown in Figure~\ref{fig:ood_all}. 
%As shown in Figure~\ref{fig:ood_all}, M$^3$ consistently enhances the performance of the merged models Math \& Code, LM \& Math, and LM \& Code on OOD datasets. 
%Specifically, when employing Task Arithmetic with M$^3$, the Math \& Code model achieves a 1.9\% improvement in win rate on LiveBench-Instruction, the LM \& Math model achieves a 1.6\% improvement in pass@1 on LiveBench-Coding, and the LM \& Code model achieves a 6\% improvement in accuracy on LiveBench-TypoFixing. 
%When incorporating Average Merging with M$^3$, the Math \& Code model achieves a 1.5\% improvement in win rate on LiveBench-Instruction, the LM \& Math model achieves a 0.7\% improvement in pass@1 on LiveBench-Coding, and the LM \& Code model achieves a 4\% improvement in accuracy on LiveBench-TypoFixing.
%When using TIES-Merging with M$^3$, the Math \& Code model achieves a 1.1\% improvement in win rate on LiveBench-Instruction, the LM \& Math model achieves a 0.6\% improvement in pass@1 on LiveBench-Coding, and the LM \& Code model achieves a 14\% improvement in accuracy on LiveBench-TypoFixing.
As illustrated in Figure~\ref{fig:ood_all}, M$^3$ consistently enhances the performance of merged models—Math \& Code, LM \& Math, and LM \& Code—on OOD datasets. Specifically, when Task Arithmetic is combined with M$^3$, the Math \& Code model demonstrates a 1.9\% improvement in win rate on LiveBench-Instruction, the LM \& Math model achieves a 1.6\% increase in pass@1 on LiveBench-Coding, and the LM \& Code model shows a significant 6\% boost in accuracy on LiveBench-TypoFixing. Similarly, when Average Merging is combined with M$^3$, the Math \& Code model attains a 1.5\% improvement in win rate on LiveBench-Instruction, the LM \& Math model achieves a 0.7\% increase in pass@1 on LiveBench-Coding, and the LM \& Code model exhibits a 4\% enhancement in accuracy on LiveBench-TypoFixing. Finally, when TIES-Merging is applied alongside M$^3$, the Math \& Code model achieves a 1.1\% improvement in win rate on LiveBench-Instruction, the LM \& Math model records a 0.6\% increase in pass@1 on LiveBench-Coding, and the LM \& Code model demonstrates a remarkable 14\% improvement in accuracy on LiveBench-TypoFixing. These results underscore the robustness and versatility of M$^3$ in enhancing model performance across diverse merging strategies and OOD tasks.    

\paragraph{\textbf{{Adversarial robustness}}}
\input{adv_robustness}
We employ three Prompt Attack Methods supported by the promptbench codebase (DeepWordBug, BERTAttack, and StressTest)~\citep{zhu2024promptbench} to evaluate the adversarial robustness of three merged models (Math \& Code, LM \& Math, and LM \& Code) obtained through the task arithmetic method. 
To balance experimental effectiveness with computational efficiency, we randomly selected the positions of the three attacked words in the prompts when executing the DeepWordBug and BERTAttack attacks. 
Adversarial robustness is assessed using the Performance Drop Rate (PDR) \citep{zhu2023promptrobust}, where a lower PDR indicates stronger robustness. Further details on PDR can be found in Appendix~\ref{sec:Performance Drop Rate (PDR) for Adversarial Robustness}. The performance of the merged LLMs is shown in Table~\ref{table:attack_results_stresstest}.

As shown in Table~\ref{table:attack_results_stresstest}, M$^3$ improves the adversarial robustness of the merged models in most cases with the StressTest Prompt Attack Method. For example, with M$^3$, the PDR of Math \& Code decreased by 3.2\% on the SST2 dataset and by 92.12\% on the CoLA dataset, while the PDR of LM \& Code decreased by 30.36\% on SST2 and by 15.75\% on CoLA.
Furthermore, in most cases, M$^3$ not only improves the adversarial robustness of the merged models but also enhances their performance metrics (accuracy and MCC) on the SST2 and CoLA datasets. Specifically, with M$^3$, Math \& Code demonstrates a 28.56\% increase in accuracy on SST2 and a 26.18\% increase in MCC on CoLA, while LM \& Code achieves a 62.62\% increase in accuracy on SST2.
These results show that M$^3$ effectively enhances both the adversarial robustness and the performance of the merged models in sentiment analysis and grammar correctness tasks. Detailed experimental results for the remaining Prompt Attack Methods (DeepWordBug and BERTAttack) are presented in Appendix~\ref{sec:Results on the Adversarial robustness}.

% However, Model Mixup did not improve the adversarial robustness of LM \& Math on the SST2 dataset. This may be due to the need for further adjustment of the randomly generated model merging hyperparameter $\lambda$. This suggests that, in certain cases, careful tuning of $\alpha$ is required, along with multiple attempts under a fixed $\alpha$, to leverage randomness in finding an optimal $\lambda$ that maximizes adversarial robustness.

\subsection{Mixup Model Merge with DARE}
DARE is a model sparsification method proposed by \citep{yu2024language}, with a more detailed introduction provided in Appendix~\ref{sec:Detailed Introduction to DARE}. We combine M$^3$ and DARE with three model merging techniques, including Average Merging, Task Arithmetic, and TIES-Merging, to compare the effects of M$^3$ and DARE individually and explore their combined impact. The experimental results are presented in Table~\ref{table:sft_models_merging_results}. Additionally, in the DARE method, the drop rate hyperparameter is set to 0.2.

From Table~\ref{table:sft_models_merging_results}, we conclude that: 
1) In most cases, M$^3$ outperforms DARE, with a particularly significant advantage on certain datasets. For instance, the Math \& Code model achieves a pass@1 score of 9.8\% on the MBPP dataset when combined with DARE, while this score increases to 19\% when combined with M$^3$. This demonstrates that M$^3$ unlocks new potential in model merging by randomly generating merging ratios, leading to performance improvements that surpass those achieved by DARE.
2) Combining DARE and M$^3$ generally results in better model merging performance. For example, the LM \& Math and LM \& Code models, enhanced by TIES-Merging with M$^3$ and DARE, achieve the best performance on the test datasets. 
While only incorporating TIES-Merging with M$^3$ to these models, the enhanced models achieve the second best performance. This suggests that M$^3$ and DARE can complement each other. Moreover, in some cases, M$^3$ alone can deliver the best results, while DARE alone only achieves the best performance in very few cases, further demonstrating the superiority of M$^3$.
%And which get from TIES-Merging with only Model Mixup achieves the second best performance.This suggests that Model Mixup and DARE can complement each other to achieve enhanced model merging outcomes. 
%Moreover, in some cases, Model Mixup alone can deliver the best results, while DARE alone only achieves the best performance in very few cases, further demonstrating the superiority of Model Mixup.

\section{Conclusion}
Inspired by the mixup method and the Vicinal Risk Minimization (VRM) principle, we propose Mixup Model Merge (M$^3$), a novel approach for merging fine-tuned LLMs by introducing randomness into the parameters linear interpolation process. Unlike traditional methods such as average merging and task arithmetic, M$^3$ leverages a Beta distribution to dynamically adjust the merging ratio, enabling more flexible exploration of the parameter space. Experimental results demonstrate that M$^3$ not only significantly enhances the performance of the merged model across various tasks but also improves its OOD and adversarial robustness. Furthermore, when combined with sparsification techniques such as DARE, our approach achieves even more favorable model merging outcomes. In summary, M$^3$ is a simple yet powerful technique that requires minimal computational resources. By merely adjusting the merging ratio, it produces a merged model with enhanced task-specific capabilities and robustness. This exciting discovery paves the way for further research into optimizing merging ratio selection in model merging processes.

\section{Limitations}
There are several limitations of the M$^3$ method: While it performs well for merging two models, (1) its scalability when merging a larger number of models, especially those with significant differences, remains uncertain. Additionally, (2) due to the inherent randomness in the merging process, multiple attempts may be required to achieve a merged model that meets expectations. This unpredictability can lead to increased computational costs, particularly in large-scale applications, resulting in a significant rise in resource consumption. Finally, (3) Our method may also be extended to a wider range of applications, such as merging fine-tuned models with Reinforcement Learning with Human Feedback (RLHF) \citep{ouyang2022training} models to reduce the alignment tax \citep{finepaft}.

%\section*{Acknowledgments}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Detailed Experimental Settings}
\subsection{Task-Specific Fine-Tuned LLMs and Datasets Details}
\label{sec:Task-Specific Fine-Tuned LLMs and Datasets Details}
We conduct model merging experiments using three task-specific LLMs fine-tuned from Llama-2-13b:
\begin{itemize}
\item \textbf{WizardLM-13B} is an instruction-following model based on Llama-2-13b, designed to improve open-domain instruction-following. Using the Evol-Instruct method \citep{xu2024wizardlm}, it generates high-complexity instruction data to reduce human annotation and enhance generalization. The model undergoes supervised fine-tuning with AI-generated data, followed by refinement via RLHF. Evaluation results show that Evol-Instruct-generated instructions outperform human-written ones, and WizardLM-13B surpasses ChatGPT in high-complexity tasks. In GPT-4 automated evaluation, it achieves over 90\% of ChatGPT's performance in 17 out of 29 tasks, demonstrating the effectiveness of AI-evolved instruction fine-tuning \citep{xu2024wizardlm}.

\item \textbf{WizardMath-13B}, optimized from Llama-2-13b, is designed for mathematical reasoning and enhances Chain-of-Thought (CoT) \citep{wei2022chain} capabilities. It uses Reinforcement Learning from Evol-Instruct Feedback to evolve math tasks and improve reasoning. Trained on GSM8K and MATH datasets, it excels in both basic and advanced math problems. In evaluations, WizardMath-Mistral 7B outperforms all open-source models with fewer training data, while WizardMath 70B surpasses GPT-3.5-Turbo, Claude 2, and even early GPT-4 versions in mathematical reasoning tasks.

\item \textbf{llama-2-13b-code-alpaca} is a code generation model fine-tuned from Llama-2-13b, designed to enhance code understanding and generation. It follows the same training approach as Stanford Alpaca \citep{alpaca} but focuses on code-related tasks. The model is fine-tuned with 20K instruction-following code samples generated using the Self-Instruct method \citep{wang2022self}. However, as it has not undergone safety fine-tuning, caution is required when using it in production environments.
\end{itemize}

We use one dataset to evaluate the instruction-following task:
\begin{itemize}
\item \textbf{AlpacaEval} \citep{li2023alpacaeval} is an LLM-based automated evaluation metric that assesses model performance by testing on a fixed set of 805 instructions and computing the win rate of the evaluated model against a baseline. The evaluation process involves an LLM-based evaluator that compares the responses and determines the probability of preferring the evaluated model.
In this paper, we use AlpacaEval 2.0 \citep{dubois2024length}. To reduce costs, we use chatgpt\_fn for evaluation.
\end{itemize}

We use two dataset to evaluate the mathematical reasoning task:
\begin{itemize}
\item \textbf{GSM8K} is a dataset of 8.5K high-quality, linguistically diverse grade school math word problems, designed to evaluate the multi-step mathematical reasoning abilities of large language models. It consists of 7.5K training problems and 1K test problems. In this paper, we use the 1K test set for evaluation \citep{cobbe2021training}.

\item \textbf{MATH} is a dataset containing 12,500 competition-level math problems, designed to evaluate and enhance the problem-solving abilities of machine learning models. It consists of 7,500 training problems and 5,000 test problems. We use the 5,000 test set for evaluation \citep{hendrycks2021measuring}. 
\end{itemize}

We used two dataset to evaluate the code generation task:
\begin{itemize}
\item \textbf{HumanEval} is a dataset consisting of 164 hand-written programming problems, designed to evaluate the functional correctness of code generation models. Each problem includes a function signature, docstring, function body, and unit tests. The dataset tests models' language comprehension, reasoning, and algorithmic abilities \citep{chen2021evaluating}.

\item \textbf{MBPP} is a dataset containing 974 programming problems designed to evaluate a model's ability to synthesize Python programs from natural language descriptions. The problems range from basic numerical operations to more complex tasks involving list and string processing. The test set consists of 500 problems, which are used for evaluation in this paper \citep{austin2021program}.
\end{itemize}

\subsection{Hyperparameter Setting Details in Model Merging Methods}
\label{sec:Hyperparameter Setting Details in Model Merging Methods}
Table~\ref{table:hyperparameter_search} presents the hyperparameter search ranges for the model merging methods. For Task Arithmetic and TIES-Merging, the scaling terms are selected from $[0.5, 1.0]$, while in TIES-Merging, the retain ratio for the largest-magnitude parameters is chosen from $[0.5, 0.7, 0.9]$. In contrast, the Average Merging method does not require any hyperparameters.
% We use the experimental results obtained with the best hyperparameters searched within this range as the baseline for the model merging experiments, i.e., the cases without M$^3$ and DARE in Table~\ref{table:sft_models_merging_results}.   
\input{hyperparameters_search_range}

Table~\ref{table:optimal hyperparameter} presents the optimal hyperparameter settings for the TIES-Merging model merging method obtained through searching. These settings are further applied to model merging experiments involving M$^3$ and DARE.
% In contrast, the hyperparameters from the original Task Arithmetic method become ineffective when applied to M$^3$.
\input{ties_hyper}

\subsection{Out-of-Distribution Dataset Selection Details}
\label{sec:Out-of-Distribution Dataset Selection Details}
LiveBench \citep{white2024livebench} is a dynamic benchmark for large language models, featuring frequently updated questions and diverse tasks. To assess OOD robustness, we evaluate math \& code, LM \& math, and LM \& code models using instruction following (LiveBench-Instruction), coding (LiveBench-Coding), and language comprehension (LiveBench-TypoFixing) category in LiveBench, respectively, deliberately avoiding the fine-tuning domains of the merged fine-tuned models.
These tasks were released after November 2023, whereas WizardLM-13B, WizardMath-13B, and llama-2-13b-code-alpaca were all introduced earlier. Furthermore, their shared Llama-2-13b backbone was trained on data only up to July 2023. Consequently, these factors collectively ensure that the evaluation remains OOD in the temporal dimension.

When assessing the OOD robustness of LM \& Code using the Language Comprehension category in LiveBench, only the typo-fixing task is considered. This decision is based on the fact that LiveBench is highly challenging, and the merged model performs poorly on other tasks in this category, with accuracy close to zero, rendering the evaluation results inconclusive and uninformative.

Finally, we acknowledge the limitations of these datasets. For large models like Llama-2-13b, identifying truly OOD datasets is difficult, as their training data likely covers similar distributions. These datasets are better described as "out-of-example", representing instances not explicitly seen during training. As discussed in \citep{wang2023robustness}, distribution shifts can occur across domains and time. While Llama-2-13b may have been trained on datasets for tasks like instruction-following, coding, and language comprehension, the datasets we selected remain valuable for OOD evaluation by capturing temporal shifts, providing insights into robustness over time.

\subsection{Adversarial Robustness Evaluation Experiments Setting Details}
\label{sec:Adversarial Robustness Evaluation Experiments Setting Details}
PromptBench \citep{zhu2024promptbench} is a unified library designed for evaluating LLMs, providing a standardized and extensible framework. It includes several key components such as prompt construction, prompt engineering, dataset and model loading, adversarial prompt attacks, dynamic evaluation protocols, and analysis tools. 

We use the Adversarial Prompt Attacks module in PromptBench aims to evaluate the robustness of LLMs against adversarial prompts. We employ three methods to perform adversarial attacks on prompts to evaluate the adversarial robustness of the merged models: DeepWordBug \citep{gao2018black}, BERTAttack \citep{li2020bert}, and StressTest \citep{naik2018stress}, representing Character-level, Word-level, and Sentence-level attacks, respectively.
\begin{itemize}
\item \textbf{DeepWordBug} introduces subtle character-level perturbations (e.g., adding, deleting, or replacing characters) to words in text to deceive language models. It aims to evaluate a model's robustness against small typographical errors that may alter the model's performance without being easily detected.
\item \textbf{BERTAttack} manipulates text at the word level by replacing words with contextually similar synonyms to mislead large language models. This method tests the model's ability to maintain accuracy despite small lexical changes that might alter the meaning of the input.
\item \textbf{StressTest} appends irrelevant or redundant sentences to the end of a prompt to distract and confuse language models. It assesses the model’s ability to handle extraneous information and maintain accuracy when faced with unnecessary distractions.
\end{itemize}

\begin{figure}[t]
  \centering
  % 第一张图像
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{ties1.pdf}
    \captionsetup{skip=2pt}  % 减小这一图像与caption的间距
    \caption{The operational steps of TIES-Merging.}
    \label{fig:TIES-Merging}
  \end{subfigure}

  \vspace{0.3cm} % 添加垂直间距，可根据需要调整

  % 第二张图像
  \begin{subfigure}[b]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{ties2.pdf}
    \captionsetup{skip=2pt}  % 减小这一图像与caption的间距
    \caption{After introducing M$^3$, the Disjoint Merge step in the TIES-Merging procedure.}
    \label{fig:TIES-Merging+M$^3$}
  \end{subfigure}
 
  \vspace{0.1cm} % 添加垂直间距，可根据需要调整
  
  \caption{The difference between M$^3$ and the original TIES-Merging is that, in the Disjoint Merge step, when two task vectors are retained for a given parameter, the mean of the task vectors is replaced by a random linear interpolation, while the other operations remain unchanged.}
  \label{fig:ties}
\end{figure}
The evaluation is conducted on the Sentiment Analysis dataset (SST2 \citep{socher2013recursive}) and the Grammar Correctness dataset (CoLA \citep{warstadt2019neural}):
\begin{itemize}
\item \textbf{SST2} \citep{socher2013recursive}: A sentiment analysis dataset designed to assess whether a given sentence conveys a positive or negative sentiment.
\item \textbf{CoLA} \citep{warstadt2019neural}: A dataset for grammar correctness, where the model must determine whether a sentence is grammatically acceptable.
\end{itemize}

\section{Additional Experimental Results on Adversarial Robustness}
\label{sec:Results on the Adversarial robustness}
\input{adv_robustness2}
\input{adv_robustness3}
All the merged models are obtained using the Task Arithmetic method.
Table~\ref{table:attack_results_deepwordbug} presents the detailed experimental results of the adversarial robustness of merged models on the SST2 and CoLA datasets applying the DeepWordBug prompt attack method.
Table~\ref{table:attack_results_bertattack} presents the detailed experimental results of the adversarial robustness of merged models on the SST2 and CoLA datasets applying the BERTAttack prompt attack method.

\section{Integrating M$^3$ into the TIES-Merging Model Merging Method}
\label{sec:Integrating M$^3$ into the TIES-Merging Model Merging Method}
% \input{latex/Pseudocode/code_ties_mixup}
Figure~\ref{fig:ties} shows the specific implementation approach to incorporating M$^3$ into TIES-Merging. After the steps of trimming parameters with lower magnitudes and resolving sign disagreements, the two models to be merged are denoted as \( M_1 \) and \( M_2 \). During the M$^3$ process, only the parameters that are preserved in both \( M_1 \) and \( M_2 \) are interpolated according to the model merging hyperparameter \( \lambda_m \) to obtain the merged parameters. For parameters that are preserved in only one of the models, no interpolation is performed, and the original value from the preserved model is retained in the merged model.

\section{Performance Drop Rate (PDR) for Adversarial Robustness}
\label{sec:Performance Drop Rate (PDR) for Adversarial Robustness}
The adversarial robustness is evaluated using the Performance Drop Rate (PDR) \citep{zhu2023promptrobust}, which is defined as follows:
\begin{align}
\text{PDR} = \frac{\text{Metric}_{\text{no attack}} - \text{Metric}_{\text{attack}}}{\text{Metric}_{\text{no attack}}},
\end{align}
\noindent where $\text{Metric}_{\text{no attack}}$ denotes the performance metric without any prompt attack, and $\text{Metric}_{\text{attack}}$ represents the performance metric under the prompt attack. A smaller PDR indicates stronger adversarial defense against prompt attacks, implying better adversarial robustness. 

\section{Detailed Introduction to DARE}
\label{sec:Detailed Introduction to DARE}
DARE (Drop And REscale) \citep{yu2024language} is a model sparsification method designed to reduce the redundancy of delta parameters in fine-tuned models while preserving their task-specific capabilities. In SFT, model parameters are optimized to unlock abilities for specific tasks, with the difference between fine-tuned and pre-trained parameters referred to as delta parameters.

However, studies have shown that delta parameters are often highly redundant. DARE addresses this redundancy by randomly dropping a proportion \( p \) of delta parameters (referred to as the drop rate) and rescaling the remaining ones by a factor of \( 1/(1 - p) \). This simple yet effective approach enables DARE to eliminate up to 99\% of delta parameters with minimal impact on model performance, particularly in large-scale models, and it can be applied using only CPUs.

Beyond sparsification, DARE serves as a versatile plug-in for merging multiple homologous fine-tuned models (fine-tuned from the same base model) by reducing parameter interference. When combined with existing model merging techniques such as Average Merging, Task Arithmetic, and TIES-Merging, DARE facilitates the fusion of models while retaining or even enhancing task performance across multiple benchmarks. This effect is especially pronounced in decoder-based LMs, where DARE boosts task generalization. 

Experiments on AlpacaEval, GSM8K, and MBPP reveal that the merged LM has the potential to outperform any individual source LM, presenting a significant new discovery. Notably, the 7B model obtained through DARE merging, SuperMario v2, ranks first among models of the same scale on the Open LLM Leaderboard \citep{beeching2023open}. These improvements were achieved without the need for retraining, positioning DARE as an efficient and resource-friendly solution for model merging.

\end{document}
