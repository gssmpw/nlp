\section{Related Works}
\paragraph{\textbf{{Model Merging}}}
Model merging is a technique that integrates the parameters of multiple models to create a unified model with enhanced or diverse capabilities ____. 
% This process leverages the unique strengths of each model to enhance overall performance without requiring original training data or costly computations.
%Model soups ____ can improve accuracy and robustness by averaging the parameters of independently fine-tuned models with different hyperparameters. 
Task arithmetic ____ leverages task vectors for model merging through arithmetic operations, incorporating a predefined scaling term to weight the contribution of different models.
Fisher Merging ____ performs parameter fusion by applying weights derived from the Fisher information matrix ____, resulting in more precise parameter integration.
%Regression Mean (RegMean) ____ minimizes the $\ell_2$ distance between the merged model and individual fine-tuned models, providing a computationally efficient closed-form solution. 
TIES-Merging ____ addresses task conflicts by removing low-magnitude parameters, resolving sign disagreements, and merging only the parameters that align with the final agreed-upon sign.
In ____, it is found that LLMs can enhance their capabilities through model merging. Additionally, it introduces DARE, a method for sparsifying the delta parameters of the model ____, significantly improving the performance of various model merging techniques. 
% ____ use model averaging to alleviate alignment tax in Reinforcement Learning with Human Feedback (RLHF) %____ on NLP tasks and introduce Adaptive Model Averaging (AMA), which optimizes the averaging ratios of different model layers to maximize alignment reward.
%In this paper, we propose a simple yet effective model merging method, Model Mixup, which randomly generates contribution ratios between two fine-tuned models, resulting in a merged model with improved performance by exploring a broader parameter space.

\paragraph{\textbf{{Mixup}}}
%____ proposed a novel data augmentation method ____, Mixup, in 2018, aiming to
%Traditional machine learning methods typically train models through ERM, which optimizes model parameters by minimizing the loss function on the training data. Under this approach, the model makes explicit predictions for each training sample, which often leads to overfitting, especially when the training data is limited or noisy.
Mixup is proposed to enhance the generalization ability of deep learning models by surpassing traditional Empirical Risk Minimization (ERM)____. It is a simple, data-agnostic augmentation technique that trains models using virtual examples created by linearly interpolating pairs of random examples and their corresponding labels. Rooted in the Vicinal Risk Minimization (VRM) principle ____, this approach improves generalization across a variety of datasets ____, and helps reduce overfitting, sensitivity to adversarial examples, and training instability, all with minimal computational cost. Given two samples $(x_i, y_i)$ and $(x_j, y_j)$, Mixup generates a new sample using the following formulas:
\begin{align}
\begin{aligned}
\tilde{x} &= \lambda x_i + (1 - \lambda) x_j \\
\tilde{y} &= \lambda y_i + (1 - \lambda) y_j
\end{aligned}
\end{align}
$\tilde{x}$ and $\tilde{y}$ represent the generated synthetic sample and its label, respectively, with $\lambda$ determining their interpolation ratio, typically ranging from 0 to 1. Here, $\lambda$ is a hyperparameter sampled from a Beta distribution, i.e., $\lambda \sim \text{Beta}(\alpha, \alpha)$, where $\alpha$ controls the strength of the interpolation between feature-target pairs.
Inspired by Mixup, we propose a novel model merging method, M$^3$, which generates the parameters of the merged model by performing linear interpolation between the parameters of two fine-tuned LLMs, with the interpolation ratio being random.