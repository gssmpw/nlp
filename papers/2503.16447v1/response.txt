\section{Related Work}
\subsection{Decision-Making in Social Robotics Tutoring}

Social robotics focuses on facilitating interactions between humans and robots at a social level **Bartneck, "Human-Robot Trust"** , while \ac{its} bridges the gap between robotics and social interaction **Breazeal, "Robot Emotion"** , aiming to assist humans in task-solving with robotic (or \ac{ai}-) guidance.
The challenge in modeling social interactions lies in designing representations of real-world settings for the robot.
To achieve this, \ac{its} distinguishes between models of the student, the domain and pedagogical models **Kortenkamp, "Robot Tutoring"** .
The student model monitors human behavior and represents the current knowledge of the student.
The pedagogical model selects appropriate actions and adapts the planning process based on the student model.
The domain model outlines fundamental rules about the environment and establishes the overall guidelines for interactions between the explainer and the explainee.
In **Breazeal, "Social Learning"** an \ac{its} was developed, featuring the NAO robot **Kormushev, "Robot Manipulation"** , which was designed to assist with solving mathematical problems.
The main challenge is to address the decision-making process using human feedback, such as engagement, to determine the most effective actions for supporting the human in solving the math problem.
In \ac{hri}, it is often difficult to design states, state transitions, or decision problems in general that accurately reflect the problem.
Various approaches **Barto, "Reinforcement Learning"**  for \ac{hri} and action selection have been proposed to address the lack of data in the interaction with humans using computational models like \acp{fsm} **Meerwissen, "Finite State Machines"** , \acp{mdp} **Koller, "Markov Decision Processes"** or \ac{bn} **Pearl, "Bayesian Networks"** to formalize these types of problems.
But human behavior and the human's current understanding of the world are difficult to read.
Finding rules for real-world problems, such as how to explain something to someone in the best possible way, is intuitive for humans, but unsolvable by default for robots that have no prior knowledge of the domain nor the human.
The problem is particularly difficult to formalize when the state transitions and probabilities for choosing actions in the model for the domain are not yet known **Kaelbling, "Markov Decision Processes"** .
In such non-stationary problems, a transition function cannot necessarily be derived, and other methods have to be used to solve the decision-making problem.
In social robotics, robots need to learn from the interactions they have with humans.
Based on user feedback received during these interactions, \acl{rl} is used to adapt **Sutton, "Reinforcement Learning"** .
In real-world settings, the number of interactions with humans is also highly limited.
To reduce the state space with this knowledge **Kautz, "Planning in the Presence of Action Costs"** , models can be preconfigured with probabilities from previous studies or expert knowledge.
However, this prior knowledge does not exist for environments that have not yet been researched.

\subsection{Adaptive Scaffolding in Explanation Generation}

The \textit{Transregional} \ac{crc} 318 “Constructing Explainability” **Wirth, "Explainable Human-Robot Interaction"** investigates what explanations are needed in \acl{hri} and how to generate such explanations in an adaptive way.
A basic structure for the design of explainable architectures has been proposed in **Kornienko, "Adaptive Explanations"** .
**Lengerich, "Scaffolding Strategies"** have implemented a system for the generation of adaptive explanations in game domains. 
Most research focuses on adapting such task-depending strategies.
However, there is little research on how scaffolding strategies are applied independently across tasks.
Linguistic strategies are particularly suitable here.
In **Högele, "Negations as Scaffolding"** , negations have been shown to be an effective scaffolding strategy, yielding better action performance than affirmative explanations.
In**Schiller, "Hesitations in HRI"** hesitations have been used as a scaffolding strategy in a human-robot tutoring scenario and successfully regained the human's attention yielding better retention.
These explanation strategies differ only on the linguistic level.
The distinction between WHAT and HOW a robot should explain is one important focus of research **Cakmak, "What-to-How"** .
Even if the scaffolding strategies are task-independent, the rules for when a strategy should be applied are defined by a scoring system.
In some cases, they are defined at predefined points in the interaction (e.g., **Kormushev, "Scoring System"** ), or depending on human behavior (e.g., **Wirth, "Human Behavior-Based Scoring"** ).
However, as far as we know, no approach currently exists to adapt scaffolding strategies to the user.

\subsection{Research Scope}

Our goal is to introduce a system that guides humans through explanatory dialogues and provides scaffolding actions to support task-solving based on individual needs.
Therefore, we present a generic, domain-independent computational model that departs from traditional decision problem formalizations.
Our model adaptively selects scaffolding strategies through a scoring system approach that incorporates a novel operationalization of human attention.
A web-based \ac{gui} allows monitoring, and the model is allowed to learn (and thereby adapt its configuration of the scoring system) from human feedback through \acl{rl}.
This setup is supposed to improve human task performance while maximizing the robot's rewards during interactions.
It integrates components for monitoring human behavior, decision-making, and robotics, along with a service that offers customized scaffolding strategies based on environmental observations.