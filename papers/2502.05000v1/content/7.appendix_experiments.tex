\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}

\section{Experiment Details}

\subsection{Dataset Details}\label{appendix:datasets}
\subsubsection{Graph Classification Datasets}
We use the following five real-world datasets to evaluate the robustness of \ModelName on the graph classification task. All the dataset is obtained from PyG TUDataset\footnote{\url{https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.TUDataset.html}}
\begin{itemize}[leftmargin=*]
\item \textbf{MUTAG}~\cite{ivanov2019understanding} contains graphs of small molecules, with nodes as atoms and edges representing chemical bonds. Labels indicate molecular toxicity. 
\item \textbf{IMDB-BINARY}~\cite{ivanov2019understanding} consists of movie-related graphs, where nodes are individuals, and edges represent relationships. Labels classify the movie as Action or Romance. 
\item \textbf{IMDB-MULTI}~\cite{ivanov2019understanding} is similar, but edges connect nodes across three genres: Comedy, Romance, and Sci-Fi, with corresponding labels. 
\item \textbf{REDDIT-BINARY}~\cite{ivanov2019understanding} features user discussion graphs from Reddit, with edges indicating responses. Graphs are labeled as either question-answer or discussion-based. 
\item \textbf{COLLAB}~\cite{ivanov2019understanding} consists of collaboration networks, where nodes are researchers, and edges represent collaborations. Labels identify the research field: High Energy Physics, Condensed Matter Physics, or Astro Physics.
\end{itemize}
Statistics of the graph classification datasets are in Table~\ref{table:dataset_g}.

\subsubsection{Node Classification Datasets}
We use the following four real-world datasets to evaluate the robustness of \ModelName~ on the node classification task. 
\begin{itemize}[leftmargin=*]
\item \textbf{Cora}~\cite{yang2016revisiting} is a citation network where nodes represent publications, with binary word vectors as features. Edges indicate citation relationships. 
\item \textbf{CiteSeer}~\cite{yang2016revisiting} is another citation network, similar to Cora, with nodes representing research papers and edges denoting citation links. 
\item \textbf{PolBlogs}~\cite{adamic2005political} is a political blog network, where edges are hyperlinks between blogs. Nodes are labeled by political affiliation: liberal or conservative. 
\item \textbf{Photo}~\cite{shchur2018pitfalls} is a co-purchase network from Amazon, where nodes are products, edges represent frequent co-purchases, and features are bag-of-words from product reviews. Class labels indicate product categories.
\end{itemize}
The statistics of the graph classification datasets are given in Table~\ref{table:dataset_n}. Cora and CiteSeer is obtained from PyG Planetoid\footnote{\url{https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html\#torch_geometric.datasets.Planetoid}}. PolBlogs is obtained from PyG PolBlogs\footnote{\url{https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.PolBlogs.html\#torch_geometric.datasets.PolBlogs}}. Photo is obtained from PyG Amazon\footnote{\url{https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Amazon.html\#torch_geometric.datasets.Amazon}}.
\input{table/dataset_g}
\input{table/dataset_n}




\subsection{Description of Baselines}\label{appendix:baselines}

\subsubsection{Graph Classification Baselines}
\begin{itemize}[leftmargin=*]
\item \textbf{IDGL}~\cite{chen2020iterative} iteratively refines graph structures and embeddings for robust learning in noisy graphs.
\item \textbf{GraphCL}~\cite{you2020graph} maximizes agreement between augmented graph views via contrastive loss.
\item \textbf{VIB-GSL}~\cite{sun2022graph} applies the Information Bottleneck to learn task-relevant graph structures.
\item \textbf{G-Mixup}~\cite{han2022g} generates synthetic graphs by mixing graphons to enhance generalization.
\item \textbf{SEP}~\cite{wu2022structural} minimizes structural entropy for optimized graph pooling.
\item \textbf{MGRL}~\cite{ma2023multi} addresses semantic bias and confidence collapse with instance-view consistency and class-view learning.
\item \textbf{SCGCN}~\cite{zhao2024graph} ensures robustness with temporal and perturbation stability.
\item \textbf{HGP-SL}~\cite{zhang2019hierarchical} combines pooling and structure learning to preserve key substructures.
\item \textbf{SubGattPool}~\cite{bandyopadhyay2020hierarchically} uses subgraph attention and hierarchical pooling for robust classification.
\item \textbf{DIR}~\cite{wu2022discovering} identifies stable causal structures via interventional separation.
\item \textbf{VGIB}~\cite{yu2022improving} filters irrelevant nodes through noise injection for improved subgraph recognition.
\end{itemize}

In our implementation, since the authors of MGRL and SubGattPool have not provided open access to their code, we reproduced their methods based on the descriptions in their papers. The implementations of other baselines can be found at the following URLs:
\begin{itemize}[leftmargin=*]
    \item \textbf{IDGL}: \url{https://github.com/hugochan/IDGL}
    \item \textbf{GraphCL}: \url{https://github.com/Shen-Lab/GraphCL}
    \item \textbf{VIB-GSL}: \url{https://github.com/VIB-GSL/VIB-GSL}
    \item \textbf{G-Mixup}: \url{https://github.com/ahxt/g-mixup}
    \item \textbf{SEP}: \url{https://github.com/Wu-Junran/SEP}
    \item \textbf{SCGCN}: \url{https://github.com/DataLab-atom/temp}
    \item \textbf{HGP-SL}: \url{https://github.com/cszhangzhen/HGP-SL}
    \item \textbf{DIR}: \url{https://github.com/Wuyxin/DIR-GNN}
    \item \textbf{VGIB}: \url{https://github.com/Samyu0304/VGIB}
\end{itemize}



\subsubsection{Node Classification Baselines.}
\begin{itemize}[leftmargin=*]
\item \textbf{GSR}~\cite{zhao2023self} refines graph structures via a pretrain-finetune pipeline using multi-view contrastive learning to estimate and adjust edge probabilities.
\item \textbf{GARNET}~\cite{deng2022garnet} improves GNN robustness by using spectral embedding and probabilistic models to filter adversarial edges.
\item \textbf{GUARD}~\cite{li2023guard} creates a universal defensive patch to remove adversarial edges, providing node-agnostic, scalable protection.
\item \textbf{SVDGCN}~\cite{entezari2020all} applies Truncated SVD preprocessing with a two-layer GCN.
\item \textbf{JaccardGCN}~\cite{wu2019adversarial} drops dissimilar edges in the graph before training a GCN.
\item \textbf{RGCN}~\cite{zhu2019robust} models node features as Gaussian distributions, using variance-based attention for robustness.
\item \textbf{Median-GCN}~\cite{chen2021understanding} improves robustness by using median aggregation instead of the weighted mean.
\item \textbf{GNNGuard}~\cite{zhang2020gnnguard} defends GNNs by pruning suspicious edges through neighbor importance estimation.
\item \textbf{SoftMedian}~\cite{geisler2021robustness} filters outliers by applying a weighted mean based on distance from the median to defend against adversarial noise.
\item \textbf{ElasticGNN}~\cite{liu2021elastic} combines 1-based and 2-based smoothing, balancing global and local smoothness for better defense.
\item \textbf{GraphADV}~\cite{xu2019topology} boosts robustness through adversarial training with gradient-based topology attacks.


\end{itemize}
The implementations of these node classification baselines can be found at the following URLs:
\begin{itemize}[leftmargin=*]
    \item \textbf{GSR}: \url{https://github.com/andyjzhao/WSDM23-GSR}
    \item \textbf{GARNET}: \url{https://github.com/cornell-zhang/GARNET}
    \item \textbf{GUARD}: \url{https://github.com/EdisonLeeeee/GUARD}
    \item \textbf{SVD}: \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/gcn\_preprocess.py}
    \item \textbf{Jaccard}: \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/gcn\_preprocess.py}
    \item \textbf{RGCN}: \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/r\_gcn.py}
    \item \textbf{Median-GCN}:  \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/median\_gcn.py}
    \item \textbf{GNNGuard}:  \url{https://github.com/mims-harvard/GNNGuard}
    \item \textbf{SoftMedian}:  \url{https://github.com/sigeisler/robustness\_of\_gnns\_at\_scale}
    \item \textbf{ElasticGCN}:  \url{https://github.com/lxiaorui/ElasticGNN}
    \item \textbf{GraphADT}:  \url{https://github.com/KaidiXu/GCN\_ADV\_Train}
\end{itemize}






\subsection{Attack Setting Details}\label{appendix:attacks}

\subsubsection{Graph Classification Attack Settings}
For graph classification attacks, we use the following three attack methods:
\begin{itemize}[leftmargin=*]
    \item \textbf{GradArgmax}~\cite{dai2018adversarial} greedily selects edges for perturbation based on the gradient of each node pair.
    \item \textbf{PR-BCD}~\cite{geisler2021robustness} performs sparsity-aware first-order optimization attacks using randomized block coordinate descent, enabling efficient attacks on large-scale graphs.
    \item \textbf{CAMA-Subgraph}~\cite{wang2023revisiting} enhances adversarial attacks in graph classification by targeting critical subgraphs. It identifies top-ranked nodes via a Class Activation Mapping (CAM) framework and perturbs edges within these subgraphs to craft more precise adversarial examples.
\end{itemize}
Note that, as the authors of CAMA-Subgraph have not provided open access to their code, we reproduced their method based on the descriptions in their papers. The reproduced code is available in our repository. For the implementation of other baselines, we used code from the following URLs:
\begin{itemize}[leftmargin=*]
    \item \textbf{GradArgmax}: \url{https://github.com/xingchenwan/grabnel/blob/main/src/attack/grad_arg_max.py}
    \item \textbf{PR-BCD}: \url{https://github.com/pyg-team/pytorch\_geometric/blob/master/torch\_geometric/contrib/nn/models/rbcd\_attack.py}
\end{itemize}

For all graphs in the dataset, we set 20\% of the total number of edges as the attack budget. We use a two-layer GCN followed by a mean pooling layer and a linear layer as the surrogate model, which shares the same architecture as the classifier for all baselines.



\subsubsection{Node Classification Attack Settings}
For targeted node classification attacks, we use the following three attack methods:
\begin{itemize}[leftmargin=*]
\item \textbf{PR-BCD}~\cite{geisler2021robustness} performs the same attack as in graph classification but targets a different task.
\item \textbf{Nettack}~\cite{zugner2018adversarial} incrementally modifies key edges or features to maximize the difference in log probabilities between correct and incorrect classes, while preserving the graph's core properties, such as the degree distribution.
\item \textbf{GR-BCD}~\cite{geisler2021robustness} is similar to PR-BCD but flips edges greedily based on the gradient concerning the adjacency matrix.
\end{itemize}
The implements of these attacks can be found from the following URLs:
\begin{itemize}[leftmargin=*]
\item \textbf{PR-BCD}: \url{https://github.com/pyg-team/pytorch\_geometric/blob/master/torch\_geometric/contrib/nn/models/rbcd\_attack.py}
\item \textbf{Nettack}: \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/targeted\_attack/nettack.py}
\item \textbf{GR-BCD}: \url{https://github.com/pyg-team/pytorch\_geometric/blob/master/torch\_geometric/contrib/nn/models/rbcd\_attack.py}
\end{itemize}

For all datasets, we set 10\% of the total number of edges as the attack budget for both PR-BCD and GR-BCD. For Nettack, following the settings from deeprobust~\cite{li2020deeprobust}, we select 40 nodes from the test set to attack with a budget of 5 edges and evaluate accuracy. These 40 nodes include 1) 10 nodes with the highest classification margin (clearly correctly classified), 2) 10 nodes with the lowest margin (still correctly classified), and 3) 20 randomly selected nodes.

For non-targeted node classification attacks, we use the following three attack methods:
\begin{itemize}[leftmargin=*]
\item \textbf{MinMax}~\cite{li2020deeprobust} generates adversarial perturbations by solving a min-max optimization. The outer step finds optimal edge perturbations, while the inner step retrains the GNN to adapt.
\item \textbf{DICE}~\cite{zugner2018metalearningu} removes edges between same-class nodes and inserts edges between nodes of different classes.
\item \textbf{Random}~\cite{li2020deeprobust} randomly adds edges to the input graph.
\end{itemize}
The implements of theses attacks can be found in the following URLs:
\begin{itemize}[leftmargin=*]
\item \textbf{MinMax}: \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/global\_attack/topology\_attack.py}
\item \textbf{DICE}: \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/global\_attack/dice.py}
\item \textbf{Random}: \url{https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/global\_attack/random\_attack.py}
\end{itemize}
For MinMax, DICE, and Random attacks, we set the attack budget to 10\%, 20\%, and 30\% of the total number of edges, respectively, for all datasets.





\subsection{Implement Details}\label{appendix:implements}
For graph classification, we randomly split the dataset into 8:1:1 for training, validation, and testing. For datasets without node features, we use normalized node degrees as features, following the approach in~\cite{sun2022graph}. The testing set is subjected to adversarial attacks. Our classifier consists of a two-layer Graph Convolutional Network (GCN) followed by a mean pooling layer and a linear layer. Both the diffusion model of \ModelName\ and the classifier are trained on the training graphs, with their performance evaluated on the attacked testing set.
For node classification, we use the transductive setting with a 1:1:8 random split for training, validation, and testing. The classifier comprises a two-layer GCN followed by a linear layer. During training, we sample batches of subgraphs, consistent with~\cite{li2023graphmaker}, and apply adversarial attacks at test time. A learning rate of 0.0003 is used for all datasets. We perform 10 random runs for each method and report the average results. \ModelName\ is implemented in PyTorch with $\sigma=2$ and $\alpha=2$. Additional important parameter values are provided in Table~\ref{table:hyperparameter}. More implement detailed information is available at \url{https://anonymous.4open.science/r/DiffSP}.


All the experiments were conducted on an Ubuntu 20.04 LTS operating system, utilizing an Intel Xeon Platinum 8358 CPU (2.60GHz) with 1TB DDR4 RAM. For GPU computations, an NVIDIA Tesla A100 SMX4 with 40GB of memory was used.
\input{table/hyperparameter}



% \section{Additional Results And Analysis}\label{appendix:additional_analysis}
% \subsection{Further Analysis of Non-Isotropic Diffusion} 
% We further analyze the core LID-Driven Non-Isotropic Diffusion Mechanism of \ModelName. In Figure~\ref{fig_non_isotripic}, we compare the sum of edge weights for clean edges during the reverse denoising process of both non-isotropic and isotropic diffusion in the IMDB-BINARY dataset under PR-BCD, GradArgmax, and CAMA-Subgraph attacks. For clean edges, the weights are positive, whereas for adversarial edges, the weights are negative. As shown in Figure~\ref{fig_non_isotripic}, the non-isotropic diffusion process introduces more noise to adversarial edges while minimizing the perturbations on the unaffected clean structure. This results in a faster and more effective recovery of the clean structure compared to isotropic diffusion.
% \vspace{-0.6em}
% \begin{figure}[!h]
% \centering
% \includegraphics[width=\linewidth]{figure/non_iso_ana_paint.pdf}
% \vspace{-2.5em}
% \caption{Non-Isotropic Diffusion Study}
% \label{fig_non_isotripic}
% \end{figure}

% \vspace{-2em}
% \subsection{Further Analysis of $k$ Selection} 
% We further analyze the impact of selecting different values of $k$ in the LID-Driven Non-Isotropic Diffusion module. We adjust $k$ within {3, 4, 5, 6, 7, 8} on the IMDB-BINARY dataset under PR-BCD attacks. Figure~\ref{fig_k_selection} shows the classification accuracy and the LID value ratio between adversarial and clean nodes. The red line (LID ratio = 1) indicates equal LID values for adversarial and clean nodes. All $k$ values demonstrate the ability to detect adversarial nodes. Additionally, better adversarial node detection leads to improved graph classification accuracy, as clean nodes experience less perturbation while adversarial nodes undergo more purification.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.8\linewidth]{figure/la_influence_gc.pdf}
% \vspace{-1em}
% \caption{$k$ Selection Study}
% \label{fig_k_selection}
% \end{figure}





% % To validate our conclusion, we compare the graph entropy of the Cora dataset under varying attack budgets in the presence of DICE attacks. The results are shown in Figure~\ref{}.

% % \subsubsection{Understanding the graph joint entropy}
% % In Eq. (\ref{graph_joint_entropy}), We can interpret the Hadamard product as computing a product kernel $k((z_{1}^{i}, z_{1}^{j}), (z_{2}^{i}, z_{2}^{j}))=k()k()$

% % \subsubsection{Understanding the graph transfer e1ntropy}
 


% % \subsection{Ablation study results} 
% % Figure~\ref{} shows the ablation study results on MUTAG, IMDB-MULTI, REDDIT-BINARY, and COLLAB datasets.

% % \subsection{Purification time step study results} 
% % Figure~\ref{} shows the purification time step study results on MUTAG, IMDB-MULTI, REDDIT-BINARY, and COLLAB datasets.


% % \subsection{Graph transfer entropy guide scale study results} 
% % Figure~\ref{} shows the graph transfer entropy guide scale study results on MUTAG, IMDB-MULTI, REDDIT-BINARY, and COLLAB datasets.

% \vspace{-2em}


\section{Limitations and Future Discussions}\label{appendix:future}
Although \ModelName~ enhances the robustness of graph learning against evasion attacks through prior-free structure purification, it still has certain limitations, which we aim to address in future work. Specifically: 1) In addition to structural disturbances, feature perturbations are common in real-world scenarios. In future steps, we plan to incorporate experiments on feature-based attacks and evaluate robustness in link prediction tasks under evasion attacks. 2) Estimating the adversarial degree of nodes is crucial for non-isotropic noise injection. We aim to develop a more accurate estimation method to further enhance the robustness of graph learning. 3) We also plan to optimize the time complexity of \ModelName~ to make it more efficient.

Furthermore, the graph entropy estimation approach proposed in this work is a promising tool. We will explore ways to enhance the properties encapsulated by graph entropy, such as designing better $\mathbf{Z}$ to capture the more local structure and feature characteristics of nodes. Additionally, we plan to utilize this graph entropy method to further investigate graph properties across diverse scenarios, facilitating more extensive research in this area.

