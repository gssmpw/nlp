\begin{figure*}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure/framework.pdf}
\vspace{-1.7em}
\caption{The overall architecture of \ModelName. \ModelName~first employs a diffusion model to learn the predictive patterns of clean graphs. Then for the adversarial graph under evasion attack: 1) \ModelName\ injects non-isotropic noise by adjusting the diffusion time for each edge based on its adversarial degree, determined by LID. 2) During the generation process, \ModelName\ reduces uncertainty and guides the generation toward the target clean graph by maximizing the transfer entropy between two successive time steps.}
\label{fig:framework}
\end{figure*}


\section{Notations and Problem Formulation} 
% In this work, we focus on enhancing robustness in graph classification tasks against adversarial evasion attacks~\cite{zugner2018adversarial}, 
In this work, we focus on enhancing robustness against adversarial evasion attacks with more threatening structural perturbation~\cite{zugner2018adversarial}, 
where attackers perturb graph structures during the test phase, after the GNNs have been fully trained on clean datasets~\cite{biggio2013evasion}.
We represent a graph as $G=(\mathbf{X},\mathbf{A})$, where $\mathbf{X}$ is the node features and $\mathbf{A}$ is the adjacency. An attacked graph is denoted as $G_{\text{adv}}=(\mathbf{X}, \mathbf{A}_{\text{adv}})$, where $\mathbf{A}_{\text{adv}}$ is the perturbed adjacency matrix.
Let $c_{\boldsymbol{\theta}}$ be the GNN classifier trained on clean graph samples, and  $\mathcal{D}_\text{test} = \{(\hat{s}_j, y_j)\}_{j=1}^M$ represent $M$ attacked samples, where each $\hat{s}_j$ is a graph or a node, and $y_j$ is the corresponding label. The attacker's goal is to maximize the number of misclassified samples, formulated as $\max \sum_{j=1}^{M} \mathbb{I}(c_{\boldsymbol{\theta}}(\hat{s}_j) \neq y_j)$, by perturbing up to $\epsilon$ edges, where $\epsilon$ is constrained by the attack budget $\Delta$.
Our objective is to purify the attacked graph, reducing the effects of adversarial perturbations, and reinforcing the robustness of the GNNs to enhance the performance of downstream tasks.