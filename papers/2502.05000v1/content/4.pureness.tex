\section{\ModelName}
% ================ Versioin 1 ================
% In this section, we introduce our framework named \textbf{\ModelName}, which enhances the graph robu\textbf{S}tness against evasion attacks through pri\textbf{O}r-free diff\textbf{U}sion \textbf{P}urification. 
In this section, we introduce our proposed framework named \ModelName\, which purifies the graph structure based on the learned predictive patterns without relying on any priors about the dataset or attack strategies. The overall architecture of \ModelName\ is shown in Figure~\ref{fig:framework}.
We first present our graph diffusion purification model which serves as the backbone of \ModelName, 
followed by detailing the two core components: the LID-Driven Non-Isotropic Diffusion Mechanism and the Graph Transfer Entropy Guided Denoising Mechanism. 
 % ($\rhd$ Section~\ref{method:part2})
% We first train a graph diffusion model to capture the clean distribution from the training dataset (Section~\ref{method:part1}). 
% For an attacked graph we need to purify, we can inject noise to drown out the adversarial information and then use the trained diffusion model to remove both noise and adversarial information, achieving graph purification. To avoid the injected noise drowning out the attack while also over-perturbed the clean part of the graph in the forward process, we inject non-isotropic noise into the attacked graph based on each node's adversarial degree estimated by LID (Section~\ref{method:part2}). This step neutralizes the adversarial information while preserving the unaffected clean parts of the graph. We then recover the clean graph through a reverse denoising process. To ensure the generated graph does not deviate from the desired outcome due to the diffusion modelâ€™s diversity, transfer entropy is applied to minimize uncertainty between consecutive steps in the reverse process, guiding the purification toward the target graph.
% The overall architecture of \ModelName\ is shown in Figure~\ref{fig:framework}. We first introduce the graph purification diffusion model, which serves as the backbone of \ModelName. We then detail the two core components: the LID-Driven Non-Isotropic Diffusion module and the Transfer Entropy-Guided Denoising module.

\subsection{Graph Diffusion Purification Model} \label{method:part1}
% Unlike previous diffusion approaches that involve both structure and node features in the diffusion process, in this work, we exclude node features from the diffusion process and keep them fixed. Because we focus on more harmful structural attacks, this allows the node features to serve as guiding information for structural recovery in the reverse denoising process.
% To achieve robustness across various scenarios, we propose using a diffusion model to learn the clean distribution and purify adversarial graphs through distribution mapping, without relying on priors. As shown in Figure~\ref{fig:framework}, \ModelName uses a diffusion model as its backbone, integrated with a LID-driven non-isotropic forward diffusion process and a graph transfer entropy-guided reverse denoising process. 

% ============= overview ================
% Under evasion attacks with structural perturbation, GNNs can be easily fooled by the addition or deletion of a few edges in the graph, resulting in incorrect decisions. This vulnerability makes lines of research arise aimed at mitigating the effects of structural perturbations.  However, existing methods often heavily rely on priors of the dataset or evasion attack strategies, which limits their robustness when these priors do not hold. 

% To achieve robustness across various scenarios, we approach the evasion attacks from the distribution perspective and propose \ModelName. The architecture is illustrated in Figure~\ref{fig:framework}. \ModelName\ first employs a diffusion model as its backbone to learn the clean distribution patterns. After training, it uses the learned pattern to purify adversarial graphs. We blur out adversarial perturbations by injecting noise into the graph during the forward diffusion process, weakening their impact. In the subsequent reverse denoising process, both noise and adversarial information are iteratively removed, getting a purified graph. 

% However, since only a few edges are perturbed in adversarial attacks, injecting isotropic noise, which refers to uniform noise that does not account for the varying characteristics of nodes, can drown out the adversarial information while erasing the valuable information. Thus making purification difficult. Additionally, the diversity inherent in the generation process will cause the generated graph to deviate from the desired clean graph. To address these challenges when using the trained diffusion model to purify attacked graphs: 1) In the forward process, \ModelName\ leverages Local Intrinsic Dimensionality (LID) to assess the adversarial degree of each node, injecting more noise into highly adversarial nodes while applying less noise to cleaner ones ($\rhd$ Section~\ref{method:part2}).  2) In the reverse process, \ModelName\ uses transfer entropy as an information metric to measure the uncertainty between graphs generated at successive reverse time steps. By increasing the transfer entropy, we reduce the generation uncertainty and guide the generation process toward the target clean graph ($\rhd$ Section~\ref{method:part2}). 
% while incorporating a LID-Driven Non-Isotropic Diffusion module to drown out adversarial information precisely, and a graph Transfer Entropy-Guided Denoising module to generate the desired clean graphs. These two processes collaborate to map the adversarial graphs into clean distribution, achieving adversarial graph purification.

For the backbone of \ModelName, we incorporate the structured diffusion model \cite{austin2021structured, vignac2022digress, li2023graphmaker}, which has shown to better preserve graph sparsity while reducing computational complexity~\cite{vignac2022digress, haefeli2022diffusion}. Since we focus on the more threatening structural perturbations~\cite{zugner2018adversarial}, we exclude node features from the diffusion process and keep them fixed.
% For the backbone of \ModelName, unlike previous diffusion models that incorporate both structure and node features in the diffusion process, we exclude node features and keep them fixed. This decision is driven by our focus on more harmful structural attacks~\cite{zhang2024can}. Thus making the node features to serve as guiding information for structural purification in the reverse denoising process.
Specifically, the noise in the forward process is represented by a series of transition matrices, \textit{i.e.}, $\big[\mathbf{Q}_{\mathbf{A}}^{(1)}, \mathbf{Q}_{\mathbf{A}}^{(2)}, \cdots, \mathbf{Q}_{\mathbf{A}}^{(T)}\big]$, where $(\mathbf{Q}_{\mathbf{A}}^{(t)})_{ij}$ denotes the probability of transitioning from state $i$ to state $j$ for an edge at time step $t$. 
% The forward diffusion process for $A^{(t)}$ at time $t$ is defined as $q(A^{(t)}|A)=A^{(t-1)}\bar{\mathbf{Q}}^{(t-1)}_{A}$.
The forward Markov diffusion process is defined as $q\big(\mathbf{A}^{(t)}|\mathbf{A}^{(0)}\big)=\mathbf{A}^{(0)}\mathbf{Q}_{\mathbf{\mathbf{A}}}^{(1)}\cdots \mathbf{Q}_{\mathbf{A}}^{(t-1)}=\mathbf{A}^{(0)}\bar{\mathbf{Q}}^{(t-1)}_{\mathbf{A}}$.
Here we utilize the marginal distributions of the edge state~\cite{vignac2022digress} as the noise prior distribution, thus $\bar{\mathbf{Q}}_{\mathbf{A}}^{(t)}$ can be expressed as $\bar{\mathbf{Q}}_{\mathbf{A}}^{(t)}=\bar{\alpha}^{(t)}\mathbf{I}+\big(1-\bar{\alpha}^{(t)}\big)\mathbf{1} \mathbf{m}_{\mathbf{A}}^{\top}$, where $\mathbf{m}_{\mathbf{A}}$ is the marginal distribution of edge states, $\bar{\alpha}^{(t)}=\cos^2\big(\frac{t/T+s}{1+s}\cdot \frac{\pi}{2}\big)$ follows the cosine schedule~\cite{nichol2021improved} with a small constant $s$, $\mathbf{I}$ is the identity matrix, and $\mathbf{1}$ is a vector of ones.
During the reverse denoising process, we use the transformer convolution layer~\cite{shi2020masked} as the denoising network $\phi(\cdot)_{\boldsymbol{\theta}}$, trained for one-step denoising $p_{\boldsymbol{\theta}}\big(\mathbf{A}^{(t-1)}|\mathbf{A}^{(t)}, t\big)$. 
We can train the denoising network to predict $\mathbf{A}^{(0)}$ instead of $\mathbf{A}^{(t-1)}$ since 
the posterior $q\big(\mathbf{A}^{(t-1)}|\mathbf{A}^{(t)}, \mathbf{A}^{(0)}, t\big) \propto \mathbf{A}^{(t)}(\mathbf{Q}_{\mathbf{A}}^{(t)})^{\top} \odot \mathbf{A}^{(0)} \bar{\mathbf{Q}}_{\mathbf{A}}^{(t-1)}$
has a closed form expression~\cite{sohl2015deep, song2019generative, li2023graphmaker}, where $\odot$ is the Hadamard product.
% $\int q(\mathbf{A}^{(t-1)}|\mathbf{A}^{(t)},\mathbf{A}^{(0)}, t)d p_{\boldsymbol{\theta}}(\mathbf{A}^{(0)}|\mathbf{A}^{(t)},t)$
% The network is trained using the cross-entropy loss the same as the Digress and GraphMaker diffusion models~\cite{vignac2022digress, li2023graphmaker}.
% , defined as $\mathcal{L}(\hat{\mathbf{A}}^{(0)}, \mathbf{A}^{(0)}) = \sum_{1 \leq i,j \leq N} \hat{e}_{ij} \log e_{ij}$, where $\hat{\mathbf{A}}^{(0)}$ and $\mathbf{A}^{(0)}$ represent the predicted and true adjacency matrices, respectively. 
Once trained, we can generate graphs by iteratively applying $\phi(\cdot)_{\boldsymbol{\theta}}$.
% to play one-step denoising.
% as $\hat{\mathbf{A}}^{(0)} = \int_{T}^{1} q(\mathbf{A}^{(t-1)} | \hat{\mathbf{A}}^{(0)}) p_{\theta}(\hat{\mathbf{A}}^{(0)} | \mathbf{A}_\text{adv}^{(t)}, \mathbf{A}, t)$.


% Once trained, we can use $\phi_{\theta}$ to purify an adversarial graph with perturbed structure $\mathbf{A}_{\text{adv}} \in \mathbb{R}^{N \times N \times 2}$. By selecting a time step $t^{\prime} \in [0, T]$, where $T$ is the total diffusion time, we apply the noisy transition matrix $\bar{\mathbf{Q}}_{\mathbf{A}}^{(t^{\prime})}$ to drown out the adversarial perturbations as $\mathbf{A}_{\text{adv}}^{(t^{\prime})} = \mathbf{A}_{\text{adv}} \bar{\mathbf{Q}}_{\mathbf{A}}^{(t^{\prime})}$. 
% Then iteratively applying $\phi_{\theta}$ to generate a purified graph, following the process: 
% \begin{equation}
% \label{initial_purify}
%     \hat{A}_{\text{adv}}^{(0)} = \prod_{t=t^{\prime}}^{1} q(A_{\text{adv}}^{(t-1)} | \hat{A}_{\text{adv}}^{(0)}) p_{\theta}(\hat{A}_{\text{adv}}^{(0)} | A_{\text{adv}}^{(t)}, X, t).
% \end{equation}
% Then iteratively applying $\phi_{\theta}$ to generate a purified graph as $\hat{\mathbf{A}}_{\text{adv}}^{(0)} = \int_{t=t^{\prime}}^{1} q(\mathbf{A}_{\text{adv}}^{(t-1)} | \hat{\mathbf{A}}_{\text{adv}}^{(0)}) p_{\theta}(\hat{\mathbf{A}}_{\text{adv}}^{(0)} | \mathbf{A}_{\text{adv}}^{(t)}, \mathbf{A}, t)$. Afterwards, $\hat{\mathbf{A}}_{\text{adv}}^{(0)}$ serves as the purified structure for the classifier $c_{\hat{\theta}}$ to perform classification.





\subsection{LID-Driven Non-Isotropic Diffusion Mechanism}
\label{method:part2}
Adversarial attacks typically target only a small subset of nodes or edges to fool the GNNs while remaining undetected. 
% Injecting isotropic noise uniformly across all nodes,  where isotropic noise means applying the same level of noise equally to each node regardless of their individual characteristics~\cite{voleti2022score}, leads to a critical problem. 
Injecting isotropic noise uniformly across all nodes, which means applying the same level of noise to each node regardless of its individual characteristics~\cite{voleti2022score}, poses a significant challenge. 
% While isotropic noise can effectively drown out adversarial perturbations in the forward diffusion process, it also inevitably disrupts the clean part of the graph, erasing both adversarial and valuable information. 
While isotropic noise can effectively drown out adversarial perturbations during the forward diffusion process, it inevitably compromises the clean and unaffected portions of the graph. 
% This brings difficulty to purification during the reverse denoising process.
As a result, both the adversarial and the valuable information are erased, making purification during the reverse denoising process more difficult.
% During the forward diffusion process, the diffusion model injects isotropic noise uniformly across all nodes, 
% In contrast, 
% This discrepancy leads to a critical problem: while isotropic noise can effectively drown out adversarial perturbations in the forward diffusion process, it also inevitably disrupts the clean structure, erasing both adversarial and valuable information. This brings difficulty to purification during the reverse denoising process.
% This creates a dilemma: a large $t^{\prime}$ removes adversarial information but compromises the clean structure, while a smaller $t^{\prime}$ preserves more of the original structure but leaves significant adversarial perturbations intact.

To remove the adversarial perturbations without losing valuable information, we design a novel LID-Driven Non-Isotropic Diffusion Mechanism. The core idea is to inject more noise into adversarial nodes identified by Local Intrinsic Dimensionality (LID) while minimizing disruption to clean nodes.
In practice, the noise level associated with different edges is distinct and independent. 
As a result, the noise associated with each edge during the forward diffusion process is represented by an independent transition matrix. The adjacency matrix $\mathbf{A}^{(t)}$ at time step $t$ is then updated as follows:
% \begin{equation}
\begin{align}
    &\mathbf{A}^{(t)}_{ij} =\mathbf{A}_{ij}\big(\bar{\mathbf{Q}}_{\mathbf{A}}^{(t)}\big)_{ij}, \label{non_isotropic_transition_matrx} \\
    &\big(\bar{\mathbf{Q}}_{\mathbf{A}}^{(t)}\big)_{ij} = \bar{\alpha}^{(t)} \mathbf{I} + \big(\mathbf{\Lambda}_{\mathbf{A}}\big)_{ij}\big(1-\bar{\alpha}^{(t)}\big)\mathbf{1} \mathbf{m}_{\mathbf{A}}^{\top},
    % \mathbf{A}^{(t)}_{ij} =\mathbf{A}_{ij}(\bar{\mathbf{Q}}_{\mathbf{A}}^{(t)})_{ij},
    % (\bar{\mathbf{Q}}_{\mathbf{A}}^{(t)})_{ij} = \bar{\alpha}^{(t)} \mathbf{I} + (\mathbf{\Lambda}_{\mathbf{A}})_{ij}(1-\bar{\alpha}^{(t)})\mathbf{1} \mathbf{m}_{\mathbf{A}}^{\top},
\end{align}
% \end{equation}
where $\mathbf{\Lambda}_{\mathbf{A}} \in \mathbb{R}^{N \times N}$ represents the adversarial degree of each edge. 

Based on the above analysis, locating the adversarial information and determining the value of $\mathbf{\Lambda}_{\mathbf{A}}$ is crucial for effective adversarial purification. 
Local Intrinsic Dimensionality (LID)~\cite{houle2017local, ma2018characterizing} measures the complexity of data distributions around a reference point $o$ by assessing how quickly the number of data points increases as the distance from the reference point expands. Let $F(r)$ denote the cumulative distribution function of the distances between the reference point $o$ and other data points at distance $r$ and $F(r)$ is positive and differentiable at $r \geq 0$, the LID of point $o$ at distance $r$ is defined as $\mathop{\lim}_{\epsilon \rightarrow 0} \frac{\ln  F((1 + \epsilon)r) / F(r) }{\ln (1 + \epsilon)}$~\cite{houle2017local}. 
According to the manifold hypothesis~\cite{feinman2017detecting}, each node $n_{i}$ in a graph lies on a low-dimensional natural manifold $S$. 
Adversarial nodes being perturbed will deviate from this natural data manifold $S$, leading to an increase in LID~\cite{ma2018characterizing}, which can quantify the dimensionality of the local data manifold. Therefore, we use LID to measure the adversarial degree, $\mathbf{\Lambda}_{\mathbf{A}}$. Higher LID values indicate that the local manifold around a node has expanded beyond its natural low-dimensional manifold $S$, signaling the presence of adversarial perturbations.
% The LID defined in Eq.~\eqref{LID_definition} is a theoretical quantity and is not practical for direct implementation. 
In this work, we use the Maximum Likelihood Estimator (MLE)~\cite{amsaleg2015estimating} to estimate the LID value of graph nodes, providing a useful trade-off between statistical efficiency and computational complexity~\cite{ma2018characterizing}. 
Specifically, let $\mathbf{\Gamma} \in \mathbb{R}^{n}$ represent the vector of estimated LID values, where $\mathbf{\Gamma}_{i}$ denotes the LID value of node $n_{i}$, which is estimated as follows:
\begin{equation}
\label{equation_lid_score}
\mathbf{\Gamma}_{i} = -\left(\frac{1}{k}\sum_{j=1}^{k}\log \frac{r_{j}(n_{i})}{r_{k}(n_{i})}\right)^{-1}.
\end{equation}
Here, $r_{j}(n_{i})$ represents the distance between node $n_{i}$ and its $j$-th nearest neighbor $n_{i}^{j}$. 
% According to the findings of~\cite{gal2016dropout}, the deeper layers of a neural network reveal more linear and ``unwrapped'' manifolds compared to the input space. Thus, we calculate $r_{j}(n_{i})$ using the Euclidean distance~\cite{dokmanic2015euclidean} between the hidden features of two nodes in the last hidden layer of the pre-trained GNN classifier $c(\cdot)_{\hat{\boldsymbol{\theta}}}$. 
Based on the observation that the deeper layers of a neural network reveal more linear and ``unwrapped'' manifolds compared to the input space~\cite{gal2016dropout}, we compute the $r_{j}(n_{i})$ as the Euclidean distance~\cite{dokmanic2015euclidean} between the hidden features of two nodes in the last hidden layer of the trained GNN classifier $c(\cdot)_{\boldsymbol{\theta}}$.
After obtaining the LID values vector $\mathbf{\Gamma}$, we can calculate $\mathbf{\Lambda}_{\mathbf{A}} = \mathbf{\Gamma} \mathbf{\Gamma}^{\top}$. 

However, in practice, using the non-isotropic transition matrix in Eq.~\eqref{non_isotropic_transition_matrx}   requires the diffusion model to predict the previously injected non-isotropic noise during the reverse process. This task is more challenging because, unlike isotropic noise, non-isotropic noise varies across different edges. As a result, the model must learn to predict various noise distributions that are both spatially and contextually dependent on the graph structure and node features. This increases the difficulty of accurately estimating and removing the noise across graph regions, making the reverse denoising process significantly more intricate. 
% Moreover, effective non-isotropic diffusion typically relies on adversarial training data, as the model must learn to handle varying levels of perturbation from adversarial attacks. 
Moreover, training the model to develop the ability to inject more noise into adversarial perturbations and remove it during the reverse process relies on having access to adversarial training data.
However, in the evasion attack settings, where the model lacks access to adversarial graphs during training, its ability to achieve precise non-isotropic denoising is limited.
Inspired by ~\cite{yu2024constructing}, we introduce the following proposition:

\begin{proposition}
\label{equivalence}
    For each edge at time $t$, the adjacency matrix is updated as $\mathbf{A}^{(t)}_{ij} =\mathbf{A}_{ij}\big(\bar{\mathbf{Q}}^{\prime(t)}_{\mathbf{A}}\big)_{ij}$, where the non-isotropic transition matrix is  $\big(\bar{\mathbf{Q}}_{\mathbf{A}}^{\prime(t)}\big)_{ij} = \bar{\alpha}^{(t)} \mathbf{I} + (\boldsymbol{\Lambda}_{\mathbf{A}})_{ij}(1-\bar{\alpha})\mathbf{1} \mathbf{m}_{\mathbf{A}}^{T}$. There exists a unique time $\hat{t}\big(\mathbf{A}_{ij}\big)\in [0, T]$ such that $\big(\bar{\mathbf{Q}^\prime}_{\mathbf{A}}^{(t)}\big)_{ij}\Leftrightarrow \big(\bar{\mathbf{Q}}_{\mathbf{A}}^{\hat{t}(\mathbf{A}_{ij})}\big)_{ij}$, where:
    \begin{equation}
        \label{equation_purification_time}
        \hat{t}\big(\mathbf{A}_{ij}\big)\!=\! T\!\left(\frac{2(1\!+\!s)}{\pi} \cos^{-1}\! \left(\sqrt{\frac{\bar{\alpha}^{(t)}}{\big[\boldsymbol{\Lambda}(\mathbf{A})_{ij} (1-\bar{\alpha}^{(t)}) + \bar{\alpha}^{(t)}\big]}}\right)\!-\!s\right).
    \end{equation}
\end{proposition}
This proposition demonstrates that non-isotropic noise can be mapped to isotropic noise by adjusting the diffusion times accordingly. The detailed proof is provided in Appendix~\ref{appendix:proof}. Building on this proposition, we bypass the need to train a diffusion model that can predict non-isotropic noise in the reverse denoising process. 
Instead, we handle the need for non-isotropic noise injection by applying isotropic noise uniformly to all edges, while varying the total diffusion time for each edge. 
By controlling the diffusion time for each edge, we can effectively manage the noise introduced to each node, ensuring that the injected noise accounts for the adversarial degree of each node. 
Let $\hat{\mathbf{A}}^{(t)\prime}$ represents the adjacency matrix at time $t$ during the reverse denoising process, we have:
\begin{equation}
\label{equation_non_isotropic_purification}
\hat{\mathbf{A}}^{(t)\prime} = \mathbf{M}^{(t)} \odot \hat{\mathbf{A}}^{(t)} + \big(1-\mathbf{M}^{(t)}\big)\odot \mathbf{A}^{(t)},
\end{equation}
where $\hat{\mathbf{A}}^{(t)}$ is the adjacency matrix predicted by $\phi(\cdot)_{\boldsymbol{\theta}}$, $\mathbf{A}^{(t)}$ is the noisy adjacency matrix obtained by $\mathbf{A}^{(t)} = \mathbf{A}\bar{\mathbf{Q}}_{\mathbf{A}}^{(t)}$ in the forward diffusion process, and $\mathbf{M}^{(t)}$ is the binary mask matrix that indicates which edges are being activated to undergo purification at time step $t$, achieving the non-isotropic diffusion. $\mathbf{M}^{(t)}_{ij}$ is defined as:
\begin{equation}
\label{equation_mask}
\mathbf{M}^{(t)}_{ij} = \left \{
\begin{aligned}
    &0,  &t > \hat{t}\big(\mathbf{A}_{ij}\big)\\
    &1,  &t \leq \hat{t}\big(\mathbf{A}_{ij}\big)
\end{aligned}
\right. ,
\end{equation}
where $\hat{t}\big(\mathbf{A}_{ij}\big)$ is obtained according to Proposition~\ref{equivalence}. This implies that clean nodes are not denoised until the specified time. In this way, adversarial information receives sufficient denoising, while valuable information is not subjected to excessive perturbations.

\subsection{Graph Transfer Entropy Guided Denoising Mechanism}
\label{method:part3}
% The inherent sampling randomness of diffusion models, while useful for generating diverse graph samples, introduces challenges to our purification goal. 
In structured diffusion models~\cite{austin2021structured}, the reverse process involves multiple rounds of sampling from the distribution, which introduces inherent randomness. 
This randomness is useful for generating diverse graph samples but creates challenges for our purification goal.
During the reverse denoising process, the diversity of diffusion can result in purified graphs that, although free from adversarial attacks and fit the clean distribution, deviate from the target graph and have different ground truth labels. 
% This misalignment between the purified graph and the target structure creates a significant challenge for accurate recovery, as it compromises the preservation of the target graphâ€™s structure integrity while attempting to remove adversarial perturbations.
This presents a significant challenge: we not only encourage the generated graph to be free from adversarial information but also aim for it to retain the same semantic information as the target clean graph.

To address this challenge, we introduce a Graph Transfer Entropy Guided Denoising Mechanism to minimize the generation uncertainty in the reverse Markov chain $\langle\hat{G}^{(T-1)} \rightarrow \hat{G}^{(T-2)} \rightarrow \dots \rightarrow \hat{G}^{(0)}\rangle$. Transfer entropy~\cite{schreiber2000measuring} is a non-parametric statistic that quantifies the directed transfer of information between random variables. The transfer entropy from $\hat{G}^{(t)}$ to $\hat{G}^{(t-1)}$ in the reverse process by knowing the adversarial graph $G_{\text{adv}}$, can be defined in the form of conditional mutual information~\cite{wyner1978definition}:
\begin{equation}
\label{transfer entropy}
    I\big(\hat{G}^{t-1}; G_{\text{adv}}| \hat{G}_{t}\big) = 
    H\big(\hat{G}^{(t-1)}|\hat{G}^{(t)}\big) - H\big(\hat{G}^{(t-1)}|\hat{G}^{(t)}, G_{\text{adv}}\big),
\end{equation}
where $I(\cdot)$ represents mutual information and $H(\cdot)$ is the Shannon entropy.
This measures the uncertainty reduced about future value $\hat{G}^{(t-1)}$ conditioned on the value $G_{\text{adv}}$, given the knowledge of past values $\hat{G}^{(t)}$.
Given the unnoticeable characteristic of adversarial attacks, which typically involve only small perturbations to critical edges without altering the overall semantic information of most nodes, the target clean graph has only minimal differences from $G_{\text{adv}}$.
% So the target clean graph just has small differences with $G_{\text{adv}}$.
Therefore, by increasing the $I\big(\hat{G}^{t-1}; G_{\text{adv}}| \hat{G}_{t}\big)$, we can mitigate the negative impacts of generative diversity on our goal and guide the direction of the denoising process, ensuring that the generation towards the target clean graph. Specifically, the purified graph will not only be free from adversarial attacks but will also share the same semantic information as the target clean graph. However, calculating Eq.~\eqref{transfer entropy} requires estimating both the entropy and joint entropy of graph data, which remains an open problem.

In this work, we propose a novel method for estimating graph entropy and joint entropy.  Let $z_{i}$ be the representations of node $n_{i}$ after message passing. By treating the set $\mathcal{Z}=\{z_{1}, z_{2}, \dots, z_{n}\}$ as a collection of variables that capture both feature and structure information of the graph, we approximate it as containing the essential information of the graph. From this perspective, the entropy of the graph can be estimated using matrix-based R\'{e}nyi $\alpha$-order entropy~\cite{yu2019multivariate}, which provides an insightful approach to calculating the graph entropy. 
Specifically, let $\mathbf{K}$ denote the Gram matrix obtained from evaluating a positive definite kernel $k$ on all pairs of $z$ with $\mathbf{K}_{ij}=\exp\Big(-\frac{\|z_{i}-z_{j}\|^{2}}{2\sigma ^{2}}\Big)$, where $\sigma$ is a hyperparameter selected follows the Silvermanâ€™s rule~\cite{silverman2018density}, the graph entropy can then be defined as the R\'{e}nyiâ€™s $\alpha$-order entropy $S_{\alpha}(\cdot)$~\cite{yu2019multivariate}:
\begingroup
\setlength{\abovedisplayskip}{0.8\abovedisplayskip}
\setlength{\belowdisplayskip}{0.8\belowdisplayskip}
\begin{equation}
\label{graph_entropy}
    H(G) 
    = S_\alpha\big(\hat{\mathbf{K}}\big)
    = \frac{1}{1-\alpha}\log\left[\sum_{1}^{n}\lambda_{i}^{\alpha}\big(\hat{\mathbf{K}}\big)\right],
\end{equation}
\endgroup
where $\hat{\mathbf{K}}_{ij}=\frac{1}{n}\frac{\mathbf{K}_{ij}}{\sqrt{\mathbf{K}_{ii}\mathbf{K}_{jj}}}$, $\lambda_{i}\big(\hat{\mathbf{K}}\big)$ denotes the $i$-th eigenvalue of $\hat{\mathbf{K}}$, and $\alpha$ is a task-dependent parameter~\cite{yu2019multivariate}. In the context of graph learning, Eq.~\eqref{graph_entropy} captures the characteristics of the graph's community structure: lower graph entropy signifies a more cohesive and well-defined community structure, whereas higher graph entropy indicates a more disordered and irregular arrangement. Further details can be found in Appendix~\ref{appendix:understanding_entropy}. For a collection of $m$ graphs with their node representations after message passing $\big\{\mathcal{Z}_{i}=\big(z_{1}^{i}, z_{2}^{i},\cdots, z_{n}^{i}\big)\big\}_{i=1}^{m}$, the joint graph entropy is defined as~\cite{yu2019multivariate}:
\begin{equation}
\label{graph_joint_entropy}
    H(G_{1}, G_{2}, \cdots, G_{m})=S_\alpha\left(\frac{\hat{\mathbf{K}}_{1}\odot \hat{\mathbf{K}}_{2} \odot \cdots \odot \hat{\mathbf{K}}_{m}}{\operatorname{tr}\big(\hat{\mathbf{K}}_{1}\odot \hat{\mathbf{K}}_{2}  \odot \cdots \odot \hat{\mathbf{K}}_{m}\big)}\right),
\end{equation}
where $\hat{\mathbf{K}}_{i}$ is the normalized Gram matrix of $G_{i}$, $\odot$ represents the Hadamard product, and $\operatorname{tr}(\cdot)$ is the matrix trace. 
Further understanding of our calculation method can be found in Appendix~\ref{appendix:understanding_entropy}.

By combining Eq. (\ref{graph_entropy}) and Eq. (\ref{graph_joint_entropy}), we can get the value of transfer entropy $I\big(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}\big)$. The detailed derivation process is provided in Appendix~\ref{appendix:derivation}. Intuitively, based on our entropy estimation method, maximizing $I\big(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}\big)$ will guide the node entanglement of the generated $\hat{G}^{(t-1)}$ towards that of $G_{\text{adv}}$, preventing the reverse denoising process from deviating from the target direction. To achieve this, we update the generation process using the negative gradient of $I\big(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}\big)$ concerning $\hat{\mathbf{A}}^{(t-1)}$:
\begin{equation}
\label{eq:intial_guide}
    \hat{\mathbf{A}}^{(t-1)} \leftarrow \hat{\mathbf{A}}^{(t-1)} + \lambda \nabla_{\hat{\mathbf{A}}^{(t-1)}} I\big(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}\big),
\end{equation}
where $\lambda$ is a hyperparameter controlling the guidance scale. Early in the denoising process, maximizing the $I\big(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}\big)$ will steer the overall direction of the generation toward better purification. However, as the graph becomes progressively cleaner, maintaining the same level of guidance could cause the re-emergence of adversarial information in the generated graph. Therefore, it is essential to adjust the guidance scale dynamically over time. 
We propose that the scale of guidance should depend on the ratio between the injected noise and the adversarial perturbation at each time step. 
% Let $\Delta$ represent the attack budget of the attacker. 
We update the guidance process in Eq.~\eqref{eq:intial_guide} as follows:
\begin{equation}
\label{equation_guide_generation}
    % \hat{\mathbf{A}}^{(t-1)} = \hat{\mathbf{A}}^{(t-1)} - \lambda \frac{\Delta}{1-\bar{\alpha}} \nabla_{\hat{\mathbf{A}}^{(t-1)}} I(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}).
    \hat{\mathbf{A}}^{(t-1)} \leftarrow \hat{\mathbf{A}}^{(t-1)} - \frac{\lambda}{1-\bar{\alpha}} \nabla_{\hat{\mathbf{A}}^{(t-1)}} I\big(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}\big).
\end{equation}

% The overall purification process of \ModelName is shown in Algorithm~\ref{algorithm:\ModelName}, provided in Appendix~\ref{appendix:algorithm}.



\subsection{Training Pipeline of \ModelName}
Under the adversarial evasion structural attacks, we train the proposed \ModelName~along with the classifier using the overall objective loss function $\mathcal{L}=\mathcal{L}_{\text{cls}}+\mathcal{L}_{\text{diff}}$, where:
\begin{align}
    &\mathcal{L}_{\text{cls}}=\text{cross-entropy}\big(\hat{y}, y\big),\\
    &\mathcal{L}_{\text{diff}}=\mathbb{E}_{q\big(\mathbf{A}^{(0)}\big)}\mathbb{E}_{q\big(\mathbf{A}^{t}|\mathbf{A}^{(0)}\big)}\big[-\log p_{\boldsymbol{\theta}}\big(\mathbf{A}^{(0)}|\mathbf{A}^{(t)}, t\big)\big].
\end{align}
The classifier loss $\mathcal{L}_{\text{cls}}$ measures the difference between the predicted label $\hat{y}$ and the ground truth $y$. The graph diffusion model loss $\mathcal{L}_{\text{diff}}$ accounts for the reverse denoising process~\cite{austin2021structured}. Initially, we train the classifier, followed by the independent training of the diffusion model. Once both models are trained, they are used together to purify adversarial graphs. The training pipeline of \ModelName~is detailed in Algorithm~\ref{algorithm:purification}, and complexity analysis is in Appendix~\ref{appendix:complexity}. 

% Trained classifier $c(\cdot)_{\boldsymbol{\hat{\theta}}}$;
% \begingroup
% \setlength{\floatsep}{0.2\floatsep}
% \setlength{\textfloatsep}{0.2\textfloatsep}
\input{table/graph_classification}

\begin{algorithm}
\caption{Overall training pipeline of \textbf{\ModelName}.}
\label{algorithm:purification}
\KwIn {Evasion attacked graph $G_{\text{adv}}=(\mathbf{X}, \mathbf{A}_{\text{adv}})$;  Classifier $c(\cdot)_{\boldsymbol{{\theta}}}$; Graph diffusion purification model $\phi(\cdot)_{\boldsymbol{\theta}}$; Hyperparameters $T, k, \lambda, \sigma, \alpha, \eta$.}
\KwOut{Purified graph $\hat{G}=(\mathbf{X}, \hat{\mathbf{A}})$; Learned parameter $\hat{\boldsymbol{\theta}}$.}
\BlankLine 
Update by back-propagation $\boldsymbol{\hat{\theta}} \leftarrow \boldsymbol{\hat{\theta}}- \eta \nabla_{\boldsymbol{\hat{\theta}}}\mathcal{L}$ \;
% Train the graph diffusion purification model by iterating $\boldsymbol{\theta}\leftarrow \boldsymbol{\theta}- \eta \nabla_{\boldsymbol{\theta}}\mathcal{L}_{\text{diff}}$ \;
\tcp{LID-Driven Non-Isotropic Diffusion}
Assess node adversarial degree $\mathbf{\Gamma}$ based on LID $\leftarrow$  Eq.~\eqref{equation_lid_score}\;
Calculate the edge adversarial degree $\boldsymbol{\Lambda}_{\mathbf{A}}=\mathbf{\Gamma} \mathbf{\Gamma}^{\top}$\;
Obtain the purification time of each edge $\hat{t}(\mathbf{A}_{ij})\leftarrow$ Eq.~\eqref{equation_purification_time}\;
\For{$t=T,T-1,\cdots,1$}{
    Establish the purification mask $\mathbf{M}^{(t-1)}\leftarrow$ Eq.~\eqref{equation_mask}\;
    % Obtain the forward noisy adjacency matrix $A_{adv}^{(t-1)}$ and the predict adjacency matrix $\hat{G}^{(t-1)}$ at time $t-1$ using $\phi_{\theta}$\;
    Execute one step denoising $\mathbf{\hat{\mathbf{A}}}^{(t-1)}\leftarrow$ Eq.~\eqref{equation_non_isotropic_purification}\;

    \tcp{Graph Transfer Entropy Guided Denoising}
    Calculate the graph transfer entropy $\leftarrow$ Eq.~\eqref{transfer entropy},~\eqref{graph_entropy},~\eqref{graph_joint_entropy}\;
    Guide the reverse denoising process $\leftarrow$ Eq.~\eqref{equation_guide_generation}\;
}
Obtain the $\hat{\textbf{A}}^{(0)}$ as the purified adjacency matrix $\hat{\mathbf{A}}$.
\end{algorithm}
% \endgroup

