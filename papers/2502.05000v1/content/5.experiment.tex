
\section{Experiment}
\textbf{Datasets.} 
We assess the robustness of \ModelName\footnote{Our code is available at \url{https://github.com/RingBDStack/DiffSP}} in graph and node classification tasks. 
For graph classification, we use MUTAG~\cite{ivanov2019understanding}, IMDB-BINARY~\cite{ivanov2019understanding}, IMDB-MULTI~\cite{ivanov2019understanding}, REDDIT-BINARY~\cite{ivanov2019understanding}, and COLLAB~\cite{ivanov2019understanding}. For node classification, we test on Cora~\cite{yang2016revisiting}, CiteSeer~\cite{yang2016revisiting}, Polblogs~\cite{adamic2005political}, and Photo~\cite{shchur2018pitfalls}. Details are in Appendix~\ref{appendix:datasets}.

\noindent\textbf{Baselines.} 
Due to the limited research on robust GNNs targeting graph classification, we compare \ModelName\ with robust representation learning and structure learning methods designed for graph classification, including IDGL~\cite{chen2020iterative}, GraphCL~\cite{you2020graph}, VIB-GSL~\cite{sun2022graph}, G-Mixup~\cite{han2022g}, SEP~\cite{wu2022structural}, MGRL~\cite{ma2023multi}, SCGCN~\cite{zhao2024graph}, HSP-SL~\cite{zhang2019hierarchical}, SubGattPool~\cite{bandyopadhyay2020hierarchically} DIR~\cite{wu2022discovering}, and VGIB~\cite{yu2022improving}.
For node classification, we choose baselines from: 1) \textit{Structure Learning Based} methods, including GSR~\cite{zhao2023self}, GARNET~\cite{deng2022garnet}, and GUARD~\cite{li2023guard}; 2) \textit{Preprocessing Based} methods, including SVDGCN~\cite{entezari2020all} and JaccardGCN~\cite{wu2019adversarial}; 3) \textit{Robust Aggregation Based} methods, including RGCN~\cite{zhu2019robust}, Median-GCN~\cite{chen2021understanding}, GNNGuard~\cite{zhang2020gnnguard}, SoftMedian~\cite{geisler2021robustness}, and ElasticGCN~\cite{liu2021elastic}; and 4) \textit{Adversarial Training Based} methods, represented by the GraphADV~\cite{xu2019topology}.
Details of baselines can be found in Appendix~\ref{appendix:baselines}.
% #######

\noindent\textbf{Adversarial Attack Settings.}
For graph classification, we evaluate the performance against three strong evasion attacks: PR-BCD~\cite{geisler2021robustness}, GradArgmax~\cite{dai2018adversarial}, and CAMA-subgraph~\cite{wang2023revisiting}. 
For node classification, we evaluate six evasion attacks: 1) \textit{Targeted Attacks}: PR-BCD~\cite{geisler2021robustness}, Nettack~\cite{zugner2018adversarial}, and GR-BCD~\cite{geisler2021robustness}; 2) \textit{Non-targeted Attacks}: MinMax~\cite{li2020deeprobust}, DICE~\cite{zugner2018metalearningu}, and Random~\cite{li2020deeprobust}.
Further details on the attack methods and budget settings are provided in Appendix~\ref{appendix:attacks}.

\noindent\textbf{Hyperparameter Settings.} Details are provided in Appendix~\ref{appendix:implements}.
% \vspace{-0.2\baselineskip}

\input{table/node_classification_targeted}
\input{table/node_classification_non_targeted}
\subsection{Adversarial Robustness}
\textbf{Graph Classification Robustness.}
We evaluated the robustness of the graph classification task under three adversarial attacks across five datasets. Since the choice of classifier affects attack effectiveness, especially in graph classification due to pooling operations, it is crucial to standardize the model architecture. Simple changes like adding a linear layer can reduce the impact of attacks. To ensure a fair comparison, we used a two-layer GCN with a linear layer and mean pooling for both the baselines and our proposed \ModelName. Each experiment was repeated 10 times, with results shown in Table~\ref{table:graph_classification}.

\textit{Result.} 1) \ModelName~ consistently outperforms all baselines under the PR-BCD attack and achieves the highest average robustness across all attacks on five datasets, with a notable 4.80\% average improvement on the IMDB-BINARY dataset.
2) It's important to note that while baselines may excel against specific attacks, they often struggle with others. In contrast, \ModelName\ maintains consistent robustness across both datasets and attacks, thanks to its ability to learn clean distributions and purify adversarial graphs without relying on specific priors about the dataset or attack strategies.


\begin{figure*}[!t]
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ablation.pdf}
    \vspace{-2.5em}
        
        \caption{Ablation Study}
        \label{fig_ablation}
    \end{minipage} 
    \begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/time_influence.pdf}
    \vspace{-2.5em}
    
    \caption{Purification Steps Study}
    \label{fig_diffusion_steps}
\end{minipage}
\begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/tg_influence.pdf}
    \vspace{-2.5em}
    
    \caption{Guide Scale Study}
    \label{fig_graph_transfer_entropy}
\end{minipage}\\
\begin{minipage}{1.0\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/visualize.pdf}
    \vspace{-2.5em}
    \caption{Visualization Study}
    \label{fig_visualization}
    \vspace{-1em}
\end{minipage}
\end{figure*}

\noindent\textbf{Node Classification Robustness.}
We evaluate the robustness of \ModelName\ on the node classification task against six attacks across four datasets, using the same other settings as in the graph classification experiments. The results are presented in Table \ref{table:node_classification_targeted} and Table \ref{table:node_classification_non_targeted}.

\textit{Result.} We have two key observations: 
1) \ModelName\ achieves the best average performance across both targeted and non-targeted attacks on all datasets, demonstrating its robust adaptability across diverse scenarios. 
2) \ModelName\ performs particularly well under stronger attacks but is less effective against weaker ones like Random and DICE. This is because these attacks introduce numerous noisy edges, many of which do not exhibit distinctly adversarial characteristics. Instead, these edges are often plausible within the graph. Consequently, these additional perturbations can mislead \ModelName, making it harder to discern the correct information within the graph, leading the generated graph to deviate from the target clean graph.

% \subsection{Model Analyses}

\vspace{-0.5em}

\subsection{Ablation Study}
In this subsection, we analyze the effectiveness of \ModelName's two core components: 1) \ModelName~ (w/o LN), which excludes the LID-Driven Non-Isotropic Diffusion Mechanism; and 2) \ModelName\ (w/o TG), which excludes the Graph Transfer Entropy Guided Denoising Mechanism. We evaluate variants on the IMDB-BINARY and COLLAB datasets under PR-BCD and GradArgmax attacks for graph classification and on the Cora and CiteSeer dataset under PR-BCD and MinMax attacks for node classification. Results are shown in Figure~\ref{fig_ablation}.

\textit{Result.} \ModelName~ consistently outperforms the other variants. \ModelName\ (w/o LN) over-perturbs the valuable parts of the graph leading to degraded performance. Similarly, \ModelName~ (w/o TG) increases the uncertainty of generation, causing deviations from the target clean graph. These reduce the robustness against evasion attacks.

\vspace{-1em}
\subsection{Study on Cross-Dataset Generalization}
We assess \ModelName's generalization ability. The goal is to determine whether \ModelName~ effectively learns the predictive patterns of clean graphs. We train \ModelName\ on IMDB-BINARY and use the trained model to purify graphs on IMDB-MULTI, and vice versa.

\textit{Result.} As shown in Table~\ref{table:transfer}, \ModelName~ trained on different datasets, still demonstrates strong robustness compared to GCN trained and tested on the same dataset. Furthermore, \ModelName~ exhibits only a small performance gap compared to when it is trained and tested on the same dataset directly.
These results highlight \ModelName's ability to learn the underlying clean distribution of a category of data and capture predictive patterns that generalize across diverse datasets.

\input{table/transfer}


\subsection{Study on Purification Steps}
We evaluate the performance as the number of diffusion steps varies. For graph classification on the IMDB-BINARY dataset, we adjust the diffusion steps from 1 to 9 under GradArgMax, PR-BCD, and CAMA-Subgraph attacks. For node classification on the Cora dataset, we vary the diffusion steps from 1 to 12 under the GR-BCD, PR-BCD, and MinMax attacks.
The results are shown in Figure~\ref{fig_diffusion_steps}.


\textit{Result.}
We observe that all-time step settings demonstrate the ability to effectively purify adversarial graphs. At smaller time steps, the overall trend shows increasing accuracy as the number of diffusion steps increases. This is likely because fewer time steps do not introduce enough noise to sufficiently suppress the adversarial information in the graph. As the diffusion steps increase, we do not see a significant decline in performance. This stability can be attributed to our LID-Driven Non-Isotropic Diffusion Mechanism, which minimizes over-perturbation of the clean graph parts. Additionally, we found that the time required for purifying increased linearly.




\subsection{Study on Scale of Graph Transfer Entropy}
To analyze the impact of the guidance scale $\lambda$, we vary $\lambda$ from $\text{1e}^{-1}$ to $\text{1e}^{5}$. The results are presented in Figure~\ref{fig_graph_transfer_entropy}. For graph classification, experiments are conducted on the IMDB-BINARY dataset under the GradArgmax, PR-BCD, and CAMA-Subgraph attacks. For node classification, experiments are performed on the Cora dataset under the PR-BCD, GR-BCD, and MinMax attacks.

\textit{Result.} The results show that smaller values of $\lambda$ have minimal effect on accuracy. However, they reduce the stability of the purification during the reverse denoising process, leading to a higher standard deviation. This instability arises because the model is less effective at reducing uncertainty and guiding the generation process when $\lambda$ is too small.  On the other hand, large $\lambda$ values decrease accuracy by overemphasizing guidance, causing the model to reintroduce adversarial information into the generated graph.



\subsection{Graph Purification Visualization}
We visualize snapshots of different purification time steps on the IMDB-BINARY dataset using NetworkX~\cite{hagberg2008exploring}, as shown in Figure~\ref{fig_visualization}. The visualization process demonstrates that \ModelName\ has mastered the ability to generate clean graphs, achieving graph purification.

% More experiments and analyses are provided in Appendix~\ref{appendix:additional_analysis}.
