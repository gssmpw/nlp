% \renewcommand{\theequation}{\thesection.\arabic{equation}}
% \renewcommand{\thefigure}{\thesection.\arabic{figure}}
% \renewcommand{\thetable}{\thesection.\arabic{table}}
% \setcounter{equation}{0}
% \setcounter{figure}{0}
% \setcounter{table}{0}

\subsection{Graph Transfer Entropy Derivation}
\label{appendix:derivation}
We first restate Eq.~\eqref{transfer entropy}.
\begin{equation}
    I\big(\hat{G}^{t-1}; G_{\text{adv}}| \hat{G}_{t}\big) = 
    H\big(\hat{G}^{(t-1)}|\hat{G}^{(t)}\big) - H\big(\hat{G}^{(t-1)}|\hat{G}^{(t)}, G_{\text{adv}}\big).\notag
\end{equation}

% \begin{derivation}
According to the definition of mutual information:
\begin{align}
    &I\big(\hat{G}^{t-1}; G_{\text{adv}}| \hat{G}_{t}\big) \\
    = &
    H\big(\hat{G}^{(t-1)}|\hat{G}^{(t)}\big) - H\big(\hat{G}^{(t-1)}|\hat{G}^{(t)}, G_{\text{adv}}\big) \\
    = &
    \frac{H\big(\hat{G}^{(t-1)},\hat{G}^{(t)}\big)}{H\big(\hat{G}^{(t)}\big)} - \frac{H\big(\hat{G}^{(t-1)}, \hat{G}^{(t)}, G_{\text{adv}}\big)}{H\big(\hat{G}^{(t)}, G_{\text{adv}}\big)}.
\end{align}

Then combined with Eq.~\eqref{graph_joint_entropy}, we have:
\begin{align}
    &I\big(\hat{G}^{t-1}; G_{\text{adv}}| \hat{G}_{t}\big) \\
    = &
    S_{\alpha}\left(\frac{\hat{\mathbf{K}}^{(t-1)}\odot \hat{\mathbf{K}}^{(t)}}{\mathrm{tr}\big(\hat{\mathbf{K}}^{(t-1)}\odot \hat{\mathbf{K}}^{(t)}\big)}\right) / S_\alpha\big(\hat{\mathbf{K}}^{(t)}\big) \\
       -&  S_{\alpha}\left(\frac{\hat{\mathbf{K}}^{(t-1)}\odot \hat{\mathbf{K}}^{(t)} \odot \mathbf{K}_{\text{adv}}}{\mathrm{tr}\big(\hat{\mathbf{K}}^{(t-1)}\odot \hat{\mathbf{K}}^{(t)} \odot \mathbf{K}_{\text{adv}}\big) }\right) / S_\alpha\left(\frac{\hat{\mathbf{K}}^{(t)} \odot \mathbf{K}_{\text{adv}}}{\mathrm{tr}\big(\hat{\mathbf{K}}^{(t)} \odot \mathbf{K}_{\text{adv}}\big)}\right),
\end{align}
where $S_\alpha(\cdot)$ is the graph entropy calculated according to Eq.~(\ref{graph_entropy}) and $\hat{\mathbf{K}}^{(t-1)}, \hat{\mathbf{K}}^{(t)}, \mathbf{K}_{\text{adv}}$ is the Gram matrix of $\hat{\mathbf{A}}^{(t-1)}, \hat{\mathbf{A}}^{(t)}, \mathbf{A}_{\text{adv}}$.
% \end{derivation}



\section{Detailed Understanding of the Proposed Graph Transfer Entropy}\label{appendix:understanding_entropy} 
% \subsubsection{Understanding the graph entropy}
In this subsection, we further elaborate on the understanding of our graph entropy estimation method in Eq. (\ref{graph_entropy}). 
% We have the following proposition:
% \begin{proposition}
%     The graph entropy, as defined in Eq. (\ref{graph_entropy}) with $\alpha=2$, serves as a measure of the strength of community structure within the graph.
% \end{proposition}
% \textit{proof.}
After message passing, the set of node representations $\mathbf{Z}$ can be treated as variables that capture both structural and node feature neighborhood information. The normalized Gram matrix $\hat{\mathbf{K}}$, obtained by applying a positive definite kernel on all pairs of $z$, measures the neighborhood similarity between each pair of nodes, taking into account both node features and neighboring structures.
Let $\lambda_{i}(\hat{\mathbf{K}})$ be the eigenvalue of $\hat{\mathbf{K}}$ with eigenvector $\mathbf{x}_{i}$. Then we have:
\begin{equation}
    \hat{\mathbf{K}}^{2} = \hat{\mathbf{K}}\big(\hat{\mathbf{K}}\mathbf{x}_{i}\big) = \hat{\mathbf{K}}\big(\lambda_{i}\big(\hat{\mathbf{K}}\big)\mathbf{x}_{i}\big) = \lambda_{i}\big(\hat{\mathbf{K}}\big)\hat{\mathbf{K}}\mathbf{x}_{i} = \lambda_{i}^{2}\big(\hat{\mathbf{K}}\big)\mathbf{x}_{i}.
\end{equation}
Thus we achieve:
\begin{equation}
\sum_{i=1}^{n} \lambda_{i}^{\alpha}\big(\hat{\mathbf{K}}\big) = \sum_{i=1}^{n} \lambda_{i}\big(\hat{\mathbf{K}}^{\alpha}\big).
\end{equation}
Since the sum of all eigenvalues of a matrix is the trace of the matrix, the graph entropy is determined by the trace of $\hat{\mathbf{K}}^{\alpha}$. By setting $\alpha = 2$, $\hat{\mathbf{K}}_{ii}^{2}$ describes the similarity of node $i$ with all other nodes, considering both node features and neighboring structures. When $\alpha = 2$, the graph entropy can be expressed as:
$H(G) = -\log \text{tr}\big(\hat{\mathbf{K}}^{2}\big)$
Therefore a lower graph entropy indicates a graph with a stronger community structure, while a higher graph entropy suggests a more chaotic graph structure with less regularity.
So maximizing the transfer entropy $I\big(\hat{G}^{(t-1)}; G_{\text{adv}} | \hat{G}^{(t)}\big)$ actually encourage the community structure of $\hat{G}^{(t-1)}$ move towards $G_{\text{adv}}$.




\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}

\section{Computational Complexity Analysis}
\label{appendix:complexity}
The overall time complexity is $\mathcal{O}(N^{2})$, where $N$ represents the number of nodes. Specifically, the graph diffusion purification model has a complexity of $\mathcal{O}(T N^{2})$. The LID-Driven Non-Isotropic Diffusion Module has a complexity of $\mathcal{O}(N)$, and the Transfer Entropy Guided Diffusion Module has a complexity of  $\mathcal{O}(N^2)$. Therefore, the overall time complexity of the purification process is $\mathcal{O}(T N^{2}) + \mathcal{O}(N) + \mathcal{O}(N^2)=\mathcal{O}(T N^{2})$. Since $T\ll N^{2}$ in our case, the overall time complexity is $\mathcal{O}(N^2)$. This is consistent with most graph diffusion models~\cite{niu2020permutation, vignac2022digress, li2023graphmaker} and robust GNNs~\cite{zhao2023self, entezari2020all,jin2020graph}. 



