% \begin{abstract}
% Adversarial attacks pose significant threats to deploying Graph Neural Networks (GNNs) in real-world applications. Lines of studies have made progress in minimizing the influence of adversarial perturbations. However, existing methods often rely on fixed priors about the dataset or attacker, limiting their ability to generalize across diverse scenarios. These approaches cannot adaptively learn the intrinsic properties of the dataset.
% In this paper, we propose a novel framework, \ModelName (Graph \textbf{P}urification through t\textbf{R}ansfer \textbf{EN}tropy-guided \textbf{N}on-i\textbf{S}otropic Diffu\textbf{S}ion), which leverages a graph diffusion generative model to learn intrinsic properties and recover the clean structure of adversarial graphs. However, two key challenges arise: (1) The graph diffusion model’s uniform noise injection to all nodes during the forward process can over-perturb the graph, erasing valuable information and making recovery difficult, and (2) the diversity of the diffusion model in the reverse denoising process may cause the generated graph to deviate from the target clean structure.
% To address these challenges, we introduce a LID-Driven Non-Isotropic Diffusion process, which injects noise selectively, focusing on adversarial nodes while preserving the clean structure. Additionally, we propose a Graph Transfer Entropy-Guided Reverse Denoising process that maximizes transfer entropy to reduce uncertainty in the reverse process, ensuring that the generated graph remains aligned with the clean structure.
% Extensive experiments on both graph and node classification demonstrate our proposed \ModelName framework's robustness and superior generalization.
% Our code is available at \textcolor{mytablecolor}{\url{https:///}}
% \end{abstract}


% \begin{abstract}
% % priors and no priors-free 继续总结凝练
% % 鲁棒和各向同性有关
% Adversarial attacks pose significant threats to deploying Graph Neural Networks (GNNs) in real-world applications. Lines of studies have made progress in minimizing the influence of adversarial perturbations. 
% They often rely on priors such as neighbor similarity in clean graphs to restore the correct structure. However, this approach is less effective on datasets where these priors do not hold.
% % Their robustness methods often rely on priors of clean graphs or attacks.
% To achieve more generalized robustness, we need methods that can learn clean graph properties and recover the correct structure based on those learned properties, rather than depending on prior assumptions.
% Driven by this goal, in this work, we approach adversarial attacks from a distribution perspective: these attacks cause the graph distribution to deviate from the original clean distribution.
% % From this perspective, we propose using a graph generative model to learn the clean graph distribution without relying on priors and to purify adversarial graphs through distribution mapping.
% % 前面不要，直接提diffusion
% % Among various graph generative models, the diffusion model’s reverse denoising process naturally aligns with the removal of adversarial perturbations,
% % making it an ideal choice for mapping between adversarial and clean distributions. 
% From this perspective, we propose using the graph diffusion model to learn the clean graph distribution and purify adversarial graphs through distribution mapping.
% % The diffusion model’s reverse denoising process naturally aligns with the removal of adversarial perturbations, making it an ideal choice for mapping between adversarial and clean distributions.
% However, in graph diffusion models, 1) the indiscriminate noise injection across all nodes during the forward process can remove useful information still present in adversarial samples, making it difficult to recover the clean structure during reverse purification. 2) the diversity of the reverse denoising process may cause the generated graph to deviate from the target clean structure.
% To address these challenges, we propose a novel framework, \ModelName, to enhance gra\textbf{P}h rob\textbf{U}stness through t\textbf{R}ansfer \textbf{EN}tropy guid\textbf{E}d non-i\textbf{S}otropic diffu\textbf{S}ion purification.
% Our method introduces a LID-based Non-Isotropic Diffusion process, where we use local intrinsic dimensionality (LID) to estimate the adversarial degree of each node, enabling selective noise injection to focus on adversarial nodes while preserving the clean structure. Additionally, we propose a Graph Transfer Entropy-Guided Denoising process, which maximizes transfer entropy at each step to reduce uncertainty during the reverse process, 
% % ensuring the generated graph stays aligned with the clean structure without deviation.
% ensuring the generated graph matches the target clean graph without deviation.
% Extensive experiments on both graph and node classification tasks demonstrate the robustness of our \ModelName framework. Our code is available at \textcolor{mytablecolor}{\url{https:///}}.
% \end{abstract}

\begin{abstract}
Adversarial evasion attacks pose significant threats to graph learning, with lines of studies that have improved the robustness of Graph Neural Networks (GNNs).
However, existing works rely on priors about clean graphs or attacking strategies, which are often heuristic and inconsistent.
To achieve robust graph learning over different types of evasion attacks and diverse datasets, we investigate this problem from a prior-free structure purification perspective.
Specifically, we propose a novel \underline{\textbf{Diff}}usion-based \underline{\textbf{S}}tructure \underline{\textbf{P}}urification framework named \textbf{\ModelName}, which creatively incorporates the graph diffusion model to learn intrinsic distributions of clean graphs and purify the perturbed structures by removing adversaries under the direction of the captured predictive patterns without relying on priors.
\ModelName~is divided into the forward diffusion process and the reverse denoising process, during which structure purification is achieved.
To avoid valuable information loss during the forward process, we propose an LID-driven non-isotropic diffusion mechanism to selectively inject noise anisotropically.
To promote semantic alignment between the clean graph and the purified graph generated during the reverse process, we reduce the generation uncertainty by the proposed graph transfer entropy guided denoising mechanism.
Extensive experiments demonstrate the superior robustness of \ModelName~against evasion attacks.
% The reverse denoising process of diffusion models naturally aligns with removing graph adversarial perturbations, making them suitable for learning clean graph distribution and removing adversarial perturbations based on the learned distributional patterns without relying on priors.
% purifying adversarial graphs through distribution mapping.
% However, the indiscriminate noise injection in graph diffusion models can erase useful information, while the diversity of the reverse process may cause generated graphs to deviate from the target clean graph, making it difficult to directly apply them for purifying adversarial graph data.
% To address these challenges, 
% In this work, we propose a novel framework \ModelName, which introduces a LID-driven non-isotropic forward diffusion process and a transfer entropy-guided reverse denoising process to precisely remove adversarial perturbations and guide the generation toward the target clean graph.
% Our code is available at \textcolor{mytablecolor}{\url{https:///}}.
\end{abstract}


\keywords{robust graph learning, adversarial evasion attack, graph structure purification, graph diffuison}