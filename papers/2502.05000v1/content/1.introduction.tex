\section{Introduction}
Graphs are essential for modeling relationships in the social networks~\cite{zhou2023hierarchical}, recommendation systems~\cite{wu2022graph}, financial transactions~\cite{chen2022antibenford}, \textit{etc}.
While Graph Neural Networks (GNNs)~\cite{kipf2016semi} have advanced this field, concerns about their robustness have arisen~\cite{zhu2019robust, zhang2024can, jin2020graph}. 
Studies show that GNNs are vulnerable to evasion attacks~\cite{sun2022adversarial}, particularly structural perturbations~\cite{zugner2018adversarial, zhang2024can} where tiny changes to the graph topology can lead to a sharp performance decrease.

% to satisfy heuristic assumptions
A wide range of works have been proposed to enhance graph robustness, categorizing into: 1) \textit{Structure Learning Based} methods~\cite{zhao2023self, in2024self, deng2022garnet} that focus on refining graph structures; 2) \textit{Preprocessing Based} methods~\cite{entezari2020all, wu2019adversarial} that focus on denoising graphs during preprocessing stage; 3) \textit{Robust Aggregation Based} methods~\cite{chen2021understanding, zhu2019robust, tang2020transferring, geisler2021robustness} that modify the aggregation process; and 4) \textit{Adversarial Training Based} methods~\cite{xu2019topology} that trains GNNs with adversarial samples.
However, most approaches heavily depend on priors regarding clean graphs or attack strategies~\cite{in2024self}. For example, the homophily prior~\cite{jin2021node,zhang2020gnnguard,zhao2023self,in2023similarity} and the low-rank prior~\cite{entezari2020all, xu2021speedup, lu2022robust, jin2020graph} are among the most commonly used assumptions. Unfortunately, when node features are unavailable, measuring the feature similarity becomes infeasible~\cite{in2024self}. Additionally, imposing low-rank constraints risks discarding information encoded in the small singular values~\cite{deng2022garnet}. 
% These prior-dependent limitations significantly hinder the ability of existing methods to achieve consistent and global robustness in graph learning.
These prior-dependent limitations significantly hinder the ability of existing methods to achieve the universal robustness in graph learning across diverse scenarios.

% This limitation arises from their reliance on heuristic priors to enhance robustness, without fully addressing the underlying distributional shifts caused by adversarial attacks.
\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{figure/compare.pdf}
\vspace{-1.8em}
\caption{Comparison of existing robust GNNs and \ModelName.
Existing robust GNNs rely on priors that limit adaptability, while \ModelName\ is prior-free with universal robustness.
}
\label{fig:compare}
\vspace{-2.3em}
\end{figure}

To achieve prior-free robustness, we aim to adaptively learn the intrinsic distribution from clean graphs, which capture the underlying correlation and predictive patterns to enhance the robustness of GNNs when facing unseen samples.
Driven by this goal, 
we model the clean graph as a probability distribution over nodes and edges, encapsulating their predictable properties~\cite{li2018learning}. Attacks are disruptions to underlying distribution, causing it to shift away from the clean distribution~\cite{li2023revisiting}.
To learn the latent distribution of clean graphs, diffusion models~\cite{niu2020permutation,vignac2022digress} are an ideal choice, as shown in Figure~\ref{fig:compare}. Instead of relying on priors, they model the implicit distributions in a data-centric manner, remaining agnostic to the dataset and attack strategies.
Unlike other generative models, the ``noising-denoising'' process of graph diffusion models is well-suited to our goal. 
When encountering an attacked graph, the trained graph diffusion model gradually injects noise to obscure adversarial information during the forward process. 
In the reverse process, step-wise denoising enables removing both the adversarial information and injected noise, achieving prior-free graph purification. 

Nevertheless, it still faces two significant challenges:
1) \textit{How can we accurately identify and remove adversarial perturbations without disrupting the unaffected portions of the graph?}
Evasion attacks typically involve subtle perturbations, making these alterations difficult to detect~\cite{sun2022adversarial}.
During the forward process, the isotropic indiscriminate noise affects both the normal and adversarial nodes, leading to excessive perturbations that can overmodify the graph. As a result, essential information may be lost, complicating the recovery of the clean structure during the reverse denoising phase.
2) \textit{How can we ensure that the purified graph preserves the same semantics as the target clean graph?}
The generation process in diffusion models involves repeated sampling from the distribution, with the inherent randomness promoting the creation of diverse graph samples. While this diversity can be beneficial in other domains of research, it poses a significant challenge to our task. Our objective is not to produce varied graphs, but to accurately recover the clean graph. Consequently, even if adversarial perturbations are successfully removed, there remains a risk that the purified graph may fail to semantically align with the target clean graph.



To address these challenges, we propose a novel  \underline{\textbf{Diff}}usion-based \underline{\textbf{S}}tructure \underline{\textbf{P}}urification framework named \textbf{\ModelName}, which creatively incorporates
the diffusion model to learn the intrinsic latent distributions of clean graphs and purify the perturbed structures by removing adversaries under the direction of the captured predictive patterns without relying on any priors.
To remove adversaries while preserving the unaffected parts ($\rhd$ \textit{Challenge 1}), we propose an LID-driven non-isotropic diffusion mechanism to selectively inject controllable noise anisotropically.
By utilizing this non-isotropic noise, \ModelName~effectively drowns out adversarial perturbations with minimal impact on normal nodes.
To promote semantic alignment between the clean graph and the purified graph generated during the reverse process ($\rhd$ \textit{Challenge 2}), we reduce the generation uncertainty by the proposed graph transfer entropy guided denoising mechanism.
Specifically, we maximize the transfer entropy between successive time steps during the reverse denoising process. This reduces uncertainty, stabilizes the graph generation, and guides the process toward achieving accurate graph purification.
The main contributions of this paper are as follows:
\begin{itemize}[leftmargin=*]
    \item  We propose \ModelName, a novel framework for adversarial graph purification against adversarial evasion attacks. To the best of our knowledge, this is the first prior-free robust graph learning framework by incorporating the graph diffusion model.
    \item We design an LID-driven non-isotropic forward diffusion process combined with a transfer entropy guided reverse denoising process, enabling precise removal of adversarial information while guiding the generation process toward target graph purification.
    \item Extensive experiments on both graph and node classification tasks on nine real-world datasets demonstrate the superior robustness of \ModelName~against nine types of evasion attacks.
\end{itemize}



