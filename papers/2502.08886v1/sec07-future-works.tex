\section{Future Directions and Open Research Problems}
\label{sec:future_works}
%
This section discusses open research directions and practical applications of GenAI to enhance IoT security.
To categorize potential research directions, we use ICS Mitigation Techniques as a starting point, identifying techniques not addressed in existing sources.
Subsequently, we explore methods to address these topics.

\smallskip
\noindent \textbf{Access Management (M0801): } A potential research direction to address this issue is training an LLM to autonomously enforce authorization policies and decisions, ensuring user identification and verification.
Current LLM implementations have not directly addressed authorization policies for IoT systems, making it an open research problem.
The current capabilities of IoT devices may not support this function, requiring external support within or connected to the network to enforce user authorization and prevent compromise.
One possible approach is to use communications as inputs to LLM and verify the source and destination of IoT devices within the ecosystem.

\smallskip
\noindent \textbf{Encrypt Network Traffic (M0808): } To establish secure communication between PLCs, LLMs could execute complex tasks to improve communication security.
This research leverages the ability of LLMs to perform complex tasks based on prompt requirements, as demonstrated by NVISOsecurity~\citet{Raman_2024}.
LLMs could execute various lightweight cryptography algorithms, adding controlled randomness to network encryption.
For instance, the LLM might execute RSA encryption for one message and elliptic curve encryption for the next.
This dynamic approach ensures secure communication, as the encryption algorithm changes based on the LLM's directives.

\smallskip
\noindent \textbf{Operating System Configuration (M0928): } While existing LLMs could configure and secure system configurations, there is no implementation that automates configuration across all devices within an IoT ecosystem.
A practical application could involve using LLM as an agent to execute commands and change configurations based on available devices.
The LLM, connected to the network, would identify devices and send device-specific commands to modify configurations across different operating systems, improving security.
This approach presents a potential research avenue for using LLMs to improve the security of the IoT system through configuration changes.

\smallskip
\noindent \textbf{Supply Chain Management (M0817): } A potential research direction involves the use of LLM to manage the supply chain of IoT devices.
Management here means coordinating according to a given policy to minimize the risks of supply chain compromises.
Studies show that LLMs could generate policies and cyber-exercise scenarios, indicating their ability to produce a detailed, domain-specific response to enhance security. 
In supply chain management, LLMs could generate policies to verify the validity of the supplier. 
Trained with vast datasets, LLMs could leverage historical data on entities, including potential IoT device suppliers.
This capability allows LLMs to create purchase or usage policies and recommendations to improve the security of the IoT ecosystem.
A study by~\citet{li2023large} optimized the management process but not the security of the supply chain.
Using this study as a starting point, there is potential to fine-tune the model to include security considerations, such as vendor risk profiles, hardware design, and supply chain attacks, allowing LLMs to mitigate risks effectively.

\smallskip
\noindent \textbf{Validate Program Inputs (M0818) : } LLMs could validate user input on IoT devices to determine their validity.
They take advantage of vast knowledge to identify patterns in user inputs.
By learning patterns of valid and invalid behaviors, LLMs could potentially replicate actions to validate inputs and learn valid user behaviors.
If input comes from invalid IP addresses, they could be classified as malicious.
However, if the input comes from a valid device but does not match the learned behaviors, a trained LLM could identify and reason whether the input is valid.
~\citet{munley2024llm4vv} developed a compiler validation test suite using LLMs to generate test cases.
This concept could be adapted to create behavioral test cases for LLMs to validate program input. 
Research could explore using LLMs to differentiate valid inputs based on user behavior.