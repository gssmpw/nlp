\section{Related Work}
Research in inference-time reasoning and self-correction has evolved along several important directions. The Self-Refine approach established a foundation by implementing iterative feedback and refinement cycles using a single model for generation, critique, and revision \citep{madaan2024self}. Then, several self-correction approaches have emerged as effective techniques for improving responses during generation \citep{shinn2024reflexion,shridhar2023art, ganguli2023capacity}. Recent work such as Critique Fine Tuning \citep{wang2025critique} deals with learning to critique towards mathematical tasks and modifying model weights.

Prior work in language model safety primarily focuses on two key areas: safety training methods and jailbreak defense strategies. In the realm of safety training, researchers have traditionally relied on supervised finetuning (SFT) followed by reinforcement learning from human feedback (RLHF) \citep{christiano2017deep}. Direct Preference Optimization (DPO) \citep{rafailov2024direct} emerged as an alternative approach that circumvents the need for a reward model by directly optimizing the policy using preference data. Constitutional AI (CAI) \citep{bai2022constitutional} further expanded upon the SFT + RLHF paradigm by incorporating a predefined "constitution" to guide behavior, where the model critiques and revises its own responses based on constitutional principles during the SFT phase.

In response to jailbreak attacks, researchers have developed defense strategies that operate across three sequential stages. The first stage, prompt detection, utilizes perplexity detection (PPL) \citep{alon2024detecting} to identify adversarial suffixes. The second stage, prompt modification, encompasses two approaches: perturbing original prompts to neutralize adversarial suffixes (S-LM) \citep{robey2023smoothllm} and adding defensive suffixes (PAT \cite{mo2024studious}, ICD \cite{Wei2023JailbreakAG}, and SR \cite{Xie2023DefendingCA}). The final stage involves model fine-tuning through synthetic safety preference data (CST) \citep{gallego2024configurable} and techniques to help models unlearn harmful knowledge (SafeUnlearn) \citep{zhang2024safe}. Notably, while traditional safety approaches never explicitly provide specifications to the policy model during training, Deliberative Alignment \citep{guan2024deliberative} introduces a novel approach where the model memorizes policies in its \emph{chain of thought} and learns to apply them in context. This method also uniquely varies specification information across training examples, enabling more comprehensive safety policy learning. Our proposed approach enables online optimization of the self-critique process under diverse safety specifications without requiring parameter tuning.