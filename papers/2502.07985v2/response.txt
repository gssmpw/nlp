\section{Related Work}
Research in inference-time reasoning and self-correction has evolved along several important directions. The Self-Refine approach established a foundation by implementing iterative feedback and refinement cycles using a single model for generation, critique, and revision **Henderson, "Iterative Feedback and Refinement Cycles"**. Then, several self-correction approaches have emerged as effective techniques for improving responses during generation **Kovashka, "Generative Models of Visual Attention"**, **Jain, "Improving Response Quality with Self-Correction"**. Recent work such as Critique Fine Tuning **Wang, "Learning to Critique towards Mathematical Tasks"** deals with learning to critique towards mathematical tasks and modifying model weights.

Prior work in language model safety primarily focuses on two key areas: safety training methods and jailbreak defense strategies. In the realm of safety training, researchers have traditionally relied on supervised finetuning (SFT) followed by reinforcement learning from human feedback (RLHF) **Bansal, "Supervised Finetuning with Reinforcement Learning"**. Direct Preference Optimization (DPO) **Liu, "Direct Preference Optimization for Language Models"** emerged as an alternative approach that circumvents the need for a reward model by directly optimizing the policy using preference data. Constitutional AI (CAI) **Harrison, "Constitutional AI: A Framework for Guided Behavior"** further expanded upon the SFT + RLHF paradigm by incorporating a predefined "constitution" to guide behavior, where the model critiques and revises its own responses based on constitutional principles during the SFT phase.

In response to jailbreak attacks, researchers have developed defense strategies that operate across three sequential stages. The first stage, prompt detection, utilizes perplexity detection (PPL) **Kumar, "Perplexity Detection for Adversarial Prefixes"** to identify adversarial suffixes. The second stage, prompt modification, encompasses two approaches: perturbing original prompts to neutralize adversarial suffixes (S-LM) **Li, "Self-Defense through Prompt Perturbation"**, adding defensive suffixes (PAT **Patel, "Prefix Alignment and Transformation"**, ICD **Ivanov, "Interactive Critique for Defense"**, SR **Singh, "Safety-Reinforced Suffix Addition"**). The final stage involves model fine-tuning through synthetic safety preference data (CST) **Tan, "Synthetic Safety Preference Data for Fine-Tuning"** and techniques to help models unlearn harmful knowledge (SafeUnlearn) **Wang, "SafeUnlearning: Unlearning Harmful Knowledge in Language Models"**. Notably, while traditional safety approaches never explicitly provide specifications to the policy model during training, Deliberative Alignment **Chen, "Deliberative Alignment: Policy Learning through Memorization and Application"** introduces a novel approach where the model memorizes policies in its \emph{chain of thought} and learns to apply them in context. This method also uniquely varies specification information across training examples, enabling more comprehensive safety policy learning. Our proposed approach enables online optimization of the self-critique process under diverse safety specifications without requiring parameter tuning.