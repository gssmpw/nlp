\documentclass{article}

\usepackage[T1]{fontenc}                % use 8-bit T1 fonts
\usepackage{booktabs}                   % professional-quality tables
\usepackage{nicefrac}                   % compact symbols for 1/2, etc.
\usepackage{microtype}                  % microtypography
\usepackage{url}                        % simple URL typesetting
\usepackage{xcolor}
\usepackage{xkcdcolors}

\usepackage{multirow}
\usepackage{wrapfig}

\usepackage{pgfplots}
%% matplotlib pgf plots require this command and assume it to do nothing
\def\mathdefault#1{#1}

% TODO: Switch paper to submission mode
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

\usepackage[preprint]{icml2025}

% Something in our setup messes up the table caption skips, so we just re-create the
% caption style as in the style guide.
\usepackage{caption}
\captionsetup[table]{position=above,skip=0.1in,font=small,labelfont=sl,labelsep=period}
\captionsetup[figure]{position=below,skip=0.1in,font=small,labelfont=sl,labelsep=period}

% Waste less space on lists
\usepackage{enumitem}
\setlist[itemize]{noitemsep,left=7pt,nosep}

% Hyperlinks. Load after biblatex
\usepackage[pdfusetitle]{hyperref}
\hypersetup{
  pdfsubject={Proceedings of the International Conference on Machine Learning 2024},
  colorlinks=true,
  linkcolor=xkcdCrimson,
  urlcolor=xkcdBluish,
  citecolor=xkcdLightNavy,
}

% Import bookmark after icml2025 so that it picks up the modified section commands
\usepackage{bookmark}                   % add PDF bookmarks

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts} % blackboard math symbols
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{thmtools,thm-restate}

\usepackage{siunitx}[=v2]

\usepackage[shortcuts=abbr]{glossaries-extra}
\glsdisablehyper{}

% TODO annotations
\usepackage[status=draft,layout=inline,author=TODO]{fixme}
\fxusetheme{color}
\newcommand{\todo}[1]{\fxnote{#1}}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% Remove useless whitespace at the top of wraptable environments
\makeatletter
\patchcmd\WF@putfigmaybe{\lower\intextsep}{}{}{\fail}%
\AddToHook{env/wraptable/begin}{\setlength{\intextsep}{0pt}}
\makeatother

% Intelligent referencing. Has to be loaded last.
\usepackage[capitalise]{cleveref}

% Render cref targets in pdf bookmarks, e.g. Theorem 3.4 instead of thm:example, and allow
% hiding color of cref links in section headings.
\usepackage{crossreftools}
\let\ORGhypersetup\hypersetup
\protected\def\hypersetup{\ORGhypersetup}
\pdfstringdefDisableCommands{%
  \def\hypersetup#1{}%
  \let\Cref\crtCref
  \let\cref\crtcref
}

% Put text in an unnumbered footnote
\newcommand\extrafootertext[1]{%
  \bgroup%
  \renewcommand\thefootnote{\fnsymbol{footnote}}%
  \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
  \footnotetext[0]{#1}%
  \egroup%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{accents}

\renewcommand\underbar[1]{\underaccent{\bar}{#1}}

\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
\newcommand{\vbeta}{\bm{\beta}}
\newcommand{\vdelta}{\bm{\delta}}
\newcommand{\vrho}{\bm{\rho}}
\newcommand{\vpsi}{\bm{\psi}}
\newcommand{\vepsilon}{\bm{\varepsilon}}
\newcommand{\pr}{\mathrm{p}}
\renewcommand{\Pr}{\mathrm{P}}
\newcommand{\qr}{\mathrm{q}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\LogUniform}{\operatorname{Log-Uniform}}
\newcommand{\LUcdf}{F}
\newcommand{\Np}{\mathcal{N}_{\mathrm{P}}}
\newcommand{\Rplus}{\mathbb{R}_{+}}
\newcommand{\Splus}{\mathrm{S}_{+}}
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\invT}{^{\shortminus{}\!\mathsf{T}}}
\newcommand{\inv}{^{\shortminus{}\hspace{-0.075em}1}}
\newcommand{\biginv}{^{\negthickspace\shortminus{}\hspace{-0.075em}1}}
\newcommand{\rsqrt}{^{\shortminus{}\hspace{-0.075em}\nicefrac{1}{2}}}
\newcommand{\bigprsqrt}{^{\negthickspace\shortminus{}\hspace{-0.075em}\nicefrac{1}{2}}}
\newcommand{\powsqrt}{^{\nicefrac{1}{2}}}
\newcommand{\kl}{D_{\mathrm{KL}}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator{\var}{\mathrm{Var}}

\newcommand{\ndim}{n}
\newcommand{\nrounds}{k}
\newcommand{\ncond}{c}
\newcommand{\nbuckets}{r}
\newcommand{\vxhat}{\hat{\vx}}
\newcommand{\model}{f_{\vtheta}}
\newcommand{\prmodel}{\pr_{\vtheta}}
\newcommand{\alphaR}{\alpha_{\mathrm{R}}}
\newcommand{\alphaM}{\alpha_{\mathrm{M}}}
\newcommand{\lambdaM}{\lambda_{\mathrm{M}}}
\newcommand{\lrecon}{\mathcal{L}_{\mathrm{R}}}
\newcommand{\lmeasurek}{\mathcal{L}_{\mathrm{M}}^{k}}
\newcommand{\lmeasureinf}{\mathcal{L}_{\mathrm{M}}^{\infty}}
\newcommand{\preskip}{c_{\mathrm{skip}}}
\newcommand{\preout}{c_{\mathrm{out}}}
\newcommand{\prein}{c_{\mathrm{in}}}

% Define shorthands to use abbreviations
\newcommand{\defabbrcmd}[1]{
\expandafter\def\csname#1\endcsname{{\gls{#1}}}
}
\newcommand{\defabbrcmds}[1]{
\expandafter\def\csname#1\endcsname{{\gls{#1}}}
\expandafter\def\csname#1s\endcsname{{\glspl{#1}}}
}

\newabbreviation{elbo}{ELBO}{evidence lower bound}
\defabbrcmds{elbo}
\newabbreviation{mc}{MC}{Monte Carlo}
\defabbrcmd{mc}
\newabbreviation{bsi}{BSI}{Bayesian Sample Inference}
\defabbrcmd{bsi}
\newabbreviation{bfn}{BFN}{Bayesian Flow Network}
\defabbrcmds{bfn}
\newabbreviation{vdm}{VDM}{Variational Diffusion Model}
\defabbrcmds{vdm}
\newabbreviation{dm}{DM}{diffusion model}
\defabbrcmds{dm}
\newabbreviation{fm}{FM}{flow matching}
\defabbrcmds{fm}
\newabbreviation{vit}{ViT}{Vision Transformer}
\defabbrcmds{vit}
\newabbreviation{dit}{DiT}{Diffusion Transformer}
\defabbrcmds{dit}
\newcommand{\unet}{U-Net}
\newcommand{\unets}{U-Nets}
\newabbreviation{cdf}{CDF}{cumulative distribution function}
\defabbrcmd{cdf}
\newabbreviation{sde}{SDE}{stochastic differential equation}
\defabbrcmds{sde}
\newabbreviation{snr}{SNR}{signal-to-noise ratio}
\defabbrcmd{snr}
\newabbreviation{fid}{FID}{Fr√©chet inception distance}
\defabbrcmd{fid}
\newcommand{\snrsym}{\nu}
\newcommand{\snrmin}{\operatorname{SNR}_{\mathrm{min}}}
\newcommand{\snrmax}{\operatorname{SNR}_{\mathrm{max}}}

%\icmltitlerunning{Bayesian Sample Inference}

\begin{document}

\twocolumn[
\icmltitle{Generative Modeling with Bayesian Sample Inference}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
  \icmlauthor{Marten Lienen}{tum}
  \icmlauthor{Marcel Kollovieh}{tum}
  \icmlauthor{Stephan G\"unnemann}{tum}
\end{icmlauthorlist}

\icmlaffiliation{tum}{School of Computation, Information and Technology \& Munich Data Science Institute, Technical University of Munich}

\icmlcorrespondingauthor{Marten Lienen}{m.lienen@tum.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{generative, bayesian, inference, diffusion}

\vskip 0.3in
]

\extrafootertext{Find our implementation at \href{https://github.com/martenlienen/bsi}{github.com/martenlienen/bsi}.}

\setlength{\columnsep}{10pt}

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
  We derive a novel generative model from the simple act of Gaussian posterior inference.
  Treating the generated sample as an unknown variable to infer lets us formulate the sampling process in the language of Bayesian probability.
  Our model uses a sequence of prediction and posterior update steps to narrow down the unknown sample from a broad initial belief.
  In addition to a rigorous theoretical analysis, we establish a connection between our model and diffusion models and show that it includes Bayesian Flow Networks (BFNs) as a special case.
  In our experiments, we demonstrate improved performance over both BFNs and Variational Diffusion Models, achieving competitive likelihood scores on CIFAR10 and ImageNet.
\end{abstract}

\section{Introduction}\label{sec:introduction}

The field of deep learning has produced a multitude of generative models over the years \citep{harshvardhan2020comprehensive}.
Variational autoencoders, for example, learn the data distribution by compressing data into a lower-dimensional representation \citep{kingma2013autoencoding}.
Normalizing flows learn to map between a prior and the data distribution in an invertible way, enabling exact likelihood computation \citep{rezende2015variational}.
Generative adversarial networks generate samples by pitting two models against each other such that one proposes artificial data samples while the other tries to distinguish real and generated \citep{goodfellow2014generative}.
Recently, \dms{} have become a cornerstone of generative modeling \citep{sohl-dickstein2015deep, ho2020denoising}.
They define a multi-step forward process that gradually adds noise to the data, turning it into pure noise.
Then, a model is trained to reverse this process, enabling the generation of new data samples by starting from noise and iteratively denoising.

\newpage
In this work, we take a Bayesian viewpoint of sample generation to propose a new generative model.
Imagine that a sample from the data distribution is fixed but unknown to us; however, we can receive noisy measurements of it.
Then we can infer the hidden sample by combining the information in these measurements.
To be more precise, we start with a broad belief about the sample in the form of a Normal distribution with low precision, i.e.\ high variance, that encompasses the entire data distribution.
Then, we can take a first noisy measurement and form the posterior belief about the sample, which will be a little more precise and a little more correct.
Repeating this process iteratively allows us to refine our estimate of the hidden sample to any desired level of precision.

We transform this inference process into a generative model by introducing a prediction model that estimates the hidden sample based on our current Gaussian belief.
Before taking the next noisy measurement, we replace the unknown sample with this prediction, effectively guiding the inference.
We train the prediction model by maximizing the likelihood that our generative process assigns to real data during training, so that it learns to map belief states onto the data distribution.
Consequently, the noisy measurements based on the predicted samples become indistinguishable from those of real samples, and our generative process converges toward producing new samples from the data distribution.

Our key \textbf{contributions} can be summarized as follows.
\begin{itemize}
  \item We present a new generative model based on iterative posterior inference from noisy predictions.
  \item We derive an evidence lower bound to enable effective likelihood optimization and show how we can reduce the variance of the training loss with importance sampling.
  \item Further, we compare our model in detail to \vdms{} \citep{kingma2023variational} and \bfns{} \citep{graves2023bayesian}.
  \item We show that \bfns{} are a special case of our model, providing a novel and simplified perspective on them, and analyze the relationship to \dms{}.
  \item Finally, we describe our model design and demonstrate in an empirical evaluation that our model surpasses both \vdms{} and \bfns{} in terms of likelihood scores on CIFAR10 and ImageNet and is competitive with other recent density estimation models.
\end{itemize}

% Figure here so that it shows up on page 2
\begin{figure*}[h]
  \centering
  \input{figures/illustration.pgf}
  \caption{We view generation as the problem of inferring the identity of an unknown sample $\vx$ from noisy observations.\ \textbf{1}.~To begin, our belief about $\vx$ is so broad as to cover the complete data distribution.\ \textbf{2}.~We use a model~$\model$ to guess which $\vx$ likely corresponds to the information we have collected so far.\ \textbf{3}.~Now, we pretend that $\mathcolor[rgb]{0.996078431372549,0.7019607843137254,0.03137254901960784}{\vxhat}$ is the true $\vx$ and take a noisy measurement $\mathcolor[rgb]{0.8274509803921568,0.28627450980392155,0.3058823529411765}{\vy}$.\ \textbf{4}.~We form the posterior belief about $\vx$ to incorporate the information contained in $\mathcolor[rgb]{0.8274509803921568,0.28627450980392155,0.3058823529411765}{\vy}$.\ \textbf{5}.~Repeat until we have identified a new sample with sufficient precision $\lambda_{i}$.}\label{fig:illustration}
\end{figure*}

\paragraph{Notation}
We parametrize Normal distributions either with a variance $\sigma^{2}$ as $\N(\vmu, \sigma^{2}\mI)$ or with a precision $\lambda = \nicefrac{1}{\sigma^{2}}$ as $\Np(\vmu, \lambda\mI)$.
Since all Normal distributions in this work are isotropic, we shorten these to $\N(\vmu, \sigma^{2})$ and $\Np(\vmu, \lambda)$.
$[n]$~is the set of integers $1, \ldots, n$ and $\Rplus$ refers to the non-negative reals.

\section{Sample Discovery through Iterative Measurement}\label{sec:sample-discovery}

Consider a sample $\vx \in \R^{\ndim}$ that is unknown to us, though we can access noisy measurements $\vy_{i}\sim \Np(\vx, \alpha_i)$ of it.
Then we can infer $\vx$ from $\vy_{i}$ through Bayesian inference.
We start with a broad initial belief $\pr(\vx) \sim \Np(\vmu_{0}, \lambda_{0})$ and update it with information contained in $\vy_{i}$ per the following lemma.
\begin{lemma}[Posterior Update]\label{thm:posterior}
  Let $\vx, \vmu \in \R^{\ndim}$ and $\lambda \in \Rplus$ such that $\vx$ is latent and $\pr(\vx) = \Np(\vx \mid \vmu, \lambda)$ is a prior on $\vx$; and $\vy \sim \Np(\vx, \alpha)$ where $\alpha \in \Rplus$.
  Then the posterior is $\pr(\vx \mid \vy) = \Np(\vx \mid \vmu', \lambda')$ with
  \begin{equation}
    \lambda' = \lambda + \alpha \quad \text{and} \quad \vmu' = \nicefrac{1}{\lambda'} \left[ \lambda\vmu + \alpha\vy \right]. \label{eq:posterior-update}
  \end{equation}
\end{lemma}
\begin{proof}
  See \citep[Section 4.4.1]{murphy2012machine}.
\end{proof}
We can now iterate over the noisy measurements and update our belief until $\pr(\vx \mid \vy_{1}, \ldots, \vy_{\nrounds}) \sim \Np(\vmu_{\nrounds}, \lambda_{\nrounds})$ identifies $\vx$ with sufficient precision.
Sufficiency depends on the application but could, for example, for images be defined such that most of the probability mass for each dimension of an image $\vx$ is contained within a single color intensity bin of width $\nicefrac{1}{256}$ for 8-bit color.
Note that, at each step, all information contained in $\vy_{1}, \ldots, \vy_{\nrounds}$ is captured in the current $\vmu_{\nrounds}$.

\section{Sample Generation with Posterior Inference}\label{sec:framework}

We turn the procedure in \cref{sec:sample-discovery} into a generative model, which we call \emph{Bayesian Sample Inference} (BSI), as follows.
% Mark bsi as expanded
\glsunset{bsi}
We begin with an initial belief $(\vmu_{0}, \lambda_{0})$ about the sample $\vx$ which we will generate in the end, with $\vmu_{0}$ sampled from a suitable prior distribution $\pr(\vmu_{0})$ and $\lambda_{0}$ fixed.
Obviously, $\vx$ is unknown a priori, so we cannot measure it, but we can estimate it from the information we have gathered so far.
Let $\model : \R^{\ndim} \times \Rplus \to \R^{\ndim}$ be a learned model with parameters $\vtheta$ that estimates which unknown sample $\vx$ we have observed so far from our current belief $(\vmu_{i}, \lambda_{i})$.
We estimate $\vx$ as $\vxhat_{i} = \model(\vmu_{i-1}, \lambda_{i-1})$ and sample a noisy measurement $\vy_{i} \sim \Np(\vxhat, \alpha_{i})$ of $\vxhat$ in place of $\vx$ with precision $\alpha_{i}$.
Then, we can then update our belief with $\vy_{i}$ and \cref{thm:posterior} to the posterior $(\vmu_{i}, \lambda_{i})$.
Now, we alternate between these two steps, i.e.\ predicting and taking a noisy measurement followed by updating our current belief, until the posterior precision $\lambda_{i}$ is sufficient.
Finally, we return $\vxhat_{i+1} = \model(\vmu_{i}, \lambda_{i})$ as our generated sample.
See \cref{alg:generate} for a formal description and \cref{fig:illustration} for a visual explanation.

\begin{algorithm}
  \caption{Sampling with posterior inference}
  \begin{algorithmic}[1]
    \INPUT Initial precision $\lambda_{0}$, precision schedule $\alpha_{i}$ for $i \in [\nrounds]$
    \OUTPUT Sample $\vxhat^{*}$
    \STATE Initialize belief $(\vmu_{0}, \lambda_{0})$ with $\vmu_{0} \sim \pr(\vmu_{0})$
    \FOR{$i = 1, 2, \ldots, \nrounds$}
    \STATE $\vxhat_{i-1} = \model(\vmu_{i - 1}, \lambda_{i - 1})$
    \STATE $\vy_{i} \sim \Np(\vxhat_{i-1}, \alpha_{i})$
    \STATE Update belief $\pr(\vx \mid \vy_{1}, \ldots, \vy_{i}) = \Np(\vmu_{i}, \lambda_{i})$:
    \STATE \quad $\lambda_{i} = \lambda_{i-1} + \alpha_{i}$
    \STATE \quad $\vmu_{i} = \nicefrac{1}{\lambda_{i}} [\lambda_{i-1} \vmu_{i-1} + \alpha_{i}\vy_{i}]$
    \ENDFOR
    \STATE Return $\vxhat^{*} = \model(\vmu_{\nrounds}, \lambda_{\nrounds})$
    \label{alg:generate}
  \end{algorithmic}
\end{algorithm}

Since the posterior precision $\lambda_{i}$ does not depend on the generated sample $\vxhat_{i}$, we can choose the number of measurement rounds $\nrounds$ and precision schedule $\alpha_{i}$ a priori such that $\lambda_{\nrounds}$ will always be sufficiently large.

We have collected the proofs of all formal statements in this section in \cref{sec:proofs}.

\subsection{Evidence Lower Bound}\label{sec:elbo}

By interpreting \bsi{} as a hierarchical latent variable model, we derive an \elbo{} \citep{kingma2013autoencoding}, i.e.\ a lower bound on $\log\pr(\vx)$ assigned to a data point by our model.
The \elbo{} will then serve as a natural training target for $\model$ to ensure that true data samples have high likelihood under our model.

We form our hierarchy out of the sequence of belief means $\{\vmu_{i}\}$, giving us
\begin{equation}
  \pr(\vx) = \E_{\pr(\vmu_{0}) \cdot \pr(\vmu_{1} \mid \pr(\vmu_{0}) \cdots \pr(\vmu_{\nrounds} \mid \vmu_{\nrounds - 1}))}[\pr(\vx \mid \vmu_{\nrounds})].
\end{equation}
The precisions $\{\lambda_{i}\}$ are not included as latent variables, because they do not depend on $\vx$.
With this hierarchy, we can derive the following \elbo{}.

\begin{restatable}{theorem}{thmelbo}\label{thm:elbo}
  Let $\vx \in \R^{\ndim}$ and $\alphaR, \alpha_{i} \in \Rplus, i \in [\nrounds]$.
  Then the log-likelihood of $\vx$ is lower-bounded as
  \begin{equation}
    \log \pr(\vx) \ge -\lrecon - \lmeasurek
  \end{equation}
  by a reconstruction term $\lrecon$ and a measurement term $\lmeasurek$,
  \begin{equation}
    \begin{aligned}
      \lrecon    & = \E_{\qr(\vmu_{\nrounds} \mid \vx, \lambda_{\nrounds})}\big[{\shortminus}\negthinspace\log\Np(\vx \mid \vxhat_{k}, \alphaR)\big] \\
      \lmeasurek & = \frac{\nrounds}{2} \E_{\substack{i \sim \mathcal{U}(0, k - 1) \\\qr(\vmu_{i} \mid \vx, \lambda_{i})}} \Big[ \alpha_{i+1} \|\vx - \vxhat_{i}\|_{2}^{2} \Big]
    \end{aligned}
  \end{equation}
  where
  \begin{equation}
    \def\arraystretch{1.4}
    \begin{array}{c}
      \qr(\vmu_{i} \mid \vx, \lambda_{i}) = \E_{\pr(\vmu_{0})} \big[\pr(\vmu_{i} \mid \vmu_{0}, \vx, \lambda_{i})\big], \\
      \vxhat_{i} = \model(\vmu_{i}, \lambda_{i}) \quad\text{and}\quad \lambda_{i} =  \lambda_{0} + \sum_{j=1}^{i} \alpha_{j}.
    \end{array}
  \end{equation}
\end{restatable}

The measurement term $\lmeasurek$ corresponds to the noisy measurement and update loop in \cref{alg:generate} and $\lrecon$ to the final computation of the sample $\vxhat^{*}$.
$\qr(\vmu_{i} \mid \vx, \lambda_{i})$ is the distribution of our belief $(\vmu_{i}, \lambda_{i})$ about the unknown sample $\vx$ after $i$ steps if we would have observed the true $\vx$ instead of $\vxhat_{1}, \ldots, \vxhat_{i}$.
$\pr(\vmu_{i} \mid \vmu_{0}, \vx, \lambda_{i})$ is the marginal distribution of possible posterior beliefs $(\vmu_{i}, \lambda_{i})$ with posterior precision $\lambda_{i}$ reachable from an initial belief $(\vmu_{0}, \lambda_{0})$.
Equivalently, $\pr(\vmu_{i} \mid \vmu_{0}, \vx, \lambda_{i})$ is the distribution of beliefs $(\vmu_{i}, \lambda_{i})$ after updating our initial belief $(\vmu_{0}, \lambda_{0})$ with a single measurement of $\vx$ with \cref{thm:posterior} -- marginalized over all possible noisy measurements $\vy$ at precision $\alpha = \lambda_{i} - \lambda_{0}$.

On closer examination, we see that $\lrecon$, measuring how accurately we can reconstruct $\vx$ at the end, only depends on the total precision $\lambda_{k}$ that we accumulated in the first phase of the algorithm.
However, $\lmeasurek$ depends both on the number of rounds $\nrounds$ and the precision schedule $\alpha_{i}$.
We can derive an \elbo{} independent of $\nrounds$ and $\alpha_{i}$ by considering the limit as $\nrounds \to \infty$ and refining the precision schedule $\{\alpha_{i}\}_{i = 1}^{\nrounds}$ into smaller and smaller steps while keeping the total precision $\alphaM = \sum_{i = 1}^{\nrounds} \alpha_{i}$ constant.

\begin{restatable}{theorem}{thminfiniteelbo}
  \label{thm:infinite-elbo}
  Let $\alphaR, \alphaM \in \Rplus$.
  For any sequence of precision schedules $\alpha_{k,i}$ for $k \in \sN, i \in [k]$ such that $\sum_{i = 1}^{k} \alpha_{k, i} = \alphaM$ and the sequence of functions $[k] \to \Rplus: i \mapsto \alpha_{k,i}$ converges uniformly to $0$, we can take the limit of \cref{thm:elbo} as $k \to \infty$ to get
  \begin{equation}
    \begin{aligned}
      \lrecon      & = \E_{\qr(\vmu_{\lambdaM} \mid \vx, \lambdaM)}\big[{\shortminus}\negthinspace\log\Np(\vx \mid \vxhat_{\lambdaM}, \alphaR)\big] \\
      \lmeasureinf & = \frac{\alphaM}{2} \E_{\substack{\lambda\sim\mathcal{U}(\lambda_{0}, \lambdaM) \\ \qr(\vmu_{\lambda} \mid \vx, \lambda)}} \big[\|\vx - \vxhat_{\lambda}\|_{2}^{2}\big]
    \end{aligned}\label{eq:infinite-elbo}
  \end{equation}
  where $\qr(\vmu_{\lambda} \mid \vx, \lambda) = \E_{\pr(\vmu_{0})} \big[\pr(\vmu_{\lambda} \mid \vmu_{0}, \vx, \lambda)\big]$, $\lambdaM = \lambda_{0} + \alphaM$ and $\vxhat_{\lambda} = \model(\vmu_{\lambda}, \lambda)$.
\end{restatable}

As long as our model is more accurate in reconstructing $\vx$ from more precise measurements, a reasonable assumption, \cref{thm:infinite-elbo} is a tighter bound on the log-likelihood than \cref{thm:elbo}.
To see this, we rewrite $\lmeasureinf$ in terms of the expected squared error at belief precision $\lambda$
\begin{equation}
  h(\lambda) = \E_{\qr(\vmu_{\lambda} \mid \vx, \lambda)} \|\vx - \vxhat_{\lambda}\|_{2}^{2} \label{eq:h}
\end{equation}
as
\begin{equation}
  \lmeasureinf = \frac{\alphaM}{2} \E_{\substack{\lambda\sim\mathcal{U}(\lambda_{0}, \lambdaM)}} [h(\lambda)]
\end{equation}
for which we have the following result.

\begin{restatable}{lemma}{thmelbotighter}
  \label{thm:elbo-tighter}
  If $h$ is strictly decreasing, $\lmeasureinf < \lmeasurek$ for any $k$ and any precision schedule $\{\alpha_{i}\}$.
\end{restatable}

\subsection{Prior Distribution}\label{sec:prior-distribution}

Let's consider possible priors of the form $\pr(\vmu_{0}) = \Np(\vzero, \gamma_{0})$ for our initial belief.
Then we have the following result for the encoding distribution $\qr(\vmu_{\lambda} \mid \vx, \lambda)$ in \cref{thm:elbo,thm:infinite-elbo}.

\begin{restatable}{lemma}{thmelboclosedform}\label{thm:elbo-encoder-closed-form}
  Let $\lambda_{0}, \gamma_{0} \in \Rplus$, $\pr(\vmu_{0}) = \Np(\vzero, \gamma_{0})$ and $\lambda \ge \lambda_{0}$.
  Then
  \begin{equation}
    \qr(\vmu_{\lambda} \mid \vx, \lambda) = \Np\Big( \nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \lambda^{2}{(\lambda - \lambda_{0} + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}})}\inv \Big).
  \end{equation}
\end{restatable}

How should we choose $\gamma_{0}$?
We start the sampling process with initial precision, i.e.\ confidence, $\lambda_{0}$.
If $\lambda_{0}$ was larger than $\gamma_{0}$, we would be unreasonably confident in our initial belief, since we know that $\vmu_{0}$ has more uncertainty than $\lambda_{0}$.
From this, we deduce that the reasonable range for $\gamma_{0}$ is $[\lambda_{0}, \infty]$.
At the same time, we want to avoid unwarranted assumptions by the prior, so we choose $\gamma_{0}~=~\lambda_{0}$ for our model, which also gives us a particularly simple form for the encoding distribution.

\begin{restatable}{corollary}{thmbsielboclosedform}\label{thm:bsi-elbo-encoder-closed-form}
  Let $\lambda_{0} \in \Rplus$, $\pr(\vmu_{0}) \sim \Np(\vzero, \lambda_{0})$ and $\lambda \ge \lambda_{0}$.
  Then
  \begin{equation}
    \qr(\vmu_{\lambda} \mid \vx, \lambda) = \Np\big(\nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \lambda\big).
  \end{equation}
\end{restatable}

\subsection{Variance Reduction}\label{sec:variance-reduction}

The squared distance $\|\vx - \vxhat_{\lambda}\|_{2}^{2}$ in $\lmeasureinf$ will necessarily vary significantly across the range of $\lambda$ with large values for small $\lambda$ where $\qr(\vmu_{\lambda} \mid \vx, \lambda) \approx \pr(\vmu_{0})$ and small values for large $\lambda$ when $\vmu_{\lambda} \approx \vx$.
We can reduce the variance of \mc{} estimates of $\lmeasureinf$ for \elbo{} evaluation or gradient computation in training with importance sampling with a suitable proposal distribution $\pr(\lambda)$.

\begin{restatable}{corollary}{thmimportancesampling}
  \label{thm:importance-sampling}
  Let $\pr(\lambda)$ be a probability distribution with support $[\lambda_{0}, \lambdaM]$.
  Then we have
  \begin{equation}
    \lmeasureinf = \frac{1}{2} \E_{\substack{\lambda\sim\pr(\lambda) \\ \qr(\vmu_{\lambda} \mid \vx, \lambda)}} \bigg[ \frac{1}{\pr(\lambda)} \|\vx - \vxhat_{\lambda}\|_{2}^{2} \bigg].
  \end{equation}
\end{restatable}

We can further rewrite $\lmeasureinf$ as
\begin{equation}
  \lmeasureinf = \frac{1}{2} \E_{\lambda\sim\pr(\lambda)} \bigg[ \frac{h(\lambda)}{\pr(\lambda)} \bigg] \label{eq:is-lm}
\end{equation}
with $h$ as defined in \cref{eq:h}.
To minimize the variance of \mc{} estimates of $\lmeasureinf$, we want to bring $\nicefrac{h(\lambda)}{\pr(\lambda)}$ as close to a constant as possible.
If it were actually constant, the variance of the \mc{} estimate would be zero.

Let's begin by examining $h$ more closely.
If we approximate $\model$ as $\model(\vmu, \lambda) = \vmu$ and assume that $\vx$ is normalized to zero mean and unit variance, we get the closed form
\begin{equation}
  \E_{\vx}[h(\lambda)] \propto \frac{\lambda_{0}^{2}}{\lambda^{2}} + \frac{1}{\lambda}. \label{eq:expected-h}
\end{equation}
While $\model(\vmu, \lambda) = \vmu$ might seem a crude approximation at first, it is not too far off for large $\lambda$ where the model just needs to predict a small correction to its input.

\cref{eq:expected-h} suggests that we should choose $\pr(\lambda) \propto \nicefrac{\lambda_{0}^{2}}{\lambda^{2}} + \nicefrac{1}{\lambda}$ to minimize the variance of \mc{} estimates.
While evaluating $\pr(\lambda)$ is simple enough, we would need to invert its \cdf{} numerically to sample from it.
Instead, we recognize that $\nicefrac{1}{\lambda}$ dominates $\nicefrac{\lambda_{0}^{2}}{\lambda^{2}}$ except for the smallest $\lambda$ and choose $\pr(\lambda) \propto \nicefrac{1}{\lambda}$, i.e.\ a standard $\LogUniform(\lambda_{0}, \lambdaM)$ distribution.

\subsection{Training \& Sampling}\label{sec:algorithms}

We train our model with the \elbo{} from \cref{thm:infinite-elbo} by optimizing $\lmeasureinf / \ndim$.
We do not optimize $\lrecon$ directly as its magnitude is negligible for sufficiently large $\alphaM$ and it is structurally similar to $\lmeasureinf$, i.e.\ both amount to a squared distance.
\cref{alg:loss} shows the resulting algorithm with our belief prior $\pr(\vmu_{0})$ and proposal distribution $\pr(\lambda)$.
Similarly, \cref{alg:generate-specific} implements the abstract \cref{alg:generate} with our belief prior.

\begin{algorithm}
  \caption{Estimating the BSI training loss}
  \begin{algorithmic}[1]
    \INPUT Data sample $\vx$
    \OUTPUT Monte Carlo estimate of $\lmeasureinf$
    \STATE Sample $t \sim \mathcal{U}(0, 1)$, $\vepsilon \sim \N(\vzero, \mI)$
    \STATE $\lambda = \exp\big((\log\lambdaM - \log\lambda_{0}) \cdot t + \log(\lambda_{0})\big)$
    \STATE $\vmu_{\lambda} = \nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx + \sqrt{\nicefrac{1}{\lambda}}\,\vepsilon$
    \STATE Return $(\log\lambdaM - \log\lambda_{0})\,\lambda \cdot \|\vx - \model(\vmu_{\lambda}, \lambda)\|_{2}^{2}$
    \label{alg:loss}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Sampling with BSI}
  \begin{algorithmic}[1]
    \INPUT Initial precision $\lambda_{0}$, precision schedule $\alpha_{i}$ for $i \in [\nrounds]$
    \OUTPUT Sample $\vxhat^{*}$
    \STATE Sample $\vepsilon_{i} \sim \N(\vzero, \mI), i = 0, \ldots, \nrounds$
    \STATE $\vmu_{0} = \sqrt{\nicefrac{1}{\lambda_{0}}}\,\vepsilon_{0}$
    \FOR{$i = 1, 2, \ldots, \nrounds$}
    \STATE $\vxhat_{i-1} = \model(\vmu_{i - 1}, \lambda_{i - 1})$
    \STATE $\lambda_{i} = \lambda_{i - 1} + \alpha_{i}$
    \STATE $\vmu_{i} = \lambda_{i}\inv \Big(\lambda_{i-1} \vmu_{i-1} + \alpha_{i}\big(\vxhat_{i-1} + \sqrt{\nicefrac{1}{\alpha_{i}}}\vepsilon_{i}\big)\Big)$
    \ENDFOR
    \STATE Return $\vxhat^{*} = \model(\vmu_{\nrounds}, \lambda_{\nrounds})$
    \label{alg:generate-specific}
  \end{algorithmic}
\end{algorithm}

\section{Discussion}\label{sec:discussion}

\begin{table*}[t]
  \centering
  \caption{Central structures of \vdms{}, \bfns{} and \bsi{}. To improve comparability, we parametrize \vdm{} in terms of the \snr{}~$\snrsym$. \bfn{} and \bsi{} are parametrized with the precision~$\alpha$ as introduced in \cref{sec:framework}. $\vepsilon_{i} \sim \N(\vzero, \mI)$ is sampling noise.}\label{tab:distributions}
  \begin{tabular*}{\textwidth}{rccccc}
    \toprule
    Model & \multicolumn{1}{c}{ELBO Encoder $\qr(\vpsi \mid \vx, \omega)$} & Latent Prior & Update Step for Sampling \\
    \midrule
    VDM   & $\qr(\vz \mid \vx, \snrsym) = \Np\Big(\sqrt{\frac{\snrsym}{1 + \snrsym}}\,\vx, 1 + \snrsym\Big)$ & $\vz_{T} \sim \Np(\vzero, 1)$ & \scalebox{0.92}{$\vz_{i} = \frac{\sqrt{\snrsym_{i + 1}(1 + \snrsym_{i + 1})}\,\vz_{i + 1} + (\snrsym_{i} - \snrsym_{i + 1})\big(\vxhat_{i} + \sqrt{\frac{1}{\snrsym_{i} - \snrsym_{i + 1}}}\vepsilon_{i}\big)}{\sqrt{\snrsym_{i}(1 + \snrsym_{i})}}$} \\
    \midrule
    BFN   & $\qr(\vmu \mid \vx, \lambda) = \Np(\nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \nicefrac{\lambda^{2}}{\lambda - \lambda_{0}})$ & $\vmu_{0} = \vzero$ & \multirow{2}{*}{$\vmu_{i} = \frac{\lambda_{i-1} \vmu_{i-1} + \alpha_{i}\big(\vxhat_{i-1} + \sqrt{\frac{1}{\alpha_{i}}}\vepsilon_{i}\big)}{\lambda_{i - 1} + \alpha_{i}}$} \\
    \cmidrule{1-3}
    BSI   & $\qr(\vmu \mid \vx, \lambda) = \Np(\nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \lambda)$ & $\vmu_{0} \sim \Np(\vzero, \lambda_{0})$ & \\
    \bottomrule
  \end{tabular*}
\end{table*}
\begin{figure*}[t]
  \centering
  \includegraphics{figures/noise-levels}
  \caption{\elbo{} encoders $\qr$, i.e.\ training inputs, of \bsi{}, \bfns{} and \vdms{}. $t$ parametrizes the precision levels by the respective model's precision schedule with $t = 0$ being pure noise, ideally, and $t = 1$ almost equaling the data. Top half shows the mean of $\qr$ and bottom half a sample. Mean $\vzero$ is gray because all models rescale the data to $[-1, 1]$.\ \bfns{} apply little noise overall and reach a deterministic state at $t = 0$. For \vdms{}, significant information about the sample is preserved in the structure of the mean at the highest noise level. In contrast, \bsi{} converges to its latent prior distribution.}\label{fig:noise-levels}
\end{figure*}

We are aware of two generative models that are closely related to \bsi{}, \bfns{} \citep{graves2023bayesian} and \vdms{} \citep{kingma2023variational}.
\bfns{} are generative models motivated from an information theory perspective with a sender and a receiver communicating about the sample.
We show in \cref{sec:bfns} that \bfns{} are a special case of our framework in \cref{sec:framework} if you choose $\gamma_{0} = \infty$ instead of $\gamma_{0} = \lambda_{0}$ for the initial belief prior, meaning that sampling always starts from the deterministic $\vmu_{0} = \vzero$.
\vdms{} are a type of \dm{} that have shown excellent performance in likelihood-based modeling.
They are similar to \bsi{} insofar as they specify the distribution of latent variables directly rather than defining a Markovian noising process as classical \dms{} do.

All three models admit an \elbo{} of the form
\begin{equation}
  -\log\pr(\vx) \le \lrecon + \frac{\bar{\omega} - \underbar{\omega}}{2} \E_{\substack{\omega\sim\mathcal{U}(\underbar{\omega}, \bar{\omega}) \\ \qr(\vpsi_{\omega} \mid \vx, \omega)}} \big[\|\vx - \vxhat_{\omega}\|_{2}^{2}\big]
\end{equation}
for a set of latent variables $\vpsi$ at precision levels $\omega$ between $\underbar{\omega}$ and $\bar{\omega}$.
For \bsi{} and \bfns{}, the precision level $\omega$ is the belief precision $\lambda$ between $\lambda_{0}$ and $\lambdaM$ and $\vpsi_{\omega} = \vmu_{\lambda}$.
For \vdms{}, the latent variables $\vpsi$ are called $\vz$ and they parametrize $\omega$ as the \snr{} $\snrsym$ between $e^{-5}$ and $e^{13.3}$.

Despite this shared \elbo{} form, the models vary significantly.
\cref{tab:distributions} lists the encoding distribution $\qr(\vpsi \mid \vx, \omega)$ for each model, their prior, from which they begin the sampling process, and the update step that the models iterate during sampling.
First, we see that \vdms{} start sampling from a standard Normal vector and \bfns{} from the deterministic $\vzero$.
Only \bsi{} allows sampling from an initial precision $\lambda_{0}$ less than $1$, which has been shown to improve sample diversity in consistency models \citep{song2024improved}.
Second, the update step shared between \bsi{} and \bfns{} is significantly simpler than the \vdm{} update with respect to the precision parameter and does not require working in log-space for numerical stability as recommended for \vdm{} \citep{kingma2023variational}.

For the encoding distribution $\qr(\vpsi \mid \vx, \omega)$, which provides the training inputs when the models optimize their \elbo{}, we turn to \cref{fig:noise-levels}.
First, we note that \bfns{} add little noise overall due to their noise variance $\nicefrac{\lambda - \lambda_{0}}{\lambda^{2}}$ going to $0$ for both small and large $\lambda$.
Next, we notice the encoding distribution $\qr(\vpsi \mid \vx, \underbar{\omega})$ with the most noise at $t = 0$.
While it agrees exactly with the latent prior used for sampling for \bsi{} and \bfns{}, for \vdms{} it becomes approximately $\Np(0.08\vx, 1)$, which differs significantly from the standard Normal prior for sampling.
In fact, the image motif is still clearly discernible in the distribution mean for \vdm{} at its maximum noise level.
The amount of signal remaining in the mean for \bsi{} at high noise levels is counteracted by much higher noise variance, e.g.\ $15.85$ at $t = 0.1$ for \bsi{} compared to $0.96$ for \vdms{}.

\paragraph{Diffusion Models}
If we currently hold the belief $(\vmu', \lambda')$, the distribution over beliefs $(\vmu, \lambda' - \alpha)$ that are $\alpha$ less precise is
\begin{equation}
  \pr(\vmu \mid \vmu', \vx) = \N\Bigg( \xi\inv \bigg[ \frac{\lambda\lambda'}{\alpha}\vmu' - \lambda_{0}\vx \bigg], \xi \Bigg)
\end{equation}
for a certain precision $\xi$.
This shows that \bsi{} can be forced into the form of a \dm{} with a non-Markovian forward or ``noising'' process.
See \cref{sec:diffusion-models} for a detailed derivation of this connection.
There we also exploit that \bfns{} are a special case of \bsi{} to derive the forward process for \bfns{} and show that it is Markov, in contrast to the \bsi{} process.

\section{Model Design}\label{sec:model-design}

In this section, we introduce a design for the prediction model in \bsi{}.
We begin by deriving a preconditioning structure for $\model$, i.e.\ a type of model structure similar to noise prediction in \dms{}.
Then, we describe how we bring $\lambda$ into a suitable range as an input for deep learning.
Finally, we give our choice of the hyperparameters $\lambda_{0}$, $\alphaM$ and $\alphaR$ and report the model architectures we used as the backbone of $\model$.

\subsection{Preconditioning}\label{sec:preconditioning}

It has long been known in the context of \dms{} that training models to predict $\vx$ directly from a noisy input can hinder learning and limit sample quality \citep{karras2022elucidating,ho2020denoising}.
For probabilistic modeling, it is especially important that the model prediction stays close to the true sample if the input is already at a low noise level to achieve high \elbos{}.
This can be seen, for example, in \cref{thm:importance-sampling} where prediction errors for high-precision input beliefs with large $\lambda$ have a higher weight.
Instead of predicting $\vx$, \dms{} commonly either predict a variation of the noise in the model input \citep{ho2020denoising,song2021denoising} or an adaptive mixture of the noise and the true sample \citep{salimans2021progressive}.
In the end, these approaches amount to adding a skip connection to the model with specific weights.

For \bsi{}, we derive such a preconditioning structure with the adaptive-mixture approach from \citet{karras2022elucidating}.
Let $\model'$ be our neural network.
Then we define the preconditioned $\model$ as
\begin{equation}
  \model(\vmu, \lambda) = \preskip \vmu + \preout \model'(\prein\vmu, \lambda) \label{eq:preconditioned-model}
\end{equation}
and find the parameters through the conditions proposed by \citet{karras2022elucidating}.
$\prein$ and $\preout$ are chosen such that the input to $\model'$ and its training target have unit variance.
$\preskip$ is then chosen to minimize $\preout$, which minimizes the influence of prediction errors and ensures that $\model$ retains most of the signal already contained in $\vmu$ at large precisions~$\lambda$.

From these conditions, we derive
\begin{equation}
  \begin{aligned}
    \preskip = \nicefrac{\lambda - \lambda_{0}}{\kappa}, \quad \preout = \sqrt{\nicefrac{1}{\kappa}}, \quad \prein = \sqrt{\nicefrac{\lambda}{\kappa}} \end{aligned} \label{eq:preconditioning}
\end{equation}
where $\kappa = 1 + \nicefrac{{(\lambda - \lambda_0)}^{2}}{\lambda}$ in \cref{sec:preconditioning-derivation}.
$\lambda$ is the precision of our current belief about $\vx$ and the input to $\model$.

\subsection{Precision Encoding}\label{sec:precision-encoding}

The magnitude of $\lambda$ makes it impractical as a feature for neural networks.
However, the \cdf{} $\LUcdf$ of $\pr(\lambda)$ is a natural way to scale $\lambda$ from $[\lambda_{0}, \lambdaM]$ to $[0, 1]$ as in \dms{} and \fm{} \citep{lipman2023flow}.
In practice, we use $\model(\vmu, t)$ instead of $\model(\vmu, \lambda)$ where
\begin{equation}
  t = \LUcdf(\lambda) = \frac{\log\lambda - \log\lambda_{0}}{\log(\lambdaM) - \log\lambda_{0}}.
\end{equation}
Compared to linear re-scaling, our method makes it easier for $\model$ to distinguish belief precisions in the high-noise regime.

\subsection{Hyperparameters}\label{sec:hyperparameters}

Apart from $\model$, \bsi{} has three hyperparameters, $\lambda_{0}$, $\alphaM$ and $\alphaR$.
$\lambda_{0}$ should be small enough that the initial belief covers the whole data distribution.
We have found experimentally that $\lambda_{0} = 10^{-2}$ works well for images rescaled to $[-1, 1]$.
This agrees with the finding of \citet{song2024improved} that large initial noise scales improve sample diversity in consistency models.

$\alphaM$ should be large enough that a noisy measurement at precision $\alphaM$ identifies an $\vx$, e.g.\ for images almost all probability mass of $\Np(\vx, \alphaM)$ should be contained within a single 8-bit color intensity bin.
We choose $\alphaM = 10^{6}$, which \citet{graves2023bayesian} also picked for \bfns{}.
While $\lmeasureinf$ dwarfs $\lrecon$, $\alphaR = 2\alphaM$ gives a slight edge in likelihood, empirically, as also observed by \citet{graves2023bayesian}.

% Here so that it goes on page 7
\begin{table*}[ht]
  \centering
  \caption{Log-likelihood in bits per dimension on the test set. Results marked with * were computed on an old version of ImageNet32/64 that was not available to the authors.}\label{tab:bpd}
  \begin{tabular}{lcccc}
    \toprule
    Model                                & Likelihood & CIFAR10 & ImageNet32                   & ImageNet64              \\
    \midrule
    Flow Matching \citep{lipman2023flow} & exact      & 2.99    & 3.53                         & 3.31                    \\
    i-DODE \citep{zheng2024improved}     & exact      & 2.56    & 3.43/3.69\textsuperscript{*} & -                       \\
    \midrule
    VDM \citep{kingma2023variational}    & ELBO       & 2.65    & 3.72\textsuperscript{*}      & 3.40\textsuperscript{*} \\
    BFN \citep{graves2023bayesian}       & ELBO       & 2.66    & -                            & -                       \\
    BSI                                  & ELBO       & 2.64    & 3.44                         & 3.22                    \\
    \bottomrule
  \end{tabular}
\end{table*}
\begin{figure*}[h]
  %--- First figure ---%
  \begin{minipage}[t]{1.9in}
    \centering
    \input{figures/importance-sampling.pgf}
    \captionof{figure}{Our log-uniform proposal distribution shrinks range of $\nicefrac{h(\lambda)}{\pr(\lambda)}$, reducing the variance of the ELBO.}\label{fig:importance-sampling}
  \end{minipage}
  \hfill
  %--- Second figure ---%
  \begin{minipage}[t]{1.75in}
    \centering
    \input{figures/elbo-convergence.pgf}
    \captionof{figure}{$\lmeasurek$ converges to $\lmeasureinf$ from above as predicted in \cref{thm:elbo-tighter}.}\label{fig:elbo-convergence}
  \end{minipage}
  \hfill
  %--- Third figure ---%
  \begin{minipage}[t]{2.85in}
    \centering
    \input{figures/fid-convergence.pgf}
    \captionof{figure}{Sampling with the EDM7 schedule improves FID over a linear schedule for few steps without retraining, but both schedules converge as sampling steps increase.}\label{fig:fid-convergence}
  \end{minipage}
\end{figure*}

\subsection{Architecture}\label{sec:architecture}

After the preconditioning and mapping $\lambda$ to a $t \in [0, 1]$, there are two more steps to turn the inputs $\vmu$ and $t$ of $\model'$ into effective features for a neural network.
Regarding $t$, we convert it into a $32$-dimensional precision embedding with a sinusoidal position encoding \citep{vaswani2017attention}.

The Fourier features proposed by \citet{kingma2023variational} are an essential component to reach high likelihoods, because they help the model distinguish fine details at high likelihoods, i.e.\ for inputs that are already close to the data distribution.
They are basically a sinusoidal embedding of every dimension of $\vmu$.
In particular, we extend $\vmu$ to the vector
\begin{equation}
  \begin{pmatrix}
    \vmu \\ \sin(2^{i}\pi \vmu) \\ \cos(2^{i}\pi \vmu)
  \end{pmatrix} \qquad i \in {n_{\mathrm{min}}, \ldots, n_{\mathrm{max}}}
\end{equation}
before passing it into the neural network.
We choose $n_{\mathrm{min}} = 6$ and $n_{\mathrm{max}} = 8$, in effect increasing the dimensionality of the input to the neural network from $\ndim$ to $7\ndim$.

For the neural network itself, we use two architectures, \unets{} \citep{ronneberger2015unet} and \vits{} \citep{dosovitskiy2020image}.
We use the \unet{} configuration proposed by \citet{kingma2023variational} which adapts the widely used configuration from \citep{ho2020denoising} for likelihood estimation.
Most notably, the \citep{kingma2023variational} version has no downsampling between layers of the \unet{}, which lets them increase the number of \unet{} levels to 32.

\vits{} are a more recent architecture inspired by the success of transformers \citep{vaswani2017attention}.
They represent images as a set of patches with a 2D position embedding and process them with global attention, in contrast to convolutional architectures like the \unet{} where communication happens primarily locally.
We opt for the \dit{} architecture \citep{peebles2023scalable} which has been shown to improve sample quality over variants of the \citep{ho2020denoising} \unet{} model.

\section{Experiments}\label{sec:experiments}

% Placed here so that it goes on page 8
\begin{figure*}[h]
  \centering
  \includegraphics{figures/samples-imagenet64}
  \caption{Samples from our model trained on ImageNet64. Generated with the linear schedule and 1024 steps. Note that we optimize our models for density estimation, not sample quality.}\label{fig:samples}
\end{figure*}

We evaluate \bsi{} as a generative image model with a focus on probabilistic modeling and log-likelihood.
For our experiments, we use the CIFAR10 \citep{krizhevsky2009learning} and ImageNet \citep{deng2009imagenet} datasets, where we use the official 32x32 and 64x64 versions of the latter \citep{chrabaszcz2017downsampled}.

\subsection{Likelihood}\label{sec:likelihood}

\begin{wraptable}[10]{r}{1.95in}
  \centering
  \caption{Test set log-likelihood on CIFAR10 of the exact same \unet{} in different models.}\label{tab:cifar10-bpd}
  \begin{tabular}{rcc}
    \toprule
    Model & Training Steps       & BPD  \\
    \midrule
    VDM   & \multirow{2}{*}{10M} & 2.65 \\
    BSI   &                      & 2.64 \\
    \midrule
    BFN   & \multirow{2}{*}{5M}  & 2.66 \\
    BSI   &                      & 2.65 \\
    \bottomrule
  \end{tabular}
\end{wraptable}
We begin by comparing \bsi{} to \vdms{} and \bfns{} directly.
For that, we trained the same \unet{} architecture used by \vdms{} and \bfns{} with the exact same hyperparameters on CIFAR10 without data augmentation.
\cref{tab:cifar10-bpd} shows that \bsi{} improves the log-likelihood slightly over both of them after training for the same number of steps that the baselines were trained.
We attribute this improvement to \bsi{}'s simpler \elbo{} encoding distribution $\qr(\vmu_{\lambda} \mid \vx, \lambda)$, see \cref{tab:distributions}.

Regarding the architecture, we observed that large \dits{} would overfit immediately on the small CIFAR10 dataset.
Small \dit{} configurations would only overfit after about 2M training steps, but were unable to compete with the \unet{} in terms of likelihood due to their reduced capacity.
On the larger ImageNet dataset, \dits{} outperformed the \unet{} configuration from \citet{kingma2023variational} in preliminary results, so we used a \dit{} for our final training runs.
In particular, we use a \dit{}-L-2 for ImageNet32 and a \dit{}-L-4 for ImageNet64 as described by \citet{peebles2023scalable}.
2 and 4 denote the size of the image patches, where we use a larger patch size on the higher resolution images to control the runtime of the \dit{}, which scales quadratically in the number of patches due to the attention operation.

\cref{tab:bpd} lists the likelihoods we achieved across the three datasets.
For our results, we report the tightest likelihood bound, i.e.\ the infinite-step \elbo{} from \cref{thm:infinite-elbo}.
\cref{sec:bits-per-dimension} describes how we adapt the \elbo{} for a continuous $\vx$ to report a likelihood for discretized data, e.g.\ 8-bit images, in bits per dimension.
See \cref{sec:model-configurations} for additional details on training and model configurations.

\paragraph{ELBO Convergence}
\cref{fig:elbo-convergence} shows how the finite step \elbo{} from \cref{thm:elbo} converges towards its infinite step counterpart as $k \to \infty$ on the test set of ImageNet32.
For this plot, we sampled 100 $\lambda$ per image for the Monte Carlo estimates of $\lmeasurek$ and $\lmeasureinf$.
The convergence trend continues right to the noise floor where the noise overshadows the signal, which we take as the sum of the standard deviations of the Monte Carlo estimators of the two terms.

\paragraph{Variance Reduction}
\cref{fig:importance-sampling} shows how importance sampling with a log-uniform distribution reduces the range of the $\nicefrac{h(\lambda)}{\pr(\lambda)}$ term in \cref{eq:is-lm} by about 4 orders of magnitude on CIFAR10 and therefore the variance of a Monte Carlo estimate of the \elbo{}.

\subsection{Sample Quality}\label{sec:sample-quality}

While optimizing the \elbo{} trains our models for density estimation, we have also examined the effect of the precision schedule $\{\alpha_{i}\}$ and number of steps $\nrounds$ on the sample quality produced by \cref{alg:generate}.
We parametrize schedules with $t_{0} < t_{1} < \ldots < t_{\nrounds}$ where $t_{0} = 0$ and $t_{\nrounds} = 1$
From these, we compute the precision schedule using the \cdf{} $\LUcdf$ of the importance sampling distribution $\pr(\lambda)$ as in \cref{sec:precision-encoding}, i.e.
\begin{equation}
  \alpha_{i} = \LUcdf\inv(t_{i}) - \LUcdf\inv(t_{i - 1}).
\end{equation}
The simplest choice is a linear spacing of $t$, i.e.\ $t_{i} = \nicefrac{i}{\nrounds}$, which corresponds to an exponentially increasing precision $\alpha_{i}$ with the log-uniform $\pr(\lambda)$.
Parametrizing the schedule with $t_{i}$ makes it easy to transfer schedules from the diffusion literature that are also parametrized by a $t \in [0, 1]$.
We compare the simple linear schedule to the one proposed by \citet{karras2022elucidating} with $\rho = 7$ denoted as EDM7.

\cref{fig:fid-convergence} compares the sample quality achieved with the two schedules in terms of \fid{}.
Both schedules' sample quality converges for large $\nrounds$, though EDM7 produces better samples for few sampling steps.
This is expected because the linear schedule with the exponential increase in precision places few steps in the high-noise regime where \dm{}-like models generate large-scale features, which have a larger influence on \fid{} than the fine details important for high likelihoods.
EDM7, in comparison, increases $\alpha_{i}$ much slower in the beginning.

See \cref{sec:additional-samples} for more generated samples.

% Better FID would require training by not optimizing the \elbo{} directly, but putting more emphasis on the high-noise regime also during training, not just post-hoc during sampling.

\section{Conclusion}\label{sec:conclusion}

We have introduced our generative model \bsi{} through a novel perspective on generative modeling that frames sample generation as iterative Bayesian inference.
We have derived an \elbo{} for both finite steps and the infinite step limit and an importance sampling distribution to minimize the training loss variance.
In addition, we have thoroughly discussed how \bsi{} relates to \bfns{} and \dms{} and shown that \bsi{} includes \bfns{} as a special case.
Our experiments have demonstrated that \bsi{} achieves better likelihood scores than both \vdms{} and \bfns{} in a direct comparison and strong likelihood scores on standard image datasets in general.
Overall, \bsi{} contributes a Bayesian perspective to the landscape of probabilistic generative modeling that is theoretically simple and empirically effective.

\section*{Acknowledgments}

This research was funded by the Bavarian State Ministry for Science and the Arts within the framework of the Geothermal Alliance Bavaria project.
We are thankful to David L\"udke and Jonas Dornbusch for their valuable feedback.

\section*{Software}

For our results, we rely on excellent software packages, notably \texttt{numpy} \citep{harris2020array}, \texttt{pytorch} \citep{paszke2019pytorch}, \texttt{einops} \citep{rogozhnikov2021einops}, \texttt{matplotlib} \citep{hunter2007matplotlib}, \texttt{h5py} \citep{collette2013python}, \texttt{hydra} \citep{yadan2019hydra} and \texttt{jupyter} \citep{granger2021jupyter}.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning.
There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{bsi}
\bibliographystyle{icml2025}

\clearpage
\onecolumn
\appendix

% Group the appendix sections in the PDF bookmarks
\bookmarksetupnext{level=part}
\pdfbookmark{Appendix}{appendix}

\section{How BSI relates to \ldots}\label{sec:relations}

\subsection{Bayesian Flow Networks}\label{sec:bfns}

\bfns{} are a recent class of generative models for continuous and discrete data motivated from an information-theoretic perspective \citep{graves2023bayesian}.
In it, a sender communicates a latent sample to a receiver while trying to minimize the transported data volume.
The sender compresses the data with entropy coding, so that minimizing the data volume is equivalent to the receiver maximizing the log-likelihood of the latent sample based on the information that it has received from the sender so far.
Finally, a sample can be generated when the receiver also assumes the role of the sender and repeatedly refines its belief.

\emph{Our generative approach in \cref{sec:framework} includes \bfns{} for continuous data as a special case}.
To see this, we begin by choosing our belief prior $\pr(\vmu_{0})$ as $\Np(\vzero, \gamma_{0})$ and letting $\gamma_{0} \to \infty$, i.e.\ the initial belief mean will always be $\vmu_{0} = \vzero$.
With \cref{thm:elbo-encoder-closed-form}, this gives us
\begin{equation}
  \qr(\vmu_{\lambda} \mid \vx, \lambda) = \Np\big( \nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \nicefrac{\lambda^{2}}{\lambda - \lambda_{0}} \big).
\end{equation}
If we now define $\alpha = \lambda - \lambda_{0}$, choose the initial precision $\lambda_{0} = 1$ and write the Normal distribution in variance format, we see that
\begin{equation}
  \qr(\vmu_{\lambda} \mid \vx, \lambda) = \N\bigg( \frac{\alpha}{1 + \alpha} \vx, \frac{\alpha}{{(1 + \alpha)}^{2}} \bigg), \label{eq:bfn-flow-distribution}
\end{equation}
which equals the \bfn{} flow distribution $\pr_{\mathrm{F}}(\vtheta \mid \vx; t)$ \citep[Equation (76)]{graves2023bayesian} if we parametrize $\lambda$ (and therefore $\alpha$) in terms of $t \in [0, 1]$ as in \cref{sec:precision-encoding}.

Since a comprehensive description of \bfns{} would go beyond the scope of this work, we will only point out the correspondence between terms from \cref{sec:framework} and their \bfn{} counterparts without explaining them in detail.
For a complete description, we refer the reader to the original work \citep{graves2023bayesian}.

The current belief $(\vmu_{i}, \lambda_{i})$ is equivalent to the input distribution $\pr_{\mathrm{I}}$ \citep[Equation (43)]{graves2023bayesian}.
\cref{thm:posterior} is the equivalent of the Bayesian update function $h$ \citep[Section 4.2]{graves2023bayesian}.
A noisy measurement $\vy \sim \Np(\vx, \alpha)$ corresponds to the sender distribution $\pr_{\mathrm{S}}$ \citep[Equation (86)]{graves2023bayesian}, while a noisy measurement $\vy \sim \Np(\vxhat, \alpha)$ of the model's current prediction $\vxhat$ of the true sample corresponds to the receiver distribution $\pr_{\mathrm{R}}$ \citep[Equation (88)]{graves2023bayesian}.
The output distribution $\pr_{\mathrm{O}}$ and the Bayesian update distribution $\pr_{\mathrm{U}}$ are just intermediate terms to derive the model and appear neither in the final training nor sampling algorithm.

Fixing the initial belief to $\vmu_{0} = 0$ with infinite precision for \bfns{} recovers the behavior described by \citet[Figures 3 and 4]{graves2023bayesian} and shown in \cref{eq:bfn-flow-distribution} that the precision ${(1 + \alpha)}^{2} / \alpha$ of the flow / encoding distribution $\qr(\vmu_{\lambda} \mid \vx, \lambda)$ in the \elbo{} first falls and then rises again as $\alpha$ grows.
In contrast, with our belief prior $\pr(\vmu_{0}) = \Np(\vzero, \lambda_{0})$ of the same precision as the initial belief $(\vmu_{0}, \lambda_{0})$ as we choose it in \cref{sec:prior-distribution}, the precision of $\qr(\vmu_{\lambda} \mid \vx, \lambda)$ grows linearly in $\lambda$ (and $\alpha$) in lockstep with the precision of the belief $(\vmu_{i}, \lambda_{i})$.
We hypothesize that this makes learning for the model easier, because the noise level in its input varies linearly instead of non-linearly across noise levels.
Furthermore, in \bsi{}, the first sampling step will already contribute to drawing a random sample, since the initial input $\vmu_{0}$ to $\model$ is random.
In \bfns{}, the initial belief is fixed to $\vzero$, which makes the first sampling step deterministic and equal across all samples.

In \cref{sec:prior-distribution}, we have argued that the reasonable range of prior precisions $\gamma_{0}$ is $[\lambda_{0}, \infty]$.
\emph{\bsi{} and \bfns{} occupy the two extremes of this range} with \bsi{} using the least informed prior $\gamma_{0} = \lambda_{0}$, i.e.\ making the fewest assumptions, and \bfns{} the most informed one $\gamma_{0} = \infty$.
Note that these extremes are the only choices in the reasonable range for which the precision $\lambda^{2}{(\lambda - \lambda_{0} + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}})}\inv$ of the encoder $\qr$ in \cref{thm:elbo-encoder-closed-form} simplifies, i.e.\ to just $\lambda$ for \bsi{} and $\lambda^{2}{(\lambda - \lambda_0)}\inv$ for \bfns{}.

In our comparison to \dms{} in \cref{sec:diffusion-models}, we see that \bsi{} and \bfns{} also differ in their associated noising process.
While \bsi{}'s noising process, i.e.\ how one could go from a more precise measurement back to a less precise one, does not form a Markov chain, \bfns{}'s does, making \bfns{} more similar to \dms{}.

In \cref{sec:diffusion-models}, we exploit that \bfns{} can be represented as a special case of \bsi{} to derive a Markovian forward process for \bfns{} as \dms{}.

\subsection{Diffusion Models}\label{sec:diffusion-models}

\dms{} are a widely used class of generative models built on the concept of inverting a diffusion process \citep{sohl-dickstein2015deep, ho2020denoising}.
Given a sample $\vx$, they define a Markov chain of increasingly noisy versions $\vx_{1}, \vx_{2}, \ldots$ of $\vx$ where $\vx_{0} = \vx$ and
\begin{equation}
  \pr(\vx_{i} \mid \vx_{i - 1}) = \N(\alpha_{i}\vx_{i - 1}, \beta_{i}) \label{eq:diffusion-markov-chain}
\end{equation}
for some coefficients $\alpha_{i}$ and $\beta_{i}$.
In training, a model learns to invert this Markov chain, which lets you finally generate data by sampling from a noise distribution and stepping along the learned, reverse Markov chain until you reach the data distribution.

While \dms{} initially achieved prominence in image generation \citep{dhariwal2021diffusion}, they have since been applied successfully across a variety of domains, such as text-to-image mapping \citep{saharia2022photorealistic}, fluid simulations \citep{lienen2024zero, saydemir2024unfolding}, adversarial attacks \citep{kollovieh2024assessing}, temporal \citep{ludke2023add} and general point processes \citep{ludke2024unlocking}, molecular dynamics \citep{lewis2024scalable}, molecular structure generation \citep{ayadi2024unified}, and time series forecasting \citep{kollovieh2023predict, kollovieh2024flow}.

\dms{} and \bsi{} are remarkably similar at first glance.
Both revolve around the concept of iteratively transforming noise into data samples, though \dms{} work with Langevin dynamics and \bsi{} uses posterior inference.
For training, both models aim to align a parametric distribution $\pr_{\vtheta}(\vx'' \mid \vx')$ with a distribution $\qr(\vx'' \mid \vx', \vx)$ that describes a less noisy version $\vx''$ of a noisy sample $\vx'$ given that the true sample is $\vx$.

However, conceptually, they approach sampling from two different perspectives.
\dms{} start with the so-called forward process, where signal is iteratively converted into noise forming a Markov chain of intermediate states as in \cref{eq:diffusion-markov-chain}.
Then, they revert this chain to derive the reverse process that enriches noise with data.
In contrast, \bsi{} defines the reverse process directly in the form of \cref{thm:update-marginal} and never uses the associated forward process directly.

We can revert \bsi{}'s process to derive its ``noising'' process.
This will let us see what \bsi{} would look like as a \dm{} and thus understand the relationship between the two.
Assume that our current belief is $(\vmu, \lambda = \lambda_{0} + \alpha)$ and we want to denoise further based on a sample $\vx$ and measurement precision $\alpha'$, i.e.\ update our belief to $(\vmu', \lambda' = \lambda_{0} + \alpha + \alpha')$.
The denoising process described by \cref{thm:update-marginal} tells us that
\begin{equation}
  \pr(\vmu' \mid \vmu, \vx) = \Np\big( \nicefrac{1}{\lambda'} \big[\lambda\vmu + \alpha'\vx\big], \nicefrac{{\lambda'}^{2}}{\alpha'} \big).
\end{equation}
To find the noising process, we revert this and get
\begin{equation}
  \pr(\vmu \mid \vmu', \vx) = \N\Bigg( \xi\inv \bigg[ \frac{\lambda\lambda'}{\alpha'}\vmu' + \lambda\bigg( \frac{\alpha}{\alpha + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}}} - 1 \bigg)\vx \bigg], \xi \Bigg) \quad \text{where} \quad \xi = \lambda^{2}\Big( {(\alpha + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}})}\inv + {\alpha'}\inv \Big) \label{eq:diffusion-bsi-forward}
\end{equation}
and $\gamma_{0}$ is the precision of the initial belief prior $\pr(\vmu_{0}) = \N(\vzero, \gamma_{0})$.
Find the proof at the end of this section.

Plugging in $\gamma_{0} = \lambda_{0}$, we get that the noising process of \bsi{} is
\begin{equation}
  \pr(\vmu \mid \vmu', \vx) = \N\Bigg( \xi\inv \bigg[ \frac{\lambda\lambda'}{\alpha'}\vmu' - \lambda_{0}\vx \bigg], \xi \Bigg) \quad \text{where} \quad \xi = \lambda\bigg(1 + \frac{\lambda}{\alpha'}\bigg).
\end{equation}
Note that this distributions depends on $\vx$ since $\lambda_{0} > 0$.
Therefore, \bsi{}'s forward process would not be Markov, i.e. you cannot add more noise to a belief state without knowing the sample $\vx$ that the belief state originated from.
While \dms{} with non-Markov forward processes exist \citep{song2021denoising, chen2024fast}, they are uncommon.
In conclusion, we see that \bsi{} can be represented as a \dm{}, though with a rather complex, non-Markovian forward process.

As we have shown in \cref{sec:bfns}, \bfns{} are a special case of our generative framework in \cref{sec:framework} if we choose $\gamma_{0} = \infty$.
Curiously, \cref{eq:diffusion-bsi-forward} shows that this is the only prior on $\vmu_{0}$ for which the associated forward process is Markov as the coefficient of $\vx$ becomes $0$.
This agrees with \citet{xue2024unifying}, who have shown that \bfns{} admit a formulation based on stochastic differential equations, like score-based \dms{}.

\begin{proof}[Proof of \cref{eq:diffusion-bsi-forward}]
  We know from \cref{thm:elbo-encoder-closed-form} that
  \begin{equation}
    \qr(\vmu \mid \vx, \lambda) = \Np\Big( \nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \lambda^{2}{(\lambda - \lambda_{0} + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}})}\inv \Big) = \Np\Big( \nicefrac{\alpha}{\lambda}\,\vx, \lambda^{2}{(\alpha + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}})}\inv \Big)
  \end{equation}
  and from \cref{thm:update-marginal} that
  \begin{equation}
    \pr(\vmu' \mid \vmu, \vx) = \Np\big( \nicefrac{1}{\lambda'} \big[\lambda\vmu + \alpha'\vx\big], \nicefrac{{\lambda'}^{2}}{\alpha'} \big).
  \end{equation}
  Therefore, $\pr(\vmu, \vmu' \mid \vx)$ is a Gaussian linear system and we can use \citep[Equation (4.125)]{murphy2012machine} to see that
  \begin{equation}
    \pr(\vmu \mid \vmu', \vx) = \Np(\vnu, \xi)
  \end{equation}
  with
  \begin{equation}
    \xi = \lambda^{2}{\bigg(\alpha + \frac{\lambda_{0}^{2}}{\gamma_{0}}\bigg)}\inv + {\bigg(\frac{\lambda}{\lambda'}\bigg)}^{2}\frac{{\lambda'}^{2}}{\alpha'} = \lambda^{2}\Big( {(\alpha + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}})}\inv + {\alpha'}\inv \Big)
  \end{equation}
  and
  \begin{equation}
    \vnu = \xi\inv \Bigg[ \frac{\lambda}{\lambda'}\frac{{\lambda'}^{2}}{\alpha'} \bigg( \vmu' - \frac{\alpha'}{\lambda'}\vx \bigg) + \lambda^{2}{(\alpha + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}})}\inv\,\nicefrac{\alpha}{\lambda}\,\vx \Bigg] = \xi\inv \Bigg[ \frac{\lambda\lambda'}{\alpha'}\vmu' + \lambda\bigg( \frac{\alpha}{\alpha + \nicefrac{\lambda_{0}^{2}}{\gamma_{0}}} - 1 \bigg)\vx \Bigg].
  \end{equation}
\end{proof}

\section{ELBO in Bits per Dimension}\label{sec:bits-per-dimension}

A common metric in probabilistic modeling is the negative log-likelihood of unseen data.
The benefits of this metric are that it is theoretically motivated by the probabilistic framework and it can be computed across domains regardless of data modality.
If the negative log-likelihood is small, the generative model assigns high likelihood to the unseen data and can thus be regarded as a good model (though likelihood and sample quality are not necessarily the same thing \citep{theis2016note}).
For models that come with an \elbo{} like \bsi{}, we can use it to upper bound the negative log-likelihood to compare against other \elbo{}-based or exact-likelihood models.

The negative log-likelihood is usually reported in bits per pixel, per color channel or, in general, per dimension.
This unit comes from the fact that an entropy coder could use the model to encode samples $\vx \in \sS^{d}$ from a finite symbol alphabet $\sS$ from the data distribution asymptotically in $-\negthinspace\log_{2} \pr_{\vtheta}(\vx) / d$ bits per dimension \citep{duda2015use}.
Note that the underlying space $\sS$ must be discrete.
If it were continuous, $\pr_{\vtheta}(\vx)$ would be a density and the theory would predict that we could compress $\vx$ into a negative number of bits.

The discreteness requirement is a natural fit for many domains.
While, for example, images are usually treated as tensors with continuous color values, the colors are actually stored as discrete values in the range $[0, 2^{8} - 1]$ for 8-bit images.
Similarly, audio data is a sequence of discrete values in, for example, a 16-bit range.

Let's say that $\sS$ is the set of integers $\{0, \ldots, \nbuckets - 1\}$.
Then we can compute an upper bound on the bits needed to encode $\vx \in \sS^{d}$ by
\begin{equation}
  -\log_{2}\pr(\vx) \le \log(2) (\lrecon' + \lmeasureinf)
\end{equation}
as per \cref{thm:elbo,thm:infinite-elbo}.
The multiplication by $\log(2)$ converts the logarithms in $\lrecon'$ and $\lmeasureinf$ to base 2.
$\lrecon'$ is the same as $\lrecon$ but with a discretized Normal likelihood to account for the discrete nature of $\vx$, i.e.
\begin{equation}
  \lrecon' = \E_{\qr(\vmu_{\lambdaM} \mid \vx, \lambdaM)}\big[{\shortminus}\negthinspace\log\Np'(\vx \mid \vxhat_{\lambdaM}, \alphaR)\big]
\end{equation}
where
\begin{equation}
  \Np'(\evx_{j} \mid \vxhat_{\lambdaM}, \alphaR) = \varPhi(r_{j} \mid \vxhat_{\lambdaM}, \alphaR) - \varPhi(l_{j} \mid \vxhat_{\lambdaM}, \alphaR).
\end{equation}
$\varPhi(r_{j} \mid \vxhat_{\lambdaM}, \alphaR)$ is the \cdf{} of $\N(\vxhat_{\lambdaM}, \alphaR)$ and $l_{j}$ and $r_{j}$ are the boundaries of the discretization interval containing $\evx_{j}$, i.e.
\begin{equation}
  l_{j} = \begin{cases}
    -\infty                                              & \text{if}\ \evx_{j} < \frac{1}{2}               \\
    \nbuckets - \frac{3}{2}                              & \text{if}\ \evx_{j} \ge \nbuckets - \frac{3}{2} \\
    \lfloor \evx_{j} - \frac{1}{2} \rfloor + \frac{1}{2} & \text{otherwise}
  \end{cases}
  \quad\text{and}\quad
  r_{j} = \begin{cases}
    \infty                                               & \text{if}\ \evx_{j} \ge \nbuckets - \frac{3}{2} \\
    \frac{1}{2}                                          & \text{if}\ \evx_{j} < \frac{1}{2}               \\
    \lfloor \evx_{j} + \frac{1}{2} \rfloor - \frac{1}{2} & \text{otherwise.}
  \end{cases}
\end{equation}

$\lmeasureinf$ is usually not discretized during \elbo{} computation as the latent variables only enter as a mean squared error instead of a log-likelihood.
In a practical implementation, the latent variable distributions would need to be discretized as well, decreasing the \elbo{} slightly \citep{kingma2023variational, townsend2019hilloc}.
If $\vx$ is discretized to a different set of discrete symbols, e.g.\ numbers between $-1$ and $1$ instead of the integers $\sS$, the boundaries of the discretization intervals and bin widths in the discretized Normal distribution have to be adapted accordingly.

\section{Preconditioning Derivation}\label{sec:preconditioning-derivation}

We will assume in this section that the data is normalized such that $\E[\vx] = \vzero$ and $\var[\vx] = \mI$.

Assume that we have a current belief $(\vmu, \lambda)$.
We derive the parameters $\preskip$, $\preout$ and $\prein$ of the preconditioned model
\begin{equation}
  \model(\vmu, \lambda) = \preskip \vmu + \preout \model'(\prein\vmu, \lambda) \label{eq:precond-model}
\end{equation}
analogously to \citet{karras2022elucidating}.
However, while we proceed in the same way, the resulting parameters for \bsi{} differ from \citet{karras2022elucidating} because \bsi{} is not included in the family of \dms{} that \citet{karras2022elucidating} consider, see \cref{sec:diffusion-models}.

First, we require that $\var_{\vx}[\prein\vmu] = \mI$ for all $\lambda$.
We know from \cref{thm:bsi-elbo-encoder-closed-form} that
\begin{equation}
  \qr(\vmu \mid \vx, \lambda) = \Np\big(\nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \lambda\big).
\end{equation}
Therefore, $\pr(\vx, \vmu)$ is a Gaussian linear system and \citep[Equation (4.126)]{murphy2012machine} tells us that the variance of the marginal distribution of $\vmu$ is
\begin{equation}
  \var_{\vx}[\vmu] = \bigg(\lambda\inv + \frac{{(\lambda - \lambda_{0})}^{2}}{\lambda^{2}}\bigg)\mI.
\end{equation}
By plugging this into our requirement
\begin{equation}
  \var_{\vx}[\prein\vmu] = \prein^{2}\var_{\vx}[\vmu] = \mI,
\end{equation}
we get immediately that
\begin{equation}
  \prein = {\bigg(\lambda\inv + \frac{{(\lambda - \lambda_{0})}^{2}}{\lambda^{2}}\bigg)}\bigprsqrt = {\bigg(\underbrace{1 + \frac{{(\lambda - \lambda_{0})}^{2}}{\lambda}}_{\eqcolon \kappa}\bigg)}\bigprsqrt \lambda\powsqrt = \sqrt{\nicefrac{\lambda}{\kappa}}.
\end{equation}

Next, we want to have the actual prediction target of $\model'$ during training to have unit variance, too.
In training, we optimize the \elbo{} from \cref{thm:infinite-elbo}, which comes down to minimizing
\begin{equation}
  \big\|\vx - \model(\vmu, \lambda)\big\|_{2}^{2}
\end{equation}
up to constant factors only depending on $\lambda$.
If we plug in \cref{eq:precond-model} and isolate $\model'$, this distance becomes
\begin{equation}
  \big\|\vx - \preskip \vmu - \preout \model'(\prein\vmu, \lambda)\big\|_{2}^{2} = \preout^{2}\big\|\model'(\prein\vmu, \lambda) - \preout\inv(\vx - \preskip \vmu)\big\|_{2}^{2}.
\end{equation}
From this, we identify $\preout\inv(\vx - \preskip \vmu)$ as the actual training target for $\model'$.
For the rest of this derivation, we denote use the shorthand $\alpha = \lambda - \lambda_{0}$ for the measurement precision accumulated in our belief $(\vmu, \lambda)$.
After \cref{thm:bsi-elbo-encoder-closed-form}, we can write $\vmu$ as $\nicefrac{\alpha}{\lambda}\,\vx + \vz$ where $\vz \sim \Np(\vzero, \lambda)$ and find that the variance of the training target is
\begin{align}
  \begin{aligned}
    \var_{\vx, \vz}[\preout\inv(\vx - \preskip \vmu)] & = \preout^{\shortminus{}2}\var_{\vx, \vz}\bigg[\vx - \preskip \bigg(\frac{\alpha}{\lambda}\,\vx + \vz\bigg)\bigg] \\
                                                      & = \preout^{\shortminus{}2}\var_{\vx, \vz}\bigg[\bigg( 1 - \preskip \frac{\alpha}{\lambda} \bigg)\vx - \preskip\vz\bigg] \\
                                                      & = \preout^{\shortminus{}2} \bigg[ {\bigg( 1 - \preskip \frac{\alpha}{\lambda} \bigg)}^{2} + \preskip^{2}\lambda\inv \bigg] \mI
  \end{aligned}
\end{align}
If we now require the effective training target to have unit variance, we see that
\begin{equation}
  \preout^{2} = {\bigg( 1 - \preskip \frac{\alpha}{\lambda} \bigg)}^{2} + \preskip^{2}\lambda\inv = \bigg[1 + \frac{\alpha^{2}}{\lambda}\bigg]\frac{1}{\lambda}\preskip^{2} - 2\frac{\alpha}{\lambda}\preskip + 1. \label{eq:precond-precout-general}
\end{equation}

Following \citet{karras2022elucidating}, we now choose $\preskip$ to minimize the impact of errors in the output of $\model'$ by minimizing the magnitude of $\preout$.
$\preout^{2}$ is a polynomial in $\preskip$ with positive leading coefficient, so we can find the minimizer as the root of
\begin{equation}
  \frac{1}{2}\,\frac{\mathrm{d}\preout^{2}}{\mathrm{d}\preskip} = \bigg[1 + \frac{\alpha^{2}}{\lambda}\bigg]\frac{1}{\lambda}\preskip - \frac{\alpha}{\lambda},
\end{equation}
which is at
\begin{equation}
  \preskip = {\bigg[1 + \frac{\alpha^{2}}{\lambda}\bigg]}\inv \alpha = \kappa\inv\alpha = \frac{\alpha}{\kappa}.
\end{equation}

Finally, we can plug $\preskip$ into \cref{eq:precond-precout-general} to get
\begin{equation}
  \begin{aligned}
    \preout^{2} & = \kappa\kappa^{\shortminus{}2} \frac{a^{2}}{\lambda} - 2\frac{\alpha}{\lambda}\kappa\inv \alpha + 1 = \kappa\inv \bigg( \frac{a^{2}}{\lambda} - 2\frac{\alpha^{2}}{\lambda} + \bigg[1 + \frac{\alpha^{2}}{\lambda}\bigg] \bigg) = \kappa\inv
  \end{aligned}
\end{equation}
and consequently $\preout = \kappa\rsqrt = \sqrt{\nicefrac{1}{\kappa}}$.

\section{Proofs}\label{sec:proofs}

\subsection{Proof of {\hypersetup{hidelinks}\cref{thm:elbo}}}\label{sec:proof-of-elbo}

We will begin with some auxiliary insights.
First, we consider the marginal distribution of the updated belief $(\vmu', \lambda')$.
This means that our current belief about a sample $\vx$ is $(\vmu, \lambda)$ and now we want to know the distribution of $\vmu'$ after updating $\vmu$ with \cref{thm:posterior} marginalized over all possible noisy measurements $\vy$ with precision $\alpha$.
Note that $\lambda'$ is deterministic as it neither depends on $\vx$ nor $\vy$.

\begin{lemma}[Update Marginal]\label{thm:update-marginal}
  Let $\vx, \vmu \in \R^{\ndim}$ and $\lambda, \alpha \in \Rplus$.
  Then the distribution of the posterior belief mean $\vmu'$ marginalized over all measurements $\vy$ made with precision $\alpha$ is
  \begin{equation}
    \pr(\vmu' \mid \vmu, \vx, \alpha) = \E_{\vy \sim \Np(\vx, \alpha\mI)}\big[ \pr(\vmu' \mid \vmu, \vx, \alpha, \vy) \big] = \Np\big( \nicefrac{1}{\lambda'} \big[\lambda\vmu + \alpha\vx\big], \nicefrac{{\lambda'}^{2}}{\alpha} \big). \label{eq:update-marginal}
  \end{equation}
\end{lemma}
\begin{proof}
  The noisy measurement is a Normal random variable $\vy \sim \Np(\vx, \alpha)$ and the mean of our posterior belief $(\vmu', \lambda')$ after observing $\vy$ is the deterministic linear transformation
  \begin{equation}
    \vmu' = \nicefrac{1}{\lambda'} \left[ \lambda\vmu + \alpha\vy \right]
  \end{equation}
  of this random variable.
  The statement follows immediately by the linear transformation property of the Normal distribution.
\end{proof}

From this, we can see that the update marginal from multiple intermediate measurements is the same as from a single measurement with the combined precision of the intermediate measurements.

\begin{lemma}\label{thm:multi-update-marginal}
  Let $\vx, \vmu, \vmu', \vmu'' \in \R^{\ndim}$ and $\lambda, \alpha, \alpha' \in \Rplus$.
  $\vmu'$ is the posterior belief mean after a measurement with precision~$\alpha$ and $\vmu''$ the posterior belief mean after a second, subsequent measurement with precision~$\alpha'$.
  Then we have that the marginal distribution of the second update is
  \begin{equation}
    \E_{\pr(\vmu' \mid \vmu, \vx, \alpha)}[\pr(\vmu'' \mid \vmu', \vx, \alpha')] = \pr(\vmu'' \mid \vmu, \vx, \alpha + \alpha').
  \end{equation}
\end{lemma}
\begin{proof}
  We know from \cref{thm:update-marginal} that $\vmu'$ is a random variable
  \begin{equation}
    \pr(\vmu' \mid \vmu, \vx, \alpha) = \Np\big( \underbrace{\nicefrac{1}{\lambda'} \big[\lambda\vmu + \alpha\vx\big]}_{\eqcolon \vnu}, \underbrace{\nicefrac{{\lambda'}^{2}}{\alpha}}_{\eqcolon \xi} \big)
  \end{equation}
  and $\vmu''$ is a random variable that depends linearly on $\vmu'$
  \begin{equation}
    \pr(\vmu'' \mid \vmu', \vx, \alpha') = \Np\big( \nicefrac{1}{\lambda''} \big[\lambda'\vmu' + \alpha'\vx\big], \nicefrac{{\lambda''}^{2}}{\alpha'} \big).
  \end{equation}
  As such, they jointly form a Gaussian linear system for which the marginal distribution of $\vmu''$ is \citep[Equation (4.126)]{murphy2012machine}
  \begin{equation}
    \E_{\pr(\vmu' \mid \vmu, \vx, \alpha)} [\pr(\vmu'' \mid \vmu', \vx, \alpha')] = \N\bigg( \nicefrac{1}{\lambda''} \big[\lambda'\vnu + \alpha'\vx\big], \frac{\alpha'}{{\lambda''}^{2}} + \frac{{\lambda'}^{2}}{{\lambda''}^{2}\xi} \bigg). \label{eq:multi-update-marginal-marginal}
  \end{equation}

  Plugging $\vnu$ into the mean expression and simplifying yields the marginal mean
  \begin{equation}
    \nicefrac{1}{\lambda''} \big[\lambda\vmu + (\alpha + \alpha')\vx\big].
  \end{equation}
  Similarly, plugging $\xi$ into the covariance expression and simplifying yields the marginal covariance
  \begin{equation}
    \frac{\alpha + \alpha'}{{\lambda''}^{2}}.
  \end{equation}
  If we now recall from \cref{thm:posterior} that
  \begin{equation}
    \lambda' = \lambda + \alpha \quad \text{and} \quad \lambda'' = \lambda' + \alpha' = \lambda + \alpha + \alpha',
  \end{equation}
  we can identify \cref{eq:multi-update-marginal-marginal} as $\pr(\vmu'' \mid \vmu, \vx, \alpha + \alpha')$.
\end{proof}

This trivially generalizes to any finite sequence of measurements, which can be collapsed into a single measurement with the total precision instead.

We will furthermore need to know the KL divergence between the update marginal distributions of the same belief but based on two different samples $\vx$ and $\vx'$.

\begin{lemma}\label{thm:kl-div-marginal-update}
  Let $\vx, \vx', \vmu \in \R^{\ndim}$ and $\lambda, \alpha \in \Rplus$.
  Then
  \begin{equation}
    \kl(\pr(\vmu' \mid \vmu, \vx, \alpha), \pr(\vmu' \mid \vmu, \vx', \alpha)) = \nicefrac{1}{2}\,\alpha \|\vx - \vx'\|_{2}^{2}.
  \end{equation}
\end{lemma}
\begin{proof}
  Both update marginal distributions -- with $\vx$ and $\vx'$ -- are Normal distributions of equal precision $\xi \coloneq \frac{{\lambda'}^{2}}{\alpha}$ as given by \cref{thm:update-marginal} and respective means of
  \begin{equation}
    \vnu = \nicefrac{1}{\lambda'} \big[\lambda\vmu + \alpha\vx\big] \quad \text{and} \quad \vnu' = \nicefrac{1}{\lambda'} \big[\lambda\vmu + \alpha\vx'\big].
  \end{equation}
  As a consequence, the closed form solution for the KL divergence between two equal-covariance Normal distributions becomes
  \begin{equation}
    \begin{aligned}
      \kl(\pr(\vmu' \mid \vmu, \vx, \alpha), \pr(\vmu' \mid \vmu, \vx', \alpha)) & = \frac{1}{2} (\vnu - \vnu')\T \xi (\vnu - \vnu') \\
                                                                                 & = \frac{1}{2} (\vx - \vx')\T \alpha {\lambda'}\inv \xi {\lambda'}\inv \alpha (\vx - \vx') \\
                                                                                 & = \frac{1}{2} (\vx - \vx')\T \alpha (\vx - \vx') \\
                                                                                 & = \frac{1}{2} \alpha \|\vx - \vx'\|_{2}^{2}
    \end{aligned}
  \end{equation}
\end{proof}

Equipped with these, we can derive the \elbo{}.

\thmelbo*
\begin{proof}
  For any distribution $\pr(\vx)$ and any latent variable $\vz$, i.e.\ any choice of prior $\pr(\vz)$, encoding distribution $\qr(\vz \mid \vx)$ and likelihood $\pr(\vx \mid \vz)$, we have the variational lower bound
  \begin{equation}
    \log\pr(\vx) \ge -\E_{\qr(\vz \mid \vx)} [-\log \pr(\vx \mid \vz)] - \kl(\qr(\vz \mid \vx), \pr(\vz)) \label{eq:elbo-proof-elbo}
  \end{equation}
  on $\log\pr(\vx)$ \citep{kingma2013autoencoding}.
  In particular, we can choose our sequence of beliefs as the latent variable $\vz = \{\vmu_{0}, \ldots, \vmu_{\nrounds}\}$ and define the likelihood of $\vx$ under this latent variable as
  \begin{equation}
    \pr(\vx \mid \vz) = \Np(\vx \mid \vxhat_{\nrounds}, \alphaR). \label{eq:elbo-proof-decoder}
  \end{equation}
  Remember that $\vxhat_{k} = \model(\vmu_{k}, \lambda_{k})$ is the model's estimate of $\vx$.

  Since the belief means $\vmu_{1}, \ldots, \vmu_{k}$ are updated only based on their predecessor after \cref{thm:posterior}, they form a Markov chain conditional on $\vx$ and we can write the encoding distribution as
  \begin{equation}
    \qr(\vz \mid \vx) = \pr(\vmu_{0})\, \prod_{i = 1}^{\nrounds} \pr(\vmu_{i} \mid \vmu_{i - 1}, \vx, \alpha_{i}). \label{eq:elbo-proof-encoding}
  \end{equation}
  Each $\pr(\vmu_{i} \mid \vmu_{i - 1}, \vx, \alpha_{i})$ is the update marginal of $\vmu_{i - 1}$ over all possible noisy measurements of $\vx$ with precision $\alpha_{i}$ from \cref{thm:update-marginal}.
  Our encoding distribution is ignorant about the influence of $\vx$ on the initial belief $\vmu_{0}$, because there is no closed form for $\pr(\vmu_{0} \mid \vx)$.
  Since we can choose any encoding, not encoding $\vx$ in $\vmu_{0}$ at all is valid.

  If we now plug \cref{eq:elbo-proof-encoding} into the first term of \cref{eq:elbo-proof-elbo}, we get
  \begin{equation}
    \E_{\qr(\vz \mid \vx)} [{\shortminus}\negthinspace\log \pr(\vx \mid \vz)] = \E_{\pr(\vmu_{0})} \E_{\pr(\vmu_{1} \mid \vmu_{0}, \vx, \alpha_{1})}\,\ldots\,\E_{\pr(\vmu_{\nrounds} \mid \vmu_{\nrounds - 1}, \vx, \alpha_{\nrounds})} [{\shortminus}\negthinspace\log \pr(\vx \mid \vz)].
  \end{equation}
  The intermediate expectations collapse into a single measurement with the sum of all precisions $\bar{\alpha}_{i} = \sum_{j = 1}^{i} \alpha_{j}$ according to \cref{thm:multi-update-marginal}, because $\vmu_{1}, \ldots, \vmu_{\nrounds - 1}$ do not appear in the inner log-likelihood, and we are left with
  \begin{equation}
    \E_{\qr(\vz \mid \vx)} [{\shortminus}\negthinspace\log \pr(\vx \mid \vz)] = \E_{\pr(\vmu_{0})} \E_{\pr(\vmu_{\nrounds} \mid \vmu_{0}, \vx, \bar{\alpha}_{\nrounds})} [{\shortminus}\negthinspace\log \pr(\vx \mid \vz)]. \label{eq:elbo-proof-lr-one}
  \end{equation}
  Since $\lambda_{i} =  \lambda_{0} + \sum_{j=1}^{i} \alpha_{j} = \lambda_{0} + \bar{\alpha}_{i}$, we can define
  \begin{equation}
    \pr(\vmu_{\nrounds} \mid \vmu_{0}, \vx, \lambda) \coloneq \pr(\vmu_{\nrounds} \mid \vmu_{0}, \vx, \alpha = \lambda - \lambda_{0}) = \pr(\vmu_{\nrounds} \mid \vmu_{0}, \vx, \bar{\alpha}_{\nrounds}).
  \end{equation}
  If we now define
  \begin{equation}
    \qr(\vmu_{k} \mid \vx, \lambda_{k}) \coloneq \E_{\pr(\vmu_{0})} \big[\pr(\vmu_{k} \mid \vmu_{0}, \vx, \lambda_{k})\big],
  \end{equation}
  we can rewrite \cref{eq:elbo-proof-lr-one} as
  \begin{equation}
    \E_{\qr(\vz \mid \vx)} [{\shortminus}\negthinspace\log \pr(\vx \mid \vz)] = \E_{\qr(\vmu_{\nrounds} \mid \vx, \lambda_{\nrounds})} [{\shortminus}\negthinspace\log \pr(\vx \mid \vmu_{k})]
  \end{equation}
  which equals the definition of $\lrecon$ after plugging in \cref{eq:elbo-proof-decoder}.

  Next, we investigate the KL-divergence in \cref{eq:elbo-proof-elbo}.
  We begin by defining the latent prior $\pr(\vz)$ autoregressively as
  \begin{equation}
    \pr(\vz) = \pr(\vmu_{0})\,\prod_{i = 1}^{\nrounds} \pr(\vmu_{i} \mid \vmu_{i - 1}, \vxhat_{i - 1}, \alpha_{i}) \label{eq:elbo-proof-latent-prior}
  \end{equation}
  where $\vxhat_{i - 1} = \model(\vmu_{i - 1}, \lambda_{i - 1})$ is the point estimate of $\vx$ produced by our model based on the belief at step $i - 1$.
  So the prior for $\vmu_{i}$ is the update marginal in \cref{thm:update-marginal} if $\vxhat_{i - 1}$ were the actual sample $\vx$.

  Now, we plug \cref{eq:elbo-proof-encoding,eq:elbo-proof-latent-prior} into the KL-divergence term from \cref{eq:elbo-proof-elbo}.
  \begin{equation}
    \begin{aligned}
      \kl(\qr(\vz \mid \vx), \pr(\vz)) & = \E_{\qr(\vz \mid \vx)} \Big[ \log \frac{\qr(\vz \mid \vx)}{\pr(\vz)} \Big] \\
                                       & = \E_{\qr(\vz \mid \vx)} \Big[ \log \frac{\pr(\vmu_{0})}{\pr(\vmu_{0})} + \sum_{i = 1}^{\nrounds} \log \frac{\pr(\vmu_{i} \mid \vmu_{i - 1}, \vx, \alpha_{i})}{\pr(\vmu_{i} \mid \vmu_{i - 1}, \vxhat_{i - 1}, \alpha_{i})} \Big] \\
                                       & = \sum_{i = 1}^{\nrounds} \E_{\qr(\vz \mid \vx)} \Big[ \log \frac{\pr(\vmu_{i} \mid \vmu_{i - 1}, \vx, \alpha_{i})}{\pr(\vmu_{i} \mid \vmu_{i - 1}, \vxhat_{i - 1}, \alpha_{i})} \Big] \\
                                       & = \sum_{i = 1}^{\nrounds} \E_{\pr(\vmu_{0})} \E_{\pr(\vmu_{1} \mid \vmu_{0}, \vx, \alpha_{1})}\,\ldots\,\E_{\pr(\vmu_{i} \mid \vmu_{i - 1}, \vx, \alpha_{i})} \Big[ \log \frac{\pr(\vmu_{i} \mid \vmu_{i - 1}, \vx, \alpha_{i})}{\pr(\vmu_{i} \mid \vmu_{i - 1}, \vxhat_{i - 1}, \alpha_{i})} \Big] \\
                                       & = \sum_{i = 1}^{\nrounds} \E_{\qr(\vmu_{i - 1} \mid \vx, \lambda_{i - 1})} \Big[ \kl\big( \pr(\vmu_{i} \mid \vmu_{i - 1}, \vx, \alpha_{i}), \pr(\vmu_{i} \mid \vmu_{i - 1}, \vxhat_{i - 1}, \alpha_{i}) \big) \Big]
    \end{aligned}
  \end{equation}
  The intermediate expectations have collapsed again according to \cref{thm:multi-update-marginal} in the same way as for the reconstruction term.

  We know the closed form for the inner KL divergences from \cref{thm:kl-div-marginal-update}, so we can further simplify the KL-divergence term to
  \begin{equation}
    \kl(\qr(\vz \mid \vx), \pr(\vz)) = \frac{1}{2} \sum_{i = 1}^{\nrounds} \E_{\qr(\vmu_{i - 1} \mid \vx, \lambda_{i - 1})} \Big[ \alpha_{i} \|\vx - \vxhat_{i - 1}\|_{2}^{2} \Big]. \label{eq:lmeasure-sum-form}
  \end{equation}
  Shifting the sum indices by 1 and replacing the sum $\sum_{i = 0}^{\nrounds - 1}$ with $\nrounds \E_{i \sim \mathcal{U}(0, \nrounds - 1)}$ yields $\lmeasurek$.
\end{proof}

\subsection{Proof of {\hypersetup{hidelinks}\cref{thm:infinite-elbo}}}\label{sec:proof-of-infinite-elbo}

\thminfiniteelbo*
\begin{proof}
  Since $\lrecon$ only depends on $\sum_{i} \alpha_{k,i}$ but not individual $\alpha_{k,i}$, the equivalence of the finite and infinite step $\lrecon$ is immediately apparent.

  For $\lmeasurek$, we will consider its sum form from \cref{eq:lmeasure-sum-form}.
  \begin{equation}
    \lmeasurek = \frac{1}{2} \sum_{i = 1}^{\nrounds} \E_{\qr(\vmu_{i - 1} \mid \vx, \lambda_{i - 1})} \Big[ \alpha_{i} \|\vx - \vxhat_{i - 1}\|_{2}^{2} \Big] = \frac{1}{2} \sum_{i = 1}^{\nrounds} \alpha_{i} \underbrace{\E_{\qr(\vmu_{i - 1} \mid \vx, \lambda_{i - 1})} \Big[ \|\vx - \vxhat_{i - 1}\|_{2}^{2} \Big]}_{\eqcolon h(\lambda_{i - 1})} \label{eq:inf-elbo-proof-riemann-sum}
  \end{equation}
  Note that $h(\lambda_{i - 1})$ is a deterministic function of $\lambda_{i - 1}$ and $\lambda_{0}, \ldots, \lambda_{\nrounds}$ is a partition of the interval $[\lambda_{0}, \lambda_{0} + \alpha_{m}] = [\lambda_{0}, \lambdaM]$ with interval lengths of $\alpha_{i}$.
  It follows that \cref{eq:inf-elbo-proof-riemann-sum} is a Riemann sum.
  Since $\model$ is a neural network, we can assume that $h(\lambda_{i - 1})$ is continuous almost everywhere.
  Combined with the fact that the interval lengths $\{\alpha_{i}\}$ converge uniformly to $0$, it follows that $\lmeasurek$ converges to the Riemann integral
  \begin{equation}
    \lim_{\nrounds \to \infty} \lmeasurek = \frac{1}{2} \int_{\lambda_{0}}^{\lambdaM} \E_{\qr(\vmu_{\lambda} \mid \vx, \lambda)} \Big[ \|(\vx - \vxhat_{\lambda})\|_{2}^{2} \Big]\,\mathrm{d}\lambda \label{eq:elbo-riemann-integral}
  \end{equation}
  as $k \to \infty$.
  It follows trivially that
  \begin{equation}
    \lim_{\nrounds \to \infty} \lmeasurek = \frac{\alphaM}{2} \int_{\lambda_{0}}^{\lambdaM} \frac{1}{\alphaM} \E_{\qr(\vmu_{\lambda} \mid \vx, \lambda)} \Big[ \|(\vx - \vxhat_{\lambda})\|_{2}^{2} \Big]\,\mathrm{d}\lambda = \frac{\alphaM}{2} \E_{\substack{\lambda \sim \mathcal{U}(\lambda_{0}, \lambdaM)\\\qr(\vmu_{\lambda} \mid \vx, \lambda)}} \Big[ \|(\vx - \vxhat_{\lambda})\|_{2}^{2} \Big] = \lmeasureinf.
  \end{equation}
\end{proof}

\subsection{Proof of {\hypersetup{hidelinks}\cref{thm:elbo-tighter}}}\label{sec:proof-of-elbo-tighter}

\thmelbotighter*
\begin{proof}
  In the proof of \cref{thm:infinite-elbo}, we have established that $\lmeasurek$ is a Riemannian sum of $h$, where we evaluate $h$ on the most-negative edge of each interval.
  Since $h$ is a non-negative, strictly decreasing function, the discretization error on the interval $[\lambda_{i - 1}, \lambda_{i}]$
  \begin{equation}
    \epsilon \coloneq \alpha_{i}h(\lambda_{i - 1}) - \int_{\lambda_{i - 1}}^{\lambda_{i}} h(\lambda)\,\mathrm{d}\lambda
  \end{equation}
  is also non-negative.
  Now consider a refinement of the discretization with $\lambda' \in (\lambda_{i - 1}, \lambda_{i})$ and the post-refinement discretization error on that interval
  \begin{equation}
    \epsilon' \coloneq (\lambda' - \lambda_{i - 1}) h(\lambda_{i - 1}) + (\lambda_{i} - \lambda') h(\lambda') - \int_{\lambda_{i - 1}}^{\lambda_{i}} h(\lambda)\,\mathrm{d}\lambda = (\lambda' - \lambda_{i - 1} - \alpha_{i}) h(\lambda_{i - 1}) + (\lambda_{i} - \lambda') h(\lambda') + \epsilon.
  \end{equation}
  Next, we express $\epsilon'$ in terms of $\epsilon$ as
  \begin{equation}
    \begin{aligned}
      \epsilon' & = (\lambda' - \lambda_{i - 1} - \alpha_{i}) h(\lambda_{i - 1}) + (\lambda_{i} - \lambda') h(\lambda') + \epsilon \\
                & = (\lambda' - \lambda_{i}) h(\lambda_{i - 1}) + (\lambda_{i} - \lambda') h(\lambda') + \epsilon \\
                & = (\lambda_{i} - \lambda') (h(\lambda') - h(\lambda_{i - 1})) + \epsilon.
    \end{aligned}
  \end{equation}
  We know that $(\lambda_{i} - \lambda') > 0$, because $\lambda' \in (\lambda_{i - 1}, \lambda_{i})$, and $(h(\lambda') - h(\lambda_{i - 1})) < 0$, because $h$ is strictly decreasing.
  It follows that $\epsilon' < \epsilon$.

  This means that any refinement of the \elbo{} with more steps reduces the non-negative error between the Riemannian sum $\lmeasurek$ and its limit $\lmeasureinf$.
  In other words, $\lmeasureinf < \lmeasurek$ for all $\nrounds$.
\end{proof}

\subsection{Proof of {\hypersetup{hidelinks}\cref{thm:elbo-encoder-closed-form}} and {\hypersetup{hidelinks}\cref{thm:bsi-elbo-encoder-closed-form}}}\label{sec:proof-prior}

The \elbo{} in \cref{thm:elbo,thm:infinite-elbo} has one part that looks like it might not be so straightforward: the encoding distribution $\qr(\vmu_{\lambda} \mid \vx, \lambda)$.
Its definition contains a marginalization over the belief prior $\pr(\vmu_{0})$, which we still need to specify.
Let's see what $\qr(\vmu_{\lambda} \mid \vx, \lambda)$ becomes if we choose a zero-mean, isotropic Normal prior $\pr(\vmu_{0})$.

\thmelboclosedform*
\begin{proof}
  Let $\pr(\vmu_{\lambda} \mid \vmu_{0}, \vx, \lambda)$ be the marginal distribution of $\vmu_{\lambda}$ after a measurement of precision $\alpha = \lambda - \lambda_{0}$, i.e.
  \begin{equation}
    \pr(\vmu_{\lambda} \mid \vmu_{0}, \vx, \lambda) = \pr(\vmu_{\lambda} \mid \vmu_{0}, \vx, \alpha = \lambda - \lambda_{0}).
  \end{equation}
  We know from \cref{thm:update-marginal} that
  \begin{equation}
    \pr(\vmu_{\lambda} \mid \vmu_{0}, \vx, \alpha = \lambda - \lambda_{0}) = \Np\big( \nicefrac{1}{\lambda} \big[\lambda_{0}\vmu_{0} + (\lambda - \lambda_{0})\vx\big], \nicefrac{\lambda^{2}}{\lambda - \lambda_{0}} \big).
  \end{equation}
  Since $\pr(\vmu_{0})$ is also Gaussian and $\vmu_{\lambda}$ depends linearly on $\vmu_{0}$, they form a Gaussian linear system for which the marginal distribution of $\vmu_{\lambda}$ is \citep[Equation (4.126)]{murphy2012machine}
  \begin{equation}
    \qr(\vmu_{\lambda} \mid \vx, \lambda) = \E_{\pr(\vmu_{0})} \big[\pr(\vmu_{\lambda} \mid \vmu_{0}, \vx, \lambda)\big] = \N\bigg( \nicefrac{1}{\lambda} \big[\lambda_{0}\vzero + (\lambda - \lambda_0)\vx\big], \frac{\lambda - \lambda_0}{\lambda^{2}} + \frac{\lambda_{0}^{2}}{\lambda^{2}\gamma_{0}} \bigg).
  \end{equation}
  By pulling $\lambda^{-2}$ out of the covariance and inverting to get a precision, we get the claimed result.
\end{proof}

If we now choose $\gamma_{0} = \lambda_{0}$, we get the simple \bsi{} prior and the result \elbo{} encoder.

\thmbsielboclosedform*
\begin{proof}
  If we choose $\gamma_{0} = \lambda_{0}$ in \cref{thm:elbo-encoder-closed-form}, we get
  \begin{equation}
    \qr(\vmu_{\lambda} \mid \vx, \lambda) = \Np\Big( \nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx, \lambda^{2}{(\lambda - \lambda_{0} + \nicefrac{\lambda_{0}^{2}}{\lambda_{0}})}\inv \Big).
  \end{equation}
  The precision simplifies to
  \begin{equation}
    \lambda^{2}{(\lambda - \lambda_{0} + \nicefrac{\lambda_{0}^{2}}{\lambda_{0}})}\inv = \lambda^{2}{(\underbrace{\lambda - \lambda_{0} + \lambda_{0}}_{\lambda})}\inv = \lambda,
  \end{equation}
  proving the result.
\end{proof}

\subsection{Proof of {\hypersetup{hidelinks}\cref{thm:importance-sampling}}}

\thmimportancesampling*
\begin{proof}
  We know from \cref{eq:elbo-riemann-integral} that $\lmeasureinf$ is the following Riemann integral.
  \begin{equation}
    \lmeasureinf = \frac{1}{2} \int_{\lambda_{0}}^{\lambdaM} \E_{\qr(\vmu_{\lambda} \mid \vx, \lambda)} \Big[ \|(\vx - \vxhat_{\lambda})\|_{2}^{2} \Big]\,\mathrm{d}\lambda
  \end{equation}
  Now we can trivially multiply by $\nicefrac{\pr(\lambda)}{\pr(\lambda)}$ inside the expectation, proving the statement.
  \begin{equation}
    \lmeasureinf = \frac{1}{2} \int_{\lambda_{0}}^{\lambdaM} \E_{\qr(\vmu_{\lambda} \mid \vx, \lambda)} \bigg[ \frac{p(\lambda)}{p(\lambda)} \|(\vx - \vxhat_{\lambda})\|_{2}^{2} \bigg]\,\mathrm{d}\lambda = \frac{1}{2} \int_{\lambda_{0}}^{\lambdaM} \pr(\lambda) \E_{\qr(\vmu_{\lambda} \mid \vx, \lambda)} \bigg[ \frac{1}{p(\lambda)} \|(\vx - \vxhat_{\lambda})\|_{2}^{2} \bigg]\,\mathrm{d}\lambda
  \end{equation}
\end{proof}

\subsection{Proof of {\hypersetup{hidelinks}\cref{eq:expected-h}}}

\begin{proof}
  We know from \cref{thm:bsi-elbo-encoder-closed-form} that we can write $\vmu_{\lambda} = \nicefrac{\lambda - \lambda_{0}}{\lambda}\,\vx + \nicefrac{1}{\sqrt{\lambda}}\,\vepsilon$ for Gaussian noise $\vepsilon \sim \N(\vzero, \mI)$ independent of $\vx$.
  Together with the assumption $\model(\vmu, \lambda) = \vmu$, we can rewrite $h$ as
  \begin{equation}
    \begin{aligned}
      h(\lambda) & = \E_{\qr(\vmu_{\lambda} \mid \vx, \lambda)} \|\vx - \vxhat_{\lambda}\|_{2}^{2} \\
                 & = \E_{\vepsilon \sim \N(\vzero, \mI)} \bigg\|\vx - \frac{\lambda - \lambda_{0}}{\lambda}\,\vx + \frac{1}{\sqrt{\lambda}}\vepsilon\bigg\|_{2}^{2} \\
                 & = \E_{\vepsilon \sim \N(\vzero, \mI)} \bigg\|\frac{\lambda_{0}}{\lambda}\vx + \frac{1}{\sqrt{\lambda}}\vepsilon\bigg\|_{2}^{2} \\
                 & = \E_{\vepsilon \sim \N(\vzero, \mI)} {\bigg(\frac{\lambda_{0}}{\lambda}\bigg)}^{2}\|\vx\|_{2}^{2} + \frac{1}{\lambda}\|\vepsilon\|_{2}^{2} - 2 \frac{\lambda_{0}}{\sqrt{\lambda^{3}}}\vx \cdot \vepsilon
    \end{aligned}
  \end{equation}
  If we now make use of our assumption that $\E[\vx] = \vzero$ and $\var[\vx] = \mI$, we can distribute the expectation across terms and get
  \begin{equation}
    \E_{\vx}[h(\lambda)] = {\bigg(\frac{\lambda_{0}}{\lambda}\bigg)}^{2} \underbrace{\E_{\vx}\big[\|\vx\|_{2}^{2}\big]}_{= \ndim} + \frac{1}{\lambda}\underbrace{\E_{\vepsilon}\big[\|\vepsilon\|_{2}^{2}\big]}_{= \ndim} - 2 \frac{\lambda_{0}}{\sqrt{\lambda^{3}}}\underbrace{\E_{\vx, \vepsilon}[\vx \cdot \vepsilon]}_{= 0} \propto \frac{\lambda_{0}^{2}}{\lambda^{2}} + \frac{1}{\lambda}.
  \end{equation}
\end{proof}

\section{Experiment Details}\label{sec:experiment-details}

\subsection{Model Configurations}\label{sec:model-configurations}

We trained each model on 4 H100 GPUs at a batch size of 128 on CIFAR10 and 512 on ImageNet.
Training progressed at about 26,300 steps per hour for the \unet{} on CIFAR10 and 6,100 steps per hour for the \dit{}-L-2 backbones on ImageNet32.
If we take the different batch sizes into account, the two model architectures needed about equal amounts of training time.
Total training time for the 10M step training on CIFAR10 came to about two weeks.

Furthermore, we take an exponential moving average (EMA) of model weights \citep{song2021scorebased,nichol2021improved}.
We provide an overview of the model and training hyperparameters in \cref{tab:model_parameters}, and show the \unet{} and \dit{} parameters in \cref{tab:unet_parameters,tab:dit_parameters}, respectively.

To reduce the variance of the training loss further, we use low-discrepancy sampling for $t$ in \cref{alg:loss} as proposed by \citet{kingma2023variational}.
Instead of sampling $b$ independent $t$ for a batch size of $b$, we set $t_{i} = \nicefrac{i - 1}{b} + \delta \mod 1, i \in [b]$ for a shared $\delta \sim \mathcal{U}(0, 1)$ where $\mod 1$ means that we discard the integer part of the result.
The marginal distribution of each $t_{i}$ is $\mathcal{U}(0, 1)$, but jointly they cover the $[0, 1]$ interval more uniformly than independent samples would, smoothing out the loss across batches.

\begin{table}[H]
  \centering
  \caption{Model and training parameters of BSI.}\label{tab:model_parameters}
  \begin{tabular}{rcccc}
    \toprule
                                            & Parameter               & CIFAR10                          & ImageNet32 & ImageNet64 \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{BSI}}    & $\alpha_0$              & \multicolumn{3}{c}{\num{1e-2}}                             \\
                                            & $\alpha_M$              & \multicolumn{3}{c}{\num{1e6}}                              \\
                                            & $\alpha_R$              & \multicolumn{3}{c}{\num{2e6}}                              \\
    \midrule
    \multirow{4}{*}{\rotatebox{90}{Optim.}} & Learning rate           & \num{2e-4}                       & \num{2e-4} & \num{3e-4} \\
                                            & Weight decay            & \multicolumn{3}{c}{\num{1e-2}}                             \\
                                            & Batch size              & 128                              & 512        & 512        \\
                                            & Steps                   & \multicolumn{3}{c}{10000000}                               \\
    \midrule
    \multirow{2}{*}{\rotatebox{90}{EMA}}    & $\beta$                 & \multicolumn{3}{c}{\num{0.9999}}                           \\
                                            & First update after step & \multicolumn{3}{c}{1000}                                   \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{table}[H]
  \centering
  \caption{Hyperparameters of the \unet{} architecture used for the CIFAR10 experiments.}\label{tab:unet_parameters}
  \begin{tabular}{cc}
    \toprule
    Parameter           & Value \\
    \midrule
    Hidden dim.         & 128   \\
    Levels              & 32    \\
    Dropout             & 0.1   \\
    Attention heads     & 1     \\
    Convolution padding & Zeros \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{table}[H]
  \centering
  \caption{Hyperparameters of the DiT architecture used for the ImageNet experiments.}\label{tab:dit_parameters}
  \begin{tabular}{cccc}
    \toprule
    Parameter       & ImageNet32               & ImageNet64 \\
    \midrule
    Architecture    & DiT-L-2                  & DiT-L-4    \\
    Hidden dim.     & \multicolumn{2}{c}{1024}              \\
    Depth           & \multicolumn{2}{c}{24}                \\
    Attention heads & \multicolumn{2}{c}{16}                \\
    Dropout         & \multicolumn{2}{c}{0.05}              \\
    Patch Size      & 2                        & 4          \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Additional Samples}\label{sec:additional-samples}

\cref{fig:samples-cifar10,fig:samples-imagenet32,fig:samples-imagenet64} show additional samples from models trained on CIFAR10, ImageNet32 and ImageNet64, respectively.

\begin{figure}[H]
  \centering
  \includegraphics{figures/samples-appendix-cifar10}
  \caption{Additional samples from our model trained on CIFAR10. Generated with the linear schedule and 1024 steps. The first two columns show samples from the dataset for comparison.}\label{fig:samples-cifar10}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics{figures/samples-appendix-imagenet32}
  \caption{Additional samples from our model trained on ImageNet32. Generated with the linear schedule and 1024 steps. The first two columns show samples from the dataset for comparison.}\label{fig:samples-imagenet32}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics{figures/samples-appendix-imagenet64}
  \caption{Additional samples from our model trained on ImageNet64. Generated with the linear schedule and 1024 steps. The first two columns show samples from the dataset for comparison.}\label{fig:samples-imagenet64}
\end{figure}

\end{document}
