% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
% \usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage[fleqn]{amsmath}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{fixltx2e}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage[most]{tcolorbox}
\usepackage{afterpage}
\newtheorem{definition}{Definition}

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ours}{\texttt{mmE5}}
\newcommand{\todo}[1]{\{\textcolor{blue}{\textbf{TODO}}\}}


\title{\ours{}: Improving Multimodal Multilingual Embeddings via \\ High-quality Synthetic Data}

\author{Haonan Chen$^{1}$\thanks{$^{*}$Work done during Haonanâ€™s internship at MSR Asia. Prof. Zhicheng Dou is the corresponding author.}, Liang Wang$^2$, Nan Yang$^2$, Yutao Zhu$^1$ \\ \textbf{Ziliang Zhao$^1$, Furu Wei$^2$, Zhicheng Dou$^{1}$} \\
        $^1$Gaoling School of Artificial Intelligence, Renmin University of China \\ 
        $^2$Microsoft Corporation \\ 
        \texttt{\{hnchen,dou\}@ruc.edu.cn} \\
        \texttt{\{wangliang,nanya,fuwei\}@microsoft.com} \\
        \url{https://github.com/haon-chen/mmE5} \\
}

\newtcolorbox[list inside=prompt]{prompt}[1][]{
    colbacktitle=black!60,
    coltitle=white,
    fontupper=\footnotesize,
    boxsep=5pt,
    left=0pt,
    right=-1pt,
    top=0pt,
    bottom=0pt,
    boxrule=1pt,
    width=\textwidth,
    #1,
}
\begin{document}
\maketitle

\begin{abstract}

Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. 
However, the limited labeled multimodal data often hinders embedding performance. 
Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. 
In this work, we identify three criteria for high-quality synthetic multimodal data. First, \textbf{broad scope} ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. 
Second, \textbf{robust cross-modal alignment} makes different modalities semantically consistent. 
Third, \textbf{high fidelity} ensures that the synthetic data maintains realistic details to enhance its reliability.
Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement.
Leveraging these high-quality synthetic and labeled datasets, we train a \textbf{m}ultimodal \textbf{m}ultilingual \textbf{E5} model \ours{}. 
Extensive experiments demonstrate that \ours{} achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark.
Our codes, datasets and models are released in \url{https://github.com/haon-chen/mmE5}.


\end{abstract}
\input{sections/1_introduction.tex}
\input{sections/2_related_work.tex}
\input{sections/3_methodology.tex}
\input{sections/4_experiments.tex}
\input{sections/5_conclusion.tex}

\input{acl2023.bbl}

\appendix

\input{sections/appendix.tex}

\end{document}
