\section{Methodology: \ours{}}

In this section, we present our method, which synthesizes high-quality multimodal data for the further finetuning of our embedding model \ours{}.
As shown in Figure~\ref{fig:framework}, our method consists of five stages: 
(1) Initially, for each data sample to be synthesized, we configure the specifics of the task, modality combination, language, and input images.
(2) We employ an MLLM to generate multi-grained descriptions for the input images, ensuring that the synthesized texts are well-aligned with the images.
(3) Utilizing this MLLM, we synthesize text data based on both the images and their descriptions.
(4) The MLLM then evaluates its synthesized data from multiple perspectives, offering revised data to enhance cross-modal alignment and fidelity.
(5) Finally, the synthesized texts and images are used to finetune an MLLM specifically for embedding tasks.
To minimize potential information loss, stages (2), (3), and (4) are executed within a single pass of the MLLM.

\subsection{Preliminaries}

An MLLM can accept text, image, or text-image pairs as input, allowing both the query side $q$ and the document side $d$ to be multimodal.
Inspired by existing works on synthetic text embedding data~\cite{E5mistral, speed}, each data sample we generate is a quadruple of (task instruction, query, positive document, hard negative document), denoted as $(t, q, d^+, d^-)$.
For each data piece, we first sample images from the large-scale open-source image corpus LAION-400M~\cite{LAION} as the query image, positive image, and hard negative image ($q_i$, $d^+_i$, $d^-_i$).
Then, with these three images as input, an MLLM $\pi_{\theta}$ can synthesize a multimodal embedding data sample $ y \sim \pi_{\theta} (y \mid q_i, d^+_i, d^-_i) $, where $y = (t, q_t, d^+_t, d^-_t)$.
As a result, the synthetic data can have a maximum of seven elements: $\{t, (q_t, q_i), (d^+_t, d^+_i), (d^-_t, d^-_i)\}$.
% Note that for different modality combinations, the input and output format of the synthetic data can be various.
% For example, for I$\rightarrow$IT and T$\rightarrow$IT types, there can be no query text and image, respectively.
More data examples can be found in Appendix~\ref{appendix: examples}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/framework.pdf}%\vspace{-1ex}
	\caption{An illustration of our method. We take the generation of an IT$\rightarrow$IT retrieval data sample as an example.}
	\vspace{-2ex}
	\label{fig:framework}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Data Synthesis Framework}

Guided by the principles of high-quality synthetic multimodal data, \ie, broad scope, robust cross-modal alignment, and high fidelity, we introduce a data synthesis framework. This framework is designed to synthesize high-quality data that transforms an MLLM for downstream embedding tasks.

\subsubsection{Data Configuration}

To prepare for the data synthesis process, we configure the input data from three aspects:

\noindent \textbf{Task and Modality Combination}
We aim to synthesize data with a broad scope by generating beyond simple retrieval data of IT$\rightarrow$IT and T$\rightarrow$IT types.
Our data cover three key multimodal embedding tasks identified by previous work~\cite{MMEB}: classification, VQA, and retrieval.
After selecting a task for synthesis, we will sample a modality combination with respect to the specific task, such as choosing from seven possible combinations for the retrieval task type.
Note that we only synthesize data of modality types that are included in the MMEB benchmark~\cite{MMEB}, which can cover most scenarios.

\noindent \textbf{Image}
Despite the powerful multimodal capabilities of modern MLLMs (\eg, GPT-4o, Llama-3.2~\cite{llama3}, and Llava-1.6), most cannot generate images, and those that can often produce low-fidelity images~\cite{VISTA}.
Following previous works~\cite{MagicLens, megapairs}, we sample real images from the LAION-400M corpus~\cite{LAION}.
First, we will sample a query image from the corpus ($q_i \in \mathcal{I}$).
Then, for the modality types involving images on the document side (\eg, IT$\rightarrow$IT), we use a small embedding model, jina-clip-v2~\cite{jina-clip}, to retrieve a similar positive image $d^+_i$ and a hard negative image $d^-_i$ efficiently.

\noindent \textbf{Language}
Most existing models only focus on high-source languages like English, harming the multilingual ability of embedding models.
To synthesize multilingual data, we sample languages from the language list of XLM-R~\cite{XLM-R} during configuration.
In order to facilitate the common usage scenarios, we give high-source languages higher weights.
Note that the generated task instruction will always be in English for effective instruction tuning.

\subsubsection{One-pass Generation with MLLM}

With the data configuration ready, we introduce a deep thinking process that involves interpreting input images, generating data, and performing self-evaluation.
To ensure that the MLLM always takes the image context into account, we execute this entire process in a single pass.

\begin{table*}[h]
\centering
\small
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{{Models}}  & \multicolumn{4}{c}{{Per Meta-Task Score}}       &\multicolumn{3}{c}{{Average Scor}e} \\
\cmidrule(r){2-5}
\cmidrule(l){6-8}
 & Class. & VQA  & Retr. & Ground. &  IND  & OOD & Overall                     \\
\midrule
\multicolumn{7}{l}{\textit{Zero-shot Setting Models}}                   \\
% \midrule
CLIP~\cite{CLIP}                                                        & 42.8           & 9.1  & 53.0      & 51.8   & - & -  & 37.8                     \\
BLIP2~\cite{blip2}                                                       & 27.0           & 4.2  & 33.9      & 47.0  & - & -   & 25.2                     \\
SigLIP~\cite{SigLIP}                                                         & 40.3           & 8.4  & 31.6      & 59.5  & - & -   & 34.8                     \\
OpenCLIP~\cite{OpenCLIP}                                                & {47.8}           & 10.9 & 52.3      & 53.3   & - & -  & 39.7                     \\
E5-V~\cite{E5-V}                                           & 21.8           & 4.9  & 11.5      & 19.0    & - & - & 13.3    \\
MagicLens~\cite{MagicLens}                                                 & 38.8           & 8.3  & 35.4      & 26.0   & - & -   & 27.8                     \\
MMRet (w/ 26M synthetic data) & {47.2}           & {18.4} & \textbf{56.5}      & {62.2}      & - & -     & {44.0} \\
\rowcolor{gray!20}
{\ours{}} (w/ 560K synthetic data)             & \textbf{60.6}           & \textbf{55.7} & \underline{54.7}      & \textbf{72.4}      & - & -     & \textbf{58.6}  \\
\midrule
\multicolumn{7}{l}{\textit{Partially Supervised Finetuning Models}$^\dag$}     \\ 
% \midrule 
UniIR~\cite{UniIR}                                   & 42.1           & {15.0} & {60.1}      & {62.2}  & -  & -  & {42.8}                     \\
MM-EMBED~\cite{mmembed}     & {48.1}           & {32.2} & {63.8}      & {57.8}      & - & -     & {50.0} \\
GME~\cite{GME}   & {56.9}              & {41.2} & {67.8}      & {53.4}      & - & -     & {55.8} \\
\midrule
\multicolumn{7}{l}{\textit{Supervised Finetuning Models}}                   \\
% \midrule
CLIP~\cite{CLIP}                     & 55.2           & 19.7 & 53.2      & 62.2      & 47.6     & 42.8     & 45.4        \\
OpenCLIP~\cite{OpenCLIP}                 & {56.0}           & 21.9 & 55.4      & 64.1      & 50.5     & 43.1     & 47.2        \\
VLM2Vec~\cite{MMEB}        & \underline{61.2}           & {49.9} & {67.4}      & \underline{86.1}      & {67.5}     & {57.1}     & {62.9}        \\
MMRet~\cite{megapairs} & {56.0}           & \underline{57.4} & \underline{69.9}      & {83.6}      & \underline{68.0}     & \underline{59.1}     & \underline{64.1} \\
\rowcolor{gray!20}
{\ours{}} (w/ synthetic data + labeled data)             & \textbf{67.6}           & \textbf{62.8} & \textbf{70.9}      & \textbf{89.7}      & \textbf{72.3}     & \textbf{66.7}     & \textbf{69.8}  \\

\bottomrule
\end{tabular}
\vspace{-0.1cm}
\caption{Results on MMEB benchmark, consisting of 36 tasks across four types: classification (Class.), VQA, retrieval (Retr.), and visual grounding (Ground.).
$^\dag$ UniIR, MM-EMBED, and GME are not strictly zero-shot models. 
UniIR and MM-EMBED are trained on the MBEIR dataset~\cite{UniIR}, which includes 10 retrieval datasets included in the MMEB. 
Similarly, GME is trained on the UMRB dataset~\cite{GME}, which shares 14 datasets with the MMEB.
For VLM2Vec, we use the LLaVA-based version with high-resolution images reported in its original paper.
The second-best performances are underlined and the best performances are in bold.
}
\label{tab:mmeb}
\end{table*}


\noindent \textbf{Multi-aspect Visual Interpretation}
To obtain a comprehensive understanding of the images, the MLLM $\pi_{\theta}$ first analyzes them from multiple perspectives:
(1) the general information,
(2) detailed description of the objects present,
(3) contextual background information, and
(4) potential connections between the image and the text that may be synthesized.
The deep understanding of the images enables $\pi_{\theta}$ to produce texts that are closely aligned with the visual content, thereby enhancing the cross-modal alignment.


\noindent \textbf{Synthesizing Data}
Using the images and their descriptions as input, we prompt $\pi_{\theta}$ to synthesize texts $(t, q_t, d^+_t, d^-_t)$.
Specifically, the text instruction $t$ is expected to connect $q_i$ with $d^+_i$.\footnote{Because of limited space, full prompts are omitted in this section. The complete prompts can be found in Appendix~\ref{appendix: prompt}.}
The query and document texts should be relevant to their respective images.
Note that the input and output formats for the synthetic data may vary depending on the combination of modalities.
For example, for I$\rightarrow$IT and T$\rightarrow$IT types, there can be no query text and image, respectively.


\noindent \textbf{Self-evaluation}
In order to further enhance the quality of the synthetic data, $\pi_{\theta}$ evaluates the data it synthesizes from:
(1) the relevance of the texts to their corresponding images,
(2) the plausibility of hard negatives,
(3) the clarity of $t$, and
(4) the diversity (creativity) of the synthesized data.
Following this evaluation, $\pi_{\theta}$ provides suggestions for potential improvements.
Finally, a revised version of each data sample is produced and utilized for the subsequent contrastive training phase.


\subsection{Finetuning Embedding Model \ours{}}

Following previous works of instruction-tuned text embedding models~\cite{bge, Llama2Vec} and multimodal embedding models~\cite{MMEB}, we apply an instruction template on each query: $ \text{[IMAGE]}~\{t\}~\textbackslash n~\{q_t\}~\{q_i\}$, where ``$\text{[IMAGE]}$'' is the image token that varies from different MLLMs.
We then append an ``$\text{[EOS]}$'' token to each query and document.
The representation of each input in an MLLM is derived from the output of the ``$\text{[EOS]}$'' token from the final layer.

We utilize the InfoNCE loss~\cite{infonce} to perform the standard contrastive learning objective on our synthetic data $\mathcal{D}$:
\begin{eqnarray}
\mathcal{L} = -\log\frac{\phi(\mathbf{q},\mathbf{d}^+)}{\phi(\mathbf{q},\mathbf{d}^+) + \sum_{{d}^-\in\mathcal{N}}{\phi(\mathbf{q},\mathbf{d}^-)}}, 
\label{equation:cl}
\end{eqnarray}
where $\mathbf{q}$ is the encoded multimodal query, $\mathbf{d}$ represents the encoded document, and $\mathcal{N}$ denotes the set of negative documents.
The function $\phi(\cdot) = \exp(\text{cos}(\cdot) / \tau)$, where ${\rm cos}(\cdot)$ denotes cosine similarity, and $\tau$ is a temperature hyperparameter.





