\section{Conclusion}
In this work, we synthesize high-quality multimodal multilingual data to train the model \ours{}.
We first define high-quality multimodal synthetic data based on three criteria: broad scope, robust cross-modal alignment, and high fidelity.
Then, we develop a data synthesis framework guided by these principles.
Finally, we train a multimodal multilingual embedding model using the high-quality synthetic data.
\ours{} achieves SOTA performances on both the general benchmark MMEB and the multilingual benchmark XTD.

% \clearpage

\section*{Limitations}

Our work has several limitations that we intend to resolve in future research:

\begin{enumerate}
    \item Our model currently relies on the proprietary MLLM GPT-4o for synthesizing multimodal data. Future work should explore aligning smaller MLLMs with the knowledge from GPT-like models to achieve more efficient data synthesis.
    \item \ours{} focus on text and image modalities. Future models should aim to extend coverage to additional modalities, such as audio and video.
    \item Due to the cost limitation and the observed scaling effect, we limited the amount of data produced for model training. Future research may consider increasing data size while preserving diversity to optimize model performance.
\end{enumerate}



