\section{Related Work}

\noindent \textbf{Multimodal Embedding}
Previous studies, such as CLIP~\cite{CLIP}, Align~\cite{ALIGN}, BLIP~\cite{BLIP}, and CoCa~\cite{CoCa}, have employed large-scale weakly supervised data to learn separate multimodal representations through pre-training.
Some works attempt to obtain universal embeddings for texts and images utilizing existing CLIP-like models~\cite{UniIR, UniVL-DR, VISTA, MARVEL}.
For instance, UniIR~\cite{UniIR} integrates separate embeddings from different modalities into unified features.
Recent approaches finetune MLLMs to leverage their multimodal reasoning capabilities for obtaining universal representations~\cite{E5-V, MMEB, GME, megapairs, mmembed}.
For example, VLM2Vec~\cite{MMEB} utilizes instruction-tuning to transform MLLMs into embedding models.

% \noindent \textbf{Multimodal Large Language Models}
\begin{table*}[!t]
\centering
\small
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.5}
    \begin{tabular}{l|c|c|l|c|c|c}
    \toprule
Method    & \# Languages & Task                                                                      & Modality Combinations                                                            & w/ MLLM & \multicolumn{1}{l|}{One Pass}  & Self-evaluation \\ \midrule
MagicLens & 1 (English)  & Retrieval                                                                 & IT→I                                                                              & $\times$        & $\surd$                            & $\times$           \\
\hline
MegaPairs     & 1 (English)  & Retrieval                                                                 & IT→I                                                                              & $\surd$        & $\times$                             & $\times$               \\
\hline
GME       & 1 (English)  & Retrieval                                                                 & T→IT, IT→IT                                                                       & $\times$        & $\times$                            & $\times$      \\
\midrule
\ours{} (Ours)      & \begin{tabular}[c]{@{}c@{}}93 (English, \\ Spanish, etc.)\end{tabular}      & \begin{tabular}[c]{@{}c@{}}Classification, \\ VQA, Retrieval\end{tabular} & \begin{tabular}[l]{@{}l@{}}IT→I, T→IT, IT→IT, \\ I→I, I→T, IT→T, T→I\end{tabular} & $\surd$        & $\surd$                             & $\surd$              \\
\bottomrule
    \end{tabular}
    % \vspace{-5px}
    \caption{Comparison of the synthetic datasets in our work with those from previous methods.
    Our synthetic datasets incorporate 93 languages, two additional tasks, and more modality combinations.
    ``IT$\rightarrow$T'' denotes a modality combination, where ``IT'' denotes images and texts on the query side and ``T'' denotes texts on the target side.
    The entire data synthesis process is executed within a single pass of an MLLM, thereby avoiding potential information loss and ensuring robust cross-modal alignment.
    We also employ real images and self-evaluation to maintain fidelity.}
    \label{tab:comparison_syndata}
\end{table*} 


\noindent \textbf{Synthetic Data} The generation of synthetic data has been extensively explored for text embedding tasks~\cite{E5mistral, speed, glan}.
With the recent emergence of MLLMs like Phi-3.5-V~\cite{Phi3} and LLaVA~\cite{Llava}, along with diffusion models like Stable Diffusion~\cite{diffusion}, researchers have been focused on synthesizing data to address the scarcity of multimodal instruction-tuning datasets.
For example, MagicLens~\cite{MagicLens} utilizes co-existing images from the same webpage and an LLM to create multimodal data triplets (query image, instruction, relevant image), \ie, IT$\rightarrow$I paradigm.
MegaPairs~\cite{megapairs} aims to synthesize more diverse data triplets by retrieving relevant images from different perspectives.
GME~\cite{GME} employs an LLM and a diffusion model to generate a fused modality dataset that includes both T$\rightarrow$IT and IT$\rightarrow$IT types.
Table~\ref{tab:comparison_syndata} presents a comparison of the synthesized data in this study with that of previous works.
