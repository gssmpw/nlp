

\section{Experiments}


\subsection{Experimental Setup}
\label{subsec:setup}


We synthesize a total of 560K multimodal embedding data samples.
The MLLM utilized for data synthesis is \textit{GPT-4o-2024-08-06}.
The backbone model for \ours{} is Llama-3.2-11B-Vision\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-11B-Vision}}.
For finetuning \ours{}, we employed LoRA~\cite{lora} with a rank of 8.
We evaluate the general embedding performance in terms of Precision@1 on the MMEB benchmark~\cite{MMEB}.
This benchmark comprises 36 multimodal embedding tasks across four categories: classification (10), VQA (10), retrieval (12), and visual grounding (4).
Our synthetic dataset is distributed among classification, VQA, and retrieval tasks in a 1:1:2 ratio.
We synthesize more retrieval data since this type contains more kinds of modality combinations.
We do not synthesize visual grounding data since they are relatively simpler for MLLM based on the MMEB results.
To evaluate multilingual multimodal capabilities, we conducted tests using the XTD benchmark~\cite{XTD}.
Following MURAL~\cite{mural}, we conduct experiments on seven languages of XTD and report Recall@10 results.
Additional details regarding the synthetic data, prompts, and implementation can be found in Appendix~\ref{appendix: syndata}, \ref{appendix: implementation}, and~\ref{appendix: prompt}, respectively.



\begin{figure}[!tbp]
\centering
\includegraphics[width=0.45\textwidth]{figures/language_distribution.pdf}
\vspace{-2pt}
\caption{Distribution of languages in the synthetic data.}
\label{fig:language_dist} 
\end{figure}

\subsection{Results on MMEB}

The overall results on the MMEB benchmark are presented in Table~\ref{tab:mmeb}.
\ours{} achieves the best performance on both zero-shot setting (with synthetic data only) and supervised setting (with IND training datasets of MMEB).
This demonstrates the quality of our synthetic data and the effectiveness of our multimodal embedding model.
Furthermore, we can make the following observations:
(1) \ours{} generalizes well on all four kinds of tasks.
This demonstrates the broad scope of our synthetic multimodal embedding data in terms of task types.
(2) With only 560K synthetic data, \ours{} manages to perform better than MMRet which uses 26M data.
This proves the quality of our synthetic data again.
(3) Intriguingly, \ours{} underperforms MMRet on retrieval tasks in a zero-shot setting.
% This is because that some models are trained on some tasks of MMEB (UniIR, MM-EMBED, and GME), which makes them not strictly on zero-shot setting.
This is because MMRet is trained on 26M pure retrieval data, which makes it perform well on retrieval tasks, but generalizes poorly on other task types.


\subsection{Multilingual Performance on XTD}


We synthesize a multilingual multimodal dataset that consists of 93 languages, in order to train our embedding model \ours{} to generalize across more languages.
The language distribution of our dataset is presented in Figure~\ref{fig:language_dist}.
Notably, the dataset primarily consists of English data samples, facilitating common usage scenarios.
For the 75 low-resource languages, we evenly synthesize data samples to obtain a balanced multilingual dataset that supports comprehensive cross-linguistic generalization.

To evaluate the multilingual capability of \ours{}, we conduct experiments across seven languages on a text-to-image retrieval benchmark XTD.
As presented in Table~\ref{tab:xtd}, \ours{} outperforms other models in terms of overall performances on all languages, demonstrating its superior multilingual multimodal embedding capability.
The following observations can be made:
(1) The multilingual performance of multimodal embedding models is largely dependent on their foundational models.
For example, jina-clip-v2 and M-CLIP outperform VLM2Vec-LLaVA, despite VLM2Vec's strong performance on MMEB.
GME exhibits robust performance on XTD, which can be attributed to the powerful multilingual MLLM, Qwen2-VL~\cite{Qwen2-VL}.
(2) The performance of \ours{} declines when labeled data is omitted, indicating that general multimodal capabilities remain essential for multilingual retrieval tasks.
(3) In a zero-shot setting, \ours{} trained on multilingual synthetic data (\ours{} w/ synthetic data only) outperforms \ours{} with the same amount of English synthetic data (\ours{} w/ english synthetic data).
This suggests that the extensive language coverage provided by our synthetic data enhances the multilingual capabilities of embedding models.

\begin{table}[!tbp]
\centering
\scriptsize
\setlength{\tabcolsep}{2pt}
\begin{tabular}{l|cccccccc}
\toprule
Model          & it   & es   & ru   & zh   & pl   & tr   & ko   & Avg. \\
\midrule
ALIGN~\cite{ALIGN}           & 87.9 & 88.8 & 82.3 & 86.5 & 79.8 & 73.5 & 76.6 & 82.2 \\
MURAL~\cite{mural}           & 91.8 & 92.9 & 87.2 & 89.7 & 91.0 & 89.5 & 88.1 & 90.0 \\
VLM2Vec~\cite{MMEB} & 83.7 & 87.1 & 86.7 & 92.8 & 76.1 & 37.2 & 63.9 & 75.4 \\
jina~\cite{jina-clip}    & 93.6 & 94.1 & 89.8 & 91.8 & 94.3 & 92.7 & 90.1 & 92.3 \\
M-CLIP~\cite{mclip}  & 93.1 & 93.6 & 90.0 & 94.0 & 94.3 & 93.1 & 89.0 & 92.4 \\
% MM-EMBED~\cite{mmembed}        & 95.4 & 96.6 & 92.8 & 95.8 & 95.8 & 85.3 & 94.0 & 93.6 \\
GME~\cite{GME}             & 95.1 & 96.4 & 92.3 & 96.4 & 94.9 & 89.8 & 93.6 & 94.1 \\
\midrule
\ours{} (full) & 96.1 & 96.2 & 93.3 & 96.3 & 95.4 & 93.6 & 96.0 & \textbf{95.3} \\
\quad w/ synthetic data only & 90.9 & 89.6 & 86.3 & 90.2 & 90.3 & 87.2 & 86.7 & 88.7 \\
\quad w/ english synthetic data & 86.3 & 86.3 & 84.2 & 88.8 & 84.9 & 81.0 & 84.4 & 85.1 \\
\bottomrule
\end{tabular}
\caption{Results on XTD benchmark, a text-to-image retrieval task covering seven languages.
}
\label{tab:xtd}
\end{table}

\subsection{Application to Other Base MLLM}


We train \ours{} based on the powerful MLLM LLaMA-3.2-Vision, which is instruction-tuned and effective in interpreting multimodal inputs.
Notably, our synthetic data and training paradigm can effectively transform other foundation MLLMs into embedding models.
We use both our synthetic data and labeled data to train LLaVA-1.6\footnote{\url{https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf}} and Phi-3.5-V\footnote{\url{https://huggingface.co/microsoft/Phi-3.5-vision-instruct}}.
The performances of \ours{} with different foundation MLLMs are presented in Table~\ref{tab:base_model}.
The results show that models trained using our method consistently outperform baseline models built on the same foundational MLLMs.
This indicates that our synthetic data can effectively enhance the capability of MLLMs to embed multimodal inputs.

\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{p{0.3\textwidth}cc}
    \toprule
        Base MLLM & Avg. on MMEB  \\
        \midrule
        Phi-3.5-V~\cite{Phi3} &   61.0 \\
        LLaVA-1.6~\cite{Llava} &  65.8  \\
        LLaMA-3-Vision~\cite{llama3} (Ours)  & \textbf{69.8}  \\
    \midrule
    \multicolumn{2}{l}{\textit{Baselines (For Reference)}} \\ 
    \midrule
        VLM2Vec (Phi-3.5-V) &   60.1 \\
        VLM2Vec (LLaVA-1.6) &   62.9 \\
        MMRet (LLaVA-1.6) &   64.1 \\
        VLM2Vec (LLaMA-3.2) &   64.8 \\
    \bottomrule
    \end{tabular}
    % \vspace{-5px}
    \caption{Performances of \ours{} with different MLLMs.}
    \label{tab:base_model}
\end{table} 

\subsection{Discussions of Data Synthesis Process}

In this section, we will further investigate the data synthesis process via zero-shot experiments.


\subsubsection{Ablation Studies}

To evaluate each component of our data synthesis framework, we conduct ablation studies of \ours{}:

\noindent \textbf{Deep Thinking Process}
To synthesize high-quality data, we introduce a deep thinking process to boost data synthesis.
As presented in Table~\ref{tab:ablation}, the performance of \ours{} declines when the Visual Interpretation and Self-evaluation components are excluded.
For example, \ours{} performs worse when utilizing the original data compared to revised data.
This indicates that the self-evaluation mechanism can enhance data fidelity, facilitating the training of a more robust embedding model.

\begin{table}[!t]
    \centering
    \small
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{lc}    
    \toprule
         Model & Avg. on MMEB  \\
        \midrule
        \ours{} (280K synthetic data only) & \textbf{57.4}  \\
        \midrule
        \quad w/o. Visual Interpertation &  57.2  \\
        \quad w/o. Self-evaluation &  56.0  \\
        \hline
        \quad w/o. Classification Data &  52.5  \\
        \quad w/o. VQA Data &  55.1  \\
        \quad w/o. Retrieval Data &  56.5  \\
        \hline
        \quad w/ IT2I only (MagicLens~\&~MegaPairs) & 30.1   \\
        \quad w/  IT2IT~\&~T2IT only (GME) &  28.6  \\
        \hline
        \quad w/o. Hard Negative &  56.2  \\
        \hline
        \quad w/ English Data only (280K) & 57.6 \\
        \quad w/o. English Data (280K) & 56.9 \\
    \bottomrule
    \end{tabular}
    % \vspace{-5px}
    \caption{Performances of ablated models on MMEB.
For efficient test, we conduct zero-shot experiments on 280K synthetic data, which has the same tasks, modality types and languages as the full synthetic data.}
    \label{tab:ablation}
\end{table} 

\noindent \textbf{Embedding Task Types}
In order to expand the scope of data, we synthesize data across three task types: classification, VQA, and retrieval.
The performance of \ours{} decreases after each type of multimodal embedding data is omitted, demonstrating that our diverse synthetic data can facilitate model generalization.
Intriguingly, the performance drops the least after removing the retrieval data, which is inconsistent with previous research~\cite{MMEB}.
One possible explanation is that our backbone, Llama-3.2 Vision, inherently exhibits more robust retrieval capabilities than Phi-3.5-V. 

\noindent \textbf{Modality Combinations}
Most prior works focus on one or two modality types, such as ``IT2I'' (\eg, MagicLens~\cite{MagicLens} and MegaPairs~\cite{megapairs}) or ``IT2IT~\&~T2IT'' (\eg, GME~\cite{GME}).
We propose to synthesize data across various modality combinations to enhance the diversity of our synthetic dataset, \ie, the scope of our synthetic multimodal data.
To evaluate the impact of these additional modality combinations, we train \ours{} with the same amount of datasets that contain types ``IT2I'' or ``IT2IT~\&~T2IT'' only.
The performance of \ours{} significantly decreased when limited to these combinations from previous works, which indicates that the additional modalities enable our embedding model to generalize more effectively across different combinations and task types.


\noindent \textbf{Hard Negative}
Each sample in our synthetic dataset incorporates a hard negative document to help \ours{} learn subtle differences.
After excluding the hard negatives, the model's performance drops significantly, which demonstrates the importance of this technique for contrastive learning.

\noindent \textbf{Language}
To investigate the impact of linguistic diversity on model performance on English benchmarks, we conducted experiments using synthetic data in two configurations: English-only and non-English languages only. 
Our model, \ours{}, demonstrated a slight performance advantage with English-only synthetic data, although the difference was minimal. 
Nonetheless, \ours{} achieved satisfactory results with 280K data samples from languages other than English. 
This suggests that our multilingual dataset enhances the embedding model's ability to generalize effectively in both multilingual and English-only contexts.


\subsubsection{Scaling Effect}


The scaling effect is an important aspect of synthetic data generation for multimodal embedding models~\cite{GME, megapairs}. 
It explores how the performance of the model varies with the size of synthetic datasets. 
Besides, the data synthesis and training processes demand significant computational resources and time.
Therefore, studying the scaling effect allows us to identify the point of diminishing returns, ensuring that resources are utilized efficiently without overproducing redundant data.

In this section, we further investigate the performance of \ours{} using synthetic datasets of varying sizes.
Specifically, we conduct zero-shot experiments on MMEB to analyze the scaling effect.
As illustrated in Figure~\ref{fig:scaling_law}, \ours{} consistently achieves better performance with increased training data, demonstrating the high quality of our synthetic data again.
This paradigm also indicates a linear-log relationship between the model performance and data size, consistent with previous works of text embedding~\cite{speed} and dense retrieval~\cite{scaling_ds}.
This finding facilitates the balancing of the cost and the multimodal embedding model performance for future works.



\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figures/scaling_law.pdf}
\vspace{-2pt}
\caption{The impact of synthetic data size on multimodal embedding performance on MMEB.}
\label{fig:scaling_law} 
\end{figure}



\subsection{Hyperparameter Analysis}

In order to analyze the training process of our multimodal embedding model, we perform experiments with \ours{} using various training settings.
For efficiency, we report zero-shot results for \ours{} trained with 280K synthetic data.
Note that we tune these hyperparameters on evaluation datasets comprising 1K samples from each training set.
However, for consistency with previous experiments, we present results on the MMEB test sets.


\noindent \textbf{LoRA Rank}
denotes the rank of the additional low-rank matrices in LoRA. 
This parameter influences the number of parameters added into the original model, balancing the model's capacity and computational efficiency.
As shown in the left part of Figure~\ref{fig:param}, the performance of \ours{} initially improves then drops.
This demonstrates a trade-off: a lower rank reduces memory and computation but may lead to underfitting if r is too small, whereas a higher rank risks harming the pre-trained multimodal reasoning capabilities of MLLM.



\noindent \textbf{Training Batch Size}
In contrastive learning, batch size plays a critical role because it directly affects the number of negative samples available for training.
As presented in the middle part of Figure~\ref{fig:param}, the performance of \ours{} consistently increases with larger batch size.
However, large batches demand significantly more GPU memory, \ie, more computational resources.


\noindent \textbf{Temperature }
The temperature parameter $\tau$ in the InfoNCE loss (Equation~\ref{equation:cl}) influences the separation between positive and negative samples in the embedding space.
We can observe that \ours{}'s performance first improves then declines with larger temperature.
This pattern suggests a trade-off:
a low $\tau$ forces the model to strongly penalize near-positive negatives which can lead to overfitting, while a high $\tau$ leads to a more uniform distribution of embeddings which may hinder the effective separation of positive and negative samples.



\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/params.pdf}
\vspace{-2pt}
\caption{The zero-shot performances of \ours{} with different training settings on MMEB (280K synthetic data for efficient test).}
\label{fig:param} 
\end{figure}


