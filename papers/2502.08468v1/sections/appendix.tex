
\clearpage


\section*{Appendix}

\section{Details about Synthetic Data}
\label{appendix: syndata}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
\small
\centering
\begin{tabular}{l|c|c}
\toprule
Task                            & Modality Combination                    & \# samples       \\ \midrule
\multirow{2}{*}{Classification} & image-to-text                &          126,177             \\
                                & (image,text)-to-text         &     13,823                  \\
\midrule
\multirow{7}{*}{Retrieval}      & image-to-text                &   98,040                    \\
                                & (image,text)-to-text         &      41,960                 \\
                                & (image,text)-to-image        &     56,185                  \\
                                & image-to-image               &     27,988                  \\
                                & (image,text)-to-(image,text) &   27,656                    \\
                                & text-to-image                &    14,090                   \\
                                & text-to-(image,text)         &  14,081    \\
\midrule
VQA                             & (image,text)-to-text         & 140,000 \\
\bottomrule
\end{tabular}
\caption{Statistics of the multimodal synthetic data used for training \ours{}.} 
\label{tab:syndata_statistics}
\end{table}

In this study, we introduce a synthetic multimodal multilingual embedding dataset designed to facilitate model learning. 
This section delves into the details of our synthetic dataset. 
The dataset is comprised of three distinct tasks and seven modality combinations, totaling 560K data samples. 
Table~\ref{tab:syndata_statistics} provides a detailed statistical overview of our synthetic data, categorized by tasks and modalities.


\section{Implementation Details}
\label{appendix: implementation}

\subsection{Data Synthesis}

For the data synthesis process, we employ the MLLM \textit{GPT-4o-2024-08-06} model to generate data samples. 
Both the temperature and top-p parameters are set to 1.0 to ensure diverse and coherent outputs. 
Our image corpus is sourced from LAION-400m~\cite{LAION}, from which we exclude images that are either corrupted or have inaccessible URLs. 
Each synthetic data sample incorporates one image sampled from this corpus as the query image. 
For modality combinations that include images on the document side, we utilize the jina-clip-v2\footnote{\url{https://huggingface.co/jinaai/jina-clip-v2}} model to retrieve a similar image, along with a hard negative image, to serve as additional inputs.

\subsection{Finetuning Embedding Model}

We train \ours{} using the open-source MLLM, Llama-3.2-11B-Vision\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-11B-Vision}}. 
The training is conducted on 64 NVIDIA A100 GPUs, each equipped with 40GB of memory. 
To optimize GPU memory usage, we employ gradient checkpointing and set the gradient accumulation steps to 4.
The model is trained with a learning rate of 2e-5 for one epoch, utilizing both synthetic and labeled data. 
LoRA~\cite{lora} is applied to the MLLM with a rank of 8. 
Each training sample incorporates one hard negative document. 
Hard negatives are mined for each subset of MMEB using VLM2Vec-LoRA\footnote{\url{https://huggingface.co/TIGER-Lab/VLM2Vec-LoRA}}, with the 70th position in the ranking list selected as the hard negative sample.

More implementation details can be found in \url{https://github.com/haon-chen/mmE5}.

\section{Prompts}
\label{appendix: prompt}


We use different prompts of data synthesis for different tasks.
For retrieval task, we design two prompts for modality combinations that involve images on the document side or not.
Let us take the prompt of generating classification data for an example to illustrate the prompt design.

First, we sample a modality combination from \{image-to-text, (image,text)-to-text\}.
If the query side does not include texts, the ``input\_text'' of the classification data sample will be an empty string.
Similarly, for modalities of retrieval task that do not include document texts, the ``positive\_document'' and ``hard\_negative\_document'' will be empty.
Following previous works of synthesizing text embedding data~\cite{E5mistral, speed}, we will randomly select a clarity and difficulty setting to enhance diversity.

Then, for the multi-aspect visual description process, we ask the MLLM to explicitly \textcolor{red}{include four perspectives of description.}
Besides, for the data synthesis process, we also ask the MLLM to \textcolor{red}{follow some specific guidelines.}  
Furthermore, the MLLM will \textcolor{red}{evaluate the initially generated data from several aspects} and provide ``possible\_improvements''.
Finally, the revised version of data will be used as the output data sample.
Note that there are no task instructions generated for the VQA task, since they are all fixed as ``Represent the given image with the following question:''.


\begin{figure*}[p]
\centering
\begin{prompt}[title={Prompt: Synthesizing Classification Data}, label=prompt:cla]

Your mission is to first produce a detailed visual description of the image (within 300 words), identifying all potential aspects for generating high-quality data for a \textcolor{blue}{\{image-to-text, (image,text)-to-text\}} classification task. \\
    
Based on the description, brainstorm a potentially useful task. \\

Here are a few examples for your reference:
\textcolor{blue}{\{example tasks\}} \\

Then, you should write one multi-modal classification example for this task in JSON format. The JSON object must contain the following keys: \\

- "description": a string, your detailed visual description, listing all required elements. \\
- "task\_instruction": a string, describing the classification task. \\
- "input\_text": \textcolor{blue}{\{"an empty string", "a string the input text specified by the classification task"\}}. \\
- "label": a string, the correct label of the image and input\_text (if not empty) based on the task instruction. \\
- "misleading\_label": a string, an incorrect label that is related to the task. \\
- "evaluation": a string, a brief summary of the evaluation of data quality. \\
- "possible\_improvements": a string, suggestions for improving the data based on the guidelines. \\
- "revised\_task\_instruction": the revised task instruction. \\
- "revised\_input\_text": the revised input text, \textcolor{blue}{\{"an empty string", "a string the input text specified by the classification task"\}}. \\
- "revised\_label": the revised label. \\
- "revised\_misleading\_label": the revised misleading label. \\

\textcolor{red}{For the description, please include the following elements:}\\
- General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features.\\
- Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another.\\
- Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place.\\
- Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions).\\

\textcolor{red}{Please adhere to the following guidelines:}\\
- Task should be suitable for the given image.\\
- Avoid generate task similar to classification of sentiment / subject / study field / genre / main topic / spam / urgency / language.\\
- The "input\_text" should be \textcolor{blue}{\{"less than 10", "at least 10", "at least 50", "at least 100", "at least 200"\}} words and diverse in expression (if not empty).\\
- The "misleading\_label" must be a valid label for the given task, but not as appropriate as the "label" for the image.\\
- The text of "task\_instruction" should be in English and others fields should be in \textcolor{blue}{\{language\}}.\\
- Avoid including the values of the "label" and "misleading\_label" fields in the "input\_text" (if not empty), that would make the task too easy.\\
- The "input\_text" (if not empty) is \textcolor{blue}{\{"clear", "understandable with some effort", "ambiguous"\}} and requires \textcolor{blue}{\{"high school", "college", "PhD"\}} level education to comprehend.\\
- \textcolor{red}{When generating the data, please evaluate the following aspects:}\\
  1. Relevance: Are the generated input texts and labels (if not empty) tightly connected to their corresponding image and task objectives? Does the task instruction effectively link the query image with the positive label?\\
  2. Plausibility: Are misleading labels sufficiently relevant to the image or labels while remaining definitively incorrect? Could they mislead the model?\\
  3. Clarity: Is the generated task clear and unambiguous, providing sufficient instruction to connect the query image with the label, without being overly specific or abstract?\\
  4. Diversity: Does the generated data introduce variation in task instructions, texts (if not empty), and labels to avoid repetitive patterns in the dataset?\\
- Provide a detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths.\\
- Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives.\\
- Avoid revisions that overly simplify the task instruction, text (if not empty), or labels, as this may reduce their utility for training.\\
- Ensure that revised data maintains consistency with the corresponding image content and classification task requirements.\\

Your output must always be a JSON object only. Do not explain yourself or output anything else. Be creative!
    
\end{prompt}
\end{figure*}



\begin{figure*}[t]
\centering
\begin{prompt}[title={Prompt: Synthesizing VQA Data}, label=prompt:vqa]

Your mission is to first produce detailed visual descriptions of the image (within 300 words), identifying all potential aspects for generating high-quality data for a visual QA task. \\
    
Based on the description, write one visual QA example based on the given image in JSON format. The JSON object must contain the following keys: \\

- "description": a string, your detailed visual description, listing all required elements.\\
- "question": a string, specifying the question based on the image content.\\
- "positive\_answer": a string, the correct answer for the question based on the image content.\\
- "hard\_negative\_answer": a string, an incorrect answer that appears plausible but is ultimately wrong.\\
- "evaluation": a string, a brief summary of the evaluation of data quality.\\
- "possible\_improvements": a string, suggestions for improving the data based on the guidelines.\\
- "revised\_question": the revised question.\\
- "revised\_positive\_answer": the revised positive answer.\\
- "revised\_hard\_negative\_answer": the revised hard negative answer.\\

\textcolor{red}{For the description, please include the following elements:}\\
- General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features.\\
- Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another.\\
- Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place.\\
- Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions).\\

\textcolor{red}{Please adhere to the following guidelines:}\\
- The "question" should be \textcolor{blue}{\{"less than 10", "at least 10", "at least 50", "at least 100", "at least 200"\}} words and diverse in expression.\\
- The "hard\_negative\_answer" must be plausible but less appropriate than the "positive\_answer".\\
- The values for all fields should be in \textcolor{blue}{\{language\}}.\\
- Avoid including explicit hints in the question that make the answer too obvious.\\
- The "question" (if not empty) is \textcolor{blue}{\{"clear", "understandable with some effort", "ambiguous"\}} and requires \textcolor{blue}{\{"high school", "college", "PhD"\}} level education to comprehend.\\
- \textcolor{red}{When generating the data, please evaluate the following aspects:}\\
  1. Relevance: Are the generated question and answers tightly linked to the image content and consistent with the task requirements?\\
  2. Plausibility: Does the "hard\_negative\_answer" closely resemble the "positive\_answer" while remaining definitively incorrect? Could it mislead the model?\\
  3. Diversity: Does the generated data introduce variation in questions, and answers to avoid repetitive patterns in the dataset?\\
- Provide a detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths.\\
- Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives.\\
- Avoid revisions that overly simplify or trivialize the "question".\\
- Ensure revised data maintain consistency with the image content and task-specific requirements.\\

Your output must always be a JSON object only. Do not explain yourself or output anything else. Be creative!
    
\end{prompt}
\end{figure*}


\begin{figure*}[t]
\centering
\begin{prompt}[title={Prompt: Synthesizing Retrieval Data (Only Query Image)}, label=prompt:ret_one_image]

Your mission is to first produce a detailed visual description of the image (within 300 words), identifying all potential aspects for generating high-quality data for a \textcolor{blue}{\{image-to-text, (image,text)-to-text\}} retrieval task.\\
    
Based on the description, brainstorm a potentially useful task. \\

Here are a few examples for your reference:
\textcolor{blue}{\{example tasks\}} \\

Then, you should write one retrieval example for this task in JSON format. The JSON object must contain the following keys: \\

- "description": a string, your detailed visual description, listing all required elements. \\
- "task\_instruction": a string, describing the retrieval task. \\
- "query": \textcolor{blue}{\{"an empty string", "a random user search query specified by the retrieval task and the query image."\}} \\
- "positive\_document": a string, the relevant document for the query image content. \\
- "hard\_negative\_document": a string, a hard negative document that only appears relevant to the query image content. \\
- "evaluation": a string, a brief summary of the evaluation of data quality. \\
- "possible\_improvements": a string, suggestions for improving the data based on the guidelines. \\
- "revised\_task\_instruction": the revised task instruction. \\
- "revised\_query": the revised query, \textcolor{blue}{\{"an empty string", "a random user search query specified by the retrieval task and the query image."\}}. \\
- "revised\_positive\_document": the revised positive document, a string, the relevant document for the query image content. \\
- "revised\_hard\_negative\_document": the revised hard negative document, a string, a hard negative document that only appears relevant to the query image content. \\

\textcolor{red}{For the description, please include the following elements:}\\
- General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features.\\
- Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another.\\
- Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place.\\
- Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions).\\

\textcolor{red}{Please adhere to the following guidelines:}

- The task should involve both query and documents (positive and hard negative, if not empty). It must directly indicate the relation without being overly detailed or abstract.\\
- The query (if not empty) should be \textcolor{blue}{\{"extremely long-tail", "long-tail", "common"\}}, \textcolor{blue}{\{"less than 5 words", "5 to 15 words", "at least 10 words"\}}, \textcolor{blue}{\{"clear", "understandable with some effort", "ambiguous"\}}, and diverse in topic.\\
- All documents (if not empty) must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of the "positive\_document" are not topically related to the query.\\
- All documents (if not empty) should be at least \textcolor{blue}{\{"10", "30", "200", "300"\}} words long.\\
- The "hard\_negative\_document" (if not empty) contains some useful information, but it should be less useful or comprehensive compared to the "positive\_document".\\
- The text of "task\_instruction" should be in English and others fields should be in \textcolor{blue}{\{language\}}.\\
- Do not provide any explanation in any document (if not empty) on why it is relevant or not relevant to the query.\\
- Do not use the word "query" or "document" in the generated content.\\
- Both the query and documents (if not empty) require \textcolor{blue}{\{"high school", "college", "PhD"\}} level education to understand.\\
- \textcolor{red}{When generating the data, please evaluate the following aspects:}\\
  1. Relevance: Are the generated query and documents (if not empty) tightly connected to their corresponding image and task objectives? Does the task instruction effectively link the query image with the positive text?\\
  2. Plausibility: Are hard negatives sufficiently similar to the query or positive examples while remaining definitively incorrect? Could they mislead the model?\\
  3. Clarity: Is the generated task clear and unambiguous, providing sufficient instruction to connect the query image with the positive document, without being overly specific or abstract?\\
  4. Diversity: Does the generated data introduce variation in task instructions, queries, and documents to avoid repetitive patterns in the dataset?\\
- Provide a detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths.\\
- Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives.\\
- Avoid revisions that overly simplify the task instruction, query, or documents, as this may reduce their utility for training.\\
- Ensure that revised data maintains consistency with the corresponding image content and retrieval task requirements.\\

Your output must always be a JSON object only. Do not explain yourself or output anything else. Be creative!
    
\end{prompt}
\end{figure*}


\begin{figure*}[t]
\centering
\begin{prompt}[title={Prompt: Synthesizing Retrieval Data (With Document Images)}, label=prompt:ret_three_image]
Your mission is to first produce detailed visual descriptions of the images (within 600 words), identifying all potential aspects for generating high-quality data for a \textcolor{blue}{\{(image,text)-to-image, image-to-image, (image,text)-to-(image,text), text-to-image, text-to-(image,text)\}} retrieval task that involves both query and document images.\\
    
Based on the description, brainstorm a potentially useful task. \\

Here are a few examples for your reference:
\textcolor{blue}{\{example tasks\}} \\

Then, you should write one retrieval example for this task in JSON format. The JSON object must contain the following keys: \\

- "description": a string, your detailed visual description, listing all required elements. \\
- "task\_instruction": a string, describing the retrieval task. \\
- "query": \textcolor{blue}{\{"an empty string", "a random user search query specified by the retrieval task and the query image."\}} \\
- "positive\_document": \textcolor{blue}{\{"an empty string", "a string, the relevant document for the query based on the query text and image content"\}} \\
- "hard\_negative\_document": \textcolor{blue}{\{"an empty string", "a string, a hard negative document that only appears relevant to the query and the query image content."\}} \\
- "evaluation": a string, a brief summary of the evaluation of data quality. \\
- "possible\_improvements": a string, suggestions for improving the data based on the guidelines. \\
- "revised\_task\_instruction": the revised task instruction. \\
- "revised\_query": the revised query, \textcolor{blue}{\{"an empty string", "a random user search query specified by the retrieval task and the query image."\}}. \\
- "revised\_positive\_document": the revised positive document, a string, \textcolor{blue}{\{"an empty string", "a string, the relevant document for the query based on the query text and image content"\}} \\
- "revised\_hard\_negative\_document": the revised hard negative document, \textcolor{blue}{\{"an empty string", "a string, a hard negative document that only appears relevant to the query and the query image content."\}} \\

\textcolor{red}{For the description, please include the following elements:}\\
- General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features.\\
- Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another.\\
- Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place.\\
- Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions).\\

\textcolor{red}{Please adhere to the following guidelines:}

- The task must connect the query image and positive image through their content. It must directly indicate the relation without being overly detailed or abstract.\\
- The query (if not empty) should be \textcolor{blue}{\{"extremely long-tail", "long-tail", "common"\}}, \textcolor{blue}{\{"less than 5 words", "5 to 15 words", "at least 10 words"\}}, \textcolor{blue}{\{"clear", "understandable with some effort", "ambiguous"\}}, and diverse in topic.\\
- The query (if not empty) should effectively associate the query image with the positive image. \\
- All documents (if not empty) must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of the "positive\_document" are not topically related to the query.\\
- All documents (if not empty) should be at least \textcolor{blue}{\{"10", "30", "200", "300"\}} words long.\\
- The "hard\_negative\_document" (if not empty) contains some useful information, but it should be less useful or comprehensive compared to the "positive\_document".\\
- The text of "task\_instruction" should be in English and others fields should be in \textcolor{blue}{\{language\}}.\\
- Do not provide any explanation in any document (if not empty) on why it is relevant or not relevant to the query.\\
- Do not use the word "query" or "document" in the generated content.\\
- Both the query and documents (if not empty) require \textcolor{blue}{\{"high school", "college", "PhD"\}} level education to understand.\\
- \textcolor{red}{When generating the data, please evaluate the following aspects:}\\
  1. Relevance: Are the generated query and documents (if present) tightly linked to their corresponding images? Does the task instruction effectively connect the query image to the positive image?\\
  2. Plausibility: Are the negative examples, including hard negatives, realistic and similar enough to the positive image to challenge the model, while still being definitively incorrect?\\
  3. Clarity: Is the generated task clear and unambiguous, providing sufficient instruction to connect the query image with the positive image, without being overly specific or abstract?\\
  4. Diversity: Does the generated data introduce variation in task instructions, queries, and documents to avoid repetitive patterns in the dataset?\\
- Provide a detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths.\\
- Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives.\\
- Avoid revisions that overly simplify the task or create unrealistic connections between the query and positive image.\\
- Ensure that revised data maintains consistency with the corresponding image content and retrieval task requirements.\\

Your output must always be a JSON object only. Do not explain yourself or output anything else. Be creative!
    
\end{prompt}
\end{figure*}



\section{Data Examples}
\label{appendix: examples}

In this section, we present the examples of the synthetic multimodal embedding data for Retrieval (Figure~\ref{fig:retrieval_example} and Figure~\ref{fig:retrieval_eval_example}), Classification (Figure~\ref{fig:classification_example}), and VQA (Figure~\ref{fig:vqa_example}) tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[p]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/data_examples/retrieval_example.pdf}%\vspace{-1ex}
	\caption{An example of the synthetic Retrieval IT2IT data (part 1). This part includes the input images, the multi-aspect descriptions, and the initially generated data.}
	\vspace{-2ex}
	\label{fig:retrieval_example}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[p]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/data_examples/retrieval_eval_example.pdf}%\vspace{-1ex}
	\caption{An example of the synthetic Retrieval IT2IT data (part 2). This part includes the evaluation, possible improvements, and the revised data.}
	\vspace{-2ex}
	\label{fig:retrieval_eval_example}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[p]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/data_examples/classification_example.pdf}%\vspace{-1ex}
	\caption{An example of the synthetic Classification IT2T data.}
	\vspace{-2ex}
	\label{fig:classification_example}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[p]
	\centering 
	\includegraphics[width=0.95\textwidth]{figures/data_examples/vqa_example.pdf}%\vspace{-1ex}
	\caption{An example of the synthetic VQA IT2T data.}
	\vspace{-2ex}
	\label{fig:vqa_example}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\afterpage{\clearpage}

\section{Detailed Results}

In this section, we present the detailed comparisons of \ours{} to baseline models on both zero-shot and supervised finetuning settings on the MMEB benchmark~\cite{MMEB}.
Due to space limitation, we omit the detailed results of partially supervised finetuning models.

\begin{table*}[ht]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Task}} & \multicolumn{8}{c}{\textit{Zero-shot Setting Models}} & \multicolumn{3}{c}{\textit{Supervised Finetuning Models}}\\ \cmidrule(lr){2-9} \cmidrule(lr){10-12} 
 & \textbf{CLIP} & \textbf{OpenCLIP} & \textbf{SigLIP} & \textbf{BLIP2} & \textbf{MagicLens} & \textbf{E5-V}  & \textbf{MMRet} & \textbf{\ours{}} & \textbf{VLM2Vec} & \textbf{MMRet} & \textbf{\ours{}}\\ 
\midrule
\multicolumn{12}{l}{\textbf{Classification (10 tasks)}} \\
\midrule
ImageNet-1K & 55.8 & 63.5 & 45.4 & 10.3 & 48.0 & 9.6 & 49.1 & 68.8 & 74.5 & 58.8 & 77.8\\
N24News & 34.7 & 38.6 & 13.9 & 36.0 & 33.7 & 23.4 & 45.8 & 54.5 &  80.3 & 71.3 & 81.7\\
HatefulMemes & 51.1 & 51.7 & 47.2 & 49.6 & 49.0 & 49.7 & 51.0 & 55.0 & 67.9 & 53.7 & 64.2\\
VOC2007 & 50.7 & 52.4 & 64.3 & 52.1 & 51.6 & 49.9 & 74.6 & 73.9 & 91.5 & 85.0 & 91.0\\
SUN397 & 43.4 & 68.8 & 39.6 & 34.5 & 57.0 & 33.1 & 60.1 & 72.7 & 75.8 & 70.0 & 77.7\\
\rowcolor[HTML]{FFEB99} Place365 & 28.5 & 37.8 & 20.0 & 21.5 & 31.5 & 8.6 & 35.3 & 39.7 & 44.0 & 43.0 & 43.0\\
\rowcolor[HTML]{FFEB99} ImageNet-A & 25.5 &14.2 &42.6 &3.2 &8.0 &2.0 & 31.6 &46.1 &43.6 & 36.1 & 56.3\\
\rowcolor[HTML]{FFEB99} ImageNet-R & 75.6 & 83.0 & 75.0 & 39.7 & 70.9 & 30.8 & 66.2 & 86.2 & 79.8 & 71.6 & 86.3\\
\rowcolor[HTML]{FFEB99} ObjectNet & 43.4 & 51.4 & 40.3 & 20.6 & 31.6 & 7.5 & 49.2 & 74.8 & 39.6 & 55.8 & 62.5\\
\rowcolor[HTML]{FFEB99} Country-211 & 19.2 & 16.8 & 14.2 & 2.5 & 6.2 & 3.1 & 9.3 & 35.1 & 14.7 & 14.7 & 35.4\\
\rowcolor[HTML]{E8E8E8} \textit{All Classification} & 42.8 & 47.8 & 40.3 & 27.0 & 38.8 & 21.8 & 47.2 & 60.7 & 61.2 & 56.0 & 67.6\\
\midrule
\multicolumn{12}{l}{\textbf{VQA (10 tasks)}} \\
\midrule
OK-VQA & 7.5 & 11.5 & 2.4 & 8.7 & 12.7 & 8.9 & 28.0 & 56.6 & 69.0 & 73.3 & 67.6\\
A-OKVQA & 3.8 & 3.3 & 1.5 & 3.2 & 2.9 & 5.9 & 11.6 & 50.0 & 54.4 & 56.7 & 56.1\\
DocVQA & 4.0 & 5.3 & 4.2 & 2.6 & 3.0 & 1.7 & 12.6 & 81.3 & 52.0 & 78.5 & 90.3\\
InfographicsVQA & 4.6 & 4.6 & 2.7 & 2.0 & 5.9 & 2.3 & 10.6 & 44.0 & 30.7 & 39.3 & 56.5\\
ChartQA & 1.4 & 1.5 & 3.0 & 0.5 & 0.9 & 2.4 & 2.4 & 35.2 & 34.8 & 41.7 & 50.5\\
Visual7W & 4.0 &2.6 &1.2 &1.3 &2.5 &5.8 & 9.0 &40.4 & 49.8& 49.5 & 51.9\\
\rowcolor[HTML]{FFEB99} ScienceQA & 9.4 & 10.2 & 7.9 & 6.8 & 5.2 & 3.6 & 23.3 & 47.3 & 42.1 & 45.2 & 55.8\\
\rowcolor[HTML]{FFEB99} VizWiz & 8.2 & 6.6 & 2.3 & 4.0 & 1.7 & 2.6 & 25.9 & 54.0 & 43.0 & 51.7 & 52.8\\
\rowcolor[HTML]{FFEB99} GQA & 41.3 & 52.5 & 57.5 & 9.7 & 43.5 & 7.8 & 41.3 & 65.4 & 61.2 & 59.0 & 61.7\\
\rowcolor[HTML]{FFEB99} TextVQA & 7.0 & 10.9 & 1.0 & 3.3 & 4.6 & 3.2 & 18.9 & 83.1 & 62.0 & 79.0 & 83.3\\
\rowcolor[HTML]{E8E8E8} \textit{Avg.} & 9.1 & 10.9 & 8.4 & 4.2 & 8.3 & 4.9 & 18.4 & 55.7 &  49.9 & 57.4 & 62.6\\
\midrule
\multicolumn{12}{l}{\textbf{Retrieval (12 tasks)}} \\
\midrule
VisDial & 30.7 & 25.4 & 21.5 & 18.0 & 24.8 & 9.2 & 62.6 & 39.1 &  80.9 & 83.0 & 74.1\\
CIRR & 12.6 & 15.4 & 15.1 & 9.8 & 39.1 & 6.1 & 65.7 & 41.6 &  49.9 &  61.4 & 54.7\\
VisualNews\_t2i & 78.9 & 74.0 & 51.0 & 48.1 & 50.7 & 13.5 & 45.7 & 51.2 &  75.4 & 74.2 & 77.6\\
VisualNews\_i2t & 79.6 & 78.0 & 52.4 & 13.5 & 21.1 & 8.1 & 53.4 & 64.9 & 80.0 & 78.1 & 83.3\\
MSCOCO\_t2i & 59.5 & 63.6 & 58.3 & 53.7 & 54.1 & 20.7 & 68.7 & 55.0 & 75.7 & 78.6 & 76.4\\
MSCOCO\_i2t & 57.7 &62.1 &55.0 &20.3 &40.0 &14.0 & 56.7 &59.1 &73.1 & 72.4 & 73.2\\
NIGHTS & 60.4 & 66.1 & 62.9 & 56.5 & 58.1 & 4.2 & 59.4 & 58.9& 65.5 & 68.3 & 68.3\\
WebQA & 67.5 & 62.1 & 58.1 & 55.4 & 43.0 & 17.7 & 76.3 & 82.9&87.6 & 90.2 & 88.0\\
\rowcolor[HTML]{FFEB99}FashionIQ & 11.4 & 13.8 & 20.1 & 9.3 & 11.2 & 2.8 & 31.5 & 21.6& 16.2 & 54.9 & 28.8\\
\rowcolor[HTML]{FFEB99}Wiki-SS-NQ & 55.0 &44.6 &55.1 &28.7 &18.7 &8.6 & 25.4 & 58.8 &60.2 & 24.9 & 65.8\\
\rowcolor[HTML]{FFEB99}OVEN &  41.1 &45.0 &56.0 &39.5 &1.6 &5.9 & 73.0 &67.6 &56.5 & 87.5 & 77.5\\
\rowcolor[HTML]{FFEB99}EDIS & 81.0 & 77.5 & 23.6 & 54.4 & 62.6 & 26.8 & 59.9 & 55.2 &87.8 & 65.6 & 83.7\\
\rowcolor[HTML]{E8E8E8} \textit{Avg.} & 53.0 & 52.3 & 31.6 & 33.9 & 35.4 & 11.5 & 56.5 & 54.7 &67.4 & 69.9 & 71.0\\
\midrule
\multicolumn{12}{l}{\textbf{Visual Grounding (4 tasks)}} \\
\midrule
MSCOCO & 33.8 & 34.5 & 46.4 & 28.9 & 22.1 & 10.8 & 42.7 & 59.0 &80.6 & 76.8 & 53.7\\
\rowcolor[HTML]{FFEB99}RefCOCO & 56.9 & 54.2 & 70.8 & 47.4 & 22.8 & 11.9 & 69.3 & 78.9 &88.7& 89.8 & 92.7\\
\rowcolor[HTML]{FFEB99}RefCOCO-matching & 61.3 &68.3 &50.8 &59.5 &35.6 &38.9 & 63.2 &80.8 &84.0 & 90.6 & 88.8\\
\rowcolor[HTML]{FFEB99}Visual7W-pointing & 55.1 & 56.3 & 70.1 & 52.0 & 23.4 & 14.3 & 73.5 & 71.2 &90.9 & 77.0 & 92.3\\
\rowcolor[HTML]{E8E8E8}\textit{Avg.} & 51.8 & 53.3 & 59.5 & 47.0 & 26.0 & 19.0 & 62.2 &72.5 &86.1 & 83.6 & 89.6\\
\midrule
\multicolumn{12}{l}{\textbf{Final Score (36 tasks)}} \\
\midrule
\text{All IND Avg.} & 37.1 & 39.3 & 32.3 & 25.3 & 31.0 & 14.9 & 43.5 & 57.2 &  67.5 & 59.1 & 72.3\\
\rowcolor[HTML]{FFEB99}\text{All OOD Avg.} & 38.7 & 40.2 & 38.0 & 25.1 & 23.7 & 11.5 & 44.3 & 60.4 &  57.1 & 68.0 & 66.7\\
\rowcolor[HTML]{E8E8E8}\text{All Avg.} & 37.8 & 39.7 & 34.8 & 25.2 & 27.8 & 13.3 & 44.0 & 58.6 & 62.9 & 64.1 & 69.8\\
\bottomrule
\end{tabular}
}
% \caption{The detailed results on the MMEB benchmark ~\cite{vlm2vec2024}, which includes 20 in-distribution datasets and 16 out-of-distribution datasets. The out-of-distribution datasets are high lighted with a gray background in the table. We report the performance of our MMRet under both zero-shot and fine-tuning settings.}
\caption{Detailed results of zero-shot setting and supervised setting models on each dataset of MMEB~\cite{MMEB}.}
\end{table*}