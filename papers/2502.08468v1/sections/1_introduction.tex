\section{Introduction}

Multimodal embedding models encode multimedia inputs, such as images and text, into latent vector representations.
They have demonstrated effectiveness across diverse downstream tasks, including classification~\cite{ImageNet-1K}, visual question answering (VQA)~\cite{TextVQA}, and cross-modal retrieval~\cite{OVEN}. 
Prior studies have focused on training multimodal embedding models using simple text-image pre-trained models such as CLIP~\cite{CLIP}. 
More recently, researchers have turned to multimodal large language models (MLLMs), including LLaVA~\cite{Llava} and Phi~\cite{Phi3}, to develop universal embedding models.

These vision-language models (VLMs) mostly rely on high-quality human-labeled datasets to achieve robust embedding capabilities. 
Such datasets suffer from data scarcity because they require high costs of multimodal annotations.
To address this, researchers have leveraged the advanced language modeling capabilities of large language models (LLMs) and MLLMs to synthesize datasets for fine-tuning multimodal embedding models~\cite{MagicLens, megapairs, GME}.
However, existing works lack a comprehensive exploration into the quality of synthetic embedding data.
Typically, most data generated by them are limited to specific modality types of English retrieval tasks, harming the generalization capabilities of the embedding models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/introduction.pdf}%\vspace{-1ex}
	\caption{An illustration of our data synthesis framework. ``X$\rightarrow$Y'' denotes a modality combination, where ``X'' represents the query side and ``Y'' denotes the target side. ``T'' denotes text and ``I'' denotes image.}
	\vspace{-2ex}
	\label{fig:introduction}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After analyzing common application scenarios of multimodal embedding models, we identify three key criteria and introduce a data synthesis framework guided by these principles:
\textbf{(1) Broad scope.}
Multimodal embedding models are commonly employed in tasks such as classification, visual question answering (VQA), and retrieval, which requires understanding various input combinations of text and images.
Additionally, multilingual contexts are increasingly popular in daily scenarios.
As shown in Figure~\ref{fig:introduction}, our framework synthesizes datasets covering three tasks, seven modality combinations, and 93 languages, ensuring that models trained on it generalize effectively across diverse scenarios.
\textbf{(2) Robust cross-modal alignment.}
In multimodal tasks, models must understand and align information across different modalities to generate meaningful representations. 
Without accurate cross-modal alignment, embeddings may fail to capture the underlying relationships, leading to poor performance in downstream tasks.
To synthesize data of robust cross-modal alignment, our framework incorporates a deep thinking process. 
Specifically, for each sampled image, we first employ an MLLM to interpret it from four perspectives before generating data: general information, object-level description, contextual background information, and task-specific brainstorming, \ie, how the image relates to the given task. 
Additionally, the entire data synthesis process is executed within a single pass of an MLLM.
By this, the MLLM can ``see'' the images at the whole time, avoiding potential information loss that might occur due to multiple I/O steps in previous works~\cite{megapairs, GME}.
\textbf{(3) High fidelity.}
The individual quality of each modality (\eg, real images, high-quality instructions, queries, and hard negatives) determines the overall usefulness of the dataset. 
To enhance fidelity, our framework uses real images sampled from an open-source corpus (LAION-400m~\cite{LAION}) as the input images.
We also apply a series of quality control measures, such as self-evaluation and refinement, ensuring that the synthetic components accurately reflect real-world distributions and maintain strong cross-modal alignment.

With the synthesized data ready, we train a \textbf{m}ultimodal \textbf{m}ultilingual \textbf{E5} model (\ours{}).
It achieves state-of-the-art performance on the 36 datasets of MMEB~\cite{MMEB}, using 45 times less training data than the previous SOTA model MMRet~\cite{megapairs} (560K compared to 26M) in a zero-shot setting. 
After incorporating labeled data, \ours{} still demonstrates the best performance.
Besides, \ours{} achieves the best results on the multilingual benchmark XTD~\cite{XTD}, demonstrating its superior multilingual capabilities.


In summary, our contributions are as follows:
\begin{itemize}
    \item Based on our analysis of common scenarios for multimodal embedding models, we identify three key criteria of high-quality synthetic data: broad scope, robust cross-modal alignment, and high fidelity.
    \item We introduce a data synthesis framework guided by the proposed principles. 
    This framework leverages an MLLM to produce high-quality synthetic datasets that cover a wide range of tasks, modality combinations, and languages. It ensures robust cross-modal alignment through a comprehensive multi-aspect interpretation process and maintains high fidelity by employing self-evaluation and refinement mechanisms.
    \item Compared to the previous leading model, \ours{} achieves SOTA performance on the MMEB benchmark while using 45Ã— less synthetic data in both zero-shot and supervised settings.
    \ours{} also demonstrates superior multilingual capabilities on the XTD benchmark.
\end{itemize}


