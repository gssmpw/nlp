\subsection{Green Claim Detection}
\label{sec:green claim}

%citations
% direct:  \cite{panchendrarajan2024claim}, \cite{vinicius_woloszyn_towards_2021, stammbach_environmental_2023, linSUSTAINABLESIGNALSijcai2023}, \citet{linSUSTAINABLESIGNALSijcai2023},  \cite{Arslan_Hassan_Li_Tremayne_2020}

Identifying greenwashing involves more than just finding text related to climate change, as a company might simply state factual information about the climate without making any commitments on it. Hence, we now focus on detecting \textit{green claims}, i.e., claims that a product, service, or corporate practice either contributes positively to environmental sustainability or is less harmful to the environment than alternatives \cite{stammbach_environmental_2023,vinicius_woloszyn_towards_2021}. The European Commission calls these claims ``environmental claims'':

\begin{table}[ht]
\centering
\begin{tabular}{p{2.5cm}p{3cm}p{8.5cm}}
\toprule
\textbf{Dataset}  & \textbf{Input}  & \textbf{Positive Label description}\\ \midrule
Green Claims \cite{vinicius_woloszyn_towards_2021}  & Marketing Tweets  & Environmental (or green) advertisements refer to all appeals that include ecological, environmental sustainability, or nature-friendly messages that target the needs and desires of environmentally concerned stakeholders. \\ \midrule
Environmental Claims \cite{stammbach_environmental_2023} & Paragraph from reports  & Environmental claims refer to the practice of suggesting or otherwise creating the impression [...] that a product or a service is environmentally friendly (i.e., it has a positive impact on the environment) or is less damaging to the environment than competing goods or services [...]
In our case, claims relate to products, services, or specific corporate environmental performance.   \\ \bottomrule
\end{tabular}
\caption{Label definitions of the datasets on green claims detection.}
\label{tab:guidelines characteristics}
\end{table}



\task  Given an input sentence or a paragraph, output a binary label, ``\textit{green claim}'' or ``\textit{not green claim}''. 


\datasets The most notable approaches aiming at identifying climate-related claims introduced annotated datasets \cite{vinicius_woloszyn_towards_2021, stammbach_environmental_2023}.
\citet{vinicius_woloszyn_towards_2021} proposed to focus on social media marketing through the detection of green claims in tweets. \citet{stammbach_environmental_2023} proposed a dataset for environmental claim detection in paragraphs from corporate reports.

\solutions Both \citet{stammbach_environmental_2023} and \citet{vinicius_woloszyn_towards_2021} evaluated fine-tuned Transformer models on the datasets. \citet{stammbach_environmental_2023} also evaluated classical approaches and experimented with a model fine-tuned on general claim detection \cite{Arslan_Hassan_Li_Tremayne_2020} applied to their domain-specific task (see Table~\ref{tab:models} in the Appendix for details). 
\citet{vinicius_woloszyn_towards_2021} and \citet{stammbach_environmental_2023} demonstrated good performances with fine-tuned Transformer (above 84\%), yet they have very different Inter-Annotator Agreement (IAA), with Krippendorff's $\alpha=0.8223$ for \citet{vinicius_woloszyn_towards_2021} and $\alpha=0.47$ for \citet{stammbach_environmental_2023}. This might be explained by the difference in context: marketing tweets might be easier to understand for humans and more self-sufficient compared to paragraphs extracted from reports.
Additionally, \citet{vinicius_woloszyn_towards_2021} experimented with adversarial attack, showing the sensibility to character-swap and word-swap. This shows that the models tend to rely heavily on particular words, showing only a superficial understanding. While it is difficult to generalize that conclusion given the small size of the dataset, it still indicates that BERT-like architecture can over-fit or rely on superficial cues instead of building an accurate representation of the paragraph. 
It is essential to assess model performance in challenging scenarios, where the presence of nonsensical or noisy inputs can reveal the fragility of model comprehension.

\paragraph{Insights}  
Green claims have been analyzed in company reporting \citep{stammbach_environmental_2023} and social media communication \citep{vinicius_woloszyn_towards_2021}. Fine-tuned models can solve the task rather well, even if challenges remain:
\begin{itemize}
\item \textit{Inter annotator agreement:} The annotator agreement remains low for green claim detection in reports~\cite{stammbach_environmental_2023}.
    \item \textit{Integration with existing claim detection literature:} While claim detection is a well-established field with extensive literature, studies on environmental claims do not fully connect with existing research on claim detection. In particular, they do not distinguish between Claims, Verifiable Claims (claims that can be checked), and Check-worthy Claims (claims that are interesting to verify)~\cite{panchendrarajan2024claim}.
    \item \textit{Evaluation Sensitivity and Real-World Robustness:} The literature shows that fine-tuned models are sensitive to adversarial attacks, meaning small perturbations of the text influence greatly the performance of the classifier. As for all tasks, it is also important evaluate models robustness in real-world settings. Poor data quality might induce perturbations in the texts, reducing the performance of models.
\end{itemize}

\subsection{Green Claim Characteristics}
\label{sec: claim characteristics}

Once we have established that a sentence is climate-related (Section~\ref{sec:climate-related topic}) and that it is a claim about the company (Section~\ref{sec:green claim}), we can endeavor to further classify the claim into fine-grained categories. Table~\ref{tab:guidelines characteristics} shows various characteristics of claims that have been studied.

\begin{table}[ht]
\centering
\begin{tabular}{p{2.5cm}p{3.5cm}p{8cm}}
\toprule
\textbf{Dataset}  & \textbf{Input}  & \textbf{Labels}\\ \midrule
Implicit/Explicit Green Claims \cite{vinicius_woloszyn_towards_2021}  & Marketing Tweets  & \textit{Implicit green claims} raise the same ecological and environmental concerns as \textit{explicit green claims} (see definition in Section \ref{sec:green claim}), but without showing any commitment from the company. If the tweet does not contain a green claim then \textit{No Claim}. \\ \midrule
Specificity \cite{bingler2023cheaptalkspecificitysentiment}  & Paragraph from reports  & A paragraph is \textit{Specific} if it includes clear, tangible, and firm-specific details about events, goals, actions, or explanations that directly impact or clarify the firm's operations, strategy, or objectives. \textit{Non-specific} otherwise. \\ \midrule
Commitments \& Actions\cite{bingler2023cheaptalkspecificitysentiment} &  Paragraph from reports  & A paragraph is a commitment or an action if it contains targets for the future or actions already taken in the past. \\ \midrule
Net Zero/Reduction \cite{tobias_schimanski_climatebert-netzero_2023} & Paragraph from Net Zero Tracker~\cite{netzerotracker2023} & The paragraph 
 contains either a \textit{Net-Zero} target, a \textit{Reduction} target or no target (\textit{None})  \\ \bottomrule
\end{tabular}
\caption{Label definitions the datasets related to characterization of green claims.}
\label{tab:guidelines characteristics}
\end{table}

\task Given an input sentence or a paragraph labeled as a green claim, output a more fine-grained characterization of the claim. This is a multi-label classification task; the labels can be about the form (e.g. specificity) or the substance (e.g. action, targets, facts). 

\datasets Existing works characterized both the content and the form of claims. On the content dimension, there are datasets for identifying if the claim is about an action or a commitment~\cite{bingler2023cheaptalkspecificitysentiment, tobias_schimanski_climatebert-netzero_2023}. \citet{bingler2023cheaptalkspecificitysentiment} introduce a dataset for the identification of commitments and actions (Yes/No), and \citet{tobias_schimanski_climatebert-netzero_2023} released one for the identification of reduction targets (net zero, reduction, general). On the form dimension, \citet{vinicius_woloszyn_towards_2021} characterized claims as implicit or explicit, and \citet{bingler2023cheaptalkspecificitysentiment} annotated their dataset on the specificity of claims (Specific/Not specific). \citet{bingler2023cheaptalkspecificitysentiment} ultimately used the specificity and commitment/action characteristics to identify cheap talks related to climate disclosure.  

\solutions \citet{vinicius_woloszyn_towards_2021, tobias_schimanski_climatebert-netzero_2023} and \citet{bingler2023cheaptalkspecificitysentiment} experimented with fine-tuned Transformers. \citet{bingler2023cheaptalkspecificitysentiment} also evaluated classical baselines (SVM, NB), and \citet{tobias_schimanski_climatebert-netzero_2023} experimented with GPT-3.5-turbo (see Table \ref{tab:models} in the Appendix for details). All approaches reached good performances, in particular fine-tuned Transformers reaching performances above 80\%. The exception remains the Specificity classification with a F1-score of 77\% (close to the baselines). This low performance  might be intrinsic to the task. Indeed, humans have disagreement when distinguishing specific and unspecific claims: 
\citet{bingler2023cheaptalkspecificitysentiment} measured a low IAA on Specificity (Krippendorff's $\alpha=0.1703$). This might indicate that the task is not well defined in the first place. 

\paragraph{Insights} 
The high performances of fine-tuned models indicate that this type of task is solved. Furthermore, the demonstrated ability of GPT-3.5 as a zero-shot classifier \cite{tobias_schimanski_climatebert-netzero_2023} highlights the potential of large language models to classify these characteristics without requiring extensive annotated datasets. 
These characteristics are particularly useful for identifying statements that may indicate greenwashing. For example, \citet{bingler2023cheaptalkspecificitysentiment} utilized the proportion of non-specific commitments as a \textit{Cheap Talk Index} demonstrating their practical applications.
However, characteristics such as specificity are inherently ambiguous and subjective, as evidenced by the low inter-annotator agreement (IAA). Therefore, future research could focus on disambiguating what constitutes a specific statement from a non-specific one more objectively.  

\subsection{Green Stance Detection}
\label{sec:stance detection}

Beyond making claims about the environmental impact of their products and processes, companies contribute to environmental discussions and can have an impact on regulatory frameworks through their communications and industry presence. It is thus helpful to understand the stance of an organization on the existence and gravity of climate change, on climate mitigation and adaptation efforts, and on climate-related regulations. 

\begin{table}[ht]
\centering
\begin{tabular}{p{3cm}p{4cm}p{3cm}p{4cm}}
\toprule
\textbf{Dataset} & \textbf{Input} & \textbf{Labels}                                                                                  & \textbf{Label details}                                         \\
\midrule
ClimateFEVER (evidence) ~\cite{diggelmann_climate-fever_2020} 
& A claim and an evidence sentence from Wikipedia  & \textit{Support, Refutes, Not Enough Information} & Determines the relation between a claim and a single evidence sentence \\
\midrule
LobbyMap (Stance)~\cite{morio2023an} & Page from a company communications (report, press release, ...) & \textit{Strongly supporting, Supporting, No or mixed position, Not supporting, Opposing} & Given the policy and the page, classifies the stance \\
\midrule
Global Warming Stance Detection (GWSD)~\cite{luo_detecting_2020}  & Sentences from news about global warming & \multicolumn{2}{p{7cm}}{Stance of the evidence (\textit{Agree, Disagree, Neutral}) toward the claim: Climate-Change is a serious concern.} \\
\midrule
ClimateStance~\cite{vaid-etal-2022-towards} & Tweets posted during COP25 filtered by keywords (relevant to climate-change) & \multicolumn{2}{p{7cm}}{Stance towards climate change prevention: \textit{Favor, Against, Ambiguous}. (Stance used as a broad notion including sentiment, evaluation, appraisal, ...)}  \\
\midrule
Stance on Remediation Effort~\cite{lai_using_2023} & Texts extracted from the TCFD sections of the financial reports  & \multicolumn{2}{p{7cm}}{The text indicates \textit{support} for climate change remediation efforts or \textit{refutation} of such efforts. (No guidelines)} \\
\bottomrule
\multicolumn{4}{c}{\textbf{Related subtask}} \\
\toprule
ClimateFEVER (claim) ~\cite{diggelmann_climate-fever_2020}  & A claim and multiple evidence sentences from Wikipedia & \textit{Support, Refutes, Debated, Not Enough Information} & Determines if a claim is supported by a set of retrieved evidence sentences  \\
\midrule
LobbyMap (Page)~\cite{morio2023an} & \multirow{2}{4cm}[-0.25cm]{Page from a company communications (report, press release, ...)} & \textit{1/0} & Contains a stance on a remediation policy  \\
LobbyMap (Query)~\cite{morio2023an} & & \textit{GHG emission regulation, Renewable energy, Carbon tax, ...} & Classifies the remediation policy \\
\bottomrule
\end{tabular}
\caption{Summary of datasets, labels, scopes, and sources for climate-related stance detection.}
\label{tab:datasets_stance}
\end{table}

\task Given two input sentences or paragraphs, one labeled as the claim and one as the evidence, predict the stance between the two: supports, refutes or neutral. Some studies fix the claim and only vary the evidence (e.g. the claim is always \textit{Climate change poses a severe threat}), training a model to predict the stance of the evidence in respect to the fixed claim. 
Other studies train a model to predict the relation between varying claims/evidences.

\paragraph{Related subtask} The first subtask is collecting the evidences when the claims are already available (e.g. if they were collected manually). In order to build these datasets, research can rely on simple heuristics such as downloading all tweets published during the COP25 filtered with keywords \cite{vaid-etal-2022-towards}. However, other researchers performed a more elaborate procedure. \citet{diggelmann_climate-fever_2020} proposed a pipeline for collecting evidence. To retrieve relevant evidence from Wikipedia for a given claim, the pipeline involves three steps: document-level retrieval using entity-linking and BM25 to identify top articles, sentence-level retrieval using sentence embeddings trained on the FEVER dataset to extract relevant sentences, and sentence re-ranking using a pretrained ALBERT model to classify and rank evidence based on relevance. 
The second one encompasses the broader process: identifying claims and finding evidence, before predicting the relation. \citet{Wang2021EvidenceBA} used a generic claim detection model trained on ClaimBuster \cite{Arslan_Hassan_Li_Tremayne_2020} for the claims and Google Search API to collect evidence. 
Finally, the last subtask is to train multiple models on each step: identifying evidences, identifying the claim and classifying the stance. This is the approach used by \cite{morio2023an}.

\datasets As described previously, researchers built datasets of evidence related to one claim while focusing stance in tweets on seriousness of climate change such as Global Warming Stance Detection (GWSD) \cite{luo_detecting_2020}, ClimateStance \cite{vaid-etal-2022-towards} stance on climate change of tweets posted during the COP25, or \citet{lai_using_2023}'s dataset which focuses on the stance toward climate change remediation efforts.
Another research initiative built a dataset of claim-evidence pairs, ClimateFEVER, and then trained a model on the stance classification \cite{diggelmann_climate-fever_2020, Wang2021EvidenceBA}.
Finally,~\citet{morio2023an} proposed a dataset to assess corporate policy engagement built upon LobbyMap, which tackles the 3 steps: finding pages with evidence, identifying the claims targeted by that evidence, and classifying the stance.

\solutions The solution proposed for to tackle stance classification are fine-tuned Transformer models \cite{vaid-etal-2022-towards, vaghefi2022deep, xiang_dare_2023, morio2023an, nicolas_webersinke_climatebert_2021, Wang2021EvidenceBA, spokoyny2023answering, lai_using_2023, luo_detecting_2020} and classical approaches \cite{spokoyny2023answering, morio2023an, luo_detecting_2020}. \citet{vaid-etal-2022-towards} also experimented with FastText and \citet{xiang_dare_2023} with LSTM-based models (See Table \ref{tab:models} in the Appendix for details). The performances are quite heterogeneous, \citet{lai_using_2023} reaching F1-scores around 90\% on classification of stance on remediation efforts, \citet{luo_detecting_2020} around 72\% on GWSD, while performance on ClimateStance could not exceed 60\%. Performances on ClimateFEVER are also quite low, however, when selecting only non-ambiguous examples, \citet{xiang_dare_2023} could reach performances around 80\%. Finally, performances on LobbyMap~\cite{morio2023an} are quite low (between 31\% and 57.3\% depending on the strictness of the metric). Overall, the performances show that the datasets are challenging (see Table~\ref{tab:reported perf stance} in the Appendix for details).

\paragraph{Exhaustivity} %\fms{misplaced here. } 
Datasets such as ClimateFEVER~\cite{diggelmann_climate-fever_2020} and LobbyMap~\cite{morio2023an} rely on automated construction methods that may not ensure exhaustivity of evidence coverage. ClimateFEVER utilizes BM25 and Wikipedia for evidence retrieval, which could result in missing relevant information, particularly when compared to more advanced retrieval methods. LobbyMap relies on the \url{LobbyMap.org} website, which was not designed to be exhaustive. These limitations should be further investigated.

\paragraph{Insights} If an organization presents itself as environmentally friendly while simultaneously promoting climate-skeptic narratives or opposing climate remediation efforts, it probably is creating a misleading portrayal of its environmental stance. On the contrary, if they are aligned, it supports the authenticity of the organization's efforts.
Fortunately, stance detection has been extensively studied, yielding promising results; however, current performance levels leave room for improvement. Future research should focus on enhancing both methods and datasets to address these gaps.
While existing datasets provide strong foundations, they may suffer from a lack of exhaustivity, particularly in evidence retrieval and coverage, which requires further investigation. %Addressing these limitations will be critical for advancing the reliability of fact-checking systems in combating greenwashing.

\subsection{Question Answering}
\label{sec:qa}

Question answering (QA) is a known task:

\task Given an input question and a set of resources (paragraphs or documents), produce an answer to the question. 

\begin{table}[ht]
\centering
\begin{tabular}{p{2cm}p{5cm}p{3cm}p{4cm}}
\toprule
\textbf{Dataset} & \textbf{Input} & \textbf{Labels}                                                                                  &                                          \\
\midrule
ClimaQA ~\cite{spokoyny2023answering} 
& The text from a \textit{response} to one of the CDP questions; and one of the \textit{questions} from the CDP questionnaire  & \multicolumn{2}{p{7cm}}{\textit{1}: the response answers this question

\textit{0}: The response does not answer this question, but another one} \\
\midrule
ClimateQA~\cite{luccioni_analyzing_2020} & \textit{Sentence} from reports and a \textit{question} based on the TCFD recommendations & \multicolumn{2}{p{7cm}}{\textit{1}: the sentence answers the question

\textit{0}: The sentences does not answer the question} \\
\midrule
ClimaINS ~\cite{spokoyny2023answering} 
& The text from a \textit{response} to one of the questions from the NAIC questionnaire & \textit{MANAGE}, \textit{RISK PLAN}, \textit{MITIGATE}, \textit{ENGAGE}, \textit{ASSESS}, \textit{RISKS} & The labels correspond the 8 questions asked in the NAIC questionnaires \\
\bottomrule
\end{tabular}
\caption{Summary of datasets, labels, scopes, and sources for climate-related QA datasets.}
\label{tab:qa input}
\end{table}

QA can be used for climate-specific applications such as structuring the information related to climate change from documents~\cite{luccioni_analyzing_2020, tobias_schimanski_climatebert-netzero_2023}, building chatbots with climate-related knowledge to make it more accessible to non-experts~\cite{s_vaghefi_chatclimate_2023, cliamtebot_2022}, or helping identify potentially misleading information~\cite{jingwei_ni_paradigm_2023}. 

\paragraph{Solution} The QA process can be divided into two main steps~\cite{krausEnhancingLargeLanguage2023}: the retrieval step and the answer generation step. The retrieval step involves locating the relevant information or answer to the user's query in external documents. The answer generation step involves formulating a response to the user's query from (1) the information retrieved from documents in the first step or (2) the pre-trained internal knowledge of a model. 

\paragraph{Retriever} Retriever systems can be generic, using techniques such as sentence similarity, BM25, or document tags to filter documents and retrieve specific passages. \citet{schimanski-etal-2024-climretrieve} published ClimRetrieve, a benchmark for climate-related information retrieval in corporate reports. They found that simple embedding approaches are limited.
Therefore, several works have specifically focused on improving the identification of answers within climate-related documents~\cite{luccioni_analyzing_2020, spokoyny2023answering}. \citet{luccioni_analyzing_2020} reformulated TCFD recommendations as questions and annotated reports to identify answers to the questions. The dataset includes questions paired with potential answers, each labeled to indicate whether the answer addresses the question or not. Based on the QA model trained on this dataset, they also proposed a tool called ClimateQA. \citet{spokoyny2023answering} focused on questions from the CDP questionnaire and the NAIC Climate Risk Disclosure survey. As the responses to those questionnaires are publicly available, they constructed two datasets, ClimaINS and ClimaQA. ClimaQA is similar to ClimateQA (question/potential answer pairs). On the contrary, ClimaINS is a QA dataset framed as a classification task: it contains the responses from the NAIC survey, and each response is labeled as answering one of the 8 questions of the survey.  Once a model has been trained on these datasets, it can be used to search for answers to the questions in other documents (effectively retrieving the information from an unstructured document). 
For ClimaINS, the authors experimented with multiple fine-tuned Transformers. As ClimaQA is framed as a retrieval task, the authors used BM25, sentence-BERT, and ClimateBERT to rank possible answers. Using the methodology evaluated on ClimaQA, they proposed a system to extract responses to CDP questions directly from unstructured reports and automatically fill the questionnaire~\cite{spokoyny2023answering}. \citet{luccioni_analyzing_2020} trained and evaluated a RoBERTa-based model called ClimateQA on finding answers to TCFD-based questions. They all reached good performance but with room for improvement (see Table~\ref{tab:reported perf question asnwering (qa)} in the Appendix for details), demonstrating the feasibility of the task but also the need for further research.

\paragraph{Answer generation} After the retrieval, the second step of question answering is to generate an answer for the question (given input resources or not). The generation can be simple such as finding a particular information in a paragraph. For example, \citet{tobias_schimanski_climatebert-netzero_2023} used a QA model (RoBERTa SQuaD v2~\cite{rajpurkar-etal-2016-squad}) to extract the target year, the percentage of reduction, and the baseline year from the reduction target of a company from an input text that contains an emission reduction target. While it is possible to rely on generalist models, as they are proficient few-shot learners~\cite{lm_few_shot_learner, thulke2024climategpt}, multiple studies proposed using domain-specific models.  
Climate Bot~\cite{cliamtebot_2022} built a dataset of climate-specific QA. Given a question and a document (scientific/news), the model should find the span answering the question.  \citet{mullappilly-etal-2023-arabic} proposed a dataset of question-answer pairs based on ClimaBench~\cite{spokoyny2023answering} and CCMRC~\cite{cliamtebot_2022} to train a model to generate answers. \citet{thulke2024climategpt} introduced a climate-specific corpus of prompt/completion pairs for Instruction Fine-Tuning created by experts and non-experts. 
\cite{cliamtebot_2022} finetuned an ALBERT model on their climate-specific QA dataset. \citet{mullappilly-etal-2023-arabic} trained a Vicuna-7B on their climate-specific answer generation dataset. And \citet{thulke2024climategpt} trained LLama-2-based models (ClimateGPT) on their domain-specific prompt/completion dataset. They also evaluated their ClimateGPT model alongside multiple LLMs on climate-related benchmarks. They found that the domain-specific models outperformed generalist ones (see Table \ref{tab:reported perf question asnwering (qa)} in the Appendix for details).

\paragraph{Retrieval Augmented Generation} By combining the last two components (a retriever and a QA system), one can develop Retrieval-Augmented Generation (RAG) systems. \citet{cliamtebot_2022} provide the first example of a RAG system in climate-related tasks based on sentence-BERT to retrieve documents and AlBERT to identify the answer. More recently, \citet{jingwei_ni_paradigm_2023}~proposed ChatReport, a methodology based on ChatGPT for analyzing corporate reports through the lens of TCFD questions using RAG.  The reports, the answers to the TCFD questions, and a TCFD conformity assessment are chunked and stored in a vector store for easy retrieval. Each chunk and/or answer can be retrieved and injected into the prompt. They added in the prompt the notions of greenwashing and cheap talk to invite the model to provide a critical analysis of the retrieved answers. Similarly, \citet{s_vaghefi_chatclimate_2023} introduced ChatClimate, a RAG-based pipeline to augment GPT-4 with knowledge about IPCC reports. While the previous LLM-based approaches rely on closed-source models, \citet{mullappilly-etal-2023-arabic} and \citet{thulke2024climategpt} proposed RAG-based pipelines relying on open-source models.
\citet{cliamtebot_2022} is the only study that evaluated their RAG pipeline using classical metrics (F1-score, BLEU, METEOR). Unfortunately, those metrics are not well aligned with human judgment for text generation \cite{chen-etal-2019-evaluating}. Therefore \citet{s_vaghefi_chatclimate_2023} and \citet{mullappilly-etal-2023-arabic} relied on human or ChatGPT evaluations. They conclude that their approaches improve on non-domain-specific RAG systems.

\paragraph{Insights} The main application of Question-Answering (QA) systems is to analyze complex documents, such as corporate climate reports, that can be particularly useful for greenwashing detection. Modern LLMs achieve strong results in QA due to instruction-following fine-tuning, which improves adherence to specific instructions, and Retrieval-Augmented Generation (RAG), which bases its answers on retrieved documents. However, retrieval systems  remain a performance bottleneck~\cite{maekawa-etal-2024-retrieval}. Furthermore, answer generation is challenging to evaluate~\cite{survey_nlg_eval}, often requiring human feedback or advanced model-based assessments.


\subsection{Classification of Deceptive Techniques}
\label{sec:deceptive}

In analyzing climate-related discourse, it can be useful to identify rhetorical strategies that might obscure or misrepresent an entity’s stance. 
For example, expressing support for climate remediation policies on one side \cite{morio2023an}, and promoting arguments downplaying the urgency of climate change on the other \cite{coanComputerassistedClassificationContrarian2021} could signal a lack of authenticity. Another example could be promoting a product through misleading rhetoric, by, for example, claiming that a product is better because it is natural, which would be an ``appeal to nature'' -- a fallacious argument \cite{vaid-etal-2022-towards, jain_supply_2023}.
Detecting these deceptive techniques could help identify misleading communications.

\begin{table}[ht]
\centering
\begin{tabular}{p{3cm}p{3cm}p{4cm}p{4cm}}
\toprule
\textbf{Dataset}  & \textbf{Input}  & \textbf{Labels}                                                                                  & \textbf{(Labels details)}                                        \\
\midrule
LogicClimate~\cite{jin-etal-2022-logical}  & texts from climatefeedback.org  & \textit{Faulty Generalization, Ad Hominem, Ad Populum, False Causality, ...} & Classifies fallacies (Multi-label)\\
\midrule
\raggedright Neutralization Techniques~\cite{bhatia_automatic_2021-1} & paragraphs from other previous works on climate-change & \textit{Denial of Responsibility, Denial of Injury, Denial of Victim, Condemnation of the Condemner, ...} & Classifies neutralization techniques  \\
\midrule
Contrarian Claims~\cite{coanComputerassistedClassificationContrarian2021} & paragraphs from conservative think tank & \textit{No Claim, Global Warming is not happening, Climate Solutions won't work, Climate impacts are not bad, ...} & Classifies arguments into  super/sub-categories of climate science denier's arguments  \\
\bottomrule
\end{tabular}
\caption{Summary of datasets, labels, scopes, and sources for tasks related to deceptive techniques in climate-related context.}
\label{tab:datasets_deceptive}
\end{table}

\task The goal is to classify statements into argumentative categories: fallacies, types of arguments, or rhetorical techniques.

\zeroshot \citet{divinus_oppong-tawiah_corporate_2023} frame greenwashing as fake news. They propose to tackle this identification through the form perspective. 
They developed a profile-deviation-based method to detect greenwashing in corporate tweets by comparing linguistic cues (e.g., quantity, specificity, complexity, diversity, hedging, affect, and vividness) to theoretically ideal profiles of truthful and deceptive communication. They compute a greenwashing score as Euclidean distances.

\datasets An organization seeking to justify limited action might adopt a rhetoric that downplays the urgency of global warming or dismisses the impact of negligent behavior. These kinds of climate contrarian arguments mostly fall into a finite set of categories (e.g. ``Solutions will not work'', or ``Human influence is not demonstrated''). Therefore, \citet{coanComputerassistedClassificationContrarian2021} constructed a taxonomy of such contrarian claims and published a large dataset annotated with such claims. 
Similarly, \citet{bhatia_automatic_2021-1} proposed a dataset to classify neutralization techniques, i.e., rationalizations that individuals use to justify deviant or unethical behavior (e.g. Denial of the victim). Finally, \citet{jin-etal-2022-logical} studied fallacious arguments and how they apply to climate change more broadly. They constructed LogicClimate, a climate-specific dataset of sentences annotated with fallacies, using articles from the \href{https://science.feedback.org/reviews/?_topic=climate}{Climate Feedback website}.

\solutions Fine-tuned Transformer architectures and classical approaches have been evaluated on each dataset~\cite{jin-etal-2022-logical, bhatia_automatic_2021-1, coanComputerassistedClassificationContrarian2021}. \citet{jin-etal-2022-logical} experimented with an ELECTRA model, fine-tuned to use the structure of the argument. The performance of fine-tuned approaches ranges from 58.77\% to 79\%. The models struggle with fallacy detection in LogicClimate, but perform well with generalist fallacy detection, contrarian claims, and neutralization techniques. \citet{thulke2024climategpt} experimented with multiple zero-shot approaches using multiple LLM on the binary classification of contrarian claims using \citet{coanComputerassistedClassificationContrarian2021}'s dataset. Their climateGPT-70B and LLama-2-Chat-70B models both reached an F1-score of 72.5\% (see Table \ref{app:table deceptive techniques} in the Appendix for details). 

\paragraph{Insights} The low performance of models on tasks such as fallacy detection and neutralization classification indicates the inherent complexity of these tasks. Fallacy detection, for one, is known to be inherently subjective~\cite{helwe-etal-2024-mafalda}. %This difficulty arises from the subtlety and diversity of deceptive techniques and the limited size of available annotated datasets. While existing datasets, such as 
The detection of neutralization techniques, too, appears to be subjective, as even human annotators achieve only a moderate performance level of 70\% F1-score.
Furthermore, both LogicClimate~\cite{jin-etal-2022-logical} and the neutralization dataset~\cite{bhatia_automatic_2021-1} are very small in size.
To address these limitations, future research could explore several directions, including increasing the size of the datasets, defining  the labels more precisely, %trying to improve IAA, 
or permitting multiple correct annotations~\cite{helwe-etal-2024-mafalda} to acknowledge viable disagreement. Additionally, the analyses on rhetorical techniques could be combined with other analyses (such as stance detection) to search for communication patterns. 

\subsection{Environmental Performance Prediction}
\label{sec:env prediction}

Greenwashing can be interpreted as a misrepresentation of the company or product's environmental performance. The environmental performance is often summarized by a quantitative metric such as the ESG score, the Finch score\footnote{The Finch Score is a sustainability rating system designed to help consumers make eco-friendly choices by evaluating products on a scale from 1 to 10. See \url{https://www.choosefinch.com/}}, or CO2 emissions. These scores are usually not directly mentioned in the company reports, but have to be inferred based on company communications:
%represent the underlying performance of the company and can not directly be extracted from reports or product descriptions; they have to be inferred.  All the relevant element to infer those score should be avaible in company communication in the product description for the Finch score, and in the company reports for ESG scores.

%\input{latex/tikz_figure/predict_graph} Oana:we still need to cut a lot, so I removed this image

\task Given an input company report, output an environment-related quantitative value (such as the ESG score or the amount of Carbon Emission), even if that value is not mentioned in the report. In variants of this task, the goal is to predict not a value, but a range for that value, out of a set of possible ranges. 
%The task then becomes a classification problem.
%This is a prediction task of a continuous value. However, these scores can also be discretized, allowing for interpretation within a classification framework.

% ESG scoring is extensively studied, usually through traditional regression methods \citet{chowdhuryEnvironmentalSocialGovernance2023}. However, several works have proposed using language models to include communications and news events as features for predicting the score.

\zeroshot \citet{jain_supply_2023} proposed to predict the Scope 3 emissions %, not directly, but via % Fabian: what would ``directly'' mean?
from the list of financial transactions of the company. They first performed classification of all transaction descriptions to map them to their emission factors (emission per \$ spent), and then compute the emissions of each transaction. For ESG score prediction, \citet{bronzini_glitter_2023} proposes to use LLMs as few-shot learners to extract triples of ESG category, action, and company from sustainable reports, and to construct a graph representation with few-shot learning. They demonstrated the triplet generation using Alpaca, WizardLM, Flan-T5, and ChatGPT on a few examples. They could analyze disclosure choices and company similarities using the constructed graph. More importantly, they also used the graph to interpret ESG scores through the interpretability analysis of OLS predictions, effectively predicting the ESG score. 

\datasets \citet{Mehra_2022} proposed to predict the changes in the ESG score instead of the actual value. They constructed their dataset by extracting the three sentences most relevant to the environment from financial reports and associated them with the E score\footnote{ESG scores are usually aggregated scores along multiple dimensions. In this study, they are focusing on the "Environment" part of the ESG score.}'s change and direction of change. Instead of relying on scores, \citet{clarkson_nlp_us_csr} proposed to focus on good/bad CSR (corporate social responsibility) performers. Their approach evaluates CSR performance based on linguistic style rather than content, aiming to identify whether language patterns alone influence the perception of CSR quality. \citet{Greenscreen} introduced a multi-modal dataset of the tweets of companies to predict a company's ESG unmanaged risk. 
Finally, focusing on products and not companies, \citet{linSUSTAINABLESIGNALSijcai2023} introduced a dataset of online product descriptions and reviews used to predict the Finch Score. All these scores can be used to identify companies and products that are actually environmentally friendly, helping users distinguish actual sustainability from greenwashing. However, they can also be used to find discrepancies between the communicated green-ness of a product or company (e.g. the percentage of climate-related texts in reports) and the actual green-ness (e.g. ESG score or quantity of emissions), which might indicate cases of greenwashing.  

\solutions \citet{Mehra_2022} evaluated fine-tuned Transformers on ESG change prediction. \citet{clarkson_nlp_us_csr} experimented with hand-chosen features plugged into random forest and SVM on CSR performers prediction. \citet{linSUSTAINABLESIGNALSijcai2023} also experimented with traditional approaches (e.g. gradient boosting) and custom architectures based on Transformer models on Finch score prediction. \citet{Greenscreen} introduced baseline models, which are simple image and text embedding models (e.g., CLIP or sentence-BERT) with a classification head on ESG score prediction. A detailed list of models is reported in Table~\ref{tab:models} in the Appendix. \citet{bronzini_glitter_2023, clarkson_nlp_us_csr, Greenscreen} did not report baselines, which makes the study difficult to analyze. \cite{Mehra_2022} reported good accuracy, demonstrating that a few sentences hold a significant amount of information to predict ESG score changes. However, there is definitely room for improvement, indicating that the information selected is not sufficient. \citet{lin-etal-2023-linear} reported the performance of the baseline (average score) reaching already mean squared error (MSE) of 11.7\% already quite low. Their Transformer-based approach reached MSE of 7.4\% improving slightly on classical approaches (Gradient Boosting reaching MSE of 8.2\%). Detailed performances can be found in Table \ref{tab:appendix env pred} in the Appendix.

\paragraph{Insights} We expected the prediction of ESG and CSR metrics to be difficult and require an extensive understanding of a company. However, \citet{bronzini_glitter_2023, linSUSTAINABLESIGNALSijcai2023, Mehra_2022} and \citet{clarkson_nlp_us_csr} showed that it is possible to build strong predictors relying only on textual elements. This can be explained because analysts reward transparency~\cite{bronzini_glitter_2023}, so the quantity and complexity of disclosure have strong predictive power~\cite{clarkson_nlp_us_csr}. Although existing studies have examined specific dimensions, future research should investigate the interaction between form~\cite{clarkson_nlp_us_csr} and content~\cite{bronzini_glitter_2023}, as this could help uncover inconsistencies that may indicate greenwashing practices.

