\section{Intermediary tasks}
\label{sec:intermediary tasks}

%Although some works do not explicitly focus on greenwashing, their analysis of company communications can still reveal potentially misleading information, which may suggest greenwashing. Consequently, many of these works serve as valuable stepping stones toward the broader goal of detecting greenwashing. We will survey them in this section.

\subsection{Pretraining Models on Climate-Related Text}
\label{sec:domain specific model}

The first step in applying a language model for a given task is typically the pretraining process, which involves training the model on relevant corpora. Although general-purpose models such as BERT~\cite{devlin-etal-2019-bert} and LLaMA~\cite{touvron2023llamaopenefficientfoundation} have demonstrated strong performance in various tasks, they are inherently limited by the knowledge and vocabulary present in their training corpora. This limitation can lead to suboptimal results when these models are applied to highly specialized domains with unique terminology and concepts. Hence, domain-specific models are usually trained on targeted datasets to improve their performance in niche areas.


\task A language model is pretrained on climate-related text, usually on the classical tasks of masked token prediction and next sentence prediction. The goal is to produce a domain-specific pre-trained language model that will be subsequently fine-tuned to specific tasks. There are two approaches: \begin{enumerate*} \item training a model with a domain-specific corpus from scratch, or \item further-training\footnote{Further-training refers to the process of training a pretrained model on its original task (e.g., next-token prediction or masked-token prediction) using additional domain-specific data to specialize its knowledge for that domain.} a generalist pretrained model on a domain-specific corpus.
\end{enumerate*}


\datasets There have been several efforts to create a climate-related corpus. \citet{nicolas_webersinke_climatebert_2021} gathered climate-related news articles, research abstracts, and corporate reports. 
\citet{vaghefi2022deep} introduced Deep Climate Change, a dataset composed of abstracts of articles from climate scientists, and built a corpus specific to climate research texts. 
\citet{schimanski_bridging_2023} introduced a dataset that focuses on text related to Environment Social and Governance (ESG). Similarly, \citet{Mehra_2022} built a dataset using text from the Knowledge Hub of Accounting for Sustainability for an ESG domain-specific corpus.
More recently, \citet{thulke2024climategpt} proposed a dataset comprising news, publications (abstracts and articles), books, patents, the English Wikipedia, policy and finance-related texts, Environmental Protection Agency documents, and ESG, and IPCC reports. \citet{mullappilly-etal-2023-arabic} proposed a climate-specific multilingual dataset. \citet{yu_climatebug_2024} published a pretraining dataset constructed with annual and sustainable reports from EU banks. 
Unfortunately, to the best of our knowledge, only~\citet{yu_climatebug_2024}'s ClimateBUG-data dataset is publicly available. 

\solutions Based on the above datasets, the models ClimateBERT, ClimateGPT-2, climateBUG-LM and EnvironmentalBERT, ESGBERT were proposed~\cite{nicolas_webersinke_climatebert_2021, vaghefi2022deep, yu_climatebug_2024, schimanski_bridging_2023, Mehra_2022}. 
More recently, generative domain-specific models such as Llama-2 for ClimateGPT~\cite{thulke2024climategpt} or Vicuna for Arabic Mini-ClimateGPT~\cite{mullappilly-etal-2023-arabic} have also been proposed. 
%This new class of generative models is proficient at zero-shot and RAG and will be detailed in Section~\ref{sec:qa}.

\paragraph{Performance of Models} The domain-specific models drastically improve the performance on domain-specific masked-language modeling~\cite{nicolas_webersinke_climatebert_2021, yu_climatebug_2024} and next token prediction~\cite{vaghefi2022deep}. They were also evaluated on domain-specific downstream tasks. These tasks are either based on pre-existing datasets such as ClimateFEVER~\cite{diggelmann_climate-fever_2020} or introduced by the authors, as in ClimateBERT's climate detection~\cite{nicolas_webersinke_climatebert_2021}. At this stage, we evaluate whether fine-tuning enhances downstream performance, with a detailed analysis of the tasks presented in subsequent sections.

\begin{table}[ht]
    \begin{subtable}[t]{0.46\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lccc}
            \toprule
            & \textbf{Cl.BERT} & \textbf{DisRoBERTa} & \textbf{NB} \\
            \midrule
            Climatext \cite{spokoyny2023answering, varini_climatext_2020} & 85.14 & \textbf{86.06} & 83.39 \\
            Climate Detection \cite{nicolas_webersinke_climatebert_2021} & \textbf{99.1}$\pm1$ & 98.6$\pm0.8$ & \\
            %Climate Detection \cite{bingler2023cheaptalkspecificitysentiment} & $97$ & & $87$ \\
            Sentiment \cite{nicolas_webersinke_climatebert_2021} & \textbf{83.8}$\pm3.6$ & 82.5$\pm4.6$ & \\ 
            % Sentiment \cite{bingler_cheap_2021} & 80 & & 72 (+5) \\
            Net-zero/Reduction \cite{tobias_schimanski_climatebert-netzero_2023} & \textbf{96.2}$\pm0.4$ & 94.4$\pm0.6$ \\ 
            \bottomrule
            \toprule
             & \textbf{Cl.BERT} & \textbf{DisRoBERTa} & \textbf{TF-IDF} \\
            \midrule
            TCFD classification \cite{sampson_tcfd-nlp_nodate} & 0.852 & 0.819 & \textbf{0.867} \\
            \bottomrule
            \toprule
             & \textbf{Cl.BERT} & \textbf{DisRoBERTa} & \textbf{SVM} \\
            \midrule
            SciDCC \cite{spokoyny2023answering, mishra2021neuralnere} & \textbf{52.97} & 51.13 & 48.02 \\
            ClimaTOPIC \cite{spokoyny2023answering} & \textbf{64.24} & 63.61 & 58.34 \\
            Cl.FEVER (claim) \cite{xiang_dare_2023, diggelmann_climate-fever_2020} & \textbf{76.8} & 72 & \\
            Cl.FEVER (claim) \cite{nicolas_webersinke_climatebert_2021, diggelmann_climate-fever_2020} & \textbf{75.7}$\pm4.4$ & 74.8$\pm3.6$ & \\
            Cl.FEVER (evid.) \cite{spokoyny2023answering, diggelmann_climate-fever_2020} & \textbf{61.54} & \textbf{61.54} & \\
            Cl.Stance \cite{spokoyny2023answering, vaid-etal-2022-towards} & \textbf{52.84} & 52.51 & 42.92 \\
            ClimateEng \cite{spokoyny2023answering, vaid-etal-2022-towards} & 71.83 & \textbf{72.33} & 51.81 \\
            ClimaINS \cite{spokoyny2023answering} & \textbf{84.80} & 84.38 & 86.00 \\
            ClimaBENCH \cite{spokoyny2023answering} & \textbf{69.44} & 69.27 & \\
            Nature \cite{Schimanski2024nature} & \textbf{94.11} & 94.03 & \\
            \bottomrule
            \toprule
            & \textbf{Cl.BERT} & \textbf{SVM+ELMo} & \textbf{SVM+BoW} \\
            \midrule
            Commitment\&Actions \cite{bingler2023cheaptalkspecificitysentiment} & \textbf{81} & 79 & 76 \\
            Specificity \cite{bingler2023cheaptalkspecificitysentiment} & \textbf{77} & 76 & 75 \\
            \bottomrule
            \toprule
             & \textbf{Cl.BERT} & \textbf{DisBERT} & \textbf{SVM} \\
            \midrule
            Env. Claim \cite{stammbach_environmental_2023} & \textbf{83.8} & 83.7 & 70.9 \\
            \bottomrule
            \toprule
             & \textbf{Cl.BERT} & \textbf{DisBERT} & \textbf{LSTM} \\ %\textbf{POS-Bi-LSTM-Attention} \\
            \midrule
            DARE sentiment \cite{xiang_dare_2023} & 87.4 & \textbf{89.9} & 88.2 \\
            \bottomrule
        \end{tabular}
        }
    \caption{F1-scores of climateBERT when compared to a similarly sized models. ROC-AUC for TCFD classification.}
    \label{tab:comparison climatebert}
    \end{subtable}  
    \begin{subtable}[t]{0.53\textwidth}
    
        \vspace{-46mm}

    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccc}
        \toprule
        & \textbf{RoBERTa} & \textbf{EnvRoBERTa} & \textbf{DisRoBERTa} & \textbf{EnvDisRoB.} \\
        \midrule
        Environment \cite{schimanski_bridging_2023} & 92.35$\pm2.29$ & \textbf{93.19}$\pm1.65$ & 90.97$\pm2$ & $92.35\pm1.65$ \\
        Social \cite{schimanski_bridging_2023} & 89.87$\pm1.35$ & \textbf{91.90}$\pm1.79$ & 90.59$\pm1.03$ & $91.24\pm1.86$ \\
        Governance \cite{schimanski_bridging_2023} & 77.03$\pm1.82$ & 78.48$\pm2.62$ & 76.65$\pm2.39$ & \textbf{78.86}$\pm1.59$ \\
    \bottomrule
        \toprule
         & \textbf{EnvDisRoB.} & \textbf{ClimateBERT} & \textbf{RoBERTa} & \textbf{DisRoBERTa} \\
        \midrule
        Water \cite{Schimanski2024nature} & 94.47$ \pm 1.37$ & \textbf{95.10}$ \pm 1.13$ & 94.55$ \pm 0.86$ & $94.98 \pm 1.16$ \\
        Forest \cite{Schimanski2024nature}   & \textbf{95.37}$ \pm 0.92$ & 95.34$ \pm 0.94$ & 94.78$ \pm 0.48$ & 95.29$ \pm 0.65$ \\
        Biodiversity \cite{Schimanski2024nature}   & \textbf{92.76}$ \pm 1.01$ & 92.49$ \pm 1.03$ & 92.46$ \pm 1.54$ & 92.29$ \pm 1.23$ \\
        Nature \cite{Schimanski2024nature}  & \textbf{94.19}$ \pm 0.81$ & 93.50$ \pm 0.64$ & 93.97$ \pm 0.26$ & 93.55$ \pm 0.72$ \\
        \bottomrule
        \end{tabular}
        }
    \caption{F1-scores of multiple models compared to EnvironmentalBERT (based on RoBERTa and distilRoBERTa).}
    \label{tab:comparison env}
    

    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcc}
        \midrule
         & \textbf{GPT-2} & \textbf{climateGPT-2} \\
        \midrule
        ClimateFEVER \cite{vaghefi2022deep} & 62 & \textbf{72} \\
        \midrule
        \midrule
         & \textbf{LLama-2-Chat (7B)} & \textbf{ClimateGPT (7B)} \\
        \midrule
        Multiple Benchmarks \cite{thulke2024climategpt} & 71.4 & \textbf{77.1} \\
        \midrule
        \end{tabular}
        }
    \caption{F1-scores of ClimateGPT-2 and custom performances (average Accuracy and F1-score over multiple benchmarks) of ClimateGPT compared to similarly sized models.}
    \label{tab:climateGPT}
    %     \centering
    %     \resizebox{\textwidth}{!}{
    %     \begin{tabular}{p{3.3cm}cc}
    %     \toprule
    %     & \textbf{CLBERT} & \textbf{Longformer} \\ \midrule
    %     climatext \cite{spokoyny2023answering, varini_climatext_2020} & 85.14  &  \textbf{87.80} \\
    %     SciDCC \cite{spokoyny2023answering} & 52.97 & \textbf{54.79} \\
    %     LobbyMap (page) \cite{lai_using_2023} & 74.4 & \textbf{76.5} \\
    %     LobbyMap (query) \cite{lai_using_2023} & 48.9 & \textbf{55} \\
    %     LobbyMap (stance) \cite{lai_using_2023} & 39 & \textbf{44.1} \\     \bottomrule
    %     \toprule
    %     & \textbf{CLBERT} & \textbf{BERT} \\ \midrule
    %     Climatext \cite{garridomerchán2023finetuning, varini_climatext_2020} & \textbf{93} & 91 \\
    %     DARE's Sentiment \cite{xiang_dare_2023} & 87.5 & \textbf{93.1} \\
    %     CLFEVER (claim) \cite{diggelmann_climate-fever_2020, xiang_dare_2023} & 76.8 & \textbf{80.7} \\     \bottomrule
    %     \toprule
    %     & \textbf{CLBERT} & \textbf{FinBERT} \\ \midrule
    %     CLBUG-data \cite{yu_climatebug_2024} & \textbf{91.07} & 90.82 \\
    %     Climate detection \cite{bjarne_brie_mandatory_2022} & \textbf{98.59} & 96.67 \\ \bottomrule
    %     \toprule
    %     & \textbf{CLBERT} & \textbf{RoBERTa} \\ \midrule
    %     Ledger classif. \cite{jain_supply_2023}  & 85.20 & \textbf{87.19} \\
    %     ClimaTOPIC \cite{spokoyny2023answering} & 64.24 & \textbf{65.22} \\
    %     ClimateEng \cite{spokoyny2023answering, vaid-etal-2022-towards} & 69.60 & \textbf{73.50} \\
    %     Env. Claims \cite{stammbach_environmental_2023} & 83.7 & \textbf{84.9} \\
    %     Net-zero/Reduction \cite{tobias_schimanski_climatebert-netzero_2023} & \textbf{96.2} & 95.8 \\
    %     CLStance \cite{spokoyny2023answering, vaid-etal-2022-towards} & 52.84 & \textbf{59.69} \\
    %     Stance \cite{lai_using_2023} & \textbf{90} & 89 \\
    %     ClimaINS \cite{spokoyny2023answering} & 84.80 & \textbf{85.35} \\
    %     ClimaBench \cite{spokoyny2023answering} & 69.44 & \textbf{71.14} \\     \bottomrule
    %     \toprule        
    %     & \textbf{CLBERT} & \textbf{SciBERT} \\ \midrule
    %     CLFEVER (evid.) \cite{diggelmann_climate-fever_2020, spokoyny2023answering} & 61.54 & \textbf{62.68} \\
    %     \bottomrule
    %     \toprule
    %     & \textbf{CLBUG-LM} & \textbf{FinBERT} \\ \midrule
    %     ClimateBUG-data \cite{yu_climatebug_2024} & \textbf{91.36} & 90.82 \\     \bottomrule
    %     \end{tabular}
    %     }
    %     \caption{Comparison of ClimateBERT with larger models (both fine-tuned with an identical experimental settings). The metrics vary depending on the datasets (F1, Accuracy, custom metrics, etc).}
    %     \label{tab:climateBERT vs larger}
    \end{subtable}
    

    \caption{Performance of domain-specific models on domain-specific tasks. The figure displayed in the tables are the values reported by authors in the corresponding studies; with the following abbreviations: \textit{CL} for Climate, \textit{Dis} for Distil, \textit{EnvDisRob} for EnvDistilRoBERTa, \textit{Evid.} for evidences. Each row reports the performances of models fine-tuned in the same experimental setting.
    Detailed performances are reported in Appendix \ref{app:perf}.}
\end{table}



% \begin{tabular}{llc}
        % \toprule
        % \textbf{Dataset}        & \textbf{Model}       & \textbf{Metric} \\ \midrule
        % Climatext \cite{spokoyny2023answering, varini_climatext_2020}         & Longformer           & 87.80                \\
        %                         & ClimateBERT          & 85.14                \\ \midrule
        % Climatext \cite{garridomerchán2023finetuning, varini_climatext_2020}         & BERT           & 91                \\
        %                         & ClimateBERT          & 93                \\ \midrule
        % ClimateBUG-data \cite{yu_climatebug_2024}         & climateBUG-LM        & 91.36                 \\
        %                         & ClimateBERT          & 91.07   \\   
        %                         & FinBERT          & 90.82  \\ \midrule
        % Climate detection \cite{bjarne_brie_mandatory_2022}             & ClimateBERT          & 98.59                 \\
        %                         & FinBERT              & 96.67                 \\ \midrule
        % SciDCC \cite{spokoyny2023answering}         & Longformer           & 54.79                 \\
        %                         & ClimateBERT          & 52.97                 \\ \midrule
        % Ledger classification \cite{jain_supply_2023}  & RoBERTa              & 87.19                 \\
        %                         & ClimateBERT          & 85.20                 \\ \midrule
        % CLIMA-TOPIC \cite{spokoyny2023answering}            & RoBERTa              & 65.22                 \\
        %                         & ClimateBERT          & 64.24                 \\ \midrule
        % ClimateEng \cite{spokoyny2023answering, vaid-etal-2022-towards}             & RoBERTa-Large        & 73.50                 \\
        %                         & ClimateBERT          & 69.60                 \\ \midrule
        % DARE's Sentiment  & BERT-base        & 93.1                 \\
        % Analysis \cite{xiang_dare_2023}       & ClimateBERT          & 87.5                 \\ \midrule
        % Environmental              & RoBERTa-Large        & 84.9             \\
        % Claims \cite{stammbach_environmental_2023}                        & ClimateBERT          & 83.7                 \\ \midrule      
        % Net-zero/Reduction \cite{tobias_schimanski_climatebert-netzero_2023}             & RoBERTa-base        & 95.8             \\
        %                         & ClimateBERT          & 96.2                 \\ \midrule     
        % ClimateFEVER             & SciBERT        & 62.68             \\
        %  (evidence) \cite{diggelmann_climate-fever_2020, spokoyny2023answering}                        & ClimateBERT          & 61.54                 \\ \midrule  
        % ClimateFEVER              & BERT        & 80.7             \\
        %   (claim) \cite{diggelmann_climate-fever_2020, xiang_dare_2023}                      & ClimateBERT          & 76.8                 \\ \midrule
        %  ClimateStance \cite{spokoyny2023answering, vaid-etal-2022-towards}             & RoBERTa        & 59.69                 \\
        %                         & ClimateBERT          & 52.84                 \\ \midrule
        % Stance \cite{lai_using_2023}             & RoBERTa        & 89                 \\
        %                         & ClimateBERT          & 90                 \\ \midrule
        % LobbyMap (page) \cite{lai_using_2023}             & longformer-large        & 76.5                 \\
        %                         & ClimateBERT          & 74.4                 \\ \midrule
        % LobbyMap (query) \cite{lai_using_2023}             & longformer-large        & 55                 \\
        %                         & ClimateBERT          & 48.9                 \\ \midrule
        % LobbyMap (stance) \cite{lai_using_2023}             & longformer-large        & 44.1                 \\
        %                         & ClimateBERT          & 39                \\ \midrule
        % ClimaINS \cite{spokoyny2023answering}             & RoBERTa        & 85.35                 \\
        %                         & ClimateBERT          & 84.80                 \\ \midrule
        % ClimaBench \cite{spokoyny2023answering}             & RoBERTa        & 71.14                 \\
        %                         & ClimateBERT          & 69.44                 \\ \bottomrule
        % \end{tabular}



As shown in Table~\ref{tab:climateGPT}, both ClimateGPT-2 and ClimateGPT significantly outperform the baseline model in their designated tasks. These models benefit from additional training on climate-related data, highlighting the advantages of domain adaptation. However, when trained from scratch, \citet{thulke2024climategpt} report substantially lower performance, underscoring the importance of large-scale pretraining data.
For smaller models (climateBERT, climateBUG-LM and EnvironmentalBERT), the domain-adaptation also enhanced the performance on domain-specific tasks, though the significance of this improvement is less pronounced.
As detailed in Table~\ref{tab:comparison climatebert}, out of the 15 evaluations of fine-tuned domain-specific climateBERT~\cite{nicolas_webersinke_climatebert_2021} on climate-related tasks compared to the fine-tuned base model (distilRoBERTa), 4 reported an improvement compared to the base model (>1\%), 8 reported a marginal improvement (<1\%) and 3 indicate no measurable difference (<=0\%). %In the last two tasks from Table~\ref{tab:comparison climatebert}, climateBERT is compared to another small model, DistilBERT; in one case, there is marginal improvement, and in the other, climateBERT is underperforming. 
When compared to DistilBERT in the last two tasks from Table~\ref{tab:comparison climatebert}, climateBERT exhibits a slight improvement in one case, while performing comparably in the other.
Additionally, climateBERT achieves performance similar to the simple baselines in certain instances (Table~\ref{tab:comparison climatebert}). 
For example, in Specificity Classification~\cite{bingler2023cheaptalkspecificitysentiment}, climateBERT outperforms an SVM with bag-of-word by 2\%, and on the TCFD classification~\cite{sampson_tcfd-nlp_nodate} climateBERT performs slightly below the TF-IDF baseline model. Likewise, climateBUG-LM~\cite{yu_climatebug_2024} demonstrates a slight improvement of less than 1\% over BERT and FinBERT on a climate-related detection task. %Similarly, climateBUG-LM~\cite{yu_climatebug_2024} improved by less than 1\%. 
As shown in Table~\ref{tab:comparison env}, the domain-adapted models shows a small improvement in the performance on the Environmental task, by 1.38\% for distilRoBERTa and 0.84\% for RoBERTa. 


\paragraph{Insights} Domain-specific language models, particularly those developed for environmental and climate-related texts, offer an interesting avenue for improving model performance through enhanced knowledge within specialized domains. Theoretically, such models should outperform general models by leveraging their tailored vocabulary and contextual understanding. However, our findings suggest that the improvements provided by the domain-specific models on the proposed domain-specific downstream tasks are limited. Indeed, we found that fine-tuning general models on domain-specific datasets consistently yields competitive performance. However, these findings do not undermine the significance of existing works that have been fundamental in demonstrating the potential of domain-specific models, particularly in generating domain-specific text (MLM and next token prediction). Moreover, ClimateGPT~\cite{thulke2024climategpt}
demonstrated significant improvement for the domain-adapted Llama-2, showing that domain-adaptation is highly relevant. Future research could focus on designing tasks where a lack of domain understanding—whether in terms of vocabulary or knowledge—significantly hinders performance.

%Further research is needed to fully understand the circumstances under which domain-specific models add measurable value. A key direction of future work lies in building more robust and/or specialized further-training datasets, such as the efforts seen with ClimateGPT, which could more effectively bridge the gap between general and specialized models. Improving the quality and coverage of these datasets may unlock the full potential of domain-specific models in complex, high-stakes fields like climate science.

% There are multiple domain-specific approaches, which can improve performance. The theoretical argument for domain-specific models is that these models have a larger  vocabulary that includes rare domain-specific words, while also the models improve the embedding of infrequent words that might be over-represented in a domain-specific corpus. %The hypothesis is that domain-specific models improve performances on domain-specific task. 



% However, using larger models seems to frequently improve performances. While we question the relevance of using domain-specific models, we also acknowledge that the explanation might come from the task that we define as domain-specific. Climate-related vocabulary is relatively popular, which might make the task not specific enough to require a dedicated model and vocabulary. This is even more true with larger models, that already  have larger vocabularies. Moreover, the relatively high performances of TF-IDF and keyword-based models on topic classification tasks highlights that they rely heavily on vocabulary cues. This might also show that tasks do not require domain-specific understanding, but only syntax and vocabulary understanding.
% maybe a contextual understanding

 
% {\color{gray}
% On another hand, \citet{thulke2024climategpt} evaluated domain-specific Llama compared to Llama-Chat and other LLMs. They found that finetuning LLama on domain-specific instruction dataset improve the accuracy of more than 8\%. \tom{I don't know exactly how to use that conclusion i am looking into it}
% \cite{thulke2024climategpt} also not the best performing models are climate related mistral 7B > climateGPT 70B, and only 2\% from climateGPT 7B.
% }\oana{for the 70B they say they did not have the time to fine tune the parameters. Here there are two things that they do which we do not know which has brought more advantage: the continuous pretraining using climate based data and the instruction fine tuning on climate related questions. we can highlight this - I am not sure if they put the results without the instruction fine tuning?}

% \fms{Any experiments or insights to share here?}

\subsection{Climate-Related Topic Detection}
\label{sec:climate-related topic}

The first step toward detecting climate-related greenwashing is identifying text addressing climate-related topics.


\task Given an input sentence or a paragraph, output a binary label,  ``\textit{climate-related}'' or ``\textit{not climate-related}''.

% \input{latex/tikz_figure/climate_graph}

\begin{table}[ht]
\centering
\begin{tabular}{p{3cm}p{4cm}p{1.3cm}p{5cm}}
\toprule
\textbf{Dataset} & \textbf{Input} & \textbf{Labels} & \textbf{Positive Label Details}  \\ \midrule
ClimateBug-data \cite{yu_climatebug_2024} & sentences from Banks' reports  & \textit{relevant/} \textit{irrelevant} & Climate change and sustainability (including ESG, SDGs related to the environment, recycling and more) \\ \midrule
ClimateBERT's climate detection \cite{bingler2023cheaptalkspecificitysentiment} & paragraphs from reports & \textit{1/0} & Climate policy, climate change or an environmental topic \\ \midrule
Climatext (Wikipedia, 10-K, claims) \cite{varini_climatext_2020} & sentences from Wikipedia, 10-Ks or web scraping & \textit{1/0} & Directly related to climate-change \\ \midrule
Climatext (wiki-doc) \cite{varini_climatext_2020} & sentences from a Wikipedia page & \textit{1/0} & Extracted from a Wikipedia page related to climate-change \\ \midrule
Sustainable signals's reviews \cite{linSUSTAINABLESIGNALSijcai2023} & online product reviews (user comments) & \textit{relevant/} \textit{irrelevant} & Contains terms related to sustainability  \\ \bottomrule
\end{tabular}
\caption{Label definitions of the datasets related to climate change and sustainability topic detection.}
\label{tab:guidelines climate-related}
\end{table}

%\fms{does this mean there is no training data? or that there is no gold standard (then how do they evaluate?) or that there is no dataset?}\tom{done} 
\zeroshot The earliest work on identifying corporate climate-related text was conducted by~\citet{doran_risk_disclosure}, where the authors collected a large corpus of 10-K filings from 1995 to 2008, and filtered climate-related reports using hand-selected keywords. While~\citet{doran_risk_disclosure} focused on identifying climate-related text within company reports to assess how businesses addressed climate change, other early efforts took a different approach by analyzing the external impacts of media attention toward climate change on companies. For instance,~\citet{Engle_hedging_climatechange} developed the WSJ Climate Change News Index based on identifying climate-related news articles in media sources using climate-change vocabulary frequency. This index served as a tool to measure the influence of climate-change attention in the media on companies. \citet{csr_report_greenwashing} proposed measuring the environmental content of CSR (corporate social responsibility) reports using a lexicon-based approach, with the goal of comparing CSR reports of environmental violators to companies with clean records. 
%These early approaches laid the groundwork for identifying climate-related content, either from a corporate disclosure perspective or by assessing the impact of climate-change attention on companies.


\datasets More recent studies have introduced several annotated datasets aimed at climate-related corporate text classification. One of the most prominent is ClimaText~\citep{varini_climatext_2020}, a large dataset from diverse sources such as Wikipedia, U.S. Security and Exchange Commission 10-K filings, and web-scraped content. The dataset is designed to cover a broad range of topics and document types to help assess climate-related discussions across different domains. It is divided into multiple subsets: wiki-doc, Wikipedia, 10-k, claims. Climatext (Wiki-doc) is annotated automatically, while Climatext (Wikipedia, 10-k and claims) is annotated by humans.
In contrast, ClimateBERT's authors~\citep{nicolas_webersinke_climatebert_2021} provide a smaller, more focused dataset consisting of paragraphs from companies' annual and sustainability reports. This dataset was created as a downstream task for evaluating the ClimateBERT model, focusing on corporate disclosures. The dataset was later extended by~\citet{bingler2023cheaptalkspecificitysentiment}, increasing its size and refining the annotation to capture both specificity and sentiment related to corporate climate discourse (see Sections~\ref{sec: climate risk},~\ref{sec: claim characteristics}).
Another large dataset, ClimateBUG-data~\citep{yu_climatebug_2024}, also focuses on corporate communications but in EU banks.
%It aims to classify whether corporate statements are climate-related, providing insights into how European financial institutions communicate about climate change.
Beyond corporate disclosures, Sustainable Signals~\citep{linSUSTAINABLESIGNALSijcai2023} introduces a dataset of product reviews, each classified based on its relevance to sustainability. In addition to focusing on consumer perspectives through reviews, this dataset also examines sustainability aspects in product descriptions, providing a broader understanding of how sustainability is communicated in the context of consumer products.

\paragraph{Labels and Guidelines} As shown in Table \ref{tab:guidelines climate-related}, although these datasets address similar tasks, they differ significantly in their label definitions. The scope of the labels varies, ranging from a narrow focus on climate change to broader topics such as sustainability and environmental impact. 
While the task of the Climatext (wiki-doc) dataset is to detect whether a sentence is related to climate change or not, the weak label actually means that the sentence originates from a Wikipedia page related to climate change. Which is not the same task, as it focuses on the source of the statement rather than the actual content or its relevance to climate-related topics. However, the weakly labeled dataset serves as a useful filtering mechanism, helping to identify sentences that are potentially interesting. It provides an initial pool of data that can be refined with human annotations. Human annotations ensure that the labels align with the task’s goals, something weak labels alone cannot achieve. 
%\fms{This is an understatement!! Discuss here that it makes no sense to detect whether a text comes from Wikipedia!! Same for "contains terms related to"; give examples!}
%Hence, there is a challenge of isolating climate-related corporate discourse, as it is often deeply embedded within broader environmental strategies and discussions. Oana: said in conclusion

\solutions The solutions proposed to tackle that task range from keyword-based models~\cite{varini_climatext_2020, bingler2023cheaptalkspecificitysentiment} to fine-tuning BERT-like models~\cite{varini_climatext_2020, nicolas_webersinke_climatebert_2021, garridomerchán2023finetuning, bingler2023cheaptalkspecificitysentiment, yu_climatebug_2024} (see Table~\ref{tab:models} in the appendix for details). The performance of the best models is above 90\%~\cite{garridomerchán2023finetuning, bingler2023cheaptalkspecificitysentiment, yu_climatebug_2024} (see Table~\ref{tab:reported perf climate} in the appendix for details). 

\paragraph{Insights} %\fms{right direction, but toothless. Tie into discussion about labels above.}\tom{I changed the labels and guidelines + Added one sentence here. It's better ?} 
The task of determining whether a statement is climate-related has been extensively studied, with numerous datasets available to support research in this area. State-of-the-art fine-tuned models now achieve near-perfect performance on these datasets, indicating that, under current conditions, the task is effectively solved.
However, the labels used across the studies vary in scope. 
%Some embrace broader concepts such as sustainability or general environmental themes as part of the positive label, while others focus exclusively on climate-related content. Oana: already said in labels
This highlights the inherent challenge of isolating climate-related discourse, given its close association with broader environmental and sustainability topics.
%These distinctions carry significant practical implications, especially when assessing how companies address climate issues. 
For example, whether to include only explicit references to climate change or incorporate broader environmental and sustainability discussions, can profoundly influence the results and interpretations of such analyses.
Moreover, relying only on weak labels might result in tasks that 
%are both misaligned and 
fail to capture the complexities of the subject.

%Future research could reproduce those works but focusing solely on climate-related content, or by distinguishing between climate-specific and other related topics (environment, ESG, ...). 

% Future research: 
% - Defining properly climate-related
% - Study the distinction between climate, environment, sustainability, ...

\subsection{Thematic Analysis}
\label{sec:sub-topics}

%citations
%direct: \citet{hyewon_kang_analyzing_2022},  \cite{yu_climatebug_2024}, \citet{spokoyny2023answering}, \citet{Schimanski2024nature}, \citet{jain_supply_2023}, \citet{vaid-etal-2022-towards}, \citet{mishra2021neuralnere}

% \citet{garridomerchán2023finetuning} performed multiple runs (25 with =/= seeds)

Once a text is known to be related to climate, one aims to know the exact topic of the text.
Companies may prioritize specific categories while underreporting others, potentially signaling selective transparency practices such as greenwashing or greenhushing.

\task Given an input sentence or a paragraph, output a subtopic related to climate change. This is a multiclass classification task that can be either supervised or unsupervised, depending on the availability of labeled data. Alternatively, it can be framed as a clustering task with the goal of discovering latent subtopic structures.
%The task is to classify a sentence or paragraph into subtopics related to climate change: a multiclass classification task that can be either supervised or unsupervised, depending on the availability of labeled data. Alternatively, it can be framed as a clustering task with the goal of discovering latent subtopic structures. Oana: I proposed a version for this, the tasks definitions will also be part of the experimental paper, otherwise that paper would miss a problem statement.

\begin{table}[ht]
\centering
\begin{tabular}{p{3cm}p{3cm}p{3cm}p{5cm}}
\toprule
\textbf{Dataset} & \textbf{Input} & \textbf{Labels}                                                                                  & \textbf{Label details}                                          \\
\midrule
TCFD rec. \cite{bingler_cheap_2021} & paragraphs from corporate annual reports & \multicolumn{2}{p{8cm}}{TCFD 4 main categories: \textit{Metrics and Targets, Risk Management, Strategy, Governance and General} (see appendix \ref{app:tcfd} for details)}             \\
TCFD \cite{sampson_tcfd-nlp_nodate} & paragraphs from  regulatory and discretionary reports & \multicolumn{2}{p{8cm}}{TCFD 11 recommendations: \textit{Metrics and Targets (board’s oversight,  management’s role), Risk Management (identified risk, impact, resilience), ...} (see Appendix \ref{app:tcfd} for details)}             \\ 
\midrule
FineBERT's ESG \cite{huangFinBERTLargeLanguage2020} & Sentences from 10-K  & \textit{Environmental, Social, Governance, General} & Environmental - e.g., climate
change, natural capital, pollution and waste, and environmental opportunities \\
ESGBERT's ESG \cite{schimanski_bridging_2023} & Sentences from reports and corporate news & \textit{Environment, Social, Governance and None} & Environmental criteria comprise a
company’s energy use, waste management, pollution, [...]
as well as compliance with governmental regulations.
Special areas of interest are climate
change and environmental sustainability.  \\
Multilingual ESG classification \cite{LEE2023119726} & Sentences from Korean reports (English and Korean)  & \textit{Environment, Social,
Governance, Neutral, and Irrelevant} & Environmental factors include the reduction of hazardous substances, eco-friendly management, climate change, carbon emission, natural resources, [...] \\ 
\midrule
SciDCC \cite{mishra2021neuralnere} & News articles (Title, Summary, Body)  & \textit{Environment, Geology, Animals, Ozone Layer, Climate, etc.} & Category in which the article was published (Automatic Label)   \\
Transaction Ledger \cite{jain_supply_2023} & Transaction ledger entry (description of transaction) & \textit{Accounting Adjustments, Administration, Advertising, Benefits \& Insurance, etc.} & Standardized commodity classes  \\
ClimateEng \cite{vaid-etal-2022-towards}  & Tweets posted during COP25 filtered by keywords (relevant to climate-change)  & \textit{Ocean/Water, Politics, Disaster, Agriculture/Forestry, General} & Sub-categories of climate-change\\
ESGBERT's Nature \cite{Schimanski2024nature} & Paragraphs from reports  & \textit{General, Nature, Biodiversity, Forest, Water} & Multi-label Nature-related topics                   \\
ClimaTOPIC \cite{spokoyny2023answering} & CDP responses (short texts) & \textit{Adaptation, Buildings, Climate Hazards, Emissions, Water, etc.} & Category of the question (Automatic Label)          \\
\bottomrule
\end{tabular}
\caption{Summary of datasets, labels, scopes, and sources for climate-related text classification tasks.}
\label{tab:datasets_subtopic}
\end{table}


\subsubsection{Classifying Climate-Related Financial Disclosure (TCFD)}
\label{sec: tcfd}

The Task Force on Climate-Related Financial Disclosures (TCFD) proposes four main categories of climate-related disclosure (Governance, Strategy, Risk Management, and Metrics and Targets) and 11 recommendations for climate-related disclosures (see Appendix \ref{app:tcfd} for details). Finding texts associated to these categories in company communication can help identify
gaps and inconsistencies in reporting.

\zeroshot \citet{dingCarbonEmissionsTCFD2023} introduced a score to quantify the extent of climate-related content in corporate disclosures, as well as 4 TCFD-category-based similarity scores. The score is based on sentence similarity with sentences from TCFD referential documents. They identified discrepancies that could signal greenwashing by comparing these scores with actual carbon emission data. \citet{auzepy_evaluating_2023} examined reports from banks endorsing the TCFD recommendations. They proposed a fine-grained analysis of TCFD categories using a zero-shot entailment classification method on sentences.

\datasets Although unsupervised methods can be employed for thematic analysis, recent studies have demonstrated the value of curated datasets for categorizing corporate disclosures. For instance, \citet{bingler_cheap_2021} leveraged report headings to annotate paragraphs into the four main TCFD categories. Similarly, \citet{sampson_tcfd-nlp_nodate} assessed the quality of climate-related disclosures using a dataset of 162k sentences, where each sentence was labeled according to the TCFD recommendations by experts through a question answering (QA) process.

\subsubsection{Classifying Environmental, Social, and Governance Disclosure (ESG)}

Beyond climate-specific reporting, Environmental, Social, and Governance (ESG) disclosures represent a broader set of non-financial metrics that evaluate a company's sustainability performance. ESG reporting has become a global standard, encompassing climate-related initiatives and other dimensions of corporate responsibility. The standardized nature of ESG metrics makes them valuable for computational models that aim to detect inconsistencies in corporate claims, such as overstating environmental impacts through greenwashing. However, as noted in \citet{berg2022aggregate}'s aggregated analysis of ESG ratings, discrepancies persist across different evaluation frameworks. These inconsistencies raise the question of whether artificial intelligence can be harnessed to improve the consistency and reliability of ESG assessments.

\zeroshot \citet{rouenEvolutionESGReports2023} proposed an ML algorithm to describe the content of ESG reports. They used industry-topic dictionaries to compute the topic frequency using a TF-IDF-based algorithm. They used that classification to identify selective disclosure. \citet{Mehra_2022} experimented with sentence similarity to extract sentences that were the most relevant to environmental factors, and \cite{bronzini_glitter_2023} selected sentences related to ESG using INSTRUCTOR-xl~\cite{su2023embeddertaskinstructionfinetunedtext}. They both subsequently used the extracted sentences to predict ESG scores.

\datasets Multiple works \cite{huangFinBERTLargeLanguage2020, schimanski_bridging_2023, LEE2023119726} proposed datasets for topic classification with the labels: Environment, Social, Governance and General.
\citet{huangFinBERTLargeLanguage2020} focused on reports while \citet{schimanski_bridging_2023} added corporate news. \citet{schimanski_bridging_2023} introduced a larger dataset for pre-training domain-specific models and an evaluation dataset of 2,000 sentences. While most ESG-related reports are typically available in English, some local companies provide them only in their native language. To partially address this, \citet{LEE2023119726} proposed a multilingual dataset constructed from Korean corporate reports.

\subsubsection{Other Topics}
\label{sec:other-sub-topics}

Although TCFD and ESG are established frameworks for analyzing corporate communications, many other topics may be of interest. These topics can follow other frameworks such as CDP, but the thematic analysis can also be unsupervised, effectively discovering the topics mentioned.

\zeroshot \citet{hyewon_kang_analyzing_2022} analyzed the content of sustainable reports with thematic analysis. It was conducted using sentence similarity between the reports and content from the SDGs website. \citet{yu_climatebug_2024} examined the themes covered in the climate-related sections of bank reports by employing a dual approach: using hand-selected keywords to target anticipated topics and applying clustering techniques to group climate-related sentences and uncover underlying themes.  \citet{bjarne_brie_mandatory_2022} leveraged ClimateBERT to extract climate-related paragraphs from corporate documents. They then applied a Structural Topic Model (STM) \citep{STM} to identify thematic clusters within the data. This STM-based approach was also used by \citet{fortes2020tracking} to identify influential topics for EUR/USD exchange rate.
Their findings highlighted a significant discrepancy between the European Central Bank (ECB) and the Federal Reserve (FED), with the ECB attributing greater importance to climate change in the context of financial stability than the FED. Such thematic analysis can be used to identify selective disclosure.

\datasets \citet{spokoyny2023answering} introduced ClimaTOPIC, a topic classification dataset derived from responses to the CDP questionnaires, organized into 12 categories that align with the thematic structure of the CDP questions. This can be used to identify sections relevant to a particular CDP topic (e.g., Emissions, Adaptation, or Energy), enabling the extraction of structured information from unstructured documents. 
\citet{Schimanski2024nature} presents a dataset focused on nature-related topics (such as Water, Forest, and Biodiversity) extracted from annual reports, sustainability reports, and earnings call transcripts (ESGBERT Nature). While less explicitly linked to climate change, these topics often reflect the consequences of environmental degradation associated with climate issues. 
%A more thorough analysis of climate-related content, categorized into subtopics (e.g., emissions, consequences, adaptations, etc.), enables a more nuanced understanding of how companies address different aspects of climate change in their communications, revealing the depth, scope, and alignment of their narratives with their stated commitments. Oana: you are not talking about the previous work, right? it is more of a future work? I removed it
While NLP is often used to analyze corporate report texts, it can also be used to analyze tables. For example, \citet{jain_supply_2023} proposes estimating Scope 3 emissions \footnote{Scope 3 emissions refer to all indirect greenhouse gas (GHG) emissions that occur in a company’s value chain, excluding emissions from the company’s own operations (Scope 1) and its purchased electricity (Scope 2).} based on corporate transactions (see Section~\ref{sec:env prediction} for details). The first step involves grouping expenses into standardized commodity classes, for which they introduced a dataset of 8K examples of corporate expenses categorized accordingly. 
%to map expenditures to CO2 emissions per dollar spent.
As previously discussed, social media and news articles offer a broader perspective for cross-referencing corporate communications. For example, \citet{vaid-etal-2022-towards} introduced ClimateEng, a climate-related topic classification dataset composed of tweets grouped into five categories: Disaster, Ocean/Water, Agriculture/Forestry, Politics, and General. 
This dataset can be used to detect greenwashing by comparing the public’s concerns with the narrative presented in corporate sustainability reports. Similarly, \citet{mishra2021neuralnere} published the Science Daily Climate Change dataset (SciDCC), consisting of 11,000 news articles grouped into 20 climate-related categories. This dataset offers another avenue for identifying greenwashing, as companies’ communications can be cross-referenced with external news coverage, potentially highlighting contradictions or omissions in corporate reports.
%It is built by first filtering nature-related sentences using keywords and GPT-3.5, then by giving a final human annotation. 

% \fms{all of these datasets are not actually climate related, no? They rather talk about nature, no? If so, we should reframe (and relabel) this task...}

\subsubsection{Models and Conclusion}

\solutions The most common solution proposed for these tasks is fine-tuning a Transformer model~\cite{huangFinBERTLargeLanguage2020, schimanski_bridging_2023, LEE2023119726, bingler_cheap_2021, sampson_tcfd-nlp_nodate, vaid-etal-2022-towards, spokoyny2023answering, Schimanski2024nature, jain_supply_2023}. To contextualize the performance of Transformer models, some studies also report performances of classical baselines~\cite{huangFinBERTLargeLanguage2020, spokoyny2023answering, Schimanski2024nature, jain_supply_2023, bingler_cheap_2021, sampson_tcfd-nlp_nodate}. \citet{bingler_cheap_2021} proposed a custom approach combining logistic regression with features from a fine-tuned language model, while \citet{sampson_tcfd-nlp_nodate} also evaluated clustering techniques and stacked models. The performance of the best-performing model is systematically high (above 80\%), except on SciDCC and ClimaTOPIC. %\fms{Be clearer here: classical models usually 20\% worse than transformers, except on dataset 1 and dataset 2. The reason ...}\tom{TO BE DISCUSSED} 
Classical approaches (SVM, NB) also reach good performances, under-performing by less than 20\% on all datasets (except on \citet{bingler_cheap_2021}'s dataset). The TF-IDF baseline even outperformed fine-tuned Transformers on \citet{sampson_tcfd-nlp_nodate}'s dataset. This shows that those topics have distinguishable vocabularies. 

\paragraph{Automatic Labels} The performance gap observed in \citet{bingler_cheap_2021} may be attributed to using paragraph-level inputs rather than sentence-level inputs aggregated for classification, as fine-tuned models also achieve around 20\% precision at the paragraph level.
The lower performances on SciDCC\cite{mishra2021neuralnere} and ClimaTOPIC\cite{spokoyny2023answering} are likely caused by the automation of the labeling process. The labels are therefore not designed to be labels. In SciDCC there are labels that are highly similar (e.g. \textit{Endangered Animals} and \textit{Extinctions}), or labels that include other labels (e.g. \textit{Environment}, \textit{Climate}, \textit{Pollution}). As they are categories from a journal, the categories changed in time. For ClimaTOPIC, the labels are question categories, which are designed to group questions not to precisely identify them. Therefore a question about "the emission of a building" might fit in either \textit{Emissions} or \textit{Building}, yet it is assigned only one label. 

\paragraph{Insights} Topic classification is a well-established area of research in natural language processing (NLP) and has been extensively applied to climate-related topics. Fine-tuned Transformer models proposed in the literature consistently demonstrate near-perfect performance on human-annotated datasets, indicating that the task is largely solved. When reported, classical keyword baselines also perform relatively well, achieving significantly better than random performance and approximately 80\%  of the performance of fine-tuned models, suggesting the topics have distinct vocabularies. Future research could investigate topics that are lexically similar but distinct in their subject or focus.
Topic detection serves as a valuable tool for structuring documents and has been utilized to analyze the extent to which companies address specific topics, enabling the detection of selective disclosure \cite{bingler2023cheaptalkspecificitysentiment, bingler_cheap_2021}. However, it is critical to ensure that labels are well-defined and data is accurately annotated. 
Lower performance observed on automatically generated labels may reflect challenges in predictability, potentially due to difficulties caused by the automatic annotation. These works might be revisited to conduct human annotations. 

\subsection{In-depth Disclosure: Climate Risk Classification}
\label{sec: climate risk}

%citations
%direct : \citet{liCorporateClimateRisk2020}, \citet{kheradmand2021a}, \citet{chou_ESG}, \citet{SAUTNER_cliamte_change_exp},\cite{marco_polignano_nlp_2022, hyewon_kang_analyzing_2022},  \citet{kolbel_ask_2021} , \citet{Friederich_climate_risk_disclosure}, \citet{bingler2023cheaptalkspecificitysentiment},  \citet{xiang_dare_2023} \cite{nicolas_webersinke_climatebert_2021} 

%Greenwashing can take many forms, but two common examples include presenting overly positive information about a company’s environmental impact and downplaying or omitting negative aspects of its environmental performance. 
Climate change can bring both risks and opportunities for companies. Potential risks are, e.g., reputational risks (e.g. environmental controversies), regulatory risks (e.g. new regulations on emissions), and physical risks (e.g. droughts impacting production).  Opportunities are, e.g., financial opportunities (e.g. benefiting from grants that aim to support less polluting industries), market opportunities (e.g. electric cars becoming more popular with environmentally conscious clients), etc. If a company systematically avoids discussing climate-related risks or disproportionately emphasizes opportunities, it creates a biased narrative that can serve as an indicator of greenwashing.

\task Given an input sentence or a paragraph, output ``opportunity'' or ``risk'' label. Some works focus only on risks, classifying them into types of risks (e.g., physical risk, reputational risk, regulatory risk, or transition risk)

\begin{table}[ht]
\centering
\begin{tabular}{p{2cm}p{4cm}p{3cm}p{4cm}}
\toprule
\textbf{Dataset} & \textbf{Input}   &  \textbf{Labels}                                                                                  &                                       \\
\midrule
Ask BERT's Climate Risk \cite{kolbel_ask_2021} & Sentences from TCFD's example reports and non-climate-related sentences &  \multicolumn{2}{p{8cm}}{Risk type: \textit{Transition risk, physical risk, and general risk (no guidelines)}}             \\ \midrule
Climate Risk \cite{Friederich_climate_risk_disclosure} & Paragraphs from European companies annual reports & \multicolumn{2}{p{8cm}}{Risk type:\textit{Acute, Chronic, Policy \& legal, Tech \& Market, Reputational, and Negative} (no guidelines)}       \\ \midrule
ClimateBERT's Sentiment \cite{bingler2023cheaptalkspecificitysentiment}  & Paragraphs from companies' annual reports            & \multicolumn{2}{p{8cm}}{\textit{Risk} or threat that negatively impacts an entity of interest (negative sentiment); or \textit{Opportunity} arising due to climate change (positive sentiment); \textit{Neutral} otherwise.} \\ \midrule
Sentiment Analysis \cite{xiang_dare_2023} & paragraphs from academic texts on climate change and health published between 2013 and 2020  & \multicolumn{2}{p{8cm}}{\textit{Risk} (negative) if it discusses climate change causing public health issues, serious consequences, or worsening trends, greenwashing. \textit{Opportunity} (positive): highlights potential benefits, positive actions, or research addressing gaps. \textit{Neutral} otherwise.} \\
\bottomrule
\end{tabular}
\caption{Summary of datasets, labels, scopes, and sources for climate-related Risk classification.}
\label{tab:datasets_risk}
\end{table}


% no dataset
\zeroshot Similarly to climate-related detection, climate risk classification has been tackled with keyword-based approaches. \citet{liCorporateClimateRisk2020}, \citet{kheradmand2021a} and \citet{chou_ESG} proposed using dictionaries of words related to climate risk to identify paragraphs dealing with climate risk. \citet{liCorporateClimateRisk2020} constructed risk measures based on the frequency of the terms in the risk dictionaries. \citet{chou_ESG} analyzed the topics mentioned in conjunction with physical and transition risks. \citet{SAUTNER_cliamte_change_exp} proposed using one dictionary for risk and one for opportunity classification.

\datasets Several studies have introduced specialized annotated datasets for risk/opportunity classification. For instance, \citet{kolbel_ask_2021} created a dataset for climate-related risk classification with three categories: physical risk, transition risk, and general risk, using active learning for annotation. Similarly, \citet{Friederich_climate_risk_disclosure} developed an annotated dataset for risk classification with five labels, covering acute physical risk, chronic physical risk, policy and legal risks, technology and market risks, and reputational transition risks. \citet{bingler2023cheaptalkspecificitysentiment} released a dataset as part of the ClimateBERT downstream tasks, focusing on classifying paragraphs from corporate reports into three categories: opportunity, neutral, or risk. Extending beyond corporate disclosures, \citet{xiang_dare_2023} compiled a climate-related risk/opportunity classification dataset of academic texts from the Web of Science and Scopus.

\solutions The solutions proposed are similar to the other tasks: fine-tuned Transformer models \cite{hyewon_kang_analyzing_2022, kolbel_ask_2021, Friederich_climate_risk_disclosure, bingler2023cheaptalkspecificitysentiment, nicolas_webersinke_climatebert_2021, xiang_dare_2023} and keyword-based-features with simple models \cite{kolbel_ask_2021, Friederich_climate_risk_disclosure, bingler2023cheaptalkspecificitysentiment} (see Table \ref{tab:models} in the appendix for details). The notable exception is \citet{xiang_dare_2023}, who evaluated LSTM-based solutions alongside the Transformers models. Most studies~\cite{hyewon_kang_analyzing_2022, kolbel_ask_2021, bingler2023cheaptalkspecificitysentiment, nicolas_webersinke_climatebert_2021, xiang_dare_2023} reported high performances, above 80\%. \citet{kolbel_ask_2021} and \citet{bingler2023cheaptalkspecificitysentiment} also reported good performances for the keyword-based baselines (72\% and 84\%). The performance reported by \citet{Friederich_climate_risk_disclosure} on climate risk tasks highlights distinct challenges across subtasks. When tasked with identifying the specific type of risk in sentences already known to be about climate risks, word-based models outperformed fine-tuned Transformers. This suggests that each risk type had a distinct vocabulary. In contrast, when working with a dataset that reflects real-world proportions of risk and non-risk examples—where the data is heavily imbalanced—classical models struggled due to the overlap in vocabulary between general and climate risk-related text. 

\paragraph{Difference between sentiment and risk} It is important to distinguish risk/opportunity classification from sentiment analysis. While risk/opportunity classification is inspired by sentiment analysis \cite{bingler2023cheaptalkspecificitysentiment}, they play a complementary role. A statement such as \textit{``With proactive climate risk management, we are ready to tackle extreme weather disruptions, ensuring resilience''} is classified as positive using a sentiment analysis model \cite{perez2021pysentimiento}, and as mentioning a risk using a risk/opportunity classification model \cite{bingler2023cheaptalkspecificitysentiment}. Sentiment analysis focuses on the form of the statement, while risk is about the content. As the literature on sentiment analysis \cite{Wankhade2022} is broad, the analysis of the tone of the statement can be done using existing models \cite{marco_polignano_nlp_2022, hyewon_kang_analyzing_2022} complementing approaches on risk classification. 

\paragraph{Insights} The performance of fine-tuned models might indicate that the task of identifying texts talking about risks is solved. However, the work by \citet{Friederich_climate_risk_disclosure} shows limitations when experimenting with heavily imbalanced datasets, which actually correspond to real-world settings. Future research could focus on such skewed settings.


