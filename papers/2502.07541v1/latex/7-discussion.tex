\section{Open Challenges in Greenwashing Detection}
\label{sec:challenges}

We discuss open challenges in the evaluation (Section~\ref{sec:eval difficulty}), model robustness (Section~\ref{sec:robustness}), and data sources (Section~\ref{sec:data}).

\subsection{Evaluation Methodology}
\label{sec:eval difficulty}

% Fabian: this does not really fit, as the evaluation problems exist also on the classification level...
%As models improve, they are able to tackle more complex tasks: moving from classification to text generation, and from extraction to reasoning. This increasing complexity goes along with a more complex evaluation.

% \fms{The following paragraph does not sound like a particularly challenging issue. There is one dataset with a hierarchical classification, and we have to measure the supercategory in addition to the fine-grained category. We can keep it, of course, but it seems too restricted in scope, and too trivial as a problem to be mentioned in the grand conclusions of a 30 page survey... Unless its a systematic problem with more instances of course.}
% \paragraph{Classification Evaluation} Many tasks in greenwashing detection are framed as classification tasks. In some cases, this framing is too simplistic, in that it is blind to the semantic similarity between the classes. For example, in the Contrarian Claims dataset~\cite{coanComputerassistedClassificationContrarian2021}, models may fail on the fine-grained classification, but still get the super-category right. %it is likely that the correct and the wrong labels that are part of the same super-category.
% %\oana{why do you use "while" here? so they fail on both super category and sub category, why is this surprising?}\tom{Same super category not =/= categories}. I see, but it seems intuitive, the classes are probably more similar when in the same super category.
% For such cases, \citet{morio2023an} proposed  strict and relaxed metrics, where the relaxed metric grouped similar labels together (e.g. ``strongly supporting'' and ``supporting'' grouped in ``supporting'' label). 
% %This helps understand how the models actually perform.% and mitigates the weakness of the datasets. 
% %We believe it is essential to provide more thorough metrics to better evaluate AI alignment with human evaluation.

\paragraph{Ambiguous Labels} Many tasks in greenwashing detection are framed as classification tasks. However, in some cases, the labels are not sufficiently distinguishable.
%One particular issue is that the main hypothesis of the classification task is that the categories are distinguishable one from another. This is not always the case. 
For example, specificity classification \cite{bingler2023cheaptalkspecificitysentiment} is the task of distinguishing specific claims from unspecific claims. It turns out that this task is itself too unspecific, % Fabian: small joke, feel free to remove...
as the low inter-annotator agreement shows. However, even for tasks that seem easy to distinguish, some edge cases remain ambiguous, e.g., the distinction between ``climate-related'' and ``not climate-related''. The climate detection model climateBERT~\cite{bingler2023cheaptalkspecificitysentiment} classifies the sentence ``This year, we are producing more than 10,000 [bicycle/electric bicycle], demonstrating our continued growth and commitment to meeting market demand.'' as ``climate-related'' for bicycle but not for electric bicycle. While electric bicycles are often associated with solutions to reduce emissions, the statement is not explicitly making the link. If we consider indirect links, then non-electric bicycles are also solutions. There lies the question of which product is inherently linked to climate-change, and which one requires contextualization. 

% ``This year, we are producing more than 10,000 [vehicles/electric vehicules/diesel vehicules], demonstrating our continued growth and commitment to meeting market demand.'' was classifed ``not climate-related'' for ``vehicles'' and ``climate-related'' for ``diesel vehicules'' and ``electric vehicules''

%}" are respectively predicted "non-climate" and "climate" by . In this example, the sentences are fundamentally similar. If the company's business model is to produce solar panels, then this statement is not necessarily about climate, at least not more than the other one.
When labels in classification tasks are not clearly distinguishable, the task is not well-defined. Consequently, it does not mean much if models perform well on such a task. It merely means that the model has learned whatever label distribution the dataset had. Work in this area should thus always first check human inter-annotator agreement on the task to make sure it is meaningful.

\paragraph{Generated Text Evaluation} Some tasks are not classification tasks, but text generation tasks. Evaluating generated text is a challenging task~\cite{survey_nlg_eval}, and different techniques are being used: 
%We argue that evaluating generated text is still a challenge deserving future research. 
%In the papers we considered for this review, to evaluate generative models, we came across solutions such as using 
BLEU, METEOR, human-based evaluation, and LLM-based evaluation~\cite{mullappilly-etal-2023-arabic, cliamtebot_2022, s_vaghefi_chatclimate_2023, thulke2024climategpt}. BLEU and METEOR are not well-suited for evaluating tasks such as question answering ~\cite{chen-etal-2019-evaluating}, as they rely only on n-gram similarity.
Human evaluation also has its shortcomings. First, it is limited to smaller sizes and variations in settings. Second, human evaluation of generated texts should be based on explicit guidelines~\cite{thulke2024climategpt,bulian2024assessinglargelanguagemodels}, but sometimes resorts to comparisons~\cite{mullappilly-etal-2023-arabic, thulke2024climategpt} or simple user preferences~\cite{mullappilly-etal-2023-arabic}.
Third, human evaluation can be influenced by factors other than the quality of the answer -- for example, the observer-expectancy effect \cite{rosenthal1976}.
For instance, in ChatClimate~\cite{s_vaghefi_chatclimate_2023}, humans evaluate the accuracy of text generated by a generic ChatGPT vs. the proposed ChatClimate chatbot. The text produced by the ChatClimate bot always gives specific references (down to page numbers), while the generic ChatGPT does not. This allows evaluators to easily identify the more precise answer based solely on the type of references used, raising the concern that this criterion may have disproportionately influenced assessments, potentially overlooking broader aspects such as overall accuracy or quality. 
LLM-based evaluation has its own challenges: Language models are known to reproduce human biases~\cite{gallegos2024biasfairnesslargelanguage}. They are sensitive to the way questions are formulated, to the order of answers in multiple-choice questions~\cite{pezeshkpour-hruschka-2024-large}, and to cues that can influence human evaluation such as using ``Our Model''/``Their model'' (which is the wording used in \citet{mullappilly-etal-2023-arabic}).
LLM-based evaluation should thus always adhere to the same rigorous standards that are applied to human evaluation.
% Fabian: too much colorful language here... I think we said the main points
%Proper control of the evaluation setting is crucial. Subtle factors, such as the specific wording used or the unintentional leakage of task-related information, can bias the results. Therefore, strict protocols and a meticulously designed evaluation environment are essential to mitigate these risks.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/model_list_upset.png}
    \caption{Upset plot~\cite{upset_plot} of the methods used in the studies mentioned in this literature review. Each bar counts the number of studies reporting a given set of methods. The number of a papers using a given method is reported on the horizontal bars. We have highlighted in red the intersection sets that include both fine-tuned Transformers and keyword-based methods.}
    \label{fig:models}
\end{figure}

\paragraph{Baselines} Figure~\ref{fig:models} shows the approaches that are commonly used in greenwashing detection tasks (more details in Table~\ref{tab:models}). We see that the most common solution is fine-tuning transformer models; more recent approaches include using LLMs as few-shot learners. The figure also shows which baselines have been used:
%. However, when evaluating, the state-of-the-art models are compared to baselines. We argue that some basic baselines are often forgotten, even though they provide important insights:
\begin{itemize}
    \item \textit{Random/Majority Baseline:} Any proposed approach should beat a random baseline. However, we find that only 9\% of papers we studied compare their results to the random or majority baseline. For example, \citet{Friederich_climate_risk_disclosure} reported performances under 50\% for a binary classification task, which is worse than a theoretical random classifier on a balanced dataset. Their dataset is not balanced, but it is difficult to draw conclusions without the performance of a random classifier. 
    Random classifiers cannot just suggest that a model underperforms, but also that a model has a comparatively good performance. For example, on SciDCC or ClimaTOPIC, the performance of models is around 50-60\%. While this looks mediocre, it is in fact an excellent performance as the random classifier performed around 5-7\% \cite{spokoyny2023answering}. In case of a heavily imbalanced dataset, the majority baseline is more meaningful. If such a baseline is not provided, but the dataset is available, it can even be computed a posteriori. 
    %\fms{one would expect here (1) an approach that has been successfully compared to such a baseline, or (2) an approach where such a baseline is clearly lacking. The following sentence does not fit this expectation: }If the dataset is not available, it is not possible to estimate that value, which is the case for 
    Unfortunately, neither is the case for the work of \citet{Friederich_climate_risk_disclosure}. Similarly, a constant value baseline can help understand the performance of a regression model. \citet{Greenscreen} and \citet{bronzini_glitter_2023} evaluated the prediction of the ESG score, but do not provide such a baseline. Thus, the reader is left to wonder whether %If the value are really concentrated, 
    a model that always predicts the mean value might not actually perform better. 
    %might have an good performance. Unfortunately, without the distribution of values it is not possible to estimate it. 
    \item \textit{Simple Baselines:} If a keyword-based baseline performs well, or even better than a proposed method, then the proposed method is likely an overkill. For example,
    \citet{rouenEvolutionESGReports2023, clarkson_nlp_us_csr} showed that even for the prediction of an ESG score (which is a complex target), linguistic features such as the quantity and complexity of text are good predictors. This suggests that simple predictors could perform well even on complex tasks. Indeed, simple baselines can outperform finetuned approaches, for example on TCFD classification \cite{sampson_tcfd-nlp_nodate} or on the ClimaINS dataset \cite{spokoyny2023answering}.
    Beyond a validation (or invalidation) of the proposed approaches, a keyword-based baseline would also allow us to understand if models rely on an ``understanding'' of the text or merely on word cues.      And yet, less than half of the works we studied report the performance of a keyword-based baseline.
    \item \textit{Human Baselines:} Using a human as a baseline can help determine whether a task is well-defined. If even a qualified human cannot achieve high accuracy, this most likely means that the task is subjective, ill-defined, or in disagreement with the dataset. Vice versa, if a human achieves high accuracy, but the model achieves even higher accuracy, this could mean that the model has acquired expert knowledge. However, with the exception of 3 studies, all studies discussed here not come with a human evaluation.
\end{itemize}

\subsection{Model Robustness}
\label{sec:robustness}

When evaluating a model, it is important to know whether the performance on a dataset is due to an inherent capacity of the model or due to random factors that may be entirely different on another dataset.

\paragraph{Model Performance Evaluation} Many factors can influence the performance of a model (e.g. data sampling, hyper-parameters, random initialization, model architecture). Thus, a better performance in an experiment could be due also to these factors rather than due to the choice of the model. This is even more true when studies report differences in performance of less than 1\%. 
%This difference might be due to the choice of model, but it could also be due to the other factors.
For this reason, the uncertainty of the evaluation should always be quantified -- 
%Quantifying the uncertainty induced by those factors helps understand if a difference is significant. This could be done using 
by confidence intervals, p-values or at least the standard deviation.
However, we found that only seven papers, out of the 61 we studied, quantify the uncertainty of the model performance~\cite{nicolas_webersinke_climatebert_2021,varini_climatext_2020,bingler2023cheaptalkspecificitysentiment,Schimanski2024nature,schimanski_bridging_2023,LEE2023119726,tobias_schimanski_climatebert-netzero_2023}. 

\paragraph{Noisy Text and Adversarial Attacks} Most studies mentioned in this survey trained and evaluated models on datasets that contain only clean examples. However, real-world data is usually noisy. Thus, the models may perform entirely differently on real-world datasets. Studies should quantify not just the performance of the models, but also their robustness, as fine-tuned models are prone to word-based adversarial attacks~\cite{vinicius_woloszyn_towards_2021}.
%, as the training data are often relatively small, and might not include syntactically diverse examples. 
%Furthermore, Noisy texts are an inherent part of real-world applications. Yet,  and  did not quantify the robustness of their approaches to noise. 

\paragraph{Imbalanced Datasets} The class of interest is relatively rare in most tasks. For example, only 5\% of text is in the positive class ``climate-related'' in the study of~\citet{yu_climatebug_2024}. This means that (1) a simple baseline that always says ``non-climate-related'' will have a very high precision, and (2) a model trained on such a dataset would probably overfit. Therefore, most studies mentioned in this review trained and evaluated the models on relatively balanced datasets. 
On the other hand, a model evaluated only on a balanced dataset may fail when tested on real-world, unbalanced datasets~\cite{Friederich_climate_risk_disclosure}. For example, a model reaching an F1-score of 90\% on a balanced dataset would significantly drop on an unbalanced dataset with 5\% of text in the positive class (to a binary F1-score of 47\%, and a macro F1-score of 71\%). This is due to the large number of false positives. Thus, studies should consider both the balanced and the unbalanced case.

\paragraph{Precision-Recall Trade-off} In most methods (and most tasks), higher precision can be traded for smaller recall. In the case of greenwashing, both metrics can be useful:
%Annotating data is a time-consuming and resource-intensive process for tasks related to greenwashing detection, often resulting in an imbalanced dataset that fails to adequately represent all relevant cases. This imbalance complicates the model's performance trade-off: 
a higher recall is desirable when providing leads for human analysis, so as to avoid missing potential greenwashing cases. However, if the goal is to accurately measure greenwashing prevalence, minimizing false positives is critical, as false accusations can have serious repercussions. Most studies, however, are conducted without the use case in mind.

\subsection{Data Sources}
\label{sec:data}

\paragraph{Close-source data and reproducibility} The studies mentioned in our review use various data sources, most of which are publicly available: annual and sustainable reports ~\cite{doran_risk_disclosure, kolbel_ask_2021, chou_ESG, huangFinBERTLargeLanguage2020, krausEnhancingLargeLanguage2023, kheradmand2021a, rouenEvolutionESGReports2023, lai_using_2023, hyewon_kang_analyzing_2022, auzepy_evaluating_2023, bjarne_brie_mandatory_2022, nicolas_webersinke_climatebert_2021, luccioni_analyzing_2020, kheradmand2021a, bronzini_glitter_2023, LEE2023119726, marco_polignano_nlp_2022, krausEnhancingLargeLanguage2023}, social media~\cite{vinicius_woloszyn_towards_2021, luo_detecting_2020, vaid-etal-2022-towards, coanComputerassistedClassificationContrarian2021, divinus_oppong-tawiah_corporate_2023, Greenscreen}, specialized websites~\cite{morio2023an, tobias_schimanski_climatebert-netzero_2023, jin-etal-2022-logical, varini_climatext_2020, coanComputerassistedClassificationContrarian2021}. However, multiple works rely on closed-source data: ESG scores, carbon emissions, and machine-readable reports that are collected through \textit{private providers} such as Refinitiv, Reuters, RavenPack, Trucost, Bloomberg, RobecoSAM, Compustat or Wharton~\cite{SAUTNER_cliamte_change_exp, bingler2023cheaptalkspecificitysentiment, stammbach_environmental_2023, Friederich_climate_risk_disclosure, bjarne_brie_mandatory_2022, LEE_greenwashing, nicolas_webersinke_climatebert_2021, liCorporateClimateRisk2020, clarkson_nlp_us_csr, liCorporateClimateRisk2020, SAUTNER_cliamte_change_exp, Mehra_2022, Greenscreen, clarkson_nlp_us_csr, schimanski_bridging_2023, avalon_vinella_leveraging_2023, krausEnhancingLargeLanguage2023}. Relying on private providers limits the reproducibility of these works, for both academic work and applications.

\paragraph{No greenwashing datasets} The most fundamental shortcoming of current work on greenwashing is that there is no comprehensive real-world dataset with instances of greenwashing. This limits the conclusions that can be drawn from current studies drastically. The lack of a gold standard dataset can be explained by several factors: 
\begin{itemize}
    \item The definitions of greenwashing are often vague and ambiguous, leaving room for interpretation, and making it difficult to annotate examples of greenwashing.
    \item Green claims are often not self-contained, but spread %rather meaning that the full content of information required to judge if a claim is misleading or not is spread: 
    across multiple paragraphs in a same document, across multiple documents, or even across different types of sources (such as tables, databases, etc.). Therefore, just collecting all the elements required to correctly identify greenwashing is a challenge on its own.
    \item Understanding the claim and annotating greenwashing requires expertise, and sometimes even debate, hence producing greenwashing annotations is a difficult task.
    %to make sure the communication is misleading, making the annotation task even harder.
    \item Greenwashing is a strong accusation that might impact a company reputation, and that can also, if not substantiated by evidence, be considered a defamation.
\end{itemize}

To build a dataset of real-world greenwashing cases, several data collection strategies can be considered. To avoid relying on a subjective annotation, researchers can rely on a third-party judgment. For example, using reputable news articles that explicitly discuss greenwashing to find actual examples, drawing on prior research focused on detecting media mentions of greenwashing~\cite{gourierGreenwashingIndex2024}. Another valuable resource are regulatory records from authorities such as the Advertising Standards Authority (ASA) or databases like \url{greenwash.com}.


\paragraph{Missing link to regulatory texts} Existing work often relies on its own definitions of greenwashing. This risks introducing subjectivity and incompatibility with other works. A safer choice is to use 
%As mentioned above, two of the challenges in building a dataset of examples of greenwashing are using definitions that are subjective and mitigating risks of defamation. Using 
regulatory texts as a reference, %helps mitigate those two issues. Firstly, it can help build a proper definition of the label. 
as this is done, e.g., in \citet{stammbach_environmental_2023}.
%rely on the definition of green claims by the European Commission to build their definition of environmental claims. Secondly, 
Recently, the European Commission adopted the corporate sustainability reporting directive (CSRD) \cite{csrd2022}, making environmental disclosure requirements explicit, and the directive on green claims \cite{eugreenclaims}, which defines requirements when making green claims. These could be good starting points for defining greenwashing in future research.
%Future research could start by finding content related to parts of the regulatory texts related to greenwashing. 
Such regulations help not just with the definition of greenwashing, but also with its judgment. Instead of emitting a judgment, a method can link a statement to a regulatory text. For example, the works that predict TCFD simply link a statement to a recommendation \cite{dingCarbonEmissionsTCFD2023, auzepy_evaluating_2023, bjarne_brie_mandatory_2022, bingler_cheap_2021, sampson_tcfd-nlp_nodate}. This approach makes the methods less prone to ad hoc definitions of scores and judgment strategies, and lets a human take the final decision. 

\paragraph{Quantifying communication vs Quantifying information} With the exception of \citet{bronzini_glitter_2023}, the studies reviewed in this work all measure how often a given text contains climate-related content, TCFD disclosures, or specific topics of interest. However, a company can repeatedly emphasize the same objective or repeatedly highlight a single initiative. With this, they skew the quantity of climate-related content in their favor, while they may still fail to implement any meaningful action. This phenomenon has been called  ``cheap talk'' \cite{bingler2023cheaptalkspecificitysentiment}.
%creating the illusion of widespread organizational change, 
%as highlighted by , companies increasingly engage in "cheap talk," where statements lack substantive commitments or actionable details. This underscores the need to assess not only the volume of climate-related communication but also the informational content conveyed. For instance, 
Future research should thus quantify not only how often some pieces of information are conveyed, but also how many different pieces there are. 
