\section{TCFD recommendations}
\label{app:tcfd}
The recommendations are directly taken from the TCFD website: \url{https://www.fsb-tcfd.org/recommendations/}

\paragraph{Governance}Disclose the organization’s governance around climate-related risks and opportunities.
\begin{itemize}
    \item Describe the board’s oversight of climate-related risks and opportunities.
    \item Describe management’s role in assessing and managing climate-related risks and opportunities.
\end{itemize}
 
\paragraph{Strategy} Disclose the actual and potential impacts of climate-related risks and opportunities on the organization’s businesses, strategy, and financial planning where such information is material.
\begin{itemize}
    \item Describe the climate-related risks and opportunities the organization has identified over the short, medium, and long term.
    \item Describe the impact of climate-related risks and opportunities on the organization’s businesses, strategy, and financial planning.
    \item Describe the resilience of the organization’s strategy, taking into consideration different climate-related scenarios, including a 2°C or lower scenario.
\end{itemize}

\paragraph{Risk Management} Disclose how the organization identifies, assesses, and manages climate-related risks.
\begin{itemize}
    \item Describe the organization’s processes for identifying and assessing climate-related risks.
    \item  Describe the organization’s processes for managing climate-related risks.
    \item Describe how processes for identifying, assessing, and managing climate-related risks are integrated into the organization’s overall risk management.
\end{itemize}

\paragraph{Metrics and Targets} Disclose the metrics and targets used to assess and manage relevant climate-related risks and opportunities where such information is material.
\begin{itemize}
    \item Disclose the metrics used by the organization to assess climate-related risks and opportunities in line with its strategy and risk management process.
    \item  Disclose Scope 1, Scope 2 and, if appropriate, Scope 3 greenhouse gas (GHG) emissions and the related risks.
    \item  Describe the targets used by the organization to manage climate-related risks and opportunities and performance against targets.
\end{itemize}


\subsection{Data sources}
\label{app:data}

\begin{table}[ht]
    \centering
    \begin{tabular}{ll}
    \toprule
    Source & Papers \\
    \midrule
        10-K reports & \cite{doran_risk_disclosure, kolbel_ask_2021, chou_ESG, huangFinBERTLargeLanguage2020, krausEnhancingLargeLanguage2023} \\
        Manually collected reports & \cite{kheradmand2021a, rouenEvolutionESGReports2023, lai_using_2023, hyewon_kang_analyzing_2022, auzepy_evaluating_2023, bjarne_brie_mandatory_2022, nicolas_webersinke_climatebert_2021, luccioni_analyzing_2020, kheradmand2021a, bronzini_glitter_2023, LEE2023119726, marco_polignano_nlp_2022, krausEnhancingLargeLanguage2023} \\ 
        Refinitiv & \cite{SAUTNER_cliamte_change_exp, bingler2023cheaptalkspecificitysentiment, stammbach_environmental_2023, Friederich_climate_risk_disclosure, bjarne_brie_mandatory_2022, LEE_greenwashing, nicolas_webersinke_climatebert_2021} \\
        Reuters & \cite{liCorporateClimateRisk2020, clarkson_nlp_us_csr}\\
        ESG score private providers & \cite{liCorporateClimateRisk2020, SAUTNER_cliamte_change_exp, Mehra_2022, Greenscreen, clarkson_nlp_us_csr, schimanski_bridging_2023} \\
        CDP & \cite{spokoyny2023answering} \\
        Open for Good Data & \cite{avalon_vinella_leveraging_2023} \\
        ClimateWatch & \cite{krausEnhancingLargeLanguage2023} \\
        Twitter/Reddit & \cite{vinicius_woloszyn_towards_2021, luo_detecting_2020, vaid-etal-2022-towards, coanComputerassistedClassificationContrarian2021, divinus_oppong-tawiah_corporate_2023, Greenscreen} \\
        LobbyMap & \cite{morio2023an}\\
        Net-zero tracker & \cite{tobias_schimanski_climatebert-netzero_2023}\\
        Fact-checking websites & \cite{jin-etal-2022-logical, varini_climatext_2020, coanComputerassistedClassificationContrarian2021} \\
        \bottomrule
    \end{tabular}
    \caption{Data Sources}
    \label{tab:data sources}
\end{table}

\paragraph{Publicly available Reports} Many studies rely on 10-K reports~\cite{doran_risk_disclosure, kolbel_ask_2021, chou_ESG, huangFinBERTLargeLanguage2020, krausEnhancingLargeLanguage2023} as they are easily accessible (\href{https://www.sec.gov/edgar}{EDGAR}), uniformly structured, and mandatory for companies listed on US financial markets. Other reports, such as sustainable reports, are not centralized and mandatory, therefore they need to be collected from other sources. They are often collected manually from the company's website~\cite{kheradmand2021a, rouenEvolutionESGReports2023, lai_using_2023, hyewon_kang_analyzing_2022, auzepy_evaluating_2023, bjarne_brie_mandatory_2022, nicolas_webersinke_climatebert_2021}, or from other websites~\cite{luccioni_analyzing_2020, kheradmand2021a, bronzini_glitter_2023, LEE2023119726, marco_polignano_nlp_2022, krausEnhancingLargeLanguage2023} such as from the \href{https://www.globalreporting.org/how-to-use-the-gri-standards/register-your-report/}{GRI}, \href{https://sasb.ifrs.org/company-use/sasb-reporters/}{SASB}, \href{https://www.tcfdhub.org/reports}{TCFD}, \href{https://www.responsibilityreports.com/}{ResponsibilityReports}, \href{https://www.annualreports.com/}{AnnualReports}, \href{https://www.ksa.or.kr}{Korean Standards Association}. \citet{marco_polignano_nlp_2022} relied on Sustainability Disclosure Database from GRI. To the best of our knowledge, it is not available any longer. 

The reports are openly available, however they are most of the time in a PDF format, and must be parsed to produce machine-readable versions. While~\citet{financial-reports-sec} proposed a machine-readable dataset for the 10-K reports, it is often not the case for other types of reports. \cite{bronzini_glitter_2023, marco_polignano_nlp_2022, hyewon_kang_analyzing_2022} provide a sample of processed reports. 

\paragraph{Private Providers} Machine-readable reports, as well as earning call transcripts are often provided through a private data provider such as Refinitiv~\cite{SAUTNER_cliamte_change_exp, bingler2023cheaptalkspecificitysentiment, stammbach_environmental_2023, Friederich_climate_risk_disclosure, bjarne_brie_mandatory_2022, LEE_greenwashing, nicolas_webersinke_climatebert_2021} or Reuters~\cite{liCorporateClimateRisk2020, clarkson_nlp_us_csr}. Similarly, financial data (e.g. ESG score) is often collected through data providers such as RavenPack or S\&P Global Trucost or Wharton’s research platform WRDS, Reuters, Bloomberg, Refinitiv, RobecoSAM and Compustat~\cite{liCorporateClimateRisk2020, SAUTNER_cliamte_change_exp, Mehra_2022, Greenscreen, clarkson_nlp_us_csr, schimanski_bridging_2023}
%Similarly, financial data (e.g. ESG score) is often collected through data providers such as RavenPack \citet{liCorporateClimateRisk2020} or S\&P Global Trucost \citet{SAUTNER_cliamte_change_exp} or Wharton’s research platform WRDS \cite{Mehra_2022, Greenscreen} or Reuters \citet{clarkson_nlp_us_csr}, Bloomberg, Refinitiv, RobecoSAM and Compustat \cite{schimanski_bridging_2023}

\paragraph{Carbon emission data} Carbon emissions of companies can be collected through the private data providers, or through \href{https://www.cdp.net/en/}{CDP} \cite{spokoyny2023answering}, \href{https://www.anderson.ucla.edu/about/centers/impactanderson/open-for-good-transparency-index}{Open for Good Data}~\cite{avalon_vinella_leveraging_2023} or \href{https://www.climatewatchdata.org/}{ClimateWatch website}~\cite{krausEnhancingLargeLanguage2023}.

% \citet{avalon_vinella_leveraging_2023}  for scope 1,2,3 emissions.

\paragraph{Media and Social Media} As companies communicate with the public mainly through marketing and social media, other data sources include Twitter and Reddit~\cite{vinicius_woloszyn_towards_2021, luo_detecting_2020, vaid-etal-2022-towards, coanComputerassistedClassificationContrarian2021, divinus_oppong-tawiah_corporate_2023, Greenscreen}.

%Reddit Climate change dataset contains posts that are related to climate from \href{https://socialgrep.com/datasets/the-reddit-climate-change-dataset}{Socialgrep}.

\paragraph{Climate expert websites} Finally, projects such as \href{https://lobbymap.org/}{LobbyMap} or \href{https://zerotracker.net/}{Net Zero Tracker}, that track corporate policy and engagements, can help build annotated datasets~\cite{morio2023an, tobias_schimanski_climatebert-netzero_2023}. Similarly, relying on a fact-checking website such as \url{https://climatefeedback.org/} or \url{www.skepticalscience.com} is an efficient way of building a dataset~\cite{jin-etal-2022-logical, varini_climatext_2020, coanComputerassistedClassificationContrarian2021}


\newpage

\section{Detailed model list}

\input{tables/models_table}

\newpage

\section{Reported Performances}\label{app:perf}

We have reported all the performances computed in each study when available. We did not include the performance table if the original study did not perform an evaluation (for example, in zero-short/unsupervised settings) or if the evaluation is a heavily modified metric (for example, when using a quantitative measure alongside other signals to compute a financial factor). We only reported traditional NLP metrics. When available, we reported the F1-score, however, if the F1-score is not computed in the study, we reported the available metrics. The computation methods of the F1-score (macro, micro, weighted) are specified when enough information is given to state it confidently. We reported one table per dataset, which might contain multiple tasks, but multiple studies can tackle the same dataset and are then grouped to be able to compare the results. 

\subsection{Climate-related Topic Classification}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.37\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{p{2cm}lc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\
            \midrule
            \citet{nicolas_webersinke_climatebert_2021} & distilRoBERTa & 98.6\% $\pm 1$ \\
            \citet{nicolas_webersinke_climatebert_2021} & ClimateBERT & 99.1\% $\pm 0.8$ \\     
            \citet{bingler2023cheaptalkspecificitysentiment} & Naive Bayes & 87\% \\
            \citet{bingler2023cheaptalkspecificitysentiment} & SVM + BoW  & 87\%  \\
            \citet{bingler2023cheaptalkspecificitysentiment} & SVM + ELMo & 89\% \\
            \citet{bingler2023cheaptalkspecificitysentiment} & ClimateBERT & 97\% \\
            \citet{bjarne_brie_mandatory_2022} & ClimateBERT & 98.59\% \\
            \citet{bjarne_brie_mandatory_2022} & FinBERT & 96.67\% \\
            \citet{bjarne_brie_mandatory_2022} & Random Forest & 91.65\% \\  
            \bottomrule
        \end{tabular}
    }
    \caption{F1-scores (weighted average) on ClimateBERT's climate detection dataset~\cite{bingler2023cheaptalkspecificitysentiment}. For \citet{nicolas_webersinke_climatebert_2021} scores are displayed as mean $\pm std$. \citet{bingler2023cheaptalkspecificitysentiment} also reported an uncertainty measure (p-value) which is not reported in this table for simplicity.}
    \label{tab:perf climatebert climate}
    % \end{subtable}
    % \begin{subtable}[t]{0.35\textwidth}
        % \vspace{-1.73cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llP{1.5cm}}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{F1} \\
        \midrule
         \citet{yu_climatebug_2024} & BERT & 90.81\% \\
         \citet{yu_climatebug_2024} & FinBERT & 90.82\% \\
         \citet{yu_climatebug_2024} & ClimateBERT & 91.07\% \\
         \citet{yu_climatebug_2024} & climateBUG-LM & 91.36\% \\
        \bottomrule
    \end{tabular}
    }
    \caption{F1-scores (macro average) on ClimateBUG-data~\cite{yu_climatebug_2024}.}
    \label{tab:perf climateBUG}
    \end{subtable}
    \begin{subtable}[t]{0.62\textwidth}
        \vspace{-2.28cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{p{3.1cm}p{2cm}ccc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{Wikipedia} & \textbf{10-K} & \textbf{Claims}  \\
        \midrule
         & & F1 (bin.) & F1 (bin.) & F1 (bin.) \\
        \midrule
        \citet{varini_climatext_2020} & best BERT & 80\% $\pm 6$ & 95\% $\pm$ 2 & 83\%  $\pm$ 1 \\
        \citet{varini_climatext_2020} & best NB & 60\% $\pm 7$ & 90\%  $\pm 3$ & 72\%  $\pm 2$ \\
        \citet{varini_climatext_2020} & best Keywords & 67\%  $\pm 7$ & 88\% $\pm 3$ & 70\%  $\pm 1$\\
        \citet{varini_climatext_2020} & BERT-AL-wiki & 69\%  $\pm 6$ & 71\% $\pm 4$ & 81\%  $\pm 1$\\
        Garrido-Merchán \cite{garridomerchán2023finetuning} & BERT-AL-wiki & - & 91\% $\pm 0.6$ & - \\
        Garrido-Merchán \cite{garridomerchán2023finetuning} & climateBERT-AL-wiki & - & 93\% $\pm 0.7$ & - \\
        
        & & \multicolumn{3}{c}{\textbf{Combined} - F1 (macro)} \\
        \midrule
        \citet{spokoyny2023answering} & Majority & \multicolumn{3}{c}{42.08\%} \\
        \citet{spokoyny2023answering} & Random & \multicolumn{3}{c}{46.86\%} \\
        \citet{spokoyny2023answering} & SVM & \multicolumn{3}{c}{83.39\%} \\
        \citet{spokoyny2023answering} & BERT & \multicolumn{3}{c}{87.04\%} \\
        \citet{spokoyny2023answering} & RoBERTa & \multicolumn{3}{c}{85.97\%} \\
        \citet{spokoyny2023answering} & DistilRoBERTa & \multicolumn{3}{c}{86.06\%} \\
        \citet{spokoyny2023answering} & Longformer & \multicolumn{3}{c}{87.80\%} \\
        \citet{spokoyny2023answering} & SciBERT & \multicolumn{3}{c}{83.29\%} \\
        \citet{spokoyny2023answering} & ClimateBERT & \multicolumn{3}{c}{85.14\%} \\
        \bottomrule
    \end{tabular}
    }
    \caption{F1-scores on ClimaText~\cite{varini_climatext_2020}. For \citet{varini_climatext_2020} scores are displayed as mean $\pm std$.}
    \label{tab:perf_table_climatext}
    \end{subtable}
    \caption{Reported performances for the datasets on climate-related topic detection (which include climate, sustainability and environment).}
    \label{tab:reported perf climate}
\end{table*}  

\newpage

\subsection{Thematic Analysis}
\label{sec:appendix subtopic}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.44\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lp{3cm}c}
        \toprule
        \textbf{Source} & \textbf{Model}      & \textbf{Prec.} \\ 
        \midrule
        \citet{bingler_cheap_2021} & TF-IDF+random forest & 24\% \\ 
        \citet{bingler_cheap_2021} & Sentence Enc. & 23\% \\ 
        \citet{bingler_cheap_2021} & RoBERTa Para. & 22\% \\ 
        \citet{bingler_cheap_2021} & RoBERTa Sent. & 75\% \\ 
        \citet{bingler_cheap_2021} & ClimateBERT* & 84\% \\
        \citet{bingler_cheap_2021} & ClimateBERT* \textcolor{white}{blahbl}F1: & 84\% \\
        \bottomrule
        \end{tabular}
        }
        \caption{Average Precisions and F1-score (macro average) on TCFD recommendations classification \cite{bingler_cheap_2021}. (*They refer to a finetuned RoBERTa with a Logistic Regression as ClimateBERT)}
    \end{subtable}
    \begin{subtable}[t]{0.46\textwidth}
    \vspace{-1.89cm}
        \resizebox{\textwidth}{!}{
    \begin{tabular}{llc}
    \toprule
    \textbf{Source} & \textbf{Model} & \textbf{ROC-AUC} \\
    \midrule
    \citet{sampson_tcfd-nlp_nodate} & TF-IDF (1) & 0.867 \\
    \citet{sampson_tcfd-nlp_nodate} & kNN (2) & 0.847 \\
    \citet{sampson_tcfd-nlp_nodate} & ClimateBERT (3) & 0.852 \\
    \citet{sampson_tcfd-nlp_nodate} & distilRoBERTa (4) & 0.819 \\
    \citet{sampson_tcfd-nlp_nodate} & (1) + (2) & 0.884 \\
    \citet{sampson_tcfd-nlp_nodate} & (1) + (3) & 0.865 \\
    \citet{sampson_tcfd-nlp_nodate} & (1) + (4) & 0.857 \\
    \bottomrule
    \end{tabular}
    }
    \caption{ROC-AUC scores on TCFD classification task \cite{sampson_tcfd-nlp_nodate}}
    \end{subtable}
    \begin{subtable}[t]{0.41\textwidth}
        % \vspace{-1.7cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{F1}  \\
        \midrule
        \citet{auzepy_evaluating_2023} & BART+MNLI & 56.68\% \\
        \bottomrule
        % source & model &  Micro F1-score: & Macro F1-score: & Weighted F1-score \\
        % \citet{auzepy_evaluating_2023} & BART+MNLI & 60.29\% & 56.68\% &   62.81\% \\
        \end{tabular}
        }
    \caption{F1-scores (macro average) on TCFD classification~\cite{auzepy_evaluating_2023}}
    \end{subtable}    
    \caption{Reported performances for the datasets on TFCD classification.}
    \label{tab:reported perf tcfd}
\end{table*}    

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.33\textwidth}
    \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{F1} \\ 
        \midrule
        \citet{huangFinBERTLargeLanguage2020} & FinBERT        & 89.6\%                    \\
        \citet{huangFinBERTLargeLanguage2020} & BERT           & 86.9\%                   \\
        \citet{huangFinBERTLargeLanguage2020} & NB             & 75.0\%                    \\
        \citet{huangFinBERTLargeLanguage2020} & SVM            & 75.0\%                    \\
        \citet{huangFinBERTLargeLanguage2020} & RF             & 78.3\%                   \\
        \citet{huangFinBERTLargeLanguage2020} & CNN            & 81.9\%                   \\
        \citet{huangFinBERTLargeLanguage2020} & LSTM           & 86.0\%                    \\
        \bottomrule
        \end{tabular}
    }
    \caption{F1-scores (micro average) on ESG classification~\cite{huangFinBERTLargeLanguage2020}}
    \label{tab:overall_f1_scores_esg_finbert}
    \end{subtable}
    \begin{subtable}[t]{0.47\textwidth}
        \vspace{-1.78cm}
        \resizebox{\textwidth}{!}{
    \begin{tabular}{llc}
    \toprule
        \textbf{Source} & \textbf{Model} & \textbf{Accuracy} \\
    \midrule
        \citet{LEE2023119726} & Multiling. BERT & 84.09\% $\pm0.092$ \\
        \citet{LEE2023119726} & KoBERT & 85.98\% $\pm0.163$ \\
        \citet{LEE2023119726} & DistilKoBERT & 84.89\% $\pm0.081$ \\
        \citet{LEE2023119726} & KB-ALBERT & 85.62\% $\pm0.026$ \\
        \citet{LEE2023119726} & KLUE (base) & 86.05\% $\pm0.125$ \\
        \citet{LEE2023119726} & KLUE (large) & 86.66\% $\pm0.099$ \\
    \bottomrule
    \end{tabular}
    }
    \caption{Accuracy on ESG classification~\cite{LEE2023119726}. Accuracy reported as mean $\pm std$}
    \end{subtable}
    \begin{subtable}[t]{0.50\textwidth}
        \centering
        % \vspace{-44pt}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        & \textbf{Environment}     & \textbf{F1}           \\ \midrule
        \citet{schimanski_bridging_2023} & BERT                            & 91.85\% $\pm 1.25$ \\
        \citet{schimanski_bridging_2023} & RoBERTa                         & 92.35\% $\pm 2.29$ \\
        \citet{schimanski_bridging_2023} & DistilRoBERTa                   & 90.97\% $\pm 2.00$ \\
        \citet{schimanski_bridging_2023} & EnvRoBERTa                      & 93.19\% $\pm 1.40$ \\
        \citet{schimanski_bridging_2023} & EnvDistilRoBERTa                & 92.35\% $\pm 1.65$ \\
        &&\\
        & \textbf{Social}          & \\ \midrule
        \citet{schimanski_bridging_2023} & BERT                            & 89.84\% $\pm 0.79$ \\
        \citet{schimanski_bridging_2023} & RoBERTa                         & 89.87\% $\pm 1.35$ \\
        \citet{schimanski_bridging_2023} & DistilRoBERTa                   & 90.59\% $\pm 1.03$ \\
        \citet{schimanski_bridging_2023} & SocRoBERTa                      & 91.90\% $\pm 1.79$ \\ 
        \citet{schimanski_bridging_2023} & SocDistilRoBERTa                & 91.24\% $\pm 1.86$ \\
        &&\\
        & \textbf{Governance}      & \\ \midrule
        \citet{schimanski_bridging_2023} & BERT                            & 76.86\% $\pm 1.77$ \\
        \citet{schimanski_bridging_2023} & RoBERTa                         & 77.03\% $\pm 1.82$ \\
        \citet{schimanski_bridging_2023} & DistilRoBERTa                   & 76.65\% $\pm 2.39$ \\
        \citet{schimanski_bridging_2023} & GovRoBERTa                      & 78.48\% $\pm 2.62$ \\
        \citet{schimanski_bridging_2023} & GovDistilRoBERTa                & 78.86\% $\pm 1.59$ \\
        \bottomrule
        \end{tabular}
    }
        \caption{F1-scores (unspecified) on environment, social, and governance datasets~\cite{Schimanski2024nature}. F1-scores are reported as mean $\pm std$}
        \label{tab:f1_scores_env_social_gov}
    \end{subtable}
    
    \caption{Reported performances for the datasets on ESG classification}
    \label{tab:reported perf esg}
\end{table*}    

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.93\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llcccc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{Water} & \textbf{Forest} & \textbf{Biodiv.} & \textbf{Nature} \\ 
        \midrule
        \citet{Schimanski2024nature} & EnvironmentalBERT & 94.47\% $\pm 1.37$ & 95.37\% $\pm 0.92$ & 92.76\% $\pm 1.01$ & 94.19\% $\pm 0.81$ \\ 
        \citet{Schimanski2024nature} & ClimateBERT       & 95.10\% $\pm 1.13$ & 95.34\% $\pm 0.94$ & 92.49\% $\pm 1.03$ & 93.50\% $\pm 0.64$ \\ 
        \citet{Schimanski2024nature} & RoBERTa           & 94.55\% $\pm 0.86$ & 94.78\% $\pm 0.48$ & 92.46\% $\pm 1.54$ & 93.97\% $\pm 0.26$ \\ 
        \citet{Schimanski2024nature} & DistilRoBERTa     & 94.98\% $\pm 1.16$ & 95.29\% $\pm 0.65$ & 92.29\% $\pm 1.23$ & 93.55\% $\pm 0.72$ \\ 
        \citet{Schimanski2024nature} & Keywords          & -                & -                & 63.03\% & 61.00\% \\ 
        \bottomrule
        \end{tabular}
    }
    \caption{F1-scores (unspecified) on Nature-related topic classification~\cite{Schimanski2024nature}. F1-scores are reported as mean $\pm std$.}
    \label{tab:appendix nature}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
    \toprule
    \textbf{Source} & \textbf{Model} & \textbf{F1} \\
    \midrule
    \citet{spokoyny2023answering} & Majority & 0.79\% \\
    \citet{spokoyny2023answering} & Random & 5.05\% \\
    \citet{spokoyny2023answering} & SVM & 48.02\% \\
    \citet{spokoyny2023answering} & BERT & 54.74\% \\
    \citet{spokoyny2023answering} & RoBERTa & 52.90\% \\
    \citet{spokoyny2023answering} & DistilRoBERTa & 51.13\% \\
    \citet{spokoyny2023answering} & Longformer & 54.79\% \\
    \citet{spokoyny2023answering} & SciBERT & 51.83\% \\
    \citet{spokoyny2023answering} & ClimateBERT & 52.97\% \\
    \bottomrule
    \end{tabular}
        }
    \caption{F1-scores (macro average) on SciDCC~\cite{mishra2021neuralnere}.}
    \label{tab:SCIDCC perf}
    % \end{subtable}
    % \begin{subtable}[t]{0.45\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        \toprule
         \textbf{Source} & \textbf{Models} & \textbf{F1} \\ 
         \midrule
        \citet{spokoyny2023answering} & Majority        & 3.65\%                      \\
        \citet{spokoyny2023answering} & Random          & 6.45\%                      \\
        \citet{spokoyny2023answering} & SVM             & 58.34\%                     \\
        \citet{spokoyny2023answering} & BERT            & 64.64\%         \\
        \citet{spokoyny2023answering} & RoBERTa         & 65.22\%         \\
        \citet{spokoyny2023answering} & DistilRoBERTa   & 63.61\%                     \\
        \citet{spokoyny2023answering} & Longformer      & 64.03\%                     \\
        \citet{spokoyny2023answering} & SciBERT         & 63.62\%                     \\
        \citet{spokoyny2023answering} & ClimateBERT     & 64.24\%                    \\
        \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (macro average) on the climaTOPIC \cite{spokoyny2023answering}.}
        \label{tab:climatopic_performance}
    \end{subtable}
    \begin{subtable}[t]{0.48\textwidth}
        \vspace{-2.32cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{p{2.7cm}lc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{F1} \\
        \midrule
             \citet{jain_supply_2023} & all-mpnet-base-v2 & 43.7\% \\
             \citet{jain_supply_2023} & TF-IDF & 69\% \\
             \citet{jain_supply_2023} & Word2Vec & 72\% \\
             \citet{jain_supply_2023} & RoBERTa & 87.19\% \\
             \citet{jain_supply_2023} & BERT & 87.14\% \\
             \citet{jain_supply_2023} & ClimateBERT & 85.20\% \\
        \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (weighted average) on ledger transaction classification \cite{jain_supply_2023}.}
        \label{tab:appendix jain}
    
         \resizebox{\textwidth}{!}{
            \begin{tabular}{llc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\ \midrule
            \citet{vaid-etal-2022-towards} & FastText      & 63.8\%            \\
            \citet{vaid-etal-2022-towards} & BERT-Base      & 69.6\%            \\
            \citet{vaid-etal-2022-towards} & BERT-Large    & 69.5\%            \\
            \citet{vaid-etal-2022-towards} & RoBERTa-Base   & 73.4\%            \\
            \citet{vaid-etal-2022-towards} & RoBERTa-Large  & 73.5\%   \\
            \citet{vaid-etal-2022-towards} & DistilBERT     & 69.4\%            \\ \midrule
            \midrule
            \citet{spokoyny2023answering} & Majority        & 13.83\%           \\
            \citet{spokoyny2023answering} & Random         & 16.71\%           \\
            \citet{spokoyny2023answering} & SVM            & 51.81\%           \\
            \citet{spokoyny2023answering} & BERT           & 71.78\%           \\
            \citet{spokoyny2023answering} & RoBERTa        & 74.58\%  \\
            \citet{spokoyny2023answering} & DistilRoBERTa  & 72.33\%           \\
            \citet{spokoyny2023answering} & Longformer     & 72.28\%           \\
            \citet{spokoyny2023answering} & SciBERT       & 70.50\%           \\
           \citet{spokoyny2023answering} & ClimateBERT    & 71.83\%           \\ \bottomrule
            \end{tabular}
        }
    \caption{F1-scores (macro average) on ClimateEng~\cite{vaid-etal-2022-towards}.}
    \label{tab:performance climateEng}
    \end{subtable}
    \caption{Reported performances for the datasets on climate-related topic detection (other topics.)}
    \label{tab:reported perf subtopic}
\end{table*}    

\newpage
~\newpage
~\newpage

\subsection{In-depth Disclosure: Climate Risk Classification}

% Intersting: majority is way worse, this might be explained because the majority class is not the same majority class in the test distribution

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.90\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llccccc}
        \toprule
        & & \textbf{Binary} &  \textbf{2 Classes} &  & \textbf{5 Classes} & \\
        \textbf{Source} & \textbf{Model} & REALISTIC & REALISTIC & REALISTIC & HARD NEG. & DISCRIMINATORY \\
        \midrule
        \citet{Friederich_climate_risk_disclosure} & SVM & 29.0\% & 35.1\% & 20.4\% & 45.7\% & 59.9\% \\
        \citet{Friederich_climate_risk_disclosure} & distilBERT & 44.4\% & 49.7\% & 24.1\% & 43.1\% & 55.8\% \\
        \citet{Friederich_climate_risk_disclosure} & RoBERTa & 49.6\% & 44.6\% & 35.6\% & 52.8\% & 59.6\% \\
        \bottomrule
        \end{tabular}
    }
    \caption{F1-scores (macro average) for climate risk disclosure~\citet{Friederich_climate_risk_disclosure}.}
    \label{tab:performances_friederich}
    \end{subtable}
    \begin{subtable}[t]{0.46\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        \toprule
            \textbf{Source} &\textbf{Model}           & \textbf{F1}   \\ \midrule
            \citet{xiang_dare_2023} & BERT-Base                 & 93.1\%         \\
            \citet{xiang_dare_2023} & BERT-Base (Domain-spe.) & 95.5\% \\
            \citet{xiang_dare_2023} & TinyBERT                 & 87.0\%         \\
            \citet{xiang_dare_2023} & DistilBERT               & 89.9\%         \\
            \citet{xiang_dare_2023} & ClimateBERT              & 87.5\%         \\
            \citet{xiang_dare_2023} & CCLA+Max-Pooling         & 82.9\%         \\
            \citet{xiang_dare_2023} & Bi-LSTM-Attention        & 71.9\%         \\
            \citet{xiang_dare_2023} & POS-Bi-LSTM-Attention    & 88.2\%         \\
            \citet{xiang_dare_2023} & DARE               & 89.4\%         \\ \bottomrule
        \end{tabular}
        }
    \caption{F1-scores (macro average) on Risk/Opportunity classification~\cite{xiang_dare_2023}.}
        \label{table:f1_performance dare}
    \end{subtable}
    \begin{subtable}[t]{0.44\textwidth}
        \vspace{-1.95cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llccc}
        \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\
            \midrule
            \citet{nicolas_webersinke_climatebert_2021} & distilRoBERTa & 82.5\% $\pm$ 4.6 \\
            \citet{nicolas_webersinke_climatebert_2021} & climateBERT & 83.8\% $\pm$ 3.6 \\     
            \citet{bingler2023cheaptalkspecificitysentiment} & Naive Bayes & 72\% \\
            \citet{bingler2023cheaptalkspecificitysentiment} & SVM + BoW  & 72\% \\
            \citet{bingler2023cheaptalkspecificitysentiment} & SVM + ELMo  & 75\%\\
            \citet{bingler2023cheaptalkspecificitysentiment} & climateBERT  & 80\% \\
            \bottomrule
        \end{tabular}    
        }
        \caption{F1-scores (weighted average) on risk/opportunity detection~\citet{nicolas_webersinke_climatebert_2021, bingler2023cheaptalkspecificitysentiment}. \citet{nicolas_webersinke_climatebert_2021}'s F1-scores are reported as mean $\pm std$. \citet{bingler2023cheaptalkspecificitysentiment} also reported an uncertainty measure (p-value) which is not shown in this table for simplicity.}
        \label{tab:perf_table_climatebert_downstream_risk}
    \end{subtable}
    \begin{subtable}[t]{0.35\textwidth}
        \resizebox{\textwidth}{!}{
       \begin{tabular}{llc}
        \toprule
         \textbf{Source} & \textbf{Model} & \textbf{F1} \\
        \midrule
        \citet{kolbel_ask_2021} & Bag-of-Words & 83.63\% \\
        \citet{kolbel_ask_2021} & TF-IDF & 80.05\% \\
        \citet{kolbel_ask_2021} & BERT & 94.66\% \\
        \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (unspecified) for risk classification (General/Transition risk/Physical Risk)~\cite{kolbel_ask_2021}.}
        \label{tab:f1_scores_cn_tp}
    \end{subtable}
    % \begin{subtable}[t]{0.45\textwidth}
    %     \resizebox{\textwidth}{!}{
    %     \begin{tabular}{ll|cc}
    % \toprule
    %     \textbf{Source} & \textbf{Model} & \textbf{F1}\\
    % \midrule
    %     \citet{huangFinBERTLargeLanguage2020} & FinBERT & 87.8\% \\
    %     \citet{huangFinBERTLargeLanguage2020} & BERT & 84.2\% \\
    %     \citet{huangFinBERTLargeLanguage2020} & NB & 71.1\% \\
    %     \citet{huangFinBERTLargeLanguage2020} & SVM & 69.6\%  \\
    %     \citet{huangFinBERTLargeLanguage2020} & RF & 66.8\% \\
    %     \citet{huangFinBERTLargeLanguage2020} & CNN & 72.5\%  \\
    %     \citet{huangFinBERTLargeLanguage2020} & LSTM & 73.3\% \\
    %     \bottomrule
    % \end{tabular}
    
    %     }
    %     \caption{F1-scores (unspecified) on Sentiment Analysis~\cite{huangFinBERTLargeLanguage2020}.}
    % \end{subtable}
    \caption{Reported performances for the datasets on risk detection and classification.}
    \label{tab:reported perf risk}
\end{table*}  

\newpage

\subsection{Green Claim Detection}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.49\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\ 
            \midrule
            \citet{stammbach_environmental_2023} & Majority baseline & 00.0\%\\ 
            \citet{stammbach_environmental_2023} & Random baseline & 33.5\%\\ 
            \citet{stammbach_environmental_2023} & ClaimBuster RoBERTa & 33.9\%\\
            \citet{stammbach_environmental_2023} & Pledge Detection RoBERTa & 26.4\%\\
            \midrule
            \citet{stammbach_environmental_2023} & TF-IDF SVM & 69.1\%\\
            \citet{stammbach_environmental_2023} & Character n-gram SVM & 70.9\%\\
            \citet{stammbach_environmental_2023} & DistilBERT & 83.7\%\\
            \citet{stammbach_environmental_2023} & ClimateBERT & 83.8\%\\
            \citet{stammbach_environmental_2023} & RoBERTa\_base & 82.4\%\\
            \citet{stammbach_environmental_2023} & RoBERTa\_large & 84.9\%\\
            \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (binary) on environmental claim detection~\cite{stammbach_environmental_2023}.}
        \label{tab:perf_env_claim}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
    \vspace{-2.24cm}
        \resizebox{\textwidth}{!}{
         \begin{tabular}{llc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\ 
            \midrule
                \citet{vinicius_woloszyn_towards_2021} & Bin RoBERTa Bal & 92.08\% \\
                \citet{vinicius_woloszyn_towards_2021} & Bin BERTweet Bal & 91.48\% \\
                \citet{vinicius_woloszyn_towards_2021} & Bin Flair Bal & 88.52\% \\
                \citet{vinicius_woloszyn_towards_2021} & Bin RoBERTa Unbal & 88.88\% \\
                \citet{vinicius_woloszyn_towards_2021} & Bin BERTweet Unbal & 84.30\% \\
                \citet{vinicius_woloszyn_towards_2021} & Bin Flair Unbal & 81.55\% \\
            \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (binary) on green claim detection~\cite{vinicius_woloszyn_towards_2021}.}
        \label{tab:perf_green_claim}
    \end{subtable}
    \caption{Reported performances for the datasets on green claim detection.}
    \label{tab:reported perf claims}
\end{table*}  

\subsection{Green Claim Characteristics}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.46\textwidth}
        \resizebox{\textwidth}{!}{
         \begin{tabular}{llc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{F1} \\ 
        \midrule
            \citet{vinicius_woloszyn_towards_2021} & Mult RoBERTa Bal. & 81.45\% \\
            \citet{vinicius_woloszyn_towards_2021} & Mult BERTweet Bal. & 75.14\% \\
            \citet{vinicius_woloszyn_towards_2021} & Mult Flair Bal. & 69.96\% \\
            \citet{vinicius_woloszyn_towards_2021} & Mult RoBERTa Unbal. & 76.82\% \\
            \citet{vinicius_woloszyn_towards_2021} & Mult BERTweet Unbal. & 59.71\% \\
            \citet{vinicius_woloszyn_towards_2021} & Mult Flair Unbal. & 38.15\% \\
        \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (unspecified) on Implicit/Explicit green claim detection~\cite{vinicius_woloszyn_towards_2021}.}
        \label{tab:perf_green_claim_implicit}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
        \vspace{-1.49cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        \toprule
         \textbf{Source} & \textbf{Model} & \textbf{Accuracy}\\
         \midrule
            \citet{tobias_schimanski_climatebert-netzero_2023} & ClimateBERT & 96.2\% $\pm 0.4$ \\
            \citet{tobias_schimanski_climatebert-netzero_2023} & DistilRoBERTa & 94.4\% $\pm 0.7$\\
            \citet{tobias_schimanski_climatebert-netzero_2023} & RoBERTa-base & 95.8\% $\pm 0.6$ \\
            \citet{tobias_schimanski_climatebert-netzero_2023} & GPT-3.5-turbo & 92.0\% \\
            \bottomrule
        \end{tabular}
        }
        \caption{Accuracy on net-zero/reduction classification~\cite{tobias_schimanski_climatebert-netzero_2023}. Accuracy is reported as mean $\pm std$.}
        \label{tab:perf_net_zero}
    \end{subtable}
    \begin{subtable}[t]{0.54\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{cccc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{Commit. \& Act.} & \textbf{Specificity}  \\
            \midrule
            \citet{bingler2023cheaptalkspecificitysentiment} & Naive Bayes & 75\%& 75\%  \\
            \citet{bingler2023cheaptalkspecificitysentiment} & SVM + BoW   & 76\%  & 75\%  \\
            \citet{bingler2023cheaptalkspecificitysentiment} & SVM + ELMo  & 79\% & 76\% \\
            \citet{bingler2023cheaptalkspecificitysentiment} & climateBERT  & 81\% & 77\% \\
            \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (weighted average) on Commitment\&Action and Specificity classification~\cite{bingler2023cheaptalkspecificitysentiment}. \citet{bingler2023cheaptalkspecificitysentiment} also reported an uncertainty measure (p-value) which is not reported in this table for simplicity.}
    \label{tab:perf_table_climatebert_downstream}
    \end{subtable}
    \caption{Reported performances for the datasets on claim characteristics classification.}
    \label{tab:reported perf claims characteristics}
\end{table*}  

\newpage

 % \begin{subtable}[t]{0.45\textwidth}
 %        \resizebox{\textwidth}{!}{
 %    \begin{tabular}{@{}lcccc@{}}
 %        \toprule
 %        \textbf{Approach} & \textbf{Climate-related} & \textbf{Sentiment} & \textbf{Commitments \& actions} & \textbf{Specificity} \\ 
 %        \midrule
 %        Naive Bayes     & 0.04***                  & 0.05***            & 0.07***                         & 0.08***              \\
 %        SVM + BoW         & 0.05***                  & 0.08***            & 0.11***                         & 0.12***              \\
 %        SVM + ELMo        & 0.03***                  & 0.05***            & 0.06***                         & 0.06***              \\
 %        \bottomrule
 %    \end{tabular}
 %    }
 %    \caption{Evaluation results for cross-validation. This table shows mean improvement of \textsc{ClimateBERT}$_{\text{CTI}}$'s F1 scores over baseline models for different downstream tasks for $n=30$ runs on 800 samples for training and 400 samples for testing. By *, **, and *** we denote $p$-levels below 10\%, 5\%, and 1\%, respectively. \cite{bingler2023cheaptalkspecificitysentiment}}
 %    \label{tab:cross-validation}
 %    \end{subtable}


\subsection{Green Stance Detection}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.44\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{p{3cm}lc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\
            \midrule
            \citet{vaghefi2022deep} & GPT-2 & 67\%\\ 
            \citet{vaghefi2022deep} & climateGPT-2 & 72\% \\
            &&\\
            \textbf{Claim} &  & \\
            \midrule
            \citet{diggelmann_climate-fever_2020} & ALBERT(FEVER) & 32.85\% \\
            \citet{xiang_dare_2023} & BERT & 80.7\% \\
            \citet{xiang_dare_2023} & RoBERTa & 72.3\% \\
            \citet{xiang_dare_2023} & DistilRoBERTa & 72.0\% \\
            \citet{xiang_dare_2023} & ClimateBERT & 76.8\% \\
            \citet{xiang_dare_2023} & POS-Bi-LSTM-Att. & 79.3\% \\
            \citet{nicolas_webersinke_climatebert_2021} & DistilRoBERTa & 74.8\% $\pm 3.6$\\
            \citet{nicolas_webersinke_climatebert_2021} & ClimateBERT & 75.7\% $\pm 4.4$ \\
            \midrule
            Subset 95 examples & & \\
            \citet{Wang2021EvidenceBA} & RoBERTa (FEVER) & 49.2\% \\
            \citet{Wang2021EvidenceBA} & + ClimateFEVER & 64.2\% \\
            % \citet{Wang2021EvidenceBA} & + UDA ratio 8 & 71.7\% \\
            \citet{Wang2021EvidenceBA} & + UDA ratio 18 & 71.8\% \\    
            &&\\
            \textbf{Evidence} &  & \\
            \midrule
            \citet{spokoyny2023answering} & Majority & 26.08\% \\
            \citet{spokoyny2023answering} & Random & 30.62\% \\
            \citet{spokoyny2023answering} & BERT & 62.47\% \\
            \citet{spokoyny2023answering} & RoBERTa & 60.74\% \\
            \citet{spokoyny2023answering} & DistilRoBERTa & 61.54\% \\
            \citet{spokoyny2023answering} & Longformer & 60.82\% \\
            \citet{spokoyny2023answering} & SciBERT & 62.68\% \\
            \citet{spokoyny2023answering} & ClimateBERT & 61.54\% \\       \bottomrule
    
        \end{tabular}
        }
        \caption{F1-score (macro average) on ClimateFEVER \citet{diggelmann_climate-fever_2020}. The prediction can be made at \textit{Claim} level or for each Claim-Evidence pair (\textit{Evidence}).  \citet{nicolas_webersinke_climatebert_2021}'s F1-scores are reported as mean $\pm std$.}
        \label{tab:perf_table_climatefever}
    
     \resizebox{\textwidth}{!}{
        \begin{tabular}{p{2.8cm}p{2.3cm}cc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{Acc.} & \textbf{F1} \\
            \midrule
            \citet{luo_detecting_2020} & Majority class & 0.43 & 17\% \\
            \citet{luo_detecting_2020} & Linear & 0.62 & 60\% \\
            \citet{luo_detecting_2020} & BERT & 0.75 & 73\% \\
            \citet{luo_detecting_2020} & Human & 0.71 & - \\
            \bottomrule
        \end{tabular}
    }
    \caption{Accuracy and F1-scores (macro average) on GWSD~\cite{luo_detecting_2020}.}
    \end{subtable}
    \begin{subtable}[t]{0.4\textwidth}
        \vspace{-4.95cm}
        \resizebox{\textwidth}{!}{
         \begin{tabular}{llc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\
            \midrule
            \citet{vaid-etal-2022-towards} & FastText & 34.3\% \\
            \citet{vaid-etal-2022-towards} & BERT-Base & 46.4\% \\
            \citet{vaid-etal-2022-towards} & BERT-Large & 48.9\% \\
            \citet{vaid-etal-2022-towards} & RoBERTa-Base & 51.0\% \\
            \citet{vaid-etal-2022-towards} & RoBERTa-Large & 48.9\% \\
            \citet{vaid-etal-2022-towards} & DistilBERT & 44.8\% \\
            \citet{spokoyny2023answering} & Majority & 29.68\% \\
            \citet{spokoyny2023answering} & Random & 25.52\% \\
            \citet{spokoyny2023answering} & SVM & 42.92\% \\
            \citet{spokoyny2023answering} & BERT & 55.37\% \\
            \citet{spokoyny2023answering} & RoBERTa & 59.69\% \\
            \citet{spokoyny2023answering} & DistilRoBERTa & 52.51\% \\
            \citet{spokoyny2023answering} & Longformer & 34.68\% \\
            \citet{spokoyny2023answering} & SciBERT & 48.67\% \\
            \citet{spokoyny2023answering} & ClimateBERT & 52.84\% \\
            \bottomrule
            \end{tabular}
            
        }
        \caption{F1-scores (macro average) on ClimateStance~\cite{vaid-etal-2022-towards}.}
            \label{tab:performance ClimateStance}
    % \end{subtable}
    % \begin{subtable}[t]{0.4\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llcc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{macro} & \textbf{bin.} \\
        \midrule
        & \textit{Baselines} & & \\
        \citet{lai_using_2023} & BERT & - & 58\% \\
        \citet{lai_using_2023} & RoBERTa & - & 89\% \\
        \citet{lai_using_2023} & ClimateBERT & 90\% & 90\% \\
        \midrule
        & \textit{SeqGAN} & & \\
        \citet{lai_using_2023} & BERT & - & 83\% \\
        \citet{lai_using_2023} & RoBERTa  & - & 87\% \\
        \citet{lai_using_2023} & ClimateBERT & 90\% & 90\% \\
        \midrule
        & \textit{MaliGAN} & & \\
        \citet{lai_using_2023} & BERT & - & 74\% \\
        \citet{lai_using_2023} & RoBERTa & - & 88\% \\
        \citet{lai_using_2023} & ClimateBERT & 91\% & 91\% \\
        \midrule
        & \textit{RankGAN} & & \\
        \citet{lai_using_2023} & BERT & - & 82\% \\
        \citet{lai_using_2023} & RoBERTa & - & 87\% \\
        \citet{lai_using_2023} & ClimateBERT & 91\% & 90\% \\
        \bottomrule
    \end{tabular}

        }
        \caption{F1-scores (binary and macro average) on stance on climate change remediation efforts)~\cite{lai_using_2023}.}
        \label{tab:appendix-lai_using}
    \end{subtable}
    %  \begin{subtable}[t]{0.86\textwidth}
    %     \resizebox{\textwidth}{!}{
    %         \begin{tabular}{ll|ccc|ccc|ccc}
    %     \toprule
    %         &  & \multicolumn{3}{c|}{\textbf{Document}} & \multicolumn{3}{c|}{\textbf{Page overlap}} & \multicolumn{3}{c}{\textbf{Strict}} \\
    %      \textbf{Source}    & \textbf{Model} & P & Q & S & P & Q & S & P & Q & S \\
    %     \midrule
    %         \citet{morio2023an} & Most-frequent & 46.7\% & 52.6\% & 36.8\% & 51.8\% & 25.6\% & 19.8\% & 41.2\% & 19.6\% & 17.5\% \\
    %         \citet{morio2023an} & Linear & 66.0\% & 61.9\% & 50.3\% & 71.4\% & 44.5\% & 36.1\% & 52.0\% & 31.2\% & 27.0\% \\
    %         \citet{morio2023an} & BERT-base & 71.0\% & 63.5\% & 51.6\% & 73.6\% & 48.1\% & 37.2\% & 50.2\% & 31.9\% & 25.8\% \\
    %         \citet{morio2023an} & ClimateBERT & 71.8\% & 64.0\% & 52.8\% & 74.4\% & 48.9\% & 39.0\% & 50.2\% & 32.2\% & 26.8\% \\
    %         \citet{morio2023an} & RoBERTa-base & 71.6\% & 64.5\% & 53.1\% & 73.8\% & 49.6\% & 38.3\% & 50.4\% & 33.4\% & 26.6\% \\
    %         \citet{morio2023an} & Longformer-base & 73.7\% & 66.9\% & 54.6\% & 75.9\% & 53.0\% & 40.8\% & 52.5\% & 36.1\% & 28.6\% \\
    %         \citet{morio2023an} & Longformer-large & 73.9\% & 68.8\% & 57.3\% & 76.5\% & 55.0\% & 44.1\% & 53.6\% & 38.7\% & 31.5\% \\
    %     \bottomrule
    %     \end{tabular}
    % }
    % \caption{Custom metrics on LobbyMap~\cite{morio2023an}.}
    % \label{tab:appendix lobbymap}
    % \end{subtable}
    \caption{Reported performance for the datasets on stance detection (Part 1)}
    \label{tab:reported perf stance}
\end{table*}  

\begin{table*}[ht]
    \centering
\begin{subtable}[t]{0.86\textwidth}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{llccccccccc}
        \toprule
            &  & \multicolumn{3}{c}{\textbf{Document}} & \multicolumn{3}{c}{\textbf{Page overlap}} & \multicolumn{3}{c}{\textbf{Strict}} \\
         \textbf{Source}    & \textbf{Model} & P & Q & S & P & Q & S & P & Q & S \\
        \midrule
            \citet{morio2023an} & Most-frequent & 46.7\% & 52.6\% & 36.8\% & 51.8\% & 25.6\% & 19.8\% & 41.2\% & 19.6\% & 17.5\% \\
            \citet{morio2023an} & Linear & 66.0\% & 61.9\% & 50.3\% & 71.4\% & 44.5\% & 36.1\% & 52.0\% & 31.2\% & 27.0\% \\
            \citet{morio2023an} & BERT-base & 71.0\% & 63.5\% & 51.6\% & 73.6\% & 48.1\% & 37.2\% & 50.2\% & 31.9\% & 25.8\% \\
            \citet{morio2023an} & ClimateBERT & 71.8\% & 64.0\% & 52.8\% & 74.4\% & 48.9\% & 39.0\% & 50.2\% & 32.2\% & 26.8\% \\
            \citet{morio2023an} & RoBERTa-base & 71.6\% & 64.5\% & 53.1\% & 73.8\% & 49.6\% & 38.3\% & 50.4\% & 33.4\% & 26.6\% \\
            \citet{morio2023an} & Longformer-base & 73.7\% & 66.9\% & 54.6\% & 75.9\% & 53.0\% & 40.8\% & 52.5\% & 36.1\% & 28.6\% \\
            \citet{morio2023an} & Longformer-large & 73.9\% & 68.8\% & 57.3\% & 76.5\% & 55.0\% & 44.1\% & 53.6\% & 38.7\% & 31.5\% \\
        \bottomrule
        \end{tabular}
    }
    \caption{Custom metrics on LobbyMap~\cite{morio2023an}.}
    \label{tab:appendix lobbymap}
    \end{subtable}
    \caption{Reported performance for the datasets on stance detection (Part 2)}
    \label{tab:reported perf stance 2}
\end{table*}  

% \begin{table*}[ht]
%     \centering
%     \begin{tabular}{ll|c}
%         source & model & F1-score  \\
%         \midrule
%         \citet{lai_using_2023} & RoBERTa & 89\% \\
%         \citet{lai_using_2023} & ClimateBERT & 90\% \\
%         \citet{lai_using_2023} & BERT & 58\% \\
%         \citet{lai_using_2023} & RoBERTa + Seq-GAN & 87\% \\
%         \citet{lai_using_2023} & ClimateBERT + Seq-GAN & 90\% \\
%         \citet{lai_using_2023} & BERT + Seq-GAN & 83\% \\
%         \citet{lai_using_2023} & RoBERTa + MALI-GAN & 88\% \\
%         \citet{lai_using_2023} & ClimateBERT + MALI-GAN & 91\% \\
%         \citet{lai_using_2023} & BERT + MALI-GAN & 74\% \\
%         \citet{lai_using_2023} & RoBERTa + RankGAN & 87\% \\
%         \citet{lai_using_2023} & ClimateBERT + RankGAN & 90\% \\
%         \citet{lai_using_2023} & BERT + RankGAN & 82\% \\
%         \midrule
%     \end{tabular}
%     \caption{Performance on \citet{lai_using_2023}'s dataset on stance on climate remediation effort}
%     \label{tab:my_label}
% \end{table*}
\newpage
~\newpage

\subsection{Question Answering}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.88\textwidth}
         \resizebox{\textwidth}{!}{
            \begin{tabular}{llcccccc}
                \toprule
                \textbf{Source} & \textbf{embeddings} & \textbf{Quest.} & \textbf{Def.} & \textbf{Concepts} & \textbf{Gen.} & \textbf{inf\_3} & \textbf{inf\_all} \\
                \midrule
                \citet{schimanski-etal-2024-climretrieve} & random & 0.037 & 0.037 & 0.037 & 0.037 & 0.037 & 0.037 \\
                \citet{schimanski-etal-2024-climretrieve} & BM25 & 0.113 & 0.114 & 0.126 & 0.139 & 0.172 & 0.174 \\
                \citet{schimanski-etal-2024-climretrieve} & ColBERTv2 & 0.109 & 0.094 & 0.112 & 0.137 & 0.130 & 0.137 \\
                \citet{schimanski-etal-2024-climretrieve} & DRAGON+ & 0.139 & 0.121 & 0.161 & 0.160 & 0.161 & 0.160 \\
                \citet{schimanski-etal-2024-climretrieve} & GTE-base & 0.161 & 0.153 & 0.171 & 0.154 & 0.171 & 0.174 \\
                \citet{schimanski-etal-2024-climretrieve} & text-embedding-ada-002 & 0.163 & 0.143 & 0.152 & 0.163 & 0.176 & 0.176 \\
                \citet{schimanski-etal-2024-climretrieve} & text-embedding-3-small & 0.163 & 0.143 & 0.152 & 0.163 & 0.179 & 0.179 \\
                \citet{schimanski-etal-2024-climretrieve} & text-embedding-3-large & 0.167 & 0.143 & 0.152 & 0.163 & 0.179 & 0.179 \\
                \bottomrule
            \end{tabular}
             }
    \caption{F1-score (custom) of the different embedding models and information retrieval approaches (question, definition, concepts, generic, inf\_3, inf\_all)~\cite{schimanski-etal-2024-climretrieve}}
    \end{subtable}
     \begin{subtable}[t]{0.67\textwidth}
        \resizebox{\textwidth}{!}{
             \begin{tabular}{llcccccc}
        \toprule
         & & \multicolumn{3}{c}{\textbf{Test}} & \multicolumn{3}{c}{\textbf{In-house annotation}} \\
         \textbf{Source} & \textbf{Model} & F1 & BLEU & METEOR & F1 & BLEU & METEOR \\
         \midrule
         \citet{cliamtebot_2022} & ALBERT & 81.6\% & 0.678 & 0.808 &  66.1\% & 0.416 & 0.694 \\
         \bottomrule
    \end{tabular}
    }
    \caption{F1-score, BLEU and METEOR on CCMRC~\cite{cliamtebot_2022}}
    \end{subtable}
    \begin{subtable}[t]{0.32\textwidth}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{llc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{F1} \\
        \midrule
        \citet{luccioni_analyzing_2020} & large & 85.5\% \\
        \citet{luccioni_analyzing_2020} & base & 82\% \\
        \bottomrule
    \end{tabular}
    }
    \caption{F1-scores (macro average) of RoBERTa on question answering~\cite{luccioni_analyzing_2020}.}
    \end{subtable}  
    \begin{subtable}[t]{0.40\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{F1} \\
        \midrule
        \citet{spokoyny2023answering} & Majority & 4.11\% \\
        \citet{spokoyny2023answering} & Random & 12.14\% \\
        \citet{spokoyny2023answering} & SVM & 86.00\% \\
        \citet{spokoyny2023answering} & BERT & 84.57\% \\
        \citet{spokoyny2023answering} & RoBERTa & 85.61\% \\
        \citet{spokoyny2023answering} & DistilRoBERTa & 84.38\% \\
        \citet{spokoyny2023answering} & Longformer & 84.35\% \\
        \citet{spokoyny2023answering} & SciBERT & 84.43\% \\
        \citet{spokoyny2023answering} & ClimateBERT & 84.80\% \\
        \bottomrule
    \end{tabular}
    }
    \caption{F1-scores (macro average) on  ClimaINS~\citet{spokoyny2023answering}}
    \label{tab:perf_table_climateins}
    \end{subtable}
        \begin{subtable}[t]{0.56\textwidth}
        \centering
        \vspace{-2.1cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llcccc}
        \toprule
        \textbf{Source} & \textbf{Model} & \multicolumn{2}{c}{\textbf{Question}} & \multicolumn{2}{c}{\textbf{Answer}} \\
                       & & R@1 & R@5 & R@1 & R@5 \\ \midrule
        \citet{thulke2024climategpt} & bge-base-en-v1.5 & 54.8         & 71.5         & 81.8         & 92.1         \\ 
        \citet{thulke2024climategpt} & bge-large-en-v1.5 & 55.8         & 73.6         & 83.3         & 93.1         \\ 
        \citet{thulke2024climategpt} & gtr-t5-large                   & 48.8         & 67.4         & 79.6         & 90.1         \\
        \citet{thulke2024climategpt} & gtr-t5-xxl & 47.6         & 66.3         & 79.2         & 89.7 \\ 
        \citet{thulke2024climategpt} & gte-large & 50.7         & 68.2         & 80.9         & 91.4         \\ 
        \citet{thulke2024climategpt} & ember-v1 & 49.5         & 68.6         & 79.7         & 91.1         \\
        \citet{thulke2024climategpt} & instructor-large & 50.0         & 68.2         & 81.7         & 91.8         \\
        \citet{thulke2024climategpt} & instructor-xl & 53.3         & 69.7         & 83.3         & 92.1         \\
        \bottomrule
        \end{tabular}
        }
        \caption{Recall (R@1 and R@5) of retrieving the correct document given the question or the answer from the IFT dataset \cite{thulke2024climategpt}.}
        \label{tab:ift_recall}
    \end{subtable}  
    \begin{subtable}[t]{0.43\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{CITIES} & \textbf{STATES} & \textbf{CORP} \\
        \midrule
        & \multicolumn{3}{c}{\textit{No Finetuning on CDP}} \\
        BM25           & 0.055               & 0.084               & 0.153             \\
        MiniLM         & 0.099               & 0.120               & 0.320             \\
        \midrule
        & \multicolumn{3}{c}{\textit{Finetuned on CDP (In-Domain)}} \\
        ClimateBERT    & 0.331               & 0.422               & 0.753             \\
        MiniLM         & 0.366               & 0.482               & 0.755             \\
        \midrule
        & \multicolumn{3}{c}{\textit{Best Model Finetuned on all}} \\
        MiniLM         & 0.352               & 0.489               & 0.745             \\
        \bottomrule
        \end{tabular}
    }
    \caption{MRR@10 scores on the three subsets of CLIMA-QA~\cite{spokoyny2023answering}. Models finetuned and evaluated on the same subset fall under In-Domain.}
    \label{tab:appendix climaqa}
    \end{subtable}
     \begin{subtable}[t]{0.52\textwidth}
        \vspace{-2.2cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lccc}
        \toprule
        \textbf{Task} & \textbf{Raw} & \textbf{Optimal} & \textbf{Confidence} \\ 
        \midrule
        \textit{Net-zero target year} & 0.95 & 0.95 & 0.97 \\ 
        \textit{Reduction target year} & 0.84 & 0.90 & 0.90 \\ 
        \textit{Reduction base year} & 0.68 & 0.80 & 0.82 \\ 
        \textit{Reduction percentage} & 0.88 & 0.91 & 0.93 \\ 
        \bottomrule
        \end{tabular}
        
    }
    \caption{Accuracy of the Q\&A approach in assessing the quantitative properties of the net zero and reduction targets~\cite{tobias_schimanski_climatebert-netzero_2023}}
    \label{tab:qa_accuracy}
    \end{subtable}
    \caption{Reported performances for the datasets on Question/Answering and Retrieval tasks.}
    \label{tab:reported perf question asnwering (qa)}
\end{table*}  

\newpage

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.90\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llccccc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{ClimaBench} & \textbf{Pira 2.0 MCQ} & \textbf{Exeter Misinf.} & \textbf{Weight. Avg.} \\
        & & Acc. & Acc. & F1 (macro) & \\
        \midrule
        \citet{thulke2024climategpt} & Stability-3B & 71.4\% & 48.7\% & 52.6\% & 62.8\% \\
        \citet{thulke2024climategpt} & Pythia-6.9B & 63.6\% & 22.9\% & 29.9\% & 48.9\% \\
        \citet{thulke2024climategpt} & Falcon-7B & 62.9\% & 19.8\% & 39.9\% & 48.3\% \\
        \citet{thulke2024climategpt} & Mistral-7B & 73.1\% & 19.0\% & 63.7\% & 53.7\% \\
        \citet{thulke2024climategpt} & Llama-2-7B & 68.5\% & 51.1\% & 59.4\% & 62.6\% \\
        \citet{thulke2024climategpt} & Jais-13B & 66.9\% & 26.4\% & 54.2\% & 54.4\% \\
        \citet{thulke2024climategpt} & Jais-13B-Chat & 65.8\% & 66.4\% & 61.3\% & 64.6\% \\
        \midrule
        \citet{thulke2024climategpt} & Llama-2-Chat-7B & 67.8\% & 72.0\% & 64.3\% & 68.5\% \\
        \citet{thulke2024climategpt} & Llama-2-Chat-13B & 68.6\% & 79.3\% & 68.6\% & 71.4\% \\
        \citet{thulke2024climategpt} & Llama-2-Chat-70B & 72.7\% & 88.2\% & 72.5\% & 77.8\% \\
        \midrule
        \citet{thulke2024climategpt} & ClimateGPT-7B & 75.3\% & 86.6\% & 65.9\% & 77.1\% \\
        \citet{thulke2024climategpt} & ClimateGPT-13B & 75.0\% & 89.0\% & 70.0\% & 78.0\% \\
        \citet{thulke2024climategpt} & ClimateGPT-70B & 72.4\% & 89.9\% & 72.5\% & 77.7\% \\
        \midrule
        \citet{thulke2024climategpt} & ClimateGPT-FSC-7B & 59.3\% & 17.2\% & 45.1\% & 46.2\% \\
        \citet{thulke2024climategpt} & ClimateGPT-FSG-7B & 53.1\% & 17.4\% & 41.5\% & 42.1\% \\
        \bottomrule
        \end{tabular}
        }
        \caption{F1 scores (macro average) and Accuracy on multiple climate benchmarks~\cite{thulke2024climategpt}.}
        \label{tab:climate-benchmarks}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
            \toprule
                \textbf{Source} & \textbf{Model} & \textbf{F1} \\
        
            \midrule
            \citet{spokoyny2023answering} & Majority & 20.10\% \\
            \citet{spokoyny2023answering} & Random & 24.09\% \\
            \citet{spokoyny2023answering} & SVM & -\% \\
            \citet{spokoyny2023answering} & BERT & 70.57\% \\
            \citet{spokoyny2023answering} & RoBERTa & 71.14\% \\
            \citet{spokoyny2023answering} & DistilRoBERTa & 69.27\% \\
            \citet{spokoyny2023answering} & Longformer & 67.72\% \\
            \citet{spokoyny2023answering} & SciBERT & 68.45\% \\
            \citet{spokoyny2023answering} & ClimateBERT & 69.44\% \\
            \bottomrule        
        \end{tabular}
        }
        \caption{F1-scores (macro average) on ClimaBench~\cite{spokoyny2023answering}}
        \label{tab:appendix climabench}
    \end{subtable}
     \begin{subtable}[t]{0.45\textwidth}
        \vspace{-2.32cm}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{Acc.} \\
            \midrule
            \citet{s_vaghefi_chatclimate_2023} & hybrid ChatClimate & 4.38 \\
            \citet{s_vaghefi_chatclimate_2023} & ChatClimate & 4.15 \\
            \citet{s_vaghefi_chatclimate_2023} & GPT-4 & 2.62 \\
            \bottomrule
        \end{tabular}
        }
        \caption{Accuracy on a set of 10 questions, human annotations~\cite{s_vaghefi_chatclimate_2023}.}
        \label{tab:climateIPCC}
    \end{subtable}
    \caption{Reported performance for the datasets on Question/Answering (few-shot and RAG) Tasks.}
     \label{tab:climate QA}
\end{table*}

\newpage

\subsection{Deceptive Technique}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.44\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{p{2cm}p{1.7cm}P{2cm}}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\
            \midrule
            \multicolumn{3}{c}{Trained on Logic applied on LogicCLimate} \\
            \midrule
            \citet{jin-etal-2022-logical} & Electra-SA & 22.72\% \\
            \citet{jin-etal-2022-logical} & Electra &  27.23\% \\
            \multicolumn{3}{c}{Trained on LogicClimate} \\
            \midrule
            \citet{jin-etal-2022-logical} & Electra-SA & 23.71\% \\
            \citet{jin-etal-2022-logical} & Electra & 29.37\% \\
            \multicolumn{3}{c}{Baseline: Trained on Logic applied on Logic} \\
            \midrule
            \citet{jin-etal-2022-logical} & Electra-SA & 58.77\% \\
            \citet{jin-etal-2022-logical} & Electra & 53.31\% \\
            \citet{jin-etal-2022-logical} & BERT & 45.80\% \\
            \bottomrule
        \end{tabular}
        }
        \caption{F1-scores (micro average) on fallacy detection~\cite{jin-etal-2022-logical}. Electra-StructureAware is abbreviated Electra-SA}
        \label{tab:appendix-fallacies}
    % \end{subtable}
    % \begin{subtable}[t]{0.45\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{p{2cm}p{1.7cm}P{2cm}}
            \toprule
                \textbf{Source} & \textbf{Model} & \textbf{F1} \\
                \midrule
                \citet{bhatia_automatic_2021-1} & BERT & 59\% \\
                \citet{bhatia_automatic_2021-1} & BERT* & 62\% \\
                \citet{bhatia_automatic_2021-1} & MTEXT & 66\% \\
                \citet{bhatia_automatic_2021-1} & MTEXT* & 67\% \\
                \citet{bhatia_automatic_2021-1} & MTEXT$_{multi}$ & 68\% \\
                \citet{bhatia_automatic_2021-1} & MTEXT*$_{multi}$ & 67\% \\
                \citet{bhatia_automatic_2021-1} & SVM & 49\% \\
                \citet{bhatia_automatic_2021-1} & Human & 70\% \\
                \bottomrule
        \end{tabular}
        }
        \caption{F1-score (micro average) on neutralization techniques classification~\cite{bhatia_automatic_2021-1}}
        \label{tab:appendix-bhatia-neutralisation}
    \end{subtable}
    \begin{subtable}[t]{0.54\textwidth}
        \centering
        \vspace{-2.6cm}
        \resizebox{\textwidth}{!}{
            % \begin{tabular}{llccc}
            % \midrule
            % Source & Model & F1-Score (\%) \\ \midrule
            % \citet{Meddeb2022CounteractingFF} & SVM—6 linguistic features & 58.18 \\
            % \citet{Meddeb2022CounteractingFF} & MNB—bag of words (k = 300) & 77.38 \\
            % \citet{Meddeb2022CounteractingFF} & SVM—bag of words (k = 300) & 76.83 \\
            % \citet{Meddeb2022CounteractingFF} & SVM—bag of words + 6 linguistic features & 77.02 \\ \midrule
            % \citet{Meddeb2022CounteractingFF} & BERT on articles & 83.90 \\
            % \citet{Meddeb2022CounteractingFF} & BERT on paragraphs & 84.96 \\
            % \citet{Meddeb2022CounteractingFF} & BERT on sentences & 64.24 \\
            % \citet{Meddeb2022CounteractingFF} & BERT on articles + 6 linguistic features & 84.75 \\ \midrule
            % \end{tabular}            
        % }
        % \caption{Results on \citet{Meddeb2022CounteractingFF}}

        \begin{tabular}{llc}
    \toprule
    \textbf{Source} & \textbf{Model} & \textbf{F1 (macro)} \\
    \midrule
    \citet{coanComputerassistedClassificationContrarian2021} & Logistic (unweighted) &  68\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & Logistic (weighted) &  72\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & SVM (unweighted) & 66\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & SVM (weighted) &  72\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & ULMFiT & 72\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & ULMFiT (weighted) & 65\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & ULMFiT (over sample) &  55\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & ULMFiT (focal Loss) &  61\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & ULMFiT-logistic &  75\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & ULMFiT-SVM & 71\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & RoBERTa &  77\% \\
    \citet{coanComputerassistedClassificationContrarian2021} & RoBERTa-logistic & 79\% \\    
    \citet{coanComputerassistedClassificationContrarian2021} & Coder (Human) \textcolor{white}{bl}Acc (macro):&  87\% \\
    \citet{thulke2024climategpt} & Llama-2-Chat-70B\textcolor{white}{bla} F1 (Bin.):& 72.5\% \\
    \citet{thulke2024climategpt} & ClimateGPT-70B\textcolor{white}{blah} F1 (Bin.):& 72.5\% \\
    \bottomrule
    \end{tabular}
    }
    \caption{F1-score (macro average and Binary) and Accuracy (macro average) on contrarian claim classification~\cite{coanComputerassistedClassificationContrarian2021}.}
    \label{tab:perf_contrarian_claim}
    \end{subtable}
    \caption{Deceptive techniques}
    \label{app:table deceptive techniques}
\end{table*}

\newpage

\subsection{Environmental Performance Prediction}

\begin{table*}[ht]
    \centering
    \begin{subtable}[t]{0.47\textwidth}
        \centering
        \vspace{-31pt}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llc}
        \toprule
            \textbf{Source} & \textbf{Model} & \textbf{MSE} \\
        \midrule
            \citet{linSUSTAINABLESIGNALSijcai2023} & Baseline & 1.173 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & Lasso & 0.848 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & Gradient Boosting & 0.818 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & NOCATE(RB) & 0.776 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & NOCATE(CB) & 0.763 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & NOCATE(DB) &  0.756 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & SUSTAINABLESIGNALS(RB) & 0.762 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & SUSTAINABLESIGNALS(CB) & 0.753 \\
            \citet{linSUSTAINABLESIGNALSijcai2023} & SUSTAINABLESIGNALS(DB) & 0.736 \\
        \bottomrule
        \end{tabular}
        }
        \caption{MSE on Finch Score prediction (Finch Score range between 0 and 10)~\cite{linSUSTAINABLESIGNALSijcai2023}}
    % \end{subtable}
    % \begin{subtable}[t]{0.47\textwidth}
    \resizebox{\textwidth}{!}{
         \begin{tabular}{lp{3cm}cc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{F1} \\
            \midrule
            \citet{clarkson_nlp_us_csr} & Random Forest & 89.48\% \\
            \citet{clarkson_nlp_us_csr} & XGBoost &  95.73\% \\
            \bottomrule
        \end{tabular}
        }
        \caption{F1-score (macro average) on good/bad CSR performer classification~\cite{clarkson_nlp_us_csr}.}
        \label{tab:perf clarkson_nlp_us_csr}
    \end{subtable}
     \begin{subtable}[t]{0.47\textwidth}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{llcc}
        \toprule
        \textbf{Source} & \textbf{Model} & \textbf{RMSE} & \textbf{wMAPE} \\
        \midrule
            \citet{bronzini_glitter_2023} & OLS & 7.76 & 7.9\% \\
            Uniform & Random & 40 & 66\% \\
            Normal & Average & 20 & 31\% \\
        \bottomrule
        \end{tabular}
        }
        \caption{RMSE and wMAPE on ESG score prediction (ESG score range between 0 and 100)~\cite{bronzini_glitter_2023}. We added the average RMSE and wMAPE for a uniform distribution of score with a random classifier; and for a normal distribution (mean=50, scale=20) with the average value as prediction.}
        \label{tab:esg bronzini}
    % \end{subtable}
    % \begin{subtable}[t]{0.47\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
         \begin{tabular}{llcc}
            \toprule
            \textbf{Source} & \textbf{Model} & \textbf{MAE} & \textbf{RMSE} \\
            \midrule
            \citet{Greenscreen} & CLIP+MLP & 0.54 & 0.88 \\
            \citet{Greenscreen} & Sent.-BERT+MLP & 0.57 & 0.91 \\
            Uniform & Random & 3.97 & 4.88 \\
            Gaussian & Average & 1.59 & 1.99 \\
            \bottomrule
        \end{tabular}
        }
        \caption{MAE and RMSE on ESG unmanaged risk score prediction (ESG risk between 0 and 13.5)~\cite{Greenscreen}. We added the average RMSE and MAE for a uniform distribution of score with a random classifier; and for a normal distribution (mean=7, scale=2) with the average value a prediction.}
        \label{tab:perf greenscreen}
    \end{subtable}    
    \begin{subtable}[t]{0.55\textwidth}
     \resizebox{\textwidth}{!}{
        \begin{tabular}{llP{1.5cm}P{2cm}}
        \toprule
            \textbf{Source} & \textbf{Model} & \textbf{Accuracy (Change)} & \textbf{Accuracy (Direction of change)} \\
        \midrule
            \citet{Mehra_2022} & majority class & 0.5791 & 0.5682 \\
            \citet{Mehra_2022} & BERT & 0.5985 & 0.4317 \\
            \citet{Mehra_2022} & ESGBERT & 0.6709 & 0.793 \\
        \bottomrule
        \end{tabular}
    }
        \caption{Accuracy on ESG-risk change classification~\cite{Mehra_2022}}
        \label{tab:appendix mehra}
    \end{subtable}
    \caption{Environmental Performance Prediction}
    \label{tab:appendix env pred}
\end{table*}

