\section{Theoretical analysis}
We prove two theorems that give bounds for the quality of the solutions, which are expected to be reached with the cost functions. We have the same initial assumptions for both proofs, which we state next. We consider that dynamic programming (DP) and greedy algorithms perform as many steps as we perform joins. We assume the reader is familiar with these two algorithms but have included their definitions in the appendix. 

By Def.~\eqref{def:binary_variables} of the binary variables, the number of steps coincides with the rank index in the cost functions. Thus, we can prove the theorems by induction on the rank parameter $r$ in the proposed algorithms, as well as the steps in the DP and greedy algorithms, by showing that for each $r$, there exists a solution from HUBO that corresponds to the same plan computed by the classical algorithms.

\begin{theorem}\label{thm:dp_bound}
Let  $H_{\text{cost}}$ be the cost HUBO defined in Subsection \ref{subsection:cost_function} and let $H_{\text{val}}$ be the binary formulation for the join order validity constraints. Let $x$ be the point that minimizes the full problem $H_{\text{cost}} + CH_{\text{val}}$. Then, the join order cost $H_{\text{cost}}(x)$ is equal to the cost computed by the dynamic programming algorithm without cross-products.
\end{theorem}
\begin{proof}
First, assume that the rank is $0$, and we are at the first iteration in the dynamic programming (DP) algorithm. By Alg.~\ref{alg:hubo_term_construction} (line 5), we include all the joins between the leaf tables to the cost function $H_{\text{cost}}$. Considering the DP algorithm, we compute the same costs for joining the leaf tables and include the combinations and costs in the DP table. Thus, the HUBO encodes the same plans as the DP algorithm.

Let us assume that we are at rank $r > 0$, and we have applied Algorithm \ref{alg:hubo_term_construction} and the DP algorithm up to $r - 1$ steps. The DP algorithm considers all the intermediate joins from the previous step (i.e., at rank $r-1$). For each of these joins, it performs the possible join for each table that is not yet included in the intermediate result with respect to the query graph. These results are kept in the DP table for the next iteration. While the DP algorithm keeps the total cost of each intermediate result in the DP table, our cost objective $H_{\text{cost}}$ encodes only the ''local'' intermediate costs, which are computed with Eq.~\eqref{eq:term_coefficient}. Based on Algorithm \ref{alg:hubo_term_construction}, we take the terms of rank $r-1$ and compute the corresponding new terms (i.e., intermediate join plans) and coefficients (i.e., intermediate join costs) for each table that is not yet included in the intermediate result with respect to the query graph. Instead of computing the total cost, we add the terms and coefficients to the cost objective $H_{\text{cost}}$. This process encodes the same cost that is stored in the DP table because we can choose to activate those $H_{\text{cost}}$ terms, which produce the total cost for each value stored in the DP table. This leads to the claim that there is a point where $H_{\text{cost}}$ achieves the same cost as the DP algorithm.
\end{proof}

\begin{theorem}\label{thm:greedy_bound}
Let $H_{\text{cost}}$ be the heuristic method's cost HUBO defined in Subsection \ref{subsection:heuristic_cost_function}, and let $H_{\text{val}}$ be the binary formulation for the join order validity constraints. Let $x$ be the point that minimizes the full problem $H_{\text{cost}} + CH_{\text{val}}$. Let $C_{\text{greedy}}$ be the cost computed by the greedy algorithm without cross-products. Then, $H(x) \leq C_{\text{greedy}}$, i.e., the cost from the greedy algorithm gives an upper bound for the cost from the heuristic algorithm.
\end{theorem}
\begin{proof}
First, assume that the rank is $0$, and we are at the first iteration in the greedy algorithm. By the definition of the heuristic method in Subsection \ref{subsection:heuristic_cost_function}, we include all terms corresponding to the joins between leaf tables to the cost function $H_{\text{cost}}$. The greedy algorithm includes only one join with the minimum cost to the join tree at the same step. Thus, our heuristic method encodes the first step of the greedy algorithm.

Let us consider a general rank $r > 0$ and assume that we have applied the heuristic HUBO construction and the greedy algorithm up to $r - 1$ steps. By the definition of our heuristic method, we select a subset of $n$-many table combinations from the previous rank $r - 1$ with the smallest coefficients. Simultaneously, in the greedy algorithm, we compute the costs of joining the previous step's intermediate result with the possible joins that are left with respect to the query graph and keep the join tree with the minimum total cost. This minimum total cost is always achieved by including the join tree corresponding to the terms with the smallest coefficient in the $H_{\text{cost}}$ at rank $r$. Since the terms with the smallest coefficients correspond to the cheapest plans, the heuristic algorithm will encode the same plan that the greedy algorithm finds. For larger numbers of $n$, $H_{\text{cost}}$ will also include other intermediate results. Since our heuristic method and the greedy algorithm keep the join plans with the minimum cost at each step, we can deduce that our algorithm encodes the same plan (and others depending on value $n$) as the greedy algorithm. This leads to the claim that $H(x) \leq C_{\text{greedy}}$.
\end{proof}

Besides the theorems, our method archives advantageous variable scalability compared to the most scalable methods \cite{Schonberger_Scherzinger_Mauerer, 10.14778/3632093.3632112}. The model in \cite{DBLP:conf/q-data/SaxenaSS24} uses the same variable definitions as \cite{10.14778/3632093.3632112}, so the scalability comparison also applies to this paper as well. The scalability is visualized in Fig.~\ref{fig:variable_scalability_cycle} for cycle query graphs. The chain, star, and tree graphs have identical relative scalability in all three cases. We want to point out that these are the only mandatory variables for the two compared methods. The previous techniques require more variables to estimate the cost thresholds, which depend on the problem. We excluded the variable scalability of \cite{Nayak_Winker_Groppe_Groppe_2024} since the growth of variable count is exponential in their work.

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/variable_scalability_cycle.png}
    \caption{We compare the number of mandatory variables in \cite{Schonberger_Scherzinger_Mauerer,10.14778/3632093.3632112} to all variables in our optimization model.}
    \label{fig:variable_scalability_cycle}
    \Description[Comparison of variable scalability between this and previous methods]{The plot shows that our method has the best variable scalability compared to the previous methods.}
\end{figure}