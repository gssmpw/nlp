\section{Discussion}

The starting point for our work has been primarily the previous quantum computing formulations \cite{Schonberger_Scherzinger_Mauerer, Winker_Calikyilmaz_Gruenwald_Groppe_2023, Schonberger_Trummer_Mauerer_2023, Nayak_Winker_Groppe_Groppe_2024, Franz_Winker_Groppe_Mauerer_2024,10.14778/3632093.3632112, DBLP:conf/q-data/SaxenaSS24} for join order selection problem. Compared to previous quantum-based join order selection research, our method has a formal guarantee of the quality of the results; our cost function models the intermediate results more accurately, and the number of binary variables is the lowest. We have also presented a fundamentally different approach to the well-researched problem since the previous methods often relied on the MILP formulation \cite{Trummer_Koch_2017}.

We have performed an extensive experimental evaluation, and the results are shown in the appendix. The experimental results demonstrate that the Theorems \ref{thm:dp_bound} and \ref{thm:greedy_bound} are respected well in practice. Our methods' scalability outperforms many previous works \cite{Schonberger_Scherzinger_Mauerer, Franz_Winker_Groppe_Mauerer_2024, Schonberger_Trummer_Mauerer_2023}, where the authors have demonstrated their algorithms with 2 to 7 relations. The method proposed in \cite{10.14778/3632093.3632112} has the best scalability and accuracy, but the algorithm lacks a guarantee of optimality and uses more variables. Still, the comparison to this approach shows accuracy similar to our method. Our evaluation also explores the method in \cite{10.14778/3632093.3632112} for the query graphs not studied in the original paper. The detailed results are in the appendix. We also note that classical solvers compete with the quantum annealers in this task even though they run locally on a laptop. This demonstrates that quantum annealers are not scalable enough to solve arbitrary problems, but their performance is crucially problem-dependent.

%While one can argue that our query graphs for methods Precise 1 and 2 are relatively small, they are still substantially bigger than the previous fully-quantum solutions considered. For example, \cite{Schonberger_Scherzinger_Mauerer} (published in SIGMOD 2023) evaluated their method with query graphs with $2$ to $5$ relations and obtained $0\%$ optimal solutions with $5$ tables, and only at most $0.15\%$ were valid. Compared to \cite{Nayak_Winker_Groppe_Groppe_2024}, the authors optimized queries with $7$ relations, but in the case of $7$ relations, they obtained almost no optimal results from the quantum computer. The other quantum computing-based methods to solve the join order selection problem have utilized quantum machine learning \cite{Winker_Calikyilmaz_Gruenwald_Groppe_2023,Franz_Winker_Groppe_Mauerer_2024}, which is a fundamentally different approach from binary optimization methods. In both cases, the authors considered queries with four relations. 

%The improvement \cite{10.14778/3632093.3632112} (published in VLDB 2023) to \cite{Schonberger_Scherzinger_Mauerer} represents some of the best results so far where the authors used the quantum-inspired digital annealer. They could scale up to 50 queries with their updated QUBO formulation, a new read-out method, and quantum-inspired hardware. %On the other hand, it is difficult to compare their results to the optimal plans precisely; for example, they omitted results from the queries having fewer than 18 tables, which would be cases you can solve exactly. Their scaled results do not highlight subtle differences that we investigated in our methods. In our evaluation, we studied accuracy with four-digit precision, and the difference started to appear after we summed the cumulative error over 20 different graphs. Also, \cite{DBLP:conf/q-data/SaxenaSS24} obtained good scalability results with a constrained quadratic method but compared their method only against the results of \cite{10.14778/3632093.3632112}. All of these methods are heuristic and have no theoretical guarantee of optimality of the join tree, even if we assume that we can find the optimal point for the binary optimization problem.

%We noticed the difference in the used cost functions when evaluating our method against the digital annealer method \cite{10.14778/3632093.3632112}, which currently makes the precise comparison difficult. The digital annealing method used the cost function from \cite{10.1007/3-540-58907-4_6}. Our and their cost functions encode the same optimal plans, but the other good plans are not necessarily the same, which makes the comparison unfair. In our and their works, the binary optimization problem is tightly built around the cost function's properties, so changing the cost function in either method would require substantial changes to the models. We still believe it is possible in both cases, and we are interested in extending this work to include alternative cost functions and join operations.

%The previous research \cite{10.14778/3632093.3632112} suggested using a special readout method that utilizes the results from quantum annealers, although they might be erroneous. This classical postprocessing method likely increases the quality of solutions substantially. In our work, we can also obtain a join order in many cases in which the result contains some errors due to the stochastic nature of the solvers.

%Due to computing cumulative cost over 20 randomly initialized query graph instances, we argue that the experimental setup is substantially more demanding and realistic than the previous quantum computing research experiments. Unfortunately, we did not have special access to quantum or quantum-inspired hardware. If more quantum computing resources are accessible, some results from quantum computers, especially D-Wave quantum annealer without hybrid features, would benefit from hyperparameter tuning.

%While these methods do not yet scale with the previous QUBO formulations, they have many exciting characteristics from quantum computing and machine learning perspectives.

%Most previous research developed validity constraints based on three binary variable types \cite{Schonberger_Scherzinger_Mauerer, Schonberger_Trummer_Mauerer_2023,10.14778/3632093.3632112, DBLP:conf/q-data/SaxenaSS24}. Our model can express validity with one or two variable types, depending on whether the query graph is a clique. We could also use substantially fewer variables, as Fig.~\ref{fig:variable_scalability_cycle} shows. In the best case, the variables are in one-to-one correspondence with the qubits, so it is important to use models that minimize their number. Moreover, we can precisely compute the cost function with the modified binary variable selection. This has a huge benefit from the usability perspective: we do not need to adjust the cost approximation thresholds manually \cite{Schonberger_Scherzinger_Mauerer, Schonberger_Trummer_Mauerer_2023, 10.14778/3632093.3632112, DBLP:conf/q-data/SaxenaSS24}.

%Previous research has not proved theoretical bounds for their formulations. In addition to providing important performance information, bounds represent a conceptually and theoretically significant contribution that allows us to relate the performance of quantum algorithms to these two standard classical methods. It helps us comprehend the connection between quantum and classical, which is usually complex.

The biggest challenge in our method is the higher-order terms. Very few methods can effectively tackle HUBO optimization problems. In this regard, quantum computing appears to be theoretically one of the most promising approaches to optimizing complex HUBOs. In future research, we are excited to study universal quantum computing capabilities to solve HUBOs.

%All binary optimization formulations also suffer because the binary optimization problem needs to be constructed, which might make the pre-processing phase expensive. Recently, \cite{DBLP:conf/q-data/TrummerV24} proposed a method where the problems are ''templates''. Having problems reconstructed offline and coefficients modified, we would be able to speed up the problem construction in many cases. 

% \begin{itemize}
%     \item Our models have a theoretical guarantee for optimal results up to cross-products.
%     \item Our models require fewer variables and fewer variable types.
%     \item Our methods do not approximate the join order cost function, but the cost is computed precisely, increasing accuracy and removing the need for tuning the cost estimation thresholds \cite{Schonberger_Scherzinger_Mauerer,10.14778/3632093.3632112}.
%     \item Our models are not a quantum adaption of a classical algorithm (as some of the previous quantum solutions are adaptions of MILP formulation \cite{Trummer_Koch_2017}) but the first higher-order unconstrained binary model to solve the problem.
%     \item In the experimental evaluation, we do not pick the best runs, as was done in \cite{10.14778/3632093.3632112}, but average performance over multiple varying query graphs. This shows that the performance of the methods is consistent.
% \end{itemize}

% \begin{itemize}
%     \item We formulated the optimization problem from the perspective of joins instead of the perspective of tables. The perspective is relatively different from the previous work.
%     \item Extending to non-inner joins, especially to outer joins
%     \item First, optimal HUBO formulation requires expensive preprocessing. While the HUBO depends only on the query graph, and the processing could even be done offline so that cardinalities and selectivities are inserted after the HUBO construction, this HUBO likely contains too many terms to be reasonably evaluated on any near-term quantum computer for query graphs that are not solvable with dynamic programming. Thus, there is no quantum advantage.
%     \item Heuristic algorithms seem to work well, but they mainly work well because of the classical preprocessing. The outputted HUBO is very easy to solve. Thus, also this method does not show that quantum computing would provide any advantage on the topic. 
%     \item Although both methods have pros and cons, the most interesting contribution is the algorithmic design ideas and the suggestion of using HUBOs with more interactions. 
% \end{itemize}

% \subsection{Comparison to previous works}

% The join order optimization problem is the most studied database optimization problem from a quantum computing perspective. Thus, we can compare this method with previous quantum algorithm design and performance work.

% If we are realistic, quantum computational methods are not yet at the level where it would be useful to integrate them into database optimization routines. We need improvements on every front before this integration becomes beneficial: quantum algorithm design for database problems, classical pre-and post-processing, and quantum hardware itself. 

% Key positive differences to the previous works:
% \begin{itemize}
%     \item The four quantum algorithms are developed with a focus on balancing accuracy and quantum computing resources.
%     \item We do not need to assume that the query graph is acyclic and the cost function does not need to satisfy so-called ASI-property (IKKBZ algorithm requirements)
%     \item The model naturally encodes cross products if needed
%     \item Join order cost is encoded in the weights instead of encoded using binary variables (Leap paper). This reduces the number of variables in the higher-order model, although it increases the number of interactions. We argue that encoding values into qubit interactions is cheaper than introducing new qubits.
%     \item We do not need to create $2^n$ variables corresponding to every possible subset (Sven's paper)
%     \item We compare the proposed method's solutions' quality to dynamic programming
%     \item The cost function is encoded in the model: we do not need to evaluate the total cost for any query plan before optimizing the final objective. In dynamic programming, the total cost for every sub-plan is constantly evaluated and stored in the DP table
%     \item The results demonstrate how the algorithm works to benchmark the solvers in a realistic problem and give insights into the current performance of these systems with respect to each other. This information is extremely crucial when the industry decides to start adopting these systems.
% \end{itemize}
% Challenges:
% \begin{itemize}
%     \item We introduce higher-order terms that make the optimization problem harder. Moreover, the number of these terms increases exponentially with respect to the size of the query graph.
%     \item Quadratization
% \end{itemize}