\begin{figure}[t]
  \small\centering
  \begin{tikzpicture}[scale=.5]
    \node[draw](Image) at (0,0) {\includegraphics[width=.1\textwidth]{figures/Dog.jpeg}};
    \node[draw,rectangle,minimum height=2cm](Backbone) at (4,0) {
        \begin{turn}{270}\begin{minipage}{1.5cm}\Large \centering Backbone\\ Network\end{minipage}\end{turn}
    };
    \node[draw,rectangle](Features) at (6.5,0) {\begin{turn}{270} Embeddings $\vec \varphi$\end{turn}};
    \node[draw,rectangle](Logits) at (10.3,0) {\begin{turn}{270} Logits $\vec z$\end{turn}};
    \node[draw,rectangle](Probabilities) at (13,0) {\begin{turn}{270} Confidences $\vec y$\end{turn}};

    % TODO: Add branches for GHOST

    \draw[->,thick] (Image) -> (Backbone);
    \draw[->,thick] (Backbone) -> (Features);
    \draw[->,thick] (Features) -> node [above] {\begin{turn}{270} Linear \end{turn}} node [below]{\begin{turn}{270} $\mathbf W$\end{turn}} (Logits);
    \draw[->,thick] (Logits) -> node [above] {\begin{turn}{270} SoftMax\end{turn}} (Probabilities);


    \node[draw, rectangle, minimum height=1.5cm, minimum width=0.9cm] (GHOST) at (8,-4.5) {\begin{turn}{270}\begin{minipage}{1.5cm}\centering Gaussians\\[1ex]$\{(\vec\mu_k,\vec\sigma_k)\}$ \end{minipage}\end{turn}};
     \node[draw, rectangle, minimum height=1.5cm, minimum width=0.5cm] (Scoring) at (10.3,-4.5) {\begin{turn}{270} GHOST\end{turn}};

    \node[draw, rectangle, minimum height=1.5cm, minimum width=.9cm] (OOD) at (12.5,-4.5) {\begin{turn}{270}\begin{minipage}{1cm} \centering GHOST score $\gamma$ \end{minipage}\end{turn}};

    \draw[->,thick,dashed] (Features) |- (GHOST);
    \draw[->,thick,dashed] (Logits) -- (Scoring);
    \draw[->,thick,dashed] (GHOST) -- (Scoring);
    \draw[->,thick,dashed] (Scoring) -- (OOD);
%    \draw[->,thick,dashed] (OOD) -- node [below] {\begin{turn}{270} SoftMax\end{turn}} (Probabilities);
    
    
  \end{tikzpicture}
  \Caption[fig:processing]{GHOST Scores}{In a pre-trained network indicated with solid arrows, an image is presented to the backbone network, which extracts deep feature embeddings $\vec\varphi$ that are then processed with a Linear layer to logits $\vec z$, and further with SoftMax to probabilities $\vec y$. 
    For training GHOST, we extract embeddings from training data, from which we model class-wise multivariate Gaussian distributions. 
    During evaluation, the Gaussian of the predicted class are used to turn the embeddings $\vec\varphi$ into z-score, which is used together with the maximum logit $z_{\hat k}$ to compute the GHOST score $\gamma$.}
\end{figure}
