
\section{Approach}
\subsection{A Gaussian Hypothesis}
\label{sec:approach}

The first works on open-set recognition and open-set deep networks \cite{scheirer2012toward,scheirer2014probability,bendale2016openmax,rudd2017evm} all focused on the most distant points within a class or the values at the class boundaries.  
Hence, it is natural that they employed extreme-value theory as their underlying model. 
Having evaluated many of these EVT-based approaches in practical settings, we found a few significant difficulties: These methods are sensitive to outliers/mislabeled data (due to their reliance on a small percentage of extreme data) and have a high cost and sensitivity of tuning their hyperparameters.  
A final difficulty with this approach is reducing the high-dimensional features into a 1-dimensional distance, typically Euclidean or Cosine.


Features within a DNN are learned using large amounts of data.   
Various papers have shown that, with some mild assumptions, convergence in a two-layer network follows a central-limit theory \cite{sirignano2020mean}, and using a mean-field analysis that was extended to some older deep network architectures \cite{lu2020mean} -- so there are inherently some reasons to hypothesize Gaussian models. 

We start by summarizing the main simple NN central-limit theory of \cite{sirignano2020mean}, which indicates that for a large number $M$ of neurons, the empirical distribution of the neural network's parameters behaves as  Gaussian distribution. 
Their theorems show that, given their assumptions, the empirical distribution of the parameters behaves as a Gaussian distribution with a specific variance-covariance structure.  
Central to the proofs of these theories is mean-field theory, and the convergence of the parameters to the mean follows from the central limit theorem. These mean-field distributional convergence results were then extended to some older deep networks \cite{lu2020mean}, but extending to new networks is complex.  
We believe that empirical testing, as we do in our experiments, is a sufficient and much easier way to evaluate the Gaussian hypothesis for any new network.

Inspired by those theories, we hypothesize that similarly, when input is from a class seen in training, each value in the network can be reasonably approximated by a multivariate Gaussian and that, importantly, out-of-distribution samples would be more likely to be inconsistent with the resulting Gaussian model.  
While the theories of \citet{sirignano2020mean,lu2020mean} are about the learnable network parameters, we hypothesize that with a Gaussian distribution per parameter, after many layers of computation, for a set of inputs from a given class, the distribution of each embedding value may also be well modeled with a Gaussian. 
Critical in this hypothesis is that for at least the embedding $\vec\varphi$ as shown in \fig{processing}, which is used to compute the per-class logits, Gaussian models are class-specific;  \fig{GHOST} shows an example model with sample values for a known and outlier. 

Due to the complexities and variations of modern DNN architectures, formally proving that this hypothesis is valid for every DNN is impractical and unlikely.  Instead, we derive a technique from this hypothesis and apply it to the most well-performing, publicly available architectures to prove its utility.



\input{figures/Network}

\begin{figure*}[bth]
    \centerline{\includegraphics[width=0.85\textwidth]{figures/GuassianModels.pdf}}
  \Caption[fig:GHOST]{GHOST modeling of a Multivariate Gaussian per Class}{Samples of Gaussians from the MAE-H network are shown on the left, sampled once every 30 dimensions.  Dimensions were sorted on mean value to improve visibility, and the spread shows how some dimensions have greater variance than others.    The plot also shows the value of per-dimension z-scores associated with a correctly classified hammerhead image (known in green) and an OOD example with a shark (red) misclassified as a hammerhead.  The z-scores of the OOD example are much larger than those of the known.  More examples in supplemental material. }
\end{figure*}

\paragraph{GHOST Training.} Consider the network processing shown in \fig{processing}.
Given a training dataset $\mathcal X=\{(x_n,t_n)\mid 1\leq n\leq N\}$ with $N$ samples $x_n$ and their class labels $1\leq t_n\leq K$ representing $K$ known classes.
Here, we apply post-processing, so we assume the backbone network to be trained on the same $K$ classes contained in $\mathcal X$.
For each correctly-classified training sample, we use the backbone to extract its $D$-dimensional embedding $\vec\varphi\in\mathbb R^D$.
For each class $k$, we model a multivariate Gaussian distribution with mean $\vec \mu_k$ and diagonal covariance $\vec\sigma_k$ from the samples of that class and collect these Gaussians for all classes as $\mathcal G = \{(\vec\mu_k, \vec\sigma_k)\mid 1\leq k\leq K\}$ via:
\begin{equation}
    \begin{aligned}
    \vec\mu_k = \frac1{N_k}&\sum\limits_{(x_n,t_n)\in\mathcal X} \mathbb I(k,t_n)\cdot \vec\varphi_n  \\ 
    \vec\sigma_k^2 = \frac1{N_k-1}&\sum\limits_{(x_n,t_n)\in\mathcal X} \mathbb I(k,t_n)\cdot(\vec\mu_k-\vec\varphi_n)^2
    \end{aligned}
\end{equation}
where the indicator function $\mathbb I(k,t_n)$ is 1 if the sample $x_n$ belongs to class $k$ and is correctly classified,\footnote{This restriction reduces the influence of mislabeled samples.} and $N_k$ is the number of correctly-classified samples for class $k$.
Hence, each feature dimension of each known class is modeled from its training data.
Together, these Gaussian models are useful for differentiating unknown samples.


\iffalse
\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
  \Input{A training dataset $T$, containing $K$ classes. }
  \Input{$F(x)$, a DNN's an $N$-dimensional pre-logit feature Vector of sample $x$;  $F(x)[n]$ be the value at dimension $n$ of $F(x)$}
  \Output{Vector of Gauss\_models with mean and standard deviations}
  \Begin{
  Gauss\_models = $\emptyset$\;
  \For{ $k \in K$:}{
	FVect = $F(\forall x \in T$ from class $k$)\  //tensor of feature vectors\;
        \For{ $n \in N$}{
        $\mu_{n,k} = $ Mean(FVect[n])\  // mean at dim $n$ over samples from $k$\;
        $\sigma_{n,k} = $ STD(FVect[n])\;
        }
	  Gauss\_models[k] = $[\mu_{n,k},\sigma_{n,k}]$\;
  }
   return Gauss\_models\;
  }
  \caption{Computing per-dimensional Gaussian models over training data $T$}
  \label{a:gmodel}
\end{algorithm}
\fi

\paragraph{GHOST Inference.} Building from the open-set theory by \citet{bendale2016openmax}, we know a provably open-set algorithm is produced if the confidence scores are monotonically decreasing with the distance of a feature from a mean.
To achieve this with our Gaussian Hypothesis, we combine our model with the intuition that there are significant deviations of DNN's embedding magnitude when an unknown sample is encountered \cite{dhamija2018reducing,cruz2024oosa} and, thus, the embedding $\vec\varphi$ deviates from all class means, even though the angular direction might overlap with a certain class' mean $\vec\mu_k$.
For a given test sample, we first compute embedding $\vec\varphi$, logits $\vec z$ and the predicted class $\hat k$ as:
\begin{equation}
    \label{eq:max-class}
    \hat k = \argmax_k z_k\,.
\end{equation}
We select the associated Gaussian $(\vec\mu_{\hat k}, \vec\sigma_{\hat k})$ to compute our z-score:
\begin{equation}
    s = \sum_{d=1}^D \frac{|\varphi_d - \mu_{\hat k, d}|}{\sigma_{\hat k, d}}
\end{equation}
which is small if the embedding is close to -- and larger the more it deviates from -- the mean.
Unlike Euclidean or cosine distance to reduce dimensionality in a geometrically fixed way, this z-score-based deviation measure adapts to the inherent ``shape'' of the embeddings differently for each class.
Some classes may have large variations in some dimension $\varphi_d$, whereas others have minor variations.  

The most obvious way to use the z-score $s$ to ensure monotonically decreasing score and generate an open-set algorithm is by dividing the predicted class' logit $z_{\hat k}$:
\begin{equation}
  \gamma = \frac{z_{\hat k}}{s}\,.
\end{equation}
If the sample is close to the class mean of the predicted class $\hat k$, $\gamma$ increases in scale, whereas a large z-score $s$ lead to a reduction of $\gamma$.
Thus, thresholding on $\gamma$ to reject items as unknown or out-of-distribution is consistent with formal open-set theory.

Note that we are not normalizing the predicted $\gamma$ score, but basically threshold this score directly, comparable to MaxLogit \cite{hendrycks2022scaling}.
As compared to OpenMax, an advantage of GHOST is that the Gaussian models $\mathcal G$ are less sensitive to outliers or mislabeled data in the training set, as the contribution of any input is only to the mean and standard deviation, which reduces noise.  
In contrast, even a single outlier can dominate the computation of Weibulls  \cite{scheirer2012toward,scheirer2014probability,bendale2016openmax,rudd2017evm}. 
Furthermore, GHOST removes the necessity of selecting a tail size for Weibull fitting, and there is no need to choose a distance metric.




\iffalse
\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
  \Input{$F(x)$, a DNN's an $N$-dimensional pre-logit feature Vector of sample $x$;  $F(x)[n]$ be the value at dimension $n$ of $F(x)$}
  \Input{Gauss\_models[k] = $[\mu_{n,k},\sigma_{n,k}]$\;}
  \Input{ $x =$ test sample}
  \Input{ $\ell(x)$ the max logit value for input $x$}
  \Input{$m(x)$ the class with maximum logit value for input $x$, }
  \Output{$\gamma(x)$  = GHOST Outlier detection score}
  \Begin{
       Let  $z(x) = \sum_n{ \frac{F(x)[n]-\mu_{n,m(x)}}{\sigma_{n,m(x)}}} $ \;
       return  $\gamma(x)  = \frac{\ell(x)}{z(x)}$ \;
        
  }

  \caption{Computing GHOST Outlier Detection Score;}
  \label{a:ghost}
\end{algorithm}
\fi

\subsection{Class-Based Evaluation}
\label{sec:metrics}
When evaluating the performance of Open-Set Recognition, we make use of the Open-set Classification Rate (OSCR) \cite{dhamija2018reducing} as our primary metric since it was specifically designed to evaluate OSR performance at a given operational threshold.
While OSCR was designed for Open-Set Recognition and the effect of unknown samples, it is related to Accuracy-Rejection Curves which examine performance of systems with respect to uncertainty of new samples from known classes\cite{nadeem2009accuracy}.
We split our test dataset in known samples $\mathcal K = \{(x_n, t_n)\mid 1\leq n\leq N_K\}$ and unknown samples $\mathcal U = \{(x_n)\mid 1\leq n \leq N_U\}$ that do not have class labels.
An OSCR plot shows the Correct Classification Rate (CCR) versus the False Positive Rate (FPR):
\begin{equation}
  \begin{aligned}
      \mathrm{CCR}(\theta)&= \frac{\bigl|\{(x_n,t_n) \in \mathcal K \wedge \hat k = t_n \wedge \gamma \geq \theta \}\bigr|}{|\mathcal K|}\,,\\
      \label{eq:oscr}
      \mathrm{FPR}(\theta)&= \frac{\bigl|\{x_n \in \mathcal U \wedge \gamma \geq \theta \}\bigr|}{|\mathcal U|}\,.
  \end{aligned}
\end{equation}
For other algorithms, we replace $\gamma$ by $z_{\hat k}$ (MaxLogit), $y_{\hat k}$ (MSP), or other prediction scores for maximum class $\hat k$.
To plot the CCR at a specific FPR=$\tau$, one can invert the FPR to compute $\theta_{\tau}=\mathrm{FPR}^{-1}(\tau)$, which allows us to line up different algorithms/classes.
We also utilize the area under the OSCR curve (AUOSCR) to compare algorithms across all thresholds, but we wish to emphasize that this suffers from many of the same problems as AUROC because it combines all possible thresholds, which is not how systems operate.   

A fact that is overlooked by all OSR evaluations is the difference in performance for different classes.
Only few researchers evaluated the variance of closed-set accuracy across classes.
Since this is related to algorithmic fairness, we go a step further and compute the variances and coefficients of variation of CCR values across classes.
First, we split our test dataset into samples from certain classes $\mathcal K_k = \{x_n\mid (x_n,k) \in \mathcal K\}$ and compute per-class CCR at FPR $\tau$:
\begin{equation}
  \label{eq:per-class}
  \mathrm{CCR}_k(\theta_\tau) = \frac{\bigl|\{x_n \in \mathcal K_k \wedge \hat k = k \wedge \gamma \geq \theta_\tau \}\bigr|}{|\mathcal K_k|}
\end{equation}
Note that we do not compute per-class thresholds/FPR here;  the same set of thresholds $\theta_\tau$ is used in all classes, but is different for each algorithm.
We follow the idea of \citet{atkinson1970measurement,formby1999coefficient,xinying2023guide} and compute the mean,  variance and coefficient of variation of per-class CCR at FPR=$\tau$:
\begin{equation}
    \begin{aligned}
    \label{eq:variation}
    \mu_{\mathrm{CCR}}(\theta_\tau) = \frac1K &\sum\limits_k \mathrm{CCR}_k (\theta_\tau)\\ 
    \sigma^2_{\mathrm{CCR}}(\theta_\tau) = \frac1{K-1} &\sum\limits_k \bigl(\mathrm{CCR}_k(\theta_\tau) - \mu_{\mathrm{CCR}}(\theta_\tau)\bigr)^2 \\
    {\cal V}_{\mathrm{CCR}}(\theta_\tau) &= \frac{\sigma_{\mathrm{CCR}}(\theta_\tau)} {\mu_{\mathrm{CCR}}(\theta_\tau)}
    \end{aligned}
\end{equation}
where ${\cal V}_{\mathrm{CCR}}$ provides a commonly used measure of inequality (unfairness) that facilitates comparisons by normalizing for changes in mean values.
We evaluate $\mu_{\mathrm{CCR}}, \sigma_{\mathrm{CCR}}$ and ${\cal V}_{\mathrm{CCR}}$ at various FPR values. 
Since the CCR at FPR=1 represents closed-set accuracy, ${\cal V}_{\mathrm{CCR}}(\theta_1)$ corresponds to the unfairness in closed-set accuracy, while the associated $\mu_{\mathrm{CCR}}(\theta_1)$ represents closed-set accuracy (because the same number of samples exist per known class). 
To highlight some of the performance differentials, we also sort the classes by their closed-set accuracy and show the average CCR over the top-10 and bottom-10 percent of classes.


In prior OSR works, the Area Under the Receiver Operating Characteristic (AUROC) has been identified as an important metric for OSR evaluations.
Binary unknown rejection is rather conceptually related to OOD, but we include AUROC as a secondary metric and additionally present ROC curves in the supplemental material.
For both metrics, the curves themselves must accompany reported Area Under-statistics because area alone cannot distinguish if competing curves cross and characterize performance at specific ranges of sensitivities. 
We present more OSCR curves in the supplement.
Additionally, in a problem where 90\% of data (or risk) comes from potentially unknown inputs, having very low FPR is important and may not be easily discernable in linear plots.
For highlighting differences for high-security applications, we follow \citet{dhamija2018reducing} and plot OSRC and ROC curves with logarithmic x-axes (as in \fig{log}).
For additional quantitative insight into high-security performance, we report FPR95 in our overall results (\tab{overall_metrics}).

We also introduce a new OSR measure that avoids integrating over thresholds.
Our goal is to determine the lowest FPR that maintains a set classification accuracy level.
We call this F@C95, where we compute the FPR at the point where CCR is 95\% of the closed-set accuracy.
This measure, analogous to FPR95 in binary out-of-distribution detection, uses CCR and can be applied overall or per class.