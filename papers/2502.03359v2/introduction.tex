\section{Introduction}
\label{sec:intro}

When deploying deep neural networks (DNNs) in real-world environments, they must handle a wide range of inputs.
The ``closed-set assumption," prevalent in most evaluations, represents a significant limitation of traditional recognition-oriented machine learning algorithms \cite{scheirer2012toward}.
This assumption presumes that the set of possible classes an algorithm will encounter is known a priori, meaning that these algorithms are not evaluated for robustness against samples from previously unseen classes.
Open-Set Recognition (OSR) challenges this assumption by requiring designs that anticipate encountering samples from unknown classes during testing.

\begin{figure}[t!]
  \centerline{\includegraphics[width=0.95\columnwidth]{figures/mae_H_OpenImage_O_teaser_percentile_OSCR.pdf}}
  \Caption[fig:teaser]{Class-wise Open-Set Recognition}{OSCR comparison using the MAE-H architecture with OpenImage-O as unknowns. Overall performance is the solid line; Average performance on easy (top 10\%)  and hard (bottom 10\%) classes shown as dashed/dotted lines, respectively. We compare GHOST with Maximum Softmax Probability (MSP) and  NNGuide.  Also, we show the area under the curve (AUC) of each method's overall OSCR. GHOST outperforms in each setting and maintains its correct classification rate as the FPR rate decreases while others fall off dramatically; hence, GHOST maintains fairness in difficult cases while improving overall OSR. 
  }
\end{figure}

Often, OSR is performed by thresholding on confidence \cite{hendrycks17baseline,vaze2022openset} or having an explicit ``other'' class \cite{ge2017gopenmax} and computing overall performance, ignoring the effects of per-class performance differentials \cite{li2023accurate}.
However, evaluating recognition systems under OSR conditions is crucial for understanding their behavior in real-world scenarios.
This paper shows that as more unknowns are rejected, there is great variation in per-class accuracy, which could lead to unfair treatment of underperforming classes, see \fig{teaser}.

Recently, research has followed two primary methodologies for adapting DNNs to OSR problems: (1) training processes that enhance feature spaces and (2) post-processing techniques applied to pre-trained DNNs to adjust their outputs for identifying known and unknown samples \cite{roady2020open}.
Although OSR training methods have occasionally proven effective \cite{zhou_learning_2021, miller2021class, dhamija2018reducing}, their application is complex due to the evolving nature of DNNs and the specific, often costly training requirements for each.
If different DNNs are trained in various ways, why should a single OSR training technique be universally applicable?
Furthermore, if an OSR technique is specific to a particular DNN, its value diminishes as state-of-the-art DNNs evolve.
In contrast, post-processing methods, such as leveraging network embeddings \cite{bendale2016openmax}, are more straightforward to implement and can be applied to almost any DNN.
These methods avoid the complexities associated with training techniques and focus instead on evaluating performance.
Thus, the challenge becomes: \textbf{\emph{how can various DNNs designed with a closed-set assumption be adapted for OSR problems?}}

Initial post-processing OSR algorithms \cite{bendale2016openmax, rudd2017evm} used distance metrics in high-dimensional feature spaces to relate inference samples to known class data from training.
However, choosing appropriate hyperparameters, such as a distance metric, is not straightforward, particularly for networks trained without distance metric learning, leading to an expensive parameter search.
Further, large-scale datasets like ImageNet \cite{deng2009imagenet} and small-scale splits \cite{neal2018counterfactual, perera_generative-discriminative_2020, yang2020convolutional, geng2020recent, zhou_learning_2021} often lack suitable train-validation-test splits for a fair parameter search.

Additionally, a major limitation with prior evaluations is their emphasis on overall performance, rather than ensuring robust performance for each individual class.
This focus can obscure significant disparities between classes, leading to an incomplete understanding of the algorithm's effectiveness and potentially resulting in unfair treatment of some classes.
For example, an algorithm might achieve high overall accuracy but fail to recognize rare or challenging classes, which is critical for applications requiring high precision across all classes.
Such evaluations can misrepresent the algorithm’s performance for underrepresented or underperforming classes, which may be overlooked when only aggregate metrics are considered.
This lack of detailed analysis can lead to skewed evaluations, where the model’s weaknesses in specific areas are not addressed, ultimately affecting its real-world applicability and fairness.
While fairness is not a major concern for ImageNet-1K, the dataset used herein, we consider it a reasonable proxy for operational open-set problems due to its size and widespread use as a feature extractor or for fine-tuning domain-specific models.

We propose a novel post-processing OSR algorithm, the Gaussian Hypothesis Open-Set Technique (GHOST), which uses per-class multivariate Gaussian models with diagonal covariance of DNN embeddings to reduce network overconfidence for unknown samples.
The use of per-class modeling is crucial for ensuring fairness across all classes.
By modeling each feature dimension separately for each class, GHOST evaluates each class on its own merits, rather than grouping them together.
This technique helps address the challenge of handling the worst-performing classes fairly and reduces the risk of the model being overly confident about samples from these difficult classes.
Importantly, GHOST eliminates the need for hyperparameters, simplifying the application of OSR techniques for end-users.
Our novel GHOST algorithm improves traditional OSR measures and fairness, achieving a win-win outcome in line with recent fairness goals presented by \citet{islam2021can,li2023accurate}.


\noindent\textbf{In summary, our main contributions are}:
\begin{itemize}
    \item We introduce GHOST, a novel, state-of-the-art, hyper\-parameter-free post-processing algorithm that models per-feature, per-class distributions to improve per-class OSR.
    \item We present an extensive experimental analysis that adapts both the previous and recent state-of-the-art methods while evaluating multiple state-of-the-art DNNs, with results showing that GHOST is statistically significantly better on both global OSR and OOD metrics.
    \item We provide the first fairness analysis in OSR, identify significant per-class differences in large-scale OSR evaluations, and demonstrate that GHOST improves fairness.
\end{itemize}

