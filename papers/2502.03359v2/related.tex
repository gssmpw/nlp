\section{Related Work}
\label{sec:related}

Some methods have been proposed to improve the training of DNNS for OSR \cite{zhang2022learning,xu2023contrastive,wan2024unlocking,wang2024exploring,li2024all, li2024prototype, sensoy2018evidential}.
We do not consider these as direct competitors, as they go beyond statistical inference and train reconstruction models and use generative techniques or other additional training processes.  
Post-processing methods, including GHOST, can all use better features, but as \citet{vaze2022openset} pointed out, better closed-set classifiers improve performance more and are continuing to evolve rapidly, so our focus is on post-processing algorithms.   

Post-hoc approaches are well-explored in out-of-distribution (OOD) detection. 
Moreover, they are used in various practical settings requiring large pre-trained networks. 
The first attempt to adapt pre-trained DNNs for OSR using statistical inference on representations extracted from a pre-trained backbone was made by \citet{bendale2016openmax}.
They sought to replace the popular SoftMax layer, which is problematic for OSR, with OpenMax.
OpenMax computes the centroid for each known class from training data and uses Extreme Value Theory to fit Weibull distributions over the distance from the centroid to the training samples.
During inference, the probabilities that a sample belongs to a known class are converted to probabilities of unknown, which are summed and effectively form an additional category representing the probability of unknown.
The Extreme Value Machine (EVM) proposed by \citet{rudd2017evm} is another OSR system based on statistical inference using distance between samples. 
It finds a set of extreme vectors in each training-set class and fits a Weibull distribution on the distance between them and the closest samples of other ``negative" classes in high-dimensional feature space.
Both systems compute distances in high-dimensional space, so a practitioner must select a distance metric that applies to their DNN backbone. 
This process often requires a search over possible metrics and other algorithm-related hyperparameters.
We might consider these methods to be direct competitors as they employ straightforward statistical measures to recognize known samples, but large scale evaluation shows they are not as effective as some simple baselines \cite{bisgin2024large}. 

Using network outputs to reject unknowns is widely used, and \citet{hendrycks17baseline,hendrycks2022scaling} showed that thresholding on Maximum  Softmax Probability (MSP) or Maximum Logits (MaxLogit) from a closed-set DNN provides good baselines for OSR. 
In addition, \citet{vaze2022openset} went so far as to argue that good closed-set classifiers with logit-based thresholding are sufficient for OSR. 
We also consider the popular energy-based OOD detection technique \cite{liu2020energy}, which computes energy based on the logit vector of a DNN (this method's performance is subpar, and so it is relegated to the supplemental). 
A recent collection of OOD methods, OpenOOD \cite{yang2022openood,zhang2023openood}, has compared many of these post-hoc methods using recent, large-scale datasets.
Herein, we consider only the best performing: Nearest Neighbor Guidance (NNGuide) \cite{park2023nearest} for OOD detection (others in the supplemental).
This method scales the confidence output from a DNN's classification layer by the sample's cosine similarity to a subset of training data, and is currently leading in the OpenOOD ImageNet-1K leaderboard\footnote{\url{http://zjysteven.github.io/OpenOOD}} and so we use it as a primary comparison.
We show that GHOST normalization, which does not need a reference set, improves performance overall, setting a new standard for large-scale OSR and OOD. 



