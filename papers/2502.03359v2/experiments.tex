\section{Experiments}
\label{sec:experiments}
Our main evaluation relies on large-scale datasets that cover both known and unknown samples.
Specifically, we draw from recent large-scale settings \cite{vaze2022openset,hendrycks17baseline,bitterwolf2023or} that differ from other evaluations which use only small-scale data with few classes and low-resolution images.
We include additional results in the supplemental material.

In particular, we use ImageNet-1K \cite{ILSVRC15} pre-trained networks and the validation set as our test set for knowns.
For unknowns, we consider multiple datasets from the literature.
We utilize a recent purpose-built OOD dataset called No ImageNet Class Objects (NINCO) \cite{bitterwolf2023or}, which consists of images specifically excluding any semantically overlapping or background ImageNet objects.
Additionally, we use the ImageNet-21K-P Open-Set splits (\emph{Easy} \& \emph{Hard}) proposed by \citet{vaze2022openset} in their semantic shift benchmark.
We also include OpenImage-O \cite{wang2022vim}, a dataset constructed from a public image database. 
Further details on each dataset and comparisons on additional datasets such as Places \cite{zhou2017places}, SUN \cite{xiao2010sun}, and Textures \cite{Cimpoi_2014_CVPR} are provided in the supplemental.


\subsection{Experimental Setup}
\paragraph{Methods.} We compare GHOST with Maximum Logit (MaxLogit) \cite{hendrycks2022scaling,vaze2022openset}, which is currently the state-of-the-art in large-scale OSR according to \citet{vaze2022openset}, and Maximum Softmax Probability (MSP) \cite{hendrycks17baseline,vaze2022openset}.
For completeness, we also compare with the current state-of-the-art in large-scale OOD according to the OpenOOD \cite{yang2022openood,zhang2023openood} leaderboard,\footnote{\url{https://zjysteven.github.io/OpenOOD}} NNGuide \cite{park2023nearest}.
Note that NNGuide has been adapted to more recent architectures than those used in their paper, as we have observed that this adaptation significantly impacts performance.
Also, GHOST results on OpenOOD's ImageNet-1K benchmark are found in the supplemental, as well as a comparison with SCALE \cite{xu2024scaling}, REACT \cite{sun2021react} and KNN \cite{sun2022out}.

\paragraph{Architectures.} We utilize two architectures: Masked Auto Encoder-trained Vision Transformer MAE-H \cite{he2022masked} and ConvNeXtV2-H \cite{woo2023convnext}.
MAE-H is a ViT-H network trained with a masked autoencoder; it is competitive with the state-of-the-art, PeCo \cite{dong2023peco}, which does not have publicly available code or checkpoints.
ConvNeXtV2-H is a recent, high-performing convolutional neural network (CNN). 
It is included to show that GHOST performance gains are not limited to transformer-based networks.
Both networks were trained exclusively with ImageNet-1K by their respective authors.
We report results on additional networks in the supplemental, offering evidence for generalizability to other architectures.



\section{Results and Discussion}
\begin{table*}[]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{l?cccccccc}
\multicolumn{1}{c?}{\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Unknowns\\ \end{tabular}}}} & \multicolumn{8}{c}{\textbf{$\uparrow$ AUOSCR}  \textbf{$\uparrow$ AUROC} \textbf{$\downarrow$ FPR95}} \\ \cline{2-9} 
\multicolumn{1}{c?}{} & \multicolumn{4}{c?}{\textbf{MAE-H}} & \multicolumn{4}{c}{\textbf{ConvNeXtV2-H}} \\ \cline{2-9} 
\multicolumn{1}{c?}{} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{GHOST} (ours)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{MSP} \end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{MaxLogit} \end{tabular}} & \multicolumn{1}{c?}{\begin{tabular}[c]{@{}c@{}}\textbf{NNGuide} \end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{GHOST} (ours)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{MSP} \end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{MaxLogit} \end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{NNGuide} \end{tabular}} \\ \noalign{\hrule height 1.5pt}
21K-P \emph{Easy} & \multicolumn{1}{c|}{$\uparrow$ \textbf{.75} $\uparrow$ \textbf{.84} $\downarrow$ \textbf{.58}} & \multicolumn{1}{c|}{$\uparrow$ .72 $\uparrow$ .80 $\downarrow$ .65} & \multicolumn{1}{c|}{$\uparrow$ .67 $\uparrow$  .75 $\downarrow$ .63} & \multicolumn{1}{c?}{$\uparrow$ .62 $\uparrow$ .69 $\downarrow$ .80} & \multicolumn{1}{c|}{$\uparrow$ \textbf{.74} $\uparrow$ \textbf{.83} $\downarrow$ \textbf{.60}} & \multicolumn{1}{c|}{$\uparrow$ .72 $\uparrow$ .79 $\downarrow$ .65} & \multicolumn{1}{c|}{$\uparrow$ .68 $\uparrow$  .75 $\downarrow$ .64} & $\uparrow$ .70 $\uparrow$ .79 $\downarrow$ .70 \\
21K-P \emph{Hard} & \multicolumn{1}{c|}{$\uparrow$ \textbf{.73} $\uparrow$ \textbf{.81} $\downarrow$ \textbf{.62}} & \multicolumn{1}{c|}{$\uparrow$ .69 $\uparrow$ .75 $\downarrow$ .75} & \multicolumn{1}{c|}{$\uparrow$ .65 $\uparrow$  .71 $\downarrow$ .74} & \multicolumn{1}{c?}{$\uparrow$ .47 $\uparrow$ .52 $\downarrow$ .89} & \multicolumn{1}{c|}{$\uparrow$ \textbf{.72} $\uparrow$ \textbf{.80} $\downarrow$ \textbf{.65}} & \multicolumn{1}{c|}{$\uparrow$ .68 $\uparrow$ .74 $\downarrow$ .76} & \multicolumn{1}{c|}{$\uparrow$ .65 $\uparrow$  .72 $\downarrow$ .74} & $\uparrow$ .60 $\uparrow$ .67 $\downarrow$ .83 \\
NINCO & \multicolumn{1}{c|}{$\uparrow$ \textbf{.81} $\uparrow$ \textbf{.91} $\downarrow$ \textbf{.47}} & \multicolumn{1}{c|}{$\uparrow$ .78 $\uparrow$ .83 $\downarrow$ .65} & \multicolumn{1}{c|}{$\uparrow$ .73 $\uparrow$  .79 $\downarrow$ .62} & \multicolumn{1}{c?}{$\uparrow$ .49 $\uparrow$ .55 $\downarrow$ .88} &             \multicolumn{1}{c|}{$\uparrow$ \textbf{.79} $\uparrow$ \textbf{.89} $\downarrow$ \textbf{.50}} & \multicolumn{1}{c|}{$\uparrow$ .75 $\uparrow$ .83 $\downarrow$ .64} & \multicolumn{1}{c|}{$\uparrow$ .73 $\uparrow$  .82 $\downarrow$ .60} & $\uparrow$ .74 $\uparrow$ .74 $\downarrow$ .78 \\
OpenImage-O & \multicolumn{1}{c|}{$\uparrow$ \textbf{.84} $\uparrow$ \textbf{.95} $\downarrow$ \textbf{.26}} & \multicolumn{1}{c|}{$\uparrow$ .76 $\uparrow$ .87 $\downarrow$ .52} & \multicolumn{1}{c|}{$\uparrow$ .71 $\uparrow$  .82 $\downarrow$ .49} & \multicolumn{1}{c?}{$\uparrow$ .68 $\uparrow$ .77 $\downarrow$ .64} &       \multicolumn{1}{c|}{$\uparrow$ \textbf{.83} $\uparrow$ \textbf{.94} $\downarrow$ \textbf{.32}} & \multicolumn{1}{c|}{$\uparrow$ .79 $\uparrow$ .88 $\downarrow$ .49} & \multicolumn{1}{c|}{$\uparrow$ .77 $\uparrow$  .87 $\downarrow$ .44} & $\uparrow$ .66 $\uparrow$ .83 $\downarrow$ .64
\end{tabular}}
\Caption[tab:overall_metrics]{Overall Quantitative Results}{On two state-of-the-art pre-trained architectures, GHOST achieves new state-of-the-art performance across all three metrics. In the supplemental, we demonstrate that the improvements provided by GHOST are statistically significant and consistent across additional unknowns and architectures. Methods such as Energy, SCALE, and others that are less effective are found in the supplemental.}
\end{table*}

\begin{table}
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l?c|c|c|c|c@{}}
\textbf{Unknowns} & \textbf{GHOST} & \textbf{MSP} & \textbf{MaxLogit} & \textbf{NNGuide} & \textbf{Energy} \\ \noalign{\hrule height 1.5pt}
21K-P \emph{Easy}   & \textbf{0.48}  & 0.53         & 0.54              & 0.76             & 0.65            \\
21K-P \emph{Hard}   & \textbf{0.53}  & 0.64         & 0.64              & 0.86             & 0.74            \\
NINCO        & \textbf{0.35}  & 0.51         & 0.51              & 0.86             & 0.62            \\
OpenImage-O & \textbf{0.17}  & 0.39         & 0.39              & 0.60             & 0.50           
\end{tabular}}
\Caption[tab:mae-fatc]{F@C95}{The corresponding minimum FPR at 95\% of closed set accuracy ($\downarrow$). Each method's results are computed on a pre-trained MAE-H. Energy is shown here as space permitted, but additional comparisons with Energy are in the supplemental.}
\end{table}


\begin{figure}[tb]
  \centerline{\includegraphics[width=0.95\columnwidth]{figures/mae_H_OpenImage_O_Coefficient_of_Variation_OSCR.pdf}}
  \Caption[fig:unfair]{Unfairness (Coefficient of Variation)}{This figure shows the unfairness of OSR algorithms across False Positive Rates using MAE-H network with OpenImages as unknowns. All algorithms include the inherent unfairness from the base classifier on the far right, but GHOST maintains its level much better as FPR rates are decreased to the left.
    }
\end{figure}

\paragraph{Global Performance.}
We present some of our quantitative results in \tab{overall_metrics}, while more datasets are found in the supplemental material.
On open-set AUOSCR, \tab{overall_metrics} shows that GHOST outperforms other methods on all datasets with an absolute gain of at least 2\%.
While OOD is not the primary focus, GHOST also outperforms other methods on the AUROC measure (\tab{overall_metrics}) with a lead of 4\%. 
It is important to note that on the NINCO \cite{bitterwolf2023or} dataset, which was specifically designed to avoid overlap with ImageNet-1K, GHOST shows clear and convincing performance gains in terms of both AUROC and AUOSCR, and some of the reduced performance for others may be a sign of overlap. 
We present results on our proposed F@C95 in \tab{mae-fatc}, where each method reports the minimum effective FPR it achieves while maintaining 95\% of the closed-set accuracy. 
On each dataset, GHOST achieves far lower F@C95 rates than other methods.

Naturally, statistical testing should be used to validate the hypothesis of superior performance.  
To this end, we present statistical analysis in the supplemental material and summarize it here.
For AUROC, GHOST very significantly outperforms all methods on all datasets with $p<10^{-6}$.
For AUOSCR, GHOST is significantly better overall and on most datasets with $p<10^{-3}$. 



\begin{table}
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l?c|c|c|c|c@{}}
\textbf{Unknowns} & \textbf{GHOST} & \textbf{MSP} & \textbf{MaxLogit} & \textbf{NNGuide} & \textbf{Energy} \\ \noalign{\hrule height 1.5pt}
21K-P \emph{Easy} & \textbf{0.32}  & 0.55         & 0.68              & 1.35             & 0.83            \\
21K-P \emph{Hard} & \textbf{0.36}  & 0.60         & 0.61              & 2.28             & 0.69          \\
NINCO & \textbf{0.21}  & 0.45         & 0.50              & 2.18             & 0.68            \\
OpenImage-O & \textbf{0.17}  & 0.38         & 0.52              & 1.16             & 0.82    
\end{tabular}}
\Caption[tab:mae-cov]{Coefficient of Variance}{The unfairness measure ${\cal V}_{\mathrm{CCR}}$ coefficients ($\downarrow$) of all methods. Each is computed on a pre-trained MAE-H at 10\% FPR and evaluated on various unknown datasets. Energy is shown here as space permitted, but additional comparisons with Energy are in the supplement.}
\end{table}


\begin{figure}[t] 
\centerline{\includegraphics[width=0.95\columnwidth]
{figures/mae_H_OpenImage-O_all-percentile_OSCR_log.pdf}}
  \Caption[fig:log]{OSCR in Logscale}{In applications with the high cost of false-positives or those with many potential unknowns, it is more important to focus on low FPR performance, in which case log FPR as shown here are more useful. The global performance is presented as a solid line, while top-10\,\% is dashed, and bottom-10\,\% is dotted.  In all cases, GHOST is significantly better at low FPR levels, and below FPR of 0.1 GHOST's bottom-10\,\% performance is better than most algorithms' top-10\,\%.}
  %\vspace{-.5ex}
\end{figure}


\paragraph{Fairness and Class-Based Evaluation.}
Fairness in OSR has two components -- the inherent differential ability of the base network and the ability of the OSR to maintain the accuracy of classes in a fair/balanced manner.  
Previous work has ignored how individual classes are impacted by OSR thresholding.  
In \fig{unfair}, we use the ${\cal V}_{\mathrm{CCR}}$ coefficient of \eqref{eq:variation}.
As this is a common measure of unfairness, lower values are more equitable.  
At the right-hand side for FPR=1, we see the network's baseline unfairness, and while GHOST maintains that for the majority of lower FPR values, other algorithms quickly degrade.  
Notably, the traditional ``recognition'' algorithms MSP/MaxLogit maintain their fairness better than the OOD-oriented algorithms, though none are even close to GHOST at lower FPRs.   
Given that we cannot compute the area under this curve as many of the curves are not bounded, we show quantitative values at 10\% FPR in \tab{mae-cov}.

To provide a more detailed analysis on the class differentials, we plot the OSCR for different classes.
For each algorithm, we select the top-10\,\% of best-performing and bottom-10\,\% of worst-performing classes based on the closed-set class-based accuracy, which is identical to CCR$_k(\theta_1)$ as defined in \eqref{eq:per-class}.
We combine these classes and plot OSCR curves.
In \fig{teaser} and \fig{log}, these best- and worst-performing classes are shown together with the global OSCR curve that includes all classes.
While \fig{teaser} presents a linear FPR-axis, \fig{log} shows a logarithmic FPR axis that allows investigation of very low FPR ranges.
Especially in \fig{teaser} it is obvious that with decreasing FPR, GHOST provides the same drop of CCR for all three lines, whereas other algorithms have different behavior.
Especially MSP has superior CCR for well-classified classes (top-10\,\%), while dropping much quicker for difficult bottom-10\,\% classes.
Furthermore, \fig{log} shows that this behavior of GHOST extends to very low FPRs, levels that are not even reached by other methods.


