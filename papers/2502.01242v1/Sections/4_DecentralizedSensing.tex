\section{Decentralized Sensing} %



\begin{figure*}[th]
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[height=6cm]{Figures/estimates_and_ncaL128.png}
\captionsetup{width=0.9\textwidth}
\caption{Geometric center of object detected by computer vision system (blue) and calibrated NCA estimate, projected to object top surface (green). Visualization of neighborhood (red) of agents (gray).}
\label{fig:estimate_projections}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[height=6cm, trim={5 5 5 5},clip]{Figures/calibrated_NCA_L128.png}
\captionsetup{width=0.9\textwidth}
\caption{Estimates for each of the NCA agents (orange), their mean (green), and object center detected by computer vision system (blue)\\
}
\label{fig:NCA_estimates}
\end{subfigure}

\caption{Estimation of Geometric Center for object in contact with sensing surface }
\label{fig:twin_image}
\end{figure*}

In this work, we estimate the global properties of objects in contact with the tactile sensing layer, specifically the \ac{GC} of the surface of an object in contact with the sensor. For objects of uniform density, this corresponds to the 2D projection of their \ac{CoM}.


\subsection{Data collection}

To train the \ac{NCA} model, a dataset was created containing two key components: readings from the sensors when objects were in contact with the sensing surface, serving as input, and the ground truth geometric center of each object, serving as the target output. The ground truth was determined using a computer vision system developed for this purpose using the OpenCV library\cite{bradski_opencv_2000}.


The dataset consisted of distinct geometric objects with uniform mass distribution but varying shape and mass, as detailed in \cref{fig:shapes}. During data collection, a predetermined face of each object was placed in direct contact with the sensor surface. Sensors readings were sampled at 20~Hz for 2.5 seconds to produce 50 samples per object per position. Between 50-150 positions were recorded for each object, depending on object size relative to sensor area,  across the entire sensor surface.

To ensure reliable detection by the  computer vision system, all objects were 3D printed from bright, mono-colored PLA. Edges of the objects not in contact with the sensor or relevant for detection were masked with black tape. Images were captured under controlled lighting conditions, and the OpenCV Python library was employed to calculate the geometric center of the face of the object in contact with the sensor. The geometric center was then mapped to the coordinate frame of the sensor board, providing ground truth data for training. 

\begin{figure*}[t]
\centerline{\includegraphics[width=0.85\linewidth]{Figures/shapes_and_lengths.png}}
\caption{Contact footprint and mass of objects used to create dataset.}
\label{fig:shapes}
\end{figure*}






\subsection{Neural Network Model}
\label{sectionNNmodel}
A decentralized system was implemented, where an array of \ac{NCA} agents received inputs directly from individual sensors. The spatial distribution of the sensors, such as those within the sensor board mirrors the lattice structure of a 2D \ac{CA}, enabling decentralized and spatially distributed sensing.

Although the sensors were implemented within the same sensor board for manufacturing simplicity, each \ac{NCA} agent could only access the local information of its corresponding sensor and its neighbors. This collectively forms a distributed network where the agents rely solely on local information; a fully decentralized computational paradigm. 

In the \acp{NCA} framework, each agent maintains a state $S$ that evolves iteratively through an asynchronous update process governed by a \ac{NN}-based update function. The state $S$ of a tile $i$ at time $t$ is updated according to:

\begin{equation}
    S_{i}^{t+1} = f_{\theta} ( \{ S_{j}^{t} \}_{j\in N(i) } )
\end{equation}

where $f_\theta$ is the additive update function parameterized by $\theta$, this function takes as input the states of tile $i$ and its neighborhood $N(i)$ from the previous time step $t-1$, enabling localized updates influenced by both the tile itself and its neighbors.

\subsubsection{Agents State}
The state $S$ of each \ac{NCA} agent encapsulates multiple components: the sensor value $V$, which captures tactile interactions with the environment; the global property estimation $E$, representing the agent's prediction of the global property; a set of hidden channels $H$, which serve as auxiliary memory or communication channels; and information from its neighborhood $N$, which encodes the states of the tiles within the agent's Moore's neighborhood, as illustrated in Fig. \ref{fig:twin_image}. 
The neighborhood $N$ is restricted to immediate neighbors and does not extend to the neighbors of neighbors. 

During training, the \ac{NN} modifies $E$ to minimize the prediction error, while the dynamics of $H$ are left to emerge as the network optimizes its functionality. In this framework, global consensus emerges iteratively through local exchanges, with agents requiring multiple update steps to converge. The number of iterations is randomly sampled from a uniform distribution in the arbitrary range of 15-30 time steps to ensure robustness to long-term stability issues, as described in \cite{wolfram_universality_nodate}.

In a distributed setting, a shared global clock cannot be assumed, in such scenarios, the agents update their states asynchronously. During each training step, only a randomly selected subset of agents updates its state. Once the predetermined number of iterations is reached, the estimation error is calculated as the mean Euclidean distance between the predicted center of the object $(x_{Ei}, y_{Ei})$ for each agent and the actual center $(x_C, y_C)$ determined via a computer vision model. Implementation and reproducibility kit available in the footnote \footnote{https://github.com/nhbess/NCA-REAL}. %

\subsubsection{Architecture}

The update function $f_\theta$ is implemented as a \ac{NN} with three main layers: The Perception Layer, which applies a $3 \times 3$ convolutional kernel to extract local features, and a Sobel filter to compute gradients of the states along the $x$ and $y$ axes; a Processing Layer utilizing a $1 \times 1$ kernel to reduce dimensionality and extract relevant features, with a with a Rectified Linear Unit (ReLU) to introduce linearity; finally, the Output Layer, also employing a $1 \times 1$ kernel, generates residual updates to the agent's state, modifying only the global property estimation and hidden channels while preserving other components of the state.


\subsubsection{Training Methodology}

Training the \ac{NCA} involves learning the parameters $\theta$ of the update function $f_\theta$ to ensure that the estimated global property $E$ converges to its true value. All agents in the system are identical and share the same neural network. The was randomly divided into two equally sized distinct sets: training and testing. The key variable of interest, the estimation $E$, is used to compute the loss function by comparing agent estimation to the true center of the object derived from the dataset.


To enhance stability, the training incorporates a pool-based strategy, in which poorly performing states are periodically replaced with empty states from the pool, as detailed in \cite{mordvintsev_growing_2020}. %
This approach mitigates training instability and ensures robust performance. The efficacy of this methodology has been demonstrated previously \cite{bessone_neural_2025}, validating its application in distributed sensing.


