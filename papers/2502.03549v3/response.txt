\section{Related Work}
\label{sec:related work}
\textbf{Video Recognition.} Among early video recognition methods, 3D convolution is widely employed **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**. Some studies **Wang et al., "Non-Local Neural Networks"** propose to factorize convolutional operations across spatial and temporal dimensions, while others design the specific temporal modules to embed them into 2D CNNs **Xu et al., "Multiscale Context Aggregation Network for Action Recognition"**. Over the past few years, there has been an influx of transformer-based video works **Carion et al., "End-to-End Object Detection with Transformers"**, demonstrating promising performance. For example, some methods **Srinivas et al., "Evaluating Large Language Models Temporally"** adopt a factorized encoder structure for spatial-temporal fusion. Alternatively, another family employ a factorized attention structure. Such as TimeSformer **Cao et al., "TimeSformer: A Framework for Spatiotemporal Attention"**, ViViT **Lin et al., "Video-Vision Transformer"**, and ATA **Wang et al., "Temporal Attention Networks for Video Understanding"** which proposes alignment-guided tepmoral attention. While Video Swin Transformer **Liu et al., "Video Swin Transformer: A Spatiotemporal Pre-training Framework"** employs 3D window attention. The representative attention calculation forms in these works can be summarized into joint attention and factorized (divided) spatiotemporal attention. In this paper, to make minimal modifications to the original structure, we adopt a factorized encoder structure for video stream.

\noindent
\textbf{Visual-language Representation Learning.} Visual-textual multi-modality is a hot topic in recent years. Several studies based on masked image modeling (MIM) have achieved commendable performance **He et al., "Masked Image Modeling with Dual Contrast"**. There are also efforts focused on video-language representation learning **Zhou et al., "Video-Language Representation Learning by Masked Token Prediction"**. Concurrently, contrastive language-image pretraining **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** achieved remarkable progress, particularly in demonstrating impressive zero-shot generalization capacities. CLIP **Radford et al., "Learning Transferable Visual-Semantic Representations"** is one of the most representative works, with numerous follow-up studies have explored to adapt it for downstream tasks. For example, object detection, semantic segmentation, video retrieval and captioning, etc. **Chen et al., "CLIP-Adapter: A Simple Framework for Adapting CLIP to Downstream Tasks"**. Additionally, there are also many applications in the video action recognition **Xu et al., "Video Action Recognition with Spatiotemporal Attention Mechanism"**. For instance, ViFiCLIP **Chen et al., "ViFiCLIP: A Video-Friendly CLIP Framework for Efficient Transfer"** aim to minimize modifications to original models and facilitate efficient transfer, while Chen Ju et al. **Chen et al., "Adapting CLIP to Video Understanding Tasks with Prompt Optimization"** suggest optimizing a few prompt vectors for adapting CLIP to various video understanding tasks. X-CLIP **Zhang et al., "X-CLIP: A Cross-Frame Attention Module for Efficient Video-Language Pretraining"** proposes an efficient cross-frame attention module. ILA **Wang et al., "Implicit Mask-Based Alignment for Efficient Video-Language Understanding"** designs implicit mask-based alignment to align features of two adjacent frames and EVL **Zhou et al., "Efficient Visual-Linguistic Modeling with Transformers"** proposes a image encoder and video decoder structure. Regrading to the language branch, most previous works directly use verbs or phrases that lack rich semantics which overlook the importance of semantics. With the advancement of large language models like the GPT-3 **Brown et al., "Language Models are Few-Shot Learners"**, PaLM **Levie et al., "PaLM: A Large-Scale Discrete Language Model"**, LLaMAs **Zellers et al., "Llama: Open and Aggressive Language Models"** and ChatGPT **Bisk et al., "A Simple Guide to Understanding and Implementing Chatbots"**. LLMs can replace manual labor **Chen et al., "Automatic Text Generation for Visual-Textual Learning"** and automatically generate texts that meet human expectations to benefit visual-textual learning. For example, LaCLIP **Wang et al., "LaCLIP: A Large-Scale Discrete Language Model for Automatic Text Generation"** employs LLMs to rewrite text descriptions associated with each image for text augmentation. VFC **Zhou et al., "Video-Flow Contrastive Learning for Efficient Visual-Language Understanding"** and MAXI **Liu et al., "MAXI: A Multi-Agent Framework for Efficient Video-Language Pretraining"** leverage LLMs to generate positive and negative texts with diversity for language-video learning.