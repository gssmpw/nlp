
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\definecolor{darkpink}{RGB}{255, 20, 147}
% extra package
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{url}
\usepackage{adjustbox}
\usepackage{bbding} 
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{booktabs}
\usepackage{listings}
\lstset{ basicstyle=\ttfamily\small, backgroundcolor=\color{gray!8},  numbers=left, numberstyle=\tiny\color{gray}, keywordstyle=\color{blue}, commentstyle=\color{green!60!black}, stringstyle=\color{orange}, breaklines=true, captionpos=b, frame=single, columns=fullflexible, tabsize=4, } 

\title{Kronecker Mask and Interpretive Prompts are Language-Action Video Learners}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
%\iclrfinalcopy
\author{Jingyi Yang\textsuperscript{\rm 1}$^*$, \quad Zitong Yu\textsuperscript{\rm 2\rm 3}\thanks{Equal Contribution}, \quad Xiuming Ni\textsuperscript{\rm 4}, \quad Jia He\textsuperscript{\rm 4}, \quad Hui Li\textsuperscript{\rm 1}\thanks{Corresponding Author} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
\textsuperscript{\rm 1}University of Science and Technology of China, \quad \textsuperscript{\rm 2}Great Bay University, 
\\
\textsuperscript{\rm 3}Dongguan Key Laboratory for Intelligence and Information Technology, \\
\textsuperscript{\rm 4}Anhui Tsinglink Information Technology Co.,Ltd.\\
\texttt{yangjingyi@mail.ustc.edu.cn}, \quad \texttt{yuzitong@gbu.edu.cn},\\ \texttt{\{nixm,hejia\}@tsinglink.com}, \quad \texttt{mythlee@.ustc.edu.cn}\\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
Contrastive language-image pretraining (CLIP) has significantly advanced image-based vision learning. A pressing topic subsequently arises: how can we effectively adapt CLIP to the video domain? Recent studies have focused on adjusting either the textual or visual branch of CLIP for action recognition. However, we argue that adaptations of both branches are crucial. In this paper, we propose \textbf{CLAVER}: a \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er}, designed to shift CLIP's focus from the alignment of static visual objects and concrete nouns to the alignment of dynamic action behaviors and abstract verbs. Specifically, we introduce a novel Kronecker mask attention for temporal modeling. Our tailored Kronecker mask offers three benefits 1) it expands the temporal receptive field for each token, 2) it serves as an effective spatiotemporal heterogeneity inductive bias, mitigating the issue of spatiotemporal homogenization, and 3) it can be seamlessly plugged into transformer-based models. Regarding the textual branch, we leverage large language models to generate diverse, sentence-level and semantically rich interpretive prompts of actions, which shift the model's focus towards the verb comprehension. Extensive experiments on various benchmarks and learning scenarios demonstrate the superiority and generality of our approach. Code is available at \href{https://github.com/yjyddq/CLAVER}{\textcolor{darkpink}{https://github.com/yjyddq/CLAVER}}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Video action recognition has long been a representative topic in video understanding. Over the past decade, there has been a continuous pursuit of learning spatiotemporal representations, giving rise to diverse architectures, such as traditional two-stream networks \cite{simonyan2014two,wang2016temporal,zhou2018temporal,karpathy2014large,xie2024fusionmamba}, 3D convolutional neural networks \cite{carreira2017quo,feichtenhofer2020x3d,feichtenhofer2019slowfast,hara2017learning,qiu2017learning,tran2015learning,tran2018closer,wang2018non,xie2018rethinking}, and Video Vision Transformers \cite{arnab2021vivit,bertasius2021space,fan2021multiscale,liu2022video,patrick2021keeping,zhao2022alignment,li2022uniformer,yan2022multiview}. Recently, there has been increasing interest in leveraging visual-language models (VLMs) like CLIP \cite{radford2021learning}, Florence \cite{yuan2021florence}, and ALIGN \cite{jia2021scaling} for various video tasks, owing to the superior generalization abilities of these models. Several studies \cite{wang2021actionclip,lin2022frozen,ni2022expanding,ju2022prompting,rasheed2023fine,tu2023implicit,chen2023video} have devoted to adapt the CLIP for video action recognition, but they often focus on adjusting a single branch. According to predecessor studies, transferring CLIP from the image domain to the video domain involves two key considerations: 1) how to perform effective temporal modeling. 2) how to design suitable text descriptions for verb understanding that align with rich text semantics in the VLM's pre-training dataset. We argue that addressing both issues simultaneously is crucial.

In addressing the issue 1), several studies \cite{wang2021actionclip,ju2022prompting,chen2023video,rasheed2023fine} implement straightforward and simple strategies such as mean pooling or 1D-temporal convolution across the temporal dimension, or employing temporal attention among class tokens. X-CLIP \cite{ni2022expanding}  and CLIP-ViP \cite{xue2022clip} introduce extra tokens for cross-frame communication. Alternatively, some studies \cite{lin2022frozen,tu2023implicit} engineer tailored modules. In our work, we aim to elucidate the distinctions and intrinsic correlations between space and time, as well as design more general spatiotemporal modeling approaches. 

% between aligning image-nouns and video-verbs is former typically requires aligning noun concepts with visual objects in images, where latter requires aligning verb concepts with action behaviors in videos, where   

Regarding the issue 2), some studies \cite{hendricks2021probing,thrush2022winoground} indicate that VLMs tend to focus on the correspondence between visual objects and nouns rather than action behaviors and verbs. We consider the essential gap is that visual objects are static and presented in lower dimensions, and nouns are concrete and easily understandable. However, action behaviors are dynamic that presented in higher dimensions, and verbs are intricate and abstract. Several existing methods \cite{ni2022expanding,lin2022frozen,rasheed2023fine,tu2023implicit} use verbs or phrases as direct text descriptions. ActionCLIP \cite{wang2021actionclip} integrates prompt templates to expand verbs or phrases into sentences. Ju et al. \cite{ju2022prompting} propose trainable continuous prompts to construct virtual prompt templates. However, these methods do not address aforementioned issues in essence. Alternatively VFC \cite{momeni2023verbs} and MAXI \cite{lin2023match} consider leveraging large language models (LLMs) to provide positive and negative text samples for contrastive learning or multiple instance learning, while ASU \cite{chen2023video} presents the concept of semantic units to supplement the semantic information of action labels.

To address these issues, we propose a \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er} (\textbf{CLAVER}, Fig. \ref{fig: Framework}) to efficiently adapt the CLIP for video action recognition. Specifically, for the issue 1), we first obtain the frame-level visual representation from the image encoder, then apply tailored Kronecker mask for temporal modeling with a wider temporal receptive field to establish long-range and wide-range dependencies among frames, while mitigating spatiotemporal homogenization. Additionally, we reveal the intrinsic correlations between space and time from the perspective of Kronecker mask attention. Regarding the issue 2), we leverage LLMs to effectively generate diverse, sentence-level, and semantically rich interpretations of actions, augmenting text descriptions during training and testing. This approach allows the text descriptions to be presented in a more flexible, sentence-level form during inference. In summary, our main contributions are four-fold:
\begin{itemize}
\item We propose the \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er} (\textbf{CLAVER}) to adapt both the visual and textual branches, efficiently shifting the alignment in CLIP from visual objects and nouns to action behaviors and verbs.
\item We propose the Kronecker mask temporal attention and Kronecker mask causal temporal attention for temporal modeling, aiming to capture the long-range and wide-range dependencies among frames with spatiotemporal heterogeneity. 
\item We introduce interpretive prompts of actions to facilitate the alignment of action behaviors and verbs, thereby improving zero-shot and few-shot generalization capabilities.
\item Extensive qualitative and quantitative experiments demonstrate the effectiveness of CLAVER. Our method achieves superior or competitive performance on Kinetics-400 and Kinetics-600 under fully-supervised scenario, and on HMDB-51 and UCF-101 under zero-shot, few-shot scenarios.
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth,height=0.28\textwidth]{Framework.pdf}
    \caption{An overview of \textbf{CLAVER}. (\textbf{Right}) Image encoder and KMT transformer are assembled as a video encoder. (\textbf{Left}) How to get the interpretive prompts for actions.}
    \label{fig: Framework}
\vspace{-0.5cm}
\end{figure*}


\section{Related Work}
\label{sec:related work}
\textbf{Video Recognition.} Among early video recognition methods, 3D convolution is widely employed \cite{qiu2017learning,tran2015learning,tran2018closer,xie2018rethinking,feichtenhofer2019slowfast,feichtenhofer2020x3d,yu2021searching}. Some studies \cite{qiu2017learning,tran2018closer,xie2018rethinking} propose to factorize convolutional operations across spatial and temporal dimensions, while others design the specific temporal modules to embed them into 2D CNNs \cite{li2020tea,lin2019tsm,liu2021tam}. Over the past few years, there has been an influx of transformer-based video works \cite{arnab2021vivit,neimark2021video,bertasius2021space,fan2021multiscale,liu2022video,yan2022multiview,li2022uniformer}, demonstrating promising performance. For example, some methods \cite{arnab2021vivit,neimark2021video,girdhar2021anticipative} adopt a factorized encoder structure for spatial-temporal fusion. Alternatively, another family employ a factorized attention structure. Such as TimeSformer \cite{bertasius2021space}, ViViT \cite{arnab2021vivit}, and ATA \cite{zhao2022alignment} which proposes alignment-guided tepmoral attention. While Video Swin Transformer \cite{liu2022video} employs 3D window attention. The representative attention calculation forms in these works can be summarized into joint attention and factorized (divided) spatiotemporal attention. In this paper, to make minimal modifications to the original structure, we adopt a factorized encoder structure for video stream. 

\noindent
\textbf{Visual-language Representation Learning.} Visual-textual multi-modality is a hot topic in recent years. Several studies based on masked image modeling (MIM) have achieved commendable performance \cite{li2020unicoder,lu2019vilbert,su2019vl,tan2019lxmert}. There are also efforts focused on video-language representation learning \cite{miech2019howto100m,sun2019learning,sun2019videobert,zhu2020actbert,xu2021videoclip}. Concurrently, contrastive language-image pretraining \cite{radford2021learning,jia2021scaling,yuan2021florence} achieved remarkable progress, particularly in demonstrating impressive zero-shot generalization capacities. CLIP \cite{radford2021learning} is one of the most representative works, with numerous follow-up studies have explored to adapt it for downstream tasks. For example, object detection, semantic segmentation, video retrieval and captioning, etc.\cite{gu2021open,vinker2022clipasso,li2022grounded,luo2022clip4clip,xu2022groupvit}. Additionally, there are also many applications in the video action recognition \cite{wang2021actionclip,ni2022expanding,lin2022frozen,ju2022prompting,pan2022st,rasheed2023fine,chen2023video,tu2023implicit,lin2023match,momeni2023verbs,yang2023aim}. For instance, ViFiCLIP \cite{rasheed2023fine} aim to minimize modifications to original models and facilitate efficient transfer, while Chen Ju et al. \cite{ju2022prompting} suggest optimizing a few prompt vectors for adapting CLIP to various video understanding tasks. X-CLIP \cite{ni2022expanding} proposes an efficient cross-frame attention module. ILA \cite{tu2023implicit} designs implicit mask-based alignment to align features of two adjacent frames and EVL \cite{lin2022frozen} proposes a image encoder and video decoder structure. Regrading to the language branch, most previous works directly use verbs or phrases that lack rich semantics which overlook the importance of semantics. With the advancement of large language models like the GPT-3 \cite{brown2020language}, PaLM \cite{chowdhery2023palm}, LLaMAs \cite{touvron2023llama,touvron2023llama2,dubey2024llama3herdmodels} and ChatGPT \cite{openai2024gpt4technicalreport}. LLMs can replace manual labor \cite{chen2021elaborative,qian2022rethinking} and automatically generate texts that meet human expectations to benefit visual-textual learning. For example, LaCLIP \cite{fan2024improving} employs LLMs to rewrite text descriptions associated with each image for text augmentation. VFC \cite{momeni2023verbs} and MAXI \cite{lin2023match} leverage LLMs to generate positive and negative texts with diversity for language-video learning. 

% In line with this, we exploit LLMs to effectively generate sentence-level and semantically rich verb interpretations with diversity for prompting the understanding of actions.

\section{Methodology}
\label{sec:methodology}
In Sec. \ref{sec:overview}, we overview our proposed contrastive language-action video learner architecture. Then, we elaborate on the detail of the Kronecker mask attention in Sec. \ref{sec:kronecker mask attention}. Finally, we present the technique details of action interpretive prompt in Sec. \ref{sec: interpretive prompt}.

\subsection{Overview}
\label{sec:overview}
Our contrastive language-action video learner architecture is illustrated in Fig. \ref{fig: Framework}. We utilize a video encoder to obtain video representations, comprising two transformers-based components: an image encoder (ViT) from CLIP, a Kronecker mask temporal transformer. The text encoder aims to align text representations with the video representations. Concretely, given a video clip \(\textit{V}=[v_0, \cdots v_t, \cdots v_{T-1}] \in \mathbb{R}^{T \times H \times W \times 3}, v_t \in \mathbb{R}^{H \times W \times 3}\) and corresponding text descriptions \(C=[c_0, \cdots c_m, \cdots c_{M-1}] \in \mathbb{R}^{M \times N}, c_m \in \mathbb{R}^{N}\), where \(T,H,W\) are the number of frames, height, width, respectively, \(M\) is the number of diverse text descriptions (share the same central concept) for an action category, \(N\) is the max sequence length. We feed texts \(C\) into the text encoder \(f_{\theta_C}(\cdot)\) to obtain text representations \(\textbf{C} = [\textbf{c}_0, \cdots \textbf{c}_{M-1}]\). For the video stream, firstly, we input the video clip \(V\) to the image encoder \(f_{\theta_I}(\cdot)\) to obtain frame-level representations \(\textbf{I}_t\).
\begin{equation}
\label{eqn: image encoder}
\textbf{I}_t = f_{\theta_I}(\mathrm{PE}(v_t)+\textbf{e}^{pos}), \quad \textbf{C} = f_{\theta_C}(C),
\end{equation}
where \(\mathrm{PE}(\cdot)\) is the patch embedding. Each frame is split into \(L=\frac{H}{P} \times \frac{W}{P}\) patches, \(L\) is the number of patches, \(P\) is the patch size, \(\textbf{e}^{pos}\) is the absolute positional embedding, \(\mathrm{PE}(v_t)+\textbf{e}^{pos} = [v_{t,0}+\textbf{e}^{pos}_0, \mathrm{PE}(v_{t,1})+\textbf{e}^{pos}_1, \cdots, \mathrm{PE}(v_{t,l})+\textbf{e}^{pos}_l, \cdots, \mathrm{PE}(v_{t,L})+\textbf{e}^{pos}_L] = [\textbf{z}_{t,0}, \cdots, \textbf{z}_{t,l}, \cdots, \textbf{z}_{t,L}]\),  \(v_{t,0}\) is the class token. \(\textbf{I}_t = f_{\theta_I}([\textbf{z}_{t,l}]_{t \in T, l \in L+1})= [\textbf{I}_{t,0}, \cdots, \textbf{I}_{t,l}, \cdots, \textbf{I}_{t,L}]\).

Then, we add absolute temporal embedding \(\textbf{e}^{tem}\) to the \(\textbf{I}_t,t \in T\) and feed them into the Kronecker mask temporal transformer \(f_{\theta_{V}}(\cdot)\). Finally, by selecting the class token from each frame and averaging them, we obtain a video representation \textbf{v} with the same dimension as \(\textbf{c}_m,m \in M\).
\begin{equation}
\label{eqn: video encoder}
\textbf{V} = f_{\theta_{V}}([\textbf{I}_t]_{t \in T}+\textbf{e}^{tem}), \quad \textbf{v} = \mathrm{Avg}([\textbf{V}_{t,0}]_{t \in T}),
\end{equation}
where \(\textbf{V} = f_{\theta_{V}}([\textbf{I}_t]_{t \in T} +\textbf{e}^{tem}) = f_{\theta_{V}}([\textbf{I}_t+\textbf{e}^{tem}_t]_{t \in T}) = 
f_{\theta_{V}}([[\textbf{I}_{0,0}+\textbf{e}^{tem}_0, \cdots, \textbf{I}_{0,L}+\textbf{e}^{tem}_0], \cdots, [\textbf{I}_{t,0}+\textbf{e}^{tem}_t, \cdots, \textbf{I}_{t,L}+\textbf{e}^{tem}_t], \cdots, [\textbf{I}_{T-1,0}+\textbf{e}^{tem}_{T-1}, \cdots, \textbf{I}_{T-1,L}+\textbf{e}^{tem}_{T-1}]]) =
[[\textbf{V}_{0,0}, \cdots, \textbf{V}_{0,L}], \cdots, [\textbf{V}_{t,0}, \cdots, \textbf{V}_{t,L}], \cdots, [\textbf{V}_{T-1,0}, \cdots, \textbf{V}_{T-1,L}]]\), \(\mathrm{Avg}(\cdot)\) is the average pooling function.
Our optimization goal is to maximize the cosine similarity between video \textbf{v} and its corresponding texts \(\textbf{c}_m \in \textbf{C}\) representations:
\begin{equation}
\label{eqn: cosine similarity loss}
\mathrm{sim}(\textbf{v},\textbf{c}_m)=\frac{\langle \textbf{v},\textbf{c}_m\rangle}{\lVert\textbf{v}\rVert \cdot \lVert\textbf{c}_m\rVert}.
\end{equation}

\begin{figure*}[t]
\centering
    \includegraphics[width=0.95\textwidth,height=0.36\textwidth]{Kronecker_Mask_Attention.pdf}
\caption{(\textbf{Left}) Red indicates the currently focal patch, green patches are visible in spatial attention, orange patches are visible in temporal attention, purple patches are visible in joint attention. (\textbf{Right}) Kronecker mask attention: Several attentions can be seen as employing tailored Kronecker masks for joint attention.}
\label{fig: Kronecker Mask Attention}
\vspace{-0.5cm}
\end{figure*}

\subsection{Kronecker Mask Attention}
\label{sec:kronecker mask attention}
For an image \(\in \mathbb{R}^{H \times W \times 3}\), it is first split into patches and then flattened into a token sequence after patch embedding. The resulting feature shape is \((L, D)\), \(L=\frac{H}{P} \times \frac{W}{P}\), where \(D\) is the hidden dimension. This process is a standard operation in ViT \cite{dosovitskiy2020image}, denoted as Spatial Attention (SA):
\begin{equation}
\label{eqn: spatial attention}
\textbf{Z}_{(L , D)} = \mathrm{Softmax}(\textbf{Q}_{(L , D)}\textbf{K}^\textbf{T}_{(L , D)}/\sqrt{D})\textbf{V}_{(L , D)},
\end{equation}
where \(\textbf{Q}\), \(\textbf{K}\), \(\textbf{V}\) represent the query, key, value matrices, respectively.

For a video \(\in \mathbb{R}^{T \times H \times W \times 3}\), \(T\) is the number of frames, previous studies \cite{arnab2021vivit,bertasius2021space,neimark2021video,guo2021ssan,tong2022videomae,feichtenhofer2022masked} typically employ either joint attention \cite{bertasius2021space,feichtenhofer2022masked} or factorized (divided) attention \cite{bertasius2021space,arnab2021vivit}. Joint attention flattens a video into a longer token sequence, resulting in a feature shape of \((T \times L, D)\). This token interaction mode is illustrated in Fig. \ref{fig: Kronecker Mask Attention} \textbf{Left} (a). Joint attention encounters spatiotemporal homogenization issues, where random shuffling of tokens not affecting (or slightly) the final pooling result. 

In contrast, factorized attention factorize joint attention into spatial attention and temporal attention to avoid spatiotemporal homogenization. They utilize the same spatial attention as Eqn. \ref{eqn: spatial attention}, while the temporal attention varies. Two common temporal attentions are pipeline temporal attention \cite{arnab2021vivit,bertasius2021space} and class-token-only temporal attention \cite{arnab2021vivit,ni2022expanding,wang2021actionclip}. Their feature shapes are \((L, T, D)\) and \((T, D)\), respectively. Pipeline temporal attention has a limited temporal receptive field, which is limited to a fixed time pipeline (tube), hence the term "pipeline temporal attention", as shown in Fig. \ref{fig: Kronecker Mask Attention} \textbf{Left} (c). It has limited scope of capture dynamic information since objects of interest do not always appear in the same 2D location across frames. Although ATA \cite{zhao2022alignment} utilizes alignment techniques to bend the time pipeline to capture dynamic objects, it still has a limited temporal receptive field, in Fig. \ref{fig: Kronecker Mask Attention} \textbf{Left} (d). Similarly, class-token-only temporal attention retains only the class token time pipeline and discard others, which both face limited receptive field and may discard lots of potentially valuable information. It is worth noting that in pipeline temporal attention, mean pooling is necessary across all tokens, otherwise, it is equivalent to class-token-only temporal attention.

To address aforementioned drawbacks, we propose Kronecker Mask Temporal Attention (KMTA). Specifically, we allow each patch (token) at timestamp \(t\) can interact with all other patches (tokens), excluding those sharing the same timestamp \(t\), as illustrated in Fig. \ref{fig: Kronecker Mask Attention}  \textbf{Left} (b). Compared to pipeline temporal attention, KMTA expands the temporal receptive field width of each token. KMTA can be achieved through joint attention incorporated a Kronecker mask, as shown in Fig. \ref{fig: Kronecker Mask Attention}  \textbf{Right} (left down). Additionally, KMTA alleviates the impact of spatiotemporal homogenization due to the presence of the Kronecker mask. The trick for obtaining the Kronecker mask is Kronecker product \(\otimes\):
\begin{equation}
\label{eqn: Kronecker product}
\textbf{A}_{m \times n} \otimes \textbf{B}_{p \times q} =
\begin{bmatrix}
a_{11}\textbf{B} & a_{12}\textbf{B} & \cdots & a_{1n}\textbf{B} \\
a_{21}\textbf{B} & a_{22}\textbf{B} & \cdots & a_{2n}\textbf{B} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1}\textbf{B} & a_{m2}\textbf{B} & \cdots & a_{mn}\textbf{B}
\end{bmatrix}_{mp \times nq}
\end{equation}
Eqn. \ref{eqn: Kronecker product} is the definition of \(\otimes\), where \(\textbf{A} \in \mathbb{R}^{m \times n}\), \(\textbf{B} \in \mathbb{R}^{p \times q}\). Thus, it is referred to as the Kronecker mask. Kronecker Mask Temporal Attention (KMTA) can be formulated as:
\begin{equation}
\label{eqn: Kronecker mask}
\textbf{M}_{(T \times L,T \times L)} = [\textbf{I}_{(T,T)} \otimes (\textbf{J}_{(L,L)}-\textbf{I}_{(L,L)})]_{1 == -\mathrm{inf}},
\end{equation}
\begin{equation}
\label{eqn: temporal attention}
\textbf{Z}_{(T \times L, D)} = \mathrm{Softmax}(\textbf{Q}_{(T \times L, D)}\textbf{K}^\textbf{T}_{(T \times L, D)}/\sqrt{D}+\textbf{M}_{(T \times L,T \times L)})\textbf{V}_{(T \times L, D)},
\end{equation}
where \(\textbf{I}_{(-,-)}\) is an identity matrix, \(\textbf{J}_{(-,-)}\) is an all-ones matrix, \([\quad]_{1==-\mathrm{inf}}\) means replacing 1 in the matrix with negative infinity (\(-\mathrm{inf}\)), and \(\textbf{M}_{(T \times L,T \times L)}\) is the Kronecker mask. A transformer equipped with KMTA is referred to as a Kronecker Mask Temporal (KMT) transformer.

In fact that both the spatial and temporal attention we mentioned above can be derived by combining a tailored Kronecker mask with joint attention. Therefore, we collectively refer to them as Kronecker Mask Attention, as depicted in Fig. \ref{fig: Kronecker Mask Attention} (\textbf{Right}). The Kronecker mask serves as a prior spatiotemporal heterogeneity inductive bias. Spatial attention allows intra-frame interactions but blocks inter-frame interactions, while Kronecker mask temporal attention allows inter-frame interactions but blocks intra-frame interactions, exhibiting a spatiotemporal structural complementarity.
\begin{wrapfigure}[8]{r}{0.57\textwidth} 
\centering
\vspace{-0.0cm}
\includegraphics[width=0.57\textwidth]{KMCT.pdf} 
\vspace{-0.5cm}
\caption{Kronecker mask causal temporal attention.} 
\label{fig: Kronecker Mask Causal Temporal Attention}
\end{wrapfigure}

Moreover, we design another type of Kronecker mask for temporal modeling, known as Kronecker mask causal temporal attention (KMCTA) that aims to alleviate the low-rank bottleneck, as shown in Fig. \ref{fig: Kronecker Mask Causal Temporal Attention}. The mask \(\tilde{\textbf{M}}_{(T \times L,T \times L)}\) of KMCTA can be formulated as:
\begin{equation}
\label{eqn: Kronecker mask causal}
\tilde{\textbf{M}}_{(T \times L,T \times L)} = [\textbf{I}_{(T,T)} \otimes (\textbf{J}_{(L,L)}-\textbf{I}_{(L,L)}) + (\textbf{U}_{(T,T)}-\textbf{I}_{(T,T)}) \otimes \textbf{J}_{(L,L)}]_{1 == -\mathrm{inf}},
\end{equation}
where \(\textbf{U}_{(-,-)}\) is an upper triangular matrix, and all elements of the upper triangle are 1. A transformer equipped with KMCTA is referred to as a KMCT transformer. KMCTA ensures the causality of time (in the time dimension, KMTA is bidirectional, while KMCTA is unidirectional). In addition, KMCTA always has a full-rank attention matrix, whereas KMTA and joint attention can not guarantee this property. The proof is detailed in \textit{Appendix} \ref{sec: Proof}. Some studies \cite{bhojanapalli2020low,han2023flatten} indicate that it is important to avoid the low-rank bottleneck to improve the representation power of transformer architecture. This problem may become more knotty when transitioning images to videos.

% The low-rank bottleneck describes a situation where, as the tokens length significantly increases (i.e. the number of tokens \(n\) far exceeds the latent dimension \(d_{head}\) in multi-head attention), the performance improvement of transformer-based structures may encounter bottlenecks. This problem may become more knotty when transitioning images to videos.


\subsection{Interpretive Prompt}
\label{sec: interpretive prompt}
\begin{wrapfigure}[9]{r}{0.48\textwidth} 
\centering
\vspace{-0.5cm}
\includegraphics[width=0.48\textwidth]{Interpretive_Prompt.pdf}
\vspace{-0.6cm}
\caption{The Interpretive Prompt scheme.} 
\label{fig: Interpretive Prompt}
\end{wrapfigure}
To address the issue 2) in Sec. \ref{sec:intro}, we prompt LLMs to generate interpretive texts that align the text semantics of video-text pairs with the rich text semantics in VLM's pre-training dataset. This design aids the model in understanding abstract verbs. This approach, termed interpretive prompt, designed from the following aspects: 1) Action decomposition: Providing detailed descriptions of actions by decomposing complex actions into simpler, more basic ones. This clarifies the include relationship between complex and basic actions, and help distinguish similar actions, as not all action concepts are of equal status. For example, the action 'playing basketball' may consist of 'running', 'jumping', and 'shooting'. 'Running', 'jumping', and 'hand movements' are considered as basic actions, while 'playing basketball' is a complex action. Similarly, 'dribbling' may also involves 'running', and 'hand movements', besides, it is also a subset of 'playing basketball'. Action decomposition enhances the separability of action concepts in semantic space, helping models understand the relationships between actions. 2) Synonym conversion: Generating synonyms for verbs and phrases that convey the same core concept but with more varied expressions. This improves zero-shot generalization as an action concept may have multiple expressions. When encountering similar action descriptions in the unseen domain, the model demonstrates stronger generalization robustness. 3) Involving body parts: Describing actions based on possible body parts involved, helping the model to localize the region where action occurs.

We leverage ChatGPT \cite{openai2024gpt4technicalreport} and LLaMA-3 \cite{dubey2024llama3herdmodels} to automatically generate action interpretations. Initially, we ask the knowledgeable ChatGPT \cite{openai2024gpt4technicalreport} to provide several examples of text descriptions that align with our expect (based on the aforementioned aspects). Subsequently, we provide the format prompt to LLaMA-3 \cite{dubey2024llama3herdmodels} for text completion, as illustrated in Fig. \ref{fig: Interpretive Prompt}. For example, "\textit{Generate a detailed text description corresponding to the video class...}" --\textit{\textbf{Command}}, accompanied by a few examples like, "\textit{abseiling $\rightarrow$ Abseiling combines several actions...}" --\textit{\textbf{Examples}}, and the action concept that LLM needs to interpret --\textit{\textbf{Action Concept}}. We feed the format prompt to LLaMA-3 multiple times to obtain diverse interpretive prompts of actions. For a given action category, the texts used during training and inference include the original verbs or phrases, template filling prefixes or suffixes, and interpretive prompts. Our interpretive prompts provide texts at the sentence, phrase, and word-levels, enhancing the flexibility of text usage during the inference. More technique details refer to \textit{Appendix} \ref{sec: Interpretive Prompt Details}. 

\subsection{Training and Inference}
\label{sec: Training and Inference}
In each training step, a batch of \(B\) videos is sampled, and \(M\) represents the total number of text descriptions of each verb or phrase. There are a total of \(K\) action categories. The training loss is formulated as follows:
\begin{equation}
\label{eqn: loss}
L=-\frac{1}{N} \sum_m^{\mathrm{sub}\{M\}} \sum_i^{B}   \mathrm{log} \frac{\exp(\mathrm{sim}(\textbf{v}^i,\textbf{c}_m^i)/\tau)}{\sum_k^{K} \exp(\mathrm{sim}(\textbf{v}^i,\textbf{c}_m^k)/\tau)},  
\end{equation}
where, \(\tau\) is the temperature parameter, \(\mathrm{sub}\{M\}\) indicates that we sample a subset of all text descriptions for a single training step. \(\textbf{v}^i\) represents the video belonging to the \(i\)-th category. \(\textbf{c}^i_m\) denotes the descriptions corresponding to the \(i\)-th category. During inference, we take the sum of the similarities between each \(\textbf{v}^i\) and all \(\textbf{c}^i_m\) as the final similarity score \(S\):
\begin{equation}
\label{eqn: inference}
S= \sum_m^{M}   \mathrm{log} \frac{\exp(\mathrm{sim}(\textbf{v}^i,\textbf{c}_m^i)/\tau)}{\sum_k^{K} \exp(\mathrm{sim}(\textbf{v}^i,\textbf{c}_m^k)/\tau)}.  
\end{equation}

\section{Experiments}
\label{sec:experiments}
\subsection{Implementation Details}
\label{sec:experiments setup}
\textbf{Architectures and hyperparameters.} We employ CLIP-B/32, CLIP-B/16, CLIP-L/14 as our backbones and derive corresponding variants: CLAVER-B/32, CLAVER-B/16, CLAVER-L/14, respectively. The frame length settings include 8 and 16. For all CLAVER variants, the number of layers in the KMT/KMCT transformer is equal to one-third of the number of layers in the image encoder. For example, if the image encoder has 12 or 24 layers, then the KMT/KMCT transformer employs 4 or 8 layers. The detailed hyperparameter settings are provided in \textit{Appendix} \ref{sec: Experiment Setting Details}.

\noindent
\textbf{Datasets and metrics.} We evaluate the performance of our method on four benchmarks: Kinetics-400 \cite{kay2017kinetics}, Kientics-600 \cite{carreira2018short}, UCF-101 \cite{soomro2012ucf101}, HMDB-51 \cite{kuehne2011hmdb}. We report the Top-1 and Top-5 accuracy as evaluation metrics.

\begin{table*}[t!]
\centering
\caption{Comparison with state-of-the-art on Kinetics-400. * indicates pretraining with a video-text collection. The FLOPs per view of each method is reported. \textcolor{red}{Red} represents optimal performance. \textbf{Bold} represents optimal performance among CLIP-based methods at the same model scale.}
\label{table: Kinetics-400}
\resizebox{0.92\textwidth}{!}{
\begin{tabular*}{1.52\linewidth}{p{5.3cm}>{\centering\arraybackslash}p{3cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2cm}}
\hline
Method & Pretrain & Frames & Top-1 & Top-5 & Views & FLOPs(G) \\
\hline
%\rowcolor{gray!25} 
\multicolumn{7}{c}{\textbf{Without Language}} \\
\hline
Uniformer-B \cite{li2022uniformer} & IN-1k & 32 & 83.0 & 95.4 & 4$\times$3  & 259 \\
TimeSformer-L \cite{bertasius2021space} & IN-21k & 96 & 80.7 & 94.7 & 1$\times$3  & 2380 \\
Mformer-HR \cite{patrick2021keeping} & IN-21k & 16 & 81.1 & 95.2 & 10$\times$3  & 959 \\
ATA \cite{zhao2022alignment} & IN-21k & 32 & 81.9 & 95.5 & 4$\times$3  & 793 \\
Swin-L (384 $\uparrow$) \cite{liu2022video} & IN-21k & 32 & 84.9 & 96.7 & 10$\times$5  & 2107 \\
MViTv2-L (312 $\uparrow$) \cite{li2112improved} & IN-21k & 40 & 86.1 & 97.0 & 5$\times$5  & 2828 \\
ViViT-H/14x2 \cite{arnab2021vivit} & JFT-300M & 32 & 84.9 & 95.8 & 4$\times$3  & 8316 \\
TokenLearner-L/10 \cite{ryoo2021tokenlearner} & JFT-300M & - & 85.4 & 96.3 & 4$\times$3  & 4076 \\
CoVeR \cite{zhang2021co} & JFT-3B & - & 87.2 & - & 1$\times$3  & - \\
\hline
%\rowcolor{gray!25} 
\multicolumn{7}{c}{\textbf{With Language}} \\
\hline
MTV-H \cite{yan2022multiview} & WTS* & 32 & \textcolor{red}{\textbf{89.1}} & \textcolor{red}{\textbf{98.2}} & 4$\times$3  & 3705 \\
%\hline
%\multicolumn{7}{c}{\textbf{CLIP ViT-B/32}} \\
\hline
X-CLIP-B/32 \cite{ni2022expanding} & CLIP-400M & 8 & 80.1 & 94.8 & 4$\times$3  & 26 \\
X-CLIP-B/32 \cite{ni2022expanding} & CLIP-400M & 16 & 81.0 & 95.1 & 4$\times$3  & 49 \\
ILA-B/32 \cite{tu2023implicit} & CLIP-400M & 8 & 80.6 & 94.9 & 4$\times$3  & 40 \\
ILA-B/32 \cite{tu2023implicit} & CLIP-400M & 16 & 81.8 & 95.4 & 4$\times$3  & 75 \\
\hline
\rowcolor{gray!25} 
\textbf{CLAVER-B/32 (KMT)} & CLIP-400M & 8 & \textbf{81.5} & \textbf{95.5} & 4$\times$3  & 33 \\
\rowcolor{gray!25} 
\textbf{CLAVER-B/32 (KMT)} & CLIP-400M & 16 & 82.4 & \textbf{95.9} & 4$\times$3  & 64 \\
\rowcolor{gray!25} 
\textbf{CLAVER-B/32 (KMCT)} & CLIP-400M & 8 & 81.4 & \textbf{95.5} & 4$\times$3  & 33 \\
\rowcolor{gray!25} 
\textbf{CLAVER-B/32 (KMCT)} & CLIP-400M & 16 & \textbf{82.6} & \textbf{95.9} & 4$\times$3  & 64 \\
%\hline
%\multicolumn{7}{c}{\textbf{CLIP ViT-B/16}} \\
\hline
Action-CLIP-B/16 \cite{wang2021actionclip} & CLIP-400M & 16 & 82.6 & 96.2 & 10$\times$3  & - \\
A6 \cite{ju2022prompting} & CLIP-400M & 16 & 76.9 & 93.5 & -  & - \\
X-CLIP-B/16 \cite{ni2022expanding} & CLIP-400M & 8 & 83.1 & 96.5 & 4$\times$3  & 92 \\
X-CLIP-B/16 \cite{ni2022expanding} & CLIP-400M & 16 & 84.3 & 96.8 & 4$\times$3  & 183 \\
EVL-B/16 \cite{lin2022frozen} & CLIP-400M & 8 & 82.9 & - & -  & 444 \\
EVL-B/16 \cite{lin2022frozen} & CLIP-400M & 16 & 83.6 & - & -  & 888 \\
ViFiCLIP-B/16 \cite{rasheed2023fine} & CLIP-400M & 16 & 83.9 & 96.3 & 4$\times$3  & 281 \\
ASU-B/16 \cite{chen2023video} & CLIP-400M & 8 & 84.1 & 96.3 & 4$\times$3  & 146 \\
ASU-B/16 \cite{chen2023video} & CLIP-400M & 16 & 84.8 & 96.7 & 4$\times$3  & 288 \\
ILA-B/16 \cite{tu2023implicit} & CLIP-400M & 8 & 83.4 & 96.3 & 4$\times$3  & 150 \\
ILA-B/16 \cite{tu2023implicit} & CLIP-400M & 16 & 85.0 & 97.0 & 4$\times$3  & 302 \\
ALT-B/16 \cite{chen2024align} & CLIP-400M & 16 & 85.5 & 96.7 & 3$\times$1  & 1308 \\
\hline
\rowcolor{gray!25} 
\textbf{CLAVER-B/16 (KMT)} & CLIP-400M & 8 & \textbf{84.3} & 96.3 & 4$\times$3  & 122 \\
\rowcolor{gray!25} 
\textbf{CLAVER-B/16 (KMT)} & CLIP-400M & 16 & 85.9 & \textbf{97.3} & 4$\times$3  & 241 \\
\rowcolor{gray!25} 
\textbf{CLAVER-B/16 (KMCT)} & CLIP-400M & 8 & 84.1 & 96.2 & 4$\times$3  & 122 \\
\rowcolor{gray!25} 
\textbf{CLAVER-B/16 (KMCT)} & CLIP-400M & 16 & \textbf{86.0} & 97.2 & 4$\times$3  & 241 \\
%\hline
%\multicolumn{7}{c}{\textbf{CLIP ViT-L/14}} \\
\hline
X-CLIP-L/14 \cite{ni2022expanding} & CLIP-400M & 8 & 87.0 & 97.7 & 4$\times$3  & 420 \\
X-CLIP-L/14 (336$\uparrow$) \cite{ni2022expanding} & CLIP-400M & 16 & 87.6 & 97.5 & 4$\times$3  & 1870 \\
EVL-L/14 \cite{lin2022frozen} & CLIP-400M & 8 & 86.3 & - & -  & 2022 \\
EVL-L/14 (336$\uparrow$) \cite{lin2022frozen}  & CLIP-400M & 32 & 87.7 & - & -  & 18196 \\
ASU-L/14 \cite{chen2023video} & CLIP-400M & 8 & 87.8 & 97.8 & 4$\times$3  & 660 \\
ASU-L/14 (336$\uparrow$) \cite{chen2023video} & CLIP-400M & 16 & 88.3 & 98.0 & 4$\times$3  & 3084 \\
ILA-L/14 \cite{tu2023implicit} & CLIP-400M & 8 & 87.6 & 97.8 & 4$\times$3  & 647 \\
ILA-L/14 (336$\uparrow$) \cite{tu2023implicit}  & CLIP-400M & 16 & 88.1 & 97.8 & 4$\times$3  & 3130 \\
ALT-B/14 \cite{chen2024align} & CLIP-400M & 16 & 87.8 & 97.7 & 3$\times$1  & 4947 \\
\hline
\rowcolor{gray!25} 
\textbf{CLAVER-L/14 (KMT)} & CLIP-400M & 8 & \textbf{88.1} & 97.7 & 4$\times$3  & 558 \\
\rowcolor{gray!25} 
\textbf{CLAVER-L/14(336$\uparrow$) (KMT)} & CLIP-400M & 16 & 88.8 & \textbf{98.1} & 4$\times$3  & 2488 \\
\rowcolor{gray!25} 
\textbf{CLAVER-L/14 (KMCT)} & CLIP-400M & 8 & 87.9 & 97.7 & 4$\times$3  & 558 \\
\rowcolor{gray!25} 
\textbf{CLAVER-L/14(336$\uparrow$) (KMCT)} & CLIP-400M & 16 & \textbf{88.9} & 98.0 & 4$\times$3  & 2488 \\
\hline
\end{tabular*}
}
\vspace{-0.3cm}
\end{table*}


 \subsection{Comparison Results}
\label{sec:Comparison Results}
\textbf{Fully-supervised Experiments.} We conducted fully supervised experiments on Kinetics-400 and Kinetics-600, respectively. In Tab. \ref{table: Kinetics-400}, we employ three variant models, CLAVER-B/32, CLAVER-B/16, and CLAVER-L/14, and sample 8 or 16 frames (\(8f\),\(16f\)) with a sparse sampling for each model, employ KMT/KMCT, respectively. CLAVER-B/16\(_{8f}\) (KMT/KMCT) and CLAVER-B/16\(_{16f}\) (KMT/KMCT) surpass several methods \cite{bertasius2021space,patrick2021keeping,zhao2022alignment,li2022uniformer} pretrained on ImageNet-1k/21k \cite{deng2009imagenet} with shorter frames. CLAVER-B/16\(_{16f}\) (KMT) outperforms Swin-L (384$\uparrow$) \cite{liu2022video} by 1.0\%, and is slightly lower than MViTv2-L (312$\uparrow$) \cite{li2112improved}, as lower resolution, shorter frames and fewer views. CLAVER-B/14\(_{8f}\) (KMT/KMCT) outperforms some methods \cite{arnab2021vivit,ryoo2021tokenlearner,zhang2021co} pretrained on JFT-300M/JFT-3B. CLAVER-B/14\(_{16f}\) (KMT) outperforms CoVeR \cite{zhang2021co} by 1.6\%, however, inferior to MTV-H \cite{yan2022multiview}, as it utilizes WTS*, which contains 70M video-text pairs with about 17B images, much larger than CLIP-400M. Compared to those approaches based on CLIP \cite{radford2021learning}, under configuration ViT-B/32\(_{8f}\) and ViT-B/32\(_{16f}\), CLAVER (KMT/KMCT) surpasses X-CLIP \cite{ni2022expanding} and ILA \cite{tu2023implicit}. Under configuration ViT-B/16\(_{8f,16f}\) and ViT-L/14\(_{8f,16f}\), CLAVER (KMT/KMCT) exceeds most methods under the same configuration. In Tab. \ref{table: Kinetics-600}, CLAVER-B/16\(_{8f}\) (KMT) achieves higher performance compared to ViViT-H/14x2 \cite{arnab2021vivit}, MViT-B-24 \cite{fan2021multiscale}. Our method has lower performance than these methods \cite{ryoo2021tokenlearner,zhang2021co,yan2022multiview,yuan2021florence,liu2022video}, as they use longer frames, or more data, or higher resolutions. In addition, CLAVER-B/16\(_{8f}\) (KMT/KMCT) outperforms ASU-B/16\(_{8f}\) \cite{chen2023video} and X-CLIP-B/16\(_{8f}\) \cite{ni2022expanding} at the same scale.


\begin{table}[t!]
\centering
\begin{minipage}[t]{0.48\linewidth}
\centering
\caption{Comparison with state-of-the-art on Kinetics-600.}
\vspace{-0.3cm}
\label{table: Kinetics-600}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\hline
\textbf{Method} & \textbf{Pretrain} & \textbf{Frames} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Views}\\
\hline
MViT-B-24 \cite{fan2021multiscale} & - & 32 & 83.8 & 96.3 & 5$\times$1 \\
Swin-L (384 $\uparrow$) \cite{liu2022video} & JFT-300M & 32 & 85.8 & 96.5 & 4$\times$3  \\
ViViT-H/14x2 \cite{arnab2021vivit}  & JFT-300M & 32 & 85.8 & 96.5 & 4$\times$3  \\
TokenLearner-L/10 \cite{ryoo2021tokenlearner} & JFT-300M & - & 86.3 & 97.0 & 4$\times$3 \\
CoVeR \cite{zhang2021co} & JFT-3B & 32 & 87.9 & - & 4$\times$3\\
MTV-H \cite{yan2022multiview} & WTS* & 32 & 89.6 & 98.3 & 4$\times$3 \\
Florence (384 $\uparrow$) \cite{yuan2021florence} & FLD-900M & - & 87.8 & - & 1$\times$3 \\
X-CLIP-B/16 \cite{ni2022expanding} & CLIP-400M & 8 & 85.3 & 97.1 & 4$\times$3 \\
ASU-B/16 \cite{chen2023video} & CLIP-400M & 8 & 85.7 & - & 4$\times$3  \\
\hline
\rowcolor{gray!25} 
\textbf{CLAVER-B/16 (KMT)} & CLIP-400M & 8 & 85.9 & 97.3 & 4$\times$3 \\
\hline
\end{tabular}
}
\end{minipage}
\hspace{0.05cm}
\begin{minipage}[t]{0.48\linewidth}
\centering
\caption{Few-shot performance on HMDB-51 and UCF-101.}
\vspace{-0.3cm}
\label{table: few-shot HMDB-51 UCF-101}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{HMDB-51}} &  & \multicolumn{4}{c}{\textbf{UCF-101}} \\
\cmidrule(r){2-5} \cmidrule(r){7-10}
& K=2 & K=4 & K=8 & K=16 &  & K=2 & K=4 & K=8 & K=16 \\
\hline
TSM \cite{lin2019tsm} & 17.5 & 20.9 & 18.4 & 31.0 &   & 25.3 & 47.0 & 64.4 & 61.0 \\
TimeSformer \cite{bertasius2021space} & 19.6 & 40.6 & 49.4 & 55.4 &  & 48.5 & 75.6 & 83.7 & 89.4 \\
Swin-B \cite{liu2022video} & 20.9 & 41.3 & 47.9 & 56.1 &  & 53.3 & 74.1 & 85.8 & 88.7 \\
Action-CLIP \cite{wang2021actionclip} & 55.0 & 56.0 & 58.0 & - &  & 80.0 & 85.0 & 89.0 & - \\
X-CLIP \cite{ni2022expanding} & 53.0 & 57.3 & 62.8 & 64.0 &  & 76.4 & 83.4 & 88.3 & 91.4 \\
X-Florence \cite{ni2022expanding} & 51.6 & 57.8 & 64.1 & 64.2 &  & 84.0 & 88.5 & 92.5 & 94.8 \\
MAXI \cite{lin2023match} & 58.0 & 60.1 & 65.0 & 66.5 & & 86.8 & 89.3 & 92.4 & 93.5 \\
ASU \cite{chen2023video} & \textbf{60.1} & 63.8 & 67.2 & 70.8 & & \textbf{91.4} & \textbf{94.6} & 96.0 & 97.2 \\
\hline
\rowcolor{gray!25} 
\textbf{CLAVER (KMT)}&  58.6 &  \textbf{63.9} & \textbf{68.0} & \textbf{72.5} &  &  89.7 & 92.9 & \textbf{96.1} & \textbf{98.0} \\
\hline
\end{tabular}
}
\end{minipage}
\vspace{-0.1cm}
\end{table}

\begin{table}[t!]
\vspace{-0.1cm}
 \label{table: zero-shot}
    \begin{minipage}[t]{0.48\linewidth}
    \centering
\caption{Zero-shot on HMDB-51 and UCF-101.}
\vspace{-0.3cm}
\label{table: zero-shot HMDB-51 UCF-101}
\resizebox{0.82\textwidth}{!}{
\begin{tabular}{ccc}
\hline
\textbf{Method} & \textbf{HMDB-51} & \textbf{UCF-101} \\
\hline
MTE \cite{xu2016multi} & 19.7 $\pm$ 1.6 & 15.8 $\pm$ 1.3 \\
ASR \cite{wang2017alternative} & 21.8 $\pm$ 0.9 & 24.4 $\pm$ 1.0 \\
ZSECOC \cite{qin2017zero} & 22.6 $\pm$ 1.2 & 15.1 $\pm$ 1.7 \\
UR \cite{zhu2018towards} & 24.4 $\pm$ 1.6 & 17.5 $\pm$ 1.6 \\
TS-GCN \cite{gao2019know} & 23.2 $\pm$ 3.0 & 34.2 $\pm$ 3.1 \\
E2E \cite{brattoli2020rethinking} & 32.7 & 48 \\
ER-ZSAR \cite{chen2021elaborative} & 35.3 $\pm$ 4.6 & 51.8 $\pm$ 2.9 \\
Action-CLIP \cite{wang2021actionclip} & 40.8 $\pm$ 5.4  & 58.3 $\pm$ 3.4 \\
X-CLIP \cite{ni2022expanding} & 44.6 $\pm$ 5.2  & 72.0 $\pm$ 2.3 \\
ASU \cite{chen2023video} & 48.1 $\pm$ 2.8  & 75.0 $\pm$ 2.3 \\
MAXI \cite{lin2023match} & 51.2 $\pm$ 1.1  & 75.2 $\pm$ 0.9 \\
OST \cite{chen2024ost} & 52.9 $\pm$ 0.9  & 75.3 $\pm$ 2.1 \\
\hline
\rowcolor{gray!25} 
\textbf{CLAVER (KMT)} & \textbf{54.1 $\pm$ 2.4} & \textbf{78.6 $\pm$ 1.7}\\
\hline
\end{tabular}} 
    \end{minipage}
    \hspace{0.2cm}
    \begin{minipage}[t]{0.48\linewidth} 
        \centering
\caption{Zero-shot on Kinetics-600.}
\vspace{-0.1cm}
\label{table: zero-shot Kinetics600}
\resizebox{0.87\textwidth}{!}{
\begin{tabular}{ccc}
\hline
\textbf{Method} & \textbf{Top-1} & \textbf{Top-5} \\
\hline
DEVISE \cite{frome2013devise} & 23.8 $\pm$ 0.3 & 51.0 $\pm$ 0.6 \\
ALE \cite{akata2015label} & 23.4 $\pm$ 0.8 & 50.3 $\pm$ 1.4 \\
SJE \cite{akata2015evaluation} & 22.3 $\pm$ 0.6 & 48.2 $\pm$ 0.4 \\
ESZSL \cite{romera2015embarrassingly} & 22.9 $\pm$ 1.2 & 48.3 $\pm$ 0.8 \\
DEM \cite{zhang2017learning} & 23.6 $\pm$ 0.7 & 49.5 $\pm$ 0.4 \\
GCN \cite{ghosh2020all} & 22.3 $\pm$ 0.6 & 49.7 $\pm$ 0.6 \\
ER-ZSAR \cite{chen2021elaborative} & 42.1 $\pm$ 1.4 & 73.1 $\pm$ 0.3 \\
X-CLIP \cite{ni2022expanding}& 65.2 $\pm$ 0.4 & 86.1 $\pm$ 0.8 \\
ASU \cite{chen2023video} & 67.6 $\pm$ 0.2& 87.2 $\pm$ 0.3 \\
MAXI \cite{lin2023match} & 70.9 $\pm$ 1.2  & 92.1 $\pm$ 0.5 \\
OST \cite{chen2024ost} & 70.5 $\pm$ 0.7  & 92.1 $\pm$ 0.3 \\
\hline
\rowcolor{gray!25} 
\textbf{CLAVER (KMT)}  & \textbf{73.8 $\pm$ 0.6}& \textbf{93.1 $\pm$ 0.6}\\
\hline
\end{tabular}}
    \end{minipage}
    \vspace{-0.5cm}
\end{table}

%  \resizebox{0.95\textwidth}{!}{   
%         \begin{tabular}{ccc}
%             \toprule
% \textbf{Text Prompts}& \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)} \\
% \hline
% Noun and Phrase of Action & 79.6 & 94.9 \\
% $+$ Prefix-Suffix & 79.8 & 95.0 \\
% $+$ 5 $\times$ Interpretive Prompt & 80.9 & 95.4 \\
% $+$ 10 $\times$ Interpretive Prompt & 81.3 & \textbf{95.5}\\
% $+$ 15 $\times$ Interpretive Prompt & \textbf{81.5} & \textbf{95.5}\\
% \bottomrule
%         \end{tabular}
% }

\textbf{Few-shot Experiments.} We pretrain CLAVER-B/16\(_{32f}\) (KMT) (\(32f\) means with 32 frames) on Kinetics-400, and then perform few-shot transfer of 2, 4, 8, 16 samples on HMDB-51 and UCF-101. Tab. \ref{table: few-shot HMDB-51 UCF-101} depicts the results of few-shot experiments. CLAVER is comparable wtih ASU \cite{chen2023video}, and consistently surpasses the X-CLIP \cite{ni2022expanding} and MAXI \cite{lin2023match} across all K ranges. Additionally, CLAVER significantly outperforms other previous methods like \cite{lin2019tsm,bertasius2021space,liu2022video}. More details about the evaluation protocols are provided in the \textit{Appendix} \ref{sec: Experiment Setting Details}.

\textbf{Zero-shot Experiments.} We also pretrain CLAVER-B/16\(_{32f}\) (KMT) on Kinetics400 for zero-shot. As shown in Tab. \ref{table: zero-shot HMDB-51 UCF-101}, on HMDB-51 \cite{kuehne2011hmdb} and UCF-101 \cite{soomro2012ucf101} benchmarks, CLAVER surpasses OST \cite{chen2024ost} , ASU \cite{chen2023video} and X-CLIP \cite{ni2022expanding} under the same configuration, and far outperforms other previous methods. Additionally, in Tab. \ref{table: zero-shot Kinetics600}, on Kinetics600 \cite{carreira2018short} benchmark, CLAVER outperforms OST \cite{chen2024ost}, MAXI \cite{lin2023match} and all other methods. More details about the evaluation protocols are provided in the \textit{Appendix} \ref{sec: Experiment Setting Details}.

\subsection{Ablation Study}
\label{sec:ablation study}
\textbf{Components ablation studies.} We performed ablation studies to evaluate the effects of each component under the CLAVER-B/32\(_{8f}\) (KMT) configuration. The results are shown in Tab. \ref{table: ablation analysis of each component}. Our baseline, denoted as CLIP-Mean, implements temporal mean pooling for CLIP. By equipping the CLIP with a KMT transformer at 1/3 scale, the Top-1 accuracy increases by 3.5\%. We only introduce interpretive prompt, the performance increases by 1\%. When we further incorporate both of them, CLAVER surpasses the CLIP-Mean by 4.1\%. In addition, we test the effect of the number of KMT transformer layers. With only one layer, the performance improvement is minimal. Increasing the number of layers to 1/6 scale results in further performance gains, and at 1/3 scale, we observe significant improvement.

\begin{table}[t!]
 \begin{minipage}[t]{0.5\textwidth}
        \centering
        \caption{Ablation of each component.}
        \vspace{-0.2cm}
        \label{table: ablation analysis of each component}
 \resizebox{0.99\textwidth}{!}{   
        \begin{tabular}{cc}
            \toprule
\textbf{Components}& \textbf{Top-1 (\%)}\\
\hline
Baseline (CLIP-Mean) & 77.4\\
Baseline + KMTA 1 layer & 78.3\\
Baseline + KMTA 1/6 scale & 79.6\\
Baseline + KMTA 1/3 scale & 80.9\\
Baseline + Interpretive Prompt & 78.4 \\
Baseline + KMTA 1/3 scale + Interpretive Prompt & \textbf{81.5} \\
\bottomrule
        \end{tabular}
}
    \end{minipage}
    \hspace{0.05cm}
    \begin{minipage}[t]{0.5\textwidth}
        \centering
 \caption{Comparison of temporal attentions.}
        \vspace{-0.2cm}
     \label{table: comparison of different temporal attention}
            \resizebox{1.0\textwidth}{!}{   
        \begin{tabular}{ccc}
            \toprule
\textbf{Temporal Modeling} & \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)}\\
\hline
Baseline (Mean Pooling)& 78.4& 94.3\\
Class-Token-Only& 78.9& 94.3\\
Pipeline Temporal Attention& 79.4& 94.4\\
Joint Attention& 80.1& 94.9\\
Kronecker Mask Causal Temporal Attention& 81.4& \textbf{95.5}\\
Kronecker Mask Temporal Attention& \textbf{81.5}& \textbf{95.5}\\
\bottomrule
        \end{tabular}
}
    \end{minipage}
\vspace{-0.2cm}
\end{table}


% \begin{table}[t]
%         \centering
%         \caption{The impact of patch size and frame length on joint attention (JA), KMTA and KMCTA. Conducting on HMDB-51 and UCF-101.}
%         \vspace{-0.2cm}
%      \label{table: Comparison of JA, KMTA and KMCTA}
%             \resizebox{0.99\textwidth}{!}{   
% \begin{tabular}{ccccccc}
%             \toprule
% \multirow{2}{*}{\textbf{HMDB-51}, \textbf{UCF-101}}&  \multicolumn{3}{c}{patch size = 32 (ViT-B/32)}& \multicolumn{3}{c}{patch size = 16 (ViT-B/16)} \\
% \cmidrule(r){2-7}
% & JA (\%)& KMTA (\%)& KMCTA (\%)& JA (\%)& KMTA (\%)& KMCTA (\%)\\
% \hline
% frame length = 8& 67.9, 93.3& 68.8, 94.1& 68.7, 93.9& 72.2, 96.2& 73.2, 96.3& 72.2, 96.1\\
% frame length = 16& 68.0(+0.1), 93.6(+0.3)&69.1(+0.3), 94.2(+0.1)& 69.4(+0.7), 94.8(+0.9)& 72.1(-0.1), 96.3(+0.1)& 72.3(-0.9), 96.6(+0.3)& 72.8(+0.6), 96.4(+0.3)\\
% \bottomrule
% \end{tabular}
% }
% \vspace{-0.2cm}
% \end{table}

\begin{table}[t]
        \centering
        \caption{The impact of patch size and frame length on joint attention (JA), KMTA and KMCTA. Conducting on HMDB-51 and UCF-101.}
        \vspace{-0.2cm}
     \label{table: Comparison of JA, KMTA and KMCTA}
            \resizebox{0.85\textwidth}{!}{   
\begin{tabular}{ccccccc}
            \toprule
\multirow{2}{*}{\textbf{HMDB-51}, \textbf{UCF-101}}&  \multicolumn{3}{c}{patch size = 32 (ViT-B/32)}& \multicolumn{3}{c}{patch size = 16 (ViT-B/16)} \\
\cmidrule(r){2-7}
& JA (\%)& KMTA (\%)& KMCTA (\%)& JA (\%)& KMTA (\%)& KMCTA (\%)\\
\hline
frame length = 8& 67.9, 93.3& 68.8, 94.1& 68.7, 93.9& 72.2, 96.2& 73.2, 96.3& 72.2, 96.1\\
frame length = 16& 68.0, 93.6&69.1, 94.2& 69.4, 94.8& 72.1, 96.3& 72.3, 96.6& 72.8, 96.4\\
\bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\end{table}


\begin{figure*}[t!]
    \centering
\includegraphics[width=0.97\textwidth,height=0.12\textwidth]{Word_Importance.png}
    \caption{Word importance of CLIP, X-CLIP, ILA and CLAVER. Darker color, higher importance.}
\vspace{-0.3cm}
    \label{fig: Word Importance}
\vspace{-0.1cm}
\end{figure*}

\textbf{Comparison of different temporal attentions and low-rank bottleneck issue.} Tab. \ref{table: comparison of different temporal attention} compares the performance of different temporal modeling methods mentioned in Sec. \ref{sec:kronecker mask attention}. We find that class-token-only temporal attention and pipeline temporal attention have inferior performance. KMTA and KMCTA outperform joint attention. The top-1 (\%) of KMTA slightly surpasses KMCTA by 0.1\%. Then, we conduct further experiments on HMDB-51 and UCF101 that observing the impact of the low-rank bottleneck issue on them. We increasing the length of token sequences by reducing patch size or increasing frame length, and observe the effects on them, as shown in Tab. \ref{table: Comparison of JA, KMTA and KMCTA}. When reducing the patch size, however, we do not observe the low-rank bottleneck. Joint attention, KMTA and KMCTA achieve better performance due to more fine-grained features as the smaller patch size results in each token representing smaller local region. In contrast, when we increase the frame length, KMCTA's performance is optimal in most configurations, and only the performance of KMCTA can steadily improve. The performance improvement of joint attention and KMTA is limited. Meanwhile, in Tab. \ref{table: Kinetics-400}, with the increase of frame length under the same backbone, the performance increase of KMCTA is also greater than that of KMTA. This indicates that KMCTA has a more significant advantage with longer frame length.

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\textwidth,height=0.32\textwidth]{Spatiotemporal_Homogenization.png}
\caption{Spatiotemporal Homogenization. (\textbf{Upper Left}) Token shuffling. (\textbf{Vertical bar chart}) Word importance refers to the degree of correlation between each word in a sentence description and the semantics of the video content, while (\textbf{Horizontal bar chart}) Similarity refers to the similarity between visual and textual representations.}
\vspace{-0.3cm}
    \label{fig: Spatiotemporal Homogenizations}
\vspace{-0.4cm}
\end{figure*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=0.75\textwidth,height=0.49\textwidth]{Spatiotemporal_Homogenization_ILA_XCLIP.png}
%     \caption{We visualize the impact of token shuffling on the ILA, XCLIP. In most cases, token shuffling has limited influence on their word importance. Surprisingly, in some figures, after token shuffling, the similarity actually increases, which is illogical. This indicates that the response of ILA and XCLIP to spatiotemporal position destruction is uncertain and has poor interpretability.}
%     \label{fig: Spatiotemporal Homogenization ILA XCLIP}
% \end{figure*}



\section{Visualization and Analysis}
\label{sec: visualization analysis}
We employ \cite{chefer2021generic} for bi-modal visualization and show the explainability of visual-textual attentions. 

\textbf{Word importance}. We visualize word importance for CLIP, X-CLIP, ILA and CLAVER in Fig. \ref{fig: Word Importance}. Observations indicate that CLIP tends to focus on nouns, whereas CLAVER prefers to verbs. Compared to previous works, CLAVER is more inclined towards verb concept, while X-CLIP \cite{ni2022expanding} and ILA \cite{tu2023implicit} show a slight inclination towards nouns. These findings indicate the effectiveness of interpretive prompts for nouns concept to verbs concept transition.

\textbf{Spatiotemporal homogenization study}. We define spatiotemporal homogenization as a phenomenon where random token shuffling has little impact on the semantics of the visual branch, which is illogical. It aims to illustrate the interpretability behind the performance of spatiotemporal modeling. In Fig. \ref{fig: Spatiotemporal Homogenizations} (\textbf{Upper Left}), we illustrate the token shuffling. We denote token shuffling before adding time embedding as PreTE shuffling, token shuffling after adding time embedding as PostTE shuffling, and None represents no shuffle. Fig. \ref{fig: Spatiotemporal Homogenizations} shows changes in word importance and similarity following both PreTE and PostTE shuffling. For joint attention, we observe that PreTE shuffling marginally affects similarity and word importance, while PostTE shuffling does not affect similarity score and word importance. Regarding KMTA, both PreTE and PostTE shuffling lead to changes in the similarity and word importance. Meanwhile, KMCTA is profoundly affected by both PreTE and PostTE shuffling, which result in lower similarity and disturbance of word importance. This phenomenon suggest that KMTA and KMCTA possess varying degrees ability in mitigating spatiotemporal homogenization and the Kronecker mask serves as a natural inductive bias for spatiotemporal structural heterogeneity. It also demonstrates that equipping learnable position/time encoding is not insufficient to alleviate spatiotemporal homogenization. Please refer to \textit{Appendix} \ref{sec: Spatiotemporal Homogenization Study} for more and explanations and visualizations.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth,height=0.36\textwidth]{Attention.png}
    \caption{Visualizing spatiotemporal attention maps of different spatio-temporal modeling.}
\vspace{-0.3cm}
    \label{fig: Spatiotemporal attention maps}
\vspace{-0.5cm}
\end{figure*}

\textbf{Visualization of spatiotemporal attention map}. Fig. \ref{fig: Spatiotemporal attention maps} presents various spatiotemporal attention maps. X-CLIP \cite{ni2022expanding} and ILA \cite{tu2023implicit} adopt factorized attention structures resulting in intertwined spatiotemporal attention maps. However, CLAVER employs a factorized encoder structure, allowing for the separation of spatial and temporal attention maps. Notably, class-token-only temporal attention discards other time tubes so that its temporal attention maps appear vacant. KMTA and KMCT have more reasonable spatial (focuses on the executor of the action) and temporal (focuses on areas where the action occurs) attention maps, while ILA and X-CLIP ignore some action dense areas. Moreover, joint attention and pipeline temporal attention will be attracted by some irrelevant backgrounds.

\section{Conclusion}
\label{sec:conclusion}
In this work, we present the \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er} (\textbf{CLAVER}) to shift from the alignment of visual objects and nouns in CLIP to the alignment of action behaviors and verbs. We propose Kronecker mask temporal attention and Kronecker mask causal temporal attention for temporal modeling. Interpretive prompts are employed to transition the focus on nouns to verbs. Extensive experiments under different evaluation settings demonstrate the effectiveness of our method.

% Limitation Discussion and Broader Impact are provided in the \textit{Appendix} \ref{sec: limitation discussion}, \ref{sec: Broader Impact}.

\section{Acknowledgement}
\label{sec:acknowledgement}
The numerical calculations in this paper have been done on the supercomputing system in the Supercomputing Center of University of Science and Technology of China. This work was supported by the National Science Foundation of China, under Grant No. 62171425. This work was supported by National Natural Science Foundation of China under Grant 62306061. This work was supported by Guangdong Basic and Applied Basic Research Foundation (Grant No. 2023A1515140037). This work was supported by Guangdong Research Team for Communication and Sensing Integrated with Intelligent Computing (Project No. 2024KCXTD047).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\section{Appendix}
\appendix
\textbf{Roadmap}

\label{sec: roadmap}
In the Appendix, we present proof in \ref{sec: Proof}, Spatiotemporal homogenization study details in \ref{sec: Spatiotemporal Homogenization Study}, some additional experiment results in \ref{sec: Supplementary Experiments}, experiment setting details in \ref{sec: Experiment Setting Details}, interpretive prompt technique details in \ref{sec: Interpretive Prompt Details}, analysis of synthetic video testing in \ref{sec:Analysis of Synthetic Video Testing}, dataset details in \ref{sec: Dataset Details}, and more visualization of spatiotemporal attention map in \ref{sec: More Attention Heat Map Visualization}.

% limitation discussions in \ref{sec: limitation discussion}, broader impact in \ref{sec: Broader Impact}, 

\input{supp}


\end{document}
