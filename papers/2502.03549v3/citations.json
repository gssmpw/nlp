[
  {
    "index": 0,
    "papers": [
      {
        "key": "qiu2017learning",
        "author": "Qiu, Zhaofan and Yao, Ting and Mei, Tao",
        "title": "Learning spatio-temporal representation with pseudo-3d residual networks"
      },
      {
        "key": "tran2015learning",
        "author": "Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar",
        "title": "Learning spatiotemporal features with 3d convolutional networks"
      },
      {
        "key": "tran2018closer",
        "author": "Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar",
        "title": "A closer look at spatiotemporal convolutions for action recognition"
      },
      {
        "key": "xie2018rethinking",
        "author": "Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin",
        "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification"
      },
      {
        "key": "feichtenhofer2019slowfast",
        "author": "Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming",
        "title": "Slowfast networks for video recognition"
      },
      {
        "key": "feichtenhofer2020x3d",
        "author": "Feichtenhofer, Christoph",
        "title": "X3d: Expanding architectures for efficient video recognition"
      },
      {
        "key": "yu2021searching",
        "author": "Yu, Zitong and Zhou, Benjia and Wan, Jun and Wang, Pichao and Chen, Haoyu and Liu, Xin and Li, Stan Z and Zhao, Guoying",
        "title": "Searching multi-rate and multi-modal temporal enhanced networks for gesture recognition"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "qiu2017learning",
        "author": "Qiu, Zhaofan and Yao, Ting and Mei, Tao",
        "title": "Learning spatio-temporal representation with pseudo-3d residual networks"
      },
      {
        "key": "tran2018closer",
        "author": "Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar",
        "title": "A closer look at spatiotemporal convolutions for action recognition"
      },
      {
        "key": "xie2018rethinking",
        "author": "Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin",
        "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2020tea",
        "author": "Li, Yan and Ji, Bin and Shi, Xintian and Zhang, Jianguo and Kang, Bin and Wang, Limin",
        "title": "Tea: Temporal excitation and aggregation for action recognition"
      },
      {
        "key": "lin2019tsm",
        "author": "Lin, Ji and Gan, Chuang and Han, Song",
        "title": "Tsm: Temporal shift module for efficient video understanding"
      },
      {
        "key": "liu2021tam",
        "author": "Liu, Zhaoyang and Wang, Limin and Wu, Wayne and Qian, Chen and Lu, Tong",
        "title": "Tam: Temporal adaptive module for video recognition"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "arnab2021vivit",
        "author": "Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\\v{c}}i{\\'c}, Mario and Schmid, Cordelia",
        "title": "Vivit: A video vision transformer"
      },
      {
        "key": "neimark2021video",
        "author": "Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan",
        "title": "Video transformer network"
      },
      {
        "key": "bertasius2021space",
        "author": "Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo",
        "title": "Is space-time attention all you need for video understanding?"
      },
      {
        "key": "fan2021multiscale",
        "author": "Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph",
        "title": "Multiscale vision transformers"
      },
      {
        "key": "liu2022video",
        "author": "Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han",
        "title": "Video swin transformer"
      },
      {
        "key": "yan2022multiview",
        "author": "Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia",
        "title": "Multiview transformers for video recognition"
      },
      {
        "key": "li2022uniformer",
        "author": "Li, Kunchang and Wang, Yali and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu",
        "title": "Uniformer: Unified transformer for efficient spatiotemporal representation learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "arnab2021vivit",
        "author": "Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\\v{c}}i{\\'c}, Mario and Schmid, Cordelia",
        "title": "Vivit: A video vision transformer"
      },
      {
        "key": "neimark2021video",
        "author": "Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan",
        "title": "Video transformer network"
      },
      {
        "key": "girdhar2021anticipative",
        "author": "Girdhar, Rohit and Grauman, Kristen",
        "title": "Anticipative video transformer"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bertasius2021space",
        "author": "Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo",
        "title": "Is space-time attention all you need for video understanding?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "arnab2021vivit",
        "author": "Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\\v{c}}i{\\'c}, Mario and Schmid, Cordelia",
        "title": "Vivit: A video vision transformer"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhao2022alignment",
        "author": "Zhao, Yizhou and Li, Zhenyang and Guo, Xun and Lu, Yan",
        "title": "Alignment-guided temporal attention for video action recognition"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2022video",
        "author": "Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han",
        "title": "Video swin transformer"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "li2020unicoder",
        "author": "Li, Gen and Duan, Nan and Fang, Yuejian and Gong, Ming and Jiang, Daxin",
        "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training"
      },
      {
        "key": "lu2019vilbert",
        "author": "Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan",
        "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks"
      },
      {
        "key": "su2019vl",
        "author": "Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng",
        "title": "Vl-bert: Pre-training of generic visual-linguistic representations"
      },
      {
        "key": "tan2019lxmert",
        "author": "Tan, Hao and Bansal, Mohit",
        "title": "Lxmert: Learning cross-modality encoder representations from transformers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "miech2019howto100m",
        "author": "Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef",
        "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips"
      },
      {
        "key": "sun2019learning",
        "author": "Sun, Chen and Baradel, Fabien and Murphy, Kevin and Schmid, Cordelia",
        "title": "Learning video representations using contrastive bidirectional transformer"
      },
      {
        "key": "sun2019videobert",
        "author": "Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia",
        "title": "Videobert: A joint model for video and language representation learning"
      },
      {
        "key": "zhu2020actbert",
        "author": "Zhu, Linchao and Yang, Yi",
        "title": "Actbert: Learning global-local video-text representations"
      },
      {
        "key": "xu2021videoclip",
        "author": "Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph",
        "title": "Videoclip: Contrastive pre-training for zero-shot video-text understanding"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "jia2021scaling",
        "author": "Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom",
        "title": "Scaling up visual and vision-language representation learning with noisy text supervision"
      },
      {
        "key": "yuan2021florence",
        "author": "Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others",
        "title": "Florence: A new foundation model for computer vision"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gu2021open",
        "author": "Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin",
        "title": "Open-vocabulary object detection via vision and language knowledge distillation"
      },
      {
        "key": "vinker2022clipasso",
        "author": "Vinker, Yael and Pajouheshgar, Ehsan and Bo, Jessica Y and Bachmann, Roman Christian and Bermano, Amit Haim and Cohen-Or, Daniel and Zamir, Amir and Shamir, Ariel",
        "title": "Clipasso: Semantically-aware object sketching"
      },
      {
        "key": "li2022grounded",
        "author": "Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others",
        "title": "Grounded language-image pre-training"
      },
      {
        "key": "luo2022clip4clip",
        "author": "Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui",
        "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning"
      },
      {
        "key": "xu2022groupvit",
        "author": "Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong",
        "title": "Groupvit: Semantic segmentation emerges from text supervision"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2021actionclip",
        "author": "Wang, Mengmeng and Xing, Jiazheng and Liu, Yong",
        "title": "Actionclip: A new paradigm for video action recognition"
      },
      {
        "key": "ni2022expanding",
        "author": "Ni, Bolin and Peng, Houwen and Chen, Minghao and Zhang, Songyang and Meng, Gaofeng and Fu, Jianlong and Xiang, Shiming and Ling, Haibin",
        "title": "Expanding language-image pretrained models for general video recognition"
      },
      {
        "key": "lin2022frozen",
        "author": "Lin, Ziyi and Geng, Shijie and Zhang, Renrui and Gao, Peng and de Melo, Gerard and Wang, Xiaogang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng",
        "title": "Frozen clip models are efficient video learners"
      },
      {
        "key": "ju2022prompting",
        "author": "Ju, Chen and Han, Tengda and Zheng, Kunhao and Zhang, Ya and Xie, Weidi",
        "title": "Prompting visual-language models for efficient video understanding"
      },
      {
        "key": "pan2022st",
        "author": "Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng",
        "title": "St-adapter: Parameter-efficient image-to-video transfer learning"
      },
      {
        "key": "rasheed2023fine",
        "author": "Rasheed, Hanoona and Khattak, Muhammad Uzair and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz",
        "title": "Fine-tuned clip models are efficient video learners"
      },
      {
        "key": "chen2023video",
        "author": "Chen, Yifei and Chen, Dapeng and Liu, Ruijin and Li, Hao and Peng, Wei",
        "title": "Video action recognition with attentive semantic units"
      },
      {
        "key": "tu2023implicit",
        "author": "Tu, Shuyuan and Dai, Qi and Wu, Zuxuan and Cheng, Zhi-Qi and Hu, Han and Jiang, Yu-Gang",
        "title": "Implicit temporal modeling with learnable alignment for video recognition"
      },
      {
        "key": "lin2023match",
        "author": "Lin, Wei and Karlinsky, Leonid and Shvetsova, Nina and Possegger, Horst and Kozinski, Mateusz and Panda, Rameswar and Feris, Rogerio and Kuehne, Hilde and Bischof, Horst",
        "title": "Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge"
      },
      {
        "key": "momeni2023verbs",
        "author": "Momeni, Liliane and Caron, Mathilde and Nagrani, Arsha and Zisserman, Andrew and Schmid, Cordelia",
        "title": "Verbs in action: Improving verb understanding in video-language models"
      },
      {
        "key": "yang2023aim",
        "author": "Yang, Taojiannan and Zhu, Yi and Xie, Yusheng and Zhang, Aston and Chen, Chen and Li, Mu",
        "title": "Aim: Adapting image models for efficient video action recognition"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "rasheed2023fine",
        "author": "Rasheed, Hanoona and Khattak, Muhammad Uzair and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz",
        "title": "Fine-tuned clip models are efficient video learners"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "ju2022prompting",
        "author": "Ju, Chen and Han, Tengda and Zheng, Kunhao and Zhang, Ya and Xie, Weidi",
        "title": "Prompting visual-language models for efficient video understanding"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ni2022expanding",
        "author": "Ni, Bolin and Peng, Houwen and Chen, Minghao and Zhang, Songyang and Meng, Gaofeng and Fu, Jianlong and Xiang, Shiming and Ling, Haibin",
        "title": "Expanding language-image pretrained models for general video recognition"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "tu2023implicit",
        "author": "Tu, Shuyuan and Dai, Qi and Wu, Zuxuan and Cheng, Zhi-Qi and Hu, Han and Jiang, Yu-Gang",
        "title": "Implicit temporal modeling with learnable alignment for video recognition"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "lin2022frozen",
        "author": "Lin, Ziyi and Geng, Shijie and Zhang, Renrui and Gao, Peng and de Melo, Gerard and Wang, Xiaogang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng",
        "title": "Frozen clip models are efficient video learners"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "chowdhery2023palm",
        "author": "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others",
        "title": "Palm: Scaling language modeling with pathways"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "touvron2023llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "dubey2024llama3herdmodels",
        "author": "Abhimanyu Dubey, Abhinav Jauhri and Abhinav Pandey and et al.",
        "title": "The Llama 3 Herd of Models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "openai2024gpt4technicalreport",
        "author": "OpenAI",
        "title": "GPT-4 Technical Report, https://arxiv.org/abs/2303.08774"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "chen2021elaborative",
        "author": "Chen, Shizhe and Huang, Dong",
        "title": "Elaborative rehearsal for zero-shot action recognition"
      },
      {
        "key": "qian2022rethinking",
        "author": "Qian, Yijun and Yu, Lijun and Liu, Wenhe and Hauptmann, Alexander G",
        "title": "Rethinking zero-shot action recognition: Learning from latent atomic actions"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "fan2024improving",
        "author": "Fan, Lijie and Krishnan, Dilip and Isola, Phillip and Katabi, Dina and Tian, Yonglong",
        "title": "Improving clip training with language rewrites"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "momeni2023verbs",
        "author": "Momeni, Liliane and Caron, Mathilde and Nagrani, Arsha and Zisserman, Andrew and Schmid, Cordelia",
        "title": "Verbs in action: Improving verb understanding in video-language models"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "lin2023match",
        "author": "Lin, Wei and Karlinsky, Leonid and Shvetsova, Nina and Possegger, Horst and Kozinski, Mateusz and Panda, Rameswar and Feris, Rogerio and Kuehne, Hilde and Bischof, Horst",
        "title": "Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge"
      }
    ]
  }
]