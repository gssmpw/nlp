\section{Proof}
\label{sec: Proof}
We conclude that there is a low-rank bottleneck problem for joint attention and KMTA when facing longer token sequence lengths (\(d \ll n\)), while KMCTA always guarantees the property of full rank under our assumption. Proofs as follow:

\subsection{Proof of joint attention cannot guarantee full rank}

Based on the Representation Theorem of Bhojanapalli et al. \cite{bhojanapalli2020low}:

\textbf{Theorem 1} (Representation Theorem \cite{bhojanapalli2020low}). \textit{If} \(d_q=d_k=d \geq n\), \textit{then given any full row rank matrix}  \(\textbf{X} \in \mathcal{R}^{n \times d}\) \textit{and an arbitrary} \(n \times n\) \textit{positive row stochastic matrix} \(\textbf{P}\), \textit{subject to the sum of each row of} \(\textbf{P}\) \textit{is equal to 1, i.e.,} \(\textbf{P}\textbf{1}=\textbf{1}\), \textit{there always exists \(d \times d\) projection matrices} \(\textbf{W}_q\) \textit{and} \(\textbf{W}_k\) \textit{such that}
\begin{equation}
\label{eqn: representation theorem}
\mathrm{Softmax}[\frac{(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T}{\sqrt{d_k}}] = \textbf{P}
\end{equation}
\textit{If} \(d_q=d_k=d < n\), \textit{there exist} \(\textbf{X}\) \textit{and} \(\textbf{P}\) \textit{such that Eqn. \ref{eqn: representation theorem} does not hold for all}  \(\textbf{W}_q\) \textit{and} \(\textbf{W}_k\).

The proof process of \textbf{Theorem 1} are referred from Bhojanapalli et al. \cite{bhojanapalli2020low}. By the way, in Bhojanapalli et al. \cite{bhojanapalli2020low}, they set \(\textbf{X} \in \mathcal{R}^{d \times n}\). However, for the convenience of our subsequent proof, we transpose the \(n\) and \(d\) dimensions, \(\textbf{X} \in \mathcal{R}^{n \times d}\). 

For \(d \geq n\) case. Since \(\textbf{X}\) exhibits full row rank, there exists a right pseudo inverse \(\textbf{X}^\dagger =\textbf{X}^T(\textbf{X}\textbf{X}^T)^{-1} \in \mathcal{R}^{d \times n}\) such that \(\textbf{X}\textbf{X}^\dagger=\textbf{I}_n\). Let \(\textbf{W}_k=\textbf{X}^\dagger\tilde{\textbf{W}}_k\) and \(\textbf{W}_q=\textbf{X}^\dagger\tilde{\textbf{W}}_q\). Then
\begin{align}
\label{eqn: representation theorem proof 1}
(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T &= \textbf{X}\textbf{W}_q\textbf{W}_k^T\textbf{X}^T \\
& = \textbf{X}\textbf{X}^\dagger\tilde{\textbf{W}}_q\tilde{\textbf{W}}_k^T(\textbf{X}^\dagger)^T\textbf{X}^T \\
& = \textbf{X}\textbf{X}^\dagger\tilde{\textbf{W}}_q\tilde{\textbf{W}}_k^T(\textbf{X}\textbf{X}^\dagger)^T \\
& = \textbf{I}_n\tilde{\textbf{W}}_q\tilde{\textbf{W}}_k^T\textbf{I}_n^T \\
& = \tilde{\textbf{W}}_q\tilde{\textbf{W}}_k^T = \tilde{\textbf{W}}_{qk}
\end{align} 
According Eqn. \ref{eqn: representation theorem}, we obtain that
\begin{align}
\label{eqn: representation theorem proof 2}
\mathrm{Softmax}[\frac{(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T}{\sqrt{d_k}}] & = \mathrm{Softmax}[\frac{\tilde{\textbf{W}}_{qk}}{\sqrt{d_k}}] \\
& =\textbf{D}^{-1}_{\tilde{\textbf{W}}_{qk}} \mathrm{exp}(\frac{\tilde{\textbf{W}}_{qk}}{\sqrt{d_k}})
\end{align} 
where \(\textbf{D}_{\tilde{\textbf{W}}_{qk}}\) is a \(n \times n\) diagonal matrix such that
\begin{align}
\label{eqn: representation theorem proof 3}
(\textbf{D}_{\tilde{\textbf{W}}_{qk}})_{ii} & =  \sum^n_{j=1} \mathrm{exp}(\frac{(\tilde{\textbf{W}}_{qk})_{ji}}{\sqrt{d_k}}) \\
& = (\mathrm{exp}(\frac{\tilde{\textbf{W}}_{qk}}{\sqrt{d_k}})\textbf{1})_{i}
\end{align} 
We can establish the desired result by showing that there always exists a \(\tilde{\textbf{W}}_{qk}\) that satisfies the following fixed point equation:
\begin{equation}
\label{eqn: representation theorem proof 4}
\textbf{D}_{\tilde{\textbf{W}}_{qk}}^{-1} \mathrm{exp}(\frac{\tilde{\textbf{W}}_{qk}}{\sqrt{d_k}}) = \textbf{P} 
\end{equation} 

Given \(\textbf{P}\), to construct such a \(\tilde{\textbf{W}}_{qk}\), we can pick an random positive diagonal matrix \(\textbf{D}_0\):
\begin{equation}
\label{eqn: representation theorem proof 5}
\tilde{\textbf{W}}_{qk} = \sqrt{d_k} \cdot \mathrm{log}(\textbf{D}_0\textbf{P})
\end{equation} 
Since \(\textbf{P}\) is a positive matrix, and \(\textbf{D}_0\) is a positive diagonal matrix, such a \(\tilde{\textbf{W}}_{qk}\) always exists. According to Eqn. \ref{eqn: representation theorem proof 3} we can conclude
\begin{align}
\label{eqn: representation theorem proof 6}
\textbf{D}_{\tilde{\textbf{W}}_{qk}} & = \mathrm{Diag}(\mathrm{exp}(\frac{\tilde{\textbf{W}}_{qk}} {\sqrt{d_k}})\textbf{1}) \\
& =
\mathrm{Diag}(\textbf{D}_0 \textbf{P} \textbf{1})) =  \mathrm{Diag}(\textbf{D}_0 \textbf{1})) = \textbf{D}_0 
\end{align} 
which indicates that \(\mathrm{exp}(\frac{\tilde{\textbf{W}}_{qk}}{\sqrt{d_k}}) = \textbf{D}_0 \textbf{P}  = \textbf{D}_{\tilde{\textbf{W}}_{qk}} \textbf{P}\). This completes the proof of \(d \geq n\) case of \textbf{Theorem 1}.

Regarding \(d < n\) case. Consider the special case of \(d=1\) and \(n=2\). There has \(\textbf{X} \in \mathcal{R}^{2 \times 1}\) and \(\textbf{W}_q, \textbf{W}_k \in \mathcal{R}^{1 \times 1}\). Suppose \(\textbf{X}=
\begin{bmatrix}
1 \\ 0 
\end{bmatrix}\), so that:
\begin{align}
\label{eqn: representation theorem proof 7}
\mathrm{Softmax}[\frac{(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T}{\sqrt{d_k}}] & = \mathrm{Softmax}[\frac{\begin{bmatrix}
1 \\ 0 
\end{bmatrix}\textbf{W}_q\textbf{W}_k^T[1,0]}{\sqrt{d_k}}] \\
& = \mathrm{Softmax}[\begin{bmatrix}
\frac{\textbf{W}_q\textbf{W}_k^T}{\sqrt{d_k}} & 0\\ 
0 & 0
\end{bmatrix}]
\end{align} 

This matrix clearly cannot be used to generate \(\textbf{P}\). Then we extend the above special case to general values of \(n\) and \(d\), (\(d<n\)). Let \(\textbf{X}=[\textbf{1}_d,\cdots,\textbf{1}_d,\textbf{0}_d]^T=[\textbf{1}_{\mathrm{mat}},\textbf{0}_d]^T \in \mathcal{R}^{n \times d}\), where \(\textbf{1}_d,\textbf{0}_d \in \mathcal{R}^d\) denotes the all ones, zeros column vector, and \(\textbf{1}_{\mathrm{mat}}\) denotes the \(d \times (n -1)\) all ones matrix. Then
\begin{align}
\label{eqn: representation theorem proof 8}
\mathrm{Softmax}[\frac{(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T}{\sqrt{d_k}}] & = \mathrm{Softmax}[\frac{[\textbf{1}_{\mathrm{mat}},\textbf{0}_d]^T \textbf{W}_q\textbf{W}_k^T[\textbf{1}_{\mathrm{mat}},\textbf{0}_d]}{\sqrt{d_k}}] \\
& = \mathrm{Softmax}[\begin{bmatrix}
\textbf{1}_{\mathrm{mat}}^T\frac{\textbf{W}_q\textbf{W}_k^T}{\sqrt{d_k}} \textbf{1}_{\mathrm{mat}} & \textbf{0}_{n-1}\\ 
\textbf{0}_{n-1} & 0
\end{bmatrix}]
\end{align} 

The basic idea of the proof remains consistency, and we can conclude the same conclusion.
According to the \textbf{Theorem 1}, we can not ensure the attention matrix of joint attention to be full rank.

\subsection{Proof of KMCTA can guarantee full rank}

\textbf{Assumption 1} The elements of \(\frac{(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T}{\sqrt{d_k}}\) does not exist negative infinity in normal cases, which means that 0 will not appear at any other position in the attention matrix \(\tilde{\textbf{A}} = \mathrm{Softmax}[\frac{(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T}{\sqrt{d_k}} + \tilde{ \textbf{M}}]\) except for the masked position. 

The softmax operation can ensure that the elements at all other positions are greater than 0 except for the masked position. The attention matrix \(\tilde{\textbf{A}}\) of KMCTA is always a lower triangular matrix due to the presence of the \(\tilde{ \textbf{M}}\) and the diagonal elements are always greater than 0, i.e., \(\forall a_{ii} > 0\):
\begin{equation}
\tilde{\textbf{A}} =
\begin{bmatrix}
a_{11} & 0 & 0 & \cdots & 0 \\
a_{21} & a_{22} & 0 & \cdots & 0 \\
a_{31} & a_{32} & a_{33} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn} \\
\end{bmatrix}
\end{equation}
The determinant of the \(\tilde{\textbf{A}}\), \(|\tilde{\textbf{A}}|= a_{11} a_{22} \cdots a_{nn} > 0 \neq 0\),  because \(\forall a_{ii} > 0\). Thus, the attention matrix \(\tilde{\textbf{A}}\) of KMCTA is always reversible, i.e. always be full rank.

\subsection{Proof of KMTA cannot guarantee full rank}

According to the \textbf{Assumption 1}. We consider a special case, when \(T\)=2 and \(L\)=2, the attention matrix \(\textbf{A} = \mathrm{Softmax}[\frac{(\textbf{X}\textbf{W}_q)(\textbf{X}\textbf{W}_k)^T}{\sqrt{d_k}} + \textbf{M}]\) of KMTA can be formulated as:
\begin{equation}
\begin{bmatrix}
a_{11} & 0 & a_{13} & a_{14} \\
0 & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & 0 \\
a_{41} & a_{42} & 0 & a_{44} \\
\end{bmatrix} \xrightarrow{} 
\begin{bmatrix}
a_{11} & 0 & a_{13} & a_{14} \\
0 & a_{22} & a_{23} & a_{24} \\
0 & 0 & a_{33} - \frac{a_{13}}{a_{11}} a_{31} - \frac{a_{23}}{a_{22}} a_{32} &  - \frac{a_{14}}{a_{11}} a_{31} - \frac{a_{24}}{a_{22}} a_{32} \\
0 & 0 & - \frac{a_{13}}{a_{11}} a_{41} - \frac{a_{23}}{a_{22}} a_{42} & a_{44}  - \frac{a_{14}}{a_{11}} a_{41} - \frac{a_{24}}{a_{22}} a_{42} \\
\end{bmatrix}
\end{equation}

where \(a_{11} + a_{13} + a_{14} = 1, a_{22} + a_{23} + a_{24} = 1, a_{31} + a_{32} + a_{33} = 1,a_{41} + a_{42} + a_{44} = 1\). When \(a_{33} - \frac{a_{13}}{a_{11}} a_{31} - \frac{a_{23}}{a_{22}} a_{32} = - \frac{a_{13}}{a_{11}} a_{41} - \frac{a_{23}}{a_{22}} a_{42}\), \(\textbf{A}\) can not always be full rank. For example:
\begin{equation}
\textbf{A} =
\begin{bmatrix}
0.4 & 0 & 0.4 & 0.2 \\
0 & 0.4 & 0.4 & 0.2 \\
0.4 & 0.5 & 0.1 & 0 \\
0.4 & 0.4 & 0 & 0.2 \\
\end{bmatrix}
\end{equation}
For a video, \(n=T \times L\), If we fix the number of attention heads, the dimension of tokens, the patch size, and the resolution of the input image, the only factor that can affect \(n\) for a video is the frame length. When we pray for a longer frame length and an increase in the number of tokens to improve performance, we may face the problems pointed out in \textbf{Theorem 1}. Because increasing the frame length will further increase \(n\), the expressive power of self attention may encounter bottlenecks. The low-rank bottleneck is vital, because it may lead to that many rows of the attention map are seriously homogenized. As the output of self-attention is the weighted sum of the same set of value vectors, the homogenization of attention weights inevitably leads to the resemblance among the aggregated features. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\textwidth,height=0.57\textwidth]{Spatiotemporal_Homogenization_ILA_XCLIP.png}
    \caption{We visualize the impact of token shuffling on the ILA, XCLIP. In most cases, token shuffling has limited influence on their word importance. Surprisingly, in some figures, after token shuffling, the similarity actually increases, which is illogical. This indicates that the response of ILA and XCLIP to spatiotemporal position destruction is uncertain and has poor interpretability.}
    \label{fig: Spatiotemporal Homogenization ILA XCLIP}
\end{figure*}


\section{Spatiotemporal Homogenization Study}
\label{sec: Spatiotemporal Homogenization Study}
Fig. \ref{fig: Spatiotemporal Homogenizations} shows that equipping learnable position/time encoding with joint attention is not insufficient to alleviate spatiotemporal homogenization. For example, (Upper Left) 'a cartoon kangaroo disco dances'. We can observe that token shuffling has a limited impact on word importance and similarity of joint attention, but it has a significant impact on the word importance of KMT and both similarity and word importance of KMCT. (Upper Right) 'a man is abseiling via using a rope'. (Lower Left) 'an otter stands on a surfboard surfing water'. (Lower Right) 'a person is cutting watermelon with a knife'. The key point is that classification accuracy does not necessarily indicate that these learnable position encoding or attention biases genuinely grasp the concept of spatiotemporal structure. Since the optimization goal of the network is usually to pursue performance, the learned positional encoding or attention bias serve to improve accuracy rather than understand the spatiotemporal concept. This is because their optimization is guided by gradients aimed at increasing accuracy, rather than by an objective function that tells them what spatiotemporal structure is. The Kronecker mask in KMTA and KMCTA pursues not only for good performance, but also for interpretability. The Kronecker mask act as a natural spatiotemporal heterogeneity inductive bias, KMTA/KMCTA exhibit better spatiotemporal complementarity with spatial attention. Additionally, in Fig. \ref{fig: Spatiotemporal Homogenization ILA XCLIP}, we show that several tailored modules (ILA and XCLIP) also encounter spatiotemporal homogenization issues during our testing. Surprisingly, in some cases like 'a man is abseiling via using a rope,' 'an otter stands on a surfboard surfing water,' and 'a person is cutting watermelon with a knife,' the similarity actually increases after token shuffling, which is illogical.

Additionally, we evaluate the performance of joint attention, Kronecker mask temporal attention (KMTA) and Kronecker mask causal temporal attention (KMCTA) on K400 \cite{kay2017kinetics} after introducing token shuffling. The results are shown in Tab. \ref{table: token shuffling}. The greater performance degradation observed in KMTA and KMCTA indicates that they are more sensitive to disturbances in the spatiotemporal structure, thus alleviating spatiotemporal homogenization.


\begin{table}[h!]
\vspace{-0.2cm}
        \centering
        \caption{Performance of joint attention, KMTA and KMCTA while introducing token shuffling.}
\vspace{-0.2cm}
        \label{table: token shuffling}
 \resizebox{0.48\textwidth}{!}{   
        \begin{tabular}{cc}
            \toprule
\textbf{Token Shuffling (Yes-Y, No-N)} & \textbf{Top-1 (\%)} \\
\hline
Joint Attention, N & 80.1\\
Kronecker Mask Temporal Attention, N & 81.5\\
Kronecker Mask Causal Temporal Attention, N & 81.4\\
Joint Attention, Y & 58.9 (-21.2)\\
Kronecker Mask Temporal Attention, Y & 43.5 (-38.0)\\
Kronecker Mask Causal Temporal Attention, Y & 42.6 (-38.8)\\
\bottomrule
        \end{tabular}
}
\vspace{-0.3cm}
\end{table}



% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth,height=0.6\textwidth]{Spatiotemporal_Homogenization_ILA_XCLIP.png}
% \caption{(\textbf{Left}) Token shuffling. The token shuffling before adding temporal embedding is called Pre TE, after termed as Post TE, and None represents no shuffle. (\textbf{Right}) We show the sensitivity of the model to randomly shuffling the token positions. The upper figure shows the changes in the word importance in sentences, and the lower figure shows the changes in the similarity between video representations and corresponding textual descriptions. 'a cartoon kangaroo disco dances'. We can observe that token shuffling has a limited impact on word importance and similarity of joint attention, but it has a significant impact on the word importance of KMT and both similarity and word importance of KMCT. (Upper Right) 'a man is abseiling via using a rope'. (Lower Left) 'an otter stands on a surfboard surfing water'. (Lower Right) 'a person is cutting watermelon with a knife'.}
%     \label{fig: Spatiotemporal Homogenization ILA XCLIP}
% \end{figure*}


\section{Additional Experiments}
\label{sec: Supplementary Experiments}
\textbf{The effects of interpretive prompt.} In Tab. \ref{table: comparison text prompts}, we can observe that, directly use verbs and phrases achieve the lowest performance. When employing prefix and suffix prompt templates, the performance slightly improved. When introducing action decomposition interpretive prompts, the performance significantly improved. When adding synonym conversion interpretive prompts, the performance is further increased. Last, we involve body parts interpretive prompts that the performance reaches the best. 

\textbf{Which branches should be finetuned?} We separately freeze the parameters of the pretrained image and text encoder. From Tab. \ref{table: finetune}, we conclude the following observations: 1) Freezing both image and text encoders, only tuning KMT transformer achieves the worst performance. 2). Finetuning only one of them will improve the performance. 3). Finetuning them simultaneously achieves the best performance.

\textbf{The Evaluation performance of zero- and few-shot experiments on Kinetics-600, HMDB-51, and UCF-101 with CLAVER-B/16 (KMCT) configuration.} The comparison of KMT and KMCT as shown in Tab. \ref{table: comparison KMT and KMCT zero-shot}, \ref{table: comparison KMT and KMCT few-shot}. In the zero-shot scenario, the performance of KMCTA and KMTA has its own wins and losses, which is due to the lack of further fine-tuning, and the model's ability is relatively dependent on previous training. In the few-shot scenario, we can observe that in most cases, the performance of KMCTA is better, which further indicates that when the scale of training data is limited, the low-rank bottleneck is more significant.
\begin{table}[h!]
\vspace{-0.2cm}
    \begin{minipage}[t!]{0.5\textwidth}
        \centering
        \caption{Ablation on interpretive prompt.}
        \vspace{-0.2cm}
        \label{table: comparison text prompts}
 \resizebox{0.95\textwidth}{!}{   
        \begin{tabular}{ccc}
            \toprule
\textbf{Text Prompts}& \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)} \\
\hline
Noun and Phrase of Action & 77.4& 93.5\\
$+$ Prefix-Suffix & 77.4& 93.6\\
$+$ Action Decomposition Interpretive Prompt& 78.0& 94.0\\
$+$ Synonym Conversion Interpretive Prompt& 78.2& 94.2\\
$+$ Involving Body Parts Interpretive Prompt& \textbf{78.4}& \textbf{94.3}\\
\bottomrule
        \end{tabular}
}
    \end{minipage}
  \begin{minipage}[t!]{0.5\textwidth}
        \centering
        \caption{Which branches should be finetuned.}
        \vspace{-0.2cm}
       \label{table: finetune}
            \resizebox{0.98\textwidth}{!}{   
        \begin{tabular}{ccccc}
            \toprule
\textbf{KMT transformer}& \textbf{Visual}& \textbf{Text}& \textbf{Top-1 (\%)}& \textbf{Top-5 (\%)}\\
\hline
$\checkmark$& $\times$& $\times$& 78.7& 94.4\\
$\checkmark$& $\checkmark$& $\times$& 81.0& 95.1\\
$\checkmark$& $\times$& $\checkmark$& 80.8& 95.2\\
$\checkmark$& $\checkmark$& $\checkmark$& \textbf{81.5}& \textbf{95.5}\\
\bottomrule
        \end{tabular}
}
    \end{minipage}
\vspace{-0.3cm}
\end{table}


\begin{table}[h!]
\vspace{-0.2cm}
    \begin{minipage}[t!]{0.5\textwidth}
        \centering
        \caption{Comparison of KMT and KMCT on zero-shot settings.}
        \vspace{-0.2cm}
        \label{table: comparison KMT and KMCT zero-shot}
 \resizebox{0.95\textwidth}{!}{   
        \begin{tabular}{cccc}
            \toprule
\textbf{Zero-shot} & \textbf{Kinetics-600 (\%)} & \textbf{HMDB-51 (\%)} & \textbf{UCF-101 (\%)} \\
\hline
CLAVER-B/16 (KMT) & 73.8 $\pm$ 0.6 & \textbf{54.1} $\pm$ 2.4& \textbf{78.6} $\pm$ 1.7\\
CLAVER-B/16 (KMCT) & \textbf{74.1} $\pm$ 0.9& 54.0 $\pm$ 2.0 & 78.4 $\pm$ 2.1 \\
\bottomrule
        \end{tabular}
}
    \end{minipage}
  \begin{minipage}[t!]{0.5\textwidth}
        \centering
        \caption{Comparison of KMT and KMCT on few-shot settings.}
        \vspace{-0.2cm}
       \label{table: comparison KMT and KMCT few-shot}
            \resizebox{0.98\textwidth}{!}{   
        \begin{tabular}{ccccccccc}
            \toprule
\textbf{CLAVER-B/16} & \multicolumn{4}{c}{\textbf{KMT}} & \multicolumn{4}{c}{\textbf{KMCT}} \\
\hline
Few-shot & K=2 & K=4 & K=8 & K=16 & K=2 & K=4 & K=8 & K=16\\
HMDB-51 (\%) & \textbf{58.6} & 63.9 & 68.0 & 72.5 & 58.3 & \textbf{64.5} & \textbf{68.6} & \textbf{72.9} \\
UCF-101 (\%) & 89.7 & \textbf{92.9} & 96.1 & 98.0 & \textbf{90.0} & 92.9 & \textbf{96.6} & \textbf{98.1}\\
\bottomrule
        \end{tabular}
}
    \end{minipage}
\vspace{-0.3cm}
\end{table}

% \textbf{Performance on Something-Something-V2.} We evaluate CLAVER on Something-Something-V2 (SSv2) benchmark. We employ CLAVER-B/16\(_{8f}\) (KMT) as the backbone and introduce the video-specific prompting module (2 layers) from X-CLIP \cite{ni2022expanding}, since the SSv2 is a dataset with relatively small semantics. For SSv2, we uniformly sample the entire video at predefined temporal intervals without group division. We adopt the same hyperparameter settings as experiments on Kinetics-400 (Fully-supervised). As shown in Tab. \ref{table: SSv2}, CLAVER-B/16\(_{8f}\) outperforms ILA-B/16\(_{8f}\) \cite{tu2023implicit}, EVL-B/16\(_{16f}\) \cite{lin2022frozen}, and X-CLIP/16\(_{8f}\) \cite{ni2022expanding}. However, the performance of our method is lower than AIM \cite{yang2023aim}, but we require fewer FLOPs. 

% \textbf{Generalization of Kronecker mask to different backbones.} We experiment with a CLIP-based model (ActionCLIP-B/16\(_{8f}\)) as well as an ImageNet-based architecture (TimeSformer-ViT-B/16\(_{8f}\)) to demonstrate the generality of Kronecker mask temporal attention (KMTA). For ActionCLIP, we insert our Kronecker mask into the ActionCLIP while keep others unchanged. For TimeSformer, we replace the pipeline temporal attention with the our Kronecker mask temporal attention (KMTA). The results are depicted in Tab. \ref{table: Generalization ability of Kronecker mask}. The utilization of KMTA results in a performance gain for ActionCLIP and TimeSformer, respectively, demonstrating KMTA can be plugged into various transformer-based backbones, seamlessly.

% \begin{table}[h!]
% \vspace{-0.3cm}
%     \begin{minipage}[t!]{0.5\textwidth}
%         \centering
%         \caption{Performance comparison with CLIP-based methods on Something-Something-V2.}
%         \label{table: SSv2}
%  \resizebox{0.99\textwidth}{!}{   
%         \begin{tabular}{cccccc}
%             \toprule
% \textbf{Model}  & \textbf{Frames} & \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)} & \textbf{View} & \textbf{GFLOPs} \\
% \hline
% X-CLIP-B/16 \cite{ni2022expanding}& 8  & 57.8 & 84.5 & 4 $\times$ 3 & 145 \\
% AIM-ViT-B/16 \cite{yang2023aim}& 8  & 66.4 & 90.5 & 1 $\times$ 3 & 624 \\
% EVL-ViT-B/16 \cite{lin2022frozen}& 16  & 61.7 & - & 1 $\times$ 3 & 345 \\
% ILA-B/16 \cite{tu2023implicit}& 8  & 65.0 & 89.2 & 4 $\times$ 3 & 214 \\
% \hline
% CLAVER-B/16 & 8  & 65.4 & 90.1 & 4 $\times$ 3 & 204\\
% \bottomrule
%         \end{tabular}
% }
%     \end{minipage}
%   \begin{minipage}[t!]{0.5\textwidth}
%         \centering
%         \caption{Generalization ability of Kronecker mask temporal attention on various backbones for Kinetics-400.}
%         \vspace{-0.2cm}
%        \label{table: Generalization ability of Kronecker mask}
%             \resizebox{0.95\textwidth}{!}{   
%         \begin{tabular}{cccc}
%             \toprule
% \textbf{Model}& \textbf{Pretraining} & \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)}\\
% \hline
% ActionCLIP \cite{wang2021actionclip} & CLIP-400M & 81.1 & 95.5 \\
% ActionCLIP+KMTA& CLIP-400M & \textbf{83.0}& \textbf{95.9}\\
% \hline
% TimeSformer \cite{bertasius2021space} & IN-21k & 78.0 & 93.7 \\
% TimeSformer + KMTA & IN-21k & \textbf{80.2}& \textbf{94.9}\\
% \bottomrule
%         \end{tabular}
% }
%     \end{minipage}
% \vspace{-0.6cm}
% \end{table}

\section{Experimental Setting Details}
\label{sec: Experiment Setting Details}
\textbf{Architectures.} CLAVER-B/32 adopts ViT-B/32 (\(L_V\)=12, \(N_{h}\)=12, \(d\)=768, \(p\)=32) and is equipped with a KMT/KMCT transformer (\(L_K\)=2, \(N_{h}\)=12, \(d\)=768, \(p\)=32). CLAVER-B/16 employs ViT-B/16 (\(L_V\)=12, \(N_{h}\)=12, \(d\)=768, \(p\)=16), along with the KMT/KMCT transformer (\(L_K\)=2, \(N_{h}\)=12, \(d\)=768, \(p\)=32). CLAVER-L/14 is equipped with ViT-L/14 (\(L_V\)=24, \(N_{h}\)=16, \(d\)=1024, \(p\)=14) and the KMT/KMCT transformer (\(L_K\)=4, \(N_{h}\)=16, \(d\)=1024, \(p\)=14). Here \(L_V\) denotes the layers of ViT, \(L_K\) denotes the layers of Kronecker mask temporal transformer, \(N_h\) refers to the number of attention heads, \(d\) represents the embedding dimension and \(p\) is the patch size.

\noindent
\textbf{Hyperparameters.} The experiments are conducted on 8 NVIDIA 80G A100 GPUs. We present the training hyperparameters in Tab. \ref{table: hyperparameters setting}. Additionally, the learning rate for updating the KMT/KMCT transformer (randomly initialized) parameters is set 10$\times$ higher than the learning rate for parameters of the text encoder or image encoder. Because the text/image encoder already possesses a ability to extract high-quality text/image representations, conversely, KMT/KMCT transformer is trained from scratch. In the experiment, we freeze the parameters of the CLIP's image encoder to reduce certain computational costs, as it already has strong image feature extraction capabilities. And then we also conducted experiments on this aspect in subsequent ablation study.

\begin{table*}[h!]
\caption{The training hyperparameters settings of experiments.}
\label{table: hyperparameters setting}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{b{3.5cm}b{4cm}b{4cm}b{4cm}}
\hline
\multicolumn{1}{c}{\textbf{Config}} & \multicolumn{1}{c}{\textbf{Fully-sup}} & \multicolumn{1}{c}{\textbf{Few-shot}} & \multicolumn{1}{c}{\textbf{Zero-shot}} \\
\hline
\multicolumn{1}{c}{Optimizer} & \multicolumn{3}{c}{AdamW} \\ 

\multicolumn{1}{c}{Base learning rate} & \multicolumn{1}{c}{12e-6} & \multicolumn{1}{c}{2e-6} & \multicolumn{1}{c}{12e-6} \\

\multicolumn{1}{c}{Minimal learning rate} & \multicolumn{1}{c}{12e-8} & \multicolumn{1}{c}{2e-8} & \multicolumn{1}{c}{12e-8} \\

\multicolumn{1}{c}{Weight decay} & \multicolumn{3}{c}{0.001} \\

\multicolumn{1}{c}{Optimizer betas} & \multicolumn{3}{c}{$\beta_1$, $\beta_2$ =0.9, 0.98}\\

\multicolumn{1}{c}{Batch size} & \multicolumn{3}{c}{128 (ViT-B) 32 (ViT-L)} \\

\multicolumn{1}{c}{Learning rate schedule} & \multicolumn{3}{c}{Cosine decay} \\

\multicolumn{1}{c}{Warmup epochs} & \multicolumn{3}{c}{5} \\

\multicolumn{1}{c}{Training epochs} & \multicolumn{1}{c}{60 (ViT-B) 40 (ViT-L)} & \multicolumn{1}{c}{80 (20 on K400)} & \multicolumn{1}{c}{0 (20 on K400)} \\

\multicolumn{1}{c}{\multirow{2}{*}{Augmentation}} & \multicolumn{3}{c}{RandomFlip, MultiScaleCrop, ColorJitter} \\
 & \multicolumn{3}{c}{GrayScale, Label smoothing, Mixup, Cutmix} \\
 \hline
\end{tabular}
}
\end{table*}

% The learning rate for updating the text encoder or image encoder parameters is set 10 times lower than that for KMT/KMCT transformer, as the text encoder already possesses a ability to extract high-quality text representations that align with images, conversely, KMT/KMCT transformer is trained from scratch. 

\textbf{Fully-supervised experiments setting.} We conduct the fully-supervised experiments on Kinetics-400\&600. During training, a sparse sampling strategy is used. The number of frames is set to 8 or 16. We spatially scale the shorter side of each frame to 256 and take a 224 crop center crop. Following, we adopt the multi-view inference with 3 spatial crops and 4 temporal clips.

\textbf{Few-shot experiments setting.} We randomly sample 2, 4, 8 and 16 videos from each class on UCF-101 and HMDB-51 constructing the training set. For evaluation, we use the first split of the test set on UCF-101 and HMDB-51.

\textbf{Zero-shot experiments setting.} We train CLAVER-B/16 with 32 frames on Kinetics-400. The same as, we apply the following two evaluation protocols in zero-shot experiments. 1) Evaluation for HMDB-51 and UCF-101.  Following, the prediction is conducted on the three splits of the test data, and we report the average top-1 accuracy and standard deviation. 2) Evaluation for Kinetics-600. Following, the 220 new categories outside Kinetics-400 in Kinetics-600 are used for evaluation. The evaluation is conducted three times. For each iteration, we randomly sampled 160 categories for evaluation from the 220 categories in Kinetics-600.



%  \begin{minipage}[h]{0.5\textwidth}
%         \centering
%         \caption{Synonyms zero shot. Synonyms indicates synonym replacement.}
%         \label{table: synonyms zero shot}
%  \resizebox{0.72\textwidth}{!}{   
%         \begin{tabular}{ccc}
%             \toprule
% \small{\textbf{Dataset}}& \small{\textbf{Top-1 (\%)}}& \small{\textbf{Top-5 (\%)}} \\
% \hline
% \small{HMBD-51}& \small{48.8 $\pm$ 3.6}& \small{81.7 $\pm$ 1.4}\\
% \small{HMBD-51 Synonyms}& \small{40.4 $\pm$ 4.0}& \small{65.5 $\pm$ 2.4}\\
% \hline
% \small{UCF-101} & \small{74.6 $\pm$ 1.9}& \small{95.9 $\pm$ 2.3}\\
% \small{UCF-101 Synonyms}& \small{70.3 $\pm$ 2.7} & \small{94.3 $\pm$ 1.9}\\
% \hline
% \small{Kinetics-600}& \small{67.7 $\pm$ 0.5}& \small{88.2 $\pm$ 0.9}\\
% \small{Kinetics-600 Synonyms}& \small{64.6 $\pm$ 0.9}& \small{85.2 $\pm$ 1.1}\\
% \bottomrule
%         \end{tabular}
% }
%     \end{minipage}


%-----------------------------------------------------------------------------------------
% \section{Limitations}
% \label{sec: limitation discussion}
% The drawbacks of KMTA and KMCTA is that they have the same higher computational complexity as joint attention in self-attention calculation, \(\mathcal{O}(T^2L^2D)\), which is also a problem with Kronecker mask attention. Although several spatiotemporal attention forms can be derived by joint attention and tailored Kronecker mask, they do not use this form for practical calculations. Since their specific Kronecker masks allow them to be converted to forms with lower computational complexity, as shown in Fig. \ref{fig: Lower computation}. 
% \begin{figure*}[h!]
%     \centering
%     \vspace{-0.1cm}
%     \includegraphics[width=0.85\textwidth,height=0.14\textwidth]{Lower Computation.pdf}
%     \caption{Convert Kronecker mask attentions to lower computational complexity forms.}
%     \label{fig: Lower computation}
% \vspace{-0.3cm}
% \end{figure*}
% For example, the feature shape of spatial attention is \((T,L,D)\), the computation complexity is \(\mathcal{O}(TL^2D)\). The feature shape of pipeline temporal attention is \((L,T,D)\), the computation complexity is \(\mathcal{O}(LT^2D)\), and the feature shape of class-token-only temporal attention is \((T,D)\), the computation complexity is \(\mathcal{O}(T^2D)\). For solutions to reduce computational complexity, the survey \textit{Efficient transformers : A survey} \cite{tay2022efficient} summarizes many related methods. Some methods reduce quadratic complexity to lower complexity, some use sparse attention, and some use window attention to reduce sequence length. As illustrate in Tab. \ref{table: Lower computation complexity of CLAVER} We attempt some strategies to reduce the computational complexity, indicating that using some existing efficient transformer algorithms can significantly reduce FLOPs while still maintaining superior performance. 

% \begin{table*}[h!]
%         \caption{Lower computation complexity of CLAVER.}
%      \label{table: Lower computation complexity of CLAVER}
% \centering
% \resizebox{0.62\textwidth}{!}{
% \begin{tabular*}{0.99\linewidth}{ccccccccc}
% \cmidrule(r){1-9}
% \multirow{3}{*}{\textbf{Method}} &  \multicolumn{4}{c}{CLAVER-B/16$_{8f}$}& \multicolumn{4}{c}{CLAVER-B/14$_{8f}$} \\
% \cmidrule(r){2-9}
%  & \multicolumn{2}{c}{KMTA} & \multicolumn{2}{c}{KMCTA} & \multicolumn{2}{c}{KMTA} & \multicolumn{2}{c}{KMCTA}\\
%  \cmidrule(r){2-9}
%  & Top-1 (\%) & FLOPs (G) & Top-1 (\%) & FLOPs (G) & Top-1 (\%) & FLOPs (G) & Top-1 (\%) & FLOPs (G) \\
% \cmidrule(r){1-9}
% Reformer \cite{kitaev2020reformer} & 84.1 & 102 & 83.8 & 102 & 88.1 & 415 & 87.5 & 415\\
% Transformer-LS \cite{zhu2021long} & 84.3 & 85 & 84.0 & 85 & 88.0 & 330 & 88.1 & 330 \\
% \cmidrule(r){1-9}
% \end{tabular*}
% }
% \end{table*}

% \begin{table*}[h!]
%     \caption{Lower computation complexity of CLAVER.}
%     \label{table: Lower computation complexity of CLAVER}
%     \vspace{-0.3cm}
%     \centering
%     \resizebox{0.9\textwidth}{!}{
%         \begin{tabular}{ccccccccc}
%             \cmidrule(r){1-9}
%             \multirow{3}{*}{\textbf{Method}} &  \multicolumn{4}{c}{CLAVER-B/16$_{8f}$} & \multicolumn{4}{c}{CLAVER-B/14$_{8f}$} \\
%             \cmidrule(r){2-9}
%             & \multicolumn{2}{c}{KMTA} & \multicolumn{2}{c}{KMCTA} & \multicolumn{2}{c}{KMTA} & \multicolumn{2}{c}{KMCTA} \\
%             \cmidrule(r){2-9}
%             & Top-1 (\%) & FLOPs (G) & Top-1 (\%) & FLOPs (G) & Top-1 (\%) & FLOPs (G) & Top-1 (\%) & FLOPs (G) \\
%             \cmidrule(r){1-9}
%             Shifted Sliding Window & 83.2 & 146 & 83.5 & 146 & 87.3 & 502 & 87.3 & 502 \\
%             Linformer \cite{wang2020linformerselfattentionlinearcomplexity} & 83.4 & 139 & 83.3 & 139 & 87.5 & 494 & 87.5 & 494 \\
%             Reformer \cite{kitaev2020reformer} & 83.5 & 102 & 83.7 & 102 & 87.7 & 415 & 87.7 & 415 \\
%             Transformer-LS \cite{zhu2021long} & 84.1 & 97 & 84.3 & 97 & 87.6 & 330 & 87.7 & 330 \\
%             \cmidrule(r){1-9}
%         \end{tabular}
%     }
% \end{table*}

% Furthermore, considering that KMTA and KMCTA are derived from tailored Kronecker masks, a good strategy to reduce the computational complexity of KMTA and KMCTA is worth exploring in the future.


% \begin{figure*}[h!]
%     \centering
% \vspace{-0.1cm}
%     \includegraphics[width=0.95\textwidth,height=0.18\textwidth]{Graph.pdf}
%     \caption{Spatiotemporal topology attention.}
%     \label{fig: Spatiotemporal topology attention}
% \vspace{-0.5cm}
% \end{figure*}


% \section{Broader Impact}
% \label{sec: Broader Impact}
% KMCTA has potential to some extent which may be reflected in generation tasks (video generation). Due to the fact that causality is commonly required in the decoder-only language model for autoregressive modeling, and the spatiotemporal heterogeneity of KMCTA may be beneficial for the rationality of video generation. It can alleviate the low-rank bottleneck, simultaneously. Furthermore, Kronecker mask has certain generality, such as for spatiotemporal graph data, we can design topology structure by tailored Kronecker mask. Fig. \ref{fig: Spatiotemporal topology attention} shows an example.
%-----------------------------------------------------------------------------------------

% For a single frame, the attention matrix of KMTA and KMCTA degrades into a identity matrix, as all positions except for the diagonal are obscured, each KMT/KMCT transformer block is equivalent to a MLP. This property lead KMTA and KMCTA be compatible with images and videos, simultaneously, as images can be seen as a special case of video. 

%     \begin{table*}[t!]
%         \centering
%         \caption{Generalization ability of KMT on various visual backbones for Kinetics-400.}
%      \label{table: generalization ability of KMT on various visual backbones for Kinetics-400}
%             \resizebox{0.45\textwidth}{!}{   
%         \begin{tabular}{cccc}
%             \toprule
% \small{\textbf{Model}}& \small{\textbf{Pretrain}} & \small{\textbf{K400}}  & \small{\textbf{SSv2}}\\
% \hline
% \small{TimeSformer}& \small{IN-21K} & \small{78.0} & \small{59.5}\\
% \small{TimeSformer + KMT}  & \small{IN-21K}& \small{80.1} & \small{61.6}\\
% \hline
% \small{ActionCLIP} & \small{CLIP-400M}& \small{81.1} & \small{57.2}\\
% \small{ACtionCLIP + KMT} & \small{CLIP-400M}& \small{82.7} & \small{59.8}\\
% \bottomrule
%         \end{tabular}
% }
%     \end{table*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=0.98\textwidth,height=0.5\textwidth]{Visualization.png}
%     \caption{Testing on synthetic videos from Imagen (\textbf{Upper}) and Sora (\textbf{Lower}). The bar chart represents the Top-5 similarity texts, orange bar denotes the original prompt of the synthetic video.}
%     \label{fig: Generative model Imagen Sora  generate samples}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth,height=0.3\textwidth]{Description_detailed.pdf}
%     \caption{The prompt for LLaMA2: \textbf{Command} $+$ \textbf{Example} $+$ \{\textbf{Action}\}. We will get text description outputs of the \textbf{Action} similar to the \textbf{Example} (generated by ChatGPT).}
%     \label{fig: detailed language description prompt}
% \end{figure*}


\section{Interpretive Prompt Details}
\label{sec: Interpretive Prompt Details}
First, we input command prompts into the ChatGPT \cite{openai2024gpt4technicalreport} chatbot API to obtain a demonstration case of a small amount (in our experiment, we set it to 3 here) of detailed descriptions of actions.


\noindent
\textbf{Question to ChatGPT:} 
\begin{lstlisting}
Below, I will provide you with some action nouns. Please provide a simple 
and detailed description (explanation) about the action decomposition
of these action nouns. Please note that the sentence length of the 
description should not exceed 76 words. 
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Certainly! Please go ahead and provide the action nouns you'd like 
descriptions for, and I'll provide concise explanations within the 
specified sentence length.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
abseiling.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Abseiling combines several actions to descend a vertical surface with a 
rope. Climbers secure themselves with a harness and utilize a descender 
device for controlled descent. Simple actions, like maintaining a 
straight body position and regulating rope tension, form the basis. 
Abseiling demands proper training, safety measures, and is popular in 
adventure sports and rescue operations, allowing individuals to 
experience controlled descent in various settings.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
air drumming.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Air drumming is a rhythmic expression where individuals simulate playing 
drums without physical instruments. Simple actions, like mimicking 
drumming motions in the air, combine to create this imaginative and 
playful activity. Enthusiasts use their hands and feet to imitate 
drumming patterns, syncing with music. It's a spontaneous, enjoyable 
gesture often done during music listening or live performances, 
showcasing one's connection to the rhythm without the need for actual 
drums or drumsticks.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
answering questions.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Answering questions involves providing responses to queries posed by 
others. Simple actions like active listening, comprehension, and concise 
articulation combine in this communicative process. It is fundamental 
in various contexts, facilitating information exchange and problem-
solving. Respondents draw on their knowledge and expertise to address 
inquiries, contributing to effective communication and fostering 
understanding between individuals or groups.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
Below, I will provide you with some action nouns or phrases. Please 
provide many words and phrases that share the same central concept 
as these action nouns or phrases but have more diverse expressions. 
Please note that your reply should not exceed 76 words in length.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
1. Running: Jogging, sprinting, dashing, hurrying, racing, pounding the pavement, speeding, trotting.
2. Eating: Dining, feasting, devouring, munching, nibbling, gobbling up, indulging, savoring.
3. Writing: Scribbling, jotting down, composing, drafting, penning, inscribing, scrawling, scripting.
4. Laughing: Chuckling, giggling, guffawing, snickering, roaring with laughter, belly laughing, cackling, chortling.
5. Reading: perusing, browsing, scanning, skimming, devouring books, absorbing, immersing oneself, delving into texts.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
Cutting in the kitchen.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Slicing, dicing, chopping, mincing, cleaving, carving, trimming, 
preparing ingredients.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
driving car.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Operating a vehicle, maneuvering behind the wheel, navigating the road, 
piloting an automobile, steering, cruising, commuting by car, motoring.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
Walking With Dog.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Strolling with a canine companion, ambling with a pet, promenading with a 
pup, hiking with a furry friend, sauntering alongside a dog, wandering 
with a four-legged buddy, leash-walking, trotting with a pooch.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
Below, I will provide you with some action nouns. Please describe 
these actions based on their nouns and possible body parts involved. 
Please note that the sentence length of your response should not exceed 
76 words.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
1. Running: The rhythmic motion of legs propelling the body forward, feet striking the ground with each stride, muscles contracting and relaxing to generate momentum.
2. Swimming: Arms slicing through water, legs kicking to maintain propulsion, torso rotating to streamline movement, lungs rhythmically inhaling and exhaling for oxygen.
3. Writing: Fingers gripping a pen or pencil, hand moving across paper or keyboard, muscles in the forearm and wrist coordinating precise movements to form letters and words.
4. Jumping: Legs bending at the knees, feet pushing off the ground, arms swinging for momentum, body soaring through the air momentarily before landing.
5. Dancing: Feet tapping, legs extending and bending, hips swaying, arms gesturing and flowing in sync with music, body moving rhythmically with expression and grace.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
Cutting in the kitchen.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:}
\begin{lstlisting}
Using a sharp knife, fingers gripping the handle, hand guiding the blade 
through ingredients on a cutting board, wrist controlling the motion, 
fingers curling slightly to hold the food steady, precision applied 
to achieve desired shapes or sizes, ensuring safety and efficiency 
during food preparation.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:} 
\begin{lstlisting}
driving car.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:} 
\begin{lstlisting}
Gripping the steering wheel, hands adjusting position, fingers pressing 
pedals for acceleration and braking, eyes scanning surroundings for 
obstacles, feet coordinating between clutch, brake, and accelerator, 
body positioned comfortably in the driver's seat, mind focused on navigation 
and traffic signals, reacting swiftly to changing road conditions.
\end{lstlisting}

\noindent
\textbf{Question to ChatGPT:}
\begin{lstlisting}
Walking With Dog.
\end{lstlisting}

\noindent
\textbf{Answered by ChatGPT:} 
\begin{lstlisting}
Leash in hand, fingers securing grip, arm relaxed as it swings alongside 
the body, legs moving in tandem with the dog's pace, feet stepping 
forward with purpose, eyes attentive to the dog's behavior and surroundings, 
occasional stops for sniffing or marking, a bond of companionship 
evident in synchronized movement.
\end{lstlisting}

Subsequently, as illustrated in Fig \ref{fig: Interpretive Prompt}, we input the examples generated by ChatGPT \cite{openai2024gpt4technicalreport} into LLaMA-3 \cite{dubey2024llama3herdmodels} multiple times in the format of \textit{\textbf{Command}} \(+\) \textit{\textbf{Examples}} \(+\) \textit{\textbf{Action Concept}}, automatically generating diverse text descriptions about the \textit{\textbf{Action Concept}} through the program. When dealing with datasets containing a large number of categories, this scheme can greatly save labor costs. In addition, it should be noted that LLaMA-3 offers controllable parameters to control the randomness (\(\tau\)) and diversity (\(p\)) of output texts. We set \(\tau\) and \(p\) to 0.90 and 0.95, respectively to ensure that there are significant variations in output content for the same input, while maintaining consistency in the central concept.

% \noindent
% \textbf{Prefix and suffix prompt templates:}  \textit{'a video of \{category\}', 'a video of a person \{category\}.', 'an example of \{category\}.', 'an example of a person \{category\}.', 'a demonstration of \{category\}.', 'a demonstration of a person \{category\}.', 'Human action of \{category\}.', '\{category\}, an action.', '\{category\} this is an action.', '\{category\}, a video of action.', 'Playing action of \{category\}.', 'Playing a kind of action, \{category\}', 'Doing a kind of action, \{category\}', 'Look, the human is \{category\}', 'Video classification of \{category\}'.}


\section{Analysis of Synthetic Video Testing}
\label{sec:Analysis of Synthetic Video Testing}
We use the original prompts of synthetic videos (several action-related examples generated by Imagen \cite{saharia2022photorealistic} and Sora \cite{videoworldsimulators2024}) as their corresponding text descriptions, and show Top-5 most relevant texts in Fig. \ref{fig: Generative model Imagen Sora  generate samples}. The results shows the robustness and generalization of CLAVER.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.85\textwidth,height=0.36\textwidth]{Visualization.png}
    \vspace{-0.3cm}
    \caption{Testing on synthetic videos from Imagen (\textbf{Upper}) and Sora (\textbf{Lower}). The bar chart represents the Top-5 similarity texts, orange bar denotes the original prompts of synthetic videos.}
    \label{fig: Generative model Imagen Sora  generate samples}
\vspace{-0.7cm}
\end{figure*}

For cases generated by Imagen, in Fig. \ref{fig: Generative model Imagen Sora  generate samples} \textbf{Upper}, we denote the Kinetics-400 action text set as \(\mathcal{A}\). The action category "lift a cup" in Fig. \ref{fig: Generative model Imagen Sora  generate samples} (a) does not appear in \(\mathcal{A}\), while, (b) "washing dishes", (c) "riding horse", (d) ''shoveling snow" do appear in \(\mathcal{A}\). All videos do not come from the real-world, which are synthetic. We observe that all the original prompts appear in the Top-5 similarity (more precisely, Top-2). Specifically, for (a), "making tea" has the highest similarity, possibly due to the presence of cups and human hands in the scene. In addition, the original text has the second-highest similarity. In (b), apart from the original prompt in Top-1, other texts such as "washing dishes" convey the same meaning as the original prompts but lack a description of the subject ("teddy") of the action. "Washing hands" and "peeling potatoes" may involve actions and scenes like hand movements and sinks, thus exhibiting high similarity. For (c), although the one with the highest similarity is not "An astronaut riding a horse" , "riding or walking horses" is the exactly describes the action in the video, with the only difference being the lack of the subject of the action. For (d), "Shoveling snow" belongs to \(\mathcal{A}\), and since the original prompt of the video is "Shoveling snow", its similarity is very high, exceeding 90\%. 

For the cases generated by Sora, as depicted in Fig. \ref{fig: Generative model Imagen Sora  generate samples} \textbf{Lower}, (e) "disco dances" concept is not in \(\mathcal{A}\) but has the highest similarity. Besides, other action concepts in the Top-5 ("country line dancing","air druming", "dancing macarena","zumba") are all dance-related. (f) is a relatively challenging sample here. Although the ground truth description is located in the second, the "smoking" in Top-1 is entirely unrelated to the content of the video, because the original prompt of the video primarily describes the scene and the action-related text is relatively short. In addition, the shot is a process from far to near, which may lead the model to arrive at an unreasonable Top-1. In (g), the Top-2 descriptions are original prompt and "reading book", both strongly related to the action in video. For (h), the original prompt only rank third, however, Top-1 "surface water" and the second "water skimming" are descriptions strongly related to the action in video.


\section{Dataset Details}
\label{sec: Dataset Details}
\textbf{\textit{Kinetics-400\&600}.} The Kinetics dataset consists of 10-second video clips collected from YouTube. Specifically, Kinetics-400 \cite{kay2017kinetics} consists of approximately 240k training videos and 20k validation videos with 400 categories, while Kinetics-600 \cite{carreira2018short} is an extension of Kinetics-400, consisting of approximately 410k training videos and 29k validation videos with 600 categories.

\noindent
\textbf{\textit{UCF-101}.} UCF-101 \cite{soomro2012ucf101} consists of 101 action categories, over 13k clips and 27 hours of video data. The database comprises realistic user uploaded videos containing camera motion and cluttered backgrounds. The training and test data are divided into three splits.

\noindent
\textbf{\textit{HMDB-51}.} HMDB-51 \cite{kuehne2011hmdb} is a collection of realistic videos from various sources, including movies and web videos. It is composed of 6,766 video clips from 51 action categories, with each category containing at least 101 clips. The dataset is divided into three splits for training and test data. In each split, each action class has 70 clips for training and 30 clips for testing. 

% \noindent
% \textbf{\textit{Something-Something-V2}.} Something-Something-V2 \cite{materzynska2020something} is a collection of 220,847 labeled video clips of humans performing pre-defined, basic actions with everyday objects. The dataset consists of 174 action categories. For each video in the training and validation sets there is an object annotation in addition to the video label, if applicable. For example, for a label like "Putting [something] onto [something]," there is also an annotated version, such as "Putting a cup onto a table." In total, there are 318,572 annotations involving 30,408 unique objects.

\noindent
\textbf{\textit{Synthetic Videos}.} We select some action-related videos from the demo cases generated by Imagen \cite{saharia2022photorealistic} and Sora \cite{videoworldsimulators2024}, and employ the original prompts and their corresponding synthetic videos as a pair of test samples. 

% The detailed information about them is as follows:

% \noindent
% \textbf{Imagen (a)} \textbf{Prompt:} \textit{"A hand lifts a cup."} \textbf{Duration:} 5-second \textbf{Frames:} 128.

% \noindent
% \textbf{Imagen (b)} \textbf{Prompt:} \textit{"A teddy bear washing the dishes."} \textbf{Duration:} 5-second \textbf{Frames:} 128.

% \noindent
% \textbf{Imagen (c)} \textbf{Prompt:} \textit{"An astronaut riding a horse."} \textbf{Duration:} 5-second \textbf{Frames:} 128.

% \noindent
% \textbf{Imagen (d)} \textbf{Prompt:} \textit{"Shoveling snow."} \textbf{Duration:} 5-second \textbf{Frames:} 128.

% \noindent
% \textbf{Sora (a)} \textbf{Prompt:} \textit{"A cartoon kangaroo disco dances."} \textbf{Duration:} 10-second \textbf{Frames:} 300.

% \noindent
% \textbf{Sora (b)} \textbf{Prompt:} \textit{"A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about."} \textbf{Duration:} 1-minute \textbf{Frames:} 1799.

% \noindent
% \textbf{Sora (c)} \textbf{Prompt:} \textit{"A young man at his 20s is sitting on a piece of cloud in the sky, reading a book."} \textbf{Duration:} 20-second \textbf{Frames:} 600.

% \noindent
% \textbf{Sora (d)} \textbf{Prompt:} \textit{"An adorable happy otter confidently stands on a surfboard wearing a yellow lifejacket, riding along turquoise tropical waters near lush tropical islands, 3D digital render art style."} \textbf{Duration:} 17-second \textbf{Frames:} 512.


\section{More Attention Heat Map Visualization}
\label{sec: More Attention Heat Map Visualization}
We visualize several spatiotemporal attention map of samples from Kinetics400, Imagen, Sora.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.79\textwidth,height=1.4\textwidth]{More_Attention_Map_Imagen.jpg}
    \caption{Attention map of synthetic videos from Imagen.}
    \label{fig: Imagen Attention Map}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.79\textwidth,height=0.71\textwidth]{More_Attention_Map_Samples.png}
    \caption{Attention map of videos from Kinetics400.}
    \label{fig: Samples Attention Map}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.79\textwidth,height=0.71\textwidth]{More_Attention_Map_Sora.jpg}
    \caption{Attention map of synthetic videos from Sora.}
    \label{fig: Sora Attention Map}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=0.89\textwidth,height=0.27\textwidth]{Word_Importance_New.jpg}
\caption{More visualization examples of transition of CLIP's attention on nouns to CLAVER's preference on verbs.}
    \label{fig: Nouns to verbs}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.87\textwidth,height=0.9\textwidth]{Attention_Matrix.jpg}
    \caption{The rank of Kronecker temporal attention (KMT), Kronecker causal temporal attention (KMCT) and joint attention (Joint) on different patch size and frame length. (The calculation of rank is done via torch.linalg.svd)}
    \label{fig: Attention Matrix}
\end{figure*}


% \bibliographystyle{plain}
% \bibliography{ref}


