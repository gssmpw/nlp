\section{Related Work}
\label{sec:related work}
\textbf{Video Recognition.} Among early video recognition methods, 3D convolution is widely employed \cite{qiu2017learning,tran2015learning,tran2018closer,xie2018rethinking,feichtenhofer2019slowfast,feichtenhofer2020x3d,yu2021searching}. Some studies \cite{qiu2017learning,tran2018closer,xie2018rethinking} propose to factorize convolutional operations across spatial and temporal dimensions, while others design the specific temporal modules to embed them into 2D CNNs \cite{li2020tea,lin2019tsm,liu2021tam}. Over the past few years, there has been an influx of transformer-based video works \cite{arnab2021vivit,neimark2021video,bertasius2021space,fan2021multiscale,liu2022video,yan2022multiview,li2022uniformer}, demonstrating promising performance. For example, some methods \cite{arnab2021vivit,neimark2021video,girdhar2021anticipative} adopt a factorized encoder structure for spatial-temporal fusion. Alternatively, another family employ a factorized attention structure. Such as TimeSformer \cite{bertasius2021space}, ViViT \cite{arnab2021vivit}, and ATA \cite{zhao2022alignment} which proposes alignment-guided tepmoral attention. While Video Swin Transformer \cite{liu2022video} employs 3D window attention. The representative attention calculation forms in these works can be summarized into joint attention and factorized (divided) spatiotemporal attention. In this paper, to make minimal modifications to the original structure, we adopt a factorized encoder structure for video stream. 

\noindent
\textbf{Visual-language Representation Learning.} Visual-textual multi-modality is a hot topic in recent years. Several studies based on masked image modeling (MIM) have achieved commendable performance \cite{li2020unicoder,lu2019vilbert,su2019vl,tan2019lxmert}. There are also efforts focused on video-language representation learning \cite{miech2019howto100m,sun2019learning,sun2019videobert,zhu2020actbert,xu2021videoclip}. Concurrently, contrastive language-image pretraining \cite{radford2021learning,jia2021scaling,yuan2021florence} achieved remarkable progress, particularly in demonstrating impressive zero-shot generalization capacities. CLIP \cite{radford2021learning} is one of the most representative works, with numerous follow-up studies have explored to adapt it for downstream tasks. For example, object detection, semantic segmentation, video retrieval and captioning, etc.\cite{gu2021open,vinker2022clipasso,li2022grounded,luo2022clip4clip,xu2022groupvit}. Additionally, there are also many applications in the video action recognition \cite{wang2021actionclip,ni2022expanding,lin2022frozen,ju2022prompting,pan2022st,rasheed2023fine,chen2023video,tu2023implicit,lin2023match,momeni2023verbs,yang2023aim}. For instance, ViFiCLIP \cite{rasheed2023fine} aim to minimize modifications to original models and facilitate efficient transfer, while Chen Ju et al. \cite{ju2022prompting} suggest optimizing a few prompt vectors for adapting CLIP to various video understanding tasks. X-CLIP \cite{ni2022expanding} proposes an efficient cross-frame attention module. ILA \cite{tu2023implicit} designs implicit mask-based alignment to align features of two adjacent frames and EVL \cite{lin2022frozen} proposes a image encoder and video decoder structure. Regrading to the language branch, most previous works directly use verbs or phrases that lack rich semantics which overlook the importance of semantics. With the advancement of large language models like the GPT-3 \cite{brown2020language}, PaLM \cite{chowdhery2023palm}, LLaMAs \cite{touvron2023llama,touvron2023llama2,dubey2024llama3herdmodels} and ChatGPT \cite{openai2024gpt4technicalreport}. LLMs can replace manual labor \cite{chen2021elaborative,qian2022rethinking} and automatically generate texts that meet human expectations to benefit visual-textual learning. For example, LaCLIP \cite{fan2024improving} employs LLMs to rewrite text descriptions associated with each image for text augmentation. VFC \cite{momeni2023verbs} and MAXI \cite{lin2023match} leverage LLMs to generate positive and negative texts with diversity for language-video learning. 

% In line with this, we exploit LLMs to effectively generate sentence-level and semantically rich verb interpretations with diversity for prompting the understanding of actions.