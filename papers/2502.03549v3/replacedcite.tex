\section{Related Work}
\label{sec:related work}
\textbf{Video Recognition.} Among early video recognition methods, 3D convolution is widely employed ____. Some studies ____ propose to factorize convolutional operations across spatial and temporal dimensions, while others design the specific temporal modules to embed them into 2D CNNs ____. Over the past few years, there has been an influx of transformer-based video works ____, demonstrating promising performance. For example, some methods ____ adopt a factorized encoder structure for spatial-temporal fusion. Alternatively, another family employ a factorized attention structure. Such as TimeSformer ____, ViViT ____, and ATA ____ which proposes alignment-guided tepmoral attention. While Video Swin Transformer ____ employs 3D window attention. The representative attention calculation forms in these works can be summarized into joint attention and factorized (divided) spatiotemporal attention. In this paper, to make minimal modifications to the original structure, we adopt a factorized encoder structure for video stream. 

\noindent
\textbf{Visual-language Representation Learning.} Visual-textual multi-modality is a hot topic in recent years. Several studies based on masked image modeling (MIM) have achieved commendable performance ____. There are also efforts focused on video-language representation learning ____. Concurrently, contrastive language-image pretraining ____ achieved remarkable progress, particularly in demonstrating impressive zero-shot generalization capacities. CLIP ____ is one of the most representative works, with numerous follow-up studies have explored to adapt it for downstream tasks. For example, object detection, semantic segmentation, video retrieval and captioning, etc.____. Additionally, there are also many applications in the video action recognition ____. For instance, ViFiCLIP ____ aim to minimize modifications to original models and facilitate efficient transfer, while Chen Ju et al. ____ suggest optimizing a few prompt vectors for adapting CLIP to various video understanding tasks. X-CLIP ____ proposes an efficient cross-frame attention module. ILA ____ designs implicit mask-based alignment to align features of two adjacent frames and EVL ____ proposes a image encoder and video decoder structure. Regrading to the language branch, most previous works directly use verbs or phrases that lack rich semantics which overlook the importance of semantics. With the advancement of large language models like the GPT-3 ____, PaLM ____, LLaMAs ____ and ChatGPT ____. LLMs can replace manual labor ____ and automatically generate texts that meet human expectations to benefit visual-textual learning. For example, LaCLIP ____ employs LLMs to rewrite text descriptions associated with each image for text augmentation. VFC ____ and MAXI ____ leverage LLMs to generate positive and negative texts with diversity for language-video learning. 

% In line with this, we exploit LLMs to effectively generate sentence-level and semantically rich verb interpretations with diversity for prompting the understanding of actions.