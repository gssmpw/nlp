\section{Related works}
\label{sec:relatedwork}
The development of foundation models has revolutionised the field of artificial intelligence, enabling generalised learning across diverse tasks and domains____. Various paradigms for developing foundational models have been introduced, including generative approaches like Masked Autoencoders (MAE)____ and its extension to 3D point clouds in Point-MAE____, multi-scale masked autoencoders such as Point-M2AE____, and contrastive learning methods exemplified by SimCLR____.  JEPA____ is a novel training paradigm for learning effective abstract representations while addressing the shortcomings of other self-supervised models. The framework was initially proposed for images and has been adapted to the domains of videos____ and, recently, to point clouds____. 

The success of these large models in computer science has led to the development of similar models in fundamental sciences like biology____, chemistry____, astronomy____ and the modelling of dynamical systems in general____. In the domain of HEP, Masked Particle Modelling (MPM)____ was one of the first attempts to build such a model. It uses a self-supervised strategy to train on the JetClass dataset, drawing inspiration from language models. The MPM method models the jets as unordered sets of particles, each attributed with continuous features such as momenta, point of detection within the detector, etc. For a given jet, a subset of the particles are ``masked'' (i.e., their information is removed and replaced with a learnable vector or ``mask'') and fed to a transformer model. The model has to predict the masked part correctly using the information of the unmasked part to recreate the representation of the jet. Another language models-inspired method, OmniJet-$\alpha$____, uses a transformer-based model to generate tokenised jets from the JetClass dataset in an autoregressive manner similar to the GPT models____. The authors show that the model generalises well to a supervised classification task on the same set of jets, especially in few-shot learning, where the fine-tuned model outperforms the model architecture trained for the same task from scratch. Other people have also used contrastive learning techniques ____. The OmniLearn method____ relies on robust first-principle HEP simulations to train a FM in a supervised manner. The model has a classifier head and a generator head to cover most learning tasks, forming a task-specific block. This task-specific block uses a shared representation learned by the point-edge transformer model while training on a jet classification task on the \textsc{JetClass} dataset. The model matches the benchmark supervised jet classification model, Particle Transformer (ParT)____. The pre-trained model is evaluated across multiple learning tasks -- it trains quicker while performing comparably to the state-of-the-art on these tasks. 

Concurrent to our work, Ref.____ adapts the JEPA paradigm for the task of top tagging --- the authors pre-train the model on $1\%$ of the top jet and light jet samples from JetClass and evaluate downstream performance on the TQTR dataset. However, unlike our data-centric approach, the authors provide a physics-motivated context for the predictor -- a subset of the \emph{subjets} created from a jet sample from which it has to learn the target \emph{subjet} embeddings. Our model attains better accuracies for top tagging. Moreover, we also perform model evaluations for an additional downstream task of quark-gluon tagging, along with detailed ablations.

Building HEP-JEPA is the first attempt to thoroughly evaluate the adoption of the JEPA paradigm to the high-energy physics domain with different model-building choices.