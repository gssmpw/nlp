\section{Related Work}
Next to Byte-Pair Encoding \cite{sennrich-etal-2016-neural}, there exist several other subword (and other) tokenizers \cite{mielke2021between}. %
    For example, character and byte-based tokenizers have been argued to be more robust to spelling variations \cite{mielke2021between, libovicky-etal-2022-dont, xue-etal-2022-byt5}. In this study we focus on BPE as it has been the most common tokenization algorithm in recent LLMs (e.g., Llama 3, Mixtral, DeepSeek v3 and GPT4). 

    There is no agreed-upon standard to evaluate tokenizers. Tokenizers have been evaluated \textit{intrinsically} (i.e., without training LLMs) and \textit{extrinsically} (i.e., considering the performance of LLMs pre-trained with the considered tokenizer). Common intrinsic methods include: the average number of subwords produced per word and correlated measures like Corpus Token Count \cite{rust-etal-2021-good, Scao2022BLOOMA1, ali-etal-2024-tokenizer,galle-2019-investigating,schmidt-etal-2024-tokenization}, %
     information theoretic measures like Shannon Entropy or RÃ©nyi efficiency \cite{zouhar-etal-2023-tokenization} and
     morphological alignment \cite{gow-smith-etal-2022-improving, uzan-etal-2024-greed}. 
    Extrinsic measures include LLM perplexity \cite{shliazhko-etal-2024-mgpt,zevallos-bel-2023-hints,gowda-may-2020-finding}, cross-entropy loss \cite{rajaraman2024toward}, computational training cost \cite{ali-etal-2024-tokenizer} and downstream task performances \cite{schmidt-etal-2024-tokenization,ali-etal-2024-tokenizer}. 
    It is computationally infeasible to train SOTA LLMs end-to-end for each version of the tokenizer one wants to evaluate. Recent work tackled this by training ``smaller'' generative language models  with 350M--2.5B parameters for each tokenizer \cite{schmidt-etal-2024-tokenization,ali-etal-2024-tokenizer}. However, even such smaller models still can take several days to train. 
    We measure downstream task performance on models with 110M parameters taking less than 15 GPU hours to train per model. %

    Tokenizer algorithms and settings can affect a LLM's performance, for example,
    on tasks including numbers like arithmetic \cite{thawani-etal-2021-representing, wallace-etal-2019-nlp},
    on tasks including domain specific vocabulary or jargon like coding or medicine \cite{gu2021domain, dehaerne2022code, zan-etal-2023-large},
    on different scripts and languages \cite{petrov_unfairnesstokenizers,rust-etal-2021-good,limisiewicz-etal-2023-tokenization,ahia-etal-2023-languages,  velayuthan-sarveswaran-2025-egalitarian}
    and when translating between languages \cite{galle-2019-investigating,libovicky-etal-2022-dont,zhang-etal-2022-robust}.
    To the best of our knowledge monolingual tokenizers have been underinvestigated in relation to language variation. Monolingual tokenizers and tokenizer settings have recently been investigated on broader selections of NLU tasks \cite{schmidt-etal-2024-tokenization,ali-etal-2024-tokenizer}, presumably with the underlying assumption that for a given language like English, there exists a best tokenizer for most, if not all, tasks. We investigate this assumption for two systematically different types of tasks: tasks requiring robustness and tasks requiring sensitivity to language variation.