\section{Related Work}
Next to Byte-Pair Encoding ____, there exist several other subword (and other) tokenizers ____. %
    For example, character and byte-based tokenizers have been argued to be more robust to spelling variations ____. In this study we focus on BPE as it has been the most common tokenization algorithm in recent LLMs (e.g., Llama 3, Mixtral, DeepSeek v3 and GPT4). 

    There is no agreed-upon standard to evaluate tokenizers. Tokenizers have been evaluated \textit{intrinsically} (i.e., without training LLMs) and \textit{extrinsically} (i.e., considering the performance of LLMs pre-trained with the considered tokenizer). Common intrinsic methods include: the average number of subwords produced per word and correlated measures like Corpus Token Count ____, %
     information theoretic measures like Shannon Entropy or RÃ©nyi efficiency ____ and
     morphological alignment ____. 
    Extrinsic measures include LLM perplexity ____, cross-entropy loss ____, computational training cost ____ and downstream task performances ____. 
    It is computationally infeasible to train SOTA LLMs end-to-end for each version of the tokenizer one wants to evaluate. Recent work tackled this by training ``smaller'' generative language models  with 350M--2.5B parameters for each tokenizer ____. However, even such smaller models still can take several days to train. 
    We measure downstream task performance on models with 110M parameters taking less than 15 GPU hours to train per model. %

    Tokenizer algorithms and settings can affect a LLM's performance, for example,
    on tasks including numbers like arithmetic ____,
    on tasks including domain specific vocabulary or jargon like coding or medicine ____,
    on different scripts and languages ____
    and when translating between languages ____.
    To the best of our knowledge monolingual tokenizers have been underinvestigated in relation to language variation. Monolingual tokenizers and tokenizer settings have recently been investigated on broader selections of NLU tasks ____, presumably with the underlying assumption that for a given language like English, there exists a best tokenizer for most, if not all, tasks. We investigate this assumption for two systematically different types of tasks: tasks requiring robustness and tasks requiring sensitivity to language variation.