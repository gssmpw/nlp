\section{Experiments}\label{sec:experiments}
Investigation of \myMethod{} is carried out \eco{through} three experiments.
Experiment 1 \eco{compares} learning with a partially observed dynamical system when \myMethod{} is applied \eco{to} the \eco{scenario} in which a fully observed counterpart of the same dynamical system is available.
Robustness of \myMethod{} with respect to observational noise is studied in experiment 2.
Finally, experiment 3 addresses robustness of \myMethod{} with respect to sparsity and irregularity in the time series 
\pt{observations}.
%data.
All of the experiments are performed on synthetic data generated by four example systems of increasing complexity. 
These systems are introduced in Section~\ref{sec:models}.
Details about experiments 1, 2 and 3 are given in Section~\ref{sec:accuracy}.
All of the experiments are implemented in MATLAB and are publicly available on Github\footnote{\url{https://github.com/janis-norden/Structural_Identifiability_Mapping}}.

%--------------------------------------------------------------------------------------------------
\subsection{Example Models}\label{sec:models}
%--------------------------------------------------------------------------------------------------

\subsubsection{\kb{Toy} Model} 
The toy model allows for intuitive visualization of the  \myMethod{} due to its $2$-dimensional parameter space (see \autoref{fig:toy_model_decision_boundary}).
The model equations are given by
\begin{align} \label{eq:TM}
    \begin{split}
        \dot{x}(t) &= -ab x(t), \\
        y(t)    &= x(t), 
    \end{split}
\end{align}
where $x \in \mathbb{R}$ is the state variable with $x(0)=1$ known, $t \in [0, 1]$, and $a,b \in \mathbb{R}^{+}$ are the system parameters. 
The parameters are further restricted to lie within the region of interest \eco{(ROI)} $R = [0.1, 3] \times [0.1, 3]$.
From Eq.~\eqref{eq:TM} it can be seen that any parameter configuration $a$ and $b$ such that $\Phi = ab$ is constant, will produce identical system output for a given value of $\Phi$. 
%This ``model" could easily be reparametrised in terms of $\Phi$.

\subsubsection{Catenary compartmental Model (CCM)}
\eco{C}ompartmental models are commonly used in modelling pharmacokinetic interactions \cite{jacquez1972, metzler1971, sager2015}. 
The $n$-compartment catenary model (CMM$n$), is a linear model of $n$ compartments which are connected to one another in a \pt{bi-directional chain}.
%series-like manner. 
Only the first compartment is assumed to have an input, \eco{whereas} all compartments are assumed to have leakage (see \autoref{fig:CCM}). 
Substance $x_{i}$ is converted to $x_{i+1}$ and vice versa. 
The model has a total of $3n - 2$ parameters comprising $2(n-1)$ conversion rates and $n$ leakages. 
The coefficients $k_{i,i-1} \geq 0$ and $k_{i-1,i} \geq 0$ describe the conversion rates between $x_{i}$ and $x_{i-1}$, while the coefficients $k_{0i} \geq 0$ govern the leakage. 
\begin{figure}[t!]
    \centering
    \includegraphics[width = \linewidth]{Systems/CCM.pdf}
    \caption{Catenary $n$-compartmental model.}
    \label{fig:CCM}
\end{figure}
The concentration of interacting substances $x_{1},\ldots, x_{n}$ is described by the set of linear ODEs:
\begin{align}\label{eq:CM_dynamics}
    \begin{split}
        \dot{\mathbf{x}}(t) &= K\mathbf{x}(t) + \mathbf{b}u(t)\\
        y(t) &= x_{1}(t),
    \end{split}
\end{align}
where $\mathbf{b} = [1,0,\ldots,0]^{\top}$, $\mathbf{x} = [x_{1},\ldots,x_{n}]^{\top}$ with $\mathbf{x}(0) = \mathbf{x}_{0}$, and $y$ is the system output.
 The matrix $K$ is then given by
\begin{equation}
K = \begin{bmatrix}
        k_{11} & k_{12} & 0 & 0 & \ldots & 0 \\
        k_{21} & k_{22} & k_{23} & 0 & \ldots & 0 \\
        0 & k_{32} & k_{33} & k_{34} & \ldots & 0 \\
        0 & 0 & k_{43} & k_{44} & \ldots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 0 & k_{n,n-1} & k_{n,n}
    \end{bmatrix},
\end{equation}
with 
\begin{equation}
    k_{ii} =
    \begin{cases}
        -k_{01} - k_{21}, & \text{for} \quad i = 1, \\
        -k_{0i} - k_{i+1,i} - k_{i-1,i}  & \text{for} \quad i = 2,3,\ldots, n-1, \\
        -k_{0n} - k_{n-1,n} & \text{for} \quad i = n. \\
    \end{cases}
\end{equation}
Employing the Laplace transform Output Equality approach, Chen~et~al.~\cite{CHEN198559} demonstrated that $2n-1$ structurally identifiable parameter combinations can be found for the \eco{CCM}, namely
\begin{align}
    \Phi^1_{i} &= k_{ii}, &\text{for} \quad i = 1,2,\ldots, n, \\
    \Phi^2_{j} &= k_{j,j-1} k_{j-1,j}, &\text{for} \quad j = 2,3,\ldots, n.
\end{align}
Using AutoRepar, we were able to reparametrise the CCM2 model to a FISPO model.
The same is not true for the CCM4 model (see Appendix~\ref{app:repar} for details).

% Description of CCM2 prednisone model used in Kerstin's paper
For experimentation, we consider the work by Bunte~et~al.~\cite{bunte2018learning} \eco{in which} the data from a clinical study concerning the interaction between the metabolites prednisone and prednisolone \eco{is analysed}. 
The authors \pt{employed} a 3-compartment model \pt{and used a probabilistic mixture of such models} to analyse the data of 12 patients and found that the patients could be stratified into 4 groups.
Their \pt{3-compartment model} is equivalent to a CCM \eco{with 2 compartments} with non-zero input and is therefore suitable for the application of a \myMethod{}.
The input in this case is $u(t) = S_{0} k_{\text{abs}} e^{-k_{\text{abs}} t}$, where $S_{0}$ is a fixed amount of prednisone formulation that is ingested and absorbed with rate $k_{\text{abs}}$ into the bloodstream.
The time interval of interest is $t \in [0, 240]$ seconds and the ROI
is $R = [0, 0.1]^{4}$.

To set up a suitable binary classification problem, we use the parametrisation of one of the clusters (C4) found in \cite{bunte2018learning} to represent one \eco{of the} class\eco{es}.
The other class is characterized by the same parametrisation, where the conversion rates $k_{12}$ and $k_{21}$ are $20 \%$ deficient (see \autoref{tab:gt_parameters}).
This model will be referred to as CCM2 and is of particular interest, since it represents a minimal realistic compartmental model for which structurally unidentifiable parameters occur.

% Description of modified variant CCM4 
We further consider a 4-compartment variant of this model by adding two additional compartments in accordance with the \eco{model schematic} in \autoref{fig:CCM}.
For \eco{the first} class, the excretion and conversion are \eco{the same as} those used for the CCM2.
The second class is characterized by the same parametrisation but now six \eco{out of seven} conversion rates are set to be $50 \%$ deficient.
The \eco{studied} time interval is the same as for CCM2 and the ROI is $R = [0, 0.1]^{10}$.

\subsubsection{\eco{C}ompartmental Model with a Loop (CML)}
To further demonstrate the applicability of \myMethod{} to models which cannot be meaningfully reparametrised in a straightforward manner, the Compartmental Model with a Loop (CML) is considered (see \autoref{fig:CML}). 
\begin{figure}[t!]
    \centering
    \includegraphics[width = 0.6\linewidth]{Systems/CML.pdf}
    \caption{Compartment Model with a Loop (CML).}
    \label{fig:CML}
\end{figure}
Similar to the CCM, the CML is a linear compartment model and dynamics Eq.~\eqref{eq:CM_dynamics} apply with coefficient matrix
\begin{equation}
    K = 
    \begin{bmatrix}
        k_{11} & k_{12} &        0 & 0      \\
        k_{21} & k_{22} &   k_{23} & 0      \\
            0  &      0 &   k_{33} & k_{34} \\
            0  & k_{42} &   k_{43} & k_{44}
    \end{bmatrix},
\end{equation}
where 
\begin{align}
    \begin{split}
        k_{11} &= -(k_{01} + k_{21}), \\
        k_{22} &= -(k_{02} + k_{12} + k_{42}), \\
        k_{33} &= -(k_{03} + k_{23} + k_{43}), \\
        k_{44} &= -(k_{04} + k_{34}).
    \end{split}
\end{align}
and the ROI is given as $R = [0, 0.1]^{10}$. 

Employing the Laplace transform approach, it can be shown that the system has 7 structurally identifiable parameter combinations $\Phi_{1},\ldots, \Phi_{7}$. 
The relations are
\begin{align} \label{eq:CML_SI}
    \begin{split}
        \Phi_{1} &=  k_{12} k_{21}, \\
        \Phi_{3} &=  k_{01} + k_{21}, \\
        \Phi_{4} &=  k_{02} + k_{12} + k_{42}, \\
    \end{split}
    \begin{split}
        \Phi_{2} &=  k_{34} k_{43}, \\
        \Phi_{6} &=  k_{04} + k_{34}\\
        \Phi_{5} &=  k_{03} + k_{23} + k_{43},
    \end{split} \\ 
    &\Phi_{7} =  k_{23} k_{42} k_{34}. \notag
\end{align}
Meshkat~\&~Sullivant~\cite{MESHKAT201446} demonstrate that for this system (Example 6.3 in their work) \kb{no} scaling transformations \kb{exist} which make the resulting reparametrised system identifiable.
We tried AutoRepar with an univariate Ansatz polynomial of degree 2 but could not find any transformations that would make the reparametrised model FISPO. 
Yet, using the same Ansatz polynomial, we were able to find the relations given in Eq.~\eqref{eq:CML_SI} by only looking at parameter identifiability.
Since the CML could not easily be reparametrised, the model is particularly interesting as a test case for \myMethod{}. 

\subsubsection{Batch reactor (BR)}
A classical model \eco{defined} to study microbial growth in a batch reactor which incorporates a Michaelis-Menten type nonlinearity is the following:
\begin{align}
    \begin{split}
        \dot{x}(t) &= \frac{\mu_{m}s(t) x(t) }{K_{s} + s(t)} - K_{d} x(t), \\
        \dot{s}(t) &= -\frac{\mu_{m} s(t) x(t)}{Y(K_{s} + s(t))}, \\
        y(t) &= x(t),
    \end{split} \label{eq:BR_model}
\end{align}
where $x$ is the concentration of microorganisms, $s$ the concentration of growth-limiting substrate, $\mu_{m}$ the maximum reaction velocity, $K_{s}$ the Michaelis-Menten constant, $Y$ the yield coefficient, and $K_{d}$ the decay rate coefficient (see e.g. \cite{button1985kinetics}).
For the present work, the time interval of interest is $t \in [0, 12]$ hours and the ROI is:
\begin{equation}
    R = [0, 10] \times [0, 50] \times [0, 1] \times[0, 5] \times [0, 1] \times [0, 1], 
\end{equation}
where the intervals refer to the allowed ranges of $b_{1}, b_{2}, \mu_{m}, K_{s}, Y$ and $K_{d}$, respectively.

It is assumed that microorganisms $x$ and substrate $s$ are prepared in mixtures for which the concentration can be controlled. 
When the mixtures are put together in the batch reactor, it is assumed that the reaction is very fast, so that the model may be regarded as having impulsive inputs $b_{1}\delta(t)$ for $x$ and $b_{2}\delta(t)$ for $s$.
Equivalently, system Eq.~\eqref{eq:BR_model} may be considered with initial conditions $x(0)=b_{1}$ and $s(0)=b_{2}$.
As demonstrated in \cite{holmberg1982practical}, if both $x$ and $s$ are observed at time $t=0$, then the model is globally structurally identifiable.
However, in \cite{chappell1992structural, evans2000extensions} it was demonstrated that when only $x$ is observed, the model becomes structurally unidentifiable.
In this case, the following combinations of parameters have been found to be structurally identifiable
\begin{align}
    \begin{split}
        \Phi_{1} =  b_{1}, \quad \Phi_{2} =  \mu_{m}, \quad \Phi_{3} =  K_{d}, \\
        \Phi_{4} =  b_{2} Y, \quad \Phi_{5} =  \frac{b_{2}}{K_{s}}.
    \end{split}
\end{align}
Realistic configurations of model parameters have been taken from \cite{holmberg1982practical} (cf. Figure 1 in their work).
As a classification task, we consider a scenario in which two reactions are compared that primarily differ in their yield coefficient $Y$.
Class 0 is characterized by a distribution of yield coefficients centred around $Y=0.6$ while class 1 is associated with a distribution that centres around a $20\%$ diminished yield coefficient, i.e., $Y=0.48$.
\autoref{fig:BR_example_ts} illustrates the classification task in the space of time series.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/BR_example_ts.pdf}
    \caption{Binary classification task for time series from the batch reactor model. 
    Displayed are 10 time series per class.
    Observational noise simulated is normally distributed with standard deviation $\sigma = 3$.
    }
    \label{fig:BR_example_ts}
\end{figure}

%------------------------------------------------------------------------------
\subsection{Experimental setup}\label{sec:accuracy}
%------------------------------------------------------------------------------
In order to test the effectiveness of \myMethod{} approach, a binary classification is implemented based on the Support Vector Machine (SVM) framework. 
For each example system, synthetic time series data corresponding to a binary classification task \mjc{are} created and the SVM classifier is trained using the discussed model-based framework. 
The performance of the resulting classifier is assessed by its generalization error. 
Additionally, the number of support vectors is considered as an indication of the classifier-complexity needed to distinguish the two classes. 
Since the example systems differ in the dimensionality of their parameter spaces, their training and test sets contain differing numbers of training examples $N_{\text{train}}$ and test examples $N_{\text{test}}$.
\eco{For each system}, $N_{\text{train}}$ and $N_{\text{test}}$ are chosen to be sufficiently large as not to be the limiting factor in the assessment of classification performance.

\subsubsection{Experiment 1}
This experiment compares classification performance for three situations: training with the fully observed (FO) dynamical model, training with the partially observed (PO) dynamical model, and training with the partially observed dynamical model together with a \myMethod{} (PO + \myMethod{}).
The synthetic data $\mathbb{D} = \{ (\mathcal{Y}^{k},c^{k}): \quad k = 1,\ldots,N \}$ used for this experiment \mjc{are} generated as follows. 

The ground truth class-conditional distributions associated with classes $c=0$ and $c=1$ are chosen as multivariate normal distributions with known means and covariance matrices: %, i.e.,
\begin{equation}
    p(\boldsymbol{\theta} \mid c_{i}) = \mathcal{N}(\boldsymbol{\theta}, \boldsymbol{\mu}_i, \Sigma_{i}), \quad i \in\{0, 1\}.%= 0, 1.
\end{equation}
The values of $\boldsymbol{\mu}_{i}$ and $\Sigma_{i}$ used for experimentation are specific to the dynamical model under consideration and are reported in Table~\ref{tab:gt_parameters}.

% Wide format only classification task ground truth
\begin{table*}[t]
    \centering
    \caption{Ground truth parameter configurations for binary classification tasks of the different models.}
    \begin{tabular}{p{1.2cm}p{0.8cm}p{0.8cm}p{4cm}p{4cm}p{4.5cm}}
        \hline
        System & $N_{\text{train}}$ & $N_{\text{test}}$ & $\boldsymbol{\mu}_{0}$ & $\boldsymbol{\mu}_{1}$ & $\Sigma_{0}, \Sigma_{1}$  \\
        \hline
        toy model & 100 & 200 & $(a,b) = (1,1)$ & $(a,b) = 0.9 \cdot (1,1)$ & $10^{-4} I_{2}$ \\
        CCM2 & 100 & 200 & $(k_{01}, k_{02}, k_{12}, k_{21})$ & $(k_{01}, k_{02}, k_{12}, k_{21})$ & $10^{-7} I_{4}$ \\
             &     &     & $ = (0.015,0.015,0.074,0.01)$      & $= (0.015,0.015,0.059,0.008)$      &  \\
        CCM4 & 800 & 1000 & $k_{0i} = 0.015$,                                    & $k_{0i} = 0.015$,  & $10^{-7} I_{10}$ \\
            &     &      & $(k_{12}, k_{23}, k_{34}, k_{21}, k_{32}, k_{43})=$  & $(k_{12}, k_{23}, k_{34}, k_{21}, k_{32}, k_{43})=$                   & \\
            &     &      & $10^{-2}(7.4,1,7.4,1,7.4,1)$ & $10^{-2}(3.7,0.5,3.7,0.5,3.7,0.5)$                   &  \\
        CML & 800 & 1000 & identical to CCM4 & identical to CCM4 & $10^{-7} I_{10}$ \\
        BR & 200 & 400 & $(b_{1}, b_{2}, \mu_{m}, K_{s}, Y, K_{d})$ & $(b_{1}, b_{2}, \mu_{m}, K_{s}, Y, K_{d})$ & $\text{diag}(10^{-2}v),$  \\
           &     &     & $= (1.25,30,0.5,3,0.6,0.05)$               & $= (1.25,30,0.5,3,0.48,0.05)$              & $v=(1, 100, 10^{-2}, 1, 10^{-2}, 10^{-4})$ \\
        \hline
    \end{tabular}
    \label{tab:gt_parameters}
\end{table*}

An equal number of example pairs $(\mathcal{Y}^{k},c^{k})$ is generated for each class by \eco{first} drawing $\boldsymbol{\theta}$ from the associated class-conditional distribution $p(\boldsymbol{\theta} \mid c^{k})$ and \eco{subsequently} integrating the dynamical system Eq.~\eqref{eq:dyn_sys} with the drawn $\boldsymbol{\theta}$ on the time interval $t \in [0, t_{\text{end}}]$.
\eco{To obtain $\mathcal{Y}$}, time points are sampled from $[0, t_{\text{end}}]$ and the trajectory of the dynamical system is evaluated at these time points.
This leads to a classification problem with balanced classes.
For the fully observed dynamical model, the system output mapping is assumed to be the identity, i.e., $h(\mathbf{x}) = \mathbf{x}$, and hence data for each state variable are generated.
For the partially observed dynamical model, the system output mapping is set to be the projection onto the first state variable $h(\mathbf{x}) = x_{1}$.

For experiment 1, \pt{ a
%the 
regular time grid} on which the data are generated is chosen to densely cover the time interval of interest\eco{: $t_{\text{dense}}$}.
The collection of observations $\mathbf{Y}^k$ is then obtained by evaluating the resulting trajectory $\mathbf{x}(t;\boldsymbol{\theta})$ on the time grid and adding observational noise, which is assumed to be Gaussian, i.e.,
 \begin{equation}\label{eq:output_scalar}
    \mathbf{y}_{i}^{k} = \mathbf{x}(t_{i}^{k}) + \boldsymbol{\epsilon},
\end{equation}
where $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{R})$ and $\mathbf{R}$ is known.
The observable output $\mathbf{y}_{i}^{k}$ of the FO model has a different dimension than the one of the PO model.
Therefore, we distinguish between $\mathbf{R}=\mathbf{R}_{FO}$ and $\mathbf{R}=\mathbf{R}_{PO}$.
Details about \eco{what} time grid is used for \eco{each} model, as well as the matrices $\mathbf{R}$, are reported in Table~\ref{tab:exp_conditions}.

\eco{Once} the set of labelled time series data $\mathbb{D}$ \eco{is obtained}, this set needs to \pt{transformed} be into a set of labelled Maximum \emph{A Posteriori} \pt{inferential model parameter estimates} 
{$\mathbb{D}_{\text{MAP}} = \pt{\{ \boldsymbol{(\theta}^k_{\text{MAP}}, c^{k}) \}_k}$.
\pt{Note that since we have chosen a flat prior over $R$, $\boldsymbol{\theta}^k_{\text{MAP}}$ will be maximum likelihood estimates constrained to $R$:}
\begin{equation}\label{eq:argmax}
    \boldsymbol{\theta}_{\text{MAP}}^{k} = \argmax_{\boldsymbol{\theta} \in R} \Big\lbrace \log(p(\mathbf{Y}^{k} \mid \boldsymbol{\theta}, \mathbf{t}^{k}; \mathbf{R})) \Big\rbrace.
\end{equation}
\eco{We remark that} the argmax operation does not determine $\boldsymbol{\theta}_{\text{MAP}}^{k}$ uniquely, due to structural unidentifiability \eco{of the system}. 
The problem given in Eq.~\eqref{eq:argmax} is solved using MATLAB's {\tt simulannealbnd} function which can be used for constrained optimization. 
The outcome of the data \pt{transformation} process is \pt{the set $\mathbb{D}_{\text{MAP}}$ and its \myMethod{} counterpart $\mathbb{D}_{\text{\myMethod{}}} = \{ (\boldsymbol{\Phi}^{k}, c^{k}) \}_k$,} where each $\boldsymbol{\Phi}^{k}$ is obtained as described in Eq.~\eqref{eq:SIM}.
For the fully observed dynamical model, only $\mathbb{D}_{\text{MAP}}$ is generated, whereas for the partially observed (and thus unidentifiable) model, both $\mathbb{D}_{\text{MAP}}$ and  $\mathbb{D}_{\text{\myMethod{}}}$ are produced.

\pt{Once the sets 
$\mathbb{D}_{\text{MAP}}$ and $\mathbb{D}_{\text{\myMethod{}}}$ have been created}, Support Vector Machine (SVM) classifiers are trained to learn the binary classification rule. 
\pt{A separate hold-out test data set (never used for training) is employed to assess the generalisation performance.}
The number of training and test examples produced is also reported in Table~\ref{tab:gt_parameters}.
As \eco{an} increasing \eco{number} of training \eco{examples} are made available, the generalization error and the relative number of support vectors are %determined 
\pt{recorded} as a function of the number of training examples per class.
For each number of available training examples per class, the classifier is trained on 20 randomly sub-sampled datasets, and the mean and standard deviation of the generalization error 
and relative number of support vectors are reported as a function of the number of training examples.
Training the classifier for 20 independent trials permits the capture of the variability in classification performance for a given number of training examples while keeping the runtime of the experiments feasibly low.

% The SVM is implemented by means of 
\kb{For SVM training we use} MATLAB's {\tt fitcsvm} function with a Gaussian Kernel.
The kernel scale and the hyper-parameter governing the penalization of misclassification ({\tt BoxConstraint} in MATLAB) are selected by means of $10$-fold cross-validation for each round of classifier training.

\subsubsection{Experiment 2}
This experiment is designed to study the robustness of \myMethod{} with respect to observational noise.
For this purpose only the PO model and the PO model + \myMethod{} are compared.
The overall setup is identical to that of experiment $1$ with a few key differences.
\eco{Firstly, t}he number of training examples per class made available is kept fixed. 
Instead, the amount of observational noise is varied.
This is done by setting $\mathbf{R} = \sigma^2 I$, where $\sigma$ varies in a range that is meaningful to the problem at hand.
The ranges used for experimentation are reported in \autoref{tab:exp_conditions}.
Changes in the observational noise are applied to both training and test data.
For each value of $\sigma$, the classifier is then evaluated on 10 randomly sub-sampled datasets, similar to experiment 1.
Again, the mean and standard deviation of generalization error and relative number of support vectors are reported.
Additionally, the three quantities $\Delta \epsilon^{\ast}$, $\sigma^{\ast}$ and $\langle \Delta \epsilon \rangle$ are computed for each example model:
$\Delta \epsilon^{\ast}$ is the maximum difference in mean generalization error and $\sigma^{\ast}$ is the noise level at which it occurs.
Further, $\langle \Delta \epsilon \rangle$ is the average of the difference between the generalization error curves obtained for the PO model and the PO model + \myMethod{}.

\subsubsection{Experiment 3}
This experiment is designed to study the robustness of \myMethod{} with respect to sparsity and irregularity \eco{of} the time series data.
Again, the overall setup is identical to that of experiment 1.
In contrast to \eco{experiment} 2, the observational noise is fixed.
Time series are generated on three different types of time grids: 
A dense grid $t_{\text{dense}}$ which corresponds to frequent and regular measurements a sparse grid $t_{\text{sparse}}$, which is regular like $t_{\text{dense}}$ but only contains $40 \%$ of the points.
\eco{I}rregular grids $t^{k}_{\text{irr}}$ \kb{contain} %with 
sparse and irregular measurements, \eco{which} are different for every observation \kb{$k$}.
As per the sparse grids, the irregular time grids contain $40 \%$ of the number of points in $t_{\text{dense}}$.
Unlike the sparse grids, points are sampled uniformly at random between the first and last time points in $t_{\text{dense}}$.
Notably all time grids contain $t=0$.
The choices for $t_{\text{dense}}$ for the different example models are reported in Table~\ref{tab:gt_parameters}. 
The configurations of $t_{\text{sparse}}$ and $t^{k}_{\text{irr}}$ follow from the choices of $t_{\text{dense}}$.
Finally, for experiment $3$, \eco{the} mean and standard deviation of the generalization error are reported for the three differ\eco{ent types of} time grids.

\begin{table}[t]
    \centering
    \caption{Experimental configurations for parameters related to time grids and observational noise.}
    \begin{tabular}{ccccc}
        \hline
        System & $t_{\text{dense}}$ & $\mathbf{R}_{FO}$ & $\mathbf{R}_{PO}$ & $\sigma$ range \\
        \hline
        toy model & $ 0,0.1,\ldots, 1$                      & 0.01  & - & $0.01, 0.05, \ldots, 0.3$ \\
        CCM2      & $0, 10,\ldots, 240$                     & $10^2 \mathbf{I}_{2}$  & $10^2$ & $10, 20,\ldots, 60$ \\
        CCM4      & $0, 10,\ldots, 240$                     & $10^2 \mathbf{I}_{4}$  & $10^2$ & $10, 20,\ldots, 60$ \\
         CML      & $0, 10,\ldots, 240$                     & $10^2 \mathbf{I}_{4}$  & $10^2$ & $10, 20,\ldots, 60$ \\
         BR       & $0, 1, \ldots, 12$                      & $\mathbf{I}_{2}$       & $1$    & $0.1, 1, 2 \ldots, 5$ \\    
         \hline
    \end{tabular}
    \label{tab:exp_conditions}
\end{table}