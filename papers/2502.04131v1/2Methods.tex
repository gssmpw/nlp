\section{Methods}\label{sec:methods}

% What is in this section?
In this section, we present a model-based approach for time series classification based on the incorporation of a given dynamical model in the form of \mjc{a set of parametrised Ordinary Differential Equations} (ODEs). 
To do so, we adopt a formalism in which individual time series observations are represented as Maximum A Posteriori (MAP) estimates.
In addition, we present the details of %our new method 
\kb{the proposed strategy, namely} a \textbf{S}tructural-\textbf{I}dentifiability \textbf{M}apping (\myMethod{}). 
\mjc{The application of a \myMethod{}} is possible whenever the underlying dynamical model is structurally unidentifiable, then structural identifiability analysis can be carried out and explicit expressions \mjc{for} identifiable parameter combinations can be determined. 
This notably includes the class of non-linear ODE models with rational expressions of the states, inputs, and parameters, for which software tools such as \emph{SIAN}\cite{Ilmer2021}, \emph{COMBOS}\cite{meshkat2014COMBOS} and \emph{Structural-Identifiability}\cite{dong2023differential} may be used to automatically determine identifiable model parameter combinations\cite{Rey_Barreiro2023}.


\subsection{Model-based representation for time series data}
In the following, we review the basic notions of Bayesian parameter estimation for dynamical models and adapt \mjc{them} for the purposes of time series classification.
Formulations similar to the one given in this work can be found in \cite{coelho2011bayesian, shen2017classification, linden2022bayesian}.

Let $\{ (\mathcal{Y}^{k},c^{k})\}, k = 1,\ldots,N,$ denote a set of $N$ labelled examples of, potentially multivariate, time series data. 
Here $\mathcal{Y}^{k} = \{ \mathbf{t}^k, \mathbf{Y}^k\}$ consists of a collection of time points $\mathbf{t}^k = \{t_{i}^{k}: i = 1,\ldots, L^{k} \}$ together with a collection of \pt{the corresponding} observations $\mathbf{Y}^k = \{\mathbf{y}_{i}^{k}: i = 1,\ldots, L^{k} \}$ for \mjc{the} time series $k$. Furthermore, $c^{k}$ is the associated class label. 
This formulation allows for \pt{different 
%sets of 
time series} $\mathcal{Y}^{k}$ to be of different lengths, as indicated by $L^{k}$, and be evaluated at different times, as indicated by $\mathbf{t}^k$. 
However, it is assumed that  all observations have the same dimension, i.e., $\mathbf{y}_{i}^{k} \in \mathbb{R}^{r}$. \emph{The task considered is the prediction of a class label $c$, given a new time series $\mathcal{Y}$ of length $L$}. 
The key idea of this framework is to regard each time series as an instance of a dynamical model from a given model class. 
Time series are considered as partial observations of an underlying dynamical model characterized by a set of Ordinary Differential Equations (ODEs)
\begin{equation}\label{eq:dyn_sys}
    \frac{d \mathbf{x}_{t}}{dt} = f(\mathbf{x}_{t};\boldsymbol{\psi}),
\end{equation}
with $\mathbf{x}_{t} \in \mathbb{R}^{d}$ denoting the state vector at time $t$.
The defining mapping $f$ is parametrized by a vector $\boldsymbol{\psi} = (\boldsymbol{\theta}, \mathbf{x}_{0})$, where \pt{$\boldsymbol{\theta}\in \mathbb{R}^{n}$} is a vector of model parameters and the initial state $\mathbf{x}_{0}$, which may or may not be known.
Observations from the underlying ODE are obtained via the measurement function
\begin{equation}\label{eq:output}
    \mathbf{y}_{i}  = \mathbf{h}(\mathbf{x}_{t_{i}}) + \boldsymbol{\epsilon}_{t_{i}},
\end{equation}
where $\boldsymbol{\epsilon}_{t_{i}}$ is the observation\eco{al} noise at time $t_{i}$. 

For simplicity, it is assumed that the initial condition \mjc{vector} $\mathbf{x}_{0}$ is known and that the observational noise is distributed as $\boldsymbol{\epsilon}_{t_{i}} \sim \mathcal{N}(\mathbf{0}, \mathbf{R})$, i.e. Gaussian with zero mean and covariance matrix $\mathbf{R}$. 
In general, both $\mathbf{x}_{0}$ and $\mathbf{R}$ could be unknown but \eco{these} \mjc{could potentially be inferred} from the data. 
The parameter configuration that is most likely to have \pt{produced an observation $\mathcal{Y}$, given a prior $p(\boldsymbol{\theta})$ over the parameters,} is the \emph{Maximum A Posterior} (MAP) estimate $\boldsymbol{\theta}_{\text{MAP}}$.
This estimate is the (global) maximum \pt{(if unique)} of the posterior distribution
\begin{equation}\label{eq:posterior}
   p(\boldsymbol{\theta} \mid \mathcal{Y}, \mathbf{R}) =  p(\boldsymbol{\theta} \mid \mathbf{Y}, \mathbf{t}, \mathbf{R}) \propto p(\mathbf{Y} \mid \boldsymbol{\theta}, \mathbf{t}, \mathbf{R}) \  p(\boldsymbol{\theta}).
\end{equation}
Under the assumptions made in Eq.\eqref{eq:output}, the likelihood function takes \eco{on} the form
\begin{equation}\label{eq:likelihood}
    p(\mathbf{Y} \mid \boldsymbol{\theta}, \mathbf{t}, \mathbf{R}) = \prod_{i = 1}^{L} \mathcal{N}(\mathbf{y}_{i} \mid \mathbf{x}_{t}(\boldsymbol{\theta}), t_{i}, \mathbf{R}). 
\end{equation}
Finally, for the purposes of this work, we assume that the prior distribution is of the \pt{``bounding box'' form}
\begin{equation}
    p(\boldsymbol{\theta}) = 
    \begin{cases}
        \frac{1}{V(R)}   &\text{if } \boldsymbol{\theta} \in R, \\
        0                &\text{otherwise,}
    \end{cases}
\end{equation}
where $R = [\theta_{1}^{\text{min}}, \theta_{1}^{\text{max}}] \times \ldots \times [\theta_{n}^{\text{min}}, \theta_{n}^{\text{max}}]$ is \eco{the} hyper-rectangle \eco{enclosed by the individual parameter bounds $\theta_i^{\min}, \theta_i^{\max}$} and $V(R)$ \mjc{is} the volume of $R$. 
The set $R$ will be referred to as Region of Interest (ROI).
This prior information essentially restricts the considered region of the parameter space to $R$ but does not provide any additional information\eco{, i.e., is uniform over the region $R$}.
\pt{Interval priors are quite common in biological models, since often only ``physiologically realistic" parameter ranges are known without further probabilistic structure.}
In order to find \eco{the} $\boldsymbol{\theta}_{\text{MAP}}$ associated with a given time series observation, Eq.~\eqref{eq:posterior} is maximized w.r.t. $\boldsymbol{\theta}$, which is equivalent to maximizing Eq.~\eqref{eq:likelihood} subject to $\boldsymbol{\theta} \in R$.

\subsection{Structural-Identifiability Mapping (\myMethod{})}
Suppose that the dynamical model given in Eq.~\eqref{eq:dyn_sys} is unidentifiable and that, by means of Structural Identifiability (SI) analysis, it is possible to find a set of identifiable parameter combinations $\boldsymbol{\Phi}$ explicitly characterized by $\boldsymbol{\Phi} = g(\boldsymbol{\theta})$, with $g: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$. 
Here, the number of identifiable parameter combinations $m$ is always less than the number of original system parameters $n$, i.e. $m < n$. 
\pt{Consider an equivalence relation on the space of our mechanistic models that identifies models that are behaviourally indistinguishable. 
The equivalence classes of models (parameters) $\mathcal{M}_{\boldsymbol{\Phi}}$ can be then defined as follows:}
\begin{equation}
    \mathcal{M}_{\boldsymbol{\Phi}} = \{ \boldsymbol{\theta} \in \mathbb{R}^{n} \mid \boldsymbol{\Phi} = g(\boldsymbol{\theta}) \}.
\end{equation}
By definition of $g$, any two parameters $\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2} \in \mathcal{M}_{\boldsymbol{\Phi}}$ will lead to identical system trajectories of the system in Eq.~\eqref{eq:dyn_sys}, \pt{given identical initial conditions}. 
\pt{We can operate in the factor set.}
Indeed, any level-set of the posterior in Eq.~\eqref{eq:posterior} can be written as a union of sets $\mathcal{M}_{\boldsymbol{\Phi}}$ and maximization of the posterior means to identify the set of equivalence classes associated with the maximal posterior value.
As far as the classification task is concerned, there is no need to resolve the available information beyond the level of equivalence classes.
\autoref{fig:toy_model_decision_boundary} provides some visual intuition on the matter.
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/decision_boundary_toy_model.pdf}
    \caption{Geometric intuition behind the mechanism of \myMethod{} with data from the toy model. Panel \textbf{a)} depicts a binary classification problem which illustrates how training data can be oriented along manifolds of the form $\Phi = g(a,b) = a b$. Panel \textbf{b)} shows the representation of the same data after applying \myMethod{}. The decision boundary between the two classes becomes simpler and, in this special case, the data even \mjc{become} linearly separable in $\Phi$-space.}
    \label{fig:toy_model_decision_boundary}
\end{figure}
We propose to utilize \myMethodFull{} (\myMethod{}) given by $g$ for time series classification as follows:
\begin{enumerate}
    \item Find the model-based representation for each time series by means of a MAP estimate, i.e.
    \begin{equation}
        \mathcal{Y}^{k} \mapsto \boldsymbol{\theta}_{\text{MAP}}^{k},
    \end{equation}
    with 
    \begin{equation}
        \boldsymbol{\theta}_{\text{MAP}}^{k} =  \argmax_{\boldsymbol{\theta}} p(\boldsymbol{\theta} \mid \mathcal{Y}^{k}, \mathbf{R}),
    \end{equation}
    with posterior as in Eq.~\eqref{eq:posterior}.
    \item Translate each MAP via $g$ to obtain a representation in the space of identifiable parameter combinations
    \begin{equation}\label{eq:SIM}
        \boldsymbol{\theta}_{\text{MAP}}^{k} \mapsto \boldsymbol{\Phi}^{k} := g(\boldsymbol{\theta}_{\text{MAP}}^{k}).
    \end{equation}
    \item Train a vectorial classifier of choice on the transformed data $\{ \boldsymbol{\Phi}^{k} \}_{k = 1}^{N}$.
\end{enumerate}

% SIM can be used when reparametrisation cannot
How is the application of \myMethod{} different from reparametrising a given dynamical model in order to make it structurally identifiable?
The answer is that \myMethod{} can \emph{always} be used when structurally identifiable combinations of parameters can be computed.
However, the reparametrisation of a given model in terms of such a set of structurally identifiable combinations is \emph{not always} possible.
In this sense, \myMethod{} focuses on the ML task at hand rather than the creation of an all-new dynamical model with more favourable identifiability properties.

% Regularization interpretation
From a classification point of view, \myMethod{} can be thought of as having a regularizing influence on the learned decision boundary in \pt{the} $\boldsymbol{\theta}$-space. 
If we were to train the classifier in the space of $\boldsymbol{\theta}$, the decision boundary learned from the data could be such that two values $\boldsymbol{\theta}_{1} \neq \boldsymbol{\theta}_{2}$ with $g(\boldsymbol{\theta}_{1}) = g(\boldsymbol{\theta}_{2})$ become associated with different classes.
\kb{This results in undesired behaviour,} %This does not make a lot of sense 
since SI analysis tells us that both values of $\boldsymbol{\theta}$ will yield identical observable output for our dynamical model and should therefore be associated with the same class.
On the other hand, training the classifier using \myMethod{}, the learned decision boundary in $\boldsymbol{\theta}$-space becomes the union of pre-images $g^{-1}(\boldsymbol{\Phi})$. 
This guarantees that any two models $\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2}$ with $g(\boldsymbol{\theta}_{1}) = g(\boldsymbol{\theta}_{2})$ are \kb{always} associated with the same class.