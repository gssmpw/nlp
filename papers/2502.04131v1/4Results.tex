\section{Results}\label{sec:results}
In the following, the results of experiments 1, 2 and 3 are presented in detail for the batch reactor model example.
The results for all other example models are qualitatively similar and therefore summarized in Tables~\ref{tab:experiment_1},~\ref{tab:experiment_2} and \ref{tab:experiment_3}.
The results for the other example models are presented in detail in Appendix~\ref{app:add_exp_outcomes}.

\subsection{Experiment 1}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/Results/Exp1_BR.pdf}
    \caption{Experiment 1 showing improved classification with partially observed batch reactor model due to \myMethod{}. 
    Displayed are learning curves obtained from classifier training based on the fully observed (FO) dynamical model (dotted green) and the partially observed (PO) dynamical model, with and without application of \myMethod{} (marked with solid blue and dashed orange curves, respectively).
    The training and test data used were generated on the dense time grid $t_{\text{dense}}$ with fixed observational noise $\sigma = 1$ on each observed component.
    }
    \label{fig:exp1_BR}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/Results/Exp2_BR.pdf}
    \caption{Experiment 2 for the partially observed batch reactor model showing that \myMethod{} is robust to observational noise.
    Displayed are generalization error and relative number of support vectors, each as a function of the observational noise.
    Classification performance is compared for the partially observed (PO) dynamical model, with and without application of \myMethod{} (marked with solid blue and dashed orange curves, respectively).
    Training and test data are generated on the dense time grid $t_{\text{dense}}$ with $N_{\text{train}} = 20$ and $N_{\text{test}} = 400$.
    }
    \label{fig:exp2_BR}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figures/Results/Exp3_BR.pdf}
    \caption{Experiment 3 for the partially observed batch reactor model showing \myMethod{} is robust to changes in regularity and sparsity of the observed time series data.
    Displayed are the learning curves obtained from classifier training for the partially observed (PO) dynamical model, with and without application of \myMethod{} (marked with solid blue and dashed orange curves, respectively).
    The training and test data used were generated on the three different time grids $t_{\text{dense}}, t_{\text{sparse}}$ and $t_{\text{irr}}$, displayed in the left, middle and right panel, respectively.
    Observational noise is fixed at $\sigma = 1$ on each observed component.
    }
    \label{fig:exp3_BR}
\end{figure*}

\begin{table*}[t!]
    \centering
    \caption{
    Summary of experiment 1 comparing classification with the fully observed (FO) model, partially observed (PO) model and partially observed model with \myMethod{} (PO + \myMethod{}).
    Mean generalization errors and standard deviations (in parentheses), evaluated at the lowest number of training examples, are shown for all example systems.
    }
    \begin{tabular}{p{1cm}|p{0.95cm}p{0.95cm}|p{1.8cm}p{1.8cm}p{1.8cm}|p{2cm}p{2cm}p{2cm}} %14.3cm
        %\cline{2-9}
        \hline
        \multicolumn{1}{c}{} & \multicolumn{2}{|c|}{Examples} & \multicolumn{3}{c|}{Generalization error at $N_{min}$} & \multicolumn{3}{c}{Generalization error at $N_{\max}$}  \\
        \hline
            System & $N_{\min}$ & $N_{\max}$ & FO & PO & PO + SIM & FO & PO & PO + SIM \\
        \hline
        CCM2 & 10 & 100 & .01 (.02) & .07 (.04) & .06 (.05) & .003 (.003) & .004 (.002) & .003 (.0008) \\
        CCM4 & 10 & 400 & .08 (.07) & .3 (.08) & .2 (.05) & 0 (0) & .01 (.005) & .01 (.002) \\
        CML & 10 & 400 & .06 (.06) & .3 (.05) & .2 (.07) & .0004 (.0005) & .01 (.002) & .008 (.002) \\
        BR & 10 & 200 & .1 (.07) & .3 (.1) & .1 (.05) & .02 (.003) & .06 (.003) & .06 (.002) \\
        \hline
    \end{tabular}
    \label{tab:experiment_1}
\end{table*}

\begin{table}[t!]
    \centering
    \caption{
    Summary of experiment 2.
    Maximum ($\Delta \epsilon^{\ast}$) and mean ($\langle \Delta \epsilon \rangle$) difference in generalization error due to SIM.
    The observational noise at which $\Delta \epsilon^{\ast}$ occurs is $\sigma^{\ast}$.
    }
    \begin{tabular}{p{1.5cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.3cm}}
    \hline
        System & $N_{train}$ & $\sigma^{\ast}$ & $\Delta \epsilon^{\ast}$ & $\langle \Delta \epsilon \rangle$ \\
        \hline
        toy model & 10 & .20 & .10 & .07 \\
        CCM2 & 10 & 4.00 & .04 & .03 \\
        CCM4 & 10 & 1.00 & .22 & .10 \\
        CML & 10 & 1.00 & .19 & .10 \\
        BR & 10 & 1.00 & .22 & .13 \\
        \hline
    \end{tabular}
    \label{tab:experiment_2}
\end{table}

\begin{table*}[t!]
    \centering
    \caption{
    Summary of experiment 3 on the effect of \myMethod{} when applied to time series on dense, sparse and irregular time grids.
    Mean generalization errors and standard deviations (in parentheses) are evaluated at the lowest number of training examples $N_{\min}=10$ for each model.
    }
    \begin{tabular}{p{2.3cm}|p{2.1cm}p{2.1cm}|p{2.1cm}p{2.1cm}|p{2.1cm}p{2.1cm}}
        %\cline{2-7}
        \hline
        \multicolumn{1}{c}{}  & \multicolumn{2}{|c|}{Dense grid} & \multicolumn{2}{c|}{Sparse grid} & \multicolumn{2}{c}{Irregular grid} \\
        \hline
        System & PO & PO + SIM & PO & PO + SIM & PO & PO + SIM \\
        \hline
        toy model & .03 (.04) & .005 (.02) & .05 (.08) & .0001 (.0006) & .07 (.08) & .004 (.01) \\
        CCM2 & .1 (.1) & .03 (.03) & .2 (.1) & .08 (.03) & .2 (.09) & .2 (.04) \\
        CCM4 & .3 (.06) & .2 (.07) & .4 (.06) & .3 (.08) & .4 (.05) & .3 (.07) \\
        CML & .3 (.07) & .2 (.09) & .4 (.07) & .3 (.05) & .4 (.07) & .3 (.08) \\
        BR & .4 (.09) & .1 (.07) & .4 (.06) & .3 (.1) & .5 (.06) & .3 (.1) \\
        \hline
    \end{tabular}
    \label{tab:experiment_3}
\end{table*}

The outcomes of experiment 1 for the batch reactor model are summarized in \autoref{fig:exp1_BR}.
% Comparing FO and PO models
Comparing the training outcomes of the fully observed (FO) dynamical model \eco{to} the partially observed (PO) dynamical model\eco{,} the results are not surprising.
The training data obtained for the FO model are a super-set of the data available for the PO model.
One would therefore expect that the classifier training with the FO model is more successful than training with the PO model.
This is indeed reflected in \autoref{fig:exp1_BR}. 
For any amount of training data available, the FO curve for the generalisation error lies significantly below the PO curve.
The same is true for the relative number of support vectors.
\kb{As expected}, using training data which include observations from all compartments, the problem of structural identifiability 
does not arise and it is possible to achieve better classification performance by fitting models of relatively low complexity.

% Comparing FO and PO vs. PO + SIM models
The outcomes become more interesting when comparing the \eco{performance} of the FO and PO models to \eco{those} for the PO model where \myMethod{} was applied. 
Considering the generalization error, it is clear that the PO model + \myMethod{} outperforms the PO model  \pt{consistently} when the number of training examples is less than 50.
Subsequently, the PO model and PO model + \myMethod{} reach comparable levels of generalisation error.
Neither the PO model nor the PO model + \myMethod{} quite reach the performance of the FO model.
A similar situation can be observed for the relative number of support vectors.
Up to 50 training examples, the mean curve for the PO model + \myMethod{} is very similar to the mean curve for the FO model.
After 50 training examples, the mean curve for the PO model + \myMethod{} becomes evermore  similar to that for the PO model.
The FO, PO and PO + \myMethod{} curves obtained \kb{for the number of support vectors} in \autoref{fig:exp1_BR} are overall very similar to one another (when accounting for the observed standard deviations) and the effect of \myMethod{} is less clearly visible.
However, in the low data regime up to 50 examples, \myMethod{} turns the classification problem into one with a less complex decision boundary associated with fewer support vectors and reduced generalisation error.

% Discuss summary table
The outcomes of experiment 1 for the other models are summarized in \autoref{tab:experiment_1} and provide a similar picture.
When comparing the generalization errors at the maximal number of training examples $N_{\max}$, the values for the PO model and the PO model + \myMethod{} are typically very similar with the values of the FO model, being significantly lower.
However, at the minimal number of training examples, $N_{\min}$, the PO model is clearly outperformed by the PO model + \myMethod{}, which, in turn, is clearly outperformed by the FO model.

The conclusion to be drawn is clear: for densely sampled time series data and with relatively low observational noise present, \myMethod{} approach significantly reduces the complexity of the classification problem.
Notably, when relatively little training data are available, the classification performance is remarkably close to the performance that would be attained if the underlying dynamical model was fully observed.
\kb{Utilizing the information from the structural identifiability analysis therefore exhibits a good alternative, when certain measurements are unobtainable,
for improving machine learning performance, in particular when training data are limited.}

\subsection{Experiment 2}
The outcomes of experiment 2 for the batch reactor model are summarized in \autoref{fig:exp2_BR}.
A few general trends are immediately evident.
As the observational noise on the training data increases, the generalization error, as well as the relative number of support vectors increase.
This is due to the fact that the overall classification problem becomes more difficult when more observational noise is present, to the point where the signal distinguishing the classes is drowned in noise and the generalization error approaches $0.5$ (random guessing).
Nevertheless, for a wide range of observational noise values, the generalization error is significantly reduced when applying \myMethod{}.
For small values of observational noise, the application of \myMethod{} leads to fewer support vectors being assigned.
However, as the noise increases, the classification task becomes more difficult with \eco{an increasing number of} training examples lying close to the decision boundary, \eco{which therefore become} assigned as support vectors.

\autoref{tab:experiment_2} summarizes the experimental outcomes for the remaining example models.
In all cases, the maximal error difference $\Delta \epsilon^{\ast}$ occurs for relatively small amounts of observational noise $\sigma^{\ast}$.
This is to be expected, since the observational noise is completely independent of any effect related to structural identifiability.
Therefore, the effect of \myMethod{} should, in principle, be strongest for zero observational noise.
The mean difference in the generalization error $\langle \Delta \epsilon \rangle$ is positive for all example models, indicating a net reduction in the generalization error due to \myMethod{} across different levels of observational noise.
In summary, experiment 2 demonstrates that the  \myMethod{} is more robust to observational noise in the sense that it leads to improved classification performance for a wide range of observational noise levels.

\subsection{Experiment 3}
The outcomes of experiment 3 for the batch reactor model are summarized in \autoref{fig:exp3_BR}. 
It is to be noted that the \eco{presentation} of \eco{the} results for experiment 3 differ from those for experiments 1 and 2 in that 
\kb{for this experiment we plot} \emph{only} the generalization error.
The learning curves are qualitatively the same as those presented in \autoref{fig:exp1_BR} and the general trend is again clearly visible. 
Using time series data with observations in $t_{\text{dense}}$ yields better generalization errors than training with data that are generated in $t_{\text{sparse}}$.
Similarly, training with data generated in $t_{\text{sparse}}$ yields better overall results than training on data that are generated in $t_{\text{irr}}$.
This is reasonable, since the data in $t_{\text{dense}}$ simply contain more information than those in $t_{\text{sparse}}$ and $t_{\text{irr}}$.
The effect of \myMethod{} appears to be robust with respect to the time grid used:
for each time grid the application of \myMethod{} yields reduced generalization errors.

Since the difference between the PO model and the PO model + \myMethod{} are most pronounced for relatively small amounts of training data, \autoref{tab:experiment_3} summarizes the outcomes of experiment 3 for all example models at the minimal number of training examples $N_{\min}$.
In all cases, the application of \myMethod{} leads to a reduction in average generalization error.