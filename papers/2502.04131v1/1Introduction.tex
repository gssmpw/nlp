\section{Introduction}

%%% 1 Setting and context %%%
\IEEEPARstart{T}{he} problem of time series classification is concerned with assigning an observed time series to one of a given set of classes. 
As time series data naturally arise in a wide variety of scientific disciplines, including medicine, engineering and the social sciences, the associated theory of classification has been applied with great success to a multitude of problems such as health monitoring~\cite{mikalsen2016learning}, maintenance of civil infrastructure~\cite{carden2008ARMA} and emotion recognition from speech~\cite{trentin2015pattern}. 
While research has been directed at this problem \mjc{over many years}, reliable and time-efficient classification of time series data remains a challenging problem to date \cite{pei2018multivariate, bianchi2021reservoir, Bradde2021multiclass}.

%%% 2 Model-based approaches %%%
In many disciplines, the classification task is accompanied by domain-specific knowledge in the form of a mechanistic model which describes the data-generating mechanism.
This kind of model is typically derived from the application of a physical, chemical, biological or sociological law, and often takes the form of a parametrised dynamical system model.
If such a model is available, then its incorporation into the classification task is beneficial in two ways: 
Firstly, interpreting the data in the context of the mechanistic model makes it possible to deal with sparsely and irregularly sampled time series data in a natural way.
Secondly, any learned classification rule becomes interpretable as it directly relates to the given mechanistic model \cite{shen2017classification}.  
The second point is of particular importance for the modelling of high-risk applications common to biomedical and engineering domains, where the interpretability of \mjc{the application of machine learning} is of critical importance.
When safety, correctness \mjc{and trustworthiness} need to be guaranteed, model-based approaches are often the only viable option.

% 3a SI-Analysis
Assuming that a mechanistic model in the form of a parametrised dynamical system model is indeed available, \mjc{then} it is not automatically guaranteed that model-based classification works well.
The degree to which variables in the dynamical model can be observed is often restricted by practical and ethical considerations. 
This limited observability may lead to ill-posed parameter estimation problems when trying to infer model parameters from given time series data.
The problem of \mjc{determining} whether a given model allows for unique inference of model parameters has been studied extensively in the literature over the past 50 years and is known as Structural Identifiability (SI).
If a given model is not structurally identifiable, then multiple parameter configurations will produce identical input-output behaviour of the model. 
It follows that parameters cannot be meaningfully estimated, regardless of the amount and quality of the available data. 
SI is to be contrasted against Practical Identifiability (PI).
PI is concerned with the situation in which ambiguity about parameter estimates arises from noise and unfavourable observation times of the available data. 

% Relation to epistemic and aleatoric uncertainty
It can be argued that
%\kb{We would argue that 
\kb{structural and practical identifiability are connected to epistemic (systematic) and aleatoric (stochastic) uncertainty, respectively. 
Epistemic (derived from Latin \emph{episteme}=\emph{knowledge}) refers to uncertainty that can be reduced by additional knowledge, while 
aleatoric uncertainty (derives from the Latin \emph{alea}=\emph{game of chance}) is not expected to be reducible \cite{Hora1996AleatoryAE}. 
The discussion about the importance of these uncertainty categories reignited \cite{KIUREGHIAN2009} and distinct handling has gained traction in the machine learning community \cite{Huellermeier2021AleatoricAE}. 
Therefore, SI analysis constitutes an important component for the understanding and reduction of epistemic uncertainty for dynamic systems.}
%For the \eco{purposes} of the present work, the scope is limited to SI only.

% 3b SI-Analysis: methods
In order to determine whether a given model is structurally identifiable, three main branches of SI Analysis have emerged: 
the Output Equality Approach~\cite{bellman1970structural, pohjanpalo1978system, walter1996identifiability}, 
the Local State Isomorphism Approach~\cite{tunali1987new, vajda1989state, sussmann1976existence}, and 
the Differential Algebra Approach~\cite{diop1991nonlinear, ljung1994global, jain2019priori}. 
Each technique has its strengths and weaknesses.
\eco{N}otably, the Output Equality Approach is not guaranteed to work for non-linear models.
\eco{H}owever, it can be used to analyse linear models in an intuitive way.

% 3c SI-Analysis: what if the model is non-identifiable?
Assuming a given model has been found to be unidentifiable, then there are a few things one can try in order to still be able to infer the model parameters from data.
A straightforward option is to set certain parameters \mjc{to be} constant such that the remaining set of parameters becomes identifiable. 
The advantages of this approach are its simplicity and the fact that all other parameters remain interpretable within the domain-specific context. 
Considerable disadvantages are the necessity for profound mechanistic insight into the model used and a reduced potential for interpretation of the model predictions \cite{WIELAND202160}.
Another option would be to approximate the behaviour of the unidentifiable model with a different model which in turn is identifiable.
However, finding such a model is typically not an easy task.

Finally, one may attempt to reparametrise an unidentifiable model such that all of the new parameters in the resulting model become identifiable. 
Reparametrisation of a given model often requires a time-consuming manual effort in which practitioners enter a cycle of model construction, quantitative simulation\eco{s} and experimental validation of model predictions.
However, recent advances in automatic reparametrisation for dynamical models show great \eco{potential}. 
Notably, the \emph{AutoRepar} extension~\cite{massonis2021autorepar} for the STRIKE GOLDD SI analysis toolbox~\cite{villaverde2016structural} for MATLAB is capable of semi-automatic reparametrisation for ordinary differential equation models involving rational expressions. 

% 3d SI-Analysis: AutoRepar
AutoRepar employs a notion of identifiability called Full Input-State-Parameter Observability (FISPO).
As the name suggests, FISPO goes beyond establishing parameter identifiability and requires that all states of some auxiliary model are observable.
This is an elegant way of treating observability and identifiability in a coherent way.
However, requiring a model to be FISPO is a stronger condition than requiring it to be identifiable.
It follows that AutoRepar is not always well-suited to finding identifiable models which are not FISPO.
Moreover, AutoRepar works by determining and removing Lie symmetries present in the model which can give rise to the unidentifiability.
This approach has two critical limitations:
Firstly, there is no method to determine the type and number of symmetries present in a given model.
Secondly, there is no upper bound on the number of terms needed in the Lie derivative series in order to obtain the infinitesimal transformation necessary to find a suitable reparametrisation \cite{massonis2020}.
In summary: even with the help of semi-automatic reparametrisation tools such as AutoRepar, reparametrising a given model such that the resulting model is identifiable and so that domain-specific interpretation is retained remains a challenge. 

% 4 Research purpose / our work
Since the reparametrisation of a given dynamical model is often very involved but structural identifiability analysis itself can frequently be carried out with much less difficulty, we propose a model-based framework for time series classification that accounts for the unidentifiability of the underlying dynamical model, \kb{referred to as} %. We call this framework 
``\myMethodFull{}" (\myMethod{}).
\pt{To this end, we employ a model-based time series classification where each individual time series is represented through a Maximum A Posteriori estimate (MAP) of the given dynamical model, for the given time series.}
We consider Ordinary Differential Equation (ODE) models in which one or more parameters are unidentifiable. 
Instead of representing individual time series as parameter vectors in the original parameter space, we consider a representation in the space of structurally identifiable parameter combinations. 
Any conventional classification framework acting on vectorial data may \kb{subsequently} be used to train a classifier in this space.

% Contributions
The contribution of this work is threefold.
Firstly, we propose a novel framework (\myMethod{}) \mjc{for} time series classification by representing time series data as identifiable parameter combinations of a given unidentifiable dynamical system. 
Secondly, we demonstrate the effectiveness of this framework by applying it to a number of relevant dynamical system models commonly encountered in computational biology. 
In particular, we demonstrate that, by accounting for the unidentifiability of the dynamical model, time series observations can be classified accurately even when there \mjc{are} only \mjc{few} data samples available.
Finally, we reaffirm the importance of carrying out SI analysis whenever machine learning is applied in conjunction with parametrised dynamical system models. 
This aspect has not received much attention, yet it is critical to the success of the machine learning application.

% oraniziation of this paper
This paper is organized as follows: 
In \autoref{sec:methods}, we review a model-based framework for time series classification and introduce our method of \myMethodFull{} (\myMethod{}). 
In \autoref{sec:experiments}, we introduce three biologically relevant example models that serve as test beds for \myMethod{} and outline the experiments that demonstrate the potential of \myMethod{}.
Experimental results are presented in \autoref{sec:results}. 
\kb{Finally,} in \autoref{sec:discussion} and \autoref{sec:conclusion}, we conclude with a discussion on the results and their implications.