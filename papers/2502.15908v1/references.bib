% !TEX root =  main.tex

% \bibitem{b1} S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, J. Gao. 2024. Large Language Models: A Survey. ArXiv:2402.06196 [cs.CL]

@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}

@article{murthy2024mobileaibench,
  title={MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases},
  author={Murthy, Rithesh and Yang, Liangwei and Tan, Juntao and Awalgaonkar, Tulika Manoj and Zhou, Yilun and Heinecke, Shelby and Desai, Sachin and Wu, Jason and Xu, Ran and Tan, Sarah and others},
  journal={arXiv preprint arXiv:2406.10290},
  year={2024}
}

% \bibitem{b2} H. Naveeda, A. Khana , S. Qiub , M. Saqibc, S. Anware, M. Usmane, N. Akhtarg, N. Barnesh, A. Miani. 2024. A Comprehensive Overview of Large Language Models. ArXiv:2307.06435 [cs.CL]
@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

\bibitem{b3} T. Marques, S. Carreira, C. Grilo, J. Ribeiroa. 2023. Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. ArXiv:2310.01434 [cs.CL]
@article{carreira2023revolutionizing,
  title={Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile},
  author={Carreira, Samuel and Marques, Tom{\'a}s and Ribeiro, Jos{\'e} and Grilo, Carlos},
  journal={arXiv preprint arXiv:2310.01434},
  year={2023}
}

%\bibitem{b4} K. Alizadeh, I. Mirzadeh, D. Belenko, S. Karen Khatamifard, M. Cho, C. C Del Mundo, M. Rastegari, M. Farajtabar. 2024. LLM in a flash: Efficient Large Language Model Inference with Limited Memory. ArXiv:2312.11514 [cs.CL]
@article{alizadeh2023llm,
  title={Llm in a flash: Efficient large language model inference with limited memory},
  author={Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2312.11514},
  year={2023}
}
%\bibitem{b5} T. Çöplü, M. Loedi, A. Bendiken, M. Makohin, J. Bouw, S. Cobb. 2023. A Performance Evaluation of a Quantized Large Language Model on Various Smartphones. ArXiv:2312.12472 [cs.LG]
@article{ccoplu2023performance,
  title={A Performance Evaluation of a Quantized Large Language Model on Various Smartphones},
  author={{\c{C}}{\"o}pl{\"u}, Tolga and Loedi, Marc and Bendiken, Arto and Makohin, Mykhailo and Bouw, Joshua J and Cobb, Stephen},
  journal={arXiv preprint arXiv:2312.12472},
  year={2023}
}
%\bibitem{b6} Y. Sens, H. Knopp, S. Peldszus, T. Berger. 2024. Large-Scale Study of Model Integration in ML-Enabled Software Systems. ArXiv:2408.06226 [cs.SE]
@article{sens2024large,
  title={A Large-Scale Study of Model Integration in ML-Enabled Software Systems},
  author={Sens, Yorick and Knopp, Henriette and Peldszus, Sven and Berger, Thorsten},
  journal={arXiv preprint arXiv:2408.06226},
  year={2024}
}
%\bibitem{b7} N. Nahar, H. Zhang, G. Lewis, S. Zhou, C. Kästner. 2024. The Product Beyond the Model – An Empirical Study of Repositories of Open-Source ML Products. ArXiv:2308.04328 [cs.SE]

@inproceedings{nahar2023dataset,
  title={The Product Beyond the Model--An Empirical Study of Repositories of Open-Source ML Products},
  author={Nahar, Nadia and Zhang, Haoran and Lewis, Grace and Zhou, Shurui and K{\"a}stner, Christian},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)},
  pages={63--75},
  year={2024},
  organization={IEEE Computer Society}
}

@article{palomba2019impact,
  title={On the impact of code smells on the energy consumption of mobile applications},
  author={Palomba, Fabio and Di Nucci, Dario and Panichella, Annibale and Zaidman, Andy and De Lucia, Andrea},
  journal={Information and Software Technology},
  volume={105},
  pages={43--55},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{viennot2014measurement,
  title={A measurement study of google play},
  author={Viennot, Nicolas and Garcia, Edward and Nieh, Jason},
  booktitle={The 2014 ACM international conference on Measurement and modeling of computer systems},
  pages={221--233},
  year={2014}
}

@article{paguthaniyaintegration,
  title={Integration Of Machine Learning Models Into Backend Systems: Challenges And Opportunities},
  author={Paguthaniya, Sajid Ali and Patel, Faruk Yusuf and Aminu, Emmanuel and Adeleye, Abdulbasit}
}

@article{ahasanuzzaman2020studying,
  title={Studying ad library integration strategies of top free-to-download apps},
  author={Ahasanuzzaman, Md and Hassan, Safwat and Hassan, Ahmed E},
  journal={IEEE Transactions on Software Engineering},
  volume={48},
  number={1},
  pages={209--224},
  year={2020},
  publisher={IEEE}
}

@inproceedings{polese2022adoption,
  title={Adoption of third-party libraries in mobile apps: a case study on open-source Android applications},
  author={Polese, Aidan and Hassan, Safwat and Tian, Yuan},
  booktitle={Proceedings of the 9th IEEE/ACM International Conference on Mobile Software Engineering and Systems},
  pages={125--135},
  year={2022}
}

%\bibitem{b8} D. Hendrycks, K. Lee, and M. Mazeika, “Using pre-training can improve model robustness and uncertainty,” in International conference on machine learning, pp. 2712–2721, PMLR, 2019.
@inproceedings{hendrycks2019using,
  title={Using pre-training can improve model robustness and uncertainty},
  author={Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
  booktitle={International conference on machine learning},
  pages={2712--2721},
  year={2019},
  organization={PMLR}
}

%\bibitem{b9} D. Erhan, A. Courville, Y. Bengio, and P. Vincent, “Why does unsupervised pre-training help deep learning?,” in Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 201– 208, JMLR Workshop and Conference Proceedings, 2010.
@inproceedings{erhan2010does,
  title={Why does unsupervised pre-training help deep learning?},
  author={Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={201--208},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
%\bibitem{b10} J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, R. McHardy. 2023. Challenges and Applications of Large Language Models. ArXiv:2307.10169 [cs.CL]
@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}
%\bibitem{b12}  L. Li, S. Qian, J. Lu, L. Yuan, R. Wang, Q. Xie. 2024. Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. ArXiv:2403.20041 [cs.CL]
@article{li2024transformer,
  title={Transformer-lite: High-efficiency deployment of large language models on mobile phone gpus},
  author={Li, Luchang and Qian, Sheng and Lu, Jie and Yuan, Lunxi and Wang, Rui and Xie, Qin},
  journal={arXiv preprint arXiv:2403.20041},
  year={2024}
}
\bibitem{b13} TensorFlow Lite. Retrieved from https://tensorflow.google.cn/lite
@misc{tflite,
    author= {{Google AI}},
    year  = {2024},
    title = {LiteRT overview},
    note  = {\url{https://ai.google.dev/edge/litert}, 
             Last accessed on 2024-11-05},
}
%\bibitem{b14} Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yu Cai, and Tianhang Yu. 2020. MNN: A universal and efficient inference engine. Proceedings of Machine Learning and Systems 2, (2020), 1–13.
@article{jiang2020mnn,
  title={MNN: A universal and efficient inference engine},
  author={Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={1--13},
  year={2020}
}
\bibitem{b15} NCNN. Retrieved from https://github.com/Tencent/ncnn
@misc{ncnn,
    author= {{Tencent}},
    year  = {2024},
    title = {ncnn},
    note  = {\url{https://github.com/Tencent/ncnn}, 
             Last accessed on 2024-11-05},
}
\bibitem{b16} llama.cpp. Retrieved from https://github.com/ggerganov/llama.cpp
@misc{whispercpp,
    author= {{ggerganov}},
    year  = {2024},
    title = {whisper.cpp},
    note  = {\url{https://github.com/ggerganov/whisper.cpp}, 
             Last accessed on 2024-11-05},
}
@misc{llamacpp,
    author= {{ggerganov}},
    year  = {2024},
    title = {llama.cpp},
    note  = {\url{https://github.com/ggerganov/llama.cpp}, 
             Last accessed on 2024-11-05},
}
\bibitem{b17} MLC-LLM. Retrieved from https://github.com/mlc-ai/mlc-llm
@misc{mlcllm,
    author= {{MLC-AI}},
    year  = {2024},
    title = {MLC-LLM},
    note  = {\url{https://github.com/mlc-ai/mlc-llm}, 
             Last accessed on 2024-11-05},
}
\bibitem{b18} LangChain. Retrieved from https://www.langchain.com/
@misc{langchain,
    author= {{LangChain}},
    year  = {2024},
    title = {LangChain},
    note  = {\url{https://www.langchain.com/}, 
             Last accessed on 2024-11-05},
}
%\bibitem{b19} Topsakal, Oguzhan \& Akinci, T. Cetin. (2023). Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast. International Conference on Applied Engineering and Natural Sciences. 1. 1050-1056. 10.59287/icaens.1127. 
@inproceedings{topsakal2023creating,
  title={Creating large language model applications utilizing langchain: A primer on developing llm apps fast},
  author={Topsakal, Oguzhan and Akinci, Tahir Cetin},
  booktitle={International Conference on Applied Engineering and Natural Sciences},
  volume={1},
  number={1},
  pages={1050--1056},
  year={2023}
}
%\bibitem{b20} L. Chen, M. Zaharia and J. Zou. 2023. How is ChatGPT’s behaviour changing over time? ArXiv:2307.09009 [cs]
@article{chen2023chatgpt,
  title={How is ChatGPT's behavior changing over time?},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2307.09009},
  year={2023}
}
\bibitem{b21} https://ollama.com
@misc{Ollama,
    author= {{Ollama}},
    year  = {2024},
    title = {Ollama},
    note  = {\url{https://ollama.com}, 
             Last accessed on 2024-11-05},
}
\bibitem{b22} https://huggingface.co/collections/open-llm-leaderboard/llm-leaderboard-best-models-652d6c7965a4619fb5c27a03
@misc{hugginfaceleaderboard,
    author= {{Hugging Face}},
    year  = {2024},
    title = {Open LLM Leaderboard best models},
    note  = {\url{https://huggingface.co/collections/open-llm-leaderboard/llm-leaderboard-best-models-652d6c7965a4619fb5c27a03}, 
             Last accessed on 2024-11-05},
}
\bibitem{b23} 
@misc{novitaai,
    author= {{Novita AI}},
    year  = {2024},
    title = {Top 10 LLM Models on Hugging Face},
    note  = {\url{https://medium.com/@marketing\_novita.ai/top-10-llm-models-on-hugging-face-365c21120bdb}, 
             Last accessed on 2024-11-05},
}
\bibitem{b24} https://www.analyticsvidhya.com/blog/2023/12/large-language-models-on-hugging-face/
@misc{snitika,
    author= {{Nitika Sharma}},
    year  = {2024},
    title = {Top 10 Large Language Models on Hugging Face},
    note  = {\url{https://www.analyticsvidhya.com/blog/2023/12/large-language-models-on-hugging-face/}, 
             Last accessed on 2024-11-05},
}
\bibitem{b25} https://analyticsindiamag.com/16-largest-closed-source-llms-you-must-know-about/
@misc{atasma,
    author= {{Tasmia Ansari}},
    year  = {2024},
    title = {16 Best Closed-Source LLMs You Must Know About},
    note  = {\url{https://analyticsindiamag.com/ai-mysteries/16-largest-closed-source-llms-you-must-know-about/}, 
             Last accessed on 2024-11-05},
}
\bibitem{b26} https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models
@misc{lben,
    author= {{Ben Lutkevich}},
    year  = {2024},
    title = {19 of the best large language models in 2024},
    note  = {\url{https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models}, 
             Last accessed on 2024-11-05},
}
\bibitem{b27} https://zapier.com/blog/best-llm/
@misc{gharry,
    author= {{Harry Guinness}},
    year  = {2024},
    title = {The best large language models (LLMs) in 2024},
    note  = {\url{https://zapier.com/blog/best-llm/}, 
             Last accessed on 2024-11-05},
}
@misc{githubapi,
    author= {{Github Docs}},
    year  = {2022},
    title = {About the REST API},
    note  = {\url{https://docs.github.com/en/rest/about-the-rest-api/about-the-rest-api?apiVersion=2022-11-28}, 
             Last accessed on 2024-11-05},
}
@misc{chatgpt,
    author= {{OpenAI}},
    year  = {2024},
    title = {ChatGPT},
    note  = {\url{https://chatgpt.com/}, 
             Last accessed on 2024-11-05},
}

@misc{huggingchat,
    author= {{HuggingChat}},
    year  = {2024},
    title = {HuggingChat},
    note  = {\url{https://huggingface.co/chat/}, 
             Last accessed on 2024-11-05},
}

@misc{openai,
    author= {{OpenAI}},
    year  = {2024},
    title = {OpenAI},
    note  = {\url{https://openai.com/},
            Last accessed on 2024-11-05},
}

@misc{cohere,
    author= {{Cohere Inc.}},
    year  = {2024},
    title = {Cohere},
    note  = {\url{https://cohere.com/},
            Last accessed on 2024-11-05},
}

%alexatm,  claude, deepseek, falcon, 

@misc{alexatm,
    author= {{Amazon Science}},
    year  = {2024},
    title = {Alexa Teacher Models},
    note  = {\url{https://github.com/amazon-science/alexa-teacher-models},
            Last accessed on 2024-11-05},
}

@misc{claude,
    author= {{Anthropic}},
    year  = {2024},
    title = {Claude},
    note  = {\url{https://www.anthropic.com/claude},
            Last accessed on 2024-11-05},
}

@misc{deepseek,
    author= {{deepseek}},
    year  = {2024},
    title = {deepseek},
    note  = {\url{https://www.deepseek.com/},
            Last accessed on 2024-11-05},
}

@misc{falcon,
    author= {{Technology Innovation Institute}},
    year  = {2024},
    title = {Falcon},
    note  = {\url{https://falconllm.tii.ae/},
            Last accessed on 2024-11-05},
}

%gemini, gemma, gpt, lamda, marcoroni, 
@misc{gemini,
    author= {{Google}},
    year  = {2024},
    title = {Gemini},
    note  = {\url{https://gemini.google.com/},
            Last accessed on 2024-11-05},
}

@misc{gemma,
    author= {{Google Deepmind}},
    year  = {2024},
    title = {Gemma},
    note  = {\url{https://github.com/google-deepmind/gemma},
            Last accessed on 2024-11-05},
}

@misc{lamda,
    author= {{Google}},
    year  = {2021},
    title = {LaMDA: our breakthrough conversation technology},
    note  = {\url{https://blog.google/technology/ai/lamda/},
            Last accessed on 2024-11-05},
}

@misc{marcoroni,
    author= {{Huggingface}},
    year  = {2024},
    title = {Marcoroni 7b - GGUF},
    note  = {\url{https://huggingface.co/TheBloke/Marcoroni-7b-GGUF},
            Last accessed on 2024-11-05},
}

%minichat, mistral, mixtral, nyxene, 

@article{minichat,
  title={Towards the law of capacity gap in distilling language models},
  author={Zhang, Chen and Song, Dawei and Ye, Zheyu and Gao, Yan},
  journal={arXiv preprint arXiv:2311.07052},
  year={2023}
}

@misc{mistral,
    author= {{Mistral AI}},
    year  = {2024},
    title = {Mistral},
    note  = {\url{https://chat.mistral.ai/},
            Last accessed on 2024-11-05},
}

@article{mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@misc{nyxene,
    author= {{Huggingface}},
    year  = {2024},
    title = {Nyxene-v3-11B},
    note  = {\url{https://huggingface.co/beberik/Nyxene-v3-11B},
            Last accessed on 2024-11-05},
}

%orca, palm, phi-1.5, phi-3,

@article{orca,
  title={Orca: Progressive learning from complex explanation traces of gpt-4},
  author={Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
  journal={arXiv preprint arXiv:2306.02707},
  year={2023}
}
@misc{palm,
    author= {{Google AI}},
    year  = {2024},
    title = {PaLM 2 models},
    note  = {\url{https://ai.google.dev/palm_docs/palm},
            Last accessed on 2024-11-05},
}
@article{phi1.5,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}
@article{phi3,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
% qwen, redpajama, shining valiant, stable beluga, 
@misc{qwen,
    author= {{Alibaba Cloud}},
    year  = {2024},
    title = {Qwen},
    note  = {\url{https://qwenlm.github.io/},
            Last accessed on 2024-11-05},
}
@misc{redpajama,
    author= {{together.ai}},
    year  = {2024},
    title = {RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens}},
    note  = {\url{https://www.together.ai/blog/redpajama},
            Last accessed on 2024-11-05},
}
@misc{shiningvaliant,
    author= {{Valiant Labs}},
    year  = {2024},
    title = {Valiant Labs},
    note  = {\url{https://www.valiantlabs.ca/},
            Last accessed on 2024-11-05},
}
@misc{stablebeluga,
    author= {{stability.ai}},
    year  = {2024},
    title = {Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned Language Models},
    note  = {\url{https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models},
            Last accessed on 2024-11-05},
}
%stablelm, starling, una-xaberius, vicuna
@misc{stablelm,
    author= {{Stability AI}},
    year  = {2024},
    title = {StableLM: Stability AI Language Models},
    note  = {\url{https://github.com/Stability-AI/StableLM},
                  Last accessed on 2024-11-05},
}
@misc{starling,
    author= {{ Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Jiantao Jiao}},
    year  = {2024},
    title = {Starling-7B: Increasing LLM Helpfulness \& Harmlessness with RLAIF},
    note  = {\url{https://starling.cs.berkeley.edu/},
            Last accessed on 2024-11-05},
}
@misc{una-xaberius,
    author= {{Huggingface}},
    year  = {2024},
    title = {fblgit/una-xaberius-34b-v1beta},
    note  = {\url{https://huggingface.co/fblgit/una-xaberius-34b-v1beta},
            Last accessed on 2024-11-05},
}
@misc{vicuna,
    author= {{Vicuna Team}},
    year  = {2023},
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%\* ChatGPT Quality},
    note  = {\url{https://lmsys.org/blog/2023-03-30-vicuna/},
            Last accessed on 2024-11-05},
}

@misc{futokeyboard,
    author= {{futo-org}},
    year  = {2024},
    title = {FUTO Keyboard},
    note  = {\url{https://github.com/futo-org/android-keyboard?tab=readme-ov-file},
            Last accessed on 2024-11-05},
}

@misc{amallo,
    author= {{Mike Thongvanh}},
    year  = {2024},
    title = {amallo},
    note  = {\url{https://github.com/mthongvanh/amallo},
            Last accessed on 2024-11-05},
}

@misc{aistudyassistant,
    author= {{Mohamed Hassan}},
    year  = {2024},
    title = {AI Study Assistant},
    note  = {\url{https://github.com/mhss1/AIStudyAssistant},
            Last accessed on 2024-11-05},
}

@misc{ChatGPT-android-app,
    author= {{matt haigh}},
    year  = {2024},
    title = {ChatGPT App},
    note  = {\url{https://github.com/matthaigh27/ChatGPT-android-app},
            Last accessed on 2024-11-05},
}

@misc{LocalGPT-Android,
    author= {{ronith256}},
    year  = {2024},
    title = {LocalGPT-Android},
    note  = {\url{https://github.com/ronith256/LocalGPT-Android},
            Last accessed on 2024-11-05},
}

@inproceedings{wang2017understanding,
  title={Understanding third-party libraries in mobile app analysis},
  author={Wang, Haoyu and Guo, Yao},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)},
  pages={515--516},
  year={2017},
  organization={IEEE}
}

@article{salza2020third,
  title={Third-party libraries in mobile apps: When, how, and why developers update them},
  author={Salza, Pasquale and Palomba, Fabio and Di Nucci, Dario and De Lucia, Andrea and Ferrucci, Filomena},
  journal={Empirical Software Engineering},
  volume={25},
  pages={2341--2377},
  year={2020},
  publisher={Springer}
}

@inproceedings{wang2020empirical,
  title={An empirical study of usages, updates and risks of third-party libraries in java projects},
  author={Wang, Ying and Chen, Bihuan and Huang, Kaifeng and Shi, Bowen and Xu, Congying and Peng, Xin and Wu, Yijian and Liu, Yang},
  booktitle={2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={35--45},
  year={2020},
  organization={IEEE}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{yang2024if,
  title={If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents},
  author={Yang, Ke and Liu, Jiateng and Wu, John and Yang, Chaoqi and Fung, Yi R and Li, Sha and Huang, Zixuan and Cao, Xu and Wang, Xingyao and Wang, Yiquan and others},
  journal={arXiv preprint arXiv:2401.00812},
  year={2024}
}

@inproceedings{fan2023large,
  title={Large language models for software engineering: Survey and open problems},
  author={Fan, Angela and Gokkaya, Beliz and Harman, Mark and Lyubarskiy, Mitya and Sengupta, Shubho and Yoo, Shin and Zhang, Jie M},
  booktitle={2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)},
  pages={31--53},
  year={2023},
  organization={IEEE}
}

@article{meyer2023chatgpt,
  title={ChatGPT and large language models in academia: opportunities and challenges},
  author={Meyer, Jesse G and Urbanowicz, Ryan J and Martin, Patrick CN and O’Connor, Karen and Li, Ruowang and Peng, Pei-Chen and Bright, Tiffani J and Tatonetti, Nicholas and Won, Kyoung Jae and Gonzalez-Hernandez, Graciela and others},
  journal={BioData Mining},
  volume={16},
  number={1},
  pages={20},
  year={2023},
  publisher={Springer}
}

@misc{appflowy,
    author= {{AppFlowy-IO}},
    year  = {2024},
    title = {AppFlowy.IO},
    note  = {\url{https://github.com/AppFlowy-IO/AppFlowy},
            Last accessed on 2024-11-05},
}

@misc{chatair,
    author= {{flyun}},
    year  = {2024},
    title = {ChatAir},
    note  = {\url{https://github.com/flyun/chatAir},
            Last accessed on 2024-11-05},
}

@misc{oldgpt,
    author= {{itsskyballs}},
    year  = {2024},
    title = {OldGPT},
    note  = {\url{https://github.com/itsskyballs/OldGPT},
            Last accessed on 2024-11-05},
}

@misc{maid,
    author= {{Mobile Artificial Intelligence Distribution}},
    year  = {2024},
    title = {Maid},
    note  = {\url{https://github.com/Mobile-Artificial-Intelligence/maid},
            Last accessed on 2024-11-05},
}

@misc{WizGPT,
    author= {{virtualwiz1}},
    year  = {2024},
    title = {WizGPT},
    note  = {\url{https://github.com/virtualwiz1/wizGPT-flutter},
            Last accessed on 2024-11-05},
}

@misc{cloc,
    author= {{Al Danial}},
    year  = {2024},
    title = {cloc},
    note  = {\url{https://github.com/AlDanial/cloc},
            Last accessed on 2024-11-05},
}

@misc{rep-pkg,
    title = {Replication Packges},
    note = {\url{https://github.com/kimberlyhau/LLM-enabledAndroidAppsReplicationPackage}}
}

@misc{chatgptratelimits,
    author= {{OpenAI}},
    year  = {2024},
    title = {Rate limits},
    note  = {\url{https://platform.openai.com/docs/guides/rate-limits},
            Last accessed on 2024-11-05},
}

\end{thebibliography}
\end{document}
