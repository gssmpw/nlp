\section{Method}
\label{sec:method}


\Cref{fig:method_overview} shows an overview of DMAD structure. Sensor embeddings are extracted from multi-view camera images and are shared across all tasks, including detection, tracking, mapping, prediction, and planning. We initialize three distinct types of queries—object, map, and motion—which attend to the sensor embeddings to extract the specific information required for each respective task. Based on the type of information learned, the decoding process is divided into two pathways. On one way, object and map decoding are jointly performed within the \textbf{Interactive semantic decoder}, where both types of queries iteratively exchange latent semantic information at each decoding layer. On the other way, motion queries extract motion information from the sensor embeddings within the \textbf{Neural-Bayes motion decoder}. Each motion query is paired with an object query, using the object’s coordinates as a reference point at each decoding layer. After decoding each frame, the motion query's predicted future waypoint becomes the object query's reference point in the next frame, similar to the recursion of a Bayes filter~\cite{thrun2005probabilistic}. The exchange of reference points is always without gradient. At last, the motion queries are passed on to the planning module. The system is fully \gls{E2E} trainable, with motion and semantic gradients propagated in distinct paths.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/interactive_decoder.pdf}
    \caption{\textbf{Interactive semantic decoding.} Object and map queries are concatenated and interact through a self-attention module before being separated to independently attend to the sensor embeddings. This process is repeated across $N$ stacked layers.}
    \label{fig:interactive_decoder}
\end{figure}

\subsection{Interactive Semantic Decoder}
\label{sec:interactive_semantic_decoder}


To leverage the semantic correlation between individual objects and map elements, we introduce the Interactive Semantic Decoder. In contrast to the unidirectional interaction in \mbox{DualAD}~\cite{doll2024dualad}, our approach enables a bidirectional exchange of information.

We initialize a set of object queries $Q_{\text{obj}} \in \mathbb{R}^{N_{\text{obj}} \times d}$ and a set of map queries $Q_{\text{map}} \in \mathbb{R}^{N_{\text{map}} \times d}$. The number of queries could be different, while the dimensions $d$ must be the same. Each decoding layer first concatenates both types of queries. Self-attention \cite{vaswani2017attention} is then applied, where both tasks exchange their semantic information. Subsequently, the two types of queries are divided, each performing self-attention and cross-attention on the sensor embeddings, respectively, as shown in \cref{fig:interactive_decoder}. 

After interactive semantic decoding, each object query is classified into a category $c$ and regressed into a vector $\transpose{[\Delta x, \Delta y, \Delta z, w, h, l, \theta]}$. The object query is associated with a reference point $\transpose{[x_{\text{ref}}, y_{\text{ref}}, z_{\text{ref}}]}$. Rather than directly learning the absolute coordinates of the object, it learns the offsets relative to its corresponding reference points. Thus, the bounding boxes can be represented as $\transpose{[x_{\text{ref}} + \Delta x, y_{\text{ref}} + \Delta y, z_{\text{ref}} + \Delta z, w, h, l, \theta]}$. Notably, velocities are not regressed, as they pertain to motion information. We design the object queries to focus solely on semantic information, \ie, the object’s category, center point, size, and orientation. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/motion_decoder.pdf}
    \caption{\textbf{Neural-Bayes motion decoding.} After each decoding layer, the semantic decoder updates the reference points, which are then shared with the motion decoder. At the end of each frame, positive object query indices are used to select corresponding motion queries and are together propagated to the subsequent frame, with the motion query predictions serving as reference points for the next frame. This process is similar to the measurement, updating, and prediction steps in a Bayes filter.
    Map queries, ego queries and sensor embeddings are omitted for simplicity.}
    \label{fig:motion_decoder}
\end{figure}

\subsection{Neural-Bayes Motion Decoder}
\label{sec:neural-bayes_motion_decoder}

We introduce a novel motion decoder operating in parallel with the semantic decoder, aimed at fully decoupling motion and semantic learning to reduce the negative transfer in semantic tasks. Given the correlation between motion and semantics, we design a recursive process to facilitate the exchange of human-readable information between the two decoders as illustrated in \cref{fig:motion_decoder}, which comprises the processes of prediction, measurement, and updating, similar to the Bayes filter \cite{thrun2005probabilistic}. \Cref{app:bayes_filter} in the supplementary material provides a brief introduction to the Bayes filter. We proceed with the elaboration of the proposed motion decoder.\\

\textbf{Initialization.} We initialize a set of motion queries $Q_{\text{mt}} \in \mathbb{R}^{N_{\text{mt}} \times d}$ in the same way we initialize object queries. The motion queries correspond one-to-one with the object queries, \ie, $N_{\text{mt}} = N_{\text{obj}}$. However, since they do not directly interact in the latent space, their dimensionalities $d$ can differ. Each motion query represents the motion state of an object, although the model does not initially know whether the object exists. Additionally, motion queries and object queries share a common set of reference points. \\

\begin{table}[t]
\resizebox{1.0\linewidth}{!}{
\small
\centering
\begin{tabular}{@{}l >{\columncolor[HTML]{EFEFEF}}l lll@{}}
\toprule
Method & NDS↑ & mAP↑ & mAVE↓ \\ \midrule
VAD~\cite{jiang2023vad} & 0.460 & 0.330 & 0.405 \\
GenAD~\cite{zheng2025genad} & 0.280 & 0.213 & 0.669 \\
PARA-Drive~\cite{weng2024drive} & 0.480 & 0.370 & - \\ \midrule
UniAD - stage 1 & 0.497 & 0.382 & 0.411 \\
UniAD - stage 2 & 0.491 \color[HTML]{CB0000} (-1.2\%) & 0.377 \color[HTML]{CB0000} (-1.3\%) & 0.412 \color[HTML]{CB0000} (+0.2\%) \\ \midrule
DMAD - stage 1 & 0.504 & 0.395 & 0.406 \\
DMAD - stage 2 & 0.506 \color[HTML]{3166FF} (+0.4\%) & 0.396 \color[HTML]{3166FF} (+0.3\%) & 0.395 \color[HTML]{3166FF} (-2.7\%) \\ \midrule
SparseDrive - stage 1 & 0.531 & 0.419 & \underline{0.257} \\
SparseDrive - stage 2 & 0.523 \color[HTML]{CB0000} (-1.5\%) & 0.417 \color[HTML]{CB0000} (-0.5\%) & 0.269 \color[HTML]{CB0000} (+4.7\%) \\ \midrule
SparseDMAD - stage 1 & \textbf{0.536} & \underline{0.424} & 0.260 \\
SparseDMAD - stage 2 & \underline{0.534} \color[HTML]{CB0000} (-0.4\%) & \textbf{0.427} \color[HTML]{3166FF} (+0.7\%) & \textbf{0.253} \color[HTML]{3166FF} (-2.7\%) \\ \bottomrule
\end{tabular}
}
\caption{\textbf{Object detection results.} The performance changes in stage 2 are expressed as percentages, with {\color[HTML]{CB0000}red} indicating a decline and {\color[HTML]{3166FF}blue} representing improvement.} 
\label{tab:detection}
\end{table}


\textbf{Measurement.} The detection, already introduced in \cref{sec:interactive_semantic_decoder}, is treated as the measurement in Bayes filter. After each semantic decoding layer, the object queries are regressed, yielding the coordinate vectors $\mathbf{ref} = \transpose{[x, y, z]}$ of the tentative object, which then serves as reference points for the next layer:
\begin{equation}
  \mathbf{ref}^{l+1} = f_{\text{reg}} ( f_{\text{Semantic-Dec}}^l (Q_{\text{obj}}^l, Z, \mathbf{ref}^{l}) ),
\end{equation}
where the superscript denotes the layer and $Z$ is the sensor embeddings. \\

\textbf{Updating.} With the reference points $\mathbf{ref}^{l}$ from the semantic decoding (the inter-layer reference points update in \cref{fig:method_overview}), the motion queries also attend to the sensor embeddings via cross-attention:
\begin{equation}
  Q_{\text{mt}}^{l+1} = f_{\text{Motion-Dec}}^l (Q_{\text{mt}}^l, Z, \mathbf{ref}^{l}),
\end{equation}
where the motion queries are updated conditioned on the measured reference points. \\

\textbf{Prediction.} We employ MLPs to extract trajectories from the motion queries. We note that motion extraction occurs in two stages: first through the unimodal trajectory construction, followed by the multimodal prediction. 

The first stage computes the unimodal velocity and future reference points, guiding the motion query to learn aggregated motion states from the past and predict the near future. It produces a single trajectory that spans from the past timestep $ t_{\text{past}} $ to the future timestep $ t_{\text{fut-1}} $. The velocity is calculated using the finite difference method on waypoints around the current timestep. We use the first future waypoint as the initial reference point for the object query in the next frame, \ie, inter-frame reference points update in \cref{fig:method_overview}, for object tracking. 

The second stage performs multimodal intention modeling and generates multiple future trajectories within the future $ t_{\text{fut-2}} $ timesteps, along with their corresponding confidence scores. 
\\

\textbf{Tracking.} Multi-object tracking is performed using the query propagation mechanism~\cite{zeng2022motr, lin2023sparse4d}. Each object query is associated with an unique instance ID. A positive query propagates across consecutive frames, ensuring that corresponding detections are assigned the same ID. During training, object queries associated with ground truth are referred to as positive queries; during inference, positivity is determined by whether the confidence score exceeds a specified threshold. The propagation of motion queries follows that of object queries, as they are related. This mechanism enables continuous measuring, updating, and predicting, similar to the Bayes filter. 

\begin{table}[t]
\centering
\resizebox{1.0\linewidth}{!}{
\small
\begin{tabular}{@{}l
>{\columncolor[HTML]{EFEFEF}}l ll@{}}
\toprule
Method & AMOTA↑ & AMOTP↓ & IDS↓ \\ \midrule
ViP3D~\cite{gu2023vip3d} & 0.217 & 1.63 & - \\
MUTR3D~\cite{zhang2022mutr3d} & 0.294 & 1.50 & 3822 \\
PARA-Drive~\cite{weng2024drive} & 0.350 & - & - \\ \midrule
UniAD - stage 1 & 0.374 & 1.31 & 816 \\
UniAD - stage 2 & 0.354 \color[HTML]{CB0000} (-5.3\%) & 1.34 \color[HTML]{CB0000} (+2.3\%) & 1381 \color[HTML]{CB0000} (+69\%) \\ \midrule
DMAD - stage 1 & 0.394 & 1.32 & 781 \\
DMAD - stage 2 & 0.393 \color[HTML]{CB0000} (-0.3\%) & 1.30 \color[HTML]{3166FF} (-1.5\%) & 767 \color[HTML]{3166FF} (-1.8\%) \\ \midrule
SparseDrive - stage 1 & \underline{0.395} & \underline{1.25} & 602 \\
SparseDrive - stage 2 & 0.376 \color[HTML]{CB0000} (-4.8\%) & 1.26 \color[HTML]{CB0000} (+0.8\%) & \textbf{559} \color[HTML]{3166FF} (-7.1\%) \\ \midrule
SparseDMAD - stage 1 & \textbf{0.396} & \textbf{1.23} & 608 \\
SparseDMAD - stage 2 & \underline{0.395} \color[HTML]{CB0000} (-0.3\%) & \textbf{1.23} \color[HTML]{3166FF} (0\%) & \underline{571} \color[HTML]{3166FF} (-6.1\%) \\ \bottomrule
\end{tabular}
}
\caption{\textbf{Multi-object tracking results.}}
\label{tab:tracking}
\end{table}
