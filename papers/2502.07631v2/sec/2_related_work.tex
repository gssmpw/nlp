\section{Related Work}
\label{sec:related_work}

\textbf{Semantic learning.} Semantic learning includes object detection and map segmentation. Multi-view cameras have become popular due to their cost-effectiveness and strong capability in capturing semantic information. Current SOTA object detection and mapping approaches are built on the DETR~\cite{carion2020end} architecture, utilizing a set of queries to extract semantic information from environment features through cross-attention~\cite{vaswani2017attention} mechanisms. Sparse methods \cite{wang2022detr3d, lin2022sparse4d} learn semantic information by projecting queries onto the corresponding image features, focusing on the relevant regions. The PETR series \cite{liu2022petr, liu2023petrv2, wang2023exploring} embed 3D positional encoding directly into 2D image features, eliminating the need for query projection. Another line of work aggregates all image features into a \gls{BEV} feature \cite{philion2020lift, li2022bevformer, yang2023bevformer, pan2024clip, liao2023maptr, maptrv2}. Propagating the object queries over time enables multi-object tracking \cite{zeng2022motr, meinhardt2022trackformer}. This same technique is also used in map perception \cite{chen2025maptracker}. Although tracking is also a motion-related task, we classify it as a semantic task, as query-based trackers learn only velocities as the motion information, which we elaborate in the supplementary material \cref{app:tracking}. \\


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/method_overview.pdf}
    \caption{\textbf{An overview of DMAD.} A backbone processes multi-view images into sensor embeddings. Map and object queries are initialized, then interactively attend to the sensor embeddings for map and object perception. Motion queries, mapped one-to-one with object queries, share reference points that are iteratively updated. Finally, motion queries corresponding to detected objects are decoded into future trajectories. The ego motion query (``e'') is used for planning. Gray dashed lines indicate operations without gradient flow.}
    \label{fig:method_overview}
\end{figure*}

\textbf{Motion learning.} By motion, we refer to trajectory prediction and planning. Trajectory prediction studies typically use the ground truth of objects' historical trajectories along with high-definition maps as inputs. Early approaches \cite{chai2019multipath, cui2019multimodal, bansal2018chauffeurnet} rasterize maps and trajectories into a \gls{BEV} image, using CNNs to extract scene features. Vectorized methods \cite{gao2020vectornet, zhou2022hivt} represent elements using polygons and polylines, using GNNs or Transformers to encode the scene \cite{ngiam2021scene, wagner2024redmotion, shi2022motion, gu2021densetnt, zhang2024simpl}. 

For planning, imitation learning is a straightforward approach to \gls{E2E} planning, where a neural network is trained to plan future trajectories or control signals directly from sensor data, minimizing the distance between the planned path and the expert driving policy \cite{bojarski2016end, prakash2021multi, chen2022learning}. Many approaches incorporate semantic tasks as auxiliary components to support \gls{E2E} planning, using the nuScenes~\cite{caesar2020nuscenes} dataset and open-loop evaluation. These methods go beyond pure motion learning and are presented in the next paragraph. 
AD-MLP~\cite{zhai2023rethinking} and Ego-MLP~\cite{li2024ego} utilize only the ego vehicle's past motion states and surpass methods that rely on sensor inputs in open-loop evaluation. It aligns with our argument that semantics and motion are heterogeneous: AD-MLP and Ego-MLP can concentrate on learning from expert motion data without interference by irrelevant semantic information, thereby achieving superior open-loop planning performance.\\

\textbf{Joint semantic and motion learning.}
\Gls{E2E} perception and prediction approaches learn semantics and motion jointly. The pioneering work FaF~\cite{luo2018fast} uses a prediction head, in addition to the detection head, to decode the object features into future trajectories. Some works \cite{casas2018intentnet, djuric2021multixnet, fadadu2022multi} enhance it with intention-based prediction and refinement. PnPNet~\cite{liang2020pnpnet} and PTP~\cite{weng2021ptp} involve tracking, \ie, jointly optimizing detection, association, and prediction tasks. While PTP performs tracking and prediction in parallel, it cannot predict newly emerging objects due to the lack of concurrent detectionâ€”a limitation our method successfully overcomes. ViP3D~\cite{gu2023vip3d} first extends the query-based detection and tracking framework \cite{zeng2022motr} to prediction. Each query represents an object and propagates across frames. In each frame, queries are decoded into bounding boxes and trajectories using high-definition maps as additional context. 

To include planning, NMP~\cite{zeng2019end} extends IntentNet~\cite{casas2018intentnet} with a sampling-based planning module, where prediction is leveraged to minimize collisions during the planning process. Other works, such as \cite{chitta2021neat, casas2021mp3, hu2022st}, incorporate map perception as an auxiliary task. With the growing popularity of query-based object detectors \cite{carion2020end, li2022bevformer} and trackers \cite{zeng2022motr, meinhardt2022trackformer}, recent modular \gls{E2E} \gls{AD} approaches represent objects as queries, similar to ViP3D~\cite{gu2023vip3d}. UniAD~\cite{hu2023planning} and its variants \cite{doll2024dualad, weng2024drive} retain the query propagation mechanism for tracking, aiming to explicitly model objects' historical motion. In contrast, VAD~\cite{jiang2023vad} and GenAD~\cite{zheng2025genad} do not perform tracking, predicting trajectories based on the temporal information embedded within the \gls{BEV} feature. The main issue with these methods is that they attempt to use a single feature (query) to represent an object's appearance and motion. Compared to pure semantic learning, motion occupies a portion of the feature channels but fails to contribute to perception, resulting a negative transfer in the perception module. Our work effectively addresses this issue.
