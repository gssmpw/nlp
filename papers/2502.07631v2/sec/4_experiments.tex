\section{Experiments}
\label{sec:experiments}

We conduct experiments on the nuScenes~\cite{caesar2020nuscenes} dataset to validate the effectiveness of our method. We present results in three parts. The first part focuses on perception (detection, tracking, and mapping).
In the second part, we evaluate motion prediction and planning.
Lastly, we provide an extensive ablation study and SHAP values~\cite{NIPS2017_7062} visualization.

\subsection{Training Configuration} 
\label{subsec:training_configuration}
We reproduce UniAD~\cite{hu2023planning} and SparseDrive~\cite{sun2024sparsedrive} as baselines. Both utilize the query propagation mechanism; however, UniAD extracts dense \gls{BEV} features from image inputs, while SparseDrive employs sparse scene representations. Beside the aforementioned tasks, UniAD additionally performs occupancy prediction. We also retain the occupancy module in comparisons with UniAD for task consistency. As occupancy prediction serves merely as another representation of upstream tasks, we describe it in the supplementary material \cref{app:occupancy}.
We adhere as closely as possible to default configurations of the baseline; however, to ensure a rigorous comparisons, some adjustments are made. Following paragraphs outline the adjustments and the rationale behind them.\\


\begin{table}[t]
\centering
\small
\begin{subtable}{\linewidth}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}l
>{\columncolor[HTML]{EFEFEF}}l ll@{}}
\toprule
Method                                                 & Lanes↑ & Drivable↑      & Dividers↑ \\ \midrule
BEVFormer~\cite{li2022bevformer} & 0.239  & \textbf{0.775} & -         \\ 
PARA-Drive~\cite{weng2024drive} & \textbf{0.330}  & \underline{0.710} & -         \\ 
\midrule
UniAD - stage 1                                        & 0.293  & 0.650          & 0.248     \\
UniAD - stage 2 &
  0.312 \color[HTML]{3166FF} (+6.5\%) &
  0.678 \color[HTML]{3166FF} (+4.3\%) &
  \underline{0.267} \color[HTML]{3166FF} (+7.7\%) \\ \midrule
DMAD - stage 1                                         & 0.292  & 0.655          & 0.242     \\
DMAD - stage 2 &
  \text{\underline{0.321} \color[HTML]{3166FF} (+9.9\%)} &
  \text0.691 \color[HTML]{3166FF} (+5.5\%) &
  \textbf{0.271} \color[HTML]{3166FF} (+12\%) \\ \bottomrule
\end{tabular}
}
\caption{Map segmentation results.}
\end{subtable}

\begin{subtable}{\linewidth}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}llll
>{\columncolor[HTML]{EFEFEF}}l }
\toprule
Method & $\text{AP}_\text{ped}$↑ & $\text{AP}_\text{divider}$↑ & $\text{AP}_\text{boundary}$↑ & mAP↑ \\ \midrule
MapTR~\cite{liao2023maptr} & \textbf{0.562} & 0.598 & \underline{0.601} & \textbf{0.587} \\
VAD~\cite{jiang2023vad} & 0.406 & 0.515 & 0.506 & 0.476 \\ \midrule
SparseDrive - stage 1 & 0.533 & 0.579 & 0.575 & 0.562 \\
SparseDrive - stage 2 & 0.494 \color[HTML]{CB0000} (-7.3\%) & 0.569 \color[HTML]{CB0000} (-1.7\%) & 0.583 \color[HTML]{3166FF} (+1.4\%) & 0.549 \color[HTML]{CB0000} (-2.3\%) \\ \midrule
SparseDMAD - stage 1 & 0.553 & \underline{0.599} & \textbf{0.606} & \underline{0.586} \\
SparseDMAD - stage 2 & \underline{0.554} \color[HTML]{3166FF} (+0.2\%) & \textbf{0.601} \color[HTML]{3166FF} (+0.3\%) & \textbf{0.606} \color[HTML]{3166FF} (0\%) & \textbf{0.587} \color[HTML]{3166FF} (+0.2\%) \\ \bottomrule
\end{tabular}
}
\caption{Vectorized mapping results.}
\end{subtable}
\caption{\textbf{Map perception results.}} 
\label{tab:mapping}
\end{table}

\textbf{Two-stage training.} We follow the two-stage training scheme of our baseline. In the first stage, we train object detection, tracking, and mapping. In the second stage, we train all modules together. Notably, because our tracking relies on reference points provided by unimodal prediction, we incorporate unimodal prediction training in the first stage. Multimodal prediction is trained only in the second stage, which is consistent with the baseline.\\

\textbf{Queue length.} Since \gls{AD} is a time-dependent task, the model typically processes a sequence of consecutive frames as a training sample. The number of input frames, \ie, the queue length $q$, defines the temporal horizon the model can capture, impacting the performance of related tasks. UniAD employs different queue lengths across its two training stages: 5 in the first stage and 3 in the second. The reduced queue length in the second stage degrades perception performance due to reduced temporal aggregation, shown in \cref{app:queue} in the supplementary material.
This degrading hinders the identification of negative transfer effects caused by the sequential structure. To mitigate this interference, we standardize the queue length to 3 across both training stages in comparisons with UniAD. Unless otherwise specified, the performance of UniAD in all result tables is reproduced with a queue length of 3 using the official codebase~\cite{contributors2023_uniadrepo}. SparseDrive does not have this issue, and we use the default setting of 4. \\

\textbf{Ego query} represents the features directly used for motion planning, which is intended to capture the motion information of the ego vehicle. SparseDrive generates the ego query from the front camera image and the estimated previous ego status, which blends semantics and motion, thus contradicting our dividing design. To align with our proposal, we eliminate the use of the front image for the ego query when applying DMAD to SparseDrive. For UniAD, we retain the planning module unchanged, as it initializes the ego query randomly.


\begin{table}[t]
\centering
\resizebox{1.0\linewidth}{!}{
\small
\begin{tabular}{@{}l
>{\columncolor[HTML]{EFEFEF}}l 
>{\columncolor[HTML]{EFEFEF}}l ll@{}}
\toprule
Method                                              & \multicolumn{2}{c}{\cellcolor[HTML]{EFEFEF}EPA↑} & \multicolumn{2}{c}{minADE↓} \\
 &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}C} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}P} &
  \multicolumn{1}{c}{C} &
  \multicolumn{1}{c}{P} \\ \midrule
ViP3D~\cite{gu2023vip3d} & 0.226 & - & 2.05 & - \\
GenAD~\cite{zheng2025genad} & \textbf{0.588} & 0.352 & 0.84 & 0.84 \\ \midrule
UniAD & 0.495 & 0.361 & \underline{0.69} & \underline0.79 \\
DMAD & \underline{0.535} \color[HTML]{3166FF} (+8.1\%) & \textbf{0.416} \color[HTML]{3166FF} (+15\%) & 0.72 \color[HTML]{CB0000} (+4.3\%) & 0.77 \color[HTML]{3166FF} (-2.5\%) \\ \midrule
SparseDrive & 0.487 & 0.406 & \textbf{0.63} & \underline{0.73} \\
SparseDMAD & 0.500 \color[HTML]{3166FF} (+2.7\%) & \underline{0.410} \color[HTML]{3166FF} (+1.0\%) & \textbf{0.63} \color[HTML]{3166FF} (0\%) & \textbf{0.71} \color[HTML]{3166FF} (-2.7\%) \\ \bottomrule
\end{tabular}
}
\caption{\textbf{Trajectory prediction results.} C and P stand for cars and pedestrians respectively. We indicate improvements to the baseline with {\color[HTML]{3166FF}blue}, and declines with {\color[HTML]{CB0000}red}.}
\label{tab:prediction}
\end{table}


\subsection{Perception}
\textbf{Metrics.} For object detection and tracking, we use the metrics defined in the nuScenes benchmark. The primary metrics for detection are nuScenes Detection Score (NDS) and mean average precision (mAP). For multiple object tracking, we report the average multi-object tracking accuracy (AMOTA) and the average multi-object tracking precision (AMOTP).
For map segmentation, we use the intersection over union (IoU) metric of drivable areas, lanes, and dividers. Vectorized mapping adopts mAP of lane divider, pedestrian crossing and road boundary.\\

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{@{}lccccclcccl}
\toprule
 &
  \multicolumn{1}{l}{Perception} &
  \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Ego states\end{tabular}} &
  \multicolumn{4}{c}{$L_2$ distances (m) ↓} &
  \multicolumn{4}{c}{Collision rates (\%) ↓} \\
Method & tasks
   & in planner
   &
  1s &
  2s &
  3s &
  \cellcolor[HTML]{EFEFEF}Avg. &
  1s &
  2s &
  3s &
  \cellcolor[HTML]{EFEFEF}Avg. \\ \midrule
{\color[HTML]{808080} Ego-MLP~\cite{zhai2023rethinking}} &
  \color[HTML]{808080}\ding{55} &
  \color[HTML]{808080}\ding{51} &
  {\color[HTML]{808080} 0.17} &
  {\color[HTML]{808080} 0.34} &
  {\color[HTML]{808080} 0.60} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{808080} 0.370} &
  {\color[HTML]{808080} 0\textsuperscript{\textdagger}} &
  {\color[HTML]{808080} 0.27\textsuperscript{\textdagger}} &
  {\color[HTML]{808080} 0.85\textsuperscript{\textdagger}} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{808080} 0.373\textsuperscript{\textdagger}} \\
{\color[HTML]{808080} AD-MLP~\cite{li2024ego}} &
  \color[HTML]{808080}\ding{55} &
  \color[HTML]{808080}\ding{51} &
  {\color[HTML]{808080} 0.14} &
  {\color[HTML]{808080} 0.10} &
  {\color[HTML]{808080} 0.41} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{808080} 0.217} &
  {\color[HTML]{808080} 0.10} &
  {\color[HTML]{808080} 0.10} &
  {\color[HTML]{808080} 0.17} &
  \cellcolor[HTML]{EFEFEF}{\color[HTML]{808080} 0.123} \\ \midrule
VAD~\cite{jiang2023vad} & \ding{51} & \ding{55} & 0.41 & 0.70 & 1.05 & \cellcolor[HTML]{EFEFEF}0.720 & 0.07 & 0.17 & 0.41 & \cellcolor[HTML]{EFEFEF}0.217 \\
DualVAD~\cite{doll2024dualad} & \ding{51} & \ding{55} & 0.30 & 0.53 & 0.82 & \cellcolor[HTML]{EFEFEF}0.550 & 0.11 & 0.19 & 0.36 & \cellcolor[HTML]{EFEFEF}0.220 \\
GenAD~\cite{zheng2025genad} & \ding{51} & \ding{55} & \underline{0.28} & \underline{0.49} & \underline{0.78} & \cellcolor[HTML]{EFEFEF}\underline{0.517} & 0.08 & 0.14 & 0.34 & \cellcolor[HTML]{EFEFEF}0.187 \\
UniAD*~\cite{hu2023planning} & \ding{51} & \ding{55} & 0.42 & 0.63 & 0.91 & \cellcolor[HTML]{EFEFEF}0.656 & 0.07 & 0.10 & 0.22 & \cellcolor[HTML]{EFEFEF}0.130 \\
PARA-Drive~\cite{weng2024drive} & \ding{51} & \ding{55} & \textbf{0.25} & \textbf{0.46} & \textbf{0.74} & \cellcolor[HTML]{EFEFEF}\textbf{0.483} & 0.14 & 0.23 & 0.39 & \cellcolor[HTML]{EFEFEF}0.253 \\ \midrule
UniAD & \ding{51} & \ding{55} & 0.48 & 0.76 & 1.12 & \cellcolor[HTML]{EFEFEF}0.784 & 0.07 & 0.11 & 0.27 & \cellcolor[HTML]{EFEFEF}0.150 \\
DMAD & \ding{51} & \ding{55} & 0.38 & 0.60 & 0.89 & \cellcolor[HTML]{EFEFEF}{0.625 \color[HTML]{3166FF} (-20\%)} & 0.07 & 0.12 & \textbf{0.19} & \cellcolor[HTML]{EFEFEF}{0.127 \color[HTML]{3166FF} (-15\%)} \\ \midrule
SparseDrive & \ding{51} & \ding{55} & 0.32 & 0.61 & 1.00 & \cellcolor[HTML]{EFEFEF}0.643 & \underline{0.01} & \textbf{0.06} & 0.22 & \cellcolor[HTML]{EFEFEF}\underline{0.097} \\
SparseDMAD & \ding{51} & \ding{55} & 0.30 & 0.61 & 1.01 & \cellcolor[HTML]{EFEFEF}{0.643 \color[HTML]{3166FF} (0\%)} & \textbf{0} & \underline{0.07} & \underline{0.21} & \cellcolor[HTML]{EFEFEF}{\textbf{0.093} \color[HTML]{3166FF} (-4.1\%)} \\ \bottomrule
\end{tabular}
\caption{\textbf{Open-loop planning.} Ego-MLP and AD-MLP are faded since both learn only the ego motion. *Results from the checkpoint in the official repository~\cite{contributors2023_uniadrepo}, trained with a queue length of 5 in stage 1. \textdagger Ego-MLP employs a different strategy in the evaluation of collision rates, therefore the results are not comparable.}
\label{tab:planning}
\end{table*}

\textbf{Object detection.} \Cref{tab:detection} presents the detection performance across two training stages. In the first stage, thanks to the interactive semantic decoding, our approach slightly outperforms the baseline. After the second stage of training, baseline's performance shows a decline. In contrast, our method preserves the perceptual performance of the first stage, benefiting from separated motion learning that mitigates negative transfer. Our method finally surpasses UniAD and SparseDrive by 3.1\% and 2.1\% in NDS, respectively.\\


\textbf{Multi-object tracking.} 
Due to using a single feature vector to represent semantics and motion, UniAD and SparseDrive exhibit negative transfer of 5.3\% and 4.8\% in AMOTA, as shown in \cref{tab:tracking}. 
Our dividing design enables object queries to learn about appearance more effectively. At the same time, unimodal predictions offer enhanced tracking reference points. Consequently, our method achieves a gain of 11.0\% and 5.1\% in AMOTA, respectively. \\


\textbf{Map perception.} UniAD does not encounter negative transfer in map segmentation. Leveraging the advantages of interactive semantic decoding, our method marginally surpasses UniAD. Our method mitigates the negative transfer in vectorized online mapping, significantly surpassing SparseDrive by 7.0\% in mAP, \mbox{(see \cref{tab:mapping})}.


\subsection{Prediction and Planning}
\textbf{Metrics.} For motion prediction, we utilize \gls{E2E} perception accuracy (EPA) proposed in ViP3D~\cite{gu2023vip3d} as the main metric. We also report the minimum average displacement error (minADE). However, since minADE is a true positive metric, it does not fully capture the predictive capabilities of the \gls{E2E} system, whereas EPA accounts for the number of false positives. For open-loop planning, we utilize $L_2$ distances and collision rates in 1, 2, and 3 seconds.\\

\textbf{Trajectory prediction.} 
We report car and pedestrian prediction metrics in \cref{tab:prediction}. 
Our method surpasses both baselines in EPA, especially achieving improvements of 8.1\% for cars and 15\% for pedestrians over UniAD. 
However, our method does not improve the minADE of cars.
One possible reason is that once detection performance exceeds a certain threshold, further detection improvements often come from reducing false negatives of challenging objects that are either distant or occluded. These hard-to-detect objects typically have limited historical motion data and larger coordinate errors, making them more difficult to predict. A similar issue is observed in UniAD~\cite{hu2023planning}: in the supplementary materials, UniAD-Large substantially surpasses UniAD-Base in EPA (thanks to better detection and tracking performance), yet it falls short of UniAD-Base in minADE.
\\




\begin{table*}[t]
\centering
\resizebox{1.0\linewidth}{!}{
\small
\begin{tabular}{@{}lcccccccccccc@{}}
\toprule
\begin{tabular}[c]{@{}l@{}}Method\\ ID\end{tabular} & \begin{tabular}[c]{@{}l@{}}Interactive \\ semantic dec.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Motion\\ queries\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inter-layer\\ ref. update\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inter-frame\\ ref. update\end{tabular} & \begin{tabular}[c]{@{}l@{}}\#Params\\ (M)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Inference\\ time (s)\end{tabular} & NDS↑ & AMOTA↑ & Lanes↑ & EPA↑ & Avg. $L_2$↓ & Avg. Col.↓ \\ \midrule
1 (UniAD) & \ding{55} & \ding{55} & \ding{55} & \ding{55} & 135.0 & 0.47 & 0.491 & 0.354 & 0.312 & 0.495 & 0.784 & 0.150 \\
2 & \ding{51} & \ding{55} & \ding{55} & \ding{55} & 135.1 & 0.48 & 0.503 & 0.382 & 0.320 & 0.524 & 0.683 & 0.150 \\
3 & \ding{55} & \ding{51} & \ding{51} & \ding{51} & 140.3 & 0.49 & 0.502 & 0.387 & 0.313 & \textbf{0.535} & 0.661 & 0.143 \\
4 & \ding{51} & \ding{51} & \ding{55} & \ding{55} & 140.4 & 0.49 & 0.481 & 0.339 & 0.322 & 0.485 & 0.655 & 0.163 \\
5 & \ding{51} & \ding{51} & \ding{51} & \ding{55} & 140.4 & 0.49 & 0.489 & 0.352 & \textbf{0.323} & 0.498 & 0.648 & 0.160 \\
6 & \ding{51} & \ding{51} & \ding{55} & \ding{51} & 140.4 & 0.49 & 0.495 & 0.364 & 0.319 & 0.512 & 0.631 & 0.137 \\
7 (DMAD) & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 140.4 & 0.49 & \textbf{0.506} & \textbf{0.393} & 0.321 & \textbf{0.535} & \textbf{0.625} & \textbf{0.127} \\ \bottomrule
\end{tabular}
}
\caption{\textbf{Ablation of DMAD.}}
\label{tab:ablation_dmad}
\end{table*}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}{1\linewidth}
        \centering
        \input{figures/normal1}
        \input{figures/normal2}
        \input{figures/normal_diff}
        \caption{SHAP values of UniAD}
        \label{fig:shap_uniad}
    \end{subfigure}
    \begin{subfigure}{1\linewidth}
        \centering
        \input{figures/dmad1}
        \input{figures/dmad2}
        \input{figures/dmad_diff}
        \caption{SHAP values of DMAD}
        \label{fig:shap_dmad}
    \end{subfigure}
    \label{fig:shap}
    \caption{\textbf{SHAP values of stage 1 (left), stage 2 (middle), and the difference (right).} Each bar represents the SHAP values of a single feature with respect to different classes. The object query consists of 256 features, forming 256 bars in each chart. The difference is computed as stage 1 minus stage 2, aggregating all classes, where \textcolor{red}{red} indicates a negative value and \textcolor{blue}{blue} signifies a positive value.}
\end{figure*}

\textbf{Open-loop planning.} 
We adopt the evaluation method of VAD~\cite{jiang2023vad}, which accommodates the widest range of models to our knowledge.
We report our results in \cref{tab:planning}. 
Notably, jointly optimizing $L_2$ distances and collision rates proves challenging. While PARA-Drive achieves the lowest $L_2$ distances, it also exhibits the highest collision rates.
We validate that the improvements in perception can be propagated to planning, achieving SOTA collision rates.
Note that the planning module of UniAD explicitly uses perception results to avoid collision, thereby deriving greater benefits from improved perception compared to SparseDrive.





\subsection{Ablation Study}
We ablate our proposed decoders, as shown in \cref{tab:ablation_dmad}, decomposing the motion decoder into three components: motion query, inter-layer, and inter-frame reference point updating. \\

\textbf{Model profile.} In methods with multi-view camera images as inputs, the primary computational cost is concentrated in the image backbone \cite{li2022bevformer}. In contrast, our approach focuses on the decoding component, resulting in minimal impact on model size and inference speed. Compared to UniAD~\cite{hu2023planning}, our decoders add 5.4M parameters and increase inference latency by 0.02 seconds on an \mbox{NVIDIA RTX 6000 Ada.}\\

\textbf{Effect of dividing and merging.} Experiments ID 1, 2, 3, 7 demonstrate the effectiveness of both proposed decoders. The standalone application of the interactive semantic decoder (ID 2) significantly enhances the performance of object detection, tracking, and map segmentation. The standalone application of the Neural-Bayes motion decoder (ID 3) markedly improves prediction and planning. Notably, ID 3 also significantly enhances detection and tracking, attributed to freeing object queries from learning velocities and the higher-quality reference points provided by the unimodal prediction. Experiments ID 4, 5, 6, 7 show the importance of inter-layer and inter-frame updating in the Neural-Bayes motion decoder. An incomplete Neural-Bayes motion decoder (ID 4, 5, 6) degrades detection and tracking results. Only combining both updates enables the model to achieve optimal performance.

\subsection{Visualizations}

We use SHAP values~\cite{NIPS2017_7062} to inspect the negative transfer in detection and tracking.
SHAP values quantify the contribution of each feature to the change in a model's output. %
We visualize the SHAP values of the object query with respect to the object classification output. Changes in SHAP values across the two training stages reveal the negative transfer in UniAD and highlight the effectiveness of our method.


\Cref{fig:shap_uniad} compares the SHAP values between stage 1 and stage 2 of UniAD, sorted in descending order. The left half of the difference bar chart predominantly shows negative values, whereas the right half shows positive values. This indicates that SHAP values in stage 1 are more uniformly distributed, while those in stage 2 are more concentrated. Compared with a flat distribution, this concentration indicates that fewer features are contributing to the classification task, reducing detection and tracking performance.
This observation aligns with our argument that during the second stage, object queries are expected to learn motion information, which does not benefit the perception task. 
Specifically, while the velocity learned in stage 1 is sufficient for tracking (predicting the next timestep), it is inadequate for the long-term prediction over 12 timesteps (6 seconds). 
Therefore, the object query is forced to learn more motion states that offer limited utility for identifying objects, interfering with the space for semantic information. 
In contrast, the SHAP values in DMAD maintain a similar distribution across both stages, as shown in \cref{fig:shap_dmad}.

Beyond SHAP values, we provide qualitative comparisons between DMAD and UniAD in the supplementary material \cref{app:vis}.
