\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Tracking as a Semantic Task}
\label{app:tracking}


We justify the similarity of detection and tracking on nuScenes~\cite{caesar2020nuscenes} by analyzing the information learned by the object query. \Gls{E2E} detection and tracking models decode each query into category, location, size, orientation, and velocity.
The category is clearly a semantic attribute, while location, size, and orientation serve as spatial complements to the category, all being time-invariant. In contrast, velocity is derived from time, making it a motion attribute.
However, measuring velocities is not a common practice in detection, but required by the nuScenes benchmark. Therefore, detection models trained on nuScenes are able to perform tracking without any additional learning effort assuming constant velocity motion \cite{zhang2022mutr3d, hu2023planning, lin2023sparse4d, gu2023vip3d}. 
Given that current modular \gls{E2E} models are all trained on nuScenes, we regard the tracking in these methods closely resembles detection, where learning semantics is dominating. 







\section{Bayes Filter}
\label{app:bayes_filter}
Bayes filter~\cite{thrun2005probabilistic} estimates an unknown distribution based on the process model and noisy measurements as follows:
\begin{equation}
  p(\mathbf{x}_t\; |\; \mathbf{z}_{1:t}) = p(\mathbf{z}_t\; |\; \mathbf{x}_t)\, p(\mathbf{x}_t\; |\; \mathbf{z}_{1:t-1}),
\end{equation}
where $\mathbf{x}$ denotes the state, $\mathbf{z}$ represents the measurement, and the subscript indicates timesteps. The task is to estimate the state $\mathbf{x}_t$ at timestep $t$ given all the measurements $\mathbf{z}_{1:t}$ in the past from timestep $1$ to $t$, which is the product of the likelihood $p(\mathbf{z}_t\; |\; \mathbf{x}_t)$ and the prediction $p(\mathbf{x}_t\; |\; \mathbf{z}_{1:t-1})$.

Some special cases of Bayes filter, \eg, Kalman filter, are widely used in traditional object tracking. The tracking process can be carried out in three steps: first, predicting the current position based on the object's historical states $\mathbf{x}_{1:t-1}$; second, identifying the detection most likely to match the prediction as the measurement; finally, updating the current state $\mathbf{x}_{t}$ according to the latest measurement $\mathbf{z}_{t-1}$. This process is recursively executed over successive timesteps. We find semantics and motion are similar to the measurement and state in Bayes filter, respectively. Therefore, we introduce the architecture of Bayes filter to transformer decoders, resulting in Neural-Bayes motion decoder.


\section{Occupancy Prediction}
\label{app:occupancy}
We retain the occupancy prediction module from UniAD to ensure task consistency, where the \gls{BEV} feature serves as the query and learns from motion prediction features (output queries) through cross-attention. Consequently, we regard occupancy prediction in UniAD as a secondary task to perception and motion prediction, as it merely offers an alternative representation of upstream tasks.

DMAD achieves similar performance ($\text{IoU}_\text{near}$: 62.7\%, $\text{IoU}_\text{far}$: 39.8\%) to UniAD ($\text{IoU}_\text{near}$: 62.9\%, $\text{IoU}_\text{far}$: 39.6\%). The advances of DMAD in upstream tasks do not generalize to occupancy prediction. The reason could be that, by dividing semantics and motion, output features of the prediction module lack spatial information desired by occupancy prediction, such as size, whereas output features of UniAD's prediction module preserve the spatial information.






\section{Queue Length}
\label{app:queue}
We adopt a different queue length configuration from that of the original UniAD. As mentioned in \cref{subsec:training_configuration}, the rationale behind our decision is that reducing the queue length in stage 2 affects the performance, hindering the observation of negative transfer. \Cref{tab:queue_length} shows an ablation study of queue length on UniAD, presenting the performance drops by reduced queue length. 
As the training time scales almost linearly to the queue length, we opt for a queue length of 3 to reduce training time of each iteration.

\begin{table*}[t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}lllllllllllll@{}}
\toprule
\begin{tabular}[c]{@{}l@{}}Queue length\\ stage 1\end{tabular} & \begin{tabular}[c]{@{}l@{}}Queue length\\ stage 2\end{tabular} & NDS↑ & mAP↑ & AMOTA↑ & AMOTP↓ & IDS↓ & Lanes↑ & Drivable↑ & EPA↑ & minADE↓ & Avg. $L_2$↓ & Avg. Col.↓ \\ \midrule
3 & 3 & 0.491 & 0.377 & 0.354 & 1.34 & 1381 & 0.312 & 0.678 & \textbf{0.495} & 0.692 & 0.784 & 0.150 \\
5 & 3 & 0.499 & 0.381 & 0.362 & 1.34 & 956 & 0.313 & \textbf{0.692} & 0.492 & \textbf{0.655} & 0.656 & 0.130 \\
5 & 5 & \textbf{0.501} & \textbf{0.384} & \textbf{0.370} & \textbf{1.32} & \textbf{885} & \textbf{0.314} & 0.690 & \textbf{0.495} & 0.714 & \textbf{0.615} & \textbf{0.123} \\ \bottomrule
\end{tabular}
}
\caption{\textbf{Effect of queue length on UniAD.}}
\label{tab:queue_length}
\end{table*}


\section{Effect of Unimodal Prediction Horizon}
We conduct experiments on the number of future steps in unimodal prediction, as shown in \cref{tab:ablation_pred}. We observe that the unimodal prediction horizon influences the proportion of motion information within the \gls{BEV} feature, thereby impacting the performance of both semantic and motion tasks. A long prediction horizon degrades the performance of semantic tasks, as the \gls{BEV} feature is forced to prioritize motion learning in order to predict distant future outcomes. Experiments show that a prediction horizon of 6 seconds minimizes the collision rates, but performs worst in tracking.
Although this phenomenon can also be referred to as negative transfer, our approach is unable to address this specific type, as the \gls{BEV} feature is shared across all tasks and is expected to encapsulate both types of information. To balance motion and semantic information within the BEV feature, we set the prediction horizon to 4 seconds.


\begin{table*}[t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}llllllllllll@{}}
\toprule
\begin{tabular}[c]{@{}l@{}}Unimodal\\ pred. horizon\end{tabular} & NDS↑ & mAP↑ & AMOTA↑ & AMOTP↓ & IDS↓ & Lanes↑ & Drivable↑ & EPA↑ & minADE↓ & Avg. $L_2$↓ & Avg. Col.↓ \\ \midrule
2s & \textbf{0.516} & \textbf{0.404} & \textbf{0.400} & \textbf{1.30} & \textbf{695} & 0.321 & 0.691 & 0.534 & 0.735 & 0.679 & 0.220 \\
4s & 0.506 & 0.396 & 0.393 & \textbf{1.30} & 767 & 0.321 & 0.691 & \textbf{0.535} & \textbf{0.723} & \textbf{0.625} & 0.127 \\
6s & 0.504 & 0.396 & 0.384 & \textbf{1.30} & 751 & \textbf{0.322} & \textbf{0.700} & 0.525 & 0.743 & 0.629 & \textbf{0.117} \\ \bottomrule
\end{tabular}
}
\caption{\textbf{Effect of unimodal prediction horizon on DMAD.}}
\label{tab:ablation_pred}
\end{table*}



\section{Visualizations of Reducing Collisions Rates}
\label{app:vis}
We provide qualitative comparisons between DMAD and UniAD in \cref{fig:compare}, showcasing how the improved perception and prediction reduces collision rates. 


\begin{figure*}[t]
  \centering
  \begin{subfigure}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/2825.png}
    \caption{The collision of UniAD is because of an inaccurate prediction of the lead vehicle.}
    \label{fig:compare1}
  \end{subfigure}
  \begin{subfigure}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/5779.png}
    \caption{Both models make inaccurate predictions of the lead vehicle during the night. However, UniAD collides with the lead vehicle due to its aggressive driving policy.}
    \label{fig:compare2}
  \end{subfigure}
  \begin{subfigure}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/3095.png}
    \caption{An inaccurate detection (the detected position is too close to the ego-vehicle) causes yielding, and then colliding with another vehicle.}
    \label{fig:compare3}
  \end{subfigure}
  \begin{subfigure}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/4490.png}
    \caption{UniAD fails to detect the lead vehicle and collides with it.}
    \label{fig:compare4}
  \end{subfigure}
  \caption{\textbf{Qualitative comparison between DMAD and UniAD.} Each subfigure demonstrates a sample where UniAD encounters collision while DMAD does not.}
  \label{fig:compare}
\end{figure*}

