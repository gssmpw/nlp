\section{Experiments and Results}
\subsection{Datasets}
We perform experiments on two publicly available brain MRI segmentation datasets: FeTS 2021 \cite{FeTS1,FeTS2,FeTS3} for brain tumor segmentation and iSeg-2019\cite{iseg} for brain matter segmentation. 
For each sample in FeTS 2021, volumes of 3 modalities are used, including T1-weighted (T1), T2-weighted (T2) and Fluid-Attenuated Inversion Recovery (FLAIR). The volume size is $240 \times 240 \times 155$. Corresponding labels of edematous tissue (ED), enhancing tumor (ET), and necrotic tumor core (NCR) are manually segmented by clinical experts. This dataset is further split into 22 partitions by the provider, according to different institutions and information extracted from images. Thus, each partition can be seen as an individual domain. For each sample in iSeg-2019 dataset, volumes of 2 modalities are available, including T1 and T2. The volume size is $144 \times 192 \times 256$. Corresponding labels of white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) are manually segmented by clinicians. 

To provide sufficient tasks for experimental analysis, we reorganize the iSeg-2019 dataset into a collection of binary segmentation tasks on every available modality. We perform cross-modality and cross-center four-category segmentation on the FeTS 2021 dataset, and cross-modality and cross-task binary segmentation on the iSeg-2019 dataset.


\subsection{Training Details}
Since the goal of this work is to demonstrate the effectiveness of transferability-guided transfer risk map on mitigating the potential local negative transfer during the fine-tuning process rather than striving for cutting-edge performance on medical image segmentation tasks, we employ the same model architecture for all experiments presented in this paper, a classic 2D U-Net\cite{unet}.  
For a fair comparison with all methods, we use the same input size, pre-processing strategy, and training loss for all methods. We adopt an Adam optimizer with a learning rate of 0.0001 to fine-tune the source model on the target data for 5k iterations. As for the transfer learning strategy, we follow the most common way which is pre-training the model on a source task and fine-tuning it on a target task. During the fine-tuning stage, the encoder is frozen and only the parameters of the decoder are updated. 
\input{table/result1}
\subsection{Evaluation on Medical Image Segmentation Task}
We apply our transferability-guided transfer risk weighted fine-tuning method on medical image segmentation tasks and compare the proposed method with the commonly used vanilla fine-tuning, class weighted fine-tuning\cite{class-weight}, and transferability map weighted fine-tuning\cite{10222912}. 

The iSeg-2019 dataset for brain matter segmentation closely resembles natural image segmentation tasks, exhibiting larger and more contiguous segmented regions, which demand the model's coarse-grained capability for object shape recognition. The FeTS 2021 dataset for brain tumor segmentation encompasses three distinct pathological labels that are characterized by small, discontinuous regions with hierarchical containment relationships. This demands a model endowed with a heightened fine-granularity for discerning local textures.

For iSeg-2019 dataset, quantitative comparisons shown in Table~\ref{result1} demonstrate that our proposed fine-tuning consistently outperforms the vanilla fine-tuning, class weighted fine-tuning and transferability map weighted fine-tuning in all transfer experiments. This approach can result in a notable increase in the Dice coefficient, reaching up to 3.49\%, with an average enhancement of 1.6\% and an overall gain of 1.81\%. 

For FeTS 2021 dataset, as shown in Table~\ref{result2}, our proposed fine-tuning method surpasses vanilla fine-tuning, class weighted fine-tuning and transferability map weighted fine-tuning across a spectrum of transfer experiments. It achieves a remarkable maximum increase of 8.3\% in the Dice coefficient, with an average enhancement of 2\% and an overall gain of 4.37\%.
\input{table/result2}
\input{table/result3}
\subsection{Study on Few- shot Scenarios}
We conducted experiments under few-shot scenarios on the iSeg-2019 dataset, testing the performance of our proposed method when the target task had only 400, 100, and 50 annotated 2d slice images, respectively. As demonstrated in the Table~\ref{result3}, employing one task as the source model, we conducted transfer fine-tuning on the remaining five tasks. Irrespective of the quantity of target images, our method consistently outperformed vanilla fine-tuning. Remarkably, even with a minimal training slice sample size of 50, our method still achieved a 2.4\% improvement and a 2.9\% gain in Dice score in average. These experimental findings validate the robustness of our approach in a few-shot context. %highlighting its capacity to facilitate model adaptation and refinement during the fine-tuning phase by mitigating the risk of local negative transfer.
\subsection{Effectiveness of Negative Transfer Mitigation}
To investigate the actual impact of local transfer risk on the fine-tuning phase, we visualized the transfer risk maps before and after applying our fine-tuning method between the source model and the target data. As shown in Fig.~\ref{transferability}, it shows the transfer risk maps before and after the fine-tuning phase between the pre-trained source model and the target tasks in the iSeg-2019 and FeTS 2021 dataset. Darker colors indicate lower transfer risk, while brighter colors signify higher transfer risk. It is evident that our fine-tuning approach has effectively condensed the extensive regions of high transfer risk into well-defined, smaller segments, concurrently reducing the transfer hardness.
\input{figures/transferability}
Additionally, we observed that regions exhibiting the most significant transfer hardness have undergone a transformation, from resembling segmentation contours of source task to the precise segmentation patterns of target tasks. The comparison of transfer risk maps pre- and post-fine-tuning validates the efficacy of our proposed approach in mitigating the risk of potential negative transfer. The simple weighting procedure not only reduces the overall transfer hardness but also refines the scope of potential negative transfer risk regions during model training, effectively redirecting the model's attention towards pertinent features.