\section{Methodology}
\subsection{Problem Definition}
In a typical source free transfer learning setting, We transfer the domain knowledge with only a pre-trained source model $\theta_S$ and without access to source data. Specifically, we are given a pre-trained model $\theta_S$ corresponding to a source task $S$ with $\mathcal{N}_S$ labeled samples $\{x_s^i,y_s^i \}_{i=1}^{\mathcal{N}_S}$ where $x_s^i \in \mathcal{X}_S, y_s^i \in \mathcal{Y}_S$, and also a target task $T$ with $\mathcal{N}_T$ labeled samples $\{x_t^i,y_t^i \}_{i=1}^{\mathcal{N}_T}$ where $x_t^i \in \mathcal{X}_T, y_t^i \in \mathcal{Y}_T$. $\mathcal{X}_S$, $\mathcal{X}_T$ represent the input image and $\mathcal{Y}_S$, $\mathcal{Y}_T$ represent their corresponding labels. The problem goal is to transfer the knowledge learned from the source to target that mitigate the negative transfer and improve the performance within the target domain.

Transferability reveals how easy it is to transfer knowledge learned from a source task to a target task. We can obtain the transferability from the source task to the target task by using the testing data of the target task $(\mathcal{X}_T, \mathcal{Y}_T)$ to calculate the expected log-likelihood of the source pretrained model $\theta_S$:
\begin{equation}
\label{Trans}
    Trs(\theta_S,\mathcal{X}_T,\mathcal{Y}_T) = \underset{x_i,y_i \in \mathcal{X}_T \times \mathcal{Y}_T}{\mathbb{E}} [logP( y_i\mid x_i;\theta_S)]
\end{equation}
\subsection{Transferability Estimation on Segmentation Task}
LEEP score is a more general metric for classification task, which is defined by the average log-likelihood of the expected empirical predictor, which predicts the dummy label distributions for target data in source label space and then computes the empirical conditional distribution of target label given the dummy source label. We select LEEP for our transferability estimation metric due to its high computational efficiency, simplicity and superior performance. We define the LEEP method as follows:
\begin{equation}
\label{LEEP}
    \operatorname{LEEP}(\theta_S,\mathcal{D}) = \frac{1}{n}\displaystyle\sum_{i = 1}^{n}\log(\displaystyle\sum_{z \in \mathcal{Z}}\hat{P}(y_i\mid z) \theta_S(x_i))
\end{equation}
where $\theta_S$ and $\mathcal{D} = \{(x_1, y_1),\cdots,(x_n, y_n)\}$ denote the source model and target dataset, respectively. $\mathcal{Z}$ denotes the dummy source label, $\hat{P}(\cdot\vert\cdot)$ denotes the empirical conditional distribution.

Then we adapt the LEEP score to work on medical segmentation task. The global feature vector extracted by source pretrained model can be precisely decomposed into distinct pixel-wise feature, each indexed according to its specific pixel coordinates. Subsequently, classification will be conducted on each pixel in input image, and output label will also be obtained at pixel-level, and transferability scores can be computed over the pixel-wise features treating each pixel as an instance of classifying. Now, pixel-level LEEP score for segmentation task can be defined as,
\begin{equation}
\label{LEEP-trans}
\resizebox{\columnwidth}{!}{%
    $\displaystyle % 进入数学模式
    \operatorname{LEEP}(\theta_S,\mathcal{D}) = 
    \frac{1}{n} \sum_{i = 1}^{n} \left[ 
        \log \prod_{j = 1}^{W} \prod_{k = 1}^{H} \sum_{z \in \mathcal{Z}} 
        \hat{P}(y_i^{j,k} \mid z^{j,k}) \theta_S(x_i^{j,k})
    \right]
    $% 结束数学模式
}
\end{equation}
where $x_i \in \mathbb{R}^{W\times H\times1}$, $y_i \in \{0,1\}^{W\times H\times C_t}$. Here $W$, $H$ denote the width and height of image, and $C_t$ represent the number of categories of the target task.
\subsection{Transferability Guided Transfer Risk Weighted Fine-tuning}
After adapting the transferability metric to medical segmentation task, we can obtain pixel-level transferability, which characterizes the hardness of transferring knowledge at a local, granular level. Based on pixel-level transferability, we compute a transfer risk map that quantifies the potential risks of negative transfer associated with them. Furthermore, we propose a novel weighted fine-tuning approach guided by the transfer risk map. 

In particular, we define a pixel-level transferability map $t \in \mathbb{R}^{W\times H}$, and $t^{j,k}$ represents the transferability score at a pixel coordinate $(j,k)$, where $j \in [1,W]$, $k \in [1,H]$. Formally,
\begin{equation}
\label{transferability map}
   t^{j,k} = \operatorname{LEEP}(\theta_S,\{x_i^{j,k},y_i^{j,k} \}_{i=1}^{N}).
\end{equation}
Subsequently, we adjust the transferability map to scale within the interval of $[0, 1]$, with values approaching 1 representing pixels of higher transfer hardness and values near 0 denoting pixels with lesser transfer hardness. To further augment the model's emphasis on regions with increased transfer difficulty, we implement an exponential scaling with a base of 10 to introduce some nonlinearity. The transfer risk map $w \in \mathbb{R}^{W\times H}$ is defined as
\begin{equation}
\label{standerd transferability map}
   t_s^{j,k} = \frac{t^{j,k}-\min(t)}{\max(t)-\min(t)}
\end{equation}
\begin{equation}
\label{transferability risk map}
    w^{j,k} = 10^{t_s^{j,k}}.
\end{equation}
Then, we can define the transfer risk weighted loss, where
\begin{equation}
\label{transferability risk loss}
   L = \frac{\sum_{i = 1}^{N}\sum_{j = 1}^{W}\sum_{k = 1}^{H}w^{j,k}loss(\theta_S(x_i^{j,k},y_i^{j,k}))}{\sum_{i = 1}^{N}\sum_{j = 1}^{W}\sum_{k = 1}^{H}1\{y_i^{j,k} \not= 0\}}.
\end{equation}
It is important to highlight that we calculate loss values across all pixels but average them exclusively over the foreground pixels to alleviate the adverse effects of class imbalance during the fine-tuning phase. Such a method effectively prevents well-learned background pixels from diluting the loss value.
