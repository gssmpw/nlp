\section{Related work}
\textbf{Backdoor attacks.}
DNNs confront a significant security threat from backdoor attacks, which can undermine the reliability of these models. In such attacks, adversaries manipulate the model's output by embedding a specific trigger within the input data. The victim model behaves normally for benign inputs but produces attacker-controlled outcomes when it detects the predefined trigger. Backdoor attacks are typically categorized into two types based on the nature of their triggers: static-pattern and dynamic-pattern attacks. Early research, such as BadNets \citep{gu2019badnets} introduced fixed-pattern triggers—like a white square placed in a corner of an image—that could be easily identified and mitigated through visual inspection or algorithmic detection.  Consequently, subsequent studies have shifted focus towards more covert methods of injecting triggers. For instance, the Blended approach \citep{chen2017targeted}  integrates the trigger imperceptibly into the host image, enhancing its stealth. Dynamic-pattern backdoor attacks represent a more advanced form of this threat. Methods like  WaNet \citep{nguyen2021wanet}, LF \citep{zeng2021rethinking}, and SSBA \citep{li2021invisible} generate sample-specific triggers that blend seamlessly with the input, making them significantly harder to detect. Furthermore, some techniques aim to preserve the relationship between the semantic information of the input and the associated label, exemplified by works such as LC \citep{shafahi2018poison} and SIG \citep{barni2019new}, thereby increasing the subtlety and effectiveness of the attack. 


\textbf{Backdoor defenses.}
As backdoor attacks garner growing attention from the research community, a comprehensive array of strategies has emerged to safeguard machine learning systems across their entire lifecycle \cite{wu2023defenses}. These defense mechanisms can be categorized into three primary phases: pre-training, in-training, and post-training.
%Pre-training defenses are designed to identify and eliminate poisoned data samples before they influence the model. Notable methods include AC \citep{chen2019detecting}, Confusion Training \citep{qi2023towards}, and VDC \citep{zhu2023vdc}. In-training defenses aim to mitigate the effects of backdoors during the training process itself. This category encompasses techniques such as filter-and-unlearn procedures like Anti-Backdoor Learning (ABL) \cite{li2021anti}, self-supervised learning approaches such as DBD \cite{huang2022backdoor}, and injecting defensive backdoors, exemplified by PDB \cite{wei2024mitigating} and NAB \cite{liu2023beating}.

This paper centers on post-training defenses aimed at eliminating backdoors within pre-trained models. To this end, a prominent strategy involves the identification and removal of neurons linked to backdoor activities. Techniques such as FP \cite{liu2018fine}, ANP \cite{wu2021adversarial} and CLP \cite{zheng2022data}. An alternative method is to fine-tune the affected model to diminish backdoor influence. Key strategies here include NC \cite{wang2019neural}, i-BAU \cite{zeng2022adversarial}, NPD \cite{zhu2023neural}, and SAU \cite{wei2024shared}, which leverage adversarial techniques to simulate potential backdoor triggers, followed by fine-tuning to enhance the model's resistance against these reconstructed poisoned samples, thereby cleansing it from backdoor effects. In addition, NAD \cite{li2021neural}  utilizes a teacher network to guide the fine-tuning of a compromised student network, aiding in the mitigation of backdoors. The FT-SAM approach \cite{Zhu_2023_ICCV} and FST \cite{min2024towards} further advances fine-tuning  for backdoor mitigation.

Despite their notable defensive success, most SOTA purification techniques operate under the assumption that an auxiliary dataset is accessible, which can be a significant limitation in practical applications. To address this challenge, some researchers have turned their attention to filtering a clean dataset from poisoned dataset~\cite{zeng2022sift}, reducing the size of required auxiliary dataset~\cite{chai2022oneshot,li2023reconstructive, Zhu_2023_ICCV}, or even developing data-free methods~\cite{zheng2022data,hong2023revisiting}. While effective against certain attacks, these methods often encounter limitations when dealing with diverse models or more sophisticated attacks~\cite{dunnett2024countering,wu2024backdoorbench}. In summary, robust backdoor purification continues to rely heavily on the availability of adequate auxiliary data, underscoring the urgent need to develop practical and adaptive purification techniques.


