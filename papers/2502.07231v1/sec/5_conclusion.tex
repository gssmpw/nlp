\section{Conclusion}
In this paper, we address a critical yet underexplored problem in backdoor purification, examining how different types of auxiliary datasets impact the effectiveness of state-of-the-art backdoor purification methods. Our findings underscore the pivotal role of auxiliary datasets in strengthening defenses against backdoor attacks, while also highlighting that not all datasets contribute equally to the purification process. Specifically, we found that in-distribution data preserves model utility but is less effective at neutralizing backdoors. In contrast, introducing entirely new datasets can cause the model to forget critical knowledge. 
%
To address these challenges, we propose Guided Input Calibration, an innovative approach designed to improve the efficiency of backdoor purification. GIC aligns the characteristics of auxiliary datasets with those of the original training set through transformations guided by the intrinsic properties of the compromised model. Extensive experiments demonstrate that GIC significantly improves the effectiveness of backdoor mitigation across various auxiliary datasets, making it a valuable tool for strengthening ML security in practical applications.


\textbf{Limitations and future work.} While our method offers promising results, we acknowledge several limitations and suggest avenues for future research. First, although GIC facilitates better alignment between auxiliary and original datasets, it still relies on some degree of similarity or the availability of relevant auxiliary dataâ€”a condition that may not always be feasible. Second, our method has been primarily tested on static ML models, leaving questions about its applicability to online learning environments or continuously updated models. For future research, we advocate exploring strategies that can function effectively without auxiliary datasets or with minimal data needs. Additionally, further investigation is needed into the role of auxiliary datasets for backdoor defense in other stages of ML systems, such as the pre-training and in-training stages.

\vspace{-0.1in}
\section*{Impact Statement}
Our primary objective is to develop a robust framework capable of effectively mitigating the complex threats associated with backdoor attacks, ensuring the integrity and dependability of applications based on ML. This work has significant positive implications for society, contributing to the protection of ML systems against malicious exploitation and promoting a safer digital environment.