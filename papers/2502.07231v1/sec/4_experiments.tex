\section{Enhancing Backdoor Purification with Guided Input Calibration}
\subsection{Guided Input Calibration}
\textbf{Motivation.} From our earlier analysis, we identified that auxiliary datasets, especially those that introduce significant semantic shifts, can degrade the model’s clean accuracy and destabilize attack success rates. This is particularly problematic in backdoor defense, where the goal is to both \textbf{remove the backdoor} and \textbf{maintain high clean accuracy}. Therefore, it is crucial that auxiliary datasets used for purification do not introduce large deviations from the model's training data, especially when substantial updates to model parameters are involved. 


\textbf{Methodology.} The core of Guided Input Calibration is a learnable transformation function $g$, which adjusts the auxiliary dataset $\mathcal{D}_{aux}$ to better align with the model's expectations. The transformation function $g$ is designed to ensure that the adjusted data closely resembles the original training data distribution. This transformation is flexible and can take various forms, such as learnable perturbations \cite{song2023deep}, visual prompts \cite{bahng2022exploring}, or spatial transformations \cite{xiao2018spatially}.

To ensure broad applicability, we consider a scenario where no clean samples from the original training dataset are available for reference. This scenario is particularly challenging due to the lack of clean features, and it reflects practical settings. Instead of directly using clean data, the optimization of $g$ is guided by the \textbf{victim model}. Since the victim model is trained to predict clean samples correctly, the model’s output provides guidance for the calibration process. Specifically, the optimization objective for $g$ is defined as follows:
\begin{equation}
\min_{g:\|g(x_i)-x_i\|_p\leq \delta}  \sum_{(x_i,y_i)\in\mathcal{D}_{aux}} \ell(f(g(x_i)), y_i),
\end{equation}
where $\ell$ is the classification loss function (e.g., cross-entropy loss), and $\delta$ is a hyperparameter controlling the distance between the original sample $x_i$ and its calibrated version $g(x_i)$. The introduction of $\delta$ serves as a constraint on the transformation process, ensuring that $g(x)$ remains close to the original sample and does not transform the input into arbitrary patterns, especially those resembling trigger patterns with significant modifications. We empirically demonstrate in the \app that the transformed images do not exhibit characteristics of poisoned data, even for attacks with minor image modifications.

Through this objective, the transformation function $g$ is optimized to minimize classification error, effectively aligning the auxiliary data with the victim model's expectations.

% \textbf{Overall Process:} The backdoor purification process with GIC can be summarized as follows:

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figs/pip.pdf}
%     \caption{pip}
%     \label{fig:pip}
% \end{figure}


\paragraph{Theoretical analysis.}
Here, we provide a theoretical foundation for the GIC. To simplify the analysis, we focus on a binary $(0-1)$ classification model $f$, which consists of a feature extractor $\phi$ and a linear classifier $W$. Given the training dataset $\mathcal{D}_{tr}$, the model $f$ is trained to minimize the Binary Cross-Entropy (BCE) loss:  
\begin{equation}
    \label{bce1}
    \min_{f} -\frac{1}{|\mathcal{D}_{tr}|} \sum_{(x_i, y_i) \in \mathcal{D}_{tr}} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right),
\end{equation}  
where $p_i = P(f(x_i) = 1) = \sigma(W^\top \phi(x_i))$ and $\sigma$ is the sigmoid activation function.  

We assume that the feature vectors have finite norms and let \( M =\max_{x} \|\phi(x)\|\) denote the largest norm across all feasible inputs. This assumption is practical and commonly encountered in machine learning models, especially neural networks. In practice, techniques such as input normalization, regularization (e.g., weight decay), and normalization layers (e.g., batch normalization) ensure that the features produced by the model are bounded, preventing arbitrary growth during training and inference.

In the GIC framework, the transformation $g(x)$ is trained to increase the prediction confidence of $f(g(x))$ for its correct label. This optimization naturally encourages $g(x)$ to resemble one of the samples in $\mathcal{D}_{tr}$, as training samples are the most confident predictions of $f$ after training. Let $x' \in \mathcal{D}_{tr}$ be such a sample whose confidence matches that of $g(x)$, i.e., $P(f(g(x)) = 1) = P(f(x') = 1)$.  

We now present the following theorem:  

\begin{theorem}
\label{thm}
Consider a model $f$ trained by solving the optimization problem in Equation~\ref{bce1}.  Under the assumption of bounded feature norm, the distance between the feature vectors $\phi(g(x))$ and $\phi(x')$ is bounded as:  
\[
\|\phi(g(x)) - \phi(x')\|^2 \leq 4M^2 - \frac{4}{\|W\|^2} \left( \log\left( \frac{1 - p}{p} \right) \right)^2,
\]  
where $p = P(f(g(x)) = 1)= P(f(x') = 1)$.
\end{theorem}

This theorem shows that as $g(x)$ is optimized to increase the prediction confidence $p$ for its correct label, the transformation encourages $g(x)$ to align with training sample $x'$ that has a similar high-confidence prediction. Specifically, the distance between the feature representations $\phi(g(x))$ and $\phi(x')$ is bounded by the equation above, which ensures that the transformation $g(x)$  does not drastically deviate from the training dataset. This alignment ensures that the auxiliary dataset, transformed by $g(x)$, produces features that are similar to those in the original training dataset. By promoting such alignment, GIC effectively ensures that the transformed auxiliary dataset remains consistent with the original training distribution, benefiting backdoor purification techniques. 

\subsection{Experiments}
\input{tables/table_main}
In this section, we conduct a series of experiments to demonstrate the efficacy of GIC, testing it against multiple backdoor attacks and across a range of defense methods.

\textbf{Experiment settings.} In our experiments, we follow the attack and defense methodologies outlined in Section~\ref{sec::impact}. For the GIC method, we use a simple perturbation function \( g(x) = x + \epsilon \), where \( \epsilon \) is a sample-specific learnable perturbation. The perturbations are optimized using the objective defined in Equation~\ref{bce1}. To prevent the learning of unintended noise or malicious patterns with large norms, we constrain \( \epsilon \) with an $L_{\infty}$ norm less than 0.1. The perturbations are learned using the Adam optimizer with a learning rate of 0.1 over 100 steps, starting from zero initialization. This setup ensures that the perturbations remain within reasonable bounds, optimizing their alignment with the victim model’s expectations. The perturbation function and hyperparameters (e.g., learning rate and range of \( \epsilon \)) can be further fine-tuned to optimize GIC's performance, and we leave a more comprehensive study of them for future work. To maintain clarity, we present the most representative experiments in the main text, focusing on the key insights into GIC's performance across different configurations. The remaining experiments, which explore additional datasets, auxiliary data types, models, and attack-defense combinations, are deferred to the \app. 


\input{tables/table_defense}
\textbf{Effectiveness of GIC.} Table~\ref{gic_badnet} presents the results of applying GIC to six representative defense methods against the BadNets attack on PreAct-ResNet18. The findings demonstrate the significant impact of GIC in enhancing backdoor purification across various auxiliary datasets and defense methods. For all tested defenses (ANP, NPD, FT, FT-SAM, FST, and SAU), GIC consistently improves the model's ACC. Notably, external datasets benefit greatly, with improvements of up to +8.64\% in FT and +7.33\% in FT-SAM. This indicates that GIC effectively calibrates auxiliary datasets to align more closely with the victim model's training distribution, thus improving their utility in backdoor purification.

However, while GIC improves ACC, it also introduces variations in ASR. In some cases, ASR increases, while in others, it decreases, indicating a complex tradeoff between ACC and ASR. As discussed earlier, shifts in the feature space can help in backdoor purification, but overly large shifts can harm the model’s utility. GIC's alignment of the auxiliary dataset with the training distribution reduces the benefits of these shifts, potentially leading to higher ASR, which comes at the cost of ACC improvement.


\input{tables/table_data}



\textbf{Effectiveness over various attacks and datasets.} 
Having evaluated GIC’s performance across various defense methods, we next investigate its effectiveness across different attacks and datasets. We evaluate GIC with the powerful FT-SAM purification technique using the most challenging external dataset $\mathcal{D}_{ext}$ for CIFAR-10 and $\mathcal{D}_{syn}$ for other datasets against various attacks. As summarized in Table~\ref{gic_defense} and Table~\ref{gic_data}, GIC consistently improves defense effectiveness, yielding significant improvements in ACC with minimal variations in ASR.

\textbf{Overall}, these results highlight GIC's adaptability and effectiveness in improving auxiliary dataset utility for backdoor defense, particularly when working with datasets exhibiting distributional shifts. GIC offers a simple yet powerful method for bridging the gap between auxiliary and training data, leading to more reliable backdoor mitigation.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/tsne_2.pdf}
    % \vspace{-0.15in}
    \caption{T-SNE visualization of feature representations for the external dataset, showing the transformation before and after applying Guided Input Calibration.}
    \label{tsnep2}
    % \vspace{-0.15in}
\end{figure}

\textbf{Understanding GIC.}  
To understand GIC, we visualize the features of external dataset before and after applying GIC. As shown in Figure~\ref{tsnep2}, the feature space analysis provides further insight into the role of GIC in aligning auxiliary data with the original training distribution. Without GIC, external data shows significant divergence from seen data in the feature space, highlighting the challenge of integrating out-of-distribution samples for backdoor purification. However, after applying GIC, the feature representations of the external data closely align with those of the seen data, demonstrating GIC’s effectiveness in transforming auxiliary samples to resemble the training data. This alignment confirms the theoretical foundation of GIC, as discussed in Theorem~\ref{thm}, where we showed that the transformation function \( g \) minimizes the discrepancy between auxiliary and training samples. 
%
The fact that GIC leads to nearly identical feature representations indicates that the optimization of \( g \) successfully adjusts the auxiliary data to match the high-confidence predictions of the victim model, ensuring better compatibility for backdoor purification. This feature space alignment not only enhances the purification process but also reduces the risk of reinforcing backdoor patterns by making the auxiliary data more consistent with the clean training distribution.
